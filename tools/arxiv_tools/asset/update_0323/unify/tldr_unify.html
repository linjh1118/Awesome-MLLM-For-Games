
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models</td>
      <td>Recent advancements in unified multimodal understanding and visual generation<br>(or multimodal generation) models have been hindered by their quadratic<br>computational complexity and dependence on large-scale training data. We<br>present OmniMamba, the first linear-architecture-based multimodal generation<br>model that generates both text and images through a unified next-token<br>prediction paradigm. The model fully leverages Mamba-2's high computational and<br>memory efficiency, extending its capabilities from text generation to<br>multimodal generation. To address the data inefficiency of existing unified<br>models, we propose two key innovations: (1) decoupled vocabularies to guide<br>modality-specific generation, and (2) task-specific LoRA for<br>parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage<br>training strategy to mitigate data imbalance between two tasks. Equipped with<br>these techniques, OmniMamba achieves competitive performance with JanusFlow<br>while surpassing Show-o across benchmarks, despite being trained on merely 2M<br>image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba<br>stands out with outstanding inference efficiency, achieving up to a 119.2 times<br>speedup and 63% GPU memory reduction for long-sequence generation compared to<br>Transformer-based counterparts. Code and models are released at<br>https://github.com/hustvl/OmniMamba</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | OmniMambaï¼šé«˜æ•ˆä¸”ç»Ÿä¸€çš„è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œè·¨æ¨¡æ€ç†è§£å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œå¤æ‚çš„è®¡ç®—ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹é€šå¸¸ä¾èµ–äºTransformeræ¶æ„ï¼Œè¿™å¯¼è‡´äº†äºŒæ¬¡è®¡ç®—å¤æ‚æ€§å’Œè¾ƒæ…¢çš„æ¨ç†é€Ÿåº¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§£è€¦è¯æ±‡è¡¨<br>OmniMambaé‡‡ç”¨äº†è§£è€¦è¯æ±‡è¡¨çš„è®¾è®¡ï¼Œä¸ºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€åˆ†åˆ«ä½¿ç”¨ç‹¬ç«‹çš„è¯æ±‡è¡¨ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ ç‰¹å®šæ¨¡æ€çš„ç”Ÿæˆï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä»»åŠ¡ç‰¹å®šLoRA<br>ä¸ºäº†æé«˜æ¨¡å‹å¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”æ€§ï¼ŒOmniMambaå¼•å…¥äº†ä»»åŠ¡ç‰¹å®šçš„LoRAæ¨¡å—ã€‚è¿™äº›æ¨¡å—è¢«åº”ç”¨äºMamba-2å±‚çš„è¾“å…¥æŠ•å½±ä¸­ï¼Œå¹¶åœ¨æ‰§è¡Œç‰¹å®šä»»åŠ¡æ—¶æ¿€æ´»ç›¸åº”çš„LoRAè·¯å¾„ï¼Œä»è€Œæé«˜äº†æ•°æ®æ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§£è€¦è®­ç»ƒç­–ç•¥<br>OmniMambaé‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„è§£è€¦è®­ç»ƒç­–ç•¥ï¼Œä»¥è§£å†³ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é¢„è®­ç»ƒï¼Œç”¨äºæ¨¡å—çš„åˆå§‹åŒ–å’Œæ¨¡æ€å¯¹é½ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ç»Ÿä¸€çš„å¾®è°ƒï¼Œç”¨äºå¤šä»»åŠ¡è®­ç»ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨è·¨æ¨¡æ€ç†è§£å’Œè§†è§‰ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒOmniMambaå–å¾—äº†ä¸JanusFlowç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶è¶…è¿‡äº†Show-oã€‚æ­¤å¤–ï¼ŒOmniMambaåœ¨æ¨ç†æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸åŸºäºTransformerçš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾119.2å€çš„åŠ é€Ÿå’Œ63%çš„GPUå†…å­˜å‡å°‘ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OmniMambaçš„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ä¸ºè·¨æ¨¡æ€ç†è§£å’Œè§†è§‰ç”Ÿæˆæ¨¡å‹çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚è§£è€¦è¯æ±‡è¡¨å’Œä»»åŠ¡ç‰¹å®šLoRAçš„è®¾è®¡å¯ä»¥æé«˜æ¨¡å‹çš„æ•°æ®æ•ˆç‡å’Œé€‚åº”æ€§ï¼Œè€Œè§£è€¦è®­ç»ƒç­–ç•¥å¯ä»¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚è¿™äº›åˆ›æ–°ç‚¹å¯¹äºå¼€å‘é«˜æ•ˆä¸”ç»Ÿä¸€çš„è·¨æ¨¡æ€æ¨¡å‹å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Dual Diffusion for Unified Image Generation and Understanding</td>
      <td>Diffusion models have gained tremendous success in text-to-image generation,<br>yet still lag behind with visual understanding tasks, an area dominated by<br>autoregressive vision-language models. We propose a large-scale and fully<br>end-to-end diffusion model for multi-modal understanding and generation that<br>significantly improves on existing diffusion-based multimodal models, and is<br>the first of its kind to support the full suite of vision-language modeling<br>capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and<br>recent advances in discrete diffusion language modeling, we leverage a<br>cross-modal maximum likelihood estimation framework that simultaneously trains<br>the conditional likelihoods of both images and text jointly under a single loss<br>function, which is back-propagated through both branches of the diffusion<br>transformer. The resulting model is highly flexible and capable of a wide range<br>of tasks including image generation, captioning, and visual question answering.<br>Our model attained competitive performance compared to recent unified image<br>understanding and generation models, demonstrating the potential of multimodal<br>diffusion modeling as a promising alternative to autoregressive next-token<br>prediction models.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŒå‘æ‰©æ•£æ¨¡å‹ï¼šç»Ÿä¸€å›¾åƒç”Ÿæˆä¸ç†è§£<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨è§†è§‰ç†è§£ä»»åŠ¡æ–¹é¢ä»ç„¶è½åäºè‡ªå›å½’è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æœ¬æ–‡æ—¨åœ¨æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€ç«¯åˆ°ç«¯çš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆï¼Œå¹¶æ˜¾è‘—æå‡ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒå‘æ‰©æ•£æ¨¡å‹<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Dual Diffusion Transformer (D-DiT) çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºå¤šæ¨¡æ€æ‰©æ•£ Transformer (MM-DiT) æ¶æ„ï¼Œå¹¶è¿›è¡Œäº†ä¿®æ”¹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å›¾åƒå’Œæ–‡æœ¬ä¸¤ç§æ¨¡æ€ä¸Šè¾“å‡ºæ‰©æ•£ç›®æ ‡ã€‚D-DiT æ¨¡å‹é€šè¿‡è”åˆè®­ç»ƒå›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ï¼Œå®ç°äº†å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰å¤šç§åŠŸèƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè”åˆæŸå¤±å‡½æ•°<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ã€ä¼˜é›…ä¸”æ˜“äºå®ç°çš„è”åˆæŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶è®­ç»ƒå›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ã€‚è¯¥æŸå¤±å‡½æ•°é€šè¿‡æœ€å°åŒ–å›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆè¯¯å·®ï¼Œå®ç°äº†ä¸¤ç§æ¨¡æ€çš„è”åˆå»ºæ¨¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒD-DiT æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŒå‘æ‰©æ•£æ¨¡å‹ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>*   **è”åˆå»ºæ¨¡**ï¼šé€šè¿‡è”åˆè®­ç»ƒå›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥å®ç°æ›´å…¨é¢çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚<br>*   **åŒå‘æ‰©æ•£**ï¼šåŒå‘æ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å›¾åƒå’Œæ–‡æœ¬ä¸¤ç§æ¨¡æ€ï¼Œå¹¶å®ç°æ›´çµæ´»çš„é‡‡æ ·æ–¹å¼ã€‚<br>*   **è”åˆæŸå¤±å‡½æ•°**ï¼šè”åˆæŸå¤±å‡½æ•°å¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼æ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶æé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>æœ¬æ–‡æå‡ºçš„åŒå‘æ‰©æ•£æ¨¡å‹ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚è¯¥æ¨¡å‹æœ‰æœ›åœ¨å›¾åƒç”Ÿæˆã€å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</td>
    </tr>
    <tr>
      <th>2</th>
      <td>LMFusion: Adapting Pretrained Language Models for Multimodal Generation</td>
      <td>We present LMFusion, a framework for empowering pretrained text-only large<br>language models (LLMs) with multimodal generative capabilities, enabling them<br>to understand and generate both text and images in arbitrary sequences.<br>LMFusion leverages existing Llama-3's weights for processing texts<br>autoregressively while introducing additional and parallel transformer modules<br>for processing images with diffusion. During training, the data from each<br>modality is routed to its dedicated modules: modality-specific feedforward<br>layers, query-key-value projections, and normalization layers process each<br>modality independently, while the shared self-attention layers allow<br>interactions across text and image features. By freezing the text-specific<br>modules and only training the image-specific modules, LMFusion preserves the<br>language capabilities of text-only LLMs while developing strong visual<br>understanding and generation abilities. Compared to methods that pretrain<br>multimodal generative models from scratch, our experiments demonstrate that,<br>LMFusion improves image understanding by 20% and image generation by 3.6% using<br>only 50% of the FLOPs while maintaining Llama-3's language capabilities. We<br>also demonstrate that this framework can adapt existing vision-language models<br>with multimodal generation ability. Overall, this framework not only leverages<br>existing computational investments in text-only LLMs but also enables the<br>parallel development of language and vision capabilities, presenting a<br>promising direction for efficient multimodal model development.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LMFusionï¼šèµ‹äºˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè¿™éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼Œç›´æ¥åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬è¯­è¨€æ¨¡å‹ä¼šå¯¼è‡´å…¶è¯­è¨€å¤„ç†èƒ½åŠ›çš„æ˜¾è‘—ä¸‹é™ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>LMFusion æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨é¢„è®­ç»ƒçš„æ–‡æœ¬è¯­è¨€æ¨¡å‹ Llama-3 ä¸Šå¼•å…¥é¢å¤–çš„å¹¶è¡Œ Transformer æ¨¡å—æ¥èµ‹äºˆå…¶å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚LMFusion çš„å…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¡ç®—å¤ç”¨<br>LMFusion åˆ©ç”¨ç°æœ‰è®¡ç®—èµ„æºï¼Œæ— éœ€åœ¨æ–‡æœ¬æ•°æ®ä¸Šé‡æ–°è®­ç»ƒï¼Œä»è€Œæ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ€§èƒ½ä¿ç•™å’Œè¿ç§»<br>LMFusion é€šè¿‡å†»ç»“æ–‡æœ¬æ¨¡å—å¹¶ä»…å¾®è°ƒå›¾åƒæ¨¡å—ï¼Œåœ¨ä¿ç•™é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›çš„åŒæ—¶ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒLMFusion åœ¨å›¾åƒç†è§£å’Œå›¾åƒç”Ÿæˆæ–¹é¢åˆ†åˆ«æé«˜äº† 20% å’Œ 3.6%ï¼ŒåŒæ—¶ä»…ä½¿ç”¨äº† 50% çš„ FLOPsï¼Œå¹¶ä¿æŒäº† Llama-3 çš„è¯­è¨€èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LMFusion çš„æ¡†æ¶ä¸ä»…é€‚ç”¨äºæ–‡æœ¬è¯­è¨€æ¨¡å‹ï¼Œè¿˜å¯ä»¥æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæé«˜å…¶å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLMFusion çš„è®¾è®¡ç†å¿µå¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è§†é¢‘ç”Ÿæˆå’ŒéŸ³é¢‘ç”Ÿæˆã€‚</td>
    </tr>
    <tr>
      <th>3</th>
      <td>MetaMorph: Multimodal Understanding and Generation via Instruction Tuning</td>
      <td>In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a<br>simple and effective extension to visual instruction tuning that enables a<br>pretrained LLM to quickly morph into an unified autoregressive model capable of<br>generating both text and visual tokens. VPiT teaches an LLM to predict discrete<br>text tokens and continuous visual tokens from any input sequence of image and<br>text data curated in an instruction-following format. Our empirical<br>investigation reveals several intriguing properties of VPiT: (1) visual<br>generation ability emerges as a natural byproduct of improved visual<br>understanding, and can be unlocked efficiently with a small amount of<br>generation data; (2) while we find understanding and generation to be mutually<br>beneficial, understanding data contributes to both capabilities more<br>effectively than generation data. Building upon these findings, we train our<br>MetaMorph model and achieve competitive performance on both visual<br>understanding and generation. In visual generation, MetaMorph can leverage the<br>world knowledge and reasoning abilities gained from LLM pretraining, and<br>overcome common failure modes exhibited by other generation models. Our results<br>suggest that LLMs may have strong "prior" vision capabilities that can be<br>efficiently adapted to both visual understanding and generation with a<br>relatively simple instruction tuning process.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MetaMorphï¼šé€šè¿‡æŒ‡ä»¤å¾®è°ƒå®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢çš„è¿›æ­¥ï¼Œä»åŸºæœ¬çš„å›¾åƒæè¿°åˆ°å¤æ‚çš„è§†è§‰æ¨ç†ï¼Œè¿™äº›æ¨¡å‹å·²ç»èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆä¸»è¦æ˜¯å›¾åƒå’Œè¯­è¨€ï¼‰å¹¶ç”Ÿæˆæ–‡æœ¬æ ‡è®°ã€‚ç„¶è€Œï¼Œç›®å‰çš„å¤šæ¨¡æ€æ¨¡å‹å¾€å¾€å°†è§†è§‰ç”Ÿæˆè§†ä¸ºä¸è§†è§‰ç†è§£æ­£äº¤çš„èƒ½åŠ›ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®å’Œæ—¶é—´è¿›è¡Œè®­ç»ƒã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½¿é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿå¿«é€Ÿè½¬å˜ä¸ºä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰æ ‡è®°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†è§†è§‰é¢„æµ‹æŒ‡ä»¤å¾®è°ƒï¼ˆVPiTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è§†è§‰æŒ‡ä»¤å¾®è°ƒæ‰©å±•ï¼Œå®ƒä½¿é¢„è®­ç»ƒçš„LLMèƒ½å¤Ÿå¿«é€Ÿè½¬å˜ä¸ºä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰æ ‡è®°ã€‚VPiT æ•™å¯¼ LLM ä»ä»»ä½•ä»¥æŒ‡ä»¤éµå¾ªæ ¼å¼ç¼–å†™çš„å›¾åƒå’Œæ–‡æœ¬æ•°æ®è¾“å…¥åºåˆ—ä¸­é¢„æµ‹ç¦»æ•£çš„æ–‡æœ¬æ ‡è®°å’Œè¿ç»­çš„è§†è§‰æ ‡è®°ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡å‘ç° VPiT å…·æœ‰ä»¥ä¸‹æœ‰è¶£ç‰¹æ€§ï¼š<br>1. è§†è§‰ç”Ÿæˆèƒ½åŠ›æ˜¯æ”¹è¿›è§†è§‰ç†è§£çš„å‰¯äº§å“ï¼Œå¹¶ä¸”å¯ä»¥ç”¨å°‘é‡çš„ç”Ÿæˆæ•°æ®è¿›è¡Œé«˜æ•ˆè§£é”ã€‚<br>2. è™½ç„¶æˆ‘ä»¬å‘ç°ç†è§£å’Œç”Ÿæˆæ˜¯ç›¸äº’æœ‰ç›Šçš„ï¼Œä½†ç†è§£æ•°æ®æ¯”ç”Ÿæˆæ•°æ®æ›´æœ‰æ•ˆåœ°è´¡çŒ®äºè¿™ä¸¤ç§èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åŸºäºä»¥ä¸Šå‘ç°ï¼Œæœ¬æ–‡è®­ç»ƒäº†ä¸€ä¸ªåä¸º MetaMorph çš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¹¶åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢ï¼ŒMetaMorph å¯ä»¥åˆ©ç”¨ä» LLM é¢„è®­ç»ƒä¸­è·å¾—çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶å…‹æœå…¶ä»–ç”Ÿæˆæ¨¡å‹ä¸­å¸¸è§çš„å¤±è´¥æ¨¡å¼ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ VPiT æ–¹æ³•ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„ LLM çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å‘ç°ç†è§£å’Œç”Ÿæˆæ˜¯ç›¸äº’æœ‰ç›Šçš„ï¼Œå¹¶ä¸”ç†è§£æ•°æ®æ¯”ç”Ÿæˆæ•°æ®æ›´æœ‰æ•ˆåœ°è´¡çŒ®äºè¿™ä¸¤ç§èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>4</th>
      <td>SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding</td>
      <td>The remarkable success of Large Language Models (LLMs) has extended to the<br>multimodal domain, achieving outstanding performance in image understanding and<br>generation. Recent efforts to develop unified Multimodal Large Language Models<br>(MLLMs) that integrate these capabilities have shown promising results.<br>However, existing approaches often involve complex designs in model<br>architecture or training pipeline, increasing the difficulty of model training<br>and scaling. In this paper, we propose SynerGen-VL, a simple yet powerful<br>encoder-free MLLM capable of both image understanding and generation. To<br>address challenges identified in existing encoder-free unified MLLMs, we<br>introduce the token folding mechanism and the vision-expert-based progressive<br>alignment pretraining strategy, which effectively support high-resolution image<br>understanding while reducing training complexity. After being trained on<br>large-scale mixed image-text data with a unified next-token prediction<br>objective, SynerGen-VL achieves or surpasses the performance of existing<br>encoder-free unified MLLMs with comparable or smaller parameter sizes, and<br>narrows the gap with task-specific state-of-the-art models, highlighting a<br>promising path toward future unified MLLMs. Our code and models shall be<br>released.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SynerGen-VLï¼šåŸºäºè§†è§‰ä¸“å®¶å’ŒTokenæŠ˜å çš„ååŒå›¾åƒç†è§£å’Œç”Ÿæˆæ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„æˆåŠŸåº”ç”¨ï¼Œç»Ÿä¸€çš„å›¾åƒç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼ˆMLLMsï¼‰æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMsæ¨¡å‹æ¶æ„æˆ–è®­ç»ƒæµç¨‹å¤æ‚ï¼Œå¢åŠ äº†æ¨¡å‹è®­ç»ƒå’Œæ‰©å±•çš„éš¾åº¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šTokenæŠ˜å æœºåˆ¶<br>ä¸ºäº†æ”¯æŒé«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒå¤æ‚æ€§ï¼ŒSynerGen-VLå¼•å…¥äº†TokenæŠ˜å æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€šè¿‡å°†è¾“å…¥çš„è§†è§‰Tokenåºåˆ—å‹ç¼©ï¼Œå‡å°‘äº†åºåˆ—é•¿åº¦ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§†è§‰ä¸“å®¶å’Œæ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥<br>ä¸ºäº†å°†è§†è§‰èƒ½åŠ›æ•´åˆåˆ°é¢„è®­ç»ƒçš„LLMä¸­ï¼ŒåŒæ—¶æœ€å°åŒ–å¯¹LLMé¢„è®­ç»ƒçŸ¥è¯†çš„å¹²æ‰°ï¼ŒSynerGen-VLå¼•å…¥äº†è§†è§‰ä¸“å®¶å’Œæ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥ã€‚è§†è§‰ä¸“å®¶æ˜¯é’ˆå¯¹å›¾åƒè¡¨ç¤ºçš„é¢å¤–å‚æ•°ï¼Œè€Œæ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥åˆ™é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„é¢„è®­ç»ƒï¼Œé€æ­¥å°†è§†è§‰ç‰¹å¾ä¸LLMçš„è¡¨ç¤ºç©ºé—´å¯¹é½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>SynerGen-VLåœ¨å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰ç¼–ç å™¨æ— å…³çš„ç»Ÿä¸€MLLMsç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”ç¼©å°äº†ä¸ç‰¹å®šä»»åŠ¡æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚åœ¨å›¾åƒç†è§£æ–¹é¢ï¼ŒSynerGen-VLåœ¨éœ€è¦é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒSynerGen-VLä¹Ÿå–å¾—äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SynerGen-VLçš„TokenæŠ˜å æœºåˆ¶å’Œè§†è§‰ä¸“å®¶ç­–ç•¥ä¸ºæ„å»ºç»Ÿä¸€çš„å›¾åƒç†è§£å’Œç”Ÿæˆæ¨¡å‹æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSynerGen-VLçš„æ¸è¿›å¼å¯¹é½é¢„è®­ç»ƒç­–ç•¥ä¹Ÿä¸ºå¦‚ä½•åœ¨ä¿æŒLLMé¢„è®­ç»ƒçŸ¥è¯†çš„åŒæ—¶æ•´åˆè§†è§‰èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Diffusion Language Models Are Versatile Protein Learners</td>
      <td>This paper introduces diffusion protein language model (DPLM), a versatile<br>protein language model that demonstrates strong generative and predictive<br>capabilities for protein sequences. We first pre-train scalable DPLMs from<br>evolutionary-scale protein sequences within a generative self-supervised<br>discrete diffusion probabilistic framework, which generalizes language modeling<br>for proteins in a principled way. After pre-training, DPLM exhibits the ability<br>to generate structurally plausible, novel, and diverse protein sequences for<br>unconditional generation. We further demonstrate the proposed diffusion<br>generative pre-training makes DPLM possess a better understanding of proteins,<br>making it a superior representation learner, which can be fine-tuned for<br>various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).<br>Moreover, DPLM can be tailored for various needs, which showcases its prowess<br>of conditional generation in several ways: (1) conditioning on partial peptide<br>sequences, e.g., generating scaffolds for functional motifs with high success<br>rate; (2) incorporating other modalities as conditioner, e.g.,<br>structure-conditioned generation for inverse folding; and (3) steering sequence<br>generation towards desired properties, e.g., satisfying specified secondary<br>structures, through a plug-and-play classifier guidance. Code is released at<br>\url{https://github.com/bytedance/dplm}.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è›‹ç™½è´¨å­¦ä¹ çš„æ–°ç¯‡ç« ï¼šæ‰©æ•£è¯­è¨€æ¨¡å‹çš„å¤šåŠŸèƒ½åº”ç”¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è›‹ç™½è´¨ä½œä¸ºç”Ÿå‘½æ´»åŠ¨çš„åŸºç¡€ï¼Œå…¶åºåˆ—å’Œç»“æ„çš„ç ”ç©¶å¯¹äºç†è§£ç”Ÿç‰©åŠŸèƒ½å’Œè®¾è®¡æ–°å‹è›‹ç™½è´¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚ï¼ŒåŸºäºæ©ç é¢„æµ‹çš„æ¨¡å‹éš¾ä»¥è¿›è¡Œåºåˆ—ç”Ÿæˆï¼Œè€ŒåŸºäºè‡ªå›å½’çš„æ¨¡å‹åˆ™åœ¨åºåˆ—ç†è§£ä¸Šæœ‰æ‰€æ¬ ç¼ºã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ‰©æ•£è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆDPLMï¼‰<br>æœ¬æ–‡æå‡ºäº†DPLMï¼Œä¸€ç§åŸºäºç¦»æ•£æ‰©æ•£æ¦‚ç‡æ¡†æ¶çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ã€‚DPLMé€šè¿‡åœ¨è¿›åŒ–å°ºåº¦ä¸Šçš„è›‹ç™½è´¨åºåˆ—ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆç»“æ„åˆç†ã€æ–°é¢–ä¸”å¤šæ ·çš„è›‹ç™½è´¨åºåˆ—ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¡ä»¶ç”Ÿæˆç­–ç•¥<br>DPLMæ”¯æŒå¤šç§æ¡ä»¶ç”Ÿæˆç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š<br>1. åŸºäºéƒ¨åˆ†è‚½åºåˆ—çš„æ¡ä»¶ç”Ÿæˆï¼Œä¾‹å¦‚ï¼Œä»¥é«˜æˆåŠŸç‡ç”ŸæˆåŠŸèƒ½åŸºåºçš„æ”¯æ¶ã€‚<br>2. ç»“åˆå…¶ä»–æ¨¡æ€ä½œä¸ºæ¡ä»¶ï¼Œä¾‹å¦‚ï¼Œç»“æ„æ¡ä»¶ç”Ÿæˆç”¨äºé€†æŠ˜å ã€‚<br>3. é€šè¿‡å³æ’å³ç”¨çš„åˆ†ç±»å™¨å¼•å¯¼ï¼Œå°†åºåˆ—ç”Ÿæˆå¼•å¯¼åˆ°æ‰€éœ€çš„å±æ€§ï¼Œä¾‹å¦‚ï¼Œæ»¡è¶³æŒ‡å®šçš„äºŒçº§ç»“æ„ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒDPLMåœ¨æ— æ¡ä»¶ç”Ÿæˆã€è›‹ç™½è´¨è¡¨ç¤ºå­¦ä¹ å’Œæ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚DPLMç”Ÿæˆçš„è›‹ç™½è´¨åºåˆ—å…·æœ‰é«˜ç»“æ„åˆç†æ€§ã€æ–°é¢–æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶ä¸”åœ¨å„ç§é¢„æµ‹ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„è›‹ç™½è´¨åºåˆ—ç¼–ç å™¨æ¨¡å‹ï¼Œå¦‚ESM-2ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DPLMä¸ºè›‹ç™½è´¨å­¦ä¹ å’Œè®¾è®¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶å¼ºå¤§çš„ç”Ÿæˆå’Œé¢„æµ‹èƒ½åŠ›ä»¥åŠçµæ´»çš„æ¡ä»¶ç”Ÿæˆç­–ç•¥ä½¿å…¶åœ¨è¯ç‰©å‘ç°ã€è›‹ç™½è´¨å·¥ç¨‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æ­¤å¤–ï¼ŒDPLMçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–åºåˆ—å»ºæ¨¡ä»»åŠ¡æä¾›å€Ÿé‰´ã€‚</td>
    </tr>
    <tr>
      <th>6</th>
      <td>TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation</td>
      <td>We present TokenFlow, a novel unified image tokenizer that bridges the<br>long-standing gap between multimodal understanding and generation. Prior<br>research attempt to employ a single reconstruction-targeted Vector Quantization<br>(VQ) encoder for unifying these two tasks. We observe that understanding and<br>generation require fundamentally different granularities of visual information.<br>This leads to a critical trade-off, particularly compromising performance in<br>multimodal understanding tasks. TokenFlow addresses this challenge through an<br>innovative dual-codebook architecture that decouples semantic and pixel-level<br>feature learning while maintaining their alignment via a shared mapping<br>mechanism. This design enables direct access to both high-level semantic<br>representations crucial for understanding tasks and fine-grained visual<br>features essential for generation through shared indices. Our extensive<br>experiments demonstrate TokenFlow's superiority across multiple dimensions.<br>Leveraging TokenFlow, we demonstrate for the first time that discrete visual<br>input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2\%<br>average improvement. For image reconstruction, we achieve a strong FID score of<br>0.63 at 384*384 resolution. Moreover, TokenFlow establishes state-of-the-art<br>performance in autoregressive image generation with a GenEval score of 0.55 at<br>256*256 resolution, achieving comparable results to SDXL.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | TokenFlowï¼šç»Ÿä¸€å›¾åƒåˆ†è¯å™¨ï¼Œè·¨è¶Šå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„é¸¿æ²Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¤šæ¨¡æ€é¢†åŸŸï¼Œè§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡é€šå¸¸éœ€è¦ä¸åŒçš„è§†è§‰ä¿¡æ¯ç²’åº¦ã€‚è§†è§‰ç†è§£éœ€è¦ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºæ¥æ”¯æŒå¤æ‚çš„æ¨ç†ï¼Œè€Œè§†è§‰ç”Ÿæˆåˆ™éœ€è¦ç²¾ç¡®çš„ç©ºé—´ç»“æ„å’Œçº¹ç†ç»†èŠ‚ç¼–ç ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€é‡‡ç”¨å•ä¸€çš„é‡å»ºç›®æ ‡å‘é‡é‡åŒ–ï¼ˆVQï¼‰ç¼–ç å™¨æ¥ç»Ÿä¸€è¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œè¿™å¯¼è‡´äº†ä¸€ä¸ªå…³é”®çš„æƒè¡¡ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­æ€§èƒ½çš„å¦¥åã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>TokenFlow é€šè¿‡å…¶ç‹¬ç‰¹çš„åŒæµè®¾è®¡è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ã€‚TokenFlow çš„å…³é”®æ´å¯Ÿæ˜¯è§£è€¦è¯­ä¹‰å’Œåƒç´ çº§ç‰¹å¾çš„å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡å…±äº«ç´¢å¼•æ˜ å°„ä¿æŒå®ƒä»¬çš„å¯¹é½ã€‚é€šè¿‡å°†å…·æœ‰è¯­ä¹‰å’Œåƒç´ çº§ç›¸ä¼¼æ€§çš„è¡¥ä¸æ˜ å°„åˆ°ç›¸åŒçš„ç´¢å¼•ï¼Œé‡åŒ–ç‰¹å¾å¯ä»¥ç›´æ¥åº”ç”¨äºè‡ªå›å½’è§†è§‰ç”Ÿæˆå’Œå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚<br><br>TokenFlow é‡‡ç”¨åŒç¼–ç å™¨æ¶æ„ï¼ŒåŒ…æ‹¬è¯­ä¹‰ç¼–ç å™¨ Esem å’Œåƒç´ ç¼–ç å™¨ Epixã€‚è¯­ä¹‰ç¼–ç å™¨ä»é¢„è®­ç»ƒçš„æ–‡æœ¬å¯¹é½è§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚ CLIP ViT-B/14ï¼‰ä¸­å­¦ä¹ ï¼Œè€Œåƒç´ ç¼–ç å™¨æ•è·è¯¦ç»†çš„è§†è§‰ä¿¡æ¯ã€‚æå–çš„ç‰¹å¾ç„¶åé€šè¿‡æœ€å°åŒ–è¯­ä¹‰å’Œåƒç´ çº§è·ç¦»çš„åŠ æƒæ±‚å’Œæ¥é‡åŒ–ï¼Œåˆ›å»ºä¸€ä¸ªè”åˆè¡¨ç¤ºç©ºé—´ã€‚<br><br>TokenFlow çš„åŒä»£ç æœ¬è®¾è®¡å…è®¸ä¸“ä¸šåŒ–å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡å…±äº«ç´¢å¼•ä¿æŒè·¨çº§åˆ«ç›¸å…³æ€§ã€‚è¿™ç§åˆ›æ–°å…è®¸åŒæ—¶è®¿é—®è¯­ä¹‰å’Œåƒç´ çº§è¡¨ç¤ºï¼Œè€Œä¸ä¼šæŸå®³ä»»ä½•æ–¹é¢ã€‚TokenFlow è¿˜å±•ç¤ºäº†æ˜¾è‘—çš„æ‰©å±•æ€§ï¼Œå³ä½¿åœ¨è¶…è¿‡ 130K æ¡ç›®çš„è¶…å¤§è§„æ¨¡ä»£ç æœ¬ä¸­ï¼Œä¹Ÿèƒ½ä¿æŒå¼‚å¸¸é«˜çš„ä»£ç æœ¬åˆ©ç”¨ç‡ï¼ˆ95%+ï¼‰ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>TokenFlow åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚åˆ©ç”¨ TokenFlowï¼Œæˆ‘ä»¬é¦–æ¬¡è¯æ˜ç¦»æ•£è§†è§‰è¾“å…¥å¯ä»¥è¶…è¶Š LLaVA-1.5 13B åœ¨ç†è§£æ€§èƒ½æ–¹é¢ï¼Œå¹³å‡æé«˜äº† 7.2%ã€‚å¯¹äºå›¾åƒé‡å»ºï¼Œæˆ‘ä»¬åœ¨ 384*384 åˆ†è¾¨ç‡ä¸‹å®ç°äº† 0.63 çš„ FID åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒTokenFlow åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹é¢å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ 256*256 åˆ†è¾¨ç‡ä¸‹å®ç°äº† 0.55 çš„ GenEval åˆ†æ•°ï¼Œå®ç°äº†ä¸ SDXL ç›¸å½“çš„ç»“æœã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>TokenFlow çš„åŒä»£ç æœ¬æ¶æ„å’Œå…±äº«æ˜ å°„æœºåˆ¶ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„ç»Ÿä¸€è§†è§‰ç¼–ç æœºåˆ¶ã€‚TokenFlow çš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥å¯å‘æœªæ¥å¤šæ¨¡æ€æ¨¡å‹çš„ç ”ç©¶å’Œå¼€å‘ï¼Œæ¨åŠ¨å¤šæ¨¡æ€é¢†åŸŸçš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>7</th>
      <td>LoRaFlow: High-Quality Signal Reconstruction using Rectified Flow</td>
      <td>LoRa technology, crucial for low-power wide-area networks, faces significant<br>performance degradation at extremely low signal-to-noise ratios (SNRs). We<br>present LoRaFlow, a novel approach using rectified flow to reconstruct<br>high-quality LoRa signals in challenging noise conditions. Unlike existing<br>neural-enhanced methods focused on classification, LoRaFlow recovers the signal<br>itself, maintaining compatibility with standard dechirp algorithms. Our method<br>combines a hybrid neural network architecture, synthetic data generation, and<br>robust augmentation strategies. This minimally invasive enhancement to LoRa<br>infrastructure potentially extends operational range and reliability without<br>overhauling existing systems. LoRaFlow opens new possibilities for robust IoT<br>communications in harsh environments and its core methodology can be<br>generalized to support various communication technologies.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LoRaFlowï¼šåŸºäºæ ¡æ­£æµçš„LoRaä¿¡å·é«˜è´¨é‡é‡å»º<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>LoRaæŠ€æœ¯æ˜¯ä½åŠŸè€—å¹¿åŸŸç½‘ç»œï¼ˆLPWANï¼‰çš„å…³é”®æŠ€æœ¯ï¼Œåœ¨ç‰©è”ç½‘ï¼ˆIoTï¼‰åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼ŒLoRaä¿¡å·åœ¨æä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ¡ä»¶ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´å’Œå¯é æ€§ã€‚ç°æœ‰çš„åŸºäºç¥ç»ç½‘ç»œçš„LoRaä¿¡å·å¢å¼ºæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œè€ŒLoRaFlowåˆ™ä¸“æ³¨äºä¿¡å·æœ¬èº«çš„é‡å»ºï¼Œä»¥æ¢å¤é«˜è´¨é‡ä¿¡å·ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ ¡æ­£æµæŠ€æœ¯<br>LoRaFlowé‡‡ç”¨æ ¡æ­£æµæŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å™ªå£°è¾“å…¥ä¸­é‡å»ºé«˜è´¨é‡çš„LoRaä¿¡å·ã€‚ä¸ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œæ ¡æ­£æµé€šè¿‡å­¦ä¹ ç›´çº¿è½¨è¿¹çš„æ˜ å°„ï¼Œå‡å°‘äº†é‡å»ºä¿¡å·æ‰€éœ€çš„æ­¥éª¤ï¼Œä»è€Œåœ¨ä½SNRæ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ··åˆç¥ç»ç½‘ç»œæ¶æ„<br>LoRaFlowçš„æ¨¡å‹æ¶æ„ç»“åˆäº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰å’Œå·ç§¯å±‚ï¼Œå……åˆ†åˆ©ç”¨äº†DiTçš„æ‰©å±•æ€§å’Œçµæ´»æ€§ï¼Œä»¥åŠå·ç§¯å±‚åœ¨å¤„ç†å¤šå°ºåº¦ç©ºé—´ä¿¡æ¯æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¿™ç§æ··åˆæ¶æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰LoRaä¿¡å·ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåˆæˆæ•°æ®ç”Ÿæˆå’Œå¢å¼ºç­–ç•¥<br>ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼ŒLoRaFlowä½¿ç”¨åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œæ¨¡æ‹Ÿäº†ä¸åŒæ‰©é¢‘å› å­ï¼ˆSFï¼‰å’Œå¸¦å®½ï¼ˆBWï¼‰çš„LoRaä¿¡å·ã€‚æ­¤å¤–ï¼ŒLoRaFlowè¿˜é‡‡ç”¨äº†å¤šç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¦‚é¢‘ç‡åŸŸæ©è”½ã€æ—¶åŸŸå¹³ç§»ã€ä¿¡å·åè½¬å’Œé¢‘è°±å›¾æ»šåŠ¨ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæœ€å°ä¾µå…¥å¼é›†æˆ<br>LoRaFlowçš„è®¾è®¡æ—¨åœ¨ä¸ç°æœ‰çš„LoRaåŸºç¡€è®¾æ–½æ— ç¼é›†æˆï¼Œæ— éœ€å¯¹ç°æœ‰ç³»ç»Ÿè¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚å®ƒé€šè¿‡åœ¨æ ‡å‡†è§£è°ƒç®—æ³•ä¹‹å‰å¯¹ä¿¡å·è¿›è¡Œå»å™ªï¼Œä»è€Œæé«˜LoRaç½‘ç»œçš„æ€§èƒ½ï¼Œè€Œä¸ä¼šå½±å“ç°æœ‰çš„LoRaç¡¬ä»¶å’Œè½¯ä»¶ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>LoRaFlowåœ¨å¤šä¸ªæ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ä¼ ç»Ÿçš„è§£è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒLoRaFlowåœ¨ä½SNRæ¡ä»¶ä¸‹å®ç°äº†æ›´é«˜çš„ä¿¡å·é‡å»ºç²¾åº¦ï¼Œå°¤å…¶æ˜¯åœ¨ç›¸ä½å’Œå¹…åº¦æ–¹é¢ã€‚æ­¤å¤–ï¼ŒLoRaFlowåœ¨ç¬¦å·é”™è¯¯ç‡ï¼ˆSERï¼‰æ–¹é¢ä¹Ÿä¼˜äºç°æœ‰çš„åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œå¦‚NELoRaã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LoRaFlowçš„æ ¸å¿ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬æ ¡æ­£æµæŠ€æœ¯ã€æ··åˆç¥ç»ç½‘ç»œæ¶æ„ã€åˆæˆæ•°æ®ç”Ÿæˆå’Œå¢å¼ºç­–ç•¥ï¼Œä»¥åŠæœ€å°ä¾µå…¥å¼é›†æˆï¼Œä¸ºLoRaä¿¡å·å¢å¼ºé¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒLoRaFlowçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–é€šä¿¡æŠ€æœ¯çš„ä¿¡å·é‡å»ºä»»åŠ¡æä¾›å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads</td>
      <td>We introduce Orthus, an autoregressive (AR) transformer that excels in<br>generating images given textual prompts, answering questions based on visual<br>inputs, and even crafting lengthy image-text interleaved contents. Unlike prior<br>arts on unified multimodal modeling, Orthus simultaneously copes with discrete<br>text tokens and continuous image features under the AR modeling principle. The<br>continuous treatment of visual signals minimizes the information loss for both<br>image understanding and generation while the fully AR formulation renders the<br>characterization of the correlation between modalities straightforward. The key<br>mechanism enabling Orthus to leverage these advantages lies in its<br>modality-specific heads -- one regular language modeling (LM) head predicts<br>discrete text tokens and one diffusion head generates continuous image features<br>conditioning on the output of the backbone. We devise an efficient strategy for<br>building Orthus -- by substituting the Vector Quantization (VQ) operation in<br>the existing unified AR model with a soft alternative, introducing a diffusion<br>head, and tuning the added modules to reconstruct images, we can create an<br>Orthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).<br>Orthus-base can further embrace post-training to better model interleaved<br>images and texts. Empirically, Orthus surpasses competing baselines including<br>Show-o and Chameleon across standard benchmarks, achieving a GenEval score of<br>0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows<br>exceptional mixed-modality generation capabilities, reflecting the potential<br>for handling intricate practical generation tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Orthusï¼šåŸºäºè‡ªå›å½’çš„å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å­˜åœ¨å»ºæ¨¡å†—ä½™å’Œä¿¡æ¯æŸå¤±çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ç¦»æ•£ä»¤ç‰Œè¿›è¡Œè‡ªå›å½’å»ºæ¨¡ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œè€Œå°†è‡ªå›å½’å»ºæ¨¡ä¸æ‰©æ•£å»ºæ¨¡ç»“åˆåˆ™éš¾ä»¥åŒæ—¶å¤„ç†å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>Orthus æ˜¯ä¸€ç§åŸºäºè‡ªå›å½’çš„å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡ä»¥ä¸‹åˆ›æ–°ç‚¹è§£å†³äº†ç°æœ‰æ–¹æ³•çš„ç—›ç‚¹ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¿ç»­å¤„ç†è§†è§‰ä¿¡å·<br>Orthus ç›´æ¥å¤„ç†è¿ç»­çš„å›¾åƒç‰¹å¾ï¼Œé¿å…äº†å›¾åƒä»¤ç‰ŒåŒ–è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ï¼Œä»è€Œæé«˜äº†å›¾åƒç†è§£å’Œç”Ÿæˆçš„è´¨é‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»Ÿä¸€çš„è‡ªå›å½’å»ºæ¨¡<br>Orthus ä½¿ç”¨ç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹æ¥å¤„ç†ç¦»æ•£çš„æ–‡æœ¬ä»¤ç‰Œå’Œè¿ç»­çš„å›¾åƒç‰¹å¾ï¼Œç®€åŒ–äº†æ¨¡æ€ä¹‹é—´çš„ç›¸å…³æ€§å»ºæ¨¡ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡æ€ç‰¹å®šå¤´éƒ¨<br>Orthus ä½¿ç”¨ä¸¤ä¸ªæ¨¡æ€ç‰¹å®šå¤´éƒ¨ï¼šä¸€ä¸ªè¯­è¨€æ¨¡å‹å¤´éƒ¨é¢„æµ‹ç¦»æ•£çš„æ–‡æœ¬ä»¤ç‰Œï¼Œä¸€ä¸ªæ‰©æ•£å¤´éƒ¨ç”Ÿæˆè¿ç»­çš„å›¾åƒç‰¹å¾ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šé«˜æ•ˆçš„æ„å»ºç­–ç•¥<br>Orthus å¯ä»¥é€šè¿‡æ›¿æ¢ç°æœ‰ç»Ÿä¸€è‡ªå›å½’æ¨¡å‹ä¸­çš„ VQ æ“ä½œå¹¶å¼•å…¥æ‰©æ•£å¤´éƒ¨æ¥é«˜æ•ˆæ„å»ºï¼Œä»è€Œæ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒOrthus è¶…è¶Šäº†åŒ…æ‹¬ Show-o å’Œ Chameleon åœ¨å†…çš„ç«äº‰åŸºçº¿ï¼Œå®ç°äº† 0.58 çš„ GenEval åˆ†æ•°å’Œ 1265.8 çš„ MME-P åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒOrthus è¿˜å±•ç¤ºäº†å‡ºè‰²çš„æ··åˆæ¨¡æ€ç”Ÿæˆèƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚ç”Ÿæˆä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Orthus çš„åˆ›æ–°æ–¹æ³•ä¸ºå¤šæ¨¡æ€å»ºæ¨¡æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶è¿ç»­å¤„ç†è§†è§‰ä¿¡å·å’Œç»Ÿä¸€çš„è‡ªå›å½’å»ºæ¨¡ç­–ç•¥å€¼å¾—å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒOrthus çš„é«˜æ•ˆæ„å»ºç­–ç•¥ä¹Ÿä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å¼€å‘æä¾›äº†å®è´µçš„ç»éªŒã€‚</td>
    </tr>
    <tr>
      <th>9</th>
      <td>JetFormer: An Autoregressive Generative Model of Raw Images and Text</td>
      <td>Removing modeling constraints and unifying architectures across domains has<br>been a key driver of the recent progress in training large multimodal models.<br>However, most of these models still rely on many separately trained components<br>such as modality-specific encoders and decoders. In this work, we further<br>streamline joint generative modeling of images and text. We propose an<br>autoregressive decoder-only transformer - JetFormer - which is trained to<br>directly maximize the likelihood of raw data, without relying on any separately<br>pretrained components, and can understand and generate both text and images.<br>Specifically, we leverage a normalizing flow model to obtain a soft-token image<br>representation that is jointly trained with an autoregressive multimodal<br>transformer. The normalizing flow model serves as both an image encoder for<br>perception tasks and an image decoder for image generation tasks during<br>inference. JetFormer achieves text-to-image generation quality competitive with<br>recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained<br>image autoencoders, which are trained with a complex mixture of losses,<br>including perceptual ones. At the same time, JetFormer demonstrates robust<br>image understanding capabilities. To the best of our knowledge, JetFormer is<br>the first model that is capable of generating high-fidelity images and<br>producing strong log-likelihood bounds.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | JetFormerï¼šä¸€ç§ç”¨äºåŸå§‹å›¾åƒå’Œæ–‡æœ¬çš„è‡ªå›å½’ç”Ÿæˆæ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨è®­ç»ƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…¶ä¸­ä¸€ä¸ªå…³é”®é©±åŠ¨åŠ›æ˜¯å»é™¤å»ºæ¨¡çº¦æŸå¹¶ç»Ÿä¸€è·¨é¢†åŸŸçš„æ¶æ„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™äº›æ¨¡å‹ä»ç„¶ä¾èµ–äºè®¸å¤šå•ç‹¬è®­ç»ƒçš„ç»„ä»¶ï¼Œä¾‹å¦‚ç‰¹å®šäºæ¨¡æ€çš„ç¼–ç å™¨å’Œè§£ç å™¨ã€‚æœ¬æ–‡æ—¨åœ¨è¿›ä¸€æ­¥ç®€åŒ–å›¾åƒå’Œæ–‡æœ¬çš„è”åˆç”Ÿæˆå»ºæ¨¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†JetFormerï¼Œä¸€ç§è‡ªå›å½’è§£ç å™¨-ä»…Transformerï¼Œå®ƒå¯ä»¥ç›´æ¥æœ€å¤§åŒ–åŸå§‹æ•°æ®çš„ä¼¼ç„¶æ€§ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å•ç‹¬é¢„è®­ç»ƒçš„ç»„ä»¶ï¼Œå¹¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨æ­£åˆ™åŒ–æµæ¨¡å‹æ¥è·å¾—è½¯ä»¤ç‰Œå›¾åƒè¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºä¸è‡ªå›å½’å¤šæ¨¡æ€Transformerè”åˆè®­ç»ƒã€‚æ­£åˆ™åŒ–æµæ¨¡å‹åœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸­ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œåœ¨æ¨ç†æ—¶ä½œä¸ºå›¾åƒè§£ç å™¨ã€‚JetFormeråœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢ä¸æœ€è¿‘çš„VQ-VAEå’ŒVAEåŸºçº¿ç›¸å½“ã€‚è¿™äº›åŸºçº¿ä¾èµ–äºé¢„è®­ç»ƒçš„å›¾åƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œè¿™äº›è‡ªåŠ¨ç¼–ç å™¨æ˜¯ç”¨å¤æ‚çš„æŸå¤±æ··åˆç‰©è®­ç»ƒçš„ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥æŸå¤±ã€‚åŒæ—¶ï¼ŒJetFormerå±•ç¤ºäº†å¼ºå¤§çš„å›¾åƒç†è§£èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒJetFormeræ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå›¾åƒå¹¶äº§ç”Ÿå¼ºå¤§çš„å¯¹æ•°ä¼¼ç„¶ç•Œé™çš„æ¨¡å‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>JetFormeråœ¨ImageNetç±»æ¡ä»¶å›¾åƒç”Ÿæˆå’ŒWeb-scaleå¤šæ¨¡æ€ç”Ÿæˆæ–¹é¢è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜JetFormeråœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œè§†è§‰è¯­è¨€ç†è§£æ–¹é¢å…·æœ‰ç«äº‰åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>JetFormerçš„è®¾è®¡ç®€å•ï¼Œæ˜“äºæ‰©å±•ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒå’Œæ–‡æœ¬ã€‚æ­¤å¤–ï¼ŒJetFormerèƒ½å¤Ÿè®¡ç®—å¯¹æ•°ä¼¼ç„¶ï¼Œè¿™å¯¹äºæ¯”è¾ƒä¸åŒçš„ç”Ÿæˆæ¨¡å‹æˆ–è¿›è¡Œhill-climbingéå¸¸æœ‰ç”¨ã€‚</td>
    </tr>
    <tr>
      <th>10</th>
      <td>MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding</td>
      <td>We introduce MUSE-VL, a Unified Vision-Language Model through Semantic<br>discrete Encoding for multimodal understanding and generation. Recently, the<br>research community has begun exploring unified models for visual generation and<br>understanding. However, existing vision tokenizers (e.g., VQGAN) only consider<br>low-level information, which makes it difficult to align with language tokens.<br>This results in high training complexity and necessitates a large amount of<br>training data to achieve optimal performance. Additionally, their performance<br>is still far from dedicated understanding models. This paper proposes Semantic<br>Discrete Encoding (SDE), which effectively aligns the information of visual<br>tokens and language tokens by adding semantic constraints to the visual<br>tokenizer. This greatly reduces the amount of training data and improves the<br>performance of the unified model. With the same LLM size, our method improved<br>the understanding performance by 4.8% compared to the previous SOTA Emu3 and<br>surpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model<br>also surpasses the existing unified models on visual generation benchmarks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MUSE-VLï¼šåŸºäºè¯­ä¹‰ç¦»æ•£ç¼–ç çš„ç»Ÿä¸€è§†è§‰è¯­è¨€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç ”ç©¶é¢†åŸŸæ—¥ç›Šå…´èµ·ï¼Œç ”ç©¶è€…ä»¬è‡´åŠ›äºå°†è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡æ•´åˆåˆ°è‡ªå›å½’çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹æ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œå¦‚ä½•å°†è§†è§‰è¾“å…¥è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œä»¥ä¾¿ä¸æ–‡æœ¬æ ‡è®°å¯¹é½ï¼Œæˆä¸ºå®ç°ç»Ÿä¸€MLLMsçš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚ç°æœ‰çš„è§†è§‰æ ‡è®°åŒ–æ–¹æ³•ï¼ˆå¦‚VQGANï¼‰ä»…è€ƒè™‘ä½çº§ä¿¡æ¯ï¼Œéš¾ä»¥ä¸è¯­è¨€æ ‡è®°å¯¹é½ï¼Œå¯¼è‡´è®­ç»ƒå¤æ‚åº¦é«˜ï¼Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®æ‰èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¸”å…¶æ€§èƒ½ä»è¿œä½äºä¸“é—¨çš„è§†è§‰ç†è§£æ¨¡å‹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†è¯­ä¹‰ç¦»æ•£ç¼–ç ï¼ˆSDEï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨è§†è§‰æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ·»åŠ è¯­ä¹‰çº¦æŸï¼Œæœ‰æ•ˆåœ°å°†è§†è§‰æ ‡è®°å’Œè¯­è¨€æ ‡è®°çš„ä¿¡æ¯å¯¹é½ã€‚SDEæ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªè¯­ä¹‰ç¼–ç å™¨å’Œä¸€ä¸ªè¯­ä¹‰è§£ç å™¨ï¼Œç”¨äºä»ç¦»æ•£ä»£ç ä¸­æå–å’Œé‡å»ºè¯­ä¹‰ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒSDEæ–¹æ³•è¿˜åŒ…å«ä¸€ä¸ªå›¾åƒè§£ç å™¨ï¼Œç”¨äºä»é‡åŒ–ç‰¹å¾ä¸­é‡å»ºåŸå§‹å›¾åƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒSDEæ–¹æ³•èƒ½å¤Ÿåœ¨å›¾åƒç¦»æ•£åŒ–è¿‡ç¨‹ä¸­è€ƒè™‘è¯­ä¹‰ä¿¡æ¯ï¼Œæ»¡è¶³è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„éœ€æ±‚ã€‚<br><br>åŸºäºSDEæ ‡è®°å™¨ï¼Œæœ¬æ–‡æå‡ºäº†MUSE-VLï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€è‡ªå›å½’å˜æ¢å™¨ã€‚MUSE-VLå°†è§†è§‰å’Œè¯­è¨€æ•°æ®å»ºæ¨¡ä¸ºç»Ÿä¸€çš„ç¦»æ•£æ ‡è®°ï¼Œå¹¶åœ¨å„ç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„SOTA Emu3ç›¸æ¯”ï¼ŒMUSE-VLåœ¨ç†è§£æ€§èƒ½ä¸Šæé«˜äº†4.8%ï¼Œå¹¶è¶…è¿‡äº†ä¸“é—¨çš„è§†è§‰ç†è§£æ¨¡å‹LLaVA-NeXT 34Bã€‚æ­¤å¤–ï¼ŒMUSE-VLåœ¨è§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¶…è¿‡äº†ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„SDEæ–¹æ³•å’ŒMUSE-VLæ¨¡å‹ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚SDEæ–¹æ³•æœ‰æ•ˆåœ°å°†è§†è§‰æ ‡è®°å’Œè¯­è¨€æ ‡è®°çš„ä¿¡æ¯å¯¹é½ï¼Œè€ŒMUSE-VLæ¨¡å‹åˆ™å°†è§†è§‰å’Œè¯­è¨€æ•°æ®å»ºæ¨¡ä¸ºç»Ÿä¸€çš„ç¦»æ•£æ ‡è®°ï¼Œå¹¶åœ¨å„ç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ–¹æ³•å’Œæ¨¡å‹ä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>11</th>
      <td>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</td>
      <td>We present JanusFlow, a powerful framework that unifies image understanding<br>and generation in a single model. JanusFlow introduces a minimalist<br>architecture that integrates autoregressive language models with rectified<br>flow, a state-of-the-art method in generative modeling. Our key finding<br>demonstrates that rectified flow can be straightforwardly trained within the<br>large language model framework, eliminating the need for complex architectural<br>modifications. To further improve the performance of our unified model, we<br>adopt two key strategies: (i) decoupling the understanding and generation<br>encoders, and (ii) aligning their representations during unified training.<br>Extensive experiments show that JanusFlow achieves comparable or superior<br>performance to specialized models in their respective domains, while<br>significantly outperforming existing unified approaches across standard<br>benchmarks. This work represents a step toward more efficient and versatile<br>vision-language models.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | JanusFlowï¼šç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„æ–°æ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å­¦ä¹ å’Œæ³›åŒ–æ–¹é¢çš„æ˜¾è‘—èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸“é—¨ç”¨äºå›¾åƒç†è§£å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¤æ‚æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å•ç‹¬è®­ç»ƒå’Œé›†æˆï¼Œå¯¼è‡´æ¶æ„å¤æ‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†JanusFlowï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šèåˆè‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œä¿®æ­£æµ<br>JanusFlowçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸ä¿®æ­£æµç›¸ç»“åˆã€‚ä¿®æ­£æµæ˜¯ä¸€ç§å…ˆè¿›çš„ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨LLMæ¡†æ¶å†…ç›´æ¥è®­ç»ƒï¼Œæ— éœ€å¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨<br>ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼ŒJanusFlowé‡‡ç”¨äº†ä¸¤ç§å…³é”®ç­–ç•¥ï¼šè§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨ï¼Œä»¥åŠåœ¨å¯¹é½è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é½å®ƒä»¬çš„è¡¨ç¤ºã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºé˜²æ­¢ä»»åŠ¡ä¹‹é—´çš„å¹²æ‰°ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>JanusFlowåœ¨å¤šæ¨¡æ€ç†è§£å’Œå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä¸“é—¨çš„æ¨¡å‹ã€‚åœ¨å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusFlowåœ¨MJHQ FID-30kã€GenEvalå’ŒDPG-Benchä¸Šåˆ†åˆ«å–å¾—äº†9.51ã€0.63å’Œ80.09%çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†åŒ…æ‹¬SDv1.5å’ŒSDXLåœ¨å†…çš„ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚åœ¨å¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusFlowåœ¨MMBenchã€SeedBenchå’ŒGQAä¸Šåˆ†åˆ«å–å¾—äº†74.9ã€70.5å’Œ60.3çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†LLaVA-v1.5å’ŒQwen-VL-Chatç­‰ä¸“é—¨æ¨¡å‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>JanusFlowçš„æˆåŠŸè¡¨æ˜ï¼Œå°†è‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œä¿®æ­£æµç›¸ç»“åˆå¯ä»¥æœ‰æ•ˆåœ°è§£å†³å¤šæ¨¡æ€å­¦ä¹ å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨ä»¥åŠè¡¨ç¤ºå¯¹é½æ­£åˆ™åŒ–ç­‰ç­–ç•¥ä¹Ÿä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>12</th>
      <td>On the Performance of Multimodal Language Models</td>
      <td>Instruction-tuned large language models (LLMs) have demonstrated promising<br>zero-shot generalization capabilities across various downstream tasks. Recent<br>research has introduced multimodal capabilities to LLMs by integrating<br>independently pretrained vision encoders through model grafting. These<br>multimodal variants undergo instruction tuning, similar to LLMs, enabling<br>effective zero-shot generalization for multimodal tasks. This study conducts a<br>comparative analysis of different multimodal instruction tuning approaches and<br>evaluates their performance across a range of tasks, including complex<br>reasoning, conversation, image captioning, multiple-choice questions (MCQs),<br>and binary classification. Through rigorous benchmarking and ablation<br>experiments, we reveal key insights for guiding architectural choices when<br>incorporating multimodal capabilities into LLMs. However, current approaches<br>have limitations; they do not sufficiently address the need for a diverse<br>multimodal instruction dataset, which is crucial for enhancing task<br>generalization. Additionally, they overlook issues related to truthfulness and<br>factuality when generating responses. These findings illuminate current<br>methodological constraints in adapting language models for image comprehension<br>and provide valuable guidance for researchers and practitioners seeking to<br>harness multimodal versions of LLMs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ€§èƒ½æ¢ç©¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¦‚ä½•å°†è§†è§‰ä¿¡æ¯ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥å¤„ç†ç°å®ä¸–ç•Œä¸­çš„å¤šæ¨¡æ€æ•°æ®ï¼Œæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶ä¸åŒå¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ–¹æ³•åœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶åˆ†æå…¶ä¼˜ç¼ºç‚¹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æ¯”è¾ƒäº†äº”ç§å…¬å¼€å¯ç”¨çš„å¤šæ¨¡æ€LLMså¾®è°ƒæ–¹æ³•ï¼šBLIP-2ã€InstructBLIPã€LLaVAã€MiniGPT4å’ŒmPLUG-Owlã€‚è¿™äº›æ–¹æ³•æ¶µç›–äº†ä¸åŒçš„æ¶æ„é€‰æ‹©ï¼ŒåŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€è§†è§‰å¤´éƒ¨å’Œæ•°æ®ä½¿ç”¨æ–¹å¼ã€‚é€šè¿‡åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ¶ˆèå®éªŒï¼Œæœ¬æ–‡æ­ç¤ºäº†ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶åˆ†æäº†å½±å“å¤šæ¨¡æ€LLMsæ€§èƒ½çš„å…³é”®å› ç´ ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒInstructBLIPåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¿™å½’åŠŸäºå…¶ä¸°å¯Œçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®ã€‚å…¶ä»–æ–¹æ³•åœ¨æœªè®­ç»ƒè¿‡çš„ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå·®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè¿‡æ‹Ÿåˆåˆ°ç‰¹å®šä»»åŠ¡ç±»å‹ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼š<br><br>* ä½¿ç”¨æ›´å¤§çš„è§†è§‰ç¼–ç å™¨ï¼ˆViT-gï¼‰å¯ä»¥æ˜¾è‘—æé«˜æ‰€æœ‰ä»»åŠ¡çš„æ€§èƒ½ã€‚<br>* è®­ç»ƒè§†è§‰å¤´éƒ¨ï¼ˆå¦‚Q-Formerï¼‰å¯ä»¥æ›´å¥½åœ°æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚<br>* åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µè®­ç»ƒè¯­è¨€æ¨¡å‹å¯ä»¥å¸¦æ¥é¢å¤–çš„æ€§èƒ½æå‡ã€‚<br>* æ•°æ®é‡åœ¨è¾¾åˆ°ä¸€å®šè§„æ¨¡åï¼Œå…¶å¯¹æ€§èƒ½çš„æå‡ä½œç”¨é€æ¸å‡å¼±ï¼Œä½†æ•°æ®å¤šæ ·æ€§ä»ç„¶è‡³å…³é‡è¦ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡ä¸ºæ„å»ºæœ‰æ•ˆçš„å¤šæ¨¡æ€LLMsæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å’Œä»»åŠ¡å¤šæ ·æ€§åœ¨æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š<br><br>* æ¢ç´¢æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚<br>* ç ”ç©¶å¦‚ä½•å‡è½»å¤šæ¨¡æ€LLMsçš„å¹»è§‰é—®é¢˜ï¼Œä½¿å…¶æ›´å¯é ã€‚<br>* å¼€å‘æ›´å…ˆè¿›çš„æ¶æ„å’Œæ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°èåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚</td>
    </tr>
    <tr>
      <th>13</th>
      <td>MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding</td>
      <td>Generating lifelike human motions from descriptive texts has experienced<br>remarkable research focus in the recent years, propelled by the emerging<br>requirements of digital humans.Despite impressive advances, existing approaches<br>are often constrained by limited control modalities, task specificity, and<br>focus solely on body motion representations.In this paper, we present<br>MotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these<br>limitations. MotionGPT-2 accommodates multiple motion-relevant tasks and<br>supporting multimodal control conditions through pre-trained Large Language<br>Models (LLMs). It quantizes multimodal inputs-such as text and single-frame<br>poses-into discrete, LLM-interpretable tokens, seamlessly integrating them into<br>the LLM's vocabulary. These tokens are then organized into unified prompts,<br>guiding the LLM to generate motion outputs through a<br>pretraining-then-finetuning paradigm. We also show that the proposed<br>MotionGPT-2 is highly adaptable to the challenging 3D holistic motion<br>generation task, enabled by the innovative motion discretization framework,<br>Part-Aware VQVAE, which ensures fine-grained representations of body and hand<br>movements. Extensive experiments and visualizations validate the effectiveness<br>of our method, demonstrating the adaptability of MotionGPT-2 across motion<br>generation, motion captioning, and generalized motion completion tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MotionGPT-2ï¼šé€šç”¨è¿åŠ¨-è¯­è¨€æ¨¡å‹ï¼Œå®ç°è¿åŠ¨ç”Ÿæˆä¸ç†è§£<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œä»æè¿°æ€§æ–‡æœ¬ç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œè¿™ä¸»è¦å¾—ç›Šäºæ•°å­—äººç±»çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸å—åˆ°ä»¥ä¸‹é™åˆ¶ï¼š<br>1. **æ§åˆ¶æ–¹å¼çš„å±€é™æ€§**ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸åªé’ˆå¯¹å•ä¸€ç±»å‹çš„æ§åˆ¶æ¡ä»¶ï¼Œä¾‹å¦‚æ–‡æœ¬æè¿°æˆ–å¤šå¸§å§¿æ€ï¼Œé™åˆ¶äº†å…¶åœ¨éœ€è¦åŒæ—¶ç”ŸæˆåŸºäºæ–‡æœ¬æè¿°å’Œå¤šå…³é”®å¸§äººä½“å§¿æ€çš„è¿åŠ¨åºåˆ—åœºæ™¯ä¸­çš„åº”ç”¨ã€‚<br>2. **ç¼ºä¹é€šç”¨ä¸–ç•ŒçŸ¥è¯†çš„ä»»åŠ¡ç‰¹å®šæ¡†æ¶**ï¼šç°æœ‰çš„æ¨¡å‹å¾€å¾€æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œä¾‹å¦‚åŸºäºæ‰©æ•£å’ŒGPTçš„æ¡†æ¶ï¼Œç¼ºä¹é€‚åº”å¤šç§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¹¶ä¸”æ— æ³•å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­åµŒå…¥çš„ä¸–ç•ŒçŸ¥è¯†ã€‚<br>3. **ä»…å…³æ³¨èº«ä½“è¿åŠ¨è¡¨ç¤º**ï¼šç°æœ‰çš„åŸºäºæ–‡æœ¬çš„è¿åŠ¨ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨ç”Ÿæˆä»…èº«ä½“è¿åŠ¨ï¼Œè€Œä¸æ˜¯æ•´ä½“è¿åŠ¨ï¼Œè¿™åœ¨æŸäº›åœºæ™¯ä¸‹ï¼ˆä¾‹å¦‚ä½“è‚²æ´»åŠ¨å’Œæ¼”å¥ä¹å™¨ï¼‰çš„åˆç†æ€§å’Œè¡¨ç°åŠ›ä»ç„¶ä¸å°½å¦‚äººæ„ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†MotionGPT-2ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„é€šç”¨è¿åŠ¨-è¯­è¨€æ¨¡å‹ï¼ˆLMLMï¼‰ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§æ§åˆ¶ä¿¡å·ï¼Œæ‰§è¡Œå„ç§ä¸è¿åŠ¨ç›¸å…³çš„ä»»åŠ¡ï¼Œå¹¶ç”Ÿæˆæ•´ä½“äººç±»è¿åŠ¨ã€‚å…¶å…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br>1. **å°†å¤šæ¨¡æ€æ§åˆ¶ä¿¡å·è½¬åŒ–ä¸ºç»Ÿä¸€è¡¨ç¤º**ï¼šMotionGPT-2è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶å’Œä»»åŠ¡æ„ŸçŸ¥æç¤ºï¼Œç”¨äºäººç±»è¿åŠ¨åˆæˆã€‚è¯¥æ¡†æ¶å…è®¸åœ¨å¤šæ¨¡æ€æ§åˆ¶æ¡ä»¶ä¸‹ç”Ÿæˆäººç±»è¿åŠ¨ï¼Œå¹¶é€šè¿‡ä»»åŠ¡æ„ŸçŸ¥æç¤ºé€‚åº”ç‰¹å®šçš„è¿åŠ¨ç›¸å…³ä»»åŠ¡ã€‚<br>2. **æ„å»ºå…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„ä»»åŠ¡æ— å…³æ¡†æ¶**ï¼šMotionGPT-2é€šè¿‡ä½¿ç”¨LLMsæ¥å…±åŒè¡¨ç¤ºè¿åŠ¨å’Œè¯­è¨€ï¼Œæ”¹è¿›äº†MotionGPTã€‚é¦–å…ˆï¼Œå°†äººç±»è¿åŠ¨åµŒå…¥åˆ°ç¦»æ•£çš„è¿åŠ¨æ ‡è®°ä¸­ï¼Œç„¶åä½¿ç”¨è¿™äº›è¿åŠ¨æ ‡è®°æ‰©å±•LLMçš„è¯æ±‡è¡¨ï¼Œåˆ›å»ºä¸€ä¸ªä¸°å¯Œçš„è¿åŠ¨-è¯­è¨€è¯æ±‡è¡¨ã€‚é€šè¿‡å°†äººç±»è¿åŠ¨å’Œè¯­è¨€çº³å…¥ç»Ÿä¸€çš„è¯æ±‡è¡¨ï¼Œè¿åŠ¨å’Œè¯­è¨€ä¹‹é—´çš„å¤æ‚å…³ç³»å˜å¾—é€æ˜ã€‚è¿›ä¸€æ­¥åœ°ï¼ŒMotionGPT-2å°†æ¥è‡ªè¯­è¨€å’Œè¿åŠ¨æç¤ºçš„æ ‡è®°ç»“åˆèµ·æ¥ç”ŸæˆæŒ‡ä»¤ï¼Œå¹¶å®ç°äº†ä¸€ç§å¤šæ¨¡æ€é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒæ–¹æ³•æ¥æœ‰æ•ˆåœ°è®­ç»ƒMotionGPT-2ã€‚<br>3. **å®ç°æ•´ä½“äººç±»è¿åŠ¨çš„ç²¾ç¡®ç¦»æ•£è¡¨ç¤º**ï¼šä¸ºäº†è§£å†³æ•´ä½“äººç±»è¿åŠ¨ç”Ÿæˆä»»åŠ¡ï¼Œæœ¬æ–‡å¼•å…¥äº†è¿åŠ¨ç¦»æ•£åŒ–æ¡†æ¶Part-Aware VQVAEï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸¤ä¸ªçº§åˆ«çš„ç¦»æ•£ç æœ¬å’Œè¿åŠ¨ç¼–ç å™¨æ¥å­¦ä¹ èº«ä½“å’Œæ‰‹çš„è¡¨ç¤ºã€‚è¿™ç§ä¸¤çº§çš„ç¦»æ•£åŒ–æ¡†æ¶æ•è·äº†ç»†å¾®çš„æ‰‹éƒ¨è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒäº†æ•´ä½“çš„èº«ä½“åŠ¨åŠ›å­¦ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨HumanML3Dã€KIT-MLå’ŒMotion-Xæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒMotionGPT-2åœ¨å¤šä¸ªè¿åŠ¨ç›¸å…³ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„å…·æœ‰ç‰¹å®šæ¡†æ¶çš„æ¨¡å‹ç›¸æ¯”ï¼ŒMotionGPT-2ä»…ä½¿ç”¨1%çš„é¢å¤–å‚æ•°å°±å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¹¶å°†è®­ç»ƒæ—¶é—´æ˜¾è‘—å‡å°‘åˆ°å…¶ä»–æ–¹æ³•çš„10%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MotionGPT-2ä¸ºè¿åŠ¨ç”Ÿæˆå’Œç†è§£é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ï¼Œå…¶å¤šæ¨¡æ€æ§åˆ¶ã€ä»»åŠ¡æ— å…³æ¡†æ¶å’Œç²¾ç¡®çš„ç¦»æ•£è¡¨ç¤ºæ–¹æ³•ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å‡ºè‰²è¡¨ç°è¡¨æ˜ï¼ŒLLMsåœ¨è¿åŠ¨ç”Ÿæˆå’Œç†è§£æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</td>
      <td>In this paper, we introduce Janus, an autoregressive framework that unifies<br>multimodal understanding and generation. Prior research often relies on a<br>single visual encoder for both tasks, such as Chameleon. However, due to the<br>differing levels of information granularity required by multimodal<br>understanding and generation, this approach can lead to suboptimal performance,<br>particularly in multimodal understanding. To address this issue, we decouple<br>visual encoding into separate pathways, while still leveraging a single,<br>unified transformer architecture for processing. The decoupling not only<br>alleviates the conflict between the visual encoder's roles in understanding and<br>generation, but also enhances the framework's flexibility. For instance, both<br>the multimodal understanding and generation components can independently select<br>their most suitable encoding methods. Experiments show that Janus surpasses<br>previous unified model and matches or exceeds the performance of task-specific<br>models. The simplicity, high flexibility, and effectiveness of Janus make it a<br>strong candidate for next-generation unified multimodal models.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Janusï¼šè§£è€¦è§†è§‰ç¼–ç ï¼Œç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹å¾€å¾€ä½¿ç”¨å•ä¸€çš„è§†è§‰ç¼–ç å™¨æ¥å¤„ç†ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºJanusï¼Œä¸€ä¸ªè§£è€¦è§†è§‰ç¼–ç çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§£è€¦è§†è§‰ç¼–ç <br>Januså¼•å…¥ä¸¤ä¸ªç‹¬ç«‹çš„è§†è§‰ç¼–ç è·¯å¾„ï¼šä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£ï¼Œä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç”Ÿæˆï¼Œå¹¶é€šè¿‡ç»Ÿä¸€çš„Transformeræ¶æ„è¿›è¡Œå¤„ç†ã€‚è¿™ç§è§£è€¦è®¾è®¡å¯ä»¥ç¼“è§£è§†è§‰ç¼–ç å™¨åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„å†²çªï¼Œå¹¶æé«˜æ¡†æ¶çš„çµæ´»æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçµæ´»æ€§å’Œå¯æ‰©å±•æ€§<br>Januså…è®¸ç†è§£å’Œç”Ÿæˆä»»åŠ¡ç‹¬ç«‹é€‰æ‹©æœ€åˆé€‚çš„ç¼–ç æ–¹æ³•ï¼Œå¹¶å¯ä»¥è½»æ¾æ‰©å±•ä»¥æ”¯æŒæ›´å¤šè¾“å…¥æ¨¡æ€ï¼Œå¦‚ç‚¹äº‘ã€è„‘ç”µå›¾ä¿¡å·æˆ–éŸ³é¢‘æ•°æ®ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Janusåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹ï¼Œå¹¶åœ¨æŸäº›æ–¹é¢ç”šè‡³è¶…è¿‡äº†ç‰¹å®šä»»åŠ¡æ¨¡å‹ã€‚åœ¨å¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusåœ¨MMBenchã€SEED-Benchå’ŒPOPEä¸Šåˆ†åˆ«å–å¾—äº†69.4ã€63.7å’Œ87.0çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†LLaVA-v1.5å’ŒQwen-VL-Chatç­‰æ¨¡å‹ã€‚åœ¨è§†è§‰ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒJanusåœ¨MSCOCO-30Kå’ŒGenEvalä¸Šåˆ†åˆ«å–å¾—äº†8.53çš„FIDåˆ†æ•°å’Œ61%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†DALL-E 2å’ŒSDXLç­‰æ¨¡å‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Janusçš„è®¾è®¡ç†å¿µä¸ºå¼€å‘ä¸‹ä¸€ä»£ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶è§£è€¦è§†è§‰ç¼–ç çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„å†²çªï¼Œå¹¶æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼ŒJanusçš„æ¶æ„ç®€å•ã€æ˜“äºæ‰©å±•ï¼Œå¯ä»¥è½»æ¾åœ°é›†æˆæ›´å¤šè¾“å…¥æ¨¡æ€ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå¼ºå¤§çš„å¤šæ¨¡æ€é€šç”¨æ¨¡å‹ã€‚</td>
    </tr>
    <tr>
      <th>15</th>
      <td>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</td>
      <td>Recent advancements in multimodal foundation models have yielded significant<br>progress in vision-language understanding. Initial attempts have also explored<br>the potential of multimodal large language models (MLLMs) for visual content<br>generation. However, existing works have insufficiently addressed the varying<br>granularity demands of different image generation tasks within a unified MLLM<br>paradigm - from the diversity required in text-to-image generation to the<br>precise controllability needed in image manipulation. In this work, we propose<br>PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA<br>unifies multi-granular visual features as both inputs and outputs of MLLMs,<br>elegantly addressing the different granularity requirements of various image<br>generation tasks within a unified MLLM framework. Following multimodal<br>pretraining and task-specific instruction tuning, PUMA demonstrates proficiency<br>in a wide range of multimodal tasks. This work represents a significant step<br>towards a truly unified MLLM capable of adapting to the granularity demands of<br>various visual tasks. The code and model will be released in<br>https://github.com/rongyaofang/PUMA.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | PUMAï¼šå¤šç²’åº¦è§†è§‰ç”Ÿæˆèµ‹èƒ½ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è§†è§‰-è¯­è¨€ç†è§£æ–¹é¢çš„æ˜¾è‘—è¿›æ­¥ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ä¹Ÿé€æ¸è¢«æ¢ç´¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œåœ¨ç»Ÿä¸€MLLMæ¡†æ¶å†…å¤„ç†ä¸åŒå›¾åƒç”Ÿæˆä»»åŠ¡æ—¶ï¼Œå¯¹ç²’åº¦éœ€æ±‚çš„å·®å¼‚è€ƒè™‘ä¸è¶³ã€‚ä¾‹å¦‚ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆéœ€è¦å¤šæ ·æ€§ï¼Œè€Œå›¾åƒç¼–è¾‘åˆ™éœ€è¦ç²¾ç¡®çš„å¯æ§æ€§ã€‚ç°æœ‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºä»è§†è§‰ç¼–ç å™¨ä¸­æå–çš„å•ç²’åº¦ç‰¹å¾ï¼Œå¿½ç•¥äº†ä¸åŒä»»åŠ¡å¯¹ç²’åº¦éœ€æ±‚çš„å·®å¼‚ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PUMAï¼ˆemPowering Unified MLLM with Multi-grAnular visual generationï¼‰ï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šç²’åº¦MLLMï¼Œå®ƒå°†å¤šç²’åº¦è§†è§‰ç‰¹å¾ä½œä¸ºMLLMçš„è¾“å…¥å’Œè¾“å‡ºï¼Œä¼˜é›…åœ°è§£å†³äº†ä¸åŒå›¾åƒç”Ÿæˆä»»åŠ¡åœ¨ç»Ÿä¸€MLLMæ¡†æ¶å†…çš„ç²’åº¦éœ€æ±‚å·®å¼‚ã€‚<br><br>PUMAçš„ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç²’åº¦è§†è§‰ç¼–ç å’Œè§£ç <br>PUMAä½¿ç”¨è¯­ä¹‰å›¾åƒç¼–ç å™¨æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ä¸€ç»„ä¸“é—¨çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºè§£ç å™¨ï¼Œä»¥å¤„ç†ä¸åŒç²’åº¦çš„ç‰¹å¾ã€‚è¿™ç§è®¾è®¡ä½¿å¾—PUMAèƒ½å¤Ÿåœ¨ä¿æŒè¯­ä¹‰å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°ç”Ÿæˆå¤šæ ·æ€§ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”å„ç§è§†è§‰ä»»åŠ¡çš„éœ€æ±‚ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç²’åº¦è‡ªå›å½’MLLM<br>PUMAè®¾è®¡äº†ä¸€ä¸ªè‡ªå›å½’MLLMï¼Œå®ƒå¯ä»¥å¤„ç†å’Œç”Ÿæˆæ–‡æœ¬æ ‡è®°ä»¥åŠå¤šç²’åº¦å›¾åƒç‰¹å¾ã€‚è¯¥æ¨¡å‹é€šè¿‡é€æ­¥é¢„æµ‹æ¯ä¸ªç²’åº¦çº§åˆ«çš„æ¯ä¸ªæ ‡è®°ï¼Œå¹¶ä»æœ€ç²—ç²’åº¦çº§åˆ«åˆ°æœ€ç»†ç²’åº¦çº§åˆ«è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œèƒ½å¤Ÿæ•è·ä¸åŒå°ºåº¦ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>PUMAåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š<br><br>* **å¤šæ ·åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ**ï¼šPUMAèƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æ¡ä»¶ç›¸å¯¹åº”çš„å¤šæ ·åŒ–å›¾åƒã€‚<br>* **å›¾åƒç¼–è¾‘**ï¼šPUMAèƒ½å¤Ÿè¿›è¡Œç²¾ç¡®çš„å›¾åƒç¼–è¾‘ï¼Œä¾‹å¦‚æ·»åŠ ã€åˆ é™¤æˆ–æ›¿æ¢å›¾åƒä¸­çš„å…ƒç´ ã€‚<br>* **æ¡ä»¶å›¾åƒç”Ÿæˆ**ï¼šPUMAèƒ½å¤Ÿæ ¹æ®ç‰¹å®šæ¡ä»¶ç”Ÿæˆå›¾åƒï¼Œä¾‹å¦‚å°†Cannyè¾¹ç¼˜å›¾åƒè½¬æ¢ä¸ºè‡ªç„¶å›¾åƒã€‚<br>* **å›¾åƒç†è§£**ï¼šPUMAåœ¨å›¾åƒç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚è§†è§‰é—®ç­”å’Œè§†è§‰æ¨ç†ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>PUMAçš„å¤šç²’åº¦è§†è§‰ç”Ÿæˆæ¡†æ¶ä¸ºMLLMsåœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶å¤šç²’åº¦ç‰¹å¾ç¼–ç å’Œè§£ç çš„è®¾è®¡ï¼Œä»¥åŠå¤šç²’åº¦è‡ªå›å½’MLLMçš„æ¶æ„ï¼Œéƒ½å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢ã€‚æ­¤å¤–ï¼ŒPUMAåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ä¹Ÿè¡¨æ˜ï¼Œå¤šç²’åº¦æ–¹æ³•åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>16</th>
      <td>MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling</td>
      <td>Recent advancements in multi-modal large language models have propelled the<br>development of joint probabilistic models capable of both image understanding<br>and generation. However, we have identified that recent methods inevitably<br>suffer from loss of image information during understanding task, due to either<br>image discretization or diffusion denoising steps. To address this issue, we<br>propose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling<br>framework. Unlike discretization line of method, MMAR takes in<br>continuous-valued image tokens to avoid information loss. Differing from<br>diffusion-based approaches, we disentangle the diffusion process from<br>auto-regressive backbone model by employing a light-weight diffusion head on<br>top each auto-regressed image patch embedding. In this way, when the model<br>transits from image generation to understanding through text generation, the<br>backbone model's hidden representation of the image is not limited to the last<br>denoising step. To successfully train our method, we also propose a<br>theoretically proven technique that addresses the numerical stability issue and<br>a training strategy that balances the generation and understanding task goals.<br>Through extensive evaluations on 18 image understanding benchmarks, MMAR<br>demonstrates much more superior performance than other joint multi-modal<br>models, matching the method that employs pretrained CLIP vision encoder,<br>meanwhile being able to generate high quality images at the same time. We also<br>showed that our method is scalable with larger data and model size.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MMARï¼šè¿ˆå‘æ— æŸå¤šæ¨¡æ€è‡ªå›å½’æ¦‚ç‡å»ºæ¨¡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¦‚ç‡æ¨¡å‹åœ¨å›¾åƒç†è§£ä»»åŠ¡ä¸­ä¸å¯é¿å…åœ°ä¼šä¸¢å¤±å›¾åƒä¿¡æ¯ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå›¾åƒç¦»æ•£åŒ–æˆ–æ‰©æ•£å»å™ªæ­¥éª¤å¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€è‡ªå›å½’ï¼ˆMMARï¼‰æ¦‚ç‡å»ºæ¨¡æ¡†æ¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¿ç»­å›¾åƒè¡¨ç¤º<br>ä¸ç¦»æ•£åŒ–æ–¹æ³•ä¸åŒï¼ŒMMARé‡‡ç”¨è¿ç»­å€¼å›¾åƒæ ‡è®°ï¼Œé¿å…äº†ä¿¡æ¯ä¸¢å¤±ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„æ‰©æ•£å¤´ï¼Œå°†æ‰©æ•£è¿‡ç¨‹ä¸è‡ªå›å½’éª¨å¹²æ¨¡å‹è§£è€¦ï¼Œä»è€Œåœ¨å›¾åƒç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸­å……åˆ†åˆ©ç”¨å›¾åƒå»ºæ¨¡èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½ç²¾åº¦è®­ç»ƒä¸‹çš„æ•°å€¼ç¨³å®šæ€§<br>é’ˆå¯¹ä½ç²¾åº¦è®­ç»ƒè®¾ç½®ä¸‹æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æ•°å€¼ç²¾åº¦æ•æ„Ÿçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç†è®ºä¸Šè¯æ˜çš„æŠ€æœ¯ï¼Œé€šè¿‡ä¼˜åŒ–æ‰©æ•£æ¨¡å‹å‚æ•°åŒ–æ¥æœ€å°åŒ–æ•°å€¼è¯¯å·®ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¹³è¡¡ç”Ÿæˆå’Œç†è§£ä»»åŠ¡çš„è®­ç»ƒç­–ç•¥<br>ä¸ºäº†å¹³è¡¡å›¾åƒç”Ÿæˆå’Œç†è§£ä»»åŠ¡çš„ç›®æ ‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§è§„æ¨¡ã€ä¸­ç­‰è´¨é‡çš„æ•°æ®æ¥å¢å¼ºæ¨¡å‹çš„æ•°æ®åˆ†å¸ƒå¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®æ¥è¿›ä¸€æ­¥æé«˜å›¾åƒç”Ÿæˆèƒ½åŠ›å¹¶ç»†åŒ–æ¨¡å‹å¯¹å›¾åƒçš„ç†è§£ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨18ä¸ªå›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMMARè¡¨ç°å‡ºæ¯”å…¶ä»–å¤šæ¨¡æ€æ¨¡å‹æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¸ä½¿ç”¨é¢„è®­ç»ƒCLIPè§†è§‰ç¼–ç å™¨çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒMMARæ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§ï¼Œéšç€æ•°æ®é‡å’Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œæ€§èƒ½ä¹Ÿä¼šå¾—åˆ°æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MMARæ¡†æ¶ä¸ºå¤šæ¨¡æ€æ¦‚ç‡å»ºæ¨¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡è¿ç»­å›¾åƒè¡¨ç¤ºå’Œä½ç²¾åº¦è®­ç»ƒä¸‹çš„æ•°å€¼ç¨³å®šæ€§æŠ€æœ¯ï¼Œå®ç°äº†æ— æŸçš„å¤šæ¨¡æ€è‡ªå›å½’æ¦‚ç‡å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ä¸ºå¹³è¡¡ç”Ÿæˆå’Œç†è§£ä»»åŠ¡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›åˆ›æ–°ç‚¹å¯¹äºå¤šæ¨¡æ€æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Looking at CTR Prediction Again: Is Attention All You Need?</td>
      <td>Click-through rate (CTR) prediction is a critical problem in web search,<br>recommendation systems and online advertisement displaying. Learning good<br>feature interactions is essential to reflect user's preferences to items. Many<br>CTR prediction models based on deep learning have been proposed, but<br>researchers usually only pay attention to whether state-of-the-art performance<br>is achieved, and ignore whether the entire framework is reasonable. In this<br>work, we use the discrete choice model in economics to redefine the CTR<br>prediction problem, and propose a general neural network framework built on<br>self-attention mechanism. It is found that most existing CTR prediction models<br>align with our proposed general framework. We also examine the expressive power<br>and model complexity of our proposed framework, along with potential extensions<br>to some existing models. And finally we demonstrate and verify our insights<br>through some experimental results on public datasets.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é‡æ–°å®¡è§†ç‚¹å‡»ç‡é¢„æµ‹ï¼šæ³¨æ„åŠ›æœºåˆ¶çœŸçš„è¶³å¤Ÿå—ï¼Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç‚¹å‡»ç‡ï¼ˆCTRï¼‰é¢„æµ‹æ˜¯ç½‘ç»œæœç´¢ã€æ¨èç³»ç»Ÿå’Œåœ¨çº¿å¹¿å‘Šæ˜¾ç¤ºä¸­çš„å…³é”®é—®é¢˜ã€‚å­¦ä¹ è‰¯å¥½çš„ç‰¹å¾äº¤äº’å¯¹äºåæ˜ ç”¨æˆ·å¯¹ç‰©å“çš„åå¥½è‡³å…³é‡è¦ã€‚è®¸å¤šåŸºäºæ·±åº¦å­¦ä¹ çš„CTRé¢„æµ‹æ¨¡å‹å·²è¢«æå‡ºï¼Œä½†ç ”ç©¶äººå‘˜é€šå¸¸åªå…³æ³¨æ˜¯å¦è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œå¿½ç•¥äº†æ•´ä¸ªæ¡†æ¶æ˜¯å¦åˆç†ã€‚æœ¬æ–‡ä½¿ç”¨ç»æµå­¦ä¸­çš„ç¦»æ•£é€‰æ‹©æ¨¡å‹é‡æ–°å®šä¹‰äº†CTRé¢„æµ‹é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é€šç”¨ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡ä½¿ç”¨ç»æµå­¦ä¸­çš„ç¦»æ•£é€‰æ‹©æ¨¡å‹é‡æ–°å®šä¹‰äº†CTRé¢„æµ‹é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„é€šç”¨ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åµŒå…¥å±‚ã€ç‰¹å¾äº¤äº’å±‚ã€èšåˆå±‚å’Œç©ºé—´å˜æ¢å±‚ï¼Œå¯ä»¥æ¶µç›–å¤§å¤šæ•°ç°æœ‰çš„CTRé¢„æµ‹æ¨¡å‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾äº¤äº’å½¢å¼ï¼Œå¯ä»¥åŒ…å«å¤§å¤šæ•°ç°æœ‰CTRé¢„æµ‹æ¨¡å‹çš„ç‰¹å¾å¤„ç†åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†è¯¥æ¡†æ¶ä¸­ç‰¹å¾äº¤äº’ç®—å­çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†æ‰©å±•å…ˆå‰æ¨¡å‹çš„æ–¹æ³•ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¸å¤§å¤šæ•°ç°æœ‰çš„CTRæ¨¡å‹ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºå®ƒä»¬ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„é€šç”¨ç¥ç»ç½‘ç»œæ¡†æ¶ä¸ºCTRé¢„æµ‹é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾äº¤äº’å½¢å¼å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æ¨èç³»ç»Ÿã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚</td>
    </tr>
    <tr>
      <th>18</th>
      <td>MIO: A Foundation Model on Multimodal Tokens</td>
      <td>In this paper, we introduce MIO, a novel foundation model built on multimodal<br>tokens, capable of understanding and generating speech, text, images, and<br>videos in an end-to-end, autoregressive manner. While the emergence of large<br>language models (LLMs) and multimodal large language models (MM-LLMs) propels<br>advancements in artificial general intelligence through their versatile<br>capabilities, they still lack true any-to-any understanding and generation.<br>Recently, the release of GPT-4o has showcased the remarkable potential of<br>any-to-any LLMs for complex real-world tasks, enabling omnidirectional input<br>and output across images, speech, and text. However, it is closed-source and<br>does not support the generation of multimodal interleaved sequences. To address<br>this gap, we present MIO, which is trained on a mixture of discrete tokens<br>across four modalities using causal multimodal modeling. MIO undergoes a<br>four-stage training process: (1) alignment pre-training, (2) interleaved<br>pre-training, (3) speech-enhanced pre-training, and (4) comprehensive<br>supervised fine-tuning on diverse textual, visual, and speech tasks. Our<br>experimental results indicate that MIO exhibits competitive, and in some cases<br>superior, performance compared to previous dual-modal baselines, any-to-any<br>model baselines, and even modality-specific baselines. Moreover, MIO<br>demonstrates advanced capabilities inherent to its any-to-any feature, such as<br>interleaved video-text generation, chain-of-visual-thought reasoning, visual<br>guideline generation, instructional image editing, etc.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MIOï¼šåŸºäºå¤šæ¨¡æ€tokençš„é€šç”¨æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMM-LLMsï¼‰çš„å…´èµ·ï¼Œäººå·¥æ™ºèƒ½åœ¨é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å–å¾—äº†å·¨å¤§è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MM-LLMsä»ç„¶ç¼ºä¹çœŸæ­£çš„â€œä»»æ„åˆ°ä»»æ„â€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå³æ— æ³•åœ¨å¤šç§æ¨¡æ€ä¹‹é—´è¿›è¡Œæ— ç¼è½¬æ¢å’Œç”Ÿæˆã€‚ä¾‹å¦‚ï¼ŒGPT-4oè™½ç„¶å±•ç¤ºäº†å¼ºå¤§çš„â€œä»»æ„åˆ°ä»»æ„â€LLMsæ½œåŠ›ï¼Œä½†å®ƒä¸æ”¯æŒå¤šæ¨¡æ€äº¤é”™åºåˆ—çš„ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MIOï¼Œä¸€ä¸ªåŸºäºå¤šæ¨¡æ€tokençš„é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè¯­éŸ³ã€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ï¼Œå¹¶æ”¯æŒå¤šæ¨¡æ€äº¤é”™åºåˆ—çš„ç”Ÿæˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ¨¡æ€tokenization<br>MIOä½¿ç”¨SEED-Tokenizerå¯¹å›¾åƒè¿›è¡Œtokenizationï¼Œä½¿ç”¨SpeechTokenizerå¯¹è¯­éŸ³è¿›è¡Œtokenizationï¼Œå¹¶å°†è¿™äº›tokenæ·»åŠ åˆ°LLMçš„è¯æ±‡è¡¨ä¸­ã€‚ç”±äºå›¾åƒtokenå’Œè¯­éŸ³tokenéƒ½åŒ…å«å› æœè¯­ä¹‰ï¼Œå› æ­¤å¯ä»¥åƒæ–‡æœ¬tokenä¸€æ ·è¿›è¡Œè‡ªå›å½’è®­ç»ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå› æœå¤šæ¨¡æ€å»ºæ¨¡<br>MIOä½¿ç”¨å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausalLMï¼‰è¿›è¡Œè®­ç»ƒï¼Œå°†å¤šæ¨¡æ€tokençš„ç”Ÿæˆç›®æ ‡ç»Ÿä¸€ä¸ºä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ¨¡æ€äº¤é”™åºåˆ—ç”Ÿæˆ<br>MIOæ”¯æŒå¤šæ¨¡æ€äº¤é”™åºåˆ—çš„ç”Ÿæˆï¼Œä¾‹å¦‚è§†é¢‘-æ–‡æœ¬äº¤é”™åºåˆ—ï¼Œè¿™ä½¿å¾—MIOèƒ½å¤Ÿè¿›è¡Œè§†è§‰æ•…äº‹è®²è¿°ã€è§†è§‰æ€ç»´é“¾æ¨ç†ç­‰é«˜çº§ä»»åŠ¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå››é˜¶æ®µè®­ç»ƒè¿‡ç¨‹<br>MIOé‡‡ç”¨å››é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š<br>1. å¯¹é½é¢„è®­ç»ƒï¼šå­¦ä¹ ä¸è¯­è¨€ç©ºé—´æ›´å¯¹é½çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚<br>2. äº¤é”™é¢„è®­ç»ƒï¼šè·å¾—å…·æœ‰æ›´ä¸°å¯Œä¸Šä¸‹æ–‡è¯­ä¹‰çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚<br>3. è¯­éŸ³å¢å¼ºé¢„è®­ç»ƒï¼šå¢å¼ºæ¨¡å‹çš„è¯­éŸ³ç›¸å…³èƒ½åŠ›ã€‚<br>4. ç»¼åˆç›‘ç£å¾®è°ƒï¼šåœ¨å¤šç§æ–‡æœ¬ã€è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MIOåœ¨å›¾åƒç†è§£ã€å›¾åƒç”Ÿæˆã€è¯­éŸ³ç†è§£å’Œè¯­éŸ³ç”Ÿæˆã€è§†é¢‘ç†è§£å’Œè§†é¢‘ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰åŒæ¨¡æ€æ¨¡å‹å’Œâ€œä»»æ„åˆ°ä»»æ„â€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MIOçš„è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹ä¸ºæ„å»ºé€šç”¨å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶å¤šæ¨¡æ€äº¤é”™åºåˆ—ç”Ÿæˆèƒ½åŠ›ä¹Ÿä¸ºè§†è§‰æ•…äº‹è®²è¿°ã€è§†è§‰æ€ç»´é“¾æ¨ç†ç­‰é«˜çº§ä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>19</th>
      <td>MonoFormer: One Transformer for Both Diffusion and Autoregression</td>
      <td>Most existing multimodality methods use separate backbones for<br>autoregression-based discrete text generation and diffusion-based continuous<br>visual generation, or the same backbone by discretizing the visual data to use<br>autoregression for both text and visual generation. In this paper, we propose<br>to study a simple idea: share one transformer for both autoregression and<br>diffusion. The feasibility comes from two main aspects: (i) Transformer is<br>successfully applied to diffusion for visual generation, and (ii) transformer<br>training for autoregression and diffusion is very similar, and the difference<br>merely lies in that diffusion uses bidirectional attention mask and<br>autoregression uses causal attention mask. Experimental results show that our<br>approach achieves comparable image generation performance to current<br>state-of-the-art methods as well as maintains the text generation capability.<br>The project is publicly available at https://monoformer.github.io/.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MonoFormerï¼šä¸€ç®­åŒé›•ï¼ŒTransformeråŒæ—¶å¤„ç†æ‰©æ•£å’Œè‡ªå›å½’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç°æœ‰çš„å¤šæ¨¡æ€æ–¹æ³•é€šå¸¸ä¸ºåŸºäºè‡ªå›å½’çš„ç¦»æ•£æ–‡æœ¬ç”Ÿæˆå’ŒåŸºäºæ‰©æ•£çš„è¿ç»­è§†è§‰ç”Ÿæˆä½¿ç”¨ç‹¬ç«‹çš„éª¨å¹²ç½‘ç»œï¼Œæˆ–è€…é€šè¿‡å°†è§†è§‰æ•°æ®ç¦»æ•£åŒ–æ¥ä½¿ç”¨è‡ªå›å½’è¿›è¡Œæ–‡æœ¬å’Œè§†è§‰ç”Ÿæˆã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜ï¼Œä¸”éš¾ä»¥å®ç°æ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å…±äº«ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>MonoFormer æå‡ºäº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æƒ³æ³•ï¼šå…±äº«ä¸€ä¸ª Transformer è¿›è¡Œè‡ªå›å½’å’Œæ‰©æ•£ã€‚å…¶å¯è¡Œæ€§ä¸»è¦åŸºäºä»¥ä¸‹ä¸¤ç‚¹ï¼š<br>1. Transformer å·²æˆåŠŸåº”ç”¨äºæ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰ç”Ÿæˆã€‚<br>2. Transformer åœ¨è‡ªå›å½’å’Œæ‰©æ•£è®­ç»ƒä¸­çš„è¿‡ç¨‹éå¸¸ç›¸ä¼¼ï¼Œä¸»è¦åŒºåˆ«åœ¨äºæ‰©æ•£ä½¿ç”¨åŒå‘æ³¨æ„åŠ›æ©ç ï¼Œè€Œè‡ªå›å½’ä½¿ç”¨å› æœæ³¨æ„åŠ›æ©ç ã€‚<br><br>MonoFormer ä½¿ç”¨ä¸€ä¸ªå…±äº«çš„ Transformer è¿›è¡Œæ–‡æœ¬ç”Ÿæˆå’Œå›¾åƒç”Ÿæˆã€‚å¯¹äºæ–‡æœ¬ç”Ÿæˆï¼ŒTransformer æ¥æ”¶æ–‡æœ¬ token åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œå¹¶é€ä¸ªé¢„æµ‹è¾“å‡º tokenã€‚å¯¹äºå›¾åƒç”Ÿæˆï¼ŒTransformer æ¥æ”¶å™ªå£°åŒ–çš„æ½œåœ¨åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹å™ªå£°ï¼Œç„¶åé€šè¿‡ VAE è§£ç å™¨ç”Ÿæˆå›¾åƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ ImageNet æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMonoFormer åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢ï¼ŒMonoFormer çš„æ€§èƒ½ç•¥ä½äº TinyLlama åŸºçº¿æ¨¡å‹ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ··åˆè®­ç»ƒå›¾åƒç”Ÿæˆæ•°æ®é›†å¯¼è‡´çš„ã€‚æœªæ¥å¯ä»¥é€šè¿‡å¢åŠ æ›´å¤šè¯­è¨€æ•°æ®æ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MonoFormer çš„ä¸»è¦åˆ›æ–°ç‚¹åœ¨äºå…±äº«ä¸€ä¸ª Transformer è¿›è¡Œè‡ªå›å½’å’Œæ‰©æ•£ï¼Œä»è€Œå®ç°äº†æ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†å…±äº«ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒMonoFormer è¿˜é‡‡ç”¨äº†é¢„è®­ç»ƒçš„ LLM è¿›è¡Œ Transformer åˆå§‹åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›æ–¹æ³•å¯ä»¥ä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„æ„å»ºæä¾›æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>20</th>
      <td>VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</td>
      <td>VILA-U is a Unified foundation model that integrates Video, Image, Language<br>understanding and generation. Traditional visual language models (VLMs) use<br>separate modules for understanding and generating visual content, which can<br>lead to misalignment and increased complexity. In contrast, VILA-U employs a<br>single autoregressive next-token prediction framework for both tasks,<br>eliminating the need for additional components like diffusion models. This<br>approach not only simplifies the model but also achieves near state-of-the-art<br>performance in visual language understanding and generation. The success of<br>VILA-U is attributed to two main factors: the unified vision tower that aligns<br>discrete visual tokens with textual inputs during pretraining, which enhances<br>visual perception, and autoregressive image generation can achieve similar<br>quality as diffusion models with high-quality dataset. This allows VILA-U to<br>perform comparably to more complex models using a fully token-based<br>autoregressive framework.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | VILA-Uï¼šç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆçš„åŸºç¡€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šå¸¸ä½¿ç”¨ç‹¬ç«‹çš„æ¨¡å—æ¥ç†è§£å’Œç”Ÿæˆè§†è§‰å†…å®¹ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å—ä¹‹é—´çš„ä¸åŒ¹é…å’Œæ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VILA-Uï¼Œä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ƒå°†è§†é¢‘ã€å›¾åƒå’Œè¯­è¨€çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡æ•´åˆåˆ°ä¸€ä¸ªè‡ªå›å½’çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¡†æ¶ä¸­ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€çš„è§†è§‰å¡”<br>VILA-Ué‡‡ç”¨ç»Ÿä¸€çš„è§†è§‰å¡”ï¼Œé€šè¿‡å‘é‡é‡åŒ–å°†è§†è§‰è¾“å…¥è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œå¹¶ä¸æ–‡æœ¬è¾“å…¥è¿›è¡Œå¯¹é½ã€‚è¿™ç§å¯¹é½åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œï¼Œå¯ä»¥å¢å¼ºè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªå›å½’å›¾åƒç”Ÿæˆ<br>VILA-Uä½¿ç”¨è‡ªå›å½’æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡åœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè‡ªå›å½’å›¾åƒç”Ÿæˆå¯ä»¥è¾¾åˆ°ä¸æ‰©æ•£æ¨¡å‹ç›¸ä¼¼çš„è´¨é‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>VILA-Uåœ¨è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ¥è¿‘æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨è§†è§‰ç†è§£æ–¹é¢ï¼ŒVILA-Uçš„æ€§èƒ½æ¥è¿‘äºä½¿ç”¨è¿ç»­è§†è§‰æ ‡è®°çš„é¢†å…ˆVLMsï¼Œç”šè‡³åœ¨ç›¸åŒçš„LLMå¤§å°ä¸‹ï¼Œæ€§èƒ½è¶…è¿‡äº†è®¸å¤šæ–¹æ³•ã€‚åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢ï¼ŒVILA-Uçš„FIDå¾—åˆ†ä¼˜äºå…¶ä»–è‡ªå›å½’æ–¹æ³•ï¼Œå¹¶ä¸”ä¸ä¸€äº›åŸºäºæ‰©æ•£çš„æ–¹æ³•å…·æœ‰å¯æ¯”çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>VILA-Uçš„ç»Ÿä¸€æ¡†æ¶å¯ä»¥ä½œä¸ºä¸€ä¸ªé€šç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒVILA-Uçš„è§†è§‰å¡”å’Œè‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚éŸ³é¢‘å’Œæ–‡æœ¬çš„ç†è§£å’Œç”Ÿæˆã€‚</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Show-o: One Single Transformer to Unify Multimodal Understanding and Generation</td>
      <td>We present a unified transformer, i.e., Show-o, that unifies multimodal<br>understanding and generation. Unlike fully autoregressive models, Show-o<br>unifies autoregressive and (discrete) diffusion modeling to adaptively handle<br>inputs and outputs of various and mixed modalities. The unified model flexibly<br>supports a wide range of vision-language tasks including visual<br>question-answering, text-to-image generation, text-guided<br>inpainting/extrapolation, and mixed-modality generation. Across various<br>benchmarks, it demonstrates comparable or superior performance to existing<br>individual models with an equivalent or larger number of parameters tailored<br>for understanding or generation. This significantly highlights its potential as<br>a next-generation foundation model. Code and models are released at<br>https://github.com/showlab/Show-o.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Show-oï¼šä¸€ä¸ªTransformerç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€æ™ºèƒ½çš„ä¸¤ä¸ªå…³é”®æ”¯æŸ±â€”â€”ç†è§£å’Œç”Ÿæˆå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å¤šæ¨¡æ€ç†è§£æ–¹é¢ï¼Œå¦‚LLaVAç­‰æ¨¡å‹åœ¨è§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼›è§†è§‰ç”Ÿæˆæ–¹é¢ï¼Œå¦‚DDPMsç­‰æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹å¤§å¤šå°†æ¯ä¸ªé¢†åŸŸç‹¬ç«‹å¤„ç†ï¼Œç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¥åŒæ—¶å¤„ç†è¿™ä¸¤ä¸ªä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€Transformeræ¶æ„<br>Show-oæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„Transformeræ¶æ„ï¼Œå°†è‡ªå›å½’å’Œæ‰©æ•£å»ºæ¨¡ç›¸ç»“åˆï¼Œä»¥é€‚åº”ä¸åŒå’Œæ··åˆæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºã€‚è¿™ä½¿å¾—Show-oèƒ½å¤Ÿçµæ´»åœ°æ”¯æŒå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†è§‰é—®ç­”ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼çš„ä¿®å¤/å¤–æ¨å’Œæ··åˆæ¨¡æ€ç”Ÿæˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šOmni-Attentionæœºåˆ¶<br>Show-oå¼•å…¥äº†Omni-Attentionæœºåˆ¶ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å› æœå’Œå…¨æ³¨æ„åŠ›æœºåˆ¶çš„å…¨é¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥åºåˆ—çš„æ ¼å¼è‡ªé€‚åº”åœ°æ··åˆå’Œå˜åŒ–ã€‚è¿™ä½¿å¾—Show-oèƒ½å¤Ÿä»¥ä¸åŒçš„æ–¹å¼å¤„ç†æ–‡æœ¬å’Œå›¾åƒä¿¡å·ï¼Œä»è€Œæé«˜æ¨¡å‹æ•ˆç‡å’Œä¸‹æ¸¸åº”ç”¨çš„æ€§èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç¦»æ•£æ‰©æ•£å»ºæ¨¡<br>Show-oé‡‡ç”¨ç¦»æ•£æ‰©æ•£å»ºæ¨¡æ¥å¤„ç†å›¾åƒç”Ÿæˆï¼Œè€Œä¸æ˜¯è¿ç»­æ‰©æ•£ã€‚è¿™ä½¿å¾—Show-oèƒ½å¤Ÿä»¥æ›´å°‘çš„é‡‡æ ·æ­¥éª¤ç”Ÿæˆå›¾åƒï¼Œä»è€Œæé«˜ç”Ÿæˆæ•ˆç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒShow-oåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºä¸ç°æœ‰æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨MSCOCOæ•°æ®é›†ä¸Šï¼ŒShow-oçš„FIDåˆ†æ•°ä¸º9.24ï¼Œä¼˜äºè®¸å¤šå‚æ•°è§„æ¨¡æ›´å¤§çš„ç”Ÿæˆæ¨¡å‹ã€‚åœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­ï¼ŒShow-oåœ¨æ‰€æœ‰å…­ä¸ªæŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—ä¼˜äºLDMç­‰æ¨¡å‹çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Show-oçš„æˆåŠŸè¡¨æ˜ï¼Œå°†è‡ªå›å½’å’Œæ‰©æ•£å»ºæ¨¡ç›¸ç»“åˆçš„ç»Ÿä¸€Transformeræ¶æ„å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚Omni-Attentionæœºåˆ¶å’Œç¦»æ•£æ‰©æ•£å»ºæ¨¡ç­‰æŠ€æœ¯ä¹Ÿä¸ºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆé¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒShow-oçš„å®éªŒç»“æœä¹Ÿä¸ºæœªæ¥ç»Ÿä¸€æ¨¡å‹çš„è®¾è®¡æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚<br><br>## ğŸ“š å‚è€ƒèµ„æ–™<br>[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://arxiv.org/abs/2408.12528)</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</td>
      <td>We introduce Transfusion, a recipe for training a multi-modal model over<br>discrete and continuous data. Transfusion combines the language modeling loss<br>function (next token prediction) with diffusion to train a single transformer<br>over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B<br>parameters from scratch on a mixture of text and image data, establishing<br>scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our<br>experiments show that Transfusion scales significantly better than quantizing<br>images and training a language model over discrete image tokens. By introducing<br>modality-specific encoding and decoding layers, we can further improve the<br>performance of Transfusion models, and even compress each image to just 16<br>patches. We further demonstrate that scaling our Transfusion recipe to 7B<br>parameters and 2T multi-modal tokens produces a model that can generate images<br>and text on a par with similar scale diffusion models and language models,<br>reaping the benefits of both worlds.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Transfusionï¼šä¸€ç§é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°å¹¶æ‰©æ•£å›¾åƒçš„å¤šæ¨¡æ€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹éœ€è¦èƒ½å¤Ÿæ„ŸçŸ¥ã€å¤„ç†å’Œç”Ÿæˆç¦»æ•£å…ƒç´ ï¼ˆå¦‚æ–‡æœ¬æˆ–ä»£ç ï¼‰å’Œè¿ç»­å…ƒç´ ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘æ•°æ®ï¼‰ã€‚è™½ç„¶åŸºäºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨ç¦»æ•£æ¨¡æ€ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æ‰©æ•£æ¨¡å‹åŠå…¶æ³›åŒ–å½¢å¼åœ¨ç”Ÿæˆè¿ç»­æ¨¡æ€æ–¹é¢å¤„äºæœ€å‰æ²¿ã€‚è®¸å¤šåŠªåŠ›æ—¨åœ¨ç»“åˆè¿™äº›æ–¹æ³•ï¼ŒåŒ…æ‹¬å°†è¯­è¨€æ¨¡å‹æ‰©å±•ä¸ºä½¿ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå·¥å…·ï¼Œæˆ–è€…å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å«æ¥åˆ°è¯­è¨€æ¨¡å‹ä¸Šã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯å°†è¿ç»­æ¨¡æ€é‡åŒ–ï¼Œå¹¶åœ¨ç¦»æ•£æ ‡è®°ä¸Šè®­ç»ƒæ ‡å‡†è¯­è¨€æ¨¡å‹ï¼Œä»è€Œç®€åŒ–æ¨¡å‹æ¶æ„ï¼Œä½†ä¼šä¸¢å¤±ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡è®­ç»ƒå•ä¸ªæ¨¡å‹æ¥é¢„æµ‹ç¦»æ•£æ–‡æœ¬æ ‡è®°å’Œæ‰©æ•£è¿ç»­å›¾åƒï¼Œä»è€Œå®Œå…¨æ•´åˆè¿™ä¸¤ç§æ¨¡æ€ï¼Œå¹¶ä¸”æ²¡æœ‰ä¿¡æ¯æŸå¤±çš„å¯èƒ½æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>Transfusion æ˜¯ä¸€ç§è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼åœ°ç”Ÿæˆç¦»æ•£å’Œè¿ç»­æ¨¡æ€ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ 50% æ–‡æœ¬å’Œ 50% å›¾åƒæ•°æ®ä¸Šé¢„è®­ç»ƒä¸€ä¸ª transformer æ¨¡å‹æ¥æ¼”ç¤º Transfusionï¼Œæ¯ä¸ªæ¨¡æ€ä½¿ç”¨ä¸åŒçš„ç›®æ ‡ï¼šæ–‡æœ¬ä½¿ç”¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼Œå›¾åƒä½¿ç”¨æ‰©æ•£ã€‚æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­éƒ½æš´éœ²äºä¸¤ç§æ¨¡æ€å’ŒæŸå¤±å‡½æ•°ã€‚æ ‡å‡†åµŒå…¥å±‚å°†æ–‡æœ¬æ ‡è®°è½¬æ¢ä¸ºå‘é‡ï¼Œè€Œåˆ†å—å±‚å°†æ¯ä¸ªå›¾åƒè¡¨ç¤ºä¸ºåˆ†å—å‘é‡ã€‚æˆ‘ä»¬å¯¹æ–‡æœ¬æ ‡è®°åº”ç”¨å› æœæ³¨æ„åŠ›ï¼Œå¹¶å¯¹å›¾åƒå—åº”ç”¨åŒå‘æ³¨æ„åŠ›ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§£ç ç®—æ³•ï¼Œè¯¥ç®—æ³•ç»“åˆäº†ä»è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ ‡å‡†åšæ³•å’Œä»æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„æ ‡å‡†åšæ³•ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ä¸ Chameleon çš„ç¦»æ•£åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒTransfusion æ¨¡å‹åœ¨æ¯ç§æ¨¡æ€ç»„åˆä¸­éƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ‰©å±•æ€§ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬å‘ç° Transfusion åœ¨ä¸åˆ° Chameleon è®¡ç®—é‡çš„ä¸‰åˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹è¶…è¿‡äº† Chameleon æ–¹æ³•ï¼Œæ— è®ºæ˜¯ FID è¿˜æ˜¯ CLIP åˆ†æ•°ã€‚å½“æ§åˆ¶ FLOPs æ—¶ï¼ŒTransfusion å®ç°äº†æ¯” Chameleon æ¨¡å‹ä½çº¦ 2 å€çš„ FID åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆä¸­è§‚å¯Ÿåˆ°ç±»ä¼¼çš„è¶‹åŠ¿ï¼Œå…¶ä¸­ Transfusion åœ¨ 21.8% çš„ FLOPs ä¸‹ä¸ Chameleon ç›¸åŒ¹é…ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒTransfusion åœ¨å­¦ä¹ æ–‡æœ¬åˆ°æ–‡æœ¬é¢„æµ‹æ–¹é¢ä¹Ÿæ›´æœ‰æ•ˆç‡ï¼Œåœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šå®ç°äº†å›°æƒ‘åº¦å¹³è¡¡ï¼Œå¤§çº¦æ˜¯ Chameleon çš„ FLOPs çš„ 50% åˆ° 60%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Transfusion æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„è®­ç»ƒçœŸæ­£å¤šæ¨¡æ€æ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡è®­ç»ƒå•ä¸ªæ¨¡å‹æ¥é¢„æµ‹ç¦»æ•£æ–‡æœ¬æ ‡è®°å’Œæ‰©æ•£è¿ç»­å›¾åƒï¼Œä»è€Œå®Œå…¨æ•´åˆäº†è¿™ä¸¤ç§æ¨¡æ€ï¼Œå¹¶ä¸”æ²¡æœ‰ä¿¡æ¯æŸå¤±ã€‚Transfusion åœ¨å„ç§æ¨¡æ€ç»„åˆä¸­éƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ‰©å±•æ€§ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬å’Œå›¾åƒã€‚æ­¤å¤–ï¼ŒTransfusion è¿˜å¯ä»¥ç”ŸæˆåŸºäºå…¶ä»–å›¾åƒçš„å›¾åƒï¼Œè¿™è¡¨æ˜å®ƒå¯ä»¥é€‚åº”å’Œæ³›åŒ–åˆ°æ–°çš„æ¨¡æ€ç»„åˆã€‚</td>
    </tr>
    <tr>
      <th>23</th>
      <td>ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation</td>
      <td>Previous open-source large multimodal models (LMMs) have faced several<br>limitations: (1) they often lack native integration, requiring adapters to<br>align visual representations with pre-trained large language models (LLMs); (2)<br>many are restricted to single-modal generation; (3) while some support<br>multimodal generation, they rely on separate diffusion models for visual<br>modeling and generation. To mitigate these limitations, we present Anole, an<br>open, autoregressive, native large multimodal model for interleaved image-text<br>generation. We build Anole from Meta AI's Chameleon, adopting an innovative<br>fine-tuning strategy that is both data-efficient and parameter-efficient. Anole<br>demonstrates high-quality, coherent multimodal generation capabilities. We have<br>open-sourced our model, training framework, and instruction tuning data.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ANOLEï¼šå¼€å¯å¤šæ¨¡æ€ç”Ÿæˆæ–°çºªå…ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼€æºå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚ç¼ºä¹åŸç”Ÿé›†æˆã€å•æ¨¡æ€ç”Ÿæˆé™åˆ¶ä»¥åŠä¾èµ–é¢å¤–çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰å»ºæ¨¡å’Œç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ANOLEï¼Œä¸€ä¸ªå¼€æ”¾ã€è‡ªå›å½’ã€åŸç”Ÿçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äºäº¤é”™å›¾åƒ-æ–‡æœ¬ç”Ÿæˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸç”Ÿå¤šæ¨¡æ€é›†æˆ<br>ANOLEåŸºäºMeta AIçš„Chameleonæ¨¡å‹ï¼Œé‡‡ç”¨åˆ›æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œå®ç°äº†è§†è§‰è¡¨ç¤ºä¸é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸç”Ÿé›†æˆï¼Œæ— éœ€é€‚é…å™¨è¿›è¡Œå¯¹é½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜æ•ˆå¾®è°ƒ<br>ANOLEé‡‡ç”¨æ•°æ®é«˜æ•ˆå’Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œä»…éœ€çº¦6000ä¸ªæ ·æœ¬å’Œä¸åˆ°4000ä¸‡ä¸ªå‚æ•°ï¼Œå³å¯æœ‰æ•ˆå®ç°è§†è§‰å’Œå¤šåª’ä½“ç”Ÿæˆèƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªå›å½’ç”Ÿæˆ<br>ANOLEé‡‡ç”¨è‡ªå›å½’æ–¹æ³•è¿›è¡Œå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯çš„å¤šæ¨¡æ€åºåˆ—ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¼€æ”¾èµ„æº<br>ANOLEå¼€æºäº†æ¨¡å‹ã€è®­ç»ƒæ¡†æ¶å’ŒæŒ‡ä»¤å¾®è°ƒæ•°æ®ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†ä¸°å¯Œçš„èµ„æºå’Œæ”¯æŒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ANOLEåœ¨å›¾åƒç”Ÿæˆå’Œäº¤é”™å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç”Ÿæˆçš„å›¾åƒè´¨é‡é«˜ï¼Œä¸ç»™å®šæŒ‡ä»¤ç´§å¯†ç›¸å…³ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒç±»å‹ã€‚æ­¤å¤–ï¼ŒANOLEèƒ½å¤Ÿç”Ÿæˆç»„ç»‡è‰¯å¥½ã€æä¾›å…¨é¢ç»†èŠ‚çš„æ–‡æœ¬ï¼Œå¹¶ä¸å›¾åƒæ— ç¼é›†æˆï¼Œå®ç°å®Œç¾çš„è§†è§‰å’Œæ–‡æœ¬å…ƒç´ äº’è¡¥ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>ANOLEä¸ºå¤šæ¨¡æ€ç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†æ–°çš„çªç ´ï¼Œå…¶åŸç”Ÿå¤šæ¨¡æ€é›†æˆã€é«˜æ•ˆå¾®è°ƒã€è‡ªå›å½’ç”Ÿæˆå’Œå¼€æ”¾èµ„æºç­‰ç‰¹ç‚¹ä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æä¾›äº†å®è´µçš„å€Ÿé‰´å’Œå¯ç¤ºã€‚æœªæ¥ï¼ŒANOLEæœ‰æœ›åœ¨ç²¾ç¡®æŒ‡ä»¤éµå¾ªã€ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•ã€å¤šæ¨¡æ€ç†è§£èƒ½åŠ›æå‡ä»¥åŠä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ç­‰æ–¹é¢å–å¾—è¿›ä¸€æ­¥å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Hybrid Alignment Training for Large Language Models</td>
      <td>Alignment training is crucial for enabling large language models (LLMs) to<br>cater to human intentions and preferences. It is typically performed based on<br>two stages with different objectives: instruction-following alignment and<br>human-preference alignment. However, aligning LLMs with these objectives in<br>sequence suffers from an inherent problem: the objectives may conflict, and the<br>LLMs cannot guarantee to simultaneously align with the instructions and human<br>preferences well. To response to these, in this work, we propose a Hybrid<br>Alignment Training (Hbat) approach, based on alternating alignment and modified<br>elastic weight consolidation methods. The basic idea is to alternate between<br>different objectives during alignment training, so that better collaboration<br>can be achieved between the two alignment tasks.We experiment with Hbat on<br>summarization and dialogue tasks. Experimental results show that the proposed<br>\textsc{Hbat} can significantly outperform all baselines. Notably, Hbat yields<br>consistent performance gains over the traditional two-stage alignment training<br>when using both proximal policy optimization and direct preference<br>optimization.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ··åˆå¯¹é½è®­ç»ƒï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œæ‰§è¡Œäººç±»æŒ‡ä»¤æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å¯¹äººç±»æ„å›¾å’Œåå¥½çš„ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µçš„å¯¹é½è®­ç»ƒï¼šæŒ‡ä»¤éµå¾ªå¯¹é½å’Œäººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™ç§é¡ºåºå¯¹é½è®­ç»ƒå­˜åœ¨å›ºæœ‰çš„é—®é¢˜ï¼šä¸¤ä¸ªç›®æ ‡å¯èƒ½å†²çªï¼ŒLLMsæ— æ³•ä¿è¯åŒæ—¶å¾ˆå¥½åœ°å¯¹é½æŒ‡ä»¤å’Œäººç±»åå¥½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆå¯¹é½è®­ç»ƒï¼ˆHBATï¼‰æ–¹æ³•ï¼Œé€šè¿‡äº¤æ›¿å¯¹é½å’Œæ”¹è¿›çš„å¼¹æ€§æƒé‡æ•´åˆæ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤æ›¿å¯¹é½<br>HBAT é‡‡ç”¨äº¤æ›¿å¯¹é½æ–¹æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº¤æ›¿å­¦ä¹ æŒ‡ä»¤éµå¾ªå¯¹é½å’Œäººç±»åå¥½å¯¹é½ï¼Œä»è€Œæ›´å¥½åœ°åè°ƒè¿™ä¸¤ä¸ªå¯¹é½ä»»åŠ¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¹è¿›çš„å¼¹æ€§æƒé‡æ•´åˆ<br>HBAT å¼•å…¥æ”¹è¿›çš„å¼¹æ€§æƒé‡æ•´åˆï¼ˆEWCï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€åœ°å¯¹æ¯ä¸ªå‚æ•°æ–½åŠ é€‚å½“çš„çº¦æŸï¼Œä»è€Œç¼“è§£ä¸å…ˆå‰ç›®æ ‡çš„ä¼˜åŒ–å†²çªã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨æ‘˜è¦å’Œå…¨æ–‡ä¸­ï¼ŒHBAT åœ¨æ‘˜è¦å’Œå¯¹è¯ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ HBAT å¯ä»¥æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒåŸºäº LLaMA2-13B æ¨¡å‹ï¼ŒHBAT åœ¨æ‘˜è¦ä»»åŠ¡ä¸Šæ¯”ä¼ ç»Ÿçš„ RLHF æ–¹æ³•æé«˜äº† 2.26 ä¸ª ROUGE-L åˆ†æ•°ï¼Œåœ¨å¯¹è¯ä»»åŠ¡ä¸Šæé«˜äº† 21.01 ä¸ª GPT-4 èƒœç‡åˆ†æ•°ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>HBAT æ–¹æ³•ä¸ºè§£å†³ LLM å¯¹é½è®­ç»ƒä¸­çš„ä¼˜åŒ–å†²çªé—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¯ä»¥åº”ç”¨äºå…¶ä»– NLP ä»»åŠ¡å’Œåå¥½å¯¹é½æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒHBAT æ–¹æ³•è¿˜å¯ä»¥ä¸å…¶ä»–ä¼˜åŒ–å¯¹é½æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡ LLM çš„æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Relational Programming with Foundation Models</td>
      <td>Foundation models have vast potential to enable diverse AI applications. The<br>powerful yet incomplete nature of these models has spurred a wide range of<br>mechanisms to augment them with capabilities such as in-context learning,<br>information retrieval, and code interpreting. We propose Vieira, a declarative<br>framework that unifies these mechanisms in a general solution for programming<br>with foundation models. Vieira follows a probabilistic relational paradigm and<br>treats foundation models as stateless functions with relational inputs and<br>outputs. It supports neuro-symbolic applications by enabling the seamless<br>combination of such models with logic programs, as well as complex, multi-modal<br>applications by streamlining the composition of diverse sub-models. We<br>implement Vieira by extending the Scallop compiler with a foreign interface<br>that supports foundation models as plugins. We implement plugins for 12<br>foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9<br>challenging tasks that span language, vision, and structured and vector<br>databases. Our evaluation shows that programs in Vieira are concise, can<br>incorporate modern foundation models, and have comparable or better accuracy<br>than competitive baselines.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºåŸºç¡€æ¨¡å‹çš„å…³è”ç¼–ç¨‹æ¡†æ¶ï¼šVieira<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒåŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPTã€CLIPç­‰ï¼‰åœ¨å„ä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç¼–ç¨‹AIåº”ç”¨æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚å®¹æ˜“äº§ç”Ÿå¹»è§‰ã€ç¼ºä¹ç»“æ„åŒ–æ•°æ®å¤„ç†èƒ½åŠ›ã€éš¾ä»¥ç»„åˆä¸åŒæ¨¡æ€çš„æ•°æ®ç­‰ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Vieiraï¼Œä¸€ä¸ªåŸºäºå…³è”ç¼–ç¨‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºç¼–ç¨‹åŸºç¡€æ¨¡å‹æä¾›ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…³è”ç¼–ç¨‹èŒƒå¼<br>Vieiraé‡‡ç”¨å…³è”ç¼–ç¨‹èŒƒå¼ï¼Œå°†åŸºç¡€æ¨¡å‹è§†ä¸ºå…·æœ‰å…³è”è¾“å…¥å’Œè¾“å‡ºçš„æ— çŠ¶æ€å‡½æ•°ã€‚è¿™ç§èŒƒå¼ä½¿å¾—åŸºç¡€æ¨¡å‹å¯ä»¥ä¸é€»è¾‘ç¨‹åºæ— ç¼ç»“åˆï¼Œæ”¯æŒç¥ç»ç¬¦å·åº”ç”¨ï¼Œå¹¶ç®€åŒ–äº†å¤šæ¨¡æ€å­æ¨¡å‹çš„ç»„åˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ’ä»¶å¼æ¡†æ¶<br>Vieiraé€šè¿‡æ‰©å±•Scallopç¼–è¯‘å™¨ï¼Œå®ç°äº†ä¸€ä¸ªæ”¯æŒåŸºç¡€æ¨¡å‹ä½œä¸ºæ’ä»¶çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªå¯å®šåˆ¶å’Œå¯æ‰©å±•çš„æ’ä»¶åº“ï¼ŒåŒ…æ‹¬GPTã€CLIPã€SAMç­‰12ä¸ªåŸºç¡€æ¨¡å‹ã€‚è¿™ä½¿å¾—Vieiraèƒ½å¤Ÿæ”¯æŒå„ç§åº”ç”¨ï¼Œå¹¶å…·æœ‰å‡å°‘å¹»è§‰ã€å¢å¼ºæ£€ç´¢å’Œå¤šæ¨¡æ€ç»„åˆæ€§ç­‰ä¼˜åŠ¿ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Vieiraåœ¨9ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†è¯­è¨€ã€è§†è§‰ã€ç»“æ„åŒ–å’Œå‘é‡æ•°æ®åº“ç­‰é¢†åŸŸã€‚ç»“æœè¡¨æ˜ï¼ŒVieiraçš„ç¨‹åºç®€æ´ï¼Œèƒ½å¤Ÿæ•´åˆç°ä»£åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å‡†ç¡®ç‡æ–¹é¢ä¸ç«äº‰åŸºçº¿ç›¸å½“æˆ–æ›´å¥½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Vieiraä¸ºç¼–ç¨‹åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶å…³è”ç¼–ç¨‹èŒƒå¼å’Œæ’ä»¶å¼æ¡†æ¶å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>* **å…³è”ç¼–ç¨‹èŒƒå¼**ï¼šå°†åŸºç¡€æ¨¡å‹è§†ä¸ºå…³è”å‡½æ•°ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä¸é€»è¾‘ç¨‹åºç»“åˆï¼Œæ”¯æŒç¥ç»ç¬¦å·åº”ç”¨å’Œå¤šæ¨¡æ€ç»„åˆã€‚<br>* **æ’ä»¶å¼æ¡†æ¶**ï¼šé€šè¿‡æ’ä»¶åº“æ”¯æŒå¤šç§åŸºç¡€æ¨¡å‹ï¼Œä½¿å¾—Vieiraèƒ½å¤Ÿé€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚<br>* **æ¦‚ç‡é€»è¾‘æ¨ç†**ï¼šVieiraæ”¯æŒæ¦‚ç‡é€»è¾‘æ¨ç†ï¼Œå¯ä»¥å¤„ç†ä¸ç¡®å®šæ€§ä¿¡æ¯ï¼Œå¹¶æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚<br><br>## æ€»ç»“<br>Vieiraæ˜¯ä¸€ä¸ªåŸºäºå…³è”ç¼–ç¨‹çš„æ¡†æ¶ï¼Œä¸ºç¼–ç¨‹åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶å…·æœ‰ç®€æ´ã€æ˜“ç”¨ã€å¯æ‰©å±•ç­‰ä¼˜ç‚¹ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚Vieiraä¸ºå¼€å‘åŸºäºåŸºç¡€æ¨¡å‹çš„AIåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</td>
    </tr>
    <tr>
      <th>26</th>
      <td>SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation</td>
      <td>The rapid evolution of multimodal foundation model has demonstrated<br>significant progresses in vision-language understanding and generation, e.g.,<br>our previous work SEED-LLaMA. However, there remains a gap between its<br>capability and the real-world applicability, primarily due to the model's<br>limited capacity to effectively respond to various user instructions and<br>interact with diverse visual data. In this work, we focus on bridging this gap<br>through integrating two enhanced features: (1) comprehending images of<br>arbitrary sizes and ratios, and (2) enabling multi-granularity image<br>generation. We present a unified and versatile foundation model, namely,<br>SEED-X, which is able to model multi-granularity visual semantics for<br>comprehension and generation tasks. Besides the competitive results on public<br>benchmarks, SEED-X demonstrates its effectiveness in handling real-world<br>applications across various domains after instruction tuning. We hope that our<br>work will inspire future research into what can be achieved by versatile<br>multimodal foundation models in real-world applications. The models, codes, and<br>datasets are released in https://github.com/AILab-CVC/SEED-X.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SEED-Xï¼šå¤šæ¨¡æ€æ¨¡å‹çš„ç»Ÿä¸€å¤šç²’åº¦ç†è§£å’Œç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œä¾‹å¦‚SEED-LLaMAï¼Œè¿™äº›æ¨¡å‹åœ¨å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œä¸»è¦åŸå› æ˜¯æ¨¡å‹éš¾ä»¥æœ‰æ•ˆå“åº”å„ç§ç”¨æˆ·æŒ‡ä»¤å’Œä¸å¤šæ ·åŒ–çš„è§†è§‰æ•°æ®è¿›è¡Œäº¤äº’ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡é›†æˆä¸¤ä¸ªå¢å¼ºåŠŸèƒ½æ¥å¼¥åˆè¿™ä¸€å·®è·ï¼šï¼ˆ1ï¼‰ç†è§£ä»»æ„å¤§å°å’Œæ¯”ä¾‹çš„å›¾åƒï¼Œä»¥åŠï¼ˆ2ï¼‰å®ç°å¤šç²’åº¦å›¾åƒç”Ÿæˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†è§‰åˆ†è¯å’Œè§£ç <br>SEED-Xé‡‡ç”¨è§†è§‰åˆ†è¯å™¨æ¥ç»Ÿä¸€å›¾åƒç†è§£å’Œç”Ÿæˆï¼Œå¹¶é¢„è®­ç»ƒä¸€ä¸ªå¤šç²’åº¦è§£ç å™¨æ¥ä¿ƒè¿›å›¾åƒç”Ÿæˆå’Œé«˜ç²¾åº¦å›¾åƒæ“ä½œã€‚é¦–å…ˆï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„ViTä½œä¸ºè§†è§‰åˆ†è¯å™¨ï¼Œå¹¶é¢„è®­ç»ƒä¸€ä¸ªè§†è§‰è§£ç å™¨ï¼Œé€šè¿‡æ¥æ”¶ViTç‰¹å¾ä½œä¸ºè¾“å…¥æ¥è§£ç é€¼çœŸçš„å›¾åƒã€‚ç„¶åï¼Œè¿›ä¸€æ­¥å¾®è°ƒè§†è§‰è§£ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæ¥æ”¶é¢å¤–çš„æ¡ä»¶å›¾åƒä½œä¸ºè¾“å…¥ï¼Œä»¥ä¿ç•™è¾“å…¥å›¾åƒçš„ç»†ç²’åº¦ç»†èŠ‚ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€åˆ†è¾¨ç‡å›¾åƒç¼–ç <br>ä¸ºäº†ä½¿å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä»»æ„å¤§å°å’Œçºµæ¨ªæ¯”çš„å›¾åƒï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€åˆ†è¾¨ç‡å›¾åƒç¼–ç ã€‚è¯¥æ–¹æ³•å°†è¾“å…¥å›¾åƒåˆ’åˆ†ä¸ºå­å›¾åƒï¼Œå¹¶ä¸ºæ¯ä¸ªå­å›¾åƒçš„ViTç‰¹å¾æ·»åŠ å¯æ‰©å±•çš„äºŒç»´ä½ç½®åµŒå…¥ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­æœªé‡åˆ°çš„å›¾åƒåˆ†è¾¨ç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>SEED-Xåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå¹¶åœ¨ç»è¿‡æŒ‡ä»¤è°ƒæ•´åï¼Œåœ¨å„ä¸ªé¢†åŸŸçš„å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEED-Xåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸æŒ‡ä»¤é«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œå¹¶ä¿ç•™è¾“å…¥å›¾åƒçš„ç»†ç²’åº¦ç»†èŠ‚ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SEED-Xçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç»Ÿä¸€å¤šç²’åº¦ç†è§£å’Œç”Ÿæˆï¼Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºçš„è§†è§‰åˆ†è¯å’Œè§£ç æ–¹æ³•ä»¥åŠåŠ¨æ€åˆ†è¾¨ç‡å›¾åƒç¼–ç æŠ€æœ¯ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</td>
      <td>In this work, we introduce Mini-Gemini, a simple and effective framework<br>enhancing multi-modality Vision Language Models (VLMs). Despite the<br>advancements in VLMs facilitating basic visual dialog and reasoning, a<br>performance gap persists compared to advanced models like GPT-4 and Gemini. We<br>try to narrow the gap by mining the potential of VLMs for better performance<br>and any-to-any workflow from three aspects, i.e., high-resolution visual<br>tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,<br>we propose to utilize an additional visual encoder for high-resolution<br>refinement without increasing the visual token count. We further construct a<br>high-quality dataset that promotes precise image comprehension and<br>reasoning-based generation, expanding the operational scope of current VLMs. In<br>general, Mini-Gemini further mines the potential of VLMs and empowers current<br>frameworks with image understanding, reasoning, and generation simultaneously.<br>Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)<br>from 2B to 34B. It is demonstrated to achieve leading performance in several<br>zero-shot benchmarks and even surpasses the developed private models. Code and<br>models are available at https://github.com/dvlab-research/MiniGemini.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Mini-Geminiï¼šæŒ–æ˜å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹çš„æ½œåŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œèµ‹äºˆå¤šæ¨¡æ€è¾“å…¥èƒ½åŠ›å·²æˆä¸ºå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡VLMsåœ¨è§†è§‰å¯¹è¯å’Œæ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä¸GPT-4å’ŒGeminiç­‰å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œä»ç„¶å­˜åœ¨æ€§èƒ½å·®è·ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æŒ–æ˜VLMsçš„æ½œåŠ›ï¼Œç¼©å°è¿™ä¸€å·®è·ï¼Œå¹¶æé«˜VLMsçš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜åˆ†è¾¨ç‡è§†è§‰æ ‡è®°<br>ä¸ºäº†å¢å¼ºè§†è§‰æ ‡è®°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŒç¼–ç å™¨ç³»ç»Ÿï¼Œå…¶ä¸­ä¸€ä¸ªç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¦ä¸€ä¸ªç”¨äºä½åˆ†è¾¨ç‡è§†è§‰åµŒå…¥ã€‚è¿™ç§è®¾è®¡å¯ä»¥åœ¨ä¸å¢åŠ è§†è§‰æ ‡è®°æ•°é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜è§†è§‰ç»†èŠ‚ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜è´¨é‡æ•°æ®<br>ä¸ºäº†æé«˜æ•°æ®è´¨é‡ï¼Œæœ¬æ–‡æ”¶é›†å’Œåˆ¶ä½œäº†æ›´å¤šåŸºäºå…¬å…±èµ„æºçš„æ•°æ®ï¼ŒåŒ…æ‹¬é«˜è´¨é‡å“åº”ã€ä»»åŠ¡å¯¼å‘æŒ‡ä»¤å’Œç”Ÿæˆç›¸å…³æ•°æ®ã€‚è¿™äº›æ•°æ®æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ‰©å±•å…¶èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šVLMå¼•å¯¼ç”Ÿæˆ<br>æœ¬æ–‡å°†VLMä¸å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œä»¥æ”¯æŒå›¾åƒå’Œæ–‡æœ¬çš„å¹¶å‘ç”Ÿæˆã€‚VLMé€šè¿‡æä¾›LLMsç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒMini-Geminiåœ¨å¤šä¸ªé›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆæ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†Gemini Proã€Qwen-VL-Pluså’ŒGPT 4Vç­‰ç§æœ‰æ¨¡å‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Mini-Geminiæ¡†æ¶ä¸ºVLMsçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶åŒç¼–ç å™¨ç³»ç»Ÿã€é«˜è´¨é‡æ•°æ®é›†å’ŒVLMå¼•å¯¼ç”Ÿæˆæ–¹æ³•ï¼Œéƒ½å¯ä»¥ä¸ºå…¶ä»–VLMsçš„è®¾è®¡å’Œåº”ç”¨æä¾›å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>28</th>
      <td>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</td>
      <td>We introduce AnyGPT, an any-to-any multimodal language model that utilizes<br>discrete representations for the unified processing of various modalities,<br>including speech, text, images, and music. AnyGPT can be trained stably without<br>any alterations to the current large language model (LLM) architecture or<br>training paradigms. Instead, it relies exclusively on data-level preprocessing,<br>facilitating the seamless integration of new modalities into LLMs, akin to the<br>incorporation of new languages. We build a multimodal text-centric dataset for<br>multimodal alignment pre-training. Utilizing generative models, we synthesize<br>the first large-scale any-to-any multimodal instruction dataset. It consists of<br>108k samples of multi-turn conversations that intricately interweave various<br>modalities, thus equipping the model to handle arbitrary combinations of<br>multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is<br>capable of facilitating any-to-any multimodal conversation while achieving<br>performance comparable to specialized models across all modalities, proving<br>that discrete representations can effectively and conveniently unify multiple<br>modalities within a language model. Demos are shown in<br>https://junzhan2000.github.io/AnyGPT.github.io/</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AnyGPTï¼šç»Ÿä¸€å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå®ç°ä»»æ„æ¨¡æ€é—´çš„æ— ç¼è½¬æ¢<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„èƒ½åŠ›ä»…é™äºæ–‡æœ¬å¤„ç†ï¼Œè€Œç°å®ä¸–ç•Œæ˜¯ inherently å¤šæ¨¡æ€çš„ï¼ŒåŒ…æ‹¬è§†è§‰ã€è¯­è¨€ã€å£°éŸ³å’Œè§¦è§‰ç­‰å¤šç§ä¿¡æ¯æ¸ é“ã€‚å› æ­¤ï¼Œå°† LLM æ‰©å±•åˆ°å¤šæ¨¡æ€æ„ŸçŸ¥æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAnyGPTï¼Œä¸€ä¸ªåŸºäº token çš„ä»»æ„æ¨¡æ€åˆ°ä»»æ„æ¨¡æ€çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹<br>AnyGPT èƒ½å¤Ÿç†è§£å’Œç”ŸæˆåŒ…æ‹¬è¯­éŸ³ã€æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³ä¹åœ¨å†…çš„å¤šç§æ¨¡æ€ã€‚å®ƒé€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€åˆ†è¯å™¨å°†åŸå§‹å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å‹ç¼©æˆç¦»æ•£çš„è¯­ä¹‰ token åºåˆ—ï¼Œç„¶åä½¿ç”¨ LLM å¯¹è¿™äº› token åºåˆ—è¿›è¡Œè‡ªå›å½’è®­ç»ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¤šæ¨¡æ€ token è¢«è§£ç å›åŸå§‹æ¨¡æ€è¡¨ç¤ºã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»º AnyInstruct-108k æ•°æ®é›†ï¼Œå®ç°ä»»æ„æ¨¡æ€é—´çš„å¯¹è¯<br>ä¸ºäº†è§£å†³å¤šæ¨¡æ€å¯¹é½æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼ŒAnyGPT æ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å¯¹é½æ•°æ®é›† AnyInstruct-108kï¼ŒåŒ…å« 108k ä¸ªå¤šè½®å¯¹è¯æ ·æœ¬ï¼Œè¿™äº›å¯¹è¯æ ·æœ¬äº¤ç»‡äº†å¤šç§æ¨¡æ€ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†ä»»æ„ç»„åˆçš„å¤šæ¨¡æ€è¾“å…¥å’Œè¾“å‡ºã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒAnyGPT åœ¨å„ç§è·¨æ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œå¹¶è¯æ˜äº†ç¦»æ•£è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆåœ°å°†å¤šä¸ªæ¨¡æ€ç»Ÿä¸€åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ LLM ä¸­ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AnyGPT çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶ä½¿ç”¨ç¦»æ•£è¡¨ç¤ºæ¥ç»Ÿä¸€å¤„ç†å¤šç§æ¨¡æ€ï¼Œå¹¶é€šè¿‡æ„å»ºå¤§è§„æ¨¡çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸ºå¤šæ¨¡æ€ LLM çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºå®ç°ä»»æ„æ¨¡æ€é—´çš„æ— ç¼è½¬æ¢æä¾›äº†å¯èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>29</th>
      <td>World Model on Million-Length Video And Language With Blockwise RingAttention</td>
      <td>Enabling long-context understanding remains a key challenge in scaling<br>existing sequence models -- a crucial component in developing generally<br>intelligent models that can process and operate over long temporal horizons<br>that potentially consist of millions of tokens. In this paper, we aim to<br>address these challenges by providing a comprehensive exploration of the full<br>development process for producing 1M context language models and video-language<br>models, setting new benchmarks in language retrieval and new capabilities in<br>long video understanding. We detail our long context data curation process,<br>progressive context extension from 4K to 1M tokens, and present an efficient<br>open-source implementation for scalable training on long sequences.<br>Additionally, we open-source a family of 7B parameter models capable of<br>processing long text documents and videos exceeding 1M tokens.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºç™¾ä¸‡é•¿åº¦è§†é¢‘å’Œè¯­è¨€çš„å—çŠ¶ç¯æ³¨æ„åŠ›ä¸–ç•Œæ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œåºåˆ—æ¨¡å‹åœ¨å¤„ç†å’Œç†è§£é•¿æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘æ–¹é¢é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¨¡å‹å¤§å¤šå±€é™äºå¤„ç†çŸ­åºåˆ—ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ã€é•¿ç¯‡å¹…çš„è¯­è¨€å’Œè§†è§‰ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œè®­ç»ƒèƒ½å¤Ÿå¤„ç†æ•°ç™¾ä¸‡ä¸ªtokençš„æ¨¡å‹éœ€è¦å…‹æœé«˜å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œä»¥åŠç¼ºä¹é•¿ä¸Šä¸‹æ–‡æ•°æ®çš„éš¾é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œå¤§å‹ä¸–ç•Œæ¨¡å‹â€ï¼ˆLWMï¼‰çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚LWMé‡‡ç”¨å—çŠ¶ç¯æ³¨æ„åŠ›ï¼ˆBlockwise RingAttentionï¼‰æŠ€æœ¯ï¼Œæ— éœ€è¿‘ä¼¼æˆ–å¼€é”€å³å¯æ‰©å±•ä¸Šä¸‹æ–‡å¤§å°ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é•¿åºåˆ—è®­ç»ƒã€‚ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¸è¿›å¼ä¸Šä¸‹æ–‡æ‰©å±•<br>LWMé€šè¿‡é€æ­¥å¢åŠ åºåˆ—é•¿åº¦ï¼Œä»4K tokenæ‰©å±•åˆ°1M tokenï¼Œæœ‰æ•ˆåœ°è§£å†³äº†é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚åŒæ—¶ï¼Œæ¨¡å‹é‡‡ç”¨RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰æŠ€æœ¯ï¼Œé€šè¿‡è°ƒæ•´Î¸å‚æ•°æ¥é€‚åº”æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å‹ç”Ÿæˆé—®ç­”æ•°æ®<br>ä¸ºäº†å¢å¼ºæ¨¡å‹åœ¨é•¿åºåˆ—å¯¹è¯ä¸­çš„èƒ½åŠ›ï¼ŒLWMé‡‡ç”¨äº†ä¸€ç§åŸºäºæ¨¡å‹çš„é—®ç­”æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨çŸ­ä¸Šä¸‹æ–‡æ¨¡å‹ä»ä¹¦ç±ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹åœ¨é•¿åºåˆ—å¯¹è¯ä¸­çš„è¡¨ç°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ¨¡æ€è®­ç»ƒ<br>LWMå°†è§†è§‰æ¨¡æ€ï¼ˆå›¾åƒå’Œè§†é¢‘ï¼‰èå…¥è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¼˜åŒ–å›¾åƒå’Œè§†é¢‘çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹æŸå¤±ï¼Œå®ç°äº†å¯¹è§†è§‰ä¿¡æ¯çš„æœ‰æ•ˆå¤„ç†ã€‚æ­¤å¤–ï¼ŒLWMè¿˜é‡‡ç”¨äº†æ©ç åºåˆ—æ‰“åŒ…ç­–ç•¥å’Œç²¾ç»†çš„æŸå¤±å¹³è¡¡ï¼Œä»¥æ›´å¥½åœ°å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>LWMåœ¨é•¿è§†é¢‘ç†è§£å’Œé•¿ä¸Šä¸‹æ–‡äº‹å®æ£€ç´¢æ–¹é¢å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚åœ¨è¯­è¨€ä»»åŠ¡ã€æ£€ç´¢ä»»åŠ¡å’ŒLOFTæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒLWMåœ¨å¤„ç†é•¿åºåˆ—æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒLWMè¿˜å±•ç¤ºäº†å›¾åƒå’Œè§†é¢‘ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬ç”Ÿæˆé™æ€å›¾åƒå’ŒåŠ¨æ€è§†é¢‘ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LWMçš„ç ”ç©¶æˆæœä¸ºé•¿ä¸Šä¸‹æ–‡åºåˆ—æ¨¡å‹çš„å¼€å‘æä¾›äº†é‡è¦å‚è€ƒã€‚å…¶æ¸è¿›å¼ä¸Šä¸‹æ–‡æ‰©å±•ã€æ¨¡å‹ç”Ÿæˆé—®ç­”æ•°æ®å’Œå¤šæ¨¡æ€è®­ç»ƒç­‰æŠ€æœ¯ï¼Œä¸ºè§£å†³é•¿åºåˆ—å»ºæ¨¡çš„æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚æ­¤å¤–ï¼ŒLWMçš„å¼€æºå®ç°å’Œæ¨¡å‹ä¹Ÿä¸ºç ”ç©¶äººå‘˜æä¾›äº†å®è´µçš„èµ„æºã€‚</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</td>
      <td>In light of recent advances in multimodal Large Language Models (LLMs), there<br>is increasing attention to scaling them from image-text data to more<br>informative real-world videos. Compared to static images, video poses unique<br>challenges for effective large-scale pre-training due to the modeling of its<br>spatiotemporal dynamics. In this paper, we address such limitations in<br>video-language pre-training with an efficient video decomposition that<br>represents each video as keyframes and temporal motions. These are then adapted<br>to an LLM using well-designed tokenizers that discretize visual and temporal<br>information as a few tokens, thus enabling unified generative pre-training of<br>videos, images, and text. At inference, the generated tokens from the LLM are<br>carefully recovered to the original continuous pixel space to create various<br>video content. Our proposed framework is both capable of comprehending and<br>generating image and video content, as demonstrated by its competitive<br>performance across 13 multimodal benchmarks in image and video understanding<br>and generation. Our code and models are available at<br>https://video-lavit.github.io.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Video-LaVITï¼šåŸºäºè§£è€¦è§†è§‰-è¿åŠ¨æ ‡è®°çš„è§†é¢‘-è¯­è¨€é¢„è®­ç»ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„çªç ´ï¼Œå°†å®ƒä»¬ä»å›¾åƒ-æ–‡æœ¬æ•°æ®æ‰©å±•åˆ°æ›´ä¸°å¯Œçš„çœŸå®ä¸–ç•Œè§†é¢‘æ•°æ®æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œä¸é™æ€å›¾åƒç›¸æ¯”ï¼Œè§†é¢‘æ•°æ®ç”±äºå…¶æ—¶ç©ºåŠ¨æ€ç‰¹æ€§ï¼Œåœ¨æœ‰æ•ˆçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ–¹é¢é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆæ— æ³•æœ‰æ•ˆåœ°ç¼–ç è§†é¢‘çš„æ—¶ç©ºåŠ¨æ€ä¿¡æ¯ï¼Œè¦ä¹ˆåœ¨å¤„ç†é•¿è§†é¢‘æ—¶è®¡ç®—æˆæœ¬è¿‡é«˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†é¢‘åˆ†è§£<br>Video-LaVIT æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘åˆ†è§£æ–¹æ³•ï¼Œå°†æ¯ä¸ªè§†é¢‘è¡¨ç¤ºä¸ºå…³é”®å¸§å’Œè¿åŠ¨å‘é‡ã€‚å…³é”®å¸§ç”¨äºæ•æ‰è§†é¢‘çš„ä¸»è¦è§†è§‰è¯­ä¹‰ï¼Œè€Œè¿åŠ¨å‘é‡åˆ™æè¿°äº†å…³é”®å¸§éšæ—¶é—´çš„åŠ¨æ€å˜åŒ–ã€‚è¿™ç§åˆ†è§£æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†è§†é¢‘è¡¨ç¤ºæ‰€éœ€çš„æ ‡è®°æ•°é‡ï¼Œä»è€Œæé«˜äº†å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè§£è€¦è§†è§‰-è¿åŠ¨æ ‡è®°åŒ–<br>Video-LaVIT è®¾è®¡äº†ä¸¤ç§æ ‡è®°åŒ–å™¨æ¥å¤„ç†è§†é¢‘æ¨¡æ€ï¼šå›¾åƒæ ‡è®°åŒ–å™¨å’Œè¿åŠ¨æ ‡è®°åŒ–å™¨ã€‚å›¾åƒæ ‡è®°åŒ–å™¨åˆ©ç”¨ç°æœ‰çš„å›¾åƒæ ‡è®°åŒ–å™¨ï¼ˆå¦‚ LaVITï¼‰æ¥å¤„ç†å…³é”®å¸§ï¼Œè€Œè¿åŠ¨æ ‡è®°åŒ–å™¨åˆ™åŸºäº VQ-VAE æ¶æ„å¼€å‘ï¼Œç”¨äºå°†è¿åŠ¨å‘é‡è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°åºåˆ—ã€‚è¿™ç§è§£è€¦çš„æ ‡è®°åŒ–æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ è§†é¢‘çš„è§†è§‰å’Œè¿åŠ¨ä¿¡æ¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§†é¢‘å»æ ‡è®°åŒ–<br>Video-LaVIT è¿˜è®¾è®¡äº†ä¸€ä¸ªè§†é¢‘å»æ ‡è®°åŒ–å™¨ï¼Œç”¨äºå°† LLM ç”Ÿæˆçš„ç¦»æ•£æ ‡è®°åºåˆ—æ¢å¤åˆ°åŸå§‹çš„è¿ç»­åƒç´ ç©ºé—´ï¼Œä»è€Œç”Ÿæˆå„ç§è§†é¢‘å†…å®¹ã€‚è§†é¢‘å»æ ‡è®°åŒ–å™¨é‡‡ç”¨æ¡ä»¶å»å™ª U-Net æ¶æ„ï¼Œå¹¶é€šè¿‡è¿åŠ¨å‘é‡è¿›è¡Œå¢å¼ºçš„è¿åŠ¨æ¡ä»¶ï¼Œä»è€Œæé«˜äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šç»Ÿä¸€ç”Ÿæˆå»ºæ¨¡<br>Video-LaVIT å°†æ‰€æœ‰æ¨¡æ€ï¼ˆè§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬ï¼‰éƒ½è§†ä¸º 1D ç¦»æ•£æ ‡è®°ï¼Œå¹¶ä½¿ç”¨ LLM è¿›è¡Œç»Ÿä¸€çš„ç”Ÿæˆé¢„è®­ç»ƒã€‚è¿™ç§ç»Ÿä¸€çš„å»ºæ¨¡æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤ŸåŒæ—¶ç†è§£å’Œç”Ÿæˆå„ç§å¤šæ¨¡æ€å†…å®¹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Video-LaVIT åœ¨ 13 ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†éå¸¸å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚åœ¨å›¾åƒç†è§£æ–¹é¢ï¼ŒVideo-LaVIT åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨è§†é¢‘ç†è§£æ–¹é¢ï¼ŒVideo-LaVIT åœ¨è§†é¢‘é—®ç­”å’Œè§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡ã€‚åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢ï¼ŒVideo-LaVIT åœ¨æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå’Œé•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Video-LaVIT çš„è§†é¢‘åˆ†è§£ã€è§£è€¦è§†è§‰-è¿åŠ¨æ ‡è®°åŒ–ã€è§†é¢‘å»æ ‡è®°åŒ–å’Œç»Ÿä¸€ç”Ÿæˆå»ºæ¨¡ç­‰åˆ›æ–°æ–¹æ³•ä¸ºè§†é¢‘-è¯­è¨€é¢„è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€ AI åŠ©æ‰‹å¥ å®šäº†åŸºç¡€ã€‚</td>
    </tr>
    <tr>
      <th>31</th>
      <td>MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer</td>
      <td>Developing generative models for interleaved image-text data has both<br>research and practical value. It requires models to understand the interleaved<br>sequences and subsequently generate images and text. However, existing attempts<br>are limited by the issue that the fixed number of visual tokens cannot<br>efficiently capture image details, which is particularly problematic in the<br>multi-image scenarios. To address this, this paper presents MM-Interleaved, an<br>end-to-end generative model for interleaved image-text data. It introduces a<br>multi-scale and multi-image feature synchronizer module, allowing direct access<br>to fine-grained image features in the previous context during the generation<br>process. MM-Interleaved is end-to-end pre-trained on both paired and<br>interleaved image-text corpora. It is further enhanced through a supervised<br>fine-tuning phase, wherein the model improves its ability to follow complex<br>multi-modal instructions. Experiments demonstrate the versatility of<br>MM-Interleaved in recognizing visual details following multi-modal instructions<br>and generating consistent images following both textual and visual conditions.<br>Code and models are available at<br>\url{https://github.com/OpenGVLab/MM-Interleaved}.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MM-Interleavedï¼šåŸºäºå¤šæ¨¡æ€ç‰¹å¾åŒæ­¥å™¨çš„äº¤é”™å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äº’è”ç½‘ä¸Šäº¤é”™å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼ˆå¦‚æ–°é—»ã€åšå®¢ï¼‰çš„æ™®åŠï¼Œå¼€å‘èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆæ­¤ç±»æ•°æ®çš„ç”Ÿæˆæ¨¡å‹å…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†äº¤é”™å›¾åƒ-æ–‡æœ¬æ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦é—®é¢˜åœ¨äºå›ºå®šæ•°é‡çš„è§†è§‰æ ‡è®°æ— æ³•æœ‰æ•ˆåœ°æ•æ‰å›¾åƒç»†èŠ‚ï¼Œå°¤å…¶æ˜¯åœ¨å¤šå›¾åƒåœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MM-Interleavedï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºå¤„ç†äº¤é”™å›¾åƒ-æ–‡æœ¬æ•°æ®ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ¨¡æ€ç‰¹å¾åŒæ­¥å™¨ï¼ˆMMFSï¼‰<br>MM-Interleavedå¼•å…¥äº†ä¸€ä¸ªå¤šå°ºåº¦ã€å¤šå›¾åƒç‰¹å¾åŒæ­¥å™¨æ¨¡å—ï¼ˆMMFSï¼‰ï¼Œå…è®¸åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç›´æ¥è®¿é—®æ¥è‡ªå…ˆå‰ä¸Šä¸‹æ–‡çš„ç»†ç²’åº¦å›¾åƒç‰¹å¾ã€‚MMFSåŸºäºå¯å˜å½¢ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å¤šå°ºåº¦å›¾åƒç‰¹å¾å›¾ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¼¥è¡¥äº†è§†è§‰æ ‡è®°æ•°é‡æœ‰é™å¯¼è‡´çš„å›¾åƒç»†èŠ‚ä¸¢å¤±é—®é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç«¯åˆ°ç«¯é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒ<br>MM-Interleavedåœ¨é…å¯¹å’Œäº¤é”™çš„å›¾åƒ-æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œç«¯åˆ°ç«¯é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒé˜¶æ®µè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªå¤æ‚çš„æ¨¡æ€æŒ‡ä»¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒMM-Interleavedåœ¨è¯†åˆ«å¤šæ¨¡æ€æŒ‡ä»¤åçš„è§†è§‰ç»†èŠ‚å’Œç”Ÿæˆä¸€è‡´å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€ç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMM-Interleavedå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ ‡è®°æ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MM-Interleavedçš„æå‡ºä¸ºå¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•MMFSå¯ä»¥æœ‰æ•ˆè§£å†³è§†è§‰æ ‡è®°æ•°é‡æœ‰é™å¯¼è‡´çš„å›¾åƒç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼ŒMM-Interleavedçš„ç«¯åˆ°ç«¯é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒç­–ç•¥ä¹Ÿä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</td>
      <td>We present Unified-IO 2, the first autoregressive multimodal model that is<br>capable of understanding and generating image, text, audio, and action. To<br>unify different modalities, we tokenize inputs and outputs -- images, text,<br>audio, action, bounding boxes, etc., into a shared semantic space and then<br>process them with a single encoder-decoder transformer model. Since training<br>with such diverse modalities is challenging, we propose various architectural<br>improvements to stabilize model training. We train our model from scratch on a<br>large multimodal pre-training corpus from diverse sources with a multimodal<br>mixture of denoisers objective. To learn an expansive set of skills, such as<br>following multimodal instructions, we construct and finetune on an ensemble of<br>120 datasets with prompts and augmentations. With a single unified model,<br>Unified-IO 2 achieves state-of-the-art performance on the GRIT benchmark and<br>strong results in more than 35 benchmarks, including image generation and<br>understanding, natural language understanding, video and audio understanding,<br>and robotic manipulation. We release all our models to the research community.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ç»Ÿä¸€è¾“å…¥è¾“å‡º2ï¼šæ‰©å±•è‡ªå›å½’å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ¶µç›–è§†è§‰ã€è¯­è¨€ã€éŸ³é¢‘å’ŒåŠ¨ä½œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæ„å»ºèƒ½å¤Ÿç†è§£å¹¶ç”Ÿæˆå¤šç§æ¨¡æ€ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’ŒåŠ¨ä½œï¼‰çš„æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™æ ·çš„æ¨¡å‹é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®é›†çš„å¤šæ ·æ€§ã€æ¨¡å‹æ¶æ„çš„è®¾è®¡ã€è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ç­‰ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸åªèƒ½å¤„ç†æœ‰é™çš„æ¨¡æ€ï¼Œæˆ–è€…éœ€è¦ä¾èµ–é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>Unified-IO 2 æ˜¯ä¸€ä¸ªè‡ªå›å½’å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’ŒåŠ¨ä½œã€‚ä¸ºäº†ç»Ÿä¸€ä¸åŒçš„æ¨¡æ€ï¼Œè¯¥æ¨¡å‹å°†è¾“å…¥å’Œè¾“å‡ºï¼ˆåŒ…æ‹¬å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ã€åŠ¨ä½œã€è¾¹ç•Œæ¡†ç­‰ï¼‰ç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œå¹¶ä½¿ç”¨å•ä¸ªç¼–ç å™¨-è§£ç å™¨Transformeræ¨¡å‹è¿›è¡Œå¤„ç†ã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†å¤šç§æ¶æ„æ”¹è¿›ï¼ŒåŒ…æ‹¬äºŒç»´æ—‹è½¬åµŒå…¥ã€QKå½’ä¸€åŒ–å’Œç¼©æ”¾ä½™å¼¦æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜ä½¿ç”¨äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ··åˆå»å™ªå™¨ç›®æ ‡ï¼Œå¹¶ç»“åˆäº†è¶…è¿‡120ä¸ªæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å­¦ä¹ å¹¿æ³›çš„æŠ€èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Unified-IO 2 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆå’Œç†è§£ã€è‡ªç„¶è¯­è¨€ç†è§£ã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£ä»¥åŠæœºå™¨äººæ“ä½œç­‰ã€‚è¯¥æ¨¡å‹åœ¨GRITåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¼ºå¤§çš„ç»“æœã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Unified-IO 2 çš„è®¾è®¡ä¸ºæ„å»ºé€šç”¨å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br>* **ç»Ÿä¸€ä»»åŠ¡è¡¨ç¤º**ï¼šå°†ä¸åŒæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œä½¿ç”¨å•ä¸ªæ¨¡å‹è¿›è¡Œå¤„ç†ã€‚<br>* **æ¶æ„æ”¹è¿›**ï¼šé‡‡ç”¨äºŒç»´æ—‹è½¬åµŒå…¥ã€QKå½’ä¸€åŒ–å’Œç¼©æ”¾ä½™å¼¦æ³¨æ„åŠ›æœºåˆ¶ç­‰æ”¹è¿›ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚<br>* **å¤šæ¨¡æ€æ··åˆå»å™ªå™¨ç›®æ ‡**ï¼šç»“åˆå»å™ªå’Œç”Ÿæˆä»»åŠ¡ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨å¤šæ¨¡æ€ä¿¡å·ã€‚<br>* **å¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒ**ï¼šä½¿ç”¨è¶…è¿‡120ä¸ªæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å­¦ä¹ å¹¿æ³›çš„æŠ€èƒ½ã€‚<br><br>## ğŸ“š å‚è€ƒæ–‡çŒ®<br>* [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172)</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Generative Multimodal Models are In-Context Learners</td>
      <td>The human ability to easily solve multimodal tasks in context (i.e., with<br>only a few demonstrations or simple instructions), is what current multimodal<br>systems have largely struggled to imitate. In this work, we demonstrate that<br>the task-agnostic in-context learning capabilities of large multimodal models<br>can be significantly enhanced by effective scaling-up. We introduce Emu2, a<br>generative multimodal model with 37 billion parameters, trained on large-scale<br>multimodal sequences with a unified autoregressive objective. Emu2 exhibits<br>strong multimodal in-context learning abilities, even emerging to solve tasks<br>that require on-the-fly reasoning, such as visual prompting and object-grounded<br>generation. The model sets a new record on multiple multimodal understanding<br>tasks in few-shot settings. When instruction-tuned to follow specific<br>instructions, Emu2 further achieves new state-of-the-art on challenging tasks<br>such as question answering benchmarks for large multimodal models and<br>open-ended subject-driven generation. These achievements demonstrate that Emu2<br>can serve as a base model and general-purpose interface for a wide range of<br>multimodal tasks. Code and models are publicly available to facilitate future<br>research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Emu2ï¼šå¤§å‹ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„çªç ´<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤šæ¨¡æ€ä»»åŠ¡æ¶‰åŠç†è§£å’Œç”Ÿæˆå•æ¨¡æ€æˆ–å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä¾‹å¦‚æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€ç³»ç»Ÿå¾€å¾€éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡æ¶æ„å’Œæ”¶é›†å¤§é‡ç›‘ç£è®­ç»ƒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»èƒ½å¤Ÿè½»æ¾åœ°åœ¨ä¸Šä¸‹æ–‡ä¸­è§£å†³æ–°ä»»åŠ¡ï¼Œå³é€šè¿‡å°‘é‡æ¼”ç¤ºæˆ–ç®€å•æŒ‡ä»¤ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡æœ‰æ•ˆæ‰©å±•å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¢å¼ºå…¶åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è¿™ç§èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šEmu2 æ¨¡å‹<br>æœ¬æ–‡æå‡ºäº† Emu2ï¼Œä¸€ä¸ªå…·æœ‰ 370 äº¿å‚æ•°çš„ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡åœ¨å¤§å‹å¤šæ¨¡æ€åºåˆ—ä¸Šè¿›è¡Œç»Ÿä¸€çš„è‡ªå›å½’ç›®æ ‡è®­ç»ƒã€‚Emu2 åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç”šè‡³èƒ½å¤Ÿè§£å†³éœ€è¦å³æ—¶æ¨ç†çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è§†è§‰æç¤ºå’ŒåŸºäºå¯¹è±¡çš„ç”Ÿæˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸Šä¸‹æ–‡å­¦ä¹ <br>Emu2 åœ¨æ ‡å‡†å¤šæ¨¡æ€æ•°æ®é›†ä¸Šï¼Œé€šè¿‡å°‘é‡ç¤ºä¾‹æˆ–æŒ‡ä»¤è¿›è¡Œå­¦ä¹ çš„èƒ½åŠ›å¾—åˆ°äº†è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼ŒEmu2 åœ¨ä¸¤ç§åœºæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼š(a) å°‘æ ·æœ¬è®¾ç½®ï¼Œå…è®¸å°½å¯èƒ½å¤šçš„ç¤ºä¾‹é€‚åº”æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ï¼›(b) æŒ‡ä»¤è°ƒæ•´ï¼Œæ¨¡å‹è¢«è°ƒæ•´ä¸ºéµå¾ªç‰¹å®šæŒ‡ä»¤ã€‚Emu2 åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹åœ¨å¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä¾‹å¦‚åœ¨å¤šä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Emu2 åœ¨å¤šä¸ªå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„è®°å½•ï¼Œå¹¶åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºè‰²ã€‚å½“é€šè¿‡æŒ‡ä»¤è°ƒæ•´ä»¥éµå¾ªç‰¹å®šæŒ‡ä»¤æ—¶ï¼ŒEmu2 åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›æˆæœï¼Œä¾‹å¦‚å¤§å‹å¤šæ¨¡æ€æ¨¡å‹é—®ç­”åŸºå‡†å’Œå¼€æ”¾å¼ä¸»é¢˜é©±åŠ¨ç”Ÿæˆã€‚è¿™äº›æˆæœè¡¨æ˜ Emu2 å¯ä»¥ä½œä¸ºå„ç§å¤šæ¨¡æ€ä»»åŠ¡çš„åŸºæ¨¡å‹å’Œé€šç”¨æ¥å£ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ Emu2 æ¨¡å‹å±•ç¤ºäº†å¤§å‹ç”Ÿæˆå¼å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„çªç ´ã€‚é€šè¿‡æœ‰æ•ˆæ‰©å±•æ¨¡å‹ï¼ŒEmu2 èƒ½å¤Ÿæ¨¡ä»¿äººç±»åœ¨ä¸Šä¸‹æ–‡ä¸­è§£å†³æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒEmu2 è¿˜å±•ç¤ºäº†åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­å¯æ§è§†è§‰ç”Ÿæˆçš„èƒ½åŠ›ï¼Œä¾‹å¦‚åŸºäºä¸»é¢˜/æ–‡æœ¬çš„ç”Ÿæˆã€‚è¿™äº›æˆæœä¸ºå¼€å‘å¯é€‚åº”ã€é€šç”¨çš„å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†é‡è¦çš„å¯ç¤ºã€‚</td>
    </tr>
    <tr>
      <th>34</th>
      <td>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</td>
      <td>This report introduces xGen-MM (also known as BLIP-3), a framework for<br>developing Large Multimodal Models (LMMs). The framework comprises meticulously<br>curated datasets, a training recipe, model architectures, and a resulting suite<br>of LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen<br>initiative on foundation AI models. Our models undergo rigorous evaluation<br>across a range of tasks, including both single and multi-image benchmarks. Our<br>pre-trained base model exhibits strong in-context learning capabilities and the<br>instruction-tuned model demonstrates competitive performance among open-source<br>LMMs with similar model sizes. In addition, we introduce a safety-tuned model<br>with DPO, aiming to mitigate harmful behaviors such as hallucinations and<br>improve safety. We open-source our models, curated large-scale datasets, and<br>our fine-tuning codebase to facilitate further advancements in LMM research.<br>Associated resources will be available on our project page above.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | xGen-MM (BLIP-3): å¼€æ”¾å¼å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å®¶æ—<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å› å…¶æ½œåœ¨çš„åº”ç”¨å’Œæ¶Œç°èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå¼€æºæ¨¡å‹ä¸é—­æºæ¨¡å‹ä¹‹é—´åœ¨å¼€æ”¾æƒé‡ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®é›†è·å–æ–¹é¢ä»å­˜åœ¨å·®è·ï¼Œè¿™é™åˆ¶äº†å¼€æºç¤¾åŒºå¯¹LMMsçš„å¤åˆ¶ã€ç†è§£å’Œæ”¹è¿›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„LMMsåœ¨æ•°æ®è§„æ¨¡ã€è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéš¾ä»¥è¾¾åˆ°ä¸é—­æºæ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ•°æ®é›†çš„ä¸°å¯Œæ€§å’Œè§„æ¨¡<br>xGen-MM (BLIP-3) æ¡†æ¶åˆ©ç”¨äº†å¤šç§å¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…æ‹¬ MINT-1Tã€OBELICSã€BLIP3-KALEã€BLIP3-OCR-200M å’Œ BLIP3-GROUNDING-50Mï¼Œä»¥æä¾›æ›´ä¸°å¯Œã€æ›´å¤§è§„æ¨¡å’Œæ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›æ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„é¢†åŸŸï¼ŒåŒ…æ‹¬ HTMLã€PDFã€ArXivã€å›¾åƒã€æ–‡æœ¬å’Œè§†è§‰å®šä½ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å‹æ¶æ„çš„æ”¹è¿›<br>xGen-MM (BLIP-3) æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ›´å¯æ‰©å±•çš„è§†è§‰æ ‡è®°é‡‡æ ·å™¨ï¼ˆperceiver resamplerï¼‰æ¥æ›¿ä»£ Q-Former å±‚ï¼Œå¹¶ç®€åŒ–äº†è®­ç»ƒç›®æ ‡ï¼Œä½¿å…¶ä»…å…³æ³¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­æ–‡æœ¬æ ‡è®°çš„è‡ªå›å½’æŸå¤±ã€‚è¿™ç§æ¶æ„æ”¹è¿›ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè®­ç»ƒæ–¹æ³•çš„ä¼˜åŒ–<br>xGen-MM (BLIP-3) æ¡†æ¶é‡‡ç”¨äº†å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥æé«˜æ¨¡å‹å¯¹ç”¨æˆ·æŸ¥è¯¢çš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ¡†æ¶è¿˜å¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå®‰å…¨å¾®è°ƒæ¥æé«˜æ¨¡å‹çš„æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ï¼Œä»è€Œå‡å°‘å¹»è§‰å’Œæœ‰å®³è¡Œä¸ºã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>xGen-MM (BLIP-3) æ¡†æ¶åœ¨å¤šä¸ªè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€OCR å’Œè§†è§‰å®šä½ã€‚ä¸ç°æœ‰çš„å¼€æº LMMs ç›¸æ¯”ï¼ŒxGen-MM (BLIP-3) åœ¨å•å›¾åƒå’Œå¤šå›¾åƒåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDPO å’Œå®‰å…¨å¾®è°ƒè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ï¼Œä½¿å…¶æ›´é€‚ç”¨äºå®é™…åº”ç”¨åœºæ™¯ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>xGen-MM (BLIP-3) æ¡†æ¶ä¸º LMMs çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†å®è´µçš„èµ„æºå’Œå·¥å…·ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ã€æ•°æ®é›†å’Œå¾®è°ƒä»£ç åº“ã€‚ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…å¯ä»¥åˆ©ç”¨è¿™äº›èµ„æºæ¥æ¢ç´¢ LMMs çš„æ½œåŠ›å’Œæ¶Œç°èƒ½åŠ›ï¼Œå¹¶å°†å…¶åº”ç”¨äºå„ç§å®é™…åº”ç”¨åœºæ™¯ã€‚æ­¤å¤–ï¼ŒxGen-MM (BLIP-3) æ¡†æ¶çš„å®‰å…¨å¾®è°ƒæ–¹æ³•ä¹Ÿä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´å¯é çš„ AI æ¨¡å‹æä¾›äº†é‡è¦çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>35</th>
      <td>VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation</td>
      <td>In this work, we introduce Vision-Language Generative Pre-trained Transformer<br>(VL-GPT), a transformer model proficient at concurrently perceiving and<br>generating visual and linguistic data. VL-GPT achieves a unified pre-training<br>approach for both image and text modalities by employing a straightforward<br>auto-regressive objective, thereby enabling the model to process image and text<br>as seamlessly as a language model processes text. To accomplish this, we<br>initially propose a novel image tokenizer-detokenizer framework for visual<br>data, specifically designed to transform raw images into a sequence of<br>continuous embeddings and reconstruct them accordingly. In combination with the<br>existing text tokenizer and detokenizer, this framework allows for the encoding<br>of interleaved image-text data into a multimodal sequence, which can<br>subsequently be fed into the transformer model. Consequently, VL-GPT can<br>perform large-scale pre-training on multimodal corpora utilizing a unified<br>auto-regressive objective (i.e., next-token prediction). Upon completion of<br>pre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance<br>across a diverse range of vision and language understanding and generation<br>tasks, including image captioning, visual question answering, text-to-image<br>generation, and more. Additionally, the pre-trained model retrains in-context<br>learning capabilities when provided with multimodal prompts. We further conduct<br>instruction tuning on our VL-GPT, highlighting its exceptional potential for<br>multimodal assistance. The source code and model weights shall be released.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | VL-GPTï¼šè§†è§‰ä¸è¯­è¨€ç†è§£ä¸ç”Ÿæˆçš„ç”Ÿæˆå¼é¢„è®­ç»ƒTransformer<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„æˆåŠŸï¼Œå¤šæ¨¡æ€ç¤¾åŒºå¯¹å¼€å‘å¤§å‹è§†è§‰-è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLæ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚ï¼Œè¾“å…¥å›¾åƒå’Œç”Ÿæˆå›¾åƒçš„å›¾åƒåµŒå…¥ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ¨¡å‹éœ€è¦åˆ†åˆ«å¯¹å›¾åƒç†è§£å’Œç”Ÿæˆè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸”éš¾ä»¥åœ¨å›¾åƒåµŒå…¥ä¸Šå®ç°è‡ªå›å½’è®­ç»ƒæŸå¤±ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå›¾åƒåˆ†è¯å™¨-è§£ç å™¨æ¡†æ¶<br>ä¸ºäº†å®ç°è§†è§‰åµŒå…¥å’Œæ–‡æœ¬æ ‡è®°çš„ç»Ÿä¸€è‡ªå›å½’è®­ç»ƒç›®æ ‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å›¾åƒåˆ†è¯å™¨-è§£ç å™¨æ¡†æ¶ï¼Œç”¨äºå°†åŸå§‹å›¾åƒè½¬æ¢ä¸ºè¿ç»­è§†è§‰åµŒå…¥å¹¶é‡å»ºå®ƒä»¬ã€‚è¯¥æ¡†æ¶ç”±å›¾åƒåˆ†è¯å™¨å’Œå›¾åƒè§£ç å™¨ç»„æˆï¼Œåˆ†åˆ«è´Ÿè´£å°†å›¾åƒç¼–ç ä¸ºè¿ç»­è§†è§‰åµŒå…¥å’Œè§£ç è§†è§‰åµŒå…¥å›åƒç´ ç©ºé—´ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šVL-GPTæ¨¡å‹<br>VL-GPTæ˜¯ä¸€ä¸ªç”Ÿæˆå¼é¢„è®­ç»ƒTransformeræ¨¡å‹ï¼Œç”¨äºè§†è§‰å’Œè¯­è¨€ç†è§£ä¸ç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹å¯ä»¥ç»Ÿä¸€åœ°è‡ªå›å½’é¢„è®­ç»ƒå¤§è§„æ¨¡å¤šæ¨¡æ€è¯­æ–™åº“ï¼Œå³é¢„æµ‹åŒ…å«è¿ç»­è§†è§‰åµŒå…¥å’Œç¦»æ•£æ–‡æœ¬æ ‡è®°çš„å¤šæ¨¡æ€åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œè€Œæ— éœ€ä»»ä½•åŒºåˆ†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>VL-GPTåœ¨å„ç§VLç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰æ€§æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨æä¾›å¤šæ¨¡æ€æç¤ºæ—¶è¡¨ç°å‡ºæœ‰å¸å¼•åŠ›çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>VL-GPTæ¨¡å‹ä¸ºå¤šæ¨¡æ€ç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œç±»ä¼¼äºGPTå®¶æ—åœ¨NLPä¸­çš„ä½œç”¨ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›ï¼Œå¯ä»¥åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼ŒVL-GPTçš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä½¿å…¶æˆä¸ºä¸€ä¸ªçµæ´»ä¸”æœ‰æ•ˆçš„å¤šæ¨¡æ€é€šç”¨åŠ©æ‰‹ã€‚</td>
    </tr>
    <tr>
      <th>36</th>
      <td>DreamLLM: Synergistic Multimodal Comprehension and Creation</td>
      <td>This paper presents DreamLLM, a learning framework that first achieves<br>versatile Multimodal Large Language Models (MLLMs) empowered with frequently<br>overlooked synergy between multimodal comprehension and creation. DreamLLM<br>operates on two fundamental principles. The first focuses on the generative<br>modeling of both language and image posteriors by direct sampling in the raw<br>multimodal space. This approach circumvents the limitations and information<br>loss inherent to external feature extractors like CLIP, and a more thorough<br>multimodal understanding is obtained. Second, DreamLLM fosters the generation<br>of raw, interleaved documents, modeling both text and image contents, along<br>with unstructured layouts. This allows DreamLLM to learn all conditional,<br>marginal, and joint multimodal distributions effectively. As a result, DreamLLM<br>is the first MLLM capable of generating free-form interleaved content.<br>Comprehensive experiments highlight DreamLLM's superior performance as a<br>zero-shot multimodal generalist, reaping from the enhanced learning synergy.<br>Project page: https://dreamllm.github.io.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | DreamLLMï¼šå¤šæ¨¡æ€ç†è§£ä¸åˆ›é€ çš„ååŒå­¦ä¹ æ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤šæ¨¡æ€ç†è§£ä¸åˆ›é€ æ˜¯æœºå™¨æ™ºèƒ½å‘å±•çš„å…³é”®æ–¹å‘ï¼Œè€Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMsä¸»è¦å…³æ³¨å¤šæ¨¡æ€ç†è§£ï¼Œè€Œå¤šæ¨¡æ€åˆ›é€ èƒ½åŠ›ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢MLLMsåœ¨å¤šæ¨¡æ€åˆ›é€ æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å®ç°å¤šæ¨¡æ€ç†è§£ä¸åˆ›é€ çš„ååŒå­¦ä¹ ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«¯åˆ°ç«¯ç”Ÿæˆ<br>ä¸åŒäºç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆä¸­é—´å›¾åƒè¡¨ç¤ºï¼ˆå¦‚CLIPåµŒå…¥ï¼‰ï¼ŒDreamLLMç›´æ¥åœ¨åŸå§‹å¤šæ¨¡æ€ç©ºé—´ä¸­è¿›è¡Œé‡‡æ ·ï¼Œç”Ÿæˆè¯­è¨€å’Œå›¾åƒçš„åéªŒåˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¤–éƒ¨ç‰¹å¾æå–å™¨ï¼ˆå¦‚CLIPï¼‰çš„å±€é™æ€§å’Œä¿¡æ¯æŸå¤±ï¼Œå®ç°äº†æ›´å…¨é¢çš„å¤šæ¨¡æ€ç†è§£ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäº¤é”™ç”Ÿæˆé¢„è®­ç»ƒï¼ˆI-GPTï¼‰<br>DreamLLMé€šè¿‡è®­ç»ƒç”Ÿæˆäº¤é”™çš„å¤šæ¨¡æ€è¯­æ–™åº“ï¼ŒåŒæ—¶ç¼–ç å’Œè§£ç äº¤é”™çš„è¯­è¨€-å›¾åƒå¤šæ¨¡æ€è¾“å…¥ã€‚è¿™ç§ç‹¬ç‰¹çš„è®­ç»ƒæ–¹å¼ä½¿å¾—DreamLLMèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ æ‰€æœ‰æ¡ä»¶ã€è¾¹ç¼˜å’Œè”åˆå¤šæ¨¡æ€åˆ†å¸ƒï¼Œä»è€Œå®ç°è‡ªç”±å½¢å¼çš„äº¤é”™å†…å®¹ç”Ÿæˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>DreamLLMåœ¨å¤šä¸ªè§†è§‰-è¯­è¨€ç†è§£å’Œå†…å®¹åˆ›é€ ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒåˆ°æ–‡æœ¬æè¿°ã€è§†è§‰é—®ç­”ã€æ–‡æœ¬ç›¸å…³é—®ç­”ã€æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆç­‰ã€‚åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ï¼ŒDreamLLMåœ¨MS-COCOæ•°æ®é›†ä¸Šå®ç°äº†8.46çš„FIDåˆ†æ•°ï¼Œå¹¶åœ¨MMBenchå’ŒMM-Vetè¯„ä¼°ä¸­åˆ†åˆ«å–å¾—äº†49.1å’Œ35.9çš„åˆ†æ•°ï¼Œåˆ·æ–°äº†ç°æœ‰è®°å½•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DreamLLMçš„ç«¯åˆ°ç«¯ç”Ÿæˆå’Œäº¤é”™ç”Ÿæˆé¢„è®­ç»ƒæ–¹æ³•ä¸ºå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶å±•ç¤ºäº†MLLMsåœ¨å¤šæ¨¡æ€åˆ›é€ æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢DreamLLMåœ¨è§†é¢‘æ•°æ®ã€3Då†…å®¹åˆ›é€ ã€å‡ ä½•ä¿æŒä»»åŠ¡ç­‰æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶æ‰©å±•åˆ°æ›´å¤šæ¨¡æ€ï¼Œå®ç°ç»Ÿä¸€çš„é›¶æ ·æœ¬å¤šæ¨¡æ€é€šç”¨æ¨¡å‹ã€‚</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Making LLaMA SEE and Draw with SEED Tokenizer</td>
      <td>The great success of Large Language Models (LLMs) has expanded the potential<br>of multimodality, contributing to the gradual evolution of General Artificial<br>Intelligence (AGI). A true AGI agent should not only possess the capability to<br>perform predefined multi-tasks but also exhibit emergent abilities in an<br>open-world context. However, despite the considerable advancements made by<br>recent multimodal LLMs, they still fall short in effectively unifying<br>comprehension and generation tasks, let alone open-world emergent abilities. We<br>contend that the key to overcoming the present impasse lies in enabling text<br>and images to be represented and processed interchangeably within a unified<br>autoregressive Transformer. To this end, we introduce SEED, an elaborate image<br>tokenizer that empowers LLMs with the ability to SEE and Draw at the same time.<br>We identify two crucial design principles: (1) Image tokens should be<br>independent of 2D physical patch positions and instead be produced with a 1D<br>causal dependency, exhibiting intrinsic interdependence that aligns with the<br>left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens<br>should capture high-level semantics consistent with the degree of semantic<br>abstraction in words, and be optimized for both discriminativeness and<br>reconstruction during the tokenizer training phase. With SEED tokens, LLM is<br>able to perform scalable multimodal autoregression under its original training<br>recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by<br>large-scale pretraining and instruction tuning on the interleaved textual and<br>visual data, demonstrating impressive performance on a broad range of<br>multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has<br>exhibited compositional emergent abilities such as multi-turn in-context<br>multimodal generation, acting like your AI assistant.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è®©LLaMAâ€œçœ‹â€ä¸â€œç”»â€ï¼šSEED Tokenizerçš„çªç ´<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæ¨åŠ¨äº†å¤šæ¨¡æ€æŠ€æœ¯çš„å‘å±•ï¼Œä¸ºé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„æ¼”è¿›å¥ å®šäº†åŸºç¡€ã€‚ç„¶è€Œï¼Œå°½ç®¡å¤šæ¨¡æ€LLMså–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆä»»åŠ¡æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œæ›´ä¸ç”¨è¯´åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„æ¶Œç°èƒ½åŠ›ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œå…‹æœè¿™ä¸€å›°å¢ƒçš„å…³é”®åœ¨äºä½¿æ–‡æœ¬å’Œå›¾åƒèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„è‡ªå›å½’Transformerä¸­ç›¸äº’è½¬æ¢å’Œè¡¨ç¤ºã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSEED Tokenizer<br>æœ¬æ–‡æå‡ºäº†SEEDï¼Œä¸€ç§åŸºäºVQçš„å›¾åƒåˆ†è¯å™¨ï¼Œå®ƒäº§ç”Ÿå…·æœ‰1Då› æœä¾èµ–æ€§å’Œå¿…è¦é«˜çº§è¯­ä¹‰çš„ç¦»æ•£è§†è§‰ä»£ç ï¼Œä»¥ä¾¿äºè§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚SEED Tokenizerçš„è®¾è®¡åŸåˆ™åŒ…æ‹¬ï¼š<br>1. å›¾åƒtokenåº”ç‹¬ç«‹äº2Dç‰©ç†å—ä½ç½®ï¼Œè€Œæ˜¯é€šè¿‡1Då› æœä¾èµ–æ€§ç”Ÿæˆï¼Œè¡¨ç°å‡ºä¸LLMsä¸­ä»å·¦åˆ°å³çš„è‡ªå›å½’é¢„æµ‹æœºåˆ¶ç›¸ä¸€è‡´çš„å†…åœ¨ç›¸äº’ä¾èµ–æ€§ã€‚<br>2. å›¾åƒtokenåº”æ•è·ä¸è¯è¯­çš„è¯­ä¹‰æŠ½è±¡ç¨‹åº¦ä¸€è‡´çš„é«˜çº§è¯­ä¹‰ï¼Œå¹¶åœ¨åˆ†è¯å™¨è®­ç»ƒé˜¶æ®µä¼˜åŒ–åŒºåˆ†åº¦å’Œé‡å»ºèƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSEED-LLaMA<br>æœ¬æ–‡é€šè¿‡å°†é¢„è®­ç»ƒçš„LLMä¸SEEDåˆ†è¯å™¨ç›¸ç»“åˆï¼Œæ„å»ºäº†SEED-LLaMAæ¨¡å‹ã€‚SEED-LLaMAåœ¨äº¤é”™æ–‡æœ¬å’Œè§†è§‰æ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥å®ç°ä¸‹ä¸€ä¸ªè¯é¢„æµ‹çš„è®­ç»ƒç›®æ ‡ã€‚è¿™ç§æ˜“äºå®ç°ä¸”ç»Ÿä¸€çš„ä»£ç†ä»»åŠ¡ä¿ƒè¿›äº†å¯æ‰©å±•çš„å¤šæ¨¡æ€é¢„è®­ç»ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>SEED-LLaMAåœ¨å¹¿æ³›çš„è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€å›¾åƒ/è§†é¢‘é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æ­¤å¤–ï¼ŒSEED-LLaMAè¿˜å±•ç¤ºäº†å¤šè½®ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ç”Ÿæˆç­‰æ¶Œç°èƒ½åŠ›ï¼Œç±»ä¼¼äºäººç±»çš„AIåŠ©æ‰‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„SEED Tokenizerå’ŒSEED-LLaMAæ¨¡å‹ä¸ºå¤šæ¨¡æ€LLMsçš„è®­ç»ƒå’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚SEED Tokenizerçš„è®¾è®¡åŸåˆ™å’ŒSEED-LLaMAçš„æ¶Œç°èƒ½åŠ›ä¸ºæœªæ¥å¤šæ¨¡æ€AIçš„å‘å±•æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>38</th>
      <td>On the Performance of Multimodal Language Models</td>
      <td>Instruction-tuned large language models (LLMs) have demonstrated promising<br>zero-shot generalization capabilities across various downstream tasks. Recent<br>research has introduced multimodal capabilities to LLMs by integrating<br>independently pretrained vision encoders through model grafting. These<br>multimodal variants undergo instruction tuning, similar to LLMs, enabling<br>effective zero-shot generalization for multimodal tasks. This study conducts a<br>comparative analysis of different multimodal instruction tuning approaches and<br>evaluates their performance across a range of tasks, including complex<br>reasoning, conversation, image captioning, multiple-choice questions (MCQs),<br>and binary classification. Through rigorous benchmarking and ablation<br>experiments, we reveal key insights for guiding architectural choices when<br>incorporating multimodal capabilities into LLMs. However, current approaches<br>have limitations; they do not sufficiently address the need for a diverse<br>multimodal instruction dataset, which is crucial for enhancing task<br>generalization. Additionally, they overlook issues related to truthfulness and<br>factuality when generating responses. These findings illuminate current<br>methodological constraints in adapting language models for image comprehension<br>and provide valuable guidance for researchers and practitioners seeking to<br>harness multimodal versions of LLMs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ€§èƒ½æ¢ç©¶ï¼šè¿ˆå‘æ›´å¼ºå¤§çš„AI<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„è®¸å¤šåº”ç”¨åœºæ™¯æ¶‰åŠå¤šæ¨¡æ€æ•°æ®ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬ï¼Œéœ€è¦ç»“åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯è¿›è¡Œå‡†ç¡®å’Œé²æ£’çš„æ¨ç†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿‘å¹´æ¥ç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å°†å¤šæ¨¡æ€èƒ½åŠ›é›†æˆåˆ°LLMsä¸­ï¼Œé€šè¿‡æ•´åˆé¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨æ¥å®ç°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡å¯¹ä¸åŒçš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤æ‚æ¨ç†ã€å¯¹è¯ã€å›¾åƒæè¿°ã€å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰å’ŒäºŒå…ƒåˆ†ç±»ã€‚é€šè¿‡ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•å’Œæ¶ˆèå®éªŒï¼Œæ­ç¤ºäº†åœ¨å°†å¤šæ¨¡æ€èƒ½åŠ›é›†æˆåˆ°LLMsæ—¶ï¼Œæ¶æ„é€‰æ‹©çš„å…³é”®è§è§£ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ›´å¤§çš„è§†è§‰ç¼–ç å™¨ï¼ˆViT-gï¼‰å¯ä»¥æ˜¾è‘—æé«˜æ‰€æœ‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ•æ‰æ›´ä¸°å¯Œçš„å›¾åƒè¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå¾®è°ƒè§†è§‰å¤´éƒ¨ï¼ˆä¾‹å¦‚Q-Formerï¼‰ä¹Ÿæ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºä¸‹æ¸¸ä»»åŠ¡æå–æ›´å¥½çš„è¡¨ç¤ºï¼Œå¹¶åŠ å¿«è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤šæ¨¡æ€è§†è§‰å¤´éƒ¨å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºæ¯”ä½¿ç”¨ä»…å›¾åƒå¤´éƒ¨æ›´å¥½çš„ä¼˜åŠ¿ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ„å»ºæœ‰æ•ˆçš„å¤šæ¨¡æ€LLMsæ—¶ï¼Œåº”å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š<br>1. ä½¿ç”¨æ›´å¤§çš„è§†è§‰ç¼–ç å™¨ä»¥æ•æ‰æ›´ä¸°å¯Œçš„å›¾åƒè¡¨ç¤ºã€‚<br>2. å¾®è°ƒè§†è§‰å¤´éƒ¨ä»¥æå–æ›´å¥½çš„è¡¨ç¤ºï¼Œå¹¶åŠ å¿«è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚<br>3. åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œå…³æ³¨æ•°æ®å¤šæ ·æ€§å’Œä»»åŠ¡å¤šæ ·æ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚<br>4. æ¢ç´¢å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä»¥å®ç°æ›´å…ˆè¿›çš„æ€§èƒ½ã€‚<br>5. å…³æ³¨ç¼“è§£å¤šæ¨¡æ€æ¨¡å‹çš„å¹»è§‰é—®é¢˜ï¼Œä½¿å…¶æ›´å¯é å’Œæœ‰ç”¨ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>æœ¬æ–‡å¯¹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶æ­ç¤ºäº†æ„å»ºæœ‰æ•ˆå¤šæ¨¡æ€LLMsçš„å…³é”®ç»„ä»¶å’Œç­–ç•¥ã€‚ç ”ç©¶ç»“æœä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å¤šæ ·æ€§å’Œä»»åŠ¡å¤šæ ·æ€§åœ¨æé«˜æ¨¡å‹èƒ½åŠ›æ–¹é¢çš„é‡è¦æ€§ã€‚</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization</td>
      <td>Recently, the remarkable advance of the Large Language Model (LLM) has<br>inspired researchers to transfer its extraordinary reasoning capability to both<br>vision and language data. However, the prevailing approaches primarily regard<br>the visual input as a prompt and focus exclusively on optimizing the text<br>generation process conditioned upon vision content by a frozen LLM. Such an<br>inequitable treatment of vision and language heavily constrains the model's<br>potential. In this paper, we break through this limitation by representing both<br>vision and language in a unified form. Specifically, we introduce a<br>well-designed visual tokenizer to translate the non-linguistic image into a<br>sequence of discrete tokens like a foreign language that LLM can read. The<br>resulting visual tokens encompass high-level semantics worthy of a word and<br>also support dynamic sequence length varying from the image. Coped with this<br>tokenizer, the presented foundation model called LaVIT can handle both image<br>and text indiscriminately under the same generative learning paradigm. This<br>unification empowers LaVIT to serve as an impressive generalist interface to<br>understand and generate multi-modal content simultaneously. Extensive<br>experiments further showcase that it outperforms the existing models by a large<br>margin on massive vision-language tasks. Our code and models are available at<br>https://github.com/jy0205/LaVIT.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LaVITï¼šç»Ÿä¸€è¯­è¨€-è§†è§‰é¢„è®­ç»ƒï¼Œçªç ´å¤šæ¨¡æ€ç†è§£çš„å±€é™<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›æ¿€å‘äº†ç ”ç©¶äººå‘˜å°†å…¶åº”ç”¨äºè§†è§‰å’Œè¯­è¨€æ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å°†è§†è§‰è¾“å…¥è§†ä¸ºæç¤ºï¼Œå¹¶é€šè¿‡å†»ç»“çš„LLMä¼˜åŒ–åŸºäºè§†è§‰å†…å®¹çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿™ç§å¯¹è§†è§‰å’Œè¯­è¨€çš„ä¸å¹³ç­‰å¤„ç†ä¸¥é‡é™åˆ¶äº†æ¨¡å‹çš„æ½œåŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€è¡¨ç¤ºå½¢å¼<br>æœ¬æ–‡æå‡ºäº†LaVITï¼ˆLanguage-VIsion Transformerï¼‰ï¼Œä¸€ç§æ–°çš„é€šç”¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå®ƒç»§æ‰¿äº†LLMæˆåŠŸçš„é¢„æµ‹ä¸‹ä¸€ä¸ªå›¾åƒ/æ–‡æœ¬æ ‡è®°çš„è‡ªå›å½’ç”Ÿæˆå­¦ä¹ èŒƒå¼ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒLaVITå¼•å…¥äº†ä¸€ç§åŠ¨æ€è§†è§‰æ ‡è®°åŒ–æœºåˆ¶ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºLLMå¯ä»¥ç†è§£çš„ç¦»æ•£æ ‡è®°åºåˆ—ï¼Œä»è€Œåœ¨ç»Ÿä¸€çš„ç”Ÿæˆç›®æ ‡ä¸‹åŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€è§†è§‰æ ‡è®°åŒ–<br>LaVITçš„è§†è§‰æ ‡è®°åŒ–æœºåˆ¶åŒ…æ‹¬ä¸€ä¸ªé€‰æ‹©å™¨å’Œåˆå¹¶å™¨ã€‚é€‰æ‹©å™¨é¦–å…ˆå†³å®šå“ªäº›è§†è§‰å—åŒ…å«ä¿¡æ¯ä¸°å¯Œçš„è¯­ä¹‰ï¼Œå¹¶å°†å…¶é€‰ä¸­ä»¥ç¼–ç æ•´ä¸ªå›¾åƒã€‚åˆå¹¶å™¨è¿›ä¸€æ­¥å‹ç¼©æœªé€‰ä¸­çš„å—åˆ°ä¿ç•™çš„å—ä¸Šï¼Œä»¥å‡å°‘è§†è§‰å—ä¹‹é—´çš„å†—ä½™ï¼Œä»è€Œä¸ºä¸åŒçš„å›¾åƒç”ŸæˆåŠ¨æ€é•¿åº¦çš„åºåˆ—ã€‚ä¿ç•™çš„è§†è§‰æ ‡è®°è¿›ä¸€æ­¥é‡åŒ–ä¸ºç¦»æ•£ä»£ç ï¼Œä½œä¸ºé¢„è®­ç»ƒæœŸé—´è§†è§‰æ ‡è®°çš„ç›‘ç£ä¿¡å·ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>LaVITåœ¨å¹¿æ³›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ç­‰ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒLaVITåœ¨ç†è§£å’Œç”Ÿæˆå¤šæ¨¡æ€å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ¥å—å¤šç§æ¨¡æ€ç»„åˆä½œä¸ºæç¤ºï¼Œç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œå¹¶ç†è§£å›¾åƒå†…å®¹ï¼Œç”Ÿæˆç®€æ´çš„æ–‡æœ¬æè¿°ï¼Œå›ç­”å…³äºå›¾åƒç»†èŠ‚çš„å„ç§é—®é¢˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LaVITçš„æˆåŠŸè¡¨æ˜ï¼Œé€šè¿‡å°†è§†è§‰å’Œè¯­è¨€ç»Ÿä¸€è¡¨ç¤ºä¸ºç¦»æ•£æ ‡è®°ï¼Œå¹¶ä½¿ç”¨ç»Ÿä¸€çš„ç”Ÿæˆç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ å¤šæ¨¡æ€äº¤äº’å’Œå¯¹é½ï¼Œä»è€Œå®ç°å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä¸ºæ„å»ºæ›´é€šç”¨ã€æ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Planting a SEED of Vision in Large Language Model</td>
      <td>We present SEED, an elaborate image tokenizer that empowers Large Language<br>Models (LLMs) with the emergent ability to SEE and Draw at the same time.<br>Research on image tokenizers has previously reached an impasse, as frameworks<br>employing quantized visual tokens have lost prominence due to subpar<br>performance and convergence in multimodal comprehension (compared to BLIP-2,<br>etc.) or generation (compared to Stable Diffusion, etc.). Despite the<br>limitations, we remain confident in its natural capacity to unify visual and<br>textual representations, facilitating scalable multimodal training with LLM's<br>original recipe. In this study, we identify two crucial principles for the<br>architecture and training of SEED that effectively ease subsequent alignment<br>with LLMs. (1) Image tokens should be independent of 2D physical patch<br>positions and instead be produced with a 1D causal dependency, exhibiting<br>intrinsic interdependence that aligns with the left-to-right autoregressive<br>prediction mechanism in LLMs. (2) Image tokens should capture high-level<br>semantics consistent with the degree of semantic abstraction in words, and be<br>optimized for both discriminativeness and reconstruction during the tokenizer<br>training phase. As a result, the off-the-shelf LLM is able to perform both<br>image-to-text and text-to-image generation by incorporating our SEED through<br>efficient LoRA tuning. Comprehensive multimodal pretraining and instruction<br>tuning, which may yield improved results, are reserved for future<br>investigation. This version of SEED was trained in 5.7 days using only 64 V100<br>GPUs and 5M publicly available image-text pairs. Our preliminary study<br>emphasizes the great potential of discrete visual tokens in versatile<br>multimodal LLMs and the importance of proper image tokenizers in broader<br>research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¤å…¥è§†è§‰ç§å­<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šçš„æ˜¾è‘—æˆåŠŸï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å°†è§†è§‰ä¿¡æ¯æ•´åˆåˆ°LLMsä¸­ï¼Œä»¥å®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€LLMsåœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šå°šæœªè¾¾åˆ°LLMsåœ¨æ–‡æœ¬ä»»åŠ¡ä¸Šçš„æ°´å¹³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSEEDçš„å›¾åƒåˆ†è¯å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒåˆ†è¯å™¨åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼Œå¹¶ä½¿LLMsèƒ½å¤ŸåŒæ—¶è¿›è¡Œâ€œçœ‹â€å’Œâ€œç”»â€ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå› æœä¾èµ–çš„å›¾åƒåˆ†è¯å™¨<br>SEEDé‡‡ç”¨å› æœQ-Formerå°†2Då›¾åƒç‰¹å¾è½¬æ¢ä¸ºå…·æœ‰1Då› æœä¾èµ–å…³ç³»çš„è¯­ä¹‰åµŒå…¥ï¼Œä»è€Œä¸LLMsä¸­çš„è‡ªå›å½’é¢„æµ‹æœºåˆ¶ç›¸åŒ¹é…ã€‚è¿™ç§è®¾è®¡ä½¿å¾—å›¾åƒåˆ†è¯å™¨èƒ½å¤Ÿæ›´å¥½åœ°ä¸LLMsè¿›è¡Œå¯¹é½ï¼Œå¹¶æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé«˜å±‚æ¬¡çš„è¯­ä¹‰è¡¨ç¤º<br>SEEDçš„å›¾åƒåˆ†è¯å™¨ä¸ä»…æ•è·äº†å›¾åƒçš„ä½å±‚æ¬¡ç»†èŠ‚ï¼Œè¿˜æ•æ‰äº†å›¾åƒçš„é«˜å±‚æ¬¡è¯­ä¹‰ä¿¡æ¯ã€‚è¿™ä½¿å¾—å›¾åƒåˆ†è¯å™¨èƒ½å¤Ÿæ›´å¥½åœ°ä¸LLMsä¸­çš„æ–‡æœ¬åˆ†è¯å™¨è¿›è¡Œå¯¹é½ï¼Œå¹¶æé«˜å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>SEEDåœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨SEEDå¯¹LLMsè¿›è¡Œå¾®è°ƒï¼ŒSEED-OPT2.7Båœ¨é›¶æ ·æœ¬å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„SEEDå›¾åƒåˆ†è¯å™¨ä¸ºå¤šæ¨¡æ€LLMsçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚SEEDçš„è®¾è®¡åŸåˆ™å’Œè®­ç»ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä»¥å®ç°æ›´å¥½çš„æ€§èƒ½å’Œæ›´ä½çš„è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼ŒSEEDçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç¦»æ•£è§†è§‰åˆ†è¯å™¨åœ¨å¤šæ¨¡æ€LLMsä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶æœ‰æœ›æ¨åŠ¨å¤šæ¨¡æ€LLMsçš„è¿›ä¸€æ­¥å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Emu: Generative Pretraining in Multimodality</td>
      <td>We present Emu, a Transformer-based multimodal foundation model, which can<br>seamlessly generate images and texts in multimodal context. This omnivore model<br>can take in any single-modality or multimodal data input indiscriminately<br>(e.g., interleaved image, text and video) through a one-model-for-all<br>autoregressive training process. First, visual signals are encoded into<br>embeddings, and together with text tokens form an interleaved input sequence.<br>Emu is then end-to-end trained with a unified objective of classifying the next<br>text token or regressing the next visual embedding in the multimodal sequence.<br>This versatile multimodality empowers the exploration of diverse pretraining<br>data sources at scale, such as videos with interleaved frames and text,<br>webpages with interleaved images and text, as well as web-scale image-text<br>pairs and video-text pairs. Emu can serve as a generalist multimodal interface<br>for both image-to-text and text-to-image tasks, and supports in-context image<br>and text generation. Across a broad range of zero-shot/few-shot tasks including<br>image captioning, visual question answering, video question answering and<br>text-to-image generation, Emu demonstrates superb performance compared to<br>state-of-the-art large multimodal models. Extended capabilities such as<br>multimodal assistants via instruction tuning are also demonstrated with<br>impressive performance.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Emuï¼šå¤šæ¨¡æ€ç”Ÿæˆå¼é¢„è®­ç»ƒçš„çªç ´<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„çªç ´ï¼Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¹Ÿé€æ¸å…´èµ·ï¼Œæ—¨åœ¨èåˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œå®ç°æ›´ä¸°å¯Œçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LMMså¤§å¤šå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š<br><br>* **è§†è§‰ä¿¡æ¯å¤„ç†å—é™**ï¼šè®¸å¤šæ¨¡å‹ä»…é¢„æµ‹æ–‡æœ¬tokenï¼Œè€Œå¿½ç•¥äº†å¯¹è§†è§‰ä¿¡æ¯çš„ç›‘ç£ï¼Œé™åˆ¶äº†æ¨¡å‹èƒ½åŠ›ã€‚<br>* **æ•°æ®æ¥æºå•ä¸€**ï¼šä¸»è¦ä¾èµ–å›¾åƒ-æ–‡æœ¬å¯¹æˆ–æ–‡æ¡£ï¼Œè€Œå¿½ç•¥äº†è§†é¢‘æ•°æ®ä½œä¸ºæ½œåœ¨çš„å¯æ‰©å±•å¤šæ¨¡æ€æ•°æ®æºã€‚<br>* **åº”ç”¨åœºæ™¯æœ‰é™**ï¼šå¤§å¤šé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œç¼ºä¹é€šç”¨çš„å¤šæ¨¡æ€æ¥å£ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡<br>* Emué‡‡ç”¨è‡ªå›å½’æ–¹å¼ï¼Œå°†å›¾åƒã€æ–‡æœ¬å’Œè§†é¢‘æ•°æ®ç»Ÿä¸€ç¼–ç ä¸ºåºåˆ—ï¼Œå¹¶è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚<br>* è§†è§‰ä¿¡å·é€šè¿‡EVA-CLIPç¼–ç ä¸ºåµŒå…¥ï¼Œå¹¶ä¸æ–‡æœ¬tokenå½¢æˆäº¤é”™åºåˆ—ã€‚<br>* Emuçš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå…ƒç´ ï¼Œæ— è®ºæ˜¯æ–‡æœ¬tokenè¿˜æ˜¯è§†è§‰åµŒå…¥ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCausal Transformer<br>* ä¸ºäº†æ›´å¥½åœ°æ•æ‰å›¾åƒç‰¹å¾ï¼ŒEmuå¼•å…¥Causal Transformerå°†2Dè§†è§‰ä¿¡å·è½¬æ¢ä¸º1Då› æœåºåˆ—ã€‚<br>* Causal Transformerç±»ä¼¼äºTransformerè§£ç å™¨ï¼ŒåŒ…å«å› æœè‡ªæ³¨æ„åŠ›å±‚ã€äº¤å‰æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚ã€‚<br>* é€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚ï¼ŒCausal Transformerèšåˆæ¥è‡ªEVA-CLIPçš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶è¾“å‡ºNä¸ªåµŒå…¥ï¼Œæ•æ‰å›¾åƒçš„å› æœä¾èµ–å…³ç³»ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§†è§‰è§£ç å™¨<br>* Emuä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å°†è§†è§‰åµŒå…¥è§£ç ä¸ºå›¾åƒã€‚<br>* è§†è§‰è§£ç å™¨åŸºäºStable Diffusionè¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶é€šè¿‡å¾®è°ƒè¿›è¡Œä¼˜åŒ–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ<br>* Emué€šè¿‡åœ¨å¤šæ¨¡æ€å¯¹è¯æ•°æ®ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¹¶æ‰§è¡Œäººç±»æŒ‡ä»¤ã€‚<br>* æŒ‡ä»¤å¾®è°ƒé‡‡ç”¨ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¨¡å—ï¼Œä»…å¯¹å¤šæ¨¡æ€å»ºæ¨¡LLMçš„è‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œé€‚é…ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>* Emuåœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€è§†é¢‘é—®ç­”å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„LMMsã€‚<br>* Emu-Iï¼ˆæŒ‡ä»¤å¾®è°ƒåçš„Emuï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³è¶…è¶Šäº†å‚æ•°é‡æ›´å¤§çš„æ¨¡å‹ã€‚<br>* Emuè¿˜å±•ç¤ºäº†åœ¨ä¸Šä¸‹æ–‡æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆã€å›¾åƒæ··åˆã€è§†é¢‘ç†è§£å’Œç°å®ä¸–ç•ŒçŸ¥è¯†æ¨ç†ç­‰æ–¹é¢çš„æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>* Emuçš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ–¹æ³•ä¸ºæ„å»ºé€šç”¨çš„å¤šæ¨¡æ€æ¥å£æä¾›äº†æ–°çš„æ€è·¯ã€‚<br>* Causal Transformeræœ‰æ•ˆåœ°æ•æ‰äº†å›¾åƒçš„å› æœä¾èµ–å…³ç³»ï¼Œæé«˜äº†æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„å¤„ç†èƒ½åŠ›ã€‚<br>* è§†è§‰è§£ç å™¨å®ç°äº†ä»è§†è§‰åµŒå…¥åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œæ‹“å±•äº†æ¨¡å‹çš„åº”ç”¨åœºæ™¯ã€‚<br>* å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¹¶æ‰§è¡Œäººç±»æŒ‡ä»¤ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚<br><br>## ğŸ“š æ€»ç»“<br>Emuä½œä¸ºä¸€æ¬¾å¤šæ¨¡æ€ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å…¶ç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ–¹æ³•ã€Causal Transformerã€è§†è§‰è§£ç å™¨å’Œå¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒç­‰åˆ›æ–°ç‚¹ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹å’Œåº”ç”¨æä¾›äº†æ–°çš„æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>42</th>
      <td>Any-to-Any Generation via Composable Diffusion</td>
      <td>We present Composable Diffusion (CoDi), a novel generative model capable of<br>generating any combination of output modalities, such as language, image,<br>video, or audio, from any combination of input modalities. Unlike existing<br>generative AI systems, CoDi can generate multiple modalities in parallel and<br>its input is not limited to a subset of modalities like text or image. Despite<br>the absence of training datasets for many combinations of modalities, we<br>propose to align modalities in both the input and output space. This allows<br>CoDi to freely condition on any input combination and generate any group of<br>modalities, even if they are not present in the training data. CoDi employs a<br>novel composable generation strategy which involves building a shared<br>multimodal space by bridging alignment in the diffusion process, enabling the<br>synchronized generation of intertwined modalities, such as temporally aligned<br>video and audio. Highly customizable and flexible, CoDi achieves strong<br>joint-modality generation quality, and outperforms or is on par with the<br>unimodal state-of-the-art for single-modality synthesis. The project page with<br>demonstrations and code is at https://codi-gen.github.io</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | CoDiï¼šä»»æ„æ¨¡æ€ç”Ÿæˆçš„æ–°çºªå…ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œè·¨æ¨¡æ€æ¨¡å‹åœ¨ç”Ÿæˆä¸€ä¸ªæ¨¡æ€ä»å¦ä¸€ä¸ªæ¨¡æ€çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°æ–‡æœ¬ã€æ–‡æœ¬åˆ°å›¾åƒæˆ–æ–‡æœ¬åˆ°éŸ³é¢‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•å¤„ç†å¤šä¸ªæ¨¡æ€å…±å­˜å’Œäº¤äº’çš„æƒ…å†µã€‚è™½ç„¶å¯ä»¥å°†æ¨¡æ€ç‰¹å®šçš„ç”Ÿæˆæ¨¡å‹åœ¨å¤šæ­¥ç”Ÿæˆè®¾ç½®ä¸­ä¸²è”èµ·æ¥ï¼Œä½†æ¯ä¸€æ­¥çš„ç”Ÿæˆèƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œå¹¶ä¸”ä¸²è¡Œå¤šæ­¥è¿‡ç¨‹å¯èƒ½æ—¢ç¹çåˆç¼“æ…¢ã€‚æ­¤å¤–ï¼Œç‹¬ç«‹ç”Ÿæˆçš„å•æ¨¡æ€æµåœ¨åæœŸå¤„ç†ä¸­æ‹¼æ¥åœ¨ä¸€èµ·æ—¶ä¸ä¼šä¿æŒä¸€è‡´å’Œå¯¹é½ï¼ˆä¾‹å¦‚ï¼ŒåŒæ­¥çš„è§†é¢‘å’ŒéŸ³é¢‘ï¼‰ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿä»ä»»ä½•ä¸€ç»„è¾“å…¥æ¡ä»¶ç”Ÿæˆä»»ä½•æ¨¡æ€ç»„åˆçš„å…¨é¢ä¸”é€šç”¨çš„æ¨¡å‹ï¼Œä»¥æ›´å‡†ç¡®åœ°æ•æ‰ä¸–ç•Œçš„å¤šæ¨¡æ€æœ¬è´¨å’Œäººç±»çš„ç†è§£ï¼Œæ— ç¼åœ°æ•´åˆæ¥è‡ªå¹¿æ³›æ¥æºçš„ä¿¡æ¯ï¼Œå¹¶ä½¿äººç±»ä¸äººå·¥æ™ºèƒ½çš„äº¤äº’æ›´åŠ æ²‰æµ¸å¼ï¼ˆä¾‹å¦‚ï¼ŒåŒæ—¶ç”Ÿæˆè¿è´¯çš„è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬æè¿°ï¼‰ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>CoDiï¼Œå³â€œå¯ç»„åˆæ‰©æ•£â€ï¼Œæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†å’Œç”Ÿæˆä»»æ„æ¨¡æ€ç»„åˆçš„æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒCoDié‡‡ç”¨äº†ä»¥ä¸‹åˆ›æ–°æ–¹æ³•ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¯ç»„åˆå¤šæ¨¡æ€æ¡ä»¶<br>CoDié€šè¿‡å°†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘çš„æç¤ºç¼–ç å™¨å¯¹é½åˆ°åŒä¸€ç©ºé—´ï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹ä»»ä½•è¾“å…¥/æç¤ºæ¨¡æ€ç»„åˆè¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚é€šè¿‡ç®€å•åœ°å¯¹é½åµŒå…¥çš„åŠ æƒæ’å€¼ï¼ŒCoDièƒ½å¤Ÿä½¿ç”¨å•æ¡ä»¶è®­ç»ƒï¼ˆå³åªæœ‰ä¸€ä¸ªè¾“å…¥ï¼‰çš„æ¨¡å‹æ‰§è¡Œé›¶æ ·æœ¬å¤šæ¡ä»¶è®­ç»ƒï¼ˆå³å¤šä¸ªè¾“å…¥ï¼‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¯ç»„åˆæ‰©æ•£<br>CoDié¦–å…ˆç‹¬ç«‹è®­ç»ƒå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ç§åä¸ºâ€œæ½œåœ¨å¯¹é½â€çš„æ–°æœºåˆ¶ï¼Œè¿™äº›æ‰©æ•£æ¨¡å‹å­¦ä¹ è·¨æ¨¡æ€è¿›è¡Œè”åˆå¤šæ¨¡æ€ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒCoDiåœ¨æ¯ä¸ªæ‰©æ•£å™¨ä¸­æ·»åŠ äº†è·¨æ¨¡æ€æ³¨æ„åŠ›å­å±‚ï¼Œå¹¶å°†ä¸åŒLDMçš„æ½œåœ¨å˜é‡æŠ•å½±åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œä»è€Œå®ç°åŒæ­¥ç”Ÿæˆç›¸äº’äº¤ç»‡çš„æ¨¡æ€ï¼Œä¾‹å¦‚æ—¶é—´å¯¹é½çš„è§†é¢‘å’ŒéŸ³é¢‘ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>CoDiåœ¨å„ç§åœºæ™¯ä¸‹è¡¨ç°å‡ºå“è¶Šçš„ç”Ÿæˆè´¨é‡ï¼Œå…¶åˆæˆè´¨é‡ä¸å•æ¨¡æ€åˆ°å•æ¨¡æ€çš„æœ€å…ˆè¿›æ°´å¹³ç›¸å½“ç”šè‡³æ›´å¥½ã€‚ä¾‹å¦‚ï¼Œåœ¨éŸ³é¢‘ç”Ÿæˆå’ŒéŸ³é¢‘å­—å¹•æ–¹é¢ï¼ŒCoDiçš„æ€§èƒ½ä¸åŸºäºè‡ªå›å½’å˜æ¢å™¨çš„æœ€å…ˆè¿›æ°´å¹³ç›¸å½“ã€‚æ­¤å¤–ï¼ŒCoDiè¿˜èƒ½å¤Ÿç”Ÿæˆå„ç§æ¨¡æ€ç»„åˆï¼Œä¾‹å¦‚æ–‡æœ¬åˆ°è§†é¢‘+éŸ³é¢‘ã€æ–‡æœ¬åˆ°å›¾åƒ+æ–‡æœ¬+éŸ³é¢‘ä»¥åŠæ–‡æœ¬+éŸ³é¢‘+å›¾åƒåˆ°è§†é¢‘+éŸ³é¢‘ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>CoDiä¸ºå¤šæ¨¡æ€ç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†çªç ´æ€§çš„è¿›å±•ï¼Œå…¶åˆ›æ–°çš„å¯ç»„åˆæ‰©æ•£æ–¹æ³•ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯ã€‚CoDiçš„è®¾è®¡å’Œè®­ç»ƒç­–ç•¥å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚è·¨æ¨¡æ€æ£€ç´¢ã€å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ å’Œå¤šæ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼ŒCoDiçš„çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ä½¿å…¶èƒ½å¤Ÿé€‚åº”å„ç§åº”ç”¨åœºæ™¯ï¼Œä¾‹å¦‚ç”Ÿæˆå¼è‰ºæœ¯ã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ã€‚</td>
    </tr>
    <tr>
      <th>43</th>
      <td>Multimodal Unified Attention Networks for Vision-and-Language Interactions</td>
      <td>Learning an effective attention mechanism for multimodal data is important in<br>many vision-and-language tasks that require a synergic understanding of both<br>the visual and textual contents. Existing state-of-the-art approaches use<br>co-attention models to associate each visual object (e.g., image region) with<br>each textual object (e.g., query word). Despite the success of these<br>co-attention models, they only model inter-modal interactions while neglecting<br>intra-modal interactions. Here we propose a general `unified attention' model<br>that simultaneously captures the intra- and inter-modal interactions of<br>multimodal features and outputs their corresponding attended representations.<br>By stacking such unified attention blocks in depth, we obtain the deep<br>Multimodal Unified Attention Network (MUAN), which can seamlessly be applied to<br>the visual question answering (VQA) and visual grounding tasks. We evaluate our<br>MUAN models on two VQA datasets and three visual grounding datasets, and the<br>results show that MUAN achieves top-level performance on both tasks without<br>bells and whistles.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€ç»Ÿä¸€æ³¨æ„åŠ›ç½‘ç»œï¼šè§†è§‰ä¸è¯­è¨€äº¤äº’çš„æ–°çªç ´<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨è®¸å¤šéœ€è¦è§†è§‰å’Œè¯­è¨€å†…å®¹ååŒç†è§£çš„è§†è§‰ä¸è¯­è¨€ä»»åŠ¡ä¸­ï¼Œå­¦ä¹ æœ‰æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å¯¹äºå¤šæ¨¡æ€æ•°æ®è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ä½¿ç”¨ååŒæ³¨æ„åŠ›æ¨¡å‹å°†æ¯ä¸ªè§†è§‰å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒåŒºåŸŸï¼‰ä¸æ¯ä¸ªæ–‡æœ¬å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼ŒæŸ¥è¯¢è¯ï¼‰ç›¸å…³è”ã€‚å°½ç®¡è¿™äº›ååŒæ³¨æ„åŠ›æ¨¡å‹å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åªæ¨¡æ‹Ÿäº†æ¨¡æ€é—´äº¤äº’ï¼Œè€Œå¿½ç•¥äº†æ¨¡æ€å†…äº¤äº’ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„â€œç»Ÿä¸€æ³¨æ„åŠ›â€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒæ—¶æ•è·å¤šæ¨¡æ€ç‰¹å¾çš„æ¨¡æ€å†…å’Œæ¨¡æ€é—´äº¤äº’ï¼Œå¹¶è¾“å‡ºç›¸åº”çš„å…³æ³¨è¡¨ç¤ºã€‚é€šè¿‡åœ¨æ·±åº¦ä¸Šå †å è¿™æ ·çš„ç»Ÿä¸€æ³¨æ„åŠ›å—ï¼Œæˆ‘ä»¬è·å¾—äº†æ·±åº¦å¤šæ¨¡æ€ç»Ÿä¸€æ³¨æ„åŠ›ç½‘ç»œï¼ˆMUANï¼‰ï¼Œå®ƒå¯ä»¥æ— ç¼åœ°åº”ç”¨äºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œè§†è§‰å®šä½ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å‹<br>æœ¬æ–‡å°†å•æ¨¡æ€çš„è‡ªæ³¨æ„åŠ›æ¨¡å‹æ‰©å±•ä¸ºç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è¡¨å¾å¤šæ¨¡æ€æ•°æ®çš„æ¨¡æ€å†…å’Œæ¨¡æ€é—´äº¤äº’ã€‚é€šè¿‡å †å è¿™æ ·çš„ç»Ÿä¸€æ³¨æ„åŠ›æ¨¡å‹ï¼ˆå³UAå—ï¼‰ï¼Œæˆ‘ä»¬è·å¾—äº†ç®€æ´çš„å¤šæ¨¡æ€ç»Ÿä¸€æ³¨æ„åŠ›ç½‘ç»œï¼ˆMUANï¼‰ï¼Œè¯¥ç½‘ç»œå¯ä»¥è¿›è¡Œç²¾ç¡®çš„å¤šæ¨¡æ€æ¨ç†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé—¨æ§è‡ªæ³¨æ„åŠ›æ¨¡å‹<br>æœ¬æ–‡å°†åŸå§‹çš„è‡ªæ³¨æ„åŠ›æ¨¡å‹ä¿®æ”¹ä¸ºé—¨æ§è‡ªæ³¨æ„åŠ›ï¼ˆGSAï¼‰æ¨¡å‹ï¼Œä½œä¸ºUAå—çš„åŸºæœ¬ç»„ä»¶ï¼Œè¿™æœ‰åŠ©äºæ›´å‡†ç¡®å’Œé²æ£’çš„æ³¨æ„åŠ›å­¦ä¹ ï¼Œå¹¶ä¸ºç‰¹å®šä»»åŠ¡æä¾›æ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ä¸¤ä¸ªVQAæ•°æ®é›†å’Œä¸‰ä¸ªè§†è§‰å®šä½æ•°æ®é›†ä¸Šè¯„ä¼°äº†MUANæ¨¡å‹ï¼Œç»“æœè¡¨æ˜MUANåœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†é¡¶çº§æ€§èƒ½ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•æ•°æ®é›†ç‰¹å®šçš„æ¨¡å‹è°ƒæ•´ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„MUANæ¨¡å‹ä¸ºå¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ¨¡æ€å†…å’Œæ¨¡æ€é—´äº¤äº’åŒæ—¶å»ºæ¨¡ï¼Œå¹¶é€šè¿‡æ·±åº¦å †å ç»Ÿä¸€æ³¨æ„åŠ›å—è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„é—¨æ§è‡ªæ³¨æ„åŠ›æ¨¡å‹ä¹Ÿä¸ºæ³¨æ„åŠ›å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæé«˜æ³¨æ„åŠ›å­¦ä¹ çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</td>
    </tr>
    <tr>
      <th>44</th>
      <td>UniMuMo: Unified Text, Music and Motion Generation</td>
      <td>We introduce UniMuMo, a unified multimodal model capable of taking arbitrary<br>text, music, and motion data as input conditions to generate outputs across all<br>three modalities. To address the lack of time-synchronized data, we align<br>unpaired music and motion data based on rhythmic patterns to leverage existing<br>large-scale music-only and motion-only datasets. By converting music, motion,<br>and text into token-based representation, our model bridges these modalities<br>through a unified encoder-decoder transformer architecture. To support multiple<br>generation tasks within a single framework, we introduce several architectural<br>improvements. We propose encoding motion with a music codebook, mapping motion<br>into the same feature space as music. We introduce a music-motion parallel<br>generation scheme that unifies all music and motion generation tasks into a<br>single transformer decoder architecture with a single training task of<br>music-motion joint generation. Moreover, the model is designed by fine-tuning<br>existing pre-trained single-modality models, significantly reducing<br>computational demands. Extensive experiments demonstrate that UniMuMo achieves<br>competitive results on all unidirectional generation benchmarks across music,<br>motion, and text modalities. Quantitative results are available in the<br>\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | UniMuMoï¼šç»Ÿä¸€æ–‡æœ¬ã€éŸ³ä¹å’ŒåŠ¨ä½œç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éŸ³ä¹ã€åŠ¨ä½œå’Œæ–‡æœ¬æ˜¯äººç±»è¡¨è¾¾å’Œæ²Ÿé€šçš„é‡è¦æ–¹å¼ã€‚å®ƒä»¬ä¹‹é—´å­˜åœ¨ç€ç´§å¯†çš„è”ç³»ï¼Œä¾‹å¦‚èˆè¹ˆåŠ¨ä½œä¸éŸ³ä¹çš„èŠ‚å¥åŒæ­¥ï¼Œæ–‡æœ¬æè¿°å¯ä»¥ä¼ è¾¾éŸ³ä¹å’ŒåŠ¨ä½œçš„æƒ…æ„Ÿå’Œå†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹é€šå¸¸åªèƒ½å¤„ç†å•å‘çš„ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ä»æ–‡æœ¬ç”ŸæˆéŸ³ä¹ï¼Œæˆ–è€…ä»éŸ³ä¹ç”ŸæˆåŠ¨ä½œï¼Œç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥æ”¯æŒå¤šç§æ¨¡æ€çš„ç”Ÿæˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šéŸ³ä¹-åŠ¨ä½œå¯¹é½<br>ä¸ºäº†è§£å†³ç¼ºä¹æ—¶é—´åŒæ­¥æ•°æ®çš„é—®é¢˜ï¼ŒUniMuMo æå‡ºäº†åŸºäºèŠ‚å¥æ¨¡å¼å¯¹é½æœªé…å¯¹çš„éŸ³é¢‘å’ŒåŠ¨ä½œæ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡æå–éŸ³ä¹èŠ‚æ‹å’ŒåŠ¨ä½œè§†è§‰èŠ‚æ‹ï¼Œå¹¶ä½¿ç”¨åŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰æ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼Œå°†åŠ¨ä½œåºåˆ—è°ƒæ•´ä»¥åŒ¹é…éŸ³ä¹èŠ‚æ‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šéŸ³ä¹-åŠ¨ä½œå¹¶è¡Œç”Ÿæˆ<br>UniMuMo å¼•å…¥äº†ä¸€ç§æ–°çš„éŸ³ä¹-åŠ¨ä½œå¹¶è¡Œç”Ÿæˆæ–¹æ¡ˆï¼Œå°†æ‰€æœ‰éŸ³ä¹å’ŒåŠ¨ä½œç”Ÿæˆä»»åŠ¡ç»Ÿä¸€åˆ°ä¸€ä¸ª Transformer è§£ç å™¨æ¶æ„ä¸­ã€‚é€šè¿‡åŒæ—¶è¿›è¡Œä¸¤ä¸ªç›¸äº’æ¡ä»¶åŒ–çš„è‡ªå›å½’ç”Ÿæˆæµï¼ˆéŸ³ä¹å’ŒåŠ¨ä½œï¼‰ï¼ŒUniMuMo èƒ½å¤Ÿåœ¨å•ä¸ªè®­ç»ƒä»»åŠ¡ä¸­å®ç°éŸ³ä¹-åŠ¨ä½œè”åˆç”Ÿæˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šéŸ³ä¹-åŠ¨ä½œè”åˆåˆ†è¯å™¨<br>UniMuMo ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„éŸ³é¢‘åˆ†è¯å™¨ Encodec å’Œä¸€ä¸ªæ–°çš„åŠ¨ä½œç¼–ç å™¨-è§£ç å™¨ï¼Œå°†éŸ³ä¹å’ŒåŠ¨ä½œåºåˆ—ç¼–ç åˆ°ç›¸åŒçš„ç‰¹å¾ç©ºé—´ä¸­ã€‚è¿™ç§è®¾è®¡ä¸ä»…æœ‰æ•ˆåœ°å¼¥åˆäº†æ¨¡æ€ä¹‹é—´çš„å·®è·ï¼Œè¿˜æ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜äº†æ€§èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šéŸ³ä¹-åŠ¨ä½œæ¡ä»¶æè¿°ç”Ÿæˆ<br>UniMuMo ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„éŸ³ä¹-åŠ¨ä½œè§£ç å™¨ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶å¾®è°ƒä¸€ä¸ª T5 è§£ç å™¨æ¥ç”ŸæˆéŸ³ä¹å’ŒåŠ¨ä½œçš„æ–‡æœ¬æè¿°ã€‚ä¸ºäº†æ›´å¥½åœ°æ•æ‰éŸ³ä¹å’ŒåŠ¨ä½œç‰¹å¾ï¼ŒUniMuMo å¼•å…¥äº†ä¸€ä¸ªå¯è®­ç»ƒçš„å…¨è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶å°†å…¶ä¸ T5 è§£ç å™¨ä¸€èµ·å¾®è°ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>UniMuMo åœ¨å„ç§å•å‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†ä¸ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°éŸ³ä¹ã€éŸ³ä¹åˆ°åŠ¨ä½œã€åŠ¨ä½œåˆ°éŸ³ä¹ã€éŸ³ä¹æè¿°å’ŒåŠ¨ä½œæè¿°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMuMo èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„éŸ³ä¹ã€åŠ¨ä½œå’Œæ–‡æœ¬å†…å®¹ï¼Œå¹¶æ”¯æŒå¤šç§æ¨¡æ€çš„ç»„åˆç”Ÿæˆã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>UniMuMo çš„åˆ›æ–°æ–¹æ³•ä¸ºå¤šæ¨¡æ€ç”Ÿæˆç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ï¼Œå¹¶ä¸ºæœªæ¥æ¨¡å‹çš„è®¾è®¡å’Œåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚å…¶æ ¸å¿ƒæ€æƒ³å’Œæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆï¼Œæˆ–è€…è§†é¢‘å’ŒéŸ³é¢‘çš„ç”Ÿæˆã€‚</td>
    </tr>
    <tr>
      <th>45</th>
      <td>MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation</td>
      <td>Medicine is inherently multimodal and multitask, with diverse data modalities<br>spanning text, imaging. However, most models in medical field are unimodal<br>single tasks and lack good generalizability and explainability. In this study,<br>we introduce MedViLaM, a unified vision-language model towards a generalist<br>model for medical data that can flexibly encode and interpret various forms of<br>medical data, including clinical language and imaging, all using the same set<br>of model weights. To facilitate the creation of such multi-task model, we have<br>curated MultiMedBench, a comprehensive pretaining dataset and benchmark<br>consisting of several distinct tasks, i.e., continuous question-answering,<br>multi-label disease classification, disease localization, generation and<br>summarization of radiology reports. MedViLaM demonstrates strong performance<br>across all MultiMedBench tasks, frequently outpacing other generalist models by<br>a significant margin. Additionally, we present instances of zero-shot<br>generalization to new medical concepts and tasks, effective transfer learning<br>across different tasks, and the emergence of zero-shot medical reasoning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MedViLaMï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŠ©åŠ›åŒ»ç–—æ•°æ®ç†è§£å’Œç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åŒ»å­¦é¢†åŸŸçš„æ•°æ®å…·æœ‰å¤šæ¨¡æ€å’Œå¤šä»»åŠ¡çš„ç‰¹ç‚¹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å½±åƒç­‰å¤šç§æ•°æ®ç±»å‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŒ»å­¦æ¨¡å‹å¤§å¤šä¸ºå•æ¨¡æ€ã€å•ä»»åŠ¡æ¨¡å‹ï¼Œç¼ºä¹è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚éš¾ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„ç–¾ç—…ç±»åˆ«å’Œæœªå®šä¹‰çš„æŒ‡ä»¤ï¼ˆä»»åŠ¡ï¼‰ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMedViLaMï¼Œä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æˆä¸ºåŒ»ç–—æ•°æ®çš„é€šç”¨æ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°ç¼–ç å’Œè§£é‡Šå„ç§å½¢å¼çš„åŒ»ç–—æ•°æ®ï¼ŒåŒ…æ‹¬ä¸´åºŠè¯­è¨€å’Œå½±åƒï¼Œæ‰€æœ‰è¿™äº›æ“ä½œéƒ½ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹æƒé‡é›†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸ºäº†ä¿ƒè¿›å¤šä»»åŠ¡æ¨¡å‹çš„åˆ›å»ºï¼Œç ”ç©¶äººå‘˜æ„å»ºäº†MultiMedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„é¢„è®­ç»ƒæ•°æ®é›†å’ŒåŸºå‡†ï¼ŒåŒ…æ‹¬å¤šä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œä¾‹å¦‚è¿ç»­é—®ç­”ã€å¤šæ ‡ç­¾ç–¾ç—…åˆ†ç±»ã€ç–¾ç—…å®šä½ã€æ”¾å°„å­¦æŠ¥å‘Šçš„ç”Ÿæˆå’Œæ‘˜è¦ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šMedViLaMåœ¨æ‰€æœ‰MultiMedBenchä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œç»å¸¸ä»¥æ˜¾è‘—çš„ä¼˜åŠ¿è¶…è¶Šå…¶ä»–é€šç”¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å±•ç¤ºäº†æ¨¡å‹å¯¹æ–°åŒ»ç–—æ¦‚å¿µå’Œä»»åŠ¡çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€ä¸åŒä»»åŠ¡ä¹‹é—´çš„æœ‰æ•ˆè¿ç§»å­¦ä¹ ï¼Œä»¥åŠé›¶æ ·æœ¬åŒ»ç–—æ¨ç†çš„å‡ºç°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒMedViLaMåœ¨å„ç§åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¿™è¡¨æ˜å…¶åœ¨æœªæ¥ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MedViLaMæ¨¡å‹åœ¨åŒ»ç–—æ•°æ®ç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ½œåŠ›ï¼Œå…¶å¤šæ¨¡æ€ã€å¤šä»»åŠ¡çš„ç‰¹ç‚¹ä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚çš„åŒ»ç–—åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¹Ÿæœ‰åŠ©äºæé«˜ä¸´åºŠåŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»åº¦ã€‚å› æ­¤ï¼ŒMedViLaMæ¨¡å‹ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>46</th>
      <td>Generative Visual Instruction Tuning</td>
      <td>We propose to use automatically generated instruction-following data to<br>improve the zero-shot capabilities of a large multimodal model with additional<br>support for generative and image editing tasks. We achieve this by curating a<br>new multimodal instruction-following set using GPT-4V and existing datasets for<br>image generation and editing. Using this instruction set and the existing<br>LLaVA-Finetune instruction set for visual understanding tasks, we produce<br>GenLLaVA, a Generative Large Language and Visual Assistant. GenLLaVA is built<br>through a strategy that combines three types of large pretrained models through<br>instruction finetuning: Mistral for language modeling, SigLIP for image-text<br>matching, and StableDiffusion for text-to-image generation. Our model<br>demonstrates visual understanding capabilities superior to LLaVA and<br>additionally demonstrates competitive results with native multimodal models<br>such as Unified-IO 2, paving the way for building advanced general-purpose<br>visual assistants by effectively re-using existing multimodal models. We<br>open-source our dataset, codebase, and model checkpoints to foster further<br>research and application in this domain.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GenLLaVAï¼šå¤šæ¨¡æ€æ¨¡å‹åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„æ–°çªç ´<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å…´èµ·ï¼Œå¦‚ä½•è®©è¿™äº›æ¨¡å‹åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸ŠåŒæ—¶è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¯¾é¢˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹å¾€å¾€åœ¨æ·»åŠ æ–°çš„åŠŸèƒ½ï¼ˆå¦‚å›¾åƒç”Ÿæˆï¼‰åï¼Œä¼šæŸå¤±åŸæœ‰çš„è§†è§‰å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿å¾—å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¿æŒåŸæœ‰èƒ½åŠ›çš„åŒæ—¶ï¼Œè¿˜èƒ½è¿›è¡Œå›¾åƒç”Ÿæˆå’Œç¼–è¾‘ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†<br>æœ¬æ–‡é€šè¿‡ä½¿ç”¨GPT-4Vå’Œç°æœ‰çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ•°æ®é›†ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†å›¾åƒç†è§£ã€å›¾åƒç”Ÿæˆå’Œå›¾åƒç¼–è¾‘çš„æ•°æ®ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šGenLLaVAæ¨¡å‹<br>æœ¬æ–‡æå‡ºäº†GenLLaVAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå°†Mistralè¯­è¨€æ¨¡å‹ã€SigLIPå›¾åƒ-æ–‡æœ¬åŒ¹é…æ¨¡å‹å’ŒStableDiffusionæ–‡æœ¬-å›¾åƒç”Ÿæˆæ¨¡å‹ç»“åˆèµ·æ¥ï¼Œå®ç°äº†å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„åŠŸèƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå•é˜¶æ®µè®­ç»ƒ<br>ä¸LLaVAæ¨¡å‹çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ä¸åŒï¼ŒGenLLaVAæ¨¡å‹é‡‡ç”¨å•é˜¶æ®µè®­ç»ƒï¼Œç›´æ¥å¾®è°ƒè§†è§‰-è¯­è¨€æŠ•å½±å™¨ã€è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç”Ÿæˆå¤´ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGenLLaVAæ¨¡å‹åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¼˜äºLLaVAæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨è®¸å¤šä»»åŠ¡ä¸Šä¸Unified-IO 2ç­‰åŸç”Ÿå¤šæ¨¡æ€æ¨¡å‹ç›¸å½“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GenLLaVAæ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•ï¼Œä¸ºæ„å»ºé«˜çº§é€šç”¨è§†è§‰åŠ©æ‰‹æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼€æºäº†æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä¸ºå¤šæ¨¡æ€AIé¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å®è´µçš„èµ„æºã€‚</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        