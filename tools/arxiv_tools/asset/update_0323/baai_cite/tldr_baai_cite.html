
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>85</th>
      <td>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</td>
      <td>With ChatGPT-like large language models (LLM) prevailing in the community,<br>how to evaluate the ability of LLMs is an open question. Existing evaluation<br>methods suffer from following shortcomings: (1) constrained evaluation<br>abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that<br>task-based evaluation, where LLM agents complete tasks in a simulated<br>environment, is a one-for-all solution to solve above problems. We present<br>AgentSims, an easy-to-use infrastructure for researchers from all disciplines<br>to test the specific capacities they are interested in. Researchers can build<br>their evaluation tasks by adding agents and buildings on an interactive GUI or<br>deploy and test new support mechanisms, i.e. memory, planning and tool-use<br>systems, by a few lines of codes. Our demo is available at<br>https://agentsims.com .</td>
      <td>## 🌟 论文解读 | AgentSims：大型语言模型评估的开放源代码沙盒<br><br>## 📌 背景痛点/本文动机<br>随着ChatGPT等大型语言模型（LLM）在社区中的普及，如何评估LLM的能力成为一个开放性问题。现有的评估方法存在以下不足：<br>1. 评估能力受限：大多数任务采用单轮问答格式，无法全面评估LLM的各种能力。<br>2. 基准易受攻击：由于LLM具有大量的预训练知识，测试集容易无意中混入训练集。<br>3. 指标不客观：现有的开放式问答指标涉及自动指标和主观指标，无法客观评估LLM的能力。<br><br>## 🚀 核心方法<br>本文提出了基于任务的评估方法，即LLM代理在模拟环境中完成任务来证明其能力。为了解决现有评估方法的不足，本文提出了AgentSims，一个易于使用的评估LLM能力的平台。AgentSims具有以下特点：<br>1. 可扩展性和可组合性：允许用户组合不同的计划、记忆和使用工具系统，研究各种系统设计的影响和有效性。<br>2. 交互式用户界面：为地图设计和代理创建提供交互式UI，降低非专业人士的入门门槛。<br>3. 标准化实现：确保实验结果的再现性。<br><br>## 📈 实验结果<br>本文展示了AgentSims在评估LLM能力方面的应用，包括：<br>1. 评估LLM的社会能力，如心智理论（ToM）。<br>2. 评估LLM的长期规划和组织能力，如担任市长或公司总裁。<br>3. 作为数据生成平台，用于数据标注和增强。<br>4. 为社会科学研究提供可控的初步实验环境。<br><br>## 💬 可借鉴之处<br>AgentSims为LLM评估提供了一个开放源代码的沙盒平台，具有以下可借鉴之处：<br>1. 基于任务的评估方法，可以更全面地评估LLM的能力。<br>2. 交互式用户界面，降低非专业人士的入门门槛。<br>3. 标准化实现，确保实验结果的再现性。<br>4. 可扩展性和可组合性，方便用户研究和开发新的支持系统。</td>
    </tr>
    <tr>
      <th>89</th>
      <td>SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning</td>
      <td>Large language models (LLMs) have demonstrated impressive results in<br>developing generalist planning agents for diverse tasks. However, grounding<br>these plans in expansive, multi-floor, and multi-room environments presents a<br>significant challenge for robotics. We introduce SayPlan, a scalable approach<br>to LLM-based, large-scale task planning for robotics using 3D scene graph<br>(3DSG) representations. To ensure the scalability of our approach, we: (1)<br>exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic<br>search' for task-relevant subgraphs from a smaller, collapsed representation of<br>the full graph; (2) reduce the planning horizon for the LLM by integrating a<br>classical path planner and (3) introduce an 'iterative replanning' pipeline<br>that refines the initial plan using feedback from a scene graph simulator,<br>correcting infeasible actions and avoiding planning failures. We evaluate our<br>approach on two large-scale environments spanning up to 3 floors and 36 rooms<br>with 140 assets and objects and show that our approach is capable of grounding<br>large-scale, long-horizon task plans from abstract, and natural language<br>instruction for a mobile manipulator robot to execute. We provide real robot<br>video demonstrations on our project page https://sayplan.github.io.</td>
      <td>## 🌟 论文解读 | SayPlan：利用3D场景图实现大规模机器人任务规划<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在任务规划领域的快速发展，它们在处理各种任务时展现出惊人的能力。然而，将这些计划在大型、多楼层和多房间的环境中落地，对于机器人来说仍然是一个巨大的挑战。现有的方法往往局限于小规模环境，且难以扩展到更复杂的场景中。<br><br>## 🚀 核心方法<br>SayPlan 提出了一个可扩展的方法，利用 3D 场景图（3DSG）表示来支持基于 LLM 的大规模任务规划。为了确保方法的可扩展性，SayPlan 采用了以下三个关键创新：<br><br>💡 创新点1：语义搜索<br>SayPlan 利用 3DSG 的层次结构，允许 LLM 通过语义搜索来识别任务相关的子图。通过将完整的 3DSG 压缩成一个更小的表示，LLM 可以专注于一个相对较小的、信息丰富的子图，从而避免超出其 token 限制。<br><br>💡 创新点2：迭代重规划<br>为了减少 LLM 的规划范围，SayPlan 集成了一个经典的路径规划器，负责连接 LLM 生成的节点。此外，SayPlan 引入了一个迭代重规划流程，使用场景图模拟器的反馈来验证和细化初始计划，从而纠正不可执行的动作并避免规划失败。<br><br>## 📈 实验结果<br>SayPlan 在两个大型环境中进行了评估，包括一个拥有 3 层楼和 36 个房间的环境，以及一个拥有 140 个资产和对象的环境。结果表明，SayPlan 能够从抽象的自然语言指令中生成可执行的、大规模的、长时任务计划，并能够在真实机器人上进行执行。<br><br>## 💬 可借鉴之处<br>SayPlan 为大规模机器人任务规划提供了一种可扩展且高效的方法。其语义搜索和迭代重规划机制可以有效地减少 LLM 的 token 消耗，并确保生成的计划符合环境约束。此外，SayPlan 的框架可以轻松地集成到现有的机器人系统中，为服务机器人领域的发展提供了新的思路。</td>
    </tr>
    <tr>
      <th>27</th>
      <td>PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models</td>
      <td>We introduce PokeLLMon, the first LLM-embodied agent that achieves<br>human-parity performance in tactical battle games, as demonstrated in Pokemon<br>battles. The design of PokeLLMon incorporates three key strategies: (i)<br>In-context reinforcement learning that instantly consumes text-based feedback<br>derived from battles to iteratively refine the policy; (ii) Knowledge-augmented<br>generation that retrieves external knowledge to counteract hallucination and<br>enables the agent to act timely and properly; (iii) Consistent action<br>generation to mitigate the panic switching phenomenon when the agent faces a<br>powerful opponent and wants to elude the battle. We show that online battles<br>against human demonstrates PokeLLMon's human-like battle strategies and<br>just-in-time decision making, achieving 49% of win rate in the Ladder<br>competitions and 56% of win rate in the invited battles. Our implementation and<br>playable battle logs are available at: https://github.com/git-disl/PokeLLMon.</td>
      <td>## 🌟 论文解读 | PokeLLMon：基于大型语言模型实现人类水平的宝可梦战斗AI<br><br>## 📌 背景痛点/本文动机<br>随着生成式AI和大型语言模型（LLMs）在自然语言处理（NLP）任务上的成功，人们开始探索LLMs如何自主地在物理世界中行动，将生成空间从文本扩展到行动，这被认为是追求通用人工智能（AGI）的关键范式。游戏是开发LLM-based代理与虚拟环境交互的合适测试平台。战术战斗游戏，如宝可梦战斗，因其状态和动作空间离散、回合制格式、战略性和复杂性，成为评估LLMs游戏能力的理想基准。<br><br>## 🚀 核心方法<br>💡 创新点1：上下文强化学习（ICRL）<br>为了解决LLMs在宝可梦战斗中出现的幻觉问题，论文提出了ICRL策略。ICRL利用战斗中即时生成的文本反馈作为“奖励”，在无需训练的情况下迭代优化动作生成策略。通过分析前一轮的行动和相应的文本反馈，代理能够不断调整其策略，从而更好地应对战斗中的变化。<br><br>💡 创新点2：知识增强生成（KAG）<br>为了进一步减少幻觉，论文引入了KAG策略。KAG通过检索外部知识，如类型优势/劣势关系和技能/能力效果，来增强生成过程。这些知识来源于宝可梦游戏中的宝可梦图鉴（Pokédex），它提供了关于宝可梦类型、技能和能力的详细信息。通过将外部知识添加到状态描述中，代理能够更准确地理解战斗情况，并做出更明智的决策。<br><br>💡 创新点3：一致性行动生成<br>为了解决代理在面对强大对手时出现的恐慌切换现象，论文提出了一致性行动生成策略。该策略通过多次独立生成行动并投票选出最一致的行动，来减少行动的不一致性。这种方法有助于代理在面对压力时保持冷静，避免过度思考和恐慌，从而做出更稳定的决策。<br><br>## 📈 实验结果<br>在线战斗结果表明，PokeLLMon在梯子比赛中取得了49%的胜率，在邀请比赛中取得了56%的胜率，展现出与人类玩家相当的比赛能力和策略。然而，PokeLLMon在面对人类玩家的消耗策略和欺骗技巧时也存在弱点，这表明未来需要进一步改进其长期规划和对手行为预测能力。<br><br>## 💬 可借鉴之处<br>PokeLLMon的设计和实现为LLMs在游戏领域的应用提供了新的思路。ICRL、KAG和一致性行动生成策略可以应用于其他游戏，帮助LLMs更好地理解和应对游戏中的挑战。此外，PokeLLMon的实验结果也揭示了LLMs在游戏中的优势和局限性，为未来研究和开发提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents</td>
      <td>Large Language Models (LLMs) have shown great success as high-level planners<br>for zero-shot game-playing agents. However, these agents are primarily<br>evaluated on Minecraft, where long-term planning is relatively straightforward.<br>In contrast, agents tested in dynamic robot environments face limitations due<br>to simplistic environments with only a few objects and interactions. To fill<br>this gap in the literature, we present NetPlay, the first LLM-powered zero-shot<br>agent for the challenging roguelike NetHack. NetHack is a particularly<br>challenging environment due to its diverse set of items and monsters, complex<br>interactions, and many ways to die.<br>  NetPlay uses an architecture designed for dynamic robot environments,<br>modified for NetHack. Like previous approaches, it prompts the LLM to choose<br>from predefined skills and tracks past interactions to enhance decision-making.<br>Given NetHack's unpredictable nature, NetPlay detects important game events to<br>interrupt running skills, enabling it to react to unforeseen circumstances.<br>While NetPlay demonstrates considerable flexibility and proficiency in<br>interacting with NetHack's mechanics, it struggles with ambiguous task<br>descriptions and a lack of explicit feedback. Our findings demonstrate that<br>NetPlay performs best with detailed context information, indicating the<br>necessity for dynamic methods in supplying context information for complex<br>games such as NetHack.</td>
      <td>## 🌟 论文解读 | LLMs 在 NetHack 中的潜力与局限性：零样本智能体<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在游戏领域展现出强大的规划能力，尤其是在 Minecraft 等游戏中。然而，这些模型在处理复杂、动态的环境时，如机器人环境，往往面临局限性。为了填补这一空白，本文提出了 NetPlay，一个基于 LLM 的零样本智能体，用于挑战性的 Rogue-like 游戏 NetHack。<br><br>## 🚀 核心方法<br>NetPlay 采用了一种专为动态机器人环境设计的架构，并针对 NetHack 进行了修改。它通过提示 LLM 从预定义的技能中选择，并跟踪过去的交互来增强决策。NetPlay 还能够检测重要的游戏事件，以便在出现意外情况时中断正在执行的技能。<br><br>## 📈 实验结果<br>实验结果表明，NetPlay 在与 NetHack 的机制交互方面表现出色，但在处理模糊的任务描述和缺乏明确反馈时存在困难。NetPlay 在提供详细上下文信息的情况下表现最佳，这表明在 NetHack 等复杂游戏中，动态提供上下文信息的方法至关重要。<br><br>## 💬 可借鉴之处<br>NetPlay 的研究结果表明，LLMs 在游戏领域具有巨大的潜力，但仍然存在局限性。未来研究可以探索如何更好地利用 LLMs 的能力，例如通过动态提供上下文信息或使用机器学习来替代手工制作的组件。此外，NetPlay 的架构可以为其他复杂游戏的设计提供参考。</td>
    </tr>
    <tr>
      <th>73</th>
      <td>Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4</td>
      <td>Unlike perfect information games, where all elements are known to every<br>player, imperfect information games emulate the real-world complexities of<br>decision-making under uncertain or incomplete information. GPT-4, the recent<br>breakthrough in large language models (LLMs) trained on massive passive data,<br>is notable for its knowledge retrieval and reasoning abilities. This paper<br>delves into the applicability of GPT-4's learned knowledge for imperfect<br>information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an<br>innovative agent that leverages GPT-4's capabilities for performing in<br>imperfect information games. With proper prompt engineering to achieve<br>different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable<br>adaptability across a range of imperfect information card games. Importantly,<br>GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it<br>can understand others and intentionally impact others' behavior. Leveraging<br>this, we design a planning strategy that enables GPT-4 to competently play<br>against different opponents, adapting its gameplay style as needed, while<br>requiring only the game rules and descriptions of observations as input. In the<br>experiments, we qualitatively showcase the capabilities of Suspicion-Agent<br>across three different imperfect information games and then quantitatively<br>evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can<br>potentially outperform traditional algorithms designed for imperfect<br>information games, without any specialized training or examples. In order to<br>encourage and foster deeper insights within the community, we make our<br>game-related data publicly available.</td>
      <td>## 🌟 论文解读 | 利用GPT-4的“心智理论”能力玩不完美信息游戏<br><br>## 📌 背景痛点/本文动机<br>在现实世界中，决策往往是在信息不完整或不确定的情况下进行的。然而，大多数现有的AI算法都是在完美信息游戏中训练的，即所有玩家都能看到所有信息。这限制了它们在现实世界中的应用。本文旨在探索如何利用大型语言模型（LLM）的知识和推理能力来处理不完美信息游戏，从而更好地模拟现实世界的决策过程。<br><br>## 🚀 核心方法<br>本文提出了一个名为Suspicion-Agent的创新型自主代理，它基于GPT-4，并利用其强大的知识检索和推理能力来玩不完美信息游戏。Suspicion-Agent的核心创新点包括：<br><br>💡 创新点1：利用GPT-4的“心智理论”（ToM）能力，即理解他人并有意影响他人行为的能力。这使得Suspicion-Agent能够预测对手的行为，并根据对手的行为调整自己的策略。<br><br>💡 创新点2：将游戏过程分解为多个子模块，如观察解释器、游戏模式分析和规划模块。每个模块都使用不同的提示来引导GPT-4执行特定的功能，从而实现更有效的决策。<br><br>## 📈 实验结果<br>在实验中，Suspicion-Agent在三个不同的不完美信息游戏中展示了其能力，并在Leduc Hold'em游戏中进行了定量评估。结果表明，Suspicion-Agent可以潜在地超越传统算法，而无需任何专门的训练或示例。<br><br>## 💬 可借鉴之处<br>本文提出的Suspicion-Agent框架为利用LLM在不完美信息游戏中进行决策提供了一个新的思路。其核心思想是将LLM的知识和推理能力与ToM能力相结合，从而实现更有效的决策。此外，本文还公开了所有与游戏相关的数据，这将有助于研究人员更好地理解LLM的能力，并开发更有效的模型。</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation</td>
      <td>Due to the dynamic and unpredictable open-world setting, navigating complex<br>environments in Minecraft poses significant challenges for multi-agent systems.<br>Agents must interact with the environment and coordinate their actions with<br>other agents to achieve common objectives. However, traditional approaches<br>often struggle to efficiently manage inter-agent communication and task<br>distribution, crucial for effective multi-agent navigation. Furthermore,<br>processing and integrating multi-modal information (such as visual, textual,<br>and auditory data) is essential for agents to comprehend their goals and<br>navigate the environment successfully and fully. To address this issue, we<br>design the HAS framework to auto-organize groups of LLM-based agents to<br>complete navigation tasks. In our approach, we devise a hierarchical<br>auto-organizing navigation system, which is characterized by 1) a hierarchical<br>system for multi-agent organization, ensuring centralized planning and<br>decentralized execution; 2) an auto-organizing and intra-communication<br>mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal<br>information platform, facilitating multi-modal perception to perform the three<br>navigation tasks with one system. To assess organizational behavior, we design<br>a series of navigation tasks in the Minecraft environment, which includes<br>searching and exploring. We aim to develop embodied organizations that push the<br>boundaries of embodied AI, moving it towards a more human-like organizational<br>structure.</td>
      <td>## 🌟 论文解读 | HAS：开放世界多智能体导航的分层自组织系统<br><br>## 📌 背景痛点/本文动机<br>在开放世界的环境中，如Minecraft，多智能体系统面临着复杂的导航挑战。传统的导航方法往往难以有效地管理智能体之间的通信和任务分配，这对于有效的多智能体导航至关重要。此外，处理和整合多模态信息（如视觉、文本和听觉数据）对于智能体理解其目标并在环境中成功导航至关重要。<br><br>## 🚀 核心方法<br>💡 创新点1：分层自组织导航系统<br>HAS框架设计了一个分层自组织导航系统，该系统具有以下特点：<br>1. 分层系统：确保集中式规划和分布式执行，提高导航效率。<br>2. 自组织机制：根据子任务动态调整关键角色和行动组，并保持组间通信，确保高效协作。<br>3. 多模态信息平台：促进多模态感知，使系统能够处理图像、对象和音频目标，并执行搜索和探索等导航任务。<br><br>💡 创新点2：多模态语言模型<br>HAS框架使用了多模态语言模型（MLM），包括管理者和执行者两种类型的模型。这些模型具有不同的功能，如规划、描述、评估和部署，以实现集中式规划和分布式执行。<br><br>💡 创新点3：多模态记忆<br>HAS框架还引入了多模态记忆机制，用于存储和检索多模态信息，从而提高规划准确性和一致性。通过检索增强生成（RAG）和多模态检索（MMR）技术，HAS能够有效地利用历史交互反馈和经验，生成更准确的计划。<br><br>## 📈 实验结果<br>在Minecraft环境中进行的实验表明，HAS框架在多模态目标搜索、连续块搜索和地图探索等任务中取得了最先进的性能。与基线方法相比，HAS在导航效率、成功率和探索能力方面均有显著提升。<br><br>## 💬 可借鉴之处<br>HAS框架为开放世界多智能体导航提供了一种新的思路和方法。其分层自组织结构、多模态语言模型和多模态记忆机制等创新点，对于提高多智能体系统的自主性、效率和适应性具有重要意义。此外，HAS框架还可以应用于其他需要多智能体协作的场景，如机器人协同、虚拟现实等。</td>
    </tr>
    <tr>
      <th>84</th>
      <td>CALYPSO: LLMs as Dungeon Masters' Assistants</td>
      <td>The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to<br>perform multiple tasks simultaneously. The DM must digest information about the<br>game setting and monsters, synthesize scenes to present to other players, and<br>respond to the players' interactions with the scene. Doing all of these tasks<br>while maintaining consistency within the narrative and story world is no small<br>feat of human cognition, making the task tiring and unapproachable to new<br>players. Large language models (LLMs) like GPT-3 and ChatGPT have shown<br>remarkable abilities to generate coherent natural language text. In this paper,<br>we conduct a formative evaluation with DMs to establish the use cases of LLMs<br>in D&D and tabletop gaming generally. We introduce CALYPSO, a system of<br>LLM-powered interfaces that support DMs with information and inspiration<br>specific to their own scenario. CALYPSO distills game context into bite-sized<br>prose and helps brainstorm ideas without distracting the DM from the game. When<br>given access to CALYPSO, DMs reported that it generated high-fidelity text<br>suitable for direct presentation to players, and low-fidelity ideas that the DM<br>could develop further while maintaining their creative agency. We see CALYPSO<br>as exemplifying a paradigm of AI-augmented tools that provide synchronous<br>creative assistance within established game worlds, and tabletop gaming more<br>broadly.</td>
      <td>## 🌟 论文解读 | CALYPSO：大型语言模型助力地下城主<br><br>## 📌 背景痛点/本文动机<br>地下城与龙（D&D）是一款经典的桌面角色扮演游戏，其中地下城主（DM）扮演着至关重要的角色。DM需要同时处理多项任务，包括消化游戏背景和怪物信息、构建场景、回应玩家互动等。这些任务对人类认知能力要求极高，对于新玩家来说尤其具有挑战性。大型语言模型（LLM）如GPT-3和ChatGPT在生成连贯的自然语言文本方面表现出色。本文旨在探索LLM在D&D和桌面游戏中的应用，并提出了CALYPSO系统，该系统利用LLM为DM提供信息和支持，帮助他们更好地进行游戏。<br><br>## 🚀 核心方法<br>💡 创新点1：CALYPSO系统<br>CALYPSO是一个由LLM驱动的界面系统，旨在支持DM在游戏中获取信息和灵感。它包括三个主要功能：<br>1. **遭遇理解**：使用GPT-3对游戏背景和怪物信息进行摘要，帮助DM快速理解遭遇。<br>2. **聚焦头脑风暴**：使用ChatGPT与DM进行对话，帮助他们进一步探索遭遇细节或生成新的想法。<br>3. **开放域聊天基线**：使用ChatGPT提供一个开放域的聊天界面，供玩家和DM进行非游戏相关的交流。<br><br>💡 创新点2：LLM的创造性辅助<br>CALYPSO系统展示了LLM作为创造性辅助工具的潜力。它不仅能够生成高保真度的文本，适合直接呈现给玩家，还能够提供低保真度的想法，供DM进一步发展和完善。这种辅助方式保留了DM的创造性自主权，使他们能够更好地专注于游戏中的认知任务。<br><br>## 📈 实验结果<br>研究结果表明，DM在使用CALYPSO系统后，普遍认为它能够生成适合直接呈现给玩家的文本，并提供有价值的灵感。DM们利用CALYPSO系统来理解复杂的怪物信息、头脑风暴非玩家角色或怪物之间的互动，并获取建议，将这些建议融入到故事中呈现给玩家，而不会影响游戏的节奏。<br><br>## 💬 可借鉴之处<br>本文的研究结果对于开发AI辅助工具在桌面游戏和其他创意领域中的应用具有重要的启示意义。CALYPSO系统的设计理念和方法可以为其他类似项目提供参考，例如：<br>1. **理解用户需求**：通过访谈和用户研究，深入了解用户的需求和痛点，从而设计出更符合用户需求的AI辅助工具。<br>2. **利用LLM的创造性潜力**：LLM在生成连贯文本和提供创造性灵感方面具有巨大潜力，可以将其应用于各种创意场景。<br>3. **保持用户的创造性自主权**：AI辅助工具应该作为用户的助手，而不是替代者，帮助用户更好地发挥自己的创造力。<br><br>总而言之，CALYPSO系统展示了LLM在桌面游戏中的应用潜力，并为开发AI辅助工具提供了有价值的经验和启示。</td>
    </tr>
    <tr>
      <th>38</th>
      <td>GlitchBench: Can large multimodal models detect video game glitches?</td>
      <td>Large multimodal models (LMMs) have evolved from large language models (LLMs)<br>to integrate multiple input modalities, such as visual inputs. This integration<br>augments the capacity of LLMs for tasks requiring visual comprehension and<br>reasoning. However, the extent and limitations of their enhanced abilities are<br>not fully understood, especially when it comes to real-world tasks. To address<br>this gap, we introduce GlitchBench, a novel benchmark derived from video game<br>quality assurance tasks, to test and evaluate the reasoning capabilities of<br>LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios<br>from video games and aims to challenge both the visual and linguistic reasoning<br>powers of LMMs in detecting and interpreting out-of-the-ordinary events. We<br>evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents<br>a new challenge for these models. Code and data are available at:<br>https://glitchbench.github.io/</td>
      <td>## 🌟 论文解读 | GlitchBench：大型多模态模型能否检测视频游戏中的错误？<br><br>## 📌 背景痛点/本文动机<br>随着大型多模态模型（LMMs）的不断发展，它们在视觉理解和推理方面的能力得到了显著提升。然而，这些模型在实际应用中的表现和局限性尚不明确。为了填补这一空白，本文提出了GlitchBench，一个基于视频游戏质量保证任务的基准测试，旨在评估LMMs在检测和解释异常事件方面的推理能力。<br><br>## 🚀 核心方法<br>💡 创新点1：GlitchBench数据集<br>GlitchBench数据集由593个游戏中的异常和错误场景组成，涵盖了205款不同类型的游戏。每个场景都包含一个视频片段、一个代表性帧、一个简短的描述以及一个指向Reddit上相关讨论的链接。此外，数据集还包括330个无错误的图像作为对比。<br><br>💡 创新点2：评估方法<br>本文评估了11个最先进的LMMs，包括GPT-4V和LLaVA，在GlitchBench上的表现。评估方法包括三个问题：<br>1. 这张图片有什么不寻常的地方？<br>2. 这张图片有什么问题？<br>3. 详细描述这张图片。<br>通过比较模型生成的文本与真实标签，评估模型在检测和解释异常事件方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，LMMs在检测违反简单物理定律的错误（如汽车在空中飞行）方面表现较好，但在检测更微妙的错误（如人体部位处于不可能的姿势）方面表现较差。GPT-4V在GlitchBench上表现最佳，准确率达到43.4%。然而，与无错误图像相比，模型在检测错误图像方面的准确率明显较低，这表明错误图像更具挑战性。<br><br>## 💬 可借鉴之处<br>本文提出的GlitchBench基准测试为评估LMMs在实际应用中的推理能力提供了一个有价值的工具。此外，本文的研究结果表明，LMMs在检测和解释异常事件方面仍存在局限性，需要进一步改进。</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game</td>
      <td>Multi-agent collaboration with Large Language Models (LLMs) demonstrates<br>proficiency in basic tasks, yet its efficiency in more complex scenarios<br>remains unexplored. In gaming environments, these agents often face situations<br>without established coordination protocols, requiring them to make intelligent<br>inferences about teammates from limited data. This problem motivates the area<br>of ad hoc teamwork, in which an agent may potentially cooperate with a variety<br>of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork<br>problem where the agent operates in an environment driven by natural language.<br>Our findings reveal the potential of LLM agents in team collaboration,<br>highlighting issues related to hallucinations in communication. To address this<br>issue, we develop CodeAct, a general agent that equips LLM with enhanced memory<br>and code-driven reasoning, enabling the repurposing of partial information for<br>rapid adaptation to new teammates.</td>
      <td>## 🌟 论文解读 | 探索大型语言模型在动态环境中的协作能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在推理和泛化能力上的不断提升，它们在构建自主代理和推动人工智能领域的发展方面展现出巨大潜力。然而，在多智能体协作中，特别是在没有预先设定的协调协议的动态环境中，LLMs的协作效率仍然是一个未充分探索的领域。本文旨在研究LLMs在动态环境中的协作能力，特别是在没有明确团队策略的情况下，如何与不同的队友进行有效合作。<br><br>## 🚀 核心方法<br>💡 创新点1：引入AvalonPlay基准<br>本文提出了AvalonPlay基准，这是一个基于自然语言的多智能体平台，用于模拟动态环境中的协作任务。在这个基准中，智能体需要在有限的信息和没有预先设定的团队策略的情况下，通过观察队友的行为来推断他们的角色，并动态调整团队策略以实现共同目标。<br><br>💡 创新点2：开发CodeAct智能体<br>为了解决LLMs在动态环境中协作时可能出现的记忆遗忘和幻觉生成等问题，本文提出了CodeAct智能体。CodeAct利用LLMs的代码驱动推理能力，通过将复杂的语义任务转化为灵活的代码结构，从而提高智能体在动态环境中的协作效率。<br><br>## 📈 实验结果<br>实验结果表明，GPT-4模型在AvalonPlay基准中表现出最佳的协作能力，而CodeAct智能体在团队选择准确性方面优于其他语义推理方法。此外，实验还发现，引入自然语言通信协议并不总是能显著提高LLMs的协作效率。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，LLMs在动态环境中的协作能力仍然面临一些挑战，如记忆遗忘和幻觉生成。为了提高LLMs的协作效率，可以借鉴本文提出的CodeAct智能体的设计思路，利用代码驱动推理和记忆检索系统来增强智能体的推理能力和信息处理能力。此外，还可以进一步研究如何减少幻觉生成的影响，并探索LLMs在现实世界场景中的应用。</td>
    </tr>
    <tr>
      <th>65</th>
      <td>Lyfe Agents: Generative agents for low-cost real-time social interactions</td>
      <td>Highly autonomous generative agents powered by large language models promise<br>to simulate intricate social behaviors in virtual societies. However, achieving<br>real-time interactions with humans at a low computational cost remains<br>challenging. Here, we introduce Lyfe Agents. They combine low-cost with<br>real-time responsiveness, all while remaining intelligent and goal-oriented.<br>Key innovations include: (1) an option-action framework, reducing the cost of<br>high-level decisions; (2) asynchronous self-monitoring for better<br>self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing<br>critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation<br>and sociability across several multi-agent scenarios in our custom LyfeGame 3D<br>virtual environment platform. When equipped with our brain-inspired techniques,<br>Lyfe Agents can exhibit human-like self-motivated social reasoning. For<br>example, the agents can solve a crime (a murder mystery) through autonomous<br>collaboration and information exchange. Meanwhile, our techniques enabled Lyfe<br>Agents to operate at a computational cost 10-100 times lower than existing<br>alternatives. Our findings underscore the transformative potential of<br>autonomous generative agents to enrich human social experiences in virtual<br>worlds.</td>
      <td>## 🌟 论文解读 | 低成本实时社交互动的生成式智能体：Lyfe Agents<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在模拟人类行为方面的潜力日益显现，生成式智能体在虚拟社会中模拟复杂社交行为的前景也变得光明。然而，实现与人类在低计算成本下的实时互动仍然是一个挑战。本文旨在创建一种既智能又自主的生成式智能体，能够在低计算成本下实现与人类的实时互动。<br><br>## 🚀 核心方法<br>💡 创新点1：选项-动作框架<br>为了减少高级决策的成本，Lyfe Agents 采用了一种选项-动作框架。在这种框架中，智能体首先选择一个高级动作（或“选项”），然后在后续步骤中在该选项内选择低级动作。这种设计允许智能体在更长时间内专注于执行选项背后的意图，从而降低成本并提高效率。<br><br>💡 创新点2：异步自我监控<br>为了提高智能体的情境意识和目标坚持性，Lyfe Agents 引入了一个自我监控模块。该模块维护一个关于最近事件的叙事风格摘要，并强调新颖和与目标相关的内容。这种自我监控摘要帮助智能体更好地理解情境，并使其行为更加一致和符合目标。<br><br>💡 创新点3：总结-遗忘记忆机制<br>为了提高记忆存储和检索的质量，Lyfe Agents 采用了一种层次化的记忆架构和总结-遗忘（SaF）方法。这种方法将记忆分为短期记忆和长期记忆，并通过聚类和总结技术将短期记忆中的信息转移到长期记忆中。此外，遗忘算法会评估并删除与现有记忆高度相似的旧记忆，以确保存储的信息是独特和相关的。<br><br>## 📈 实验结果<br>在自定义的 LyfeGame 3D 虚拟环境平台上，Lyfe Agents 在多个多智能体场景中展示了其自我激励和社会性。例如，智能体能够通过自主协作和信息交换解决犯罪（谋杀谜案）。此外，与现有方法相比，Lyfe Agents 的计算成本降低了 10-100 倍。<br><br>## 💬 可借鉴之处<br>Lyfe Agents 的设计原则和架构组件为构建低成本、实时响应的生成式智能体提供了有价值的参考。其选项-动作框架、异步自我监控和总结-遗忘记忆机制等创新点可以应用于其他生成式智能体框架，以提高其自主性和社交推理能力。此外，LyfeGame 虚拟环境平台也为研究生成式智能体的社会行为和用户交互提供了有价值的工具。</td>
    </tr>
    <tr>
      <th>77</th>
      <td>Agents: An Open-source Framework for Autonomous Language Agents</td>
      <td>Recent advances on large language models (LLMs) enable researchers and<br>developers to build autonomous language agents that can automatically solve<br>various tasks and interact with environments, humans, and other agents using<br>natural language interfaces. We consider language agents as a promising<br>direction towards artificial general intelligence and release Agents, an<br>open-source library with the goal of opening up these advances to a wider<br>non-specialist audience. Agents is carefully engineered to support important<br>features including planning, memory, tool usage, multi-agent communication, and<br>fine-grained symbolic control. Agents is user-friendly as it enables<br>non-specialists to build, customize, test, tune, and deploy state-of-the-art<br>autonomous language agents without much coding. The library is also<br>research-friendly as its modularized design makes it easily extensible for<br>researchers. Agents is available at https://github.com/aiwaves-cn/agents.</td>
      <td>## 🌟 论文解读 | 开源框架 Agents：构建自主语言代理的利器<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）的进步使得研究人员和开发者能够构建自主语言代理，这些代理能够自动解决各种任务，并通过自然语言界面与环境、人类和其他代理进行交互。然而，现有的语言代理框架往往缺乏易用性和可扩展性，难以满足非专业人士的需求。为了解决这个问题，本文提出了 Agents，一个开源的自主语言代理框架，旨在让更广泛的非专业人士能够轻松构建、定制、测试、调整和部署最先进的自主语言代理。<br><br>## 🚀 核心方法<br>💡 创新点1：支持关键功能<br>Agents 框架精心设计，支持规划、记忆、工具使用、多代理通信和细粒度符号控制等关键功能。这使得语言代理能够更好地适应各种任务和环境。<br><br>💡 创新点2：易用性和可扩展性<br>Agents 框架的用户友好性体现在其允许非专业人士轻松构建、定制、测试、调整和部署自主语言代理，而无需大量编码。同时，其模块化设计使得研究人员可以轻松扩展框架，以满足他们的研究需求。<br><br>💡 创新点3：Agent Hub 平台<br>Agents 框架引入了 Agent Hub 平台，允许用户分享他们微调的语言代理，并搜索/下载其他用户分享的有用语言代理。这大大降低了从头开始设计和调整语言代理的难度。<br><br>💡 创新点4：自动创建代理系统<br>为了减少用户手动指定 SOP 的繁琐工作，Agents 框架实现了一个自动 SOP 生成流程。该流程基于检索增强生成 (RAG)，可以自动创建其他代理和多代理系统。<br><br>## 📈 实验结果<br>论文展示了使用 Agents 框架构建的单代理系统和多代理系统的案例研究，包括闲聊机器人、基于知识库和搜索引擎的客户服务代理、购物助手代理和销售代理等。这些案例研究展示了 Agents 框架的易用性和可扩展性，以及构建各种用例的语言代理的可能性。<br><br>## 💬 可借鉴之处<br>Agents 框架为构建自主语言代理提供了一个强大的工具，其易用性和可扩展性使其成为研究人员和开发者的理想选择。此外，Agent Hub 平台和自动 SOP 生成流程进一步提高了框架的实用性和效率。</td>
    </tr>
    <tr>
      <th>60</th>
      <td>MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents</td>
      <td>Significant advancements have occurred in the application of Large Language<br>Models (LLMs) for various tasks and social simulations. Despite this, their<br>capacities to coordinate within task-oriented social contexts are<br>under-explored. Such capabilities are crucial if LLMs are to effectively mimic<br>human-like social behavior and produce meaningful results. To bridge this gap,<br>we introduce collaborative generative agents, endowing LLM-based Agents with<br>consistent behavior patterns and task-solving abilities. We situate these<br>agents in a simulated job fair environment as a case study to scrutinize their<br>coordination skills. We propose a novel framework that equips collaborative<br>generative agents with human-like reasoning abilities and specialized skills.<br>Our evaluation demonstrates that these agents show promising performance.<br>However, we also uncover limitations that hinder their effectiveness in more<br>complex coordination tasks. Our work provides valuable insights into the role<br>and evolution of LLMs in task-oriented social simulations.</td>
      <td>## 🌟 论文解读 | MetaAgents：基于LLM的任务导向协调的协作生成式智能体<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们在模拟人类行为和执行任务方面的能力日益增强。然而，LLMs在任务导向的社会环境中的协调能力尚未得到充分探索。为了使LLMs能够有效地模拟人类的社会行为并产生有意义的结果，这种能力至关重要。<br><br>## 🚀 核心方法<br>本文提出了协作生成式智能体（Collaborative Generative Agents），为基于LLM的智能体赋予了一致的行为模式和任务解决能力。为了研究这些智能体的协调能力，本文构建了一个模拟的招聘会环境，并提出了一个包含感知、记忆、推理和执行模块的框架。该框架使协作生成式智能体具备类似人类的推理能力和专业技能。<br><br>## 📈 实验结果<br>在模拟的招聘会环境中，协作生成式智能体在识别合格求职者、设计工作流程和分配角色方面表现出良好的性能。然而，随着招聘会复杂性的增加，智能体在协调方面遇到了挑战，这主要归因于LLMs的目标或意图不匹配。<br><br>## 💬 可借鉴之处<br>本文提出的协作生成式智能体框架为LLMs在任务导向的社会模拟中的应用提供了有价值的见解。该框架可以应用于各种场景，例如招聘、团队协作和社交网络模拟。此外，本文还揭示了LLMs在协调任务中面临的挑战，为未来的研究提供了方向。</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</td>
      <td>Code generation problems differ from common natural language problems - they<br>require matching the exact syntax of the target language, identifying happy<br>paths and edge cases, paying attention to numerous small details in the problem<br>spec, and addressing other code-specific issues and requirements. Hence, many<br>of the optimizations and tricks that have been successful in natural language<br>generation may not be effective for code tasks. In this work, we propose a new<br>approach to code generation by LLMs, which we call AlphaCodium - a test-based,<br>multi-stage, code-oriented iterative flow, that improves the performances of<br>LLMs on code problems. We tested AlphaCodium on a challenging code generation<br>dataset called CodeContests, which includes competitive programming problems<br>from platforms such as Codeforces. The proposed flow consistently and<br>significantly improves results. On the validation set, for example, GPT-4<br>accuracy (pass@5) increased from 19% with a single well-designed direct prompt<br>to 44% with the AlphaCodium flow. Many of the principles and best practices<br>acquired in this work, we believe, are broadly applicable to general code<br>generation tasks. Full implementation is available at:<br>https://github.com/Codium-ai/AlphaCodium</td>
      <td>## 🌟 论文解读 | AlphaCodium：从提示工程到流程工程，提升代码生成性能<br><br>## 📌 背景痛点/本文动机<br>代码生成问题与常见的自然语言问题不同，它需要匹配目标语言的精确语法，识别正常路径和边缘情况，关注问题规范中的许多小细节，并解决其他代码特定的问题和要求。因此，许多在自然语言生成中成功的优化和技巧可能对代码任务无效。本文提出了一种新的代码生成方法，称为AlphaCodium，它是一种基于测试的多阶段、代码导向的迭代流程，旨在提高大型语言模型（LLMs）在代码问题上的性能。<br><br>## 🚀 核心方法<br>💡 创新点1：测试导向的迭代流程<br>AlphaCodium的核心是迭代流程，其中生成的代码会反复运行并针对输入输出测试进行修复。这种方法允许模型逐步改进其解决方案，直到找到正确的答案。<br><br>💡 创新点2：多阶段处理<br>AlphaCodium流程分为两个主要阶段：预处理阶段和代码迭代阶段。在预处理阶段，模型会对问题进行自然语言推理，例如生成问题反思和公共测试推理。在代码迭代阶段，模型会生成代码解决方案，并在公共和AI生成的测试上进行迭代和修复。<br><br>💡 创新点3：代码导向的设计概念<br>AlphaCodium还采用了多种代码导向的设计概念，例如YAML结构化输出、通过项目符号分析进行语义推理、生成模块化代码、软决策和双重验证、鼓励探索以及测试锚点。这些概念有助于提高代码生成的质量和效率。<br><br>## 📈 实验结果<br>在CodeContests数据集上进行的实验表明，AlphaCodium流程显著提高了LLMs在代码问题上的性能。例如，GPT-4在验证集上的准确率（pass@5）从使用单个精心设计的直接提示的19%提高到使用AlphaCodium流程的44%。<br><br>## 💬 可借鉴之处<br>AlphaCodium的许多原则和最佳实践可以广泛应用于一般的代码生成任务。例如，使用结构化输出、生成模块化代码、通过项目符号分析进行语义推理、软决策和双重验证、鼓励探索以及测试锚点等技术都可以帮助提高代码生成的质量和效率。<br><br>## 📚 总结<br>AlphaCodium是一种创新的代码生成方法，它通过测试导向的迭代流程和多阶段处理，显著提高了LLMs在代码问题上的性能。该方法还采用了多种代码导向的设计概念，进一步提高了代码生成的质量和效率。AlphaCodium的许多原则和最佳实践可以广泛应用于一般的代码生成任务，为代码生成领域的研究和应用提供了新的思路和方向。</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot</td>
      <td>As the field of AI continues to evolve, a significant dimension of this<br>progression is the development of Large Language Models and their potential to<br>enhance multi-agent artificial intelligence systems. This paper explores the<br>cooperative capabilities of Large Language Model-augmented Autonomous Agents<br>(LAAs) using the well-known Meltin Pot environments along with reference models<br>such as GPT4 and GPT3.5. Preliminary results suggest that while these agents<br>demonstrate a propensity for cooperation, they still struggle with effective<br>collaboration in given environments, emphasizing the need for more robust<br>architectures. The study's contributions include an abstraction layer to adapt<br>Melting Pot game scenarios for LLMs, the implementation of a reusable<br>architecture for LLM-mediated agent development - which includes short and<br>long-term memories and different cognitive modules, and the evaluation of<br>cooperation capabilities using a set of metrics tied to the Melting Pot's<br>"Commons Harvest" game. The paper closes, by discussing the limitations of the<br>current architectural framework and the potential of a new set of modules that<br>fosters better cooperation among LAAs.</td>
      <td>## 🌟 论文解读 | 大型语言模型增强的自主智能体能否合作？<br><br>## 📌 背景痛点/本文动机<br>随着人工智能领域的不断发展，大型语言模型（LLMs）在多智能体人工智能系统中的应用潜力日益凸显。然而，当前的研究对于LLM增强的自主智能体（LAAs）的合作能力探讨相对较少。本文旨在评估LAAs在合作方面的能力，并探讨如何提升其合作效果。<br><br>## 🚀 核心方法<br>💡 创新点1：将Melting Pot场景转换为文本表示，以便LLMs可以轻松操作。<br>💡 创新点2：实现了一个可重用的LAAs开发架构，该架构包括短期和长期记忆以及不同的认知模块，如感知、规划、反思和行动。<br>💡 创新点3：通过自然语言定义“个性”，使智能体明确是否应该合作。<br>💡 创新点4：使用Melting Pot的“公共资源收获”游戏来评估LLM中介智能体的合作能力。<br><br>## 📈 实验结果<br>实验结果表明，尽管智能体表现出合作的倾向，但它们在特定环境中仍然难以有效协作。这突显了需要更强大的架构来促进LAAs之间的合作。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，为了提高LAAs的合作能力，需要更全面的架构，包括增强的理解能力、有效的沟通机制、可信的承诺机制以及明确的社交结构或制度。此外，本文提出的改进架构，包括理解模块、沟通模块、宪法模块和声誉系统，为未来研究提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</td>
      <td>Many reinforcement learning environments (e.g., Minecraft) provide only<br>sparse rewards that indicate task completion or failure with binary values. The<br>challenge in exploration efficiency in such environments makes it difficult for<br>reinforcement-learning-based agents to learn complex tasks. To address this,<br>this paper introduces an advanced learning system, named Auto MC-Reward, that<br>leverages Large Language Models (LLMs) to automatically design dense reward<br>functions, thereby enhancing the learning efficiency. Auto MC-Reward consists<br>of three important components: Reward Designer, Reward Critic, and Trajectory<br>Analyzer. Given the environment information and task descriptions, the Reward<br>Designer first design the reward function by coding an executable Python<br>function with predefined observation inputs. Then, our Reward Critic will be<br>responsible for verifying the code, checking whether the code is<br>self-consistent and free of syntax and semantic errors. Further, the Trajectory<br>Analyzer summarizes possible failure causes and provides refinement suggestions<br>according to collected trajectories. In the next round, Reward Designer will<br>further refine and iterate the dense reward function based on feedback.<br>Experiments demonstrate a significant improvement in the success rate and<br>learning efficiency of our agents in complex tasks in Minecraft, such as<br>obtaining diamond with the efficient ability to avoid lava, and efficiently<br>explore trees and animals that are sparse in the plains biome.</td>
      <td>## 🌟 论文解读 | Auto MC-Reward：利用大型语言模型自动设计密集奖励函数，提升Minecraft中强化学习的效率<br><br>## 📌 背景痛点/本文动机<br>Minecraft 等强化学习环境通常只提供稀疏奖励，即只有任务完成或失败时才会获得奖励。这种奖励机制使得强化学习代理在探索效率方面面临挑战，难以学习复杂任务。为了解决这个问题，本文提出了 Auto MC-Reward，一个利用大型语言模型 (LLM) 自动设计密集奖励函数的先进学习系统，从而提高学习效率。<br><br>## 🚀 核心方法<br>💡 创新点1：Auto MC-Reward 由三个关键组件组成：奖励设计器、奖励评论家和轨迹分析器。奖励设计器根据环境信息和任务描述，通过编写可执行的 Python 函数来设计奖励函数。奖励评论家负责验证代码，检查代码是否自洽且没有语法和语义错误。轨迹分析器根据收集的轨迹总结可能的失败原因，并提供改进建议。<br><br>💡 创新点2：Auto MC-Reward 利用 LLM 的任务理解和经验总结能力，为学习提供详细和即时的奖励指导。奖励设计器首先根据环境和任务的基本描述，使用 LLM 设计与任务相关的密集奖励函数。然后，奖励评论家对设计的奖励函数进行自我验证。为了解决 LLM 理解的潜在偏差或疏忽，还提出了基于 LLM 的轨迹分析器，用于分析和总结训练代理的轨迹，并帮助奖励设计器改进奖励函数。<br><br>## 📈 实验结果<br>Auto MC-Reward 在一系列代表性基准测试中进行了验证，包括地下水平探索钻石和探索平原生物群落中的树木和动物。实验结果表明，与原始稀疏奖励和现有密集奖励方法相比，Auto MC-Reward 在这些任务上取得了显著更好的结果，显示出其在稀疏奖励任务上高效学习的先进能力。通过迭代改进奖励函数的设计，Auto MC-Reward 使代理能够有效地学习对新任务有益的新行为，例如避免熔岩，从而大大提高了成功率。此外，Auto MC-Reward 仅使用原始信息就实现了高钻石获取成功率（36.5%），证明了其解决长期任务的能力。<br><br>## 💬 可借鉴之处<br>Auto MC-Reward 为解决稀疏奖励任务中的探索效率问题提供了一种新的思路。其利用 LLM 自动设计密集奖励函数的方法，可以有效地提高强化学习代理的学习效率。此外，Auto MC-Reward 的三个组件（奖励设计器、奖励评论家和轨迹分析器）可以独立运行，使得数据分析和奖励函数更新更加灵活。</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Large Multimodal Agents: A Survey</td>
      <td>Large language models (LLMs) have achieved superior performance in powering<br>text-based AI agents, endowing them with decision-making and reasoning<br>abilities akin to humans. Concurrently, there is an emerging research trend<br>focused on extending these LLM-powered AI agents into the multimodal domain.<br>This extension enables AI agents to interpret and respond to diverse multimodal<br>user queries, thereby handling more intricate and nuanced tasks. In this paper,<br>we conduct a systematic review of LLM-driven multimodal agents, which we refer<br>to as large multimodal agents ( LMAs for short). First, we introduce the<br>essential components involved in developing LMAs and categorize the current<br>body of research into four distinct types. Subsequently, we review the<br>collaborative frameworks integrating multiple LMAs , enhancing collective<br>efficacy. One of the critical challenges in this field is the diverse<br>evaluation methods used across existing studies, hindering effective comparison<br>among different LMAs . Therefore, we compile these evaluation methodologies and<br>establish a comprehensive framework to bridge the gaps. This framework aims to<br>standardize evaluations, facilitating more meaningful comparisons. Concluding<br>our review, we highlight the extensive applications of LMAs and propose<br>possible future research directions. Our discussion aims to provide valuable<br>insights and guidelines for future research in this rapidly evolving field. An<br>up-to-date resource list is available at<br>https://github.com/jun0wanan/awesome-large-multimodal-agents.</td>
      <td>## 🌟 论文解读 | 大型多模态智能体：迈向通用人工智能的桥梁<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在文本处理任务中展现出卓越的性能，研究人员开始探索将这些模型应用于多模态领域，以构建能够理解和响应多模态用户查询的智能体。然而，现有的研究往往孤立地进行，缺乏对现有框架的总结和比较。本文旨在填补这一空白，对LLM驱动的多模态智能体（LMAs）进行系统性的综述。<br><br>## 🚀 核心方法<br>本文首先介绍了LMAs的核心组件，包括感知、规划、行动和记忆，并提出了一个新的分类框架，将现有研究分为四类：<br><br>* **类型I：使用闭源LLMs作为规划器，无长期记忆**。这类LMAs主要使用提示技术来引导闭源LLMs进行决策和规划，完成图像编辑、视觉定位和视觉问答等任务。<br>* **类型II：使用微调的LLMs作为规划器，无长期记忆**。这类LMAs通过收集多模态指令遵循数据或使用自指令来微调开源LLMs，使其具备决策、规划和工具调用的能力。<br>* **类型III：具有间接长期记忆的规划器**。这类LMAs的LLMs作为中央规划器，并配备长期记忆。规划器通过调用相关工具来访问和检索长期记忆，以增强推理和规划能力。<br>* **类型IV：具有原生长期记忆的规划器**。这类LMAs的LLMs直接与长期记忆交互，无需工具来访问长期记忆。例如，在Minecraft等开放世界环境中，这类LMAs能够完成超过200个不同的任务。<br><br>此外，本文还回顾了多模态智能体的协作框架，并探讨了评估LMAs性能的现有方法，包括主观评估和客观评估。为了解决现有研究中评估方法多样性的问题，本文建立了一个综合的评估框架，旨在标准化评估过程，促进更有意义的比较。<br><br>## 📈 实验结果<br>本文没有提供具体的实验结果，而是对现有研究进行了综述和分析。<br><br>## 💬 可借鉴之处<br>* **LMAs的框架设计**：本文提出的分类框架可以帮助研究人员更好地理解LMAs的不同类型和特点，并为构建新的LMAs提供参考。<br>* **LMAs的评估方法**：本文提出的综合评估框架可以为LMAs的性能评估提供标准和指导，促进LMAs的进一步发展。<br>* **LMAs的应用场景**：本文列举了LMAs在GUI自动化、机器人与具身AI、游戏开发、自动驾驶、视频理解、视觉生成与编辑、复杂视觉推理任务、音频编辑与生成等领域的应用，为LMAs的未来发展提供了方向。<br><br>## 🌈 未来展望<br>本文展望了LMAs的未来发展方向，包括：<br><br>* **框架设计**：从单个智能体和多个智能体两个角度出发，构建更加统一和协作的LMAs框架。<br>* **评估方法**：建立系统化和标准化的评估框架，并开发更贴近真实场景的评估数据集。<br>* **应用场景**：探索LMAs在更多领域的应用，例如人机交互、医疗健康、教育等。<br><br>总而言之，本文为LMAs的研究和应用提供了全面的概述和深入的见解，为推动LMAs的发展和应用奠定了基础。</td>
    </tr>
    <tr>
      <th>111</th>
      <td>PaLM-E: An Embodied Multimodal Language Model</td>
      <td>Large language models excel at a wide range of complex tasks. However,<br>enabling general inference in the real world, e.g., for robotics problems,<br>raises the challenge of grounding. We propose embodied language models to<br>directly incorporate real-world continuous sensor modalities into language<br>models and thereby establish the link between words and percepts. Input to our<br>embodied language model are multi-modal sentences that interleave visual,<br>continuous state estimation, and textual input encodings. We train these<br>encodings end-to-end, in conjunction with a pre-trained large language model,<br>for multiple embodied tasks including sequential robotic manipulation planning,<br>visual question answering, and captioning. Our evaluations show that PaLM-E, a<br>single large embodied multimodal model, can address a variety of embodied<br>reasoning tasks, from a variety of observation modalities, on multiple<br>embodiments, and further, exhibits positive transfer: the model benefits from<br>diverse joint training across internet-scale language, vision, and<br>visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,<br>in addition to being trained on robotics tasks, is a visual-language generalist<br>with state-of-the-art performance on OK-VQA, and retains generalist language<br>capabilities with increasing scale.</td>
      <td>## 🌟 论文解读 | PaLM-E：将真实世界传感器数据融入语言模型，实现更接地气的推理<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在各种任务中表现出强大的推理能力，但在现实世界的推理中，例如机器人问题，存在一个关键挑战：接地（grounding）。虽然LLMs在大量文本数据上训练，可能产生与物理世界相关的表示，但将这些表示与现实世界的视觉和物理传感器模态连接起来对于解决更广泛的现实世界问题至关重要。<br><br>## 🚀 核心方法<br>本文提出了具身语言模型（embodied language models），将真实世界连续传感器模态的数据直接融入语言模型，从而建立词语与感知之间的联系。具身语言模型的输入是多模态句子，这些句子交替包含视觉、连续状态估计和文本输入编码。我们端到端地训练这些编码，与预训练的大型语言模型相结合，用于多个具身任务，包括顺序机器人操作规划、视觉问答和字幕生成。<br><br>## 📈 实验结果<br>实验结果表明，PaLM-E，一个单一的、大型具身多模态模型，可以解决各种具身推理任务，包括来自多种观察模态的多种具身形式。此外，该模型表现出积极的迁移：模型受益于跨互联网规模的语言、视觉和视觉语言领域的多样化联合训练。我们的最大模型PaLM-E-562B，除了在机器人任务上进行训练外，还是一个视觉语言通才，在OK-VQA上取得了最先进的性能，并且随着规模的增加，保留了通用的语言能力。<br><br>## 💬 可借鉴之处<br>PaLM-E的研究表明，通过将具身数据混合到多模态大型语言模型的训练中，可以训练出一个通用的、迁移学习的、多具身决策代理。此外，PaLM-E还展示了在视觉问答和字幕生成等通用视觉语言任务上的竞争力，并且随着语言模型规模的增加，多模态微调时的灾难性遗忘显著减少。</td>
    </tr>
    <tr>
      <th>44</th>
      <td>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</td>
      <td>Can we avoid wars at the crossroads of history? This question has been<br>pursued by individuals, scholars, policymakers, and organizations throughout<br>human history. In this research, we attempt to answer the question based on the<br>recent advances of Artificial Intelligence (AI) and Large Language Models<br>(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to<br>simulate the participating countries, their decisions, and the consequences, in<br>historical international conflicts, including the World War I (WWI), the World<br>War II (WWII), and the Warring States Period (WSP) in Ancient China. By<br>evaluating the simulation effectiveness, we examine the advancements and<br>limitations of cutting-edge AI systems' abilities in studying complex<br>collective human behaviors such as international conflicts under diverse<br>settings. In these simulations, the emergent interactions among agents also<br>offer a novel perspective for examining the triggers and conditions that lead<br>to war. Our findings offer data-driven and AI-augmented insights that can<br>redefine how we approach conflict resolution and peacekeeping strategies. The<br>implications stretch beyond historical analysis, offering a blueprint for using<br>AI to understand human history and possibly prevent future international<br>conflicts. Code and data are available at<br>\url{https://github.com/agiresearch/WarAgent}.</td>
      <td>## 🌟 论文解读 | 利用大型语言模型模拟历史战争，探索和平的可能性<br><br>## 📌 背景痛点/本文动机<br>战争与和平是人类历史永恒的主题，理解战争的原因和预防战争的发生一直是人类追求的目标。传统的战争研究方法主要依赖于历史分析和文献回顾，但这些方法往往受限于静态视角和事后诸葛亮的偏见。随着人工智能和大型语言模型（LLM）的快速发展，我们有机会利用这些先进技术来模拟历史事件，探索战争与和平的动态过程，并为冲突解决和和平维护提供新的视角。<br><br>## 🚀 核心方法<br>本文提出了 WarAgent，一个基于 LLM 的多智能体 AI 系统，用于模拟历史国际冲突，包括第一次世界大战（WWI）、第二次世界大战（WWII）和中国古代战国时期（WSP）。WarAgent 通过模拟参与国家的决策过程和互动，探索了以下关键问题：<br><br>* **模拟有效性**：LLM 基于多智能体系统能否有效地复制历史事件中战略规划和决策过程的演变？<br>* **战争起因**：哪些因素是导致战争爆发的关键因素？LLM 基于多智能体系统能否识别这些因素？<br>* **战争不可避免性**：历史上的战争是否真的不可避免？LLM 基于多智能体系统能否揭示导致战争（或和平）的条件？<br><br>## 📈 实验结果<br>实验结果表明，WarAgent 能够有效地模拟历史事件，并在一定程度上复制历史决策过程和互动。例如，在 WWI 模拟中，WarAgent 能够重现主要国家之间的联盟形成、战争宣言和动员情况，与历史事件具有较高的吻合度。此外，通过改变触发事件和国家的初始条件，WarAgent 能够探索不同的战争起因和战争不可避免性，为理解历史事件和预防未来冲突提供了新的视角。<br><br>## 💬 可借鉴之处<br>* **LLM 在历史模拟中的应用**：WarAgent 为 LLM 在历史模拟中的应用提供了新的思路，为理解复杂的人类行为和社会动态提供了新的工具。<br>* **多智能体系统在冲突解决中的应用**：WarAgent 的设计理念可以为冲突解决和和平维护提供新的思路，例如通过模拟不同政策的影响来评估冲突解决策略的有效性。<br>* **历史教学的新方法**：WarAgent 可以作为一种新的历史教学方法，帮助学生和教师探索“如果”场景，并理解历史事件的复杂因果关系。<br><br>## 🌟 未来展望<br>WarAgent 的研究为 LLM 在历史模拟中的应用开辟了新的道路，未来可以进一步探索以下方向：<br><br>* **时间驱动模拟**：将 WarAgent 的回合制模拟扩展为时间驱动模拟，以更准确地模拟历史事件的时间动态。<br>* **停止条件**：研究更有效的停止条件，以更清晰地结束模拟并分析结果。<br>* **新的研究问题**：探索更多与历史事件和冲突解决相关的研究问题，例如外交沟通与冲突可能性之间的关系、非国家行为体对地缘政治的影响等。<br><br>通过不断改进和扩展 WarAgent，我们可以更好地理解历史事件，并为构建更加和平的未来提供新的思路。</td>
    </tr>
    <tr>
      <th>107</th>
      <td>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions</td>
      <td>Large language models (LLMs) such as ChatGPT and GPT-4 have recently<br>demonstrated their remarkable abilities of communicating with human users. In<br>this technical report, we take an initiative to investigate their capacities of<br>playing text games, in which a player has to understand the environment and<br>respond to situations by having dialogues with the game world. Our experiments<br>show that ChatGPT performs competitively compared to all the existing systems<br>but still exhibits a low level of intelligence. Precisely, ChatGPT can not<br>construct the world model by playing the game or even reading the game manual;<br>it may fail to leverage the world knowledge that it already has; it cannot<br>infer the goal of each step as the game progresses. Our results open up new<br>research questions at the intersection of artificial intelligence, machine<br>learning, and natural language processing.</td>
      <td>## 🌟 论文解读 | 大型语言模型在文本游戏中的表现：现状与开放性问题<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）如ChatGPT和GPT-4在理解和响应人类语言查询方面展现出令人印象深刻的能力，研究界对其是否能够实现通用人工智能（AGI）的潜力产生了广泛讨论。本文旨在通过将LLMs置于文本游戏的环境中，评估其在理解环境、做出决策和与游戏世界进行交互方面的智能水平，从而为LLMs的能力和局限性提供新的见解。<br><br>## 🚀 核心方法<br>💡 创新点1：使用文本游戏作为评估LLMs智能水平的测试床。文本游戏要求玩家通过文本命令与游戏世界进行交互，从而提供了一个可控的环境来评估LLMs的智能水平。<br>💡 创新点2：通过分析LLMs在文本游戏中的表现，揭示了LLMs在构建世界模型、推断目标和导航能力方面的局限性。研究发现，尽管LLMs在文本游戏中的表现优于现有系统，但它们仍然缺乏构建世界模型、推断目标和进行有效导航的能力。<br><br>## 📈 实验结果<br>实验结果表明，ChatGPT在文本游戏中的表现优于现有系统，但仍然存在一些局限性。ChatGPT无法通过游戏或阅读游戏手册来构建世界模型，无法充分利用已有的世界知识，也无法推断游戏进行过程中每一步的目标。这些结果表明，LLMs在实现人类水平的智能方面仍然存在一些挑战。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，文本游戏可以作为评估LLMs智能水平的有效测试床。通过分析LLMs在文本游戏中的表现，可以揭示LLMs在构建世界模型、推断目标和导航能力方面的局限性，并为LLMs的未来发展提供新的方向。此外，本文的研究结果也为LLMs在游戏领域的应用提供了新的思路，例如开发基于LLMs的智能游戏助手或游戏角色。</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Q-Cogni: An Integrated Causal Reinforcement Learning Framework</td>
      <td>We present Q-Cogni, an algorithmically integrated causal reinforcement<br>learning framework that redesigns Q-Learning with an autonomous causal<br>structure discovery method to improve the learning process with causal<br>inference. Q-Cogni achieves optimal learning with a pre-learned structural<br>causal model of the environment that can be queried during the learning process<br>to infer cause-and-effect relationships embedded in a state-action space. We<br>leverage on the sample efficient techniques of reinforcement learning, enable<br>reasoning about a broader set of policies and bring higher degrees of<br>interpretability to decisions made by the reinforcement learning agent. We<br>apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against<br>state-of-the-art reinforcement learning algorithms. We report results that<br>demonstrate better policies, improved learning efficiency and superior<br>interpretability of the agent's decision making. We also compare this approach<br>with traditional shortest-path search algorithms and demonstrate the benefits<br>of our causal reinforcement learning framework to high dimensional problems.<br>Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New<br>York City using the Taxi & Limousine Commission trip record data and compare<br>with shortest-path search, reporting results that show 85% of the cases with an<br>equal or better policy derived from Q-Cogni in a real-world domain.</td>
      <td></td>
    </tr>
    <tr>
      <th>115</th>
      <td>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling</td>
      <td>Reinforcement learning (RL) agents typically learn tabula rasa, without prior<br>knowledge of the world. However, if initialized with knowledge of high-level<br>subgoals and transitions between subgoals, RL agents could utilize this<br>Abstract World Model (AWM) for planning and exploration. We propose using<br>few-shot large language models (LLMs) to hypothesize an AWM, that will be<br>verified through world experience, to improve sample efficiency of RL agents.<br>Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft<br>in two phases: (1) the Dream phase where the agent uses an LLM to decompose a<br>task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase<br>where the agent learns a modular policy for each subgoal and verifies or<br>corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and<br>then verifying the AWM based on agent experience not only increases sample<br>efficiency over contemporary methods by an order of magnitude but is also<br>robust to and corrects errors in the LLM, successfully blending noisy<br>internet-scale information from LLMs with knowledge grounded in environment<br>dynamics.</td>
      <td>## 🌟 论文解读 | 基于语言引导的世界建模的具身决策：让强化学习更高效<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）通常从零开始学习，没有关于世界的先验知识。然而，如果 RL 代理在初始化时具有关于高级子目标和子目标之间转换的知识，它们可以利用这种抽象世界模型（AWM）进行规划和探索。本文提出使用少量样本的大型语言模型（LLMs）来假设 AWM，并通过世界经验进行验证，以提高 RL 代理的样本效率。<br><br>## 🚀 核心方法<br>💡 创新点1：使用少量样本的 LLM 来假设 AWM<br>本文提出使用少量样本的 LLM 来假设 AWM，并通过世界经验进行验证，以提高 RL 代理的样本效率。这种方法可以有效地利用 LLM 中的大规模、嘈杂的先验知识，并将其与基于环境动态的知识相结合。<br><br>💡 创新点2：将 LLM 引导的探索应用于 Minecraft 中的物品制作<br>本文提出的 DECKARD 代理将 LLM 引导的探索应用于 Minecraft 中的物品制作。DECKARD 代理分为两个阶段：梦想阶段和清醒阶段。在梦想阶段，代理使用 LLM 将任务分解为一系列子目标，形成假设的 AWM。在清醒阶段，代理学习每个子目标的模块化策略，并验证或纠正假设的 AWM。<br><br>## 📈 实验结果<br>实验结果表明，与没有 LLM 引导的代理相比，DECKARD 代理在开放探索和目标驱动任务中都表现出更高的样本效率。在开放探索中，DECKARD 代理能够更快地发现新的 AWM 节点。在目标驱动任务中，DECKARD 代理能够更快地学习制作物品，并且对 LLM 输出中的错误具有鲁棒性。<br><br>## 💬 可借鉴之处<br>本文提出的基于语言引导的世界建模的具身决策方法为 RL 代理利用大规模、嘈杂的先验知识提供了新的思路。这种方法可以应用于各种 RL 任务，例如机器人控制、游戏玩法和自动驾驶等。此外，本文提出的模块化 RL 策略也可以用于提高 RL 代理的泛化能力。</td>
    </tr>
    <tr>
      <th>87</th>
      <td>EmoLLM: Multimodal Emotional Understanding Meets Large Language Models</td>
      <td>Multi-modal large language models (MLLMs) have achieved remarkable<br>performance on objective multimodal perception tasks, but their ability to<br>interpret subjective, emotionally nuanced multimodal content remains largely<br>unexplored. Thus, it impedes their ability to effectively understand and react<br>to the intricate emotions expressed by humans through multimodal media. To<br>bridge this gap, we introduce EmoBench, the first comprehensive benchmark<br>designed specifically to evaluate the emotional capabilities of MLLMs across<br>five popular emotional tasks, using a diverse dataset of 287k images and videos<br>paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a<br>novel model for multimodal emotional understanding, incorporating with two core<br>techniques. 1) Multi-perspective Visual Projection, it captures diverse<br>emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it<br>guides MLLMs to reason about emotions in the correct direction. Experimental<br>results demonstrate that EmoLLM significantly elevates multimodal emotional<br>understanding performance, with an average improvement of 12.1% across multiple<br>foundation models on EmoBench. Our work contributes to the advancement of MLLMs<br>by facilitating a deeper and more nuanced comprehension of intricate human<br>emotions, paving the way for the development of artificial emotional<br>intelligence capabilities with wide-ranging applications in areas such as<br>human-computer interaction, mental health support, and empathetic AI systems.<br>Code, data, and model will be released.</td>
      <td>## 🌟 论文解读 | EmoLLM：多模态情感理解与大型语言模型的结合<br><br>## 📌 背景痛点/本文动机<br>随着多模态大型语言模型（MLLMs）在目标多模态感知任务上取得了显著成果，但它们在解释主观、情感丰富的多模态内容方面的能力仍然没有得到充分探索。这阻碍了它们有效地理解和反应人类通过多模态媒体表达的情感。为了弥合这一差距，本文提出了EmoBench，这是第一个专门设计用于评估MLLMs在五个流行情感任务中的情感能力的全面基准，使用了一个包含287k图像和视频以及相应文本指令的多样化数据集。同时，本文提出了EmoLLM，这是一种用于多模态情感理解的新型模型，结合了两种核心技术。<br><br>## 🚀 核心方法<br>💡 创新点1：多视角视觉投影<br>它从多个视角捕获视觉数据中的多样化情感线索。<br><br>💡 创新点2：EmoPrompt<br>它引导MLLMs在正确的方向上推理情感。<br><br>## 📈 实验结果<br>实验结果表明，EmoLLM显著提高了多模态情感理解性能，在EmoBench上多个基础模型平均提高了12.1%。<br><br>## 💬 可借鉴之处<br>本文提出的EmoBench基准和EmoLLM模型为MLLMs在情感理解方面的研究提供了新的思路和方法，有助于推动MLLMs在情感智能领域的进一步发展。</td>
    </tr>
    <tr>
      <th>63</th>
      <td>Beyond Win Rates: A Clustering-Based Approach to Character Balance Analysis in Team-Based Games</td>
      <td>Character diversity in competitive games, while enriching gameplay, often<br>introduces balance challenges that can negatively impact player experience and<br>strategic depth. Traditional balance assessments rely on aggregate metrics like<br>win rates and pick rates, which offer limited insight into the intricate<br>dynamics of team-based games and nuanced character roles. This paper proposes a<br>novel clustering-based methodology to analyze character balance, leveraging<br>in-game data from Valorant to account for team composition influences and<br>reveal latent character roles. By applying hierarchical agglomerative<br>clustering with Jensen-Shannon Divergence to professional match data from the<br>Valorant Champions Tour 2022, our approach identifies distinct clusters of<br>agents exhibiting similar co-occurrence patterns within team compositions. This<br>method not only complements existing quantitative metrics but also provides a<br>more holistic and interpretable perspective on character synergies and<br>potential imbalances, offering game developers a valuable tool for informed and<br>context-aware balance adjustments.</td>
      <td>## 🌟 论文解读 | 基于聚类的团队游戏角色平衡分析方法<br><br>## 📌 背景痛点/本文动机<br>在团队竞技游戏中，角色多样性虽然丰富了游戏玩法，但也带来了平衡挑战。传统的平衡评估方法主要依赖于胜率和选择率等聚合指标，这些指标无法全面反映团队游戏的复杂动态和角色之间的微妙关系。因此，本文提出了一种基于聚类的角色平衡分析方法，旨在更全面地理解角色之间的协同作用和潜在的不平衡。<br><br>## 🚀 核心方法<br>💡 创新点1：聚类分析<br>本文使用层次凝聚聚类算法，结合Jensen-Shannon散度作为距离度量，对角色之间的关系进行分析。通过分析角色在团队中的共现模式，将角色分为具有相似功能的组，从而揭示潜在的角色类型和协同作用。<br><br>💡 创新点2：Jensen-Shannon散度<br>Jensen-Shannon散度是一种对称的度量，用于量化两个概率分布之间的差异。在本研究中，它被用来衡量角色之间的相似性，从而更准确地反映角色在团队中的功能角色。<br><br>## 📈 实验结果<br>通过对《Valorant》2022年冠军巡回赛的专业比赛数据进行聚类分析，本文发现了一些具有相似功能的角色组，例如控制者、哨兵、决斗者和发起者。这些结果与游戏中的角色分类相吻合，但也揭示了一些新的角色类型和协同作用。<br><br>## 💬 可借鉴之处<br>本文提出的基于聚类的角色平衡分析方法为游戏开发者提供了一种新的视角，可以帮助他们更全面地理解角色之间的关系，并做出更明智的平衡调整。此外，该方法还可以用于评估角色调整对游戏平衡的影响，从而帮助开发者更好地维护游戏的公平性和可玩性。</td>
    </tr>
    <tr>
      <th>17</th>
      <td>SOTOPIA-\( π \): Interactive Learning of Socially Intelligent Language Agents</td>
      <td>Humans learn social skills through both imitation and social interaction.<br>This social learning process is largely understudied by existing research on<br>building language agents. Motivated by this gap, we propose an interactive<br>learning method, SOTOPIA-\( \pi \), improving the social intelligence of language<br>agents. This method leverages behavior cloning and self-reinforcement training<br>on filtered social interaction data according to large language model (LLM)<br>ratings. We show that our training method allows a 7B LLM to reach the social<br>goal completion ability of an expert model (GPT-4-based agent), while improving<br>the safety of language agents and maintaining general QA ability on the MMLU<br>benchmark. We also find that this training paradigm uncovers some difficulties<br>in LLM-based evaluation of social intelligence: LLM-based evaluators<br>overestimate the abilities of the language agents trained specifically for<br>social interaction.</td>
      <td>## 🌟 论文解读 | SOTOPIA-\( π \): 提升语言模型社交智能的交互式学习方法<br><br>## 📌 背景痛点/本文动机<br>人类通过模仿和社会互动学习社交技能，但现有研究在构建语言模型时对此过程关注不足。本文提出了一种交互式学习方法 SOTOPIA-\( π \)，旨在提升语言模型的社交智能。<br><br>## 🚀 核心方法<br>💡 创新点1：利用行为克隆和自我强化训练<br>SOTOPIA-\( π \) 利用行为克隆和自我强化训练，在经过大型语言模型（LLM）评分过滤的社会互动数据上进行训练。行为克隆从具有强大社交技能的专家模型（如 GPT-4）的行为轨迹中学习，而自我强化则从模型自身的高评分行为中学习。<br><br>💡 创新点2：LLM 评分作为训练信号<br>SOTOPIA-\( π \) 使用 GPT-4 对社交互动中的积极行为进行评分，并将这些评分作为训练信号。这种方法无需人工参与，且具有高效和可扩展性。<br><br>## 📈 实验结果<br>实验结果表明，SOTOPIA-\( π \) 可以显著提升语言模型的社交目标完成能力，使其接近专家模型（如 GPT-4）的性能。此外，该方法还能提高语言模型的安全性，并保持其在 MMLU 基准测试中的问答能力。<br><br>## 💬 可借鉴之处<br>SOTOPIA-\( π \) 为提升语言模型的社交智能提供了一种有效的方法，并揭示了 LLM 评分在评估社交智能方面的局限性。未来研究可以探索在线强化学习、从人类数据中学习、更稳健的评估方法等方向，以进一步提升语言模型的社交智能。</td>
    </tr>
    <tr>
      <th>93</th>
      <td>ChessGPT: Bridging Policy Learning and Language Modeling</td>
      <td>When solving decision-making tasks, humans typically depend on information<br>from two key sources: (1) Historical policy data, which provides interaction<br>replay from the environment, and (2) Analytical insights in natural language<br>form, exposing the invaluable thought process or strategic considerations.<br>Despite this, the majority of preceding research focuses on only one source:<br>they either use historical replay exclusively to directly learn policy or value<br>functions, or engaged in language model training utilizing mere language<br>corpus. In this paper, we argue that a powerful autonomous agent should cover<br>both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning<br>and language modeling by integrating data from these two sources in Chess<br>games. Specifically, we build a large-scale game and language dataset related<br>to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and<br>ChessGPT, integrating policy learning and language modeling. Finally, we<br>propose a full evaluation framework for evaluating language model's chess<br>ability. Experimental results validate our model and dataset's effectiveness.<br>We open source our code, model, and dataset at<br>https://github.com/waterhorse1/ChessGPT.</td>
      <td>## 🌟 论文解读 | ChessGPT：策略学习与语言模型融合的桥梁<br><br>## 📌 背景痛点/本文动机<br>在解决决策任务时，人类通常依赖于两种关键信息来源：历史策略数据和自然语言形式的策略分析。然而，现有的研究大多只关注其中一种来源，要么是直接从历史回放中学习策略或价值函数，要么是利用语言语料库进行语言模型训练。本文认为，一个强大的自主代理应该同时利用这两种来源，因此提出了ChessGPT，一个通过整合国际象棋游戏中的数据来连接策略学习和语言模型的GPT模型。<br><br>## 🚀 核心方法<br>💡 创新点1：构建大规模游戏和语言数据集<br>本文构建了一个包含大量国际象棋游戏数据和自然语言数据的综合数据集，包括在线游戏回放、专业棋手比赛、计算机引擎游戏、棋盘游戏、棋盘游戏分析、棋盘游戏博客、棋盘游戏书籍、棋盘游戏论坛、棋盘游戏视频等。<br><br>💡 创新点2：提出ChessCLIP和ChessGPT模型<br>本文提出了两种模型，ChessCLIP和ChessGPT，利用上述数据集进行训练。ChessCLIP通过对比学习将策略和语言模态连接起来，而ChessGPT则通过因果语言模型进行策略学习。<br><br>💡 创新点3：提出全面的评估框架<br>本文设计了一个全面的评估框架，用于评估语言模型在国际象棋方面的能力，包括建模能力、价值判断能力和策略能力。<br><br>## 📈 实验结果<br>实验结果表明，ChessGPT模型在所有评估任务中都优于其他LLM基线模型，证明了模型和数据集的有效性。<br><br>## 💬 可借鉴之处<br>本文提出的ChessGPT模型和数据集为研究策略学习和语言模型之间的相互作用提供了新的思路和方法，并为开发更强大的自主代理提供了新的可能性。</td>
    </tr>
    <tr>
      <th>23</th>
      <td>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</td>
      <td>We present PCA-Bench, a multimodal decision-making benchmark for evaluating<br>the integrated capabilities of Multimodal Large Language Models (MLLMs).<br>Departing from previous benchmarks focusing on simplistic tasks and individual<br>model capability, PCA-Bench introduces three complex scenarios: autonomous<br>driving, domestic robotics, and open-world games. Given task instructions and<br>diverse contexts, the model is required to seamlessly integrate multiple<br>capabilities of Perception, Cognition, and Action in a reasoning chain to make<br>accurate decisions. Moreover, PCA-Bench features error localization<br>capabilities, scrutinizing model inaccuracies in areas such as perception,<br>knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To<br>balance accuracy and efficiency in evaluation, we propose PCA-Eval, an<br>automatic evaluation protocol, and assess 10 prevalent MLLMs. The results<br>reveal significant performance disparities between open-source models and<br>powerful proprietary models like GPT-4 Vision. To address this, we introduce<br>Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing<br>instruction tuning examples in multimodal embodied environments. EIE generates<br>7,510 training examples in PCA-Bench and enhances the performance of<br>open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision<br>accuracy), thereby validating the effectiveness of EIE. Our findings suggest<br>that robust MLLMs like GPT4-Vision show promise for decision-making in embodied<br>agents, opening new avenues for MLLM research.</td>
      <td>## 🌟 论文解读 | PCA-Bench：评估多模态大语言模型在感知-认知-行动链中的决策能力<br><br>## 📌 背景痛点/本文动机<br>随着多模态大语言模型（MLLMs）在处理复杂任务方面的能力日益增强，现有的评估基准往往只关注单个模型能力的评估，而忽略了模型在感知、认知和行动方面的综合能力。此外，现有的基准缺乏对模型错误进行定位的能力，这使得难以确定模型在哪些方面需要改进。<br><br>## 🚀 核心方法<br>💡 创新点1：PCA-Bench<br>本文提出了PCA-Bench，这是一个用于评估MLLMs在感知-认知-行动链中决策能力的多模态决策基准。PCA-Bench引入了三个复杂的场景：自动驾驶、家庭机器人和开放世界游戏。在这些场景中，模型需要根据任务指令和不同的上下文，无缝地整合感知、认知和行动的能力，以做出准确的决策。<br><br>💡 创新点2：PCA-Eval<br>为了平衡评估的准确性和效率，本文提出了PCA-Eval，这是一个自动评估协议。PCA-Eval利用LLMs强大的语义解析能力，根据数据注释中的锚点信息，自动进行错误定位。实验结果表明，PCA-Eval与人类评估结果具有高度的一致性，平均Kappa系数达到0.8+。<br><br>💡 创新点3：Embodied-Instruction-Evolution (EIE)<br>为了解决PCA-Bench数据集标注工作量大的问题，本文提出了Embodied-Instruction-Evolution (EIE)框架。EIE利用LLMs自动合成多模态具身环境中的指令调整示例，从而减少了人工劳动，并提高了PCA-Bench的多样性和可扩展性。<br><br>## 📈 实验结果<br>实验结果表明，GPT-4 Vision在感知、认知和行动方面都表现出色，超过了现有的开源MLLMs。EIE方法能够显著提高开源MLLMs的性能，在某些指标上甚至超过了GPT-4 Vision。PCA-Eval能够有效地定位模型错误，从而提高了模型评估的可靠性。<br><br>## 💬 可借鉴之处<br>本文提出的PCA-Bench和PCA-Eval为评估MLLMs的决策能力提供了一个新的基准和评估工具。EIE框架为自动合成多模态具身环境中的指令调整示例提供了一种有效的方法。本文的研究结果表明，强大的MLLMs在具身智能体中的决策能力具有很大的潜力，为MLLMs的研究开辟了新的方向。</td>
    </tr>
    <tr>
      <th>96</th>
      <td>The Text-Based Adventure AI Competition</td>
      <td>In 2016, 2017, and 2018 at the IEEE Conference on Computational Intelligence<br>in Games, the authors of this paper ran a competition for agents that can play<br>classic text-based adventure games. This competition fills a gap in existing<br>game AI competitions that have typically focussed on traditional card/board<br>games or modern video games with graphical interfaces. By providing a platform<br>for evaluating agents in text-based adventures, the competition provides a<br>novel benchmark for game AI with unique challenges for natural language<br>understanding and generation. This paper summarises the three competitions ran<br>in 2016, 2017, and 2018 (including details of open source implementations of<br>both the competition framework and our competitors) and presents the results of<br>an improved evaluation of these competitors across 20 games.</td>
      <td>## 🌟 论文解读 | 文本冒险游戏AI竞赛：探索自然语言理解和生成的挑战<br><br>## 📌 背景痛点/本文动机<br>随着图形化界面的游戏AI竞赛的兴起，文本冒险游戏AI竞赛填补了现有游戏AI竞赛的空白。文本冒险游戏为游戏AI提供了独特的挑战，特别是在自然语言理解和生成方面。本文介绍了2016年至2018年在IEEE计算智能与游戏会议上举办的文本冒险游戏AI竞赛，并总结了竞赛的成果和经验。<br><br>## 🚀 核心方法<br>💡 创新点1：竞赛框架<br>竞赛框架基于ZPlet Java解释器，用于评估软件代理在文本冒险游戏中的能力。代理通过一个接口与游戏交互，该接口接收游戏环境描述并返回代理想要执行的动作。<br><br>💡 创新点2：多种代理算法<br>竞赛中提交了多种代理算法，包括BYUAgent 2016、Golovin、CARL (BYUAgent 2017) 和 NAIL。这些代理算法使用了不同的方法来生成命令和与游戏交互，例如基于word2vec的动词-名词匹配、命令模式生成和知识图谱构建。<br><br>## 📈 实验结果<br>竞赛结果表明，NAIL代理在2018年竞赛中表现最佳，其次是CARL和Golovin。与2016年和2017年相比，所有代理的性能都有所提高。然而，即使是表现最好的代理，也只能完成测试游戏中的一小部分，并且在许多游戏中无法获得任何分数。<br><br>## 💬 可借鉴之处<br>本文提出的文本冒险游戏AI竞赛为游戏AI研究提供了一个新的基准，并强调了自然语言理解和生成在游戏AI中的重要性。竞赛结果表明，现有的代理算法在处理文本冒险游戏的复杂性和多样性方面仍然面临挑战。未来的研究可以探索更有效的自然语言处理方法，以及如何将文本冒险游戏AI应用于现实世界的自然语言处理任务。</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models</td>
      <td>While the situation has improved for text-only models, it again seems to be<br>the case currently that multimodal (text and image) models develop faster than<br>ways to evaluate them. In this paper, we bring a recently developed evaluation<br>paradigm from text models to multimodal models, namely evaluation through the<br>goal-oriented game (self) play, complementing reference-based and<br>preference-based evaluation. Specifically, we define games that challenge a<br>model's capability to represent a situation from visual information and align<br>such representations through dialogue. We find that the largest closed models<br>perform rather well on the games that we define, while even the best<br>open-weight models struggle with them. On further analysis, we find that the<br>exceptional deep captioning capabilities of the largest models drive some of<br>the performance. There is still room to grow for both kinds of models, ensuring<br>the continued relevance of the benchmark.</td>
      <td>## 🌟 论文解读 | 游戏化评估：探究大型多模态模型中的多模态和对话式接地<br><br>## 📌 背景痛点/本文动机<br>随着大型多模态模型（LMMs）的快速发展，现有的评估方法主要依赖于参考式评估，难以全面评估模型在复杂场景下的交互能力。本文旨在探索一种新的评估范式，即通过目标导向的游戏（自我）玩法来评估多模态模型，以补充现有的参考式和偏好式评估方法。<br><br>## 🚀 核心方法<br>💡 创新点1：将游戏化评估范式应用于多模态模型<br>本文借鉴了文本模型中新兴的游戏化评估方法，并将其应用于多模态模型。通过定义三种对话游戏（参考游戏、图像比较游戏和导航游戏），挑战模型从视觉信息中构建情境模型并通过对话进行对齐的能力。<br><br>💡 创新点2：构建多模态游戏框架<br>本文使用 clemgame/clembench 框架来实现游戏化评估。该框架通过自然语言提示模板来定义游戏目标，并通过程序化的游戏大师来控制游戏流程和评分规则。<br><br>## 📈 实验结果<br>实验结果表明，大型闭源模型在本文定义的游戏中表现良好，而即使是最好的开源模型也难以应对这些挑战。进一步分析发现，大型模型在深度图像描述方面的出色能力推动了部分性能提升。这表明，无论是闭源模型还是开源模型，都仍有很大的发展空间。<br><br>## 💬 可借鉴之处<br>本文提出的游戏化评估方法为多模态模型的评估提供了新的思路，可以帮助研究人员更全面地评估模型在复杂场景下的交互能力。此外，本文构建的多模态游戏框架也为其他研究人员提供了可复现的实验平台。<br><br>## 📚 参考文献<br>* Chalamalasetti, V., et al. (2023). clemgame: A framework for evaluating language models through goal-oriented games. arXiv preprint arXiv:2304.01213.<br>* Chiang, D., et al. (2024). Chatbot Arena: A preference-based evaluation platform for conversational agents. arXiv preprint arXiv:2403.04528.<br>* Qiao, Y., et al. (2023). Evaluating language models through interactive games. arXiv preprint arXiv:2303.08574.</td>
    </tr>
    <tr>
      <th>46</th>
      <td>DesignGPT: Multi-Agent Collaboration in Design</td>
      <td>Generative AI faces many challenges when entering the product design<br>workflow, such as interface usability and interaction patterns. Therefore,<br>based on design thinking and design process, we developed the DesignGPT<br>multi-agent collaboration framework, which uses artificial intelligence agents<br>to simulate the roles of different positions in the design company and allows<br>human designers to collaborate with them in natural language. Experimental<br>results show that compared with separate AI tools, DesignGPT improves the<br>performance of designers, highlighting the potential of applying multi-agent<br>systems that integrate design domain knowledge to product scheme design.</td>
      <td>## 🌟 论文解读 | DesignGPT：设计流程中的多智能体协作<br><br>## 📌 背景痛点/本文动机<br>随着人工智能（AI）的发展，生成式AI工具在产品设计中展现出巨大的潜力，如MidJourney和Stable Diffusion等图像生成工具，以及ChatGPT等文本生成工具。然而，现有的生成式AI工具在产品设计的实际应用中面临着界面可用性和交互模式等挑战，且设计思维与机器思维之间存在天然鸿沟。如何让AI更好地理解设计思维，是设计师与AI交互的一大挑战。<br><br>## 🚀 核心方法<br>本文提出了DesignGPT，一个基于多智能体协作的设计框架，旨在帮助设计师与AI智能体进行自然语言协作，完成产品方案设计。DesignGPT的核心组件包括需求导入表单、角色定义与选择以及会议室。系统初始化了多个模拟设计公司不同职位的员工角色，如虚拟用户、老板、产品经理、设计总监、CMF设计师、评分记录员和技术人员，每个角色都有明确的职责和任务。设计师用户可以输入设计需求，选择角色并开始会议，与AI智能体进行自然语言交流，共同完成需求分析、设计提案、详细设计和技术迭代等工作。<br><br>## 📈 实验结果<br>为了评估DesignGPT的有效性，本文进行了一项在线实验，将参与者随机分为两组，分别使用DesignGPT和单独的AI工具进行设计。结果显示，与单独使用AI工具相比，DesignGPT在设计的创新性、完整性和可行性方面均有显著提升。此外，参与者对DesignGPT的交互形式给予了高度评价，认为AI角色能够熟练运用设计流程和设计表达，并从多个角度推导设计方案，突破了以往设计方案构思中视角相对单一的问题。<br><br>## 💬 可借鉴之处<br>DesignGPT的研究为理解AI在设计流程中的作用提供了重要的理论和实践意义。其多智能体协作的设计框架为设计师与AI的协作提供了新的思路，有助于提高设计效率和设计质量。此外，DesignGPT的实验结果也表明，将设计领域知识与多智能体系统相结合，在产品方案设计中具有巨大的潜力。</td>
    </tr>
    <tr>
      <th>78</th>
      <td>Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf</td>
      <td>Communication games, which we refer to as incomplete information games that<br>heavily depend on natural language communication, hold significant research<br>value in fields such as economics, social science, and artificial intelligence.<br>In this work, we explore the problem of how to engage large language models<br>(LLMs) in communication games, and in response, propose a tuning-free<br>framework. Our approach keeps LLMs frozen, and relies on the retrieval and<br>reflection on past communications and experiences for improvement. An empirical<br>study on the representative and widely-studied communication game,<br>``Werewolf'', demonstrates that our framework can effectively play Werewolf<br>game without tuning the parameters of the LLMs. More importantly, strategic<br>behaviors begin to emerge in our experiments, suggesting that it will be a<br>fruitful journey to engage LLMs in communication games and associated domains.</td>
      <td>## 🌟 论文解读 | 探索大型语言模型在沟通游戏中的应用：以狼人杀为例的实证研究<br><br>## 📌 背景痛点/本文动机<br>沟通游戏，如狼人杀，是一种重要的研究工具，可以用来探索经济学、社会科学和人工智能等领域中的各种问题。然而，现有的AI代理在玩这类游戏时，要么对语言的使用有严格的限制，要么需要大量的人工标注数据，这使得AI代理在自然地玩这类游戏方面仍然面临挑战。<br><br>## 🚀 核心方法<br>本文提出了一种无需微调的大型语言模型（LLM）框架，用于玩沟通游戏，并以狼人杀为例进行了实证研究。该框架的核心方法包括：<br><br>💡 创新点1：历史信息收集<br>为了解决LLM的上下文长度限制问题，本文提出了一种从三个角度（新鲜度、信息量和完整性）收集历史信息的方法。具体来说，该方法包括：<br>- 收集最近的K条消息；<br>- 使用规则匹配和启发式指标选择最有信息量的N条消息；<br>- 通过回答问题的方式，从整个历史中提取更多信息。<br><br>💡 创新点2：经验学习<br>为了使LLM能够从经验中学习，本文提出了一种非参数学习机制。具体来说，该方法包括：<br>- 在每轮游戏结束后，收集所有玩家的响应、反思和得分，形成经验池；<br>- 在新的一轮游戏中，根据当前情况从经验池中检索最相关的经验，并从中提取建议，以指导LLM的推理。<br><br>## 📈 实验结果<br>实验结果表明，本文提出的框架能够有效地玩狼人杀游戏，并且能够从经验中学习，而无需微调LLM的参数。此外，实验中还观察到一些策略性行为，如信任、对抗、伪装和领导，这些行为并非预先编程，而是自发地从LLM中涌现出来的。<br><br>## 💬 可借鉴之处<br>本文提出的框架和方法为使用LLM玩沟通游戏提供了一种新的思路，并为进一步研究LLM在沟通游戏中的应用提供了有价值的参考。此外，本文提出的经验学习机制也可以应用于其他领域，例如对话系统和推荐系统。</td>
    </tr>
    <tr>
      <th>14</th>
      <td>MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control</td>
      <td>It is a long-lasting goal to design a generalist-embodied agent that can<br>follow diverse instructions in human-like ways. However, existing approaches<br>often fail to steadily follow instructions due to difficulties in understanding<br>abstract and sequential natural language instructions. To this end, we<br>introduce MineDreamer, an open-ended embodied agent built upon the challenging<br>Minecraft simulator with an innovative paradigm that enhances<br>instruction-following ability in low-level control signal generation.<br>Specifically, MineDreamer is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs) and diffusion models, and we employ a<br>Chain-of-Imagination (CoI) mechanism to envision the step-by-step process of<br>executing instructions and translating imaginations into more precise visual<br>prompts tailored to the current state; subsequently, the agent generates<br>keyboard-and-mouse actions to efficiently achieve these imaginations, steadily<br>following the instructions at each step. Extensive experiments demonstrate that<br>MineDreamer follows single and multi-step instructions steadily, significantly<br>outperforming the best generalist agent baseline and nearly doubling its<br>performance. Moreover, qualitative analysis of the agent's imaginative ability<br>reveals its generalization and comprehension of the open world.</td>
      <td>## 🌟 论文解读 | MineDreamer：基于想象链的模拟世界控制指令跟随<br><br>## 📌 背景痛点/本文动机<br>在人工智能领域，设计一个能够以人类方式理解和执行多样化指令的通用型具身智能体一直是长期目标。然而，现有的方法往往难以稳定地遵循指令，尤其是在理解和执行抽象和顺序的自然语言指令方面存在困难。<br><br>## 🚀 核心方法<br>💡 创新点1：引入“想象链”（Chain-of-Imagination, CoI）机制<br>MineDreamer 通过 CoI 机制，使智能体能够根据指令和当前状态逐步想象并执行指令。这种方法模拟了人类在解决问题时，根据当前状态逐步想象下一步目标的过程。<br><br>💡 创新点2：多模态大型语言模型（MLLM）增强的扩散模型<br>MineDreamer 使用 MLLM 增强的扩散模型来生成包含物理规则和环境理解的想象图像，这些图像作为更精确的视觉提示，引导智能体生成低级控制信号。<br><br>💡 创新点3：目标漂移收集方法<br>为了训练 Imaginator，MineDreamer 使用了目标漂移收集方法来收集大量具身数据，帮助 Imaginator 理解如何逐步完成指令以及如何重复完成指令。<br><br>## 📈 实验结果<br>MineDreamer 在执行单步和多步指令方面表现出色，显著优于最佳通用型智能体基线，性能几乎翻倍。此外，对智能体想象能力的定性分析表明，它能够理解和适应开放世界的环境。<br><br>## 💬 可借鉴之处<br>MineDreamer 的 CoI 机制为解决指令跟随问题提供了一种新颖的方法，其 MLLM 增强的扩散模型和目标漂移收集方法也为具身智能体的发展提供了新的思路。此外，MineDreamer 的成功也表明，具身智能体在开放世界环境中具有巨大的潜力。</td>
    </tr>
    <tr>
      <th>86</th>
      <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</td>
      <td>Remarkable progress has been made on automated problem solving through<br>societies of agents based on large language models (LLMs). Existing LLM-based<br>multi-agent systems can already solve simple dialogue tasks. Solutions to more<br>complex tasks, however, are complicated through logic inconsistencies due to<br>cascading hallucinations caused by naively chaining LLMs. Here we introduce<br>MetaGPT, an innovative meta-programming framework incorporating efficient human<br>workflows into LLM-based multi-agent collaborations. MetaGPT encodes<br>Standardized Operating Procedures (SOPs) into prompt sequences for more<br>streamlined workflows, thus allowing agents with human-like domain expertise to<br>verify intermediate results and reduce errors. MetaGPT utilizes an assembly<br>line paradigm to assign diverse roles to various agents, efficiently breaking<br>down complex tasks into subtasks involving many agents working together. On<br>collaborative software engineering benchmarks, MetaGPT generates more coherent<br>solutions than previous chat-based multi-agent systems. Our project can be<br>found at https://github.com/geekan/MetaGPT</td>
      <td>## 🌟 论文解读 | MetaGPT：基于大型语言模型的元编程多智能体协作框架<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，基于LLMs的自主智能体在自动化问题解决方面取得了显著进展。然而，现有的基于LLMs的多智能体系统在解决复杂任务时，由于简单地将LLMs串联起来导致的级联幻觉，常常出现逻辑不一致的问题。为了解决这个问题，本文提出了MetaGPT，一个创新的元编程框架，将高效的人类工作流程融入到基于LLMs的多智能体协作中。<br><br>## 🚀 核心方法<br>💡 创新点1：将标准化操作流程（SOPs）编码到提示序列中，以实现更流畅的工作流程。这使得具有人类水平的领域专业知识的智能体能够验证中间结果并减少错误。<br>💡 创新点2：采用流水线范式，为各种智能体分配不同的角色，有效地将复杂任务分解为涉及多个智能体协作的子任务。<br>💡 创新点3：引入可执行反馈机制，在运行时调试和执行代码，显著提高代码生成质量。<br><br>## 📈 实验结果<br>在协作软件工程基准测试中，MetaGPT生成的解决方案比之前的基于聊天的多智能体系统更连贯。在HumanEval和MBPP基准测试中，MetaGPT取得了最先进的性能，分别达到了85.9%和87.7%的Pass@1。在自生成的软件开发基准测试中，MetaGPT在几乎所有指标上都优于ChatDev，并且实现了100%的任务完成率，证明了其鲁棒性和效率。<br><br>## 💬 可借鉴之处<br>MetaGPT框架为基于LLMs的多智能体系统开发提供了一个灵活且功能强大的平台。其将SOPs融入设计中的创新方法，为解决复杂问题提供了新的思路。此外，可执行反馈机制也为提高代码生成质量提供了有效途径。MetaGPT的成功经验可以启发未来研究，探索更多基于人类实践的人工多智能体系统技术。</td>
    </tr>
    <tr>
      <th>106</th>
      <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
      <td>Believable proxies of human behavior can empower interactive applications<br>ranging from immersive environments to rehearsal spaces for interpersonal<br>communication to prototyping tools. In this paper, we introduce generative<br>agents--computational software agents that simulate believable human behavior.<br>Generative agents wake up, cook breakfast, and head to work; artists paint,<br>while authors write; they form opinions, notice each other, and initiate<br>conversations; they remember and reflect on days past as they plan the next<br>day. To enable generative agents, we describe an architecture that extends a<br>large language model to store a complete record of the agent's experiences<br>using natural language, synthesize those memories over time into higher-level<br>reflections, and retrieve them dynamically to plan behavior. We instantiate<br>generative agents to populate an interactive sandbox environment inspired by<br>The Sims, where end users can interact with a small town of twenty five agents<br>using natural language. In an evaluation, these generative agents produce<br>believable individual and emergent social behaviors: for example, starting with<br>only a single user-specified notion that one agent wants to throw a Valentine's<br>Day party, the agents autonomously spread invitations to the party over the<br>next two days, make new acquaintances, ask each other out on dates to the<br>party, and coordinate to show up for the party together at the right time. We<br>demonstrate through ablation that the components of our agent<br>architecture--observation, planning, and reflection--each contribute critically<br>to the believability of agent behavior. By fusing large language models with<br>computational, interactive agents, this work introduces architectural and<br>interaction patterns for enabling believable simulations of human behavior.</td>
      <td>## 🌟 论文解读 | 生成式智能体：模拟人类行为的交互式模拟<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，人们对于能够模拟人类行为的智能体产生了浓厚的兴趣。这些智能体可以应用于各种场景，例如沉浸式环境、人际沟通演练空间、原型设计工具等。然而，要创建一个能够长期保持一致性和可信度的智能体仍然是一个挑战。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：生成式智能体<br>本文提出了生成式智能体的概念，即利用生成模型模拟可信的人类行为。这些智能体能够进行日常活动，如起床、做饭、上班等，并能够形成自己的观点、与他人互动、发起对话等。<br><br>💡 创新点2：智能体架构<br>为了实现生成式智能体，本文提出了一种新的架构，该架构扩展了大型语言模型，使其能够存储智能体的经验记录，并将这些记忆随着时间的推移合成更高层次的反思，并动态地检索它们来规划行为。<br><br>## 📈 实验结果<br>本文通过在模拟环境中创建一个由25个智能体组成的小镇，展示了生成式智能体的潜力。实验结果表明，这些智能体能够产生可信的个体和群体行为，例如，在用户指定一个智能体想要举办情人节派对的情况下，智能体能够自主地邀请其他智能体参加派对，并协调在正确的时间一起到达派对地点。<br><br>## 💬 可借鉴之处<br>本文提出的生成式智能体架构为创建可信的人类行为模拟提供了新的思路。该架构可以应用于各种领域，例如角色扮演、社交原型设计、虚拟世界和游戏等。此外，本文还讨论了生成式智能体在交互式系统中的应用机会、伦理和社会风险。</td>
    </tr>
    <tr>
      <th>62</th>
      <td>AvalonBench: Evaluating LLMs Playing the Game of Avalon</td>
      <td>In this paper, we explore the potential of Large Language Models (LLMs)<br>Agents in playing the strategic social deduction game, Resistance Avalon.<br>Players in Avalon are challenged not only to make informed decisions based on<br>dynamically evolving game phases, but also to engage in discussions where they<br>must deceive, deduce, and negotiate with other players. These characteristics<br>make Avalon a compelling test-bed to study the decision-making and<br>language-processing capabilities of LLM Agents. To facilitate research in this<br>line, we introduce AvalonBench - a comprehensive game environment tailored for<br>evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game<br>environment for Avalon, (2) rule-based bots as baseline opponents, and (3)<br>ReAct-style LLM agents with tailored prompts for each role. Notably, our<br>evaluations based on AvalonBench highlight a clear capability gap. For<br>instance, models like ChatGPT playing good-role got a win rate of 22.2% against<br>rule-based bots playing evil, while good-role bot achieves 38.2% win rate in<br>the same setting. We envision AvalonBench could be a good test-bed for<br>developing more advanced LLMs (with self-playing) and agent frameworks that can<br>effectively model the layered complexities of such game environments.</td>
      <td>## 🌟 论文解读 | AvalonBench：评估大型语言模型在社交推理游戏中的表现<br><br>## 📌 背景痛点/本文动机<br>社交推理游戏如 Resistance Avalon 对玩家的推理、沟通和决策能力提出了挑战。这些游戏要求玩家在动态变化的游戏阶段做出明智的决策，并在讨论中欺骗、推理和与其他玩家协商。这些特点使得 Avalon 成为研究大型语言模型（LLM）代理的决策和语言处理能力的理想测试平台。然而，目前缺乏一个全面的评估 LLM 代理在多代理游戏环境中的性能的基准测试平台。<br><br>## 🚀 核心方法<br>本文提出了 AvalonBench，一个专门用于评估多代理 LLM 代理的游戏环境。AvalonBench 包含以下三个关键组成部分：<br><br>1. **Avalon 游戏环境**：为代理提供游戏平台，记录所有玩家的行动并推动游戏进程。<br>2. **基于规则的机器人**：作为基线对手，为代理提供可比较的基准。<br>3. **ReAct 风格的 LLM 代理**：针对每个角色定制提示，以评估 LLM 代理在不同角色下的表现。<br><br>## 📈 实验结果<br>本文使用 AvalonBench 对 ChatGPT-3.5 和 Llama2 模型进行了评估，并与基于规则的机器人进行了比较。结果显示，即使在有讨论的情况下，LLM 代理的表现也远低于基于规则的机器人。例如，ChatGPT-3.5 在扮演好人角色时，在与扮演坏人的基于规则的机器人对抗中，胜率为 22.2%，而好人角色的机器人胜率为 38.2%。这表明当前 LLM 代理在推理、说服、协商和欺骗能力方面存在明显差距。<br><br>## 💬 可借鉴之处<br>AvalonBench 为研究 LLM 代理在社交推理游戏中的表现提供了一个有价值的测试平台。该平台可以帮助研究人员开发更先进的 LLM 代理，并探索如何将决策技术集成到 LLM 中，以提高其在复杂游戏环境中的表现。此外，AvalonBench 还可以用于评估 LLM 代理在多代理协作、沟通和策略制定方面的能力。</td>
    </tr>
    <tr>
      <th>116</th>
      <td>Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction</td>
      <td>We study the problem of learning goal-conditioned policies in Minecraft, a<br>popular, widely accessible yet challenging open-ended environment for<br>developing human-level multi-task agents. We first identify two main challenges<br>of learning such policies: 1) the indistinguishability of tasks from the state<br>distribution, due to the vast scene diversity, and 2) the non-stationary nature<br>of environment dynamics caused by partial observability. To tackle the first<br>challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage<br>the emergence of goal-relevant visual state representations. To tackle the<br>second challenge, the policy is further fueled by an adaptive horizon<br>prediction module that helps alleviate the learning uncertainty brought by the<br>non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method<br>significantly outperforms the best baseline so far; in many of them, we double<br>the performance. Our ablation and exploratory studies then explain how our<br>approach beat the counterparts and also unveil the surprising bonus of<br>zero-shot generalization to new scenes (biomes). We hope our agent could help<br>shed some light on learning goal-conditioned, multi-task agents in challenging,<br>open-ended environments like Minecraft.</td>
      <td>## 🌟 论文解读 | 在开放世界中实现多任务控制：目标感知表示学习和自适应预测<br><br>## 📌 背景痛点/本文动机<br>开放世界环境，如Minecraft，为开发能够执行各种任务的智能体提供了丰富的平台。然而，这些环境也带来了独特的挑战，包括：<br>1. **状态分布的多样性**：由于场景的多样性，不同任务的状态难以区分，这使得学习目标条件策略变得困难。<br>2. **环境动态的非平稳性**：由于部分可观察性，环境动态具有非平稳性，导致学习的不确定性增加。<br><br>## 🚀 核心方法<br>为了解决这些挑战，本文提出了以下创新方法：<br>💡 创新点1：**目标感知骨干网络（GSB**）<br>   - GSB通过在多个层次上融合目标信息，鼓励出现与目标相关的视觉状态表示，从而解决状态分布多样性的问题。<br>   - GSB由多个目标卷积块（g-conv block）组成，这些块通过通道调制将目标信息与视觉特征融合。<br><br>💡 创新点2：**自适应预测模块**<br>   - 为了应对环境动态的非平稳性，本文引入了自适应预测模块，该模块预测从当前状态到目标的剩余时间步数（即距离到目标的距离）。<br>   - 自适应预测模块通过预测剩余时间步数，帮助智能体更好地理解目标的完成程度，从而提高决策的准确性。<br><br>## 📈 实验结果<br>在Minecraft的20个任务上进行的实验表明，本文提出的方法显著优于现有基线，在许多任务中性能翻倍。消融研究和探索性研究解释了本文方法如何优于现有方法，并揭示了令人惊讶的零样本泛化到新场景（生物群落）的额外优势。<br><br>## 💬 可借鉴之处<br>本文提出的GSB和自适应预测模块为在开放世界环境中学习目标条件策略提供了新的思路，并为开发能够在复杂环境中执行多任务的智能体提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cradle: Empowering Foundation Agents Towards General Computer Control</td>
      <td>Despite the success in specific scenarios, existing foundation agents still<br>struggle to generalize across various virtual scenarios, mainly due to the<br>dramatically different encapsulations of environments with manually designed<br>observation and action spaces. To handle this issue, we propose the General<br>Computer Control (GCC) setting to restrict foundation agents to interact with<br>software through the most unified and standardized interface, i.e., using<br>screenshots as input and keyboard and mouse actions as output. We introduce<br>Cradle, a modular and flexible LMM-powered framework, as a preliminary attempt<br>towards GCC. Enhanced by six key modules, Cradle can understand input<br>screenshots and output executable code for low-level keyboard and mouse control<br>after high-level planning, so that Cradle can interact with any software and<br>complete long-horizon complex tasks without relying on any built-in APIs.<br>Experimental results show that Cradle exhibits remarkable generalizability and<br>impressive performance across four previously unexplored commercial video<br>games, five software applications, and a comprehensive benchmark, OSWorld.<br>Cradle is the first to enable foundation agents to follow the main storyline<br>and complete 40-minute-long real missions in the complex AAA game Red Dead<br>Redemption 2 (RDR2). Cradle can also create a city of a thousand people in<br>Cities: Skylines, farm and harvest parsnips in Stardew Valley, and trade and<br>bargain with a maximal weekly total profit of 87% in Dealer's Life 2. Cradle<br>can not only operate daily software, like Chrome, Outlook, and Feishu, but also<br>edit images and videos using Meitu and CapCut. Cradle greatly extends the reach<br>of foundation agents by enabling the easy conversion of any software,<br>especially complex games, into benchmarks to evaluate agents' various abilities<br>and facilitate further data collection, thus paving the way for generalist<br>agents.</td>
      <td>## 🌟 论文解读 | Cradle：迈向通用计算机控制的基石<br><br>## 📌 背景痛点/本文动机<br>现有的基础智能体在特定场景中取得了成功，但在泛化到各种虚拟场景时仍然面临挑战。这主要是因为环境封装的巨大差异，包括手动设计的观察和动作空间。为了解决这个问题，本文提出了通用计算机控制（GCC）设置，限制基础智能体通过最统一和标准化的接口与软件进行交互，即使用屏幕截图作为输入和键盘鼠标操作作为输出。<br><br>## 🚀 核心方法<br>💡 创新点1：GCC设置<br>本文提出了GCC设置，旨在让基础智能体通过统一的接口与软件进行交互，从而提高其泛化能力。GCC设置要求智能体仅通过屏幕截图作为输入和键盘鼠标操作作为输出与软件进行交互。<br><br>💡 创新点2：Cradle框架<br>为了实现GCC设置，本文提出了Cradle框架，这是一个模块化和灵活的LMM（大型多模态模型）驱动框架。Cradle框架由六个关键模块组成：信息收集、自我反思、任务推理、技能管理、动作规划和记忆。这些模块协同工作，使Cradle能够理解输入屏幕截图，并在高级规划后输出可执行的代码，以进行低级键盘和鼠标控制。这样，Cradle可以与任何软件进行交互，并完成长期复杂的任务，而无需依赖任何内置API。<br><br>## 📈 实验结果<br>实验结果表明，Cradle在四个以前未探索过的商业视频游戏、五个软件应用程序和一个全面的基准测试OSWorld中表现出显著的泛化能力和令人印象深刻的性能。Cradle是第一个能够使基础智能体遵循复杂AAA游戏《荒野大镖客救赎2》（RDR2）的主线剧情并完成40分钟长的真实任务的框架。此外，Cradle还可以在《城市：天际线》中创建一个拥有千人的城市，在《星露谷物语》中种植和收获欧芹，以及在《经销商生活2》中以87%的最大每周总利润进行交易和讨价还价。Cradle不仅可以操作日常软件，如Chrome、Outlook和飞书，还可以使用美图和剪映编辑图像和视频。<br><br>## 💬 可借鉴之处<br>Cradle框架为通用计算机控制提供了一个有前景的解决方案。其模块化和灵活的设计使其能够适应各种环境和任务。此外，Cradle框架的实验结果表明，它能够在复杂的虚拟环境中表现出色，并具有显著的泛化能力。这些发现为开发更强大的基础智能体和推动通用人工智能（AGI）的发展提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>59</th>
      <td>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</td>
      <td>Large vision-language models (VLMs) have achieved substantial progress in<br>multimodal perception and reasoning. When integrated into an embodied agent,<br>existing embodied VLM works either output detailed action sequences at the<br>manipulation level or only provide plans at an abstract level, leaving a gap<br>between high-level planning and real-world manipulation. To bridge this gap, we<br>introduce Octopus, an embodied vision-language programmer that uses executable<br>code generation as a medium to connect planning and manipulation. Octopus is<br>designed to 1) proficiently comprehend an agent's visual and textual task<br>objectives, 2) formulate intricate action sequences, and 3) generate executable<br>code. To facilitate Octopus model development, we introduce OctoVerse: a suite<br>of environments tailored for benchmarking vision-based code generators on a<br>wide spectrum of tasks, ranging from mundane daily chores in simulators to<br>sophisticated interactions in complex video games such as Grand Theft Auto<br>(GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an<br>explorative agent that generates training data, i.e., action blueprints and<br>corresponding executable code. We also collect feedback that enables an<br>enhanced training scheme called Reinforcement Learning with Environmental<br>Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's<br>functionality and present compelling results, showing that the proposed RLEF<br>refines the agent's decision-making. By open-sourcing our simulation<br>environments, dataset, and model architecture, we aspire to ignite further<br>innovation and foster collaborative applications within the broader embodied AI<br>community.</td>
      <td>## 🌟 论文解读 | Octopus：基于环境反馈的具身视觉-语言编程器<br><br>## 📌 背景痛点/本文动机<br>随着大型视觉-语言模型（VLMs）在多模态感知和推理方面取得显著进展，将它们集成到具身智能体中成为可能。然而，现有的具身VLM工作要么在操作层面输出详细的动作序列，要么仅在抽象层面提供计划，导致高级规划和现实世界操作之间存在差距。本文旨在解决这个问题，提出了一种名为Octopus的具身视觉-语言编程器，它使用可执行代码生成作为连接规划和操作的媒介。<br><br>## 🚀 核心方法<br>💡 创新点1：Octopus能够熟练地理解智能体的视觉和文本任务目标，制定复杂的动作序列，并生成可执行代码。<br>💡 创新点2：为了促进Octopus模型的发展，本文引入了OctoVerse，这是一套为在各种任务上评估基于视觉的代码生成器而量身定制的环境，包括从模拟器中的日常家务到复杂视频游戏（如GTA和Minecraft）中的复杂交互。<br>💡 创新点3：为了训练Octopus，本文利用GPT-4控制一个探索性智能体，生成训练数据，即动作蓝图和相应的可执行代码。同时，收集反馈，以实现一种称为环境反馈强化学习（RLEF）的增强训练方案。<br><br>## 📈 实验结果<br>通过一系列实验，本文展示了Octopus的功能，并展示了令人信服的结果，表明所提出的RLEF细化了智能体的决策。Octopus在各种场景中表现出强大的适应性，在任务规划、代码生成和执行方面优于现有模型。RLEF的集成进一步增强了Octopus的性能，展示了这种训练方法的有效性。<br><br>## 💬 可借鉴之处<br>本文提出的Octopus模型和OctoVerse环境为具身视觉-语言编程领域提供了新的思路和方法。Octopus模型的设计和训练过程可以借鉴到其他具身智能体中，以提高其在现实世界中的操作能力。OctoVerse环境可以用于评估和比较不同的具身视觉-语言编程模型，推动该领域的研究和发展。</td>
    </tr>
    <tr>
      <th>100</th>
      <td>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory</td>
      <td>The captivating realm of Minecraft has attracted substantial research<br>interest in recent years, serving as a rich platform for developing intelligent<br>agents capable of functioning in open-world environments. However, the current<br>research landscape predominantly focuses on specific objectives, such as the<br>popular "ObtainDiamond" task, and has not yet shown effective generalization to<br>a broader spectrum of tasks. Furthermore, the current leading success rate for<br>the "ObtainDiamond" task stands at around 20%, highlighting the limitations of<br>Reinforcement Learning (RL) based controllers used in existing methods. To<br>tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel<br>framework integrates Large Language Models (LLMs) with text-based knowledge and<br>memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These<br>agents, equipped with the logic and common sense capabilities of LLMs, can<br>skillfully navigate complex, sparse-reward environments with text-based<br>interactions. We develop a set of structured actions and leverage LLMs to<br>generate action plans for the agents to execute. The resulting LLM-based agent<br>markedly surpasses previous methods, achieving a remarkable improvement of<br>+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior<br>robustness compared to traditional RL-based controllers. Notably, our agent is<br>the first to procure all items in the Minecraft Overworld technology tree,<br>demonstrating its extensive capabilities. GITM does not need any GPU for<br>training, but a single CPU node with 32 CPU cores is enough. This research<br>shows the potential of LLMs in developing capable agents for handling<br>long-horizon, complex tasks and adapting to uncertainties in open-world<br>environments. See the project website at https://github.com/OpenGVLab/GITM.</td>
      <td>## 🌟 论文解读 | Minecraft中的幽灵：通过大型语言模型和基于文本的知识与记忆，创建开放世界环境中的通用能力智能体<br><br>## 📌 背景痛点/本文动机<br>Minecraft作为一款开放世界游戏，吸引了大量研究兴趣，成为开发能够在开放世界中运行的智能体的丰富平台。然而，目前的研究主要集中在特定目标上，如流行的“ObtainDiamond”任务，尚未在更广泛的任务上展现出有效的泛化能力。此外，现有方法在“ObtainDiamond”任务上的最高成功率仅为约20%，突显了现有基于强化学习（RL）的控制器方法的局限性。为了解决这些挑战，本文提出了Ghost in the Minecraft（GITM）框架，该框架将大型语言模型（LLMs）与基于文本的知识和记忆相结合，旨在创建能够在Minecraft中运行的通用能力智能体（GCAs）。<br><br>## 🚀 核心方法<br>💡 创新点1：LLM分解器<br>LLM分解器负责将任务目标分解为一系列更易于实现的子目标。通过解决每个子目标，可以逐步实现任务目标。LLM分解器利用从互联网收集的文本知识，将目标分解为子目标树。<br><br>💡 创新点2：LLM规划器<br>LLM规划器负责为每个子目标生成一系列结构化操作。结构化操作具有明确的语义和相应的反馈，使LLMs能够在认知层面理解周围环境并做出决策。LLM规划器还记录和总结成功的操作列表，以增强未来的规划。<br><br>💡 创新点3：LLM接口<br>LLM接口负责将结构化操作转换为键盘/鼠标操作，并与环境进行交互。它还从环境中提取观察结果，并将其转换为反馈消息。<br><br>## 📈 实验结果<br>本文的实验结果表明，基于LLM的智能体在“ObtainDiamond”任务上的成功率显著提高，达到了47.5%，超过了现有的方法。此外，该智能体是第一个在Minecraft Overworld中获取所有物品的智能体，展示了其广泛的技能。<br><br>## 💬 可借鉴之处<br>本文提出的GITM框架为开发能够在开放世界中运行的通用能力智能体提供了一种新的思路。通过利用LLMs的常识和推理能力，以及基于文本的知识和记忆，该框架能够使智能体有效地处理开放世界环境中的各种挑战。此外，该框架还具有高效的学习效率和良好的泛化能力，使其在开发通用能力智能体方面具有巨大的潜力。</td>
    </tr>
    <tr>
      <th>119</th>
      <td>Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</td>
      <td>Language models show a surprising range of capabilities, but the source of<br>their apparent competence is unclear. Do these networks just memorize a<br>collection of surface statistics, or do they rely on internal representations<br>of the process that generates the sequences they see? We investigate this<br>question by applying a variant of the GPT model to the task of predicting legal<br>moves in a simple board game, Othello. Although the network has no a priori<br>knowledge of the game or its rules, we uncover evidence of an emergent<br>nonlinear internal representation of the board state. Interventional<br>experiments indicate this representation can be used to control the output of<br>the network and create "latent saliency maps" that can help explain predictions<br>in human terms.</td>
      <td>## 🌟 论文解读 | 探索序列模型在合成任务中的涌现世界表示<br><br>## 📌 背景痛点/本文动机<br>语言模型展现出惊人的能力，但其背后的原理尚不明确。这些模型是仅仅记忆了一系列表面统计信息，还是依赖于对生成序列过程的内部表示？本文通过将GPT模型应用于预测简单棋盘游戏Othello的合法移动任务，探讨了这个问题。<br><br>## 🚀 核心方法<br>💡 创新点1：使用Othello作为测试平台<br>本文选择Othello作为测试平台，因为它比国际象棋简单，但游戏树足够大，避免了记忆的可能性。通过训练一个GPT变体模型（Othello-GPT）来预测Othello的合法移动，尽管模型没有先验的游戏知识，但仍然能够以高精度生成合法移动。<br><br>💡 创新点2：使用探针技术探索内部表示<br>为了研究Othello-GPT是否计算了游戏状态的内部表示，本文使用了探针技术。探针是一种分类器或回归器，其输入由网络的内部激活组成，并训练以预测感兴趣的特征。通过训练探针来预测网络内部激活后的棋盘状态，发现非线性探针能够以高精度预测棋盘状态，这表明模型内部存在一个非线性的棋盘状态表示。<br><br>💡 创新点3：干预实验验证表示的因果作用<br>为了确定棋盘状态信息是否影响模型的预测，本文进行了一系列干预实验。通过修改Othello-GPT的内部激活，并测量由此产生的效果，发现干预后的预测与预期的棋盘状态相匹配，这表明涌现的世界表示对模型的预测具有因果作用。<br><br>💡 创新点4：创建潜在显著性图<br>本文还展示了如何使用干预技术创建潜在显著性图，这些图可以帮助解释模型的预测。通过干预每个棋盘格的状态，并观察预测概率的变化，可以生成一个表示每个棋盘格对当前棋盘状态预测的显著性的可视化图。<br><br>## 📈 实验结果<br>实验结果表明，Othello-GPT确实维护了一个游戏棋盘状态的表示，并且这个表示是非线性的。此外，这些表示与模型的预测具有因果联系。潜在显著性图揭示了Othello-GPT训练数据集的不同版本之间的显著差异。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，序列模型可以学习到复杂的内部表示，并且这些表示可以用于解释模型的预测。此外，干预实验和潜在显著性图等技术可以用于更好地理解模型的内部工作机制。这些发现对于自然语言处理等领域的研究具有重要意义，可以帮助我们更好地理解语言模型的内部表示，并开发更可靠的解释工具。</td>
    </tr>
    <tr>
      <th>54</th>
      <td>SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents</td>
      <td>Humans are social beings; we pursue social goals in our daily interactions,<br>which is a crucial aspect of social intelligence. Yet, AI systems' abilities in<br>this realm remain elusive. We present SOTOPIA, an open-ended environment to<br>simulate complex social interactions between artificial agents and evaluate<br>their social intelligence. In our environment, agents role-play and interact<br>under a wide variety of scenarios; they coordinate, collaborate, exchange, and<br>compete with each other to achieve complex social goals. We simulate the<br>role-play interaction between LLM-based agents and humans within this task<br>space and evaluate their performance with a holistic evaluation framework<br>called SOTOPIA-Eval. With SOTOPIA, we find significant differences between<br>these models in terms of their social intelligence, and we identify a subset of<br>SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.<br>We find that on this subset, GPT-4 achieves a significantly lower goal<br>completion rate than humans and struggles to exhibit social commonsense<br>reasoning and strategic communication skills. These findings demonstrate<br>SOTOPIA's promise as a general platform for research on evaluating and<br>improving social intelligence in artificial agents.</td>
      <td>## 🌟 论文解读 | SOTOPIA：评估语言模型社交智能的开放环境<br><br>## 📌 背景痛点/本文动机<br>人类在社会互动中追求复杂的社会目标，这是社交智能的关键方面。然而，AI系统在社交领域的智能仍然难以捉摸。现有的社交智能评估方法要么是非交互式的，要么缺乏对多样化目标驱动行为的关注，或者专注于特定任务。为了研究动态和目标驱动的社交智能，本文提出了SOTOPIA，一个开放式的通用领域环境，用于模拟人工代理之间的复杂社交互动并评估其社交智能。<br><br>## 🚀 核心方法<br>💡 创新点1：SOTOPIA环境<br>SOTOPIA是一个交互式环境，支持多轮模拟通信，代理可以使用语言和非语言沟通以及物理动作。它具有多样化的任务空间，包括自动生成的场景、目标、角色、关系和其他代理的策略，从而构建了一个庞大且多样化的任务空间。<br><br>💡 创新点2：SOTOPIA-EVAL评估框架<br>SOTOPIA-EVAL是一个多维度的评估框架，从多个社交维度分析代理的表现，包括目标完成度、可信度、知识获取、秘密保持、关系维护、社会规则遵守和财务及物质利益。<br><br>## 📈 实验结果<br>实验结果表明，GPT-4在SOTOPIA-EVAL的某些维度上可以作为人类判断的代理，尤其是在目标完成度方面。在模型之间，GPT-4在大多数维度上表现最佳，其次是GPT-3.5、Llama-2-70b-chat和MPT-30b-chat。然而，所有模型在遵守社会规则和保持秘密方面都存在风险。与人类相比，GPT-4在目标完成度方面显著低于人类，并且在展示社交常识推理和战略沟通技能方面存在困难。<br><br>## 💬 可借鉴之处<br>SOTOPIA为评估和改进人工代理的社交智能提供了一个有前景的平台。SOTOPIA-EVAL框架可以用于评估代理在多个社交维度上的表现，并帮助研究人员理解模型之间以及模型与人类之间的社交智能差异。此外，SOTOPIA还可以用于训练更具社交智能的语言代理。</td>
    </tr>
    <tr>
      <th>123</th>
      <td>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos</td>
      <td>Pretraining on noisy, internet-scale datasets has been heavily studied as a<br>technique for training models with broad, general capabilities for text,<br>images, and other modalities. However, for many sequential decision domains<br>such as robotics, video games, and computer use, publicly available data does<br>not contain the labels required to train behavioral priors in the same way. We<br>extend the internet-scale pretraining paradigm to sequential decision domains<br>through semi-supervised imitation learning wherein agents learn to act by<br>watching online unlabeled videos. Specifically, we show that with a small<br>amount of labeled data we can train an inverse dynamics model accurate enough<br>to label a huge unlabeled source of online data -- here, online videos of<br>people playing Minecraft -- from which we can then train a general behavioral<br>prior. Despite using the native human interface (mouse and keyboard at 20Hz),<br>we show that this behavioral prior has nontrivial zero-shot capabilities and<br>that it can be fine-tuned, with both imitation learning and reinforcement<br>learning, to hard-exploration tasks that are impossible to learn from scratch<br>via reinforcement learning. For many tasks our models exhibit human-level<br>performance, and we are the first to report computer agents that can craft<br>diamond tools, which can take proficient humans upwards of 20 minutes (24,000<br>environment actions) of gameplay to accomplish.</td>
      <td>## 🌟 论文解读 | 视频预训练（VPT）：通过观看无标签在线视频学习行动<br><br>## 📌 背景痛点/本文动机<br>近年来，在自然语言处理和计算机视觉等领域，通过在大型互联网数据集上进行预训练，已经证明了训练大型通用基础模型的有效性。然而，对于许多序列决策领域，如机器人、视频游戏和计算机使用，公开可用的数据并不包含训练行为先验所需的标签。本文旨在通过利用互联网上大量未标记的视频数据，将这些预训练范式扩展到序列决策领域。<br><br>## 🚀 核心方法<br>💡 创新点1：半监督模仿学习<br>本文提出了一种半监督模仿学习方法，通过观看在线未标记的视频，使智能体学会行动。具体来说，使用少量标记数据训练一个逆动力学模型，该模型足够准确，可以标记大量未标记的在线数据（例如，人们玩Minecraft的视频），然后从中训练一个通用的行为先验。<br><br>💡 创新点2：逆动力学模型<br>与行为克隆相比，逆动力学建模任务更简单，因为它是非因果的，这意味着它可以查看过去和未来的帧来推断动作。在大多数情况下，环境机制比环境中可能发生的人类行为的广度要简单得多，这表明非因果逆动力学模型可能需要比因果行为克隆模型少得多的数据来训练。<br><br>## 📈 实验结果<br>实验结果表明，尽管使用了原生人类界面（20Hz的鼠标和键盘），但这种方法的行为先验具有非平凡的零样本能力，并且可以通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。对于许多任务，模型表现出人类水平的性能，并且是第一个报告能够制作钻石工具的计算机代理，这需要熟练的人类玩家超过20分钟（24,000个环境动作）的游戏时间才能完成。<br><br>## 💬 可借鉴之处<br>本文提出的视频预训练（VPT）方法为利用互联网上大量未标记的数据进行序列决策领域的预训练提供了一种新的思路。该方法不仅能够有效地利用少量标记数据，而且能够通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。此外，该方法还可以应用于任何具有大量未标记数据的领域，具有广泛的应用前景。</td>
    </tr>
    <tr>
      <th>97</th>
      <td>AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation</td>
      <td>We propose a novel framework for learning high-level cognitive capabilities<br>in robot manipulation tasks, such as making a smiley face using building<br>blocks. These tasks often involve complex multi-step reasoning, presenting<br>significant challenges due to the limited paired data connecting human<br>instructions (e.g., making a smiley face) and robot actions (e.g., end-effector<br>movement). Existing approaches relieve this challenge by adopting an open-loop<br>paradigm decomposing high-level instructions into simple sub-task plans, and<br>executing them step-by-step using low-level control models. However, these<br>approaches are short of instant observations in multi-step reasoning, leading<br>to sub-optimal results. To address this issue, we propose to automatically<br>collect a cognitive robot dataset by Large Language Models (LLMs). The<br>resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of<br>multi-step text plans and paired observation sequences. To enable efficient<br>data acquisition, we employ elaborated multi-round prompt designs that<br>effectively reduce the burden of extensive human involvement. We further<br>propose a closed-loop multi-modal embodied planning model that autoregressively<br>generates plans by taking image observations as input. To facilitate effective<br>learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and<br>finetune additional vision adapter and Q-former to enable fine-grained spatial<br>perception for manipulation tasks. We conduct experiments to verify the<br>superiority over existing open and closed-loop methods, and achieve a<br>significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4<br>based robot tasks. Real-world demos are shown in<br>https://www.youtube.com/watch?v=ayAzID1_qQk .</td>
      <td>## 🌟 论文解读 | AlphaBlock：机器人操作中的视觉-语言推理<br><br>## 📌 背景痛点/本文动机<br>在机器人操作任务中，如使用积木制作笑脸，机器人需要理解和执行复杂的语言指令，这涉及到感知、推理和操作。然而，现有的方法通常采用开环范式，将高级指令分解为简单的子任务计划，并使用低级控制模型逐步执行。这种方法缺乏多步推理中的即时观察，导致结果不理想。<br><br>## 🚀 核心方法<br>💡 创新点1：自动收集认知机器人数据集<br>为了解决这个问题，本文提出了一种自动收集认知机器人数据集的方法，通过大型语言模型（LLMs）自动生成多步文本计划和配对的观察序列。这种方法有效地减少了人类参与的负担，并收集了35个综合的高水平任务，包括多步文本计划和配对的观察序列。<br><br>💡 创新点2：闭环多模态具身规划模型<br>本文进一步提出了一种闭环多模态具身规划模型，该模型通过输入图像观察来自回归地生成计划。为了促进有效学习，本文利用了MiniGPT-4，包括冻结的视觉编码器和LLM，并微调了额外的视觉适配器和Q-former，以实现操作任务中的细粒度空间感知。<br><br>## 📈 实验结果<br>实验结果表明，与现有的开环和闭环方法相比，本文提出的CogLoop框架在机器人任务中取得了显著的性能提升。与ChatGPT和GPT-4相比，CogLoop的成功率分别提高了21.4%和14.5%。<br><br>## 💬 可借鉴之处<br>本文提出的CogLoop框架为机器人操作中的视觉-语言推理提供了一种新的思路。通过自动收集认知机器人数据集和闭环多模态具身规划模型，机器人可以更好地理解和执行高级指令，从而实现更智能的交互和操作。这种方法在家庭机器人、制造业和医疗保健等领域具有广泛的应用前景。</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</td>
      <td>Grounding the reasoning ability of large language models (LLMs) for embodied<br>tasks is challenging due to the complexity of the physical world. Especially,<br>LLM planning for multi-agent collaboration requires communication of agents or<br>credit assignment as the feedback to re-adjust the proposed plans and achieve<br>effective coordination. However, existing methods that overly rely on physical<br>verification or self-reflection suffer from excessive and inefficient querying<br>of LLMs. In this paper, we propose a novel framework for multi-agent<br>collaboration that introduces Reinforced Advantage feedback (ReAd) for<br>efficient self-refinement of plans. Specifically, we perform critic regression<br>to learn a sequential advantage function from LLM-planned data, and then treat<br>the LLM planner as an optimizer to generate actions that maximize the advantage<br>function. It endows the LLM with the foresight to discern whether the action<br>contributes to accomplishing the final task. We provide theoretical analysis by<br>extending advantage-weighted regression in reinforcement learning to<br>multi-agent systems. Experiments on Overcooked-AI and a difficult variant of<br>RoCoBench show that ReAd surpasses baselines in success rate, and also<br>significantly decreases the interaction steps of agents and query rounds of<br>LLMs, demonstrating its high efficiency for grounding LLMs. More results are<br>given at https://read-llm.github.io/.</td>
      <td>## 🌟 论文解读 | 提升大型语言模型在具身多智能体协作中的推理能力<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在处理具身任务时，由于物理世界的复杂性，其推理能力难以落地。特别是在多智能体协作中，LLM 规划需要智能体之间的通信或信用分配作为反馈，以调整计划并实现有效协调。然而，现有方法过度依赖物理验证或自我反思，导致对 LLM 的查询过多且效率低下。<br><br>## 🚀 核心方法<br>本文提出了一种名为 ReAd（Reinforced Advantage feedback）的框架，用于多智能体协作，以实现高效的自适应计划。具体来说，ReAd 通过以下步骤实现：<br><br>1. **优势函数学习**：使用 LLM 规划数据，通过批评回归学习一个序列优势函数。<br>2. **LLM 规划器优化**：将 LLM 规划器视为优化器，生成最大化优势函数的动作。<br>3. **前瞻性推理**：赋予 LLM 前瞻性，使其能够判断动作是否有助于完成任务。<br><br>## 📈 实验结果<br>在 Overcooked-AI 和 RoCoBench 的困难变体上进行实验，结果表明 ReAd 在成功率方面优于基线方法，并且显著减少了智能体的交互步骤和 LLM 的查询轮次，证明了其在落地 LLM 方面的高效性。<br><br>## 💬 可借鉴之处<br>ReAd 框架为 LLM 在具身多智能体协作中的推理能力落地提供了一种高效的方法。其核心思想是将 LLM 规划器视为优化器，并通过优势函数引导其生成更有效的动作。这种方法可以应用于各种需要 LLM 协作的场景，例如机器人协作、虚拟现实等。</td>
    </tr>
    <tr>
      <th>113</th>
      <td>MarioGPT: Open-Ended Text2Level Generation through Large Language Models</td>
      <td>Procedural Content Generation (PCG) is a technique to generate complex and<br>diverse environments in an automated way. However, while generating content<br>with PCG methods is often straightforward, generating meaningful content that<br>reflects specific intentions and constraints remains challenging. Furthermore,<br>many PCG algorithms lack the ability to generate content in an open-ended<br>manner. Recently, Large Language Models (LLMs) have shown to be incredibly<br>effective in many diverse domains. These trained LLMs can be fine-tuned,<br>re-using information and accelerating training for new tasks. Here, we<br>introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game<br>levels, in our case Super Mario Bros levels. MarioGPT can not only generate<br>diverse levels, but can be text-prompted for controllable level generation,<br>addressing one of the key challenges of current PCG techniques. As far as we<br>know, MarioGPT is the first text-to-level model and combined with novelty<br>search it enables the generation of diverse levels with varying play-style<br>dynamics (i.e. player paths) and the open-ended discovery of an increasingly<br>diverse range of content. Code available at<br>https://github.com/shyamsn97/mario-gpt.</td>
      <td>## 🌟 论文解读 | MarioGPT：基于大型语言模型的开放性文本到关卡生成<br><br>## 📌 背景痛点/本文动机<br>随着游戏行业的发展，玩家对游戏内容的多样性和复杂性要求越来越高。传统的关卡设计往往需要大量人力和时间，且难以满足个性化需求。因此，如何高效、自动地生成具有多样性和可玩性的游戏关卡成为了一个重要的研究方向。<br><br>## 🚀 核心方法<br>💡 创新点1：MarioGPT模型<br>本文提出了一种名为MarioGPT的模型，该模型基于GPT-2语言模型进行微调，用于生成马里奥游戏关卡。MarioGPT能够根据自然语言提示生成多样化的关卡，并通过文本提示实现可控的关卡生成。<br><br>💡 创新点2：结合新颖性搜索<br>为了实现开放性的关卡生成，本文将MarioGPT与新颖性搜索算法相结合。新颖性搜索算法能够引导模型不断生成具有新颖性的关卡，从而实现无限的内容探索。<br><br>## 📈 实验结果<br>实验结果表明，MarioGPT在生成马里奥游戏关卡方面表现出色。与基线模型相比，MarioGPT在非空气瓷砖预测准确率方面取得了显著提升。此外，MarioGPT生成的关卡中，有88.4%是可玩的，并且生成的路径与实际玩家的路径相似度较高。<br><br>## 💬 可借鉴之处<br>本文提出的MarioGPT模型为游戏关卡生成提供了一种新的思路。通过结合大型语言模型和新颖性搜索算法，可以实现高效、自动地生成具有多样性和可玩性的游戏关卡。此外，本文提出的文本提示方法也为关卡生成提供了更多的控制性。<br><br>## 🌟 未来展望<br>未来，可以进一步探索以下方向：<br>1. 扩展MarioGPT模型，使其能够生成更复杂、更详细的关卡。<br>2. 将人类反馈引入关卡生成过程，通过强化学习等方法使生成的关卡更符合玩家的需求。<br>3. 探索更有效的搜索方法，进一步提高关卡生成的多样性和新颖性。</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Finding the Needle in a Haystack: Detecting Bug Occurrences in Gameplay Videos</td>
      <td>The presence of bugs in video games can bring significant consequences for<br>developers. To avoid these consequences, developers can leverage gameplay<br>videos to identify and fix these bugs. Video hosting websites such as YouTube<br>provide access to millions of game videos, including videos that depict bug<br>occurrences, but the large amount of content can make finding bug instances<br>challenging. We present an automated approach that uses machine learning to<br>predict whether a segment of a gameplay video contains the depiction of a bug.<br>We analyzed 4,412 segments of 198 gameplay videos to predict whether a segment<br>contains an instance of a bug. Additionally, we investigated how our approach<br>performs when applied across different specific genres of video games and on<br>videos from the same game. We also analyzed the videos in the dataset to<br>investigate what characteristics of the visual features might explain the<br>classifier's prediction. Finally, we conducted a user study to examine the<br>benefits of our automated approach against a manual analysis. Our findings<br>indicate that our approach is effective at detecting segments of a video that<br>contain bugs, achieving a high F1 score of 0.88, outperforming the current<br>state-of-the-art technique for bug classification of gameplay video segments.</td>
      <td>## 🌟 论文解读 | 游戏视频中的“寻针”：自动检测游戏中的bug<br><br>## 📌 背景痛点/本文动机<br>游戏行业是一个价值数十亿美元的产业，游戏中的bug会对开发者造成重大影响。为了减少这些影响，开发者可以利用游戏视频来识别和修复这些bug。视频托管网站如YouTube提供了数百万个游戏视频，包括描绘bug出现的视频，但大量内容使得找到bug实例变得具有挑战性。本文提出了一种自动化的方法，使用机器学习来预测游戏视频片段是否包含bug的描绘。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了一种自动化的方法，使用机器学习来预测游戏视频片段是否包含bug的描绘。该方法通过分析视频片段的文本和视觉特征，训练神经网络模型进行预测。<br>💡 创新点2：本文还进行了初步分析，解释了视频帧图像的哪些特征可能与bug的出现有关。通过分析视频帧，研究人员可以了解哪些元素或行为可能导致bug的出现，从而帮助开发者更好地进行测试和修复。<br><br>## 📈 实验结果<br>本文使用了一个包含4,412个游戏视频片段的数据集，其中包括198个不同的游戏视频。实验结果表明，该方法在检测包含bug的视频片段方面非常有效，F1分数达到了0.88，超过了当前最先进的游戏视频片段bug分类技术。<br><br>## 💬 可借鉴之处<br>本文提出的方法可以帮助游戏开发者更有效地识别和修复游戏中的bug。通过使用机器学习模型，开发者可以自动分析大量的游戏视频，快速找到包含bug的片段，从而节省时间和精力。此外，本文的分析结果还可以帮助开发者了解哪些特征可能与bug的出现有关，从而更好地进行测试和修复。</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Creative Agents: Empowering Agents with Imagination for Creative Tasks</td>
      <td>We study building embodied agents for open-ended creative tasks. While<br>existing methods build instruction-following agents that can perform diverse<br>open-ended tasks, none of them demonstrates creativity -- the ability to give<br>novel and diverse task solutions implicit in the language instructions. This<br>limitation comes from their inability to convert abstract language instructions<br>into concrete task goals in the environment and perform long-horizon planning<br>for such complicated goals. Given the observation that humans perform creative<br>tasks with the help of imagination, we propose a class of solutions for<br>creative agents, where the controller is enhanced with an imaginator that<br>generates detailed imaginations of task outcomes conditioned on language<br>instructions. We introduce several approaches to implementing the components of<br>creative agents. We implement the imaginator with either a large language model<br>for textual imagination or a diffusion model for visual imagination. The<br>controller can either be a behavior-cloning policy learned from data or a<br>pre-trained foundation model generating executable codes in the environment. We<br>benchmark creative tasks with the challenging open-world game Minecraft, where<br>the agents are asked to create diverse buildings given free-form language<br>instructions. In addition, we propose novel evaluation metrics for open-ended<br>creative tasks utilizing GPT-4V, which holds many advantages over existing<br>metrics. We perform a detailed experimental analysis of creative agents,<br>showing that creative agents are the first AI agents accomplishing diverse<br>building creation in the survival mode of Minecraft. Our benchmark and models<br>are open-source for future research on creative agents<br>(https://github.com/PKU-RL/Creative-Agents).</td>
      <td>## 🌟 论文解读 | 创意智能体：赋予智能体想象力以完成创意任务<br><br>## 📌 背景痛点/本文动机<br>现有的智能体大多只能执行预定义的任务，缺乏处理开放性任务的能力，尤其是那些需要创造力的任务。例如，在Minecraft游戏中，现有的智能体可以执行简单的指令，如“收集石头”或“建造一个雪人”，但无法完成更复杂的创意任务，如“建造一个砂岩宫殿”。这是因为它们无法将抽象的语言指令转换为具体的任务目标，并执行长期规划。<br><br>## 🚀 核心方法<br>本文提出了“创意智能体”的概念，通过赋予智能体想象力来处理开放性创意任务。创意智能体由两个主要部分组成：想象器和控制器。<br><br>💡 创新点1：想象器<br>想象器负责根据语言指令生成任务结果的详细想象。本文提出了两种实现想象器的方法：<br>- **文本想象**：使用大型语言模型（LLM）如GPT-4，通过Chain-of-Thought（CoT）技术生成文本想象。<br>- **视觉想象**：使用扩散模型如Stable Diffusion，生成与文本描述相符的视觉想象。<br><br>💡 创新点2：控制器<br>控制器负责将想象转换为可执行的计划。本文提出了两种实现控制器的方法：<br>- **行为克隆控制器**：从环境中学习行为克隆策略，将图像想象转换为建筑蓝图，并执行相应的动作。<br>- **基于GPT-4(V)的控制器**：利用GPT-4(V)的视觉语言理解和代码生成能力，直接生成可执行代码来完成任务。<br><br>## 📈 实验结果<br>本文在Minecraft游戏中进行了实验，结果表明，创意智能体能够根据自由形式的语言指令创建多样化和视觉上吸引人的建筑。其中，使用扩散模型进行视觉想象，并结合GPT-4(V)进行控制的智能体表现最佳。<br><br>## 💬 可借鉴之处<br>本文提出的创意智能体框架为开放性学习和创意AI智能体研究提供了新的思路和方法。未来可以进一步探索如何提高行为克隆控制器的性能，以及如何增强智能体的创造力。</td>
    </tr>
    <tr>
      <th>52</th>
      <td>LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay</td>
      <td>This paper explores the open research problem of understanding the social<br>behaviors of LLM-based agents. Using Avalon as a testbed, we employ system<br>prompts to guide LLM agents in gameplay. While previous studies have touched on<br>gameplay with LLM agents, research on their social behaviors is lacking. We<br>propose a novel framework, tailored for Avalon, features a multi-agent system<br>facilitating efficient communication and interaction. We evaluate its<br>performance based on game success and analyze LLM agents' social behaviors.<br>Results affirm the framework's effectiveness in creating adaptive agents and<br>suggest LLM-based agents' potential in navigating dynamic social interactions.<br>By examining collaboration and confrontation behaviors, we offer insights into<br>this field's research and applications. Our code is publicly available at<br>https://github.com/3DAgentWorld/LLM-Game-Agent.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的智能体社会行为研究：在阿瓦隆游戏中的协作与对抗<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）的快速发展，基于LLM的智能体在模拟复杂环境中的行为方面展现出巨大潜力。然而，目前的研究主要集中在LLM智能体在游戏中的表现，而对其社会行为的理解却相对缺乏。本文旨在探索LLM智能体在阿瓦隆游戏中的社会行为，包括协作与对抗，以期为LLM智能体在社会和战略环境中的应用提供新的见解。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了一种新的框架，用于LLM智能体在阿瓦隆游戏中的协作与对抗。该框架包括多个模块，如记忆存储和总结、分析和规划、游戏动作和响应生成、经验学习等，以模拟人类思维过程，帮助LLM智能体更好地理解游戏并做出决策。<br><br>💡 创新点2：本文采用系统提示来引导LLM智能体进行游戏，并使用ChatGPT来分析智能体的社会行为。通过观察智能体在游戏中的协作、对抗、领导、说服、伪装等行为，本文揭示了LLM智能体在社会环境中的行为特点。<br><br>## 📈 实验结果<br>实验结果表明，本文提出的框架在阿瓦隆游戏中取得了优异的性能，与基线方法相比，智能体的胜率显著提高。此外，通过分析智能体的社会行为，本文发现LLM智能体能够展现出与人类相似的社会行为，如领导、说服、伪装等，这表明LLM智能体在社会环境中的应用潜力巨大。<br><br>## 💬 可借鉴之处<br>本文提出的框架和实验方法为研究LLM智能体在社会环境中的行为提供了新的思路。此外，本文的研究结果也为LLM智能体在社会和战略环境中的应用提供了新的见解，有助于推动LLM智能体在社会环境中的应用。</td>
    </tr>
    <tr>
      <th>103</th>
      <td>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback</td>
      <td>We study whether multiple large language models (LLMs) can autonomously<br>improve each other in a negotiation game by playing, reflecting, and<br>criticizing. We are interested in this question because if LLMs were able to<br>improve each other, it would imply the possibility of creating strong AI agents<br>with minimal human intervention. We ask two LLMs to negotiate with each other,<br>playing the roles of a buyer and a seller, respectively. They aim to reach a<br>deal with the buyer targeting a lower price and the seller a higher one. A<br>third language model, playing the critic, provides feedback to a player to<br>improve the player's negotiation strategies. We let the two agents play<br>multiple rounds, using previous negotiation history and AI feedback as<br>in-context demonstrations to improve the model's negotiation strategy<br>iteratively. We use different LLMs (GPT and Claude) for different roles and use<br>the deal price as the evaluation metric. Our experiments reveal multiple<br>intriguing findings: (1) Only a subset of the language models we consider can<br>self-play and improve the deal price from AI feedback, weaker models either do<br>not understand the game's rules or cannot incorporate AI feedback for further<br>improvement. (2) Models' abilities to learn from the feedback differ when<br>playing different roles. For example, it is harder for Claude-instant to<br>improve as the buyer than as the seller. (3) When unrolling the game to<br>multiple rounds, stronger agents can consistently improve their performance by<br>meaningfully using previous experiences and iterative AI feedback, yet have a<br>higher risk of breaking the deal. We hope our work provides insightful initial<br>explorations of having models autonomously improve each other with game playing<br>and AI feedback.</td>
      <td>## 🌟 论文解读 | 语言模型在谈判游戏中通过自我博弈和AI反馈进行自我提升<br><br>## 📌 背景痛点/本文动机<br>本文研究了多个大型语言模型（LLMs）是否能够在谈判游戏中通过自我博弈、反思和批评来自主地相互提升。这一研究问题具有重要意义，因为如果LLMs能够相互提升，那么就有可能以极小的人类干预来创建强大的AI代理。本文通过让两个LLMs进行谈判，一个扮演买家，一个扮演卖家，并让第三个LLM作为批评者提供反馈来改善玩家的谈判策略，从而探索了这一可能性。<br><br>## 🚀 核心方法<br>💡 创新点1：本文提出了“从AI反馈中进行上下文学习”（ICL-AIF）的方法。具体来说，使用AI批评者的反馈以及之前的谈判历史作为上下文演示，以迭代地改进模型的谈判策略。<br><br>💡 创新点2：本文使用不同的LLMs（GPT和Claude）来扮演不同的角色，并使用交易价格作为评估指标。通过实验，本文发现只有一部分LLMs能够通过自我博弈和AI反馈来提高交易价格，而较弱的模型要么不理解游戏规则，要么无法将AI反馈纳入进一步的改进。<br><br>## 📈 实验结果<br>实验结果表明，只有一部分LLMs能够通过自我博弈和AI反馈来提高交易价格，而较弱的模型要么不理解游戏规则，要么无法将AI反馈纳入进一步的改进。此外，模型在扮演不同角色时，从反馈中学习的能力也有所不同。例如，Claude-instant在扮演买家时比扮演卖家时更难提升。当游戏扩展到多轮时，更强的代理可以通过有意义地使用之前的经验和迭代AI反馈来持续提高其性能，但同时也存在更高的交易破裂风险。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，LLMs在谈判游戏中具有自我提升的潜力，但同时也存在一些挑战和风险。未来研究可以探索如何更好地利用AI反馈来提升LLMs的性能，并确保其行为符合人类的期望和价值观。此外，本文的研究方法也可以为其他多智能体游戏场景下的AI学习提供参考。</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Predicting Outcomes in Video Games with Long Short Term Memory Networks</td>
      <td>Forecasting winners in E-sports with real-time analytics has the potential to<br>further engage audiences watching major tournament events. However, making such<br>real-time predictions is challenging due to unpredictable variables within the<br>game involving diverse player strategies and decision-making. Our work attempts<br>to enhance audience engagement within video game tournaments by introducing a<br>real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)<br>based approach enables efficient predictions of win-lose outcomes by only using<br>the health indicator of each player as a time series. As a proof of concept, we<br>evaluate our model's performance within a classic, two-player arcade game,<br>Super Street Fighter II Turbo. We also benchmark our method against state of<br>the art methods for time series forecasting; i.e. Transformer models found in<br>large language models (LLMs). Finally, we open-source our data set and code in<br>hopes of furthering work in predictive analysis for arcade games.</td>
      <td>## 🌟 论文解读 | 利用长短期记忆网络预测电子竞技比赛结果<br><br>## 📌 背景痛点/本文动机<br>随着电子竞技（Esports）的日益流行，观众对于实时比赛结果的预测产生了浓厚的兴趣。然而，由于游戏中的变量众多，包括玩家策略和决策的不确定性，实时预测比赛结果一直是一个挑战。本文旨在通过引入一种实时预测方法来增强观众在电子游戏锦标赛中的参与度。<br><br>## 🚀 核心方法<br>💡 创新点1：使用长短期记忆网络（LSTM）进行实时预测<br>本文提出了一种基于LSTM的实时预测方法，该方法仅使用每个玩家的健康指示器作为时间序列来预测胜负结果。这种方法能够有效地处理时间序列数据，并捕捉游戏中的动态变化。<br><br>💡 创新点2：在经典的双人街机游戏《超级街头霸王II Turbo》中评估模型性能<br>为了验证模型的有效性，本文在经典的双人街机游戏《超级街头霸王II Turbo》中评估了模型的性能。通过分析玩家健康指示器的变化，模型能够在比赛的不同阶段进行实时预测。<br><br>💡 创新点3：与大型语言模型中的Transformer模型进行基准测试<br>为了进一步验证模型的有效性，本文将LSTM模型与大型语言模型中的Transformer模型进行了基准测试。结果表明，LSTM模型在预测比赛结果方面表现出了更高的准确率。<br><br>## 📈 实验结果<br>实验结果表明，LSTM模型在预测比赛结果方面表现出了较高的准确率。在比赛的不同阶段（25%，75%，95%），LSTM模型的ROC-AUC分数均高于Transformer模型。此外，LSTM模型的训练时间也相对较短，更适合实时预测场景。<br><br>## 💬 可借鉴之处<br>本文提出的基于LSTM的实时预测方法为电子竞技比赛结果的预测提供了一种新的思路。该方法可以应用于其他电子竞技游戏，并有助于提高观众在比赛中的参与度。此外，本文还开源了数据集和代码，为其他研究人员提供了进一步研究的便利。</td>
    </tr>
    <tr>
      <th>72</th>
      <td>SimulBench: Evaluating Language Models with Creative Simulation Tasks</td>
      <td>We introduce SimulBench, a benchmark designed to evaluate large language<br>models (LLMs) across a diverse collection of creative simulation scenarios,<br>such as acting as a Linux terminal or playing text games with users. While<br>these simulation tasks serve as effective measures of an LLM's general<br>intelligence, they are seldom incorporated into existing benchmarks. A major<br>challenge is to develop an evaluation framework for testing different LLMs<br>fairly while preserving the multi-round interactive nature of simulation tasks<br>between users and AI. To tackle this issue, we suggest using a fixed LLM as a<br>user agent to engage with an LLM to collect dialogues first under different<br>tasks. Then, challenging dialogue scripts are extracted for evaluating<br>different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4<br>is employed as the evaluator, tasked with reviewing the quality of the final<br>response generated by the target LLMs given multi-turn dialogue scripts. Our<br>comprehensive experiments indicate that these simulation tasks continue to pose<br>a significant challenge with their unique natures and show the gap between<br>proprietary models and the most advanced open LLMs. For example, GPT-4-turbo<br>outperforms LLaMA-3-70b-Chat on 18.55\% more cases.</td>
      <td>## 🌟 论文解读 | SimulBench：评估语言模型在创意模拟任务中的表现<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在模拟复杂任务方面的能力日益增强，评估这些模型在模拟任务中的表现变得至关重要。然而，现有的评估基准主要集中在单轮、静态的用户与LLMs之间的交互，缺乏对多轮交互和复杂模拟能力的评估。此外，现有的基准主要集中在与人类相关的模拟任务上，而忽略了非人类中心的模拟任务，如Linux终端或文本游戏等。<br><br>## 🚀 核心方法<br>💡 创新点1：SimulBench基准<br>本文提出了SimulBench，一个旨在评估LLMs在创意模拟任务中的表现的基准。SimulBench包含109个独特的模拟任务，涵盖了各种接口，如Linux终端、SQL执行器、文本游戏等。<br><br>💡 创新点2：多轮脚本评估框架<br>为了公平地评估不同LLMs，SimulBench采用了一个三阶段的评估框架。首先，使用一个固定的LLM作为用户代理与另一个LLM进行多轮对话，收集对话历史。然后，从这些对话历史中提取具有挑战性的对话脚本，用于评估不同的目标LLMs。最后，使用GPT-4作为评估者，对目标LLMs在给定多轮对话脚本下的最终响应质量进行评估。<br><br>## 📈 实验结果<br>实验结果表明，SimulBench中的模拟任务对LLMs来说仍然是一个巨大的挑战，并且显示了专有模型和最先进的开源LLMs之间的差距。例如，GPT-4-turbo在18.55%的情况下优于LLaMA-3-70b-Chat。<br><br>## 💬 可借鉴之处<br>SimulBench基准为评估LLMs在模拟任务中的表现提供了一个有价值的工具。其多轮脚本评估框架可以确保公平的比较，并有助于研究人员更好地理解LLMs在不同模拟任务中的表现。此外，SimulBench的实验结果也揭示了LLMs在处理复杂模拟任务时的挑战和局限性，为未来的研究提供了方向。</td>
    </tr>
    <tr>
      <th>91</th>
      <td>Embodied Task Planning with Large Language Models</td>
      <td>Equipping embodied agents with commonsense is important for robots to<br>successfully complete complex human instructions in general environments.<br>Recent large language models (LLM) can embed rich semantic knowledge for agents<br>in plan generation of complex tasks, while they lack the information about the<br>realistic world and usually yield infeasible action sequences. In this paper,<br>we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning<br>with physical scene constraint, where the agent generates executable plans<br>according to the existed objects in the scene by aligning LLMs with the visual<br>perception models. Specifically, we first construct a multimodal dataset<br>containing triplets of indoor scenes, instructions and action plans, where we<br>provide the designed prompts and the list of existing objects in the scene for<br>GPT-3.5 to generate a large number of instructions and corresponding planned<br>actions. The generated data is leveraged for grounded plan tuning of<br>pre-trained LLMs. During inference, we discover the objects in the scene by<br>extending open-vocabulary object detectors to multi-view RGB images collected<br>in different achievable locations. Experimental results show that the generated<br>plan from our TaPA framework can achieve higher success rate than LLaVA and<br>GPT-3.5 by a sizable margin, which indicates the practicality of embodied task<br>planning in general and complex environments.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的具身任务规划<br><br>## 📌 背景痛点/本文动机<br>随着人工智能的发展，机器人被期望能够在各种环境中完成复杂的任务，例如家庭服务、医疗护理和农业采摘等。然而，由于训练样本有限和任务的多样性，直接训练一个能够在不同部署场景中工作的具身代理是不可行的。大型语言模型（LLM）可以从大量网络数据中获取丰富的常识知识，这些知识可以潜在地被具身代理利用来生成符合人类要求的自然语言命令的动作计划。然而，LLM无法感知周围场景，并且可能会生成不可执行的行动，因为它们需要与非存在的对象进行交互。因此，将LLM生成的任务计划与物理世界相结合是构建能够完成复杂任务的具身代理的必要条件。<br><br>## 🚀 核心方法<br>本文提出了一种名为TaPA的具身任务规划代理，用于在物理场景中进行具身任务规划。TaPA通过将LLM与视觉感知模型相结合，根据场景中存在的对象生成可执行的计划。具体来说，本文首先构建了一个包含室内场景、指令和动作计划三元组的多元数据集，其中为GPT-3.5提供了设计的提示和场景中现有对象的列表，以生成大量指令和相应的计划动作。生成的数据用于对预训练的LLM进行基于物理场景约束的接地计划微调。在推理过程中，通过扩展开放词汇对象检测器到从不同可达位置收集的多视图RGB图像，发现场景中的对象。实验结果表明，与LLaVA和GPT-3.5相比，TaPA框架生成的计划可以取得更高的成功率，这表明具身任务规划在一般和复杂环境中的实用性。<br><br>## 📈 实验结果<br>实验结果表明，与LLaVA和GPT-3.5相比，TaPA框架生成的计划可以取得更高的成功率，这表明具身任务规划在一般和复杂环境中的实用性。<br><br>## 💬 可借鉴之处<br>本文提出的TaPA框架为具身任务规划提供了一种新的思路，通过将LLM与视觉感知模型相结合，可以生成更符合物理场景约束的可执行计划。此外，本文构建的多元数据集也为其他研究提供了有价值的资源。</td>
    </tr>
    <tr>
      <th>53</th>
      <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
      <td>Recent studies have presented compelling evidence that large language models<br>(LLMs) can equip embodied agents with the self-driven capability to interact<br>with the world, which marks an initial step toward versatile robotics. However,<br>these efforts tend to overlook the visual richness of open worlds, rendering<br>the entire interactive process akin to "a blindfolded text-based game."<br>Consequently, LLM-based agents frequently encounter challenges in intuitively<br>comprehending their surroundings and producing responses that are easy to<br>understand. In this paper, we propose Steve-Eye, an end-to-end trained large<br>multimodal model designed to address this limitation. Steve-Eye integrates the<br>LLM with a visual encoder which enables it to process visual-text inputs and<br>generate multimodal feedback. In addition, we use a semi-automatic strategy to<br>collect an extensive dataset comprising 850K open-world instruction pairs,<br>empowering our model to encompass three essential functions for an agent:<br>multimodal perception, foundational knowledge base, and skill prediction and<br>planning. Lastly, we develop three open-world evaluation benchmarks, then carry<br>out extensive experiments from a wide range of perspectives to validate our<br>model's capability to strategically act and plan. Codes and datasets will be<br>released.</td>
      <td>## 🌟 论文解读 | Steve-Eye：为基于LLM的具身智能体赋予开放世界的视觉感知能力<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在赋予具身智能体与世界互动的能力方面取得了显著进展，这标志着通用机器人技术迈出了第一步。然而，这些研究往往忽略了开放世界的视觉丰富性，导致整个交互过程类似于“一个蒙着眼睛的基于文本的游戏”。因此，基于LLM的智能体在直观地理解周围环境和生成易于理解的响应方面经常遇到挑战。<br><br>## 🚀 核心方法<br>为了解决这一局限性，本文提出了Steve-Eye，这是一个端到端训练的大型多模态模型，旨在赋予基于LLM的具身智能体在开放世界中进行视觉感知的能力。Steve-Eye将LLM与视觉编码器相结合，使其能够处理视觉-文本输入并生成多模态反馈。此外，我们使用半自动策略收集了一个包含850K开放世界指令对的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库和技能预测与规划。最后，我们开发了三个开放世界评估基准，然后从广泛的视角进行大量实验，以验证我们的模型在战略行动和规划方面的能力。<br><br>## 📈 实验结果<br>实验结果表明，Steve-Eye在开放世界场景中显著优于基于LLM的智能体。具体来说，我们在以下三个基准上进行了评估：<br>1. 环境视觉描述（ENV-VC）：评估智能体感知和描述其周围环境的能力。<br>2. 基础知识问答（FK-QA）：评估智能体掌握对决策至关重要的基本知识的熟练程度。<br>3. 技能预测与规划（SPP）：量化智能体在战略行动和规划方面的能力。<br><br>## 💬 可借鉴之处<br>Steve-Eye的研究成果为开发能够在开放世界中有效互动的具身智能体提供了重要的启示。其多模态感知、基础知识库和技能预测与规划功能为智能体在复杂环境中进行自主行动和规划提供了强大的支持。此外，Steve-Eye的开放世界评估基准为评估具身智能体的性能提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>48</th>
      <td>MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs</td>
      <td>While Vision-Language Models (VLMs) hold promise for tasks requiring<br>extensive collaboration, traditional multi-agent simulators have facilitated<br>rich explorations of an interactive artificial society that reflects collective<br>behavior. However, these existing simulators face significant limitations.<br>Firstly, they struggle with handling large numbers of agents due to high<br>resource demands. Secondly, they often assume agents possess perfect<br>information and limitless capabilities, hindering the ecological validity of<br>simulated social interactions. To bridge this gap, we propose a multi-agent<br>Minecraft simulator, MineLand, that bridges this gap by introducing three key<br>features: large-scale scalability, limited multimodal senses, and physical<br>needs. Our simulator supports 64 or more agents. Agents have limited visual,<br>auditory, and environmental awareness, forcing them to actively communicate and<br>collaborate to fulfill physical needs like food and resources. Additionally, we<br>further introduce an AI agent framework, Alex, inspired by multitasking theory,<br>enabling agents to handle intricate coordination and scheduling. Our<br>experiments demonstrate that the simulator, the corresponding benchmark, and<br>the AI agent framework contribute to more ecological and nuanced collective<br>behavior.The source code of MineLand and Alex is openly available at<br>https://github.com/cocacola-lab/MineLand.</td>
      <td>## 🌟 论文解读 | MineLand：模拟大规模多智能体交互的Minecraft模拟器<br><br>## 📌 背景痛点/本文动机<br>传统的多智能体模拟器在处理大规模场景时面临资源消耗过大的问题，并且通常假设智能体拥有完美信息和无限能力，这与现实世界中的人类交互存在较大差距。为了解决这个问题，本文提出了MineLand，一个基于Minecraft的多智能体模拟器，旨在模拟更接近现实世界的多智能体交互。<br><br>## 🚀 核心方法<br>💡 创新点1：大规模可扩展性<br>MineLand通过将每个Minecraft客户端简化为单个线程，优化了性能开销，从而支持64个或更多智能体在主流消费级桌面PC上运行。<br><br>💡 创新点2：有限的模态感知<br>MineLand模拟了人类的视觉和听觉机制，对智能体的感知能力施加了限制，包括距离衰减、环境遮挡和方向约束，使其更接近现实世界。<br><br>💡 创新点3：物理需求<br>MineLand将真实的物理需求（如食物、氧气和饥饿）集成到智能体中，使其需要管理资源并与其他智能体竞争或合作，以维持生存。<br><br>💡 创新点4：多任务处理框架Alex<br>MineLand引入了基于多任务理论的AI智能体框架Alex，允许智能体同时执行复杂的协调和调度，以处理多个任务。<br><br>## 📈 实验结果<br>实验结果表明，MineLand在支持大规模智能体、有限的模态感知和物理需求方面表现出色。此外，Alex框架能够有效地处理多任务，并在合作模式下提高效率。<br><br>## 💬 可借鉴之处<br>MineLand为研究多智能体交互提供了一个强大的平台，其创新的设计和功能可以应用于人类动力学、社会心理学、机器人技术和游戏设计等领域。此外，Alex框架的多任务处理机制为开发更智能的AI智能体提供了新的思路。</td>
    </tr>
    <tr>
      <th>57</th>
      <td>GameGPT: Multi-agent Collaborative Framework for Game Development</td>
      <td>The large language model (LLM) based agents have demonstrated their capacity<br>to automate and expedite software development processes. In this paper, we<br>focus on game development and propose a multi-agent collaborative framework,<br>dubbed GameGPT, to automate game development. While many studies have<br>pinpointed hallucination as a primary roadblock for deploying LLMs in<br>production, we identify another concern: redundancy. Our framework presents a<br>series of methods to mitigate both concerns. These methods include dual<br>collaboration and layered approaches with several in-house lexicons, to<br>mitigate the hallucination and redundancy in the planning, task identification,<br>and implementation phases. Furthermore, a decoupling approach is also<br>introduced to achieve code generation with better precision.</td>
      <td>## 🌟 论文解读 | GameGPT：游戏开发的多智能体协作框架<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）在自然语言处理和计算机视觉领域的突破，它们在自动化和加速软件开发过程方面展现出巨大潜力。然而，在游戏开发领域，LLM的应用面临着两大挑战：幻觉和冗余。幻觉指的是LLM在生成内容时可能出现的不准确或不相关的情况，而冗余则是指LLM可能会生成不必要的任务或代码片段。这些挑战限制了LLM在游戏开发中的实际应用。<br><br>## 🚀 核心方法<br>💡 创新点1：多智能体协作框架<br>GameGPT提出了一种多智能体协作框架，旨在解决LLM在游戏开发中的幻觉和冗余问题。该框架由多个具有不同角色的智能体组成，包括游戏内容设计师、游戏开发经理、计划审查员、游戏开发工程师、任务审查员、游戏引擎工程师、代码审查员和游戏引擎测试工程师。这些智能体协同工作，共同完成游戏开发的各个阶段，从而提高开发效率和准确性。<br><br>💡 创新点2：双重协作和分层方法<br>为了解决幻觉和冗余问题，GameGPT采用了双重协作和分层方法。双重协作包括LLM与小型专家深度学习模型之间的交互，以及执行角色和审查角色之间的协作。分层方法则通过使用多个内部词典来指导LLM的决策过程，从而减少幻觉和冗余的发生。<br><br>💡 创新点3：代码解耦<br>为了提高代码生成的精度，GameGPT引入了代码解耦方法。该方法将游戏设计脚本分解成多个可管理的代码片段，从而简化LLM的推理过程，并减少幻觉和冗余的发生。<br><br>## 📈 实验结果<br>实验结果表明，GameGPT框架在游戏开发过程中能够有效地进行决策和决策修正，从而提高开发效率和准确性。此外，GameGPT框架还具有可扩展性，可以用于开发中到大型的游戏项目。<br><br>## 💬 可借鉴之处<br>GameGPT框架为游戏开发自动化提供了一种新的思路和方法。其多智能体协作框架、双重协作和分层方法以及代码解耦方法等创新点，可以为其他领域的软件开发自动化提供借鉴。此外，GameGPT框架还可以与其他AI技术相结合，进一步提高游戏开发的自动化水平。</td>
    </tr>
    <tr>
      <th>110</th>
      <td>LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation</td>
      <td>To assist with everyday human activities, robots must solve complex<br>long-horizon tasks and generalize to new settings. Recent deep reinforcement<br>learning (RL) methods show promise in fully autonomous learning, but they<br>struggle to reach long-term goals in large environments. On the other hand,<br>Task and Motion Planning (TAMP) approaches excel at solving and generalizing<br>across long-horizon tasks, thanks to their powerful state and action<br>abstractions. But they assume predefined skill sets, which limits their<br>real-world applications. In this work, we combine the benefits of these two<br>paradigms and propose an integrated task planning and skill learning framework<br>named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the<br>symbolic interface of a task planner to guide RL-based skill learning and<br>creates abstract state space to enable skill reuse. More importantly, LEAGUE<br>learns manipulation skills in-situ of the task planning system, continuously<br>growing its capability and the set of tasks that it can solve. We evaluate<br>LEAGUE on four challenging simulated task domains and show that LEAGUE<br>outperforms baselines by large margins. We also show that the learned skills<br>can be reused to accelerate learning in new tasks domains and transfer to a<br>physical robot platform.</td>
      <td>## 🌟 论文解读 | LEAGUE：基于引导的技能学习和抽象，助力机器人解决长期操作任务<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，机器人已经逐渐走进我们的日常生活，并在各种场景中发挥着重要作用。然而，要让机器人真正实现自主学习和操作，仍然面临着许多挑战。其中，长期操作任务（long-horizon tasks）的解决和泛化能力是机器人领域的一大难题。现有的深度强化学习（DRL）方法在自主学习方面表现出色，但在大型环境中实现长期目标仍然存在困难。另一方面，任务和运动规划（TAMP）方法擅长解决和泛化长期任务，但由于其依赖于预定义的技能集，限制了其在现实世界中的应用。<br><br>## 🚀 核心方法<br>为了克服上述挑战，本文提出了LEAGUE（Learning and Abstraction with Guidance）框架，该框架结合了DRL和TAMP的优势，实现了长期操作任务的解决和泛化。LEAGUE的核心创新点包括：<br><br>💡 创新点1：利用任务规划器的符号接口指导基于强化学习的技能学习，并创建抽象状态空间以实现技能复用。<br>💡 创新点2：在任务规划系统中学习操作技能，不断扩展其能力和可解决的任务集。<br><br>## 📈 实验结果<br>本文在四个具有挑战性的模拟任务领域对LEAGUE进行了评估，结果表明LEAGUE在性能上显著优于基线方法。此外，本文还展示了学习到的技能可以复用于加速新任务领域的学习，并迁移到物理机器人平台。<br><br>## 💬 可借鉴之处<br>LEAGUE框架为机器人解决长期操作任务提供了一种新的思路，其核心思想可以应用于其他领域，例如自动驾驶、游戏AI等。此外，LEAGUE框架中的技能学习和抽象方法也可以为其他强化学习算法提供借鉴。</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</td>
      <td>Video games have served as useful benchmarks for the decision making<br>community, but going beyond Atari games towards training agents in modern games<br>has been prohibitively expensive for the vast majority of the research<br>community. Recent progress in the research, development and open release of<br>large vision models has the potential to amortize some of these costs across<br>the community. However, it is currently unclear which of these models have<br>learnt representations that retain information critical for sequential decision<br>making. Towards enabling wider participation in the research of gameplaying<br>agents in modern games, we present a systematic study of imitation learning<br>with publicly available visual encoders compared to the typical, task-specific,<br>end-to-end training approach in Minecraft, Minecraft Dungeons and<br>Counter-Strike: Global Offensive.</td>
      <td>## 🌟 论文解读 | 视觉编码器在现代视频游戏中的高效模仿学习<br><br>## 📌 背景痛点/本文动机<br>视频游戏一直是决策制定社区的有用基准，但将研究扩展到现代游戏对于大多数研究社区来说成本高昂。近年来，大型视觉模型的研究、开发和公开发布有可能在整个社区中分摊这些成本。然而，目前尚不清楚这些模型中的哪些模型已经学习了保留对顺序决策至关重要的信息的表示。为了使更广泛的社区参与现代游戏中的游戏代理研究，本文对Minecraft、Minecraft Dungeons和Counter-Strike: Global Offensive中的模仿学习进行了系统研究，并与典型的、特定任务的端到端训练方法进行了比较。<br><br>## 🚀 核心方法<br>💡 创新点1：本文比较了端到端训练的视觉编码器和公开可用的预训练编码器在模仿学习中的表现。端到端训练的编码器在相对较小的图像上训练，而预训练编码器则是在大型数据集上训练的，可能提供有用且通用的表示，而无需额外的训练。<br><br>💡 创新点2：本文研究了不同数量的训练数据对视觉编码器性能的影响。结果表明，即使在使用少量高质量数据的情况下，预训练编码器也能表现出与特定任务编码器相当或更好的性能。<br><br>## 📈 实验结果<br>本文在三个现代视频游戏（Minecraft、Minecraft Dungeons和Counter-Strike: Global Offensive）中进行了实验，结果表明：<br><br>1. 小图像（128×128）足以训练现代视频游戏中的代理，即使在使用少量高质量数据的情况下也能取得良好的性能。<br>2. 预训练编码器，特别是DINOv2，在模仿学习中表现出色，并且在使用少量数据时仍然有效。<br>3. 端到端训练的编码器在处理更真实世界的图像时表现更好，但在使用预训练编码器时需要仔细考虑图像大小和比例。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，预训练编码器在现代视频游戏的模仿学习中具有巨大的潜力。研究人员可以利用这些编码器来训练代理，从而降低成本并提高效率。此外，本文还强调了图像大小和比例对预训练编码器性能的重要性，这为未来的研究提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>75</th>
      <td>True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning</td>
      <td>Despite the impressive performance across numerous tasks, large language<br>models (LLMs) often fail in solving simple decision-making tasks due to the<br>misalignment of the knowledge in LLMs with environments. On the contrary,<br>reinforcement learning (RL) agents learn policies from scratch, which makes<br>them always align with environments but difficult to incorporate prior<br>knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a<br>novel general online framework that deploys LLMs as decision-making agents to<br>efficiently interact and align with embodied environments via RL without<br>requiring any prepared datasets or prior knowledge of the environments.<br>Firstly, we query the joint probabilities of each valid action with LLMs to<br>form behavior policies. Then, to enhance the stability and robustness of the<br>policies, we propose two normalization methods and summarize four prompt design<br>principles. Finally, we design a novel parameter-efficient training<br>architecture where the actor and critic share one frozen LLM equipped with<br>low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to<br>evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency<br>and performance compared to the conventional RL method, PPO, and prompt tuning<br>method, SayCan, in both classical decision-making environment, Overcooked, and<br>simulated household environment, VirtualHome. ii) Benefiting from LLMs'<br>open-vocabulary feature, TWOSOME shows superior generalization ability to<br>unseen tasks. iii) Under our framework, there is no significant loss of the<br>LLMs' original ability during online PPO finetuning.</td>
      <td>## 🌟 论文解读 | 知识源于实践：通过强化学习将大型语言模型与具身环境对齐<br><br>## 📌 背景痛点/本文动机<br>尽管大型语言模型（LLMs）在众多任务中表现出色，但它们在解决简单的决策任务时往往失败，这主要是因为LLMs中的知识与环境的错位。相反，强化学习（RL）代理从零开始学习策略，这使得它们始终与环境保持一致，但难以融入先验知识以提高探索效率。为了缩小这一差距，本文提出了TWOSOME，一个新颖的通用在线框架，该框架将LLMs作为决策代理，通过RL高效地与具身环境交互并对齐，而无需任何准备好的数据集或对环境的先验知识。<br><br>## 🚀 核心方法<br>💡 创新点1：行为策略生成<br>TWOSOME不是让LLMs直接生成动作，而是查询LLMs中每个有效动作的联合概率，以形成行为策略。这消除了由于无效动作造成的错位问题。<br><br>💡 创新点2：动作提示归一化<br>为了增强策略的稳定性和鲁棒性，本文提出了两种归一化方法：token归一化和word归一化，以解决动作分布不平衡的问题。<br><br>💡 创新点3：参数高效的训练架构<br>本文设计了一种新颖的参数高效训练架构，其中actor和critic共享一个冻结的LLM，并配备低秩适配器（LoRA），由PPO更新。<br><br>💡 创新点4：提示设计原则<br>本文总结了四个提示设计原则，以增强LLMs的推理能力，从而提高LLMs与具身环境之间的对齐。<br><br>## 📈 实验结果<br>在经典决策环境Overcooked和模拟家庭环境VirtualHome中，TWOSOME在样本效率和性能方面显著优于传统的RL方法PPO和提示调整方法SayCan。此外，TWOSOME还表现出对未见任务的优越泛化能力。<br><br>## 💬 可借鉴之处<br>本文提出的TWOSOME框架为将LLMs与具身环境对齐提供了一个有效的解决方案，并为开发通用的自主代理迈出了重要的一步。该框架具有以下可借鉴之处：<br><br>*   利用LLMs的先验知识来提高RL代理的样本效率。<br>*   通过归一化方法解决动作分布不平衡的问题。<br>*   设计有效的提示以提高LLMs的推理能力。<br>*   使用参数高效的训练架构来降低训练成本。<br><br>## 🎯 未来展望<br>未来可以进一步研究如何将TWOSOME框架应用于更复杂的具身环境，并探索LLMs与RL代理之间的更深入交互。</td>
    </tr>
    <tr>
      <th>61</th>
      <td>Humanoid Agents: Platform for Simulating Human-like Generative Agents</td>
      <td>Just as computational simulations of atoms, molecules and cells have shaped<br>the way we study the sciences, true-to-life simulations of human-like agents<br>can be valuable tools for studying human behavior. We propose Humanoid Agents,<br>a system that guides Generative Agents to behave more like humans by<br>introducing three elements of System 1 processing: Basic needs (e.g. hunger,<br>health and energy), Emotion and Closeness in Relationships. Humanoid Agents are<br>able to use these dynamic elements to adapt their daily activities and<br>conversations with other agents, as supported with empirical experiments. Our<br>system is designed to be extensible to various settings, three of which we<br>demonstrate, as well as to other elements influencing human behavior (e.g.<br>empathy, moral values and cultural background). Our platform also includes a<br>Unity WebGL game interface for visualization and an interactive analytics<br>dashboard to show agent statuses over time. Our platform is available on<br>https://www.humanoidagents.com/ and code is on<br>https://github.com/HumanoidAgents/HumanoidAgents</td>
      <td>## 🌟 论文解读 | 人类化智能体：模拟人类行为的生成式智能体平台<br><br>## 📌 背景痛点/本文动机<br>随着生成式智能体（Generative Agents）的出现，人们开始尝试使用高级自然语言处理系统来模拟人类行为。然而，现有的生成式智能体主要关注逻辑和计划，缺乏对人类直觉和即时反应的模拟。为了解决这个问题，本文提出了人类化智能体（Humanoid Agents）平台，旨在通过引入基本需求、情感和关系亲密度等元素，使智能体更接近人类的真实行为。<br><br>## 🚀 核心方法<br>💡 创新点1：引入系统1思维<br>本文借鉴了心理学中的系统1思维，即直觉、无意识和即时的思维过程。通过引入基本需求、情感和关系亲密度等元素，使智能体能够根据自身状态和环境变化做出更自然的反应。<br><br>💡 创新点2：动态调整行为<br>人类化智能体平台允许智能体根据自身的基本需求、情感和关系亲密度动态调整其日常活动和对话。例如，当智能体感到饥饿时，它会寻找食物；当它感到孤独时，它会尝试与其他智能体进行交流。<br><br>## 📈 实验结果<br>实验结果表明，人类化智能体能够有效地模拟人类行为。与人类标注相比，该系统能够准确预测活动是否满足基本需求、活动表达的情感以及对话是否使智能体之间的关系更加亲密。<br><br>## 💬 可借鉴之处<br>本文提出的Humanoid Agents平台为研究人类行为提供了一个有价值的工具。该平台可以扩展到各种场景，并支持更多影响人类行为的元素，如同理心、道德价值观和文化背景。此外，该平台还提供了Unity WebGL游戏界面和交互式分析仪表板，方便用户可视化智能体的状态和行为。<br><br>## 🌐 平台访问<br>- 平台网站：https://www.humanoidagents.com/<br>- 代码仓库：https://github.com/HumanoidAgents/HumanoidAgents</td>
    </tr>
    <tr>
      <th>98</th>
      <td>Playing repeated games with Large Language Models</td>
      <td>Large Language Models (LLMs) are transforming society and permeating into<br>diverse applications. As a result, LLMs will frequently interact with us and<br>other agents. It is, therefore, of great societal value to understand how LLMs<br>behave in interactive social settings. Here, we propose to use behavioral game<br>theory to study LLM's cooperation and coordination behavior. To do so, we let<br>different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with<br>each other and with other, human-like strategies. Our results show that LLMs<br>generally perform well in such tasks and also uncover persistent behavioral<br>signatures. In a large set of two players-two strategies games, we find that<br>LLMs are particularly good at games where valuing their own self-interest pays<br>off, like the iterated Prisoner's Dilemma family. However, they behave<br>sub-optimally in games that require coordination. We, therefore, further focus<br>on two games from these distinct families. In the canonical iterated Prisoner's<br>Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting<br>after another agent has defected only once. In the Battle of the Sexes, we find<br>that GPT-4 cannot match the behavior of the simple convention to alternate<br>between options. We verify that these behavioral signatures are stable across<br>robustness checks. Finally, we show how GPT-4's behavior can be modified by<br>providing further information about the other player as well as by asking it to<br>predict the other player's actions before making a choice. These results enrich<br>our understanding of LLM's social behavior and pave the way for a behavioral<br>game theory for machines.</td>
      <td>## 🌟 论文解读 | 大型语言模型在重复博弈中的行为研究<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在社会各领域的广泛应用，它们与人类和其他智能体之间的互动日益频繁。因此，理解LLMs在社交互动中的行为变得至关重要。本文旨在通过行为博弈论来研究LLMs的合作与协调行为。<br><br>## 🚀 核心方法<br>💡 创新点1：使用行为博弈论研究LLMs<br>本文提出使用行为博弈论来研究LLMs的合作与协调行为。通过让不同的LLMs（如GPT-3、GPT-3.5和GPT-4）与其他LLMs或人类策略进行有限重复博弈，分析它们的行为模式。<br><br>💡 创新点2：研究LLMs在不同博弈类型中的表现<br>本文研究了LLMs在多种博弈类型中的表现，包括双赢游戏、不公平游戏、循环游戏、有偏游戏和囚徒困境等。通过分析LLMs在不同博弈类型中的得分和策略选择，揭示了它们在不同情境下的行为特征。<br><br>## 📈 实验结果<br>实验结果表明，LLMs在追求自身利益的博弈中表现良好，尤其是在囚徒困境等博弈中。然而，在需要协调的博弈中，LLMs的表现较差。例如，在经典的囚徒困境中，GPT-4表现出极端的不宽容，一旦对方背叛，它就会持续背叛。而在性别之战中，GPT-4无法与交替选择的人类策略相匹配。<br><br>## 💬 可借鉴之处<br>本文的研究结果有助于我们更好地理解LLMs的社交行为，并为机器行为博弈论的发展奠定了基础。此外，本文还提出了两种改进LLMs行为的方法：提供关于其他玩家的更多信息，以及让LLMs预测其他玩家的行为。这些方法可以帮助LLMs更好地适应社交环境，并与人类进行更有效的互动。</td>
    </tr>
    <tr>
      <th>108</th>
      <td>Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey</td>
      <td>Knowledge-enhanced language models (KELMs) have emerged as promising tools to<br>bridge the gap between large-scale language models and domain-specific<br>knowledge. KELMs can achieve higher factual accuracy and mitigate<br>hallucinations by leveraging knowledge graphs (KGs). They are frequently<br>combined with adapter modules to reduce the computational load and risk of<br>catastrophic forgetting. In this paper, we conduct a systematic literature<br>review (SLR) on adapter-based approaches to KELMs. We provide a structured<br>overview of existing methodologies in the field through quantitative and<br>qualitative analysis and explore the strengths and potential shortcomings of<br>individual approaches. We show that general knowledge and domain-specific<br>approaches have been frequently explored along with various adapter<br>architectures and downstream tasks. We particularly focused on the popular<br>biomedical domain, where we provided an insightful performance comparison of<br>existing KELMs. We outline the main trends and propose promising future<br>directions.</td>
      <td>## 🌟 论文解读 | 基于适配器的知识增强语言模型方法综述<br><br>## 📌 背景痛点/本文动机<br>随着自然语言处理（NLP）领域大型语言模型（LLMs）的兴起，尽管它们在解决复杂推理任务和生成新文本方面表现出色，但它们往往缺乏对结构化知识层次（如概念之间的关系和推理能力）的认识。这可能导致下游任务中的预测不准确，以及在文本生成中产生所谓的“幻觉”，特别是在医疗保健或法律等高风险领域。<br><br>## 🚀 核心方法<br>本文综述了基于适配器的知识增强语言模型（KELMs）方法，旨在通过利用知识图谱（KGs）来提高LLMs的可靠性。适配器模块被证明是一种有效且计算效率高的解决方案，可以增强LLMs的任务性能，同时避免灾难性遗忘和任务间的干扰。<br><br>### 适配器类型<br>- **Houlsby Adapter**: 在每个Transformer层中添加两个适配器模块，分别位于多头注意力层和两个前馈层之后。<br>- **Bapna and Firat Adapter**: 仅在每个Transformer层中的多头注意力层之后添加一个适配器模块。<br>- **Pfeiffer Adapter and AdapterFusion**: 使用AdapterFusion算法来组合不同任务上训练的适配器，从而实现信息共享。<br>- **K-Adapter**: 作为“外部插件”，由多个适配器层组成，可以插入到预训练模型的不同Transformer层中。<br><br>### 知识增强方法<br>- **通用知识**: 利用ConceptNet和DBpedia等知识图谱，将常识和世界知识注入LLMs。<br>- **语言知识**: 将语言知识（如动词意义和论元结构）注入适配器，以提高事件提取和机器翻译等任务的性能。<br>- **领域特定知识**: 利用适配器进行领域适应，例如在医疗保健领域，通过UMLS等知识图谱来增强LLMs。<br><br>## 📈 实验结果<br>基于适配器的KELMs在下游任务中的性能始终优于基础LLMs。例如，DAKI和MoP框架在PubMedQA任务上的准确率分别提高了约7%和8%。此外，适配器调优比常规微调更能缓解遗忘问题。<br><br>## 💬 可借鉴之处<br>- **适配器架构**: 研究人员可以探索更高效的适配器架构，以克服序列数据处理的延迟并实现硬件并行化。<br>- **领域应用**: 除了医疗保健领域，适配器增强的LLMs还可以应用于其他高度结构化的领域，如法律或金融领域。<br>- **任务类型**: 未来可以探索将知识增强应用于生成任务，以提高生成文本的事实性和信息性。<br><br>## 📚 总结<br>本文综述了基于适配器的知识增强语言模型方法，并分析了不同适配器架构和知识增强方法的优缺点。随着研究的不断深入，适配器增强的LLMs有望在更多领域和任务中发挥重要作用。</td>
    </tr>
    <tr>
      <th>66</th>
      <td>Adaptive Multi-Goal Exploration</td>
      <td>We introduce a generic strategy for provably efficient multi-goal<br>exploration. It relies on AdaGoal, a novel goal selection scheme that leverages<br>a measure of uncertainty in reaching states to adaptively target goals that are<br>neither too difficult nor too easy. We show how AdaGoal can be used to tackle<br>the objective of learning an \( \epsilon \)-optimal goal-conditioned policy for the<br>(initially unknown) set of goal states that are reachable within \( L \) steps in<br>expectation from a reference state \( s_0 \) in a reward-free Markov decision<br>process. In the tabular case with \( S \) states and \( A \) actions, our algorithm<br>requires \( \tilde{O}(L^3 S A \epsilon^{-2}) \) exploration steps, which is nearly<br>minimax optimal. We also readily instantiate AdaGoal in linear mixture Markov<br>decision processes, yielding the first goal-oriented PAC guarantee with linear<br>function approximation. Beyond its strong theoretical guarantees, we anchor<br>AdaGoal in goal-conditioned deep reinforcement learning, both conceptually and<br>empirically, by connecting its idea of selecting "uncertain" goals to<br>maximizing value ensemble disagreement.</td>
      <td>## 🌟 论文解读 | 自适应多目标探索：AdaGoal策略<br><br>## 📌 背景痛点/本文动机<br>在强化学习中，当外部奖励信号缺失或不具信息性时，智能体需要通过探索环境来学习，而不是简单地最大化奖励。这种情况下，智能体需要自主设定目标并学习有效地达到这些目标。然而，现有的无监督目标条件强化学习（GC-RL）方法往往缺乏理论支持和保证，尤其是在深度学习环境中。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：多目标探索（MGE）目标<br>本文提出了一个通用的策略，用于证明有效的多目标探索。该策略依赖于AdaGoal，这是一种新颖的目标选择方案，它利用到达状态的不确定性度量来适应性地选择既不太难也不太容易的目标。<br><br>💡 创新点2：AdaGoal目标选择方案<br>AdaGoal通过解决一个简单的优化问题来适应性地选择中等难度的目标状态。它还提供了一个算法停止规则和一个候选目标状态集，智能体对这些状态有信心可以可靠地到达。<br><br>💡 创新点3：AdaGoal-UCBVI和AdaGoal-UCRL·VTR算法<br>本文设计了AdaGoal-UCBVI算法，用于在表格MDP中解决MGE问题，并证明了它几乎是最优的样本复杂度。此外，还设计了AdaGoal-UCRL·VTR算法，用于线性混合MDP，这是第一个具有线性函数近似的面向目标的PAC保证。<br><br>💡 创新点4：AdaGoal在深度GC-RL中的应用<br>本文将AdaGoal的概念与深度GC-RL中的最大化价值集合分歧联系起来，从而在概念和经验上将其锚定在目标条件深度强化学习中。<br><br>## 📈 实验结果<br>在实验中，AdaGoal-UCBVI和AdaGoal-UCRL·VTR算法在表格MDP和线性混合MDP中均表现出良好的性能，证明了其有效性和鲁棒性。<br><br>## 💬 可借鉴之处<br>本文提出的AdaGoal策略为无监督目标条件强化学习提供了一种新的思路，具有以下可借鉴之处：<br><br>*   **自适应目标选择**：AdaGoal通过考虑目标状态的不确定性来选择目标，从而更有效地探索环境。<br>*   **理论保证**：AdaGoal-UCBVI和AdaGoal-UCRL·VTR算法具有几乎最优的样本复杂度，为无监督目标条件强化学习提供了理论支持。<br>*   **深度学习应用**：AdaGoal的概念可以与深度学习技术相结合，从而在复杂的任务中实现有效的目标探索。<br><br>## 🌟 总结<br>本文提出的AdaGoal策略为无监督目标条件强化学习提供了一种新的思路，并取得了显著的成果。AdaGoal策略具有自适应目标选择、理论保证和深度学习应用等优点，为未来的研究提供了重要的参考价值。</td>
    </tr>
    <tr>
      <th>64</th>
      <td>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</td>
      <td>The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by<br>Large Language Models (LLMs) make them promising candidates for developing<br>coordination agents. In this study, we introduce a new LLM-Coordination<br>Benchmark aimed at a detailed analysis of LLMs within the context of Pure<br>Coordination Games, where participating agents need to cooperate for the most<br>gain. This benchmark evaluates LLMs through two distinct tasks: (1)<br>\emph{Agentic Coordination}, where LLMs act as proactive participants for<br>cooperation in 4 pure coordination games; (2) \emph{Coordination Question<br>Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice<br>questions from the 4 games for evaluation of three key reasoning abilities:<br>Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to<br>enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture<br>for Coordination (CAC) framework that can easily integrate different LLMs as<br>plug-and-play modules for pure coordination games. Our findings indicate that<br>LLM agents equipped with GPT-4-turbo achieve comparable performance to<br>state-of-the-art reinforcement learning methods in games that require<br>commonsense actions based on the environment. Besides, zero-shot coordination<br>experiments reveal that, unlike RL methods, LLM agents are robust to new unseen<br>partners. However, results on Coordination QA show a large room for improvement<br>in the Theory of Mind reasoning and joint planning abilities of LLMs. The<br>analysis also sheds light on how the ability of LLMs to understand their<br>environment and their partner's beliefs and intentions plays a part in their<br>ability to plan for coordination. Our code is available at<br>\url{https://github.com/eric-ai-lab/llm_coordination}.</td>
      <td>## 🌟 论文解读 | LLM-Coordination：评估和分析大型语言模型的多智能体协调能力<br><br>## 📌 背景痛点/本文动机<br>在许多日常任务和关键操作中，如烹饪和救援行动，合作是至关重要的。这些场景可以被视为纯协调游戏，其中所有参与方都从选择完全一致的战略中受益，避免任何利益冲突。这些游戏要求代理推理他们的环境并计划，同时考虑他们的伙伴的信念和意图。大型语言模型（LLMs）最近在物理和虚拟环境中的涌现规划能力、令人印象深刻的推理能力和对心智理论的暗示，使它们成为开发协调代理的有希望的候选者。然而，LLMs在协调游戏中的必要条件、优势和局限性仍然不清楚。本文旨在通过进行LLMs的多智能体协调能力的全面评估和分析来弥合这一差距。<br><br>## 🚀 核心方法<br>本文提出了一个新的LLM-Coordination基准，旨在对LLMs在纯协调游戏中的能力进行详细分析。该基准通过两个不同的任务评估LLMs：<br>1. **代理协调**：LLMs作为积极合作参与者参与4个纯协调游戏。<br>2. **协调问答（QA）**：LLMs被提示回答来自4个游戏的198个多项选择题，以评估三个关键推理能力：环境理解、心智理论推理和联合规划。<br><br>此外，为了使LLMs能够进行多智能体协调，本文引入了一个协调认知架构（CAC）框架，该框架可以轻松地将不同的LLMs作为即插即用模块集成到纯协调游戏中。<br><br>## 📈 实验结果<br>实验结果表明，配备GPT-4-turbo的LLM代理在需要基于环境的常识行动的游戏中，其性能与最先进的强化学习方法相当。此外，零样本协调实验表明，与RL方法不同，LLM代理对新未见伙伴具有鲁棒性。然而，协调QA的结果表明，LLMs的心智理论推理和联合规划能力还有很大的改进空间。分析还揭示了LLMs理解其环境和其伙伴的信念和意图的能力如何影响它们协调计划的能力。<br><br>## 💬 可借鉴之处<br>本文提出的LLM-Coordination基准和CAC框架为评估和分析LLMs的多智能体协调能力提供了一个有价值的工具。此外，本文的结果突出了LLMs在协调任务中的优势和局限性，并为未来研究提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>71</th>
      <td>Motif: Intrinsic Motivation from Artificial Intelligence Feedback</td>
      <td>Exploring rich environments and evaluating one's actions without prior<br>knowledge is immensely challenging. In this paper, we propose Motif, a general<br>method to interface such prior knowledge from a Large Language Model (LLM) with<br>an agent. Motif is based on the idea of grounding LLMs for decision-making<br>without requiring them to interact with the environment: it elicits preferences<br>from an LLM over pairs of captions to construct an intrinsic reward, which is<br>then used to train agents with reinforcement learning. We evaluate Motif's<br>performance and behavior on the challenging, open-ended and<br>procedurally-generated NetHack game. Surprisingly, by only learning to maximize<br>its intrinsic reward, Motif achieves a higher game score than an algorithm<br>directly trained to maximize the score itself. When combining Motif's intrinsic<br>reward with the environment reward, our method significantly outperforms<br>existing approaches and makes progress on tasks where no advancements have ever<br>been made without demonstrations. Finally, we show that Motif mostly generates<br>intuitive human-aligned behaviors which can be steered easily through prompt<br>modifications, while scaling well with the LLM size and the amount of<br>information given in the prompt.</td>
      <td>## 🌟 论文解读 | Motif：从人工智能反馈中获取内在动机<br><br>## 📌 背景痛点/本文动机<br>在复杂环境中，没有先验知识的智能体探索和评估其行为极具挑战性。本文提出了一种名为 Motif 的新方法，旨在将大型语言模型 (LLM) 中的先验知识与智能体进行交互，从而帮助智能体在没有与环境的直接交互的情况下进行决策。<br><br>## 🚀 核心方法<br>💡 创新点1：利用 LLM 的偏好构建内在奖励<br>Motif 的核心思想是，通过 LLM 对事件标题的偏好来构建内在奖励函数，并将其用于强化学习训练智能体。LLM 表达对成对事件标题的偏好，这些标题只需粗略描述环境中发生的事件，而不需要精细的逐步描述。LLM 不需要理解低级动作空间，这可能是复合的或连续的。<br><br>💡 创新点2：内在奖励与外在奖励的结合<br>Motif 的内在奖励可以单独使用，也可以与来自环境的奖励信号结合使用。实验表明，当内在奖励与外在奖励结合使用时，Motif 的性能显著优于现有方法，并在没有演示的情况下取得了进展。<br><br>## 📈 实验结果<br>Motif 在 NetHack 学习环境 (NLE) 上进行了评估，这是一个具有挑战性、开放性和程序生成的游戏。结果表明，仅通过学习最大化其内在奖励，Motif 就取得了比直接训练以最大化分数的算法更高的游戏分数。当将 Motif 的内在奖励与环境的奖励相结合时，该方法显著优于现有方法，并在没有演示的情况下取得了进展。<br><br>## 💬 可借鉴之处<br>Motif 为利用 LLM 的先验知识和常识来创建智能体提供了一种通用的方法。它通过将 LLM 的高层次知识与智能体操作的底层传感器运动现实之间的差距，从而有效地将知识提炼出来。Motif 具有可扩展性，可以与更大规模的 LLM 或特定领域的微调 LLM 结合使用，并可以通过提示修改轻松地引导智能体的行为。</td>
    </tr>
    <tr>
      <th>101</th>
      <td>SPRING: Studying the Paper and Reasoning to Play Games</td>
      <td>Open-world survival games pose significant challenges for AI algorithms due<br>to their multi-tasking, deep exploration, and goal prioritization requirements.<br>Despite reinforcement learning (RL) being popular for solving games, its high<br>sample complexity limits its effectiveness in complex open-world games like<br>Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's<br>original academic paper and use the knowledge learned to reason and play the<br>game through a large language model (LLM). Prompted with the LaTeX source as<br>game context and a description of the agent's current observation, our SPRING<br>framework employs a directed acyclic graph (DAG) with game-related questions as<br>nodes and dependencies as edges. We identify the optimal action to take in the<br>environment by traversing the DAG and calculating LLM responses for each node<br>in topological order, with the LLM's answer to final node directly translating<br>to environment actions. In our experiments, we study the quality of in-context<br>"reasoning" induced by different forms of prompts under the setting of the<br>Crafter open-world environment. Our experiments suggest that LLMs, when<br>prompted with consistent chain-of-thought, have great potential in completing<br>sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4<br>outperforms all state-of-the-art RL baselines, trained for 1M steps, without<br>any training. Finally, we show the potential of games as a test bed for LLMs.</td>
      <td>## 🌟 论文解读 | SPRING：通过阅读论文和推理来玩游戏<br><br>## 📌 背景痛点/本文动机<br>开放世界生存游戏对AI算法提出了重大挑战，因为它们需要多任务处理、深度探索和目标优先级排序。尽管强化学习（RL）在解决游戏问题方面很受欢迎，但其高样本复杂度限制了其在像Crafter或Minecraft这样的复杂开放世界游戏中的有效性。<br><br>## 🚀 核心方法<br>SPRING是一种新颖的方法，它通过阅读游戏的原始学术论文并使用从大型语言模型（LLM）中学习到的知识来推理和玩游戏。SPRING框架使用一个有向无环图（DAG），其中游戏相关问题作为节点，依赖关系作为边。通过遍历DAG并按拓扑顺序计算LLM对每个节点的响应，我们可以确定在环境中采取的最佳行动，LLM对最终节点的答案直接转换为环境行动。<br><br>## 📈 实验结果<br>在Crafter开放世界环境中，我们的实验研究了不同形式的提示在上下文中引起的“推理”质量。我们的实验表明，当LLM被提示进行一致的思维链时，它们在完成复杂的高级轨迹方面具有巨大的潜力。定量地说，SPRING与GPT-4的表现优于所有最先进的RL基线，这些基线经过1M步的训练，而SPRING没有经过任何训练。<br><br>## 💬 可借鉴之处<br>SPRING是第一个通过从学术论文中明确提取多个交互和科技树依赖来应对竞争性RL基准的方法。我们是第一个在具有挑战性的开放世界游戏中展示SOTA性能的零样本LLM-based（GPT-4）策略。我们研究了不同提示引起的上下文“推理”质量，并通过DAG中的问题链提出了一种受控的思维链提示，用于决策。</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Embodied LLM Agents Learn to Cooperate in Organized Teams</td>
      <td>Large Language Models (LLMs) have emerged as integral tools for reasoning,<br>planning, and decision-making, drawing upon their extensive world knowledge and<br>proficiency in language-related tasks. LLMs thus hold tremendous potential for<br>natural language interaction within multi-agent systems to foster cooperation.<br>However, LLM agents tend to over-report and comply with any instruction, which<br>may result in information redundancy and confusion in multi-agent cooperation.<br>Inspired by human organizations, this paper introduces a framework that imposes<br>prompt-based organization structures on LLM agents to mitigate these problems.<br>Through a series of experiments with embodied LLM agents and human-agent<br>collaboration, our results highlight the impact of designated leadership on<br>team efficiency, shedding light on the leadership qualities displayed by LLM<br>agents and their spontaneous cooperative behaviors. Further, we harness the<br>potential of LLMs to propose enhanced organizational prompts, via a<br>Criticize-Reflect process, resulting in novel organization structures that<br>reduce communication costs and enhance team efficiency.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的智能体在组织化团队中学习协作<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在推理、规划和决策方面的应用日益广泛，它们在多智能体系统中的协作潜力也逐渐显现。然而，LLM智能体在多智能体协作中存在过度报告和盲目服从指令的问题，这可能导致信息冗余和混乱。为了解决这些问题，本文提出了一种基于提示的组织结构框架，旨在提高LLM智能体在多智能体系统中的协作效率。<br><br>## 🚀 核心方法<br>💡 创新点1：设计了一种新型的多LLM智能体架构，支持≥3个智能体在物理/模拟环境中进行灵活的通信和协作。<br>💡 创新点2：开发了一种基于LLMs的“批评-反思”框架，用于自动优化组织提示，从而生成更有效的组织结构。<br><br>## 📈 实验结果<br>实验结果表明，具有指定领导者的层次化组织结构能够显著提高团队效率，并且LLM智能体能够通过通信动态地选举和调整领导者。此外，通过“批评-反思”框架，LLM智能体能够自发地形成新颖、有效的团队结构，从而降低通信成本并提高团队效率。<br><br>## 💬 可借鉴之处<br>本文提出的基于提示的组织结构框架和“批评-反思”框架为LLM智能体在多智能体系统中的协作提供了新的思路和方法。这些方法可以应用于各种场景，例如自动驾驶网络、无人机群等，以提高智能体系统的协作效率和性能。</td>
    </tr>
    <tr>
      <th>28</th>
      <td>SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models</td>
      <td>Large language models (LLMs) have recently garnered significant<br>accomplishments in various exploratory tasks, even surpassing the performance<br>of traditional reinforcement learning-based methods that have historically<br>dominated the agent-based field. The purpose of this paper is to investigate<br>the efficacy of LLMs in executing real-time strategy war tasks within the<br>StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an<br>embodied agent leveraging LLM for real-time strategy implementation in the<br>StarCraft II game environment. The SwarmBrain comprises two key components: 1)<br>a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed<br>to orchestrate macro-level strategies from a high-level perspective. This<br>matrix emulates the overarching consciousness of the Zerg intelligence brain,<br>synthesizing strategic foresight with the aim of allocating resources,<br>directing expansion, and coordinating multi-pronged assaults. 2) a Swarm<br>ReflexNet, which is agile counterpart to the calculated deliberation of the<br>Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the<br>Swarm ReflexNet employs a condition-response state machine framework, enabling<br>expedited tactical responses for fundamental Zerg unit maneuvers. In the<br>experimental setup, SwarmBrain is in control of the Zerg race in confrontation<br>with an Computer-controlled Terran adversary. Experimental results show the<br>capacity of SwarmBrain to conduct economic augmentation, territorial expansion,<br>and tactical formulation, and it shows the SwarmBrain is capable of achieving<br>victory against Computer players set at different difficulty levels.</td>
      <td>## 🌟 论文解读 | SwarmBrain：基于大型语言模型的实时策略游戏StarCraft II的智能体<br><br>## 📌 背景痛点/本文动机<br>实时策略（RTS）游戏，如《星际争霸II》，因其复杂的战场环境和快速决策的需求，对人工智能（AI）提出了巨大挑战。传统的基于强化学习（RL）的方法在处理这种复杂环境时遇到了困难，尤其是在将高级目标直接映射到低级键盘和鼠标输入方面。而大型语言模型（LLM）因其对复杂语境的高层次抽象和理解能力，在探索性任务中取得了显著成就，但在实时策略游戏中，LLM的推理延迟限制了其直接应用。<br><br>## 🚀 核心方法<br>本文提出了SwarmBrain，一个基于LLM的智能体，用于在《星际争霸II》中执行实时策略任务。SwarmBrain由两个关键组件组成：<br><br>💡 创新点1：Overmind Intelligence Matrix<br>这是一个由最先进的LLM驱动的矩阵，负责从高层次视角协调宏观策略。它模拟了Zerg智能脑的总体意识，结合战略远见，旨在分配资源、指导扩张和协调多方面的攻击。<br><br>💡 创新点2：Swarm ReflexNet<br>这是Overmind Intelligence Matrix的敏捷对应物，采用条件响应状态机框架，为基本的Zerg单位操作提供快速战术响应。由于LLM推理的固有延迟，Swarm ReflexNet能够快速响应，而无需等待LLM的深入思考。<br><br>## 📈 实验结果<br>SwarmBrain在与不同难度级别的计算机对手的对抗中表现出色。在非常容易、容易、中等和中等困难级别，SwarmBrain的成功率为100%。即使在困难级别，SwarmBrain也能在76%的比赛中取得胜利。<br><br>## 💬 可借鉴之处<br>SwarmBrain的设计展示了LLM在实时策略游戏中的应用潜力。通过结合宏观策略规划和快速战术响应，SwarmBrain能够在复杂的游戏环境中取得成功。此外，SwarmBrain的设计也为我们提供了关于如何将LLM应用于其他需要快速决策的领域的见解。</td>
    </tr>
    <tr>
      <th>49</th>
      <td>ADaPT: As-Needed Decomposition and Planning with Language Models</td>
      <td>Large Language Models (LLMs) are increasingly being used for interactive<br>decision-making tasks requiring planning and adapting to the environment.<br>Recent works employ LLMs-as-agents in broadly two ways: iteratively determining<br>the next action (iterative executors) or generating plans and executing<br>sub-tasks using LLMs (plan-and-execute). However, these methods struggle with<br>task complexity, as the inability to execute any sub-task may lead to task<br>failure. To address these shortcomings, we introduce As-Needed Decomposition<br>and Planning for complex Tasks (ADaPT), an approach that explicitly plans and<br>decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute<br>them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity<br>and LLM capability. Our results demonstrate that ADaPT substantially<br>outperforms established strong baselines, achieving success rates up to 28.3%<br>higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel<br>compositional dataset that we introduce. Through extensive analysis, we<br>illustrate the importance of multilevel decomposition and establish that ADaPT<br>dynamically adjusts to the capabilities of the executor LLM as well as to task<br>complexity.</td>
      <td>## 🌟 论文解读 | ADaPT：按需分解与规划，提升大型语言模型在复杂任务中的表现<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们也逐渐被应用于需要规划和适应环境的交互式决策任务中。然而，现有的方法在处理复杂任务时面临着挑战，因为LLMs在执行子任务时可能会失败，从而导致整个任务的失败。<br><br>## 🚀 核心方法<br>为了解决上述问题，本文提出了ADaPT（As-Needed Decomposition and Planning for complex Tasks），一种按需分解和规划复杂任务的方法。ADaPT的核心思想是，当LLM作为执行者无法执行子任务时，将其分解为更小的子任务，并递归地进行分解，以适应任务的复杂性和LLM的能力。<br><br>### 💡 创新点1：按需分解<br>ADaPT通过递归地分解子任务，动态地适应任务的复杂性和LLM的能力。当LLM作为执行者无法执行子任务时，它会调用LLM作为规划者来生成更小的子任务，并递归地调用ADaPT来执行这些子任务。<br><br>### 💡 创新点2：多级分解<br>ADaPT支持多级分解，这意味着它可以进一步分解子任务，直到它们变得足够简单，可以被LLM作为执行者成功执行。这种多级分解的能力使得ADaPT能够处理更复杂的任务，并提高任务的成功率。<br><br>## 📈 实验结果<br>在ALFWorld、WebShop和TextCraft三个数据集上进行的实验结果表明，ADaPT显著优于现有的强基线方法，在ALFWorld上提高了28.3%的成功率，在WebShop上提高了27%，在TextCraft上提高了33%。<br><br>## 💬 可借鉴之处<br>ADaPT提供了一种有效的方法来处理LLMs在复杂任务中的执行失败问题。它通过按需分解和规划，动态地适应任务的复杂性和LLM的能力，从而提高了任务的成功率。此外，ADaPT的多级分解能力使其能够处理更复杂的任务，并提高任务的成功率。</td>
    </tr>
    <tr>
      <th>7</th>
      <td>A Pilot Study on Teacher-Facing Real-Time Classroom Game Dashboards</td>
      <td>Educational games are an increasingly popular teaching tool in modern<br>classrooms. However, the development of complementary tools for teachers<br>facilitating classroom gameplay is lacking. We present the results of a<br>participatory design process for a teacher-facing, real-time game data<br>dashboard. This two-phase process included a workshop to elicit teachers'<br>requirements for such a tool, and a pilot study of our dashboard prototype. We<br>analyze post-gameplay survey and interview data to understand teachers'<br>experiences with the tool in terms of evidence of co-design, feasibility, and<br>effectiveness. Our results indicate the participatory design yielded a tool<br>both useful for and usable by teachers within the context of a real class<br>gameplay session. We advocate for the continued development of data-driven<br>teacher tools to improve the effectiveness of games deployed in the classroom.</td>
      <td>## 🌟 论文解读 | 教育游戏中的实时课堂游戏仪表板：教师视角的探索<br><br>## 📌 背景痛点/本文动机<br>教育游戏在现代课堂中越来越受欢迎，但缺乏辅助教师进行课堂游戏的教学工具。教师通常只能通过查看每个学生的电脑屏幕来了解他们的游戏情况，这不仅耗时，而且可能无法全面了解游戏细节，从而影响教学效果。<br><br>## 🚀 核心方法<br>本文提出了一种参与式设计方法，通过两个阶段的研究来开发一个面向教师的实时游戏数据仪表板：<br>1. **参与式设计工作坊**：与教师和其他利益相关者合作，通过设计活动收集他们对仪表板的需求和期望。<br>2. **原型开发和试点研究**：根据工作坊的结果开发仪表板原型，并在多个课堂环境中进行试点测试，收集教师反馈。<br><br>## 📈 实验结果<br>试点研究表明，参与式设计过程产生了对教师有用且易于使用的工具。教师反馈表明，仪表板帮助他们更好地理解游戏和学生的游戏情况，提高了他们对游戏教学的支持能力。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，参与式设计是开发教师支持工具的有效方法。未来可以进一步扩展仪表板的功能，包括班级概述和总结数据，以提高其可用性和有效性。此外，还可以探索如何将仪表板与其他教育技术工具集成，以更好地支持游戏教学。</td>
    </tr>
    <tr>
      <th>69</th>
      <td>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</td>
      <td>The advent of Large Language Models (LLMs) has paved the way for complex<br>tasks such as role-playing, which enhances user interactions by enabling models<br>to imitate various characters. However, the closed-source nature of<br>state-of-the-art LLMs and their general-purpose training limit role-playing<br>optimization. In this paper, we introduce RoleLLM, a framework to benchmark,<br>elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four<br>stages: (1) Role Profile Construction for 100 roles; (2) Context-Based<br>Instruction Generation (Context-Instruct) for role-specific knowledge<br>extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style<br>imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning<br>open-source models along with role customization. By Context-Instruct and<br>RoleGPT, we create RoleBench, the first systematic and fine-grained<br>character-level benchmark dataset for role-playing with 168,093 samples.<br>Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),<br>significantly enhancing role-playing abilities and even achieving comparable<br>results with RoleGPT (using GPT-4).</td>
      <td>## 🌟 论文解读 | RoleLLM：解锁大型语言模型的角色扮演能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的兴起，角色扮演等复杂任务成为可能，为用户交互提供了更多可能性。然而，现有开源LLMs在角色扮演方面存在优化不足的问题，而最先进的LLMs如GPT-4等则因其闭源性质而限制了其在角色扮演方面的应用。本文旨在解决这些问题，提出RoleLLM框架，以提升LLMs的角色扮演能力。<br><br>## 🚀 核心方法<br>💡 创新点1：Role Profile Construction<br>构建了100个角色的详细档案，包括角色描述、口头禅以及从剧本中提取的对话片段，为角色扮演提供了丰富的背景知识。<br><br>💡 创新点2：Context-Based Instruction Generation (Context-Instruct)<br>利用GPT模型从角色档案中生成高质量的问答对，以提取角色特定的知识和记忆。<br><br>💡 创新点3：Role Prompting using GPT (RoleGPT)<br>通过对话工程和检索增强技术，利用GPT模型生成符合角色说话风格的回答，以模仿角色的说话风格。<br><br>💡 创新点4：Role-Conditioned Instruction Tuning (RoCIT)<br>利用Context-Instruct和RoleGPT生成的数据，对开源LLMs进行微调，以提升其角色扮演能力，并生成RoleLLaMA和RoleGLM等模型。<br><br>## 📈 实验结果<br>实验结果表明，RoleLLM框架在角色扮演方面取得了显著成果。RoleLLaMA和RoleGLM在模仿角色说话风格、回答准确性和角色特定知识掌握方面表现出色，甚至在某些情况下与RoleGPT（使用GPT-4）相当。<br><br>## 💬 可借鉴之处<br>RoleLLM框架为LLMs的角色扮演能力提升提供了新的思路和方法，其创新点包括角色档案构建、基于上下文的指令生成、角色提示和角色条件指令微调等。此外，RoleBench数据集的构建也为角色扮演能力的评估和提升提供了重要的参考。</td>
    </tr>
    <tr>
      <th>80</th>
      <td>Towards Ontology Construction with Language Models</td>
      <td>We present a method for automatically constructing a concept hierarchy for a<br>given domain by querying a large language model. We apply this method to<br>various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can<br>be of considerable help for constructing concept hierarchies.</td>
      <td>## 🌟 论文解读 | 利用大型语言模型构建本体<br><br>## 📌 背景痛点/本文动机<br>本体是领域内概念及其关系的正式表示，是高度结构化的知识。然而，手动构建和编辑本体是一项耗时且成本高昂的工程任务。现有的方法通常需要领域专家的参与，但领域专家的知识和本体工程专业知识往往不在同一人手中。此外，现有的方法通常假设本体的模式（即要使用的概念和属性名称的集合）是预先选择的，然后作为方法的输入提供。然而，当使用大型语言模型（LLM）时，这似乎不是一个好的选择，因为至少有两个原因。首先，为整个领域设计模式本身就是一个非平凡的任务，需要领域专家并涉及许多设计决策。实际上，设计模式和概念层次结构是紧密相连的。其次，LLM的主要优势在于根据用户提供的上下文生成关键词和短语，因此它们是提出给定领域概念和属性名称的完美工具。因此，本文提出了一种基于LLM的本体构建方法，旨在解决上述问题。<br><br>## 🚀 核心方法<br>本文提出了一种基于LLM的本体构建方法，该方法通过查询大型语言模型来自动构建给定领域的概念层次结构。该方法的核心思想是，LLM隐含地包含大量的知识，并且可以像专家一样回答问题，从而帮助构建概念层次结构。具体来说，该方法包括以下几个步骤：<br><br>1. **种子概念选择**：选择一个种子概念，例如“动物”，作为构建概念层次结构的起点。<br>2. **概念层次结构探索**：通过重复询问LLM提供已经存在于层次结构中的概念的相关子概念，并使用遍历算法将新概念放置在层次结构中。<br>3. **概念描述**：询问LLM提供每个概念的文本描述，以便于人类用户理解和解释LLM提出的概念。<br>4. **概念验证**：通过向LLM提出额外的查询来验证LLM的输出，以过滤掉错误的答案。<br>5. **概念插入**：使用KRIS算法将新概念插入到概念层次结构中，并处理子概念/超概念关系和同义词检测。<br><br>## 📈 实验结果<br>本文使用OpenAI的GPT 3.5对各种领域（如动物、饮料、音乐和植物）进行了实验。结果表明，LLM可以有效地帮助构建概念层次结构，尽管仍然存在一些幻觉和错误。通过验证和仔细的提示工程，可以显著减少这些错误。此外，本文还讨论了如何将人类领域专家的交互纳入本体构建过程，以及如何评估构建的本体。<br><br>## 💬 可借鉴之处<br>本文提出的方法为利用LLM构建本体提供了一种新的思路，具有以下可借鉴之处：<br><br>* **利用LLM的知识和生成能力**：LLM可以像专家一样回答问题，并提供相关子概念和概念描述，从而帮助构建概念层次结构。<br>* **概念验证和提示工程**：通过验证和提示工程，可以显著减少LLM的幻觉和错误，提高构建本体的质量。<br>* **人类领域专家的交互**：将人类领域专家的交互纳入本体构建过程，可以帮助解决设计决策和引入“神秘”概念的问题。<br>* **本体评估**：使用手动评估或现有分类法进行本体评估，可以帮助评估构建本体的质量和完整性。<br><br>总而言之，本文提出的方法为利用LLM构建本体提供了一种新的思路，并展示了LLM在构建本体方面的潜力。</td>
    </tr>
    <tr>
      <th>121</th>
      <td>Social Simulacra: Creating Populated Prototypes for Social Computing Systems</td>
      <td>Social computing prototypes probe the social behaviors that may arise in an<br>envisioned system design. This prototyping practice is currently limited to<br>recruiting small groups of people. Unfortunately, many challenges do not arise<br>until a system is populated at a larger scale. Can a designer understand how a<br>social system might behave when populated, and make adjustments to the design<br>before the system falls prey to such challenges? We introduce social simulacra,<br>a prototyping technique that generates a breadth of realistic social<br>interactions that may emerge when a social computing system is populated.<br>Social simulacra take as input the designer's description of a community's<br>design -- goal, rules, and member personas -- and produce as output an instance<br>of that design with simulated behavior, including posts, replies, and<br>anti-social behaviors. We demonstrate that social simulacra shift the behaviors<br>that they generate appropriately in response to design changes, and that they<br>enable exploration of "what if?" scenarios where community members or<br>moderators intervene. To power social simulacra, we contribute techniques for<br>prompting a large language model to generate thousands of distinct community<br>members and their social interactions with each other; these techniques are<br>enabled by the observation that large language models' training data already<br>includes a wide variety of positive and negative behavior on social media<br>platforms. In evaluations, we show that participants are often unable to<br>distinguish social simulacra from actual community behavior and that social<br>computing designers successfully refine their social computing designs when<br>using social simulacra.</td>
      <td>## 🌟 论文解读 | 社交模拟：为社交计算系统创建人口原型<br><br>## 📌 背景痛点/本文动机<br>社交计算系统的设计往往需要考虑大量用户的行为和互动，而传统的原型设计方法通常只能招募少量用户进行测试。然而，许多挑战和问题只有在系统大规模运行时才会出现。因此，设计师很难预测社交系统在人口众多时的行为，也无法在系统出现问题之前进行调整。<br><br>## 🚀 核心方法<br>本文提出了“社交模拟”技术，利用大型语言模型生成大量用户和他们的社交互动，帮助设计师预测和评估社交系统在人口众多时的行为。社交模拟技术包括以下几个关键步骤：<br><br>💡 创新点1：生成多样化的用户角色<br>设计师只需提供少量种子角色，社交模拟技术就可以生成大量非重复但主题相关的用户角色，从而模拟不同用户之间的互动。<br><br>💡 创新点2：生成符合社区目标和规则的互动内容<br>社交模拟技术可以根据设计师提供的社区目标、规则和用户角色，生成符合社区目标和规则的互动内容，包括帖子、回复和反社会行为。<br><br>💡 创新点3：探索“如果”场景<br>社交模拟技术可以帮助设计师探索“如果”场景，例如，如果社区成员或版主进行干预，社交系统会发生什么变化。<br><br>## 📈 实验结果<br>本文通过两个评估实验验证了社交模拟技术的有效性：<br><br>1. 技术评估：参与者无法区分真实社区行为和社交模拟生成的行为，表明社交模拟可以创建可信的内容。<br>2. 设计师评估：社交计算设计师使用社交模拟技术成功改进了他们的社交系统设计，例如，识别未考虑到的积极用例和负面行为，并调整规则和干预策略。<br><br>## 💬 可借鉴之处<br>社交模拟技术为社交计算系统的设计提供了一种新的原型设计方法，可以帮助设计师更好地预测和评估社交系统在人口众多时的行为，从而设计出更安全、更有效的社交系统。此外，社交模拟技术还可以用于探索“如果”场景，帮助设计师更好地理解社交系统的动态变化。</td>
    </tr>
    <tr>
      <th>45</th>
      <td>See and Think: Embodied Agent in Virtual Environment</td>
      <td>Large language models (LLMs) have achieved impressive pro-gress on several<br>open-world tasks. Recently, using LLMs to build embodied agents has been a<br>hotspot. This paper proposes STEVE, a comprehensive and visionary embodied<br>agent in the Minecraft virtual environment. STEVE comprises three key<br>components: vision perception, language instruction, and code action. Vision<br>perception involves interpreting visual information in the environment, which<br>is then integrated into the LLMs component with agent state and task<br>instruction. Language instruction is responsible for iterative reasoning and<br>decomposing complex tasks into manageable guidelines. Code action generates<br>executable skill actions based on retrieval in skill database, enabling the<br>agent to interact effectively within the Minecraft environment. We also collect<br>STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge<br>question-answering pairs, and 200+ skill-code pairs. We conduct continuous<br>block search, knowledge question and answering, and tech tree mastery to<br>evaluate the performance. Extensive experiments show that STEVE achieves at<br>most 1.5x faster unlocking key tech trees and 2.5x quicker in block search<br>tasks.</td>
      <td>## 🌟 论文解读 | See and Think: Embodied Agent in Virtual Environment<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，设计能够表现出智能行为和适应性的智能体一直是人工智能领域的一个长期而重要的挑战。然而，近年来，大型语言模型（LLMs）在开发方面取得了显著进展，展现出其作为多功能、通用型助手的潜力。尽管如此，在许多开放世界环境中，如Minecraft，当代智能体主要使用LLMs进行文本交互。然而，这种对文本通信的依赖限制了它们在这些世界中的交互，包括低级案例。Minecraft要求智能体具备各种技能，从制作基本物品到执行复杂任务。然而，由LLMs驱动的智能体往往产生不可预测的输出。它们交互的有效性在很大程度上取决于精心设计的提示，旨在将LLM的理解与环境的上下文和预期目标相一致。这种提示工程过程不仅费力，而且无法实现培养自主、自我驱动的智能体的目标。此外，文本通信在自然传达某些世界概念方面存在局限性，例如制作配方，这些概念通常通过视觉更有效地传达。<br><br>## 🚀 核心方法<br>💡 创新点1：提出STEVE，一个在虚拟环境中具有视觉感知、语言指令和代码动作的智能体，与之前最先进的方法相比，在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。<br>💡 创新点2：提出STEVE-7B/13B，一系列通过使用Llama-2-7B/13B的Minecraft知识问答对进行微调获得的大型语言模型。<br>💡 创新点3：收集STEVE-21K数据集，包括600多个视觉-环境对、20K个知识问答对和200多个技能-代码对，以证明STEVE的有效性能。<br><br>## 📈 实验结果<br>实验结果表明，STEVE在连续块搜索、知识问答和科技树掌握方面表现出色。与AutoGPT和Voyager等基线方法相比，STEVE在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。此外，STEVE在知识问答任务中表现出色，其性能优于Llama-2和GPT-4等更广泛的模型。<br><br>## 💬 可借鉴之处<br>本文提出的STEVE框架为构建具有视觉感知和语言指令能力的智能体提供了一个有价值的参考。此外，STEVE-21K数据集为研究人员提供了进行多模态学习研究的有用资源。</td>
    </tr>
    <tr>
      <th>50</th>
      <td>Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</td>
      <td>The automatic evaluation of LLM-based agent intelligence is critical in<br>developing advanced LLM-based agents. Although considerable effort has been<br>devoted to developing human-annotated evaluation datasets, such as AlpacaEval,<br>existing techniques are costly, time-consuming, and lack adaptability. In this<br>paper, inspired by the popular language game ``Who is Spy'', we propose to use<br>the word guessing game to assess the intelligence performance of LLMs. Given a<br>word, the LLM is asked to describe the word and determine its identity (spy or<br>not) based on its and other players' descriptions. Ideally, an advanced agent<br>should possess the ability to accurately describe a given word using an<br>aggressive description while concurrently maximizing confusion in the<br>conservative description, enhancing its participation in the game. To this end,<br>we first develop DEEP to evaluate LLMs' expression and disguising abilities.<br>DEEP requires LLM to describe a word in aggressive and conservative modes. We<br>then introduce SpyGame, an interactive multi-agent framework designed to assess<br>LLMs' intelligence through participation in a competitive language-based board<br>game. Incorporating multi-agent interaction, SpyGame requires the target LLM to<br>possess linguistic skills and strategic thinking, providing a more<br>comprehensive evaluation of LLMs' human-like cognitive abilities and<br>adaptability in complex communication situations. The proposed evaluation<br>framework is very easy to implement. We collected words from multiple sources,<br>domains, and languages and used the proposed evaluation framework to conduct<br>experiments. Extensive experiments demonstrate that the proposed DEEP and<br>SpyGame effectively evaluate the capabilities of various LLMs, capturing their<br>ability to adapt to novel situations and engage in strategic communication.</td>
      <td>## 🌟 论文解读 | 利用猜词游戏评估大型语言模型的智能<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）如ChatGPT、GPT-4和Bard等在各个任务中展现出惊人的性能，开发基于LLMs的智能代理变得越来越重要。然而，现有的评估LLMs智能的方法存在两个主要问题：1）人工标注成本高，耗时且缺乏可扩展性和适应性；2）无法全面反映LLMs的智能。为了解决这些问题，本文提出了一种新的评估方法，即利用猜词游戏来评估LLMs的智能。<br><br>## 🚀 核心方法<br>💡 创新点1：DEEP框架<br>本文首先提出了DEEP框架，用于评估LLMs的表达和伪装能力。DEEP要求LLMs以激进和保守两种模式描述一个给定的词，并利用GPT-4来判断这些描述是否准确。激进模式要求LLMs提供清晰、详细和准确的描述，而保守模式则要求LLMs提供模糊的描述以伪装目标词。<br><br>💡 创新点2：SpyGame框架<br>本文还提出了SpyGame框架，这是一个交互式多智能体框架，旨在通过参与竞争性语言游戏“谁是卧底”来评估LLMs的智能。SpyGame要求LLMs具备语言技能和战略思维能力，从而更全面地评估LLMs在复杂沟通情境中的人类认知能力和适应性。<br><br>## 📈 实验结果<br>本文对四种开源LLMs和两种闭源LLMs进行了实验，结果表明，闭源LLMs（如GPT-4和GPT-3.5）在激进和保守模式下的表现明显优于开源模型。此外，SpyGame框架能够有效地评估LLMs在多智能体交互中的能力，捕捉它们适应新情况并进行战略沟通的能力。<br><br>## 💬 可借鉴之处<br>本文提出的DEEP和SpyGame框架为评估LLMs的智能提供了一种新的方法，具有以下可借鉴之处：<br>1. 利用游戏进行评估，更具互动性和趣味性。<br>2. 关注LLMs的表达和伪装能力，更全面地评估其智能。<br>3. SpyGame框架支持人类参与，更贴近真实场景。<br>4. 针对多智能体交互中的偏差问题，提出了有效的解决方案。<br><br>总而言之，本文提出的评估方法为LLMs的智能评估提供了新的思路，有助于推动LLMs的发展和应用。</td>
    </tr>
    <tr>
      <th>122</th>
      <td>Inner Monologue: Embodied Reasoning through Planning with Language Models</td>
      <td>Recent works have shown how the reasoning capabilities of Large Language<br>Models (LLMs) can be applied to domains beyond natural language processing,<br>such as planning and interaction for robots. These embodied problems require an<br>agent to understand many semantic aspects of the world: the repertoire of<br>skills available, how these skills influence the world, and how changes to the<br>world map back to the language. LLMs planning in embodied environments need to<br>consider not just what skills to do, but also how and when to do them - answers<br>that change over time in response to the agent's own choices. In this work, we<br>investigate to what extent LLMs used in such embodied contexts can reason over<br>sources of feedback provided through natural language, without any additional<br>training. We propose that by leveraging environment feedback, LLMs are able to<br>form an inner monologue that allows them to more richly process and plan in<br>robotic control scenarios. We investigate a variety of sources of feedback,<br>such as success detection, scene description, and human interaction. We find<br>that closed-loop language feedback significantly improves high-level<br>instruction completion on three domains, including simulated and real table top<br>rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen<br>environment in the real world.</td>
      <td>## 🌟 论文解读 | 内心独白：通过语言模型规划实现具身推理<br><br>## 📌 背景痛点/本文动机<br>随着人工智能的发展，机器人需要具备更高级的推理能力，以便在复杂的环境中执行任务。传统的机器人规划方法往往依赖于优化或符号推理，缺乏对世界语义知识的理解。而大型语言模型（LLMs）在自然语言处理方面表现出色，并展现出丰富的世界知识。本文旨在探索LLMs在具身环境中的推理能力，并研究如何利用环境反馈来提高机器人的规划能力。<br><br>## 🚀 核心方法<br>本文提出了“内心独白”框架，通过将环境反馈与LLMs相结合，实现机器人对复杂任务的推理和规划。具体来说，本文的核心方法包括：<br><br>💡 创新点1：利用环境反馈形成“内心独白”<br>通过将环境反馈（如成功检测、场景描述、人类交互等）不断注入LLMs的规划语言提示中，机器人可以形成“内心独白”，从而更丰富地处理和规划机器人控制场景。<br><br>💡 创新点2：多种反馈来源<br>本文研究了多种反馈来源，包括成功检测、场景描述和人类交互。通过将这些反馈与LLMs相结合，机器人可以更好地理解环境，并根据反馈进行重新规划。<br><br>## 📈 实验结果<br>本文在模拟和真实世界的机器人平台上进行了实验，结果表明，利用环境反馈的“内心独白”框架可以显著提高机器人执行高级指令的能力。在模拟的桌面整理任务和真实的厨房移动操作任务中，与基线方法相比，本文提出的方法在任务完成率方面取得了显著的提升。<br><br>## 💬 可借鉴之处<br>本文提出的“内心独白”框架为机器人规划提供了一种新的思路，具有以下可借鉴之处：<br><br>*   利用LLMs的语义知识进行推理和规划<br>*   通过环境反馈实现机器人对复杂任务的适应性<br>*   结合多种反馈来源提高机器人的规划能力<br><br>## 🌟 总结<br>本文提出的“内心独白”框架为机器人规划提供了一种新的思路，通过利用LLMs的语义知识和环境反馈，机器人可以更好地理解环境并执行复杂任务。未来，随着LLMs和感知技术的发展，相信“内心独白”框架将在机器人领域发挥更大的作用。</td>
    </tr>
    <tr>
      <th>24</th>
      <td>What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</td>
      <td>This study introduces "CosmoAgent," an innovative artificial intelligence<br>system that utilizes Large Language Models (LLMs) to simulate complex<br>interactions between human and extraterrestrial civilizations. This paper<br>introduces a mathematical model for quantifying the levels of civilization<br>development and further employs a state transition matrix approach to evaluate<br>their trajectories. Through this methodology, our study quantitatively analyzes<br>the growth trajectories of civilizations, providing insights into future<br>decision-making at critical points of growth and saturation. Furthermore, this<br>paper acknowledges the vast diversity of potential living conditions across the<br>universe, which could foster unique cosmologies, ethical codes, and worldviews<br>among different civilizations. Recognizing the Earth-centric bias inherent in<br>current LLM designs, we propose the novel concept of using LLM agents with<br>diverse ethical paradigms and simulating interactions between entities with<br>distinct moral principles. This innovative research not only introduces a novel<br>method for comprehending potential inter-civilizational dynamics but also holds<br>practical value in enabling entities with divergent value systems to<br>strategize, prevent conflicts, and engage in games under conditions of<br>asymmetric information. The accompanying code is available at<br>https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.</td>
      <td>## 🌟 论文解读 | 用大型语言模型模拟外星文明：探索宇宙中的互动与冲突<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，其在模拟复杂社会动态方面的潜力日益凸显。然而，现有的LLMs设计往往带有地球中心主义的偏见，难以全面模拟外星文明的多样性和独特性。本文旨在通过引入具有不同伦理范式和道德原则的LLM代理，模拟人类与外星文明之间的复杂互动，从而为理解潜在星际动态提供新的视角。<br><br>## 🚀 核心方法<br>💡 创新点1：CosmoAgent多智能体系统<br>本文提出了CosmoAgent，一个基于LLMs的多智能体系统，用于模拟宇宙中不同文明之间的互动。CosmoAgent通过模拟文明的决策过程，包括选择隐藏、战斗或合作，来探索文明发展的轨迹和潜在冲突。<br><br>💡 创新点2：文明发展模型<br>本文引入了一个数学模型来量化文明的发展水平，并使用状态转移矩阵方法来评估文明的轨迹。该模型考虑了五个关键资源：军事能力、技术发展、生产能力、消费和储存，以及不同文明的世界观（和平主义、军国主义和孤立主义）对决策的影响。<br><br>💡 创新点3：信息不对称的模拟<br>为了模拟宇宙中文明之间的互动，本文考虑了信息不对称的情况，即文明之间的观测数据滞后于实际发展。LLMs代理需要根据过时的信息做出决策，这增加了模拟的复杂性和现实性。<br><br>💡 创新点4：道德多样性的模拟<br>本文提出了使用具有不同伦理范式的LLM代理来模拟具有不同道德原则的实体之间的互动。这有助于理解不同文明如何共存，以及道德框架如何影响星际互动。<br><br>## 📈 实验结果<br>实验结果表明，具有军国主义世界观的文明倾向于对较弱文明发动攻击，而孤立主义文明则更倾向于在观察一段时间后选择性地与其他文明合作。此外，信息不对称会延迟冲突的发生，为较弱文明提供了反击的机会。<br><br>## 💬 可借鉴之处<br>本文的研究不仅为理解潜在星际动态提供了新的视角，还为解决具有不同价值体系的实体之间的冲突提供了策略。此外，本文的研究方法可以应用于其他领域，如模拟古代社会、人类文明模式和社会生态系统。</td>
    </tr>
    <tr>
      <th>76</th>
      <td>MindAgent: Emergent Gaming Interaction</td>
      <td>Large Language Models (LLMs) have the capacity of performing complex<br>scheduling in a multi-agent system and can coordinate these agents into<br>completing sophisticated tasks that require extensive collaboration. However,<br>despite the introduction of numerous gaming frameworks, the community has<br>insufficient benchmarks towards building general multi-agents collaboration<br>infrastructure that encompass both LLM and human-NPCs collaborations. In this<br>work, we propose a novel infrastructure - MindAgent - to evaluate planning and<br>coordination emergent capabilities for gaming interaction. In particular, our<br>infrastructure leverages existing gaming framework, to i) require understanding<br>of the coordinator for a multi-agent system, ii) collaborate with human players<br>via un-finetuned proper instructions, and iii) establish an in-context learning<br>on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new<br>gaming scenario and related benchmark that dispatch a multi-agent collaboration<br>efficiency and supervise multiple agents playing the game simultaneously. We<br>conduct comprehensive evaluations with new auto-metric CoS for calculating the<br>collaboration efficiency. Finally, our infrastructure can be deployed into<br>real-world gaming scenarios in a customized VR version of CUISINEWORLD and<br>adapted in existing broader Minecraft gaming domain. We hope our findings on<br>LLMs and the new infrastructure for general-purpose scheduling and coordination<br>can help shed light on how such skills can be obtained by learning from large<br>language corpora.</td>
      <td>## 🌟 论文解读 | MindAgent：大型语言模型在游戏交互中的涌现式规划与协调能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在各个领域的应用日益广泛，其在多智能体系统中的规划与协调能力也逐渐受到关注。然而，现有的游戏框架和基准测试还不足以评估LLMs在游戏交互中的涌现式规划与协调能力，尤其是在LLMs与人类NPCs协作的场景下。本文旨在解决这个问题，提出了一种名为MindAgent的新型基础设施，用于评估LLMs在游戏交互中的规划与协调能力。<br><br>## 🚀 核心方法<br>💡 创新点1：CUISINEWORLD游戏场景与基准测试<br>本文设计了一个名为CUISINEWORLD的游戏场景，模拟了一个虚拟厨房环境，其中多智能体系统需要协调多个代理，完成尽可能多的菜肴订单。CUISINEWORLD游戏场景具有多种任务结构和难度，是评估LLMs涌现式多智能体规划能力的理想测试平台。<br><br>💡 创新点2：MindAgent基础设施<br>MindAgent是一个用于LLMs交互式多智能体规划的基础设施，它展示了LLMs的涌现式多智能体规划能力，并引入了多种提示技术，以促进LLMs的规划能力，包括提供少量示例、规划理由和环境反馈。<br><br>## 📈 实验结果<br>本文在CUISINEWORLD游戏场景中进行了广泛的实验，结果表明：<br>1. 零样本多智能体规划：强大的预训练LLMs（如GPT-4）能够通过阅读简单的游戏指令和食谱，调度多个代理（2到4个）完成菜肴，甚至与人类玩家协作。<br>2. 基于高级提示的规划：通过利用涌现式上下文学习能力，可以显著提高LLMs的多智能体规划性能。例如，添加少量专家演示、解释某些行动的理由，以及在规划过程中提供实时反馈。<br>3. 通用潜力：LLMs表现出成为通用多智能体规划器的巨大潜力，因为它能够通过少量示例泛化到更多代理，并适应新的游戏领域，如Minecraft。<br><br>## 💬 可借鉴之处<br>本文提出的MindAgent基础设施和CUISINEWORLD游戏场景为评估LLMs在游戏交互中的涌现式规划与协调能力提供了新的思路和方法。此外，本文的研究结果也表明，LLMs在多智能体规划方面具有巨大的潜力，有望在未来推动游戏AI的发展。</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Software Agents Interaction Algorithms in Virtual Learning Environment</td>
      <td>This paper highlights the multi-agent learning virtual environment and agents<br>communication algorithms. The researcher proposed three algorithms required<br>software agents interaction in virtual learning information system environment.<br>The first proposed algorithm is agents interaction localization algorithm, the<br>second one is the dynamic agents distribution algorithm (load distribution<br>algorithm), and the third model is Agent communication algorithm based on using<br>agents intermediaries. The main objectives of these algorithms are to reduce<br>the response time for any agents changes in virtual learning environment (VLE)<br>by increasing the information exchange intensity between software agents and<br>reduce the overall network load, and to improve the communication between<br>mobile agents in distributed information system to support effectiveness.<br>Finally the paper describe the algorithms of information exchange between<br>mobile agents in VLE based on the expansion of the address structure and the<br>use of an agent, intermediary agents, matchmaking agents, brokers and their<br>entrepreneurial functions</td>
      <td>## 🌟 论文解读 | 虚拟学习环境中软件代理交互算法的革新<br><br>## 📌 背景痛点/本文动机<br>随着信息技术的飞速发展，虚拟学习环境（VLE）已经成为教育领域的重要工具。然而，现有的VLE系统在处理大量信息交互时，面临着网络负载增加、信息交换效率低下等问题。为了解决这些问题，本文提出了一种基于软件代理的交互算法，旨在提高VLE系统的效率和响应速度。<br><br>## 🚀 核心方法<br>💡 创新点1：代理交互定位算法<br>该算法通过分析代理之间的通信依赖性，将频繁交互的代理分组到同一主机上，从而将跨主机的交互转化为主机内的交互，减少网络负载并提高信息交换效率。<br><br>💡 创新点2：动态代理分配算法（负载分配算法）<br>该算法通过监控主机负载，将代理分组并动态分配到不同的主机上，以实现负载均衡，避免某些主机过载，从而提高系统的稳定性和性能。<br><br>💡 创新点3：基于代理中介的通信模型<br>该模型利用代理中介（如经纪人代理和配对代理）来促进代理之间的通信。代理中介可以帮助代理查找具有相似兴趣的代理，并提供消息转发和匹配服务，从而提高通信效率和灵活性。<br><br>## 📈 实验结果<br>本文提出的算法在虚拟学习环境中进行了实验验证，结果表明，这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。<br><br>## 💬 可借鉴之处<br>本文提出的算法和模型为虚拟学习环境的设计和优化提供了新的思路和方法。其中，代理交互定位算法和动态代理分配算法可以应用于其他分布式系统中，以提高系统的效率和性能。基于代理中介的通信模型可以应用于其他多代理系统中，以促进代理之间的协作和通信。<br><br>## 📚 总结<br>本文提出的基于软件代理的交互算法为虚拟学习环境的设计和优化提供了新的思路和方法。这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。本文的研究成果对于虚拟学习环境和其他分布式系统的设计和优化具有重要的参考价值。</td>
    </tr>
    <tr>
      <th>51</th>
      <td>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</td>
      <td>Agents built with large language models (LLMs) have shown great potential<br>across a wide range of domains. However, in complex decision-making tasks, pure<br>LLM-based agents tend to exhibit intrinsic bias in their choice of actions,<br>which is inherited from the model's training data and results in suboptimal<br>performance. To develop strategic language agents, i.e., agents that generate<br>flexible language actions and possess strong decision-making abilities, we<br>propose a novel framework that powers LLM-based agents with reinforcement<br>learning (RL). We consider Werewolf, a popular social deduction game, as a<br>challenging testbed that emphasizes versatile communication and strategic<br>gameplay. To mitigate the intrinsic bias in language actions, our agents use an<br>LLM to perform deductive reasoning and generate a diverse set of action<br>candidates. Then an RL policy trained to optimize the decision-making ability<br>chooses an action from the candidates to play in the game. Extensive<br>experiments show that our agents overcome the intrinsic bias and outperform<br>existing LLM-based agents in the Werewolf game. We also conduct human-agent<br>experiments and find that our agents achieve human-level performance and<br>demonstrate strong strategic play.</td>
      <td>## 🌟 论文解读 | 语言智能体在狼人杀游戏中的战略决策<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在构建智能体方面的广泛应用，其在复杂决策任务中表现出的内在偏差问题逐渐凸显。这种偏差源于模型训练数据，导致LLM-based智能体在战略决策方面表现不佳。为了解决这个问题，本文提出了一种新的框架，将LLM与强化学习（RL）相结合，以构建具有灵活语言行动和强大决策能力的战略语言智能体。<br><br>## 🚀 核心方法<br>💡 创新点1：隐藏角色推理<br>本文使用LLM对游戏中的信息进行分类，区分真伪，并推断每个玩家的隐藏角色，为后续决策提供基础。<br><br>💡 创新点2：多样化行动生成<br>为了克服LLM的内在偏差，本文提出了一种多样化行动生成方法，通过提示LLM生成一系列行动候选者，从而避免固定模式并提高决策的灵活性。<br><br>💡 创新点3：基于群体的强化学习训练<br>本文采用基于群体的强化学习训练方法，通过训练一个RL策略来优化行动候选者的分布，并通过与各种智能体进行对抗来提高策略的鲁棒性。<br><br>## 📈 实验结果<br>本文在狼人杀游戏中进行了广泛的实验，结果表明，与现有的LLM-based智能体相比，本文提出的战略语言智能体能够克服内在偏差，并在游戏中表现出更强的战略决策能力。此外，与人类玩家的对局实验也表明，本文提出的智能体能够达到人类水平的游戏表现。<br><br>## 💬 可借鉴之处<br>本文提出的框架为构建具有强大决策能力的战略语言智能体提供了一种新的思路，其核心方法可以应用于其他需要灵活语言行动和战略决策的场景。此外，本文提出的多样化行动生成方法和基于群体的强化学习训练方法也为解决LLM内在偏差问题提供了新的思路。</td>
    </tr>
    <tr>
      <th>90</th>
      <td>Building Cooperative Embodied Agents Modularly with Large Language Models</td>
      <td>In this work, we address challenging multi-agent cooperation problems with<br>decentralized control, raw sensory observations, costly communication, and<br>multi-objective tasks instantiated in various embodied environments. While<br>previous research either presupposes a cost-free communication channel or<br>relies on a centralized controller with shared observations, we harness the<br>commonsense knowledge, reasoning ability, language comprehension, and text<br>generation prowess of LLMs and seamlessly incorporate them into a<br>cognitive-inspired modular framework that integrates with perception, memory,<br>and execution. Thus building a Cooperative Embodied Language Agent CoELA, who<br>can plan, communicate, and cooperate with others to accomplish long-horizon<br>tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA<br>driven by GPT-4 can surpass strong planning-based methods and exhibit emergent<br>effective communication. Though current Open LMs like LLAMA-2 still<br>underperform, we fine-tune a CoELA with data collected with our agents and show<br>how they can achieve promising performance. We also conducted a user study for<br>human-agent interaction and discovered that CoELA communicating in natural<br>language can earn more trust and cooperate more effectively with humans. Our<br>research underscores the potential of LLMs for future research in multi-agent<br>cooperation. Videos can be found on the project website<br>https://vis-www.cs.umass.edu/Co-LLM-Agents/.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型构建协作型具身智能体<br><br>## 📌 背景痛点/本文动机<br>在现实世界中，人类擅长与他人合作和沟通以解决复杂任务。然而，构建能够与人类或其他智能体协作的具身智能体仍然是一个具有挑战性的任务，因为它们需要具备感知、部分观察、长期规划、自然语言沟通等复杂能力。<br><br>## 🚀 核心方法<br>本文提出了一个名为 CoELA 的协作型具身语言智能体，它利用大型语言模型 (LLM) 的常识知识、推理能力、语言理解和文本生成能力，并将其集成到一个认知启发的模块化框架中，该框架与感知、记忆和执行模块相结合。<br><br>CoELA 框架包括五个关键模块：<br>1. **感知模块**：用于感知环境中的原始感官观察并提取有用信息。<br>2. **记忆模块**：模拟人类的长期记忆，存储智能体对世界和其他智能体的理解和经验。<br>3. **通信模块**：利用 LLM 的强大对话生成和理解能力，决定发送什么信息。<br>4. **规划模块**：利用 LLM 的强大推理能力，根据所有可用信息做出决策，并制定高级计划。<br>5. **执行模块**：根据规划模块生成的计划，生成可执行的低级动作。<br><br>## 📈 实验结果<br>在 C-WAH 和 TDW-MAT 两个具身环境中进行的实验表明，CoELA 能够感知复杂观察、推理世界和其他智能体的状态、有效沟通，并相应地制定长期计划。CoELA 驱动的 GPT-4 能够超越基于规划的强方法，并表现出有效的沟通。尽管当前的开放 LLM（如 LLAMA-2）仍然表现不佳，但作者通过在具身环境中收集的数据对 CoELA 进行微调，并展示了其有希望的性能。<br><br>## 💬 可借鉴之处<br>1. **模块化框架**：CoELA 的模块化框架可以有效地将 LLM 的能力与感知、记忆和执行模块相结合，从而构建协作型具身智能体。<br>2. **自然语言沟通**：CoELA 使用自然语言进行沟通，使其能够更好地与人类或其他智能体协作。<br>3. **微调 LLM**：作者通过在具身环境中收集的数据对 CoELA 进行微调，并展示了其有希望的性能，这为构建更好的协作型具身智能体提供了新的思路。<br><br>## 🌈 未来展望<br>CoELA 的研究结果表明，LLM 在多智能体协作领域具有巨大的潜力。未来研究可以探索以下方向：<br>1. **多模态 LLM**：开发能够有效处理视觉模态并流畅生成自然语言的 LLM，以更好地利用 3D 空间信息。<br>2. **低级控制**：开发能够直接进行低级控制的智能体，以更好地理解低级动作的执行。<br>3. **复杂推理**：开发具有更强指令遵循和推理能力的 LLM，以提高智能体的鲁棒性。<br><br>总而言之，CoELA 为构建协作型具身智能体提供了一个有希望的方向，并为未来研究开辟了新的可能性。</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Will GPT-4 Run DOOM?</td>
      <td>We show that GPT-4's reasoning and planning capabilities extend to the 1993<br>first-person shooter Doom. This large language model (LLM) is able to run and<br>play the game with only a few instructions, plus a textual<br>description--generated by the model itself from screenshots--about the state of<br>the game being observed. We find that GPT-4 can play the game to a passable<br>degree: it is able to manipulate doors, combat enemies, and perform pathing.<br>More complex prompting strategies involving multiple model calls provide better<br>results. While further work is required to enable the LLM to play the game as<br>well as its classical, reinforcement learning-based counterparts, we note that<br>GPT-4 required no training, leaning instead on its own reasoning and<br>observational capabilities. We hope our work pushes the boundaries on<br>intelligent, LLM-based agents in video games. We conclude by discussing the<br>ethical implications of our work.</td>
      <td>## 🌟 论文解读 | GPT-4能否玩转经典射击游戏Doom？<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）如GPT-4的出现，它们在理解和执行指令方面的能力得到了显著提升。然而，LLM在复杂环境中的推理和规划能力仍然是一个未解之谜。本文旨在探索LLM在视频游戏中的智能代理应用，特别是评估GPT-4在经典射击游戏Doom中的表现。<br><br>## 🚀 核心方法<br>💡 创新点1：视觉输入与文本描述的结合<br>本文提出了一种新的方法，将GPT-4的视觉输入能力与文本描述相结合，以理解游戏状态。GPT-4V模型从游戏截图中生成游戏状态的文本描述，而GPT-4模型则根据这些描述和之前的行动历史来做出决策。<br><br>💡 创新点2：多层次规划策略<br>为了提高GPT-4在游戏中的表现，本文提出了多种规划策略，包括简单的指令、分步的关卡攻略、更细粒度的计划生成，以及基于多个专家意见的k-levels策略。这些策略旨在为GPT-4提供更多的上下文信息，以增强其推理和规划能力。<br><br>## 📈 实验结果<br>实验结果表明，GPT-4能够在一定程度上玩转Doom游戏，能够执行开门、战斗敌人、路径规划等基本操作。然而，GPT-4的推理深度有限，缺乏长期规划和记忆能力，例如，当敌人离开视野时，模型会忘记它们的存在。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，LLM在视频游戏中的应用具有巨大的潜力，但仍面临一些挑战。未来研究可以探索更精细的规划策略，以及如何提高LLM的推理深度和记忆能力。此外，本文也提醒我们，LLM技术的快速发展需要更加谨慎的评估和监管，以防止潜在的滥用风险。</td>
    </tr>
    <tr>
      <th>47</th>
      <td>MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration</td>
      <td>Large Language Models (LLMs) have significantly advanced natural language<br>processing, demonstrating exceptional reasoning, tool usage, and memory<br>capabilities. As their applications expand into multi-agent environments, there<br>arises a need for a comprehensive evaluation framework that captures LLMs'<br>reasoning, planning, collaboration, and other social abilities. This work<br>introduces a novel competition-based benchmark framework specifically designed<br>to assess LLMs within multi-agent settings, providing quantitative metrics to<br>evaluate their judgment, reasoning, deception, self-awareness, cooperation,<br>coordination, and rationality. We utilize two social deduction games alongside<br>three game-theory scenarios to create diverse environments. Our frame is<br>fortified with the probabilistic graphic modeling (PGM) method, enhancing the<br>LLMs' capabilities in navigating complex social and cognitive dimensions. We<br>evaluate seven LLMs, quantitatively highlighting a significant capability gap<br>of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.<br>It also confirms that our PGM enhancement boosts the abilities of all selected<br>models by an average of 37%. Our data and code can be found here<br>https://github.com/cathyxl/MAgIC.</td>
      <td>## 🌟 论文解读 | MAgIC：大型语言模型在认知、适应性、理性和协作中的多智能体研究<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的显著进步，它们在推理、工具使用和记忆能力方面展现出卓越的能力。然而，当LLMs的应用扩展到多智能体环境时，需要一种全面的评估框架来捕捉LLMs的推理、规划、协作和其他社交能力。本文提出了一个基于竞争的基准框架，旨在评估LLMs在多智能体环境中的表现，并提供定量指标来评估它们的判断、推理、欺骗、自我意识、合作、协调和理性。<br><br>## 🚀 核心方法<br>💡 创新点1：竞争基准框架<br>本文提出了一个基于竞争的基准框架，通过将LLMs置于多智能体场景中的竞争中，评估它们在多智能体系统中的真实能力。该框架包括两个社交推理游戏和三个博弈论场景，以创建多样化的环境。<br><br>💡 创新点2：概率图模型（PGM）增强<br>为了增强LLMs在复杂社交和认知维度中的导航能力，本文将概率图模型（PGM）方法与LLMs相结合。PGM能够描述随机变量之间的依赖关系，从而帮助LLMs更好地理解全局信息，并做出更明智的决策。<br><br>## 📈 实验结果<br>本文评估了七个LLMs，包括GPT-o1、GPT-4、GPT-3.5-turbo、PaLM 2、Claude 2、Cohere和Llama-2-70B。结果表明，GPT-o1在所有评估维度中表现最佳，而Llama-2-70B表现最差。此外，PGM增强方法将所有选定模型的平均能力提高了37%。<br><br>## 💬 可借鉴之处<br>本文提出的竞争基准框架和PGM增强方法为评估和提升LLMs在多智能体环境中的能力提供了有价值的工具。此外，本文的研究结果为LLMs在多智能体系统中的应用提供了重要的参考和指导。</td>
    </tr>
    <tr>
      <th>42</th>
      <td>Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation</td>
      <td>Building embodied agents on integrating Large Language Models (LLMs) and<br>Reinforcement Learning (RL) have revolutionized human-AI interaction:<br>researchers can now leverage language instructions to plan decision-making for<br>open-ended tasks. However, existing research faces challenges in meeting the<br>requirement of open-endedness. They typically either train LLM/RL models to<br>adapt to a fixed counterpart, limiting exploration of novel skills and<br>hindering the efficacy of human-AI interaction. To this end, we present<br>OpenPAL, a co-training framework comprising two stages: (1) fine-tuning a<br>pre-trained LLM to translate human instructions into goals for planning, and<br>goal-conditioned training a policy for decision-making; (2) co-training to<br>align the LLM and policy, achieving instruction open-endedness. We conducted<br>experiments using Contra, an open-ended FPS game, demonstrating that an agent<br>trained with OpenPAL not only comprehends arbitrary instructions but also<br>exhibits efficient execution. These results suggest that OpenPAL holds the<br>potential to construct open-ended embodied agents in practical scenarios.</td>
      <td>## 🌟 论文解读 | OpenPAL：构建开放式的具身智能体<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）和强化学习（RL）的兴起，构建能够与人类进行交互的具身智能体成为可能。然而，现有的研究在实现开放性方面面临挑战，它们通常训练LLM/RL模型以适应固定的对手，限制了探索新技能的能力，并阻碍了人机交互的有效性。<br><br>## 🚀 核心方法<br>OpenPAL是一个两阶段的协同训练框架，旨在实现双向适应，从而构建开放式的具身智能体。<br><br>💡 创新点1：多步微调<br>OpenPAL首先通过多步微调预训练的LLM，使其能够将人类指令翻译成规划目标，并训练一个目标条件策略来进行决策。<br><br>💡 创新点2：协同训练<br>在第二阶段，OpenPAL通过协同训练来对齐LLM和策略，实现指令的开放性。具体来说，OpenPAL交替执行以下两个过程：<br>1. 使用代理反馈强化学习（RLAF）对LLM进行训练，奖励LLM生成能够被代理执行的指令。<br>2. 使用LLM生成的目标对策略进行目标条件强化学习（GCRL）训练。<br><br>## 📈 实验结果<br>OpenPAL在Contra（一个开放式的FPS游戏）上进行了实验，结果表明，使用OpenPAL训练的智能体不仅能够理解任意指令，而且能够高效地执行这些指令。<br><br>## 💬 可借鉴之处<br>OpenPAL为构建开放式的具身智能体提供了一种有效的方法，其多步微调和协同训练的设计值得借鉴。此外，OpenPAL在Contra上的成功应用表明，该方法在复杂场景下的人机交互中具有潜力。</td>
    </tr>
    <tr>
      <th>94</th>
      <td>OMNI: Open-endedness via Models of human Notions of Interestingness</td>
      <td>Open-ended algorithms aim to learn new, interesting behaviors forever. That<br>requires a vast environment search space, but there are thus infinitely many<br>possible tasks. Even after filtering for tasks the current agent can learn<br>(i.e., learning progress), countless learnable yet uninteresting tasks remain<br>(e.g., minor variations of previously learned tasks). An Achilles Heel of<br>open-endedness research is the inability to quantify (and thus prioritize)<br>tasks that are not just learnable, but also \( \textit{interesting} \) (e.g.,<br>worthwhile and novel). We propose solving this problem by<br>\( \textit{Open-endedness via Models of human Notions of Interestingness} \)<br>(OMNI). The insight is that we can utilize foundation models (FMs) as a model<br>of interestingness (MoI), because they \( \textit{already} \) internalize human<br>concepts of interestingness from training on vast amounts of human-generated<br>data, where humans naturally write about what they find interesting or boring.<br>We show that FM-based MoIs improve open-ended learning by focusing on tasks<br>that are both learnable \( \textit{and interesting} \), outperforming baselines<br>based on uniform task sampling or learning progress alone. This approach has<br>the potential to dramatically advance the ability to intelligently select which<br>tasks to focus on next (i.e., auto-curricula), and could be seen as AI<br>selecting its own next task to learn, facilitating self-improving AI and<br>AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/</td>
      <td>## 🌟 论文解读 | OMNI：基于人类兴趣概念的开放性学习<br><br>## 📌 背景痛点/本文动机<br>开放性学习算法旨在让AI像人类一样，在复杂多变的环境中不断学习新技能。然而，面对无限可能的学习任务，如何选择哪些任务进行学习成为一大挑战。现有的方法，如基于学习进度的自动课程学习，容易陷入重复或无趣的任务中，无法有效引导AI进行有意义的学习。<br><br>## 🚀 核心方法（介绍本文的几个创新点）<br>💡 创新点1：OMNI框架<br>OMNI框架通过结合学习进度模型和人类兴趣模型，实现了对学习任务的智能筛选。学习进度模型负责识别当前AI能够学习的任务，而人类兴趣模型则负责评估这些任务是否有趣，从而确保AI能够专注于有意义的学习。<br><br>💡 创新点2：利用预训练基础模型<br>OMNI框架利用预训练的基础模型（如GPT-3和GPT-4）作为人类兴趣模型，从而避免了手动定义兴趣指标的困难。这些基础模型在大量人类生成的数据上进行训练，已经内化了人类对有趣事物的概念，能够有效地评估任务的有趣程度。<br><br>## 📈 实验结果<br>OMNI框架在Crafter、BabyAI和AI2-THOR等环境中进行了实验，结果表明，与基于均匀任务采样或仅基于学习进度的方法相比，OMNI框架能够显著提高AI的学习效率，学习更多有趣且具有挑战性的任务。<br><br>## 💬 可借鉴之处<br>OMNI框架为开放性学习算法提供了新的思路，其核心思想可以应用于其他领域，例如：<br>* **多模态模型**：将视觉-语言模型等融入人类兴趣模型，以更全面地评估任务的有趣程度。<br>* **自主学习**：让人类兴趣模型能够自主分析任务成功率等指标，并据此调整对有趣程度的评估。<br>* **安全性与价值对齐**：通过引入人类反馈或AI反馈，确保OMNI框架能够选择符合人类价值观和期望的任务进行学习。<br><br>## 🌈 未来展望<br>OMNI框架为开放性学习算法的发展开辟了新的方向，未来可以进一步探索以下方向：<br>* **更复杂的任务空间**：将OMNI框架应用于更复杂的任务空间，例如虚拟现实环境或真实世界环境。<br>* **更智能的人类兴趣模型**：开发更智能的人类兴趣模型，使其能够更好地理解人类对有趣事物的概念，并能够根据AI的学习进度动态调整评估标准。<br>* **安全性与价值对齐**：探索更有效的方法，确保OMNI框架能够选择符合人类价值观和期望的任务进行学习，从而避免潜在的安全风险。</td>
    </tr>
    <tr>
      <th>36</th>
      <td>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception</td>
      <td>It is a long-lasting goal to design an embodied system that can solve<br>long-horizon open-world tasks in human-like ways. However, existing approaches<br>usually struggle with compound difficulties caused by the logic-aware<br>decomposition and context-aware execution of these tasks. To this end, we<br>introduce MP5, an open-ended multimodal embodied system built upon the<br>challenging Minecraft simulator, which can decompose feasible sub-objectives,<br>design sophisticated situation-aware plans, and perform embodied action<br>control, with frequent communication with a goal-conditioned active perception<br>scheme. Specifically, MP5 is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs), and the system is modulated into functional<br>modules that can be scheduled and collaborated to ultimately solve pre-defined<br>context- and process-dependent tasks. Extensive experiments prove that MP5 can<br>achieve a 22% success rate on difficult process-dependent tasks and a 91%<br>success rate on tasks that heavily depend on the context. Moreover, MP5<br>exhibits a remarkable ability to address many open-ended tasks that are<br>entirely novel.</td>
      <td>## 🌟 论文解读 | MP5：基于主动感知的多模态开放式具身系统<br><br>## 📌 背景痛点/本文动机<br>在人工智能领域，设计一个能够以人类方式解决开放世界长时任务的具身系统一直是长期目标。然而，现有的方法通常难以应对这些任务中逻辑感知分解和上下文感知执行所带来的复合困难。为了解决这个问题，本文提出了MP5，一个基于Minecraft模拟器的开放式多模态具身系统，它能够分解可行的子目标，设计复杂的情境感知计划，并执行具身动作控制，同时与目标条件下的主动感知方案进行频繁通信。<br><br>## 🚀 核心方法<br>💡 创新点1：MP5基于最新的多模态大型语言模型（MLLMs）构建，并将系统分解为可调度和协作的功能模块，以解决预定义的上下文和过程依赖任务。<br>💡 创新点2：MP5包括一个主动感知方案，通过感知器与巡逻器之间的多轮交互，主动感知观察到的图像中的上下文信息，以解决上下文依赖任务。<br><br>## 📈 实验结果<br>MP5在困难的过程依赖任务上实现了22%的成功率，在高度依赖上下文的任务上实现了91%的成功率。此外，MP5表现出解决许多完全新颖的开放式任务的能力。<br><br>## 💬 可借鉴之处<br>MP5的设计和实现为解决开放世界长时任务提供了新的思路和方法，其主动感知方案和模块化设计对于开发更智能、更灵活的具身系统具有重要的参考价值。</td>
    </tr>
    <tr>
      <th>105</th>
      <td>ArK: Augmented Reality with Knowledge Interactive Emergent Ability</td>
      <td>Despite the growing adoption of mixed reality and interactive AI agents, it<br>remains challenging for these systems to generate high quality 2D/3D scenes in<br>unseen environments. The common practice requires deploying an AI agent to<br>collect large amounts of data for model training for every new task. This<br>process is costly, or even impossible, for many domains. In this study, we<br>develop an infinite agent that learns to transfer knowledge memory from general<br>foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene<br>understanding and generation in the physical or virtual world. The heart of our<br>approach is an emerging mechanism, dubbed Augmented Reality with Knowledge<br>Inference Interaction (ArK), which leverages knowledge-memory to generate<br>scenes in unseen physical world and virtual reality environments. The knowledge<br>interactive emergent ability (Figure 1) is demonstrated as the observation<br>learns i) micro-action of cross-modality: in multi-modality models to collect a<br>large amount of relevant knowledge memory data for each interaction task (e.g.,<br>unseen scene understanding) from the physical reality; and ii) macro-behavior<br>of reality-agnostic: in mix-reality environments to improve interactions that<br>tailor to different characterized roles, target variables, collaborative<br>information, and so on. We validate the effectiveness of ArK on the scene<br>generation and editing tasks. We show that our ArK approach, combined with<br>large foundation models, significantly improves the quality of generated 2D/3D<br>scenes, compared to baselines, demonstrating the potential benefit of<br>incorporating ArK in generative AI for applications such as metaverse and<br>gaming simulation.</td>
      <td>## 🌟 论文解读 | ArK：基于知识的增强现实交互式涌现能力<br><br>## 📌 背景痛点/本文动机<br>随着混合现实和交互式AI代理的日益普及，这些系统在生成高质量2D/3D场景方面仍然面临挑战。传统的做法需要部署AI代理来收集大量数据以进行模型训练，这对于许多领域来说既昂贵又不可能。本文旨在解决这一难题，通过开发一个无限代理，该代理能够从通用基础模型（如GPT4、DALLE）中学习，并将知识记忆转移到新领域或场景中，以实现物理或虚拟世界中的场景理解和生成。<br><br>## 🚀 核心方法<br>💡 创新点1：ArK机制<br>本文的核心是ArK机制，即“增强现实与知识推理交互”。该机制利用知识记忆来生成未见过的物理世界和虚拟现实环境中的场景。ArK机制通过观察学习实现：<br>- 跨模态的微动作：在多模态模型中，为每个交互任务（例如，未见过的场景理解）从物理现实中收集大量相关的知识记忆数据。<br>- 现实无关的宏行为：在混合现实环境中，通过调整交互以适应不同的角色、目标变量、协作信息等，从而提高交互效果。<br><br>💡 创新点2：无限知识记忆代理<br>本文开发了一个无限知识记忆代理，用于物理世界中的场景理解和生成。该代理学习从通用基础模型中转移知识记忆，并能够理解和生成未见过的场景。此外，该代理还能够通过强化学习和模仿学习来提高其性能。<br><br>## 📈 实验结果<br>本文在场景生成和编辑任务上验证了ArK的有效性。实验结果表明，与基线相比，ArK方法结合大型基础模型显著提高了生成的2D/3D场景的质量，证明了将ArK纳入生成式AI中的潜在益处，例如在元宇宙和游戏模拟中的应用。<br><br>## 💬 可借鉴之处<br>本文提出的ArK机制和无限知识记忆代理为生成式AI领域提供了新的思路和方法。ArK机制能够有效地利用知识记忆来生成高质量的2D/3D场景，而无限知识记忆代理则能够通过学习和推理来提高其性能。这些创新点为生成式AI的发展和应用提供了新的可能性。</td>
    </tr>
    <tr>
      <th>120</th>
      <td>Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors</td>
      <td>Video game testing requires game-specific knowledge as well as common sense<br>reasoning about the events in the game. While AI-driven agents can satisfy the<br>first requirement, it is not yet possible to meet the second requirement<br>automatically. Therefore, video game testing often still relies on manual<br>testing, and human testers are required to play the game thoroughly to detect<br>bugs. As a result, it is challenging to fully automate game testing. In this<br>study, we explore the possibility of leveraging the zero-shot capabilities of<br>large language models for video game bug detection. By formulating the bug<br>detection problem as a question-answering task, we show that large language<br>models can identify which event is buggy in a sequence of textual descriptions<br>of events from a game. To this end, we introduce the GameBugDescriptions<br>benchmark dataset, which consists of 167 buggy gameplay videos and a total of<br>334 question-answer pairs across 8 games. We extensively evaluate the<br>performance of six models across the OPT and InstructGPT large language model<br>families on our benchmark dataset. Our results show promising results for<br>employing language models to detect video game bugs. With the proper prompting<br>technique, we could achieve an accuracy of 70.66%, and on some video games, up<br>to 78.94%. Our code, evaluation data and the benchmark can be found on<br>https://asgaardlab.github.io/LLMxBugs</td>
      <td>## 🌟 论文解读 | 大型语言模型在零样本视频游戏漏洞检测中的潜力<br><br>## 📌 背景痛点/本文动机<br>视频游戏测试需要游戏特定的知识和对游戏事件的常识推理。虽然 AI 驱动的代理可以满足第一个要求，但自动满足第二个要求仍然不可能。因此，视频游戏测试通常仍然依赖于手动测试，需要人类测试者彻底地玩游戏来检测漏洞。这使得完全自动化游戏测试具有挑战性。<br><br>## 🚀 核心方法<br>💡 创新点1：将漏洞检测问题表述为问答任务，利用大型语言模型的零样本能力来识别游戏事件序列中的漏洞事件。<br>💡 创新点2：引入 GameBugDescriptions 基准数据集，包含 167 个有漏洞的游戏玩法视频和 334 个问答对，涵盖 8 个游戏。<br>💡 创新点3：在基准数据集上评估了 OPT 和 InstructGPT 大型语言模型家族的六个模型的性能。<br>💡 创新点4：分析了语言模型对不同事件描述的鲁棒性。<br><br>## 📈 实验结果<br>实验结果表明，大型语言模型在视频游戏漏洞检测方面具有很大的潜力。通过适当的提示技术，可以实现 70.66% 的准确率，在某些视频游戏中甚至可以达到 78.94%。<br><br>## 💬 可借鉴之处<br>这篇论文展示了大型语言模型在视频游戏漏洞检测方面的潜力，为自动化游戏测试提供了新的思路。此外，论文中提出的 GameBugDescriptions 基准数据集可以用于评估和比较不同语言模型在漏洞检测任务上的性能。</td>
    </tr>
    <tr>
      <th>118</th>
      <td>Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models</td>
      <td>In recent years, much progress has been made in learning robotic manipulation<br>policies that follow natural language instructions. Such methods typically<br>learn from corpora of robot-language data that was either collected with<br>specific tasks in mind or expensively re-labelled by humans with rich language<br>descriptions in hindsight. Recently, large-scale pretrained vision-language<br>models (VLMs) like CLIP or ViLD have been applied to robotics for learning<br>representations and scene descriptors. Can these pretrained models serve as<br>automatic labelers for robot data, effectively importing Internet-scale<br>knowledge into existing datasets to make them useful even for tasks that are<br>not reflected in their ground truth annotations? To accomplish this, we<br>introduce Data-driven Instruction Augmentation for Language-conditioned control<br>(DIAL): we utilize semi-supervised language labels leveraging the semantic<br>understanding of CLIP to propagate knowledge onto large datasets of unlabelled<br>demonstration data and then train language-conditioned policies on the<br>augmented datasets. This method enables cheaper acquisition of useful language<br>descriptions compared to expensive human labels, allowing for more efficient<br>label coverage of large-scale datasets. We apply DIAL to a challenging<br>real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations<br>do not contain crowd-sourced language annotations. DIAL enables imitation<br>learning policies to acquire new capabilities and generalize to 60 novel<br>instructions unseen in the original dataset.</td>
      <td>## 🌟 论文解读 | 利用视觉语言模型增强指令，实现机器人技能获取<br><br>## 📌 背景痛点/本文动机<br>随着深度学习的发展，机器人控制策略已经能够通过自然语言指令进行学习。然而，这些方法通常依赖于大量标注过的机器人语言数据，这些数据要么是针对特定任务收集的，要么是事后由人类进行昂贵标注的。为了解决这个问题，本文提出了一种名为DIAL（Data-driven Instruction Augmentation for Language-conditioned control）的方法，利用预训练的视觉语言模型（VLM）自动为机器人数据添加标签，从而将互联网规模的知识引入现有数据集，使其即使在真实标签有限的情况下也能用于各种任务。<br><br>## 🚀 核心方法<br>💡 创新点1：利用预训练的VLM进行指令增强<br>DIAL方法首先在包含少量人工标注的自然语言描述的小型数据集上微调VLM，然后使用微调后的VLM为更大的未标注演示数据集生成替代指令。这些指令可以包含更丰富的语义概念，例如空间概念或不同的表述方式。<br><br>💡 创新点2：基于行为克隆的语言条件策略训练<br>在生成替代指令后，DIAL方法使用行为克隆在原始和重新标注的数据集上训练语言条件策略。这种方法可以更有效地覆盖大规模数据集，并使机器人能够理解和执行新的指令。<br><br>## 📈 实验结果<br>在真实世界的机器人操作领域，DIAL方法在80,000个演示中，只有3.5%包含众包语言注释的情况下，通过大规模研究超过1,300次真实世界评估，发现DIAL使模仿学习策略能够获得新的能力，并推广到60个原始数据集中未见的全新指令。<br><br>## 💬 可借鉴之处<br>DIAL方法提供了一种廉价且自动化的选项，可以从离线控制数据集中提取额外的语义知识。它通过利用预训练的VLM自动为机器人数据添加标签，从而将互联网规模的知识引入现有数据集，使其即使在真实标签有限的情况下也能用于各种任务。这种方法可以更有效地覆盖大规模数据集，并使机器人能够理解和执行新的指令。</td>
    </tr>
    <tr>
      <th>2</th>
      <td>VideoGameBunny: Towards vision assistants for video games</td>
      <td>Large multimodal models (LMMs) hold substantial promise across various<br>domains, from personal assistance in daily tasks to sophisticated applications<br>like medical diagnostics. However, their capabilities have limitations in the<br>video game domain, such as challenges with scene understanding, hallucinations,<br>and inaccurate descriptions of video game content, especially in open-source<br>models. This paper describes the development of VideoGameBunny, a LLaVA-style<br>model based on Bunny, specifically tailored for understanding images from video<br>games. We release intermediate checkpoints, training logs, and an extensive<br>dataset comprising 185,259 video game images from 413 titles, along with<br>389,565 image-instruction pairs that include image captions, question-answer<br>pairs, and a JSON representation of 16 elements of 136,974 images. Our<br>experiments show that our high quality game-related data has the potential to<br>make a relatively small model outperform the much larger state-of-the-art model<br>LLaVa-1.6-34b (which has more than 4x the number of parameters). Our study<br>paves the way for future research in video game understanding on tasks such as<br>playing, commentary, and debugging. Code and data are available at<br>https://videogamebunny.github.io/</td>
      <td>## 🌟 论文解读 | 视频游戏助手：VideoGameBunny<br><br>## 📌 背景痛点/本文动机<br>随着视频游戏产业的蓬勃发展，大型多模态模型（LMMs）在游戏领域的应用潜力巨大。然而，现有的LMMs在理解游戏内容方面存在局限性，例如场景理解困难、幻觉现象以及游戏内容的描述不准确。本文旨在解决这些问题，通过开发一个专门针对游戏内容理解的模型，即VideoGameBunny，来提升LMMs在游戏领域的表现。<br><br>## 🚀 核心方法<br>💡 创新点1：VideoGameBunny模型<br>VideoGameBunny是一个基于Bunny模型的大型多模态模型，经过专门针对游戏内容的微调，使其能够更好地理解和处理游戏图像。该模型采用了LLaVA-style架构，通过多层感知器（MLP）将视觉嵌入与语言模型相结合，从而实现图像和文本的融合。<br><br>💡 创新点2：游戏相关数据集<br>为了训练VideoGameBunny，本文构建了一个包含185,259张游戏图像和389,565个图像-指令对的数据集。这些图像来自413款不同的游戏，涵盖了各种游戏类型、图形风格和游戏机制。图像-指令对包括图像标题、问答对和图像的JSON表示，为模型提供了丰富的游戏相关数据。<br><br>## 📈 实验结果<br>实验结果表明，VideoGameBunny在游戏内容理解任务上取得了显著的性能提升。与现有的SOTA模型LLaVa-1.6-34b相比，VideoGameBunny在游戏相关问答基准测试中取得了更高的准确率。此外，实验还发现，图像-to-JSON数据集对模型性能的提升最为显著，而短标题数据集则可能对模型性能产生负面影响。<br><br>## 💬 可借鉴之处<br>VideoGameBunny的研究成果为游戏内容理解领域提供了重要的参考。通过构建专门针对游戏内容的数据集和模型，可以有效地提升LMMs在游戏领域的表现。此外，本文还探讨了不同类型的数据集和混合策略对模型性能的影响，为未来研究提供了有价值的指导。</td>
    </tr>
    <tr>
      <th>114</th>
      <td>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</td>
      <td>We investigate the challenge of task planning for multi-task embodied agents<br>in open-world environments. Two main difficulties are identified: 1) executing<br>plans in an open-world environment (e.g., Minecraft) necessitates accurate and<br>multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla<br>planners do not consider how easy the current agent can achieve a given<br>sub-task when ordering parallel sub-goals within a complicated plan, the<br>resulting plan could be inefficient or even infeasible. To this end, we propose<br>"\( \underline{D} \)escribe, \( \underline{E} \)xplain, \( \underline{P} \)lan and<br>\( \underline{S} \)elect" (\( \textbf{DEPS} \)), an interactive planning approach based<br>on Large Language Models (LLMs). DEPS facilitates better error correction on<br>initial LLM-generated \( \textit{plan} \) by integrating \( \textit{description} \) of<br>the plan execution process and providing self-\( \textit{explanation} \) of<br>feedback when encountering failures during the extended planning phases.<br>Furthermore, it includes a goal \( \textit{selector} \), which is a trainable<br>module that ranks parallel candidate sub-goals based on the estimated steps of<br>completion, consequently refining the initial plan. Our experiments mark the<br>milestone of the first zero-shot multi-task agent that can robustly accomplish<br>70+ Minecraft tasks and nearly double the overall performances. Further testing<br>reveals our method's general effectiveness in popularly adopted non-open-ended<br>domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and<br>exploratory studies detail how our design beats the counterparts and provide a<br>promising update on the \( \texttt{ObtainDiamond} \) grand challenge with our<br>approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的交互式规划，助力开放世界多任务智能体<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，多任务智能体面临着两大挑战：1）执行计划需要精确的多步推理，因为任务具有长期性；2）传统的规划器在排序复杂的计划中的并行子目标时，没有考虑当前智能体完成给定子任务的难易程度，导致生成的计划可能效率低下甚至不可行。<br><br>## 🚀 核心方法<br>本文提出了“描述、解释、规划和选择”（DEPS）的交互式规划方法，基于大型语言模型（LLMs）来解决上述挑战。<br><br>💡 创新点1：描述、解释和规划<br>DEPS 通过集成计划执行过程的描述和提供自我解释的反馈，更好地纠正初始 LLM 生成的计划中的错误。当遇到失败时，描述器会总结当前情况并发送给 LLM，LLM 作为解释器定位错误，然后根据描述器和解释器的信息更新计划。<br><br>💡 创新点2：目标选择器<br>DEPS 包含一个可训练的目标选择器模块，该模块根据完成每个并行候选子目标的估计步骤对它们进行排序，从而细化初始计划。选择器使用预测剩余时间步数来完成每个目标，并根据当前状态选择最接近的目标。<br><br>## 📈 实验结果<br>实验结果表明，DEPS 在开放世界环境（如 Minecraft）中取得了显著的成果，能够稳健地完成 70 多个任务，并且整体性能几乎翻倍。此外，DEPS 在非开放世界环境（如 ALFWorld 和桌面操作）中也表现出良好的效果。<br><br>## 💬 可借鉴之处<br>DEPS 的交互式规划方法为开放世界多任务智能体的开发提供了新的思路。通过集成描述、解释和规划，以及使用目标选择器，DEPS 能够生成更可靠和高效的计划，从而提高智能体在开放世界环境中的任务完成能力。</td>
    </tr>
    <tr>
      <th>79</th>
      <td>Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis</td>
      <td>Since the introduction of ChatGPT and GPT-4, these models have been tested<br>across a large number of tasks. Their adeptness across domains is evident, but<br>their aptitude in playing games, and specifically their aptitude in the realm<br>of poker has remained unexplored. Poker is a game that requires decision making<br>under uncertainty and incomplete information. In this paper, we put ChatGPT and<br>GPT-4 through the poker test and evaluate their poker skills. Our findings<br>reveal that while both models display an advanced understanding of poker,<br>encompassing concepts like the valuation of starting hands, playing positions<br>and other intricacies of game theory optimal (GTO) poker, both ChatGPT and<br>GPT-4 are NOT game theory optimal poker players.<br>  Profitable strategies in poker are evaluated in expectations over large<br>samples. Through a series of experiments, we first discover the characteristics<br>of optimal prompts and model parameters for playing poker with these models.<br>Our observations then unveil the distinct playing personas of the two models.<br>We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This<br>exploration then sheds light on the divergent poker tactics of the two models:<br>ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker<br>vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which<br>means that it has a propensity to only engage with premium hands and folds a<br>majority of hands. When subjected to the same directive, GPT-4 plays like a<br>maniac, showcasing a loose and aggressive style of play. Both strategies,<br>although relatively advanced, are not game theory optimal.</td>
      <td>## 🌟 论文解读 | ChatGPT 和 GPT-4 在德州扑克中的表现：一场前翻牌分析<br><br>## 📌 背景痛点/本文动机<br>随着 ChatGPT 和 GPT-4 的推出，这些模型在各种任务中表现出色，但在游戏，尤其是扑克游戏方面的能力尚未得到充分探索。扑克是一种需要在不完整信息和不确定性下做出决策的游戏，因此本文旨在评估 ChatGPT 和 GPT-4 在德州扑克中的表现，特别是它们在前翻牌阶段的决策能力。<br><br>## 🚀 核心方法<br>本文通过一系列实验，评估了 ChatGPT 和 GPT-4 在德州扑克前翻牌阶段的决策能力。实验中，研究人员使用了不同的提示和模型参数，以探索两种模型在扑克游戏中的最佳表现。他们还分析了两种模型的独特游戏风格，并比较了它们与游戏理论最优（GTO）策略的差异。<br><br>## 📈 实验结果<br>实验结果表明，ChatGPT 和 GPT-4 都对扑克游戏有深入的理解，包括起手牌的估值、游戏位置和其他游戏理论最优策略的细节。然而，两种模型都不是游戏理论最优的扑克玩家。ChatGPT 倾向于保守的游戏风格，只参与优质牌局，而 GPT-4 则表现出更加激进的游戏风格，参与更多的牌局。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，尽管 ChatGPT 和 GPT-4 在扑克游戏方面表现出色，但它们仍然存在局限性。这些模型在理解游戏理论最优策略方面存在偏差，这可能是由于它们在训练过程中没有针对扑克游戏进行专门训练。此外，本文的研究结果也表明，不同的提示和模型参数对模型在扑克游戏中的表现有显著影响。因此，未来的研究可以探索如何通过优化提示和模型参数来提高模型在扑克游戏中的表现。</td>
    </tr>
    <tr>
      <th>56</th>
      <td>LLaMA Rider: Spurring Large Language Models to Explore the Open World</td>
      <td>Recently, various studies have leveraged Large Language Models (LLMs) to help<br>decision-making and planning in environments, and try to align the LLMs'<br>knowledge with the world conditions. Nonetheless, the capacity of LLMs to<br>continuously acquire environmental knowledge and adapt in an open world remains<br>uncertain. In this paper, we propose an approach to spur LLMs to explore the<br>open world, gather experiences, and learn to improve their task-solving<br>capabilities. In this approach, a multi-round feedback-revision mechanism is<br>utilized to encourage LLMs to actively select appropriate revision actions<br>guided by feedback information from the environment. This facilitates<br>exploration and enhances the model's performance. Besides, we integrate<br>sub-task relabeling to assist LLMs in maintaining consistency in sub-task<br>planning and help the model learn the combinatorial nature between tasks,<br>enabling it to complete a wider range of tasks through training based on the<br>acquired exploration experiences. By evaluation in Minecraft, an open-ended<br>sandbox world, we demonstrate that our approach LLaMA-Rider enhances the<br>efficiency of the LLM in exploring the environment, and effectively improves<br>the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k<br>instances of collected data, showing minimal training costs compared to the<br>baseline using reinforcement learning.</td>
      <td>## 🌟 论文解读 | LLaMA Rider：激发大型语言模型探索开放世界<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在模拟人类智能方面取得了显著进展。许多研究开始利用LLMs的能力来帮助智能体在环境中进行决策，并发现LLMs具有一定的规划和完成任务的能力。然而，LLMs的知识来源于预训练时使用的语言语料库，可能与特定环境存在差异。为了将LLMs与实际环境相结合，一些研究通过提示工程设计特定机制，为LLMs提供环境信息。然而，LLMs在环境中并不会改进或获取新知识。此外，对于更复杂的任务，需要更复杂的机制和提示，这会导致LLMs生成成本高，并且依赖于像GPT-4这样具有足够知识的强大模型。还有一些研究通过微调来将LLMs与实际环境相结合，但这通常需要依赖于特定任务的训练数据集。强化学习（RL）方法也被研究，但这些方法将LLMs训练为特定任务的策略，并且我们发现RL方法难以扩展到更大的模型或更复杂的任务。<br><br>## 🚀 核心方法<br>本文提出了一种名为LLaMA-Rider的方法，旨在通过LLMs在开放环境中的探索来增强其能力。LLaMA-Rider是一个两阶段的学习框架，包括探索阶段和学习阶段。<br><br>### 💡 创新点1：探索阶段<br>在探索阶段，LLaMA-Rider利用反馈-修正机制来鼓励LLMs主动选择适当的修正动作，以适应环境。LLMs在环境中进行探索，收集经验，并通过反馈信息来改进其决策。此外，LLaMA-Rider还使用子任务重标记来帮助LLMs保持子任务规划的连贯性，并学习任务之间的组合性质。<br><br>### 💡 创新点2：学习阶段<br>在学习阶段，LLaMA-Rider将收集到的经验处理成数据集，并使用监督微调（SFT）来训练LLMs。除了从成功任务中获得的经验外，LLaMA-Rider还收集部分完成的子任务的经验，因为有些任务在探索阶段很难完成。开放环境中的许多任务通常具有组合性，这意味着过去任务的经验可以经常帮助完成其他任务。LLaMA-Rider使用子任务重标记来提高数据利用率，并帮助LLMs学习任务之间的组合性。<br><br>## 📈 实验结果<br>本文在Minecraft模拟器MineDojo上评估了LLaMA-Rider方法。实验结果表明，LLaMA-Rider能够有效地探索环境，并通过微调仅使用1.3k个收集到的数据实例来提高LLMs完成任务的能力，与使用强化学习的方法相比，训练成本更低。<br><br>## 💬 可借鉴之处<br>LLaMA-Rider方法为LLMs在开放环境中的探索和学习提供了一种有效的方法。其反馈-修正机制和子任务重标记技术可以帮助LLMs更好地适应环境，并提高其完成任务的能力。此外，LLaMA-Rider方法还可以扩展到其他开放环境，并具有终身探索和学习的潜力。</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Stay Focused: Problem Drift in Multi-Agent Debate</td>
      <td>Multi-agent debate - multiple instances of large language models discussing<br>problems in turn-based interaction - has shown promise for solving knowledge<br>and reasoning tasks. However, these methods show limitations, particularly when<br>scaling them to longer reasoning chains. In this study, we unveil a new issue<br>of multi-agent debate: discussions drift away from the initial problem over<br>multiple turns. We define this phenomenon as problem drift and quantify its<br>presence across ten tasks (i.e., three generative, three knowledge, three<br>reasoning, and one instruction-following task). To identify the reasons for<br>this issue, we perform a human study with eight experts on discussions<br>suffering from problem drift, who find the most common issues are a lack of<br>progress (35% of cases), low-quality feedback (26% of cases), and a lack of<br>clarity (25% of cases). To systematically address the issue of problem drift,<br>we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem<br>drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of<br>problem drift cases. Our study can be seen as a first step to understanding a<br>key limitation of multi-agent debate, highlighting pathways for improving their<br>effectiveness in the future.</td>
      <td>## 🌟 论文解读 | 多智能体辩论中的问题漂移：识别与缓解<br><br>## 📌 背景痛点/本文动机<br>多智能体辩论作为一种新兴的AI技术，通过多个大型语言模型（LLMs）进行轮换式交互，展现出在解决知识和推理任务方面的潜力。然而，随着辩论链的延长，这些方法在扩展性方面存在局限性。本文揭示了多智能体辩论中的一个新问题：在多轮交互中，讨论逐渐偏离初始问题。这种现象被称为“问题漂移”，本文旨在量化问题漂移的存在，并探索其背后的原因。<br><br>## 🚀 核心方法<br>💡 创新点1：问题漂移的量化<br>本文提出了一个名为FOCUS的指标，用于衡量多轮交互中任务性能的衰减程度。通过FOCUS指标，可以识别出哪些讨论出现了问题漂移，并量化其影响程度。<br><br>💡 创新点2：问题漂移的检测与缓解<br>为了在测试时检测问题漂移，本文提出了DRIFTJudge方法，该方法基于LLM-as-a-judge，通过评估连续轮次的解决方案来判断是否存在问题漂移。此外，本文还提出了DRIFTPolicy方法，通过向参与辩论的智能体提供针对性的反馈，以减少问题漂移的发生。<br><br>## 📈 实验结果<br>本文在十个任务（包括生成、知识、推理和指令遵循任务）上进行了实验，发现问题漂移普遍存在，尤其是在生成任务中。通过人类专家的研究，本文确定了导致问题漂移的八个主要原因，包括缺乏进展、低质量反馈、缺乏清晰度等。实验结果表明，DRIFTPolicy方法可以有效减少31%的问题漂移案例。<br><br>## 💬 可借鉴之处<br>本文的研究揭示了多智能体辩论中的一个关键局限性，并为提高其有效性提供了新的思路。未来研究可以探索智能体探索新想法的作用，并比较人类和智能体辩论之间的内在差异。此外，本文提出的DRIFTJudge和DRIFTPolicy方法可以为其他多智能体系统提供参考，以检测和缓解问题漂移现象。</td>
    </tr>
    <tr>
      <th>39</th>
      <td>A Framework for Complementary Companion Character Behavior in Video Games</td>
      <td>We propose a game development framework capable of governing the behavior of<br>complementary companions in a video game. A "complementary" action is<br>contrasted with a mimicking action and is defined as any action by a friendly<br>non-player character that furthers the player's strategy. This is determined<br>through a combination of both player action and game state prediction processes<br>while allowing the AI companion to experiment. We determine the location of<br>interest for companion actions based on a dynamic set of regions customized to<br>the individual player. A user study shows promising results; a majority of<br>participants familiar with game design react positively to the companion<br>behavior, stating that they would consider using the frame-work in future games<br>themselves.</td>
      <td>## 🌟 论文解读 | 游戏AI新框架：互补型伙伴行为助力玩家策略<br><br>## 📌 背景痛点/本文动机<br>随着游戏复杂性的提升，玩家对游戏中的非玩家角色（NPC）的智能和互动性要求越来越高。然而，现有的游戏AI往往只能模仿玩家的行为，缺乏主动性和创造性，无法真正辅助玩家达成目标。本文提出了一种新的游戏开发框架，旨在实现互补型伙伴行为，即NPC能够根据玩家的策略和游戏状态，主动采取行动，帮助玩家更轻松地达成目标。<br><br>## 🚀 核心方法<br>💡 创新点1：互补型行为定义<br>本文首先对“互补型行为”进行了明确定义，即NPC的行为能够促进玩家的策略，而不是简单地模仿玩家。这要求NPC能够预测玩家的下一步行动，并采取相应的行动来辅助玩家。<br><br>💡 创新点2：动态决策过程<br>本文提出了一种动态的决策过程，包括以下步骤：<br>1. 预测玩家的下一步行动<br>2. 检查预测行动是否可行，若不可行则寻找替代行动<br>3. 判断是否能够辅助玩家当前行动<br>4. 若无法辅助，则随机选择执行预测行动或寻找最佳游戏状态的行动<br>5. 若所有行动均不可行，则尝试执行玩家未尝试过的行动或采取默认行为<br>6. 随机选择执行未尝试过的行动，以鼓励NPC进行探索<br><br>💡 创新点3：动态区域系统<br>为了更精确地确定NPC的行动位置，本文提出了一种动态区域系统。该系统根据玩家的行动动态划分地图区域，使得NPC的行动更加灵活和针对性。<br><br>💡 创新点4：玩家行动预测<br>本文使用分类器对玩家的历史数据进行训练，以预测玩家的下一步行动。分类器的类型和训练方式可以根据游戏开发者进行调整。<br><br>💡 创新点5：游戏状态预测<br>本文通过模拟NPC执行每个可能的行动，并评估其结果，来预测游戏状态的变化。这有助于NPC选择能够带来最佳结果的行动。<br><br>## 📈 实验结果<br>本文通过用户研究验证了框架的有效性。结果显示，大多数参与者对NPC的互补型行为表示满意，并认为该框架对游戏开发者具有潜在价值。<br><br>## 💬 可借鉴之处<br>本文提出的框架为游戏开发者提供了一种新的思路，可以帮助他们创建更具智能和互动性的NPC。该框架的可配置性和灵活性使其适用于各种类型的游戏。此外，本文提出的动态区域系统和玩家行动预测方法也可以应用于其他领域，例如机器人控制和智能交通系统。</td>
    </tr>
    <tr>
      <th>58</th>
      <td>GROOT: Learning to Follow Instructions by Watching Gameplay Videos</td>
      <td>We study the problem of building a controller that can follow open-ended<br>instructions in open-world environments. We propose to follow reference videos<br>as instructions, which offer expressive goal specifications while eliminating<br>the need for expensive text-gameplay annotations. A new learning framework is<br>derived to allow learning such instruction-following controllers from gameplay<br>videos while producing a video instruction encoder that induces a structured<br>goal space. We implement our agent GROOT in a simple yet effective<br>encoder-decoder architecture based on causal transformers. We evaluate GROOT<br>against open-world counterparts and human players on a proposed Minecraft<br>SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the<br>human-machine gap as well as exhibiting a 70% winning rate over the best<br>generalist agent baseline. Qualitative analysis of the induced goal space<br>further demonstrates some interesting emergent properties, including the goal<br>composition and complex gameplay behavior synthesis. The project page is<br>available at https://craftjarvis-groot.github.io.</td>
      <td>## 🌟 论文解读 | GROOT：通过观看游戏视频学习指令遵循<br><br>## 📌 背景痛点/本文动机<br>在开放世界环境中，构建能够遵循开放指令的控制器一直是人工智能领域的长期目标。然而，现有的控制器通常只能完成预定义的、有限的程序性任务，这限制了它们在开放世界环境中的应用。本文旨在解决这个问题，提出了一种新的学习框架，通过观看游戏视频来学习指令遵循控制器。<br><br>## 🚀 核心方法<br>💡 创新点1：将目标指定为参考游戏视频片段，从而提供丰富的目标规范，同时消除对昂贵的文本-游戏注释的需求。<br>💡 创新点2：引入了一种新的学习框架，该框架同时产生一个目标空间和一个视频指令遵循控制器，从而实现从游戏视频中学习指令遵循控制器。<br><br>## 📈 实验结果<br>在Minecraft SkillForge基准测试中，GROOT在整体Elo评分比较中超过了最先进的基线，并且在解决具有挑战性的获取钻石任务中表现出色。<br><br>## 💬 可借鉴之处<br>本文提出的学习框架和GROOT代理的架构设计为构建能够遵循开放指令的控制器提供了新的思路和方法。此外，本文还展示了目标空间和控制器策略的潜在应用，为解决开放世界环境中的复杂任务提供了新的可能性。</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Scaling Instructable Agents Across Many Simulated Worlds</td>
      <td>Building embodied AI systems that can follow arbitrary language instructions<br>in any 3D environment is a key challenge for creating general AI. Accomplishing<br>this goal requires learning to ground language in perception and embodied<br>actions, in order to accomplish complex tasks. The Scalable, Instructable,<br>Multiworld Agent (SIMA) project tackles this by training agents to follow<br>free-form instructions across a diverse range of virtual 3D environments,<br>including curated research environments as well as open-ended, commercial video<br>games. Our goal is to develop an instructable agent that can accomplish<br>anything a human can do in any simulated 3D environment. Our approach focuses<br>on language-driven generality while imposing minimal assumptions. Our agents<br>interact with environments in real-time using a generic, human-like interface:<br>the inputs are image observations and language instructions and the outputs are<br>keyboard-and-mouse actions. This general approach is challenging, but it allows<br>agents to ground language across many visually complex and semantically rich<br>environments while also allowing us to readily run agents in new environments.<br>In this paper we describe our motivation and goal, the initial progress we have<br>made, and promising preliminary results on several diverse research<br>environments and a variety of commercial video games.</td>
      <td>## 🌟 论文解读 | SIMA：跨越多个模拟世界的可指令智能体<br><br>## 📌 背景痛点/本文动机<br>尽管大型语言模型在自然语言处理方面取得了显著进展，但将它们与我们所处的具身世界连接起来仍然是一个巨大的挑战。现代AI在语言能力方面表现出色，但在感知和行动方面却远不及人类。为了克服这一挑战，我们需要开发能够理解语言指令并在复杂环境中执行任务的具身AI系统。<br><br>## 🚀 核心方法<br>SIMA项目旨在构建一个能够遵循任意语言指令并在任何虚拟3D环境中通过键盘和鼠标操作进行行动的系统。该项目的主要创新点包括：<br><br>💡 **多环境训练**：SIMA在多种虚拟3D环境中训练智能体，包括定制的研究环境和开放式的商业视频游戏。这种多样化的训练环境有助于智能体学习更通用的技能，并提高其在不同场景下的适应能力。<br><br>💡 **语言驱动**：SIMA的智能体通过自然语言指令进行训练，这有助于它们学习更通用的语言表示和抽象，并提高学习效率。<br><br>💡 **人类界面**：SIMA的智能体使用与人类相同的键盘和鼠标控制方式与虚拟环境交互，这有助于它们模仿人类行为，并提高其在新环境中的迁移能力。<br><br>💡 **数据收集**：SIMA收集了大量由人类专家生成的游戏数据，包括视频、语言指令、记录的动作和各种注释。这些数据被用于训练智能体，并提高其学习效率。<br><br>💡 **评估方法**：SIMA使用多种评估方法来评估智能体的性能，包括动作日志概率、静态视觉输入、地面实况评估、光学字符识别和人工评估。这些评估方法有助于全面评估智能体的性能，并确保其能够遵循语言指令。<br><br>## 📈 实验结果<br>SIMA项目取得了初步的成功，智能体能够在多种环境中完成各种任务，包括导航、资源收集、对象管理和战斗等。实验结果表明，SIMA的智能体在环境泛化、零样本迁移和语言条件行为方面表现出色。<br><br>## 💬 可借鉴之处<br>SIMA项目为具身AI研究提供了一个重要的平台，其创新方法和技术可以应用于其他领域，例如机器人、虚拟现实和增强现实等。此外，SIMA项目的研究成果也有助于我们更好地理解语言与感知和行动之间的关系，并为通用人工智能的发展提供新的思路。</td>
    </tr>
    <tr>
      <th>83</th>
      <td>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</td>
      <td>AutoGen is an open-source framework that allows developers to build LLM<br>applications via multiple agents that can converse with each other to<br>accomplish tasks. AutoGen agents are customizable, conversable, and can operate<br>in various modes that employ combinations of LLMs, human inputs, and tools.<br>Using AutoGen, developers can also flexibly define agent interaction behaviors.<br>Both natural language and computer code can be used to program flexible<br>conversation patterns for different applications. AutoGen serves as a generic<br>infrastructure to build diverse applications of various complexities and LLM<br>capacities. Empirical studies demonstrate the effectiveness of the framework in<br>many example applications, with domains ranging from mathematics, coding,<br>question answering, operations research, online decision-making, entertainment,<br>etc.</td>
      <td>## 🌟 论文解读 | AutoGen：通过多智能体对话赋能下一代LLM应用<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在各个领域的应用越来越广泛，如何有效地利用LLMs构建复杂的应用程序成为了一个挑战。传统的单智能体模型往往难以处理复杂的任务，而多智能体模型则可以协同工作，发挥各自的优势，从而提高任务处理的效率和效果。<br><br>## 🚀 核心方法<br>AutoGen是一个开源框架，它允许开发者通过多个可以相互对话的智能体来构建LLM应用程序。AutoGen的核心创新点包括：<br><br>💡 **可定制和可对话的智能体**：AutoGen的智能体可以基于LLMs、人类输入、工具或它们的组合来构建，并且可以相互对话，接收、反应和响应消息。这使得开发者可以轻松地创建具有不同角色和能力的智能体，例如编写代码、执行代码、获取人类反馈、验证输出等。<br><br>💡 **对话编程**：AutoGen采用了一种以对话为中心的编程范式，将复杂的LLM应用程序工作流程简化为多智能体对话。开发者可以通过定义一组具有特定能力和角色的可对话智能体，并编程它们之间的交互行为来构建应用程序。AutoGen支持使用自然语言和编程语言来编程灵活的对话模式，并提供了一系列设计模式来简化对话编程，例如统一的对话接口、自动回复机制、基于编程和自然语言的控制流管理等。<br><br>## 📈 实验结果<br>AutoGen在多个领域的应用程序中展示了其有效性，包括数学问题解决、代码生成、问答、决策制定、在线决策、娱乐等。实验结果表明，AutoGen可以帮助实现许多任务上的卓越性能，并减少开发工作量。<br><br>## 💬 可借鉴之处<br>AutoGen为构建复杂的多智能体LLM应用程序提供了一个通用的框架，具有以下可借鉴之处：<br><br>* **模块化设计**：AutoGen的智能体可以独立开发、测试和维护，从而简化了整体开发和代码管理。<br>* **灵活的对话编程**：AutoGen支持使用自然语言和编程语言来编程灵活的对话模式，使得开发者可以根据不同的应用需求进行定制。<br>* **可扩展性**：AutoGen的智能体可以轻松地扩展和定制，以满足不同的应用需求。<br><br>## 🌟 总结<br>AutoGen是一个强大的框架，可以帮助开发者构建复杂的多智能体LLM应用程序。它具有模块化设计、灵活的对话编程和可扩展性等优点，为LLM应用程序的开发提供了新的思路和方法。</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization</td>
      <td>Large Language Models (LLMs) exhibit robust problem-solving capabilities for<br>diverse tasks. However, most LLM-based agents are designed as specific task<br>solvers with sophisticated prompt engineering, rather than agents capable of<br>learning and evolving through interactions. These task solvers necessitate<br>manually crafted prompts to inform task rules and regulate LLM behaviors,<br>inherently incapacitating to address complex dynamic scenarios e.g., large<br>interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent<br>with Policy-level Reflection and Optimization that can learn a wealth of<br>expertise from interactive experiences and progressively elevate its behavioral<br>policy. Specifically, it involves a dynamic belief generation and reflection<br>process for policy evolution. Rather than action-level reflection, Agent-Pro<br>iteratively reflects on past trajectories and beliefs, fine-tuning its<br>irrational beliefs for a better policy. Moreover, a depth-first search is<br>employed for policy optimization, ensuring continual enhancement in policy<br>payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,<br>outperforming vanilla LLM and specialized models. Our results show Agent-Pro<br>can learn and evolve in complex and dynamic scenes, which also benefits<br>numerous LLM-based applications.</td>
      <td>## 🌟 论文解读 | Agent-Pro：基于策略级反思和优化的学习进化<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在解决各种任务方面表现出强大的能力，但大多数基于LLMs的智能体都是为特定任务设计的，需要复杂的提示工程来告知任务规则和调节LLMs的行为。这使得它们难以应对复杂动态的场景，例如大型互动游戏。本文提出了一种名为Agent-Pro的LLM-based智能体，它具有策略级反思和优化能力，可以从互动经验中学习大量专业知识，并逐步提升其行为策略。<br><br>## 🚀 核心方法<br>💡 创新点1：策略级反思和优化<br>Agent-Pro通过策略级反思和优化来学习进化。它不仅反思过去的轨迹和信念，还通过深度优先搜索来优化策略，确保策略收益的持续提升。<br><br>💡 创新点2：信念感知决策过程<br>Agent-Pro采用信念感知决策过程，通过更新自身信念和世界信念来生成更合理的行为。它能够根据信念来预测行动，并在游戏结束后根据结果来反思和调整信念。<br><br>## 📈 实验结果<br>Agent-Pro在两个游戏（Blackjack和Texas Hold'em）中进行了评估，结果表明它能够学习并进化，在复杂和动态的场景中表现出色。与传统的LLMs和专门模型相比，Agent-Pro在游戏中的收益更高。<br><br>## 💬 可借鉴之处<br>Agent-Pro的设计理念和方法为构建能够学习和进化的LLM-based智能体提供了新的思路。其策略级反思和优化机制可以帮助智能体从互动经验中学习，并逐步提升其行为策略。此外，信念感知决策过程可以帮助智能体在不确定的场景中做出更合理的决策。这些方法可以应用于各种复杂的任务，例如商业谈判、安全监控等。</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Enhance Reasoning for Large Language Models in the Game Werewolf</td>
      <td>This paper presents an innovative framework that integrates Large Language<br>Models (LLMs) with an external Thinker module to enhance the reasoning<br>capabilities of LLM-based agents. Unlike augmenting LLMs with prompt<br>engineering, Thinker directly harnesses knowledge from databases and employs<br>various optimization techniques. The framework forms a reasoning hierarchy<br>where LLMs handle intuitive System-1 tasks such as natural language processing,<br>while the Thinker focuses on cognitive System-2 tasks that require complex<br>logical analysis and domain-specific knowledge. Our framework is presented<br>using a 9-player Werewolf game that demands dual-system reasoning. We introduce<br>a communication protocol between LLMs and the Thinker, and train the Thinker<br>using data from 18800 human sessions and reinforcement learning. Experiments<br>demonstrate the framework's effectiveness in deductive reasoning, speech<br>generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to<br>surpass GPT4 when integrated with the Thinker. This paper also contributes the<br>largest dataset for social deduction games to date.</td>
      <td>## 🌟 论文解读 | 大型语言模型推理能力提升：以狼人杀游戏为案例<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理（NLP）任务上的突破，其在推理、规划和决策等领域的潜力也逐渐显现。然而，LLMs在处理复杂推理任务时仍面临挑战，尤其是在需要领域特定知识和深度逻辑分析的任务中。本文旨在通过引入外部推理模块，即“思考者”（Thinker），来增强LLM的推理能力，使其在特定任务中表现更佳。<br><br>## 🚀 核心方法<br>💡 创新点1：双系统推理框架<br>本文提出了一个创新的框架，将LLMs与外部Thinker模块相结合，形成了一个推理层次结构。LLMs负责处理直观的System-1任务，如自然语言处理和常识推理，而Thinker则专注于需要复杂逻辑分析和领域特定知识的System-2任务。<br><br>💡 创新点2：Thinker模块的设计与训练<br>Thinker模块直接从数据库中获取知识，并采用各种优化技术进行训练。它通过模仿学习、强化学习和基于群体的训练等方法，学习生成合理的游戏动作和LLM的语音指令。<br><br>💡 创新点3：数据集贡献<br>本文收集了18,800场真实人类游戏会话数据，构建了迄今为止最大的社交推理游戏数据集，为研究提供了宝贵资源。<br><br>## 📈 实验结果<br>实验结果表明，引入Thinker模块显著提高了LLMs的推理和生成能力。在狼人杀游戏中，Thinker模块在推理、语音生成和在线游戏评估方面均表现出色。此外，通过将Thinker与一个较小的LLM模型（6B）进行微调，其性能甚至超过了GPT4。<br><br>## 💬 可借鉴之处<br>本文提出的框架和方法为LLMs在复杂推理任务中的应用提供了新的思路。通过将LLMs与外部推理模块相结合，可以有效提升LLMs在特定领域的推理能力，使其在更多实际应用中发挥更大的作用。此外，本文构建的大规模数据集也为社交推理游戏的研究提供了重要的数据基础。</td>
    </tr>
    <tr>
      <th>82</th>
      <td>GameEval: Evaluating LLMs on Conversational Games</td>
      <td>The rapid advancements in large language models (LLMs) have presented<br>challenges in evaluating those models. Existing evaluation methods are either<br>reference-based or preference based, which inevitably need human intervention<br>or introduce test bias caused by evaluator models. In this paper, we propose<br>GameEval, a novel approach to evaluating LLMs through goal-driven<br>conversational games, overcoming the limitations of previous methods. GameEval<br>treats LLMs as game players and assigns them distinct roles with specific goals<br>achieved by launching conversations of various forms, including discussion,<br>question answering, and voting. We design three unique games with cooperative<br>or adversarial objectives, accompanied by corresponding evaluation metrics, to<br>show how this new paradigm comprehensively evaluates model performance.Through<br>extensive experiments, we show that GameEval can effectively differentiate the<br>capabilities of various LLMs, providing a comprehensive assessment of their<br>integrated abilities to solve complex problems. Our public anonymous code is<br>available at https://github.com/GameEval/GameEval.</td>
      <td>## 🌟 论文解读 | GameEval：通过对话游戏评估大型语言模型<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，如何有效地评估这些模型的能力成为一个挑战。现有的评估方法主要分为两类：基于参考和基于偏好的方法。基于参考的方法需要与预先确定的答案进行比较，而基于偏好的方法则依赖于人类或模型评估者的偏好。这两种方法都存在局限性，例如获取高质量标注的成本高、时间消耗大，以及引入评估者的偏好偏差等。<br><br>## 🚀 核心方法<br>本文提出了GameEval，一种通过目标驱动的对话游戏来评估LLMs的新方法。GameEval将LLMs视为游戏玩家，并为其分配具有特定目标的独特角色，通过启动各种形式的对话（包括讨论、问答和投票）来实现这些目标。本文设计了三种独特的游戏，包括合作和对抗目标，并配备了相应的评估指标，以展示这种新范式如何全面评估模型性能。<br><br>## 📈 实验结果<br>通过广泛的实验，本文展示了GameEval能够有效地区分不同LLMs的能力，并提供对其解决复杂问题综合能力的全面评估。实验结果表明，GameEval在区分ChatGPT和GPT-4等模型的能力方面表现出色，而现有方法则难以做到这一点。<br><br>## 💬 可借鉴之处<br>GameEval提供了一种新的评估LLMs的方法，可以更全面地评估模型的能力，并减少评估偏差。此外，GameEval还可以用于设计新的游戏，以评估LLMs在现实世界复杂场景中的能力。</td>
    </tr>
    <tr>
      <th>102</th>
      <td>Improving Factuality and Reasoning in Language Models through Multiagent Debate</td>
      <td>Large language models (LLMs) have demonstrated remarkable capabilities in<br>language generation, understanding, and few-shot learning in recent years. An<br>extensive body of work has explored how their performance may be further<br>improved through the tools of prompting, ranging from verification,<br>self-consistency, or intermediate scratchpads. In this paper, we present a<br>complementary approach to improve language responses where multiple language<br>model instances propose and debate their individual responses and reasoning<br>processes over multiple rounds to arrive at a common final answer. Our findings<br>indicate that this approach significantly enhances mathematical and strategic<br>reasoning across a number of tasks. We also demonstrate that our approach<br>improves the factual validity of generated content, reducing fallacious answers<br>and hallucinations that contemporary models are prone to. Our approach may be<br>directly applied to existing black-box models and uses identical procedure and<br>prompts for all tasks we investigate. Overall, our findings suggest that such<br>"society of minds" approach has the potential to significantly advance the<br>capabilities of LLMs and pave the way for further breakthroughs in language<br>generation and understanding.</td>
      <td>## 🌟 论文解读 | 通过多智能体辩论提升语言模型的事实性和推理能力<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）在语言生成、理解和少样本学习方面取得了显著进展。然而，这些模型在事实性和推理方面仍存在不足，容易产生错误的事实和推理跳跃。本文提出了一种新的方法，通过多智能体辩论来提升LLMs的事实性和推理能力。<br><br>## 🚀 核心方法<br>💡 创新点1：多智能体辩论<br>本文提出了一种新的方法，通过多智能体辩论来提升LLMs的事实性和推理能力。具体来说，给定一个查询，多个LLM实例（或智能体）首先生成各自的候选答案。然后，每个智能体阅读并批评其他智能体的答案，并使用这些内容来更新自己的答案。这个过程会重复进行多轮，直到智能体们达成一个共同的最终答案。<br><br>💡 创新点2：提升事实性和推理能力<br>本文发现，多智能体辩论方法在多个推理、事实性和问答任务上显著优于单模型基线，如零样本思维链和反思。此外，该方法还能提高生成内容的事实性，减少当代模型容易产生的错误答案和幻觉。<br><br>## 📈 实验结果<br>本文在多个推理和事实性任务上评估了多智能体辩论方法，包括算术、小学数学、国际象棋移动预测、传记生成和MMLU。结果表明，多智能体辩论方法在这些任务上均取得了显著的性能提升。<br><br>## 💬 可借鉴之处<br>本文提出的多智能体辩论方法为提升LLMs的事实性和推理能力提供了一种新的思路。该方法简单有效，可以应用于各种不同的推理和事实性任务。此外，该方法还可以与其他模型生成改进方法相结合，进一步提升LLMs的性能。</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        