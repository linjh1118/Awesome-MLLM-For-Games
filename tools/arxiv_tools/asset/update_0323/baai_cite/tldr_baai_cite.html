
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>85</th>
      <td>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</td>
      <td>With ChatGPT-like large language models (LLM) prevailing in the community,<br>how to evaluate the ability of LLMs is an open question. Existing evaluation<br>methods suffer from following shortcomings: (1) constrained evaluation<br>abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that<br>task-based evaluation, where LLM agents complete tasks in a simulated<br>environment, is a one-for-all solution to solve above problems. We present<br>AgentSims, an easy-to-use infrastructure for researchers from all disciplines<br>to test the specific capacities they are interested in. Researchers can build<br>their evaluation tasks by adding agents and buildings on an interactive GUI or<br>deploy and test new support mechanisms, i.e. memory, planning and tool-use<br>systems, by a few lines of codes. Our demo is available at<br>https://agentsims.com .</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AgentSimsï¼šå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å¼€æ”¾æºä»£ç æ²™ç›’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¤¾åŒºä¸­çš„æ™®åŠï¼Œå¦‚ä½•è¯„ä¼°LLMçš„èƒ½åŠ›æˆä¸ºä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸è¶³ï¼š<br>1. è¯„ä¼°èƒ½åŠ›å—é™ï¼šå¤§å¤šæ•°ä»»åŠ¡é‡‡ç”¨å•è½®é—®ç­”æ ¼å¼ï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMçš„å„ç§èƒ½åŠ›ã€‚<br>2. åŸºå‡†æ˜“å—æ”»å‡»ï¼šç”±äºLLMå…·æœ‰å¤§é‡çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œæµ‹è¯•é›†å®¹æ˜“æ— æ„ä¸­æ··å…¥è®­ç»ƒé›†ã€‚<br>3. æŒ‡æ ‡ä¸å®¢è§‚ï¼šç°æœ‰çš„å¼€æ”¾å¼é—®ç­”æŒ‡æ ‡æ¶‰åŠè‡ªåŠ¨æŒ‡æ ‡å’Œä¸»è§‚æŒ‡æ ‡ï¼Œæ— æ³•å®¢è§‚è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†åŸºäºä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•ï¼Œå³LLMä»£ç†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å®Œæˆä»»åŠ¡æ¥è¯æ˜å…¶èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†AgentSimsï¼Œä¸€ä¸ªæ˜“äºä½¿ç”¨çš„è¯„ä¼°LLMèƒ½åŠ›çš„å¹³å°ã€‚AgentSimså…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š<br>1. å¯æ‰©å±•æ€§å’Œå¯ç»„åˆæ€§ï¼šå…è®¸ç”¨æˆ·ç»„åˆä¸åŒçš„è®¡åˆ’ã€è®°å¿†å’Œä½¿ç”¨å·¥å…·ç³»ç»Ÿï¼Œç ”ç©¶å„ç§ç³»ç»Ÿè®¾è®¡çš„å½±å“å’Œæœ‰æ•ˆæ€§ã€‚<br>2. äº¤äº’å¼ç”¨æˆ·ç•Œé¢ï¼šä¸ºåœ°å›¾è®¾è®¡å’Œä»£ç†åˆ›å»ºæä¾›äº¤äº’å¼UIï¼Œé™ä½éä¸“ä¸šäººå£«çš„å…¥é—¨é—¨æ§›ã€‚<br>3. æ ‡å‡†åŒ–å®ç°ï¼šç¡®ä¿å®éªŒç»“æœçš„å†ç°æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å±•ç¤ºäº†AgentSimsåœ¨è¯„ä¼°LLMèƒ½åŠ›æ–¹é¢çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š<br>1. è¯„ä¼°LLMçš„ç¤¾ä¼šèƒ½åŠ›ï¼Œå¦‚å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰ã€‚<br>2. è¯„ä¼°LLMçš„é•¿æœŸè§„åˆ’å’Œç»„ç»‡èƒ½åŠ›ï¼Œå¦‚æ‹…ä»»å¸‚é•¿æˆ–å…¬å¸æ€»è£ã€‚<br>3. ä½œä¸ºæ•°æ®ç”Ÿæˆå¹³å°ï¼Œç”¨äºæ•°æ®æ ‡æ³¨å’Œå¢å¼ºã€‚<br>4. ä¸ºç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›å¯æ§çš„åˆæ­¥å®éªŒç¯å¢ƒã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AgentSimsä¸ºLLMè¯„ä¼°æä¾›äº†ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„æ²™ç›’å¹³å°ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br>1. åŸºäºä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚<br>2. äº¤äº’å¼ç”¨æˆ·ç•Œé¢ï¼Œé™ä½éä¸“ä¸šäººå£«çš„å…¥é—¨é—¨æ§›ã€‚<br>3. æ ‡å‡†åŒ–å®ç°ï¼Œç¡®ä¿å®éªŒç»“æœçš„å†ç°æ€§ã€‚<br>4. å¯æ‰©å±•æ€§å’Œå¯ç»„åˆæ€§ï¼Œæ–¹ä¾¿ç”¨æˆ·ç ”ç©¶å’Œå¼€å‘æ–°çš„æ”¯æŒç³»ç»Ÿã€‚</td>
    </tr>
    <tr>
      <th>89</th>
      <td>SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning</td>
      <td>Large language models (LLMs) have demonstrated impressive results in<br>developing generalist planning agents for diverse tasks. However, grounding<br>these plans in expansive, multi-floor, and multi-room environments presents a<br>significant challenge for robotics. We introduce SayPlan, a scalable approach<br>to LLM-based, large-scale task planning for robotics using 3D scene graph<br>(3DSG) representations. To ensure the scalability of our approach, we: (1)<br>exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic<br>search' for task-relevant subgraphs from a smaller, collapsed representation of<br>the full graph; (2) reduce the planning horizon for the LLM by integrating a<br>classical path planner and (3) introduce an 'iterative replanning' pipeline<br>that refines the initial plan using feedback from a scene graph simulator,<br>correcting infeasible actions and avoiding planning failures. We evaluate our<br>approach on two large-scale environments spanning up to 3 floors and 36 rooms<br>with 140 assets and objects and show that our approach is capable of grounding<br>large-scale, long-horizon task plans from abstract, and natural language<br>instruction for a mobile manipulator robot to execute. We provide real robot<br>video demonstrations on our project page https://sayplan.github.io.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SayPlanï¼šåˆ©ç”¨3Dåœºæ™¯å›¾å®ç°å¤§è§„æ¨¡æœºå™¨äººä»»åŠ¡è§„åˆ’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»»åŠ¡è§„åˆ’é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨å¤„ç†å„ç§ä»»åŠ¡æ—¶å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è®¡åˆ’åœ¨å¤§å‹ã€å¤šæ¥¼å±‚å’Œå¤šæˆ¿é—´çš„ç¯å¢ƒä¸­è½åœ°ï¼Œå¯¹äºæœºå™¨äººæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€å±€é™äºå°è§„æ¨¡ç¯å¢ƒï¼Œä¸”éš¾ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ä¸­ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>SayPlan æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ–¹æ³•ï¼Œåˆ©ç”¨ 3D åœºæ™¯å›¾ï¼ˆ3DSGï¼‰è¡¨ç¤ºæ¥æ”¯æŒåŸºäº LLM çš„å¤§è§„æ¨¡ä»»åŠ¡è§„åˆ’ã€‚ä¸ºäº†ç¡®ä¿æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼ŒSayPlan é‡‡ç”¨äº†ä»¥ä¸‹ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯­ä¹‰æœç´¢<br>SayPlan åˆ©ç”¨ 3DSG çš„å±‚æ¬¡ç»“æ„ï¼Œå…è®¸ LLM é€šè¿‡è¯­ä¹‰æœç´¢æ¥è¯†åˆ«ä»»åŠ¡ç›¸å…³çš„å­å›¾ã€‚é€šè¿‡å°†å®Œæ•´çš„ 3DSG å‹ç¼©æˆä¸€ä¸ªæ›´å°çš„è¡¨ç¤ºï¼ŒLLM å¯ä»¥ä¸“æ³¨äºä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ã€ä¿¡æ¯ä¸°å¯Œçš„å­å›¾ï¼Œä»è€Œé¿å…è¶…å‡ºå…¶ token é™åˆ¶ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¿­ä»£é‡è§„åˆ’<br>ä¸ºäº†å‡å°‘ LLM çš„è§„åˆ’èŒƒå›´ï¼ŒSayPlan é›†æˆäº†ä¸€ä¸ªç»å…¸çš„è·¯å¾„è§„åˆ’å™¨ï¼Œè´Ÿè´£è¿æ¥ LLM ç”Ÿæˆçš„èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼ŒSayPlan å¼•å…¥äº†ä¸€ä¸ªè¿­ä»£é‡è§„åˆ’æµç¨‹ï¼Œä½¿ç”¨åœºæ™¯å›¾æ¨¡æ‹Ÿå™¨çš„åé¦ˆæ¥éªŒè¯å’Œç»†åŒ–åˆå§‹è®¡åˆ’ï¼Œä»è€Œçº æ­£ä¸å¯æ‰§è¡Œçš„åŠ¨ä½œå¹¶é¿å…è§„åˆ’å¤±è´¥ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>SayPlan åœ¨ä¸¤ä¸ªå¤§å‹ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ‹¥æœ‰ 3 å±‚æ¥¼å’Œ 36 ä¸ªæˆ¿é—´çš„ç¯å¢ƒï¼Œä»¥åŠä¸€ä¸ªæ‹¥æœ‰ 140 ä¸ªèµ„äº§å’Œå¯¹è±¡çš„ç¯å¢ƒã€‚ç»“æœè¡¨æ˜ï¼ŒSayPlan èƒ½å¤Ÿä»æŠ½è±¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­ç”Ÿæˆå¯æ‰§è¡Œçš„ã€å¤§è§„æ¨¡çš„ã€é•¿æ—¶ä»»åŠ¡è®¡åˆ’ï¼Œå¹¶èƒ½å¤Ÿåœ¨çœŸå®æœºå™¨äººä¸Šè¿›è¡Œæ‰§è¡Œã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SayPlan ä¸ºå¤§è§„æ¨¡æœºå™¨äººä»»åŠ¡è§„åˆ’æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æ–¹æ³•ã€‚å…¶è¯­ä¹‰æœç´¢å’Œè¿­ä»£é‡è§„åˆ’æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘ LLM çš„ token æ¶ˆè€—ï¼Œå¹¶ç¡®ä¿ç”Ÿæˆçš„è®¡åˆ’ç¬¦åˆç¯å¢ƒçº¦æŸã€‚æ­¤å¤–ï¼ŒSayPlan çš„æ¡†æ¶å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰çš„æœºå™¨äººç³»ç»Ÿä¸­ï¼Œä¸ºæœåŠ¡æœºå™¨äººé¢†åŸŸçš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>27</th>
      <td>PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models</td>
      <td>We introduce PokeLLMon, the first LLM-embodied agent that achieves<br>human-parity performance in tactical battle games, as demonstrated in Pokemon<br>battles. The design of PokeLLMon incorporates three key strategies: (i)<br>In-context reinforcement learning that instantly consumes text-based feedback<br>derived from battles to iteratively refine the policy; (ii) Knowledge-augmented<br>generation that retrieves external knowledge to counteract hallucination and<br>enables the agent to act timely and properly; (iii) Consistent action<br>generation to mitigate the panic switching phenomenon when the agent faces a<br>powerful opponent and wants to elude the battle. We show that online battles<br>against human demonstrates PokeLLMon's human-like battle strategies and<br>just-in-time decision making, achieving 49% of win rate in the Ladder<br>competitions and 56% of win rate in the invited battles. Our implementation and<br>playable battle logs are available at: https://github.com/git-disl/PokeLLMon.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | PokeLLMonï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å®ç°äººç±»æ°´å¹³çš„å®å¯æ¢¦æˆ˜æ–—AI<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ç”Ÿæˆå¼AIå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„æˆåŠŸï¼Œäººä»¬å¼€å§‹æ¢ç´¢LLMså¦‚ä½•è‡ªä¸»åœ°åœ¨ç‰©ç†ä¸–ç•Œä¸­è¡ŒåŠ¨ï¼Œå°†ç”Ÿæˆç©ºé—´ä»æ–‡æœ¬æ‰©å±•åˆ°è¡ŒåŠ¨ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯è¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å…³é”®èŒƒå¼ã€‚æ¸¸æˆæ˜¯å¼€å‘LLM-basedä»£ç†ä¸è™šæ‹Ÿç¯å¢ƒäº¤äº’çš„åˆé€‚æµ‹è¯•å¹³å°ã€‚æˆ˜æœ¯æˆ˜æ–—æ¸¸æˆï¼Œå¦‚å®å¯æ¢¦æˆ˜æ–—ï¼Œå› å…¶çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç¦»æ•£ã€å›åˆåˆ¶æ ¼å¼ã€æˆ˜ç•¥æ€§å’Œå¤æ‚æ€§ï¼Œæˆä¸ºè¯„ä¼°LLMsæ¸¸æˆèƒ½åŠ›çš„ç†æƒ³åŸºå‡†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰<br>ä¸ºäº†è§£å†³LLMsåœ¨å®å¯æ¢¦æˆ˜æ–—ä¸­å‡ºç°çš„å¹»è§‰é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ICRLç­–ç•¥ã€‚ICRLåˆ©ç”¨æˆ˜æ–—ä¸­å³æ—¶ç”Ÿæˆçš„æ–‡æœ¬åé¦ˆä½œä¸ºâ€œå¥–åŠ±â€ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹è¿­ä»£ä¼˜åŒ–åŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚é€šè¿‡åˆ†æå‰ä¸€è½®çš„è¡ŒåŠ¨å’Œç›¸åº”çš„æ–‡æœ¬åé¦ˆï¼Œä»£ç†èƒ½å¤Ÿä¸æ–­è°ƒæ•´å…¶ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹æˆ˜æ–—ä¸­çš„å˜åŒ–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçŸ¥è¯†å¢å¼ºç”Ÿæˆï¼ˆKAGï¼‰<br>ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘å¹»è§‰ï¼Œè®ºæ–‡å¼•å…¥äº†KAGç­–ç•¥ã€‚KAGé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†ï¼Œå¦‚ç±»å‹ä¼˜åŠ¿/åŠ£åŠ¿å…³ç³»å’ŒæŠ€èƒ½/èƒ½åŠ›æ•ˆæœï¼Œæ¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹ã€‚è¿™äº›çŸ¥è¯†æ¥æºäºå®å¯æ¢¦æ¸¸æˆä¸­çš„å®å¯æ¢¦å›¾é‰´ï¼ˆPokÃ©dexï¼‰ï¼Œå®ƒæä¾›äº†å…³äºå®å¯æ¢¦ç±»å‹ã€æŠ€èƒ½å’Œèƒ½åŠ›çš„è¯¦ç»†ä¿¡æ¯ã€‚é€šè¿‡å°†å¤–éƒ¨çŸ¥è¯†æ·»åŠ åˆ°çŠ¶æ€æè¿°ä¸­ï¼Œä»£ç†èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£æˆ˜æ–—æƒ…å†µï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆ<br>ä¸ºäº†è§£å†³ä»£ç†åœ¨é¢å¯¹å¼ºå¤§å¯¹æ‰‹æ—¶å‡ºç°çš„ææ…Œåˆ‡æ¢ç°è±¡ï¼Œè®ºæ–‡æå‡ºäº†ä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆç­–ç•¥ã€‚è¯¥ç­–ç•¥é€šè¿‡å¤šæ¬¡ç‹¬ç«‹ç”Ÿæˆè¡ŒåŠ¨å¹¶æŠ•ç¥¨é€‰å‡ºæœ€ä¸€è‡´çš„è¡ŒåŠ¨ï¼Œæ¥å‡å°‘è¡ŒåŠ¨çš„ä¸ä¸€è‡´æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºä»£ç†åœ¨é¢å¯¹å‹åŠ›æ—¶ä¿æŒå†·é™ï¼Œé¿å…è¿‡åº¦æ€è€ƒå’Œææ…Œï¼Œä»è€Œåšå‡ºæ›´ç¨³å®šçš„å†³ç­–ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨çº¿æˆ˜æ–—ç»“æœè¡¨æ˜ï¼ŒPokeLLMonåœ¨æ¢¯å­æ¯”èµ›ä¸­å–å¾—äº†49%çš„èƒœç‡ï¼Œåœ¨é‚€è¯·æ¯”èµ›ä¸­å–å¾—äº†56%çš„èƒœç‡ï¼Œå±•ç°å‡ºä¸äººç±»ç©å®¶ç›¸å½“çš„æ¯”èµ›èƒ½åŠ›å’Œç­–ç•¥ã€‚ç„¶è€Œï¼ŒPokeLLMonåœ¨é¢å¯¹äººç±»ç©å®¶çš„æ¶ˆè€—ç­–ç•¥å’Œæ¬ºéª—æŠ€å·§æ—¶ä¹Ÿå­˜åœ¨å¼±ç‚¹ï¼Œè¿™è¡¨æ˜æœªæ¥éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›å…¶é•¿æœŸè§„åˆ’å’Œå¯¹æ‰‹è¡Œä¸ºé¢„æµ‹èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>PokeLLMonçš„è®¾è®¡å’Œå®ç°ä¸ºLLMsåœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚ICRLã€KAGå’Œä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆç­–ç•¥å¯ä»¥åº”ç”¨äºå…¶ä»–æ¸¸æˆï¼Œå¸®åŠ©LLMsæ›´å¥½åœ°ç†è§£å’Œåº”å¯¹æ¸¸æˆä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒPokeLLMonçš„å®éªŒç»“æœä¹Ÿæ­ç¤ºäº†LLMsåœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents</td>
      <td>Large Language Models (LLMs) have shown great success as high-level planners<br>for zero-shot game-playing agents. However, these agents are primarily<br>evaluated on Minecraft, where long-term planning is relatively straightforward.<br>In contrast, agents tested in dynamic robot environments face limitations due<br>to simplistic environments with only a few objects and interactions. To fill<br>this gap in the literature, we present NetPlay, the first LLM-powered zero-shot<br>agent for the challenging roguelike NetHack. NetHack is a particularly<br>challenging environment due to its diverse set of items and monsters, complex<br>interactions, and many ways to die.<br>  NetPlay uses an architecture designed for dynamic robot environments,<br>modified for NetHack. Like previous approaches, it prompts the LLM to choose<br>from predefined skills and tracks past interactions to enhance decision-making.<br>Given NetHack's unpredictable nature, NetPlay detects important game events to<br>interrupt running skills, enabling it to react to unforeseen circumstances.<br>While NetPlay demonstrates considerable flexibility and proficiency in<br>interacting with NetHack's mechanics, it struggles with ambiguous task<br>descriptions and a lack of explicit feedback. Our findings demonstrate that<br>NetPlay performs best with detailed context information, indicating the<br>necessity for dynamic methods in supplying context information for complex<br>games such as NetHack.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LLMs åœ¨ NetHack ä¸­çš„æ½œåŠ›ä¸å±€é™æ€§ï¼šé›¶æ ·æœ¬æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¸¸æˆé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„è§„åˆ’èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ Minecraft ç­‰æ¸¸æˆä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒæ—¶ï¼Œå¦‚æœºå™¨äººç¯å¢ƒï¼Œå¾€å¾€é¢ä¸´å±€é™æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº† NetPlayï¼Œä¸€ä¸ªåŸºäº LLM çš„é›¶æ ·æœ¬æ™ºèƒ½ä½“ï¼Œç”¨äºæŒ‘æˆ˜æ€§çš„ Rogue-like æ¸¸æˆ NetHackã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>NetPlay é‡‡ç”¨äº†ä¸€ç§ä¸“ä¸ºåŠ¨æ€æœºå™¨äººç¯å¢ƒè®¾è®¡çš„æ¶æ„ï¼Œå¹¶é’ˆå¯¹ NetHack è¿›è¡Œäº†ä¿®æ”¹ã€‚å®ƒé€šè¿‡æç¤º LLM ä»é¢„å®šä¹‰çš„æŠ€èƒ½ä¸­é€‰æ‹©ï¼Œå¹¶è·Ÿè¸ªè¿‡å»çš„äº¤äº’æ¥å¢å¼ºå†³ç­–ã€‚NetPlay è¿˜èƒ½å¤Ÿæ£€æµ‹é‡è¦çš„æ¸¸æˆäº‹ä»¶ï¼Œä»¥ä¾¿åœ¨å‡ºç°æ„å¤–æƒ…å†µæ—¶ä¸­æ–­æ­£åœ¨æ‰§è¡Œçš„æŠ€èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒNetPlay åœ¨ä¸ NetHack çš„æœºåˆ¶äº¤äº’æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ¨¡ç³Šçš„ä»»åŠ¡æè¿°å’Œç¼ºä¹æ˜ç¡®åé¦ˆæ—¶å­˜åœ¨å›°éš¾ã€‚NetPlay åœ¨æä¾›è¯¦ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æƒ…å†µä¸‹è¡¨ç°æœ€ä½³ï¼Œè¿™è¡¨æ˜åœ¨ NetHack ç­‰å¤æ‚æ¸¸æˆä¸­ï¼ŒåŠ¨æ€æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ–¹æ³•è‡³å…³é‡è¦ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>NetPlay çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMs åœ¨æ¸¸æˆé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨ LLMs çš„èƒ½åŠ›ï¼Œä¾‹å¦‚é€šè¿‡åŠ¨æ€æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–ä½¿ç”¨æœºå™¨å­¦ä¹ æ¥æ›¿ä»£æ‰‹å·¥åˆ¶ä½œçš„ç»„ä»¶ã€‚æ­¤å¤–ï¼ŒNetPlay çš„æ¶æ„å¯ä»¥ä¸ºå…¶ä»–å¤æ‚æ¸¸æˆçš„è®¾è®¡æä¾›å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>73</th>
      <td>Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4</td>
      <td>Unlike perfect information games, where all elements are known to every<br>player, imperfect information games emulate the real-world complexities of<br>decision-making under uncertain or incomplete information. GPT-4, the recent<br>breakthrough in large language models (LLMs) trained on massive passive data,<br>is notable for its knowledge retrieval and reasoning abilities. This paper<br>delves into the applicability of GPT-4's learned knowledge for imperfect<br>information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an<br>innovative agent that leverages GPT-4's capabilities for performing in<br>imperfect information games. With proper prompt engineering to achieve<br>different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable<br>adaptability across a range of imperfect information card games. Importantly,<br>GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it<br>can understand others and intentionally impact others' behavior. Leveraging<br>this, we design a planning strategy that enables GPT-4 to competently play<br>against different opponents, adapting its gameplay style as needed, while<br>requiring only the game rules and descriptions of observations as input. In the<br>experiments, we qualitatively showcase the capabilities of Suspicion-Agent<br>across three different imperfect information games and then quantitatively<br>evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can<br>potentially outperform traditional algorithms designed for imperfect<br>information games, without any specialized training or examples. In order to<br>encourage and foster deeper insights within the community, we make our<br>game-related data publicly available.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€èƒ½åŠ›ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå†³ç­–å¾€å¾€æ˜¯åœ¨ä¿¡æ¯ä¸å®Œæ•´æˆ–ä¸ç¡®å®šçš„æƒ…å†µä¸‹è¿›è¡Œçš„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„AIç®—æ³•éƒ½æ˜¯åœ¨å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è®­ç»ƒçš„ï¼Œå³æ‰€æœ‰ç©å®¶éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰ä¿¡æ¯ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ¥å¤„ç†ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å†³ç­–è¿‡ç¨‹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSuspicion-Agentçš„åˆ›æ–°å‹è‡ªä¸»ä»£ç†ï¼Œå®ƒåŸºäºGPT-4ï¼Œå¹¶åˆ©ç”¨å…¶å¼ºå¤§çš„çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›æ¥ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆã€‚Suspicion-Agentçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€ï¼ˆToMï¼‰èƒ½åŠ›ï¼Œå³ç†è§£ä»–äººå¹¶æœ‰æ„å½±å“ä»–äººè¡Œä¸ºçš„èƒ½åŠ›ã€‚è¿™ä½¿å¾—Suspicion-Agentèƒ½å¤Ÿé¢„æµ‹å¯¹æ‰‹çš„è¡Œä¸ºï¼Œå¹¶æ ¹æ®å¯¹æ‰‹çš„è¡Œä¸ºè°ƒæ•´è‡ªå·±çš„ç­–ç•¥ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°†æ¸¸æˆè¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå­æ¨¡å—ï¼Œå¦‚è§‚å¯Ÿè§£é‡Šå™¨ã€æ¸¸æˆæ¨¡å¼åˆ†æå’Œè§„åˆ’æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½ä½¿ç”¨ä¸åŒçš„æç¤ºæ¥å¼•å¯¼GPT-4æ‰§è¡Œç‰¹å®šçš„åŠŸèƒ½ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨å®éªŒä¸­ï¼ŒSuspicion-Agentåœ¨ä¸‰ä¸ªä¸åŒçš„ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­å±•ç¤ºäº†å…¶èƒ½åŠ›ï¼Œå¹¶åœ¨Leduc Hold'emæ¸¸æˆä¸­è¿›è¡Œäº†å®šé‡è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSuspicion-Agentå¯ä»¥æ½œåœ¨åœ°è¶…è¶Šä¼ ç»Ÿç®—æ³•ï¼Œè€Œæ— éœ€ä»»ä½•ä¸“é—¨çš„è®­ç»ƒæˆ–ç¤ºä¾‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„Suspicion-Agentæ¡†æ¶ä¸ºåˆ©ç”¨LLMåœ¨ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è¿›è¡Œå†³ç­–æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†LLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸ToMèƒ½åŠ›ç›¸ç»“åˆï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å…¬å¼€äº†æ‰€æœ‰ä¸æ¸¸æˆç›¸å…³çš„æ•°æ®ï¼Œè¿™å°†æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMçš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘æ›´æœ‰æ•ˆçš„æ¨¡å‹ã€‚</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation</td>
      <td>Due to the dynamic and unpredictable open-world setting, navigating complex<br>environments in Minecraft poses significant challenges for multi-agent systems.<br>Agents must interact with the environment and coordinate their actions with<br>other agents to achieve common objectives. However, traditional approaches<br>often struggle to efficiently manage inter-agent communication and task<br>distribution, crucial for effective multi-agent navigation. Furthermore,<br>processing and integrating multi-modal information (such as visual, textual,<br>and auditory data) is essential for agents to comprehend their goals and<br>navigate the environment successfully and fully. To address this issue, we<br>design the HAS framework to auto-organize groups of LLM-based agents to<br>complete navigation tasks. In our approach, we devise a hierarchical<br>auto-organizing navigation system, which is characterized by 1) a hierarchical<br>system for multi-agent organization, ensuring centralized planning and<br>decentralized execution; 2) an auto-organizing and intra-communication<br>mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal<br>information platform, facilitating multi-modal perception to perform the three<br>navigation tasks with one system. To assess organizational behavior, we design<br>a series of navigation tasks in the Minecraft environment, which includes<br>searching and exploring. We aim to develop embodied organizations that push the<br>boundaries of embodied AI, moving it towards a more human-like organizational<br>structure.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | HASï¼šå¼€æ”¾ä¸–ç•Œå¤šæ™ºèƒ½ä½“å¯¼èˆªçš„åˆ†å±‚è‡ªç»„ç»‡ç³»ç»Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œçš„ç¯å¢ƒä¸­ï¼Œå¦‚Minecraftï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿé¢ä¸´ç€å¤æ‚çš„å¯¼èˆªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¯¼èˆªæ–¹æ³•å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°ç®¡ç†æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡å’Œä»»åŠ¡åˆ†é…ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„å¤šæ™ºèƒ½ä½“å¯¼èˆªè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå¤„ç†å’Œæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚è§†è§‰ã€æ–‡æœ¬å’Œå¬è§‰æ•°æ®ï¼‰å¯¹äºæ™ºèƒ½ä½“ç†è§£å…¶ç›®æ ‡å¹¶åœ¨ç¯å¢ƒä¸­æˆåŠŸå¯¼èˆªè‡³å…³é‡è¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ†å±‚è‡ªç»„ç»‡å¯¼èˆªç³»ç»Ÿ<br>HASæ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚è‡ªç»„ç»‡å¯¼èˆªç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š<br>1. åˆ†å±‚ç³»ç»Ÿï¼šç¡®ä¿é›†ä¸­å¼è§„åˆ’å’Œåˆ†å¸ƒå¼æ‰§è¡Œï¼Œæé«˜å¯¼èˆªæ•ˆç‡ã€‚<br>2. è‡ªç»„ç»‡æœºåˆ¶ï¼šæ ¹æ®å­ä»»åŠ¡åŠ¨æ€è°ƒæ•´å…³é”®è§’è‰²å’Œè¡ŒåŠ¨ç»„ï¼Œå¹¶ä¿æŒç»„é—´é€šä¿¡ï¼Œç¡®ä¿é«˜æ•ˆåä½œã€‚<br>3. å¤šæ¨¡æ€ä¿¡æ¯å¹³å°ï¼šä¿ƒè¿›å¤šæ¨¡æ€æ„ŸçŸ¥ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å›¾åƒã€å¯¹è±¡å’ŒéŸ³é¢‘ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œæœç´¢å’Œæ¢ç´¢ç­‰å¯¼èˆªä»»åŠ¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹<br>HASæ¡†æ¶ä½¿ç”¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼ŒåŒ…æ‹¬ç®¡ç†è€…å’Œæ‰§è¡Œè€…ä¸¤ç§ç±»å‹çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å…·æœ‰ä¸åŒçš„åŠŸèƒ½ï¼Œå¦‚è§„åˆ’ã€æè¿°ã€è¯„ä¼°å’Œéƒ¨ç½²ï¼Œä»¥å®ç°é›†ä¸­å¼è§„åˆ’å’Œåˆ†å¸ƒå¼æ‰§è¡Œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ¨¡æ€è®°å¿†<br>HASæ¡†æ¶è¿˜å¼•å…¥äº†å¤šæ¨¡æ€è®°å¿†æœºåˆ¶ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»è€Œæé«˜è§„åˆ’å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡æ€æ£€ç´¢ï¼ˆMMRï¼‰æŠ€æœ¯ï¼ŒHASèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å†å²äº¤äº’åé¦ˆå’Œç»éªŒï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„è®¡åˆ’ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraftç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHASæ¡†æ¶åœ¨å¤šæ¨¡æ€ç›®æ ‡æœç´¢ã€è¿ç»­å—æœç´¢å’Œåœ°å›¾æ¢ç´¢ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒHASåœ¨å¯¼èˆªæ•ˆç‡ã€æˆåŠŸç‡å’Œæ¢ç´¢èƒ½åŠ›æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>HASæ¡†æ¶ä¸ºå¼€æ”¾ä¸–ç•Œå¤šæ™ºèƒ½ä½“å¯¼èˆªæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶åˆ†å±‚è‡ªç»„ç»‡ç»“æ„ã€å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è®°å¿†æœºåˆ¶ç­‰åˆ›æ–°ç‚¹ï¼Œå¯¹äºæé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªä¸»æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼ŒHASæ¡†æ¶è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å¤šæ™ºèƒ½ä½“åä½œçš„åœºæ™¯ï¼Œå¦‚æœºå™¨äººååŒã€è™šæ‹Ÿç°å®ç­‰ã€‚</td>
    </tr>
    <tr>
      <th>84</th>
      <td>CALYPSO: LLMs as Dungeon Masters' Assistants</td>
      <td>The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to<br>perform multiple tasks simultaneously. The DM must digest information about the<br>game setting and monsters, synthesize scenes to present to other players, and<br>respond to the players' interactions with the scene. Doing all of these tasks<br>while maintaining consistency within the narrative and story world is no small<br>feat of human cognition, making the task tiring and unapproachable to new<br>players. Large language models (LLMs) like GPT-3 and ChatGPT have shown<br>remarkable abilities to generate coherent natural language text. In this paper,<br>we conduct a formative evaluation with DMs to establish the use cases of LLMs<br>in D&D and tabletop gaming generally. We introduce CALYPSO, a system of<br>LLM-powered interfaces that support DMs with information and inspiration<br>specific to their own scenario. CALYPSO distills game context into bite-sized<br>prose and helps brainstorm ideas without distracting the DM from the game. When<br>given access to CALYPSO, DMs reported that it generated high-fidelity text<br>suitable for direct presentation to players, and low-fidelity ideas that the DM<br>could develop further while maintaining their creative agency. We see CALYPSO<br>as exemplifying a paradigm of AI-augmented tools that provide synchronous<br>creative assistance within established game worlds, and tabletop gaming more<br>broadly.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | CALYPSOï¼šå¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›åœ°ä¸‹åŸä¸»<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ°ä¸‹åŸä¸é¾™ï¼ˆD&Dï¼‰æ˜¯ä¸€æ¬¾ç»å…¸çš„æ¡Œé¢è§’è‰²æ‰®æ¼”æ¸¸æˆï¼Œå…¶ä¸­åœ°ä¸‹åŸä¸»ï¼ˆDMï¼‰æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚DMéœ€è¦åŒæ—¶å¤„ç†å¤šé¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¶ˆåŒ–æ¸¸æˆèƒŒæ™¯å’Œæ€ªç‰©ä¿¡æ¯ã€æ„å»ºåœºæ™¯ã€å›åº”ç©å®¶äº’åŠ¨ç­‰ã€‚è¿™äº›ä»»åŠ¡å¯¹äººç±»è®¤çŸ¥èƒ½åŠ›è¦æ±‚æé«˜ï¼Œå¯¹äºæ–°ç©å®¶æ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-3å’ŒChatGPTåœ¨ç”Ÿæˆè¿è´¯çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMåœ¨D&Då’Œæ¡Œé¢æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†CALYPSOç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨LLMä¸ºDMæä¾›ä¿¡æ¯å’Œæ”¯æŒï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°è¿›è¡Œæ¸¸æˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCALYPSOç³»ç»Ÿ<br>CALYPSOæ˜¯ä¸€ä¸ªç”±LLMé©±åŠ¨çš„ç•Œé¢ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¯æŒDMåœ¨æ¸¸æˆä¸­è·å–ä¿¡æ¯å’Œçµæ„Ÿã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦åŠŸèƒ½ï¼š<br>1. **é­é‡ç†è§£**ï¼šä½¿ç”¨GPT-3å¯¹æ¸¸æˆèƒŒæ™¯å’Œæ€ªç‰©ä¿¡æ¯è¿›è¡Œæ‘˜è¦ï¼Œå¸®åŠ©DMå¿«é€Ÿç†è§£é­é‡ã€‚<br>2. **èšç„¦å¤´è„‘é£æš´**ï¼šä½¿ç”¨ChatGPTä¸DMè¿›è¡Œå¯¹è¯ï¼Œå¸®åŠ©ä»–ä»¬è¿›ä¸€æ­¥æ¢ç´¢é­é‡ç»†èŠ‚æˆ–ç”Ÿæˆæ–°çš„æƒ³æ³•ã€‚<br>3. **å¼€æ”¾åŸŸèŠå¤©åŸºçº¿**ï¼šä½¿ç”¨ChatGPTæä¾›ä¸€ä¸ªå¼€æ”¾åŸŸçš„èŠå¤©ç•Œé¢ï¼Œä¾›ç©å®¶å’ŒDMè¿›è¡Œéæ¸¸æˆç›¸å…³çš„äº¤æµã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMçš„åˆ›é€ æ€§è¾…åŠ©<br>CALYPSOç³»ç»Ÿå±•ç¤ºäº†LLMä½œä¸ºåˆ›é€ æ€§è¾…åŠ©å·¥å…·çš„æ½œåŠ›ã€‚å®ƒä¸ä»…èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„æ–‡æœ¬ï¼Œé€‚åˆç›´æ¥å‘ˆç°ç»™ç©å®¶ï¼Œè¿˜èƒ½å¤Ÿæä¾›ä½ä¿çœŸåº¦çš„æƒ³æ³•ï¼Œä¾›DMè¿›ä¸€æ­¥å‘å±•å’Œå®Œå–„ã€‚è¿™ç§è¾…åŠ©æ–¹å¼ä¿ç•™äº†DMçš„åˆ›é€ æ€§è‡ªä¸»æƒï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¸“æ³¨äºæ¸¸æˆä¸­çš„è®¤çŸ¥ä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDMåœ¨ä½¿ç”¨CALYPSOç³»ç»Ÿåï¼Œæ™®éè®¤ä¸ºå®ƒèƒ½å¤Ÿç”Ÿæˆé€‚åˆç›´æ¥å‘ˆç°ç»™ç©å®¶çš„æ–‡æœ¬ï¼Œå¹¶æä¾›æœ‰ä»·å€¼çš„çµæ„Ÿã€‚DMä»¬åˆ©ç”¨CALYPSOç³»ç»Ÿæ¥ç†è§£å¤æ‚çš„æ€ªç‰©ä¿¡æ¯ã€å¤´è„‘é£æš´éç©å®¶è§’è‰²æˆ–æ€ªç‰©ä¹‹é—´çš„äº’åŠ¨ï¼Œå¹¶è·å–å»ºè®®ï¼Œå°†è¿™äº›å»ºè®®èå…¥åˆ°æ•…äº‹ä¸­å‘ˆç°ç»™ç©å®¶ï¼Œè€Œä¸ä¼šå½±å“æ¸¸æˆçš„èŠ‚å¥ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºå¼€å‘AIè¾…åŠ©å·¥å…·åœ¨æ¡Œé¢æ¸¸æˆå’Œå…¶ä»–åˆ›æ„é¢†åŸŸä¸­çš„åº”ç”¨å…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚CALYPSOç³»ç»Ÿçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–ç±»ä¼¼é¡¹ç›®æä¾›å‚è€ƒï¼Œä¾‹å¦‚ï¼š<br>1. **ç†è§£ç”¨æˆ·éœ€æ±‚**ï¼šé€šè¿‡è®¿è°ˆå’Œç”¨æˆ·ç ”ç©¶ï¼Œæ·±å…¥äº†è§£ç”¨æˆ·çš„éœ€æ±‚å’Œç—›ç‚¹ï¼Œä»è€Œè®¾è®¡å‡ºæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„AIè¾…åŠ©å·¥å…·ã€‚<br>2. **åˆ©ç”¨LLMçš„åˆ›é€ æ€§æ½œåŠ›**ï¼šLLMåœ¨ç”Ÿæˆè¿è´¯æ–‡æœ¬å’Œæä¾›åˆ›é€ æ€§çµæ„Ÿæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä»¥å°†å…¶åº”ç”¨äºå„ç§åˆ›æ„åœºæ™¯ã€‚<br>3. **ä¿æŒç”¨æˆ·çš„åˆ›é€ æ€§è‡ªä¸»æƒ**ï¼šAIè¾…åŠ©å·¥å…·åº”è¯¥ä½œä¸ºç”¨æˆ·çš„åŠ©æ‰‹ï¼Œè€Œä¸æ˜¯æ›¿ä»£è€…ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°å‘æŒ¥è‡ªå·±çš„åˆ›é€ åŠ›ã€‚<br><br>æ€»è€Œè¨€ä¹‹ï¼ŒCALYPSOç³»ç»Ÿå±•ç¤ºäº†LLMåœ¨æ¡Œé¢æ¸¸æˆä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶ä¸ºå¼€å‘AIè¾…åŠ©å·¥å…·æä¾›äº†æœ‰ä»·å€¼çš„ç»éªŒå’Œå¯ç¤ºã€‚</td>
    </tr>
    <tr>
      <th>38</th>
      <td>GlitchBench: Can large multimodal models detect video game glitches?</td>
      <td>Large multimodal models (LMMs) have evolved from large language models (LLMs)<br>to integrate multiple input modalities, such as visual inputs. This integration<br>augments the capacity of LLMs for tasks requiring visual comprehension and<br>reasoning. However, the extent and limitations of their enhanced abilities are<br>not fully understood, especially when it comes to real-world tasks. To address<br>this gap, we introduce GlitchBench, a novel benchmark derived from video game<br>quality assurance tasks, to test and evaluate the reasoning capabilities of<br>LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios<br>from video games and aims to challenge both the visual and linguistic reasoning<br>powers of LMMs in detecting and interpreting out-of-the-ordinary events. We<br>evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents<br>a new challenge for these models. Code and data are available at:<br>https://glitchbench.github.io/</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GlitchBenchï¼šå¤§å‹å¤šæ¨¡æ€æ¨¡å‹èƒ½å¦æ£€æµ‹è§†é¢‘æ¸¸æˆä¸­çš„é”™è¯¯ï¼Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„ä¸æ–­å‘å±•ï¼Œå®ƒä»¬åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œå±€é™æ€§å°šä¸æ˜ç¡®ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†GlitchBenchï¼Œä¸€ä¸ªåŸºäºè§†é¢‘æ¸¸æˆè´¨é‡ä¿è¯ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LMMsåœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGlitchBenchæ•°æ®é›†<br>GlitchBenchæ•°æ®é›†ç”±593ä¸ªæ¸¸æˆä¸­çš„å¼‚å¸¸å’Œé”™è¯¯åœºæ™¯ç»„æˆï¼Œæ¶µç›–äº†205æ¬¾ä¸åŒç±»å‹çš„æ¸¸æˆã€‚æ¯ä¸ªåœºæ™¯éƒ½åŒ…å«ä¸€ä¸ªè§†é¢‘ç‰‡æ®µã€ä¸€ä¸ªä»£è¡¨æ€§å¸§ã€ä¸€ä¸ªç®€çŸ­çš„æè¿°ä»¥åŠä¸€ä¸ªæŒ‡å‘Redditä¸Šç›¸å…³è®¨è®ºçš„é“¾æ¥ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†è¿˜åŒ…æ‹¬330ä¸ªæ— é”™è¯¯çš„å›¾åƒä½œä¸ºå¯¹æ¯”ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°æ–¹æ³•<br>æœ¬æ–‡è¯„ä¼°äº†11ä¸ªæœ€å…ˆè¿›çš„LMMsï¼ŒåŒ…æ‹¬GPT-4Vå’ŒLLaVAï¼Œåœ¨GlitchBenchä¸Šçš„è¡¨ç°ã€‚è¯„ä¼°æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé—®é¢˜ï¼š<br>1. è¿™å¼ å›¾ç‰‡æœ‰ä»€ä¹ˆä¸å¯»å¸¸çš„åœ°æ–¹ï¼Ÿ<br>2. è¿™å¼ å›¾ç‰‡æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ<br>3. è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ã€‚<br>é€šè¿‡æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸çœŸå®æ ‡ç­¾ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒLMMsåœ¨æ£€æµ‹è¿åç®€å•ç‰©ç†å®šå¾‹çš„é”™è¯¯ï¼ˆå¦‚æ±½è½¦åœ¨ç©ºä¸­é£è¡Œï¼‰æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨æ£€æµ‹æ›´å¾®å¦™çš„é”™è¯¯ï¼ˆå¦‚äººä½“éƒ¨ä½å¤„äºä¸å¯èƒ½çš„å§¿åŠ¿ï¼‰æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚GPT-4Våœ¨GlitchBenchä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡è¾¾åˆ°43.4%ã€‚ç„¶è€Œï¼Œä¸æ— é”™è¯¯å›¾åƒç›¸æ¯”ï¼Œæ¨¡å‹åœ¨æ£€æµ‹é”™è¯¯å›¾åƒæ–¹é¢çš„å‡†ç¡®ç‡æ˜æ˜¾è¾ƒä½ï¼Œè¿™è¡¨æ˜é”™è¯¯å›¾åƒæ›´å…·æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GlitchBenchåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°LMMsåœ¨å®é™…åº”ç”¨ä¸­çš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLMMsåœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game</td>
      <td>Multi-agent collaboration with Large Language Models (LLMs) demonstrates<br>proficiency in basic tasks, yet its efficiency in more complex scenarios<br>remains unexplored. In gaming environments, these agents often face situations<br>without established coordination protocols, requiring them to make intelligent<br>inferences about teammates from limited data. This problem motivates the area<br>of ad hoc teamwork, in which an agent may potentially cooperate with a variety<br>of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork<br>problem where the agent operates in an environment driven by natural language.<br>Our findings reveal the potential of LLM agents in team collaboration,<br>highlighting issues related to hallucinations in communication. To address this<br>issue, we develop CodeAct, a general agent that equips LLM with enhanced memory<br>and code-driven reasoning, enabling the repurposing of partial information for<br>rapid adaptation to new teammates.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸æ–­æå‡ï¼Œå®ƒä»¬åœ¨æ„å»ºè‡ªä¸»ä»£ç†å’Œæ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰é¢„å…ˆè®¾å®šçš„åè°ƒåè®®çš„åŠ¨æ€ç¯å¢ƒä¸­ï¼ŒLLMsçš„åä½œæ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ˜ç¡®å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä¸ä¸åŒçš„é˜Ÿå‹è¿›è¡Œæœ‰æ•ˆåˆä½œã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥AvalonPlayåŸºå‡†<br>æœ¬æ–‡æå‡ºäº†AvalonPlayåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€çš„å¤šæ™ºèƒ½ä½“å¹³å°ï¼Œç”¨äºæ¨¡æ‹ŸåŠ¨æ€ç¯å¢ƒä¸­çš„åä½œä»»åŠ¡ã€‚åœ¨è¿™ä¸ªåŸºå‡†ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨æœ‰é™çš„ä¿¡æ¯å’Œæ²¡æœ‰é¢„å…ˆè®¾å®šçš„å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è§‚å¯Ÿé˜Ÿå‹çš„è¡Œä¸ºæ¥æ¨æ–­ä»–ä»¬çš„è§’è‰²ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å›¢é˜Ÿç­–ç•¥ä»¥å®ç°å…±åŒç›®æ ‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘CodeActæ™ºèƒ½ä½“<br>ä¸ºäº†è§£å†³LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­åä½œæ—¶å¯èƒ½å‡ºç°çš„è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CodeActæ™ºèƒ½ä½“ã€‚CodeActåˆ©ç”¨LLMsçš„ä»£ç é©±åŠ¨æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å°†å¤æ‚çš„è¯­ä¹‰ä»»åŠ¡è½¬åŒ–ä¸ºçµæ´»çš„ä»£ç ç»“æ„ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œæ•ˆç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4æ¨¡å‹åœ¨AvalonPlayåŸºå‡†ä¸­è¡¨ç°å‡ºæœ€ä½³çš„åä½œèƒ½åŠ›ï¼Œè€ŒCodeActæ™ºèƒ½ä½“åœ¨å›¢é˜Ÿé€‰æ‹©å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–è¯­ä¹‰æ¨ç†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œå¼•å…¥è‡ªç„¶è¯­è¨€é€šä¿¡åè®®å¹¶ä¸æ€»æ˜¯èƒ½æ˜¾è‘—æé«˜LLMsçš„åä½œæ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆã€‚ä¸ºäº†æé«˜LLMsçš„åä½œæ•ˆç‡ï¼Œå¯ä»¥å€Ÿé‰´æœ¬æ–‡æå‡ºçš„CodeActæ™ºèƒ½ä½“çš„è®¾è®¡æ€è·¯ï¼Œåˆ©ç”¨ä»£ç é©±åŠ¨æ¨ç†å’Œè®°å¿†æ£€ç´¢ç³»ç»Ÿæ¥å¢å¼ºæ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›å’Œä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å‡å°‘å¹»è§‰ç”Ÿæˆçš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚</td>
    </tr>
    <tr>
      <th>65</th>
      <td>Lyfe Agents: Generative agents for low-cost real-time social interactions</td>
      <td>Highly autonomous generative agents powered by large language models promise<br>to simulate intricate social behaviors in virtual societies. However, achieving<br>real-time interactions with humans at a low computational cost remains<br>challenging. Here, we introduce Lyfe Agents. They combine low-cost with<br>real-time responsiveness, all while remaining intelligent and goal-oriented.<br>Key innovations include: (1) an option-action framework, reducing the cost of<br>high-level decisions; (2) asynchronous self-monitoring for better<br>self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing<br>critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation<br>and sociability across several multi-agent scenarios in our custom LyfeGame 3D<br>virtual environment platform. When equipped with our brain-inspired techniques,<br>Lyfe Agents can exhibit human-like self-motivated social reasoning. For<br>example, the agents can solve a crime (a murder mystery) through autonomous<br>collaboration and information exchange. Meanwhile, our techniques enabled Lyfe<br>Agents to operate at a computational cost 10-100 times lower than existing<br>alternatives. Our findings underscore the transformative potential of<br>autonomous generative agents to enrich human social experiences in virtual<br>worlds.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ä½æˆæœ¬å®æ—¶ç¤¾äº¤äº’åŠ¨çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼šLyfe Agents<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç¤¾ä¼šä¸­æ¨¡æ‹Ÿå¤æ‚ç¤¾äº¤è¡Œä¸ºçš„å‰æ™¯ä¹Ÿå˜å¾—å…‰æ˜ã€‚ç„¶è€Œï¼Œå®ç°ä¸äººç±»åœ¨ä½è®¡ç®—æˆæœ¬ä¸‹çš„å®æ—¶äº’åŠ¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨åˆ›å»ºä¸€ç§æ—¢æ™ºèƒ½åˆè‡ªä¸»çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨ä½è®¡ç®—æˆæœ¬ä¸‹å®ç°ä¸äººç±»çš„å®æ—¶äº’åŠ¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€‰é¡¹-åŠ¨ä½œæ¡†æ¶<br>ä¸ºäº†å‡å°‘é«˜çº§å†³ç­–çš„æˆæœ¬ï¼ŒLyfe Agents é‡‡ç”¨äº†ä¸€ç§é€‰é¡¹-åŠ¨ä½œæ¡†æ¶ã€‚åœ¨è¿™ç§æ¡†æ¶ä¸­ï¼Œæ™ºèƒ½ä½“é¦–å…ˆé€‰æ‹©ä¸€ä¸ªé«˜çº§åŠ¨ä½œï¼ˆæˆ–â€œé€‰é¡¹â€ï¼‰ï¼Œç„¶ååœ¨åç»­æ­¥éª¤ä¸­åœ¨è¯¥é€‰é¡¹å†…é€‰æ‹©ä½çº§åŠ¨ä½œã€‚è¿™ç§è®¾è®¡å…è®¸æ™ºèƒ½ä½“åœ¨æ›´é•¿æ—¶é—´å†…ä¸“æ³¨äºæ‰§è¡Œé€‰é¡¹èƒŒåçš„æ„å›¾ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼‚æ­¥è‡ªæˆ‘ç›‘æ§<br>ä¸ºäº†æé«˜æ™ºèƒ½ä½“çš„æƒ…å¢ƒæ„è¯†å’Œç›®æ ‡åšæŒæ€§ï¼ŒLyfe Agents å¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘ç›‘æ§æ¨¡å—ã€‚è¯¥æ¨¡å—ç»´æŠ¤ä¸€ä¸ªå…³äºæœ€è¿‘äº‹ä»¶çš„å™äº‹é£æ ¼æ‘˜è¦ï¼Œå¹¶å¼ºè°ƒæ–°é¢–å’Œä¸ç›®æ ‡ç›¸å…³çš„å†…å®¹ã€‚è¿™ç§è‡ªæˆ‘ç›‘æ§æ‘˜è¦å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£æƒ…å¢ƒï¼Œå¹¶ä½¿å…¶è¡Œä¸ºæ›´åŠ ä¸€è‡´å’Œç¬¦åˆç›®æ ‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ€»ç»“-é—å¿˜è®°å¿†æœºåˆ¶<br>ä¸ºäº†æé«˜è®°å¿†å­˜å‚¨å’Œæ£€ç´¢çš„è´¨é‡ï¼ŒLyfe Agents é‡‡ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–çš„è®°å¿†æ¶æ„å’Œæ€»ç»“-é—å¿˜ï¼ˆSaFï¼‰æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°†è®°å¿†åˆ†ä¸ºçŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†ï¼Œå¹¶é€šè¿‡èšç±»å’Œæ€»ç»“æŠ€æœ¯å°†çŸ­æœŸè®°å¿†ä¸­çš„ä¿¡æ¯è½¬ç§»åˆ°é•¿æœŸè®°å¿†ä¸­ã€‚æ­¤å¤–ï¼Œé—å¿˜ç®—æ³•ä¼šè¯„ä¼°å¹¶åˆ é™¤ä¸ç°æœ‰è®°å¿†é«˜åº¦ç›¸ä¼¼çš„æ—§è®°å¿†ï¼Œä»¥ç¡®ä¿å­˜å‚¨çš„ä¿¡æ¯æ˜¯ç‹¬ç‰¹å’Œç›¸å…³çš„ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨è‡ªå®šä¹‰çš„ LyfeGame 3D è™šæ‹Ÿç¯å¢ƒå¹³å°ä¸Šï¼ŒLyfe Agents åœ¨å¤šä¸ªå¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­å±•ç¤ºäº†å…¶è‡ªæˆ‘æ¿€åŠ±å’Œç¤¾ä¼šæ€§ã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è‡ªä¸»åä½œå’Œä¿¡æ¯äº¤æ¢è§£å†³çŠ¯ç½ªï¼ˆè°‹æ€è°œæ¡ˆï¼‰ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLyfe Agents çš„è®¡ç®—æˆæœ¬é™ä½äº† 10-100 å€ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Lyfe Agents çš„è®¾è®¡åŸåˆ™å’Œæ¶æ„ç»„ä»¶ä¸ºæ„å»ºä½æˆæœ¬ã€å®æ—¶å“åº”çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚å…¶é€‰é¡¹-åŠ¨ä½œæ¡†æ¶ã€å¼‚æ­¥è‡ªæˆ‘ç›‘æ§å’Œæ€»ç»“-é—å¿˜è®°å¿†æœºåˆ¶ç­‰åˆ›æ–°ç‚¹å¯ä»¥åº”ç”¨äºå…¶ä»–ç”Ÿæˆå¼æ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥æé«˜å…¶è‡ªä¸»æ€§å’Œç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLyfeGame è™šæ‹Ÿç¯å¢ƒå¹³å°ä¹Ÿä¸ºç ”ç©¶ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸ºå’Œç”¨æˆ·äº¤äº’æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</td>
    </tr>
    <tr>
      <th>77</th>
      <td>Agents: An Open-source Framework for Autonomous Language Agents</td>
      <td>Recent advances on large language models (LLMs) enable researchers and<br>developers to build autonomous language agents that can automatically solve<br>various tasks and interact with environments, humans, and other agents using<br>natural language interfaces. We consider language agents as a promising<br>direction towards artificial general intelligence and release Agents, an<br>open-source library with the goal of opening up these advances to a wider<br>non-specialist audience. Agents is carefully engineered to support important<br>features including planning, memory, tool usage, multi-agent communication, and<br>fine-grained symbolic control. Agents is user-friendly as it enables<br>non-specialists to build, customize, test, tune, and deploy state-of-the-art<br>autonomous language agents without much coding. The library is also<br>research-friendly as its modularized design makes it easily extensible for<br>researchers. Agents is available at https://github.com/aiwaves-cn/agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¼€æºæ¡†æ¶ Agentsï¼šæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†çš„åˆ©å™¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…èƒ½å¤Ÿæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿè‡ªåŠ¨è§£å†³å„ç§ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢ä¸ç¯å¢ƒã€äººç±»å’Œå…¶ä»–ä»£ç†è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­è¨€ä»£ç†æ¡†æ¶å¾€å¾€ç¼ºä¹æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œéš¾ä»¥æ»¡è¶³éä¸“ä¸šäººå£«çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Agentsï¼Œä¸€ä¸ªå¼€æºçš„è‡ªä¸»è¯­è¨€ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨è®©æ›´å¹¿æ³›çš„éä¸“ä¸šäººå£«èƒ½å¤Ÿè½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²æœ€å…ˆè¿›çš„è‡ªä¸»è¯­è¨€ä»£ç†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ”¯æŒå…³é”®åŠŸèƒ½<br>Agents æ¡†æ¶ç²¾å¿ƒè®¾è®¡ï¼Œæ”¯æŒè§„åˆ’ã€è®°å¿†ã€å·¥å…·ä½¿ç”¨ã€å¤šä»£ç†é€šä¿¡å’Œç»†ç²’åº¦ç¬¦å·æ§åˆ¶ç­‰å…³é”®åŠŸèƒ½ã€‚è¿™ä½¿å¾—è¯­è¨€ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å„ç§ä»»åŠ¡å’Œç¯å¢ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§<br>Agents æ¡†æ¶çš„ç”¨æˆ·å‹å¥½æ€§ä½“ç°åœ¨å…¶å…è®¸éä¸“ä¸šäººå£«è½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²è‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè€Œæ— éœ€å¤§é‡ç¼–ç ã€‚åŒæ—¶ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾æ‰©å±•æ¡†æ¶ï¼Œä»¥æ»¡è¶³ä»–ä»¬çš„ç ”ç©¶éœ€æ±‚ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAgent Hub å¹³å°<br>Agents æ¡†æ¶å¼•å…¥äº† Agent Hub å¹³å°ï¼Œå…è®¸ç”¨æˆ·åˆ†äº«ä»–ä»¬å¾®è°ƒçš„è¯­è¨€ä»£ç†ï¼Œå¹¶æœç´¢/ä¸‹è½½å…¶ä»–ç”¨æˆ·åˆ†äº«çš„æœ‰ç”¨è¯­è¨€ä»£ç†ã€‚è¿™å¤§å¤§é™ä½äº†ä»å¤´å¼€å§‹è®¾è®¡å’Œè°ƒæ•´è¯­è¨€ä»£ç†çš„éš¾åº¦ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè‡ªåŠ¨åˆ›å»ºä»£ç†ç³»ç»Ÿ<br>ä¸ºäº†å‡å°‘ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®š SOP çš„ç¹çå·¥ä½œï¼ŒAgents æ¡†æ¶å®ç°äº†ä¸€ä¸ªè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹ã€‚è¯¥æµç¨‹åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)ï¼Œå¯ä»¥è‡ªåŠ¨åˆ›å»ºå…¶ä»–ä»£ç†å’Œå¤šä»£ç†ç³»ç»Ÿã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>è®ºæ–‡å±•ç¤ºäº†ä½¿ç”¨ Agents æ¡†æ¶æ„å»ºçš„å•ä»£ç†ç³»ç»Ÿå’Œå¤šä»£ç†ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒåŒ…æ‹¬é—²èŠæœºå™¨äººã€åŸºäºçŸ¥è¯†åº“å’Œæœç´¢å¼•æ“çš„å®¢æˆ·æœåŠ¡ä»£ç†ã€è´­ç‰©åŠ©æ‰‹ä»£ç†å’Œé”€å”®ä»£ç†ç­‰ã€‚è¿™äº›æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº† Agents æ¡†æ¶çš„æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠæ„å»ºå„ç§ç”¨ä¾‹çš„è¯­è¨€ä»£ç†çš„å¯èƒ½æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Agents æ¡†æ¶ä¸ºæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå…¶æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…çš„ç†æƒ³é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒAgent Hub å¹³å°å’Œè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹è¿›ä¸€æ­¥æé«˜äº†æ¡†æ¶çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚</td>
    </tr>
    <tr>
      <th>60</th>
      <td>MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents</td>
      <td>Significant advancements have occurred in the application of Large Language<br>Models (LLMs) for various tasks and social simulations. Despite this, their<br>capacities to coordinate within task-oriented social contexts are<br>under-explored. Such capabilities are crucial if LLMs are to effectively mimic<br>human-like social behavior and produce meaningful results. To bridge this gap,<br>we introduce collaborative generative agents, endowing LLM-based Agents with<br>consistent behavior patterns and task-solving abilities. We situate these<br>agents in a simulated job fair environment as a case study to scrutinize their<br>coordination skills. We propose a novel framework that equips collaborative<br>generative agents with human-like reasoning abilities and specialized skills.<br>Our evaluation demonstrates that these agents show promising performance.<br>However, we also uncover limitations that hinder their effectiveness in more<br>complex coordination tasks. Our work provides valuable insights into the role<br>and evolution of LLMs in task-oriented social simulations.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MetaAgentsï¼šåŸºäºLLMçš„ä»»åŠ¡å¯¼å‘åè°ƒçš„åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºå’Œæ‰§è¡Œä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºã€‚ç„¶è€Œï¼ŒLLMsåœ¨ä»»åŠ¡å¯¼å‘çš„ç¤¾ä¼šç¯å¢ƒä¸­çš„åè°ƒèƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†ä½¿LLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿäººç±»çš„ç¤¾ä¼šè¡Œä¸ºå¹¶äº§ç”Ÿæœ‰æ„ä¹‰çš„ç»“æœï¼Œè¿™ç§èƒ½åŠ›è‡³å…³é‡è¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼ˆCollaborative Generative Agentsï¼‰ï¼Œä¸ºåŸºäºLLMçš„æ™ºèƒ½ä½“èµ‹äºˆäº†ä¸€è‡´çš„è¡Œä¸ºæ¨¡å¼å’Œä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚ä¸ºäº†ç ”ç©¶è¿™äº›æ™ºèƒ½ä½“çš„åè°ƒèƒ½åŠ›ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ‹›è˜ä¼šç¯å¢ƒï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒ…å«æ„ŸçŸ¥ã€è®°å¿†ã€æ¨ç†å’Œæ‰§è¡Œæ¨¡å—çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“å…·å¤‡ç±»ä¼¼äººç±»çš„æ¨ç†èƒ½åŠ›å’Œä¸“ä¸šæŠ€èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨æ¨¡æ‹Ÿçš„æ‹›è˜ä¼šç¯å¢ƒä¸­ï¼Œåä½œç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨è¯†åˆ«åˆæ ¼æ±‚èŒè€…ã€è®¾è®¡å·¥ä½œæµç¨‹å’Œåˆ†é…è§’è‰²æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ‹›è˜ä¼šå¤æ‚æ€§çš„å¢åŠ ï¼Œæ™ºèƒ½ä½“åœ¨åè°ƒæ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦å½’å› äºLLMsçš„ç›®æ ‡æˆ–æ„å›¾ä¸åŒ¹é…ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“æ¡†æ¶ä¸ºLLMsåœ¨ä»»åŠ¡å¯¼å‘çš„ç¤¾ä¼šæ¨¡æ‹Ÿä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚è¯¥æ¡†æ¶å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ‹›è˜ã€å›¢é˜Ÿåä½œå’Œç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†LLMsåœ¨åè°ƒä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering</td>
      <td>Code generation problems differ from common natural language problems - they<br>require matching the exact syntax of the target language, identifying happy<br>paths and edge cases, paying attention to numerous small details in the problem<br>spec, and addressing other code-specific issues and requirements. Hence, many<br>of the optimizations and tricks that have been successful in natural language<br>generation may not be effective for code tasks. In this work, we propose a new<br>approach to code generation by LLMs, which we call AlphaCodium - a test-based,<br>multi-stage, code-oriented iterative flow, that improves the performances of<br>LLMs on code problems. We tested AlphaCodium on a challenging code generation<br>dataset called CodeContests, which includes competitive programming problems<br>from platforms such as Codeforces. The proposed flow consistently and<br>significantly improves results. On the validation set, for example, GPT-4<br>accuracy (pass@5) increased from 19% with a single well-designed direct prompt<br>to 44% with the AlphaCodium flow. Many of the principles and best practices<br>acquired in this work, we believe, are broadly applicable to general code<br>generation tasks. Full implementation is available at:<br>https://github.com/Codium-ai/AlphaCodium</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AlphaCodiumï¼šä»æç¤ºå·¥ç¨‹åˆ°æµç¨‹å·¥ç¨‹ï¼Œæå‡ä»£ç ç”Ÿæˆæ€§èƒ½<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ä»£ç ç”Ÿæˆé—®é¢˜ä¸å¸¸è§çš„è‡ªç„¶è¯­è¨€é—®é¢˜ä¸åŒï¼Œå®ƒéœ€è¦åŒ¹é…ç›®æ ‡è¯­è¨€çš„ç²¾ç¡®è¯­æ³•ï¼Œè¯†åˆ«æ­£å¸¸è·¯å¾„å’Œè¾¹ç¼˜æƒ…å†µï¼Œå…³æ³¨é—®é¢˜è§„èŒƒä¸­çš„è®¸å¤šå°ç»†èŠ‚ï¼Œå¹¶è§£å†³å…¶ä»–ä»£ç ç‰¹å®šçš„é—®é¢˜å’Œè¦æ±‚ã€‚å› æ­¤ï¼Œè®¸å¤šåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­æˆåŠŸçš„ä¼˜åŒ–å’ŒæŠ€å·§å¯èƒ½å¯¹ä»£ç ä»»åŠ¡æ— æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºAlphaCodiumï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºæµ‹è¯•çš„å¤šé˜¶æ®µã€ä»£ç å¯¼å‘çš„è¿­ä»£æµç¨‹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæµ‹è¯•å¯¼å‘çš„è¿­ä»£æµç¨‹<br>AlphaCodiumçš„æ ¸å¿ƒæ˜¯è¿­ä»£æµç¨‹ï¼Œå…¶ä¸­ç”Ÿæˆçš„ä»£ç ä¼šåå¤è¿è¡Œå¹¶é’ˆå¯¹è¾“å…¥è¾“å‡ºæµ‹è¯•è¿›è¡Œä¿®å¤ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹é€æ­¥æ”¹è¿›å…¶è§£å†³æ–¹æ¡ˆï¼Œç›´åˆ°æ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé˜¶æ®µå¤„ç†<br>AlphaCodiumæµç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„å¤„ç†é˜¶æ®µå’Œä»£ç è¿­ä»£é˜¶æ®µã€‚åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œæ¨¡å‹ä¼šå¯¹é—®é¢˜è¿›è¡Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œä¾‹å¦‚ç”Ÿæˆé—®é¢˜åæ€å’Œå…¬å…±æµ‹è¯•æ¨ç†ã€‚åœ¨ä»£ç è¿­ä»£é˜¶æ®µï¼Œæ¨¡å‹ä¼šç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨å…¬å…±å’ŒAIç”Ÿæˆçš„æµ‹è¯•ä¸Šè¿›è¡Œè¿­ä»£å’Œä¿®å¤ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µ<br>AlphaCodiumè¿˜é‡‡ç”¨äº†å¤šç§ä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µï¼Œä¾‹å¦‚YAMLç»“æ„åŒ–è¾“å‡ºã€é€šè¿‡é¡¹ç›®ç¬¦å·åˆ†æè¿›è¡Œè¯­ä¹‰æ¨ç†ã€ç”Ÿæˆæ¨¡å—åŒ–ä»£ç ã€è½¯å†³ç­–å’ŒåŒé‡éªŒè¯ã€é¼“åŠ±æ¢ç´¢ä»¥åŠæµ‹è¯•é”šç‚¹ã€‚è¿™äº›æ¦‚å¿µæœ‰åŠ©äºæé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨CodeContestsæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒAlphaCodiumæµç¨‹æ˜¾è‘—æé«˜äº†LLMsåœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒGPT-4åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼ˆpass@5ï¼‰ä»ä½¿ç”¨å•ä¸ªç²¾å¿ƒè®¾è®¡çš„ç›´æ¥æç¤ºçš„19%æé«˜åˆ°ä½¿ç”¨AlphaCodiumæµç¨‹çš„44%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AlphaCodiumçš„è®¸å¤šåŸåˆ™å’Œæœ€ä½³å®è·µå¯ä»¥å¹¿æ³›åº”ç”¨äºä¸€èˆ¬çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºã€ç”Ÿæˆæ¨¡å—åŒ–ä»£ç ã€é€šè¿‡é¡¹ç›®ç¬¦å·åˆ†æè¿›è¡Œè¯­ä¹‰æ¨ç†ã€è½¯å†³ç­–å’ŒåŒé‡éªŒè¯ã€é¼“åŠ±æ¢ç´¢ä»¥åŠæµ‹è¯•é”šç‚¹ç­‰æŠ€æœ¯éƒ½å¯ä»¥å¸®åŠ©æé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚<br><br>## ğŸ“š æ€»ç»“<br>AlphaCodiumæ˜¯ä¸€ç§åˆ›æ–°çš„ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡æµ‹è¯•å¯¼å‘çš„è¿­ä»£æµç¨‹å’Œå¤šé˜¶æ®µå¤„ç†ï¼Œæ˜¾è‘—æé«˜äº†LLMsåœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†å¤šç§ä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µï¼Œè¿›ä¸€æ­¥æé«˜äº†ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚AlphaCodiumçš„è®¸å¤šåŸåˆ™å’Œæœ€ä½³å®è·µå¯ä»¥å¹¿æ³›åº”ç”¨äºä¸€èˆ¬çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œä¸ºä»£ç ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot</td>
      <td>As the field of AI continues to evolve, a significant dimension of this<br>progression is the development of Large Language Models and their potential to<br>enhance multi-agent artificial intelligence systems. This paper explores the<br>cooperative capabilities of Large Language Model-augmented Autonomous Agents<br>(LAAs) using the well-known Meltin Pot environments along with reference models<br>such as GPT4 and GPT3.5. Preliminary results suggest that while these agents<br>demonstrate a propensity for cooperation, they still struggle with effective<br>collaboration in given environments, emphasizing the need for more robust<br>architectures. The study's contributions include an abstraction layer to adapt<br>Melting Pot game scenarios for LLMs, the implementation of a reusable<br>architecture for LLM-mediated agent development - which includes short and<br>long-term memories and different cognitive modules, and the evaluation of<br>cooperation capabilities using a set of metrics tied to the Melting Pot's<br>"Commons Harvest" game. The paper closes, by discussing the limitations of the<br>current architectural framework and the potential of a new set of modules that<br>fosters better cooperation among LAAs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„è‡ªä¸»æ™ºèƒ½ä½“èƒ½å¦åˆä½œï¼Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸æ–­å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶å¯¹äºLLMå¢å¼ºçš„è‡ªä¸»æ™ºèƒ½ä½“ï¼ˆLAAsï¼‰çš„åˆä½œèƒ½åŠ›æ¢è®¨ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æ—¨åœ¨è¯„ä¼°LAAsåœ¨åˆä½œæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æ¢è®¨å¦‚ä½•æå‡å…¶åˆä½œæ•ˆæœã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†Melting Potåœºæ™¯è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤ºï¼Œä»¥ä¾¿LLMså¯ä»¥è½»æ¾æ“ä½œã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®ç°äº†ä¸€ä¸ªå¯é‡ç”¨çš„LAAså¼€å‘æ¶æ„ï¼Œè¯¥æ¶æ„åŒ…æ‹¬çŸ­æœŸå’Œé•¿æœŸè®°å¿†ä»¥åŠä¸åŒçš„è®¤çŸ¥æ¨¡å—ï¼Œå¦‚æ„ŸçŸ¥ã€è§„åˆ’ã€åæ€å’Œè¡ŒåŠ¨ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé€šè¿‡è‡ªç„¶è¯­è¨€å®šä¹‰â€œä¸ªæ€§â€ï¼Œä½¿æ™ºèƒ½ä½“æ˜ç¡®æ˜¯å¦åº”è¯¥åˆä½œã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šä½¿ç”¨Melting Potçš„â€œå…¬å…±èµ„æºæ”¶è·â€æ¸¸æˆæ¥è¯„ä¼°LLMä¸­ä»‹æ™ºèƒ½ä½“çš„åˆä½œèƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ™ºèƒ½ä½“è¡¨ç°å‡ºåˆä½œçš„å€¾å‘ï¼Œä½†å®ƒä»¬åœ¨ç‰¹å®šç¯å¢ƒä¸­ä»ç„¶éš¾ä»¥æœ‰æ•ˆåä½œã€‚è¿™çªæ˜¾äº†éœ€è¦æ›´å¼ºå¤§çš„æ¶æ„æ¥ä¿ƒè¿›LAAsä¹‹é—´çš„åˆä½œã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ºäº†æé«˜LAAsçš„åˆä½œèƒ½åŠ›ï¼Œéœ€è¦æ›´å…¨é¢çš„æ¶æ„ï¼ŒåŒ…æ‹¬å¢å¼ºçš„ç†è§£èƒ½åŠ›ã€æœ‰æ•ˆçš„æ²Ÿé€šæœºåˆ¶ã€å¯ä¿¡çš„æ‰¿è¯ºæœºåˆ¶ä»¥åŠæ˜ç¡®çš„ç¤¾äº¤ç»“æ„æˆ–åˆ¶åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ”¹è¿›æ¶æ„ï¼ŒåŒ…æ‹¬ç†è§£æ¨¡å—ã€æ²Ÿé€šæ¨¡å—ã€å®ªæ³•æ¨¡å—å’Œå£°èª‰ç³»ç»Ÿï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</td>
      <td>Many reinforcement learning environments (e.g., Minecraft) provide only<br>sparse rewards that indicate task completion or failure with binary values. The<br>challenge in exploration efficiency in such environments makes it difficult for<br>reinforcement-learning-based agents to learn complex tasks. To address this,<br>this paper introduces an advanced learning system, named Auto MC-Reward, that<br>leverages Large Language Models (LLMs) to automatically design dense reward<br>functions, thereby enhancing the learning efficiency. Auto MC-Reward consists<br>of three important components: Reward Designer, Reward Critic, and Trajectory<br>Analyzer. Given the environment information and task descriptions, the Reward<br>Designer first design the reward function by coding an executable Python<br>function with predefined observation inputs. Then, our Reward Critic will be<br>responsible for verifying the code, checking whether the code is<br>self-consistent and free of syntax and semantic errors. Further, the Trajectory<br>Analyzer summarizes possible failure causes and provides refinement suggestions<br>according to collected trajectories. In the next round, Reward Designer will<br>further refine and iterate the dense reward function based on feedback.<br>Experiments demonstrate a significant improvement in the success rate and<br>learning efficiency of our agents in complex tasks in Minecraft, such as<br>obtaining diamond with the efficient ability to avoid lava, and efficiently<br>explore trees and animals that are sparse in the plains biome.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Auto MC-Rewardï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°ï¼Œæå‡Minecraftä¸­å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>Minecraft ç­‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒé€šå¸¸åªæä¾›ç¨€ç–å¥–åŠ±ï¼Œå³åªæœ‰ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥æ—¶æ‰ä¼šè·å¾—å¥–åŠ±ã€‚è¿™ç§å¥–åŠ±æœºåˆ¶ä½¿å¾—å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ¢ç´¢æ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å­¦ä¹ å¤æ‚ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Auto MC-Rewardï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„å…ˆè¿›å­¦ä¹ ç³»ç»Ÿï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAuto MC-Reward ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ã€‚å¥–åŠ±è®¾è®¡å™¨æ ¹æ®ç¯å¢ƒä¿¡æ¯å’Œä»»åŠ¡æè¿°ï¼Œé€šè¿‡ç¼–å†™å¯æ‰§è¡Œçš„ Python å‡½æ•°æ¥è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚å¥–åŠ±è¯„è®ºå®¶è´Ÿè´£éªŒè¯ä»£ç ï¼Œæ£€æŸ¥ä»£ç æ˜¯å¦è‡ªæ´½ä¸”æ²¡æœ‰è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯ã€‚è½¨è¿¹åˆ†æå™¨æ ¹æ®æ”¶é›†çš„è½¨è¿¹æ€»ç»“å¯èƒ½çš„å¤±è´¥åŸå› ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAuto MC-Reward åˆ©ç”¨ LLM çš„ä»»åŠ¡ç†è§£å’Œç»éªŒæ€»ç»“èƒ½åŠ›ï¼Œä¸ºå­¦ä¹ æä¾›è¯¦ç»†å’Œå³æ—¶çš„å¥–åŠ±æŒ‡å¯¼ã€‚å¥–åŠ±è®¾è®¡å™¨é¦–å…ˆæ ¹æ®ç¯å¢ƒå’Œä»»åŠ¡çš„åŸºæœ¬æè¿°ï¼Œä½¿ç”¨ LLM è®¾è®¡ä¸ä»»åŠ¡ç›¸å…³çš„å¯†é›†å¥–åŠ±å‡½æ•°ã€‚ç„¶åï¼Œå¥–åŠ±è¯„è®ºå®¶å¯¹è®¾è®¡çš„å¥–åŠ±å‡½æ•°è¿›è¡Œè‡ªæˆ‘éªŒè¯ã€‚ä¸ºäº†è§£å†³ LLM ç†è§£çš„æ½œåœ¨åå·®æˆ–ç–å¿½ï¼Œè¿˜æå‡ºäº†åŸºäº LLM çš„è½¨è¿¹åˆ†æå™¨ï¼Œç”¨äºåˆ†æå’Œæ€»ç»“è®­ç»ƒä»£ç†çš„è½¨è¿¹ï¼Œå¹¶å¸®åŠ©å¥–åŠ±è®¾è®¡å™¨æ”¹è¿›å¥–åŠ±å‡½æ•°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Auto MC-Reward åœ¨ä¸€ç³»åˆ—ä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬åœ°ä¸‹æ°´å¹³æ¢ç´¢é’»çŸ³å’Œæ¢ç´¢å¹³åŸç”Ÿç‰©ç¾¤è½ä¸­çš„æ ‘æœ¨å’ŒåŠ¨ç‰©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹ç¨€ç–å¥–åŠ±å’Œç°æœ‰å¯†é›†å¥–åŠ±æ–¹æ³•ç›¸æ¯”ï¼ŒAuto MC-Reward åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ›´å¥½çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸Šé«˜æ•ˆå­¦ä¹ çš„å…ˆè¿›èƒ½åŠ›ã€‚é€šè¿‡è¿­ä»£æ”¹è¿›å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼ŒAuto MC-Reward ä½¿ä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å¯¹æ–°ä»»åŠ¡æœ‰ç›Šçš„æ–°è¡Œä¸ºï¼Œä¾‹å¦‚é¿å…ç†”å²©ï¼Œä»è€Œå¤§å¤§æé«˜äº†æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward ä»…ä½¿ç”¨åŸå§‹ä¿¡æ¯å°±å®ç°äº†é«˜é’»çŸ³è·å–æˆåŠŸç‡ï¼ˆ36.5%ï¼‰ï¼Œè¯æ˜äº†å…¶è§£å†³é•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Auto MC-Reward ä¸ºè§£å†³ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸­çš„æ¢ç´¢æ•ˆç‡é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚å…¶åˆ©ç”¨ LLM è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward çš„ä¸‰ä¸ªç»„ä»¶ï¼ˆå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ï¼‰å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä½¿å¾—æ•°æ®åˆ†æå’Œå¥–åŠ±å‡½æ•°æ›´æ–°æ›´åŠ çµæ´»ã€‚</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Large Multimodal Agents: A Survey</td>
      <td>Large language models (LLMs) have achieved superior performance in powering<br>text-based AI agents, endowing them with decision-making and reasoning<br>abilities akin to humans. Concurrently, there is an emerging research trend<br>focused on extending these LLM-powered AI agents into the multimodal domain.<br>This extension enables AI agents to interpret and respond to diverse multimodal<br>user queries, thereby handling more intricate and nuanced tasks. In this paper,<br>we conduct a systematic review of LLM-driven multimodal agents, which we refer<br>to as large multimodal agents ( LMAs for short). First, we introduce the<br>essential components involved in developing LMAs and categorize the current<br>body of research into four distinct types. Subsequently, we review the<br>collaborative frameworks integrating multiple LMAs , enhancing collective<br>efficacy. One of the critical challenges in this field is the diverse<br>evaluation methods used across existing studies, hindering effective comparison<br>among different LMAs . Therefore, we compile these evaluation methodologies and<br>establish a comprehensive framework to bridge the gaps. This framework aims to<br>standardize evaluations, facilitating more meaningful comparisons. Concluding<br>our review, we highlight the extensive applications of LMAs and propose<br>possible future research directions. Our discussion aims to provide valuable<br>insights and guidelines for future research in this rapidly evolving field. An<br>up-to-date resource list is available at<br>https://github.com/jun0wanan/awesome-large-multimodal-agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼šè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„æ¡¥æ¢<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å°†è¿™äº›æ¨¡å‹åº”ç”¨äºå¤šæ¨¡æ€é¢†åŸŸï¼Œä»¥æ„å»ºèƒ½å¤Ÿç†è§£å’Œå“åº”å¤šæ¨¡æ€ç”¨æˆ·æŸ¥è¯¢çš„æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¾€å¾€å­¤ç«‹åœ°è¿›è¡Œï¼Œç¼ºä¹å¯¹ç°æœ‰æ¡†æ¶çš„æ€»ç»“å’Œæ¯”è¾ƒã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¯¹LLMé©±åŠ¨çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼ˆLMAsï¼‰è¿›è¡Œç³»ç»Ÿæ€§çš„ç»¼è¿°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡é¦–å…ˆä»‹ç»äº†LMAsçš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€è§„åˆ’ã€è¡ŒåŠ¨å’Œè®°å¿†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ¡†æ¶ï¼Œå°†ç°æœ‰ç ”ç©¶åˆ†ä¸ºå››ç±»ï¼š<br><br>* **ç±»å‹Iï¼šä½¿ç”¨é—­æºLLMsä½œä¸ºè§„åˆ’å™¨ï¼Œæ— é•¿æœŸè®°å¿†**ã€‚è¿™ç±»LMAsä¸»è¦ä½¿ç”¨æç¤ºæŠ€æœ¯æ¥å¼•å¯¼é—­æºLLMsè¿›è¡Œå†³ç­–å’Œè§„åˆ’ï¼Œå®Œæˆå›¾åƒç¼–è¾‘ã€è§†è§‰å®šä½å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ã€‚<br>* **ç±»å‹IIï¼šä½¿ç”¨å¾®è°ƒçš„LLMsä½œä¸ºè§„åˆ’å™¨ï¼Œæ— é•¿æœŸè®°å¿†**ã€‚è¿™ç±»LMAsé€šè¿‡æ”¶é›†å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªæ•°æ®æˆ–ä½¿ç”¨è‡ªæŒ‡ä»¤æ¥å¾®è°ƒå¼€æºLLMsï¼Œä½¿å…¶å…·å¤‡å†³ç­–ã€è§„åˆ’å’Œå·¥å…·è°ƒç”¨çš„èƒ½åŠ›ã€‚<br>* **ç±»å‹IIIï¼šå…·æœ‰é—´æ¥é•¿æœŸè®°å¿†çš„è§„åˆ’å™¨**ã€‚è¿™ç±»LMAsçš„LLMsä½œä¸ºä¸­å¤®è§„åˆ’å™¨ï¼Œå¹¶é…å¤‡é•¿æœŸè®°å¿†ã€‚è§„åˆ’å™¨é€šè¿‡è°ƒç”¨ç›¸å…³å·¥å…·æ¥è®¿é—®å’Œæ£€ç´¢é•¿æœŸè®°å¿†ï¼Œä»¥å¢å¼ºæ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚<br>* **ç±»å‹IVï¼šå…·æœ‰åŸç”Ÿé•¿æœŸè®°å¿†çš„è§„åˆ’å™¨**ã€‚è¿™ç±»LMAsçš„LLMsç›´æ¥ä¸é•¿æœŸè®°å¿†äº¤äº’ï¼Œæ— éœ€å·¥å…·æ¥è®¿é—®é•¿æœŸè®°å¿†ã€‚ä¾‹å¦‚ï¼Œåœ¨Minecraftç­‰å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œè¿™ç±»LMAsèƒ½å¤Ÿå®Œæˆè¶…è¿‡200ä¸ªä¸åŒçš„ä»»åŠ¡ã€‚<br><br>æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å›é¡¾äº†å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„åä½œæ¡†æ¶ï¼Œå¹¶æ¢è®¨äº†è¯„ä¼°LMAsæ€§èƒ½çš„ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸»è§‚è¯„ä¼°å’Œå®¢è§‚è¯„ä¼°ã€‚ä¸ºäº†è§£å†³ç°æœ‰ç ”ç©¶ä¸­è¯„ä¼°æ–¹æ³•å¤šæ ·æ€§çš„é—®é¢˜ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç»¼åˆçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–è¯„ä¼°è¿‡ç¨‹ï¼Œä¿ƒè¿›æ›´æœ‰æ„ä¹‰çš„æ¯”è¾ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯å¯¹ç°æœ‰ç ”ç©¶è¿›è¡Œäº†ç»¼è¿°å’Œåˆ†æã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>* **LMAsçš„æ¡†æ¶è®¾è®¡**ï¼šæœ¬æ–‡æå‡ºçš„åˆ†ç±»æ¡†æ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LMAsçš„ä¸åŒç±»å‹å’Œç‰¹ç‚¹ï¼Œå¹¶ä¸ºæ„å»ºæ–°çš„LMAsæä¾›å‚è€ƒã€‚<br>* **LMAsçš„è¯„ä¼°æ–¹æ³•**ï¼šæœ¬æ–‡æå‡ºçš„ç»¼åˆè¯„ä¼°æ¡†æ¶å¯ä»¥ä¸ºLMAsçš„æ€§èƒ½è¯„ä¼°æä¾›æ ‡å‡†å’ŒæŒ‡å¯¼ï¼Œä¿ƒè¿›LMAsçš„è¿›ä¸€æ­¥å‘å±•ã€‚<br>* **LMAsçš„åº”ç”¨åœºæ™¯**ï¼šæœ¬æ–‡åˆ—ä¸¾äº†LMAsåœ¨GUIè‡ªåŠ¨åŒ–ã€æœºå™¨äººä¸å…·èº«AIã€æ¸¸æˆå¼€å‘ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç†è§£ã€è§†è§‰ç”Ÿæˆä¸ç¼–è¾‘ã€å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ã€éŸ³é¢‘ç¼–è¾‘ä¸ç”Ÿæˆç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œä¸ºLMAsçš„æœªæ¥å‘å±•æä¾›äº†æ–¹å‘ã€‚<br><br>## ğŸŒˆ æœªæ¥å±•æœ›<br>æœ¬æ–‡å±•æœ›äº†LMAsçš„æœªæ¥å‘å±•æ–¹å‘ï¼ŒåŒ…æ‹¬ï¼š<br><br>* **æ¡†æ¶è®¾è®¡**ï¼šä»å•ä¸ªæ™ºèƒ½ä½“å’Œå¤šä¸ªæ™ºèƒ½ä½“ä¸¤ä¸ªè§’åº¦å‡ºå‘ï¼Œæ„å»ºæ›´åŠ ç»Ÿä¸€å’Œåä½œçš„LMAsæ¡†æ¶ã€‚<br>* **è¯„ä¼°æ–¹æ³•**ï¼šå»ºç«‹ç³»ç»ŸåŒ–å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¼€å‘æ›´è´´è¿‘çœŸå®åœºæ™¯çš„è¯„ä¼°æ•°æ®é›†ã€‚<br>* **åº”ç”¨åœºæ™¯**ï¼šæ¢ç´¢LMAsåœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚äººæœºäº¤äº’ã€åŒ»ç–—å¥åº·ã€æ•™è‚²ç­‰ã€‚<br><br>æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡ä¸ºLMAsçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å…¨é¢çš„æ¦‚è¿°å’Œæ·±å…¥çš„è§è§£ï¼Œä¸ºæ¨åŠ¨LMAsçš„å‘å±•å’Œåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</td>
    </tr>
    <tr>
      <th>111</th>
      <td>PaLM-E: An Embodied Multimodal Language Model</td>
      <td>Large language models excel at a wide range of complex tasks. However,<br>enabling general inference in the real world, e.g., for robotics problems,<br>raises the challenge of grounding. We propose embodied language models to<br>directly incorporate real-world continuous sensor modalities into language<br>models and thereby establish the link between words and percepts. Input to our<br>embodied language model are multi-modal sentences that interleave visual,<br>continuous state estimation, and textual input encodings. We train these<br>encodings end-to-end, in conjunction with a pre-trained large language model,<br>for multiple embodied tasks including sequential robotic manipulation planning,<br>visual question answering, and captioning. Our evaluations show that PaLM-E, a<br>single large embodied multimodal model, can address a variety of embodied<br>reasoning tasks, from a variety of observation modalities, on multiple<br>embodiments, and further, exhibits positive transfer: the model benefits from<br>diverse joint training across internet-scale language, vision, and<br>visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,<br>in addition to being trained on robotics tasks, is a visual-language generalist<br>with state-of-the-art performance on OK-VQA, and retains generalist language<br>capabilities with increasing scale.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | PaLM-Eï¼šå°†çœŸå®ä¸–ç•Œä¼ æ„Ÿå™¨æ•°æ®èå…¥è¯­è¨€æ¨¡å‹ï¼Œå®ç°æ›´æ¥åœ°æ°”çš„æ¨ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„æ¨ç†ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººé—®é¢˜ï¼Œå­˜åœ¨ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¥åœ°ï¼ˆgroundingï¼‰ã€‚è™½ç„¶LLMsåœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œå¯èƒ½äº§ç”Ÿä¸ç‰©ç†ä¸–ç•Œç›¸å…³çš„è¡¨ç¤ºï¼Œä½†å°†è¿™äº›è¡¨ç¤ºä¸ç°å®ä¸–ç•Œçš„è§†è§‰å’Œç‰©ç†ä¼ æ„Ÿå™¨æ¨¡æ€è¿æ¥èµ·æ¥å¯¹äºè§£å†³æ›´å¹¿æ³›çš„ç°å®ä¸–ç•Œé—®é¢˜è‡³å…³é‡è¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†å…·èº«è¯­è¨€æ¨¡å‹ï¼ˆembodied language modelsï¼‰ï¼Œå°†çœŸå®ä¸–ç•Œè¿ç»­ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ•°æ®ç›´æ¥èå…¥è¯­è¨€æ¨¡å‹ï¼Œä»è€Œå»ºç«‹è¯è¯­ä¸æ„ŸçŸ¥ä¹‹é—´çš„è”ç³»ã€‚å…·èº«è¯­è¨€æ¨¡å‹çš„è¾“å…¥æ˜¯å¤šæ¨¡æ€å¥å­ï¼Œè¿™äº›å¥å­äº¤æ›¿åŒ…å«è§†è§‰ã€è¿ç»­çŠ¶æ€ä¼°è®¡å’Œæ–‡æœ¬è¾“å…¥ç¼–ç ã€‚æˆ‘ä»¬ç«¯åˆ°ç«¯åœ°è®­ç»ƒè¿™äº›ç¼–ç ï¼Œä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œç”¨äºå¤šä¸ªå…·èº«ä»»åŠ¡ï¼ŒåŒ…æ‹¬é¡ºåºæœºå™¨äººæ“ä½œè§„åˆ’ã€è§†è§‰é—®ç­”å’Œå­—å¹•ç”Ÿæˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒPaLM-Eï¼Œä¸€ä¸ªå•ä¸€çš„ã€å¤§å‹å…·èº«å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¯ä»¥è§£å†³å„ç§å…·èº«æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¥è‡ªå¤šç§è§‚å¯Ÿæ¨¡æ€çš„å¤šç§å…·èº«å½¢å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºç§¯æçš„è¿ç§»ï¼šæ¨¡å‹å—ç›Šäºè·¨äº’è”ç½‘è§„æ¨¡çš„è¯­è¨€ã€è§†è§‰å’Œè§†è§‰è¯­è¨€é¢†åŸŸçš„å¤šæ ·åŒ–è”åˆè®­ç»ƒã€‚æˆ‘ä»¬çš„æœ€å¤§æ¨¡å‹PaLM-E-562Bï¼Œé™¤äº†åœ¨æœºå™¨äººä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒå¤–ï¼Œè¿˜æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€é€šæ‰ï¼Œåœ¨OK-VQAä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”éšç€è§„æ¨¡çš„å¢åŠ ï¼Œä¿ç•™äº†é€šç”¨çš„è¯­è¨€èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>PaLM-Eçš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å°†å…·èº«æ•°æ®æ··åˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œå¯ä»¥è®­ç»ƒå‡ºä¸€ä¸ªé€šç”¨çš„ã€è¿ç§»å­¦ä¹ çš„ã€å¤šå…·èº«å†³ç­–ä»£ç†ã€‚æ­¤å¤–ï¼ŒPaLM-Eè¿˜å±•ç¤ºäº†åœ¨è§†è§‰é—®ç­”å’Œå­—å¹•ç”Ÿæˆç­‰é€šç”¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„ç«äº‰åŠ›ï¼Œå¹¶ä¸”éšç€è¯­è¨€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå¤šæ¨¡æ€å¾®è°ƒæ—¶çš„ç¾éš¾æ€§é—å¿˜æ˜¾è‘—å‡å°‘ã€‚</td>
    </tr>
    <tr>
      <th>44</th>
      <td>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</td>
      <td>Can we avoid wars at the crossroads of history? This question has been<br>pursued by individuals, scholars, policymakers, and organizations throughout<br>human history. In this research, we attempt to answer the question based on the<br>recent advances of Artificial Intelligence (AI) and Large Language Models<br>(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to<br>simulate the participating countries, their decisions, and the consequences, in<br>historical international conflicts, including the World War I (WWI), the World<br>War II (WWII), and the Warring States Period (WSP) in Ancient China. By<br>evaluating the simulation effectiveness, we examine the advancements and<br>limitations of cutting-edge AI systems' abilities in studying complex<br>collective human behaviors such as international conflicts under diverse<br>settings. In these simulations, the emergent interactions among agents also<br>offer a novel perspective for examining the triggers and conditions that lead<br>to war. Our findings offer data-driven and AI-augmented insights that can<br>redefine how we approach conflict resolution and peacekeeping strategies. The<br>implications stretch beyond historical analysis, offering a blueprint for using<br>AI to understand human history and possibly prevent future international<br>conflicts. Code and data are available at<br>\url{https://github.com/agiresearch/WarAgent}.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå†å²æˆ˜äº‰ï¼Œæ¢ç´¢å’Œå¹³çš„å¯èƒ½æ€§<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æˆ˜äº‰ä¸å’Œå¹³æ˜¯äººç±»å†å²æ°¸æ’çš„ä¸»é¢˜ï¼Œç†è§£æˆ˜äº‰çš„åŸå› å’Œé¢„é˜²æˆ˜äº‰çš„å‘ç”Ÿä¸€ç›´æ˜¯äººç±»è¿½æ±‚çš„ç›®æ ‡ã€‚ä¼ ç»Ÿçš„æˆ˜äº‰ç ”ç©¶æ–¹æ³•ä¸»è¦ä¾èµ–äºå†å²åˆ†æå’Œæ–‡çŒ®å›é¡¾ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å—é™äºé™æ€è§†è§’å’Œäº‹åè¯¸è‘›äº®çš„åè§ã€‚éšç€äººå·¥æ™ºèƒ½å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬æœ‰æœºä¼šåˆ©ç”¨è¿™äº›å…ˆè¿›æŠ€æœ¯æ¥æ¨¡æ‹Ÿå†å²äº‹ä»¶ï¼Œæ¢ç´¢æˆ˜äº‰ä¸å’Œå¹³çš„åŠ¨æ€è¿‡ç¨‹ï¼Œå¹¶ä¸ºå†²çªè§£å†³å’Œå’Œå¹³ç»´æŠ¤æä¾›æ–°çš„è§†è§’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº† WarAgentï¼Œä¸€ä¸ªåŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ AI ç³»ç»Ÿï¼Œç”¨äºæ¨¡æ‹Ÿå†å²å›½é™…å†²çªï¼ŒåŒ…æ‹¬ç¬¬ä¸€æ¬¡ä¸–ç•Œå¤§æˆ˜ï¼ˆWWIï¼‰ã€ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ˜ï¼ˆWWIIï¼‰å’Œä¸­å›½å¤ä»£æˆ˜å›½æ—¶æœŸï¼ˆWSPï¼‰ã€‚WarAgent é€šè¿‡æ¨¡æ‹Ÿå‚ä¸å›½å®¶çš„å†³ç­–è¿‡ç¨‹å’Œäº’åŠ¨ï¼Œæ¢ç´¢äº†ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š<br><br>* **æ¨¡æ‹Ÿæœ‰æ•ˆæ€§**ï¼šLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦æœ‰æ•ˆåœ°å¤åˆ¶å†å²äº‹ä»¶ä¸­æˆ˜ç•¥è§„åˆ’å’Œå†³ç­–è¿‡ç¨‹çš„æ¼”å˜ï¼Ÿ<br>* **æˆ˜äº‰èµ·å› **ï¼šå“ªäº›å› ç´ æ˜¯å¯¼è‡´æˆ˜äº‰çˆ†å‘çš„å…³é”®å› ç´ ï¼ŸLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦è¯†åˆ«è¿™äº›å› ç´ ï¼Ÿ<br>* **æˆ˜äº‰ä¸å¯é¿å…æ€§**ï¼šå†å²ä¸Šçš„æˆ˜äº‰æ˜¯å¦çœŸçš„ä¸å¯é¿å…ï¼ŸLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦æ­ç¤ºå¯¼è‡´æˆ˜äº‰ï¼ˆæˆ–å’Œå¹³ï¼‰çš„æ¡ä»¶ï¼Ÿ<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒWarAgent èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå†å²äº‹ä»¶ï¼Œå¹¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¤åˆ¶å†å²å†³ç­–è¿‡ç¨‹å’Œäº’åŠ¨ã€‚ä¾‹å¦‚ï¼Œåœ¨ WWI æ¨¡æ‹Ÿä¸­ï¼ŒWarAgent èƒ½å¤Ÿé‡ç°ä¸»è¦å›½å®¶ä¹‹é—´çš„è”ç›Ÿå½¢æˆã€æˆ˜äº‰å®£è¨€å’ŒåŠ¨å‘˜æƒ…å†µï¼Œä¸å†å²äº‹ä»¶å…·æœ‰è¾ƒé«˜çš„å»åˆåº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹å˜è§¦å‘äº‹ä»¶å’Œå›½å®¶çš„åˆå§‹æ¡ä»¶ï¼ŒWarAgent èƒ½å¤Ÿæ¢ç´¢ä¸åŒçš„æˆ˜äº‰èµ·å› å’Œæˆ˜äº‰ä¸å¯é¿å…æ€§ï¼Œä¸ºç†è§£å†å²äº‹ä»¶å’Œé¢„é˜²æœªæ¥å†²çªæä¾›äº†æ–°çš„è§†è§’ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>* **LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨**ï¼šWarAgent ä¸º LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œä¸ºç†è§£å¤æ‚çš„äººç±»è¡Œä¸ºå’Œç¤¾ä¼šåŠ¨æ€æä¾›äº†æ–°çš„å·¥å…·ã€‚<br>* **å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å†²çªè§£å†³ä¸­çš„åº”ç”¨**ï¼šWarAgent çš„è®¾è®¡ç†å¿µå¯ä»¥ä¸ºå†²çªè§£å†³å’Œå’Œå¹³ç»´æŠ¤æä¾›æ–°çš„æ€è·¯ï¼Œä¾‹å¦‚é€šè¿‡æ¨¡æ‹Ÿä¸åŒæ”¿ç­–çš„å½±å“æ¥è¯„ä¼°å†²çªè§£å†³ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚<br>* **å†å²æ•™å­¦çš„æ–°æ–¹æ³•**ï¼šWarAgent å¯ä»¥ä½œä¸ºä¸€ç§æ–°çš„å†å²æ•™å­¦æ–¹æ³•ï¼Œå¸®åŠ©å­¦ç”Ÿå’Œæ•™å¸ˆæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯ï¼Œå¹¶ç†è§£å†å²äº‹ä»¶çš„å¤æ‚å› æœå…³ç³»ã€‚<br><br>## ğŸŒŸ æœªæ¥å±•æœ›<br>WarAgent çš„ç ”ç©¶ä¸º LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„é“è·¯ï¼Œæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š<br><br>* **æ—¶é—´é©±åŠ¨æ¨¡æ‹Ÿ**ï¼šå°† WarAgent çš„å›åˆåˆ¶æ¨¡æ‹Ÿæ‰©å±•ä¸ºæ—¶é—´é©±åŠ¨æ¨¡æ‹Ÿï¼Œä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå†å²äº‹ä»¶çš„æ—¶é—´åŠ¨æ€ã€‚<br>* **åœæ­¢æ¡ä»¶**ï¼šç ”ç©¶æ›´æœ‰æ•ˆçš„åœæ­¢æ¡ä»¶ï¼Œä»¥æ›´æ¸…æ™°åœ°ç»“æŸæ¨¡æ‹Ÿå¹¶åˆ†æç»“æœã€‚<br>* **æ–°çš„ç ”ç©¶é—®é¢˜**ï¼šæ¢ç´¢æ›´å¤šä¸å†å²äº‹ä»¶å’Œå†²çªè§£å†³ç›¸å…³çš„ç ”ç©¶é—®é¢˜ï¼Œä¾‹å¦‚å¤–äº¤æ²Ÿé€šä¸å†²çªå¯èƒ½æ€§ä¹‹é—´çš„å…³ç³»ã€éå›½å®¶è¡Œä¸ºä½“å¯¹åœ°ç¼˜æ”¿æ²»çš„å½±å“ç­‰ã€‚<br><br>é€šè¿‡ä¸æ–­æ”¹è¿›å’Œæ‰©å±• WarAgentï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£å†å²äº‹ä»¶ï¼Œå¹¶ä¸ºæ„å»ºæ›´åŠ å’Œå¹³çš„æœªæ¥æä¾›æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>107</th>
      <td>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions</td>
      <td>Large language models (LLMs) such as ChatGPT and GPT-4 have recently<br>demonstrated their remarkable abilities of communicating with human users. In<br>this technical report, we take an initiative to investigate their capacities of<br>playing text games, in which a player has to understand the environment and<br>respond to situations by having dialogues with the game world. Our experiments<br>show that ChatGPT performs competitively compared to all the existing systems<br>but still exhibits a low level of intelligence. Precisely, ChatGPT can not<br>construct the world model by playing the game or even reading the game manual;<br>it may fail to leverage the world knowledge that it already has; it cannot<br>infer the goal of each step as the game progresses. Our results open up new<br>research questions at the intersection of artificial intelligence, machine<br>learning, and natural language processing.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼šç°çŠ¶ä¸å¼€æ”¾æ€§é—®é¢˜<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå’ŒGPT-4åœ¨ç†è§£å’Œå“åº”äººç±»è¯­è¨€æŸ¥è¯¢æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç ”ç©¶ç•Œå¯¹å…¶æ˜¯å¦èƒ½å¤Ÿå®ç°é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„æ½œåŠ›äº§ç”Ÿäº†å¹¿æ³›è®¨è®ºã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å°†LLMsç½®äºæ–‡æœ¬æ¸¸æˆçš„ç¯å¢ƒä¸­ï¼Œè¯„ä¼°å…¶åœ¨ç†è§£ç¯å¢ƒã€åšå‡ºå†³ç­–å’Œä¸æ¸¸æˆä¸–ç•Œè¿›è¡Œäº¤äº’æ–¹é¢çš„æ™ºèƒ½æ°´å¹³ï¼Œä»è€Œä¸ºLLMsçš„èƒ½åŠ›å’Œå±€é™æ€§æä¾›æ–°çš„è§è§£ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨æ–‡æœ¬æ¸¸æˆä½œä¸ºè¯„ä¼°LLMsæ™ºèƒ½æ°´å¹³çš„æµ‹è¯•åºŠã€‚æ–‡æœ¬æ¸¸æˆè¦æ±‚ç©å®¶é€šè¿‡æ–‡æœ¬å‘½ä»¤ä¸æ¸¸æˆä¸–ç•Œè¿›è¡Œäº¤äº’ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªå¯æ§çš„ç¯å¢ƒæ¥è¯„ä¼°LLMsçš„æ™ºèƒ½æ°´å¹³ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€šè¿‡åˆ†æLLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†LLMsåœ¨æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œå¯¼èˆªèƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œè¿›è¡Œæœ‰æ•ˆå¯¼èˆªçš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒChatGPTåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ChatGPTæ— æ³•é€šè¿‡æ¸¸æˆæˆ–é˜…è¯»æ¸¸æˆæ‰‹å†Œæ¥æ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å·²æœ‰çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä¹Ÿæ— æ³•æ¨æ–­æ¸¸æˆè¿›è¡Œè¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ç›®æ ‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½æ–¹é¢ä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ–‡æœ¬æ¸¸æˆå¯ä»¥ä½œä¸ºè¯„ä¼°LLMsæ™ºèƒ½æ°´å¹³çš„æœ‰æ•ˆæµ‹è¯•åºŠã€‚é€šè¿‡åˆ†æLLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œå¯ä»¥æ­ç¤ºLLMsåœ¨æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œå¯¼èˆªèƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºLLMsçš„æœªæ¥å‘å±•æä¾›æ–°çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿä¸ºLLMsåœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œä¾‹å¦‚å¼€å‘åŸºäºLLMsçš„æ™ºèƒ½æ¸¸æˆåŠ©æ‰‹æˆ–æ¸¸æˆè§’è‰²ã€‚</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Q-Cogni: An Integrated Causal Reinforcement Learning Framework</td>
      <td>We present Q-Cogni, an algorithmically integrated causal reinforcement<br>learning framework that redesigns Q-Learning with an autonomous causal<br>structure discovery method to improve the learning process with causal<br>inference. Q-Cogni achieves optimal learning with a pre-learned structural<br>causal model of the environment that can be queried during the learning process<br>to infer cause-and-effect relationships embedded in a state-action space. We<br>leverage on the sample efficient techniques of reinforcement learning, enable<br>reasoning about a broader set of policies and bring higher degrees of<br>interpretability to decisions made by the reinforcement learning agent. We<br>apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against<br>state-of-the-art reinforcement learning algorithms. We report results that<br>demonstrate better policies, improved learning efficiency and superior<br>interpretability of the agent's decision making. We also compare this approach<br>with traditional shortest-path search algorithms and demonstrate the benefits<br>of our causal reinforcement learning framework to high dimensional problems.<br>Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New<br>York City using the Taxi & Limousine Commission trip record data and compare<br>with shortest-path search, reporting results that show 85% of the cases with an<br>equal or better policy derived from Q-Cogni in a real-world domain.</td>
      <td></td>
    </tr>
    <tr>
      <th>115</th>
      <td>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling</td>
      <td>Reinforcement learning (RL) agents typically learn tabula rasa, without prior<br>knowledge of the world. However, if initialized with knowledge of high-level<br>subgoals and transitions between subgoals, RL agents could utilize this<br>Abstract World Model (AWM) for planning and exploration. We propose using<br>few-shot large language models (LLMs) to hypothesize an AWM, that will be<br>verified through world experience, to improve sample efficiency of RL agents.<br>Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft<br>in two phases: (1) the Dream phase where the agent uses an LLM to decompose a<br>task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase<br>where the agent learns a modular policy for each subgoal and verifies or<br>corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and<br>then verifying the AWM based on agent experience not only increases sample<br>efficiency over contemporary methods by an order of magnitude but is also<br>robust to and corrects errors in the LLM, successfully blending noisy<br>internet-scale information from LLMs with knowledge grounded in environment<br>dynamics.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºè¯­è¨€å¼•å¯¼çš„ä¸–ç•Œå»ºæ¨¡çš„å…·èº«å†³ç­–ï¼šè®©å¼ºåŒ–å­¦ä¹ æ›´é«˜æ•ˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šå¸¸ä»é›¶å¼€å§‹å­¦ä¹ ï¼Œæ²¡æœ‰å…³äºä¸–ç•Œçš„å…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¦‚æœ RL ä»£ç†åœ¨åˆå§‹åŒ–æ—¶å…·æœ‰å…³äºé«˜çº§å­ç›®æ ‡å’Œå­ç›®æ ‡ä¹‹é—´è½¬æ¢çš„çŸ¥è¯†ï¼Œå®ƒä»¬å¯ä»¥åˆ©ç”¨è¿™ç§æŠ½è±¡ä¸–ç•Œæ¨¡å‹ï¼ˆAWMï¼‰è¿›è¡Œè§„åˆ’å’Œæ¢ç´¢ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å°‘é‡æ ·æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å‡è®¾ AWMï¼Œå¹¶é€šè¿‡ä¸–ç•Œç»éªŒè¿›è¡ŒéªŒè¯ï¼Œä»¥æé«˜ RL ä»£ç†çš„æ ·æœ¬æ•ˆç‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨å°‘é‡æ ·æœ¬çš„ LLM æ¥å‡è®¾ AWM<br>æœ¬æ–‡æå‡ºä½¿ç”¨å°‘é‡æ ·æœ¬çš„ LLM æ¥å‡è®¾ AWMï¼Œå¹¶é€šè¿‡ä¸–ç•Œç»éªŒè¿›è¡ŒéªŒè¯ï¼Œä»¥æé«˜ RL ä»£ç†çš„æ ·æœ¬æ•ˆç‡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ LLM ä¸­çš„å¤§è§„æ¨¡ã€å˜ˆæ‚çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…¶ä¸åŸºäºç¯å¢ƒåŠ¨æ€çš„çŸ¥è¯†ç›¸ç»“åˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°† LLM å¼•å¯¼çš„æ¢ç´¢åº”ç”¨äº Minecraft ä¸­çš„ç‰©å“åˆ¶ä½œ<br>æœ¬æ–‡æå‡ºçš„ DECKARD ä»£ç†å°† LLM å¼•å¯¼çš„æ¢ç´¢åº”ç”¨äº Minecraft ä¸­çš„ç‰©å“åˆ¶ä½œã€‚DECKARD ä»£ç†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ¢¦æƒ³é˜¶æ®µå’Œæ¸…é†’é˜¶æ®µã€‚åœ¨æ¢¦æƒ³é˜¶æ®µï¼Œä»£ç†ä½¿ç”¨ LLM å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—å­ç›®æ ‡ï¼Œå½¢æˆå‡è®¾çš„ AWMã€‚åœ¨æ¸…é†’é˜¶æ®µï¼Œä»£ç†å­¦ä¹ æ¯ä¸ªå­ç›®æ ‡çš„æ¨¡å—åŒ–ç­–ç•¥ï¼Œå¹¶éªŒè¯æˆ–çº æ­£å‡è®¾çš„ AWMã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ²¡æœ‰ LLM å¼•å¯¼çš„ä»£ç†ç›¸æ¯”ï¼ŒDECKARD ä»£ç†åœ¨å¼€æ”¾æ¢ç´¢å’Œç›®æ ‡é©±åŠ¨ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨å¼€æ”¾æ¢ç´¢ä¸­ï¼ŒDECKARD ä»£ç†èƒ½å¤Ÿæ›´å¿«åœ°å‘ç°æ–°çš„ AWM èŠ‚ç‚¹ã€‚åœ¨ç›®æ ‡é©±åŠ¨ä»»åŠ¡ä¸­ï¼ŒDECKARD ä»£ç†èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ¶ä½œç‰©å“ï¼Œå¹¶ä¸”å¯¹ LLM è¾“å‡ºä¸­çš„é”™è¯¯å…·æœ‰é²æ£’æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºè¯­è¨€å¼•å¯¼çš„ä¸–ç•Œå»ºæ¨¡çš„å…·èº«å†³ç­–æ–¹æ³•ä¸º RL ä»£ç†åˆ©ç”¨å¤§è§„æ¨¡ã€å˜ˆæ‚çš„å…ˆéªŒçŸ¥è¯†æä¾›äº†æ–°çš„æ€è·¯ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§ RL ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆç©æ³•å’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å—åŒ– RL ç­–ç•¥ä¹Ÿå¯ä»¥ç”¨äºæé«˜ RL ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>87</th>
      <td>EmoLLM: Multimodal Emotional Understanding Meets Large Language Models</td>
      <td>Multi-modal large language models (MLLMs) have achieved remarkable<br>performance on objective multimodal perception tasks, but their ability to<br>interpret subjective, emotionally nuanced multimodal content remains largely<br>unexplored. Thus, it impedes their ability to effectively understand and react<br>to the intricate emotions expressed by humans through multimodal media. To<br>bridge this gap, we introduce EmoBench, the first comprehensive benchmark<br>designed specifically to evaluate the emotional capabilities of MLLMs across<br>five popular emotional tasks, using a diverse dataset of 287k images and videos<br>paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a<br>novel model for multimodal emotional understanding, incorporating with two core<br>techniques. 1) Multi-perspective Visual Projection, it captures diverse<br>emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it<br>guides MLLMs to reason about emotions in the correct direction. Experimental<br>results demonstrate that EmoLLM significantly elevates multimodal emotional<br>understanding performance, with an average improvement of 12.1% across multiple<br>foundation models on EmoBench. Our work contributes to the advancement of MLLMs<br>by facilitating a deeper and more nuanced comprehension of intricate human<br>emotions, paving the way for the development of artificial emotional<br>intelligence capabilities with wide-ranging applications in areas such as<br>human-computer interaction, mental health support, and empathetic AI systems.<br>Code, data, and model will be released.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | EmoLLMï¼šå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç›®æ ‡å¤šæ¨¡æ€æ„ŸçŸ¥ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®ƒä»¬åœ¨è§£é‡Šä¸»è§‚ã€æƒ…æ„Ÿä¸°å¯Œçš„å¤šæ¨¡æ€å†…å®¹æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™é˜»ç¢äº†å®ƒä»¬æœ‰æ•ˆåœ°ç†è§£å’Œååº”äººç±»é€šè¿‡å¤šæ¨¡æ€åª’ä½“è¡¨è¾¾çš„æƒ…æ„Ÿã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†EmoBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsåœ¨äº”ä¸ªæµè¡Œæƒ…æ„Ÿä»»åŠ¡ä¸­çš„æƒ…æ„Ÿèƒ½åŠ›çš„å…¨é¢åŸºå‡†ï¼Œä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«287kå›¾åƒå’Œè§†é¢‘ä»¥åŠç›¸åº”æ–‡æœ¬æŒ‡ä»¤çš„å¤šæ ·åŒ–æ•°æ®é›†ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†EmoLLMï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£çš„æ–°å‹æ¨¡å‹ï¼Œç»“åˆäº†ä¸¤ç§æ ¸å¿ƒæŠ€æœ¯ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šè§†è§’è§†è§‰æŠ•å½±<br>å®ƒä»å¤šä¸ªè§†è§’æ•è·è§†è§‰æ•°æ®ä¸­çš„å¤šæ ·åŒ–æƒ…æ„Ÿçº¿ç´¢ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šEmoPrompt<br>å®ƒå¼•å¯¼MLLMsåœ¨æ­£ç¡®çš„æ–¹å‘ä¸Šæ¨ç†æƒ…æ„Ÿã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒEmoLLMæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£æ€§èƒ½ï¼Œåœ¨EmoBenchä¸Šå¤šä¸ªåŸºç¡€æ¨¡å‹å¹³å‡æé«˜äº†12.1%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„EmoBenchåŸºå‡†å’ŒEmoLLMæ¨¡å‹ä¸ºMLLMsåœ¨æƒ…æ„Ÿç†è§£æ–¹é¢çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨åŠ¨MLLMsåœ¨æƒ…æ„Ÿæ™ºèƒ½é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>63</th>
      <td>Beyond Win Rates: A Clustering-Based Approach to Character Balance Analysis in Team-Based Games</td>
      <td>Character diversity in competitive games, while enriching gameplay, often<br>introduces balance challenges that can negatively impact player experience and<br>strategic depth. Traditional balance assessments rely on aggregate metrics like<br>win rates and pick rates, which offer limited insight into the intricate<br>dynamics of team-based games and nuanced character roles. This paper proposes a<br>novel clustering-based methodology to analyze character balance, leveraging<br>in-game data from Valorant to account for team composition influences and<br>reveal latent character roles. By applying hierarchical agglomerative<br>clustering with Jensen-Shannon Divergence to professional match data from the<br>Valorant Champions Tour 2022, our approach identifies distinct clusters of<br>agents exhibiting similar co-occurrence patterns within team compositions. This<br>method not only complements existing quantitative metrics but also provides a<br>more holistic and interpretable perspective on character synergies and<br>potential imbalances, offering game developers a valuable tool for informed and<br>context-aware balance adjustments.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºèšç±»çš„å›¢é˜Ÿæ¸¸æˆè§’è‰²å¹³è¡¡åˆ†ææ–¹æ³•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å›¢é˜Ÿç«æŠ€æ¸¸æˆä¸­ï¼Œè§’è‰²å¤šæ ·æ€§è™½ç„¶ä¸°å¯Œäº†æ¸¸æˆç©æ³•ï¼Œä½†ä¹Ÿå¸¦æ¥äº†å¹³è¡¡æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¹³è¡¡è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºèƒœç‡å’Œé€‰æ‹©ç‡ç­‰èšåˆæŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡æ— æ³•å…¨é¢åæ˜ å›¢é˜Ÿæ¸¸æˆçš„å¤æ‚åŠ¨æ€å’Œè§’è‰²ä¹‹é—´çš„å¾®å¦™å…³ç³»ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„è§’è‰²å¹³è¡¡åˆ†ææ–¹æ³•ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°ç†è§£è§’è‰²ä¹‹é—´çš„ååŒä½œç”¨å’Œæ½œåœ¨çš„ä¸å¹³è¡¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šèšç±»åˆ†æ<br>æœ¬æ–‡ä½¿ç”¨å±‚æ¬¡å‡èšèšç±»ç®—æ³•ï¼Œç»“åˆJensen-Shannonæ•£åº¦ä½œä¸ºè·ç¦»åº¦é‡ï¼Œå¯¹è§’è‰²ä¹‹é—´çš„å…³ç³»è¿›è¡Œåˆ†æã€‚é€šè¿‡åˆ†æè§’è‰²åœ¨å›¢é˜Ÿä¸­çš„å…±ç°æ¨¡å¼ï¼Œå°†è§’è‰²åˆ†ä¸ºå…·æœ‰ç›¸ä¼¼åŠŸèƒ½çš„ç»„ï¼Œä»è€Œæ­ç¤ºæ½œåœ¨çš„è§’è‰²ç±»å‹å’ŒååŒä½œç”¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šJensen-Shannonæ•£åº¦<br>Jensen-Shannonæ•£åº¦æ˜¯ä¸€ç§å¯¹ç§°çš„åº¦é‡ï¼Œç”¨äºé‡åŒ–ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œå®ƒè¢«ç”¨æ¥è¡¡é‡è§’è‰²ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œæ›´å‡†ç¡®åœ°åæ˜ è§’è‰²åœ¨å›¢é˜Ÿä¸­çš„åŠŸèƒ½è§’è‰²ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>é€šè¿‡å¯¹ã€ŠValorantã€‹2022å¹´å† å†›å·¡å›èµ›çš„ä¸“ä¸šæ¯”èµ›æ•°æ®è¿›è¡Œèšç±»åˆ†æï¼Œæœ¬æ–‡å‘ç°äº†ä¸€äº›å…·æœ‰ç›¸ä¼¼åŠŸèƒ½çš„è§’è‰²ç»„ï¼Œä¾‹å¦‚æ§åˆ¶è€…ã€å“¨å…µã€å†³æ–—è€…å’Œå‘èµ·è€…ã€‚è¿™äº›ç»“æœä¸æ¸¸æˆä¸­çš„è§’è‰²åˆ†ç±»ç›¸å»åˆï¼Œä½†ä¹Ÿæ­ç¤ºäº†ä¸€äº›æ–°çš„è§’è‰²ç±»å‹å’ŒååŒä½œç”¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºèšç±»çš„è§’è‰²å¹³è¡¡åˆ†ææ–¹æ³•ä¸ºæ¸¸æˆå¼€å‘è€…æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå¯ä»¥å¸®åŠ©ä»–ä»¬æ›´å…¨é¢åœ°ç†è§£è§’è‰²ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å¹³è¡¡è°ƒæ•´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¯„ä¼°è§’è‰²è°ƒæ•´å¯¹æ¸¸æˆå¹³è¡¡çš„å½±å“ï¼Œä»è€Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç»´æŠ¤æ¸¸æˆçš„å…¬å¹³æ€§å’Œå¯ç©æ€§ã€‚</td>
    </tr>
    <tr>
      <th>17</th>
      <td>SOTOPIA-\( Ï€ \): Interactive Learning of Socially Intelligent Language Agents</td>
      <td>Humans learn social skills through both imitation and social interaction.<br>This social learning process is largely understudied by existing research on<br>building language agents. Motivated by this gap, we propose an interactive<br>learning method, SOTOPIA-\( \pi \), improving the social intelligence of language<br>agents. This method leverages behavior cloning and self-reinforcement training<br>on filtered social interaction data according to large language model (LLM)<br>ratings. We show that our training method allows a 7B LLM to reach the social<br>goal completion ability of an expert model (GPT-4-based agent), while improving<br>the safety of language agents and maintaining general QA ability on the MMLU<br>benchmark. We also find that this training paradigm uncovers some difficulties<br>in LLM-based evaluation of social intelligence: LLM-based evaluators<br>overestimate the abilities of the language agents trained specifically for<br>social interaction.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SOTOPIA-\( Ï€ \): æå‡è¯­è¨€æ¨¡å‹ç¤¾äº¤æ™ºèƒ½çš„äº¤äº’å¼å­¦ä¹ æ–¹æ³•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>äººç±»é€šè¿‡æ¨¡ä»¿å’Œç¤¾ä¼šäº’åŠ¨å­¦ä¹ ç¤¾äº¤æŠ€èƒ½ï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨æ„å»ºè¯­è¨€æ¨¡å‹æ—¶å¯¹æ­¤è¿‡ç¨‹å…³æ³¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼å­¦ä¹ æ–¹æ³• SOTOPIA-\( Ï€ \)ï¼Œæ—¨åœ¨æå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨è¡Œä¸ºå…‹éš†å’Œè‡ªæˆ‘å¼ºåŒ–è®­ç»ƒ<br>SOTOPIA-\( Ï€ \) åˆ©ç”¨è¡Œä¸ºå…‹éš†å’Œè‡ªæˆ‘å¼ºåŒ–è®­ç»ƒï¼Œåœ¨ç»è¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„åˆ†è¿‡æ»¤çš„ç¤¾ä¼šäº’åŠ¨æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¡Œä¸ºå…‹éš†ä»å…·æœ‰å¼ºå¤§ç¤¾äº¤æŠ€èƒ½çš„ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰çš„è¡Œä¸ºè½¨è¿¹ä¸­å­¦ä¹ ï¼Œè€Œè‡ªæˆ‘å¼ºåŒ–åˆ™ä»æ¨¡å‹è‡ªèº«çš„é«˜è¯„åˆ†è¡Œä¸ºä¸­å­¦ä¹ ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLM è¯„åˆ†ä½œä¸ºè®­ç»ƒä¿¡å·<br>SOTOPIA-\( Ï€ \) ä½¿ç”¨ GPT-4 å¯¹ç¤¾äº¤äº’åŠ¨ä¸­çš„ç§¯æè¡Œä¸ºè¿›è¡Œè¯„åˆ†ï¼Œå¹¶å°†è¿™äº›è¯„åˆ†ä½œä¸ºè®­ç»ƒä¿¡å·ã€‚è¿™ç§æ–¹æ³•æ— éœ€äººå·¥å‚ä¸ï¼Œä¸”å…·æœ‰é«˜æ•ˆå’Œå¯æ‰©å±•æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSOTOPIA-\( Ï€ \) å¯ä»¥æ˜¾è‘—æå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤ç›®æ ‡å®Œæˆèƒ½åŠ›ï¼Œä½¿å…¶æ¥è¿‘ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æé«˜è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå¹¶ä¿æŒå…¶åœ¨ MMLU åŸºå‡†æµ‹è¯•ä¸­çš„é—®ç­”èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SOTOPIA-\( Ï€ \) ä¸ºæå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº† LLM è¯„åˆ†åœ¨è¯„ä¼°ç¤¾äº¤æ™ºèƒ½æ–¹é¢çš„å±€é™æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢åœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€ä»äººç±»æ•°æ®ä¸­å­¦ä¹ ã€æ›´ç¨³å¥çš„è¯„ä¼°æ–¹æ³•ç­‰æ–¹å‘ï¼Œä»¥è¿›ä¸€æ­¥æå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½ã€‚</td>
    </tr>
    <tr>
      <th>93</th>
      <td>ChessGPT: Bridging Policy Learning and Language Modeling</td>
      <td>When solving decision-making tasks, humans typically depend on information<br>from two key sources: (1) Historical policy data, which provides interaction<br>replay from the environment, and (2) Analytical insights in natural language<br>form, exposing the invaluable thought process or strategic considerations.<br>Despite this, the majority of preceding research focuses on only one source:<br>they either use historical replay exclusively to directly learn policy or value<br>functions, or engaged in language model training utilizing mere language<br>corpus. In this paper, we argue that a powerful autonomous agent should cover<br>both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning<br>and language modeling by integrating data from these two sources in Chess<br>games. Specifically, we build a large-scale game and language dataset related<br>to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and<br>ChessGPT, integrating policy learning and language modeling. Finally, we<br>propose a full evaluation framework for evaluating language model's chess<br>ability. Experimental results validate our model and dataset's effectiveness.<br>We open source our code, model, and dataset at<br>https://github.com/waterhorse1/ChessGPT.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ChessGPTï¼šç­–ç•¥å­¦ä¹ ä¸è¯­è¨€æ¨¡å‹èåˆçš„æ¡¥æ¢<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨è§£å†³å†³ç­–ä»»åŠ¡æ—¶ï¼Œäººç±»é€šå¸¸ä¾èµ–äºä¸¤ç§å…³é”®ä¿¡æ¯æ¥æºï¼šå†å²ç­–ç•¥æ•°æ®å’Œè‡ªç„¶è¯­è¨€å½¢å¼çš„ç­–ç•¥åˆ†æã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¤§å¤šåªå…³æ³¨å…¶ä¸­ä¸€ç§æ¥æºï¼Œè¦ä¹ˆæ˜¯ç›´æ¥ä»å†å²å›æ”¾ä¸­å­¦ä¹ ç­–ç•¥æˆ–ä»·å€¼å‡½æ•°ï¼Œè¦ä¹ˆæ˜¯åˆ©ç”¨è¯­è¨€è¯­æ–™åº“è¿›è¡Œè¯­è¨€æ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡è®¤ä¸ºï¼Œä¸€ä¸ªå¼ºå¤§çš„è‡ªä¸»ä»£ç†åº”è¯¥åŒæ—¶åˆ©ç”¨è¿™ä¸¤ç§æ¥æºï¼Œå› æ­¤æå‡ºäº†ChessGPTï¼Œä¸€ä¸ªé€šè¿‡æ•´åˆå›½é™…è±¡æ£‹æ¸¸æˆä¸­çš„æ•°æ®æ¥è¿æ¥ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹çš„GPTæ¨¡å‹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡æ¸¸æˆå’Œè¯­è¨€æ•°æ®é›†<br>æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤§é‡å›½é™…è±¡æ£‹æ¸¸æˆæ•°æ®å’Œè‡ªç„¶è¯­è¨€æ•°æ®çš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…æ‹¬åœ¨çº¿æ¸¸æˆå›æ”¾ã€ä¸“ä¸šæ£‹æ‰‹æ¯”èµ›ã€è®¡ç®—æœºå¼•æ“æ¸¸æˆã€æ£‹ç›˜æ¸¸æˆã€æ£‹ç›˜æ¸¸æˆåˆ†æã€æ£‹ç›˜æ¸¸æˆåšå®¢ã€æ£‹ç›˜æ¸¸æˆä¹¦ç±ã€æ£‹ç›˜æ¸¸æˆè®ºå›ã€æ£‹ç›˜æ¸¸æˆè§†é¢‘ç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºChessCLIPå’ŒChessGPTæ¨¡å‹<br>æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ¨¡å‹ï¼ŒChessCLIPå’ŒChessGPTï¼Œåˆ©ç”¨ä¸Šè¿°æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ChessCLIPé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†ç­–ç•¥å’Œè¯­è¨€æ¨¡æ€è¿æ¥èµ·æ¥ï¼Œè€ŒChessGPTåˆ™é€šè¿‡å› æœè¯­è¨€æ¨¡å‹è¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºå…¨é¢çš„è¯„ä¼°æ¡†æ¶<br>æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å›½é™…è±¡æ£‹æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å»ºæ¨¡èƒ½åŠ›ã€ä»·å€¼åˆ¤æ–­èƒ½åŠ›å’Œç­–ç•¥èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒChessGPTæ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­éƒ½ä¼˜äºå…¶ä»–LLMåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†æ¨¡å‹å’Œæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ChessGPTæ¨¡å‹å’Œæ•°æ®é›†ä¸ºç ”ç©¶ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§çš„è‡ªä¸»ä»£ç†æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>23</th>
      <td>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</td>
      <td>We present PCA-Bench, a multimodal decision-making benchmark for evaluating<br>the integrated capabilities of Multimodal Large Language Models (MLLMs).<br>Departing from previous benchmarks focusing on simplistic tasks and individual<br>model capability, PCA-Bench introduces three complex scenarios: autonomous<br>driving, domestic robotics, and open-world games. Given task instructions and<br>diverse contexts, the model is required to seamlessly integrate multiple<br>capabilities of Perception, Cognition, and Action in a reasoning chain to make<br>accurate decisions. Moreover, PCA-Bench features error localization<br>capabilities, scrutinizing model inaccuracies in areas such as perception,<br>knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To<br>balance accuracy and efficiency in evaluation, we propose PCA-Eval, an<br>automatic evaluation protocol, and assess 10 prevalent MLLMs. The results<br>reveal significant performance disparities between open-source models and<br>powerful proprietary models like GPT-4 Vision. To address this, we introduce<br>Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing<br>instruction tuning examples in multimodal embodied environments. EIE generates<br>7,510 training examples in PCA-Bench and enhances the performance of<br>open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision<br>accuracy), thereby validating the effectiveness of EIE. Our findings suggest<br>that robust MLLMs like GPT4-Vision show promise for decision-making in embodied<br>agents, opening new avenues for MLLM research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | PCA-Benchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­çš„å†³ç­–èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†å¾€å¾€åªå…³æ³¨å•ä¸ªæ¨¡å‹èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹åœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢çš„ç»¼åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ç¼ºä¹å¯¹æ¨¡å‹é”™è¯¯è¿›è¡Œå®šä½çš„èƒ½åŠ›ï¼Œè¿™ä½¿å¾—éš¾ä»¥ç¡®å®šæ¨¡å‹åœ¨å“ªäº›æ–¹é¢éœ€è¦æ”¹è¿›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPCA-Bench<br>æœ¬æ–‡æå‡ºäº†PCA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­å†³ç­–èƒ½åŠ›çš„å¤šæ¨¡æ€å†³ç­–åŸºå‡†ã€‚PCA-Benchå¼•å…¥äº†ä¸‰ä¸ªå¤æ‚çš„åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœºå™¨äººå’Œå¼€æ”¾ä¸–ç•Œæ¸¸æˆã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®ä»»åŠ¡æŒ‡ä»¤å’Œä¸åŒçš„ä¸Šä¸‹æ–‡ï¼Œæ— ç¼åœ°æ•´åˆæ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œä»¥åšå‡ºå‡†ç¡®çš„å†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šPCA-Eval<br>ä¸ºäº†å¹³è¡¡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†PCA-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ã€‚PCA-Evalåˆ©ç”¨LLMså¼ºå¤§çš„è¯­ä¹‰è§£æèƒ½åŠ›ï¼Œæ ¹æ®æ•°æ®æ³¨é‡Šä¸­çš„é”šç‚¹ä¿¡æ¯ï¼Œè‡ªåŠ¨è¿›è¡Œé”™è¯¯å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCA-Evalä¸äººç±»è¯„ä¼°ç»“æœå…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ï¼Œå¹³å‡Kappaç³»æ•°è¾¾åˆ°0.8+ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEmbodied-Instruction-Evolution (EIE)<br>ä¸ºäº†è§£å†³PCA-Benchæ•°æ®é›†æ ‡æ³¨å·¥ä½œé‡å¤§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Embodied-Instruction-Evolution (EIE)æ¡†æ¶ã€‚EIEåˆ©ç”¨LLMsè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹ï¼Œä»è€Œå‡å°‘äº†äººå·¥åŠ³åŠ¨ï¼Œå¹¶æé«˜äº†PCA-Benchçš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4 Visionåœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç°æœ‰çš„å¼€æºMLLMsã€‚EIEæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜å¼€æºMLLMsçš„æ€§èƒ½ï¼Œåœ¨æŸäº›æŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡äº†GPT-4 Visionã€‚PCA-Evalèƒ½å¤Ÿæœ‰æ•ˆåœ°å®šä½æ¨¡å‹é”™è¯¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„PCA-Benchå’ŒPCA-Evalä¸ºè¯„ä¼°MLLMsçš„å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œè¯„ä¼°å·¥å…·ã€‚EIEæ¡†æ¶ä¸ºè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„MLLMsåœ¨å…·èº«æ™ºèƒ½ä½“ä¸­çš„å†³ç­–èƒ½åŠ›å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä¸ºMLLMsçš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>96</th>
      <td>The Text-Based Adventure AI Competition</td>
      <td>In 2016, 2017, and 2018 at the IEEE Conference on Computational Intelligence<br>in Games, the authors of this paper ran a competition for agents that can play<br>classic text-based adventure games. This competition fills a gap in existing<br>game AI competitions that have typically focussed on traditional card/board<br>games or modern video games with graphical interfaces. By providing a platform<br>for evaluating agents in text-based adventures, the competition provides a<br>novel benchmark for game AI with unique challenges for natural language<br>understanding and generation. This paper summarises the three competitions ran<br>in 2016, 2017, and 2018 (including details of open source implementations of<br>both the competition framework and our competitors) and presents the results of<br>an improved evaluation of these competitors across 20 games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›ï¼šæ¢ç´¢è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„æŒ‘æˆ˜<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å›¾å½¢åŒ–ç•Œé¢çš„æ¸¸æˆAIç«èµ›çš„å…´èµ·ï¼Œæ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›å¡«è¡¥äº†ç°æœ‰æ¸¸æˆAIç«èµ›çš„ç©ºç™½ã€‚æ–‡æœ¬å†’é™©æ¸¸æˆä¸ºæ¸¸æˆAIæä¾›äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†2016å¹´è‡³2018å¹´åœ¨IEEEè®¡ç®—æ™ºèƒ½ä¸æ¸¸æˆä¼šè®®ä¸Šä¸¾åŠçš„æ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›ï¼Œå¹¶æ€»ç»“äº†ç«èµ›çš„æˆæœå’Œç»éªŒã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«èµ›æ¡†æ¶<br>ç«èµ›æ¡†æ¶åŸºäºZPlet Javaè§£é‡Šå™¨ï¼Œç”¨äºè¯„ä¼°è½¯ä»¶ä»£ç†åœ¨æ–‡æœ¬å†’é™©æ¸¸æˆä¸­çš„èƒ½åŠ›ã€‚ä»£ç†é€šè¿‡ä¸€ä¸ªæ¥å£ä¸æ¸¸æˆäº¤äº’ï¼Œè¯¥æ¥å£æ¥æ”¶æ¸¸æˆç¯å¢ƒæè¿°å¹¶è¿”å›ä»£ç†æƒ³è¦æ‰§è¡Œçš„åŠ¨ä½œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç§ä»£ç†ç®—æ³•<br>ç«èµ›ä¸­æäº¤äº†å¤šç§ä»£ç†ç®—æ³•ï¼ŒåŒ…æ‹¬BYUAgent 2016ã€Golovinã€CARL (BYUAgent 2017) å’Œ NAILã€‚è¿™äº›ä»£ç†ç®—æ³•ä½¿ç”¨äº†ä¸åŒçš„æ–¹æ³•æ¥ç”Ÿæˆå‘½ä»¤å’Œä¸æ¸¸æˆäº¤äº’ï¼Œä¾‹å¦‚åŸºäºword2vecçš„åŠ¨è¯-åè¯åŒ¹é…ã€å‘½ä»¤æ¨¡å¼ç”Ÿæˆå’ŒçŸ¥è¯†å›¾è°±æ„å»ºã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ç«èµ›ç»“æœè¡¨æ˜ï¼ŒNAILä»£ç†åœ¨2018å¹´ç«èµ›ä¸­è¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯CARLå’ŒGolovinã€‚ä¸2016å¹´å’Œ2017å¹´ç›¸æ¯”ï¼Œæ‰€æœ‰ä»£ç†çš„æ€§èƒ½éƒ½æœ‰æ‰€æé«˜ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„ä»£ç†ï¼Œä¹Ÿåªèƒ½å®Œæˆæµ‹è¯•æ¸¸æˆä¸­çš„ä¸€å°éƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæ¸¸æˆä¸­æ— æ³•è·å¾—ä»»ä½•åˆ†æ•°ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›ä¸ºæ¸¸æˆAIç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œå¹¶å¼ºè°ƒäº†è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆåœ¨æ¸¸æˆAIä¸­çš„é‡è¦æ€§ã€‚ç«èµ›ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„ä»£ç†ç®—æ³•åœ¨å¤„ç†æ–‡æœ¬å†’é™©æ¸¸æˆçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´æœ‰æ•ˆçš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•å°†æ–‡æœ¬å†’é™©æ¸¸æˆAIåº”ç”¨äºç°å®ä¸–ç•Œçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models</td>
      <td>While the situation has improved for text-only models, it again seems to be<br>the case currently that multimodal (text and image) models develop faster than<br>ways to evaluate them. In this paper, we bring a recently developed evaluation<br>paradigm from text models to multimodal models, namely evaluation through the<br>goal-oriented game (self) play, complementing reference-based and<br>preference-based evaluation. Specifically, we define games that challenge a<br>model's capability to represent a situation from visual information and align<br>such representations through dialogue. We find that the largest closed models<br>perform rather well on the games that we define, while even the best<br>open-weight models struggle with them. On further analysis, we find that the<br>exceptional deep captioning capabilities of the largest models drive some of<br>the performance. There is still room to grow for both kinds of models, ensuring<br>the continued relevance of the benchmark.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆåŒ–è¯„ä¼°ï¼šæ¢ç©¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„å¤šæ¨¡æ€å’Œå¯¹è¯å¼æ¥åœ°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå‚è€ƒå¼è¯„ä¼°ï¼Œéš¾ä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§æ–°çš„è¯„ä¼°èŒƒå¼ï¼Œå³é€šè¿‡ç›®æ ‡å¯¼å‘çš„æ¸¸æˆï¼ˆè‡ªæˆ‘ï¼‰ç©æ³•æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ï¼Œä»¥è¡¥å……ç°æœ‰çš„å‚è€ƒå¼å’Œåå¥½å¼è¯„ä¼°æ–¹æ³•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ¸¸æˆåŒ–è¯„ä¼°èŒƒå¼åº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹<br>æœ¬æ–‡å€Ÿé‰´äº†æ–‡æœ¬æ¨¡å‹ä¸­æ–°å…´çš„æ¸¸æˆåŒ–è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹ã€‚é€šè¿‡å®šä¹‰ä¸‰ç§å¯¹è¯æ¸¸æˆï¼ˆå‚è€ƒæ¸¸æˆã€å›¾åƒæ¯”è¾ƒæ¸¸æˆå’Œå¯¼èˆªæ¸¸æˆï¼‰ï¼ŒæŒ‘æˆ˜æ¨¡å‹ä»è§†è§‰ä¿¡æ¯ä¸­æ„å»ºæƒ…å¢ƒæ¨¡å‹å¹¶é€šè¿‡å¯¹è¯è¿›è¡Œå¯¹é½çš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå¤šæ¨¡æ€æ¸¸æˆæ¡†æ¶<br>æœ¬æ–‡ä½¿ç”¨ clemgame/clembench æ¡†æ¶æ¥å®ç°æ¸¸æˆåŒ–è¯„ä¼°ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæ¨¡æ¿æ¥å®šä¹‰æ¸¸æˆç›®æ ‡ï¼Œå¹¶é€šè¿‡ç¨‹åºåŒ–çš„æ¸¸æˆå¤§å¸ˆæ¥æ§åˆ¶æ¸¸æˆæµç¨‹å’Œè¯„åˆ†è§„åˆ™ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹é—­æºæ¨¡å‹åœ¨æœ¬æ–‡å®šä¹‰çš„æ¸¸æˆä¸­è¡¨ç°è‰¯å¥½ï¼Œè€Œå³ä½¿æ˜¯æœ€å¥½çš„å¼€æºæ¨¡å‹ä¹Ÿéš¾ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼Œå¤§å‹æ¨¡å‹åœ¨æ·±åº¦å›¾åƒæè¿°æ–¹é¢çš„å‡ºè‰²èƒ½åŠ›æ¨åŠ¨äº†éƒ¨åˆ†æ€§èƒ½æå‡ã€‚è¿™è¡¨æ˜ï¼Œæ— è®ºæ˜¯é—­æºæ¨¡å‹è¿˜æ˜¯å¼€æºæ¨¡å‹ï¼Œéƒ½ä»æœ‰å¾ˆå¤§çš„å‘å±•ç©ºé—´ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¸¸æˆåŒ–è¯„ä¼°æ–¹æ³•ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤šæ¨¡æ€æ¸¸æˆæ¡†æ¶ä¹Ÿä¸ºå…¶ä»–ç ”ç©¶äººå‘˜æä¾›äº†å¯å¤ç°çš„å®éªŒå¹³å°ã€‚<br><br>## ğŸ“š å‚è€ƒæ–‡çŒ®<br>* Chalamalasetti, V., et al. (2023). clemgame: A framework for evaluating language models through goal-oriented games. arXiv preprint arXiv:2304.01213.<br>* Chiang, D., et al. (2024). Chatbot Arena: A preference-based evaluation platform for conversational agents. arXiv preprint arXiv:2403.04528.<br>* Qiao, Y., et al. (2023). Evaluating language models through interactive games. arXiv preprint arXiv:2303.08574.</td>
    </tr>
    <tr>
      <th>46</th>
      <td>DesignGPT: Multi-Agent Collaboration in Design</td>
      <td>Generative AI faces many challenges when entering the product design<br>workflow, such as interface usability and interaction patterns. Therefore,<br>based on design thinking and design process, we developed the DesignGPT<br>multi-agent collaboration framework, which uses artificial intelligence agents<br>to simulate the roles of different positions in the design company and allows<br>human designers to collaborate with them in natural language. Experimental<br>results show that compared with separate AI tools, DesignGPT improves the<br>performance of designers, highlighting the potential of applying multi-agent<br>systems that integrate design domain knowledge to product scheme design.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | DesignGPTï¼šè®¾è®¡æµç¨‹ä¸­çš„å¤šæ™ºèƒ½ä½“åä½œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‘å±•ï¼Œç”Ÿæˆå¼AIå·¥å…·åœ¨äº§å“è®¾è®¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå¦‚MidJourneyå’ŒStable Diffusionç­‰å›¾åƒç”Ÿæˆå·¥å…·ï¼Œä»¥åŠChatGPTç­‰æ–‡æœ¬ç”Ÿæˆå·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆå¼AIå·¥å…·åœ¨äº§å“è®¾è®¡çš„å®é™…åº”ç”¨ä¸­é¢ä¸´ç€ç•Œé¢å¯ç”¨æ€§å’Œäº¤äº’æ¨¡å¼ç­‰æŒ‘æˆ˜ï¼Œä¸”è®¾è®¡æ€ç»´ä¸æœºå™¨æ€ç»´ä¹‹é—´å­˜åœ¨å¤©ç„¶é¸¿æ²Ÿã€‚å¦‚ä½•è®©AIæ›´å¥½åœ°ç†è§£è®¾è®¡æ€ç»´ï¼Œæ˜¯è®¾è®¡å¸ˆä¸AIäº¤äº’çš„ä¸€å¤§æŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†DesignGPTï¼Œä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„è®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©è®¾è®¡å¸ˆä¸AIæ™ºèƒ½ä½“è¿›è¡Œè‡ªç„¶è¯­è¨€åä½œï¼Œå®Œæˆäº§å“æ–¹æ¡ˆè®¾è®¡ã€‚DesignGPTçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬éœ€æ±‚å¯¼å…¥è¡¨å•ã€è§’è‰²å®šä¹‰ä¸é€‰æ‹©ä»¥åŠä¼šè®®å®¤ã€‚ç³»ç»Ÿåˆå§‹åŒ–äº†å¤šä¸ªæ¨¡æ‹Ÿè®¾è®¡å…¬å¸ä¸åŒèŒä½çš„å‘˜å·¥è§’è‰²ï¼Œå¦‚è™šæ‹Ÿç”¨æˆ·ã€è€æ¿ã€äº§å“ç»ç†ã€è®¾è®¡æ€»ç›‘ã€CMFè®¾è®¡å¸ˆã€è¯„åˆ†è®°å½•å‘˜å’ŒæŠ€æœ¯äººå‘˜ï¼Œæ¯ä¸ªè§’è‰²éƒ½æœ‰æ˜ç¡®çš„èŒè´£å’Œä»»åŠ¡ã€‚è®¾è®¡å¸ˆç”¨æˆ·å¯ä»¥è¾“å…¥è®¾è®¡éœ€æ±‚ï¼Œé€‰æ‹©è§’è‰²å¹¶å¼€å§‹ä¼šè®®ï¼Œä¸AIæ™ºèƒ½ä½“è¿›è¡Œè‡ªç„¶è¯­è¨€äº¤æµï¼Œå…±åŒå®Œæˆéœ€æ±‚åˆ†æã€è®¾è®¡ææ¡ˆã€è¯¦ç»†è®¾è®¡å’ŒæŠ€æœ¯è¿­ä»£ç­‰å·¥ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ä¸ºäº†è¯„ä¼°DesignGPTçš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡è¿›è¡Œäº†ä¸€é¡¹åœ¨çº¿å®éªŒï¼Œå°†å‚ä¸è€…éšæœºåˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«ä½¿ç”¨DesignGPTå’Œå•ç‹¬çš„AIå·¥å…·è¿›è¡Œè®¾è®¡ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸å•ç‹¬ä½¿ç”¨AIå·¥å…·ç›¸æ¯”ï¼ŒDesignGPTåœ¨è®¾è®¡çš„åˆ›æ–°æ€§ã€å®Œæ•´æ€§å’Œå¯è¡Œæ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œå‚ä¸è€…å¯¹DesignGPTçš„äº¤äº’å½¢å¼ç»™äºˆäº†é«˜åº¦è¯„ä»·ï¼Œè®¤ä¸ºAIè§’è‰²èƒ½å¤Ÿç†Ÿç»ƒè¿ç”¨è®¾è®¡æµç¨‹å’Œè®¾è®¡è¡¨è¾¾ï¼Œå¹¶ä»å¤šä¸ªè§’åº¦æ¨å¯¼è®¾è®¡æ–¹æ¡ˆï¼Œçªç ´äº†ä»¥å¾€è®¾è®¡æ–¹æ¡ˆæ„æ€ä¸­è§†è§’ç›¸å¯¹å•ä¸€çš„é—®é¢˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DesignGPTçš„ç ”ç©¶ä¸ºç†è§£AIåœ¨è®¾è®¡æµç¨‹ä¸­çš„ä½œç”¨æä¾›äº†é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚å…¶å¤šæ™ºèƒ½ä½“åä½œçš„è®¾è®¡æ¡†æ¶ä¸ºè®¾è®¡å¸ˆä¸AIçš„åä½œæä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæé«˜è®¾è®¡æ•ˆç‡å’Œè®¾è®¡è´¨é‡ã€‚æ­¤å¤–ï¼ŒDesignGPTçš„å®éªŒç»“æœä¹Ÿè¡¨æ˜ï¼Œå°†è®¾è®¡é¢†åŸŸçŸ¥è¯†ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç›¸ç»“åˆï¼Œåœ¨äº§å“æ–¹æ¡ˆè®¾è®¡ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>78</th>
      <td>Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf</td>
      <td>Communication games, which we refer to as incomplete information games that<br>heavily depend on natural language communication, hold significant research<br>value in fields such as economics, social science, and artificial intelligence.<br>In this work, we explore the problem of how to engage large language models<br>(LLMs) in communication games, and in response, propose a tuning-free<br>framework. Our approach keeps LLMs frozen, and relies on the retrieval and<br>reflection on past communications and experiences for improvement. An empirical<br>study on the representative and widely-studied communication game,<br>``Werewolf'', demonstrates that our framework can effectively play Werewolf<br>game without tuning the parameters of the LLMs. More importantly, strategic<br>behaviors begin to emerge in our experiments, suggesting that it will be a<br>fruitful journey to engage LLMs in communication games and associated domains.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ²Ÿé€šæ¸¸æˆä¸­çš„åº”ç”¨ï¼šä»¥ç‹¼äººæ€ä¸ºä¾‹çš„å®è¯ç ”ç©¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ²Ÿé€šæ¸¸æˆï¼Œå¦‚ç‹¼äººæ€ï¼Œæ˜¯ä¸€ç§é‡è¦çš„ç ”ç©¶å·¥å…·ï¼Œå¯ä»¥ç”¨æ¥æ¢ç´¢ç»æµå­¦ã€ç¤¾ä¼šç§‘å­¦å’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸä¸­çš„å„ç§é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIä»£ç†åœ¨ç©è¿™ç±»æ¸¸æˆæ—¶ï¼Œè¦ä¹ˆå¯¹è¯­è¨€çš„ä½¿ç”¨æœ‰ä¸¥æ ¼çš„é™åˆ¶ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨æ•°æ®ï¼Œè¿™ä½¿å¾—AIä»£ç†åœ¨è‡ªç„¶åœ°ç©è¿™ç±»æ¸¸æˆæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œç”¨äºç©æ²Ÿé€šæ¸¸æˆï¼Œå¹¶ä»¥ç‹¼äººæ€ä¸ºä¾‹è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå†å²ä¿¡æ¯æ”¶é›†<br>ä¸ºäº†è§£å†³LLMçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ä¸‰ä¸ªè§’åº¦ï¼ˆæ–°é²œåº¦ã€ä¿¡æ¯é‡å’Œå®Œæ•´æ€§ï¼‰æ”¶é›†å†å²ä¿¡æ¯çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ï¼š<br>- æ”¶é›†æœ€è¿‘çš„Kæ¡æ¶ˆæ¯ï¼›<br>- ä½¿ç”¨è§„åˆ™åŒ¹é…å’Œå¯å‘å¼æŒ‡æ ‡é€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„Næ¡æ¶ˆæ¯ï¼›<br>- é€šè¿‡å›ç­”é—®é¢˜çš„æ–¹å¼ï¼Œä»æ•´ä¸ªå†å²ä¸­æå–æ›´å¤šä¿¡æ¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»éªŒå­¦ä¹ <br>ä¸ºäº†ä½¿LLMèƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§éå‚æ•°å­¦ä¹ æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ï¼š<br>- åœ¨æ¯è½®æ¸¸æˆç»“æŸåï¼Œæ”¶é›†æ‰€æœ‰ç©å®¶çš„å“åº”ã€åæ€å’Œå¾—åˆ†ï¼Œå½¢æˆç»éªŒæ± ï¼›<br>- åœ¨æ–°çš„ä¸€è½®æ¸¸æˆä¸­ï¼Œæ ¹æ®å½“å‰æƒ…å†µä»ç»éªŒæ± ä¸­æ£€ç´¢æœ€ç›¸å…³çš„ç»éªŒï¼Œå¹¶ä»ä¸­æå–å»ºè®®ï¼Œä»¥æŒ‡å¯¼LLMçš„æ¨ç†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ç©ç‹¼äººæ€æ¸¸æˆï¼Œå¹¶ä¸”èƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€å¾®è°ƒLLMçš„å‚æ•°ã€‚æ­¤å¤–ï¼Œå®éªŒä¸­è¿˜è§‚å¯Ÿåˆ°ä¸€äº›ç­–ç•¥æ€§è¡Œä¸ºï¼Œå¦‚ä¿¡ä»»ã€å¯¹æŠ—ã€ä¼ªè£…å’Œé¢†å¯¼ï¼Œè¿™äº›è¡Œä¸ºå¹¶éé¢„å…ˆç¼–ç¨‹ï¼Œè€Œæ˜¯è‡ªå‘åœ°ä»LLMä¸­æ¶Œç°å‡ºæ¥çš„ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºä½¿ç”¨LLMç©æ²Ÿé€šæ¸¸æˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ç ”ç©¶LLMåœ¨æ²Ÿé€šæ¸¸æˆä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ç»éªŒå­¦ä¹ æœºåˆ¶ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿå’Œæ¨èç³»ç»Ÿã€‚</td>
    </tr>
    <tr>
      <th>14</th>
      <td>MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control</td>
      <td>It is a long-lasting goal to design a generalist-embodied agent that can<br>follow diverse instructions in human-like ways. However, existing approaches<br>often fail to steadily follow instructions due to difficulties in understanding<br>abstract and sequential natural language instructions. To this end, we<br>introduce MineDreamer, an open-ended embodied agent built upon the challenging<br>Minecraft simulator with an innovative paradigm that enhances<br>instruction-following ability in low-level control signal generation.<br>Specifically, MineDreamer is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs) and diffusion models, and we employ a<br>Chain-of-Imagination (CoI) mechanism to envision the step-by-step process of<br>executing instructions and translating imaginations into more precise visual<br>prompts tailored to the current state; subsequently, the agent generates<br>keyboard-and-mouse actions to efficiently achieve these imaginations, steadily<br>following the instructions at each step. Extensive experiments demonstrate that<br>MineDreamer follows single and multi-step instructions steadily, significantly<br>outperforming the best generalist agent baseline and nearly doubling its<br>performance. Moreover, qualitative analysis of the agent's imaginative ability<br>reveals its generalization and comprehension of the open world.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MineDreamerï¼šåŸºäºæƒ³è±¡é“¾çš„æ¨¡æ‹Ÿä¸–ç•Œæ§åˆ¶æŒ‡ä»¤è·Ÿéš<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä»¥äººç±»æ–¹å¼ç†è§£å’Œæ‰§è¡Œå¤šæ ·åŒ–æŒ‡ä»¤çš„é€šç”¨å‹å…·èº«æ™ºèƒ½ä½“ä¸€ç›´æ˜¯é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€éš¾ä»¥ç¨³å®šåœ°éµå¾ªæŒ‡ä»¤ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å’Œæ‰§è¡ŒæŠ½è±¡å’Œé¡ºåºçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢å­˜åœ¨å›°éš¾ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥â€œæƒ³è±¡é“¾â€ï¼ˆChain-of-Imagination, CoIï¼‰æœºåˆ¶<br>MineDreamer é€šè¿‡ CoI æœºåˆ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®æŒ‡ä»¤å’Œå½“å‰çŠ¶æ€é€æ­¥æƒ³è±¡å¹¶æ‰§è¡ŒæŒ‡ä»¤ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»åœ¨è§£å†³é—®é¢˜æ—¶ï¼Œæ ¹æ®å½“å‰çŠ¶æ€é€æ­¥æƒ³è±¡ä¸‹ä¸€æ­¥ç›®æ ‡çš„è¿‡ç¨‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¢å¼ºçš„æ‰©æ•£æ¨¡å‹<br>MineDreamer ä½¿ç”¨ MLLM å¢å¼ºçš„æ‰©æ•£æ¨¡å‹æ¥ç”ŸæˆåŒ…å«ç‰©ç†è§„åˆ™å’Œç¯å¢ƒç†è§£çš„æƒ³è±¡å›¾åƒï¼Œè¿™äº›å›¾åƒä½œä¸ºæ›´ç²¾ç¡®çš„è§†è§‰æç¤ºï¼Œå¼•å¯¼æ™ºèƒ½ä½“ç”Ÿæˆä½çº§æ§åˆ¶ä¿¡å·ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•<br>ä¸ºäº†è®­ç»ƒ Imaginatorï¼ŒMineDreamer ä½¿ç”¨äº†ç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•æ¥æ”¶é›†å¤§é‡å…·èº«æ•°æ®ï¼Œå¸®åŠ© Imaginator ç†è§£å¦‚ä½•é€æ­¥å®ŒæˆæŒ‡ä»¤ä»¥åŠå¦‚ä½•é‡å¤å®ŒæˆæŒ‡ä»¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MineDreamer åœ¨æ‰§è¡Œå•æ­¥å’Œå¤šæ­¥æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºæœ€ä½³é€šç”¨å‹æ™ºèƒ½ä½“åŸºçº¿ï¼Œæ€§èƒ½å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼Œå¯¹æ™ºèƒ½ä½“æƒ³è±¡èƒ½åŠ›çš„å®šæ€§åˆ†æè¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿç†è§£å’Œé€‚åº”å¼€æ”¾ä¸–ç•Œçš„ç¯å¢ƒã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MineDreamer çš„ CoI æœºåˆ¶ä¸ºè§£å†³æŒ‡ä»¤è·Ÿéšé—®é¢˜æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå…¶ MLLM å¢å¼ºçš„æ‰©æ•£æ¨¡å‹å’Œç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•ä¹Ÿä¸ºå…·èº«æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒMineDreamer çš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œå…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>86</th>
      <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</td>
      <td>Remarkable progress has been made on automated problem solving through<br>societies of agents based on large language models (LLMs). Existing LLM-based<br>multi-agent systems can already solve simple dialogue tasks. Solutions to more<br>complex tasks, however, are complicated through logic inconsistencies due to<br>cascading hallucinations caused by naively chaining LLMs. Here we introduce<br>MetaGPT, an innovative meta-programming framework incorporating efficient human<br>workflows into LLM-based multi-agent collaborations. MetaGPT encodes<br>Standardized Operating Procedures (SOPs) into prompt sequences for more<br>streamlined workflows, thus allowing agents with human-like domain expertise to<br>verify intermediate results and reduce errors. MetaGPT utilizes an assembly<br>line paradigm to assign diverse roles to various agents, efficiently breaking<br>down complex tasks into subtasks involving many agents working together. On<br>collaborative software engineering benchmarks, MetaGPT generates more coherent<br>solutions than previous chat-based multi-agent systems. Our project can be<br>found at https://github.com/geekan/MetaGPT</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MetaGPTï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…ƒç¼–ç¨‹å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäºLLMsçš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–é—®é¢˜è§£å†³æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ—¶ï¼Œç”±äºç®€å•åœ°å°†LLMsä¸²è”èµ·æ¥å¯¼è‡´çš„çº§è”å¹»è§‰ï¼Œå¸¸å¸¸å‡ºç°é€»è¾‘ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MetaGPTï¼Œä¸€ä¸ªåˆ›æ–°çš„å…ƒç¼–ç¨‹æ¡†æ¶ï¼Œå°†é«˜æ•ˆçš„äººç±»å·¥ä½œæµç¨‹èå…¥åˆ°åŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“åä½œä¸­ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ ‡å‡†åŒ–æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰ç¼–ç åˆ°æç¤ºåºåˆ—ä¸­ï¼Œä»¥å®ç°æ›´æµç•…çš„å·¥ä½œæµç¨‹ã€‚è¿™ä½¿å¾—å…·æœ‰äººç±»æ°´å¹³çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„æ™ºèƒ½ä½“èƒ½å¤ŸéªŒè¯ä¸­é—´ç»“æœå¹¶å‡å°‘é”™è¯¯ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡‡ç”¨æµæ°´çº¿èŒƒå¼ï¼Œä¸ºå„ç§æ™ºèƒ½ä½“åˆ†é…ä¸åŒçš„è§’è‰²ï¼Œæœ‰æ•ˆåœ°å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ¶‰åŠå¤šä¸ªæ™ºèƒ½ä½“åä½œçš„å­ä»»åŠ¡ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥å¯æ‰§è¡Œåé¦ˆæœºåˆ¶ï¼Œåœ¨è¿è¡Œæ—¶è°ƒè¯•å’Œæ‰§è¡Œä»£ç ï¼Œæ˜¾è‘—æé«˜ä»£ç ç”Ÿæˆè´¨é‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨åä½œè½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaGPTç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆæ¯”ä¹‹å‰çš„åŸºäºèŠå¤©çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ›´è¿è´¯ã€‚åœ¨HumanEvalå’ŒMBPPåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaGPTå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«è¾¾åˆ°äº†85.9%å’Œ87.7%çš„Pass@1ã€‚åœ¨è‡ªç”Ÿæˆçš„è½¯ä»¶å¼€å‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaGPTåœ¨å‡ ä¹æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºChatDevï¼Œå¹¶ä¸”å®ç°äº†100%çš„ä»»åŠ¡å®Œæˆç‡ï¼Œè¯æ˜äº†å…¶é²æ£’æ€§å’Œæ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MetaGPTæ¡†æ¶ä¸ºåŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¼€å‘æä¾›äº†ä¸€ä¸ªçµæ´»ä¸”åŠŸèƒ½å¼ºå¤§çš„å¹³å°ã€‚å…¶å°†SOPsèå…¥è®¾è®¡ä¸­çš„åˆ›æ–°æ–¹æ³•ï¼Œä¸ºè§£å†³å¤æ‚é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œå¯æ‰§è¡Œåé¦ˆæœºåˆ¶ä¹Ÿä¸ºæé«˜ä»£ç ç”Ÿæˆè´¨é‡æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚MetaGPTçš„æˆåŠŸç»éªŒå¯ä»¥å¯å‘æœªæ¥ç ”ç©¶ï¼Œæ¢ç´¢æ›´å¤šåŸºäºäººç±»å®è·µçš„äººå·¥å¤šæ™ºèƒ½ä½“ç³»ç»ŸæŠ€æœ¯ã€‚</td>
    </tr>
    <tr>
      <th>106</th>
      <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
      <td>Believable proxies of human behavior can empower interactive applications<br>ranging from immersive environments to rehearsal spaces for interpersonal<br>communication to prototyping tools. In this paper, we introduce generative<br>agents--computational software agents that simulate believable human behavior.<br>Generative agents wake up, cook breakfast, and head to work; artists paint,<br>while authors write; they form opinions, notice each other, and initiate<br>conversations; they remember and reflect on days past as they plan the next<br>day. To enable generative agents, we describe an architecture that extends a<br>large language model to store a complete record of the agent's experiences<br>using natural language, synthesize those memories over time into higher-level<br>reflections, and retrieve them dynamically to plan behavior. We instantiate<br>generative agents to populate an interactive sandbox environment inspired by<br>The Sims, where end users can interact with a small town of twenty five agents<br>using natural language. In an evaluation, these generative agents produce<br>believable individual and emergent social behaviors: for example, starting with<br>only a single user-specified notion that one agent wants to throw a Valentine's<br>Day party, the agents autonomously spread invitations to the party over the<br>next two days, make new acquaintances, ask each other out on dates to the<br>party, and coordinate to show up for the party together at the right time. We<br>demonstrate through ablation that the components of our agent<br>architecture--observation, planning, and reflection--each contribute critically<br>to the believability of agent behavior. By fusing large language models with<br>computational, interactive agents, this work introduces architectural and<br>interaction patterns for enabling believable simulations of human behavior.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„äº¤äº’å¼æ¨¡æ‹Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œäººä»¬å¯¹äºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„æ™ºèƒ½ä½“äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚è¿™äº›æ™ºèƒ½ä½“å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ²‰æµ¸å¼ç¯å¢ƒã€äººé™…æ²Ÿé€šæ¼”ç»ƒç©ºé—´ã€åŸå‹è®¾è®¡å·¥å…·ç­‰ã€‚ç„¶è€Œï¼Œè¦åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿé•¿æœŸä¿æŒä¸€è‡´æ€§å’Œå¯ä¿¡åº¦çš„æ™ºèƒ½ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¼æ™ºèƒ½ä½“<br>æœ¬æ–‡æå‡ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„æ¦‚å¿µï¼Œå³åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¨¡æ‹Ÿå¯ä¿¡çš„äººç±»è¡Œä¸ºã€‚è¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œæ—¥å¸¸æ´»åŠ¨ï¼Œå¦‚èµ·åºŠã€åšé¥­ã€ä¸Šç­ç­‰ï¼Œå¹¶èƒ½å¤Ÿå½¢æˆè‡ªå·±çš„è§‚ç‚¹ã€ä¸ä»–äººäº’åŠ¨ã€å‘èµ·å¯¹è¯ç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ™ºèƒ½ä½“æ¶æ„<br>ä¸ºäº†å®ç°ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œè¯¥æ¶æ„æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­˜å‚¨æ™ºèƒ½ä½“çš„ç»éªŒè®°å½•ï¼Œå¹¶å°†è¿™äº›è®°å¿†éšç€æ—¶é—´çš„æ¨ç§»åˆæˆæ›´é«˜å±‚æ¬¡çš„åæ€ï¼Œå¹¶åŠ¨æ€åœ°æ£€ç´¢å®ƒä»¬æ¥è§„åˆ’è¡Œä¸ºã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­åˆ›å»ºä¸€ä¸ªç”±25ä¸ªæ™ºèƒ½ä½“ç»„æˆçš„å°é•‡ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿäº§ç”Ÿå¯ä¿¡çš„ä¸ªä½“å’Œç¾¤ä½“è¡Œä¸ºï¼Œä¾‹å¦‚ï¼Œåœ¨ç”¨æˆ·æŒ‡å®šä¸€ä¸ªæ™ºèƒ½ä½“æƒ³è¦ä¸¾åŠæƒ…äººèŠ‚æ´¾å¯¹çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»åœ°é‚€è¯·å…¶ä»–æ™ºèƒ½ä½“å‚åŠ æ´¾å¯¹ï¼Œå¹¶åè°ƒåœ¨æ­£ç¡®çš„æ—¶é—´ä¸€èµ·åˆ°è¾¾æ´¾å¯¹åœ°ç‚¹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“æ¶æ„ä¸ºåˆ›å»ºå¯ä¿¡çš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚è¯¥æ¶æ„å¯ä»¥åº”ç”¨äºå„ç§é¢†åŸŸï¼Œä¾‹å¦‚è§’è‰²æ‰®æ¼”ã€ç¤¾äº¤åŸå‹è®¾è®¡ã€è™šæ‹Ÿä¸–ç•Œå’Œæ¸¸æˆç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨äº¤äº’å¼ç³»ç»Ÿä¸­çš„åº”ç”¨æœºä¼šã€ä¼¦ç†å’Œç¤¾ä¼šé£é™©ã€‚</td>
    </tr>
    <tr>
      <th>62</th>
      <td>AvalonBench: Evaluating LLMs Playing the Game of Avalon</td>
      <td>In this paper, we explore the potential of Large Language Models (LLMs)<br>Agents in playing the strategic social deduction game, Resistance Avalon.<br>Players in Avalon are challenged not only to make informed decisions based on<br>dynamically evolving game phases, but also to engage in discussions where they<br>must deceive, deduce, and negotiate with other players. These characteristics<br>make Avalon a compelling test-bed to study the decision-making and<br>language-processing capabilities of LLM Agents. To facilitate research in this<br>line, we introduce AvalonBench - a comprehensive game environment tailored for<br>evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game<br>environment for Avalon, (2) rule-based bots as baseline opponents, and (3)<br>ReAct-style LLM agents with tailored prompts for each role. Notably, our<br>evaluations based on AvalonBench highlight a clear capability gap. For<br>instance, models like ChatGPT playing good-role got a win rate of 22.2% against<br>rule-based bots playing evil, while good-role bot achieves 38.2% win rate in<br>the same setting. We envision AvalonBench could be a good test-bed for<br>developing more advanced LLMs (with self-playing) and agent frameworks that can<br>effectively model the layered complexities of such game environments.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AvalonBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç¤¾äº¤æ¨ç†æ¸¸æˆå¦‚ Resistance Avalon å¯¹ç©å®¶çš„æ¨ç†ã€æ²Ÿé€šå’Œå†³ç­–èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚è¿™äº›æ¸¸æˆè¦æ±‚ç©å®¶åœ¨åŠ¨æ€å˜åŒ–çš„æ¸¸æˆé˜¶æ®µåšå‡ºæ˜æ™ºçš„å†³ç­–ï¼Œå¹¶åœ¨è®¨è®ºä¸­æ¬ºéª—ã€æ¨ç†å’Œä¸å…¶ä»–ç©å®¶åå•†ã€‚è¿™äº›ç‰¹ç‚¹ä½¿å¾— Avalon æˆä¸ºç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å†³ç­–å’Œè¯­è¨€å¤„ç†èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„è¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†æ¸¸æˆç¯å¢ƒä¸­çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº† AvalonBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šä»£ç† LLM ä»£ç†çš„æ¸¸æˆç¯å¢ƒã€‚AvalonBench åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š<br><br>1. **Avalon æ¸¸æˆç¯å¢ƒ**ï¼šä¸ºä»£ç†æä¾›æ¸¸æˆå¹³å°ï¼Œè®°å½•æ‰€æœ‰ç©å®¶çš„è¡ŒåŠ¨å¹¶æ¨åŠ¨æ¸¸æˆè¿›ç¨‹ã€‚<br>2. **åŸºäºè§„åˆ™çš„æœºå™¨äºº**ï¼šä½œä¸ºåŸºçº¿å¯¹æ‰‹ï¼Œä¸ºä»£ç†æä¾›å¯æ¯”è¾ƒçš„åŸºå‡†ã€‚<br>3. **ReAct é£æ ¼çš„ LLM ä»£ç†**ï¼šé’ˆå¯¹æ¯ä¸ªè§’è‰²å®šåˆ¶æç¤ºï¼Œä»¥è¯„ä¼° LLM ä»£ç†åœ¨ä¸åŒè§’è‰²ä¸‹çš„è¡¨ç°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨ AvalonBench å¯¹ ChatGPT-3.5 å’Œ Llama2 æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸åŸºäºè§„åˆ™çš„æœºå™¨äººè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ‰è®¨è®ºçš„æƒ…å†µä¸‹ï¼ŒLLM ä»£ç†çš„è¡¨ç°ä¹Ÿè¿œä½äºåŸºäºè§„åˆ™çš„æœºå™¨äººã€‚ä¾‹å¦‚ï¼ŒChatGPT-3.5 åœ¨æ‰®æ¼”å¥½äººè§’è‰²æ—¶ï¼Œåœ¨ä¸æ‰®æ¼”åäººçš„åŸºäºè§„åˆ™çš„æœºå™¨äººå¯¹æŠ—ä¸­ï¼Œèƒœç‡ä¸º 22.2%ï¼Œè€Œå¥½äººè§’è‰²çš„æœºå™¨äººèƒœç‡ä¸º 38.2%ã€‚è¿™è¡¨æ˜å½“å‰ LLM ä»£ç†åœ¨æ¨ç†ã€è¯´æœã€åå•†å’Œæ¬ºéª—èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾å·®è·ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AvalonBench ä¸ºç ”ç©¶ LLM ä»£ç†åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å…ˆè¿›çš„ LLM ä»£ç†ï¼Œå¹¶æ¢ç´¢å¦‚ä½•å°†å†³ç­–æŠ€æœ¯é›†æˆåˆ° LLM ä¸­ï¼Œä»¥æé«˜å…¶åœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒAvalonBench è¿˜å¯ä»¥ç”¨äºè¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†åä½œã€æ²Ÿé€šå’Œç­–ç•¥åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>116</th>
      <td>Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction</td>
      <td>We study the problem of learning goal-conditioned policies in Minecraft, a<br>popular, widely accessible yet challenging open-ended environment for<br>developing human-level multi-task agents. We first identify two main challenges<br>of learning such policies: 1) the indistinguishability of tasks from the state<br>distribution, due to the vast scene diversity, and 2) the non-stationary nature<br>of environment dynamics caused by partial observability. To tackle the first<br>challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage<br>the emergence of goal-relevant visual state representations. To tackle the<br>second challenge, the policy is further fueled by an adaptive horizon<br>prediction module that helps alleviate the learning uncertainty brought by the<br>non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method<br>significantly outperforms the best baseline so far; in many of them, we double<br>the performance. Our ablation and exploratory studies then explain how our<br>approach beat the counterparts and also unveil the surprising bonus of<br>zero-shot generalization to new scenes (biomes). We hope our agent could help<br>shed some light on learning goal-conditioned, multi-task agents in challenging,<br>open-ended environments like Minecraft.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åœ¨å¼€æ”¾ä¸–ç•Œä¸­å®ç°å¤šä»»åŠ¡æ§åˆ¶ï¼šç›®æ ‡æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ å’Œè‡ªé€‚åº”é¢„æµ‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œå¦‚Minecraftï¼Œä¸ºå¼€å‘èƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡çš„æ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å¹³å°ã€‚ç„¶è€Œï¼Œè¿™äº›ç¯å¢ƒä¹Ÿå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ï¼š<br>1. **çŠ¶æ€åˆ†å¸ƒçš„å¤šæ ·æ€§**ï¼šç”±äºåœºæ™¯çš„å¤šæ ·æ€§ï¼Œä¸åŒä»»åŠ¡çš„çŠ¶æ€éš¾ä»¥åŒºåˆ†ï¼Œè¿™ä½¿å¾—å­¦ä¹ ç›®æ ‡æ¡ä»¶ç­–ç•¥å˜å¾—å›°éš¾ã€‚<br>2. **ç¯å¢ƒåŠ¨æ€çš„éå¹³ç¨³æ€§**ï¼šç”±äºéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œç¯å¢ƒåŠ¨æ€å…·æœ‰éå¹³ç¨³æ€§ï¼Œå¯¼è‡´å­¦ä¹ çš„ä¸ç¡®å®šæ€§å¢åŠ ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä»¥ä¸‹åˆ›æ–°æ–¹æ³•ï¼š<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼š**ç›®æ ‡æ„ŸçŸ¥éª¨å¹²ç½‘ç»œï¼ˆGSB**ï¼‰<br>   - GSBé€šè¿‡åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šèåˆç›®æ ‡ä¿¡æ¯ï¼Œé¼“åŠ±å‡ºç°ä¸ç›®æ ‡ç›¸å…³çš„è§†è§‰çŠ¶æ€è¡¨ç¤ºï¼Œä»è€Œè§£å†³çŠ¶æ€åˆ†å¸ƒå¤šæ ·æ€§çš„é—®é¢˜ã€‚<br>   - GSBç”±å¤šä¸ªç›®æ ‡å·ç§¯å—ï¼ˆg-conv blockï¼‰ç»„æˆï¼Œè¿™äº›å—é€šè¿‡é€šé“è°ƒåˆ¶å°†ç›®æ ‡ä¿¡æ¯ä¸è§†è§‰ç‰¹å¾èåˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼š**è‡ªé€‚åº”é¢„æµ‹æ¨¡å—**<br>   - ä¸ºäº†åº”å¯¹ç¯å¢ƒåŠ¨æ€çš„éå¹³ç¨³æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†è‡ªé€‚åº”é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹ä»å½“å‰çŠ¶æ€åˆ°ç›®æ ‡çš„å‰©ä½™æ—¶é—´æ­¥æ•°ï¼ˆå³è·ç¦»åˆ°ç›®æ ‡çš„è·ç¦»ï¼‰ã€‚<br>   - è‡ªé€‚åº”é¢„æµ‹æ¨¡å—é€šè¿‡é¢„æµ‹å‰©ä½™æ—¶é—´æ­¥æ•°ï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£ç›®æ ‡çš„å®Œæˆç¨‹åº¦ï¼Œä»è€Œæé«˜å†³ç­–çš„å‡†ç¡®æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraftçš„20ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåœ¨è®¸å¤šä»»åŠ¡ä¸­æ€§èƒ½ç¿»å€ã€‚æ¶ˆèç ”ç©¶å’Œæ¢ç´¢æ€§ç ”ç©¶è§£é‡Šäº†æœ¬æ–‡æ–¹æ³•å¦‚ä½•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº†ä»¤äººæƒŠè®¶çš„é›¶æ ·æœ¬æ³›åŒ–åˆ°æ–°åœºæ™¯ï¼ˆç”Ÿç‰©ç¾¤è½ï¼‰çš„é¢å¤–ä¼˜åŠ¿ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GSBå’Œè‡ªé€‚åº”é¢„æµ‹æ¨¡å—ä¸ºåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å­¦ä¹ ç›®æ ‡æ¡ä»¶ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œå¤šä»»åŠ¡çš„æ™ºèƒ½ä½“æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Cradle: Empowering Foundation Agents Towards General Computer Control</td>
      <td>Despite the success in specific scenarios, existing foundation agents still<br>struggle to generalize across various virtual scenarios, mainly due to the<br>dramatically different encapsulations of environments with manually designed<br>observation and action spaces. To handle this issue, we propose the General<br>Computer Control (GCC) setting to restrict foundation agents to interact with<br>software through the most unified and standardized interface, i.e., using<br>screenshots as input and keyboard and mouse actions as output. We introduce<br>Cradle, a modular and flexible LMM-powered framework, as a preliminary attempt<br>towards GCC. Enhanced by six key modules, Cradle can understand input<br>screenshots and output executable code for low-level keyboard and mouse control<br>after high-level planning, so that Cradle can interact with any software and<br>complete long-horizon complex tasks without relying on any built-in APIs.<br>Experimental results show that Cradle exhibits remarkable generalizability and<br>impressive performance across four previously unexplored commercial video<br>games, five software applications, and a comprehensive benchmark, OSWorld.<br>Cradle is the first to enable foundation agents to follow the main storyline<br>and complete 40-minute-long real missions in the complex AAA game Red Dead<br>Redemption 2 (RDR2). Cradle can also create a city of a thousand people in<br>Cities: Skylines, farm and harvest parsnips in Stardew Valley, and trade and<br>bargain with a maximal weekly total profit of 87% in Dealer's Life 2. Cradle<br>can not only operate daily software, like Chrome, Outlook, and Feishu, but also<br>edit images and videos using Meitu and CapCut. Cradle greatly extends the reach<br>of foundation agents by enabling the easy conversion of any software,<br>especially complex games, into benchmarks to evaluate agents' various abilities<br>and facilitate further data collection, thus paving the way for generalist<br>agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Cradleï¼šè¿ˆå‘é€šç”¨è®¡ç®—æœºæ§åˆ¶çš„åŸºçŸ³<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç°æœ‰çš„åŸºç¡€æ™ºèƒ½ä½“åœ¨ç‰¹å®šåœºæ™¯ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨æ³›åŒ–åˆ°å„ç§è™šæ‹Ÿåœºæ™¯æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºç¯å¢ƒå°è£…çš„å·¨å¤§å·®å¼‚ï¼ŒåŒ…æ‹¬æ‰‹åŠ¨è®¾è®¡çš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é€šç”¨è®¡ç®—æœºæ§åˆ¶ï¼ˆGCCï¼‰è®¾ç½®ï¼Œé™åˆ¶åŸºç¡€æ™ºèƒ½ä½“é€šè¿‡æœ€ç»Ÿä¸€å’Œæ ‡å‡†åŒ–çš„æ¥å£ä¸è½¯ä»¶è¿›è¡Œäº¤äº’ï¼Œå³ä½¿ç”¨å±å¹•æˆªå›¾ä½œä¸ºè¾“å…¥å’Œé”®ç›˜é¼ æ ‡æ“ä½œä½œä¸ºè¾“å‡ºã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGCCè®¾ç½®<br>æœ¬æ–‡æå‡ºäº†GCCè®¾ç½®ï¼Œæ—¨åœ¨è®©åŸºç¡€æ™ºèƒ½ä½“é€šè¿‡ç»Ÿä¸€çš„æ¥å£ä¸è½¯ä»¶è¿›è¡Œäº¤äº’ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚GCCè®¾ç½®è¦æ±‚æ™ºèƒ½ä½“ä»…é€šè¿‡å±å¹•æˆªå›¾ä½œä¸ºè¾“å…¥å’Œé”®ç›˜é¼ æ ‡æ“ä½œä½œä¸ºè¾“å‡ºä¸è½¯ä»¶è¿›è¡Œäº¤äº’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCradleæ¡†æ¶<br>ä¸ºäº†å®ç°GCCè®¾ç½®ï¼Œæœ¬æ–‡æå‡ºäº†Cradleæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å’Œçµæ´»çš„LMMï¼ˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼‰é©±åŠ¨æ¡†æ¶ã€‚Cradleæ¡†æ¶ç”±å…­ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼šä¿¡æ¯æ”¶é›†ã€è‡ªæˆ‘åæ€ã€ä»»åŠ¡æ¨ç†ã€æŠ€èƒ½ç®¡ç†ã€åŠ¨ä½œè§„åˆ’å’Œè®°å¿†ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œä½¿Cradleèƒ½å¤Ÿç†è§£è¾“å…¥å±å¹•æˆªå›¾ï¼Œå¹¶åœ¨é«˜çº§è§„åˆ’åè¾“å‡ºå¯æ‰§è¡Œçš„ä»£ç ï¼Œä»¥è¿›è¡Œä½çº§é”®ç›˜å’Œé¼ æ ‡æ§åˆ¶ã€‚è¿™æ ·ï¼ŒCradleå¯ä»¥ä¸ä»»ä½•è½¯ä»¶è¿›è¡Œäº¤äº’ï¼Œå¹¶å®Œæˆé•¿æœŸå¤æ‚çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å†…ç½®APIã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒCradleåœ¨å››ä¸ªä»¥å‰æœªæ¢ç´¢è¿‡çš„å•†ä¸šè§†é¢‘æ¸¸æˆã€äº”ä¸ªè½¯ä»¶åº”ç”¨ç¨‹åºå’Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•OSWorldä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›å’Œä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚Cradleæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿä½¿åŸºç¡€æ™ºèƒ½ä½“éµå¾ªå¤æ‚AAAæ¸¸æˆã€Šè’é‡å¤§é•–å®¢æ•‘èµ2ã€‹ï¼ˆRDR2ï¼‰çš„ä¸»çº¿å‰§æƒ…å¹¶å®Œæˆ40åˆ†é’Ÿé•¿çš„çœŸå®ä»»åŠ¡çš„æ¡†æ¶ã€‚æ­¤å¤–ï¼ŒCradleè¿˜å¯ä»¥åœ¨ã€ŠåŸå¸‚ï¼šå¤©é™…çº¿ã€‹ä¸­åˆ›å»ºä¸€ä¸ªæ‹¥æœ‰åƒäººçš„åŸå¸‚ï¼Œåœ¨ã€Šæ˜Ÿéœ²è°·ç‰©è¯­ã€‹ä¸­ç§æ¤å’Œæ”¶è·æ¬§èŠ¹ï¼Œä»¥åŠåœ¨ã€Šç»é”€å•†ç”Ÿæ´»2ã€‹ä¸­ä»¥87%çš„æœ€å¤§æ¯å‘¨æ€»åˆ©æ¶¦è¿›è¡Œäº¤æ˜“å’Œè®¨ä»·è¿˜ä»·ã€‚Cradleä¸ä»…å¯ä»¥æ“ä½œæ—¥å¸¸è½¯ä»¶ï¼Œå¦‚Chromeã€Outlookå’Œé£ä¹¦ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ç¾å›¾å’Œå‰ªæ˜ ç¼–è¾‘å›¾åƒå’Œè§†é¢‘ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Cradleæ¡†æ¶ä¸ºé€šç”¨è®¡ç®—æœºæ§åˆ¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å…¶æ¨¡å—åŒ–å’Œçµæ´»çš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿé€‚åº”å„ç§ç¯å¢ƒå’Œä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒCradleæ¡†æ¶çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿåœ¨å¤æ‚çš„è™šæ‹Ÿç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å…·æœ‰æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´å¼ºå¤§çš„åŸºç¡€æ™ºèƒ½ä½“å’Œæ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>59</th>
      <td>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</td>
      <td>Large vision-language models (VLMs) have achieved substantial progress in<br>multimodal perception and reasoning. When integrated into an embodied agent,<br>existing embodied VLM works either output detailed action sequences at the<br>manipulation level or only provide plans at an abstract level, leaving a gap<br>between high-level planning and real-world manipulation. To bridge this gap, we<br>introduce Octopus, an embodied vision-language programmer that uses executable<br>code generation as a medium to connect planning and manipulation. Octopus is<br>designed to 1) proficiently comprehend an agent's visual and textual task<br>objectives, 2) formulate intricate action sequences, and 3) generate executable<br>code. To facilitate Octopus model development, we introduce OctoVerse: a suite<br>of environments tailored for benchmarking vision-based code generators on a<br>wide spectrum of tasks, ranging from mundane daily chores in simulators to<br>sophisticated interactions in complex video games such as Grand Theft Auto<br>(GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an<br>explorative agent that generates training data, i.e., action blueprints and<br>corresponding executable code. We also collect feedback that enables an<br>enhanced training scheme called Reinforcement Learning with Environmental<br>Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's<br>functionality and present compelling results, showing that the proposed RLEF<br>refines the agent's decision-making. By open-sourcing our simulation<br>environments, dataset, and model architecture, we aspire to ignite further<br>innovation and foster collaborative applications within the broader embodied AI<br>community.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Octopusï¼šåŸºäºç¯å¢ƒåé¦ˆçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹å™¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°†å®ƒä»¬é›†æˆåˆ°å…·èº«æ™ºèƒ½ä½“ä¸­æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…·èº«VLMå·¥ä½œè¦ä¹ˆåœ¨æ“ä½œå±‚é¢è¾“å‡ºè¯¦ç»†çš„åŠ¨ä½œåºåˆ—ï¼Œè¦ä¹ˆä»…åœ¨æŠ½è±¡å±‚é¢æä¾›è®¡åˆ’ï¼Œå¯¼è‡´é«˜çº§è§„åˆ’å’Œç°å®ä¸–ç•Œæ“ä½œä¹‹é—´å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºOctopusçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹å™¨ï¼Œå®ƒä½¿ç”¨å¯æ‰§è¡Œä»£ç ç”Ÿæˆä½œä¸ºè¿æ¥è§„åˆ’å’Œæ“ä½œçš„åª’ä»‹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šOctopusèƒ½å¤Ÿç†Ÿç»ƒåœ°ç†è§£æ™ºèƒ½ä½“çš„è§†è§‰å’Œæ–‡æœ¬ä»»åŠ¡ç›®æ ‡ï¼Œåˆ¶å®šå¤æ‚çš„åŠ¨ä½œåºåˆ—ï¼Œå¹¶ç”Ÿæˆå¯æ‰§è¡Œä»£ç ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸ºäº†ä¿ƒè¿›Octopusæ¨¡å‹çš„å‘å±•ï¼Œæœ¬æ–‡å¼•å…¥äº†OctoVerseï¼Œè¿™æ˜¯ä¸€å¥—ä¸ºåœ¨å„ç§ä»»åŠ¡ä¸Šè¯„ä¼°åŸºäºè§†è§‰çš„ä»£ç ç”Ÿæˆå™¨è€Œé‡èº«å®šåˆ¶çš„ç¯å¢ƒï¼ŒåŒ…æ‹¬ä»æ¨¡æ‹Ÿå™¨ä¸­çš„æ—¥å¸¸å®¶åŠ¡åˆ°å¤æ‚è§†é¢‘æ¸¸æˆï¼ˆå¦‚GTAå’ŒMinecraftï¼‰ä¸­çš„å¤æ‚äº¤äº’ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸ºäº†è®­ç»ƒOctopusï¼Œæœ¬æ–‡åˆ©ç”¨GPT-4æ§åˆ¶ä¸€ä¸ªæ¢ç´¢æ€§æ™ºèƒ½ä½“ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå³åŠ¨ä½œè“å›¾å’Œç›¸åº”çš„å¯æ‰§è¡Œä»£ç ã€‚åŒæ—¶ï¼Œæ”¶é›†åé¦ˆï¼Œä»¥å®ç°ä¸€ç§ç§°ä¸ºç¯å¢ƒåé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLEFï¼‰çš„å¢å¼ºè®­ç»ƒæ–¹æ¡ˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæœ¬æ–‡å±•ç¤ºäº†Octopusçš„åŠŸèƒ½ï¼Œå¹¶å±•ç¤ºäº†ä»¤äººä¿¡æœçš„ç»“æœï¼Œè¡¨æ˜æ‰€æå‡ºçš„RLEFç»†åŒ–äº†æ™ºèƒ½ä½“çš„å†³ç­–ã€‚Octopusåœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ï¼Œåœ¨ä»»åŠ¡è§„åˆ’ã€ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œæ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚RLEFçš„é›†æˆè¿›ä¸€æ­¥å¢å¼ºäº†Octopusçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è¿™ç§è®­ç»ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„Octopusæ¨¡å‹å’ŒOctoVerseç¯å¢ƒä¸ºå…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚Octopusæ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹å¯ä»¥å€Ÿé‰´åˆ°å…¶ä»–å…·èº«æ™ºèƒ½ä½“ä¸­ï¼Œä»¥æé«˜å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„æ“ä½œèƒ½åŠ›ã€‚OctoVerseç¯å¢ƒå¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹æ¨¡å‹ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>100</th>
      <td>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory</td>
      <td>The captivating realm of Minecraft has attracted substantial research<br>interest in recent years, serving as a rich platform for developing intelligent<br>agents capable of functioning in open-world environments. However, the current<br>research landscape predominantly focuses on specific objectives, such as the<br>popular "ObtainDiamond" task, and has not yet shown effective generalization to<br>a broader spectrum of tasks. Furthermore, the current leading success rate for<br>the "ObtainDiamond" task stands at around 20%, highlighting the limitations of<br>Reinforcement Learning (RL) based controllers used in existing methods. To<br>tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel<br>framework integrates Large Language Models (LLMs) with text-based knowledge and<br>memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These<br>agents, equipped with the logic and common sense capabilities of LLMs, can<br>skillfully navigate complex, sparse-reward environments with text-based<br>interactions. We develop a set of structured actions and leverage LLMs to<br>generate action plans for the agents to execute. The resulting LLM-based agent<br>markedly surpasses previous methods, achieving a remarkable improvement of<br>+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior<br>robustness compared to traditional RL-based controllers. Notably, our agent is<br>the first to procure all items in the Minecraft Overworld technology tree,<br>demonstrating its extensive capabilities. GITM does not need any GPU for<br>training, but a single CPU node with 32 CPU cores is enough. This research<br>shows the potential of LLMs in developing capable agents for handling<br>long-horizon, complex tasks and adapting to uncertainties in open-world<br>environments. See the project website at https://github.com/OpenGVLab/GITM.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Minecraftä¸­çš„å¹½çµï¼šé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºæ–‡æœ¬çš„çŸ¥è¯†ä¸è®°å¿†ï¼Œåˆ›å»ºå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>Minecraftä½œä¸ºä¸€æ¬¾å¼€æ”¾ä¸–ç•Œæ¸¸æˆï¼Œå¸å¼•äº†å¤§é‡ç ”ç©¶å…´è¶£ï¼Œæˆä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„æ™ºèƒ½ä½“çš„ä¸°å¯Œå¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šç›®æ ‡ä¸Šï¼Œå¦‚æµè¡Œçš„â€œObtainDiamondâ€ä»»åŠ¡ï¼Œå°šæœªåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æœ€é«˜æˆåŠŸç‡ä»…ä¸ºçº¦20%ï¼Œçªæ˜¾äº†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ§åˆ¶å™¨æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Ghost in the Minecraftï¼ˆGITMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ç›¸ç»“åˆï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿåœ¨Minecraftä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“ï¼ˆGCAsï¼‰ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLMåˆ†è§£å™¨<br>LLMåˆ†è§£å™¨è´Ÿè´£å°†ä»»åŠ¡ç›®æ ‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´æ˜“äºå®ç°çš„å­ç›®æ ‡ã€‚é€šè¿‡è§£å†³æ¯ä¸ªå­ç›®æ ‡ï¼Œå¯ä»¥é€æ­¥å®ç°ä»»åŠ¡ç›®æ ‡ã€‚LLMåˆ†è§£å™¨åˆ©ç”¨ä»äº’è”ç½‘æ”¶é›†çš„æ–‡æœ¬çŸ¥è¯†ï¼Œå°†ç›®æ ‡åˆ†è§£ä¸ºå­ç›®æ ‡æ ‘ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMè§„åˆ’å™¨<br>LLMè§„åˆ’å™¨è´Ÿè´£ä¸ºæ¯ä¸ªå­ç›®æ ‡ç”Ÿæˆä¸€ç³»åˆ—ç»“æ„åŒ–æ“ä½œã€‚ç»“æ„åŒ–æ“ä½œå…·æœ‰æ˜ç¡®çš„è¯­ä¹‰å’Œç›¸åº”çš„åé¦ˆï¼Œä½¿LLMsèƒ½å¤Ÿåœ¨è®¤çŸ¥å±‚é¢ç†è§£å‘¨å›´ç¯å¢ƒå¹¶åšå‡ºå†³ç­–ã€‚LLMè§„åˆ’å™¨è¿˜è®°å½•å’Œæ€»ç»“æˆåŠŸçš„æ“ä½œåˆ—è¡¨ï¼Œä»¥å¢å¼ºæœªæ¥çš„è§„åˆ’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šLLMæ¥å£<br>LLMæ¥å£è´Ÿè´£å°†ç»“æ„åŒ–æ“ä½œè½¬æ¢ä¸ºé”®ç›˜/é¼ æ ‡æ“ä½œï¼Œå¹¶ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚å®ƒè¿˜ä»ç¯å¢ƒä¸­æå–è§‚å¯Ÿç»“æœï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåé¦ˆæ¶ˆæ¯ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°äº†47.5%ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ™ºèƒ½ä½“æ˜¯ç¬¬ä¸€ä¸ªåœ¨Minecraft Overworldä¸­è·å–æ‰€æœ‰ç‰©å“çš„æ™ºèƒ½ä½“ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„æŠ€èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„GITMæ¡†æ¶ä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡åˆ©ç”¨LLMsçš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠåŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“æœ‰æ•ˆåœ°å¤„ç†å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å„ç§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å…·æœ‰é«˜æ•ˆçš„å­¦ä¹ æ•ˆç‡å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¼€å‘é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>119</th>
      <td>Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</td>
      <td>Language models show a surprising range of capabilities, but the source of<br>their apparent competence is unclear. Do these networks just memorize a<br>collection of surface statistics, or do they rely on internal representations<br>of the process that generates the sequences they see? We investigate this<br>question by applying a variant of the GPT model to the task of predicting legal<br>moves in a simple board game, Othello. Although the network has no a priori<br>knowledge of the game or its rules, we uncover evidence of an emergent<br>nonlinear internal representation of the board state. Interventional<br>experiments indicate this representation can be used to control the output of<br>the network and create "latent saliency maps" that can help explain predictions<br>in human terms.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢åºåˆ—æ¨¡å‹åœ¨åˆæˆä»»åŠ¡ä¸­çš„æ¶Œç°ä¸–ç•Œè¡¨ç¤º<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¯­è¨€æ¨¡å‹å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„åŸç†å°šä¸æ˜ç¡®ã€‚è¿™äº›æ¨¡å‹æ˜¯ä»…ä»…è®°å¿†äº†ä¸€ç³»åˆ—è¡¨é¢ç»Ÿè®¡ä¿¡æ¯ï¼Œè¿˜æ˜¯ä¾èµ–äºå¯¹ç”Ÿæˆåºåˆ—è¿‡ç¨‹çš„å†…éƒ¨è¡¨ç¤ºï¼Ÿæœ¬æ–‡é€šè¿‡å°†GPTæ¨¡å‹åº”ç”¨äºé¢„æµ‹ç®€å•æ£‹ç›˜æ¸¸æˆOthelloçš„åˆæ³•ç§»åŠ¨ä»»åŠ¡ï¼Œæ¢è®¨äº†è¿™ä¸ªé—®é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨Othelloä½œä¸ºæµ‹è¯•å¹³å°<br>æœ¬æ–‡é€‰æ‹©Othelloä½œä¸ºæµ‹è¯•å¹³å°ï¼Œå› ä¸ºå®ƒæ¯”å›½é™…è±¡æ£‹ç®€å•ï¼Œä½†æ¸¸æˆæ ‘è¶³å¤Ÿå¤§ï¼Œé¿å…äº†è®°å¿†çš„å¯èƒ½æ€§ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªGPTå˜ä½“æ¨¡å‹ï¼ˆOthello-GPTï¼‰æ¥é¢„æµ‹Othelloçš„åˆæ³•ç§»åŠ¨ï¼Œå°½ç®¡æ¨¡å‹æ²¡æœ‰å…ˆéªŒçš„æ¸¸æˆçŸ¥è¯†ï¼Œä½†ä»ç„¶èƒ½å¤Ÿä»¥é«˜ç²¾åº¦ç”Ÿæˆåˆæ³•ç§»åŠ¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨æ¢é’ˆæŠ€æœ¯æ¢ç´¢å†…éƒ¨è¡¨ç¤º<br>ä¸ºäº†ç ”ç©¶Othello-GPTæ˜¯å¦è®¡ç®—äº†æ¸¸æˆçŠ¶æ€çš„å†…éƒ¨è¡¨ç¤ºï¼Œæœ¬æ–‡ä½¿ç”¨äº†æ¢é’ˆæŠ€æœ¯ã€‚æ¢é’ˆæ˜¯ä¸€ç§åˆ†ç±»å™¨æˆ–å›å½’å™¨ï¼Œå…¶è¾“å…¥ç”±ç½‘ç»œçš„å†…éƒ¨æ¿€æ´»ç»„æˆï¼Œå¹¶è®­ç»ƒä»¥é¢„æµ‹æ„Ÿå…´è¶£çš„ç‰¹å¾ã€‚é€šè¿‡è®­ç»ƒæ¢é’ˆæ¥é¢„æµ‹ç½‘ç»œå†…éƒ¨æ¿€æ´»åçš„æ£‹ç›˜çŠ¶æ€ï¼Œå‘ç°éçº¿æ€§æ¢é’ˆèƒ½å¤Ÿä»¥é«˜ç²¾åº¦é¢„æµ‹æ£‹ç›˜çŠ¶æ€ï¼Œè¿™è¡¨æ˜æ¨¡å‹å†…éƒ¨å­˜åœ¨ä¸€ä¸ªéçº¿æ€§çš„æ£‹ç›˜çŠ¶æ€è¡¨ç¤ºã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¹²é¢„å®éªŒéªŒè¯è¡¨ç¤ºçš„å› æœä½œç”¨<br>ä¸ºäº†ç¡®å®šæ£‹ç›˜çŠ¶æ€ä¿¡æ¯æ˜¯å¦å½±å“æ¨¡å‹çš„é¢„æµ‹ï¼Œæœ¬æ–‡è¿›è¡Œäº†ä¸€ç³»åˆ—å¹²é¢„å®éªŒã€‚é€šè¿‡ä¿®æ”¹Othello-GPTçš„å†…éƒ¨æ¿€æ´»ï¼Œå¹¶æµ‹é‡ç”±æ­¤äº§ç”Ÿçš„æ•ˆæœï¼Œå‘ç°å¹²é¢„åçš„é¢„æµ‹ä¸é¢„æœŸçš„æ£‹ç›˜çŠ¶æ€ç›¸åŒ¹é…ï¼Œè¿™è¡¨æ˜æ¶Œç°çš„ä¸–ç•Œè¡¨ç¤ºå¯¹æ¨¡å‹çš„é¢„æµ‹å…·æœ‰å› æœä½œç”¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ›å»ºæ½œåœ¨æ˜¾è‘—æ€§å›¾<br>æœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¹²é¢„æŠ€æœ¯åˆ›å»ºæ½œåœ¨æ˜¾è‘—æ€§å›¾ï¼Œè¿™äº›å›¾å¯ä»¥å¸®åŠ©è§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚é€šè¿‡å¹²é¢„æ¯ä¸ªæ£‹ç›˜æ ¼çš„çŠ¶æ€ï¼Œå¹¶è§‚å¯Ÿé¢„æµ‹æ¦‚ç‡çš„å˜åŒ–ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªè¡¨ç¤ºæ¯ä¸ªæ£‹ç›˜æ ¼å¯¹å½“å‰æ£‹ç›˜çŠ¶æ€é¢„æµ‹çš„æ˜¾è‘—æ€§çš„å¯è§†åŒ–å›¾ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒOthello-GPTç¡®å®ç»´æŠ¤äº†ä¸€ä¸ªæ¸¸æˆæ£‹ç›˜çŠ¶æ€çš„è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™ä¸ªè¡¨ç¤ºæ˜¯éçº¿æ€§çš„ã€‚æ­¤å¤–ï¼Œè¿™äº›è¡¨ç¤ºä¸æ¨¡å‹çš„é¢„æµ‹å…·æœ‰å› æœè”ç³»ã€‚æ½œåœ¨æ˜¾è‘—æ€§å›¾æ­ç¤ºäº†Othello-GPTè®­ç»ƒæ•°æ®é›†çš„ä¸åŒç‰ˆæœ¬ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåºåˆ—æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¤æ‚çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™äº›è¡¨ç¤ºå¯ä»¥ç”¨äºè§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œå¹²é¢„å®éªŒå’Œæ½œåœ¨æ˜¾è‘—æ€§å›¾ç­‰æŠ€æœ¯å¯ä»¥ç”¨äºæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚è¿™äº›å‘ç°å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶å¼€å‘æ›´å¯é çš„è§£é‡Šå·¥å…·ã€‚</td>
    </tr>
    <tr>
      <th>54</th>
      <td>SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents</td>
      <td>Humans are social beings; we pursue social goals in our daily interactions,<br>which is a crucial aspect of social intelligence. Yet, AI systems' abilities in<br>this realm remain elusive. We present SOTOPIA, an open-ended environment to<br>simulate complex social interactions between artificial agents and evaluate<br>their social intelligence. In our environment, agents role-play and interact<br>under a wide variety of scenarios; they coordinate, collaborate, exchange, and<br>compete with each other to achieve complex social goals. We simulate the<br>role-play interaction between LLM-based agents and humans within this task<br>space and evaluate their performance with a holistic evaluation framework<br>called SOTOPIA-Eval. With SOTOPIA, we find significant differences between<br>these models in terms of their social intelligence, and we identify a subset of<br>SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.<br>We find that on this subset, GPT-4 achieves a significantly lower goal<br>completion rate than humans and struggles to exhibit social commonsense<br>reasoning and strategic communication skills. These findings demonstrate<br>SOTOPIA's promise as a general platform for research on evaluating and<br>improving social intelligence in artificial agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SOTOPIAï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹ç¤¾äº¤æ™ºèƒ½çš„å¼€æ”¾ç¯å¢ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>äººç±»åœ¨ç¤¾ä¼šäº’åŠ¨ä¸­è¿½æ±‚å¤æ‚çš„ç¤¾ä¼šç›®æ ‡ï¼Œè¿™æ˜¯ç¤¾äº¤æ™ºèƒ½çš„å…³é”®æ–¹é¢ã€‚ç„¶è€Œï¼ŒAIç³»ç»Ÿåœ¨ç¤¾äº¤é¢†åŸŸçš„æ™ºèƒ½ä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚ç°æœ‰çš„ç¤¾äº¤æ™ºèƒ½è¯„ä¼°æ–¹æ³•è¦ä¹ˆæ˜¯éäº¤äº’å¼çš„ï¼Œè¦ä¹ˆç¼ºä¹å¯¹å¤šæ ·åŒ–ç›®æ ‡é©±åŠ¨è¡Œä¸ºçš„å…³æ³¨ï¼Œæˆ–è€…ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ã€‚ä¸ºäº†ç ”ç©¶åŠ¨æ€å’Œç›®æ ‡é©±åŠ¨çš„ç¤¾äº¤æ™ºèƒ½ï¼Œæœ¬æ–‡æå‡ºäº†SOTOPIAï¼Œä¸€ä¸ªå¼€æ”¾å¼çš„é€šç”¨é¢†åŸŸç¯å¢ƒï¼Œç”¨äºæ¨¡æ‹Ÿäººå·¥ä»£ç†ä¹‹é—´çš„å¤æ‚ç¤¾äº¤äº’åŠ¨å¹¶è¯„ä¼°å…¶ç¤¾äº¤æ™ºèƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSOTOPIAç¯å¢ƒ<br>SOTOPIAæ˜¯ä¸€ä¸ªäº¤äº’å¼ç¯å¢ƒï¼Œæ”¯æŒå¤šè½®æ¨¡æ‹Ÿé€šä¿¡ï¼Œä»£ç†å¯ä»¥ä½¿ç”¨è¯­è¨€å’Œéè¯­è¨€æ²Ÿé€šä»¥åŠç‰©ç†åŠ¨ä½œã€‚å®ƒå…·æœ‰å¤šæ ·åŒ–çš„ä»»åŠ¡ç©ºé—´ï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç”Ÿæˆçš„åœºæ™¯ã€ç›®æ ‡ã€è§’è‰²ã€å…³ç³»å’Œå…¶ä»–ä»£ç†çš„ç­–ç•¥ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªåºå¤§ä¸”å¤šæ ·åŒ–çš„ä»»åŠ¡ç©ºé—´ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSOTOPIA-EVALè¯„ä¼°æ¡†æ¶<br>SOTOPIA-EVALæ˜¯ä¸€ä¸ªå¤šç»´åº¦çš„è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç¤¾äº¤ç»´åº¦åˆ†æä»£ç†çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ç›®æ ‡å®Œæˆåº¦ã€å¯ä¿¡åº¦ã€çŸ¥è¯†è·å–ã€ç§˜å¯†ä¿æŒã€å…³ç³»ç»´æŠ¤ã€ç¤¾ä¼šè§„åˆ™éµå®ˆå’Œè´¢åŠ¡åŠç‰©è´¨åˆ©ç›Šã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4åœ¨SOTOPIA-EVALçš„æŸäº›ç»´åº¦ä¸Šå¯ä»¥ä½œä¸ºäººç±»åˆ¤æ–­çš„ä»£ç†ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡å®Œæˆåº¦æ–¹é¢ã€‚åœ¨æ¨¡å‹ä¹‹é—´ï¼ŒGPT-4åœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯GPT-3.5ã€Llama-2-70b-chatå’ŒMPT-30b-chatã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨éµå®ˆç¤¾ä¼šè§„åˆ™å’Œä¿æŒç§˜å¯†æ–¹é¢éƒ½å­˜åœ¨é£é™©ã€‚ä¸äººç±»ç›¸æ¯”ï¼ŒGPT-4åœ¨ç›®æ ‡å®Œæˆåº¦æ–¹é¢æ˜¾è‘—ä½äºäººç±»ï¼Œå¹¶ä¸”åœ¨å±•ç¤ºç¤¾äº¤å¸¸è¯†æ¨ç†å’Œæˆ˜ç•¥æ²Ÿé€šæŠ€èƒ½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SOTOPIAä¸ºè¯„ä¼°å’Œæ”¹è¿›äººå·¥ä»£ç†çš„ç¤¾äº¤æ™ºèƒ½æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„å¹³å°ã€‚SOTOPIA-EVALæ¡†æ¶å¯ä»¥ç”¨äºè¯„ä¼°ä»£ç†åœ¨å¤šä¸ªç¤¾äº¤ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¸®åŠ©ç ”ç©¶äººå‘˜ç†è§£æ¨¡å‹ä¹‹é—´ä»¥åŠæ¨¡å‹ä¸äººç±»ä¹‹é—´çš„ç¤¾äº¤æ™ºèƒ½å·®å¼‚ã€‚æ­¤å¤–ï¼ŒSOTOPIAè¿˜å¯ä»¥ç”¨äºè®­ç»ƒæ›´å…·ç¤¾äº¤æ™ºèƒ½çš„è¯­è¨€ä»£ç†ã€‚</td>
    </tr>
    <tr>
      <th>123</th>
      <td>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos</td>
      <td>Pretraining on noisy, internet-scale datasets has been heavily studied as a<br>technique for training models with broad, general capabilities for text,<br>images, and other modalities. However, for many sequential decision domains<br>such as robotics, video games, and computer use, publicly available data does<br>not contain the labels required to train behavioral priors in the same way. We<br>extend the internet-scale pretraining paradigm to sequential decision domains<br>through semi-supervised imitation learning wherein agents learn to act by<br>watching online unlabeled videos. Specifically, we show that with a small<br>amount of labeled data we can train an inverse dynamics model accurate enough<br>to label a huge unlabeled source of online data -- here, online videos of<br>people playing Minecraft -- from which we can then train a general behavioral<br>prior. Despite using the native human interface (mouse and keyboard at 20Hz),<br>we show that this behavioral prior has nontrivial zero-shot capabilities and<br>that it can be fine-tuned, with both imitation learning and reinforcement<br>learning, to hard-exploration tasks that are impossible to learn from scratch<br>via reinforcement learning. For many tasks our models exhibit human-level<br>performance, and we are the first to report computer agents that can craft<br>diamond tools, which can take proficient humans upwards of 20 minutes (24,000<br>environment actions) of gameplay to accomplish.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è§†é¢‘é¢„è®­ç»ƒï¼ˆVPTï¼‰ï¼šé€šè¿‡è§‚çœ‹æ— æ ‡ç­¾åœ¨çº¿è§†é¢‘å­¦ä¹ è¡ŒåŠ¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸï¼Œé€šè¿‡åœ¨å¤§å‹äº’è”ç½‘æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå·²ç»è¯æ˜äº†è®­ç»ƒå¤§å‹é€šç”¨åŸºç¡€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¯¹äºè®¸å¤šåºåˆ—å†³ç­–é¢†åŸŸï¼Œå¦‚æœºå™¨äººã€è§†é¢‘æ¸¸æˆå’Œè®¡ç®—æœºä½¿ç”¨ï¼Œå…¬å¼€å¯ç”¨çš„æ•°æ®å¹¶ä¸åŒ…å«è®­ç»ƒè¡Œä¸ºå…ˆéªŒæ‰€éœ€çš„æ ‡ç­¾ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ©ç”¨äº’è”ç½‘ä¸Šå¤§é‡æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ï¼Œå°†è¿™äº›é¢„è®­ç»ƒèŒƒå¼æ‰©å±•åˆ°åºåˆ—å†³ç­–é¢†åŸŸã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠç›‘ç£æ¨¡ä»¿å­¦ä¹ <br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠç›‘ç£æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è§‚çœ‹åœ¨çº¿æœªæ ‡è®°çš„è§†é¢‘ï¼Œä½¿æ™ºèƒ½ä½“å­¦ä¼šè¡ŒåŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®è®­ç»ƒä¸€ä¸ªé€†åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¶³å¤Ÿå‡†ç¡®ï¼Œå¯ä»¥æ ‡è®°å¤§é‡æœªæ ‡è®°çš„åœ¨çº¿æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œäººä»¬ç©Minecraftçš„è§†é¢‘ï¼‰ï¼Œç„¶åä»ä¸­è®­ç»ƒä¸€ä¸ªé€šç”¨çš„è¡Œä¸ºå…ˆéªŒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€†åŠ¨åŠ›å­¦æ¨¡å‹<br>ä¸è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œé€†åŠ¨åŠ›å­¦å»ºæ¨¡ä»»åŠ¡æ›´ç®€å•ï¼Œå› ä¸ºå®ƒæ˜¯éå› æœçš„ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥æŸ¥çœ‹è¿‡å»å’Œæœªæ¥çš„å¸§æ¥æ¨æ–­åŠ¨ä½œã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç¯å¢ƒæœºåˆ¶æ¯”ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿçš„äººç±»è¡Œä¸ºçš„å¹¿åº¦è¦ç®€å•å¾—å¤šï¼Œè¿™è¡¨æ˜éå› æœé€†åŠ¨åŠ›å­¦æ¨¡å‹å¯èƒ½éœ€è¦æ¯”å› æœè¡Œä¸ºå…‹éš†æ¨¡å‹å°‘å¾—å¤šçš„æ•°æ®æ¥è®­ç»ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ä½¿ç”¨äº†åŸç”Ÿäººç±»ç•Œé¢ï¼ˆ20Hzçš„é¼ æ ‡å’Œé”®ç›˜ï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•çš„è¡Œä¸ºå…ˆéªŒå…·æœ‰éå¹³å‡¡çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ æ— æ³•ä»å¤´å­¦ä¹ çš„å›°éš¾æ¢ç´¢ä»»åŠ¡ã€‚å¯¹äºè®¸å¤šä»»åŠ¡ï¼Œæ¨¡å‹è¡¨ç°å‡ºäººç±»æ°´å¹³çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªæŠ¥å‘Šèƒ½å¤Ÿåˆ¶ä½œé’»çŸ³å·¥å…·çš„è®¡ç®—æœºä»£ç†ï¼Œè¿™éœ€è¦ç†Ÿç»ƒçš„äººç±»ç©å®¶è¶…è¿‡20åˆ†é’Ÿï¼ˆ24,000ä¸ªç¯å¢ƒåŠ¨ä½œï¼‰çš„æ¸¸æˆæ—¶é—´æ‰èƒ½å®Œæˆã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„è§†é¢‘é¢„è®­ç»ƒï¼ˆVPTï¼‰æ–¹æ³•ä¸ºåˆ©ç”¨äº’è”ç½‘ä¸Šå¤§é‡æœªæ ‡è®°çš„æ•°æ®è¿›è¡Œåºåˆ—å†³ç­–é¢†åŸŸçš„é¢„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å°‘é‡æ ‡è®°æ•°æ®ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ æ— æ³•ä»å¤´å­¦ä¹ çš„å›°éš¾æ¢ç´¢ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºä»»ä½•å…·æœ‰å¤§é‡æœªæ ‡è®°æ•°æ®çš„é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</td>
    </tr>
    <tr>
      <th>97</th>
      <td>AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation</td>
      <td>We propose a novel framework for learning high-level cognitive capabilities<br>in robot manipulation tasks, such as making a smiley face using building<br>blocks. These tasks often involve complex multi-step reasoning, presenting<br>significant challenges due to the limited paired data connecting human<br>instructions (e.g., making a smiley face) and robot actions (e.g., end-effector<br>movement). Existing approaches relieve this challenge by adopting an open-loop<br>paradigm decomposing high-level instructions into simple sub-task plans, and<br>executing them step-by-step using low-level control models. However, these<br>approaches are short of instant observations in multi-step reasoning, leading<br>to sub-optimal results. To address this issue, we propose to automatically<br>collect a cognitive robot dataset by Large Language Models (LLMs). The<br>resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of<br>multi-step text plans and paired observation sequences. To enable efficient<br>data acquisition, we employ elaborated multi-round prompt designs that<br>effectively reduce the burden of extensive human involvement. We further<br>propose a closed-loop multi-modal embodied planning model that autoregressively<br>generates plans by taking image observations as input. To facilitate effective<br>learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and<br>finetune additional vision adapter and Q-former to enable fine-grained spatial<br>perception for manipulation tasks. We conduct experiments to verify the<br>superiority over existing open and closed-loop methods, and achieve a<br>significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4<br>based robot tasks. Real-world demos are shown in<br>https://www.youtube.com/watch?v=ayAzID1_qQk .</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AlphaBlockï¼šæœºå™¨äººæ“ä½œä¸­çš„è§†è§‰-è¯­è¨€æ¨ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå¦‚ä½¿ç”¨ç§¯æœ¨åˆ¶ä½œç¬‘è„¸ï¼Œæœºå™¨äººéœ€è¦ç†è§£å’Œæ‰§è¡Œå¤æ‚çš„è¯­è¨€æŒ‡ä»¤ï¼Œè¿™æ¶‰åŠåˆ°æ„ŸçŸ¥ã€æ¨ç†å’Œæ“ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å¼€ç¯èŒƒå¼ï¼Œå°†é«˜çº§æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•çš„å­ä»»åŠ¡è®¡åˆ’ï¼Œå¹¶ä½¿ç”¨ä½çº§æ§åˆ¶æ¨¡å‹é€æ­¥æ‰§è¡Œã€‚è¿™ç§æ–¹æ³•ç¼ºä¹å¤šæ­¥æ¨ç†ä¸­çš„å³æ—¶è§‚å¯Ÿï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªåŠ¨æ”¶é›†è®¤çŸ¥æœºå™¨äººæ•°æ®é›†<br>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ”¶é›†è®¤çŸ¥æœºå™¨äººæ•°æ®é›†çš„æ–¹æ³•ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆå¤šæ­¥æ–‡æœ¬è®¡åˆ’å’Œé…å¯¹çš„è§‚å¯Ÿåºåˆ—ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†äººç±»å‚ä¸çš„è´Ÿæ‹…ï¼Œå¹¶æ”¶é›†äº†35ä¸ªç»¼åˆçš„é«˜æ°´å¹³ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤šæ­¥æ–‡æœ¬è®¡åˆ’å’Œé…å¯¹çš„è§‚å¯Ÿåºåˆ—ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé—­ç¯å¤šæ¨¡æ€å…·èº«è§„åˆ’æ¨¡å‹<br>æœ¬æ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§é—­ç¯å¤šæ¨¡æ€å…·èº«è§„åˆ’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¾“å…¥å›¾åƒè§‚å¯Ÿæ¥è‡ªå›å½’åœ°ç”Ÿæˆè®¡åˆ’ã€‚ä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆå­¦ä¹ ï¼Œæœ¬æ–‡åˆ©ç”¨äº†MiniGPT-4ï¼ŒåŒ…æ‹¬å†»ç»“çš„è§†è§‰ç¼–ç å™¨å’ŒLLMï¼Œå¹¶å¾®è°ƒäº†é¢å¤–çš„è§†è§‰é€‚é…å™¨å’ŒQ-formerï¼Œä»¥å®ç°æ“ä½œä»»åŠ¡ä¸­çš„ç»†ç²’åº¦ç©ºé—´æ„ŸçŸ¥ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¼€ç¯å’Œé—­ç¯æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„CogLoopæ¡†æ¶åœ¨æœºå™¨äººä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ChatGPTå’ŒGPT-4ç›¸æ¯”ï¼ŒCogLoopçš„æˆåŠŸç‡åˆ†åˆ«æé«˜äº†21.4%å’Œ14.5%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„CogLoopæ¡†æ¶ä¸ºæœºå™¨äººæ“ä½œä¸­çš„è§†è§‰-è¯­è¨€æ¨ç†æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡è‡ªåŠ¨æ”¶é›†è®¤çŸ¥æœºå™¨äººæ•°æ®é›†å’Œé—­ç¯å¤šæ¨¡æ€å…·èº«è§„åˆ’æ¨¡å‹ï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œé«˜çº§æŒ‡ä»¤ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äº¤äº’å’Œæ“ä½œã€‚è¿™ç§æ–¹æ³•åœ¨å®¶åº­æœºå™¨äººã€åˆ¶é€ ä¸šå’ŒåŒ»ç–—ä¿å¥ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</td>
      <td>Grounding the reasoning ability of large language models (LLMs) for embodied<br>tasks is challenging due to the complexity of the physical world. Especially,<br>LLM planning for multi-agent collaboration requires communication of agents or<br>credit assignment as the feedback to re-adjust the proposed plans and achieve<br>effective coordination. However, existing methods that overly rely on physical<br>verification or self-reflection suffer from excessive and inefficient querying<br>of LLMs. In this paper, we propose a novel framework for multi-agent<br>collaboration that introduces Reinforced Advantage feedback (ReAd) for<br>efficient self-refinement of plans. Specifically, we perform critic regression<br>to learn a sequential advantage function from LLM-planned data, and then treat<br>the LLM planner as an optimizer to generate actions that maximize the advantage<br>function. It endows the LLM with the foresight to discern whether the action<br>contributes to accomplishing the final task. We provide theoretical analysis by<br>extending advantage-weighted regression in reinforcement learning to<br>multi-agent systems. Experiments on Overcooked-AI and a difficult variant of<br>RoCoBench show that ReAd surpasses baselines in success rate, and also<br>significantly decreases the interaction steps of agents and query rounds of<br>LLMs, demonstrating its high efficiency for grounding LLMs. More results are<br>given at https://read-llm.github.io/.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…·èº«å¤šæ™ºèƒ½ä½“åä½œä¸­çš„æ¨ç†èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å…·èº«ä»»åŠ¡æ—¶ï¼Œç”±äºç‰©ç†ä¸–ç•Œçš„å¤æ‚æ€§ï¼Œå…¶æ¨ç†èƒ½åŠ›éš¾ä»¥è½åœ°ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­ï¼ŒLLM è§„åˆ’éœ€è¦æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡æˆ–ä¿¡ç”¨åˆ†é…ä½œä¸ºåé¦ˆï¼Œä»¥è°ƒæ•´è®¡åˆ’å¹¶å®ç°æœ‰æ•ˆåè°ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–ç‰©ç†éªŒè¯æˆ–è‡ªæˆ‘åæ€ï¼Œå¯¼è‡´å¯¹ LLM çš„æŸ¥è¯¢è¿‡å¤šä¸”æ•ˆç‡ä½ä¸‹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º ReAdï¼ˆReinforced Advantage feedbackï¼‰çš„æ¡†æ¶ï¼Œç”¨äºå¤šæ™ºèƒ½ä½“åä½œï¼Œä»¥å®ç°é«˜æ•ˆçš„è‡ªé€‚åº”è®¡åˆ’ã€‚å…·ä½“æ¥è¯´ï¼ŒReAd é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š<br><br>1. **ä¼˜åŠ¿å‡½æ•°å­¦ä¹ **ï¼šä½¿ç”¨ LLM è§„åˆ’æ•°æ®ï¼Œé€šè¿‡æ‰¹è¯„å›å½’å­¦ä¹ ä¸€ä¸ªåºåˆ—ä¼˜åŠ¿å‡½æ•°ã€‚<br>2. **LLM è§„åˆ’å™¨ä¼˜åŒ–**ï¼šå°† LLM è§„åˆ’å™¨è§†ä¸ºä¼˜åŒ–å™¨ï¼Œç”Ÿæˆæœ€å¤§åŒ–ä¼˜åŠ¿å‡½æ•°çš„åŠ¨ä½œã€‚<br>3. **å‰ç»æ€§æ¨ç†**ï¼šèµ‹äºˆ LLM å‰ç»æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿåˆ¤æ–­åŠ¨ä½œæ˜¯å¦æœ‰åŠ©äºå®Œæˆä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ Overcooked-AI å’Œ RoCoBench çš„å›°éš¾å˜ä½“ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜ ReAd åœ¨æˆåŠŸç‡æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†æ™ºèƒ½ä½“çš„äº¤äº’æ­¥éª¤å’Œ LLM çš„æŸ¥è¯¢è½®æ¬¡ï¼Œè¯æ˜äº†å…¶åœ¨è½åœ° LLM æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>ReAd æ¡†æ¶ä¸º LLM åœ¨å…·èº«å¤šæ™ºèƒ½ä½“åä½œä¸­çš„æ¨ç†èƒ½åŠ›è½åœ°æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°† LLM è§„åˆ’å™¨è§†ä¸ºä¼˜åŒ–å™¨ï¼Œå¹¶é€šè¿‡ä¼˜åŠ¿å‡½æ•°å¼•å¯¼å…¶ç”Ÿæˆæ›´æœ‰æ•ˆçš„åŠ¨ä½œã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦ LLM åä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººåä½œã€è™šæ‹Ÿç°å®ç­‰ã€‚</td>
    </tr>
    <tr>
      <th>113</th>
      <td>MarioGPT: Open-Ended Text2Level Generation through Large Language Models</td>
      <td>Procedural Content Generation (PCG) is a technique to generate complex and<br>diverse environments in an automated way. However, while generating content<br>with PCG methods is often straightforward, generating meaningful content that<br>reflects specific intentions and constraints remains challenging. Furthermore,<br>many PCG algorithms lack the ability to generate content in an open-ended<br>manner. Recently, Large Language Models (LLMs) have shown to be incredibly<br>effective in many diverse domains. These trained LLMs can be fine-tuned,<br>re-using information and accelerating training for new tasks. Here, we<br>introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game<br>levels, in our case Super Mario Bros levels. MarioGPT can not only generate<br>diverse levels, but can be text-prompted for controllable level generation,<br>addressing one of the key challenges of current PCG techniques. As far as we<br>know, MarioGPT is the first text-to-level model and combined with novelty<br>search it enables the generation of diverse levels with varying play-style<br>dynamics (i.e. player paths) and the open-ended discovery of an increasingly<br>diverse range of content. Code available at<br>https://github.com/shyamsn97/mario-gpt.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MarioGPTï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾æ€§æ–‡æœ¬åˆ°å…³å¡ç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æ¸¸æˆè¡Œä¸šçš„å‘å±•ï¼Œç©å®¶å¯¹æ¸¸æˆå†…å®¹çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§è¦æ±‚è¶Šæ¥è¶Šé«˜ã€‚ä¼ ç»Ÿçš„å…³å¡è®¾è®¡å¾€å¾€éœ€è¦å¤§é‡äººåŠ›å’Œæ—¶é—´ï¼Œä¸”éš¾ä»¥æ»¡è¶³ä¸ªæ€§åŒ–éœ€æ±‚ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆã€è‡ªåŠ¨åœ°ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§å’Œå¯ç©æ€§çš„æ¸¸æˆå…³å¡æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMarioGPTæ¨¡å‹<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMarioGPTçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºGPT-2è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç”¨äºç”Ÿæˆé©¬é‡Œå¥¥æ¸¸æˆå…³å¡ã€‚MarioGPTèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå¤šæ ·åŒ–çš„å…³å¡ï¼Œå¹¶é€šè¿‡æ–‡æœ¬æç¤ºå®ç°å¯æ§çš„å…³å¡ç”Ÿæˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆæ–°é¢–æ€§æœç´¢<br>ä¸ºäº†å®ç°å¼€æ”¾æ€§çš„å…³å¡ç”Ÿæˆï¼Œæœ¬æ–‡å°†MarioGPTä¸æ–°é¢–æ€§æœç´¢ç®—æ³•ç›¸ç»“åˆã€‚æ–°é¢–æ€§æœç´¢ç®—æ³•èƒ½å¤Ÿå¼•å¯¼æ¨¡å‹ä¸æ–­ç”Ÿæˆå…·æœ‰æ–°é¢–æ€§çš„å…³å¡ï¼Œä»è€Œå®ç°æ— é™çš„å†…å®¹æ¢ç´¢ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒMarioGPTåœ¨ç”Ÿæˆé©¬é‡Œå¥¥æ¸¸æˆå…³å¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒMarioGPTåœ¨éç©ºæ°”ç“·ç –é¢„æµ‹å‡†ç¡®ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼ŒMarioGPTç”Ÿæˆçš„å…³å¡ä¸­ï¼Œæœ‰88.4%æ˜¯å¯ç©çš„ï¼Œå¹¶ä¸”ç”Ÿæˆçš„è·¯å¾„ä¸å®é™…ç©å®¶çš„è·¯å¾„ç›¸ä¼¼åº¦è¾ƒé«˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„MarioGPTæ¨¡å‹ä¸ºæ¸¸æˆå…³å¡ç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–°é¢–æ€§æœç´¢ç®—æ³•ï¼Œå¯ä»¥å®ç°é«˜æ•ˆã€è‡ªåŠ¨åœ°ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§å’Œå¯ç©æ€§çš„æ¸¸æˆå…³å¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ–‡æœ¬æç¤ºæ–¹æ³•ä¹Ÿä¸ºå…³å¡ç”Ÿæˆæä¾›äº†æ›´å¤šçš„æ§åˆ¶æ€§ã€‚<br><br>## ğŸŒŸ æœªæ¥å±•æœ›<br>æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š<br>1. æ‰©å±•MarioGPTæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´å¤æ‚ã€æ›´è¯¦ç»†çš„å…³å¡ã€‚<br>2. å°†äººç±»åé¦ˆå¼•å…¥å…³å¡ç”Ÿæˆè¿‡ç¨‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ä½¿ç”Ÿæˆçš„å…³å¡æ›´ç¬¦åˆç©å®¶çš„éœ€æ±‚ã€‚<br>3. æ¢ç´¢æ›´æœ‰æ•ˆçš„æœç´¢æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜å…³å¡ç”Ÿæˆçš„å¤šæ ·æ€§å’Œæ–°é¢–æ€§ã€‚</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Finding the Needle in a Haystack: Detecting Bug Occurrences in Gameplay Videos</td>
      <td>The presence of bugs in video games can bring significant consequences for<br>developers. To avoid these consequences, developers can leverage gameplay<br>videos to identify and fix these bugs. Video hosting websites such as YouTube<br>provide access to millions of game videos, including videos that depict bug<br>occurrences, but the large amount of content can make finding bug instances<br>challenging. We present an automated approach that uses machine learning to<br>predict whether a segment of a gameplay video contains the depiction of a bug.<br>We analyzed 4,412 segments of 198 gameplay videos to predict whether a segment<br>contains an instance of a bug. Additionally, we investigated how our approach<br>performs when applied across different specific genres of video games and on<br>videos from the same game. We also analyzed the videos in the dataset to<br>investigate what characteristics of the visual features might explain the<br>classifier's prediction. Finally, we conducted a user study to examine the<br>benefits of our automated approach against a manual analysis. Our findings<br>indicate that our approach is effective at detecting segments of a video that<br>contain bugs, achieving a high F1 score of 0.88, outperforming the current<br>state-of-the-art technique for bug classification of gameplay video segments.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆè§†é¢‘ä¸­çš„â€œå¯»é’ˆâ€ï¼šè‡ªåŠ¨æ£€æµ‹æ¸¸æˆä¸­çš„bug<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ¸¸æˆè¡Œä¸šæ˜¯ä¸€ä¸ªä»·å€¼æ•°åäº¿ç¾å…ƒçš„äº§ä¸šï¼Œæ¸¸æˆä¸­çš„bugä¼šå¯¹å¼€å‘è€…é€ æˆé‡å¤§å½±å“ã€‚ä¸ºäº†å‡å°‘è¿™äº›å½±å“ï¼Œå¼€å‘è€…å¯ä»¥åˆ©ç”¨æ¸¸æˆè§†é¢‘æ¥è¯†åˆ«å’Œä¿®å¤è¿™äº›bugã€‚è§†é¢‘æ‰˜ç®¡ç½‘ç«™å¦‚YouTubeæä¾›äº†æ•°ç™¾ä¸‡ä¸ªæ¸¸æˆè§†é¢‘ï¼ŒåŒ…æ‹¬æç»˜bugå‡ºç°çš„è§†é¢‘ï¼Œä½†å¤§é‡å†…å®¹ä½¿å¾—æ‰¾åˆ°bugå®ä¾‹å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ æ¥é¢„æµ‹æ¸¸æˆè§†é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«bugçš„æç»˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ æ¥é¢„æµ‹æ¸¸æˆè§†é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«bugçš„æç»˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æè§†é¢‘ç‰‡æ®µçš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡è¿˜è¿›è¡Œäº†åˆæ­¥åˆ†æï¼Œè§£é‡Šäº†è§†é¢‘å¸§å›¾åƒçš„å“ªäº›ç‰¹å¾å¯èƒ½ä¸bugçš„å‡ºç°æœ‰å…³ã€‚é€šè¿‡åˆ†æè§†é¢‘å¸§ï¼Œç ”ç©¶äººå‘˜å¯ä»¥äº†è§£å“ªäº›å…ƒç´ æˆ–è¡Œä¸ºå¯èƒ½å¯¼è‡´bugçš„å‡ºç°ï¼Œä»è€Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°è¿›è¡Œæµ‹è¯•å’Œä¿®å¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«4,412ä¸ªæ¸¸æˆè§†é¢‘ç‰‡æ®µçš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬198ä¸ªä¸åŒçš„æ¸¸æˆè§†é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹åŒ…å«bugçš„è§†é¢‘ç‰‡æ®µæ–¹é¢éå¸¸æœ‰æ•ˆï¼ŒF1åˆ†æ•°è¾¾åˆ°äº†0.88ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ¸¸æˆè§†é¢‘ç‰‡æ®µbugåˆ†ç±»æŠ€æœ¯ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥å¸®åŠ©æ¸¸æˆå¼€å‘è€…æ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œä¿®å¤æ¸¸æˆä¸­çš„bugã€‚é€šè¿‡ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¼€å‘è€…å¯ä»¥è‡ªåŠ¨åˆ†æå¤§é‡çš„æ¸¸æˆè§†é¢‘ï¼Œå¿«é€Ÿæ‰¾åˆ°åŒ…å«bugçš„ç‰‡æ®µï¼Œä»è€ŒèŠ‚çœæ—¶é—´å’Œç²¾åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„åˆ†æç»“æœè¿˜å¯ä»¥å¸®åŠ©å¼€å‘è€…äº†è§£å“ªäº›ç‰¹å¾å¯èƒ½ä¸bugçš„å‡ºç°æœ‰å…³ï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œæµ‹è¯•å’Œä¿®å¤ã€‚</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Creative Agents: Empowering Agents with Imagination for Creative Tasks</td>
      <td>We study building embodied agents for open-ended creative tasks. While<br>existing methods build instruction-following agents that can perform diverse<br>open-ended tasks, none of them demonstrates creativity -- the ability to give<br>novel and diverse task solutions implicit in the language instructions. This<br>limitation comes from their inability to convert abstract language instructions<br>into concrete task goals in the environment and perform long-horizon planning<br>for such complicated goals. Given the observation that humans perform creative<br>tasks with the help of imagination, we propose a class of solutions for<br>creative agents, where the controller is enhanced with an imaginator that<br>generates detailed imaginations of task outcomes conditioned on language<br>instructions. We introduce several approaches to implementing the components of<br>creative agents. We implement the imaginator with either a large language model<br>for textual imagination or a diffusion model for visual imagination. The<br>controller can either be a behavior-cloning policy learned from data or a<br>pre-trained foundation model generating executable codes in the environment. We<br>benchmark creative tasks with the challenging open-world game Minecraft, where<br>the agents are asked to create diverse buildings given free-form language<br>instructions. In addition, we propose novel evaluation metrics for open-ended<br>creative tasks utilizing GPT-4V, which holds many advantages over existing<br>metrics. We perform a detailed experimental analysis of creative agents,<br>showing that creative agents are the first AI agents accomplishing diverse<br>building creation in the survival mode of Minecraft. Our benchmark and models<br>are open-source for future research on creative agents<br>(https://github.com/PKU-RL/Creative-Agents).</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ›æ„æ™ºèƒ½ä½“ï¼šèµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›ä»¥å®Œæˆåˆ›æ„ä»»åŠ¡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç°æœ‰çš„æ™ºèƒ½ä½“å¤§å¤šåªèƒ½æ‰§è¡Œé¢„å®šä¹‰çš„ä»»åŠ¡ï¼Œç¼ºä¹å¤„ç†å¼€æ”¾æ€§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦åˆ›é€ åŠ›çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨Minecraftæ¸¸æˆä¸­ï¼Œç°æœ‰çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œç®€å•çš„æŒ‡ä»¤ï¼Œå¦‚â€œæ”¶é›†çŸ³å¤´â€æˆ–â€œå»ºé€ ä¸€ä¸ªé›ªäººâ€ï¼Œä½†æ— æ³•å®Œæˆæ›´å¤æ‚çš„åˆ›æ„ä»»åŠ¡ï¼Œå¦‚â€œå»ºé€ ä¸€ä¸ªç ‚å²©å®«æ®¿â€ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬æ— æ³•å°†æŠ½è±¡çš„è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºå…·ä½“çš„ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œé•¿æœŸè§„åˆ’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œåˆ›æ„æ™ºèƒ½ä½“â€çš„æ¦‚å¿µï¼Œé€šè¿‡èµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›æ¥å¤„ç†å¼€æ”¾æ€§åˆ›æ„ä»»åŠ¡ã€‚åˆ›æ„æ™ºèƒ½ä½“ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šæƒ³è±¡å™¨å’Œæ§åˆ¶å™¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæƒ³è±¡å™¨<br>æƒ³è±¡å™¨è´Ÿè´£æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»»åŠ¡ç»“æœçš„è¯¦ç»†æƒ³è±¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æƒ³è±¡å™¨çš„æ–¹æ³•ï¼š<br>- **æ–‡æœ¬æƒ³è±¡**ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4ï¼Œé€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æŠ€æœ¯ç”Ÿæˆæ–‡æœ¬æƒ³è±¡ã€‚<br>- **è§†è§‰æƒ³è±¡**ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionï¼Œç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸ç¬¦çš„è§†è§‰æƒ³è±¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ§åˆ¶å™¨<br>æ§åˆ¶å™¨è´Ÿè´£å°†æƒ³è±¡è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æ§åˆ¶å™¨çš„æ–¹æ³•ï¼š<br>- **è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨**ï¼šä»ç¯å¢ƒä¸­å­¦ä¹ è¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œå°†å›¾åƒæƒ³è±¡è½¬æ¢ä¸ºå»ºç­‘è“å›¾ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚<br>- **åŸºäºGPT-4(V)çš„æ§åˆ¶å™¨**ï¼šåˆ©ç”¨GPT-4(V)çš„è§†è§‰è¯­è¨€ç†è§£å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œç›´æ¥ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ¥å®Œæˆä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Minecraftæ¸¸æˆä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ›æ„æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æŒ‡ä»¤åˆ›å»ºå¤šæ ·åŒ–å’Œè§†è§‰ä¸Šå¸å¼•äººçš„å»ºç­‘ã€‚å…¶ä¸­ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œå¹¶ç»“åˆGPT-4(V)è¿›è¡Œæ§åˆ¶çš„æ™ºèƒ½ä½“è¡¨ç°æœ€ä½³ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åˆ›æ„æ™ºèƒ½ä½“æ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ å’Œåˆ›æ„AIæ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•æé«˜è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨çš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•å¢å¼ºæ™ºèƒ½ä½“çš„åˆ›é€ åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>52</th>
      <td>LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay</td>
      <td>This paper explores the open research problem of understanding the social<br>behaviors of LLM-based agents. Using Avalon as a testbed, we employ system<br>prompts to guide LLM agents in gameplay. While previous studies have touched on<br>gameplay with LLM agents, research on their social behaviors is lacking. We<br>propose a novel framework, tailored for Avalon, features a multi-agent system<br>facilitating efficient communication and interaction. We evaluate its<br>performance based on game success and analyze LLM agents' social behaviors.<br>Results affirm the framework's effectiveness in creating adaptive agents and<br>suggest LLM-based agents' potential in navigating dynamic social interactions.<br>By examining collaboration and confrontation behaviors, we offer insights into<br>this field's research and applications. Our code is publicly available at<br>https://github.com/3DAgentWorld/LLM-Game-Agent.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ç¤¾ä¼šè¡Œä¸ºç ”ç©¶ï¼šåœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­çš„åä½œä¸å¯¹æŠ—<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨æ¨¡æ‹Ÿå¤æ‚ç¯å¢ƒä¸­çš„è¡Œä¸ºæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨LLMæ™ºèƒ½ä½“åœ¨æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œè€Œå¯¹å…¶ç¤¾ä¼šè¡Œä¸ºçš„ç†è§£å´ç›¸å¯¹ç¼ºä¹ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMæ™ºèƒ½ä½“åœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­çš„ç¤¾ä¼šè¡Œä¸ºï¼ŒåŒ…æ‹¬åä½œä¸å¯¹æŠ—ï¼Œä»¥æœŸä¸ºLLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šå’Œæˆ˜ç•¥ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›æ–°çš„è§è§£ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºLLMæ™ºèƒ½ä½“åœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­çš„åä½œä¸å¯¹æŠ—ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œå¦‚è®°å¿†å­˜å‚¨å’Œæ€»ç»“ã€åˆ†æå’Œè§„åˆ’ã€æ¸¸æˆåŠ¨ä½œå’Œå“åº”ç”Ÿæˆã€ç»éªŒå­¦ä¹ ç­‰ï¼Œä»¥æ¨¡æ‹Ÿäººç±»æ€ç»´è¿‡ç¨‹ï¼Œå¸®åŠ©LLMæ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£æ¸¸æˆå¹¶åšå‡ºå†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡é‡‡ç”¨ç³»ç»Ÿæç¤ºæ¥å¼•å¯¼LLMæ™ºèƒ½ä½“è¿›è¡Œæ¸¸æˆï¼Œå¹¶ä½¿ç”¨ChatGPTæ¥åˆ†ææ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸ºã€‚é€šè¿‡è§‚å¯Ÿæ™ºèƒ½ä½“åœ¨æ¸¸æˆä¸­çš„åä½œã€å¯¹æŠ—ã€é¢†å¯¼ã€è¯´æœã€ä¼ªè£…ç­‰è¡Œä¸ºï¼Œæœ¬æ–‡æ­ç¤ºäº†LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„è¡Œä¸ºç‰¹ç‚¹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶åœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ™ºèƒ½ä½“çš„èƒœç‡æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†ææ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸ºï¼Œæœ¬æ–‡å‘ç°LLMæ™ºèƒ½ä½“èƒ½å¤Ÿå±•ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„ç¤¾ä¼šè¡Œä¸ºï¼Œå¦‚é¢†å¯¼ã€è¯´æœã€ä¼ªè£…ç­‰ï¼Œè¿™è¡¨æ˜LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œå®éªŒæ–¹æ³•ä¸ºç ”ç©¶LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„è¡Œä¸ºæä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿä¸ºLLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šå’Œæˆ˜ç•¥ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§è§£ï¼Œæœ‰åŠ©äºæ¨åŠ¨LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„åº”ç”¨ã€‚</td>
    </tr>
    <tr>
      <th>103</th>
      <td>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback</td>
      <td>We study whether multiple large language models (LLMs) can autonomously<br>improve each other in a negotiation game by playing, reflecting, and<br>criticizing. We are interested in this question because if LLMs were able to<br>improve each other, it would imply the possibility of creating strong AI agents<br>with minimal human intervention. We ask two LLMs to negotiate with each other,<br>playing the roles of a buyer and a seller, respectively. They aim to reach a<br>deal with the buyer targeting a lower price and the seller a higher one. A<br>third language model, playing the critic, provides feedback to a player to<br>improve the player's negotiation strategies. We let the two agents play<br>multiple rounds, using previous negotiation history and AI feedback as<br>in-context demonstrations to improve the model's negotiation strategy<br>iteratively. We use different LLMs (GPT and Claude) for different roles and use<br>the deal price as the evaluation metric. Our experiments reveal multiple<br>intriguing findings: (1) Only a subset of the language models we consider can<br>self-play and improve the deal price from AI feedback, weaker models either do<br>not understand the game's rules or cannot incorporate AI feedback for further<br>improvement. (2) Models' abilities to learn from the feedback differ when<br>playing different roles. For example, it is harder for Claude-instant to<br>improve as the buyer than as the seller. (3) When unrolling the game to<br>multiple rounds, stronger agents can consistently improve their performance by<br>meaningfully using previous experiences and iterative AI feedback, yet have a<br>higher risk of breaking the deal. We hope our work provides insightful initial<br>explorations of having models autonomously improve each other with game playing<br>and AI feedback.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹åœ¨è°ˆåˆ¤æ¸¸æˆä¸­é€šè¿‡è‡ªæˆ‘åšå¼ˆå’ŒAIåé¦ˆè¿›è¡Œè‡ªæˆ‘æå‡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æœ¬æ–‡ç ”ç©¶äº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤Ÿåœ¨è°ˆåˆ¤æ¸¸æˆä¸­é€šè¿‡è‡ªæˆ‘åšå¼ˆã€åæ€å’Œæ‰¹è¯„æ¥è‡ªä¸»åœ°ç›¸äº’æå‡ã€‚è¿™ä¸€ç ”ç©¶é—®é¢˜å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå¦‚æœLLMsèƒ½å¤Ÿç›¸äº’æå‡ï¼Œé‚£ä¹ˆå°±æœ‰å¯èƒ½ä»¥æå°çš„äººç±»å¹²é¢„æ¥åˆ›å»ºå¼ºå¤§çš„AIä»£ç†ã€‚æœ¬æ–‡é€šè¿‡è®©ä¸¤ä¸ªLLMsè¿›è¡Œè°ˆåˆ¤ï¼Œä¸€ä¸ªæ‰®æ¼”ä¹°å®¶ï¼Œä¸€ä¸ªæ‰®æ¼”å–å®¶ï¼Œå¹¶è®©ç¬¬ä¸‰ä¸ªLLMä½œä¸ºæ‰¹è¯„è€…æä¾›åé¦ˆæ¥æ”¹å–„ç©å®¶çš„è°ˆåˆ¤ç­–ç•¥ï¼Œä»è€Œæ¢ç´¢äº†è¿™ä¸€å¯èƒ½æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†â€œä»AIåé¦ˆä¸­è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ â€ï¼ˆICL-AIFï¼‰çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨AIæ‰¹è¯„è€…çš„åé¦ˆä»¥åŠä¹‹å‰çš„è°ˆåˆ¤å†å²ä½œä¸ºä¸Šä¸‹æ–‡æ¼”ç¤ºï¼Œä»¥è¿­ä»£åœ°æ”¹è¿›æ¨¡å‹çš„è°ˆåˆ¤ç­–ç•¥ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡ä½¿ç”¨ä¸åŒçš„LLMsï¼ˆGPTå’ŒClaudeï¼‰æ¥æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¹¶ä½¿ç”¨äº¤æ˜“ä»·æ ¼ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡å‘ç°åªæœ‰ä¸€éƒ¨åˆ†LLMsèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘åšå¼ˆå’ŒAIåé¦ˆæ¥æé«˜äº¤æ˜“ä»·æ ¼ï¼Œè€Œè¾ƒå¼±çš„æ¨¡å‹è¦ä¹ˆä¸ç†è§£æ¸¸æˆè§„åˆ™ï¼Œè¦ä¹ˆæ— æ³•å°†AIåé¦ˆçº³å…¥è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œåªæœ‰ä¸€éƒ¨åˆ†LLMsèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘åšå¼ˆå’ŒAIåé¦ˆæ¥æé«˜äº¤æ˜“ä»·æ ¼ï¼Œè€Œè¾ƒå¼±çš„æ¨¡å‹è¦ä¹ˆä¸ç†è§£æ¸¸æˆè§„åˆ™ï¼Œè¦ä¹ˆæ— æ³•å°†AIåé¦ˆçº³å…¥è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ‰®æ¼”ä¸åŒè§’è‰²æ—¶ï¼Œä»åé¦ˆä¸­å­¦ä¹ çš„èƒ½åŠ›ä¹Ÿæœ‰æ‰€ä¸åŒã€‚ä¾‹å¦‚ï¼ŒClaude-instantåœ¨æ‰®æ¼”ä¹°å®¶æ—¶æ¯”æ‰®æ¼”å–å®¶æ—¶æ›´éš¾æå‡ã€‚å½“æ¸¸æˆæ‰©å±•åˆ°å¤šè½®æ—¶ï¼Œæ›´å¼ºçš„ä»£ç†å¯ä»¥é€šè¿‡æœ‰æ„ä¹‰åœ°ä½¿ç”¨ä¹‹å‰çš„ç»éªŒå’Œè¿­ä»£AIåé¦ˆæ¥æŒç»­æé«˜å…¶æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨æ›´é«˜çš„äº¤æ˜“ç ´è£‚é£é™©ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨è°ˆåˆ¤æ¸¸æˆä¸­å…·æœ‰è‡ªæˆ‘æå‡çš„æ½œåŠ›ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨ä¸€äº›æŒ‘æˆ˜å’Œé£é™©ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨AIåé¦ˆæ¥æå‡LLMsçš„æ€§èƒ½ï¼Œå¹¶ç¡®ä¿å…¶è¡Œä¸ºç¬¦åˆäººç±»çš„æœŸæœ›å’Œä»·å€¼è§‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–å¤šæ™ºèƒ½ä½“æ¸¸æˆåœºæ™¯ä¸‹çš„AIå­¦ä¹ æä¾›å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Predicting Outcomes in Video Games with Long Short Term Memory Networks</td>
      <td>Forecasting winners in E-sports with real-time analytics has the potential to<br>further engage audiences watching major tournament events. However, making such<br>real-time predictions is challenging due to unpredictable variables within the<br>game involving diverse player strategies and decision-making. Our work attempts<br>to enhance audience engagement within video game tournaments by introducing a<br>real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)<br>based approach enables efficient predictions of win-lose outcomes by only using<br>the health indicator of each player as a time series. As a proof of concept, we<br>evaluate our model's performance within a classic, two-player arcade game,<br>Super Street Fighter II Turbo. We also benchmark our method against state of<br>the art methods for time series forecasting; i.e. Transformer models found in<br>large language models (LLMs). Finally, we open-source our data set and code in<br>hopes of furthering work in predictive analysis for arcade games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œé¢„æµ‹ç”µå­ç«æŠ€æ¯”èµ›ç»“æœ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ç”µå­ç«æŠ€ï¼ˆEsportsï¼‰çš„æ—¥ç›Šæµè¡Œï¼Œè§‚ä¼—å¯¹äºå®æ—¶æ¯”èµ›ç»“æœçš„é¢„æµ‹äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚ç„¶è€Œï¼Œç”±äºæ¸¸æˆä¸­çš„å˜é‡ä¼—å¤šï¼ŒåŒ…æ‹¬ç©å®¶ç­–ç•¥å’Œå†³ç­–çš„ä¸ç¡®å®šæ€§ï¼Œå®æ—¶é¢„æµ‹æ¯”èµ›ç»“æœä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§å®æ—¶é¢„æµ‹æ–¹æ³•æ¥å¢å¼ºè§‚ä¼—åœ¨ç”µå­æ¸¸æˆé”¦æ ‡èµ›ä¸­çš„å‚ä¸åº¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰è¿›è¡Œå®æ—¶é¢„æµ‹<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLSTMçš„å®æ—¶é¢„æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æ¯ä¸ªç©å®¶çš„å¥åº·æŒ‡ç¤ºå™¨ä½œä¸ºæ—¶é—´åºåˆ—æ¥é¢„æµ‹èƒœè´Ÿç»“æœã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶æ•æ‰æ¸¸æˆä¸­çš„åŠ¨æ€å˜åŒ–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨ç»å…¸çš„åŒäººè¡—æœºæ¸¸æˆã€Šè¶…çº§è¡—å¤´éœ¸ç‹II Turboã€‹ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½<br>ä¸ºäº†éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡åœ¨ç»å…¸çš„åŒäººè¡—æœºæ¸¸æˆã€Šè¶…çº§è¡—å¤´éœ¸ç‹II Turboã€‹ä¸­è¯„ä¼°äº†æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æç©å®¶å¥åº·æŒ‡ç¤ºå™¨çš„å˜åŒ–ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¯”èµ›çš„ä¸åŒé˜¶æ®µè¿›è¡Œå®æ—¶é¢„æµ‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„Transformeræ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•<br>ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡å°†LSTMæ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„Transformeræ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨é¢„æµ‹æ¯”èµ›ç»“æœæ–¹é¢è¡¨ç°å‡ºäº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨é¢„æµ‹æ¯”èµ›ç»“æœæ–¹é¢è¡¨ç°å‡ºäº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚åœ¨æ¯”èµ›çš„ä¸åŒé˜¶æ®µï¼ˆ25%ï¼Œ75%ï¼Œ95%ï¼‰ï¼ŒLSTMæ¨¡å‹çš„ROC-AUCåˆ†æ•°å‡é«˜äºTransformeræ¨¡å‹ã€‚æ­¤å¤–ï¼ŒLSTMæ¨¡å‹çš„è®­ç»ƒæ—¶é—´ä¹Ÿç›¸å¯¹è¾ƒçŸ­ï¼Œæ›´é€‚åˆå®æ—¶é¢„æµ‹åœºæ™¯ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºLSTMçš„å®æ—¶é¢„æµ‹æ–¹æ³•ä¸ºç”µå­ç«æŠ€æ¯”èµ›ç»“æœçš„é¢„æµ‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ç”µå­ç«æŠ€æ¸¸æˆï¼Œå¹¶æœ‰åŠ©äºæé«˜è§‚ä¼—åœ¨æ¯”èµ›ä¸­çš„å‚ä¸åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼€æºäº†æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºå…¶ä»–ç ”ç©¶äººå‘˜æä¾›äº†è¿›ä¸€æ­¥ç ”ç©¶çš„ä¾¿åˆ©ã€‚</td>
    </tr>
    <tr>
      <th>72</th>
      <td>SimulBench: Evaluating Language Models with Creative Simulation Tasks</td>
      <td>We introduce SimulBench, a benchmark designed to evaluate large language<br>models (LLMs) across a diverse collection of creative simulation scenarios,<br>such as acting as a Linux terminal or playing text games with users. While<br>these simulation tasks serve as effective measures of an LLM's general<br>intelligence, they are seldom incorporated into existing benchmarks. A major<br>challenge is to develop an evaluation framework for testing different LLMs<br>fairly while preserving the multi-round interactive nature of simulation tasks<br>between users and AI. To tackle this issue, we suggest using a fixed LLM as a<br>user agent to engage with an LLM to collect dialogues first under different<br>tasks. Then, challenging dialogue scripts are extracted for evaluating<br>different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4<br>is employed as the evaluator, tasked with reviewing the quality of the final<br>response generated by the target LLMs given multi-turn dialogue scripts. Our<br>comprehensive experiments indicate that these simulation tasks continue to pose<br>a significant challenge with their unique natures and show the gap between<br>proprietary models and the most advanced open LLMs. For example, GPT-4-turbo<br>outperforms LLaMA-3-70b-Chat on 18.55\% more cases.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SimulBenchï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨åˆ›æ„æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿå¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å•è½®ã€é™æ€çš„ç”¨æˆ·ä¸LLMsä¹‹é—´çš„äº¤äº’ï¼Œç¼ºä¹å¯¹å¤šè½®äº¤äº’å’Œå¤æ‚æ¨¡æ‹Ÿèƒ½åŠ›çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ä¸äººç±»ç›¸å…³çš„æ¨¡æ‹Ÿä»»åŠ¡ä¸Šï¼Œè€Œå¿½ç•¥äº†éäººç±»ä¸­å¿ƒçš„æ¨¡æ‹Ÿä»»åŠ¡ï¼Œå¦‚Linuxç»ˆç«¯æˆ–æ–‡æœ¬æ¸¸æˆç­‰ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSimulBenchåŸºå‡†<br>æœ¬æ–‡æå‡ºäº†SimulBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMsåœ¨åˆ›æ„æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†ã€‚SimulBenchåŒ…å«109ä¸ªç‹¬ç‰¹çš„æ¨¡æ‹Ÿä»»åŠ¡ï¼Œæ¶µç›–äº†å„ç§æ¥å£ï¼Œå¦‚Linuxç»ˆç«¯ã€SQLæ‰§è¡Œå™¨ã€æ–‡æœ¬æ¸¸æˆç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šè½®è„šæœ¬è¯„ä¼°æ¡†æ¶<br>ä¸ºäº†å…¬å¹³åœ°è¯„ä¼°ä¸åŒLLMsï¼ŒSimulBenché‡‡ç”¨äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è¯„ä¼°æ¡†æ¶ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„LLMä½œä¸ºç”¨æˆ·ä»£ç†ä¸å¦ä¸€ä¸ªLLMè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œæ”¶é›†å¯¹è¯å†å²ã€‚ç„¶åï¼Œä»è¿™äº›å¯¹è¯å†å²ä¸­æå–å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹è¯è„šæœ¬ï¼Œç”¨äºè¯„ä¼°ä¸åŒçš„ç›®æ ‡LLMsã€‚æœ€åï¼Œä½¿ç”¨GPT-4ä½œä¸ºè¯„ä¼°è€…ï¼Œå¯¹ç›®æ ‡LLMsåœ¨ç»™å®šå¤šè½®å¯¹è¯è„šæœ¬ä¸‹çš„æœ€ç»ˆå“åº”è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSimulBenchä¸­çš„æ¨¡æ‹Ÿä»»åŠ¡å¯¹LLMsæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸”æ˜¾ç¤ºäº†ä¸“æœ‰æ¨¡å‹å’Œæœ€å…ˆè¿›çš„å¼€æºLLMsä¹‹é—´çš„å·®è·ã€‚ä¾‹å¦‚ï¼ŒGPT-4-turboåœ¨18.55%çš„æƒ…å†µä¸‹ä¼˜äºLLaMA-3-70b-Chatã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SimulBenchåŸºå‡†ä¸ºè¯„ä¼°LLMsåœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å…¶å¤šè½®è„šæœ¬è¯„ä¼°æ¡†æ¶å¯ä»¥ç¡®ä¿å…¬å¹³çš„æ¯”è¾ƒï¼Œå¹¶æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMsåœ¨ä¸åŒæ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒSimulBenchçš„å®éªŒç»“æœä¹Ÿæ­ç¤ºäº†LLMsåœ¨å¤„ç†å¤æ‚æ¨¡æ‹Ÿä»»åŠ¡æ—¶çš„æŒ‘æˆ˜å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>91</th>
      <td>Embodied Task Planning with Large Language Models</td>
      <td>Equipping embodied agents with commonsense is important for robots to<br>successfully complete complex human instructions in general environments.<br>Recent large language models (LLM) can embed rich semantic knowledge for agents<br>in plan generation of complex tasks, while they lack the information about the<br>realistic world and usually yield infeasible action sequences. In this paper,<br>we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning<br>with physical scene constraint, where the agent generates executable plans<br>according to the existed objects in the scene by aligning LLMs with the visual<br>perception models. Specifically, we first construct a multimodal dataset<br>containing triplets of indoor scenes, instructions and action plans, where we<br>provide the designed prompts and the list of existing objects in the scene for<br>GPT-3.5 to generate a large number of instructions and corresponding planned<br>actions. The generated data is leveraged for grounded plan tuning of<br>pre-trained LLMs. During inference, we discover the objects in the scene by<br>extending open-vocabulary object detectors to multi-view RGB images collected<br>in different achievable locations. Experimental results show that the generated<br>plan from our TaPA framework can achieve higher success rate than LLaVA and<br>GPT-3.5 by a sizable margin, which indicates the practicality of embodied task<br>planning in general and complex environments.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…·èº«ä»»åŠ¡è§„åˆ’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæœºå™¨äººè¢«æœŸæœ›èƒ½å¤Ÿåœ¨å„ç§ç¯å¢ƒä¸­å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€åŒ»ç–—æŠ¤ç†å’Œå†œä¸šé‡‡æ‘˜ç­‰ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ ·æœ¬æœ‰é™å’Œä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œç›´æ¥è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿåœ¨ä¸åŒéƒ¨ç½²åœºæ™¯ä¸­å·¥ä½œçš„å…·èº«ä»£ç†æ˜¯ä¸å¯è¡Œçš„ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ä»å¤§é‡ç½‘ç»œæ•°æ®ä¸­è·å–ä¸°å¯Œçš„å¸¸è¯†çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯ä»¥æ½œåœ¨åœ°è¢«å…·èº«ä»£ç†åˆ©ç”¨æ¥ç”Ÿæˆç¬¦åˆäººç±»è¦æ±‚çš„è‡ªç„¶è¯­è¨€å‘½ä»¤çš„åŠ¨ä½œè®¡åˆ’ã€‚ç„¶è€Œï¼ŒLLMæ— æ³•æ„ŸçŸ¥å‘¨å›´åœºæ™¯ï¼Œå¹¶ä¸”å¯èƒ½ä¼šç”Ÿæˆä¸å¯æ‰§è¡Œçš„è¡ŒåŠ¨ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ä¸éå­˜åœ¨çš„å¯¹è±¡è¿›è¡Œäº¤äº’ã€‚å› æ­¤ï¼Œå°†LLMç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’ä¸ç‰©ç†ä¸–ç•Œç›¸ç»“åˆæ˜¯æ„å»ºèƒ½å¤Ÿå®Œæˆå¤æ‚ä»»åŠ¡çš„å…·èº«ä»£ç†çš„å¿…è¦æ¡ä»¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTaPAçš„å…·èº«ä»»åŠ¡è§„åˆ’ä»£ç†ï¼Œç”¨äºåœ¨ç‰©ç†åœºæ™¯ä¸­è¿›è¡Œå…·èº«ä»»åŠ¡è§„åˆ’ã€‚TaPAé€šè¿‡å°†LLMä¸è§†è§‰æ„ŸçŸ¥æ¨¡å‹ç›¸ç»“åˆï¼Œæ ¹æ®åœºæ™¯ä¸­å­˜åœ¨çš„å¯¹è±¡ç”Ÿæˆå¯æ‰§è¡Œçš„è®¡åˆ’ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«å®¤å†…åœºæ™¯ã€æŒ‡ä»¤å’ŒåŠ¨ä½œè®¡åˆ’ä¸‰å…ƒç»„çš„å¤šå…ƒæ•°æ®é›†ï¼Œå…¶ä¸­ä¸ºGPT-3.5æä¾›äº†è®¾è®¡çš„æç¤ºå’Œåœºæ™¯ä¸­ç°æœ‰å¯¹è±¡çš„åˆ—è¡¨ï¼Œä»¥ç”Ÿæˆå¤§é‡æŒ‡ä»¤å’Œç›¸åº”çš„è®¡åˆ’åŠ¨ä½œã€‚ç”Ÿæˆçš„æ•°æ®ç”¨äºå¯¹é¢„è®­ç»ƒçš„LLMè¿›è¡ŒåŸºäºç‰©ç†åœºæ™¯çº¦æŸçš„æ¥åœ°è®¡åˆ’å¾®è°ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ‰©å±•å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨åˆ°ä»ä¸åŒå¯è¾¾ä½ç½®æ”¶é›†çš„å¤šè§†å›¾RGBå›¾åƒï¼Œå‘ç°åœºæ™¯ä¸­çš„å¯¹è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸LLaVAå’ŒGPT-3.5ç›¸æ¯”ï¼ŒTaPAæ¡†æ¶ç”Ÿæˆçš„è®¡åˆ’å¯ä»¥å–å¾—æ›´é«˜çš„æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜å…·èº«ä»»åŠ¡è§„åˆ’åœ¨ä¸€èˆ¬å’Œå¤æ‚ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸LLaVAå’ŒGPT-3.5ç›¸æ¯”ï¼ŒTaPAæ¡†æ¶ç”Ÿæˆçš„è®¡åˆ’å¯ä»¥å–å¾—æ›´é«˜çš„æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜å…·èº«ä»»åŠ¡è§„åˆ’åœ¨ä¸€èˆ¬å’Œå¤æ‚ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„TaPAæ¡†æ¶ä¸ºå…·èº«ä»»åŠ¡è§„åˆ’æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡å°†LLMä¸è§†è§‰æ„ŸçŸ¥æ¨¡å‹ç›¸ç»“åˆï¼Œå¯ä»¥ç”Ÿæˆæ›´ç¬¦åˆç‰©ç†åœºæ™¯çº¦æŸçš„å¯æ‰§è¡Œè®¡åˆ’ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤šå…ƒæ•°æ®é›†ä¹Ÿä¸ºå…¶ä»–ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</td>
    </tr>
    <tr>
      <th>53</th>
      <td>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</td>
      <td>Recent studies have presented compelling evidence that large language models<br>(LLMs) can equip embodied agents with the self-driven capability to interact<br>with the world, which marks an initial step toward versatile robotics. However,<br>these efforts tend to overlook the visual richness of open worlds, rendering<br>the entire interactive process akin to "a blindfolded text-based game."<br>Consequently, LLM-based agents frequently encounter challenges in intuitively<br>comprehending their surroundings and producing responses that are easy to<br>understand. In this paper, we propose Steve-Eye, an end-to-end trained large<br>multimodal model designed to address this limitation. Steve-Eye integrates the<br>LLM with a visual encoder which enables it to process visual-text inputs and<br>generate multimodal feedback. In addition, we use a semi-automatic strategy to<br>collect an extensive dataset comprising 850K open-world instruction pairs,<br>empowering our model to encompass three essential functions for an agent:<br>multimodal perception, foundational knowledge base, and skill prediction and<br>planning. Lastly, we develop three open-world evaluation benchmarks, then carry<br>out extensive experiments from a wide range of perspectives to validate our<br>model's capability to strategically act and plan. Codes and datasets will be<br>released.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Steve-Eyeï¼šä¸ºåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“èµ‹äºˆå¼€æ”¾ä¸–ç•Œçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ‹äºˆå…·èº«æ™ºèƒ½ä½“ä¸ä¸–ç•Œäº’åŠ¨çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™æ ‡å¿—ç€é€šç”¨æœºå™¨äººæŠ€æœ¯è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾€å¾€å¿½ç•¥äº†å¼€æ”¾ä¸–ç•Œçš„è§†è§‰ä¸°å¯Œæ€§ï¼Œå¯¼è‡´æ•´ä¸ªäº¤äº’è¿‡ç¨‹ç±»ä¼¼äºâ€œä¸€ä¸ªè’™ç€çœ¼ç›çš„åŸºäºæ–‡æœ¬çš„æ¸¸æˆâ€ã€‚å› æ­¤ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨ç›´è§‚åœ°ç†è§£å‘¨å›´ç¯å¢ƒå’Œç”Ÿæˆæ˜“äºç†è§£çš„å“åº”æ–¹é¢ç»å¸¸é‡åˆ°æŒ‘æˆ˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Steve-Eyeï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨èµ‹äºˆåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œè§†è§‰æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚Steve-Eyeå°†LLMä¸è§†è§‰ç¼–ç å™¨ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰-æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆå¤šæ¨¡æ€åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠè‡ªåŠ¨ç­–ç•¥æ”¶é›†äº†ä¸€ä¸ªåŒ…å«850Kå¼€æ”¾ä¸–ç•ŒæŒ‡ä»¤å¯¹çš„å¹¿æ³›æ•°æ®é›†ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ¶µç›–æ™ºèƒ½ä½“çš„ä¸‰ä¸ªåŸºæœ¬åŠŸèƒ½ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸‰ä¸ªå¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ï¼Œç„¶åä»å¹¿æ³›çš„è§†è§’è¿›è¡Œå¤§é‡å®éªŒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSteve-Eyeåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºäºLLMçš„æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹ä¸‰ä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼š<br>1. ç¯å¢ƒè§†è§‰æè¿°ï¼ˆENV-VCï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æ„ŸçŸ¥å’Œæè¿°å…¶å‘¨å›´ç¯å¢ƒçš„èƒ½åŠ›ã€‚<br>2. åŸºç¡€çŸ¥è¯†é—®ç­”ï¼ˆFK-QAï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æŒæ¡å¯¹å†³ç­–è‡³å…³é‡è¦çš„åŸºæœ¬çŸ¥è¯†çš„ç†Ÿç»ƒç¨‹åº¦ã€‚<br>3. æŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ï¼ˆSPPï¼‰ï¼šé‡åŒ–æ™ºèƒ½ä½“åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Steve-Eyeçš„ç ”ç©¶æˆæœä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æœ‰æ•ˆäº’åŠ¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚å…¶å¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’åŠŸèƒ½ä¸ºæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»è¡ŒåŠ¨å’Œè§„åˆ’æä¾›äº†å¼ºå¤§çš„æ”¯æŒã€‚æ­¤å¤–ï¼ŒSteve-Eyeçš„å¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ä¸ºè¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>48</th>
      <td>MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs</td>
      <td>While Vision-Language Models (VLMs) hold promise for tasks requiring<br>extensive collaboration, traditional multi-agent simulators have facilitated<br>rich explorations of an interactive artificial society that reflects collective<br>behavior. However, these existing simulators face significant limitations.<br>Firstly, they struggle with handling large numbers of agents due to high<br>resource demands. Secondly, they often assume agents possess perfect<br>information and limitless capabilities, hindering the ecological validity of<br>simulated social interactions. To bridge this gap, we propose a multi-agent<br>Minecraft simulator, MineLand, that bridges this gap by introducing three key<br>features: large-scale scalability, limited multimodal senses, and physical<br>needs. Our simulator supports 64 or more agents. Agents have limited visual,<br>auditory, and environmental awareness, forcing them to actively communicate and<br>collaborate to fulfill physical needs like food and resources. Additionally, we<br>further introduce an AI agent framework, Alex, inspired by multitasking theory,<br>enabling agents to handle intricate coordination and scheduling. Our<br>experiments demonstrate that the simulator, the corresponding benchmark, and<br>the AI agent framework contribute to more ecological and nuanced collective<br>behavior.The source code of MineLand and Alex is openly available at<br>https://github.com/cocacola-lab/MineLand.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MineLandï¼šæ¨¡æ‹Ÿå¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“äº¤äº’çš„Minecraftæ¨¡æ‹Ÿå™¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶é¢ä¸´èµ„æºæ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ï¼Œå¹¶ä¸”é€šå¸¸å‡è®¾æ™ºèƒ½ä½“æ‹¥æœ‰å®Œç¾ä¿¡æ¯å’Œæ— é™èƒ½åŠ›ï¼Œè¿™ä¸ç°å®ä¸–ç•Œä¸­çš„äººç±»äº¤äº’å­˜åœ¨è¾ƒå¤§å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MineLandï¼Œä¸€ä¸ªåŸºäºMinecraftçš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿæ›´æ¥è¿‘ç°å®ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“äº¤äº’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤§è§„æ¨¡å¯æ‰©å±•æ€§<br>MineLandé€šè¿‡å°†æ¯ä¸ªMinecraftå®¢æˆ·ç«¯ç®€åŒ–ä¸ºå•ä¸ªçº¿ç¨‹ï¼Œä¼˜åŒ–äº†æ€§èƒ½å¼€é”€ï¼Œä»è€Œæ”¯æŒ64ä¸ªæˆ–æ›´å¤šæ™ºèƒ½ä½“åœ¨ä¸»æµæ¶ˆè´¹çº§æ¡Œé¢PCä¸Šè¿è¡Œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ‰é™çš„æ¨¡æ€æ„ŸçŸ¥<br>MineLandæ¨¡æ‹Ÿäº†äººç±»çš„è§†è§‰å’Œå¬è§‰æœºåˆ¶ï¼Œå¯¹æ™ºèƒ½ä½“çš„æ„ŸçŸ¥èƒ½åŠ›æ–½åŠ äº†é™åˆ¶ï¼ŒåŒ…æ‹¬è·ç¦»è¡°å‡ã€ç¯å¢ƒé®æŒ¡å’Œæ–¹å‘çº¦æŸï¼Œä½¿å…¶æ›´æ¥è¿‘ç°å®ä¸–ç•Œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç‰©ç†éœ€æ±‚<br>MineLandå°†çœŸå®çš„ç‰©ç†éœ€æ±‚ï¼ˆå¦‚é£Ÿç‰©ã€æ°§æ°”å’Œé¥¥é¥¿ï¼‰é›†æˆåˆ°æ™ºèƒ½ä½“ä¸­ï¼Œä½¿å…¶éœ€è¦ç®¡ç†èµ„æºå¹¶ä¸å…¶ä»–æ™ºèƒ½ä½“ç«äº‰æˆ–åˆä½œï¼Œä»¥ç»´æŒç”Ÿå­˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šä»»åŠ¡å¤„ç†æ¡†æ¶Alex<br>MineLandå¼•å…¥äº†åŸºäºå¤šä»»åŠ¡ç†è®ºçš„AIæ™ºèƒ½ä½“æ¡†æ¶Alexï¼Œå…è®¸æ™ºèƒ½ä½“åŒæ—¶æ‰§è¡Œå¤æ‚çš„åè°ƒå’Œè°ƒåº¦ï¼Œä»¥å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒMineLandåœ¨æ”¯æŒå¤§è§„æ¨¡æ™ºèƒ½ä½“ã€æœ‰é™çš„æ¨¡æ€æ„ŸçŸ¥å’Œç‰©ç†éœ€æ±‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒAlexæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šä»»åŠ¡ï¼Œå¹¶åœ¨åˆä½œæ¨¡å¼ä¸‹æé«˜æ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MineLandä¸ºç ”ç©¶å¤šæ™ºèƒ½ä½“äº¤äº’æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ï¼Œå…¶åˆ›æ–°çš„è®¾è®¡å’ŒåŠŸèƒ½å¯ä»¥åº”ç”¨äºäººç±»åŠ¨åŠ›å­¦ã€ç¤¾ä¼šå¿ƒç†å­¦ã€æœºå™¨äººæŠ€æœ¯å’Œæ¸¸æˆè®¾è®¡ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼ŒAlexæ¡†æ¶çš„å¤šä»»åŠ¡å¤„ç†æœºåˆ¶ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„AIæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>57</th>
      <td>GameGPT: Multi-agent Collaborative Framework for Game Development</td>
      <td>The large language model (LLM) based agents have demonstrated their capacity<br>to automate and expedite software development processes. In this paper, we<br>focus on game development and propose a multi-agent collaborative framework,<br>dubbed GameGPT, to automate game development. While many studies have<br>pinpointed hallucination as a primary roadblock for deploying LLMs in<br>production, we identify another concern: redundancy. Our framework presents a<br>series of methods to mitigate both concerns. These methods include dual<br>collaboration and layered approaches with several in-house lexicons, to<br>mitigate the hallucination and redundancy in the planning, task identification,<br>and implementation phases. Furthermore, a decoupling approach is also<br>introduced to achieve code generation with better precision.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GameGPTï¼šæ¸¸æˆå¼€å‘çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„çªç ´ï¼Œå®ƒä»¬åœ¨è‡ªåŠ¨åŒ–å’ŒåŠ é€Ÿè½¯ä»¶å¼€å‘è¿‡ç¨‹æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨æ¸¸æˆå¼€å‘é¢†åŸŸï¼ŒLLMçš„åº”ç”¨é¢ä¸´ç€ä¸¤å¤§æŒ‘æˆ˜ï¼šå¹»è§‰å’Œå†—ä½™ã€‚å¹»è§‰æŒ‡çš„æ˜¯LLMåœ¨ç”Ÿæˆå†…å®¹æ—¶å¯èƒ½å‡ºç°çš„ä¸å‡†ç¡®æˆ–ä¸ç›¸å…³çš„æƒ…å†µï¼Œè€Œå†—ä½™åˆ™æ˜¯æŒ‡LLMå¯èƒ½ä¼šç”Ÿæˆä¸å¿…è¦çš„ä»»åŠ¡æˆ–ä»£ç ç‰‡æ®µã€‚è¿™äº›æŒ‘æˆ˜é™åˆ¶äº†LLMåœ¨æ¸¸æˆå¼€å‘ä¸­çš„å®é™…åº”ç”¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶<br>GameGPTæå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LLMåœ¨æ¸¸æˆå¼€å‘ä¸­çš„å¹»è§‰å’Œå†—ä½™é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±å¤šä¸ªå…·æœ‰ä¸åŒè§’è‰²çš„æ™ºèƒ½ä½“ç»„æˆï¼ŒåŒ…æ‹¬æ¸¸æˆå†…å®¹è®¾è®¡å¸ˆã€æ¸¸æˆå¼€å‘ç»ç†ã€è®¡åˆ’å®¡æŸ¥å‘˜ã€æ¸¸æˆå¼€å‘å·¥ç¨‹å¸ˆã€ä»»åŠ¡å®¡æŸ¥å‘˜ã€æ¸¸æˆå¼•æ“å·¥ç¨‹å¸ˆã€ä»£ç å®¡æŸ¥å‘˜å’Œæ¸¸æˆå¼•æ“æµ‹è¯•å·¥ç¨‹å¸ˆã€‚è¿™äº›æ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œå…±åŒå®Œæˆæ¸¸æˆå¼€å‘çš„å„ä¸ªé˜¶æ®µï¼Œä»è€Œæé«˜å¼€å‘æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒé‡åä½œå’Œåˆ†å±‚æ–¹æ³•<br>ä¸ºäº†è§£å†³å¹»è§‰å’Œå†—ä½™é—®é¢˜ï¼ŒGameGPTé‡‡ç”¨äº†åŒé‡åä½œå’Œåˆ†å±‚æ–¹æ³•ã€‚åŒé‡åä½œåŒ…æ‹¬LLMä¸å°å‹ä¸“å®¶æ·±åº¦å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠæ‰§è¡Œè§’è‰²å’Œå®¡æŸ¥è§’è‰²ä¹‹é—´çš„åä½œã€‚åˆ†å±‚æ–¹æ³•åˆ™é€šè¿‡ä½¿ç”¨å¤šä¸ªå†…éƒ¨è¯å…¸æ¥æŒ‡å¯¼LLMçš„å†³ç­–è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘å¹»è§‰å’Œå†—ä½™çš„å‘ç”Ÿã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»£ç è§£è€¦<br>ä¸ºäº†æé«˜ä»£ç ç”Ÿæˆçš„ç²¾åº¦ï¼ŒGameGPTå¼•å…¥äº†ä»£ç è§£è€¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æ¸¸æˆè®¾è®¡è„šæœ¬åˆ†è§£æˆå¤šä¸ªå¯ç®¡ç†çš„ä»£ç ç‰‡æ®µï¼Œä»è€Œç®€åŒ–LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å‡å°‘å¹»è§‰å’Œå†—ä½™çš„å‘ç”Ÿã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGameGPTæ¡†æ¶åœ¨æ¸¸æˆå¼€å‘è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå†³ç­–å’Œå†³ç­–ä¿®æ­£ï¼Œä»è€Œæé«˜å¼€å‘æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒGameGPTæ¡†æ¶è¿˜å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ç”¨äºå¼€å‘ä¸­åˆ°å¤§å‹çš„æ¸¸æˆé¡¹ç›®ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GameGPTæ¡†æ¶ä¸ºæ¸¸æˆå¼€å‘è‡ªåŠ¨åŒ–æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ã€åŒé‡åä½œå’Œåˆ†å±‚æ–¹æ³•ä»¥åŠä»£ç è§£è€¦æ–¹æ³•ç­‰åˆ›æ–°ç‚¹ï¼Œå¯ä»¥ä¸ºå…¶ä»–é¢†åŸŸçš„è½¯ä»¶å¼€å‘è‡ªåŠ¨åŒ–æä¾›å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒGameGPTæ¡†æ¶è¿˜å¯ä»¥ä¸å…¶ä»–AIæŠ€æœ¯ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜æ¸¸æˆå¼€å‘çš„è‡ªåŠ¨åŒ–æ°´å¹³ã€‚</td>
    </tr>
    <tr>
      <th>110</th>
      <td>LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation</td>
      <td>To assist with everyday human activities, robots must solve complex<br>long-horizon tasks and generalize to new settings. Recent deep reinforcement<br>learning (RL) methods show promise in fully autonomous learning, but they<br>struggle to reach long-term goals in large environments. On the other hand,<br>Task and Motion Planning (TAMP) approaches excel at solving and generalizing<br>across long-horizon tasks, thanks to their powerful state and action<br>abstractions. But they assume predefined skill sets, which limits their<br>real-world applications. In this work, we combine the benefits of these two<br>paradigms and propose an integrated task planning and skill learning framework<br>named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the<br>symbolic interface of a task planner to guide RL-based skill learning and<br>creates abstract state space to enable skill reuse. More importantly, LEAGUE<br>learns manipulation skills in-situ of the task planning system, continuously<br>growing its capability and the set of tasks that it can solve. We evaluate<br>LEAGUE on four challenging simulated task domains and show that LEAGUE<br>outperforms baselines by large margins. We also show that the learned skills<br>can be reused to accelerate learning in new tasks domains and transfer to a<br>physical robot platform.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LEAGUEï¼šåŸºäºå¼•å¯¼çš„æŠ€èƒ½å­¦ä¹ å’ŒæŠ½è±¡ï¼ŒåŠ©åŠ›æœºå™¨äººè§£å†³é•¿æœŸæ“ä½œä»»åŠ¡<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæœºå™¨äººå·²ç»é€æ¸èµ°è¿›æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œå¹¶åœ¨å„ç§åœºæ™¯ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œè¦è®©æœºå™¨äººçœŸæ­£å®ç°è‡ªä¸»å­¦ä¹ å’Œæ“ä½œï¼Œä»ç„¶é¢ä¸´ç€è®¸å¤šæŒ‘æˆ˜ã€‚å…¶ä¸­ï¼Œé•¿æœŸæ“ä½œä»»åŠ¡ï¼ˆlong-horizon tasksï¼‰çš„è§£å†³å’Œæ³›åŒ–èƒ½åŠ›æ˜¯æœºå™¨äººé¢†åŸŸçš„ä¸€å¤§éš¾é¢˜ã€‚ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ–¹æ³•åœ¨è‡ªä¸»å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤§å‹ç¯å¢ƒä¸­å®ç°é•¿æœŸç›®æ ‡ä»ç„¶å­˜åœ¨å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ–¹æ³•æ“…é•¿è§£å†³å’Œæ³›åŒ–é•¿æœŸä»»åŠ¡ï¼Œä½†ç”±äºå…¶ä¾èµ–äºé¢„å®šä¹‰çš„æŠ€èƒ½é›†ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†å…‹æœä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†LEAGUEï¼ˆLearning and Abstraction with Guidanceï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†DRLå’ŒTAMPçš„ä¼˜åŠ¿ï¼Œå®ç°äº†é•¿æœŸæ“ä½œä»»åŠ¡çš„è§£å†³å’Œæ³›åŒ–ã€‚LEAGUEçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ä»»åŠ¡è§„åˆ’å™¨çš„ç¬¦å·æ¥å£æŒ‡å¯¼åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ€èƒ½å­¦ä¹ ï¼Œå¹¶åˆ›å»ºæŠ½è±¡çŠ¶æ€ç©ºé—´ä»¥å®ç°æŠ€èƒ½å¤ç”¨ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨ä»»åŠ¡è§„åˆ’ç³»ç»Ÿä¸­å­¦ä¹ æ“ä½œæŠ€èƒ½ï¼Œä¸æ–­æ‰©å±•å…¶èƒ½åŠ›å’Œå¯è§£å†³çš„ä»»åŠ¡é›†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿä»»åŠ¡é¢†åŸŸå¯¹LEAGUEè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜LEAGUEåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†å­¦ä¹ åˆ°çš„æŠ€èƒ½å¯ä»¥å¤ç”¨äºåŠ é€Ÿæ–°ä»»åŠ¡é¢†åŸŸçš„å­¦ä¹ ï¼Œå¹¶è¿ç§»åˆ°ç‰©ç†æœºå™¨äººå¹³å°ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LEAGUEæ¡†æ¶ä¸ºæœºå™¨äººè§£å†³é•¿æœŸæ“ä½œä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚æ­¤å¤–ï¼ŒLEAGUEæ¡†æ¶ä¸­çš„æŠ€èƒ½å­¦ä¹ å’ŒæŠ½è±¡æ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›å€Ÿé‰´ã€‚</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</td>
      <td>Video games have served as useful benchmarks for the decision making<br>community, but going beyond Atari games towards training agents in modern games<br>has been prohibitively expensive for the vast majority of the research<br>community. Recent progress in the research, development and open release of<br>large vision models has the potential to amortize some of these costs across<br>the community. However, it is currently unclear which of these models have<br>learnt representations that retain information critical for sequential decision<br>making. Towards enabling wider participation in the research of gameplaying<br>agents in modern games, we present a systematic study of imitation learning<br>with publicly available visual encoders compared to the typical, task-specific,<br>end-to-end training approach in Minecraft, Minecraft Dungeons and<br>Counter-Strike: Global Offensive.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è§†è§‰ç¼–ç å™¨åœ¨ç°ä»£è§†é¢‘æ¸¸æˆä¸­çš„é«˜æ•ˆæ¨¡ä»¿å­¦ä¹ <br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è§†é¢‘æ¸¸æˆä¸€ç›´æ˜¯å†³ç­–åˆ¶å®šç¤¾åŒºçš„æœ‰ç”¨åŸºå‡†ï¼Œä½†å°†ç ”ç©¶æ‰©å±•åˆ°ç°ä»£æ¸¸æˆå¯¹äºå¤§å¤šæ•°ç ”ç©¶ç¤¾åŒºæ¥è¯´æˆæœ¬é«˜æ˜‚ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰æ¨¡å‹çš„ç ”ç©¶ã€å¼€å‘å’Œå…¬å¼€å‘å¸ƒæœ‰å¯èƒ½åœ¨æ•´ä¸ªç¤¾åŒºä¸­åˆ†æ‘Šè¿™äº›æˆæœ¬ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹ä¸­çš„å“ªäº›æ¨¡å‹å·²ç»å­¦ä¹ äº†ä¿ç•™å¯¹é¡ºåºå†³ç­–è‡³å…³é‡è¦çš„ä¿¡æ¯çš„è¡¨ç¤ºã€‚ä¸ºäº†ä½¿æ›´å¹¿æ³›çš„ç¤¾åŒºå‚ä¸ç°ä»£æ¸¸æˆä¸­çš„æ¸¸æˆä»£ç†ç ”ç©¶ï¼Œæœ¬æ–‡å¯¹Minecraftã€Minecraft Dungeonså’ŒCounter-Strike: Global Offensiveä¸­çš„æ¨¡ä»¿å­¦ä¹ è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œå¹¶ä¸å…¸å‹çš„ã€ç‰¹å®šä»»åŠ¡çš„ç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æ¯”è¾ƒäº†ç«¯åˆ°ç«¯è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒç¼–ç å™¨åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¼–ç å™¨åœ¨ç›¸å¯¹è¾ƒå°çš„å›¾åƒä¸Šè®­ç»ƒï¼Œè€Œé¢„è®­ç»ƒç¼–ç å™¨åˆ™æ˜¯åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œå¯èƒ½æä¾›æœ‰ç”¨ä¸”é€šç”¨çš„è¡¨ç¤ºï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡ç ”ç©¶äº†ä¸åŒæ•°é‡çš„è®­ç»ƒæ•°æ®å¯¹è§†è§‰ç¼–ç å™¨æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒç¼–ç å™¨ä¹Ÿèƒ½è¡¨ç°å‡ºä¸ç‰¹å®šä»»åŠ¡ç¼–ç å™¨ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ä¸‰ä¸ªç°ä»£è§†é¢‘æ¸¸æˆï¼ˆMinecraftã€Minecraft Dungeonså’ŒCounter-Strike: Global Offensiveï¼‰ä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š<br><br>1. å°å›¾åƒï¼ˆ128Ã—128ï¼‰è¶³ä»¥è®­ç»ƒç°ä»£è§†é¢‘æ¸¸æˆä¸­çš„ä»£ç†ï¼Œå³ä½¿åœ¨ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚<br>2. é¢„è®­ç»ƒç¼–ç å™¨ï¼Œç‰¹åˆ«æ˜¯DINOv2ï¼Œåœ¨æ¨¡ä»¿å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨å°‘é‡æ•°æ®æ—¶ä»ç„¶æœ‰æ•ˆã€‚<br>3. ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¼–ç å™¨åœ¨å¤„ç†æ›´çœŸå®ä¸–ç•Œçš„å›¾åƒæ—¶è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨ä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨æ—¶éœ€è¦ä»”ç»†è€ƒè™‘å›¾åƒå¤§å°å’Œæ¯”ä¾‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒç¼–ç å™¨åœ¨ç°ä»£è§†é¢‘æ¸¸æˆçš„æ¨¡ä»¿å­¦ä¹ ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨è¿™äº›ç¼–ç å™¨æ¥è®­ç»ƒä»£ç†ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†å›¾åƒå¤§å°å’Œæ¯”ä¾‹å¯¹é¢„è®­ç»ƒç¼–ç å™¨æ€§èƒ½çš„é‡è¦æ€§ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>75</th>
      <td>True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning</td>
      <td>Despite the impressive performance across numerous tasks, large language<br>models (LLMs) often fail in solving simple decision-making tasks due to the<br>misalignment of the knowledge in LLMs with environments. On the contrary,<br>reinforcement learning (RL) agents learn policies from scratch, which makes<br>them always align with environments but difficult to incorporate prior<br>knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a<br>novel general online framework that deploys LLMs as decision-making agents to<br>efficiently interact and align with embodied environments via RL without<br>requiring any prepared datasets or prior knowledge of the environments.<br>Firstly, we query the joint probabilities of each valid action with LLMs to<br>form behavior policies. Then, to enhance the stability and robustness of the<br>policies, we propose two normalization methods and summarize four prompt design<br>principles. Finally, we design a novel parameter-efficient training<br>architecture where the actor and critic share one frozen LLM equipped with<br>low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to<br>evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency<br>and performance compared to the conventional RL method, PPO, and prompt tuning<br>method, SayCan, in both classical decision-making environment, Overcooked, and<br>simulated household environment, VirtualHome. ii) Benefiting from LLMs'<br>open-vocabulary feature, TWOSOME shows superior generalization ability to<br>unseen tasks. iii) Under our framework, there is no significant loss of the<br>LLMs' original ability during online PPO finetuning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | çŸ¥è¯†æºäºå®è·µï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸å…·èº«ç¯å¢ƒå¯¹é½<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨è§£å†³ç®€å•çš„å†³ç­–ä»»åŠ¡æ—¶å¾€å¾€å¤±è´¥ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºLLMsä¸­çš„çŸ¥è¯†ä¸ç¯å¢ƒçš„é”™ä½ã€‚ç›¸åï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ä»é›¶å¼€å§‹å­¦ä¹ ç­–ç•¥ï¼Œè¿™ä½¿å¾—å®ƒä»¬å§‹ç»ˆä¸ç¯å¢ƒä¿æŒä¸€è‡´ï¼Œä½†éš¾ä»¥èå…¥å…ˆéªŒçŸ¥è¯†ä»¥æé«˜æ¢ç´¢æ•ˆç‡ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†TWOSOMEï¼Œä¸€ä¸ªæ–°é¢–çš„é€šç”¨åœ¨çº¿æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†LLMsä½œä¸ºå†³ç­–ä»£ç†ï¼Œé€šè¿‡RLé«˜æ•ˆåœ°ä¸å…·èº«ç¯å¢ƒäº¤äº’å¹¶å¯¹é½ï¼Œè€Œæ— éœ€ä»»ä½•å‡†å¤‡å¥½çš„æ•°æ®é›†æˆ–å¯¹ç¯å¢ƒçš„å…ˆéªŒçŸ¥è¯†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¡Œä¸ºç­–ç•¥ç”Ÿæˆ<br>TWOSOMEä¸æ˜¯è®©LLMsç›´æ¥ç”ŸæˆåŠ¨ä½œï¼Œè€Œæ˜¯æŸ¥è¯¢LLMsä¸­æ¯ä¸ªæœ‰æ•ˆåŠ¨ä½œçš„è”åˆæ¦‚ç‡ï¼Œä»¥å½¢æˆè¡Œä¸ºç­–ç•¥ã€‚è¿™æ¶ˆé™¤äº†ç”±äºæ— æ•ˆåŠ¨ä½œé€ æˆçš„é”™ä½é—®é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨ä½œæç¤ºå½’ä¸€åŒ–<br>ä¸ºäº†å¢å¼ºç­–ç•¥çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§å½’ä¸€åŒ–æ–¹æ³•ï¼štokenå½’ä¸€åŒ–å’Œwordå½’ä¸€åŒ–ï¼Œä»¥è§£å†³åŠ¨ä½œåˆ†å¸ƒä¸å¹³è¡¡çš„é—®é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå‚æ•°é«˜æ•ˆçš„è®­ç»ƒæ¶æ„<br>æœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å‚æ•°é«˜æ•ˆè®­ç»ƒæ¶æ„ï¼Œå…¶ä¸­actorå’Œcriticå…±äº«ä¸€ä¸ªå†»ç»“çš„LLMï¼Œå¹¶é…å¤‡ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰ï¼Œç”±PPOæ›´æ–°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæç¤ºè®¾è®¡åŸåˆ™<br>æœ¬æ–‡æ€»ç»“äº†å››ä¸ªæç¤ºè®¾è®¡åŸåˆ™ï¼Œä»¥å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæé«˜LLMsä¸å…·èº«ç¯å¢ƒä¹‹é—´çš„å¯¹é½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ç»å…¸å†³ç­–ç¯å¢ƒOvercookedå’Œæ¨¡æ‹Ÿå®¶åº­ç¯å¢ƒVirtualHomeä¸­ï¼ŒTWOSOMEåœ¨æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„RLæ–¹æ³•PPOå’Œæç¤ºè°ƒæ•´æ–¹æ³•SayCanã€‚æ­¤å¤–ï¼ŒTWOSOMEè¿˜è¡¨ç°å‡ºå¯¹æœªè§ä»»åŠ¡çš„ä¼˜è¶Šæ³›åŒ–èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„TWOSOMEæ¡†æ¶ä¸ºå°†LLMsä¸å…·èº«ç¯å¢ƒå¯¹é½æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¼€å‘é€šç”¨çš„è‡ªä¸»ä»£ç†è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚è¯¥æ¡†æ¶å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>*   åˆ©ç”¨LLMsçš„å…ˆéªŒçŸ¥è¯†æ¥æé«˜RLä»£ç†çš„æ ·æœ¬æ•ˆç‡ã€‚<br>*   é€šè¿‡å½’ä¸€åŒ–æ–¹æ³•è§£å†³åŠ¨ä½œåˆ†å¸ƒä¸å¹³è¡¡çš„é—®é¢˜ã€‚<br>*   è®¾è®¡æœ‰æ•ˆçš„æç¤ºä»¥æé«˜LLMsçš„æ¨ç†èƒ½åŠ›ã€‚<br>*   ä½¿ç”¨å‚æ•°é«˜æ•ˆçš„è®­ç»ƒæ¶æ„æ¥é™ä½è®­ç»ƒæˆæœ¬ã€‚<br><br>## ğŸ¯ æœªæ¥å±•æœ›<br>æœªæ¥å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å°†TWOSOMEæ¡†æ¶åº”ç”¨äºæ›´å¤æ‚çš„å…·èº«ç¯å¢ƒï¼Œå¹¶æ¢ç´¢LLMsä¸RLä»£ç†ä¹‹é—´çš„æ›´æ·±å…¥äº¤äº’ã€‚</td>
    </tr>
    <tr>
      <th>61</th>
      <td>Humanoid Agents: Platform for Simulating Human-like Generative Agents</td>
      <td>Just as computational simulations of atoms, molecules and cells have shaped<br>the way we study the sciences, true-to-life simulations of human-like agents<br>can be valuable tools for studying human behavior. We propose Humanoid Agents,<br>a system that guides Generative Agents to behave more like humans by<br>introducing three elements of System 1 processing: Basic needs (e.g. hunger,<br>health and energy), Emotion and Closeness in Relationships. Humanoid Agents are<br>able to use these dynamic elements to adapt their daily activities and<br>conversations with other agents, as supported with empirical experiments. Our<br>system is designed to be extensible to various settings, three of which we<br>demonstrate, as well as to other elements influencing human behavior (e.g.<br>empathy, moral values and cultural background). Our platform also includes a<br>Unity WebGL game interface for visualization and an interactive analytics<br>dashboard to show agent statuses over time. Our platform is available on<br>https://www.humanoidagents.com/ and code is on<br>https://github.com/HumanoidAgents/HumanoidAgents</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | äººç±»åŒ–æ™ºèƒ½ä½“ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“å¹³å°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼ˆGenerative Agentsï¼‰çš„å‡ºç°ï¼Œäººä»¬å¼€å§‹å°è¯•ä½¿ç”¨é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæ¥æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ä¸»è¦å…³æ³¨é€»è¾‘å’Œè®¡åˆ’ï¼Œç¼ºä¹å¯¹äººç±»ç›´è§‰å’Œå³æ—¶ååº”çš„æ¨¡æ‹Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†äººç±»åŒ–æ™ºèƒ½ä½“ï¼ˆHumanoid Agentsï¼‰å¹³å°ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦ç­‰å…ƒç´ ï¼Œä½¿æ™ºèƒ½ä½“æ›´æ¥è¿‘äººç±»çš„çœŸå®è¡Œä¸ºã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥ç³»ç»Ÿ1æ€ç»´<br>æœ¬æ–‡å€Ÿé‰´äº†å¿ƒç†å­¦ä¸­çš„ç³»ç»Ÿ1æ€ç»´ï¼Œå³ç›´è§‰ã€æ— æ„è¯†å’Œå³æ—¶çš„æ€ç»´è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦ç­‰å…ƒç´ ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªèº«çŠ¶æ€å’Œç¯å¢ƒå˜åŒ–åšå‡ºæ›´è‡ªç„¶çš„ååº”ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€è°ƒæ•´è¡Œä¸º<br>äººç±»åŒ–æ™ºèƒ½ä½“å¹³å°å…è®¸æ™ºèƒ½ä½“æ ¹æ®è‡ªèº«çš„åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦åŠ¨æ€è°ƒæ•´å…¶æ—¥å¸¸æ´»åŠ¨å’Œå¯¹è¯ã€‚ä¾‹å¦‚ï¼Œå½“æ™ºèƒ½ä½“æ„Ÿåˆ°é¥¥é¥¿æ—¶ï¼Œå®ƒä¼šå¯»æ‰¾é£Ÿç‰©ï¼›å½“å®ƒæ„Ÿåˆ°å­¤ç‹¬æ—¶ï¼Œå®ƒä¼šå°è¯•ä¸å…¶ä»–æ™ºèƒ½ä½“è¿›è¡Œäº¤æµã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»åŒ–æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚ä¸äººç±»æ ‡æ³¨ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹æ´»åŠ¨æ˜¯å¦æ»¡è¶³åŸºæœ¬éœ€æ±‚ã€æ´»åŠ¨è¡¨è¾¾çš„æƒ…æ„Ÿä»¥åŠå¯¹è¯æ˜¯å¦ä½¿æ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»æ›´åŠ äº²å¯†ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„Humanoid Agentså¹³å°ä¸ºç ”ç©¶äººç±»è¡Œä¸ºæä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚è¯¥å¹³å°å¯ä»¥æ‰©å±•åˆ°å„ç§åœºæ™¯ï¼Œå¹¶æ”¯æŒæ›´å¤šå½±å“äººç±»è¡Œä¸ºçš„å…ƒç´ ï¼Œå¦‚åŒç†å¿ƒã€é“å¾·ä»·å€¼è§‚å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥å¹³å°è¿˜æä¾›äº†Unity WebGLæ¸¸æˆç•Œé¢å’Œäº¤äº’å¼åˆ†æä»ªè¡¨æ¿ï¼Œæ–¹ä¾¿ç”¨æˆ·å¯è§†åŒ–æ™ºèƒ½ä½“çš„çŠ¶æ€å’Œè¡Œä¸ºã€‚<br><br>## ğŸŒ å¹³å°è®¿é—®<br>- å¹³å°ç½‘ç«™ï¼šhttps://www.humanoidagents.com/<br>- ä»£ç ä»“åº“ï¼šhttps://github.com/HumanoidAgents/HumanoidAgents</td>
    </tr>
    <tr>
      <th>98</th>
      <td>Playing repeated games with Large Language Models</td>
      <td>Large Language Models (LLMs) are transforming society and permeating into<br>diverse applications. As a result, LLMs will frequently interact with us and<br>other agents. It is, therefore, of great societal value to understand how LLMs<br>behave in interactive social settings. Here, we propose to use behavioral game<br>theory to study LLM's cooperation and coordination behavior. To do so, we let<br>different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with<br>each other and with other, human-like strategies. Our results show that LLMs<br>generally perform well in such tasks and also uncover persistent behavioral<br>signatures. In a large set of two players-two strategies games, we find that<br>LLMs are particularly good at games where valuing their own self-interest pays<br>off, like the iterated Prisoner's Dilemma family. However, they behave<br>sub-optimally in games that require coordination. We, therefore, further focus<br>on two games from these distinct families. In the canonical iterated Prisoner's<br>Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting<br>after another agent has defected only once. In the Battle of the Sexes, we find<br>that GPT-4 cannot match the behavior of the simple convention to alternate<br>between options. We verify that these behavioral signatures are stable across<br>robustness checks. Finally, we show how GPT-4's behavior can be modified by<br>providing further information about the other player as well as by asking it to<br>predict the other player's actions before making a choice. These results enrich<br>our understanding of LLM's social behavior and pave the way for a behavioral<br>game theory for machines.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡å¤åšå¼ˆä¸­çš„è¡Œä¸ºç ”ç©¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šå„é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬ä¸äººç±»å’Œå…¶ä»–æ™ºèƒ½ä½“ä¹‹é—´çš„äº’åŠ¨æ—¥ç›Šé¢‘ç¹ã€‚å› æ­¤ï¼Œç†è§£LLMsåœ¨ç¤¾äº¤äº’åŠ¨ä¸­çš„è¡Œä¸ºå˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è¡Œä¸ºåšå¼ˆè®ºæ¥ç ”ç©¶LLMsçš„åˆä½œä¸åè°ƒè¡Œä¸ºã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨è¡Œä¸ºåšå¼ˆè®ºç ”ç©¶LLMs<br>æœ¬æ–‡æå‡ºä½¿ç”¨è¡Œä¸ºåšå¼ˆè®ºæ¥ç ”ç©¶LLMsçš„åˆä½œä¸åè°ƒè¡Œä¸ºã€‚é€šè¿‡è®©ä¸åŒçš„LLMsï¼ˆå¦‚GPT-3ã€GPT-3.5å’ŒGPT-4ï¼‰ä¸å…¶ä»–LLMsæˆ–äººç±»ç­–ç•¥è¿›è¡Œæœ‰é™é‡å¤åšå¼ˆï¼Œåˆ†æå®ƒä»¬çš„è¡Œä¸ºæ¨¡å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç ”ç©¶LLMsåœ¨ä¸åŒåšå¼ˆç±»å‹ä¸­çš„è¡¨ç°<br>æœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨å¤šç§åšå¼ˆç±»å‹ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬åŒèµ¢æ¸¸æˆã€ä¸å…¬å¹³æ¸¸æˆã€å¾ªç¯æ¸¸æˆã€æœ‰åæ¸¸æˆå’Œå›šå¾’å›°å¢ƒç­‰ã€‚é€šè¿‡åˆ†æLLMsåœ¨ä¸åŒåšå¼ˆç±»å‹ä¸­çš„å¾—åˆ†å’Œç­–ç•¥é€‰æ‹©ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡Œä¸ºç‰¹å¾ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨è¿½æ±‚è‡ªèº«åˆ©ç›Šçš„åšå¼ˆä¸­è¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶æ˜¯åœ¨å›šå¾’å›°å¢ƒç­‰åšå¼ˆä¸­ã€‚ç„¶è€Œï¼Œåœ¨éœ€è¦åè°ƒçš„åšå¼ˆä¸­ï¼ŒLLMsçš„è¡¨ç°è¾ƒå·®ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»å…¸çš„å›šå¾’å›°å¢ƒä¸­ï¼ŒGPT-4è¡¨ç°å‡ºæç«¯çš„ä¸å®½å®¹ï¼Œä¸€æ—¦å¯¹æ–¹èƒŒå›ï¼Œå®ƒå°±ä¼šæŒç»­èƒŒå›ã€‚è€Œåœ¨æ€§åˆ«ä¹‹æˆ˜ä¸­ï¼ŒGPT-4æ— æ³•ä¸äº¤æ›¿é€‰æ‹©çš„äººç±»ç­–ç•¥ç›¸åŒ¹é…ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœæœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£LLMsçš„ç¤¾äº¤è¡Œä¸ºï¼Œå¹¶ä¸ºæœºå™¨è¡Œä¸ºåšå¼ˆè®ºçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸¤ç§æ”¹è¿›LLMsè¡Œä¸ºçš„æ–¹æ³•ï¼šæä¾›å…³äºå…¶ä»–ç©å®¶çš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠè®©LLMsé¢„æµ‹å…¶ä»–ç©å®¶çš„è¡Œä¸ºã€‚è¿™äº›æ–¹æ³•å¯ä»¥å¸®åŠ©LLMsæ›´å¥½åœ°é€‚åº”ç¤¾äº¤ç¯å¢ƒï¼Œå¹¶ä¸äººç±»è¿›è¡Œæ›´æœ‰æ•ˆçš„äº’åŠ¨ã€‚</td>
    </tr>
    <tr>
      <th>108</th>
      <td>Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey</td>
      <td>Knowledge-enhanced language models (KELMs) have emerged as promising tools to<br>bridge the gap between large-scale language models and domain-specific<br>knowledge. KELMs can achieve higher factual accuracy and mitigate<br>hallucinations by leveraging knowledge graphs (KGs). They are frequently<br>combined with adapter modules to reduce the computational load and risk of<br>catastrophic forgetting. In this paper, we conduct a systematic literature<br>review (SLR) on adapter-based approaches to KELMs. We provide a structured<br>overview of existing methodologies in the field through quantitative and<br>qualitative analysis and explore the strengths and potential shortcomings of<br>individual approaches. We show that general knowledge and domain-specific<br>approaches have been frequently explored along with various adapter<br>architectures and downstream tasks. We particularly focused on the popular<br>biomedical domain, where we provided an insightful performance comparison of<br>existing KELMs. We outline the main trends and propose promising future<br>directions.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºé€‚é…å™¨çš„çŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹æ–¹æ³•ç»¼è¿°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œå°½ç®¡å®ƒä»¬åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡å’Œç”Ÿæˆæ–°æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹å¯¹ç»“æ„åŒ–çŸ¥è¯†å±‚æ¬¡ï¼ˆå¦‚æ¦‚å¿µä¹‹é—´çš„å…³ç³»å’Œæ¨ç†èƒ½åŠ›ï¼‰çš„è®¤è¯†ã€‚è¿™å¯èƒ½å¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é¢„æµ‹ä¸å‡†ç¡®ï¼Œä»¥åŠåœ¨æ–‡æœ¬ç”Ÿæˆä¸­äº§ç”Ÿæ‰€è°“çš„â€œå¹»è§‰â€ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ä¿å¥æˆ–æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡ç»¼è¿°äº†åŸºäºé€‚é…å™¨çš„çŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆKELMsï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰æ¥æé«˜LLMsçš„å¯é æ€§ã€‚é€‚é…å™¨æ¨¡å—è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆä¸”è®¡ç®—æ•ˆç‡é«˜çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å¢å¼ºLLMsçš„ä»»åŠ¡æ€§èƒ½ï¼ŒåŒæ—¶é¿å…ç¾éš¾æ€§é—å¿˜å’Œä»»åŠ¡é—´çš„å¹²æ‰°ã€‚<br><br>### é€‚é…å™¨ç±»å‹<br>- **Houlsby Adapter**: åœ¨æ¯ä¸ªTransformerå±‚ä¸­æ·»åŠ ä¸¤ä¸ªé€‚é…å™¨æ¨¡å—ï¼Œåˆ†åˆ«ä½äºå¤šå¤´æ³¨æ„åŠ›å±‚å’Œä¸¤ä¸ªå‰é¦ˆå±‚ä¹‹åã€‚<br>- **Bapna and Firat Adapter**: ä»…åœ¨æ¯ä¸ªTransformerå±‚ä¸­çš„å¤šå¤´æ³¨æ„åŠ›å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªé€‚é…å™¨æ¨¡å—ã€‚<br>- **Pfeiffer Adapter and AdapterFusion**: ä½¿ç”¨AdapterFusionç®—æ³•æ¥ç»„åˆä¸åŒä»»åŠ¡ä¸Šè®­ç»ƒçš„é€‚é…å™¨ï¼Œä»è€Œå®ç°ä¿¡æ¯å…±äº«ã€‚<br>- **K-Adapter**: ä½œä¸ºâ€œå¤–éƒ¨æ’ä»¶â€ï¼Œç”±å¤šä¸ªé€‚é…å™¨å±‚ç»„æˆï¼Œå¯ä»¥æ’å…¥åˆ°é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒTransformerå±‚ä¸­ã€‚<br><br>### çŸ¥è¯†å¢å¼ºæ–¹æ³•<br>- **é€šç”¨çŸ¥è¯†**: åˆ©ç”¨ConceptNetå’ŒDBpediaç­‰çŸ¥è¯†å›¾è°±ï¼Œå°†å¸¸è¯†å’Œä¸–ç•ŒçŸ¥è¯†æ³¨å…¥LLMsã€‚<br>- **è¯­è¨€çŸ¥è¯†**: å°†è¯­è¨€çŸ¥è¯†ï¼ˆå¦‚åŠ¨è¯æ„ä¹‰å’Œè®ºå…ƒç»“æ„ï¼‰æ³¨å…¥é€‚é…å™¨ï¼Œä»¥æé«˜äº‹ä»¶æå–å’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚<br>- **é¢†åŸŸç‰¹å®šçŸ¥è¯†**: åˆ©ç”¨é€‚é…å™¨è¿›è¡Œé¢†åŸŸé€‚åº”ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œé€šè¿‡UMLSç­‰çŸ¥è¯†å›¾è°±æ¥å¢å¼ºLLMsã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åŸºäºé€‚é…å™¨çš„KELMsåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½å§‹ç»ˆä¼˜äºåŸºç¡€LLMsã€‚ä¾‹å¦‚ï¼ŒDAKIå’ŒMoPæ¡†æ¶åœ¨PubMedQAä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†çº¦7%å’Œ8%ã€‚æ­¤å¤–ï¼Œé€‚é…å™¨è°ƒä¼˜æ¯”å¸¸è§„å¾®è°ƒæ›´èƒ½ç¼“è§£é—å¿˜é—®é¢˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>- **é€‚é…å™¨æ¶æ„**: ç ”ç©¶äººå‘˜å¯ä»¥æ¢ç´¢æ›´é«˜æ•ˆçš„é€‚é…å™¨æ¶æ„ï¼Œä»¥å…‹æœåºåˆ—æ•°æ®å¤„ç†çš„å»¶è¿Ÿå¹¶å®ç°ç¡¬ä»¶å¹¶è¡ŒåŒ–ã€‚<br>- **é¢†åŸŸåº”ç”¨**: é™¤äº†åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œé€‚é…å™¨å¢å¼ºçš„LLMsè¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–é«˜åº¦ç»“æ„åŒ–çš„é¢†åŸŸï¼Œå¦‚æ³•å¾‹æˆ–é‡‘èé¢†åŸŸã€‚<br>- **ä»»åŠ¡ç±»å‹**: æœªæ¥å¯ä»¥æ¢ç´¢å°†çŸ¥è¯†å¢å¼ºåº”ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œä»¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„äº‹å®æ€§å’Œä¿¡æ¯æ€§ã€‚<br><br>## ğŸ“š æ€»ç»“<br>æœ¬æ–‡ç»¼è¿°äº†åŸºäºé€‚é…å™¨çš„çŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œå¹¶åˆ†æäº†ä¸åŒé€‚é…å™¨æ¶æ„å’ŒçŸ¥è¯†å¢å¼ºæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚éšç€ç ”ç©¶çš„ä¸æ–­æ·±å…¥ï¼Œé€‚é…å™¨å¢å¼ºçš„LLMsæœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå’Œä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚</td>
    </tr>
    <tr>
      <th>66</th>
      <td>Adaptive Multi-Goal Exploration</td>
      <td>We introduce a generic strategy for provably efficient multi-goal<br>exploration. It relies on AdaGoal, a novel goal selection scheme that leverages<br>a measure of uncertainty in reaching states to adaptively target goals that are<br>neither too difficult nor too easy. We show how AdaGoal can be used to tackle<br>the objective of learning an \( \epsilon \)-optimal goal-conditioned policy for the<br>(initially unknown) set of goal states that are reachable within \( L \) steps in<br>expectation from a reference state \( s_0 \) in a reward-free Markov decision<br>process. In the tabular case with \( S \) states and \( A \) actions, our algorithm<br>requires \( \tilde{O}(L^3 S A \epsilon^{-2}) \) exploration steps, which is nearly<br>minimax optimal. We also readily instantiate AdaGoal in linear mixture Markov<br>decision processes, yielding the first goal-oriented PAC guarantee with linear<br>function approximation. Beyond its strong theoretical guarantees, we anchor<br>AdaGoal in goal-conditioned deep reinforcement learning, both conceptually and<br>empirically, by connecting its idea of selecting "uncertain" goals to<br>maximizing value ensemble disagreement.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªé€‚åº”å¤šç›®æ ‡æ¢ç´¢ï¼šAdaGoalç­–ç•¥<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå½“å¤–éƒ¨å¥–åŠ±ä¿¡å·ç¼ºå¤±æˆ–ä¸å…·ä¿¡æ¯æ€§æ—¶ï¼Œæ™ºèƒ½ä½“éœ€è¦é€šè¿‡æ¢ç´¢ç¯å¢ƒæ¥å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æœ€å¤§åŒ–å¥–åŠ±ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“éœ€è¦è‡ªä¸»è®¾å®šç›®æ ‡å¹¶å­¦ä¹ æœ‰æ•ˆåœ°è¾¾åˆ°è¿™äº›ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGC-RLï¼‰æ–¹æ³•å¾€å¾€ç¼ºä¹ç†è®ºæ”¯æŒå’Œä¿è¯ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦å­¦ä¹ ç¯å¢ƒä¸­ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç›®æ ‡æ¢ç´¢ï¼ˆMGEï¼‰ç›®æ ‡<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„ç­–ç•¥ï¼Œç”¨äºè¯æ˜æœ‰æ•ˆçš„å¤šç›®æ ‡æ¢ç´¢ã€‚è¯¥ç­–ç•¥ä¾èµ–äºAdaGoalï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç›®æ ‡é€‰æ‹©æ–¹æ¡ˆï¼Œå®ƒåˆ©ç”¨åˆ°è¾¾çŠ¶æ€çš„ä¸ç¡®å®šæ€§åº¦é‡æ¥é€‚åº”æ€§åœ°é€‰æ‹©æ—¢ä¸å¤ªéš¾ä¹Ÿä¸å¤ªå®¹æ˜“çš„ç›®æ ‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAdaGoalç›®æ ‡é€‰æ‹©æ–¹æ¡ˆ<br>AdaGoalé€šè¿‡è§£å†³ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜æ¥é€‚åº”æ€§åœ°é€‰æ‹©ä¸­ç­‰éš¾åº¦çš„ç›®æ ‡çŠ¶æ€ã€‚å®ƒè¿˜æä¾›äº†ä¸€ä¸ªç®—æ³•åœæ­¢è§„åˆ™å’Œä¸€ä¸ªå€™é€‰ç›®æ ‡çŠ¶æ€é›†ï¼Œæ™ºèƒ½ä½“å¯¹è¿™äº›çŠ¶æ€æœ‰ä¿¡å¿ƒå¯ä»¥å¯é åœ°åˆ°è¾¾ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAdaGoal-UCBVIå’ŒAdaGoal-UCRLÂ·VTRç®—æ³•<br>æœ¬æ–‡è®¾è®¡äº†AdaGoal-UCBVIç®—æ³•ï¼Œç”¨äºåœ¨è¡¨æ ¼MDPä¸­è§£å†³MGEé—®é¢˜ï¼Œå¹¶è¯æ˜äº†å®ƒå‡ ä¹æ˜¯æœ€ä¼˜çš„æ ·æœ¬å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†AdaGoal-UCRLÂ·VTRç®—æ³•ï¼Œç”¨äºçº¿æ€§æ··åˆMDPï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰çº¿æ€§å‡½æ•°è¿‘ä¼¼çš„é¢å‘ç›®æ ‡çš„PACä¿è¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šAdaGoalåœ¨æ·±åº¦GC-RLä¸­çš„åº”ç”¨<br>æœ¬æ–‡å°†AdaGoalçš„æ¦‚å¿µä¸æ·±åº¦GC-RLä¸­çš„æœ€å¤§åŒ–ä»·å€¼é›†åˆåˆ†æ­§è”ç³»èµ·æ¥ï¼Œä»è€Œåœ¨æ¦‚å¿µå’Œç»éªŒä¸Šå°†å…¶é”šå®šåœ¨ç›®æ ‡æ¡ä»¶æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨å®éªŒä¸­ï¼ŒAdaGoal-UCBVIå’ŒAdaGoal-UCRLÂ·VTRç®—æ³•åœ¨è¡¨æ ¼MDPå’Œçº¿æ€§æ··åˆMDPä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„AdaGoalç­–ç•¥ä¸ºæ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>*   **è‡ªé€‚åº”ç›®æ ‡é€‰æ‹©**ï¼šAdaGoalé€šè¿‡è€ƒè™‘ç›®æ ‡çŠ¶æ€çš„ä¸ç¡®å®šæ€§æ¥é€‰æ‹©ç›®æ ‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒã€‚<br>*   **ç†è®ºä¿è¯**ï¼šAdaGoal-UCBVIå’ŒAdaGoal-UCRLÂ·VTRç®—æ³•å…·æœ‰å‡ ä¹æœ€ä¼˜çš„æ ·æœ¬å¤æ‚åº¦ï¼Œä¸ºæ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æä¾›äº†ç†è®ºæ”¯æŒã€‚<br>*   **æ·±åº¦å­¦ä¹ åº”ç”¨**ï¼šAdaGoalçš„æ¦‚å¿µå¯ä»¥ä¸æ·±åº¦å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œä»è€Œåœ¨å¤æ‚çš„ä»»åŠ¡ä¸­å®ç°æœ‰æ•ˆçš„ç›®æ ‡æ¢ç´¢ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>æœ¬æ–‡æå‡ºçš„AdaGoalç­–ç•¥ä¸ºæ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚AdaGoalç­–ç•¥å…·æœ‰è‡ªé€‚åº”ç›®æ ‡é€‰æ‹©ã€ç†è®ºä¿è¯å’Œæ·±åº¦å­¦ä¹ åº”ç”¨ç­‰ä¼˜ç‚¹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>64</th>
      <td>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</td>
      <td>The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by<br>Large Language Models (LLMs) make them promising candidates for developing<br>coordination agents. In this study, we introduce a new LLM-Coordination<br>Benchmark aimed at a detailed analysis of LLMs within the context of Pure<br>Coordination Games, where participating agents need to cooperate for the most<br>gain. This benchmark evaluates LLMs through two distinct tasks: (1)<br>\emph{Agentic Coordination}, where LLMs act as proactive participants for<br>cooperation in 4 pure coordination games; (2) \emph{Coordination Question<br>Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice<br>questions from the 4 games for evaluation of three key reasoning abilities:<br>Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to<br>enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture<br>for Coordination (CAC) framework that can easily integrate different LLMs as<br>plug-and-play modules for pure coordination games. Our findings indicate that<br>LLM agents equipped with GPT-4-turbo achieve comparable performance to<br>state-of-the-art reinforcement learning methods in games that require<br>commonsense actions based on the environment. Besides, zero-shot coordination<br>experiments reveal that, unlike RL methods, LLM agents are robust to new unseen<br>partners. However, results on Coordination QA show a large room for improvement<br>in the Theory of Mind reasoning and joint planning abilities of LLMs. The<br>analysis also sheds light on how the ability of LLMs to understand their<br>environment and their partner's beliefs and intentions plays a part in their<br>ability to plan for coordination. Our code is available at<br>\url{https://github.com/eric-ai-lab/llm_coordination}.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LLM-Coordinationï¼šè¯„ä¼°å’Œåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨è®¸å¤šæ—¥å¸¸ä»»åŠ¡å’Œå…³é”®æ“ä½œä¸­ï¼Œå¦‚çƒ¹é¥ªå’Œæ•‘æ´è¡ŒåŠ¨ï¼Œåˆä½œæ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™äº›åœºæ™¯å¯ä»¥è¢«è§†ä¸ºçº¯åè°ƒæ¸¸æˆï¼Œå…¶ä¸­æ‰€æœ‰å‚ä¸æ–¹éƒ½ä»é€‰æ‹©å®Œå…¨ä¸€è‡´çš„æˆ˜ç•¥ä¸­å—ç›Šï¼Œé¿å…ä»»ä½•åˆ©ç›Šå†²çªã€‚è¿™äº›æ¸¸æˆè¦æ±‚ä»£ç†æ¨ç†ä»–ä»¬çš„ç¯å¢ƒå¹¶è®¡åˆ’ï¼ŒåŒæ—¶è€ƒè™‘ä»–ä»¬çš„ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨ç‰©ç†å’Œè™šæ‹Ÿç¯å¢ƒä¸­çš„æ¶Œç°è§„åˆ’èƒ½åŠ›ã€ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›å’Œå¯¹å¿ƒæ™ºç†è®ºçš„æš—ç¤ºï¼Œä½¿å®ƒä»¬æˆä¸ºå¼€å‘åè°ƒä»£ç†çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ã€‚ç„¶è€Œï¼ŒLLMsåœ¨åè°ƒæ¸¸æˆä¸­çš„å¿…è¦æ¡ä»¶ã€ä¼˜åŠ¿å’Œå±€é™æ€§ä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è¿›è¡ŒLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›çš„å…¨é¢è¯„ä¼°å’Œåˆ†ææ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„LLM-CoordinationåŸºå‡†ï¼Œæ—¨åœ¨å¯¹LLMsåœ¨çº¯åè°ƒæ¸¸æˆä¸­çš„èƒ½åŠ›è¿›è¡Œè¯¦ç»†åˆ†æã€‚è¯¥åŸºå‡†é€šè¿‡ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡è¯„ä¼°LLMsï¼š<br>1. **ä»£ç†åè°ƒ**ï¼šLLMsä½œä¸ºç§¯æåˆä½œå‚ä¸è€…å‚ä¸4ä¸ªçº¯åè°ƒæ¸¸æˆã€‚<br>2. **åè°ƒé—®ç­”ï¼ˆQAï¼‰**ï¼šLLMsè¢«æç¤ºå›ç­”æ¥è‡ª4ä¸ªæ¸¸æˆçš„198ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œä»¥è¯„ä¼°ä¸‰ä¸ªå…³é”®æ¨ç†èƒ½åŠ›ï¼šç¯å¢ƒç†è§£ã€å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’ã€‚<br><br>æ­¤å¤–ï¼Œä¸ºäº†ä½¿LLMsèƒ½å¤Ÿè¿›è¡Œå¤šæ™ºèƒ½ä½“åè°ƒï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåè°ƒè®¤çŸ¥æ¶æ„ï¼ˆCACï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è½»æ¾åœ°å°†ä¸åŒçš„LLMsä½œä¸ºå³æ’å³ç”¨æ¨¡å—é›†æˆåˆ°çº¯åè°ƒæ¸¸æˆä¸­ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œé…å¤‡GPT-4-turboçš„LLMä»£ç†åœ¨éœ€è¦åŸºäºç¯å¢ƒçš„å¸¸è¯†è¡ŒåŠ¨çš„æ¸¸æˆä¸­ï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œé›¶æ ·æœ¬åè°ƒå®éªŒè¡¨æ˜ï¼Œä¸RLæ–¹æ³•ä¸åŒï¼ŒLLMä»£ç†å¯¹æ–°æœªè§ä¼™ä¼´å…·æœ‰é²æ£’æ€§ã€‚ç„¶è€Œï¼Œåè°ƒQAçš„ç»“æœè¡¨æ˜ï¼ŒLLMsçš„å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚åˆ†æè¿˜æ­ç¤ºäº†LLMsç†è§£å…¶ç¯å¢ƒå’Œå…¶ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾çš„èƒ½åŠ›å¦‚ä½•å½±å“å®ƒä»¬åè°ƒè®¡åˆ’çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„LLM-CoordinationåŸºå‡†å’ŒCACæ¡†æ¶ä¸ºè¯„ä¼°å’Œåˆ†æLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç»“æœçªå‡ºäº†LLMsåœ¨åè°ƒä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>71</th>
      <td>Motif: Intrinsic Motivation from Artificial Intelligence Feedback</td>
      <td>Exploring rich environments and evaluating one's actions without prior<br>knowledge is immensely challenging. In this paper, we propose Motif, a general<br>method to interface such prior knowledge from a Large Language Model (LLM) with<br>an agent. Motif is based on the idea of grounding LLMs for decision-making<br>without requiring them to interact with the environment: it elicits preferences<br>from an LLM over pairs of captions to construct an intrinsic reward, which is<br>then used to train agents with reinforcement learning. We evaluate Motif's<br>performance and behavior on the challenging, open-ended and<br>procedurally-generated NetHack game. Surprisingly, by only learning to maximize<br>its intrinsic reward, Motif achieves a higher game score than an algorithm<br>directly trained to maximize the score itself. When combining Motif's intrinsic<br>reward with the environment reward, our method significantly outperforms<br>existing approaches and makes progress on tasks where no advancements have ever<br>been made without demonstrations. Finally, we show that Motif mostly generates<br>intuitive human-aligned behaviors which can be steered easily through prompt<br>modifications, while scaling well with the LLM size and the amount of<br>information given in the prompt.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Motifï¼šä»äººå·¥æ™ºèƒ½åé¦ˆä¸­è·å–å†…åœ¨åŠ¨æœº<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œæ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„æ™ºèƒ½ä½“æ¢ç´¢å’Œè¯„ä¼°å…¶è¡Œä¸ºæå…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Motif çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸­çš„å…ˆéªŒçŸ¥è¯†ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ï¼Œä»è€Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨æ²¡æœ‰ä¸ç¯å¢ƒçš„ç›´æ¥äº¤äº’çš„æƒ…å†µä¸‹è¿›è¡Œå†³ç­–ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ LLM çš„åå¥½æ„å»ºå†…åœ¨å¥–åŠ±<br>Motif çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡ LLM å¯¹äº‹ä»¶æ ‡é¢˜çš„åå¥½æ¥æ„å»ºå†…åœ¨å¥–åŠ±å‡½æ•°ï¼Œå¹¶å°†å…¶ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä½“ã€‚LLM è¡¨è¾¾å¯¹æˆå¯¹äº‹ä»¶æ ‡é¢˜çš„åå¥½ï¼Œè¿™äº›æ ‡é¢˜åªéœ€ç²—ç•¥æè¿°ç¯å¢ƒä¸­å‘ç”Ÿçš„äº‹ä»¶ï¼Œè€Œä¸éœ€è¦ç²¾ç»†çš„é€æ­¥æè¿°ã€‚LLM ä¸éœ€è¦ç†è§£ä½çº§åŠ¨ä½œç©ºé—´ï¼Œè¿™å¯èƒ½æ˜¯å¤åˆçš„æˆ–è¿ç»­çš„ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå†…åœ¨å¥–åŠ±ä¸å¤–åœ¨å¥–åŠ±çš„ç»“åˆ<br>Motif çš„å†…åœ¨å¥–åŠ±å¯ä»¥å•ç‹¬ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä¸æ¥è‡ªç¯å¢ƒçš„å¥–åŠ±ä¿¡å·ç»“åˆä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å†…åœ¨å¥–åŠ±ä¸å¤–åœ¨å¥–åŠ±ç»“åˆä½¿ç”¨æ—¶ï¼ŒMotif çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ²¡æœ‰æ¼”ç¤ºçš„æƒ…å†µä¸‹å–å¾—äº†è¿›å±•ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Motif åœ¨ NetHack å­¦ä¹ ç¯å¢ƒ (NLE) ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ã€å¼€æ”¾æ€§å’Œç¨‹åºç”Ÿæˆçš„æ¸¸æˆã€‚ç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡å­¦ä¹ æœ€å¤§åŒ–å…¶å†…åœ¨å¥–åŠ±ï¼ŒMotif å°±å–å¾—äº†æ¯”ç›´æ¥è®­ç»ƒä»¥æœ€å¤§åŒ–åˆ†æ•°çš„ç®—æ³•æ›´é«˜çš„æ¸¸æˆåˆ†æ•°ã€‚å½“å°† Motif çš„å†…åœ¨å¥–åŠ±ä¸ç¯å¢ƒçš„å¥–åŠ±ç›¸ç»“åˆæ—¶ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ²¡æœ‰æ¼”ç¤ºçš„æƒ…å†µä¸‹å–å¾—äº†è¿›å±•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Motif ä¸ºåˆ©ç”¨ LLM çš„å…ˆéªŒçŸ¥è¯†å’Œå¸¸è¯†æ¥åˆ›å»ºæ™ºèƒ½ä½“æä¾›äº†ä¸€ç§é€šç”¨çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å°† LLM çš„é«˜å±‚æ¬¡çŸ¥è¯†ä¸æ™ºèƒ½ä½“æ“ä½œçš„åº•å±‚ä¼ æ„Ÿå™¨è¿åŠ¨ç°å®ä¹‹é—´çš„å·®è·ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†çŸ¥è¯†æç‚¼å‡ºæ¥ã€‚Motif å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ä¸æ›´å¤§è§„æ¨¡çš„ LLM æˆ–ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒ LLM ç»“åˆä½¿ç”¨ï¼Œå¹¶å¯ä»¥é€šè¿‡æç¤ºä¿®æ”¹è½»æ¾åœ°å¼•å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚</td>
    </tr>
    <tr>
      <th>101</th>
      <td>SPRING: Studying the Paper and Reasoning to Play Games</td>
      <td>Open-world survival games pose significant challenges for AI algorithms due<br>to their multi-tasking, deep exploration, and goal prioritization requirements.<br>Despite reinforcement learning (RL) being popular for solving games, its high<br>sample complexity limits its effectiveness in complex open-world games like<br>Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's<br>original academic paper and use the knowledge learned to reason and play the<br>game through a large language model (LLM). Prompted with the LaTeX source as<br>game context and a description of the agent's current observation, our SPRING<br>framework employs a directed acyclic graph (DAG) with game-related questions as<br>nodes and dependencies as edges. We identify the optimal action to take in the<br>environment by traversing the DAG and calculating LLM responses for each node<br>in topological order, with the LLM's answer to final node directly translating<br>to environment actions. In our experiments, we study the quality of in-context<br>"reasoning" induced by different forms of prompts under the setting of the<br>Crafter open-world environment. Our experiments suggest that LLMs, when<br>prompted with consistent chain-of-thought, have great potential in completing<br>sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4<br>outperforms all state-of-the-art RL baselines, trained for 1M steps, without<br>any training. Finally, we show the potential of games as a test bed for LLMs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SPRINGï¼šé€šè¿‡é˜…è¯»è®ºæ–‡å’Œæ¨ç†æ¥ç©æ¸¸æˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼€æ”¾ä¸–ç•Œç”Ÿå­˜æ¸¸æˆå¯¹AIç®—æ³•æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤šä»»åŠ¡å¤„ç†ã€æ·±åº¦æ¢ç´¢å’Œç›®æ ‡ä¼˜å…ˆçº§æ’åºã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§£å†³æ¸¸æˆé—®é¢˜æ–¹é¢å¾ˆå—æ¬¢è¿ï¼Œä½†å…¶é«˜æ ·æœ¬å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨åƒCrafteræˆ–Minecraftè¿™æ ·çš„å¤æ‚å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>SPRINGæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡é˜…è¯»æ¸¸æˆçš„åŸå§‹å­¦æœ¯è®ºæ–‡å¹¶ä½¿ç”¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å­¦ä¹ åˆ°çš„çŸ¥è¯†æ¥æ¨ç†å’Œç©æ¸¸æˆã€‚SPRINGæ¡†æ¶ä½¿ç”¨ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œå…¶ä¸­æ¸¸æˆç›¸å…³é—®é¢˜ä½œä¸ºèŠ‚ç‚¹ï¼Œä¾èµ–å…³ç³»ä½œä¸ºè¾¹ã€‚é€šè¿‡éå†DAGå¹¶æŒ‰æ‹“æ‰‘é¡ºåºè®¡ç®—LLMå¯¹æ¯ä¸ªèŠ‚ç‚¹çš„å“åº”ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šåœ¨ç¯å¢ƒä¸­é‡‡å–çš„æœ€ä½³è¡ŒåŠ¨ï¼ŒLLMå¯¹æœ€ç»ˆèŠ‚ç‚¹çš„ç­”æ¡ˆç›´æ¥è½¬æ¢ä¸ºç¯å¢ƒè¡ŒåŠ¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Crafterå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬çš„å®éªŒç ”ç©¶äº†ä¸åŒå½¢å¼çš„æç¤ºåœ¨ä¸Šä¸‹æ–‡ä¸­å¼•èµ·çš„â€œæ¨ç†â€è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“LLMè¢«æç¤ºè¿›è¡Œä¸€è‡´çš„æ€ç»´é“¾æ—¶ï¼Œå®ƒä»¬åœ¨å®Œæˆå¤æ‚çš„é«˜çº§è½¨è¿¹æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚å®šé‡åœ°è¯´ï¼ŒSPRINGä¸GPT-4çš„è¡¨ç°ä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„RLåŸºçº¿ï¼Œè¿™äº›åŸºçº¿ç»è¿‡1Mæ­¥çš„è®­ç»ƒï¼Œè€ŒSPRINGæ²¡æœ‰ç»è¿‡ä»»ä½•è®­ç»ƒã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SPRINGæ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡ä»å­¦æœ¯è®ºæ–‡ä¸­æ˜ç¡®æå–å¤šä¸ªäº¤äº’å’Œç§‘æŠ€æ ‘ä¾èµ–æ¥åº”å¯¹ç«äº‰æ€§RLåŸºå‡†çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­å±•ç¤ºSOTAæ€§èƒ½çš„é›¶æ ·æœ¬LLM-basedï¼ˆGPT-4ï¼‰ç­–ç•¥ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸åŒæç¤ºå¼•èµ·çš„ä¸Šä¸‹æ–‡â€œæ¨ç†â€è´¨é‡ï¼Œå¹¶é€šè¿‡DAGä¸­çš„é—®é¢˜é“¾æå‡ºäº†ä¸€ç§å—æ§çš„æ€ç»´é“¾æç¤ºï¼Œç”¨äºå†³ç­–ã€‚</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Embodied LLM Agents Learn to Cooperate in Organized Teams</td>
      <td>Large Language Models (LLMs) have emerged as integral tools for reasoning,<br>planning, and decision-making, drawing upon their extensive world knowledge and<br>proficiency in language-related tasks. LLMs thus hold tremendous potential for<br>natural language interaction within multi-agent systems to foster cooperation.<br>However, LLM agents tend to over-report and comply with any instruction, which<br>may result in information redundancy and confusion in multi-agent cooperation.<br>Inspired by human organizations, this paper introduces a framework that imposes<br>prompt-based organization structures on LLM agents to mitigate these problems.<br>Through a series of experiments with embodied LLM agents and human-agent<br>collaboration, our results highlight the impact of designated leadership on<br>team efficiency, shedding light on the leadership qualities displayed by LLM<br>agents and their spontaneous cooperative behaviors. Further, we harness the<br>potential of LLMs to propose enhanced organizational prompts, via a<br>Criticize-Reflect process, resulting in novel organization structures that<br>reduce communication costs and enhance team efficiency.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨ç»„ç»‡åŒ–å›¢é˜Ÿä¸­å­¦ä¹ åä½œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ã€è§„åˆ’å’Œå†³ç­–æ–¹é¢çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åä½œæ½œåŠ›ä¹Ÿé€æ¸æ˜¾ç°ã€‚ç„¶è€Œï¼ŒLLMæ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­å­˜åœ¨è¿‡åº¦æŠ¥å‘Šå’Œç›²ç›®æœä»æŒ‡ä»¤çš„é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¿¡æ¯å†—ä½™å’Œæ··ä¹±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„ç»„ç»‡ç»“æ„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMæ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åä½œæ•ˆç‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡äº†ä¸€ç§æ–°å‹çš„å¤šLLMæ™ºèƒ½ä½“æ¶æ„ï¼Œæ”¯æŒâ‰¥3ä¸ªæ™ºèƒ½ä½“åœ¨ç‰©ç†/æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œçµæ´»çš„é€šä¿¡å’Œåä½œã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘äº†ä¸€ç§åŸºäºLLMsçš„â€œæ‰¹è¯„-åæ€â€æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ä¼˜åŒ–ç»„ç»‡æç¤ºï¼Œä»è€Œç”Ÿæˆæ›´æœ‰æ•ˆçš„ç»„ç»‡ç»“æ„ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰æŒ‡å®šé¢†å¯¼è€…çš„å±‚æ¬¡åŒ–ç»„ç»‡ç»“æ„èƒ½å¤Ÿæ˜¾è‘—æé«˜å›¢é˜Ÿæ•ˆç‡ï¼Œå¹¶ä¸”LLMæ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡é€šä¿¡åŠ¨æ€åœ°é€‰ä¸¾å’Œè°ƒæ•´é¢†å¯¼è€…ã€‚æ­¤å¤–ï¼Œé€šè¿‡â€œæ‰¹è¯„-åæ€â€æ¡†æ¶ï¼ŒLLMæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªå‘åœ°å½¢æˆæ–°é¢–ã€æœ‰æ•ˆçš„å›¢é˜Ÿç»“æ„ï¼Œä»è€Œé™ä½é€šä¿¡æˆæœ¬å¹¶æé«˜å›¢é˜Ÿæ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºæç¤ºçš„ç»„ç»‡ç»“æ„æ¡†æ¶å’Œâ€œæ‰¹è¯„-åæ€â€æ¡†æ¶ä¸ºLLMæ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åä½œæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ç½‘ç»œã€æ— äººæœºç¾¤ç­‰ï¼Œä»¥æé«˜æ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæ•ˆç‡å’Œæ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>28</th>
      <td>SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models</td>
      <td>Large language models (LLMs) have recently garnered significant<br>accomplishments in various exploratory tasks, even surpassing the performance<br>of traditional reinforcement learning-based methods that have historically<br>dominated the agent-based field. The purpose of this paper is to investigate<br>the efficacy of LLMs in executing real-time strategy war tasks within the<br>StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an<br>embodied agent leveraging LLM for real-time strategy implementation in the<br>StarCraft II game environment. The SwarmBrain comprises two key components: 1)<br>a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed<br>to orchestrate macro-level strategies from a high-level perspective. This<br>matrix emulates the overarching consciousness of the Zerg intelligence brain,<br>synthesizing strategic foresight with the aim of allocating resources,<br>directing expansion, and coordinating multi-pronged assaults. 2) a Swarm<br>ReflexNet, which is agile counterpart to the calculated deliberation of the<br>Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the<br>Swarm ReflexNet employs a condition-response state machine framework, enabling<br>expedited tactical responses for fundamental Zerg unit maneuvers. In the<br>experimental setup, SwarmBrain is in control of the Zerg race in confrontation<br>with an Computer-controlled Terran adversary. Experimental results show the<br>capacity of SwarmBrain to conduct economic augmentation, territorial expansion,<br>and tactical formulation, and it shows the SwarmBrain is capable of achieving<br>victory against Computer players set at different difficulty levels.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SwarmBrainï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®æ—¶ç­–ç•¥æ¸¸æˆStarCraft IIçš„æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å®æ—¶ç­–ç•¥ï¼ˆRTSï¼‰æ¸¸æˆï¼Œå¦‚ã€Šæ˜Ÿé™…äº‰éœ¸IIã€‹ï¼Œå› å…¶å¤æ‚çš„æˆ˜åœºç¯å¢ƒå’Œå¿«é€Ÿå†³ç­–çš„éœ€æ±‚ï¼Œå¯¹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•åœ¨å¤„ç†è¿™ç§å¤æ‚ç¯å¢ƒæ—¶é‡åˆ°äº†å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å°†é«˜çº§ç›®æ ‡ç›´æ¥æ˜ å°„åˆ°ä½çº§é”®ç›˜å’Œé¼ æ ‡è¾“å…¥æ–¹é¢ã€‚è€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶å¯¹å¤æ‚è¯­å¢ƒçš„é«˜å±‚æ¬¡æŠ½è±¡å’Œç†è§£èƒ½åŠ›ï¼Œåœ¨æ¢ç´¢æ€§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†åœ¨å®æ—¶ç­–ç•¥æ¸¸æˆä¸­ï¼ŒLLMçš„æ¨ç†å»¶è¿Ÿé™åˆ¶äº†å…¶ç›´æ¥åº”ç”¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†SwarmBrainï¼Œä¸€ä¸ªåŸºäºLLMçš„æ™ºèƒ½ä½“ï¼Œç”¨äºåœ¨ã€Šæ˜Ÿé™…äº‰éœ¸IIã€‹ä¸­æ‰§è¡Œå®æ—¶ç­–ç•¥ä»»åŠ¡ã€‚SwarmBrainç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šOvermind Intelligence Matrix<br>è¿™æ˜¯ä¸€ä¸ªç”±æœ€å…ˆè¿›çš„LLMé©±åŠ¨çš„çŸ©é˜µï¼Œè´Ÿè´£ä»é«˜å±‚æ¬¡è§†è§’åè°ƒå®è§‚ç­–ç•¥ã€‚å®ƒæ¨¡æ‹Ÿäº†Zergæ™ºèƒ½è„‘çš„æ€»ä½“æ„è¯†ï¼Œç»“åˆæˆ˜ç•¥è¿œè§ï¼Œæ—¨åœ¨åˆ†é…èµ„æºã€æŒ‡å¯¼æ‰©å¼ å’Œåè°ƒå¤šæ–¹é¢çš„æ”»å‡»ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSwarm ReflexNet<br>è¿™æ˜¯Overmind Intelligence Matrixçš„æ•æ·å¯¹åº”ç‰©ï¼Œé‡‡ç”¨æ¡ä»¶å“åº”çŠ¶æ€æœºæ¡†æ¶ï¼Œä¸ºåŸºæœ¬çš„Zergå•ä½æ“ä½œæä¾›å¿«é€Ÿæˆ˜æœ¯å“åº”ã€‚ç”±äºLLMæ¨ç†çš„å›ºæœ‰å»¶è¿Ÿï¼ŒSwarm ReflexNetèƒ½å¤Ÿå¿«é€Ÿå“åº”ï¼Œè€Œæ— éœ€ç­‰å¾…LLMçš„æ·±å…¥æ€è€ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>SwarmBrainåœ¨ä¸ä¸åŒéš¾åº¦çº§åˆ«çš„è®¡ç®—æœºå¯¹æ‰‹çš„å¯¹æŠ—ä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨éå¸¸å®¹æ˜“ã€å®¹æ˜“ã€ä¸­ç­‰å’Œä¸­ç­‰å›°éš¾çº§åˆ«ï¼ŒSwarmBrainçš„æˆåŠŸç‡ä¸º100%ã€‚å³ä½¿åœ¨å›°éš¾çº§åˆ«ï¼ŒSwarmBrainä¹Ÿèƒ½åœ¨76%çš„æ¯”èµ›ä¸­å–å¾—èƒœåˆ©ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SwarmBrainçš„è®¾è®¡å±•ç¤ºäº†LLMåœ¨å®æ—¶ç­–ç•¥æ¸¸æˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ç»“åˆå®è§‚ç­–ç•¥è§„åˆ’å’Œå¿«é€Ÿæˆ˜æœ¯å“åº”ï¼ŒSwarmBrainèƒ½å¤Ÿåœ¨å¤æ‚çš„æ¸¸æˆç¯å¢ƒä¸­å–å¾—æˆåŠŸã€‚æ­¤å¤–ï¼ŒSwarmBrainçš„è®¾è®¡ä¹Ÿä¸ºæˆ‘ä»¬æä¾›äº†å…³äºå¦‚ä½•å°†LLMåº”ç”¨äºå…¶ä»–éœ€è¦å¿«é€Ÿå†³ç­–çš„é¢†åŸŸçš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>49</th>
      <td>ADaPT: As-Needed Decomposition and Planning with Language Models</td>
      <td>Large Language Models (LLMs) are increasingly being used for interactive<br>decision-making tasks requiring planning and adapting to the environment.<br>Recent works employ LLMs-as-agents in broadly two ways: iteratively determining<br>the next action (iterative executors) or generating plans and executing<br>sub-tasks using LLMs (plan-and-execute). However, these methods struggle with<br>task complexity, as the inability to execute any sub-task may lead to task<br>failure. To address these shortcomings, we introduce As-Needed Decomposition<br>and Planning for complex Tasks (ADaPT), an approach that explicitly plans and<br>decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute<br>them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity<br>and LLM capability. Our results demonstrate that ADaPT substantially<br>outperforms established strong baselines, achieving success rates up to 28.3%<br>higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel<br>compositional dataset that we introduce. Through extensive analysis, we<br>illustrate the importance of multilevel decomposition and establish that ADaPT<br>dynamically adjusts to the capabilities of the executor LLM as well as to task<br>complexity.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ADaPTï¼šæŒ‰éœ€åˆ†è§£ä¸è§„åˆ’ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬ä¹Ÿé€æ¸è¢«åº”ç”¨äºéœ€è¦è§„åˆ’å’Œé€‚åº”ç¯å¢ƒçš„äº¤äº’å¼å†³ç­–ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºLLMsåœ¨æ‰§è¡Œå­ä»»åŠ¡æ—¶å¯èƒ½ä¼šå¤±è´¥ï¼Œä»è€Œå¯¼è‡´æ•´ä¸ªä»»åŠ¡çš„å¤±è´¥ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ADaPTï¼ˆAs-Needed Decomposition and Planning for complex Tasksï¼‰ï¼Œä¸€ç§æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’å¤æ‚ä»»åŠ¡çš„æ–¹æ³•ã€‚ADaPTçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå°†å…¶åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è¿›è¡Œåˆ†è§£ï¼Œä»¥é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæŒ‰éœ€åˆ†è§£<br>ADaPTé€šè¿‡é€’å½’åœ°åˆ†è§£å­ä»»åŠ¡ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚å½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå®ƒä¼šè°ƒç”¨LLMä½œä¸ºè§„åˆ’è€…æ¥ç”Ÿæˆæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è°ƒç”¨ADaPTæ¥æ‰§è¡Œè¿™äº›å­ä»»åŠ¡ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šçº§åˆ†è§£<br>ADaPTæ”¯æŒå¤šçº§åˆ†è§£ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥è¿›ä¸€æ­¥åˆ†è§£å­ä»»åŠ¡ï¼Œç›´åˆ°å®ƒä»¬å˜å¾—è¶³å¤Ÿç®€å•ï¼Œå¯ä»¥è¢«LLMä½œä¸ºæ‰§è¡Œè€…æˆåŠŸæ‰§è¡Œã€‚è¿™ç§å¤šçº§åˆ†è§£çš„èƒ½åŠ›ä½¿å¾—ADaPTèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ALFWorldã€WebShopå’ŒTextCraftä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒADaPTæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ–¹æ³•ï¼Œåœ¨ALFWorldä¸Šæé«˜äº†28.3%çš„æˆåŠŸç‡ï¼Œåœ¨WebShopä¸Šæé«˜äº†27%ï¼Œåœ¨TextCraftä¸Šæé«˜äº†33%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>ADaPTæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å¤„ç†LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ‰§è¡Œå¤±è´¥é—®é¢˜ã€‚å®ƒé€šè¿‡æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†ä»»åŠ¡çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒADaPTçš„å¤šçº§åˆ†è§£èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚</td>
    </tr>
    <tr>
      <th>7</th>
      <td>A Pilot Study on Teacher-Facing Real-Time Classroom Game Dashboards</td>
      <td>Educational games are an increasingly popular teaching tool in modern<br>classrooms. However, the development of complementary tools for teachers<br>facilitating classroom gameplay is lacking. We present the results of a<br>participatory design process for a teacher-facing, real-time game data<br>dashboard. This two-phase process included a workshop to elicit teachers'<br>requirements for such a tool, and a pilot study of our dashboard prototype. We<br>analyze post-gameplay survey and interview data to understand teachers'<br>experiences with the tool in terms of evidence of co-design, feasibility, and<br>effectiveness. Our results indicate the participatory design yielded a tool<br>both useful for and usable by teachers within the context of a real class<br>gameplay session. We advocate for the continued development of data-driven<br>teacher tools to improve the effectiveness of games deployed in the classroom.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ•™è‚²æ¸¸æˆä¸­çš„å®æ—¶è¯¾å ‚æ¸¸æˆä»ªè¡¨æ¿ï¼šæ•™å¸ˆè§†è§’çš„æ¢ç´¢<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æ•™è‚²æ¸¸æˆåœ¨ç°ä»£è¯¾å ‚ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ç¼ºä¹è¾…åŠ©æ•™å¸ˆè¿›è¡Œè¯¾å ‚æ¸¸æˆçš„æ•™å­¦å·¥å…·ã€‚æ•™å¸ˆé€šå¸¸åªèƒ½é€šè¿‡æŸ¥çœ‹æ¯ä¸ªå­¦ç”Ÿçš„ç”µè„‘å±å¹•æ¥äº†è§£ä»–ä»¬çš„æ¸¸æˆæƒ…å†µï¼Œè¿™ä¸ä»…è€—æ—¶ï¼Œè€Œä¸”å¯èƒ½æ— æ³•å…¨é¢äº†è§£æ¸¸æˆç»†èŠ‚ï¼Œä»è€Œå½±å“æ•™å­¦æ•ˆæœã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§å‚ä¸å¼è®¾è®¡æ–¹æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„ç ”ç©¶æ¥å¼€å‘ä¸€ä¸ªé¢å‘æ•™å¸ˆçš„å®æ—¶æ¸¸æˆæ•°æ®ä»ªè¡¨æ¿ï¼š<br>1. **å‚ä¸å¼è®¾è®¡å·¥ä½œåŠ**ï¼šä¸æ•™å¸ˆå’Œå…¶ä»–åˆ©ç›Šç›¸å…³è€…åˆä½œï¼Œé€šè¿‡è®¾è®¡æ´»åŠ¨æ”¶é›†ä»–ä»¬å¯¹ä»ªè¡¨æ¿çš„éœ€æ±‚å’ŒæœŸæœ›ã€‚<br>2. **åŸå‹å¼€å‘å’Œè¯•ç‚¹ç ”ç©¶**ï¼šæ ¹æ®å·¥ä½œåŠçš„ç»“æœå¼€å‘ä»ªè¡¨æ¿åŸå‹ï¼Œå¹¶åœ¨å¤šä¸ªè¯¾å ‚ç¯å¢ƒä¸­è¿›è¡Œè¯•ç‚¹æµ‹è¯•ï¼Œæ”¶é›†æ•™å¸ˆåé¦ˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>è¯•ç‚¹ç ”ç©¶è¡¨æ˜ï¼Œå‚ä¸å¼è®¾è®¡è¿‡ç¨‹äº§ç”Ÿäº†å¯¹æ•™å¸ˆæœ‰ç”¨ä¸”æ˜“äºä½¿ç”¨çš„å·¥å…·ã€‚æ•™å¸ˆåé¦ˆè¡¨æ˜ï¼Œä»ªè¡¨æ¿å¸®åŠ©ä»–ä»¬æ›´å¥½åœ°ç†è§£æ¸¸æˆå’Œå­¦ç”Ÿçš„æ¸¸æˆæƒ…å†µï¼Œæé«˜äº†ä»–ä»¬å¯¹æ¸¸æˆæ•™å­¦çš„æ”¯æŒèƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå‚ä¸å¼è®¾è®¡æ˜¯å¼€å‘æ•™å¸ˆæ”¯æŒå·¥å…·çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•ä»ªè¡¨æ¿çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç­çº§æ¦‚è¿°å’Œæ€»ç»“æ•°æ®ï¼Œä»¥æé«˜å…¶å¯ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥æ¢ç´¢å¦‚ä½•å°†ä»ªè¡¨æ¿ä¸å…¶ä»–æ•™è‚²æŠ€æœ¯å·¥å…·é›†æˆï¼Œä»¥æ›´å¥½åœ°æ”¯æŒæ¸¸æˆæ•™å­¦ã€‚</td>
    </tr>
    <tr>
      <th>69</th>
      <td>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</td>
      <td>The advent of Large Language Models (LLMs) has paved the way for complex<br>tasks such as role-playing, which enhances user interactions by enabling models<br>to imitate various characters. However, the closed-source nature of<br>state-of-the-art LLMs and their general-purpose training limit role-playing<br>optimization. In this paper, we introduce RoleLLM, a framework to benchmark,<br>elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four<br>stages: (1) Role Profile Construction for 100 roles; (2) Context-Based<br>Instruction Generation (Context-Instruct) for role-specific knowledge<br>extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style<br>imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning<br>open-source models along with role customization. By Context-Instruct and<br>RoleGPT, we create RoleBench, the first systematic and fine-grained<br>character-level benchmark dataset for role-playing with 168,093 samples.<br>Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),<br>significantly enhancing role-playing abilities and even achieving comparable<br>results with RoleGPT (using GPT-4).</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | RoleLLMï¼šè§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„è§’è‰²æ‰®æ¼”èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œè§’è‰²æ‰®æ¼”ç­‰å¤æ‚ä»»åŠ¡æˆä¸ºå¯èƒ½ï¼Œä¸ºç”¨æˆ·äº¤äº’æä¾›äº†æ›´å¤šå¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰å¼€æºLLMsåœ¨è§’è‰²æ‰®æ¼”æ–¹é¢å­˜åœ¨ä¼˜åŒ–ä¸è¶³çš„é—®é¢˜ï¼Œè€Œæœ€å…ˆè¿›çš„LLMså¦‚GPT-4ç­‰åˆ™å› å…¶é—­æºæ€§è´¨è€Œé™åˆ¶äº†å…¶åœ¨è§’è‰²æ‰®æ¼”æ–¹é¢çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºRoleLLMæ¡†æ¶ï¼Œä»¥æå‡LLMsçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šRole Profile Construction<br>æ„å»ºäº†100ä¸ªè§’è‰²çš„è¯¦ç»†æ¡£æ¡ˆï¼ŒåŒ…æ‹¬è§’è‰²æè¿°ã€å£å¤´ç¦…ä»¥åŠä»å‰§æœ¬ä¸­æå–çš„å¯¹è¯ç‰‡æ®µï¼Œä¸ºè§’è‰²æ‰®æ¼”æä¾›äº†ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šContext-Based Instruction Generation (Context-Instruct)<br>åˆ©ç”¨GPTæ¨¡å‹ä»è§’è‰²æ¡£æ¡ˆä¸­ç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œä»¥æå–è§’è‰²ç‰¹å®šçš„çŸ¥è¯†å’Œè®°å¿†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRole Prompting using GPT (RoleGPT)<br>é€šè¿‡å¯¹è¯å·¥ç¨‹å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œåˆ©ç”¨GPTæ¨¡å‹ç”Ÿæˆç¬¦åˆè§’è‰²è¯´è¯é£æ ¼çš„å›ç­”ï¼Œä»¥æ¨¡ä»¿è§’è‰²çš„è¯´è¯é£æ ¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šRole-Conditioned Instruction Tuning (RoCIT)<br>åˆ©ç”¨Context-Instructå’ŒRoleGPTç”Ÿæˆçš„æ•°æ®ï¼Œå¯¹å¼€æºLLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥æå‡å…¶è§’è‰²æ‰®æ¼”èƒ½åŠ›ï¼Œå¹¶ç”ŸæˆRoleLLaMAå’ŒRoleGLMç­‰æ¨¡å‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒRoleLLMæ¡†æ¶åœ¨è§’è‰²æ‰®æ¼”æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚RoleLLaMAå’ŒRoleGLMåœ¨æ¨¡ä»¿è§’è‰²è¯´è¯é£æ ¼ã€å›ç­”å‡†ç¡®æ€§å’Œè§’è‰²ç‰¹å®šçŸ¥è¯†æŒæ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¸RoleGPTï¼ˆä½¿ç”¨GPT-4ï¼‰ç›¸å½“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>RoleLLMæ¡†æ¶ä¸ºLLMsçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›æå‡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶åˆ›æ–°ç‚¹åŒ…æ‹¬è§’è‰²æ¡£æ¡ˆæ„å»ºã€åŸºäºä¸Šä¸‹æ–‡çš„æŒ‡ä»¤ç”Ÿæˆã€è§’è‰²æç¤ºå’Œè§’è‰²æ¡ä»¶æŒ‡ä»¤å¾®è°ƒç­‰ã€‚æ­¤å¤–ï¼ŒRoleBenchæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè§’è‰²æ‰®æ¼”èƒ½åŠ›çš„è¯„ä¼°å’Œæå‡æä¾›äº†é‡è¦çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>80</th>
      <td>Towards Ontology Construction with Language Models</td>
      <td>We present a method for automatically constructing a concept hierarchy for a<br>given domain by querying a large language model. We apply this method to<br>various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can<br>be of considerable help for constructing concept hierarchies.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºæœ¬ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>æœ¬ä½“æ˜¯é¢†åŸŸå†…æ¦‚å¿µåŠå…¶å…³ç³»çš„æ­£å¼è¡¨ç¤ºï¼Œæ˜¯é«˜åº¦ç»“æ„åŒ–çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ„å»ºå’Œç¼–è¾‘æœ¬ä½“æ˜¯ä¸€é¡¹è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„å·¥ç¨‹ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦é¢†åŸŸä¸“å®¶çš„å‚ä¸ï¼Œä½†é¢†åŸŸä¸“å®¶çš„çŸ¥è¯†å’Œæœ¬ä½“å·¥ç¨‹ä¸“ä¸šçŸ¥è¯†å¾€å¾€ä¸åœ¨åŒä¸€äººæ‰‹ä¸­ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸å‡è®¾æœ¬ä½“çš„æ¨¡å¼ï¼ˆå³è¦ä½¿ç”¨çš„æ¦‚å¿µå’Œå±æ€§åç§°çš„é›†åˆï¼‰æ˜¯é¢„å…ˆé€‰æ‹©çš„ï¼Œç„¶åä½œä¸ºæ–¹æ³•çš„è¾“å…¥æä¾›ã€‚ç„¶è€Œï¼Œå½“ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œè¿™ä¼¼ä¹ä¸æ˜¯ä¸€ä¸ªå¥½çš„é€‰æ‹©ï¼Œå› ä¸ºè‡³å°‘æœ‰ä¸¤ä¸ªåŸå› ã€‚é¦–å…ˆï¼Œä¸ºæ•´ä¸ªé¢†åŸŸè®¾è®¡æ¨¡å¼æœ¬èº«å°±æ˜¯ä¸€ä¸ªéå¹³å‡¡çš„ä»»åŠ¡ï¼Œéœ€è¦é¢†åŸŸä¸“å®¶å¹¶æ¶‰åŠè®¸å¤šè®¾è®¡å†³ç­–ã€‚å®é™…ä¸Šï¼Œè®¾è®¡æ¨¡å¼å’Œæ¦‚å¿µå±‚æ¬¡ç»“æ„æ˜¯ç´§å¯†ç›¸è¿çš„ã€‚å…¶æ¬¡ï¼ŒLLMçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºæ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå…³é”®è¯å’ŒçŸ­è¯­ï¼Œå› æ­¤å®ƒä»¬æ˜¯æå‡ºç»™å®šé¢†åŸŸæ¦‚å¿µå’Œå±æ€§åç§°çš„å®Œç¾å·¥å…·ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æœ¬ä½“æ„å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æœ¬ä½“æ„å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹æ¥è‡ªåŠ¨æ„å»ºç»™å®šé¢†åŸŸçš„æ¦‚å¿µå±‚æ¬¡ç»“æ„ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼ŒLLMéšå«åœ°åŒ…å«å¤§é‡çš„çŸ¥è¯†ï¼Œå¹¶ä¸”å¯ä»¥åƒä¸“å®¶ä¸€æ ·å›ç­”é—®é¢˜ï¼Œä»è€Œå¸®åŠ©æ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š<br><br>1. **ç§å­æ¦‚å¿µé€‰æ‹©**ï¼šé€‰æ‹©ä¸€ä¸ªç§å­æ¦‚å¿µï¼Œä¾‹å¦‚â€œåŠ¨ç‰©â€ï¼Œä½œä¸ºæ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„çš„èµ·ç‚¹ã€‚<br>2. **æ¦‚å¿µå±‚æ¬¡ç»“æ„æ¢ç´¢**ï¼šé€šè¿‡é‡å¤è¯¢é—®LLMæä¾›å·²ç»å­˜åœ¨äºå±‚æ¬¡ç»“æ„ä¸­çš„æ¦‚å¿µçš„ç›¸å…³å­æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨éå†ç®—æ³•å°†æ–°æ¦‚å¿µæ”¾ç½®åœ¨å±‚æ¬¡ç»“æ„ä¸­ã€‚<br>3. **æ¦‚å¿µæè¿°**ï¼šè¯¢é—®LLMæä¾›æ¯ä¸ªæ¦‚å¿µçš„æ–‡æœ¬æè¿°ï¼Œä»¥ä¾¿äºäººç±»ç”¨æˆ·ç†è§£å’Œè§£é‡ŠLLMæå‡ºçš„æ¦‚å¿µã€‚<br>4. **æ¦‚å¿µéªŒè¯**ï¼šé€šè¿‡å‘LLMæå‡ºé¢å¤–çš„æŸ¥è¯¢æ¥éªŒè¯LLMçš„è¾“å‡ºï¼Œä»¥è¿‡æ»¤æ‰é”™è¯¯çš„ç­”æ¡ˆã€‚<br>5. **æ¦‚å¿µæ’å…¥**ï¼šä½¿ç”¨KRISç®—æ³•å°†æ–°æ¦‚å¿µæ’å…¥åˆ°æ¦‚å¿µå±‚æ¬¡ç»“æ„ä¸­ï¼Œå¹¶å¤„ç†å­æ¦‚å¿µ/è¶…æ¦‚å¿µå…³ç³»å’ŒåŒä¹‰è¯æ£€æµ‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨OpenAIçš„GPT 3.5å¯¹å„ç§é¢†åŸŸï¼ˆå¦‚åŠ¨ç‰©ã€é¥®æ–™ã€éŸ³ä¹å’Œæ¤ç‰©ï¼‰è¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°å¸®åŠ©æ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„ï¼Œå°½ç®¡ä»ç„¶å­˜åœ¨ä¸€äº›å¹»è§‰å’Œé”™è¯¯ã€‚é€šè¿‡éªŒè¯å’Œä»”ç»†çš„æç¤ºå·¥ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘è¿™äº›é”™è¯¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†å¦‚ä½•å°†äººç±»é¢†åŸŸä¸“å®¶çš„äº¤äº’çº³å…¥æœ¬ä½“æ„å»ºè¿‡ç¨‹ï¼Œä»¥åŠå¦‚ä½•è¯„ä¼°æ„å»ºçš„æœ¬ä½“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºåˆ©ç”¨LLMæ„å»ºæœ¬ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>* **åˆ©ç”¨LLMçš„çŸ¥è¯†å’Œç”Ÿæˆèƒ½åŠ›**ï¼šLLMå¯ä»¥åƒä¸“å®¶ä¸€æ ·å›ç­”é—®é¢˜ï¼Œå¹¶æä¾›ç›¸å…³å­æ¦‚å¿µå’Œæ¦‚å¿µæè¿°ï¼Œä»è€Œå¸®åŠ©æ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„ã€‚<br>* **æ¦‚å¿µéªŒè¯å’Œæç¤ºå·¥ç¨‹**ï¼šé€šè¿‡éªŒè¯å’Œæç¤ºå·¥ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘LLMçš„å¹»è§‰å’Œé”™è¯¯ï¼Œæé«˜æ„å»ºæœ¬ä½“çš„è´¨é‡ã€‚<br>* **äººç±»é¢†åŸŸä¸“å®¶çš„äº¤äº’**ï¼šå°†äººç±»é¢†åŸŸä¸“å®¶çš„äº¤äº’çº³å…¥æœ¬ä½“æ„å»ºè¿‡ç¨‹ï¼Œå¯ä»¥å¸®åŠ©è§£å†³è®¾è®¡å†³ç­–å’Œå¼•å…¥â€œç¥ç§˜â€æ¦‚å¿µçš„é—®é¢˜ã€‚<br>* **æœ¬ä½“è¯„ä¼°**ï¼šä½¿ç”¨æ‰‹åŠ¨è¯„ä¼°æˆ–ç°æœ‰åˆ†ç±»æ³•è¿›è¡Œæœ¬ä½“è¯„ä¼°ï¼Œå¯ä»¥å¸®åŠ©è¯„ä¼°æ„å»ºæœ¬ä½“çš„è´¨é‡å’Œå®Œæ•´æ€§ã€‚<br><br>æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºåˆ©ç”¨LLMæ„å»ºæœ¬ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶å±•ç¤ºäº†LLMåœ¨æ„å»ºæœ¬ä½“æ–¹é¢çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>121</th>
      <td>Social Simulacra: Creating Populated Prototypes for Social Computing Systems</td>
      <td>Social computing prototypes probe the social behaviors that may arise in an<br>envisioned system design. This prototyping practice is currently limited to<br>recruiting small groups of people. Unfortunately, many challenges do not arise<br>until a system is populated at a larger scale. Can a designer understand how a<br>social system might behave when populated, and make adjustments to the design<br>before the system falls prey to such challenges? We introduce social simulacra,<br>a prototyping technique that generates a breadth of realistic social<br>interactions that may emerge when a social computing system is populated.<br>Social simulacra take as input the designer's description of a community's<br>design -- goal, rules, and member personas -- and produce as output an instance<br>of that design with simulated behavior, including posts, replies, and<br>anti-social behaviors. We demonstrate that social simulacra shift the behaviors<br>that they generate appropriately in response to design changes, and that they<br>enable exploration of "what if?" scenarios where community members or<br>moderators intervene. To power social simulacra, we contribute techniques for<br>prompting a large language model to generate thousands of distinct community<br>members and their social interactions with each other; these techniques are<br>enabled by the observation that large language models' training data already<br>includes a wide variety of positive and negative behavior on social media<br>platforms. In evaluations, we show that participants are often unable to<br>distinguish social simulacra from actual community behavior and that social<br>computing designers successfully refine their social computing designs when<br>using social simulacra.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ç¤¾äº¤æ¨¡æ‹Ÿï¼šä¸ºç¤¾äº¤è®¡ç®—ç³»ç»Ÿåˆ›å»ºäººå£åŸå‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç¤¾äº¤è®¡ç®—ç³»ç»Ÿçš„è®¾è®¡å¾€å¾€éœ€è¦è€ƒè™‘å¤§é‡ç”¨æˆ·çš„è¡Œä¸ºå’Œäº’åŠ¨ï¼Œè€Œä¼ ç»Ÿçš„åŸå‹è®¾è®¡æ–¹æ³•é€šå¸¸åªèƒ½æ‹›å‹Ÿå°‘é‡ç”¨æˆ·è¿›è¡Œæµ‹è¯•ã€‚ç„¶è€Œï¼Œè®¸å¤šæŒ‘æˆ˜å’Œé—®é¢˜åªæœ‰åœ¨ç³»ç»Ÿå¤§è§„æ¨¡è¿è¡Œæ—¶æ‰ä¼šå‡ºç°ã€‚å› æ­¤ï¼Œè®¾è®¡å¸ˆå¾ˆéš¾é¢„æµ‹ç¤¾äº¤ç³»ç»Ÿåœ¨äººå£ä¼—å¤šæ—¶çš„è¡Œä¸ºï¼Œä¹Ÿæ— æ³•åœ¨ç³»ç»Ÿå‡ºç°é—®é¢˜ä¹‹å‰è¿›è¡Œè°ƒæ•´ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œç¤¾äº¤æ¨¡æ‹Ÿâ€æŠ€æœ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤§é‡ç”¨æˆ·å’Œä»–ä»¬çš„ç¤¾äº¤äº’åŠ¨ï¼Œå¸®åŠ©è®¾è®¡å¸ˆé¢„æµ‹å’Œè¯„ä¼°ç¤¾äº¤ç³»ç»Ÿåœ¨äººå£ä¼—å¤šæ—¶çš„è¡Œä¸ºã€‚ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¤šæ ·åŒ–çš„ç”¨æˆ·è§’è‰²<br>è®¾è®¡å¸ˆåªéœ€æä¾›å°‘é‡ç§å­è§’è‰²ï¼Œç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯å°±å¯ä»¥ç”Ÿæˆå¤§é‡éé‡å¤ä½†ä¸»é¢˜ç›¸å…³çš„ç”¨æˆ·è§’è‰²ï¼Œä»è€Œæ¨¡æ‹Ÿä¸åŒç”¨æˆ·ä¹‹é—´çš„äº’åŠ¨ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç”Ÿæˆç¬¦åˆç¤¾åŒºç›®æ ‡å’Œè§„åˆ™çš„äº’åŠ¨å†…å®¹<br>ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯å¯ä»¥æ ¹æ®è®¾è®¡å¸ˆæä¾›çš„ç¤¾åŒºç›®æ ‡ã€è§„åˆ™å’Œç”¨æˆ·è§’è‰²ï¼Œç”Ÿæˆç¬¦åˆç¤¾åŒºç›®æ ‡å’Œè§„åˆ™çš„äº’åŠ¨å†…å®¹ï¼ŒåŒ…æ‹¬å¸–å­ã€å›å¤å’Œåç¤¾ä¼šè¡Œä¸ºã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯<br>ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯å¯ä»¥å¸®åŠ©è®¾è®¡å¸ˆæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯ï¼Œä¾‹å¦‚ï¼Œå¦‚æœç¤¾åŒºæˆå‘˜æˆ–ç‰ˆä¸»è¿›è¡Œå¹²é¢„ï¼Œç¤¾äº¤ç³»ç»Ÿä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡ä¸¤ä¸ªè¯„ä¼°å®éªŒéªŒè¯äº†ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼š<br><br>1. æŠ€æœ¯è¯„ä¼°ï¼šå‚ä¸è€…æ— æ³•åŒºåˆ†çœŸå®ç¤¾åŒºè¡Œä¸ºå’Œç¤¾äº¤æ¨¡æ‹Ÿç”Ÿæˆçš„è¡Œä¸ºï¼Œè¡¨æ˜ç¤¾äº¤æ¨¡æ‹Ÿå¯ä»¥åˆ›å»ºå¯ä¿¡çš„å†…å®¹ã€‚<br>2. è®¾è®¡å¸ˆè¯„ä¼°ï¼šç¤¾äº¤è®¡ç®—è®¾è®¡å¸ˆä½¿ç”¨ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯æˆåŠŸæ”¹è¿›äº†ä»–ä»¬çš„ç¤¾äº¤ç³»ç»Ÿè®¾è®¡ï¼Œä¾‹å¦‚ï¼Œè¯†åˆ«æœªè€ƒè™‘åˆ°çš„ç§¯æç”¨ä¾‹å’Œè´Ÿé¢è¡Œä¸ºï¼Œå¹¶è°ƒæ•´è§„åˆ™å’Œå¹²é¢„ç­–ç•¥ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯ä¸ºç¤¾äº¤è®¡ç®—ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†ä¸€ç§æ–°çš„åŸå‹è®¾è®¡æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©è®¾è®¡å¸ˆæ›´å¥½åœ°é¢„æµ‹å’Œè¯„ä¼°ç¤¾äº¤ç³»ç»Ÿåœ¨äººå£ä¼—å¤šæ—¶çš„è¡Œä¸ºï¼Œä»è€Œè®¾è®¡å‡ºæ›´å®‰å…¨ã€æ›´æœ‰æ•ˆçš„ç¤¾äº¤ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯è¿˜å¯ä»¥ç”¨äºæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯ï¼Œå¸®åŠ©è®¾è®¡å¸ˆæ›´å¥½åœ°ç†è§£ç¤¾äº¤ç³»ç»Ÿçš„åŠ¨æ€å˜åŒ–ã€‚</td>
    </tr>
    <tr>
      <th>45</th>
      <td>See and Think: Embodied Agent in Virtual Environment</td>
      <td>Large language models (LLMs) have achieved impressive pro-gress on several<br>open-world tasks. Recently, using LLMs to build embodied agents has been a<br>hotspot. This paper proposes STEVE, a comprehensive and visionary embodied<br>agent in the Minecraft virtual environment. STEVE comprises three key<br>components: vision perception, language instruction, and code action. Vision<br>perception involves interpreting visual information in the environment, which<br>is then integrated into the LLMs component with agent state and task<br>instruction. Language instruction is responsible for iterative reasoning and<br>decomposing complex tasks into manageable guidelines. Code action generates<br>executable skill actions based on retrieval in skill database, enabling the<br>agent to interact effectively within the Minecraft environment. We also collect<br>STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge<br>question-answering pairs, and 200+ skill-code pairs. We conduct continuous<br>block search, knowledge question and answering, and tech tree mastery to<br>evaluate the performance. Extensive experiments show that STEVE achieves at<br>most 1.5x faster unlocking key tech trees and 2.5x quicker in block search<br>tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | See and Think: Embodied Agent in Virtual Environment<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œè®¾è®¡èƒ½å¤Ÿè¡¨ç°å‡ºæ™ºèƒ½è¡Œä¸ºå’Œé€‚åº”æ€§çš„æ™ºèƒ½ä½“ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé•¿æœŸè€Œé‡è¦çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼€å‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç°å‡ºå…¶ä½œä¸ºå¤šåŠŸèƒ½ã€é€šç”¨å‹åŠ©æ‰‹çš„æ½œåŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œåœ¨è®¸å¤šå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¦‚Minecraftï¼Œå½“ä»£æ™ºèƒ½ä½“ä¸»è¦ä½¿ç”¨LLMsè¿›è¡Œæ–‡æœ¬äº¤äº’ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹æ–‡æœ¬é€šä¿¡çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨è¿™äº›ä¸–ç•Œä¸­çš„äº¤äº’ï¼ŒåŒ…æ‹¬ä½çº§æ¡ˆä¾‹ã€‚Minecraftè¦æ±‚æ™ºèƒ½ä½“å…·å¤‡å„ç§æŠ€èƒ½ï¼Œä»åˆ¶ä½œåŸºæœ¬ç‰©å“åˆ°æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±LLMsé©±åŠ¨çš„æ™ºèƒ½ä½“å¾€å¾€äº§ç”Ÿä¸å¯é¢„æµ‹çš„è¾“å‡ºã€‚å®ƒä»¬äº¤äº’çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ—¨åœ¨å°†LLMçš„ç†è§£ä¸ç¯å¢ƒçš„ä¸Šä¸‹æ–‡å’Œé¢„æœŸç›®æ ‡ç›¸ä¸€è‡´ã€‚è¿™ç§æç¤ºå·¥ç¨‹è¿‡ç¨‹ä¸ä»…è´¹åŠ›ï¼Œè€Œä¸”æ— æ³•å®ç°åŸ¹å…»è‡ªä¸»ã€è‡ªæˆ‘é©±åŠ¨çš„æ™ºèƒ½ä½“çš„ç›®æ ‡ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬é€šä¿¡åœ¨è‡ªç„¶ä¼ è¾¾æŸäº›ä¸–ç•Œæ¦‚å¿µæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚åˆ¶ä½œé…æ–¹ï¼Œè¿™äº›æ¦‚å¿µé€šå¸¸é€šè¿‡è§†è§‰æ›´æœ‰æ•ˆåœ°ä¼ è¾¾ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSTEVEï¼Œä¸€ä¸ªåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å…·æœ‰è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€æŒ‡ä»¤å’Œä»£ç åŠ¨ä½œçš„æ™ºèƒ½ä½“ï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨è§£é”å…³é”®æŠ€æœ¯æ ‘æ–¹é¢æœ€å¤šå¿«1.5å€ï¼Œåœ¨å—æœç´¢ä»»åŠ¡ä¸­æœ€å¤šå¿«2.3å€ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSTEVE-7B/13Bï¼Œä¸€ç³»åˆ—é€šè¿‡ä½¿ç”¨Llama-2-7B/13Bçš„MinecraftçŸ¥è¯†é—®ç­”å¯¹è¿›è¡Œå¾®è°ƒè·å¾—çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¶é›†STEVE-21Kæ•°æ®é›†ï¼ŒåŒ…æ‹¬600å¤šä¸ªè§†è§‰-ç¯å¢ƒå¯¹ã€20Kä¸ªçŸ¥è¯†é—®ç­”å¯¹å’Œ200å¤šä¸ªæŠ€èƒ½-ä»£ç å¯¹ï¼Œä»¥è¯æ˜STEVEçš„æœ‰æ•ˆæ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEVEåœ¨è¿ç»­å—æœç´¢ã€çŸ¥è¯†é—®ç­”å’Œç§‘æŠ€æ ‘æŒæ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸AutoGPTå’ŒVoyagerç­‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒSTEVEåœ¨è§£é”å…³é”®æŠ€æœ¯æ ‘æ–¹é¢æœ€å¤šå¿«1.5å€ï¼Œåœ¨å—æœç´¢ä»»åŠ¡ä¸­æœ€å¤šå¿«2.3å€ã€‚æ­¤å¤–ï¼ŒSTEVEåœ¨çŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¼˜äºLlama-2å’ŒGPT-4ç­‰æ›´å¹¿æ³›çš„æ¨¡å‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„STEVEæ¡†æ¶ä¸ºæ„å»ºå…·æœ‰è§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æŒ‡ä»¤èƒ½åŠ›çš„æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼ŒSTEVE-21Kæ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†è¿›è¡Œå¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶çš„æœ‰ç”¨èµ„æºã€‚</td>
    </tr>
    <tr>
      <th>50</th>
      <td>Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</td>
      <td>The automatic evaluation of LLM-based agent intelligence is critical in<br>developing advanced LLM-based agents. Although considerable effort has been<br>devoted to developing human-annotated evaluation datasets, such as AlpacaEval,<br>existing techniques are costly, time-consuming, and lack adaptability. In this<br>paper, inspired by the popular language game ``Who is Spy'', we propose to use<br>the word guessing game to assess the intelligence performance of LLMs. Given a<br>word, the LLM is asked to describe the word and determine its identity (spy or<br>not) based on its and other players' descriptions. Ideally, an advanced agent<br>should possess the ability to accurately describe a given word using an<br>aggressive description while concurrently maximizing confusion in the<br>conservative description, enhancing its participation in the game. To this end,<br>we first develop DEEP to evaluate LLMs' expression and disguising abilities.<br>DEEP requires LLM to describe a word in aggressive and conservative modes. We<br>then introduce SpyGame, an interactive multi-agent framework designed to assess<br>LLMs' intelligence through participation in a competitive language-based board<br>game. Incorporating multi-agent interaction, SpyGame requires the target LLM to<br>possess linguistic skills and strategic thinking, providing a more<br>comprehensive evaluation of LLMs' human-like cognitive abilities and<br>adaptability in complex communication situations. The proposed evaluation<br>framework is very easy to implement. We collected words from multiple sources,<br>domains, and languages and used the proposed evaluation framework to conduct<br>experiments. Extensive experiments demonstrate that the proposed DEEP and<br>SpyGame effectively evaluate the capabilities of various LLMs, capturing their<br>ability to adapt to novel situations and engage in strategic communication.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨çŒœè¯æ¸¸æˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTã€GPT-4å’ŒBardç­‰åœ¨å„ä¸ªä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººçš„æ€§èƒ½ï¼Œå¼€å‘åŸºäºLLMsçš„æ™ºèƒ½ä»£ç†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°LLMsæ™ºèƒ½çš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š1ï¼‰äººå·¥æ ‡æ³¨æˆæœ¬é«˜ï¼Œè€—æ—¶ä¸”ç¼ºä¹å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼›2ï¼‰æ— æ³•å…¨é¢åæ˜ LLMsçš„æ™ºèƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œå³åˆ©ç”¨çŒœè¯æ¸¸æˆæ¥è¯„ä¼°LLMsçš„æ™ºèƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDEEPæ¡†æ¶<br>æœ¬æ–‡é¦–å…ˆæå‡ºäº†DEEPæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsçš„è¡¨è¾¾å’Œä¼ªè£…èƒ½åŠ›ã€‚DEEPè¦æ±‚LLMsä»¥æ¿€è¿›å’Œä¿å®ˆä¸¤ç§æ¨¡å¼æè¿°ä¸€ä¸ªç»™å®šçš„è¯ï¼Œå¹¶åˆ©ç”¨GPT-4æ¥åˆ¤æ–­è¿™äº›æè¿°æ˜¯å¦å‡†ç¡®ã€‚æ¿€è¿›æ¨¡å¼è¦æ±‚LLMsæä¾›æ¸…æ™°ã€è¯¦ç»†å’Œå‡†ç¡®çš„æè¿°ï¼Œè€Œä¿å®ˆæ¨¡å¼åˆ™è¦æ±‚LLMsæä¾›æ¨¡ç³Šçš„æè¿°ä»¥ä¼ªè£…ç›®æ ‡è¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSpyGameæ¡†æ¶<br>æœ¬æ–‡è¿˜æå‡ºäº†SpyGameæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªäº¤äº’å¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‚ä¸ç«äº‰æ€§è¯­è¨€æ¸¸æˆâ€œè°æ˜¯å§åº•â€æ¥è¯„ä¼°LLMsçš„æ™ºèƒ½ã€‚SpyGameè¦æ±‚LLMså…·å¤‡è¯­è¨€æŠ€èƒ½å’Œæˆ˜ç•¥æ€ç»´èƒ½åŠ›ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°LLMsåœ¨å¤æ‚æ²Ÿé€šæƒ…å¢ƒä¸­çš„äººç±»è®¤çŸ¥èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¯¹å››ç§å¼€æºLLMså’Œä¸¤ç§é—­æºLLMsè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œé—­æºLLMsï¼ˆå¦‚GPT-4å’ŒGPT-3.5ï¼‰åœ¨æ¿€è¿›å’Œä¿å®ˆæ¨¡å¼ä¸‹çš„è¡¨ç°æ˜æ˜¾ä¼˜äºå¼€æºæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSpyGameæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°LLMsåœ¨å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„èƒ½åŠ›ï¼Œæ•æ‰å®ƒä»¬é€‚åº”æ–°æƒ…å†µå¹¶è¿›è¡Œæˆ˜ç•¥æ²Ÿé€šçš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„DEEPå’ŒSpyGameæ¡†æ¶ä¸ºè¯„ä¼°LLMsçš„æ™ºèƒ½æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br>1. åˆ©ç”¨æ¸¸æˆè¿›è¡Œè¯„ä¼°ï¼Œæ›´å…·äº’åŠ¨æ€§å’Œè¶£å‘³æ€§ã€‚<br>2. å…³æ³¨LLMsçš„è¡¨è¾¾å’Œä¼ªè£…èƒ½åŠ›ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°å…¶æ™ºèƒ½ã€‚<br>3. SpyGameæ¡†æ¶æ”¯æŒäººç±»å‚ä¸ï¼Œæ›´è´´è¿‘çœŸå®åœºæ™¯ã€‚<br>4. é’ˆå¯¹å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„åå·®é—®é¢˜ï¼Œæå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚<br><br>æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡æå‡ºçš„è¯„ä¼°æ–¹æ³•ä¸ºLLMsçš„æ™ºèƒ½è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæ¨åŠ¨LLMsçš„å‘å±•å’Œåº”ç”¨ã€‚</td>
    </tr>
    <tr>
      <th>122</th>
      <td>Inner Monologue: Embodied Reasoning through Planning with Language Models</td>
      <td>Recent works have shown how the reasoning capabilities of Large Language<br>Models (LLMs) can be applied to domains beyond natural language processing,<br>such as planning and interaction for robots. These embodied problems require an<br>agent to understand many semantic aspects of the world: the repertoire of<br>skills available, how these skills influence the world, and how changes to the<br>world map back to the language. LLMs planning in embodied environments need to<br>consider not just what skills to do, but also how and when to do them - answers<br>that change over time in response to the agent's own choices. In this work, we<br>investigate to what extent LLMs used in such embodied contexts can reason over<br>sources of feedback provided through natural language, without any additional<br>training. We propose that by leveraging environment feedback, LLMs are able to<br>form an inner monologue that allows them to more richly process and plan in<br>robotic control scenarios. We investigate a variety of sources of feedback,<br>such as success detection, scene description, and human interaction. We find<br>that closed-loop language feedback significantly improves high-level<br>instruction completion on three domains, including simulated and real table top<br>rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen<br>environment in the real world.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å†…å¿ƒç‹¬ç™½ï¼šé€šè¿‡è¯­è¨€æ¨¡å‹è§„åˆ’å®ç°å…·èº«æ¨ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæœºå™¨äººéœ€è¦å…·å¤‡æ›´é«˜çº§çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥ä¾¿åœ¨å¤æ‚çš„ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡ã€‚ä¼ ç»Ÿçš„æœºå™¨äººè§„åˆ’æ–¹æ³•å¾€å¾€ä¾èµ–äºä¼˜åŒ–æˆ–ç¬¦å·æ¨ç†ï¼Œç¼ºä¹å¯¹ä¸–ç•Œè¯­ä¹‰çŸ¥è¯†çš„ç†è§£ã€‚è€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶å±•ç°å‡ºä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMsåœ¨å…·èº«ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç ”ç©¶å¦‚ä½•åˆ©ç”¨ç¯å¢ƒåé¦ˆæ¥æé«˜æœºå™¨äººçš„è§„åˆ’èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶ï¼Œé€šè¿‡å°†ç¯å¢ƒåé¦ˆä¸LLMsç›¸ç»“åˆï¼Œå®ç°æœºå™¨äººå¯¹å¤æ‚ä»»åŠ¡çš„æ¨ç†å’Œè§„åˆ’ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡çš„æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ç¯å¢ƒåé¦ˆå½¢æˆâ€œå†…å¿ƒç‹¬ç™½â€<br>é€šè¿‡å°†ç¯å¢ƒåé¦ˆï¼ˆå¦‚æˆåŠŸæ£€æµ‹ã€åœºæ™¯æè¿°ã€äººç±»äº¤äº’ç­‰ï¼‰ä¸æ–­æ³¨å…¥LLMsçš„è§„åˆ’è¯­è¨€æç¤ºä¸­ï¼Œæœºå™¨äººå¯ä»¥å½¢æˆâ€œå†…å¿ƒç‹¬ç™½â€ï¼Œä»è€Œæ›´ä¸°å¯Œåœ°å¤„ç†å’Œè§„åˆ’æœºå™¨äººæ§åˆ¶åœºæ™¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç§åé¦ˆæ¥æº<br>æœ¬æ–‡ç ”ç©¶äº†å¤šç§åé¦ˆæ¥æºï¼ŒåŒ…æ‹¬æˆåŠŸæ£€æµ‹ã€åœºæ™¯æè¿°å’Œäººç±»äº¤äº’ã€‚é€šè¿‡å°†è¿™äº›åé¦ˆä¸LLMsç›¸ç»“åˆï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£ç¯å¢ƒï¼Œå¹¶æ ¹æ®åé¦ˆè¿›è¡Œé‡æ–°è§„åˆ’ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ç¯å¢ƒåé¦ˆçš„â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶å¯ä»¥æ˜¾è‘—æé«˜æœºå™¨äººæ‰§è¡Œé«˜çº§æŒ‡ä»¤çš„èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿçš„æ¡Œé¢æ•´ç†ä»»åŠ¡å’ŒçœŸå®çš„å¨æˆ¿ç§»åŠ¨æ“ä½œä»»åŠ¡ä¸­ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä»»åŠ¡å®Œæˆç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶ä¸ºæœºå™¨äººè§„åˆ’æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>*   åˆ©ç”¨LLMsçš„è¯­ä¹‰çŸ¥è¯†è¿›è¡Œæ¨ç†å’Œè§„åˆ’<br>*   é€šè¿‡ç¯å¢ƒåé¦ˆå®ç°æœºå™¨äººå¯¹å¤æ‚ä»»åŠ¡çš„é€‚åº”æ€§<br>*   ç»“åˆå¤šç§åé¦ˆæ¥æºæé«˜æœºå™¨äººçš„è§„åˆ’èƒ½åŠ›<br><br>## ğŸŒŸ æ€»ç»“<br>æœ¬æ–‡æå‡ºçš„â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶ä¸ºæœºå™¨äººè§„åˆ’æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡åˆ©ç”¨LLMsçš„è¯­ä¹‰çŸ¥è¯†å’Œç¯å¢ƒåé¦ˆï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£ç¯å¢ƒå¹¶æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚æœªæ¥ï¼Œéšç€LLMså’Œæ„ŸçŸ¥æŠ€æœ¯çš„å‘å±•ï¼Œç›¸ä¿¡â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶å°†åœ¨æœºå™¨äººé¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚</td>
    </tr>
    <tr>
      <th>24</th>
      <td>What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</td>
      <td>This study introduces "CosmoAgent," an innovative artificial intelligence<br>system that utilizes Large Language Models (LLMs) to simulate complex<br>interactions between human and extraterrestrial civilizations. This paper<br>introduces a mathematical model for quantifying the levels of civilization<br>development and further employs a state transition matrix approach to evaluate<br>their trajectories. Through this methodology, our study quantitatively analyzes<br>the growth trajectories of civilizations, providing insights into future<br>decision-making at critical points of growth and saturation. Furthermore, this<br>paper acknowledges the vast diversity of potential living conditions across the<br>universe, which could foster unique cosmologies, ethical codes, and worldviews<br>among different civilizations. Recognizing the Earth-centric bias inherent in<br>current LLM designs, we propose the novel concept of using LLM agents with<br>diverse ethical paradigms and simulating interactions between entities with<br>distinct moral principles. This innovative research not only introduces a novel<br>method for comprehending potential inter-civilizational dynamics but also holds<br>practical value in enabling entities with divergent value systems to<br>strategize, prevent conflicts, and engage in games under conditions of<br>asymmetric information. The accompanying code is available at<br>https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¤–æ˜Ÿæ–‡æ˜ï¼šæ¢ç´¢å®‡å®™ä¸­çš„äº’åŠ¨ä¸å†²çª<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨æ¨¡æ‹Ÿå¤æ‚ç¤¾ä¼šåŠ¨æ€æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMsè®¾è®¡å¾€å¾€å¸¦æœ‰åœ°çƒä¸­å¿ƒä¸»ä¹‰çš„åè§ï¼Œéš¾ä»¥å…¨é¢æ¨¡æ‹Ÿå¤–æ˜Ÿæ–‡æ˜çš„å¤šæ ·æ€§å’Œç‹¬ç‰¹æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥å…·æœ‰ä¸åŒä¼¦ç†èŒƒå¼å’Œé“å¾·åŸåˆ™çš„LLMä»£ç†ï¼Œæ¨¡æ‹Ÿäººç±»ä¸å¤–æ˜Ÿæ–‡æ˜ä¹‹é—´çš„å¤æ‚äº’åŠ¨ï¼Œä»è€Œä¸ºç†è§£æ½œåœ¨æ˜Ÿé™…åŠ¨æ€æä¾›æ–°çš„è§†è§’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCosmoAgentå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ<br>æœ¬æ–‡æå‡ºäº†CosmoAgentï¼Œä¸€ä¸ªåŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºæ¨¡æ‹Ÿå®‡å®™ä¸­ä¸åŒæ–‡æ˜ä¹‹é—´çš„äº’åŠ¨ã€‚CosmoAgenté€šè¿‡æ¨¡æ‹Ÿæ–‡æ˜çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€‰æ‹©éšè—ã€æˆ˜æ–—æˆ–åˆä½œï¼Œæ¥æ¢ç´¢æ–‡æ˜å‘å±•çš„è½¨è¿¹å’Œæ½œåœ¨å†²çªã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–‡æ˜å‘å±•æ¨¡å‹<br>æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ•°å­¦æ¨¡å‹æ¥é‡åŒ–æ–‡æ˜çš„å‘å±•æ°´å¹³ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µæ–¹æ³•æ¥è¯„ä¼°æ–‡æ˜çš„è½¨è¿¹ã€‚è¯¥æ¨¡å‹è€ƒè™‘äº†äº”ä¸ªå…³é”®èµ„æºï¼šå†›äº‹èƒ½åŠ›ã€æŠ€æœ¯å‘å±•ã€ç”Ÿäº§èƒ½åŠ›ã€æ¶ˆè´¹å’Œå‚¨å­˜ï¼Œä»¥åŠä¸åŒæ–‡æ˜çš„ä¸–ç•Œè§‚ï¼ˆå’Œå¹³ä¸»ä¹‰ã€å†›å›½ä¸»ä¹‰å’Œå­¤ç«‹ä¸»ä¹‰ï¼‰å¯¹å†³ç­–çš„å½±å“ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¿¡æ¯ä¸å¯¹ç§°çš„æ¨¡æ‹Ÿ<br>ä¸ºäº†æ¨¡æ‹Ÿå®‡å®™ä¸­æ–‡æ˜ä¹‹é—´çš„äº’åŠ¨ï¼Œæœ¬æ–‡è€ƒè™‘äº†ä¿¡æ¯ä¸å¯¹ç§°çš„æƒ…å†µï¼Œå³æ–‡æ˜ä¹‹é—´çš„è§‚æµ‹æ•°æ®æ»åäºå®é™…å‘å±•ã€‚LLMsä»£ç†éœ€è¦æ ¹æ®è¿‡æ—¶çš„ä¿¡æ¯åšå‡ºå†³ç­–ï¼Œè¿™å¢åŠ äº†æ¨¡æ‹Ÿçš„å¤æ‚æ€§å’Œç°å®æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šé“å¾·å¤šæ ·æ€§çš„æ¨¡æ‹Ÿ<br>æœ¬æ–‡æå‡ºäº†ä½¿ç”¨å…·æœ‰ä¸åŒä¼¦ç†èŒƒå¼çš„LLMä»£ç†æ¥æ¨¡æ‹Ÿå…·æœ‰ä¸åŒé“å¾·åŸåˆ™çš„å®ä½“ä¹‹é—´çš„äº’åŠ¨ã€‚è¿™æœ‰åŠ©äºç†è§£ä¸åŒæ–‡æ˜å¦‚ä½•å…±å­˜ï¼Œä»¥åŠé“å¾·æ¡†æ¶å¦‚ä½•å½±å“æ˜Ÿé™…äº’åŠ¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰å†›å›½ä¸»ä¹‰ä¸–ç•Œè§‚çš„æ–‡æ˜å€¾å‘äºå¯¹è¾ƒå¼±æ–‡æ˜å‘åŠ¨æ”»å‡»ï¼Œè€Œå­¤ç«‹ä¸»ä¹‰æ–‡æ˜åˆ™æ›´å€¾å‘äºåœ¨è§‚å¯Ÿä¸€æ®µæ—¶é—´åé€‰æ‹©æ€§åœ°ä¸å…¶ä»–æ–‡æ˜åˆä½œã€‚æ­¤å¤–ï¼Œä¿¡æ¯ä¸å¯¹ç§°ä¼šå»¶è¿Ÿå†²çªçš„å‘ç”Ÿï¼Œä¸ºè¾ƒå¼±æ–‡æ˜æä¾›äº†åå‡»çš„æœºä¼šã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ä¸ä»…ä¸ºç†è§£æ½œåœ¨æ˜Ÿé™…åŠ¨æ€æä¾›äº†æ–°çš„è§†è§’ï¼Œè¿˜ä¸ºè§£å†³å…·æœ‰ä¸åŒä»·å€¼ä½“ç³»çš„å®ä½“ä¹‹é—´çš„å†²çªæä¾›äº†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œå¦‚æ¨¡æ‹Ÿå¤ä»£ç¤¾ä¼šã€äººç±»æ–‡æ˜æ¨¡å¼å’Œç¤¾ä¼šç”Ÿæ€ç³»ç»Ÿã€‚</td>
    </tr>
    <tr>
      <th>76</th>
      <td>MindAgent: Emergent Gaming Interaction</td>
      <td>Large Language Models (LLMs) have the capacity of performing complex<br>scheduling in a multi-agent system and can coordinate these agents into<br>completing sophisticated tasks that require extensive collaboration. However,<br>despite the introduction of numerous gaming frameworks, the community has<br>insufficient benchmarks towards building general multi-agents collaboration<br>infrastructure that encompass both LLM and human-NPCs collaborations. In this<br>work, we propose a novel infrastructure - MindAgent - to evaluate planning and<br>coordination emergent capabilities for gaming interaction. In particular, our<br>infrastructure leverages existing gaming framework, to i) require understanding<br>of the coordinator for a multi-agent system, ii) collaborate with human players<br>via un-finetuned proper instructions, and iii) establish an in-context learning<br>on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new<br>gaming scenario and related benchmark that dispatch a multi-agent collaboration<br>efficiency and supervise multiple agents playing the game simultaneously. We<br>conduct comprehensive evaluations with new auto-metric CoS for calculating the<br>collaboration efficiency. Finally, our infrastructure can be deployed into<br>real-world gaming scenarios in a customized VR version of CUISINEWORLD and<br>adapted in existing broader Minecraft gaming domain. We hope our findings on<br>LLMs and the new infrastructure for general-purpose scheduling and coordination<br>can help shed light on how such skills can be obtained by learning from large<br>language corpora.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MindAgentï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ä¹Ÿé€æ¸å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆæ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•è¿˜ä¸è¶³ä»¥è¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨LLMsä¸äººç±»NPCsåä½œçš„åœºæ™¯ä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMindAgentçš„æ–°å‹åŸºç¡€è®¾æ–½ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸åŸºå‡†æµ‹è¯•<br>æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåä¸ºCUISINEWORLDçš„æ¸¸æˆåœºæ™¯ï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªè™šæ‹Ÿå¨æˆ¿ç¯å¢ƒï¼Œå…¶ä¸­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿéœ€è¦åè°ƒå¤šä¸ªä»£ç†ï¼Œå®Œæˆå°½å¯èƒ½å¤šçš„èœè‚´è®¢å•ã€‚CUISINEWORLDæ¸¸æˆåœºæ™¯å…·æœ‰å¤šç§ä»»åŠ¡ç»“æ„å’Œéš¾åº¦ï¼Œæ˜¯è¯„ä¼°LLMsæ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMindAgentåŸºç¡€è®¾æ–½<br>MindAgentæ˜¯ä¸€ä¸ªç”¨äºLLMsäº¤äº’å¼å¤šæ™ºèƒ½ä½“è§„åˆ’çš„åŸºç¡€è®¾æ–½ï¼Œå®ƒå±•ç¤ºäº†LLMsçš„æ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†å¤šç§æç¤ºæŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›LLMsçš„è§„åˆ’èƒ½åŠ›ï¼ŒåŒ…æ‹¬æä¾›å°‘é‡ç¤ºä¾‹ã€è§„åˆ’ç†ç”±å’Œç¯å¢ƒåé¦ˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨CUISINEWORLDæ¸¸æˆåœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š<br>1. é›¶æ ·æœ¬å¤šæ™ºèƒ½ä½“è§„åˆ’ï¼šå¼ºå¤§çš„é¢„è®­ç»ƒLLMsï¼ˆå¦‚GPT-4ï¼‰èƒ½å¤Ÿé€šè¿‡é˜…è¯»ç®€å•çš„æ¸¸æˆæŒ‡ä»¤å’Œé£Ÿè°±ï¼Œè°ƒåº¦å¤šä¸ªä»£ç†ï¼ˆ2åˆ°4ä¸ªï¼‰å®Œæˆèœè‚´ï¼Œç”šè‡³ä¸äººç±»ç©å®¶åä½œã€‚<br>2. åŸºäºé«˜çº§æç¤ºçš„è§„åˆ’ï¼šé€šè¿‡åˆ©ç”¨æ¶Œç°å¼ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜LLMsçš„å¤šæ™ºèƒ½ä½“è§„åˆ’æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ å°‘é‡ä¸“å®¶æ¼”ç¤ºã€è§£é‡ŠæŸäº›è¡ŒåŠ¨çš„ç†ç”±ï¼Œä»¥åŠåœ¨è§„åˆ’è¿‡ç¨‹ä¸­æä¾›å®æ—¶åé¦ˆã€‚<br>3. é€šç”¨æ½œåŠ›ï¼šLLMsè¡¨ç°å‡ºæˆä¸ºé€šç”¨å¤šæ™ºèƒ½ä½“è§„åˆ’å™¨çš„å·¨å¤§æ½œåŠ›ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿé€šè¿‡å°‘é‡ç¤ºä¾‹æ³›åŒ–åˆ°æ›´å¤šä»£ç†ï¼Œå¹¶é€‚åº”æ–°çš„æ¸¸æˆé¢†åŸŸï¼Œå¦‚Minecraftã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„MindAgentåŸºç¡€è®¾æ–½å’ŒCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸ºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼ŒLLMsåœ¨å¤šæ™ºèƒ½ä½“è§„åˆ’æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæœ‰æœ›åœ¨æœªæ¥æ¨åŠ¨æ¸¸æˆAIçš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Software Agents Interaction Algorithms in Virtual Learning Environment</td>
      <td>This paper highlights the multi-agent learning virtual environment and agents<br>communication algorithms. The researcher proposed three algorithms required<br>software agents interaction in virtual learning information system environment.<br>The first proposed algorithm is agents interaction localization algorithm, the<br>second one is the dynamic agents distribution algorithm (load distribution<br>algorithm), and the third model is Agent communication algorithm based on using<br>agents intermediaries. The main objectives of these algorithms are to reduce<br>the response time for any agents changes in virtual learning environment (VLE)<br>by increasing the information exchange intensity between software agents and<br>reduce the overall network load, and to improve the communication between<br>mobile agents in distributed information system to support effectiveness.<br>Finally the paper describe the algorithms of information exchange between<br>mobile agents in VLE based on the expansion of the address structure and the<br>use of an agent, intermediary agents, matchmaking agents, brokers and their<br>entrepreneurial functions</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒä¸­è½¯ä»¶ä»£ç†äº¤äº’ç®—æ³•çš„é©æ–°<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ä¿¡æ¯æŠ€æœ¯çš„é£é€Ÿå‘å±•ï¼Œè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒï¼ˆVLEï¼‰å·²ç»æˆä¸ºæ•™è‚²é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLEç³»ç»Ÿåœ¨å¤„ç†å¤§é‡ä¿¡æ¯äº¤äº’æ—¶ï¼Œé¢ä¸´ç€ç½‘ç»œè´Ÿè½½å¢åŠ ã€ä¿¡æ¯äº¤æ¢æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè½¯ä»¶ä»£ç†çš„äº¤äº’ç®—æ³•ï¼Œæ—¨åœ¨æé«˜VLEç³»ç»Ÿçš„æ•ˆç‡å’Œå“åº”é€Ÿåº¦ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»£ç†äº¤äº’å®šä½ç®—æ³•<br>è¯¥ç®—æ³•é€šè¿‡åˆ†æä»£ç†ä¹‹é—´çš„é€šä¿¡ä¾èµ–æ€§ï¼Œå°†é¢‘ç¹äº¤äº’çš„ä»£ç†åˆ†ç»„åˆ°åŒä¸€ä¸»æœºä¸Šï¼Œä»è€Œå°†è·¨ä¸»æœºçš„äº¤äº’è½¬åŒ–ä¸ºä¸»æœºå†…çš„äº¤äº’ï¼Œå‡å°‘ç½‘ç»œè´Ÿè½½å¹¶æé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€ä»£ç†åˆ†é…ç®—æ³•ï¼ˆè´Ÿè½½åˆ†é…ç®—æ³•ï¼‰<br>è¯¥ç®—æ³•é€šè¿‡ç›‘æ§ä¸»æœºè´Ÿè½½ï¼Œå°†ä»£ç†åˆ†ç»„å¹¶åŠ¨æ€åˆ†é…åˆ°ä¸åŒçš„ä¸»æœºä¸Šï¼Œä»¥å®ç°è´Ÿè½½å‡è¡¡ï¼Œé¿å…æŸäº›ä¸»æœºè¿‡è½½ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºä»£ç†ä¸­ä»‹çš„é€šä¿¡æ¨¡å‹<br>è¯¥æ¨¡å‹åˆ©ç”¨ä»£ç†ä¸­ä»‹ï¼ˆå¦‚ç»çºªäººä»£ç†å’Œé…å¯¹ä»£ç†ï¼‰æ¥ä¿ƒè¿›ä»£ç†ä¹‹é—´çš„é€šä¿¡ã€‚ä»£ç†ä¸­ä»‹å¯ä»¥å¸®åŠ©ä»£ç†æŸ¥æ‰¾å…·æœ‰ç›¸ä¼¼å…´è¶£çš„ä»£ç†ï¼Œå¹¶æä¾›æ¶ˆæ¯è½¬å‘å’ŒåŒ¹é…æœåŠ¡ï¼Œä»è€Œæé«˜é€šä¿¡æ•ˆç‡å’Œçµæ´»æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡æå‡ºçš„ç®—æ³•åœ¨è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜ï¼Œè¿™äº›ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç½‘ç»œè´Ÿè½½ï¼Œæé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ï¼Œå¹¶æé«˜ä»£ç†ä¹‹é—´çš„é€šä¿¡æ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ç®—æ³•å’Œæ¨¡å‹ä¸ºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒçš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶ä¸­ï¼Œä»£ç†äº¤äº’å®šä½ç®—æ³•å’ŒåŠ¨æ€ä»£ç†åˆ†é…ç®—æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œä»¥æé«˜ç³»ç»Ÿçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚åŸºäºä»£ç†ä¸­ä»‹çš„é€šä¿¡æ¨¡å‹å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šä»£ç†ç³»ç»Ÿä¸­ï¼Œä»¥ä¿ƒè¿›ä»£ç†ä¹‹é—´çš„åä½œå’Œé€šä¿¡ã€‚<br><br>## ğŸ“š æ€»ç»“<br>æœ¬æ–‡æå‡ºçš„åŸºäºè½¯ä»¶ä»£ç†çš„äº¤äº’ç®—æ³•ä¸ºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒçš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚è¿™äº›ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç½‘ç»œè´Ÿè½½ï¼Œæé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ï¼Œå¹¶æé«˜ä»£ç†ä¹‹é—´çš„é€šä¿¡æ•ˆç‡ã€‚æœ¬æ–‡çš„ç ”ç©¶æˆæœå¯¹äºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒå’Œå…¶ä»–åˆ†å¸ƒå¼ç³»ç»Ÿçš„è®¾è®¡å’Œä¼˜åŒ–å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>51</th>
      <td>Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game</td>
      <td>Agents built with large language models (LLMs) have shown great potential<br>across a wide range of domains. However, in complex decision-making tasks, pure<br>LLM-based agents tend to exhibit intrinsic bias in their choice of actions,<br>which is inherited from the model's training data and results in suboptimal<br>performance. To develop strategic language agents, i.e., agents that generate<br>flexible language actions and possess strong decision-making abilities, we<br>propose a novel framework that powers LLM-based agents with reinforcement<br>learning (RL). We consider Werewolf, a popular social deduction game, as a<br>challenging testbed that emphasizes versatile communication and strategic<br>gameplay. To mitigate the intrinsic bias in language actions, our agents use an<br>LLM to perform deductive reasoning and generate a diverse set of action<br>candidates. Then an RL policy trained to optimize the decision-making ability<br>chooses an action from the candidates to play in the game. Extensive<br>experiments show that our agents overcome the intrinsic bias and outperform<br>existing LLM-based agents in the Werewolf game. We also conduct human-agent<br>experiments and find that our agents achieve human-level performance and<br>demonstrate strong strategic play.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ™ºèƒ½ä½“åœ¨ç‹¼äººæ€æ¸¸æˆä¸­çš„æˆ˜ç•¥å†³ç­–<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ„å»ºæ™ºèƒ½ä½“æ–¹é¢çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„å†…åœ¨åå·®é—®é¢˜é€æ¸å‡¸æ˜¾ã€‚è¿™ç§åå·®æºäºæ¨¡å‹è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´LLM-basedæ™ºèƒ½ä½“åœ¨æˆ˜ç•¥å†³ç­–æ–¹é¢è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†LLMä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆï¼Œä»¥æ„å»ºå…·æœ‰çµæ´»è¯­è¨€è¡ŒåŠ¨å’Œå¼ºå¤§å†³ç­–èƒ½åŠ›çš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šéšè—è§’è‰²æ¨ç†<br>æœ¬æ–‡ä½¿ç”¨LLMå¯¹æ¸¸æˆä¸­çš„ä¿¡æ¯è¿›è¡Œåˆ†ç±»ï¼ŒåŒºåˆ†çœŸä¼ªï¼Œå¹¶æ¨æ–­æ¯ä¸ªç©å®¶çš„éšè—è§’è‰²ï¼Œä¸ºåç»­å†³ç­–æä¾›åŸºç¡€ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆ<br>ä¸ºäº†å…‹æœLLMçš„å†…åœ¨åå·®ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æç¤ºLLMç”Ÿæˆä¸€ç³»åˆ—è¡ŒåŠ¨å€™é€‰è€…ï¼Œä»è€Œé¿å…å›ºå®šæ¨¡å¼å¹¶æé«˜å†³ç­–çš„çµæ´»æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ<br>æœ¬æ–‡é‡‡ç”¨åŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªRLç­–ç•¥æ¥ä¼˜åŒ–è¡ŒåŠ¨å€™é€‰è€…çš„åˆ†å¸ƒï¼Œå¹¶é€šè¿‡ä¸å„ç§æ™ºèƒ½ä½“è¿›è¡Œå¯¹æŠ—æ¥æé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ç‹¼äººæ€æ¸¸æˆä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„LLM-basedæ™ºèƒ½ä½“ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“èƒ½å¤Ÿå…‹æœå†…åœ¨åå·®ï¼Œå¹¶åœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸äººç±»ç©å®¶çš„å¯¹å±€å®éªŒä¹Ÿè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ™ºèƒ½ä½“èƒ½å¤Ÿè¾¾åˆ°äººç±»æ°´å¹³çš„æ¸¸æˆè¡¨ç°ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºæ„å»ºå…·æœ‰å¼ºå¤§å†³ç­–èƒ½åŠ›çš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦çµæ´»è¯­è¨€è¡ŒåŠ¨å’Œæˆ˜ç•¥å†³ç­–çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆæ–¹æ³•å’ŒåŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ä¹Ÿä¸ºè§£å†³LLMå†…åœ¨åå·®é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>90</th>
      <td>Building Cooperative Embodied Agents Modularly with Large Language Models</td>
      <td>In this work, we address challenging multi-agent cooperation problems with<br>decentralized control, raw sensory observations, costly communication, and<br>multi-objective tasks instantiated in various embodied environments. While<br>previous research either presupposes a cost-free communication channel or<br>relies on a centralized controller with shared observations, we harness the<br>commonsense knowledge, reasoning ability, language comprehension, and text<br>generation prowess of LLMs and seamlessly incorporate them into a<br>cognitive-inspired modular framework that integrates with perception, memory,<br>and execution. Thus building a Cooperative Embodied Language Agent CoELA, who<br>can plan, communicate, and cooperate with others to accomplish long-horizon<br>tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA<br>driven by GPT-4 can surpass strong planning-based methods and exhibit emergent<br>effective communication. Though current Open LMs like LLAMA-2 still<br>underperform, we fine-tune a CoELA with data collected with our agents and show<br>how they can achieve promising performance. We also conducted a user study for<br>human-agent interaction and discovered that CoELA communicating in natural<br>language can earn more trust and cooperate more effectively with humans. Our<br>research underscores the potential of LLMs for future research in multi-agent<br>cooperation. Videos can be found on the project website<br>https://vis-www.cs.umass.edu/Co-LLM-Agents/.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºåä½œå‹å…·èº«æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œäººç±»æ“…é•¿ä¸ä»–äººåˆä½œå’Œæ²Ÿé€šä»¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ„å»ºèƒ½å¤Ÿä¸äººç±»æˆ–å…¶ä»–æ™ºèƒ½ä½“åä½œçš„å…·èº«æ™ºèƒ½ä½“ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å…·å¤‡æ„ŸçŸ¥ã€éƒ¨åˆ†è§‚å¯Ÿã€é•¿æœŸè§„åˆ’ã€è‡ªç„¶è¯­è¨€æ²Ÿé€šç­‰å¤æ‚èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º CoELA çš„åä½œå‹å…·èº«è¯­è¨€æ™ºèƒ½ä½“ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„å¸¸è¯†çŸ¥è¯†ã€æ¨ç†èƒ½åŠ›ã€è¯­è¨€ç†è§£å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶å°†å…¶é›†æˆåˆ°ä¸€ä¸ªè®¤çŸ¥å¯å‘çš„æ¨¡å—åŒ–æ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶ä¸æ„ŸçŸ¥ã€è®°å¿†å’Œæ‰§è¡Œæ¨¡å—ç›¸ç»“åˆã€‚<br><br>CoELA æ¡†æ¶åŒ…æ‹¬äº”ä¸ªå…³é”®æ¨¡å—ï¼š<br>1. **æ„ŸçŸ¥æ¨¡å—**ï¼šç”¨äºæ„ŸçŸ¥ç¯å¢ƒä¸­çš„åŸå§‹æ„Ÿå®˜è§‚å¯Ÿå¹¶æå–æœ‰ç”¨ä¿¡æ¯ã€‚<br>2. **è®°å¿†æ¨¡å—**ï¼šæ¨¡æ‹Ÿäººç±»çš„é•¿æœŸè®°å¿†ï¼Œå­˜å‚¨æ™ºèƒ½ä½“å¯¹ä¸–ç•Œå’Œå…¶ä»–æ™ºèƒ½ä½“çš„ç†è§£å’Œç»éªŒã€‚<br>3. **é€šä¿¡æ¨¡å—**ï¼šåˆ©ç”¨ LLM çš„å¼ºå¤§å¯¹è¯ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ï¼Œå†³å®šå‘é€ä»€ä¹ˆä¿¡æ¯ã€‚<br>4. **è§„åˆ’æ¨¡å—**ï¼šåˆ©ç”¨ LLM çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œæ ¹æ®æ‰€æœ‰å¯ç”¨ä¿¡æ¯åšå‡ºå†³ç­–ï¼Œå¹¶åˆ¶å®šé«˜çº§è®¡åˆ’ã€‚<br>5. **æ‰§è¡Œæ¨¡å—**ï¼šæ ¹æ®è§„åˆ’æ¨¡å—ç”Ÿæˆçš„è®¡åˆ’ï¼Œç”Ÿæˆå¯æ‰§è¡Œçš„ä½çº§åŠ¨ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ C-WAH å’Œ TDW-MAT ä¸¤ä¸ªå…·èº«ç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCoELA èƒ½å¤Ÿæ„ŸçŸ¥å¤æ‚è§‚å¯Ÿã€æ¨ç†ä¸–ç•Œå’Œå…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ã€æœ‰æ•ˆæ²Ÿé€šï¼Œå¹¶ç›¸åº”åœ°åˆ¶å®šé•¿æœŸè®¡åˆ’ã€‚CoELA é©±åŠ¨çš„ GPT-4 èƒ½å¤Ÿè¶…è¶ŠåŸºäºè§„åˆ’çš„å¼ºæ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºæœ‰æ•ˆçš„æ²Ÿé€šã€‚å°½ç®¡å½“å‰çš„å¼€æ”¾ LLMï¼ˆå¦‚ LLAMA-2ï¼‰ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œä½†ä½œè€…é€šè¿‡åœ¨å…·èº«ç¯å¢ƒä¸­æ”¶é›†çš„æ•°æ®å¯¹ CoELA è¿›è¡Œå¾®è°ƒï¼Œå¹¶å±•ç¤ºäº†å…¶æœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>1. **æ¨¡å—åŒ–æ¡†æ¶**ï¼šCoELA çš„æ¨¡å—åŒ–æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°å°† LLM çš„èƒ½åŠ›ä¸æ„ŸçŸ¥ã€è®°å¿†å’Œæ‰§è¡Œæ¨¡å—ç›¸ç»“åˆï¼Œä»è€Œæ„å»ºåä½œå‹å…·èº«æ™ºèƒ½ä½“ã€‚<br>2. **è‡ªç„¶è¯­è¨€æ²Ÿé€š**ï¼šCoELA ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œæ²Ÿé€šï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ä¸äººç±»æˆ–å…¶ä»–æ™ºèƒ½ä½“åä½œã€‚<br>3. **å¾®è°ƒ LLM**ï¼šä½œè€…é€šè¿‡åœ¨å…·èº«ç¯å¢ƒä¸­æ”¶é›†çš„æ•°æ®å¯¹ CoELA è¿›è¡Œå¾®è°ƒï¼Œå¹¶å±•ç¤ºäº†å…¶æœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œè¿™ä¸ºæ„å»ºæ›´å¥½çš„åä½œå‹å…·èº«æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚<br><br>## ğŸŒˆ æœªæ¥å±•æœ›<br>CoELA çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLM åœ¨å¤šæ™ºèƒ½ä½“åä½œé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š<br>1. **å¤šæ¨¡æ€ LLM**ï¼šå¼€å‘èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è§†è§‰æ¨¡æ€å¹¶æµç•…ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„ LLMï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨ 3D ç©ºé—´ä¿¡æ¯ã€‚<br>2. **ä½çº§æ§åˆ¶**ï¼šå¼€å‘èƒ½å¤Ÿç›´æ¥è¿›è¡Œä½çº§æ§åˆ¶çš„æ™ºèƒ½ä½“ï¼Œä»¥æ›´å¥½åœ°ç†è§£ä½çº§åŠ¨ä½œçš„æ‰§è¡Œã€‚<br>3. **å¤æ‚æ¨ç†**ï¼šå¼€å‘å…·æœ‰æ›´å¼ºæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›çš„ LLMï¼Œä»¥æé«˜æ™ºèƒ½ä½“çš„é²æ£’æ€§ã€‚<br><br>æ€»è€Œè¨€ä¹‹ï¼ŒCoELA ä¸ºæ„å»ºåä½œå‹å…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Will GPT-4 Run DOOM?</td>
      <td>We show that GPT-4's reasoning and planning capabilities extend to the 1993<br>first-person shooter Doom. This large language model (LLM) is able to run and<br>play the game with only a few instructions, plus a textual<br>description--generated by the model itself from screenshots--about the state of<br>the game being observed. We find that GPT-4 can play the game to a passable<br>degree: it is able to manipulate doors, combat enemies, and perform pathing.<br>More complex prompting strategies involving multiple model calls provide better<br>results. While further work is required to enable the LLM to play the game as<br>well as its classical, reinforcement learning-based counterparts, we note that<br>GPT-4 required no training, leaning instead on its own reasoning and<br>observational capabilities. We hope our work pushes the boundaries on<br>intelligent, LLM-based agents in video games. We conclude by discussing the<br>ethical implications of our work.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GPT-4èƒ½å¦ç©è½¬ç»å…¸å°„å‡»æ¸¸æˆDoomï¼Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4çš„å‡ºç°ï¼Œå®ƒä»¬åœ¨ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼ŒLLMåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£ä¹‹è°œã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMåœ¨è§†é¢‘æ¸¸æˆä¸­çš„æ™ºèƒ½ä»£ç†åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯è¯„ä¼°GPT-4åœ¨ç»å…¸å°„å‡»æ¸¸æˆDoomä¸­çš„è¡¨ç°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†è§‰è¾“å…¥ä¸æ–‡æœ¬æè¿°çš„ç»“åˆ<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†GPT-4çš„è§†è§‰è¾“å…¥èƒ½åŠ›ä¸æ–‡æœ¬æè¿°ç›¸ç»“åˆï¼Œä»¥ç†è§£æ¸¸æˆçŠ¶æ€ã€‚GPT-4Væ¨¡å‹ä»æ¸¸æˆæˆªå›¾ä¸­ç”Ÿæˆæ¸¸æˆçŠ¶æ€çš„æ–‡æœ¬æè¿°ï¼Œè€ŒGPT-4æ¨¡å‹åˆ™æ ¹æ®è¿™äº›æè¿°å’Œä¹‹å‰çš„è¡ŒåŠ¨å†å²æ¥åšå‡ºå†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šå±‚æ¬¡è§„åˆ’ç­–ç•¥<br>ä¸ºäº†æé«˜GPT-4åœ¨æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œæœ¬æ–‡æå‡ºäº†å¤šç§è§„åˆ’ç­–ç•¥ï¼ŒåŒ…æ‹¬ç®€å•çš„æŒ‡ä»¤ã€åˆ†æ­¥çš„å…³å¡æ”»ç•¥ã€æ›´ç»†ç²’åº¦çš„è®¡åˆ’ç”Ÿæˆï¼Œä»¥åŠåŸºäºå¤šä¸ªä¸“å®¶æ„è§çš„k-levelsç­–ç•¥ã€‚è¿™äº›ç­–ç•¥æ—¨åœ¨ä¸ºGPT-4æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥å¢å¼ºå…¶æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4èƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šç©è½¬Doomæ¸¸æˆï¼Œèƒ½å¤Ÿæ‰§è¡Œå¼€é—¨ã€æˆ˜æ–—æ•Œäººã€è·¯å¾„è§„åˆ’ç­‰åŸºæœ¬æ“ä½œã€‚ç„¶è€Œï¼ŒGPT-4çš„æ¨ç†æ·±åº¦æœ‰é™ï¼Œç¼ºä¹é•¿æœŸè§„åˆ’å’Œè®°å¿†èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œå½“æ•Œäººç¦»å¼€è§†é‡æ—¶ï¼Œæ¨¡å‹ä¼šå¿˜è®°å®ƒä»¬çš„å­˜åœ¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´ç²¾ç»†çš„è§„åˆ’ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•æé«˜LLMçš„æ¨ç†æ·±åº¦å’Œè®°å¿†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä¹Ÿæé†’æˆ‘ä»¬ï¼ŒLLMæŠ€æœ¯çš„å¿«é€Ÿå‘å±•éœ€è¦æ›´åŠ è°¨æ…çš„è¯„ä¼°å’Œç›‘ç®¡ï¼Œä»¥é˜²æ­¢æ½œåœ¨çš„æ»¥ç”¨é£é™©ã€‚</td>
    </tr>
    <tr>
      <th>47</th>
      <td>MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration</td>
      <td>Large Language Models (LLMs) have significantly advanced natural language<br>processing, demonstrating exceptional reasoning, tool usage, and memory<br>capabilities. As their applications expand into multi-agent environments, there<br>arises a need for a comprehensive evaluation framework that captures LLMs'<br>reasoning, planning, collaboration, and other social abilities. This work<br>introduces a novel competition-based benchmark framework specifically designed<br>to assess LLMs within multi-agent settings, providing quantitative metrics to<br>evaluate their judgment, reasoning, deception, self-awareness, cooperation,<br>coordination, and rationality. We utilize two social deduction games alongside<br>three game-theory scenarios to create diverse environments. Our frame is<br>fortified with the probabilistic graphic modeling (PGM) method, enhancing the<br>LLMs' capabilities in navigating complex social and cognitive dimensions. We<br>evaluate seven LLMs, quantitatively highlighting a significant capability gap<br>of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.<br>It also confirms that our PGM enhancement boosts the abilities of all selected<br>models by an average of 37%. Our data and code can be found here<br>https://github.com/cathyxl/MAgIC.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MAgICï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¤çŸ¥ã€é€‚åº”æ€§ã€ç†æ€§å’Œåä½œä¸­çš„å¤šæ™ºèƒ½ä½“ç ”ç©¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œå®ƒä»¬åœ¨æ¨ç†ã€å·¥å…·ä½¿ç”¨å’Œè®°å¿†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“LLMsçš„åº”ç”¨æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“ç¯å¢ƒæ—¶ï¼Œéœ€è¦ä¸€ç§å…¨é¢çš„è¯„ä¼°æ¡†æ¶æ¥æ•æ‰LLMsçš„æ¨ç†ã€è§„åˆ’ã€åä½œå’Œå…¶ä»–ç¤¾äº¤èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç«äº‰çš„åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå¹¶æä¾›å®šé‡æŒ‡æ ‡æ¥è¯„ä¼°å®ƒä»¬çš„åˆ¤æ–­ã€æ¨ç†ã€æ¬ºéª—ã€è‡ªæˆ‘æ„è¯†ã€åˆä½œã€åè°ƒå’Œç†æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«äº‰åŸºå‡†æ¡†æ¶<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç«äº‰çš„åŸºå‡†æ¡†æ¶ï¼Œé€šè¿‡å°†LLMsç½®äºå¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„ç«äº‰ä¸­ï¼Œè¯„ä¼°å®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„çœŸå®èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªç¤¾äº¤æ¨ç†æ¸¸æˆå’Œä¸‰ä¸ªåšå¼ˆè®ºåœºæ™¯ï¼Œä»¥åˆ›å»ºå¤šæ ·åŒ–çš„ç¯å¢ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¦‚ç‡å›¾æ¨¡å‹ï¼ˆPGMï¼‰å¢å¼º<br>ä¸ºäº†å¢å¼ºLLMsåœ¨å¤æ‚ç¤¾äº¤å’Œè®¤çŸ¥ç»´åº¦ä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œæœ¬æ–‡å°†æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆPGMï¼‰æ–¹æ³•ä¸LLMsç›¸ç»“åˆã€‚PGMèƒ½å¤Ÿæè¿°éšæœºå˜é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œå¸®åŠ©LLMsæ›´å¥½åœ°ç†è§£å…¨å±€ä¿¡æ¯ï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡è¯„ä¼°äº†ä¸ƒä¸ªLLMsï¼ŒåŒ…æ‹¬GPT-o1ã€GPT-4ã€GPT-3.5-turboã€PaLM 2ã€Claude 2ã€Cohereå’ŒLlama-2-70Bã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-o1åœ¨æ‰€æœ‰è¯„ä¼°ç»´åº¦ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒLlama-2-70Bè¡¨ç°æœ€å·®ã€‚æ­¤å¤–ï¼ŒPGMå¢å¼ºæ–¹æ³•å°†æ‰€æœ‰é€‰å®šæ¨¡å‹çš„å¹³å‡èƒ½åŠ›æé«˜äº†37%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ç«äº‰åŸºå‡†æ¡†æ¶å’ŒPGMå¢å¼ºæ–¹æ³•ä¸ºè¯„ä¼°å’Œæå‡LLMsåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„èƒ½åŠ›æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºLLMsåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åº”ç”¨æä¾›äº†é‡è¦çš„å‚è€ƒå’ŒæŒ‡å¯¼ã€‚</td>
    </tr>
    <tr>
      <th>42</th>
      <td>Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation</td>
      <td>Building embodied agents on integrating Large Language Models (LLMs) and<br>Reinforcement Learning (RL) have revolutionized human-AI interaction:<br>researchers can now leverage language instructions to plan decision-making for<br>open-ended tasks. However, existing research faces challenges in meeting the<br>requirement of open-endedness. They typically either train LLM/RL models to<br>adapt to a fixed counterpart, limiting exploration of novel skills and<br>hindering the efficacy of human-AI interaction. To this end, we present<br>OpenPAL, a co-training framework comprising two stages: (1) fine-tuning a<br>pre-trained LLM to translate human instructions into goals for planning, and<br>goal-conditioned training a policy for decision-making; (2) co-training to<br>align the LLM and policy, achieving instruction open-endedness. We conducted<br>experiments using Contra, an open-ended FPS game, demonstrating that an agent<br>trained with OpenPAL not only comprehends arbitrary instructions but also<br>exhibits efficient execution. These results suggest that OpenPAL holds the<br>potential to construct open-ended embodied agents in practical scenarios.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | OpenPALï¼šæ„å»ºå¼€æ”¾å¼çš„å…·èº«æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å…´èµ·ï¼Œæ„å»ºèƒ½å¤Ÿä¸äººç±»è¿›è¡Œäº¤äº’çš„å…·èº«æ™ºèƒ½ä½“æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶åœ¨å®ç°å¼€æ”¾æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå®ƒä»¬é€šå¸¸è®­ç»ƒLLM/RLæ¨¡å‹ä»¥é€‚åº”å›ºå®šçš„å¯¹æ‰‹ï¼Œé™åˆ¶äº†æ¢ç´¢æ–°æŠ€èƒ½çš„èƒ½åŠ›ï¼Œå¹¶é˜»ç¢äº†äººæœºäº¤äº’çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>OpenPALæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„ååŒè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åŒå‘é€‚åº”ï¼Œä»è€Œæ„å»ºå¼€æ”¾å¼çš„å…·èº«æ™ºèƒ½ä½“ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ­¥å¾®è°ƒ<br>OpenPALé¦–å…ˆé€šè¿‡å¤šæ­¥å¾®è°ƒé¢„è®­ç»ƒçš„LLMï¼Œä½¿å…¶èƒ½å¤Ÿå°†äººç±»æŒ‡ä»¤ç¿»è¯‘æˆè§„åˆ’ç›®æ ‡ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªç›®æ ‡æ¡ä»¶ç­–ç•¥æ¥è¿›è¡Œå†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šååŒè®­ç»ƒ<br>åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒOpenPALé€šè¿‡ååŒè®­ç»ƒæ¥å¯¹é½LLMå’Œç­–ç•¥ï¼Œå®ç°æŒ‡ä»¤çš„å¼€æ”¾æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒOpenPALäº¤æ›¿æ‰§è¡Œä»¥ä¸‹ä¸¤ä¸ªè¿‡ç¨‹ï¼š<br>1. ä½¿ç”¨ä»£ç†åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLAFï¼‰å¯¹LLMè¿›è¡Œè®­ç»ƒï¼Œå¥–åŠ±LLMç”Ÿæˆèƒ½å¤Ÿè¢«ä»£ç†æ‰§è¡Œçš„æŒ‡ä»¤ã€‚<br>2. ä½¿ç”¨LLMç”Ÿæˆçš„ç›®æ ‡å¯¹ç­–ç•¥è¿›è¡Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGCRLï¼‰è®­ç»ƒã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>OpenPALåœ¨Contraï¼ˆä¸€ä¸ªå¼€æ”¾å¼çš„FPSæ¸¸æˆï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä½¿ç”¨OpenPALè®­ç»ƒçš„æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿç†è§£ä»»æ„æŒ‡ä»¤ï¼Œè€Œä¸”èƒ½å¤Ÿé«˜æ•ˆåœ°æ‰§è¡Œè¿™äº›æŒ‡ä»¤ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OpenPALä¸ºæ„å»ºå¼€æ”¾å¼çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå…¶å¤šæ­¥å¾®è°ƒå’ŒååŒè®­ç»ƒçš„è®¾è®¡å€¼å¾—å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒOpenPALåœ¨Contraä¸Šçš„æˆåŠŸåº”ç”¨è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äººæœºäº¤äº’ä¸­å…·æœ‰æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>94</th>
      <td>OMNI: Open-endedness via Models of human Notions of Interestingness</td>
      <td>Open-ended algorithms aim to learn new, interesting behaviors forever. That<br>requires a vast environment search space, but there are thus infinitely many<br>possible tasks. Even after filtering for tasks the current agent can learn<br>(i.e., learning progress), countless learnable yet uninteresting tasks remain<br>(e.g., minor variations of previously learned tasks). An Achilles Heel of<br>open-endedness research is the inability to quantify (and thus prioritize)<br>tasks that are not just learnable, but also \( \textit{interesting} \) (e.g.,<br>worthwhile and novel). We propose solving this problem by<br>\( \textit{Open-endedness via Models of human Notions of Interestingness} \)<br>(OMNI). The insight is that we can utilize foundation models (FMs) as a model<br>of interestingness (MoI), because they \( \textit{already} \) internalize human<br>concepts of interestingness from training on vast amounts of human-generated<br>data, where humans naturally write about what they find interesting or boring.<br>We show that FM-based MoIs improve open-ended learning by focusing on tasks<br>that are both learnable \( \textit{and interesting} \), outperforming baselines<br>based on uniform task sampling or learning progress alone. This approach has<br>the potential to dramatically advance the ability to intelligently select which<br>tasks to focus on next (i.e., auto-curricula), and could be seen as AI<br>selecting its own next task to learn, facilitating self-improving AI and<br>AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | OMNIï¼šåŸºäºäººç±»å…´è¶£æ¦‚å¿µçš„å¼€æ”¾æ€§å­¦ä¹ <br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼€æ”¾æ€§å­¦ä¹ ç®—æ³•æ—¨åœ¨è®©AIåƒäººç±»ä¸€æ ·ï¼Œåœ¨å¤æ‚å¤šå˜çš„ç¯å¢ƒä¸­ä¸æ–­å­¦ä¹ æ–°æŠ€èƒ½ã€‚ç„¶è€Œï¼Œé¢å¯¹æ— é™å¯èƒ½çš„å­¦ä¹ ä»»åŠ¡ï¼Œå¦‚ä½•é€‰æ‹©å“ªäº›ä»»åŠ¡è¿›è¡Œå­¦ä¹ æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚åŸºäºå­¦ä¹ è¿›åº¦çš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼Œå®¹æ˜“é™·å…¥é‡å¤æˆ–æ— è¶£çš„ä»»åŠ¡ä¸­ï¼Œæ— æ³•æœ‰æ•ˆå¼•å¯¼AIè¿›è¡Œæœ‰æ„ä¹‰çš„å­¦ä¹ ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šOMNIæ¡†æ¶<br>OMNIæ¡†æ¶é€šè¿‡ç»“åˆå­¦ä¹ è¿›åº¦æ¨¡å‹å’Œäººç±»å…´è¶£æ¨¡å‹ï¼Œå®ç°äº†å¯¹å­¦ä¹ ä»»åŠ¡çš„æ™ºèƒ½ç­›é€‰ã€‚å­¦ä¹ è¿›åº¦æ¨¡å‹è´Ÿè´£è¯†åˆ«å½“å‰AIèƒ½å¤Ÿå­¦ä¹ çš„ä»»åŠ¡ï¼Œè€Œäººç±»å…´è¶£æ¨¡å‹åˆ™è´Ÿè´£è¯„ä¼°è¿™äº›ä»»åŠ¡æ˜¯å¦æœ‰è¶£ï¼Œä»è€Œç¡®ä¿AIèƒ½å¤Ÿä¸“æ³¨äºæœ‰æ„ä¹‰çš„å­¦ä¹ ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹<br>OMNIæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT-3å’ŒGPT-4ï¼‰ä½œä¸ºäººç±»å…´è¶£æ¨¡å‹ï¼Œä»è€Œé¿å…äº†æ‰‹åŠ¨å®šä¹‰å…´è¶£æŒ‡æ ‡çš„å›°éš¾ã€‚è¿™äº›åŸºç¡€æ¨¡å‹åœ¨å¤§é‡äººç±»ç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå·²ç»å†…åŒ–äº†äººç±»å¯¹æœ‰è¶£äº‹ç‰©çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°ä»»åŠ¡çš„æœ‰è¶£ç¨‹åº¦ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>OMNIæ¡†æ¶åœ¨Crafterã€BabyAIå’ŒAI2-THORç­‰ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸åŸºäºå‡åŒ€ä»»åŠ¡é‡‡æ ·æˆ–ä»…åŸºäºå­¦ä¹ è¿›åº¦çš„æ–¹æ³•ç›¸æ¯”ï¼ŒOMNIæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æé«˜AIçš„å­¦ä¹ æ•ˆç‡ï¼Œå­¦ä¹ æ›´å¤šæœ‰è¶£ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OMNIæ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ ç®—æ³•æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚ï¼š<br>* **å¤šæ¨¡æ€æ¨¡å‹**ï¼šå°†è§†è§‰-è¯­è¨€æ¨¡å‹ç­‰èå…¥äººç±»å…´è¶£æ¨¡å‹ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°ä»»åŠ¡çš„æœ‰è¶£ç¨‹åº¦ã€‚<br>* **è‡ªä¸»å­¦ä¹ **ï¼šè®©äººç±»å…´è¶£æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»åˆ†æä»»åŠ¡æˆåŠŸç‡ç­‰æŒ‡æ ‡ï¼Œå¹¶æ®æ­¤è°ƒæ•´å¯¹æœ‰è¶£ç¨‹åº¦çš„è¯„ä¼°ã€‚<br>* **å®‰å…¨æ€§ä¸ä»·å€¼å¯¹é½**ï¼šé€šè¿‡å¼•å…¥äººç±»åé¦ˆæˆ–AIåé¦ˆï¼Œç¡®ä¿OMNIæ¡†æ¶èƒ½å¤Ÿé€‰æ‹©ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›çš„ä»»åŠ¡è¿›è¡Œå­¦ä¹ ã€‚<br><br>## ğŸŒˆ æœªæ¥å±•æœ›<br>OMNIæ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ ç®—æ³•çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ï¼Œæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š<br>* **æ›´å¤æ‚çš„ä»»åŠ¡ç©ºé—´**ï¼šå°†OMNIæ¡†æ¶åº”ç”¨äºæ›´å¤æ‚çš„ä»»åŠ¡ç©ºé—´ï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®ç¯å¢ƒæˆ–çœŸå®ä¸–ç•Œç¯å¢ƒã€‚<br>* **æ›´æ™ºèƒ½çš„äººç±»å…´è¶£æ¨¡å‹**ï¼šå¼€å‘æ›´æ™ºèƒ½çš„äººç±»å…´è¶£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»å¯¹æœ‰è¶£äº‹ç‰©çš„æ¦‚å¿µï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®AIçš„å­¦ä¹ è¿›åº¦åŠ¨æ€è°ƒæ•´è¯„ä¼°æ ‡å‡†ã€‚<br>* **å®‰å…¨æ€§ä¸ä»·å€¼å¯¹é½**ï¼šæ¢ç´¢æ›´æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç¡®ä¿OMNIæ¡†æ¶èƒ½å¤Ÿé€‰æ‹©ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›çš„ä»»åŠ¡è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œé¿å…æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚</td>
    </tr>
    <tr>
      <th>36</th>
      <td>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception</td>
      <td>It is a long-lasting goal to design an embodied system that can solve<br>long-horizon open-world tasks in human-like ways. However, existing approaches<br>usually struggle with compound difficulties caused by the logic-aware<br>decomposition and context-aware execution of these tasks. To this end, we<br>introduce MP5, an open-ended multimodal embodied system built upon the<br>challenging Minecraft simulator, which can decompose feasible sub-objectives,<br>design sophisticated situation-aware plans, and perform embodied action<br>control, with frequent communication with a goal-conditioned active perception<br>scheme. Specifically, MP5 is developed on top of recent advances in Multimodal<br>Large Language Models (MLLMs), and the system is modulated into functional<br>modules that can be scheduled and collaborated to ultimately solve pre-defined<br>context- and process-dependent tasks. Extensive experiments prove that MP5 can<br>achieve a 22% success rate on difficult process-dependent tasks and a 91%<br>success rate on tasks that heavily depend on the context. Moreover, MP5<br>exhibits a remarkable ability to address many open-ended tasks that are<br>entirely novel.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MP5ï¼šåŸºäºä¸»åŠ¨æ„ŸçŸ¥çš„å¤šæ¨¡æ€å¼€æ”¾å¼å…·èº«ç³»ç»Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä»¥äººç±»æ–¹å¼è§£å†³å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡çš„å…·èº«ç³»ç»Ÿä¸€ç›´æ˜¯é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éš¾ä»¥åº”å¯¹è¿™äº›ä»»åŠ¡ä¸­é€»è¾‘æ„ŸçŸ¥åˆ†è§£å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‰§è¡Œæ‰€å¸¦æ¥çš„å¤åˆå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MP5ï¼Œä¸€ä¸ªåŸºäºMinecraftæ¨¡æ‹Ÿå™¨çš„å¼€æ”¾å¼å¤šæ¨¡æ€å…·èº«ç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿåˆ†è§£å¯è¡Œçš„å­ç›®æ ‡ï¼Œè®¾è®¡å¤æ‚çš„æƒ…å¢ƒæ„ŸçŸ¥è®¡åˆ’ï¼Œå¹¶æ‰§è¡Œå…·èº«åŠ¨ä½œæ§åˆ¶ï¼ŒåŒæ—¶ä¸ç›®æ ‡æ¡ä»¶ä¸‹çš„ä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆè¿›è¡Œé¢‘ç¹é€šä¿¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMP5åŸºäºæœ€æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºï¼Œå¹¶å°†ç³»ç»Ÿåˆ†è§£ä¸ºå¯è°ƒåº¦å’Œåä½œçš„åŠŸèƒ½æ¨¡å—ï¼Œä»¥è§£å†³é¢„å®šä¹‰çš„ä¸Šä¸‹æ–‡å’Œè¿‡ç¨‹ä¾èµ–ä»»åŠ¡ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMP5åŒ…æ‹¬ä¸€ä¸ªä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆï¼Œé€šè¿‡æ„ŸçŸ¥å™¨ä¸å·¡é€»å™¨ä¹‹é—´çš„å¤šè½®äº¤äº’ï¼Œä¸»åŠ¨æ„ŸçŸ¥è§‚å¯Ÿåˆ°çš„å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡ä¾èµ–ä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MP5åœ¨å›°éš¾çš„è¿‡ç¨‹ä¾èµ–ä»»åŠ¡ä¸Šå®ç°äº†22%çš„æˆåŠŸç‡ï¼Œåœ¨é«˜åº¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ä¸Šå®ç°äº†91%çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒMP5è¡¨ç°å‡ºè§£å†³è®¸å¤šå®Œå…¨æ–°é¢–çš„å¼€æ”¾å¼ä»»åŠ¡çš„èƒ½åŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MP5çš„è®¾è®¡å’Œå®ç°ä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶ä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆå’Œæ¨¡å—åŒ–è®¾è®¡å¯¹äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„å…·èº«ç³»ç»Ÿå…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>105</th>
      <td>ArK: Augmented Reality with Knowledge Interactive Emergent Ability</td>
      <td>Despite the growing adoption of mixed reality and interactive AI agents, it<br>remains challenging for these systems to generate high quality 2D/3D scenes in<br>unseen environments. The common practice requires deploying an AI agent to<br>collect large amounts of data for model training for every new task. This<br>process is costly, or even impossible, for many domains. In this study, we<br>develop an infinite agent that learns to transfer knowledge memory from general<br>foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene<br>understanding and generation in the physical or virtual world. The heart of our<br>approach is an emerging mechanism, dubbed Augmented Reality with Knowledge<br>Inference Interaction (ArK), which leverages knowledge-memory to generate<br>scenes in unseen physical world and virtual reality environments. The knowledge<br>interactive emergent ability (Figure 1) is demonstrated as the observation<br>learns i) micro-action of cross-modality: in multi-modality models to collect a<br>large amount of relevant knowledge memory data for each interaction task (e.g.,<br>unseen scene understanding) from the physical reality; and ii) macro-behavior<br>of reality-agnostic: in mix-reality environments to improve interactions that<br>tailor to different characterized roles, target variables, collaborative<br>information, and so on. We validate the effectiveness of ArK on the scene<br>generation and editing tasks. We show that our ArK approach, combined with<br>large foundation models, significantly improves the quality of generated 2D/3D<br>scenes, compared to baselines, demonstrating the potential benefit of<br>incorporating ArK in generative AI for applications such as metaverse and<br>gaming simulation.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ArKï¼šåŸºäºçŸ¥è¯†çš„å¢å¼ºç°å®äº¤äº’å¼æ¶Œç°èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æ··åˆç°å®å’Œäº¤äº’å¼AIä»£ç†çš„æ—¥ç›Šæ™®åŠï¼Œè¿™äº›ç³»ç»Ÿåœ¨ç”Ÿæˆé«˜è´¨é‡2D/3Dåœºæ™¯æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åšæ³•éœ€è¦éƒ¨ç½²AIä»£ç†æ¥æ”¶é›†å¤§é‡æ•°æ®ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¿™å¯¹äºè®¸å¤šé¢†åŸŸæ¥è¯´æ—¢æ˜‚è´µåˆä¸å¯èƒ½ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œé€šè¿‡å¼€å‘ä¸€ä¸ªæ— é™ä»£ç†ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿä»é€šç”¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT4ã€DALLEï¼‰ä¸­å­¦ä¹ ï¼Œå¹¶å°†çŸ¥è¯†è®°å¿†è½¬ç§»åˆ°æ–°é¢†åŸŸæˆ–åœºæ™¯ä¸­ï¼Œä»¥å®ç°ç‰©ç†æˆ–è™šæ‹Ÿä¸–ç•Œä¸­çš„åœºæ™¯ç†è§£å’Œç”Ÿæˆã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šArKæœºåˆ¶<br>æœ¬æ–‡çš„æ ¸å¿ƒæ˜¯ArKæœºåˆ¶ï¼Œå³â€œå¢å¼ºç°å®ä¸çŸ¥è¯†æ¨ç†äº¤äº’â€ã€‚è¯¥æœºåˆ¶åˆ©ç”¨çŸ¥è¯†è®°å¿†æ¥ç”Ÿæˆæœªè§è¿‡çš„ç‰©ç†ä¸–ç•Œå’Œè™šæ‹Ÿç°å®ç¯å¢ƒä¸­çš„åœºæ™¯ã€‚ArKæœºåˆ¶é€šè¿‡è§‚å¯Ÿå­¦ä¹ å®ç°ï¼š<br>- è·¨æ¨¡æ€çš„å¾®åŠ¨ä½œï¼šåœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­ï¼Œä¸ºæ¯ä¸ªäº¤äº’ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œæœªè§è¿‡çš„åœºæ™¯ç†è§£ï¼‰ä»ç‰©ç†ç°å®ä¸­æ”¶é›†å¤§é‡ç›¸å…³çš„çŸ¥è¯†è®°å¿†æ•°æ®ã€‚<br>- ç°å®æ— å…³çš„å®è¡Œä¸ºï¼šåœ¨æ··åˆç°å®ç¯å¢ƒä¸­ï¼Œé€šè¿‡è°ƒæ•´äº¤äº’ä»¥é€‚åº”ä¸åŒçš„è§’è‰²ã€ç›®æ ‡å˜é‡ã€åä½œä¿¡æ¯ç­‰ï¼Œä»è€Œæé«˜äº¤äº’æ•ˆæœã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— é™çŸ¥è¯†è®°å¿†ä»£ç†<br>æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªæ— é™çŸ¥è¯†è®°å¿†ä»£ç†ï¼Œç”¨äºç‰©ç†ä¸–ç•Œä¸­çš„åœºæ™¯ç†è§£å’Œç”Ÿæˆã€‚è¯¥ä»£ç†å­¦ä¹ ä»é€šç”¨åŸºç¡€æ¨¡å‹ä¸­è½¬ç§»çŸ¥è¯†è®°å¿†ï¼Œå¹¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆæœªè§è¿‡çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥ä»£ç†è¿˜èƒ½å¤Ÿé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ æ¥æé«˜å…¶æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨åœºæ™¯ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸ŠéªŒè¯äº†ArKçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒArKæ–¹æ³•ç»“åˆå¤§å‹åŸºç¡€æ¨¡å‹æ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„2D/3Dåœºæ™¯çš„è´¨é‡ï¼Œè¯æ˜äº†å°†ArKçº³å…¥ç”Ÿæˆå¼AIä¸­çš„æ½œåœ¨ç›Šå¤„ï¼Œä¾‹å¦‚åœ¨å…ƒå®‡å®™å’Œæ¸¸æˆæ¨¡æ‹Ÿä¸­çš„åº”ç”¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ArKæœºåˆ¶å’Œæ— é™çŸ¥è¯†è®°å¿†ä»£ç†ä¸ºç”Ÿæˆå¼AIé¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚ArKæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨çŸ¥è¯†è®°å¿†æ¥ç”Ÿæˆé«˜è´¨é‡çš„2D/3Dåœºæ™¯ï¼Œè€Œæ— é™çŸ¥è¯†è®°å¿†ä»£ç†åˆ™èƒ½å¤Ÿé€šè¿‡å­¦ä¹ å’Œæ¨ç†æ¥æé«˜å…¶æ€§èƒ½ã€‚è¿™äº›åˆ›æ–°ç‚¹ä¸ºç”Ÿæˆå¼AIçš„å‘å±•å’Œåº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>120</th>
      <td>Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors</td>
      <td>Video game testing requires game-specific knowledge as well as common sense<br>reasoning about the events in the game. While AI-driven agents can satisfy the<br>first requirement, it is not yet possible to meet the second requirement<br>automatically. Therefore, video game testing often still relies on manual<br>testing, and human testers are required to play the game thoroughly to detect<br>bugs. As a result, it is challenging to fully automate game testing. In this<br>study, we explore the possibility of leveraging the zero-shot capabilities of<br>large language models for video game bug detection. By formulating the bug<br>detection problem as a question-answering task, we show that large language<br>models can identify which event is buggy in a sequence of textual descriptions<br>of events from a game. To this end, we introduce the GameBugDescriptions<br>benchmark dataset, which consists of 167 buggy gameplay videos and a total of<br>334 question-answer pairs across 8 games. We extensively evaluate the<br>performance of six models across the OPT and InstructGPT large language model<br>families on our benchmark dataset. Our results show promising results for<br>employing language models to detect video game bugs. With the proper prompting<br>technique, we could achieve an accuracy of 70.66%, and on some video games, up<br>to 78.94%. Our code, evaluation data and the benchmark can be found on<br>https://asgaardlab.github.io/LLMxBugs</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹ä¸­çš„æ½œåŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è§†é¢‘æ¸¸æˆæµ‹è¯•éœ€è¦æ¸¸æˆç‰¹å®šçš„çŸ¥è¯†å’Œå¯¹æ¸¸æˆäº‹ä»¶çš„å¸¸è¯†æ¨ç†ã€‚è™½ç„¶ AI é©±åŠ¨çš„ä»£ç†å¯ä»¥æ»¡è¶³ç¬¬ä¸€ä¸ªè¦æ±‚ï¼Œä½†è‡ªåŠ¨æ»¡è¶³ç¬¬äºŒä¸ªè¦æ±‚ä»ç„¶ä¸å¯èƒ½ã€‚å› æ­¤ï¼Œè§†é¢‘æ¸¸æˆæµ‹è¯•é€šå¸¸ä»ç„¶ä¾èµ–äºæ‰‹åŠ¨æµ‹è¯•ï¼Œéœ€è¦äººç±»æµ‹è¯•è€…å½»åº•åœ°ç©æ¸¸æˆæ¥æ£€æµ‹æ¼æ´ã€‚è¿™ä½¿å¾—å®Œå…¨è‡ªåŠ¨åŒ–æ¸¸æˆæµ‹è¯•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ¼æ´æ£€æµ‹é—®é¢˜è¡¨è¿°ä¸ºé—®ç­”ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›æ¥è¯†åˆ«æ¸¸æˆäº‹ä»¶åºåˆ—ä¸­çš„æ¼æ´äº‹ä»¶ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥ GameBugDescriptions åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å« 167 ä¸ªæœ‰æ¼æ´çš„æ¸¸æˆç©æ³•è§†é¢‘å’Œ 334 ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›– 8 ä¸ªæ¸¸æˆã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº† OPT å’Œ InstructGPT å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—çš„å…­ä¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ†æäº†è¯­è¨€æ¨¡å‹å¯¹ä¸åŒäº‹ä»¶æè¿°çš„é²æ£’æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹æ–¹é¢å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚é€šè¿‡é€‚å½“çš„æç¤ºæŠ€æœ¯ï¼Œå¯ä»¥å®ç° 70.66% çš„å‡†ç¡®ç‡ï¼Œåœ¨æŸäº›è§†é¢‘æ¸¸æˆä¸­ç”šè‡³å¯ä»¥è¾¾åˆ° 78.94%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–æ¸¸æˆæµ‹è¯•æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä¸­æå‡ºçš„ GameBugDescriptions åŸºå‡†æ•°æ®é›†å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒè¯­è¨€æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>118</th>
      <td>Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models</td>
      <td>In recent years, much progress has been made in learning robotic manipulation<br>policies that follow natural language instructions. Such methods typically<br>learn from corpora of robot-language data that was either collected with<br>specific tasks in mind or expensively re-labelled by humans with rich language<br>descriptions in hindsight. Recently, large-scale pretrained vision-language<br>models (VLMs) like CLIP or ViLD have been applied to robotics for learning<br>representations and scene descriptors. Can these pretrained models serve as<br>automatic labelers for robot data, effectively importing Internet-scale<br>knowledge into existing datasets to make them useful even for tasks that are<br>not reflected in their ground truth annotations? To accomplish this, we<br>introduce Data-driven Instruction Augmentation for Language-conditioned control<br>(DIAL): we utilize semi-supervised language labels leveraging the semantic<br>understanding of CLIP to propagate knowledge onto large datasets of unlabelled<br>demonstration data and then train language-conditioned policies on the<br>augmented datasets. This method enables cheaper acquisition of useful language<br>descriptions compared to expensive human labels, allowing for more efficient<br>label coverage of large-scale datasets. We apply DIAL to a challenging<br>real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations<br>do not contain crowd-sourced language annotations. DIAL enables imitation<br>learning policies to acquire new capabilities and generalize to 60 novel<br>instructions unseen in the original dataset.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºæŒ‡ä»¤ï¼Œå®ç°æœºå™¨äººæŠ€èƒ½è·å–<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œæœºå™¨äººæ§åˆ¶ç­–ç•¥å·²ç»èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œå­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡æ ‡æ³¨è¿‡çš„æœºå™¨äººè¯­è¨€æ•°æ®ï¼Œè¿™äº›æ•°æ®è¦ä¹ˆæ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡æ”¶é›†çš„ï¼Œè¦ä¹ˆæ˜¯äº‹åç”±äººç±»è¿›è¡Œæ˜‚è´µæ ‡æ³¨çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDIALï¼ˆData-driven Instruction Augmentation for Language-conditioned controlï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è‡ªåŠ¨ä¸ºæœºå™¨äººæ•°æ®æ·»åŠ æ ‡ç­¾ï¼Œä»è€Œå°†äº’è”ç½‘è§„æ¨¡çš„çŸ¥è¯†å¼•å…¥ç°æœ‰æ•°æ®é›†ï¼Œä½¿å…¶å³ä½¿åœ¨çœŸå®æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ç”¨äºå„ç§ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„VLMè¿›è¡ŒæŒ‡ä»¤å¢å¼º<br>DIALæ–¹æ³•é¦–å…ˆåœ¨åŒ…å«å°‘é‡äººå·¥æ ‡æ³¨çš„è‡ªç„¶è¯­è¨€æè¿°çš„å°å‹æ•°æ®é›†ä¸Šå¾®è°ƒVLMï¼Œç„¶åä½¿ç”¨å¾®è°ƒåçš„VLMä¸ºæ›´å¤§çš„æœªæ ‡æ³¨æ¼”ç¤ºæ•°æ®é›†ç”Ÿæˆæ›¿ä»£æŒ‡ä»¤ã€‚è¿™äº›æŒ‡ä»¤å¯ä»¥åŒ…å«æ›´ä¸°å¯Œçš„è¯­ä¹‰æ¦‚å¿µï¼Œä¾‹å¦‚ç©ºé—´æ¦‚å¿µæˆ–ä¸åŒçš„è¡¨è¿°æ–¹å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºè¡Œä¸ºå…‹éš†çš„è¯­è¨€æ¡ä»¶ç­–ç•¥è®­ç»ƒ<br>åœ¨ç”Ÿæˆæ›¿ä»£æŒ‡ä»¤åï¼ŒDIALæ–¹æ³•ä½¿ç”¨è¡Œä¸ºå…‹éš†åœ¨åŸå§‹å’Œé‡æ–°æ ‡æ³¨çš„æ•°æ®é›†ä¸Šè®­ç»ƒè¯­è¨€æ¡ä»¶ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°è¦†ç›–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä½¿æœºå™¨äººèƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œæ–°çš„æŒ‡ä»¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œé¢†åŸŸï¼ŒDIALæ–¹æ³•åœ¨80,000ä¸ªæ¼”ç¤ºä¸­ï¼Œåªæœ‰3.5%åŒ…å«ä¼—åŒ…è¯­è¨€æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¤§è§„æ¨¡ç ”ç©¶è¶…è¿‡1,300æ¬¡çœŸå®ä¸–ç•Œè¯„ä¼°ï¼Œå‘ç°DIALä½¿æ¨¡ä»¿å­¦ä¹ ç­–ç•¥èƒ½å¤Ÿè·å¾—æ–°çš„èƒ½åŠ›ï¼Œå¹¶æ¨å¹¿åˆ°60ä¸ªåŸå§‹æ•°æ®é›†ä¸­æœªè§çš„å…¨æ–°æŒ‡ä»¤ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DIALæ–¹æ³•æä¾›äº†ä¸€ç§å»‰ä»·ä¸”è‡ªåŠ¨åŒ–çš„é€‰é¡¹ï¼Œå¯ä»¥ä»ç¦»çº¿æ§åˆ¶æ•°æ®é›†ä¸­æå–é¢å¤–çš„è¯­ä¹‰çŸ¥è¯†ã€‚å®ƒé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„VLMè‡ªåŠ¨ä¸ºæœºå™¨äººæ•°æ®æ·»åŠ æ ‡ç­¾ï¼Œä»è€Œå°†äº’è”ç½‘è§„æ¨¡çš„çŸ¥è¯†å¼•å…¥ç°æœ‰æ•°æ®é›†ï¼Œä½¿å…¶å³ä½¿åœ¨çœŸå®æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ç”¨äºå„ç§ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°è¦†ç›–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä½¿æœºå™¨äººèƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œæ–°çš„æŒ‡ä»¤ã€‚</td>
    </tr>
    <tr>
      <th>2</th>
      <td>VideoGameBunny: Towards vision assistants for video games</td>
      <td>Large multimodal models (LMMs) hold substantial promise across various<br>domains, from personal assistance in daily tasks to sophisticated applications<br>like medical diagnostics. However, their capabilities have limitations in the<br>video game domain, such as challenges with scene understanding, hallucinations,<br>and inaccurate descriptions of video game content, especially in open-source<br>models. This paper describes the development of VideoGameBunny, a LLaVA-style<br>model based on Bunny, specifically tailored for understanding images from video<br>games. We release intermediate checkpoints, training logs, and an extensive<br>dataset comprising 185,259 video game images from 413 titles, along with<br>389,565 image-instruction pairs that include image captions, question-answer<br>pairs, and a JSON representation of 16 elements of 136,974 images. Our<br>experiments show that our high quality game-related data has the potential to<br>make a relatively small model outperform the much larger state-of-the-art model<br>LLaVa-1.6-34b (which has more than 4x the number of parameters). Our study<br>paves the way for future research in video game understanding on tasks such as<br>playing, commentary, and debugging. Code and data are available at<br>https://videogamebunny.github.io/</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è§†é¢‘æ¸¸æˆåŠ©æ‰‹ï¼šVideoGameBunny<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€è§†é¢‘æ¸¸æˆäº§ä¸šçš„è“¬å‹ƒå‘å±•ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LMMsåœ¨ç†è§£æ¸¸æˆå†…å®¹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚åœºæ™¯ç†è§£å›°éš¾ã€å¹»è§‰ç°è±¡ä»¥åŠæ¸¸æˆå†…å®¹çš„æè¿°ä¸å‡†ç¡®ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡å¼€å‘ä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ¸¸æˆå†…å®¹ç†è§£çš„æ¨¡å‹ï¼Œå³VideoGameBunnyï¼Œæ¥æå‡LMMsåœ¨æ¸¸æˆé¢†åŸŸçš„è¡¨ç°ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šVideoGameBunnyæ¨¡å‹<br>VideoGameBunnyæ˜¯ä¸€ä¸ªåŸºäºBunnyæ¨¡å‹çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»è¿‡ä¸“é—¨é’ˆå¯¹æ¸¸æˆå†…å®¹çš„å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†æ¸¸æˆå›¾åƒã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†LLaVA-styleæ¶æ„ï¼Œé€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å°†è§†è§‰åµŒå…¥ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œä»è€Œå®ç°å›¾åƒå’Œæ–‡æœ¬çš„èåˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¸¸æˆç›¸å…³æ•°æ®é›†<br>ä¸ºäº†è®­ç»ƒVideoGameBunnyï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«185,259å¼ æ¸¸æˆå›¾åƒå’Œ389,565ä¸ªå›¾åƒ-æŒ‡ä»¤å¯¹çš„æ•°æ®é›†ã€‚è¿™äº›å›¾åƒæ¥è‡ª413æ¬¾ä¸åŒçš„æ¸¸æˆï¼Œæ¶µç›–äº†å„ç§æ¸¸æˆç±»å‹ã€å›¾å½¢é£æ ¼å’Œæ¸¸æˆæœºåˆ¶ã€‚å›¾åƒ-æŒ‡ä»¤å¯¹åŒ…æ‹¬å›¾åƒæ ‡é¢˜ã€é—®ç­”å¯¹å’Œå›¾åƒçš„JSONè¡¨ç¤ºï¼Œä¸ºæ¨¡å‹æä¾›äº†ä¸°å¯Œçš„æ¸¸æˆç›¸å…³æ•°æ®ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoGameBunnyåœ¨æ¸¸æˆå†…å®¹ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰çš„SOTAæ¨¡å‹LLaVa-1.6-34bç›¸æ¯”ï¼ŒVideoGameBunnyåœ¨æ¸¸æˆç›¸å…³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œå›¾åƒ-to-JSONæ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡æœ€ä¸ºæ˜¾è‘—ï¼Œè€ŒçŸ­æ ‡é¢˜æ•°æ®é›†åˆ™å¯èƒ½å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>VideoGameBunnyçš„ç ”ç©¶æˆæœä¸ºæ¸¸æˆå†…å®¹ç†è§£é¢†åŸŸæä¾›äº†é‡è¦çš„å‚è€ƒã€‚é€šè¿‡æ„å»ºä¸“é—¨é’ˆå¯¹æ¸¸æˆå†…å®¹çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æå‡LMMsåœ¨æ¸¸æˆé¢†åŸŸçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ä¸åŒç±»å‹çš„æ•°æ®é›†å’Œæ··åˆç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚</td>
    </tr>
    <tr>
      <th>114</th>
      <td>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</td>
      <td>We investigate the challenge of task planning for multi-task embodied agents<br>in open-world environments. Two main difficulties are identified: 1) executing<br>plans in an open-world environment (e.g., Minecraft) necessitates accurate and<br>multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla<br>planners do not consider how easy the current agent can achieve a given<br>sub-task when ordering parallel sub-goals within a complicated plan, the<br>resulting plan could be inefficient or even infeasible. To this end, we propose<br>"\( \underline{D} \)escribe, \( \underline{E} \)xplain, \( \underline{P} \)lan and<br>\( \underline{S} \)elect" (\( \textbf{DEPS} \)), an interactive planning approach based<br>on Large Language Models (LLMs). DEPS facilitates better error correction on<br>initial LLM-generated \( \textit{plan} \) by integrating \( \textit{description} \) of<br>the plan execution process and providing self-\( \textit{explanation} \) of<br>feedback when encountering failures during the extended planning phases.<br>Furthermore, it includes a goal \( \textit{selector} \), which is a trainable<br>module that ranks parallel candidate sub-goals based on the estimated steps of<br>completion, consequently refining the initial plan. Our experiments mark the<br>milestone of the first zero-shot multi-task agent that can robustly accomplish<br>70+ Minecraft tasks and nearly double the overall performances. Further testing<br>reveals our method's general effectiveness in popularly adopted non-open-ended<br>domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and<br>exploratory studies detail how our design beats the counterparts and provide a<br>promising update on the \( \texttt{ObtainDiamond} \) grand challenge with our<br>approach. The code is released at https://github.com/CraftJarvis/MC-Planner.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å¼è§„åˆ’ï¼ŒåŠ©åŠ›å¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¤šä»»åŠ¡æ™ºèƒ½ä½“é¢ä¸´ç€ä¸¤å¤§æŒ‘æˆ˜ï¼š1ï¼‰æ‰§è¡Œè®¡åˆ’éœ€è¦ç²¾ç¡®çš„å¤šæ­¥æ¨ç†ï¼Œå› ä¸ºä»»åŠ¡å…·æœ‰é•¿æœŸæ€§ï¼›2ï¼‰ä¼ ç»Ÿçš„è§„åˆ’å™¨åœ¨æ’åºå¤æ‚çš„è®¡åˆ’ä¸­çš„å¹¶è¡Œå­ç›®æ ‡æ—¶ï¼Œæ²¡æœ‰è€ƒè™‘å½“å‰æ™ºèƒ½ä½“å®Œæˆç»™å®šå­ä»»åŠ¡çš„éš¾æ˜“ç¨‹åº¦ï¼Œå¯¼è‡´ç”Ÿæˆçš„è®¡åˆ’å¯èƒ½æ•ˆç‡ä½ä¸‹ç”šè‡³ä¸å¯è¡Œã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œæè¿°ã€è§£é‡Šã€è§„åˆ’å’Œé€‰æ‹©â€ï¼ˆDEPSï¼‰çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæè¿°ã€è§£é‡Šå’Œè§„åˆ’<br>DEPS é€šè¿‡é›†æˆè®¡åˆ’æ‰§è¡Œè¿‡ç¨‹çš„æè¿°å’Œæä¾›è‡ªæˆ‘è§£é‡Šçš„åé¦ˆï¼Œæ›´å¥½åœ°çº æ­£åˆå§‹ LLM ç”Ÿæˆçš„è®¡åˆ’ä¸­çš„é”™è¯¯ã€‚å½“é‡åˆ°å¤±è´¥æ—¶ï¼Œæè¿°å™¨ä¼šæ€»ç»“å½“å‰æƒ…å†µå¹¶å‘é€ç»™ LLMï¼ŒLLM ä½œä¸ºè§£é‡Šå™¨å®šä½é”™è¯¯ï¼Œç„¶åæ ¹æ®æè¿°å™¨å’Œè§£é‡Šå™¨çš„ä¿¡æ¯æ›´æ–°è®¡åˆ’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç›®æ ‡é€‰æ‹©å™¨<br>DEPS åŒ…å«ä¸€ä¸ªå¯è®­ç»ƒçš„ç›®æ ‡é€‰æ‹©å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®å®Œæˆæ¯ä¸ªå¹¶è¡Œå€™é€‰å­ç›®æ ‡çš„ä¼°è®¡æ­¥éª¤å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œä»è€Œç»†åŒ–åˆå§‹è®¡åˆ’ã€‚é€‰æ‹©å™¨ä½¿ç”¨é¢„æµ‹å‰©ä½™æ—¶é—´æ­¥æ•°æ¥å®Œæˆæ¯ä¸ªç›®æ ‡ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©æœ€æ¥è¿‘çš„ç›®æ ‡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒDEPS åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ Minecraftï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œèƒ½å¤Ÿç¨³å¥åœ°å®Œæˆ 70 å¤šä¸ªä»»åŠ¡ï¼Œå¹¶ä¸”æ•´ä½“æ€§èƒ½å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼ŒDEPS åœ¨éå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ ALFWorld å’Œæ¡Œé¢æ“ä½œï¼‰ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DEPS çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ä¸ºå¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡é›†æˆæè¿°ã€è§£é‡Šå’Œè§„åˆ’ï¼Œä»¥åŠä½¿ç”¨ç›®æ ‡é€‰æ‹©å™¨ï¼ŒDEPS èƒ½å¤Ÿç”Ÿæˆæ›´å¯é å’Œé«˜æ•ˆçš„è®¡åˆ’ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>79</th>
      <td>Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis</td>
      <td>Since the introduction of ChatGPT and GPT-4, these models have been tested<br>across a large number of tasks. Their adeptness across domains is evident, but<br>their aptitude in playing games, and specifically their aptitude in the realm<br>of poker has remained unexplored. Poker is a game that requires decision making<br>under uncertainty and incomplete information. In this paper, we put ChatGPT and<br>GPT-4 through the poker test and evaluate their poker skills. Our findings<br>reveal that while both models display an advanced understanding of poker,<br>encompassing concepts like the valuation of starting hands, playing positions<br>and other intricacies of game theory optimal (GTO) poker, both ChatGPT and<br>GPT-4 are NOT game theory optimal poker players.<br>  Profitable strategies in poker are evaluated in expectations over large<br>samples. Through a series of experiments, we first discover the characteristics<br>of optimal prompts and model parameters for playing poker with these models.<br>Our observations then unveil the distinct playing personas of the two models.<br>We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This<br>exploration then sheds light on the divergent poker tactics of the two models:<br>ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker<br>vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which<br>means that it has a propensity to only engage with premium hands and folds a<br>majority of hands. When subjected to the same directive, GPT-4 plays like a<br>maniac, showcasing a loose and aggressive style of play. Both strategies,<br>although relatively advanced, are not game theory optimal.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹ä¸­çš„è¡¨ç°ï¼šä¸€åœºå‰ç¿»ç‰Œåˆ†æ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ ChatGPT å’Œ GPT-4 çš„æ¨å‡ºï¼Œè¿™äº›æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¸¸æˆï¼Œå°¤å…¶æ˜¯æ‰‘å…‹æ¸¸æˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ‰‘å…‹æ˜¯ä¸€ç§éœ€è¦åœ¨ä¸å®Œæ•´ä¿¡æ¯å’Œä¸ç¡®å®šæ€§ä¸‹åšå‡ºå†³ç­–çš„æ¸¸æˆï¼Œå› æ­¤æœ¬æ–‡æ—¨åœ¨è¯„ä¼° ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨å‰ç¿»ç‰Œé˜¶æ®µçš„å†³ç­–èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œè¯„ä¼°äº† ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹å‰ç¿»ç‰Œé˜¶æ®µçš„å†³ç­–èƒ½åŠ›ã€‚å®éªŒä¸­ï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨äº†ä¸åŒçš„æç¤ºå’Œæ¨¡å‹å‚æ•°ï¼Œä»¥æ¢ç´¢ä¸¤ç§æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„æœ€ä½³è¡¨ç°ã€‚ä»–ä»¬è¿˜åˆ†æäº†ä¸¤ç§æ¨¡å‹çš„ç‹¬ç‰¹æ¸¸æˆé£æ ¼ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬ä¸æ¸¸æˆç†è®ºæœ€ä¼˜ï¼ˆGTOï¼‰ç­–ç•¥çš„å·®å¼‚ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒChatGPT å’Œ GPT-4 éƒ½å¯¹æ‰‘å…‹æ¸¸æˆæœ‰æ·±å…¥çš„ç†è§£ï¼ŒåŒ…æ‹¬èµ·æ‰‹ç‰Œçš„ä¼°å€¼ã€æ¸¸æˆä½ç½®å’Œå…¶ä»–æ¸¸æˆç†è®ºæœ€ä¼˜ç­–ç•¥çš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œä¸¤ç§æ¨¡å‹éƒ½ä¸æ˜¯æ¸¸æˆç†è®ºæœ€ä¼˜çš„æ‰‘å…‹ç©å®¶ã€‚ChatGPT å€¾å‘äºä¿å®ˆçš„æ¸¸æˆé£æ ¼ï¼Œåªå‚ä¸ä¼˜è´¨ç‰Œå±€ï¼Œè€Œ GPT-4 åˆ™è¡¨ç°å‡ºæ›´åŠ æ¿€è¿›çš„æ¸¸æˆé£æ ¼ï¼Œå‚ä¸æ›´å¤šçš„ç‰Œå±€ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ ChatGPT å’Œ GPT-4 åœ¨æ‰‘å…‹æ¸¸æˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚è¿™äº›æ¨¡å‹åœ¨ç†è§£æ¸¸æˆç†è®ºæœ€ä¼˜ç­–ç•¥æ–¹é¢å­˜åœ¨åå·®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰é’ˆå¯¹æ‰‘å…‹æ¸¸æˆè¿›è¡Œä¸“é—¨è®­ç»ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼Œä¸åŒçš„æç¤ºå’Œæ¨¡å‹å‚æ•°å¯¹æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°æœ‰æ˜¾è‘—å½±å“ã€‚å› æ­¤ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•é€šè¿‡ä¼˜åŒ–æç¤ºå’Œæ¨¡å‹å‚æ•°æ¥æé«˜æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°ã€‚</td>
    </tr>
    <tr>
      <th>56</th>
      <td>LLaMA Rider: Spurring Large Language Models to Explore the Open World</td>
      <td>Recently, various studies have leveraged Large Language Models (LLMs) to help<br>decision-making and planning in environments, and try to align the LLMs'<br>knowledge with the world conditions. Nonetheless, the capacity of LLMs to<br>continuously acquire environmental knowledge and adapt in an open world remains<br>uncertain. In this paper, we propose an approach to spur LLMs to explore the<br>open world, gather experiences, and learn to improve their task-solving<br>capabilities. In this approach, a multi-round feedback-revision mechanism is<br>utilized to encourage LLMs to actively select appropriate revision actions<br>guided by feedback information from the environment. This facilitates<br>exploration and enhances the model's performance. Besides, we integrate<br>sub-task relabeling to assist LLMs in maintaining consistency in sub-task<br>planning and help the model learn the combinatorial nature between tasks,<br>enabling it to complete a wider range of tasks through training based on the<br>acquired exploration experiences. By evaluation in Minecraft, an open-ended<br>sandbox world, we demonstrate that our approach LLaMA-Rider enhances the<br>efficiency of the LLM in exploring the environment, and effectively improves<br>the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k<br>instances of collected data, showing minimal training costs compared to the<br>baseline using reinforcement learning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | LLaMA Riderï¼šæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹æ¢ç´¢å¼€æ”¾ä¸–ç•Œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è®¸å¤šç ”ç©¶å¼€å§‹åˆ©ç”¨LLMsçš„èƒ½åŠ›æ¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ï¼Œå¹¶å‘ç°LLMså…·æœ‰ä¸€å®šçš„è§„åˆ’å’Œå®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMsçš„çŸ¥è¯†æ¥æºäºé¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯­è¨€è¯­æ–™åº“ï¼Œå¯èƒ½ä¸ç‰¹å®šç¯å¢ƒå­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä¸€äº›ç ”ç©¶é€šè¿‡æç¤ºå·¥ç¨‹è®¾è®¡ç‰¹å®šæœºåˆ¶ï¼Œä¸ºLLMsæä¾›ç¯å¢ƒä¿¡æ¯ã€‚ç„¶è€Œï¼ŒLLMsåœ¨ç¯å¢ƒä¸­å¹¶ä¸ä¼šæ”¹è¿›æˆ–è·å–æ–°çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦æ›´å¤æ‚çš„æœºåˆ¶å’Œæç¤ºï¼Œè¿™ä¼šå¯¼è‡´LLMsç”Ÿæˆæˆæœ¬é«˜ï¼Œå¹¶ä¸”ä¾èµ–äºåƒGPT-4è¿™æ ·å…·æœ‰è¶³å¤ŸçŸ¥è¯†çš„å¼ºå¤§æ¨¡å‹ã€‚è¿˜æœ‰ä¸€äº›ç ”ç©¶é€šè¿‡å¾®è°ƒæ¥å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä½†è¿™é€šå¸¸éœ€è¦ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®é›†ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¹Ÿè¢«ç ”ç©¶ï¼Œä½†è¿™äº›æ–¹æ³•å°†LLMsè®­ç»ƒä¸ºç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°RLæ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹æˆ–æ›´å¤æ‚çš„ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLaMA-Riderçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡LLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢æ¥å¢å¼ºå…¶èƒ½åŠ›ã€‚LLaMA-Rideræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æ¢ç´¢é˜¶æ®µå’Œå­¦ä¹ é˜¶æ®µã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢é˜¶æ®µ<br>åœ¨æ¢ç´¢é˜¶æ®µï¼ŒLLaMA-Rideråˆ©ç”¨åé¦ˆ-ä¿®æ­£æœºåˆ¶æ¥é¼“åŠ±LLMsä¸»åŠ¨é€‰æ‹©é€‚å½“çš„ä¿®æ­£åŠ¨ä½œï¼Œä»¥é€‚åº”ç¯å¢ƒã€‚LLMsåœ¨ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢ï¼Œæ”¶é›†ç»éªŒï¼Œå¹¶é€šè¿‡åé¦ˆä¿¡æ¯æ¥æ”¹è¿›å…¶å†³ç­–ã€‚æ­¤å¤–ï¼ŒLLaMA-Riderè¿˜ä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥å¸®åŠ©LLMsä¿æŒå­ä»»åŠ¡è§„åˆ’çš„è¿è´¯æ€§ï¼Œå¹¶å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§è´¨ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå­¦ä¹ é˜¶æ®µ<br>åœ¨å­¦ä¹ é˜¶æ®µï¼ŒLLaMA-Riderå°†æ”¶é›†åˆ°çš„ç»éªŒå¤„ç†æˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è®­ç»ƒLLMsã€‚é™¤äº†ä»æˆåŠŸä»»åŠ¡ä¸­è·å¾—çš„ç»éªŒå¤–ï¼ŒLLaMA-Riderè¿˜æ”¶é›†éƒ¨åˆ†å®Œæˆçš„å­ä»»åŠ¡çš„ç»éªŒï¼Œå› ä¸ºæœ‰äº›ä»»åŠ¡åœ¨æ¢ç´¢é˜¶æ®µå¾ˆéš¾å®Œæˆã€‚å¼€æ”¾ç¯å¢ƒä¸­çš„è®¸å¤šä»»åŠ¡é€šå¸¸å…·æœ‰ç»„åˆæ€§ï¼Œè¿™æ„å‘³ç€è¿‡å»ä»»åŠ¡çš„ç»éªŒå¯ä»¥ç»å¸¸å¸®åŠ©å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚LLaMA-Riderä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å¸®åŠ©LLMså­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨Minecraftæ¨¡æ‹Ÿå™¨MineDojoä¸Šè¯„ä¼°äº†LLaMA-Rideræ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMA-Riderèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒï¼Œå¹¶é€šè¿‡å¾®è°ƒä»…ä½¿ç”¨1.3kä¸ªæ”¶é›†åˆ°çš„æ•°æ®å®ä¾‹æ¥æé«˜LLMså®Œæˆä»»åŠ¡çš„èƒ½åŠ›ï¼Œä¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæˆæœ¬æ›´ä½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LLaMA-Rideræ–¹æ³•ä¸ºLLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢å’Œå­¦ä¹ æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å…¶åé¦ˆ-ä¿®æ­£æœºåˆ¶å’Œå­ä»»åŠ¡é‡æ ‡è®°æŠ€æœ¯å¯ä»¥å¸®åŠ©LLMsæ›´å¥½åœ°é€‚åº”ç¯å¢ƒï¼Œå¹¶æé«˜å…¶å®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLLaMA-Rideræ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¼€æ”¾ç¯å¢ƒï¼Œå¹¶å…·æœ‰ç»ˆèº«æ¢ç´¢å’Œå­¦ä¹ çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Stay Focused: Problem Drift in Multi-Agent Debate</td>
      <td>Multi-agent debate - multiple instances of large language models discussing<br>problems in turn-based interaction - has shown promise for solving knowledge<br>and reasoning tasks. However, these methods show limitations, particularly when<br>scaling them to longer reasoning chains. In this study, we unveil a new issue<br>of multi-agent debate: discussions drift away from the initial problem over<br>multiple turns. We define this phenomenon as problem drift and quantify its<br>presence across ten tasks (i.e., three generative, three knowledge, three<br>reasoning, and one instruction-following task). To identify the reasons for<br>this issue, we perform a human study with eight experts on discussions<br>suffering from problem drift, who find the most common issues are a lack of<br>progress (35% of cases), low-quality feedback (26% of cases), and a lack of<br>clarity (25% of cases). To systematically address the issue of problem drift,<br>we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem<br>drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of<br>problem drift cases. Our study can be seen as a first step to understanding a<br>key limitation of multi-agent debate, highlighting pathways for improving their<br>effectiveness in the future.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„é—®é¢˜æ¼‚ç§»ï¼šè¯†åˆ«ä¸ç¼“è§£<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤šæ™ºèƒ½ä½“è¾©è®ºä½œä¸ºä¸€ç§æ–°å…´çš„AIæŠ€æœ¯ï¼Œé€šè¿‡å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè½®æ¢å¼äº¤äº’ï¼Œå±•ç°å‡ºåœ¨è§£å†³çŸ¥è¯†å’Œæ¨ç†ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œéšç€è¾©è®ºé“¾çš„å»¶é•¿ï¼Œè¿™äº›æ–¹æ³•åœ¨æ‰©å±•æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„ä¸€ä¸ªæ–°é—®é¢˜ï¼šåœ¨å¤šè½®äº¤äº’ä¸­ï¼Œè®¨è®ºé€æ¸åç¦»åˆå§‹é—®é¢˜ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œé—®é¢˜æ¼‚ç§»â€ï¼Œæœ¬æ–‡æ—¨åœ¨é‡åŒ–é—®é¢˜æ¼‚ç§»çš„å­˜åœ¨ï¼Œå¹¶æ¢ç´¢å…¶èƒŒåçš„åŸå› ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé—®é¢˜æ¼‚ç§»çš„é‡åŒ–<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºFOCUSçš„æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å¤šè½®äº¤äº’ä¸­ä»»åŠ¡æ€§èƒ½çš„è¡°å‡ç¨‹åº¦ã€‚é€šè¿‡FOCUSæŒ‡æ ‡ï¼Œå¯ä»¥è¯†åˆ«å‡ºå“ªäº›è®¨è®ºå‡ºç°äº†é—®é¢˜æ¼‚ç§»ï¼Œå¹¶é‡åŒ–å…¶å½±å“ç¨‹åº¦ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé—®é¢˜æ¼‚ç§»çš„æ£€æµ‹ä¸ç¼“è§£<br>ä¸ºäº†åœ¨æµ‹è¯•æ—¶æ£€æµ‹é—®é¢˜æ¼‚ç§»ï¼Œæœ¬æ–‡æå‡ºäº†DRIFTJudgeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºLLM-as-a-judgeï¼Œé€šè¿‡è¯„ä¼°è¿ç»­è½®æ¬¡çš„è§£å†³æ–¹æ¡ˆæ¥åˆ¤æ–­æ˜¯å¦å­˜åœ¨é—®é¢˜æ¼‚ç§»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†DRIFTPolicyæ–¹æ³•ï¼Œé€šè¿‡å‘å‚ä¸è¾©è®ºçš„æ™ºèƒ½ä½“æä¾›é’ˆå¯¹æ€§çš„åé¦ˆï¼Œä»¥å‡å°‘é—®é¢˜æ¼‚ç§»çš„å‘ç”Ÿã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨åä¸ªä»»åŠ¡ï¼ˆåŒ…æ‹¬ç”Ÿæˆã€çŸ¥è¯†ã€æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå‘ç°é—®é¢˜æ¼‚ç§»æ™®éå­˜åœ¨ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ã€‚é€šè¿‡äººç±»ä¸“å®¶çš„ç ”ç©¶ï¼Œæœ¬æ–‡ç¡®å®šäº†å¯¼è‡´é—®é¢˜æ¼‚ç§»çš„å…«ä¸ªä¸»è¦åŸå› ï¼ŒåŒ…æ‹¬ç¼ºä¹è¿›å±•ã€ä½è´¨é‡åé¦ˆã€ç¼ºä¹æ¸…æ™°åº¦ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIFTPolicyæ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡å°‘31%çš„é—®é¢˜æ¼‚ç§»æ¡ˆä¾‹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„ä¸€ä¸ªå…³é”®å±€é™æ€§ï¼Œå¹¶ä¸ºæé«˜å…¶æœ‰æ•ˆæ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ™ºèƒ½ä½“æ¢ç´¢æ–°æƒ³æ³•çš„ä½œç”¨ï¼Œå¹¶æ¯”è¾ƒäººç±»å’Œæ™ºèƒ½ä½“è¾©è®ºä¹‹é—´çš„å†…åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„DRIFTJudgeå’ŒDRIFTPolicyæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›å‚è€ƒï¼Œä»¥æ£€æµ‹å’Œç¼“è§£é—®é¢˜æ¼‚ç§»ç°è±¡ã€‚</td>
    </tr>
    <tr>
      <th>39</th>
      <td>A Framework for Complementary Companion Character Behavior in Video Games</td>
      <td>We propose a game development framework capable of governing the behavior of<br>complementary companions in a video game. A "complementary" action is<br>contrasted with a mimicking action and is defined as any action by a friendly<br>non-player character that furthers the player's strategy. This is determined<br>through a combination of both player action and game state prediction processes<br>while allowing the AI companion to experiment. We determine the location of<br>interest for companion actions based on a dynamic set of regions customized to<br>the individual player. A user study shows promising results; a majority of<br>participants familiar with game design react positively to the companion<br>behavior, stating that they would consider using the frame-work in future games<br>themselves.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆAIæ–°æ¡†æ¶ï¼šäº’è¡¥å‹ä¼™ä¼´è¡Œä¸ºåŠ©åŠ›ç©å®¶ç­–ç•¥<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€æ¸¸æˆå¤æ‚æ€§çš„æå‡ï¼Œç©å®¶å¯¹æ¸¸æˆä¸­çš„éç©å®¶è§’è‰²ï¼ˆNPCï¼‰çš„æ™ºèƒ½å’Œäº’åŠ¨æ€§è¦æ±‚è¶Šæ¥è¶Šé«˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆAIå¾€å¾€åªèƒ½æ¨¡ä»¿ç©å®¶çš„è¡Œä¸ºï¼Œç¼ºä¹ä¸»åŠ¨æ€§å’Œåˆ›é€ æ€§ï¼Œæ— æ³•çœŸæ­£è¾…åŠ©ç©å®¶è¾¾æˆç›®æ ‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¸¸æˆå¼€å‘æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°äº’è¡¥å‹ä¼™ä¼´è¡Œä¸ºï¼Œå³NPCèƒ½å¤Ÿæ ¹æ®ç©å®¶çš„ç­–ç•¥å’Œæ¸¸æˆçŠ¶æ€ï¼Œä¸»åŠ¨é‡‡å–è¡ŒåŠ¨ï¼Œå¸®åŠ©ç©å®¶æ›´è½»æ¾åœ°è¾¾æˆç›®æ ‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº’è¡¥å‹è¡Œä¸ºå®šä¹‰<br>æœ¬æ–‡é¦–å…ˆå¯¹â€œäº’è¡¥å‹è¡Œä¸ºâ€è¿›è¡Œäº†æ˜ç¡®å®šä¹‰ï¼Œå³NPCçš„è¡Œä¸ºèƒ½å¤Ÿä¿ƒè¿›ç©å®¶çš„ç­–ç•¥ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿ç©å®¶ã€‚è¿™è¦æ±‚NPCèƒ½å¤Ÿé¢„æµ‹ç©å®¶çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå¹¶é‡‡å–ç›¸åº”çš„è¡ŒåŠ¨æ¥è¾…åŠ©ç©å®¶ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€å†³ç­–è¿‡ç¨‹<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š<br>1. é¢„æµ‹ç©å®¶çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨<br>2. æ£€æŸ¥é¢„æµ‹è¡ŒåŠ¨æ˜¯å¦å¯è¡Œï¼Œè‹¥ä¸å¯è¡Œåˆ™å¯»æ‰¾æ›¿ä»£è¡ŒåŠ¨<br>3. åˆ¤æ–­æ˜¯å¦èƒ½å¤Ÿè¾…åŠ©ç©å®¶å½“å‰è¡ŒåŠ¨<br>4. è‹¥æ— æ³•è¾…åŠ©ï¼Œåˆ™éšæœºé€‰æ‹©æ‰§è¡Œé¢„æµ‹è¡ŒåŠ¨æˆ–å¯»æ‰¾æœ€ä½³æ¸¸æˆçŠ¶æ€çš„è¡ŒåŠ¨<br>5. è‹¥æ‰€æœ‰è¡ŒåŠ¨å‡ä¸å¯è¡Œï¼Œåˆ™å°è¯•æ‰§è¡Œç©å®¶æœªå°è¯•è¿‡çš„è¡ŒåŠ¨æˆ–é‡‡å–é»˜è®¤è¡Œä¸º<br>6. éšæœºé€‰æ‹©æ‰§è¡Œæœªå°è¯•è¿‡çš„è¡ŒåŠ¨ï¼Œä»¥é¼“åŠ±NPCè¿›è¡Œæ¢ç´¢<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŠ¨æ€åŒºåŸŸç³»ç»Ÿ<br>ä¸ºäº†æ›´ç²¾ç¡®åœ°ç¡®å®šNPCçš„è¡ŒåŠ¨ä½ç½®ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€åŒºåŸŸç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ ¹æ®ç©å®¶çš„è¡ŒåŠ¨åŠ¨æ€åˆ’åˆ†åœ°å›¾åŒºåŸŸï¼Œä½¿å¾—NPCçš„è¡ŒåŠ¨æ›´åŠ çµæ´»å’Œé’ˆå¯¹æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šç©å®¶è¡ŒåŠ¨é¢„æµ‹<br>æœ¬æ–‡ä½¿ç”¨åˆ†ç±»å™¨å¯¹ç©å®¶çš„å†å²æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥é¢„æµ‹ç©å®¶çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚åˆ†ç±»å™¨çš„ç±»å‹å’Œè®­ç»ƒæ–¹å¼å¯ä»¥æ ¹æ®æ¸¸æˆå¼€å‘è€…è¿›è¡Œè°ƒæ•´ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šæ¸¸æˆçŠ¶æ€é¢„æµ‹<br>æœ¬æ–‡é€šè¿‡æ¨¡æ‹ŸNPCæ‰§è¡Œæ¯ä¸ªå¯èƒ½çš„è¡ŒåŠ¨ï¼Œå¹¶è¯„ä¼°å…¶ç»“æœï¼Œæ¥é¢„æµ‹æ¸¸æˆçŠ¶æ€çš„å˜åŒ–ã€‚è¿™æœ‰åŠ©äºNPCé€‰æ‹©èƒ½å¤Ÿå¸¦æ¥æœ€ä½³ç»“æœçš„è¡ŒåŠ¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°å‚ä¸è€…å¯¹NPCçš„äº’è¡¥å‹è¡Œä¸ºè¡¨ç¤ºæ»¡æ„ï¼Œå¹¶è®¤ä¸ºè¯¥æ¡†æ¶å¯¹æ¸¸æˆå¼€å‘è€…å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºæ¸¸æˆå¼€å‘è€…æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¯ä»¥å¸®åŠ©ä»–ä»¬åˆ›å»ºæ›´å…·æ™ºèƒ½å’Œäº’åŠ¨æ€§çš„NPCã€‚è¯¥æ¡†æ¶çš„å¯é…ç½®æ€§å’Œçµæ´»æ€§ä½¿å…¶é€‚ç”¨äºå„ç§ç±»å‹çš„æ¸¸æˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŠ¨æ€åŒºåŸŸç³»ç»Ÿå’Œç©å®¶è¡ŒåŠ¨é¢„æµ‹æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿã€‚</td>
    </tr>
    <tr>
      <th>58</th>
      <td>GROOT: Learning to Follow Instructions by Watching Gameplay Videos</td>
      <td>We study the problem of building a controller that can follow open-ended<br>instructions in open-world environments. We propose to follow reference videos<br>as instructions, which offer expressive goal specifications while eliminating<br>the need for expensive text-gameplay annotations. A new learning framework is<br>derived to allow learning such instruction-following controllers from gameplay<br>videos while producing a video instruction encoder that induces a structured<br>goal space. We implement our agent GROOT in a simple yet effective<br>encoder-decoder architecture based on causal transformers. We evaluate GROOT<br>against open-world counterparts and human players on a proposed Minecraft<br>SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the<br>human-machine gap as well as exhibiting a 70% winning rate over the best<br>generalist agent baseline. Qualitative analysis of the induced goal space<br>further demonstrates some interesting emergent properties, including the goal<br>composition and complex gameplay behavior synthesis. The project page is<br>available at https://craftjarvis-groot.github.io.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GROOTï¼šé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘å­¦ä¹ æŒ‡ä»¤éµå¾ª<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ§åˆ¶å™¨é€šå¸¸åªèƒ½å®Œæˆé¢„å®šä¹‰çš„ã€æœ‰é™çš„ç¨‹åºæ€§ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘æ¥å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†ç›®æ ‡æŒ‡å®šä¸ºå‚è€ƒæ¸¸æˆè§†é¢‘ç‰‡æ®µï¼Œä»è€Œæä¾›ä¸°å¯Œçš„ç›®æ ‡è§„èŒƒï¼ŒåŒæ—¶æ¶ˆé™¤å¯¹æ˜‚è´µçš„æ–‡æœ¬-æ¸¸æˆæ³¨é‡Šçš„éœ€æ±‚ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥äº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶äº§ç”Ÿä¸€ä¸ªç›®æ ‡ç©ºé—´å’Œä¸€ä¸ªè§†é¢‘æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ï¼Œä»è€Œå®ç°ä»æ¸¸æˆè§†é¢‘ä¸­å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Minecraft SkillForgeåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGROOTåœ¨æ•´ä½“Eloè¯„åˆ†æ¯”è¾ƒä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå¹¶ä¸”åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„è·å–é’»çŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„å­¦ä¹ æ¡†æ¶å’ŒGROOTä»£ç†çš„æ¶æ„è®¾è®¡ä¸ºæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†ç›®æ ‡ç©ºé—´å’Œæ§åˆ¶å™¨ç­–ç•¥çš„æ½œåœ¨åº”ç”¨ï¼Œä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å¤æ‚ä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Scaling Instructable Agents Across Many Simulated Worlds</td>
      <td>Building embodied AI systems that can follow arbitrary language instructions<br>in any 3D environment is a key challenge for creating general AI. Accomplishing<br>this goal requires learning to ground language in perception and embodied<br>actions, in order to accomplish complex tasks. The Scalable, Instructable,<br>Multiworld Agent (SIMA) project tackles this by training agents to follow<br>free-form instructions across a diverse range of virtual 3D environments,<br>including curated research environments as well as open-ended, commercial video<br>games. Our goal is to develop an instructable agent that can accomplish<br>anything a human can do in any simulated 3D environment. Our approach focuses<br>on language-driven generality while imposing minimal assumptions. Our agents<br>interact with environments in real-time using a generic, human-like interface:<br>the inputs are image observations and language instructions and the outputs are<br>keyboard-and-mouse actions. This general approach is challenging, but it allows<br>agents to ground language across many visually complex and semantically rich<br>environments while also allowing us to readily run agents in new environments.<br>In this paper we describe our motivation and goal, the initial progress we have<br>made, and promising preliminary results on several diverse research<br>environments and a variety of commercial video games.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | SIMAï¼šè·¨è¶Šå¤šä¸ªæ¨¡æ‹Ÿä¸–ç•Œçš„å¯æŒ‡ä»¤æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†å®ƒä»¬ä¸æˆ‘ä»¬æ‰€å¤„çš„å…·èº«ä¸–ç•Œè¿æ¥èµ·æ¥ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°ä»£AIåœ¨è¯­è¨€èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ„ŸçŸ¥å’Œè¡ŒåŠ¨æ–¹é¢å´è¿œä¸åŠäººç±»ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘èƒ½å¤Ÿç†è§£è¯­è¨€æŒ‡ä»¤å¹¶åœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡çš„å…·èº«AIç³»ç»Ÿã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>SIMAé¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªèƒ½å¤Ÿéµå¾ªä»»æ„è¯­è¨€æŒ‡ä»¤å¹¶åœ¨ä»»ä½•è™šæ‹Ÿ3Dç¯å¢ƒä¸­é€šè¿‡é”®ç›˜å’Œé¼ æ ‡æ“ä½œè¿›è¡Œè¡ŒåŠ¨çš„ç³»ç»Ÿã€‚è¯¥é¡¹ç›®çš„ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ **å¤šç¯å¢ƒè®­ç»ƒ**ï¼šSIMAåœ¨å¤šç§è™šæ‹Ÿ3Dç¯å¢ƒä¸­è®­ç»ƒæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬å®šåˆ¶çš„ç ”ç©¶ç¯å¢ƒå’Œå¼€æ”¾å¼çš„å•†ä¸šè§†é¢‘æ¸¸æˆã€‚è¿™ç§å¤šæ ·åŒ–çš„è®­ç»ƒç¯å¢ƒæœ‰åŠ©äºæ™ºèƒ½ä½“å­¦ä¹ æ›´é€šç”¨çš„æŠ€èƒ½ï¼Œå¹¶æé«˜å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚<br><br>ğŸ’¡ **è¯­è¨€é©±åŠ¨**ï¼šSIMAçš„æ™ºèƒ½ä½“é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œè®­ç»ƒï¼Œè¿™æœ‰åŠ©äºå®ƒä»¬å­¦ä¹ æ›´é€šç”¨çš„è¯­è¨€è¡¨ç¤ºå’ŒæŠ½è±¡ï¼Œå¹¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚<br><br>ğŸ’¡ **äººç±»ç•Œé¢**ï¼šSIMAçš„æ™ºèƒ½ä½“ä½¿ç”¨ä¸äººç±»ç›¸åŒçš„é”®ç›˜å’Œé¼ æ ‡æ§åˆ¶æ–¹å¼ä¸è™šæ‹Ÿç¯å¢ƒäº¤äº’ï¼Œè¿™æœ‰åŠ©äºå®ƒä»¬æ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œå¹¶æé«˜å…¶åœ¨æ–°ç¯å¢ƒä¸­çš„è¿ç§»èƒ½åŠ›ã€‚<br><br>ğŸ’¡ **æ•°æ®æ”¶é›†**ï¼šSIMAæ”¶é›†äº†å¤§é‡ç”±äººç±»ä¸“å®¶ç”Ÿæˆçš„æ¸¸æˆæ•°æ®ï¼ŒåŒ…æ‹¬è§†é¢‘ã€è¯­è¨€æŒ‡ä»¤ã€è®°å½•çš„åŠ¨ä½œå’Œå„ç§æ³¨é‡Šã€‚è¿™äº›æ•°æ®è¢«ç”¨äºè®­ç»ƒæ™ºèƒ½ä½“ï¼Œå¹¶æé«˜å…¶å­¦ä¹ æ•ˆç‡ã€‚<br><br>ğŸ’¡ **è¯„ä¼°æ–¹æ³•**ï¼šSIMAä½¿ç”¨å¤šç§è¯„ä¼°æ–¹æ³•æ¥è¯„ä¼°æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŠ¨ä½œæ—¥å¿—æ¦‚ç‡ã€é™æ€è§†è§‰è¾“å…¥ã€åœ°é¢å®å†µè¯„ä¼°ã€å…‰å­¦å­—ç¬¦è¯†åˆ«å’Œäººå·¥è¯„ä¼°ã€‚è¿™äº›è¯„ä¼°æ–¹æ³•æœ‰åŠ©äºå…¨é¢è¯„ä¼°æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼Œå¹¶ç¡®ä¿å…¶èƒ½å¤Ÿéµå¾ªè¯­è¨€æŒ‡ä»¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>SIMAé¡¹ç›®å–å¾—äº†åˆæ­¥çš„æˆåŠŸï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤šç§ç¯å¢ƒä¸­å®Œæˆå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¼èˆªã€èµ„æºæ”¶é›†ã€å¯¹è±¡ç®¡ç†å’Œæˆ˜æ–—ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIMAçš„æ™ºèƒ½ä½“åœ¨ç¯å¢ƒæ³›åŒ–ã€é›¶æ ·æœ¬è¿ç§»å’Œè¯­è¨€æ¡ä»¶è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>SIMAé¡¹ç›®ä¸ºå…·èº«AIç ”ç©¶æä¾›äº†ä¸€ä¸ªé‡è¦çš„å¹³å°ï¼Œå…¶åˆ›æ–°æ–¹æ³•å’ŒæŠ€æœ¯å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚æ­¤å¤–ï¼ŒSIMAé¡¹ç›®çš„ç ”ç©¶æˆæœä¹Ÿæœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯­è¨€ä¸æ„ŸçŸ¥å’Œè¡ŒåŠ¨ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸ºé€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>83</th>
      <td>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</td>
      <td>AutoGen is an open-source framework that allows developers to build LLM<br>applications via multiple agents that can converse with each other to<br>accomplish tasks. AutoGen agents are customizable, conversable, and can operate<br>in various modes that employ combinations of LLMs, human inputs, and tools.<br>Using AutoGen, developers can also flexibly define agent interaction behaviors.<br>Both natural language and computer code can be used to program flexible<br>conversation patterns for different applications. AutoGen serves as a generic<br>infrastructure to build diverse applications of various complexities and LLM<br>capacities. Empirical studies demonstrate the effectiveness of the framework in<br>many example applications, with domains ranging from mathematics, coding,<br>question answering, operations research, online decision-making, entertainment,<br>etc.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AutoGenï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“å¯¹è¯èµ‹èƒ½ä¸‹ä¸€ä»£LLMåº”ç”¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨LLMsæ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºæˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“æ¨¡å‹å¾€å¾€éš¾ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼Œè€Œå¤šæ™ºèƒ½ä½“æ¨¡å‹åˆ™å¯ä»¥ååŒå·¥ä½œï¼Œå‘æŒ¥å„è‡ªçš„ä¼˜åŠ¿ï¼Œä»è€Œæé«˜ä»»åŠ¡å¤„ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>AutoGenæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œå®ƒå…è®¸å¼€å‘è€…é€šè¿‡å¤šä¸ªå¯ä»¥ç›¸äº’å¯¹è¯çš„æ™ºèƒ½ä½“æ¥æ„å»ºLLMåº”ç”¨ç¨‹åºã€‚AutoGençš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ **å¯å®šåˆ¶å’Œå¯å¯¹è¯çš„æ™ºèƒ½ä½“**ï¼šAutoGençš„æ™ºèƒ½ä½“å¯ä»¥åŸºäºLLMsã€äººç±»è¾“å…¥ã€å·¥å…·æˆ–å®ƒä»¬çš„ç»„åˆæ¥æ„å»ºï¼Œå¹¶ä¸”å¯ä»¥ç›¸äº’å¯¹è¯ï¼Œæ¥æ”¶ã€ååº”å’Œå“åº”æ¶ˆæ¯ã€‚è¿™ä½¿å¾—å¼€å‘è€…å¯ä»¥è½»æ¾åœ°åˆ›å»ºå…·æœ‰ä¸åŒè§’è‰²å’Œèƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼Œä¾‹å¦‚ç¼–å†™ä»£ç ã€æ‰§è¡Œä»£ç ã€è·å–äººç±»åé¦ˆã€éªŒè¯è¾“å‡ºç­‰ã€‚<br><br>ğŸ’¡ **å¯¹è¯ç¼–ç¨‹**ï¼šAutoGené‡‡ç”¨äº†ä¸€ç§ä»¥å¯¹è¯ä¸ºä¸­å¿ƒçš„ç¼–ç¨‹èŒƒå¼ï¼Œå°†å¤æ‚çš„LLMåº”ç”¨ç¨‹åºå·¥ä½œæµç¨‹ç®€åŒ–ä¸ºå¤šæ™ºèƒ½ä½“å¯¹è¯ã€‚å¼€å‘è€…å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ç»„å…·æœ‰ç‰¹å®šèƒ½åŠ›å’Œè§’è‰²çš„å¯å¯¹è¯æ™ºèƒ½ä½“ï¼Œå¹¶ç¼–ç¨‹å®ƒä»¬ä¹‹é—´çš„äº¤äº’è¡Œä¸ºæ¥æ„å»ºåº”ç”¨ç¨‹åºã€‚AutoGenæ”¯æŒä½¿ç”¨è‡ªç„¶è¯­è¨€å’Œç¼–ç¨‹è¯­è¨€æ¥ç¼–ç¨‹çµæ´»çš„å¯¹è¯æ¨¡å¼ï¼Œå¹¶æä¾›äº†ä¸€ç³»åˆ—è®¾è®¡æ¨¡å¼æ¥ç®€åŒ–å¯¹è¯ç¼–ç¨‹ï¼Œä¾‹å¦‚ç»Ÿä¸€çš„å¯¹è¯æ¥å£ã€è‡ªåŠ¨å›å¤æœºåˆ¶ã€åŸºäºç¼–ç¨‹å’Œè‡ªç„¶è¯­è¨€çš„æ§åˆ¶æµç®¡ç†ç­‰ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>AutoGenåœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ç¨‹åºä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ•°å­¦é—®é¢˜è§£å†³ã€ä»£ç ç”Ÿæˆã€é—®ç­”ã€å†³ç­–åˆ¶å®šã€åœ¨çº¿å†³ç­–ã€å¨±ä¹ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoGenå¯ä»¥å¸®åŠ©å®ç°è®¸å¤šä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ï¼Œå¹¶å‡å°‘å¼€å‘å·¥ä½œé‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AutoGenä¸ºæ„å»ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“LLMåº”ç”¨ç¨‹åºæä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>* **æ¨¡å—åŒ–è®¾è®¡**ï¼šAutoGençš„æ™ºèƒ½ä½“å¯ä»¥ç‹¬ç«‹å¼€å‘ã€æµ‹è¯•å’Œç»´æŠ¤ï¼Œä»è€Œç®€åŒ–äº†æ•´ä½“å¼€å‘å’Œä»£ç ç®¡ç†ã€‚<br>* **çµæ´»çš„å¯¹è¯ç¼–ç¨‹**ï¼šAutoGenæ”¯æŒä½¿ç”¨è‡ªç„¶è¯­è¨€å’Œç¼–ç¨‹è¯­è¨€æ¥ç¼–ç¨‹çµæ´»çš„å¯¹è¯æ¨¡å¼ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥æ ¹æ®ä¸åŒçš„åº”ç”¨éœ€æ±‚è¿›è¡Œå®šåˆ¶ã€‚<br>* **å¯æ‰©å±•æ€§**ï¼šAutoGençš„æ™ºèƒ½ä½“å¯ä»¥è½»æ¾åœ°æ‰©å±•å’Œå®šåˆ¶ï¼Œä»¥æ»¡è¶³ä¸åŒçš„åº”ç”¨éœ€æ±‚ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>AutoGenæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…æ„å»ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“LLMåº”ç”¨ç¨‹åºã€‚å®ƒå…·æœ‰æ¨¡å—åŒ–è®¾è®¡ã€çµæ´»çš„å¯¹è¯ç¼–ç¨‹å’Œå¯æ‰©å±•æ€§ç­‰ä¼˜ç‚¹ï¼Œä¸ºLLMåº”ç”¨ç¨‹åºçš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization</td>
      <td>Large Language Models (LLMs) exhibit robust problem-solving capabilities for<br>diverse tasks. However, most LLM-based agents are designed as specific task<br>solvers with sophisticated prompt engineering, rather than agents capable of<br>learning and evolving through interactions. These task solvers necessitate<br>manually crafted prompts to inform task rules and regulate LLM behaviors,<br>inherently incapacitating to address complex dynamic scenarios e.g., large<br>interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent<br>with Policy-level Reflection and Optimization that can learn a wealth of<br>expertise from interactive experiences and progressively elevate its behavioral<br>policy. Specifically, it involves a dynamic belief generation and reflection<br>process for policy evolution. Rather than action-level reflection, Agent-Pro<br>iteratively reflects on past trajectories and beliefs, fine-tuning its<br>irrational beliefs for a better policy. Moreover, a depth-first search is<br>employed for policy optimization, ensuring continual enhancement in policy<br>payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,<br>outperforming vanilla LLM and specialized models. Our results show Agent-Pro<br>can learn and evolve in complex and dynamic scenes, which also benefits<br>numerous LLM-based applications.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-Proï¼šåŸºäºç­–ç•¥çº§åæ€å’Œä¼˜åŒ–çš„å­¦ä¹ è¿›åŒ–<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å„ç§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°åŸºäºLLMsçš„æ™ºèƒ½ä½“éƒ½æ˜¯ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡çš„ï¼Œéœ€è¦å¤æ‚çš„æç¤ºå·¥ç¨‹æ¥å‘ŠçŸ¥ä»»åŠ¡è§„åˆ™å’Œè°ƒèŠ‚LLMsçš„è¡Œä¸ºã€‚è¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥åº”å¯¹å¤æ‚åŠ¨æ€çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤§å‹äº’åŠ¨æ¸¸æˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgent-Proçš„LLM-basedæ™ºèƒ½ä½“ï¼Œå®ƒå…·æœ‰ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä»äº’åŠ¨ç»éªŒä¸­å­¦ä¹ å¤§é‡ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶é€æ­¥æå‡å…¶è¡Œä¸ºç­–ç•¥ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç­–ç•¥çº§åæ€å’Œä¼˜åŒ–<br>Agent-Proé€šè¿‡ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–æ¥å­¦ä¹ è¿›åŒ–ã€‚å®ƒä¸ä»…åæ€è¿‡å»çš„è½¨è¿¹å’Œä¿¡å¿µï¼Œè¿˜é€šè¿‡æ·±åº¦ä¼˜å…ˆæœç´¢æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿ç­–ç•¥æ”¶ç›Šçš„æŒç»­æå‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹<br>Agent-Proé‡‡ç”¨ä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡æ›´æ–°è‡ªèº«ä¿¡å¿µå’Œä¸–ç•Œä¿¡å¿µæ¥ç”Ÿæˆæ›´åˆç†çš„è¡Œä¸ºã€‚å®ƒèƒ½å¤Ÿæ ¹æ®ä¿¡å¿µæ¥é¢„æµ‹è¡ŒåŠ¨ï¼Œå¹¶åœ¨æ¸¸æˆç»“æŸåæ ¹æ®ç»“æœæ¥åæ€å’Œè°ƒæ•´ä¿¡å¿µã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>Agent-Proåœ¨ä¸¤ä¸ªæ¸¸æˆï¼ˆBlackjackå’ŒTexas Hold'emï¼‰ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å®ƒèƒ½å¤Ÿå­¦ä¹ å¹¶è¿›åŒ–ï¼Œåœ¨å¤æ‚å’ŒåŠ¨æ€çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„LLMså’Œä¸“é—¨æ¨¡å‹ç›¸æ¯”ï¼ŒAgent-Proåœ¨æ¸¸æˆä¸­çš„æ”¶ç›Šæ›´é«˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Agent-Proçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¸ºæ„å»ºèƒ½å¤Ÿå­¦ä¹ å’Œè¿›åŒ–çš„LLM-basedæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–æœºåˆ¶å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“ä»äº’åŠ¨ç»éªŒä¸­å­¦ä¹ ï¼Œå¹¶é€æ­¥æå‡å…¶è¡Œä¸ºç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šçš„åœºæ™¯ä¸­åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å•†ä¸šè°ˆåˆ¤ã€å®‰å…¨ç›‘æ§ç­‰ã€‚</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Enhance Reasoning for Large Language Models in the Game Werewolf</td>
      <td>This paper presents an innovative framework that integrates Large Language<br>Models (LLMs) with an external Thinker module to enhance the reasoning<br>capabilities of LLM-based agents. Unlike augmenting LLMs with prompt<br>engineering, Thinker directly harnesses knowledge from databases and employs<br>various optimization techniques. The framework forms a reasoning hierarchy<br>where LLMs handle intuitive System-1 tasks such as natural language processing,<br>while the Thinker focuses on cognitive System-2 tasks that require complex<br>logical analysis and domain-specific knowledge. Our framework is presented<br>using a 9-player Werewolf game that demands dual-system reasoning. We introduce<br>a communication protocol between LLMs and the Thinker, and train the Thinker<br>using data from 18800 human sessions and reinforcement learning. Experiments<br>demonstrate the framework's effectiveness in deductive reasoning, speech<br>generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to<br>surpass GPT4 when integrated with the Thinker. This paper also contributes the<br>largest dataset for social deduction games to date.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ï¼šä»¥ç‹¼äººæ€æ¸¸æˆä¸ºæ¡ˆä¾‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„çªç ´ï¼Œå…¶åœ¨æ¨ç†ã€è§„åˆ’å’Œå†³ç­–ç­‰é¢†åŸŸçš„æ½œåŠ›ä¹Ÿé€æ¸æ˜¾ç°ã€‚ç„¶è€Œï¼ŒLLMsåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œæ·±åº¦é€»è¾‘åˆ†æçš„ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥å¤–éƒ¨æ¨ç†æ¨¡å—ï¼Œå³â€œæ€è€ƒè€…â€ï¼ˆThinkerï¼‰ï¼Œæ¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒç³»ç»Ÿæ¨ç†æ¡†æ¶<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œå°†LLMsä¸å¤–éƒ¨Thinkeræ¨¡å—ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªæ¨ç†å±‚æ¬¡ç»“æ„ã€‚LLMsè´Ÿè´£å¤„ç†ç›´è§‚çš„System-1ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¸¸è¯†æ¨ç†ï¼Œè€ŒThinkeråˆ™ä¸“æ³¨äºéœ€è¦å¤æ‚é€»è¾‘åˆ†æå’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„System-2ä»»åŠ¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šThinkeræ¨¡å—çš„è®¾è®¡ä¸è®­ç»ƒ<br>Thinkeræ¨¡å—ç›´æ¥ä»æ•°æ®åº“ä¸­è·å–çŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚å®ƒé€šè¿‡æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºç¾¤ä½“çš„è®­ç»ƒç­‰æ–¹æ³•ï¼Œå­¦ä¹ ç”Ÿæˆåˆç†çš„æ¸¸æˆåŠ¨ä½œå’ŒLLMçš„è¯­éŸ³æŒ‡ä»¤ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é›†è´¡çŒ®<br>æœ¬æ–‡æ”¶é›†äº†18,800åœºçœŸå®äººç±»æ¸¸æˆä¼šè¯æ•°æ®ï¼Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç¤¾äº¤æ¨ç†æ¸¸æˆæ•°æ®é›†ï¼Œä¸ºç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥Thinkeræ¨¡å—æ˜¾è‘—æé«˜äº†LLMsçš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼ŒThinkeræ¨¡å—åœ¨æ¨ç†ã€è¯­éŸ³ç”Ÿæˆå’Œåœ¨çº¿æ¸¸æˆè¯„ä¼°æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†Thinkerä¸ä¸€ä¸ªè¾ƒå°çš„LLMæ¨¡å‹ï¼ˆ6Bï¼‰è¿›è¡Œå¾®è°ƒï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¿‡äº†GPT4ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡å°†LLMsä¸å¤–éƒ¨æ¨ç†æ¨¡å—ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆæå‡LLMsåœ¨ç‰¹å®šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†ä¹Ÿä¸ºç¤¾äº¤æ¨ç†æ¸¸æˆçš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®åŸºç¡€ã€‚</td>
    </tr>
    <tr>
      <th>82</th>
      <td>GameEval: Evaluating LLMs on Conversational Games</td>
      <td>The rapid advancements in large language models (LLMs) have presented<br>challenges in evaluating those models. Existing evaluation methods are either<br>reference-based or preference based, which inevitably need human intervention<br>or introduce test bias caused by evaluator models. In this paper, we propose<br>GameEval, a novel approach to evaluating LLMs through goal-driven<br>conversational games, overcoming the limitations of previous methods. GameEval<br>treats LLMs as game players and assigns them distinct roles with specific goals<br>achieved by launching conversations of various forms, including discussion,<br>question answering, and voting. We design three unique games with cooperative<br>or adversarial objectives, accompanied by corresponding evaluation metrics, to<br>show how this new paradigm comprehensively evaluates model performance.Through<br>extensive experiments, we show that GameEval can effectively differentiate the<br>capabilities of various LLMs, providing a comprehensive assessment of their<br>integrated abilities to solve complex problems. Our public anonymous code is<br>available at https://github.com/GameEval/GameEval.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GameEvalï¼šé€šè¿‡å¯¹è¯æ¸¸æˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›æˆä¸ºä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºå‚è€ƒå’ŒåŸºäºåå¥½çš„æ–¹æ³•ã€‚åŸºäºå‚è€ƒçš„æ–¹æ³•éœ€è¦ä¸é¢„å…ˆç¡®å®šçš„ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œè€ŒåŸºäºåå¥½çš„æ–¹æ³•åˆ™ä¾èµ–äºäººç±»æˆ–æ¨¡å‹è¯„ä¼°è€…çš„åå¥½ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚è·å–é«˜è´¨é‡æ ‡æ³¨çš„æˆæœ¬é«˜ã€æ—¶é—´æ¶ˆè€—å¤§ï¼Œä»¥åŠå¼•å…¥è¯„ä¼°è€…çš„åå¥½åå·®ç­‰ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†GameEvalï¼Œä¸€ç§é€šè¿‡ç›®æ ‡é©±åŠ¨çš„å¯¹è¯æ¸¸æˆæ¥è¯„ä¼°LLMsçš„æ–°æ–¹æ³•ã€‚GameEvalå°†LLMsè§†ä¸ºæ¸¸æˆç©å®¶ï¼Œå¹¶ä¸ºå…¶åˆ†é…å…·æœ‰ç‰¹å®šç›®æ ‡çš„ç‹¬ç‰¹è§’è‰²ï¼Œé€šè¿‡å¯åŠ¨å„ç§å½¢å¼çš„å¯¹è¯ï¼ˆåŒ…æ‹¬è®¨è®ºã€é—®ç­”å’ŒæŠ•ç¥¨ï¼‰æ¥å®ç°è¿™äº›ç›®æ ‡ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸‰ç§ç‹¬ç‰¹çš„æ¸¸æˆï¼ŒåŒ…æ‹¬åˆä½œå’Œå¯¹æŠ—ç›®æ ‡ï¼Œå¹¶é…å¤‡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å±•ç¤ºè¿™ç§æ–°èŒƒå¼å¦‚ä½•å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæœ¬æ–‡å±•ç¤ºäº†GameEvalèƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒLLMsçš„èƒ½åŠ›ï¼Œå¹¶æä¾›å¯¹å…¶è§£å†³å¤æ‚é—®é¢˜ç»¼åˆèƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGameEvalåœ¨åŒºåˆ†ChatGPTå’ŒGPT-4ç­‰æ¨¡å‹çš„èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œç°æœ‰æ–¹æ³•åˆ™éš¾ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GameEvalæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°LLMsçš„æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶å‡å°‘è¯„ä¼°åå·®ã€‚æ­¤å¤–ï¼ŒGameEvalè¿˜å¯ä»¥ç”¨äºè®¾è®¡æ–°çš„æ¸¸æˆï¼Œä»¥è¯„ä¼°LLMsåœ¨ç°å®ä¸–ç•Œå¤æ‚åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>102</th>
      <td>Improving Factuality and Reasoning in Language Models through Multiagent Debate</td>
      <td>Large language models (LLMs) have demonstrated remarkable capabilities in<br>language generation, understanding, and few-shot learning in recent years. An<br>extensive body of work has explored how their performance may be further<br>improved through the tools of prompting, ranging from verification,<br>self-consistency, or intermediate scratchpads. In this paper, we present a<br>complementary approach to improve language responses where multiple language<br>model instances propose and debate their individual responses and reasoning<br>processes over multiple rounds to arrive at a common final answer. Our findings<br>indicate that this approach significantly enhances mathematical and strategic<br>reasoning across a number of tasks. We also demonstrate that our approach<br>improves the factual validity of generated content, reducing fallacious answers<br>and hallucinations that contemporary models are prone to. Our approach may be<br>directly applied to existing black-box models and uses identical procedure and<br>prompts for all tasks we investigate. Overall, our findings suggest that such<br>"society of minds" approach has the potential to significantly advance the<br>capabilities of LLMs and pave the way for further breakthroughs in language<br>generation and understanding.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºæå‡è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç”Ÿæˆã€ç†è§£å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨äº‹å®æ€§å’Œæ¨ç†æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“äº§ç”Ÿé”™è¯¯çš„äº‹å®å’Œæ¨ç†è·³è·ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºæ¥æå‡LLMsçš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ™ºèƒ½ä½“è¾©è®º<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºæ¥æå‡LLMsçš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæŸ¥è¯¢ï¼Œå¤šä¸ªLLMå®ä¾‹ï¼ˆæˆ–æ™ºèƒ½ä½“ï¼‰é¦–å…ˆç”Ÿæˆå„è‡ªçš„å€™é€‰ç­”æ¡ˆã€‚ç„¶åï¼Œæ¯ä¸ªæ™ºèƒ½ä½“é˜…è¯»å¹¶æ‰¹è¯„å…¶ä»–æ™ºèƒ½ä½“çš„ç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨è¿™äº›å†…å®¹æ¥æ›´æ–°è‡ªå·±çš„ç­”æ¡ˆã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œå¤šè½®ï¼Œç›´åˆ°æ™ºèƒ½ä½“ä»¬è¾¾æˆä¸€ä¸ªå…±åŒçš„æœ€ç»ˆç­”æ¡ˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›<br>æœ¬æ–‡å‘ç°ï¼Œå¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•åœ¨å¤šä¸ªæ¨ç†ã€äº‹å®æ€§å’Œé—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡å‹åŸºçº¿ï¼Œå¦‚é›¶æ ·æœ¬æ€ç»´é“¾å’Œåæ€ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æé«˜ç”Ÿæˆå†…å®¹çš„äº‹å®æ€§ï¼Œå‡å°‘å½“ä»£æ¨¡å‹å®¹æ˜“äº§ç”Ÿçš„é”™è¯¯ç­”æ¡ˆå’Œå¹»è§‰ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨å¤šä¸ªæ¨ç†å’Œäº‹å®æ€§ä»»åŠ¡ä¸Šè¯„ä¼°äº†å¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ç®—æœ¯ã€å°å­¦æ•°å­¦ã€å›½é™…è±¡æ£‹ç§»åŠ¨é¢„æµ‹ã€ä¼ è®°ç”Ÿæˆå’ŒMMLUã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•åœ¨è¿™äº›ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„å¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•ä¸ºæå‡LLMsçš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œå¯ä»¥åº”ç”¨äºå„ç§ä¸åŒçš„æ¨ç†å’Œäº‹å®æ€§ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ä¸å…¶ä»–æ¨¡å‹ç”Ÿæˆæ”¹è¿›æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡LLMsçš„æ€§èƒ½ã€‚</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        