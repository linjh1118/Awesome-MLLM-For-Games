# Paper List of baai_cite.md

- [24/08] **Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**  
[[Paper](http://arxiv.org/pdf/2408.03615v2)] [[Code/Page]()] [[TLDR/Notes](#optimus-1--hybrid-multimodal-memory-empowered-agents-excel-in-long-horizon-tasks)]

- [24/07] **VideoGameBunny: Towards vision assistants for video games**  
[[Paper](http://arxiv.org/pdf/2407.15295v1)] [[Code/Page](https://videogamebunny.github.io/)] [[TLDR/Notes](#videogamebunny--towards-vision-assistants-for-video-games)]

- [24/07] **Autoverse: An Evolvable Game Language for Learning Robust Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2407.04221v2)] [[Code/Page]()] [[TLDR/Notes](#autoverse--an-evolvable-game-language-for-learning-robust-embodied-agents)]

- [24/03] **Cradle: Empowering Foundation Agents Towards General Computer Control**  
[[Paper](http://arxiv.org/pdf/2403.03186v3)] [[Code/Page]()] [[TLDR/Notes](#cradle--empowering-foundation-agents-towards-general-computer-control)]

- [24/06] **Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models**  
[[Paper](http://arxiv.org/pdf/2406.14035v3)] [[Code/Page]()] [[TLDR/Notes](#using-game-play-to-investigate-multimodal-and-conversational-grounding-in-large-multimodal-models)]

- [24/05] **Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration**  
[[Paper](http://arxiv.org/pdf/2405.14314v2)] [[Code/Page](https://read-llm.github.io/.)] [[TLDR/Notes](#towards-efficient-llm-grounding-for-embodied-multi-agent-collaboration)]

- [22/10] **A Pilot Study on Teacher-Facing Real-Time Classroom Game Dashboards**  
[[Paper](http://arxiv.org/pdf/2210.09427v1)] [[Code/Page]()] [[TLDR/Notes](#a-pilot-study-on-teacher-facing-real-time-classroom-game-dashboards)]

- [24/01] **Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering**  
[[Paper](http://arxiv.org/pdf/2401.08500v1)] [[Code/Page](https://github.com/Codium-ai/AlphaCodium)] [[TLDR/Notes](#code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering)]

- [24/04] **Self-playing Adversarial Language Game Enhances LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2404.10642v3)] [[Code/Page](https://github.com/Linear95/SPAG.)] [[TLDR/Notes](#self-playing-adversarial-language-game-enhances-llm-reasoning)]

- [24/02] **Predicting Outcomes in Video Games with Long Short Term Memory Networks**  
[[Paper](http://arxiv.org/pdf/2402.15923v1)] [[Code/Page]()] [[TLDR/Notes](#predicting-outcomes-in-video-games-with-long-short-term-memory-networks)]

- [24/03] **Embodied LLM Agents Learn to Cooperate in Organized Teams**  
[[Paper](http://arxiv.org/pdf/2403.12482v2)] [[Code/Page]()] [[TLDR/Notes](#embodied-llm-agents-learn-to-cooperate-in-organized-teams)]

- [24/03] **Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot**  
[[Paper](http://arxiv.org/pdf/2403.11381v2)] [[Code/Page]()] [[TLDR/Notes](#can-llm-augmented-autonomous-agents-cooperate---an-evaluation-of-their-cooperative-capabilities-through-melting-pot)]

- [24/03] **EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2403.12014v2)] [[Code/Page]()] [[TLDR/Notes](#envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents)]

- [24/03] **MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control**  
[[Paper](http://arxiv.org/pdf/2403.12037v2)] [[Code/Page]()] [[TLDR/Notes](#minedreamer--learning-to-follow-instructions-via-chain-of-imagination-for-simulated-world-control)]

- [24/04] **Scaling Instructable Agents Across Many Simulated Worlds**  
[[Paper](http://arxiv.org/pdf/2404.10179v3)] [[Code/Page]()] [[TLDR/Notes](#scaling-instructable-agents-across-many-simulated-worlds)]

- [24/03] **Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation**  
[[Paper](http://arxiv.org/pdf/2403.08282v2)] [[Code/Page]()] [[TLDR/Notes](#hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation)]

- [24/03] **SOTOPIA-$Ï€$: Interactive Learning of Socially Intelligent Language Agents**  
[[Paper](http://arxiv.org/pdf/2403.08715v3)] [[Code/Page]()] [[TLDR/Notes](#sotopia-$Ï€$--interactive-learning-of-socially-intelligent-language-agents)]

- [24/03] **Will GPT-4 Run DOOM?**  
[[Paper](http://arxiv.org/pdf/2403.05468v1)] [[Code/Page]()] [[TLDR/Notes](#will-gpt-4-run-doom-)]

- [24/02] **Large Multimodal Agents: A Survey**  
[[Paper](http://arxiv.org/pdf/2402.15116v1)] [[Code/Page](https://github.com/jun0wanan/awesome-large-multimodal-agents.)] [[TLDR/Notes](#large-multimodal-agents--a-survey)]

- [24/03] **Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents**  
[[Paper](http://arxiv.org/pdf/2403.00690v1)] [[Code/Page]()] [[TLDR/Notes](#playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents)]

- [23/02] **Q-Cogni: An Integrated Causal Reinforcement Learning Framework**  
[[Paper](http://arxiv.org/pdf/2302.13240v1)] [[Code/Page]()] [[TLDR/Notes](#q-cogni--an-integrated-causal-reinforcement-learning-framework)]

- [24/02] **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**  
[[Paper](http://arxiv.org/pdf/2402.17574v3)] [[Code/Page]()] [[TLDR/Notes](#agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization)]

- [24/02] **PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain**  
[[Paper](http://arxiv.org/pdf/2402.15527v1)] [[Code/Page]()] [[TLDR/Notes](#pca-bench--evaluating-multimodal-large-language-models-in-perception-cognition-action-chain)]

- [24/02] **What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents**  
[[Paper](http://arxiv.org/pdf/2402.13184v5)] [[Code/Page](https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.)] [[TLDR/Notes](#what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents)]

- [14/03] **Software Agents Interaction Algorithms in Virtual Learning Environment**  
[[Paper](http://arxiv.org/pdf/1403.5734v2)] [[Code/Page]()] [[TLDR/Notes](#software-agents-interaction-algorithms-in-virtual-learning-environment)]

- [24/02] **Enhance Reasoning for Large Language Models in the Game Werewolf**  
[[Paper](http://arxiv.org/pdf/2402.02330v2)] [[Code/Page]()] [[TLDR/Notes](#enhance-reasoning-for-large-language-models-in-the-game-werewolf)]

- [24/02] **PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2402.01118v3)] [[Code/Page](https://github.com/git-disl/PokeLLMon.)] [[TLDR/Notes](#pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models)]

- [24/01] **SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models**  
[[Paper](http://arxiv.org/pdf/2401.17749v1)] [[Code/Page]()] [[TLDR/Notes](#swarmbrain--embodied-agent-for-real-time-strategy-game-starcraft-ii-via-large-language-models)]

- [24/01] **CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents**  
[[Paper](http://arxiv.org/pdf/2401.10568v2)] [[Code/Page](https://github.com/bigai-ai/civrealm.)] [[TLDR/Notes](#civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents)]

- [23/11] **Finding the Needle in a Haystack: Detecting Bug Occurrences in Gameplay Videos**  
[[Paper](http://arxiv.org/pdf/2311.10926v1)] [[Code/Page]()] [[TLDR/Notes](#finding-the-needle-in-a-haystack--detecting-bug-occurrences-in-gameplay-videos)]

- [24/01] **PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model**  
[[Paper](http://arxiv.org/pdf/2401.06781v1)] [[Code/Page]()] [[TLDR/Notes](#pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model)]

- [23/12] **Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game**  
[[Paper](http://arxiv.org/pdf/2312.17515v1)] [[Code/Page]()] [[TLDR/Notes](#cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game)]

- [23/12] **LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination**  
[[Paper](http://arxiv.org/pdf/2312.15224v2)] [[Code/Page]()] [[TLDR/Notes](#llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination)]

- [23/12] **Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach**  
[[Paper](http://arxiv.org/pdf/2312.11865v3)] [[Code/Page]()] [[TLDR/Notes](#large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach)]

- [23/12] **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**  
[[Paper](http://arxiv.org/pdf/2312.09238v2)] [[Code/Page]()] [[TLDR/Notes](#auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft)]

- [23/12] **MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception**  
[[Paper](http://arxiv.org/pdf/2312.07472v4)] [[Code/Page]()] [[TLDR/Notes](#mp5--a-multi-modal-open-ended-embodied-system-in-minecraft-via-active-perception)]

- [25/02] **Stay Focused: Problem Drift in Multi-Agent Debate**  
[[Paper](http://arxiv.org/pdf/2502.19559v1)] [[Code/Page]()] [[TLDR/Notes](#stay-focused--problem-drift-in-multi-agent-debate)]

- [23/12] **GlitchBench: Can large multimodal models detect video game glitches?**  
[[Paper](http://arxiv.org/pdf/2312.05291v2)] [[Code/Page](https://glitchbench.github.io/)] [[TLDR/Notes](#glitchbench--can-large-multimodal-models-detect-video-game-glitches-)]

- [18/08] **A Framework for Complementary Companion Character Behavior in Video Games**  
[[Paper](http://arxiv.org/pdf/1808.09079v1)] [[Code/Page]()] [[TLDR/Notes](#a-framework-for-complementary-companion-character-behavior-in-video-games)]

- [23/12] **Creative Agents: Empowering Agents with Imagination for Creative Tasks**  
[[Paper](http://arxiv.org/pdf/2312.02519v1)] [[Code/Page](https://github.com/PKU-RL/Creative-Agents).)] [[TLDR/Notes](#creative-agents--empowering-agents-with-imagination-for-creative-tasks)]

- [23/12] **Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games**  
[[Paper](http://arxiv.org/pdf/2312.02312v1)] [[Code/Page]()] [[TLDR/Notes](#visual-encoders-for-data-efficient-imitation-learning-in-modern-video-games)]

- [24/01] **Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation**  
[[Paper](http://arxiv.org/pdf/2401.00006v3)] [[Code/Page]()] [[TLDR/Notes](#building-open-ended-embodied-agent-via-language-policy-bidirectional-adaptation)]

- [23/12] **Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games**  
[[Paper](http://arxiv.org/pdf/2312.00746v2)] [[Code/Page]()] [[TLDR/Notes](#deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games)]

- [23/11] **War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars**  
[[Paper](http://arxiv.org/pdf/2311.17227v2)] [[Code/Page](https://github.com/agiresearch/WarAgent}.)] [[TLDR/Notes](#war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars)]

- [23/11] **See and Think: Embodied Agent in Virtual Environment**  
[[Paper](http://arxiv.org/pdf/2311.15209v3)] [[Code/Page]()] [[TLDR/Notes](#see-and-think--embodied-agent-in-virtual-environment)]

- [23/11] **DesignGPT: Multi-Agent Collaboration in Design**  
[[Paper](http://arxiv.org/pdf/2311.11591v1)] [[Code/Page]()] [[TLDR/Notes](#designgpt--multi-agent-collaboration-in-design)]

- [23/11] **MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration**  
[[Paper](http://arxiv.org/pdf/2311.08562v3)] [[Code/Page](https://github.com/cathyxl/MAgIC.)] [[TLDR/Notes](#magic--investigation-of-large-language-model-powered-multi-agent-in-cognition--adaptability--rationality-and-collaboration)]

- [24/03] **MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs**  
[[Paper](http://arxiv.org/pdf/2403.19267v2)] [[Code/Page](https://github.com/cocacola-lab/MineLand.)] [[TLDR/Notes](#mineland--simulating-large-scale-multi-agent-interactions-with-limited-multimodal-senses-and-physical-needs)]

- [23/11] **ADaPT: As-Needed Decomposition and Planning with Language Models**  
[[Paper](http://arxiv.org/pdf/2311.05772v2)] [[Code/Page]()] [[TLDR/Notes](#adapt--as-needed-decomposition-and-planning-with-language-models)]

- [23/10] **Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.20499v2)] [[Code/Page]()] [[TLDR/Notes](#leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models)]

- [23/10] **Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game**  
[[Paper](http://arxiv.org/pdf/2310.18940v3)] [[Code/Page]()] [[TLDR/Notes](#language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game)]

- [23/10] **LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay**  
[[Paper](http://arxiv.org/pdf/2310.14985v4)] [[Code/Page](https://github.com/3DAgentWorld/LLM-Game-Agent.)] [[TLDR/Notes](#llm-based-agent-society-investigation--collaboration-and-confrontation-in-avalon-gameplay)]

- [23/10] **Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds**  
[[Paper](http://arxiv.org/pdf/2310.13255v2)] [[Code/Page]()] [[TLDR/Notes](#steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds)]

- [23/10] **SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents**  
[[Paper](http://arxiv.org/pdf/2310.11667v2)] [[Code/Page]()] [[TLDR/Notes](#sotopia--interactive-evaluation-for-social-intelligence-in-language-agents)]

- [23/10] **Character-LLM: A Trainable Agent for Role-Playing**  
[[Paper](http://arxiv.org/pdf/2310.10158v2)] [[Code/Page]()] [[TLDR/Notes](#character-llm--a-trainable-agent-for-role-playing)]

- [23/10] **LLaMA Rider: Spurring Large Language Models to Explore the Open World**  
[[Paper](http://arxiv.org/pdf/2310.08922v1)] [[Code/Page]()] [[TLDR/Notes](#llama-rider--spurring-large-language-models-to-explore-the-open-world)]

- [23/10] **GameGPT: Multi-agent Collaborative Framework for Game Development**  
[[Paper](http://arxiv.org/pdf/2310.08067v1)] [[Code/Page]()] [[TLDR/Notes](#gamegpt--multi-agent-collaborative-framework-for-game-development)]

- [23/10] **GROOT: Learning to Follow Instructions by Watching Gameplay Videos**  
[[Paper](http://arxiv.org/pdf/2310.08235v2)] [[Code/Page](https://craftjarvis-groot.github.io.)] [[TLDR/Notes](#groot--learning-to-follow-instructions-by-watching-gameplay-videos)]

- [23/10] **Octopus: Embodied Vision-Language Programmer from Environmental Feedback**  
[[Paper](http://arxiv.org/pdf/2310.08588v2)] [[Code/Page]()] [[TLDR/Notes](#octopus--embodied-vision-language-programmer-from-environmental-feedback)]

- [23/10] **MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.06500v1)] [[Code/Page]()] [[TLDR/Notes](#metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents)]

- [23/10] **Humanoid Agents: Platform for Simulating Human-like Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.05418v1)] [[Code/Page](https://www.humanoidagents.com/)] [[TLDR/Notes](#humanoid-agents--platform-for-simulating-human-like-generative-agents)]

- [23/10] **AvalonBench: Evaluating LLMs Playing the Game of Avalon**  
[[Paper](http://arxiv.org/pdf/2310.05036v3)] [[Code/Page]()] [[TLDR/Notes](#avalonbench--evaluating-llms-playing-the-game-of-avalon)]

- [25/02] **Beyond Win Rates: A Clustering-Based Approach to Character Balance Analysis in Team-Based Games**  
[[Paper](http://arxiv.org/pdf/2502.01250v1)] [[Code/Page]()] [[TLDR/Notes](#beyond-win-rates--a-clustering-based-approach-to-character-balance-analysis-in-team-based-games)]

- [23/10] **LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03903v2)] [[Code/Page](https://github.com/eric-ai-lab/llm_coordination}.)] [[TLDR/Notes](#llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models)]

- [23/10] **Lyfe Agents: Generative agents for low-cost real-time social interactions**  
[[Paper](http://arxiv.org/pdf/2310.02172v1)] [[Code/Page]()] [[TLDR/Notes](#lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions)]

- [21/11] **Adaptive Multi-Goal Exploration**  
[[Paper](http://arxiv.org/pdf/2111.12045v2)] [[Code/Page]()] [[TLDR/Notes](#adaptive-multi-goal-exploration)]

- [23/10] **SmartPlay: A Benchmark for LLMs as Intelligent Agents**  
[[Paper](http://arxiv.org/pdf/2310.01557v5)] [[Code/Page]()] [[TLDR/Notes](#smartplay--a-benchmark-for-llms-as-intelligent-agents)]

- [23/10] **Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation**  
[[Paper](http://arxiv.org/pdf/2310.01320v3)] [[Code/Page]()] [[TLDR/Notes](#avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation)]

- [23/10] **RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.00746v3)] [[Code/Page]()] [[TLDR/Notes](#rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models)]

- [23/09] **AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback**  
[[Paper](http://arxiv.org/pdf/2309.17176v3)] [[Code/Page]()] [[TLDR/Notes](#adarefiner--refining-decisions-of-language-models-with-adaptive-feedback)]

- [23/10] **Motif: Intrinsic Motivation from Artificial Intelligence Feedback**  
[[Paper](http://arxiv.org/pdf/2310.00166v1)] [[Code/Page]()] [[TLDR/Notes](#motif--intrinsic-motivation-from-artificial-intelligence-feedback)]

- [24/09] **SimulBench: Evaluating Language Models with Creative Simulation Tasks**  
[[Paper](http://arxiv.org/pdf/2409.07641v1)] [[Code/Page]()] [[TLDR/Notes](#simulbench--evaluating-language-models-with-creative-simulation-tasks)]

- [23/09] **Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4**  
[[Paper](http://arxiv.org/pdf/2309.17277v3)] [[Code/Page]()] [[TLDR/Notes](#suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4)]

- [23/09] **AutoAgents: A Framework for Automatic Agent Generation**  
[[Paper](http://arxiv.org/pdf/2309.17288v3)] [[Code/Page](https://github.com/Link-AGI/AutoAgents.)] [[TLDR/Notes](#autoagents--a-framework-for-automatic-agent-generation)]

- [24/01] **True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2401.14151v2)] [[Code/Page]()] [[TLDR/Notes](#true-knowledge-comes-from-practice--aligning-llms-with-embodied-environments-via-reinforcement-learning)]

- [23/09] **MindAgent: Emergent Gaming Interaction**  
[[Paper](http://arxiv.org/pdf/2309.09971v2)] [[Code/Page]()] [[TLDR/Notes](#mindagent--emergent-gaming-interaction)]

- [23/09] **Agents: An Open-source Framework for Autonomous Language Agents**  
[[Paper](http://arxiv.org/pdf/2309.07870v3)] [[Code/Page](https://github.com/aiwaves-cn/agents.)] [[TLDR/Notes](#agents--an-open-source-framework-for-autonomous-language-agents)]

- [23/09] **Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf**  
[[Paper](http://arxiv.org/pdf/2309.04658v2)] [[Code/Page]()] [[TLDR/Notes](#exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf)]

- [23/08] **Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis**  
[[Paper](http://arxiv.org/pdf/2308.12466v2)] [[Code/Page]()] [[TLDR/Notes](#are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis)]

- [23/09] **Towards Ontology Construction with Language Models**  
[[Paper](http://arxiv.org/pdf/2309.09898v1)] [[Code/Page]()] [[TLDR/Notes](#towards-ontology-construction-with-language-models)]

- [23/08] **AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors**  
[[Paper](http://arxiv.org/pdf/2308.10848v3)] [[Code/Page](https://github.com/OpenBMB/AgentVerse}.)] [[TLDR/Notes](#agentverse--facilitating-multi-agent-collaboration-and-exploring-emergent-behaviors)]

- [23/08] **GameEval: Evaluating LLMs on Conversational Games**  
[[Paper](http://arxiv.org/pdf/2308.10032v1)] [[Code/Page](https://github.com/GameEval/GameEval.)] [[TLDR/Notes](#gameeval--evaluating-llms-on-conversational-games)]

- [23/08] **AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation**  
[[Paper](http://arxiv.org/pdf/2308.08155v2)] [[Code/Page]()] [[TLDR/Notes](#autogen--enabling-next-gen-llm-applications-via-multi-agent-conversation)]

- [23/08] **CALYPSO: LLMs as Dungeon Masters' Assistants**  
[[Paper](http://arxiv.org/pdf/2308.07540v1)] [[Code/Page]()] [[TLDR/Notes](#calypso--llms-as-dungeon-masters--assistants)]

- [23/08] **AgentSims: An Open-Source Sandbox for Large Language Model Evaluation**  
[[Paper](http://arxiv.org/pdf/2308.04026v1)] [[Code/Page](https://agentsims.com)] [[TLDR/Notes](#agentsims--an-open-source-sandbox-for-large-language-model-evaluation)]

- [23/08] **MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework**  
[[Paper](http://arxiv.org/pdf/2308.00352v7)] [[Code/Page](https://github.com/geekan/MetaGPT)] [[TLDR/Notes](#metagpt--meta-programming-for-a-multi-agent-collaborative-framework)]

- [24/06] **EmoLLM: Multimodal Emotional Understanding Meets Large Language Models**  
[[Paper](http://arxiv.org/pdf/2406.16442v2)] [[Code/Page]()] [[TLDR/Notes](#emollm--multimodal-emotional-understanding-meets-large-language-models)]

- [23/07] **Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors**  
[[Paper](http://arxiv.org/pdf/2307.11922v1)] [[Code/Page]()] [[TLDR/Notes](#selective-perception--optimizing-state-descriptions-with-reinforcement-learning-for-language-model-actors)]

- [23/07] **SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning**  
[[Paper](http://arxiv.org/pdf/2307.06135v2)] [[Code/Page](https://sayplan.github.io.)] [[TLDR/Notes](#sayplan--grounding-large-language-models-using-3d-scene-graphs-for-scalable-robot-task-planning)]

- [23/07] **Building Cooperative Embodied Agents Modularly with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2307.02485v2)] [[Code/Page](https://vis-www.cs.umass.edu/Co-LLM-Agents/.)] [[TLDR/Notes](#building-cooperative-embodied-agents-modularly-with-large-language-models)]

- [23/07] **Embodied Task Planning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2307.01848v1)] [[Code/Page]()] [[TLDR/Notes](#embodied-task-planning-with-large-language-models)]

- [23/06] **SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling**  
[[Paper](http://arxiv.org/pdf/2306.11886v3)] [[Code/Page](https://clvrai.com/sprint.)] [[TLDR/Notes](#sprint--scalable-policy-pre-training-via-language-instruction-relabeling)]

- [23/06] **ChessGPT: Bridging Policy Learning and Language Modeling**  
[[Paper](http://arxiv.org/pdf/2306.09200v2)] [[Code/Page](https://github.com/waterhorse1/ChessGPT.)] [[TLDR/Notes](#chessgpt--bridging-policy-learning-and-language-modeling)]

- [23/06] **OMNI: Open-endedness via Models of human Notions of Interestingness**  
[[Paper](http://arxiv.org/pdf/2306.01711v3)] [[Code/Page](https://www.jennyzhangzt.com/omni/)] [[TLDR/Notes](#omni--open-endedness-via-models-of-human-notions-of-interestingness)]

- [23/06] **STEVE-1: A Generative Model for Text-to-Behavior in Minecraft**  
[[Paper](http://arxiv.org/pdf/2306.00937v3)] [[Code/Page]()] [[TLDR/Notes](#steve-1--a-generative-model-for-text-to-behavior-in-minecraft)]

- [18/08] **The Text-Based Adventure AI Competition**  
[[Paper](http://arxiv.org/pdf/1808.01262v4)] [[Code/Page]()] [[TLDR/Notes](#the-text-based-adventure-ai-competition)]

- [23/05] **AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation**  
[[Paper](http://arxiv.org/pdf/2305.18898v1)] [[Code/Page](https://www.youtube.com/watch?v=ayAzID1_qQk)] [[TLDR/Notes](#alphablock--embodied-finetuning-for-vision-language-reasoning-in-robot-manipulation)]

- [23/05] **Playing repeated games with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.16867v1)] [[Code/Page]()] [[TLDR/Notes](#playing-repeated-games-with-large-language-models)]

- [23/05] **Voyager: An Open-Ended Embodied Agent with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.16291v2)] [[Code/Page](https://voyager.minedojo.org/.)] [[TLDR/Notes](#voyager--an-open-ended-embodied-agent-with-large-language-models)]

- [23/05] **Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory**  
[[Paper](http://arxiv.org/pdf/2305.17144v2)] [[Code/Page](https://github.com/OpenGVLab/GITM.)] [[TLDR/Notes](#ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory)]

- [23/05] **SPRING: Studying the Paper and Reasoning to Play Games**  
[[Paper](http://arxiv.org/pdf/2305.15486v3)] [[Code/Page]()] [[TLDR/Notes](#spring--studying-the-paper-and-reasoning-to-play-games)]

- [23/05] **Improving Factuality and Reasoning in Language Models through Multiagent Debate**  
[[Paper](http://arxiv.org/pdf/2305.14325v1)] [[Code/Page]()] [[TLDR/Notes](#improving-factuality-and-reasoning-in-language-models-through-multiagent-debate)]

- [23/05] **Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback**  
[[Paper](http://arxiv.org/pdf/2305.10142v1)] [[Code/Page]()] [[TLDR/Notes](#improving-language-model-negotiation-with-self-play-and-in-context-learning-from-ai-feedback)]

- [23/05] **TidyBot: Personalized Robot Assistance with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.05658v2)] [[Code/Page]()] [[TLDR/Notes](#tidybot--personalized-robot-assistance-with-large-language-models)]

- [23/05] **ArK: Augmented Reality with Knowledge Interactive Emergent Ability**  
[[Paper](http://arxiv.org/pdf/2305.00970v1)] [[Code/Page]()] [[TLDR/Notes](#ark--augmented-reality-with-knowledge-interactive-emergent-ability)]

- [23/04] **Generative Agents: Interactive Simulacra of Human Behavior**  
[[Paper](http://arxiv.org/pdf/2304.03442v2)] [[Code/Page]()] [[TLDR/Notes](#generative-agents--interactive-simulacra-of-human-behavior)]

- [23/04] **Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions**  
[[Paper](http://arxiv.org/pdf/2304.02868v1)] [[Code/Page]()] [[TLDR/Notes](#can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions)]

- [24/11] **Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey**  
[[Paper](http://arxiv.org/pdf/2411.16403v1)] [[Code/Page]()] [[TLDR/Notes](#adapter-based-approaches-to-knowledge-enhanced-language-models----a-survey)]

- [23/03] **CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society**  
[[Paper](http://arxiv.org/pdf/2303.17760v2)] [[Code/Page](https://github.com/camel-ai/camel.)] [[TLDR/Notes](#camel--communicative-agents-for--mind--exploration-of-large-language-model-society)]

- [22/10] **LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation**  
[[Paper](http://arxiv.org/pdf/2210.12631v2)] [[Code/Page]()] [[TLDR/Notes](#league--guided-skill-learning-and-abstraction-for-long-horizon-manipulation)]

- [23/03] **PaLM-E: An Embodied Multimodal Language Model**  
[[Paper](http://arxiv.org/pdf/2303.03378v1)] [[Code/Page]()] [[TLDR/Notes](#palm-e--an-embodied-multimodal-language-model)]

- [23/02] **Guiding Pretraining in Reinforcement Learning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2302.06692v2)] [[Code/Page](https://github.com/yuqingd/ellm.)] [[TLDR/Notes](#guiding-pretraining-in-reinforcement-learning-with-large-language-models)]

- [23/02] **MarioGPT: Open-Ended Text2Level Generation through Large Language Models**  
[[Paper](http://arxiv.org/pdf/2302.05981v3)] [[Code/Page](https://github.com/shyamsn97/mario-gpt.)] [[TLDR/Notes](#mariogpt--open-ended-text2level-generation-through-large-language-models)]

- [23/02] **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**  
[[Paper](http://arxiv.org/pdf/2302.01560v3)] [[Code/Page](https://github.com/CraftJarvis/MC-Planner.)] [[TLDR/Notes](#describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents)]

- [23/01] **Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling**  
[[Paper](http://arxiv.org/pdf/2301.12050v2)] [[Code/Page]()] [[TLDR/Notes](#do-embodied-agents-dream-of-pixelated-sheep--embodied-decision-making-using-language-guided-world-modelling)]

- [23/01] **Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction**  
[[Paper](http://arxiv.org/pdf/2301.10034v3)] [[Code/Page]()] [[TLDR/Notes](#open-world-multi-task-control-through-goal-aware-representation-learning-and-adaptive-horizon-prediction)]

- [23/10] **Welfare Diplomacy: Benchmarking Language Model Cooperation**  
[[Paper](http://arxiv.org/pdf/2310.08901v1)] [[Code/Page](https://github.com/mukobi/welfare-diplomacy.)] [[TLDR/Notes](#welfare-diplomacy--benchmarking-language-model-cooperation)]

- [22/11] **Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2211.11736v3)] [[Code/Page]()] [[TLDR/Notes](#robotic-skill-acquisition-via-instruction-augmentation-with-vision-language-models)]

- [22/10] **Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task**  
[[Paper](http://arxiv.org/pdf/2210.13382v5)] [[Code/Page]()] [[TLDR/Notes](#emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task)]

- [22/10] **Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors**  
[[Paper](http://arxiv.org/pdf/2210.02506v1)] [[Code/Page](https://asgaardlab.github.io/LLMxBugs)] [[TLDR/Notes](#large-language-models-are-pretty-good-zero-shot-video-game-bug-detectors)]

- [22/08] **Social Simulacra: Creating Populated Prototypes for Social Computing Systems**  
[[Paper](http://arxiv.org/pdf/2208.04024v1)] [[Code/Page]()] [[TLDR/Notes](#social-simulacra--creating-populated-prototypes-for-social-computing-systems)]

- [22/07] **Inner Monologue: Embodied Reasoning through Planning with Language Models**  
[[Paper](http://arxiv.org/pdf/2207.05608v1)] [[Code/Page]()] [[TLDR/Notes](#inner-monologue--embodied-reasoning-through-planning-with-language-models)]

- [22/06] **Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos**  
[[Paper](http://arxiv.org/pdf/2206.11795v1)] [[Code/Page]()] [[TLDR/Notes](#video-pretraining-(vpt)--learning-to-act-by-watching-unlabeled-online-videos)]

- [22/06] **MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**  
[[Paper](http://arxiv.org/pdf/2206.08853v2)] [[Code/Page](https://minedojo.org))] [[TLDR/Notes](#minedojo--building-open-ended-embodied-agents-with-internet-scale-knowledge)]



# TLDR/Notes
## face-recognition-methods-&-applications
### Abstract
Face recognition presents a challenging problem in the field of image
analysis and computer vision. The security of information is becoming very
significant and difficult. Security cameras are presently common in airports,
Offices, University, ATM, Bank and in any locations with a security system.
Face recognition is a biometric system used to identify or verify a person from
a digital image. Face Recognition system is used in security. Face recognition
system should be able to automatically detect a face in an image. This involves
extracts its features and then recognize it, regardless of lighting,
expression, illumination, ageing, transformations (translate, rotate and scale
image) and pose, which is a difficult task. This paper contains three sections.
The first section describes the common methods like holistic matching method,
feature extraction method and hybrid methods. The second section describes
applications with examples and finally third section describes the future
research directions of face recognition.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ã€Šäººè„¸è¯†åˆ«æ–¹æ³•ä¸åº”ç”¨ã€‹ï¼šæ·±å…¥æ¢ç´¢äººè„¸è¯†åˆ«æŠ€æœ¯çš„å¥¥ç§˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ä¿¡æ¯å®‰å…¨å˜å¾—è¶Šæ¥è¶Šé‡è¦å’Œå¤æ‚ï¼Œäººè„¸è¯†åˆ«ä½œä¸ºä¸€ç§ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿï¼Œåœ¨å›¾åƒåˆ†æå’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨äººè„¸è¯†åˆ«çš„å¸¸è§æ–¹æ³•ã€åº”ç”¨å®ä¾‹ä»¥åŠæœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥æœŸä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œå®è·µæä¾›å‚è€ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨é¢æ¢³ç†äººè„¸è¯†åˆ«æ–¹æ³•
æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†ä¸‰ç§ä¸»æµçš„äººè„¸è¯†åˆ«æ–¹æ³•ï¼šæ•´ä½“åŒ¹é…æ–¹æ³•ã€ç‰¹å¾æå–æ–¹æ³•å’Œæ··åˆæ–¹æ³•ã€‚æ•´ä½“åŒ¹é…æ–¹æ³•å¦‚Eigenfacesï¼Œé€šè¿‡ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æå–é¢éƒ¨ç‰¹å¾ï¼›ç‰¹å¾æå–æ–¹æ³•å…³æ³¨å±€éƒ¨ç‰¹å¾ï¼Œå¦‚çœ¼ç›ã€é¼»å­å’Œå˜´å·´ï¼›æ··åˆæ–¹æ³•ç»“åˆäº†æ•´ä½“å’Œå±€éƒ¨ç‰¹å¾ï¼Œé€šå¸¸ä½¿ç”¨3Då›¾åƒè¿›è¡Œè¯†åˆ«ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸°å¯Œçš„äººè„¸è¯†åˆ«åº”ç”¨æ¡ˆä¾‹
æ–‡ç« ä¸ä»…ä»‹ç»äº†äººè„¸è¯†åˆ«çš„åŸºæœ¬æ–¹æ³•ï¼Œè¿˜æä¾›äº†å¤šä¸ªå®é™…åº”ç”¨æ¡ˆä¾‹ï¼Œå¦‚é€‰æ°‘æ³¨å†Œç³»ç»Ÿä¸­çš„é‡å¤èº«ä»½è¯†åˆ«ã€è®¡ç®—æœºç™»å½•ç›‘æ§ã€æœºåœºå®‰å…¨ç³»ç»Ÿã€å›¾åƒæ•°æ®åº“è°ƒæŸ¥ç­‰ï¼Œå±•ç¤ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰è¯¦ç»†æè¿°å…·ä½“çš„å®éªŒç»“æœï¼Œä½†é€šè¿‡æ–‡çŒ®ç»¼è¿°å’Œæ¡ˆä¾‹åˆ†æï¼Œå±•ç¤ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨å¤šç§åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ–¹æ³•å¤šæ ·æ€§**ï¼šæœ¬æ–‡æä¾›äº†å¤šç§äººè„¸è¯†åˆ«æ–¹æ³•ï¼Œç ”ç©¶è€…å¯ä»¥æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚
2. **å®é™…åº”ç”¨æ¡ˆä¾‹**ï¼šé€šè¿‡å®é™…æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨ç°å®ä¸–ç•Œä¸­çš„å…·ä½“åº”ç”¨ï¼Œä¸ºå…¶ä»–ç ”ç©¶è€…æä¾›äº†å®è·µå‚è€ƒã€‚
3. **æœªæ¥ç ”ç©¶æ–¹å‘**ï¼šæ–‡ç« æŒ‡å‡ºäº†äººè„¸è¯†åˆ«æŠ€æœ¯çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œå¦‚2Då’Œ3Däººè„¸è¯†åˆ«ã€å¤§è§„æ¨¡åº”ç”¨ç­‰ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†æ–¹å‘æŒ‡å¼•ã€‚

æ€»ä¹‹ï¼Œæœ¬æ–‡å¯¹äººè„¸è¯†åˆ«æŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„æ¢³ç†å’Œæ¢è®¨ï¼Œå¯¹äºäººè„¸è¯†åˆ«é¢†åŸŸçš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆå…·æœ‰å¾ˆé«˜çš„å‚è€ƒä»·å€¼ã€‚

## optimus-1--hybrid-multimodal-memory-empowered-agents-excel-in-long-horizon-tasks
### Abstract
Building a general-purpose agent is a long-standing vision in the field of
artificial intelligence. Existing agents have made remarkable progress in many
domains, yet they still struggle to complete long-horizon tasks in an open
world. We attribute this to the lack of necessary world knowledge and
multimodal experience that can guide agents through a variety of long-horizon
tasks. In this paper, we propose a Hybrid Multimodal Memory module to address
the above challenges. It 1) transforms knowledge into Hierarchical Directed
Knowledge Graph that allows agents to explicitly represent and learn world
knowledge, and 2) summarises historical information into Abstracted Multimodal
Experience Pool that provide agents with rich references for in-context
learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,
Optimus-1, is constructed with dedicated Knowledge-guided Planner and
Experience-Driven Reflector, contributing to a better planning and reflection
in the face of long-horizon tasks in Minecraft. Extensive experimental results
show that Optimus-1 significantly outperforms all existing agents on
challenging long-horizon task benchmarks, and exhibits near human-level
performance on many tasks. In addition, we introduce various Multimodal Large
Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show
that Optimus-1 exhibits strong generalization with the help of the Hybrid
Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Optimus-1ï¼šæ··åˆå¤šæ¨¡æ€è®°å¿†èµ‹èƒ½ä»£ç†åœ¨é•¿æ—¶ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ„å»ºä¸€ä¸ªé€šç”¨ä»£ç†æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸé•¿æœŸä»¥æ¥çš„æ„¿æ™¯ã€‚å°½ç®¡ç°æœ‰çš„ä»£ç†åœ¨è®¸å¤šé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥åœ¨å¼€æ”¾ä¸–ç•Œä¸­å®Œæˆé•¿æ—¶ä»»åŠ¡ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¿…è¦çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¤šæ¨¡æ€ç»éªŒï¼Œè¿™äº›çŸ¥è¯†å’Œç»éªŒå¯ä»¥æŒ‡å¯¼ä»£ç†å®Œæˆå„ç§é•¿æ—¶ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”±åˆ†å±‚æœ‰å‘çŸ¥è¯†å›¾ï¼ˆHDKGï¼‰å’ŒæŠ½è±¡å¤šæ¨¡æ€ç»éªŒæ± ï¼ˆAMEPï¼‰ç»„æˆã€‚HDKGå°†çŸ¥è¯†è½¬åŒ–ä¸ºåˆ†å±‚æœ‰å‘å›¾ç»“æ„ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ˜¾å¼åœ°è¡¨ç¤ºå’Œå­¦ä¹ ä¸–ç•ŒçŸ¥è¯†ã€‚AMEPå°†å†å²ä¿¡æ¯æ€»ç»“ä¸ºæŠ½è±¡å¤šæ¨¡æ€ç»éªŒæ± ï¼Œä¸ºä»£ç†æä¾›ä¸°å¯Œçš„å‚è€ƒï¼Œä»¥ä¾¿è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šOptimus-1ä»£ç†
åœ¨æ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—çš„åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€ä»£ç†Optimus-1ï¼Œè¯¥ä»£ç†ç”±çŸ¥è¯†å¼•å¯¼è§„åˆ’å™¨ã€ç»éªŒé©±åŠ¨åæ€å™¨å’ŒåŠ¨ä½œæ§åˆ¶å™¨ç»„æˆã€‚çŸ¥è¯†å¼•å¯¼è§„åˆ’å™¨åˆ©ç”¨HDKGæ¥æ•è·å®Œæˆä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„å­ç›®æ ‡åºåˆ—ã€‚åŠ¨ä½œæ§åˆ¶å™¨æ ¹æ®å­ç›®æ ‡å’Œå½“å‰è§‚å¯Ÿç”Ÿæˆä½çº§åŠ¨ä½œï¼Œä¸æ¸¸æˆç¯å¢ƒäº¤äº’ä»¥æ›´æ–°ä»£ç†çš„çŠ¶æ€ã€‚ç»éªŒé©±åŠ¨åæ€å™¨å®šæœŸæ¿€æ´»ï¼Œä»AMEPä¸­æ£€ç´¢ç›¸å…³çš„å¤šæ¨¡æ€ç»éªŒï¼Œä»¥è¯„ä¼°å½“å‰å­ç›®æ ‡æ˜¯å¦å¯ä»¥æˆåŠŸæ‰§è¡Œã€‚å¦‚æœä¸è¡Œï¼Œå®ƒä¼šæŒ‡ç¤ºçŸ¥è¯†å¼•å¯¼è§„åˆ’å™¨ä¿®æ”¹å…¶è®¡åˆ’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Minecraftä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒOptimus-1åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿æ—¶ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºæ‰€æœ‰ç°æœ‰ä»£ç†ï¼Œå¹¶åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†å„ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºOptimus-1çš„éª¨å¹²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—çš„å¸®åŠ©ä¸‹ï¼ŒOptimus-1åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºGPT-4VåŸºçº¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ··åˆå¤šæ¨¡æ€è®°å¿†æ¨¡å—å’ŒOptimus-1ä»£ç†ä¸ºæ„å»ºèƒ½å¤Ÿå®Œæˆé•¿æ—¶ä»»åŠ¡çš„é€šç”¨ä»£ç†æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„éå‚æ•°å­¦ä¹ æ–¹æ³•ä¹Ÿä¸ºä»£ç†çš„å­¦ä¹ å’Œè¿›åŒ–æä¾›äº†æ–°çš„æ€è·¯ã€‚

## videogamebunny--towards-vision-assistants-for-video-games
### Abstract
Large multimodal models (LMMs) hold substantial promise across various
domains, from personal assistance in daily tasks to sophisticated applications
like medical diagnostics. However, their capabilities have limitations in the
video game domain, such as challenges with scene understanding, hallucinations,
and inaccurate descriptions of video game content, especially in open-source
models. This paper describes the development of VideoGameBunny, a LLaVA-style
model based on Bunny, specifically tailored for understanding images from video
games. We release intermediate checkpoints, training logs, and an extensive
dataset comprising 185,259 video game images from 413 titles, along with
389,565 image-instruction pairs that include image captions, question-answer
pairs, and a JSON representation of 16 elements of 136,974 images. Our
experiments show that our high quality game-related data has the potential to
make a relatively small model outperform the much larger state-of-the-art model
LLaVa-1.6-34b (which has more than 4x the number of parameters). Our study
paves the way for future research in video game understanding on tasks such as
playing, commentary, and debugging. Code and data are available at
https://videogamebunny.github.io/
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§†é¢‘æ¸¸æˆåŠ©æ‰‹ï¼šVideoGameBunny

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è§†é¢‘æ¸¸æˆäº§ä¸šçš„è“¬å‹ƒå‘å±•ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LMMsåœ¨ç†è§£æ¸¸æˆå†…å®¹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚åœºæ™¯ç†è§£å›°éš¾ã€å¹»è§‰ç°è±¡ä»¥åŠæ¸¸æˆå†…å®¹çš„æè¿°ä¸å‡†ç¡®ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡å¼€å‘ä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ¸¸æˆå†…å®¹ç†è§£çš„æ¨¡å‹ï¼Œå³VideoGameBunnyï¼Œæ¥æå‡LMMsåœ¨æ¸¸æˆé¢†åŸŸçš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šVideoGameBunnyæ¨¡å‹
VideoGameBunnyæ˜¯ä¸€ä¸ªåŸºäºBunnyæ¨¡å‹çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»è¿‡ä¸“é—¨é’ˆå¯¹æ¸¸æˆå†…å®¹çš„å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†æ¸¸æˆå›¾åƒã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†LLaVA-styleæ¶æ„ï¼Œé€šè¿‡å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å°†è§†è§‰åµŒå…¥ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œä»è€Œå®ç°å›¾åƒå’Œæ–‡æœ¬çš„èåˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¸¸æˆç›¸å…³æ•°æ®é›†
ä¸ºäº†è®­ç»ƒVideoGameBunnyï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«185,259å¼ æ¸¸æˆå›¾åƒå’Œ389,565ä¸ªå›¾åƒ-æŒ‡ä»¤å¯¹çš„æ•°æ®é›†ã€‚è¿™äº›å›¾åƒæ¥è‡ª413æ¬¾ä¸åŒçš„æ¸¸æˆï¼Œæ¶µç›–äº†å„ç§æ¸¸æˆç±»å‹ã€å›¾å½¢é£æ ¼å’Œæ¸¸æˆæœºåˆ¶ã€‚å›¾åƒ-æŒ‡ä»¤å¯¹åŒ…æ‹¬å›¾åƒæ ‡é¢˜ã€é—®ç­”å¯¹å’Œå›¾åƒçš„JSONè¡¨ç¤ºï¼Œä¸ºæ¨¡å‹æä¾›äº†ä¸°å¯Œçš„æ¸¸æˆç›¸å…³æ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoGameBunnyåœ¨æ¸¸æˆå†…å®¹ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ç°æœ‰çš„SOTAæ¨¡å‹LLaVa-1.6-34bç›¸æ¯”ï¼ŒVideoGameBunnyåœ¨æ¸¸æˆç›¸å…³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œå›¾åƒ-to-JSONæ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡æœ€ä¸ºæ˜¾è‘—ï¼Œè€ŒçŸ­æ ‡é¢˜æ•°æ®é›†åˆ™å¯èƒ½å¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
VideoGameBunnyçš„ç ”ç©¶æˆæœä¸ºæ¸¸æˆå†…å®¹ç†è§£é¢†åŸŸæä¾›äº†é‡è¦çš„å‚è€ƒã€‚é€šè¿‡æ„å»ºä¸“é—¨é’ˆå¯¹æ¸¸æˆå†…å®¹çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æå‡LMMsåœ¨æ¸¸æˆé¢†åŸŸçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ä¸åŒç±»å‹çš„æ•°æ®é›†å’Œæ··åˆç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚

## autoverse--an-evolvable-game-language-for-learning-robust-embodied-agents
### Abstract
We introduce Autoverse, an evolvable, domain-specific language for
single-player 2D grid-based games, and demonstrate its use as a scalable
training ground for Open-Ended Learning (OEL) algorithms. Autoverse uses
cellular-automaton-like rewrite rules to describe game mechanics, allowing it
to express various game environments (e.g. mazes, dungeons, sokoban puzzles)
that are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite
rule can be expressed as a series of simple convolutions, allowing for
environments to be parallelized on the GPU, thereby drastically accelerating RL
training. Using Autoverse, we propose jump-starting open-ended learning by
imitation learning from search. In such an approach, we first evolve Autoverse
environments (their rules and initial map topology) to maximize the number of
iterations required by greedy tree search to discover a new best solution,
producing a curriculum of increasingly complex environments and playtraces. We
then distill these expert playtraces into a neural-network-based policy using
imitation learning. Finally, we use the learned policy as a starting point for
open-ended RL, where new training environments are continually evolved to
maximize the RL player agent's value function error (a proxy for its regret, or
the learnability of generated environments), finding that this approach
improves the performance and generality of resultant player agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Autoverseï¼šç”¨äºå­¦ä¹ é²æ£’å…·èº«æ™ºèƒ½ä½“çš„å¯è¿›åŒ–æ¸¸æˆè¯­è¨€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œå¼€æ”¾å¼å­¦ä¹ ï¼ˆOELï¼‰çš„ç›®æ ‡æ˜¯è®­ç»ƒå‡ºèƒ½åŠ›é€æ¸å¢å¼ºã€è¡Œä¸ºé€æ¸å¤æ‚çš„æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„OELç¯å¢ƒå¾€å¾€ç¼ºä¹å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆæœæœ‰é™ã€‚æ­¤å¤–ï¼Œä»ç”Ÿæˆçš„ç¯å¢ƒä¸­â€œå†·å¯åŠ¨â€å­¦ä¹ ç­–ç•¥çš„å¤æ‚æ€§ä¹Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAutoverseï¼Œä¸€ç§å¯è¿›åŒ–çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰ï¼Œç”¨äºå•ç©å®¶2Dç½‘æ ¼æ¸¸æˆã€‚å®ƒä½¿ç”¨ç±»ä¼¼ç»†èƒè‡ªåŠ¨æœºçš„é‡å†™è§„åˆ™æ¥æè¿°æ¸¸æˆæœºåˆ¶ï¼Œèƒ½å¤Ÿè¡¨è¾¾å„ç§æ¸¸æˆç¯å¢ƒï¼Œå¦‚è¿·å®«ã€åœ°ç‰¢ã€æ¨ç®±å­è°œé¢˜ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨GPUå¹¶è¡ŒåŒ–é‡å†™è§„åˆ™ï¼Œé€šè¿‡å·ç§¯æ“ä½œå®ç°ï¼Œä»è€ŒåŠ é€Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé€šè¿‡æ¨¡ä»¿å­¦ä¹ ä»æœç´¢ä¸­å¯åŠ¨å¼€æ”¾å¼å­¦ä¹ ã€‚é¦–å…ˆï¼Œé€šè¿‡è¿›åŒ–Autoverseç¯å¢ƒï¼ˆè§„åˆ™å’Œåˆå§‹åœ°å›¾æ‹“æ‰‘ï¼‰æ¥æœ€å¤§åŒ–è´ªå©ªæ ‘æœç´¢å‘ç°æ–°æœ€ä½³è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ï¼Œäº§ç”Ÿä¸€ä¸ªè¶Šæ¥è¶Šå¤æ‚çš„ç¯å¢ƒå’Œæ¸¸æˆè½¨è¿¹çš„æ•™ç¨‹ã€‚ç„¶åï¼Œä½¿ç”¨æ¨¡ä»¿å­¦ä¹ å°†è¿™äº›ä¸“å®¶æ¸¸æˆè½¨è¿¹æç‚¼æˆä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„ç­–ç•¥ã€‚æœ€åï¼Œä½¿ç”¨å­¦ä¹ åˆ°çš„ç­–ç•¥ä½œä¸ºå¼€æ”¾å¼RLçš„èµ·ç‚¹ï¼Œå…¶ä¸­æ–°çš„è®­ç»ƒç¯å¢ƒä¸æ–­è¿›åŒ–ä»¥æœ€å¤§åŒ–RLç©å®¶æ™ºèƒ½ä½“çš„ä»·å€¼å‡½æ•°è¯¯å·®ï¼ˆä½œä¸ºå…¶é—æ†¾æˆ–ç”Ÿæˆç¯å¢ƒçš„å¯å­¦ä¹ æ€§çš„ä»£ç†ï¼‰ï¼Œå‘ç°è¿™ç§æ–¹æ³•æé«˜äº†ç©å®¶æ™ºèƒ½ä½“çš„æ€§èƒ½å’Œé€šç”¨æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ¨¡ä»¿å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæ›´å¤§çš„è§‚å¯ŸèŒƒå›´å’Œè§‚å¯Ÿç¯å¢ƒè§„åˆ™é›†å¯ä»¥æé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿›åŒ–è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ç¯å¢ƒå…·æœ‰ä¸åŒçš„åŠ¨æ€ç‰¹æ€§ï¼ŒåŒ…æ‹¬é«˜åº¦ä¸ç¨³å®šã€éƒ¨åˆ†ç¨³å®šå’Œå¹³è¡¡ç¨³å®š/æ··æ²Œæ¨¡å¼çš„ç¯å¢ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Autoverseä¸ºå¼€æ”¾å¼å­¦ä¹ æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æµ‹è¯•å¹³å°ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è¿›åŒ–ç­–ç•¥æ¥æœç´¢å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¸¸æˆç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜æå‡ºäº†ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¥å¯åŠ¨å¼€æ”¾å¼å­¦ä¹ çš„æƒ³æ³•ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¿™äº›æ–¹æ³•å’Œæ€æƒ³å¯ä»¥åº”ç”¨äºå…¶ä»–å¼€æ”¾å¼å­¦ä¹ ç¯å¢ƒçš„è®¾è®¡å’Œè®­ç»ƒä¸­ã€‚

## cradle--empowering-foundation-agents-towards-general-computer-control
### Abstract
Despite the success in specific scenarios, existing foundation agents still
struggle to generalize across various virtual scenarios, mainly due to the
dramatically different encapsulations of environments with manually designed
observation and action spaces. To handle this issue, we propose the General
Computer Control (GCC) setting to restrict foundation agents to interact with
software through the most unified and standardized interface, i.e., using
screenshots as input and keyboard and mouse actions as output. We introduce
Cradle, a modular and flexible LMM-powered framework, as a preliminary attempt
towards GCC. Enhanced by six key modules, Cradle can understand input
screenshots and output executable code for low-level keyboard and mouse control
after high-level planning, so that Cradle can interact with any software and
complete long-horizon complex tasks without relying on any built-in APIs.
Experimental results show that Cradle exhibits remarkable generalizability and
impressive performance across four previously unexplored commercial video
games, five software applications, and a comprehensive benchmark, OSWorld.
Cradle is the first to enable foundation agents to follow the main storyline
and complete 40-minute-long real missions in the complex AAA game Red Dead
Redemption 2 (RDR2). Cradle can also create a city of a thousand people in
Cities: Skylines, farm and harvest parsnips in Stardew Valley, and trade and
bargain with a maximal weekly total profit of 87% in Dealer's Life 2. Cradle
can not only operate daily software, like Chrome, Outlook, and Feishu, but also
edit images and videos using Meitu and CapCut. Cradle greatly extends the reach
of foundation agents by enabling the easy conversion of any software,
especially complex games, into benchmarks to evaluate agents' various abilities
and facilitate further data collection, thus paving the way for generalist
agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Cradleï¼šè¿ˆå‘é€šç”¨è®¡ç®—æœºæ§åˆ¶çš„åŸºçŸ³

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰çš„åŸºç¡€æ™ºèƒ½ä½“åœ¨ç‰¹å®šåœºæ™¯ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨æ³›åŒ–åˆ°å„ç§è™šæ‹Ÿåœºæ™¯æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºç¯å¢ƒå°è£…çš„å·¨å¤§å·®å¼‚ï¼ŒåŒ…æ‹¬æ‰‹åŠ¨è®¾è®¡çš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é€šç”¨è®¡ç®—æœºæ§åˆ¶ï¼ˆGCCï¼‰è®¾ç½®ï¼Œé™åˆ¶åŸºç¡€æ™ºèƒ½ä½“é€šè¿‡æœ€ç»Ÿä¸€å’Œæ ‡å‡†åŒ–çš„æ¥å£ä¸è½¯ä»¶è¿›è¡Œäº¤äº’ï¼Œå³ä½¿ç”¨å±å¹•æˆªå›¾ä½œä¸ºè¾“å…¥å’Œé”®ç›˜é¼ æ ‡æ“ä½œä½œä¸ºè¾“å‡ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGCCè®¾ç½®
æœ¬æ–‡æå‡ºäº†GCCè®¾ç½®ï¼Œæ—¨åœ¨è®©åŸºç¡€æ™ºèƒ½ä½“é€šè¿‡ç»Ÿä¸€çš„æ¥å£ä¸è½¯ä»¶è¿›è¡Œäº¤äº’ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚GCCè®¾ç½®è¦æ±‚æ™ºèƒ½ä½“ä»…é€šè¿‡å±å¹•æˆªå›¾ä½œä¸ºè¾“å…¥å’Œé”®ç›˜é¼ æ ‡æ“ä½œä½œä¸ºè¾“å‡ºä¸è½¯ä»¶è¿›è¡Œäº¤äº’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCradleæ¡†æ¶
ä¸ºäº†å®ç°GCCè®¾ç½®ï¼Œæœ¬æ–‡æå‡ºäº†Cradleæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å’Œçµæ´»çš„LMMï¼ˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼‰é©±åŠ¨æ¡†æ¶ã€‚Cradleæ¡†æ¶ç”±å…­ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼šä¿¡æ¯æ”¶é›†ã€è‡ªæˆ‘åæ€ã€ä»»åŠ¡æ¨ç†ã€æŠ€èƒ½ç®¡ç†ã€åŠ¨ä½œè§„åˆ’å’Œè®°å¿†ã€‚è¿™äº›æ¨¡å—ååŒå·¥ä½œï¼Œä½¿Cradleèƒ½å¤Ÿç†è§£è¾“å…¥å±å¹•æˆªå›¾ï¼Œå¹¶åœ¨é«˜çº§è§„åˆ’åè¾“å‡ºå¯æ‰§è¡Œçš„ä»£ç ï¼Œä»¥è¿›è¡Œä½çº§é”®ç›˜å’Œé¼ æ ‡æ§åˆ¶ã€‚è¿™æ ·ï¼ŒCradleå¯ä»¥ä¸ä»»ä½•è½¯ä»¶è¿›è¡Œäº¤äº’ï¼Œå¹¶å®Œæˆé•¿æœŸå¤æ‚çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å†…ç½®APIã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒCradleåœ¨å››ä¸ªä»¥å‰æœªæ¢ç´¢è¿‡çš„å•†ä¸šè§†é¢‘æ¸¸æˆã€äº”ä¸ªè½¯ä»¶åº”ç”¨ç¨‹åºå’Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•OSWorldä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›å’Œä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚Cradleæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿä½¿åŸºç¡€æ™ºèƒ½ä½“éµå¾ªå¤æ‚AAAæ¸¸æˆã€Šè’é‡å¤§é•–å®¢æ•‘èµ2ã€‹ï¼ˆRDR2ï¼‰çš„ä¸»çº¿å‰§æƒ…å¹¶å®Œæˆ40åˆ†é’Ÿé•¿çš„çœŸå®ä»»åŠ¡çš„æ¡†æ¶ã€‚æ­¤å¤–ï¼ŒCradleè¿˜å¯ä»¥åœ¨ã€ŠåŸå¸‚ï¼šå¤©é™…çº¿ã€‹ä¸­åˆ›å»ºä¸€ä¸ªæ‹¥æœ‰åƒäººçš„åŸå¸‚ï¼Œåœ¨ã€Šæ˜Ÿéœ²è°·ç‰©è¯­ã€‹ä¸­ç§æ¤å’Œæ”¶è·æ¬§èŠ¹ï¼Œä»¥åŠåœ¨ã€Šç»é”€å•†ç”Ÿæ´»2ã€‹ä¸­ä»¥87%çš„æœ€å¤§æ¯å‘¨æ€»åˆ©æ¶¦è¿›è¡Œäº¤æ˜“å’Œè®¨ä»·è¿˜ä»·ã€‚Cradleä¸ä»…å¯ä»¥æ“ä½œæ—¥å¸¸è½¯ä»¶ï¼Œå¦‚Chromeã€Outlookå’Œé£ä¹¦ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ç¾å›¾å’Œå‰ªæ˜ ç¼–è¾‘å›¾åƒå’Œè§†é¢‘ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Cradleæ¡†æ¶ä¸ºé€šç”¨è®¡ç®—æœºæ§åˆ¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å…¶æ¨¡å—åŒ–å’Œçµæ´»çš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿé€‚åº”å„ç§ç¯å¢ƒå’Œä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒCradleæ¡†æ¶çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿåœ¨å¤æ‚çš„è™šæ‹Ÿç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å…·æœ‰æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æ›´å¼ºå¤§çš„åŸºç¡€æ™ºèƒ½ä½“å’Œæ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## using-game-play-to-investigate-multimodal-and-conversational-grounding-in-large-multimodal-models
### Abstract
While the situation has improved for text-only models, it again seems to be
the case currently that multimodal (text and image) models develop faster than
ways to evaluate them. In this paper, we bring a recently developed evaluation
paradigm from text models to multimodal models, namely evaluation through the
goal-oriented game (self) play, complementing reference-based and
preference-based evaluation. Specifically, we define games that challenge a
model's capability to represent a situation from visual information and align
such representations through dialogue. We find that the largest closed models
perform rather well on the games that we define, while even the best
open-weight models struggle with them. On further analysis, we find that the
exceptional deep captioning capabilities of the largest models drive some of
the performance. There is still room to grow for both kinds of models, ensuring
the continued relevance of the benchmark.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆåŒ–è¯„ä¼°ï¼šæ¢ç©¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„å¤šæ¨¡æ€å’Œå¯¹è¯å¼æ¥åœ°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºå‚è€ƒå¼è¯„ä¼°ï¼Œéš¾ä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’èƒ½åŠ›ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢ä¸€ç§æ–°çš„è¯„ä¼°èŒƒå¼ï¼Œå³é€šè¿‡ç›®æ ‡å¯¼å‘çš„æ¸¸æˆï¼ˆè‡ªæˆ‘ï¼‰ç©æ³•æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ï¼Œä»¥è¡¥å……ç°æœ‰çš„å‚è€ƒå¼å’Œåå¥½å¼è¯„ä¼°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ¸¸æˆåŒ–è¯„ä¼°èŒƒå¼åº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹
æœ¬æ–‡å€Ÿé‰´äº†æ–‡æœ¬æ¨¡å‹ä¸­æ–°å…´çš„æ¸¸æˆåŒ–è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºå¤šæ¨¡æ€æ¨¡å‹ã€‚é€šè¿‡å®šä¹‰ä¸‰ç§å¯¹è¯æ¸¸æˆï¼ˆå‚è€ƒæ¸¸æˆã€å›¾åƒæ¯”è¾ƒæ¸¸æˆå’Œå¯¼èˆªæ¸¸æˆï¼‰ï¼ŒæŒ‘æˆ˜æ¨¡å‹ä»è§†è§‰ä¿¡æ¯ä¸­æ„å»ºæƒ…å¢ƒæ¨¡å‹å¹¶é€šè¿‡å¯¹è¯è¿›è¡Œå¯¹é½çš„èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ„å»ºå¤šæ¨¡æ€æ¸¸æˆæ¡†æ¶
æœ¬æ–‡ä½¿ç”¨ clemgame/clembench æ¡†æ¶æ¥å®ç°æ¸¸æˆåŒ–è¯„ä¼°ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæ¨¡æ¿æ¥å®šä¹‰æ¸¸æˆç›®æ ‡ï¼Œå¹¶é€šè¿‡ç¨‹åºåŒ–çš„æ¸¸æˆå¤§å¸ˆæ¥æ§åˆ¶æ¸¸æˆæµç¨‹å’Œè¯„åˆ†è§„åˆ™ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹é—­æºæ¨¡å‹åœ¨æœ¬æ–‡å®šä¹‰çš„æ¸¸æˆä¸­è¡¨ç°è‰¯å¥½ï¼Œè€Œå³ä½¿æ˜¯æœ€å¥½çš„å¼€æºæ¨¡å‹ä¹Ÿéš¾ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼Œå¤§å‹æ¨¡å‹åœ¨æ·±åº¦å›¾åƒæè¿°æ–¹é¢çš„å‡ºè‰²èƒ½åŠ›æ¨åŠ¨äº†éƒ¨åˆ†æ€§èƒ½æå‡ã€‚è¿™è¡¨æ˜ï¼Œæ— è®ºæ˜¯é—­æºæ¨¡å‹è¿˜æ˜¯å¼€æºæ¨¡å‹ï¼Œéƒ½ä»æœ‰å¾ˆå¤§çš„å‘å±•ç©ºé—´ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¸¸æˆåŒ–è¯„ä¼°æ–¹æ³•ä¸ºå¤šæ¨¡æ€æ¨¡å‹çš„è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äº¤äº’èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤šæ¨¡æ€æ¸¸æˆæ¡†æ¶ä¹Ÿä¸ºå…¶ä»–ç ”ç©¶äººå‘˜æä¾›äº†å¯å¤ç°çš„å®éªŒå¹³å°ã€‚

### ğŸ“š å‚è€ƒæ–‡çŒ®
* Chalamalasetti, V., et al. (2023). clemgame: A framework for evaluating language models through goal-oriented games. arXiv preprint arXiv:2304.01213.
* Chiang, D., et al. (2024). Chatbot Arena: A preference-based evaluation platform for conversational agents. arXiv preprint arXiv:2403.04528.
* Qiao, Y., et al. (2023). Evaluating language models through interactive games. arXiv preprint arXiv:2303.08574.

## towards-efficient-llm-grounding-for-embodied-multi-agent-collaboration
### Abstract
Grounding the reasoning ability of large language models (LLMs) for embodied
tasks is challenging due to the complexity of the physical world. Especially,
LLM planning for multi-agent collaboration requires communication of agents or
credit assignment as the feedback to re-adjust the proposed plans and achieve
effective coordination. However, existing methods that overly rely on physical
verification or self-reflection suffer from excessive and inefficient querying
of LLMs. In this paper, we propose a novel framework for multi-agent
collaboration that introduces Reinforced Advantage feedback (ReAd) for
efficient self-refinement of plans. Specifically, we perform critic regression
to learn a sequential advantage function from LLM-planned data, and then treat
the LLM planner as an optimizer to generate actions that maximize the advantage
function. It endows the LLM with the foresight to discern whether the action
contributes to accomplishing the final task. We provide theoretical analysis by
extending advantage-weighted regression in reinforcement learning to
multi-agent systems. Experiments on Overcooked-AI and a difficult variant of
RoCoBench show that ReAd surpasses baselines in success rate, and also
significantly decreases the interaction steps of agents and query rounds of
LLMs, demonstrating its high efficiency for grounding LLMs. More results are
given at https://read-llm.github.io/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…·èº«å¤šæ™ºèƒ½ä½“åä½œä¸­çš„æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å…·èº«ä»»åŠ¡æ—¶ï¼Œç”±äºç‰©ç†ä¸–ç•Œçš„å¤æ‚æ€§ï¼Œå…¶æ¨ç†èƒ½åŠ›éš¾ä»¥è½åœ°ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­ï¼ŒLLM è§„åˆ’éœ€è¦æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡æˆ–ä¿¡ç”¨åˆ†é…ä½œä¸ºåé¦ˆï¼Œä»¥è°ƒæ•´è®¡åˆ’å¹¶å®ç°æœ‰æ•ˆåè°ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¿‡åº¦ä¾èµ–ç‰©ç†éªŒè¯æˆ–è‡ªæˆ‘åæ€ï¼Œå¯¼è‡´å¯¹ LLM çš„æŸ¥è¯¢è¿‡å¤šä¸”æ•ˆç‡ä½ä¸‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º ReAdï¼ˆReinforced Advantage feedbackï¼‰çš„æ¡†æ¶ï¼Œç”¨äºå¤šæ™ºèƒ½ä½“åä½œï¼Œä»¥å®ç°é«˜æ•ˆçš„è‡ªé€‚åº”è®¡åˆ’ã€‚å…·ä½“æ¥è¯´ï¼ŒReAd é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š

1. **ä¼˜åŠ¿å‡½æ•°å­¦ä¹ **ï¼šä½¿ç”¨ LLM è§„åˆ’æ•°æ®ï¼Œé€šè¿‡æ‰¹è¯„å›å½’å­¦ä¹ ä¸€ä¸ªåºåˆ—ä¼˜åŠ¿å‡½æ•°ã€‚
2. **LLM è§„åˆ’å™¨ä¼˜åŒ–**ï¼šå°† LLM è§„åˆ’å™¨è§†ä¸ºä¼˜åŒ–å™¨ï¼Œç”Ÿæˆæœ€å¤§åŒ–ä¼˜åŠ¿å‡½æ•°çš„åŠ¨ä½œã€‚
3. **å‰ç»æ€§æ¨ç†**ï¼šèµ‹äºˆ LLM å‰ç»æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿåˆ¤æ–­åŠ¨ä½œæ˜¯å¦æœ‰åŠ©äºå®Œæˆä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ Overcooked-AI å’Œ RoCoBench çš„å›°éš¾å˜ä½“ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜ ReAd åœ¨æˆåŠŸç‡æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†æ™ºèƒ½ä½“çš„äº¤äº’æ­¥éª¤å’Œ LLM çš„æŸ¥è¯¢è½®æ¬¡ï¼Œè¯æ˜äº†å…¶åœ¨è½åœ° LLM æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ReAd æ¡†æ¶ä¸º LLM åœ¨å…·èº«å¤šæ™ºèƒ½ä½“åä½œä¸­çš„æ¨ç†èƒ½åŠ›è½åœ°æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°† LLM è§„åˆ’å™¨è§†ä¸ºä¼˜åŒ–å™¨ï¼Œå¹¶é€šè¿‡ä¼˜åŠ¿å‡½æ•°å¼•å¯¼å…¶ç”Ÿæˆæ›´æœ‰æ•ˆçš„åŠ¨ä½œã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦ LLM åä½œçš„åœºæ™¯ï¼Œä¾‹å¦‚æœºå™¨äººåä½œã€è™šæ‹Ÿç°å®ç­‰ã€‚

## a-pilot-study-on-teacher-facing-real-time-classroom-game-dashboards
### Abstract
Educational games are an increasingly popular teaching tool in modern
classrooms. However, the development of complementary tools for teachers
facilitating classroom gameplay is lacking. We present the results of a
participatory design process for a teacher-facing, real-time game data
dashboard. This two-phase process included a workshop to elicit teachers'
requirements for such a tool, and a pilot study of our dashboard prototype. We
analyze post-gameplay survey and interview data to understand teachers'
experiences with the tool in terms of evidence of co-design, feasibility, and
effectiveness. Our results indicate the participatory design yielded a tool
both useful for and usable by teachers within the context of a real class
gameplay session. We advocate for the continued development of data-driven
teacher tools to improve the effectiveness of games deployed in the classroom.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ•™è‚²æ¸¸æˆä¸­çš„å®æ—¶è¯¾å ‚æ¸¸æˆä»ªè¡¨æ¿ï¼šæ•™å¸ˆè§†è§’çš„æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ•™è‚²æ¸¸æˆåœ¨ç°ä»£è¯¾å ‚ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ç¼ºä¹è¾…åŠ©æ•™å¸ˆè¿›è¡Œè¯¾å ‚æ¸¸æˆçš„æ•™å­¦å·¥å…·ã€‚æ•™å¸ˆé€šå¸¸åªèƒ½é€šè¿‡æŸ¥çœ‹æ¯ä¸ªå­¦ç”Ÿçš„ç”µè„‘å±å¹•æ¥äº†è§£ä»–ä»¬çš„æ¸¸æˆæƒ…å†µï¼Œè¿™ä¸ä»…è€—æ—¶ï¼Œè€Œä¸”å¯èƒ½æ— æ³•å…¨é¢äº†è§£æ¸¸æˆç»†èŠ‚ï¼Œä»è€Œå½±å“æ•™å­¦æ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§å‚ä¸å¼è®¾è®¡æ–¹æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„ç ”ç©¶æ¥å¼€å‘ä¸€ä¸ªé¢å‘æ•™å¸ˆçš„å®æ—¶æ¸¸æˆæ•°æ®ä»ªè¡¨æ¿ï¼š
1. **å‚ä¸å¼è®¾è®¡å·¥ä½œåŠ**ï¼šä¸æ•™å¸ˆå’Œå…¶ä»–åˆ©ç›Šç›¸å…³è€…åˆä½œï¼Œé€šè¿‡è®¾è®¡æ´»åŠ¨æ”¶é›†ä»–ä»¬å¯¹ä»ªè¡¨æ¿çš„éœ€æ±‚å’ŒæœŸæœ›ã€‚
2. **åŸå‹å¼€å‘å’Œè¯•ç‚¹ç ”ç©¶**ï¼šæ ¹æ®å·¥ä½œåŠçš„ç»“æœå¼€å‘ä»ªè¡¨æ¿åŸå‹ï¼Œå¹¶åœ¨å¤šä¸ªè¯¾å ‚ç¯å¢ƒä¸­è¿›è¡Œè¯•ç‚¹æµ‹è¯•ï¼Œæ”¶é›†æ•™å¸ˆåé¦ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è¯•ç‚¹ç ”ç©¶è¡¨æ˜ï¼Œå‚ä¸å¼è®¾è®¡è¿‡ç¨‹äº§ç”Ÿäº†å¯¹æ•™å¸ˆæœ‰ç”¨ä¸”æ˜“äºä½¿ç”¨çš„å·¥å…·ã€‚æ•™å¸ˆåé¦ˆè¡¨æ˜ï¼Œä»ªè¡¨æ¿å¸®åŠ©ä»–ä»¬æ›´å¥½åœ°ç†è§£æ¸¸æˆå’Œå­¦ç”Ÿçš„æ¸¸æˆæƒ…å†µï¼Œæé«˜äº†ä»–ä»¬å¯¹æ¸¸æˆæ•™å­¦çš„æ”¯æŒèƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå‚ä¸å¼è®¾è®¡æ˜¯å¼€å‘æ•™å¸ˆæ”¯æŒå·¥å…·çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•ä»ªè¡¨æ¿çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç­çº§æ¦‚è¿°å’Œæ€»ç»“æ•°æ®ï¼Œä»¥æé«˜å…¶å¯ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥æ¢ç´¢å¦‚ä½•å°†ä»ªè¡¨æ¿ä¸å…¶ä»–æ•™è‚²æŠ€æœ¯å·¥å…·é›†æˆï¼Œä»¥æ›´å¥½åœ°æ”¯æŒæ¸¸æˆæ•™å­¦ã€‚

## code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering
### Abstract
Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
https://github.com/Codium-ai/AlphaCodium
### ğŸŒŸ è®ºæ–‡è§£è¯» | AlphaCodiumï¼šä»æç¤ºå·¥ç¨‹åˆ°æµç¨‹å·¥ç¨‹ï¼Œæå‡ä»£ç ç”Ÿæˆæ€§èƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä»£ç ç”Ÿæˆé—®é¢˜ä¸å¸¸è§çš„è‡ªç„¶è¯­è¨€é—®é¢˜ä¸åŒï¼Œå®ƒéœ€è¦åŒ¹é…ç›®æ ‡è¯­è¨€çš„ç²¾ç¡®è¯­æ³•ï¼Œè¯†åˆ«æ­£å¸¸è·¯å¾„å’Œè¾¹ç¼˜æƒ…å†µï¼Œå…³æ³¨é—®é¢˜è§„èŒƒä¸­çš„è®¸å¤šå°ç»†èŠ‚ï¼Œå¹¶è§£å†³å…¶ä»–ä»£ç ç‰¹å®šçš„é—®é¢˜å’Œè¦æ±‚ã€‚å› æ­¤ï¼Œè®¸å¤šåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­æˆåŠŸçš„ä¼˜åŒ–å’ŒæŠ€å·§å¯èƒ½å¯¹ä»£ç ä»»åŠ¡æ— æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºAlphaCodiumï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºæµ‹è¯•çš„å¤šé˜¶æ®µã€ä»£ç å¯¼å‘çš„è¿­ä»£æµç¨‹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæµ‹è¯•å¯¼å‘çš„è¿­ä»£æµç¨‹
AlphaCodiumçš„æ ¸å¿ƒæ˜¯è¿­ä»£æµç¨‹ï¼Œå…¶ä¸­ç”Ÿæˆçš„ä»£ç ä¼šåå¤è¿è¡Œå¹¶é’ˆå¯¹è¾“å…¥è¾“å‡ºæµ‹è¯•è¿›è¡Œä¿®å¤ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹é€æ­¥æ”¹è¿›å…¶è§£å†³æ–¹æ¡ˆï¼Œç›´åˆ°æ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé˜¶æ®µå¤„ç†
AlphaCodiumæµç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„å¤„ç†é˜¶æ®µå’Œä»£ç è¿­ä»£é˜¶æ®µã€‚åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œæ¨¡å‹ä¼šå¯¹é—®é¢˜è¿›è¡Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œä¾‹å¦‚ç”Ÿæˆé—®é¢˜åæ€å’Œå…¬å…±æµ‹è¯•æ¨ç†ã€‚åœ¨ä»£ç è¿­ä»£é˜¶æ®µï¼Œæ¨¡å‹ä¼šç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨å…¬å…±å’ŒAIç”Ÿæˆçš„æµ‹è¯•ä¸Šè¿›è¡Œè¿­ä»£å’Œä¿®å¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µ
AlphaCodiumè¿˜é‡‡ç”¨äº†å¤šç§ä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µï¼Œä¾‹å¦‚YAMLç»“æ„åŒ–è¾“å‡ºã€é€šè¿‡é¡¹ç›®ç¬¦å·åˆ†æè¿›è¡Œè¯­ä¹‰æ¨ç†ã€ç”Ÿæˆæ¨¡å—åŒ–ä»£ç ã€è½¯å†³ç­–å’ŒåŒé‡éªŒè¯ã€é¼“åŠ±æ¢ç´¢ä»¥åŠæµ‹è¯•é”šç‚¹ã€‚è¿™äº›æ¦‚å¿µæœ‰åŠ©äºæé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨CodeContestsæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒAlphaCodiumæµç¨‹æ˜¾è‘—æé«˜äº†LLMsåœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒGPT-4åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ï¼ˆpass@5ï¼‰ä»ä½¿ç”¨å•ä¸ªç²¾å¿ƒè®¾è®¡çš„ç›´æ¥æç¤ºçš„19%æé«˜åˆ°ä½¿ç”¨AlphaCodiumæµç¨‹çš„44%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AlphaCodiumçš„è®¸å¤šåŸåˆ™å’Œæœ€ä½³å®è·µå¯ä»¥å¹¿æ³›åº”ç”¨äºä¸€èˆ¬çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºã€ç”Ÿæˆæ¨¡å—åŒ–ä»£ç ã€é€šè¿‡é¡¹ç›®ç¬¦å·åˆ†æè¿›è¡Œè¯­ä¹‰æ¨ç†ã€è½¯å†³ç­–å’ŒåŒé‡éªŒè¯ã€é¼“åŠ±æ¢ç´¢ä»¥åŠæµ‹è¯•é”šç‚¹ç­‰æŠ€æœ¯éƒ½å¯ä»¥å¸®åŠ©æé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚

### ğŸ“š æ€»ç»“
AlphaCodiumæ˜¯ä¸€ç§åˆ›æ–°çš„ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡æµ‹è¯•å¯¼å‘çš„è¿­ä»£æµç¨‹å’Œå¤šé˜¶æ®µå¤„ç†ï¼Œæ˜¾è‘—æé«˜äº†LLMsåœ¨ä»£ç é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•è¿˜é‡‡ç”¨äº†å¤šç§ä»£ç å¯¼å‘çš„è®¾è®¡æ¦‚å¿µï¼Œè¿›ä¸€æ­¥æé«˜äº†ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚AlphaCodiumçš„è®¸å¤šåŸåˆ™å’Œæœ€ä½³å®è·µå¯ä»¥å¹¿æ³›åº”ç”¨äºä¸€èˆ¬çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œä¸ºä»£ç ç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚

## self-playing-adversarial-language-game-enhances-llm-reasoning
### Abstract
We explore the potential of self-play training for large language models
(LLMs) in a two-player adversarial language game called Adversarial Taboo. In
this game, an attacker and a defender communicate around a target word only
visible to the attacker. The attacker aims to induce the defender to speak the
target word unconsciously, while the defender tries to infer the target word
from the attacker's utterances. To win the game, both players must have
sufficient knowledge about the target word and high-level reasoning ability to
infer and express in this information-reserved conversation. Hence, we are
curious about whether LLMs' reasoning ability can be further enhanced by
Self-Playing this Adversarial language Game (SPAG). With this goal, we select
several open-source LLMs and let each act as the attacker and play with a copy
of itself as the defender on an extensive range of target words. Through
reinforcement learning on the game outcomes, we observe that the LLMs'
performances uniformly improve on a broad range of reasoning benchmarks.
Furthermore, iteratively adopting this self-play process can continuously
promote LLMs' reasoning abilities. The code is available at
https://github.com/Linear95/SPAG.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªå¯¹æŠ—è¯­è¨€æ¸¸æˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¨ç†èƒ½åŠ›æå‡æ–¹æ³•ï¼Œå¦‚æç¤ºå·¥ç¨‹å’Œå·¥å…·è°ƒç”¨ï¼Œä¾èµ–äºé¢å¤–çš„æ•°æ®æ”¶é›†å’Œè®¾è®¡ï¼Œä¸”å®¹æ˜“è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è‡ªå¯¹æŠ—è¯­è¨€æ¸¸æˆï¼ˆSPAGï¼‰æ¥æå‡LLMsçš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªå¯¹æŠ—è¯­è¨€æ¸¸æˆï¼ˆAdversarial Tabooï¼‰
æœ¬æ–‡é€‰æ‹©äº†ä¸€ç§åä¸ºAdversarial Tabooçš„è‡ªå¯¹æŠ—è¯­è¨€æ¸¸æˆï¼Œå…¶ä¸­æ”»å‡»è€…å’Œé˜²å¾¡è€…å›´ç»•ä¸€ä¸ªç›®æ ‡è¯è¿›è¡Œå¯¹è¯ã€‚æ”»å‡»è€…è¯•å›¾è¯±å¯¼é˜²å¾¡è€…æ— æ„è¯†åœ°è¯´å‡ºç›®æ ‡è¯ï¼Œè€Œé˜²å¾¡è€…åˆ™è¯•å›¾ä»æ”»å‡»è€…çš„å‘è¨€ä¸­æ¨æ–­å‡ºç›®æ ‡è¯ã€‚è¿™ç§æ¸¸æˆéœ€è¦ç©å®¶å…·å¤‡ä¸°å¯Œçš„çŸ¥è¯†å’Œé«˜çº§æ¨ç†èƒ½åŠ›ï¼Œä»è€Œä¸ºLLMsçš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†è‰¯å¥½çš„è®­ç»ƒç¯å¢ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªç©è®­ç»ƒï¼ˆSelf-Playï¼‰
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPAGçš„è‡ªç©è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è®©LLMsåœ¨Adversarial Tabooæ¸¸æˆä¸­è‡ªæˆ‘å¯¹æŠ—ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å…¶ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒLLMsé¦–å…ˆé€šè¿‡æ¨¡ä»¿å­¦ä¹ æ¥å­¦ä¹ æ¸¸æˆè§„åˆ™ï¼Œç„¶åè¿›è¡Œè‡ªæˆ‘å¯¹æŠ—ï¼Œå¹¶æ”¶é›†æ¸¸æˆå›åˆã€‚æœ€åï¼ŒLLMsé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å…¶ç­–ç•¥ï¼Œä»è€Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAGè®­ç»ƒç­–ç•¥èƒ½å¤Ÿæ˜¾è‘—æå‡LLMsåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„æç¤ºå·¥ç¨‹å’Œå·¥å…·è°ƒç”¨æ–¹æ³•ç›¸æ¯”ï¼ŒSPAGè®­ç»ƒç­–ç•¥å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´å¹¿æ³›çš„åº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼ŒSPAGè®­ç»ƒç­–ç•¥è¿˜èƒ½å¤Ÿæå‡LLMsåœ¨Adversarial Tabooæ¸¸æˆä¸­çš„èƒœç‡ï¼Œè¡¨æ˜å…¶åœ¨æ¸¸æˆæŠ€èƒ½æ–¹é¢çš„æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„SPAGè®­ç»ƒç­–ç•¥ä¸ºLLMsçš„æ¨ç†èƒ½åŠ›æå‡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡è®¾è®¡å„ç§è‡ªç©ç¯å¢ƒï¼ŒLLMså¯ä»¥ä¸æ–­å­¦ä¹ å’Œæå‡å…¶æ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸­å–å¾—æ›´å¥½çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æé†’å¼€å‘è€…åœ¨ä½¿ç”¨è‡ªç©è®­ç»ƒç­–ç•¥æ—¶ï¼Œéœ€è¦æ³¨æ„æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œå¹¶è¿›è¡Œå……åˆ†çš„å®‰å…¨æ£€æŸ¥ã€‚

## predicting-outcomes-in-video-games-with-long-short-term-memory-networks
### Abstract
Forecasting winners in E-sports with real-time analytics has the potential to
further engage audiences watching major tournament events. However, making such
real-time predictions is challenging due to unpredictable variables within the
game involving diverse player strategies and decision-making. Our work attempts
to enhance audience engagement within video game tournaments by introducing a
real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)
based approach enables efficient predictions of win-lose outcomes by only using
the health indicator of each player as a time series. As a proof of concept, we
evaluate our model's performance within a classic, two-player arcade game,
Super Street Fighter II Turbo. We also benchmark our method against state of
the art methods for time series forecasting; i.e. Transformer models found in
large language models (LLMs). Finally, we open-source our data set and code in
hopes of furthering work in predictive analysis for arcade games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œé¢„æµ‹ç”µå­ç«æŠ€æ¯”èµ›ç»“æœ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç”µå­ç«æŠ€ï¼ˆEsportsï¼‰çš„æ—¥ç›Šæµè¡Œï¼Œè§‚ä¼—å¯¹äºå®æ—¶æ¯”èµ›ç»“æœçš„é¢„æµ‹äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚ç„¶è€Œï¼Œç”±äºæ¸¸æˆä¸­çš„å˜é‡ä¼—å¤šï¼ŒåŒ…æ‹¬ç©å®¶ç­–ç•¥å’Œå†³ç­–çš„ä¸ç¡®å®šæ€§ï¼Œå®æ—¶é¢„æµ‹æ¯”èµ›ç»“æœä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§å®æ—¶é¢„æµ‹æ–¹æ³•æ¥å¢å¼ºè§‚ä¼—åœ¨ç”µå­æ¸¸æˆé”¦æ ‡èµ›ä¸­çš„å‚ä¸åº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰è¿›è¡Œå®æ—¶é¢„æµ‹
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLSTMçš„å®æ—¶é¢„æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æ¯ä¸ªç©å®¶çš„å¥åº·æŒ‡ç¤ºå™¨ä½œä¸ºæ—¶é—´åºåˆ—æ¥é¢„æµ‹èƒœè´Ÿç»“æœã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶æ•æ‰æ¸¸æˆä¸­çš„åŠ¨æ€å˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨ç»å…¸çš„åŒäººè¡—æœºæ¸¸æˆã€Šè¶…çº§è¡—å¤´éœ¸ç‹II Turboã€‹ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½
ä¸ºäº†éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡åœ¨ç»å…¸çš„åŒäººè¡—æœºæ¸¸æˆã€Šè¶…çº§è¡—å¤´éœ¸ç‹II Turboã€‹ä¸­è¯„ä¼°äº†æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡åˆ†æç©å®¶å¥åº·æŒ‡ç¤ºå™¨çš„å˜åŒ–ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¯”èµ›çš„ä¸åŒé˜¶æ®µè¿›è¡Œå®æ—¶é¢„æµ‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„Transformeræ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•
ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡å°†LSTMæ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„Transformeræ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨é¢„æµ‹æ¯”èµ›ç»“æœæ–¹é¢è¡¨ç°å‡ºäº†æ›´é«˜çš„å‡†ç¡®ç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨é¢„æµ‹æ¯”èµ›ç»“æœæ–¹é¢è¡¨ç°å‡ºäº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚åœ¨æ¯”èµ›çš„ä¸åŒé˜¶æ®µï¼ˆ25%ï¼Œ75%ï¼Œ95%ï¼‰ï¼ŒLSTMæ¨¡å‹çš„ROC-AUCåˆ†æ•°å‡é«˜äºTransformeræ¨¡å‹ã€‚æ­¤å¤–ï¼ŒLSTMæ¨¡å‹çš„è®­ç»ƒæ—¶é—´ä¹Ÿç›¸å¯¹è¾ƒçŸ­ï¼Œæ›´é€‚åˆå®æ—¶é¢„æµ‹åœºæ™¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºLSTMçš„å®æ—¶é¢„æµ‹æ–¹æ³•ä¸ºç”µå­ç«æŠ€æ¯”èµ›ç»“æœçš„é¢„æµ‹æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ç”µå­ç«æŠ€æ¸¸æˆï¼Œå¹¶æœ‰åŠ©äºæé«˜è§‚ä¼—åœ¨æ¯”èµ›ä¸­çš„å‚ä¸åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼€æºäº†æ•°æ®é›†å’Œä»£ç ï¼Œä¸ºå…¶ä»–ç ”ç©¶äººå‘˜æä¾›äº†è¿›ä¸€æ­¥ç ”ç©¶çš„ä¾¿åˆ©ã€‚

## embodied-llm-agents-learn-to-cooperate-in-organized-teams
### Abstract
Large Language Models (LLMs) have emerged as integral tools for reasoning,
planning, and decision-making, drawing upon their extensive world knowledge and
proficiency in language-related tasks. LLMs thus hold tremendous potential for
natural language interaction within multi-agent systems to foster cooperation.
However, LLM agents tend to over-report and comply with any instruction, which
may result in information redundancy and confusion in multi-agent cooperation.
Inspired by human organizations, this paper introduces a framework that imposes
prompt-based organization structures on LLM agents to mitigate these problems.
Through a series of experiments with embodied LLM agents and human-agent
collaboration, our results highlight the impact of designated leadership on
team efficiency, shedding light on the leadership qualities displayed by LLM
agents and their spontaneous cooperative behaviors. Further, we harness the
potential of LLMs to propose enhanced organizational prompts, via a
Criticize-Reflect process, resulting in novel organization structures that
reduce communication costs and enhance team efficiency.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“åœ¨ç»„ç»‡åŒ–å›¢é˜Ÿä¸­å­¦ä¹ åä½œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ã€è§„åˆ’å’Œå†³ç­–æ–¹é¢çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åä½œæ½œåŠ›ä¹Ÿé€æ¸æ˜¾ç°ã€‚ç„¶è€Œï¼ŒLLMæ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­å­˜åœ¨è¿‡åº¦æŠ¥å‘Šå’Œç›²ç›®æœä»æŒ‡ä»¤çš„é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¿¡æ¯å†—ä½™å’Œæ··ä¹±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„ç»„ç»‡ç»“æ„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMæ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åä½œæ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®¾è®¡äº†ä¸€ç§æ–°å‹çš„å¤šLLMæ™ºèƒ½ä½“æ¶æ„ï¼Œæ”¯æŒâ‰¥3ä¸ªæ™ºèƒ½ä½“åœ¨ç‰©ç†/æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œçµæ´»çš„é€šä¿¡å’Œåä½œã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘äº†ä¸€ç§åŸºäºLLMsçš„â€œæ‰¹è¯„-åæ€â€æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ä¼˜åŒ–ç»„ç»‡æç¤ºï¼Œä»è€Œç”Ÿæˆæ›´æœ‰æ•ˆçš„ç»„ç»‡ç»“æ„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰æŒ‡å®šé¢†å¯¼è€…çš„å±‚æ¬¡åŒ–ç»„ç»‡ç»“æ„èƒ½å¤Ÿæ˜¾è‘—æé«˜å›¢é˜Ÿæ•ˆç‡ï¼Œå¹¶ä¸”LLMæ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡é€šä¿¡åŠ¨æ€åœ°é€‰ä¸¾å’Œè°ƒæ•´é¢†å¯¼è€…ã€‚æ­¤å¤–ï¼Œé€šè¿‡â€œæ‰¹è¯„-åæ€â€æ¡†æ¶ï¼ŒLLMæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªå‘åœ°å½¢æˆæ–°é¢–ã€æœ‰æ•ˆçš„å›¢é˜Ÿç»“æ„ï¼Œä»è€Œé™ä½é€šä¿¡æˆæœ¬å¹¶æé«˜å›¢é˜Ÿæ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºæç¤ºçš„ç»„ç»‡ç»“æ„æ¡†æ¶å’Œâ€œæ‰¹è¯„-åæ€â€æ¡†æ¶ä¸ºLLMæ™ºèƒ½ä½“åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åä½œæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ç½‘ç»œã€æ— äººæœºç¾¤ç­‰ï¼Œä»¥æé«˜æ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæ•ˆç‡å’Œæ€§èƒ½ã€‚

## can-llm-augmented-autonomous-agents-cooperate---an-evaluation-of-their-cooperative-capabilities-through-melting-pot
### Abstract
As the field of AI continues to evolve, a significant dimension of this
progression is the development of Large Language Models and their potential to
enhance multi-agent artificial intelligence systems. This paper explores the
cooperative capabilities of Large Language Model-augmented Autonomous Agents
(LAAs) using the well-known Meltin Pot environments along with reference models
such as GPT4 and GPT3.5. Preliminary results suggest that while these agents
demonstrate a propensity for cooperation, they still struggle with effective
collaboration in given environments, emphasizing the need for more robust
architectures. The study's contributions include an abstraction layer to adapt
Melting Pot game scenarios for LLMs, the implementation of a reusable
architecture for LLM-mediated agent development - which includes short and
long-term memories and different cognitive modules, and the evaluation of
cooperation capabilities using a set of metrics tied to the Melting Pot's
"Commons Harvest" game. The paper closes, by discussing the limitations of the
current architectural framework and the potential of a new set of modules that
fosters better cooperation among LAAs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„è‡ªä¸»æ™ºèƒ½ä½“èƒ½å¦åˆä½œï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸æ–­å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„åº”ç”¨æ½œåŠ›æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶å¯¹äºLLMå¢å¼ºçš„è‡ªä¸»æ™ºèƒ½ä½“ï¼ˆLAAsï¼‰çš„åˆä½œèƒ½åŠ›æ¢è®¨ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æ—¨åœ¨è¯„ä¼°LAAsåœ¨åˆä½œæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æ¢è®¨å¦‚ä½•æå‡å…¶åˆä½œæ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†Melting Potåœºæ™¯è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤ºï¼Œä»¥ä¾¿LLMså¯ä»¥è½»æ¾æ“ä½œã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå®ç°äº†ä¸€ä¸ªå¯é‡ç”¨çš„LAAså¼€å‘æ¶æ„ï¼Œè¯¥æ¶æ„åŒ…æ‹¬çŸ­æœŸå’Œé•¿æœŸè®°å¿†ä»¥åŠä¸åŒçš„è®¤çŸ¥æ¨¡å—ï¼Œå¦‚æ„ŸçŸ¥ã€è§„åˆ’ã€åæ€å’Œè¡ŒåŠ¨ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé€šè¿‡è‡ªç„¶è¯­è¨€å®šä¹‰â€œä¸ªæ€§â€ï¼Œä½¿æ™ºèƒ½ä½“æ˜ç¡®æ˜¯å¦åº”è¯¥åˆä½œã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šä½¿ç”¨Melting Potçš„â€œå…¬å…±èµ„æºæ”¶è·â€æ¸¸æˆæ¥è¯„ä¼°LLMä¸­ä»‹æ™ºèƒ½ä½“çš„åˆä½œèƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡æ™ºèƒ½ä½“è¡¨ç°å‡ºåˆä½œçš„å€¾å‘ï¼Œä½†å®ƒä»¬åœ¨ç‰¹å®šç¯å¢ƒä¸­ä»ç„¶éš¾ä»¥æœ‰æ•ˆåä½œã€‚è¿™çªæ˜¾äº†éœ€è¦æ›´å¼ºå¤§çš„æ¶æ„æ¥ä¿ƒè¿›LAAsä¹‹é—´çš„åˆä½œã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ºäº†æé«˜LAAsçš„åˆä½œèƒ½åŠ›ï¼Œéœ€è¦æ›´å…¨é¢çš„æ¶æ„ï¼ŒåŒ…æ‹¬å¢å¼ºçš„ç†è§£èƒ½åŠ›ã€æœ‰æ•ˆçš„æ²Ÿé€šæœºåˆ¶ã€å¯ä¿¡çš„æ‰¿è¯ºæœºåˆ¶ä»¥åŠæ˜ç¡®çš„ç¤¾äº¤ç»“æ„æˆ–åˆ¶åº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ”¹è¿›æ¶æ„ï¼ŒåŒ…æ‹¬ç†è§£æ¨¡å—ã€æ²Ÿé€šæ¨¡å—ã€å®ªæ³•æ¨¡å—å’Œå£°èª‰ç³»ç»Ÿï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents
### Abstract
Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller RL agents learn useful skills that they are weak at? We propose EnvGen,
a novel framework to address this question. We first prompt an LLM to generate
training environments by giving it the task description and simulator
objectives that the agents should learn and then asking it to generate a set of
environment configurations (e.g., different terrains, items initially given to
agents, etc.). Next, we train a small RL agent in a mixture of the original and
LLM-generated environments. Then, we enable the LLM to continuously adapt the
generated environments to progressively improve the skills that the agent is
weak at, by providing feedback to the LLM in the form of the agent's
performance. We demonstrate the usefulness of EnvGen with comprehensive
experiments in Crafter and Heist environments. We find that a small RL agent
trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and
learns long-horizon tasks significantly faster. We also show that using an LLM
to adapt environments dynamically outperforms curriculum learning approaches
and how the environments are adapted to help improve RL agents' weaker skills
over time. Additionally, EnvGen is substantially more efficient as it only uses
a small number of LLM calls (e.g., 4 in total), whereas LLM agents require
thousands of calls. Lastly, we present detailed ablation studies for EnvGen
design choices.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EnvGenï¼šåˆ©ç”¨LLMç”Ÿæˆå’Œé€‚åº”ç¯å¢ƒï¼Œè®­ç»ƒå…·èº«æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­çš„å…´èµ·ï¼Œå¦‚ä½•è®©æ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ å¹¶æŒæ¡å„ç§æŠ€èƒ½æˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å¤„ç†é•¿æ—¶ä»»åŠ¡æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ™ºèƒ½ä½“è™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†è°ƒç”¨æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶EnvGenï¼Œæ—¨åœ¨åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥ç”Ÿæˆå’Œé€‚åº”è®­ç»ƒç¯å¢ƒï¼Œå¸®åŠ©å°å‹RLæ™ºèƒ½ä½“å­¦ä¹ å®ƒä»¬ä¸æ“…é•¿çš„æŠ€èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLMç”Ÿæˆç¯å¢ƒ
EnvGené¦–å…ˆé€šè¿‡å‘LLMæä¾›ä»»åŠ¡æè¿°å’Œæ¨¡æ‹Ÿå™¨ç›®æ ‡ï¼Œè®©LLMç”Ÿæˆä¸€ç³»åˆ—ç¯å¢ƒé…ç½®ï¼Œä¾‹å¦‚ä¸åŒçš„åœ°å½¢ã€åˆå§‹ç‰©å“ç­‰ã€‚è¿™äº›ç¯å¢ƒå¯ä»¥å¹¶è¡Œè®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶å¿«é€Ÿå­¦ä¹ ä¸åŒçš„æŠ€èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMé€‚åº”ç¯å¢ƒ
EnvGené€šè¿‡å°†æ™ºèƒ½ä½“åœ¨åŸå§‹ç¯å¢ƒä¸­çš„è¡¨ç°åé¦ˆç»™LLMï¼Œè®©LLMä¸æ–­è°ƒæ•´ç”Ÿæˆçš„ç¯å¢ƒï¼Œä½¿å…¶æ›´åŠ ä¸“æ³¨äºæ™ºèƒ½ä½“ä¸æ“…é•¿çš„æŠ€èƒ½ã€‚è¿™ç§åŠ¨æ€é€‚åº”è¿‡ç¨‹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“é€æ­¥æé«˜å…¶æŠ€èƒ½æ°´å¹³ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Crafterå’ŒHeistæ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨EnvGenè®­ç»ƒçš„å°å‹RLæ™ºèƒ½ä½“åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†åŒ…æ‹¬GPT-4åœ¨å†…çš„SOTAæ–¹æ³•ï¼Œå¹¶ä¸”å­¦ä¹ é•¿æ—¶ä»»åŠ¡çš„é€Ÿåº¦æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼ŒEnvGençš„æ•ˆç‡ä¹Ÿè¿œé«˜äºç›´æ¥ä½¿ç”¨LLMä½œä¸ºæ™ºèƒ½ä½“çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒåªéœ€è¦å¾ˆå°‘çš„LLMè°ƒç”¨æ¬¡æ•°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
EnvGenæä¾›äº†ä¸€ç§åˆ©ç”¨LLMæ¨ç†èƒ½åŠ›æ¥æé«˜RLæ™ºèƒ½ä½“æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§å¼€æ”¾ä¸–ç•Œæ¸¸æˆå’Œæ¨¡æ‹Ÿå™¨ï¼Œå¸®åŠ©æ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ å¹¶æŒæ¡å„ç§æŠ€èƒ½ã€‚æ­¤å¤–ï¼ŒEnvGençš„åŠ¨æ€é€‚åº”æœºåˆ¶ä¹Ÿä¸ºRLæ™ºèƒ½ä½“çš„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## minedreamer--learning-to-follow-instructions-via-chain-of-imagination-for-simulated-world-control
### Abstract
It is a long-lasting goal to design a generalist-embodied agent that can
follow diverse instructions in human-like ways. However, existing approaches
often fail to steadily follow instructions due to difficulties in understanding
abstract and sequential natural language instructions. To this end, we
introduce MineDreamer, an open-ended embodied agent built upon the challenging
Minecraft simulator with an innovative paradigm that enhances
instruction-following ability in low-level control signal generation.
Specifically, MineDreamer is developed on top of recent advances in Multimodal
Large Language Models (MLLMs) and diffusion models, and we employ a
Chain-of-Imagination (CoI) mechanism to envision the step-by-step process of
executing instructions and translating imaginations into more precise visual
prompts tailored to the current state; subsequently, the agent generates
keyboard-and-mouse actions to efficiently achieve these imaginations, steadily
following the instructions at each step. Extensive experiments demonstrate that
MineDreamer follows single and multi-step instructions steadily, significantly
outperforming the best generalist agent baseline and nearly doubling its
performance. Moreover, qualitative analysis of the agent's imaginative ability
reveals its generalization and comprehension of the open world.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MineDreamerï¼šåŸºäºæƒ³è±¡é“¾çš„æ¨¡æ‹Ÿä¸–ç•Œæ§åˆ¶æŒ‡ä»¤è·Ÿéš

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä»¥äººç±»æ–¹å¼ç†è§£å’Œæ‰§è¡Œå¤šæ ·åŒ–æŒ‡ä»¤çš„é€šç”¨å‹å…·èº«æ™ºèƒ½ä½“ä¸€ç›´æ˜¯é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€éš¾ä»¥ç¨³å®šåœ°éµå¾ªæŒ‡ä»¤ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£å’Œæ‰§è¡ŒæŠ½è±¡å’Œé¡ºåºçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ–¹é¢å­˜åœ¨å›°éš¾ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥â€œæƒ³è±¡é“¾â€ï¼ˆChain-of-Imagination, CoIï¼‰æœºåˆ¶
MineDreamer é€šè¿‡ CoI æœºåˆ¶ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®æŒ‡ä»¤å’Œå½“å‰çŠ¶æ€é€æ­¥æƒ³è±¡å¹¶æ‰§è¡ŒæŒ‡ä»¤ã€‚è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»åœ¨è§£å†³é—®é¢˜æ—¶ï¼Œæ ¹æ®å½“å‰çŠ¶æ€é€æ­¥æƒ³è±¡ä¸‹ä¸€æ­¥ç›®æ ‡çš„è¿‡ç¨‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¢å¼ºçš„æ‰©æ•£æ¨¡å‹
MineDreamer ä½¿ç”¨ MLLM å¢å¼ºçš„æ‰©æ•£æ¨¡å‹æ¥ç”ŸæˆåŒ…å«ç‰©ç†è§„åˆ™å’Œç¯å¢ƒç†è§£çš„æƒ³è±¡å›¾åƒï¼Œè¿™äº›å›¾åƒä½œä¸ºæ›´ç²¾ç¡®çš„è§†è§‰æç¤ºï¼Œå¼•å¯¼æ™ºèƒ½ä½“ç”Ÿæˆä½çº§æ§åˆ¶ä¿¡å·ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•
ä¸ºäº†è®­ç»ƒ Imaginatorï¼ŒMineDreamer ä½¿ç”¨äº†ç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•æ¥æ”¶é›†å¤§é‡å…·èº«æ•°æ®ï¼Œå¸®åŠ© Imaginator ç†è§£å¦‚ä½•é€æ­¥å®ŒæˆæŒ‡ä»¤ä»¥åŠå¦‚ä½•é‡å¤å®ŒæˆæŒ‡ä»¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
MineDreamer åœ¨æ‰§è¡Œå•æ­¥å’Œå¤šæ­¥æŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºæœ€ä½³é€šç”¨å‹æ™ºèƒ½ä½“åŸºçº¿ï¼Œæ€§èƒ½å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼Œå¯¹æ™ºèƒ½ä½“æƒ³è±¡èƒ½åŠ›çš„å®šæ€§åˆ†æè¡¨æ˜ï¼Œå®ƒèƒ½å¤Ÿç†è§£å’Œé€‚åº”å¼€æ”¾ä¸–ç•Œçš„ç¯å¢ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MineDreamer çš„ CoI æœºåˆ¶ä¸ºè§£å†³æŒ‡ä»¤è·Ÿéšé—®é¢˜æä¾›äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå…¶ MLLM å¢å¼ºçš„æ‰©æ•£æ¨¡å‹å’Œç›®æ ‡æ¼‚ç§»æ”¶é›†æ–¹æ³•ä¹Ÿä¸ºå…·èº«æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒMineDreamer çš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œå…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## scaling-instructable-agents-across-many-simulated-worlds
### Abstract
Building embodied AI systems that can follow arbitrary language instructions
in any 3D environment is a key challenge for creating general AI. Accomplishing
this goal requires learning to ground language in perception and embodied
actions, in order to accomplish complex tasks. The Scalable, Instructable,
Multiworld Agent (SIMA) project tackles this by training agents to follow
free-form instructions across a diverse range of virtual 3D environments,
including curated research environments as well as open-ended, commercial video
games. Our goal is to develop an instructable agent that can accomplish
anything a human can do in any simulated 3D environment. Our approach focuses
on language-driven generality while imposing minimal assumptions. Our agents
interact with environments in real-time using a generic, human-like interface:
the inputs are image observations and language instructions and the outputs are
keyboard-and-mouse actions. This general approach is challenging, but it allows
agents to ground language across many visually complex and semantically rich
environments while also allowing us to readily run agents in new environments.
In this paper we describe our motivation and goal, the initial progress we have
made, and promising preliminary results on several diverse research
environments and a variety of commercial video games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SIMAï¼šè·¨è¶Šå¤šä¸ªæ¨¡æ‹Ÿä¸–ç•Œçš„å¯æŒ‡ä»¤æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†å®ƒä»¬ä¸æˆ‘ä»¬æ‰€å¤„çš„å…·èº«ä¸–ç•Œè¿æ¥èµ·æ¥ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°ä»£AIåœ¨è¯­è¨€èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ„ŸçŸ¥å’Œè¡ŒåŠ¨æ–¹é¢å´è¿œä¸åŠäººç±»ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘èƒ½å¤Ÿç†è§£è¯­è¨€æŒ‡ä»¤å¹¶åœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡çš„å…·èº«AIç³»ç»Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
SIMAé¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªèƒ½å¤Ÿéµå¾ªä»»æ„è¯­è¨€æŒ‡ä»¤å¹¶åœ¨ä»»ä½•è™šæ‹Ÿ3Dç¯å¢ƒä¸­é€šè¿‡é”®ç›˜å’Œé¼ æ ‡æ“ä½œè¿›è¡Œè¡ŒåŠ¨çš„ç³»ç»Ÿã€‚è¯¥é¡¹ç›®çš„ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ **å¤šç¯å¢ƒè®­ç»ƒ**ï¼šSIMAåœ¨å¤šç§è™šæ‹Ÿ3Dç¯å¢ƒä¸­è®­ç»ƒæ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬å®šåˆ¶çš„ç ”ç©¶ç¯å¢ƒå’Œå¼€æ”¾å¼çš„å•†ä¸šè§†é¢‘æ¸¸æˆã€‚è¿™ç§å¤šæ ·åŒ–çš„è®­ç»ƒç¯å¢ƒæœ‰åŠ©äºæ™ºèƒ½ä½“å­¦ä¹ æ›´é€šç”¨çš„æŠ€èƒ½ï¼Œå¹¶æé«˜å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚åº”èƒ½åŠ›ã€‚

ğŸ’¡ **è¯­è¨€é©±åŠ¨**ï¼šSIMAçš„æ™ºèƒ½ä½“é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œè®­ç»ƒï¼Œè¿™æœ‰åŠ©äºå®ƒä»¬å­¦ä¹ æ›´é€šç”¨çš„è¯­è¨€è¡¨ç¤ºå’ŒæŠ½è±¡ï¼Œå¹¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚

ğŸ’¡ **äººç±»ç•Œé¢**ï¼šSIMAçš„æ™ºèƒ½ä½“ä½¿ç”¨ä¸äººç±»ç›¸åŒçš„é”®ç›˜å’Œé¼ æ ‡æ§åˆ¶æ–¹å¼ä¸è™šæ‹Ÿç¯å¢ƒäº¤äº’ï¼Œè¿™æœ‰åŠ©äºå®ƒä»¬æ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œå¹¶æé«˜å…¶åœ¨æ–°ç¯å¢ƒä¸­çš„è¿ç§»èƒ½åŠ›ã€‚

ğŸ’¡ **æ•°æ®æ”¶é›†**ï¼šSIMAæ”¶é›†äº†å¤§é‡ç”±äººç±»ä¸“å®¶ç”Ÿæˆçš„æ¸¸æˆæ•°æ®ï¼ŒåŒ…æ‹¬è§†é¢‘ã€è¯­è¨€æŒ‡ä»¤ã€è®°å½•çš„åŠ¨ä½œå’Œå„ç§æ³¨é‡Šã€‚è¿™äº›æ•°æ®è¢«ç”¨äºè®­ç»ƒæ™ºèƒ½ä½“ï¼Œå¹¶æé«˜å…¶å­¦ä¹ æ•ˆç‡ã€‚

ğŸ’¡ **è¯„ä¼°æ–¹æ³•**ï¼šSIMAä½¿ç”¨å¤šç§è¯„ä¼°æ–¹æ³•æ¥è¯„ä¼°æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åŠ¨ä½œæ—¥å¿—æ¦‚ç‡ã€é™æ€è§†è§‰è¾“å…¥ã€åœ°é¢å®å†µè¯„ä¼°ã€å…‰å­¦å­—ç¬¦è¯†åˆ«å’Œäººå·¥è¯„ä¼°ã€‚è¿™äº›è¯„ä¼°æ–¹æ³•æœ‰åŠ©äºå…¨é¢è¯„ä¼°æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼Œå¹¶ç¡®ä¿å…¶èƒ½å¤Ÿéµå¾ªè¯­è¨€æŒ‡ä»¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SIMAé¡¹ç›®å–å¾—äº†åˆæ­¥çš„æˆåŠŸï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤šç§ç¯å¢ƒä¸­å®Œæˆå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¼èˆªã€èµ„æºæ”¶é›†ã€å¯¹è±¡ç®¡ç†å’Œæˆ˜æ–—ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIMAçš„æ™ºèƒ½ä½“åœ¨ç¯å¢ƒæ³›åŒ–ã€é›¶æ ·æœ¬è¿ç§»å’Œè¯­è¨€æ¡ä»¶è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SIMAé¡¹ç›®ä¸ºå…·èº«AIç ”ç©¶æä¾›äº†ä¸€ä¸ªé‡è¦çš„å¹³å°ï¼Œå…¶åˆ›æ–°æ–¹æ³•å’ŒæŠ€æœ¯å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰ã€‚æ­¤å¤–ï¼ŒSIMAé¡¹ç›®çš„ç ”ç©¶æˆæœä¹Ÿæœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯­è¨€ä¸æ„ŸçŸ¥å’Œè¡ŒåŠ¨ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸ºé€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›æ–°çš„æ€è·¯ã€‚

## hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation
### Abstract
Due to the dynamic and unpredictable open-world setting, navigating complex
environments in Minecraft poses significant challenges for multi-agent systems.
Agents must interact with the environment and coordinate their actions with
other agents to achieve common objectives. However, traditional approaches
often struggle to efficiently manage inter-agent communication and task
distribution, crucial for effective multi-agent navigation. Furthermore,
processing and integrating multi-modal information (such as visual, textual,
and auditory data) is essential for agents to comprehend their goals and
navigate the environment successfully and fully. To address this issue, we
design the HAS framework to auto-organize groups of LLM-based agents to
complete navigation tasks. In our approach, we devise a hierarchical
auto-organizing navigation system, which is characterized by 1) a hierarchical
system for multi-agent organization, ensuring centralized planning and
decentralized execution; 2) an auto-organizing and intra-communication
mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal
information platform, facilitating multi-modal perception to perform the three
navigation tasks with one system. To assess organizational behavior, we design
a series of navigation tasks in the Minecraft environment, which includes
searching and exploring. We aim to develop embodied organizations that push the
boundaries of embodied AI, moving it towards a more human-like organizational
structure.
### ğŸŒŸ è®ºæ–‡è§£è¯» | HASï¼šå¼€æ”¾ä¸–ç•Œå¤šæ™ºèƒ½ä½“å¯¼èˆªçš„åˆ†å±‚è‡ªç»„ç»‡ç³»ç»Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œçš„ç¯å¢ƒä¸­ï¼Œå¦‚Minecraftï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿé¢ä¸´ç€å¤æ‚çš„å¯¼èˆªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¯¼èˆªæ–¹æ³•å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°ç®¡ç†æ™ºèƒ½ä½“ä¹‹é—´çš„é€šä¿¡å’Œä»»åŠ¡åˆ†é…ï¼Œè¿™å¯¹äºæœ‰æ•ˆçš„å¤šæ™ºèƒ½ä½“å¯¼èˆªè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå¤„ç†å’Œæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚è§†è§‰ã€æ–‡æœ¬å’Œå¬è§‰æ•°æ®ï¼‰å¯¹äºæ™ºèƒ½ä½“ç†è§£å…¶ç›®æ ‡å¹¶åœ¨ç¯å¢ƒä¸­æˆåŠŸå¯¼èˆªè‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ†å±‚è‡ªç»„ç»‡å¯¼èˆªç³»ç»Ÿ
HASæ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªåˆ†å±‚è‡ªç»„ç»‡å¯¼èˆªç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. åˆ†å±‚ç³»ç»Ÿï¼šç¡®ä¿é›†ä¸­å¼è§„åˆ’å’Œåˆ†å¸ƒå¼æ‰§è¡Œï¼Œæé«˜å¯¼èˆªæ•ˆç‡ã€‚
2. è‡ªç»„ç»‡æœºåˆ¶ï¼šæ ¹æ®å­ä»»åŠ¡åŠ¨æ€è°ƒæ•´å…³é”®è§’è‰²å’Œè¡ŒåŠ¨ç»„ï¼Œå¹¶ä¿æŒç»„é—´é€šä¿¡ï¼Œç¡®ä¿é«˜æ•ˆåä½œã€‚
3. å¤šæ¨¡æ€ä¿¡æ¯å¹³å°ï¼šä¿ƒè¿›å¤šæ¨¡æ€æ„ŸçŸ¥ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å›¾åƒã€å¯¹è±¡å’ŒéŸ³é¢‘ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œæœç´¢å’Œæ¢ç´¢ç­‰å¯¼èˆªä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹
HASæ¡†æ¶ä½¿ç”¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼ŒåŒ…æ‹¬ç®¡ç†è€…å’Œæ‰§è¡Œè€…ä¸¤ç§ç±»å‹çš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å…·æœ‰ä¸åŒçš„åŠŸèƒ½ï¼Œå¦‚è§„åˆ’ã€æè¿°ã€è¯„ä¼°å’Œéƒ¨ç½²ï¼Œä»¥å®ç°é›†ä¸­å¼è§„åˆ’å’Œåˆ†å¸ƒå¼æ‰§è¡Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¤šæ¨¡æ€è®°å¿†
HASæ¡†æ¶è¿˜å¼•å…¥äº†å¤šæ¨¡æ€è®°å¿†æœºåˆ¶ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»è€Œæé«˜è§„åˆ’å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡æ€æ£€ç´¢ï¼ˆMMRï¼‰æŠ€æœ¯ï¼ŒHASèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å†å²äº¤äº’åé¦ˆå’Œç»éªŒï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„è®¡åˆ’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Minecraftç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHASæ¡†æ¶åœ¨å¤šæ¨¡æ€ç›®æ ‡æœç´¢ã€è¿ç»­å—æœç´¢å’Œåœ°å›¾æ¢ç´¢ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒHASåœ¨å¯¼èˆªæ•ˆç‡ã€æˆåŠŸç‡å’Œæ¢ç´¢èƒ½åŠ›æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
HASæ¡†æ¶ä¸ºå¼€æ”¾ä¸–ç•Œå¤šæ™ºèƒ½ä½“å¯¼èˆªæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶åˆ†å±‚è‡ªç»„ç»‡ç»“æ„ã€å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è®°å¿†æœºåˆ¶ç­‰åˆ›æ–°ç‚¹ï¼Œå¯¹äºæé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªä¸»æ€§ã€æ•ˆç‡å’Œé€‚åº”æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–ï¼ŒHASæ¡†æ¶è¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦å¤šæ™ºèƒ½ä½“åä½œçš„åœºæ™¯ï¼Œå¦‚æœºå™¨äººååŒã€è™šæ‹Ÿç°å®ç­‰ã€‚

## sotopia-$Ï€$--interactive-learning-of-socially-intelligent-language-agents
### Abstract
Humans learn social skills through both imitation and social interaction.
This social learning process is largely understudied by existing research on
building language agents. Motivated by this gap, we propose an interactive
learning method, SOTOPIA-$\pi$, improving the social intelligence of language
agents. This method leverages behavior cloning and self-reinforcement training
on filtered social interaction data according to large language model (LLM)
ratings. We show that our training method allows a 7B LLM to reach the social
goal completion ability of an expert model (GPT-4-based agent), while improving
the safety of language agents and maintaining general QA ability on the MMLU
benchmark. We also find that this training paradigm uncovers some difficulties
in LLM-based evaluation of social intelligence: LLM-based evaluators
overestimate the abilities of the language agents trained specifically for
social interaction.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SOTOPIA-$Ï€$: æå‡è¯­è¨€æ¨¡å‹ç¤¾äº¤æ™ºèƒ½çš„äº¤äº’å¼å­¦ä¹ æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»é€šè¿‡æ¨¡ä»¿å’Œç¤¾ä¼šäº’åŠ¨å­¦ä¹ ç¤¾äº¤æŠ€èƒ½ï¼Œä½†ç°æœ‰ç ”ç©¶åœ¨æ„å»ºè¯­è¨€æ¨¡å‹æ—¶å¯¹æ­¤è¿‡ç¨‹å…³æ³¨ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§äº¤äº’å¼å­¦ä¹ æ–¹æ³• SOTOPIA-$Ï€$ï¼Œæ—¨åœ¨æå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨è¡Œä¸ºå…‹éš†å’Œè‡ªæˆ‘å¼ºåŒ–è®­ç»ƒ
SOTOPIA-$Ï€$ åˆ©ç”¨è¡Œä¸ºå…‹éš†å’Œè‡ªæˆ‘å¼ºåŒ–è®­ç»ƒï¼Œåœ¨ç»è¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„åˆ†è¿‡æ»¤çš„ç¤¾ä¼šäº’åŠ¨æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¡Œä¸ºå…‹éš†ä»å…·æœ‰å¼ºå¤§ç¤¾äº¤æŠ€èƒ½çš„ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰çš„è¡Œä¸ºè½¨è¿¹ä¸­å­¦ä¹ ï¼Œè€Œè‡ªæˆ‘å¼ºåŒ–åˆ™ä»æ¨¡å‹è‡ªèº«çš„é«˜è¯„åˆ†è¡Œä¸ºä¸­å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLM è¯„åˆ†ä½œä¸ºè®­ç»ƒä¿¡å·
SOTOPIA-$Ï€$ ä½¿ç”¨ GPT-4 å¯¹ç¤¾äº¤äº’åŠ¨ä¸­çš„ç§¯æè¡Œä¸ºè¿›è¡Œè¯„åˆ†ï¼Œå¹¶å°†è¿™äº›è¯„åˆ†ä½œä¸ºè®­ç»ƒä¿¡å·ã€‚è¿™ç§æ–¹æ³•æ— éœ€äººå·¥å‚ä¸ï¼Œä¸”å…·æœ‰é«˜æ•ˆå’Œå¯æ‰©å±•æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒSOTOPIA-$Ï€$ å¯ä»¥æ˜¾è‘—æå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤ç›®æ ‡å®Œæˆèƒ½åŠ›ï¼Œä½¿å…¶æ¥è¿‘ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æé«˜è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå¹¶ä¿æŒå…¶åœ¨ MMLU åŸºå‡†æµ‹è¯•ä¸­çš„é—®ç­”èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SOTOPIA-$Ï€$ ä¸ºæå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº† LLM è¯„åˆ†åœ¨è¯„ä¼°ç¤¾äº¤æ™ºèƒ½æ–¹é¢çš„å±€é™æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢åœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€ä»äººç±»æ•°æ®ä¸­å­¦ä¹ ã€æ›´ç¨³å¥çš„è¯„ä¼°æ–¹æ³•ç­‰æ–¹å‘ï¼Œä»¥è¿›ä¸€æ­¥æå‡è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½ã€‚

## will-gpt-4-run-doom-
### Abstract
We show that GPT-4's reasoning and planning capabilities extend to the 1993
first-person shooter Doom. This large language model (LLM) is able to run and
play the game with only a few instructions, plus a textual
description--generated by the model itself from screenshots--about the state of
the game being observed. We find that GPT-4 can play the game to a passable
degree: it is able to manipulate doors, combat enemies, and perform pathing.
More complex prompting strategies involving multiple model calls provide better
results. While further work is required to enable the LLM to play the game as
well as its classical, reinforcement learning-based counterparts, we note that
GPT-4 required no training, leaning instead on its own reasoning and
observational capabilities. We hope our work pushes the boundaries on
intelligent, LLM-based agents in video games. We conclude by discussing the
ethical implications of our work.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GPT-4èƒ½å¦ç©è½¬ç»å…¸å°„å‡»æ¸¸æˆDoomï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4çš„å‡ºç°ï¼Œå®ƒä»¬åœ¨ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼ŒLLMåœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£ä¹‹è°œã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMåœ¨è§†é¢‘æ¸¸æˆä¸­çš„æ™ºèƒ½ä»£ç†åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯è¯„ä¼°GPT-4åœ¨ç»å…¸å°„å‡»æ¸¸æˆDoomä¸­çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†è§‰è¾“å…¥ä¸æ–‡æœ¬æè¿°çš„ç»“åˆ
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†GPT-4çš„è§†è§‰è¾“å…¥èƒ½åŠ›ä¸æ–‡æœ¬æè¿°ç›¸ç»“åˆï¼Œä»¥ç†è§£æ¸¸æˆçŠ¶æ€ã€‚GPT-4Væ¨¡å‹ä»æ¸¸æˆæˆªå›¾ä¸­ç”Ÿæˆæ¸¸æˆçŠ¶æ€çš„æ–‡æœ¬æè¿°ï¼Œè€ŒGPT-4æ¨¡å‹åˆ™æ ¹æ®è¿™äº›æè¿°å’Œä¹‹å‰çš„è¡ŒåŠ¨å†å²æ¥åšå‡ºå†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šå±‚æ¬¡è§„åˆ’ç­–ç•¥
ä¸ºäº†æé«˜GPT-4åœ¨æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œæœ¬æ–‡æå‡ºäº†å¤šç§è§„åˆ’ç­–ç•¥ï¼ŒåŒ…æ‹¬ç®€å•çš„æŒ‡ä»¤ã€åˆ†æ­¥çš„å…³å¡æ”»ç•¥ã€æ›´ç»†ç²’åº¦çš„è®¡åˆ’ç”Ÿæˆï¼Œä»¥åŠåŸºäºå¤šä¸ªä¸“å®¶æ„è§çš„k-levelsç­–ç•¥ã€‚è¿™äº›ç­–ç•¥æ—¨åœ¨ä¸ºGPT-4æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥å¢å¼ºå…¶æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4èƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šç©è½¬Doomæ¸¸æˆï¼Œèƒ½å¤Ÿæ‰§è¡Œå¼€é—¨ã€æˆ˜æ–—æ•Œäººã€è·¯å¾„è§„åˆ’ç­‰åŸºæœ¬æ“ä½œã€‚ç„¶è€Œï¼ŒGPT-4çš„æ¨ç†æ·±åº¦æœ‰é™ï¼Œç¼ºä¹é•¿æœŸè§„åˆ’å’Œè®°å¿†èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œå½“æ•Œäººç¦»å¼€è§†é‡æ—¶ï¼Œæ¨¡å‹ä¼šå¿˜è®°å®ƒä»¬çš„å­˜åœ¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨è§†é¢‘æ¸¸æˆä¸­çš„åº”ç”¨å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´ç²¾ç»†çš„è§„åˆ’ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•æé«˜LLMçš„æ¨ç†æ·±åº¦å’Œè®°å¿†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä¹Ÿæé†’æˆ‘ä»¬ï¼ŒLLMæŠ€æœ¯çš„å¿«é€Ÿå‘å±•éœ€è¦æ›´åŠ è°¨æ…çš„è¯„ä¼°å’Œç›‘ç®¡ï¼Œä»¥é˜²æ­¢æ½œåœ¨çš„æ»¥ç”¨é£é™©ã€‚

## large-multimodal-agents--a-survey
### Abstract
Large language models (LLMs) have achieved superior performance in powering
text-based AI agents, endowing them with decision-making and reasoning
abilities akin to humans. Concurrently, there is an emerging research trend
focused on extending these LLM-powered AI agents into the multimodal domain.
This extension enables AI agents to interpret and respond to diverse multimodal
user queries, thereby handling more intricate and nuanced tasks. In this paper,
we conduct a systematic review of LLM-driven multimodal agents, which we refer
to as large multimodal agents ( LMAs for short). First, we introduce the
essential components involved in developing LMAs and categorize the current
body of research into four distinct types. Subsequently, we review the
collaborative frameworks integrating multiple LMAs , enhancing collective
efficacy. One of the critical challenges in this field is the diverse
evaluation methods used across existing studies, hindering effective comparison
among different LMAs . Therefore, we compile these evaluation methodologies and
establish a comprehensive framework to bridge the gaps. This framework aims to
standardize evaluations, facilitating more meaningful comparisons. Concluding
our review, we highlight the extensive applications of LMAs and propose
possible future research directions. Our discussion aims to provide valuable
insights and guidelines for future research in this rapidly evolving field. An
up-to-date resource list is available at
https://github.com/jun0wanan/awesome-large-multimodal-agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼šè¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„æ¡¥æ¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å°†è¿™äº›æ¨¡å‹åº”ç”¨äºå¤šæ¨¡æ€é¢†åŸŸï¼Œä»¥æ„å»ºèƒ½å¤Ÿç†è§£å’Œå“åº”å¤šæ¨¡æ€ç”¨æˆ·æŸ¥è¯¢çš„æ™ºèƒ½ä½“ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¾€å¾€å­¤ç«‹åœ°è¿›è¡Œï¼Œç¼ºä¹å¯¹ç°æœ‰æ¡†æ¶çš„æ€»ç»“å’Œæ¯”è¾ƒã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¯¹LLMé©±åŠ¨çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼ˆLMAsï¼‰è¿›è¡Œç³»ç»Ÿæ€§çš„ç»¼è¿°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡é¦–å…ˆä»‹ç»äº†LMAsçš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€è§„åˆ’ã€è¡ŒåŠ¨å’Œè®°å¿†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ¡†æ¶ï¼Œå°†ç°æœ‰ç ”ç©¶åˆ†ä¸ºå››ç±»ï¼š

* **ç±»å‹Iï¼šä½¿ç”¨é—­æºLLMsä½œä¸ºè§„åˆ’å™¨ï¼Œæ— é•¿æœŸè®°å¿†**ã€‚è¿™ç±»LMAsä¸»è¦ä½¿ç”¨æç¤ºæŠ€æœ¯æ¥å¼•å¯¼é—­æºLLMsè¿›è¡Œå†³ç­–å’Œè§„åˆ’ï¼Œå®Œæˆå›¾åƒç¼–è¾‘ã€è§†è§‰å®šä½å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ã€‚
* **ç±»å‹IIï¼šä½¿ç”¨å¾®è°ƒçš„LLMsä½œä¸ºè§„åˆ’å™¨ï¼Œæ— é•¿æœŸè®°å¿†**ã€‚è¿™ç±»LMAsé€šè¿‡æ”¶é›†å¤šæ¨¡æ€æŒ‡ä»¤éµå¾ªæ•°æ®æˆ–ä½¿ç”¨è‡ªæŒ‡ä»¤æ¥å¾®è°ƒå¼€æºLLMsï¼Œä½¿å…¶å…·å¤‡å†³ç­–ã€è§„åˆ’å’Œå·¥å…·è°ƒç”¨çš„èƒ½åŠ›ã€‚
* **ç±»å‹IIIï¼šå…·æœ‰é—´æ¥é•¿æœŸè®°å¿†çš„è§„åˆ’å™¨**ã€‚è¿™ç±»LMAsçš„LLMsä½œä¸ºä¸­å¤®è§„åˆ’å™¨ï¼Œå¹¶é…å¤‡é•¿æœŸè®°å¿†ã€‚è§„åˆ’å™¨é€šè¿‡è°ƒç”¨ç›¸å…³å·¥å…·æ¥è®¿é—®å’Œæ£€ç´¢é•¿æœŸè®°å¿†ï¼Œä»¥å¢å¼ºæ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚
* **ç±»å‹IVï¼šå…·æœ‰åŸç”Ÿé•¿æœŸè®°å¿†çš„è§„åˆ’å™¨**ã€‚è¿™ç±»LMAsçš„LLMsç›´æ¥ä¸é•¿æœŸè®°å¿†äº¤äº’ï¼Œæ— éœ€å·¥å…·æ¥è®¿é—®é•¿æœŸè®°å¿†ã€‚ä¾‹å¦‚ï¼Œåœ¨Minecraftç­‰å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œè¿™ç±»LMAsèƒ½å¤Ÿå®Œæˆè¶…è¿‡200ä¸ªä¸åŒçš„ä»»åŠ¡ã€‚

æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å›é¡¾äº†å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„åä½œæ¡†æ¶ï¼Œå¹¶æ¢è®¨äº†è¯„ä¼°LMAsæ€§èƒ½çš„ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸»è§‚è¯„ä¼°å’Œå®¢è§‚è¯„ä¼°ã€‚ä¸ºäº†è§£å†³ç°æœ‰ç ”ç©¶ä¸­è¯„ä¼°æ–¹æ³•å¤šæ ·æ€§çš„é—®é¢˜ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç»¼åˆçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–è¯„ä¼°è¿‡ç¨‹ï¼Œä¿ƒè¿›æ›´æœ‰æ„ä¹‰çš„æ¯”è¾ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æ²¡æœ‰æä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯å¯¹ç°æœ‰ç ”ç©¶è¿›è¡Œäº†ç»¼è¿°å’Œåˆ†æã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **LMAsçš„æ¡†æ¶è®¾è®¡**ï¼šæœ¬æ–‡æå‡ºçš„åˆ†ç±»æ¡†æ¶å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LMAsçš„ä¸åŒç±»å‹å’Œç‰¹ç‚¹ï¼Œå¹¶ä¸ºæ„å»ºæ–°çš„LMAsæä¾›å‚è€ƒã€‚
* **LMAsçš„è¯„ä¼°æ–¹æ³•**ï¼šæœ¬æ–‡æå‡ºçš„ç»¼åˆè¯„ä¼°æ¡†æ¶å¯ä»¥ä¸ºLMAsçš„æ€§èƒ½è¯„ä¼°æä¾›æ ‡å‡†å’ŒæŒ‡å¯¼ï¼Œä¿ƒè¿›LMAsçš„è¿›ä¸€æ­¥å‘å±•ã€‚
* **LMAsçš„åº”ç”¨åœºæ™¯**ï¼šæœ¬æ–‡åˆ—ä¸¾äº†LMAsåœ¨GUIè‡ªåŠ¨åŒ–ã€æœºå™¨äººä¸å…·èº«AIã€æ¸¸æˆå¼€å‘ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç†è§£ã€è§†è§‰ç”Ÿæˆä¸ç¼–è¾‘ã€å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡ã€éŸ³é¢‘ç¼–è¾‘ä¸ç”Ÿæˆç­‰é¢†åŸŸçš„åº”ç”¨ï¼Œä¸ºLMAsçš„æœªæ¥å‘å±•æä¾›äº†æ–¹å‘ã€‚

### ğŸŒˆ æœªæ¥å±•æœ›
æœ¬æ–‡å±•æœ›äº†LMAsçš„æœªæ¥å‘å±•æ–¹å‘ï¼ŒåŒ…æ‹¬ï¼š

* **æ¡†æ¶è®¾è®¡**ï¼šä»å•ä¸ªæ™ºèƒ½ä½“å’Œå¤šä¸ªæ™ºèƒ½ä½“ä¸¤ä¸ªè§’åº¦å‡ºå‘ï¼Œæ„å»ºæ›´åŠ ç»Ÿä¸€å’Œåä½œçš„LMAsæ¡†æ¶ã€‚
* **è¯„ä¼°æ–¹æ³•**ï¼šå»ºç«‹ç³»ç»ŸåŒ–å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¼€å‘æ›´è´´è¿‘çœŸå®åœºæ™¯çš„è¯„ä¼°æ•°æ®é›†ã€‚
* **åº”ç”¨åœºæ™¯**ï¼šæ¢ç´¢LMAsåœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨ï¼Œä¾‹å¦‚äººæœºäº¤äº’ã€åŒ»ç–—å¥åº·ã€æ•™è‚²ç­‰ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡ä¸ºLMAsçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†å…¨é¢çš„æ¦‚è¿°å’Œæ·±å…¥çš„è§è§£ï¼Œä¸ºæ¨åŠ¨LMAsçš„å‘å±•å’Œåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚

## playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents
### Abstract
Large Language Models (LLMs) have shown great success as high-level planners
for zero-shot game-playing agents. However, these agents are primarily
evaluated on Minecraft, where long-term planning is relatively straightforward.
In contrast, agents tested in dynamic robot environments face limitations due
to simplistic environments with only a few objects and interactions. To fill
this gap in the literature, we present NetPlay, the first LLM-powered zero-shot
agent for the challenging roguelike NetHack. NetHack is a particularly
challenging environment due to its diverse set of items and monsters, complex
interactions, and many ways to die.
  NetPlay uses an architecture designed for dynamic robot environments,
modified for NetHack. Like previous approaches, it prompts the LLM to choose
from predefined skills and tracks past interactions to enhance decision-making.
Given NetHack's unpredictable nature, NetPlay detects important game events to
interrupt running skills, enabling it to react to unforeseen circumstances.
While NetPlay demonstrates considerable flexibility and proficiency in
interacting with NetHack's mechanics, it struggles with ambiguous task
descriptions and a lack of explicit feedback. Our findings demonstrate that
NetPlay performs best with detailed context information, indicating the
necessity for dynamic methods in supplying context information for complex
games such as NetHack.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLMs åœ¨ NetHack ä¸­çš„æ½œåŠ›ä¸å±€é™æ€§ï¼šé›¶æ ·æœ¬æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¸¸æˆé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„è§„åˆ’èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ Minecraft ç­‰æ¸¸æˆä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å¤æ‚ã€åŠ¨æ€çš„ç¯å¢ƒæ—¶ï¼Œå¦‚æœºå™¨äººç¯å¢ƒï¼Œå¾€å¾€é¢ä¸´å±€é™æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº† NetPlayï¼Œä¸€ä¸ªåŸºäº LLM çš„é›¶æ ·æœ¬æ™ºèƒ½ä½“ï¼Œç”¨äºæŒ‘æˆ˜æ€§çš„ Rogue-like æ¸¸æˆ NetHackã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
NetPlay é‡‡ç”¨äº†ä¸€ç§ä¸“ä¸ºåŠ¨æ€æœºå™¨äººç¯å¢ƒè®¾è®¡çš„æ¶æ„ï¼Œå¹¶é’ˆå¯¹ NetHack è¿›è¡Œäº†ä¿®æ”¹ã€‚å®ƒé€šè¿‡æç¤º LLM ä»é¢„å®šä¹‰çš„æŠ€èƒ½ä¸­é€‰æ‹©ï¼Œå¹¶è·Ÿè¸ªè¿‡å»çš„äº¤äº’æ¥å¢å¼ºå†³ç­–ã€‚NetPlay è¿˜èƒ½å¤Ÿæ£€æµ‹é‡è¦çš„æ¸¸æˆäº‹ä»¶ï¼Œä»¥ä¾¿åœ¨å‡ºç°æ„å¤–æƒ…å†µæ—¶ä¸­æ–­æ­£åœ¨æ‰§è¡Œçš„æŠ€èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒNetPlay åœ¨ä¸ NetHack çš„æœºåˆ¶äº¤äº’æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ¨¡ç³Šçš„ä»»åŠ¡æè¿°å’Œç¼ºä¹æ˜ç¡®åé¦ˆæ—¶å­˜åœ¨å›°éš¾ã€‚NetPlay åœ¨æä¾›è¯¦ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æƒ…å†µä¸‹è¡¨ç°æœ€ä½³ï¼Œè¿™è¡¨æ˜åœ¨ NetHack ç­‰å¤æ‚æ¸¸æˆä¸­ï¼ŒåŠ¨æ€æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ–¹æ³•è‡³å…³é‡è¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
NetPlay çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMs åœ¨æ¸¸æˆé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨ LLMs çš„èƒ½åŠ›ï¼Œä¾‹å¦‚é€šè¿‡åŠ¨æ€æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–ä½¿ç”¨æœºå™¨å­¦ä¹ æ¥æ›¿ä»£æ‰‹å·¥åˆ¶ä½œçš„ç»„ä»¶ã€‚æ­¤å¤–ï¼ŒNetPlay çš„æ¶æ„å¯ä»¥ä¸ºå…¶ä»–å¤æ‚æ¸¸æˆçš„è®¾è®¡æä¾›å‚è€ƒã€‚

## q-cogni--an-integrated-causal-reinforcement-learning-framework
### Abstract
We present Q-Cogni, an algorithmically integrated causal reinforcement
learning framework that redesigns Q-Learning with an autonomous causal
structure discovery method to improve the learning process with causal
inference. Q-Cogni achieves optimal learning with a pre-learned structural
causal model of the environment that can be queried during the learning process
to infer cause-and-effect relationships embedded in a state-action space. We
leverage on the sample efficient techniques of reinforcement learning, enable
reasoning about a broader set of policies and bring higher degrees of
interpretability to decisions made by the reinforcement learning agent. We
apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against
state-of-the-art reinforcement learning algorithms. We report results that
demonstrate better policies, improved learning efficiency and superior
interpretability of the agent's decision making. We also compare this approach
with traditional shortest-path search algorithms and demonstrate the benefits
of our causal reinforcement learning framework to high dimensional problems.
Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New
York City using the Taxi & Limousine Commission trip record data and compare
with shortest-path search, reporting results that show 85% of the cases with an
equal or better policy derived from Q-Cogni in a real-world domain.


## agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization
### Abstract
Large Language Models (LLMs) exhibit robust problem-solving capabilities for
diverse tasks. However, most LLM-based agents are designed as specific task
solvers with sophisticated prompt engineering, rather than agents capable of
learning and evolving through interactions. These task solvers necessitate
manually crafted prompts to inform task rules and regulate LLM behaviors,
inherently incapacitating to address complex dynamic scenarios e.g., large
interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent
with Policy-level Reflection and Optimization that can learn a wealth of
expertise from interactive experiences and progressively elevate its behavioral
policy. Specifically, it involves a dynamic belief generation and reflection
process for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent-Proï¼šåŸºäºç­–ç•¥çº§åæ€å’Œä¼˜åŒ–çš„å­¦ä¹ è¿›åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å„ç§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°åŸºäºLLMsçš„æ™ºèƒ½ä½“éƒ½æ˜¯ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡çš„ï¼Œéœ€è¦å¤æ‚çš„æç¤ºå·¥ç¨‹æ¥å‘ŠçŸ¥ä»»åŠ¡è§„åˆ™å’Œè°ƒèŠ‚LLMsçš„è¡Œä¸ºã€‚è¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥åº”å¯¹å¤æ‚åŠ¨æ€çš„åœºæ™¯ï¼Œä¾‹å¦‚å¤§å‹äº’åŠ¨æ¸¸æˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAgent-Proçš„LLM-basedæ™ºèƒ½ä½“ï¼Œå®ƒå…·æœ‰ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ä»äº’åŠ¨ç»éªŒä¸­å­¦ä¹ å¤§é‡ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶é€æ­¥æå‡å…¶è¡Œä¸ºç­–ç•¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç­–ç•¥çº§åæ€å’Œä¼˜åŒ–
Agent-Proé€šè¿‡ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–æ¥å­¦ä¹ è¿›åŒ–ã€‚å®ƒä¸ä»…åæ€è¿‡å»çš„è½¨è¿¹å’Œä¿¡å¿µï¼Œè¿˜é€šè¿‡æ·±åº¦ä¼˜å…ˆæœç´¢æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿ç­–ç•¥æ”¶ç›Šçš„æŒç»­æå‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹
Agent-Proé‡‡ç”¨ä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡æ›´æ–°è‡ªèº«ä¿¡å¿µå’Œä¸–ç•Œä¿¡å¿µæ¥ç”Ÿæˆæ›´åˆç†çš„è¡Œä¸ºã€‚å®ƒèƒ½å¤Ÿæ ¹æ®ä¿¡å¿µæ¥é¢„æµ‹è¡ŒåŠ¨ï¼Œå¹¶åœ¨æ¸¸æˆç»“æŸåæ ¹æ®ç»“æœæ¥åæ€å’Œè°ƒæ•´ä¿¡å¿µã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Agent-Proåœ¨ä¸¤ä¸ªæ¸¸æˆï¼ˆBlackjackå’ŒTexas Hold'emï¼‰ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å®ƒèƒ½å¤Ÿå­¦ä¹ å¹¶è¿›åŒ–ï¼Œåœ¨å¤æ‚å’ŒåŠ¨æ€çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„LLMså’Œä¸“é—¨æ¨¡å‹ç›¸æ¯”ï¼ŒAgent-Proåœ¨æ¸¸æˆä¸­çš„æ”¶ç›Šæ›´é«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Agent-Proçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¸ºæ„å»ºèƒ½å¤Ÿå­¦ä¹ å’Œè¿›åŒ–çš„LLM-basedæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶ç­–ç•¥çº§åæ€å’Œä¼˜åŒ–æœºåˆ¶å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“ä»äº’åŠ¨ç»éªŒä¸­å­¦ä¹ ï¼Œå¹¶é€æ­¥æå‡å…¶è¡Œä¸ºç­–ç•¥ã€‚æ­¤å¤–ï¼Œä¿¡å¿µæ„ŸçŸ¥å†³ç­–è¿‡ç¨‹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨ä¸ç¡®å®šçš„åœºæ™¯ä¸­åšå‡ºæ›´åˆç†çš„å†³ç­–ã€‚è¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å•†ä¸šè°ˆåˆ¤ã€å®‰å…¨ç›‘æ§ç­‰ã€‚

## pca-bench--evaluating-multimodal-large-language-models-in-perception-cognition-action-chain
### Abstract
We present PCA-Bench, a multimodal decision-making benchmark for evaluating
the integrated capabilities of Multimodal Large Language Models (MLLMs).
Departing from previous benchmarks focusing on simplistic tasks and individual
model capability, PCA-Bench introduces three complex scenarios: autonomous
driving, domestic robotics, and open-world games. Given task instructions and
diverse contexts, the model is required to seamlessly integrate multiple
capabilities of Perception, Cognition, and Action in a reasoning chain to make
accurate decisions. Moreover, PCA-Bench features error localization
capabilities, scrutinizing model inaccuracies in areas such as perception,
knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To
balance accuracy and efficiency in evaluation, we propose PCA-Eval, an
automatic evaluation protocol, and assess 10 prevalent MLLMs. The results
reveal significant performance disparities between open-source models and
powerful proprietary models like GPT-4 Vision. To address this, we introduce
Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing
instruction tuning examples in multimodal embodied environments. EIE generates
7,510 training examples in PCA-Bench and enhances the performance of
open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision
accuracy), thereby validating the effectiveness of EIE. Our findings suggest
that robust MLLMs like GPT4-Vision show promise for decision-making in embodied
agents, opening new avenues for MLLM research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PCA-Benchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­çš„å†³ç­–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†å¾€å¾€åªå…³æ³¨å•ä¸ªæ¨¡å‹èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹åœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢çš„ç»¼åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ç¼ºä¹å¯¹æ¨¡å‹é”™è¯¯è¿›è¡Œå®šä½çš„èƒ½åŠ›ï¼Œè¿™ä½¿å¾—éš¾ä»¥ç¡®å®šæ¨¡å‹åœ¨å“ªäº›æ–¹é¢éœ€è¦æ”¹è¿›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šPCA-Bench
æœ¬æ–‡æå‡ºäº†PCA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨é“¾ä¸­å†³ç­–èƒ½åŠ›çš„å¤šæ¨¡æ€å†³ç­–åŸºå‡†ã€‚PCA-Benchå¼•å…¥äº†ä¸‰ä¸ªå¤æ‚çš„åœºæ™¯ï¼šè‡ªåŠ¨é©¾é©¶ã€å®¶åº­æœºå™¨äººå’Œå¼€æ”¾ä¸–ç•Œæ¸¸æˆã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæ¨¡å‹éœ€è¦æ ¹æ®ä»»åŠ¡æŒ‡ä»¤å’Œä¸åŒçš„ä¸Šä¸‹æ–‡ï¼Œæ— ç¼åœ°æ•´åˆæ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œä»¥åšå‡ºå‡†ç¡®çš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šPCA-Eval
ä¸ºäº†å¹³è¡¡è¯„ä¼°çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†PCA-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ã€‚PCA-Evalåˆ©ç”¨LLMså¼ºå¤§çš„è¯­ä¹‰è§£æèƒ½åŠ›ï¼Œæ ¹æ®æ•°æ®æ³¨é‡Šä¸­çš„é”šç‚¹ä¿¡æ¯ï¼Œè‡ªåŠ¨è¿›è¡Œé”™è¯¯å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCA-Evalä¸äººç±»è¯„ä¼°ç»“æœå…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§ï¼Œå¹³å‡Kappaç³»æ•°è¾¾åˆ°0.8+ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šEmbodied-Instruction-Evolution (EIE)
ä¸ºäº†è§£å†³PCA-Benchæ•°æ®é›†æ ‡æ³¨å·¥ä½œé‡å¤§çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Embodied-Instruction-Evolution (EIE)æ¡†æ¶ã€‚EIEåˆ©ç”¨LLMsè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹ï¼Œä»è€Œå‡å°‘äº†äººå·¥åŠ³åŠ¨ï¼Œå¹¶æé«˜äº†PCA-Benchçš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4 Visionåœ¨æ„ŸçŸ¥ã€è®¤çŸ¥å’Œè¡ŒåŠ¨æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç°æœ‰çš„å¼€æºMLLMsã€‚EIEæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜å¼€æºMLLMsçš„æ€§èƒ½ï¼Œåœ¨æŸäº›æŒ‡æ ‡ä¸Šç”šè‡³è¶…è¿‡äº†GPT-4 Visionã€‚PCA-Evalèƒ½å¤Ÿæœ‰æ•ˆåœ°å®šä½æ¨¡å‹é”™è¯¯ï¼Œä»è€Œæé«˜äº†æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„PCA-Benchå’ŒPCA-Evalä¸ºè¯„ä¼°MLLMsçš„å†³ç­–èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œè¯„ä¼°å·¥å…·ã€‚EIEæ¡†æ¶ä¸ºè‡ªåŠ¨åˆæˆå¤šæ¨¡æ€å…·èº«ç¯å¢ƒä¸­çš„æŒ‡ä»¤è°ƒæ•´ç¤ºä¾‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºå¤§çš„MLLMsåœ¨å…·èº«æ™ºèƒ½ä½“ä¸­çš„å†³ç­–èƒ½åŠ›å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œä¸ºMLLMsçš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚

## what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents
### Abstract
This study introduces "CosmoAgent," an innovative artificial intelligence
system that utilizes Large Language Models (LLMs) to simulate complex
interactions between human and extraterrestrial civilizations. This paper
introduces a mathematical model for quantifying the levels of civilization
development and further employs a state transition matrix approach to evaluate
their trajectories. Through this methodology, our study quantitatively analyzes
the growth trajectories of civilizations, providing insights into future
decision-making at critical points of growth and saturation. Furthermore, this
paper acknowledges the vast diversity of potential living conditions across the
universe, which could foster unique cosmologies, ethical codes, and worldviews
among different civilizations. Recognizing the Earth-centric bias inherent in
current LLM designs, we propose the novel concept of using LLM agents with
diverse ethical paradigms and simulating interactions between entities with
distinct moral principles. This innovative research not only introduces a novel
method for comprehending potential inter-civilizational dynamics but also holds
practical value in enabling entities with divergent value systems to
strategize, prevent conflicts, and engage in games under conditions of
asymmetric information. The accompanying code is available at
https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå¤–æ˜Ÿæ–‡æ˜ï¼šæ¢ç´¢å®‡å®™ä¸­çš„äº’åŠ¨ä¸å†²çª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨æ¨¡æ‹Ÿå¤æ‚ç¤¾ä¼šåŠ¨æ€æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMsè®¾è®¡å¾€å¾€å¸¦æœ‰åœ°çƒä¸­å¿ƒä¸»ä¹‰çš„åè§ï¼Œéš¾ä»¥å…¨é¢æ¨¡æ‹Ÿå¤–æ˜Ÿæ–‡æ˜çš„å¤šæ ·æ€§å’Œç‹¬ç‰¹æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥å…·æœ‰ä¸åŒä¼¦ç†èŒƒå¼å’Œé“å¾·åŸåˆ™çš„LLMä»£ç†ï¼Œæ¨¡æ‹Ÿäººç±»ä¸å¤–æ˜Ÿæ–‡æ˜ä¹‹é—´çš„å¤æ‚äº’åŠ¨ï¼Œä»è€Œä¸ºç†è§£æ½œåœ¨æ˜Ÿé™…åŠ¨æ€æä¾›æ–°çš„è§†è§’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCosmoAgentå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
æœ¬æ–‡æå‡ºäº†CosmoAgentï¼Œä¸€ä¸ªåŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºæ¨¡æ‹Ÿå®‡å®™ä¸­ä¸åŒæ–‡æ˜ä¹‹é—´çš„äº’åŠ¨ã€‚CosmoAgenté€šè¿‡æ¨¡æ‹Ÿæ–‡æ˜çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€‰æ‹©éšè—ã€æˆ˜æ–—æˆ–åˆä½œï¼Œæ¥æ¢ç´¢æ–‡æ˜å‘å±•çš„è½¨è¿¹å’Œæ½œåœ¨å†²çªã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ–‡æ˜å‘å±•æ¨¡å‹
æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ•°å­¦æ¨¡å‹æ¥é‡åŒ–æ–‡æ˜çš„å‘å±•æ°´å¹³ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€è½¬ç§»çŸ©é˜µæ–¹æ³•æ¥è¯„ä¼°æ–‡æ˜çš„è½¨è¿¹ã€‚è¯¥æ¨¡å‹è€ƒè™‘äº†äº”ä¸ªå…³é”®èµ„æºï¼šå†›äº‹èƒ½åŠ›ã€æŠ€æœ¯å‘å±•ã€ç”Ÿäº§èƒ½åŠ›ã€æ¶ˆè´¹å’Œå‚¨å­˜ï¼Œä»¥åŠä¸åŒæ–‡æ˜çš„ä¸–ç•Œè§‚ï¼ˆå’Œå¹³ä¸»ä¹‰ã€å†›å›½ä¸»ä¹‰å’Œå­¤ç«‹ä¸»ä¹‰ï¼‰å¯¹å†³ç­–çš„å½±å“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¿¡æ¯ä¸å¯¹ç§°çš„æ¨¡æ‹Ÿ
ä¸ºäº†æ¨¡æ‹Ÿå®‡å®™ä¸­æ–‡æ˜ä¹‹é—´çš„äº’åŠ¨ï¼Œæœ¬æ–‡è€ƒè™‘äº†ä¿¡æ¯ä¸å¯¹ç§°çš„æƒ…å†µï¼Œå³æ–‡æ˜ä¹‹é—´çš„è§‚æµ‹æ•°æ®æ»åäºå®é™…å‘å±•ã€‚LLMsä»£ç†éœ€è¦æ ¹æ®è¿‡æ—¶çš„ä¿¡æ¯åšå‡ºå†³ç­–ï¼Œè¿™å¢åŠ äº†æ¨¡æ‹Ÿçš„å¤æ‚æ€§å’Œç°å®æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šé“å¾·å¤šæ ·æ€§çš„æ¨¡æ‹Ÿ
æœ¬æ–‡æå‡ºäº†ä½¿ç”¨å…·æœ‰ä¸åŒä¼¦ç†èŒƒå¼çš„LLMä»£ç†æ¥æ¨¡æ‹Ÿå…·æœ‰ä¸åŒé“å¾·åŸåˆ™çš„å®ä½“ä¹‹é—´çš„äº’åŠ¨ã€‚è¿™æœ‰åŠ©äºç†è§£ä¸åŒæ–‡æ˜å¦‚ä½•å…±å­˜ï¼Œä»¥åŠé“å¾·æ¡†æ¶å¦‚ä½•å½±å“æ˜Ÿé™…äº’åŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå…·æœ‰å†›å›½ä¸»ä¹‰ä¸–ç•Œè§‚çš„æ–‡æ˜å€¾å‘äºå¯¹è¾ƒå¼±æ–‡æ˜å‘åŠ¨æ”»å‡»ï¼Œè€Œå­¤ç«‹ä¸»ä¹‰æ–‡æ˜åˆ™æ›´å€¾å‘äºåœ¨è§‚å¯Ÿä¸€æ®µæ—¶é—´åé€‰æ‹©æ€§åœ°ä¸å…¶ä»–æ–‡æ˜åˆä½œã€‚æ­¤å¤–ï¼Œä¿¡æ¯ä¸å¯¹ç§°ä¼šå»¶è¿Ÿå†²çªçš„å‘ç”Ÿï¼Œä¸ºè¾ƒå¼±æ–‡æ˜æä¾›äº†åå‡»çš„æœºä¼šã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ä»…ä¸ºç†è§£æ½œåœ¨æ˜Ÿé™…åŠ¨æ€æä¾›äº†æ–°çš„è§†è§’ï¼Œè¿˜ä¸ºè§£å†³å…·æœ‰ä¸åŒä»·å€¼ä½“ç³»çš„å®ä½“ä¹‹é—´çš„å†²çªæä¾›äº†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œå¦‚æ¨¡æ‹Ÿå¤ä»£ç¤¾ä¼šã€äººç±»æ–‡æ˜æ¨¡å¼å’Œç¤¾ä¼šç”Ÿæ€ç³»ç»Ÿã€‚

## software-agents-interaction-algorithms-in-virtual-learning-environment
### Abstract
This paper highlights the multi-agent learning virtual environment and agents
communication algorithms. The researcher proposed three algorithms required
software agents interaction in virtual learning information system environment.
The first proposed algorithm is agents interaction localization algorithm, the
second one is the dynamic agents distribution algorithm (load distribution
algorithm), and the third model is Agent communication algorithm based on using
agents intermediaries. The main objectives of these algorithms are to reduce
the response time for any agents changes in virtual learning environment (VLE)
by increasing the information exchange intensity between software agents and
reduce the overall network load, and to improve the communication between
mobile agents in distributed information system to support effectiveness.
Finally the paper describe the algorithms of information exchange between
mobile agents in VLE based on the expansion of the address structure and the
use of an agent, intermediary agents, matchmaking agents, brokers and their
entrepreneurial functions
### ğŸŒŸ è®ºæ–‡è§£è¯» | è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒä¸­è½¯ä»¶ä»£ç†äº¤äº’ç®—æ³•çš„é©æ–°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ä¿¡æ¯æŠ€æœ¯çš„é£é€Ÿå‘å±•ï¼Œè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒï¼ˆVLEï¼‰å·²ç»æˆä¸ºæ•™è‚²é¢†åŸŸçš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VLEç³»ç»Ÿåœ¨å¤„ç†å¤§é‡ä¿¡æ¯äº¤äº’æ—¶ï¼Œé¢ä¸´ç€ç½‘ç»œè´Ÿè½½å¢åŠ ã€ä¿¡æ¯äº¤æ¢æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè½¯ä»¶ä»£ç†çš„äº¤äº’ç®—æ³•ï¼Œæ—¨åœ¨æé«˜VLEç³»ç»Ÿçš„æ•ˆç‡å’Œå“åº”é€Ÿåº¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä»£ç†äº¤äº’å®šä½ç®—æ³•
è¯¥ç®—æ³•é€šè¿‡åˆ†æä»£ç†ä¹‹é—´çš„é€šä¿¡ä¾èµ–æ€§ï¼Œå°†é¢‘ç¹äº¤äº’çš„ä»£ç†åˆ†ç»„åˆ°åŒä¸€ä¸»æœºä¸Šï¼Œä»è€Œå°†è·¨ä¸»æœºçš„äº¤äº’è½¬åŒ–ä¸ºä¸»æœºå†…çš„äº¤äº’ï¼Œå‡å°‘ç½‘ç»œè´Ÿè½½å¹¶æé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€ä»£ç†åˆ†é…ç®—æ³•ï¼ˆè´Ÿè½½åˆ†é…ç®—æ³•ï¼‰
è¯¥ç®—æ³•é€šè¿‡ç›‘æ§ä¸»æœºè´Ÿè½½ï¼Œå°†ä»£ç†åˆ†ç»„å¹¶åŠ¨æ€åˆ†é…åˆ°ä¸åŒçš„ä¸»æœºä¸Šï¼Œä»¥å®ç°è´Ÿè½½å‡è¡¡ï¼Œé¿å…æŸäº›ä¸»æœºè¿‡è½½ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºä»£ç†ä¸­ä»‹çš„é€šä¿¡æ¨¡å‹
è¯¥æ¨¡å‹åˆ©ç”¨ä»£ç†ä¸­ä»‹ï¼ˆå¦‚ç»çºªäººä»£ç†å’Œé…å¯¹ä»£ç†ï¼‰æ¥ä¿ƒè¿›ä»£ç†ä¹‹é—´çš„é€šä¿¡ã€‚ä»£ç†ä¸­ä»‹å¯ä»¥å¸®åŠ©ä»£ç†æŸ¥æ‰¾å…·æœ‰ç›¸ä¼¼å…´è¶£çš„ä»£ç†ï¼Œå¹¶æä¾›æ¶ˆæ¯è½¬å‘å’ŒåŒ¹é…æœåŠ¡ï¼Œä»è€Œæé«˜é€šä¿¡æ•ˆç‡å’Œçµæ´»æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡æå‡ºçš„ç®—æ³•åœ¨è™šæ‹Ÿå­¦ä¹ ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜ï¼Œè¿™äº›ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç½‘ç»œè´Ÿè½½ï¼Œæé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ï¼Œå¹¶æé«˜ä»£ç†ä¹‹é—´çš„é€šä¿¡æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ç®—æ³•å’Œæ¨¡å‹ä¸ºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒçš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶ä¸­ï¼Œä»£ç†äº¤äº’å®šä½ç®—æ³•å’ŒåŠ¨æ€ä»£ç†åˆ†é…ç®—æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œä»¥æé«˜ç³»ç»Ÿçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚åŸºäºä»£ç†ä¸­ä»‹çš„é€šä¿¡æ¨¡å‹å¯ä»¥åº”ç”¨äºå…¶ä»–å¤šä»£ç†ç³»ç»Ÿä¸­ï¼Œä»¥ä¿ƒè¿›ä»£ç†ä¹‹é—´çš„åä½œå’Œé€šä¿¡ã€‚

### ğŸ“š æ€»ç»“
æœ¬æ–‡æå‡ºçš„åŸºäºè½¯ä»¶ä»£ç†çš„äº¤äº’ç®—æ³•ä¸ºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒçš„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚è¿™äº›ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ç½‘ç»œè´Ÿè½½ï¼Œæé«˜ä¿¡æ¯äº¤æ¢æ•ˆç‡ï¼Œå¹¶æé«˜ä»£ç†ä¹‹é—´çš„é€šä¿¡æ•ˆç‡ã€‚æœ¬æ–‡çš„ç ”ç©¶æˆæœå¯¹äºè™šæ‹Ÿå­¦ä¹ ç¯å¢ƒå’Œå…¶ä»–åˆ†å¸ƒå¼ç³»ç»Ÿçš„è®¾è®¡å’Œä¼˜åŒ–å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## enhance-reasoning-for-large-language-models-in-the-game-werewolf
### Abstract
This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ï¼šä»¥ç‹¼äººæ€æ¸¸æˆä¸ºæ¡ˆä¾‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„çªç ´ï¼Œå…¶åœ¨æ¨ç†ã€è§„åˆ’å’Œå†³ç­–ç­‰é¢†åŸŸçš„æ½œåŠ›ä¹Ÿé€æ¸æ˜¾ç°ã€‚ç„¶è€Œï¼ŒLLMsåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œæ·±åº¦é€»è¾‘åˆ†æçš„ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥å¤–éƒ¨æ¨ç†æ¨¡å—ï¼Œå³â€œæ€è€ƒè€…â€ï¼ˆThinkerï¼‰ï¼Œæ¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒç³»ç»Ÿæ¨ç†æ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œå°†LLMsä¸å¤–éƒ¨Thinkeræ¨¡å—ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªæ¨ç†å±‚æ¬¡ç»“æ„ã€‚LLMsè´Ÿè´£å¤„ç†ç›´è§‚çš„System-1ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¸¸è¯†æ¨ç†ï¼Œè€ŒThinkeråˆ™ä¸“æ³¨äºéœ€è¦å¤æ‚é€»è¾‘åˆ†æå’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„System-2ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šThinkeræ¨¡å—çš„è®¾è®¡ä¸è®­ç»ƒ
Thinkeræ¨¡å—ç›´æ¥ä»æ•°æ®åº“ä¸­è·å–çŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚å®ƒé€šè¿‡æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºç¾¤ä½“çš„è®­ç»ƒç­‰æ–¹æ³•ï¼Œå­¦ä¹ ç”Ÿæˆåˆç†çš„æ¸¸æˆåŠ¨ä½œå’ŒLLMçš„è¯­éŸ³æŒ‡ä»¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é›†è´¡çŒ®
æœ¬æ–‡æ”¶é›†äº†18,800åœºçœŸå®äººç±»æ¸¸æˆä¼šè¯æ•°æ®ï¼Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç¤¾äº¤æ¨ç†æ¸¸æˆæ•°æ®é›†ï¼Œä¸ºç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥Thinkeræ¨¡å—æ˜¾è‘—æé«˜äº†LLMsçš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼ŒThinkeræ¨¡å—åœ¨æ¨ç†ã€è¯­éŸ³ç”Ÿæˆå’Œåœ¨çº¿æ¸¸æˆè¯„ä¼°æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†Thinkerä¸ä¸€ä¸ªè¾ƒå°çš„LLMæ¨¡å‹ï¼ˆ6Bï¼‰è¿›è¡Œå¾®è°ƒï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¿‡äº†GPT4ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡å°†LLMsä¸å¤–éƒ¨æ¨ç†æ¨¡å—ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆæå‡LLMsåœ¨ç‰¹å®šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†ä¹Ÿä¸ºç¤¾äº¤æ¨ç†æ¸¸æˆçš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®åŸºç¡€ã€‚

## pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models
### Abstract
We introduce PokeLLMon, the first LLM-embodied agent that achieves
human-parity performance in tactical battle games, as demonstrated in Pokemon
battles. The design of PokeLLMon incorporates three key strategies: (i)
In-context reinforcement learning that instantly consumes text-based feedback
derived from battles to iteratively refine the policy; (ii) Knowledge-augmented
generation that retrieves external knowledge to counteract hallucination and
enables the agent to act timely and properly; (iii) Consistent action
generation to mitigate the panic switching phenomenon when the agent faces a
powerful opponent and wants to elude the battle. We show that online battles
against human demonstrates PokeLLMon's human-like battle strategies and
just-in-time decision making, achieving 49% of win rate in the Ladder
competitions and 56% of win rate in the invited battles. Our implementation and
playable battle logs are available at: https://github.com/git-disl/PokeLLMon.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PokeLLMonï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å®ç°äººç±»æ°´å¹³çš„å®å¯æ¢¦æˆ˜æ–—AI

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç”Ÿæˆå¼AIå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„æˆåŠŸï¼Œäººä»¬å¼€å§‹æ¢ç´¢LLMså¦‚ä½•è‡ªä¸»åœ°åœ¨ç‰©ç†ä¸–ç•Œä¸­è¡ŒåŠ¨ï¼Œå°†ç”Ÿæˆç©ºé—´ä»æ–‡æœ¬æ‰©å±•åˆ°è¡ŒåŠ¨ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯è¿½æ±‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å…³é”®èŒƒå¼ã€‚æ¸¸æˆæ˜¯å¼€å‘LLM-basedä»£ç†ä¸è™šæ‹Ÿç¯å¢ƒäº¤äº’çš„åˆé€‚æµ‹è¯•å¹³å°ã€‚æˆ˜æœ¯æˆ˜æ–—æ¸¸æˆï¼Œå¦‚å®å¯æ¢¦æˆ˜æ–—ï¼Œå› å…¶çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç¦»æ•£ã€å›åˆåˆ¶æ ¼å¼ã€æˆ˜ç•¥æ€§å’Œå¤æ‚æ€§ï¼Œæˆä¸ºè¯„ä¼°LLMsæ¸¸æˆèƒ½åŠ›çš„ç†æƒ³åŸºå‡†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼ˆICRLï¼‰
ä¸ºäº†è§£å†³LLMsåœ¨å®å¯æ¢¦æˆ˜æ–—ä¸­å‡ºç°çš„å¹»è§‰é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ICRLç­–ç•¥ã€‚ICRLåˆ©ç”¨æˆ˜æ–—ä¸­å³æ—¶ç”Ÿæˆçš„æ–‡æœ¬åé¦ˆä½œä¸ºâ€œå¥–åŠ±â€ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹è¿­ä»£ä¼˜åŒ–åŠ¨ä½œç”Ÿæˆç­–ç•¥ã€‚é€šè¿‡åˆ†æå‰ä¸€è½®çš„è¡ŒåŠ¨å’Œç›¸åº”çš„æ–‡æœ¬åé¦ˆï¼Œä»£ç†èƒ½å¤Ÿä¸æ–­è°ƒæ•´å…¶ç­–ç•¥ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹æˆ˜æ–—ä¸­çš„å˜åŒ–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šçŸ¥è¯†å¢å¼ºç”Ÿæˆï¼ˆKAGï¼‰
ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘å¹»è§‰ï¼Œè®ºæ–‡å¼•å…¥äº†KAGç­–ç•¥ã€‚KAGé€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†ï¼Œå¦‚ç±»å‹ä¼˜åŠ¿/åŠ£åŠ¿å…³ç³»å’ŒæŠ€èƒ½/èƒ½åŠ›æ•ˆæœï¼Œæ¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹ã€‚è¿™äº›çŸ¥è¯†æ¥æºäºå®å¯æ¢¦æ¸¸æˆä¸­çš„å®å¯æ¢¦å›¾é‰´ï¼ˆPokÃ©dexï¼‰ï¼Œå®ƒæä¾›äº†å…³äºå®å¯æ¢¦ç±»å‹ã€æŠ€èƒ½å’Œèƒ½åŠ›çš„è¯¦ç»†ä¿¡æ¯ã€‚é€šè¿‡å°†å¤–éƒ¨çŸ¥è¯†æ·»åŠ åˆ°çŠ¶æ€æè¿°ä¸­ï¼Œä»£ç†èƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£æˆ˜æ–—æƒ…å†µï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆ
ä¸ºäº†è§£å†³ä»£ç†åœ¨é¢å¯¹å¼ºå¤§å¯¹æ‰‹æ—¶å‡ºç°çš„ææ…Œåˆ‡æ¢ç°è±¡ï¼Œè®ºæ–‡æå‡ºäº†ä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆç­–ç•¥ã€‚è¯¥ç­–ç•¥é€šè¿‡å¤šæ¬¡ç‹¬ç«‹ç”Ÿæˆè¡ŒåŠ¨å¹¶æŠ•ç¥¨é€‰å‡ºæœ€ä¸€è‡´çš„è¡ŒåŠ¨ï¼Œæ¥å‡å°‘è¡ŒåŠ¨çš„ä¸ä¸€è‡´æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºä»£ç†åœ¨é¢å¯¹å‹åŠ›æ—¶ä¿æŒå†·é™ï¼Œé¿å…è¿‡åº¦æ€è€ƒå’Œææ…Œï¼Œä»è€Œåšå‡ºæ›´ç¨³å®šçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨çº¿æˆ˜æ–—ç»“æœè¡¨æ˜ï¼ŒPokeLLMonåœ¨æ¢¯å­æ¯”èµ›ä¸­å–å¾—äº†49%çš„èƒœç‡ï¼Œåœ¨é‚€è¯·æ¯”èµ›ä¸­å–å¾—äº†56%çš„èƒœç‡ï¼Œå±•ç°å‡ºä¸äººç±»ç©å®¶ç›¸å½“çš„æ¯”èµ›èƒ½åŠ›å’Œç­–ç•¥ã€‚ç„¶è€Œï¼ŒPokeLLMonåœ¨é¢å¯¹äººç±»ç©å®¶çš„æ¶ˆè€—ç­–ç•¥å’Œæ¬ºéª—æŠ€å·§æ—¶ä¹Ÿå­˜åœ¨å¼±ç‚¹ï¼Œè¿™è¡¨æ˜æœªæ¥éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›å…¶é•¿æœŸè§„åˆ’å’Œå¯¹æ‰‹è¡Œä¸ºé¢„æµ‹èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PokeLLMonçš„è®¾è®¡å’Œå®ç°ä¸ºLLMsåœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚ICRLã€KAGå’Œä¸€è‡´æ€§è¡ŒåŠ¨ç”Ÿæˆç­–ç•¥å¯ä»¥åº”ç”¨äºå…¶ä»–æ¸¸æˆï¼Œå¸®åŠ©LLMsæ›´å¥½åœ°ç†è§£å’Œåº”å¯¹æ¸¸æˆä¸­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒPokeLLMonçš„å®éªŒç»“æœä¹Ÿæ­ç¤ºäº†LLMsåœ¨æ¸¸æˆä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶å’Œå¼€å‘æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## swarmbrain--embodied-agent-for-real-time-strategy-game-starcraft-ii-via-large-language-models
### Abstract
Large language models (LLMs) have recently garnered significant
accomplishments in various exploratory tasks, even surpassing the performance
of traditional reinforcement learning-based methods that have historically
dominated the agent-based field. The purpose of this paper is to investigate
the efficacy of LLMs in executing real-time strategy war tasks within the
StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an
embodied agent leveraging LLM for real-time strategy implementation in the
StarCraft II game environment. The SwarmBrain comprises two key components: 1)
a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed
to orchestrate macro-level strategies from a high-level perspective. This
matrix emulates the overarching consciousness of the Zerg intelligence brain,
synthesizing strategic foresight with the aim of allocating resources,
directing expansion, and coordinating multi-pronged assaults. 2) a Swarm
ReflexNet, which is agile counterpart to the calculated deliberation of the
Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the
Swarm ReflexNet employs a condition-response state machine framework, enabling
expedited tactical responses for fundamental Zerg unit maneuvers. In the
experimental setup, SwarmBrain is in control of the Zerg race in confrontation
with an Computer-controlled Terran adversary. Experimental results show the
capacity of SwarmBrain to conduct economic augmentation, territorial expansion,
and tactical formulation, and it shows the SwarmBrain is capable of achieving
victory against Computer players set at different difficulty levels.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SwarmBrainï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®æ—¶ç­–ç•¥æ¸¸æˆStarCraft IIçš„æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å®æ—¶ç­–ç•¥ï¼ˆRTSï¼‰æ¸¸æˆï¼Œå¦‚ã€Šæ˜Ÿé™…äº‰éœ¸IIã€‹ï¼Œå› å…¶å¤æ‚çš„æˆ˜åœºç¯å¢ƒå’Œå¿«é€Ÿå†³ç­–çš„éœ€æ±‚ï¼Œå¯¹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æå‡ºäº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•åœ¨å¤„ç†è¿™ç§å¤æ‚ç¯å¢ƒæ—¶é‡åˆ°äº†å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å°†é«˜çº§ç›®æ ‡ç›´æ¥æ˜ å°„åˆ°ä½çº§é”®ç›˜å’Œé¼ æ ‡è¾“å…¥æ–¹é¢ã€‚è€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶å¯¹å¤æ‚è¯­å¢ƒçš„é«˜å±‚æ¬¡æŠ½è±¡å’Œç†è§£èƒ½åŠ›ï¼Œåœ¨æ¢ç´¢æ€§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†åœ¨å®æ—¶ç­–ç•¥æ¸¸æˆä¸­ï¼ŒLLMçš„æ¨ç†å»¶è¿Ÿé™åˆ¶äº†å…¶ç›´æ¥åº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†SwarmBrainï¼Œä¸€ä¸ªåŸºäºLLMçš„æ™ºèƒ½ä½“ï¼Œç”¨äºåœ¨ã€Šæ˜Ÿé™…äº‰éœ¸IIã€‹ä¸­æ‰§è¡Œå®æ—¶ç­–ç•¥ä»»åŠ¡ã€‚SwarmBrainç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šOvermind Intelligence Matrix
è¿™æ˜¯ä¸€ä¸ªç”±æœ€å…ˆè¿›çš„LLMé©±åŠ¨çš„çŸ©é˜µï¼Œè´Ÿè´£ä»é«˜å±‚æ¬¡è§†è§’åè°ƒå®è§‚ç­–ç•¥ã€‚å®ƒæ¨¡æ‹Ÿäº†Zergæ™ºèƒ½è„‘çš„æ€»ä½“æ„è¯†ï¼Œç»“åˆæˆ˜ç•¥è¿œè§ï¼Œæ—¨åœ¨åˆ†é…èµ„æºã€æŒ‡å¯¼æ‰©å¼ å’Œåè°ƒå¤šæ–¹é¢çš„æ”»å‡»ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSwarm ReflexNet
è¿™æ˜¯Overmind Intelligence Matrixçš„æ•æ·å¯¹åº”ç‰©ï¼Œé‡‡ç”¨æ¡ä»¶å“åº”çŠ¶æ€æœºæ¡†æ¶ï¼Œä¸ºåŸºæœ¬çš„Zergå•ä½æ“ä½œæä¾›å¿«é€Ÿæˆ˜æœ¯å“åº”ã€‚ç”±äºLLMæ¨ç†çš„å›ºæœ‰å»¶è¿Ÿï¼ŒSwarm ReflexNetèƒ½å¤Ÿå¿«é€Ÿå“åº”ï¼Œè€Œæ— éœ€ç­‰å¾…LLMçš„æ·±å…¥æ€è€ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SwarmBrainåœ¨ä¸ä¸åŒéš¾åº¦çº§åˆ«çš„è®¡ç®—æœºå¯¹æ‰‹çš„å¯¹æŠ—ä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨éå¸¸å®¹æ˜“ã€å®¹æ˜“ã€ä¸­ç­‰å’Œä¸­ç­‰å›°éš¾çº§åˆ«ï¼ŒSwarmBrainçš„æˆåŠŸç‡ä¸º100%ã€‚å³ä½¿åœ¨å›°éš¾çº§åˆ«ï¼ŒSwarmBrainä¹Ÿèƒ½åœ¨76%çš„æ¯”èµ›ä¸­å–å¾—èƒœåˆ©ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SwarmBrainçš„è®¾è®¡å±•ç¤ºäº†LLMåœ¨å®æ—¶ç­–ç•¥æ¸¸æˆä¸­çš„åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡ç»“åˆå®è§‚ç­–ç•¥è§„åˆ’å’Œå¿«é€Ÿæˆ˜æœ¯å“åº”ï¼ŒSwarmBrainèƒ½å¤Ÿåœ¨å¤æ‚çš„æ¸¸æˆç¯å¢ƒä¸­å–å¾—æˆåŠŸã€‚æ­¤å¤–ï¼ŒSwarmBrainçš„è®¾è®¡ä¹Ÿä¸ºæˆ‘ä»¬æä¾›äº†å…³äºå¦‚ä½•å°†LLMåº”ç”¨äºå…¶ä»–éœ€è¦å¿«é€Ÿå†³ç­–çš„é¢†åŸŸçš„è§è§£ã€‚

## civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents
### Abstract
The generalization of decision-making agents encompasses two fundamental
elements: learning from past experiences and reasoning in novel contexts.
However, the predominant emphasis in most interactive environments is on
learning, often at the expense of complexity in reasoning. In this paper, we
introduce CivRealm, an environment inspired by the Civilization game.
Civilization's profound alignment with human history and society necessitates
sophisticated learning, while its ever-changing situations demand strong
reasoning to generalize. Particularly, CivRealm sets up an
imperfect-information general-sum game with a changing number of players; it
presents a plethora of complex features, challenging the agent to deal with
open-ended stochastic environments that require diplomacy and negotiation
skills. Within CivRealm, we provide interfaces for two typical agent types:
tensor-based agents that focus on learning, and language-based agents that
emphasize reasoning. To catalyze further research, we present initial results
for both paradigms. The canonical RL-based agents exhibit reasonable
performance in mini-games, whereas both RL- and LLM-based agents struggle to
make substantial progress in the full game. Overall, CivRealm stands as a
unique learning and reasoning challenge for decision-making agents. The code is
available at https://github.com/bigai-ai/civrealm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CivRealmï¼šå†³ç­–æ™ºèƒ½ä½“çš„å­¦ä¹ ä¸æ¨ç†ä¹‹æ—…

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„å†³ç­–æ™ºèƒ½ä½“ç¯å¢ƒå¾€å¾€è¿‡äºå¼ºè°ƒå­¦ä¹ ï¼Œè€Œå¿½è§†äº†æ¨ç†çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åŒæ—¶å…·å¤‡å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›ï¼Œæ‰èƒ½æ›´å¥½åœ°é€‚åº”å¤æ‚å¤šå˜çš„ç¯å¢ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CivRealmï¼Œä¸€ä¸ªåŸºäºæ–‡æ˜æ¸¸æˆçš„äº¤äº’å¼ç¯å¢ƒï¼Œæ—¨åœ¨æ¨åŠ¨å†³ç­–æ™ºèƒ½ä½“å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›çš„è¾¹ç•Œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCivRealmç¯å¢ƒ
CivRealmæ˜¯ä¸€ä¸ªåŸºäºæ–‡æ˜æ¸¸æˆçš„å¼€æ”¾ç¯å¢ƒï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
* **ä¸å®Œå…¨ä¿¡æ¯**ï¼šç©å®¶åªèƒ½è·å–è‡ªå·±å•ä½å‘ç°çš„ä¿¡æ¯ï¼Œéœ€è¦æ¨ç†å…¶ä»–ç©å®¶çš„æ„å›¾ã€‚
* **éšæœºæ€§**ï¼šæ¸¸æˆä¸­æœ‰éšæœºäº‹ä»¶å’Œå±æœºï¼Œéœ€è¦æ™ºèƒ½ä½“çµæ´»åº”å¯¹ã€‚
* **å¤šç›®æ ‡**ï¼šæœ‰å¤šç§èƒœåˆ©è·¯å¾„ï¼Œéœ€è¦å¹³è¡¡ç»æµã€å†›äº‹ã€å¤–äº¤ã€æ–‡åŒ–å’Œç§‘æŠ€å‘å±•ã€‚
* **åŠ¨æ€ç©ºé—´**ï¼šæ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç©å®¶çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¼šåŠ¨æ€å˜åŒ–ã€‚
* **å¤šæ™ºèƒ½ä½“**ï¼šå¤šä¸ªç©å®¶å¯ä»¥äº’åŠ¨ï¼Œéœ€è¦æ™ºèƒ½ä½“è¿›è¡Œåˆä½œå’Œç«äº‰ã€‚
* **åŠ¨æ€ç©å®¶æ•°é‡**ï¼šæ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œç©å®¶æ•°é‡ä¼šå‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦æ™ºèƒ½ä½“é€‚åº”æ–°çš„ç¯å¢ƒã€‚
* **é€šä¿¡**ï¼šç©å®¶å¯ä»¥é€šè¿‡å¤–äº¤è¡ŒåŠ¨å’Œè‡ªç„¶è¯­è¨€èŠå¤©è¿›è¡Œäº¤æµã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ”¯æŒå¤šç§æ™ºèƒ½ä½“ç±»å‹
CivRealmæä¾›äº†ä¸¤ç§APIæ¥å£ï¼Œåˆ†åˆ«æ”¯æŒåŸºäºå¼ é‡çš„æ™ºèƒ½ä½“å’ŒåŸºäºè¯­è¨€çš„æ™ºèƒ½ä½“ï¼š
* **åŸºäºå¼ é‡çš„æ™ºèƒ½ä½“**ï¼šä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼Œæ“…é•¿å­¦ä¹ å’Œæ¨¡å¼è¯†åˆ«ã€‚
* **åŸºäºè¯­è¨€çš„æ™ºèƒ½ä½“**ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç­‰æ–¹æ³•ï¼Œæ“…é•¿æ¨ç†å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºå‡†æ–¹æ³•å’Œè¯„ä¼°æŒ‡æ ‡
æœ¬æ–‡æå‡ºäº†ä¸‰ç§åŸºå‡†æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š
* **åŸºäºå¼ é‡çš„å¼ºåŒ–å­¦ä¹ **ï¼šä½¿ç”¨AlphaStarçš„æ¶æ„ï¼Œæ“…é•¿å¤„ç†å¤æ‚åŠ¨æ€å’Œæµ·é‡ä¿¡æ¯ã€‚
* **BaseLang**ï¼šåŸºäºAutoGPTçš„æ¶æ„ï¼Œæ“…é•¿æ¨ç†å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚
* **Mastaba**ï¼šåŸºäºBaseLangçš„æ¶æ„ï¼Œå¼•å…¥å±‚æ¬¡ç»“æ„ï¼Œæé«˜å…¨å±€è§†è§’å’Œå†³ç­–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¼ é‡çš„å¼ºåŒ–å­¦ä¹ åœ¨è¿·ä½ æ¸¸æˆä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®Œæ•´æ¸¸æˆä¸­ä»ç„¶å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚çŸ­è§†ç­–ç•¥å’Œéš¾ä»¥å¤„ç†ç¨€ç–å¥–åŠ±ã€‚åŸºäºè¯­è¨€çš„æ™ºèƒ½ä½“åœ¨å®Œæ•´æ¸¸æˆä¸­è¡¨ç°æ›´ä½³ï¼Œä½†ä»ç„¶éœ€è¦æ”¹è¿›æ¨ç†èƒ½åŠ›å’Œå…¨å±€è§†è§’ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
CivRealmä¸ºå†³ç­–æ™ºèƒ½ä½“çš„å­¦ä¹ å’Œæ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„æŒ‘æˆ˜å¹³å°ï¼Œå¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCivRealmè¿˜å¯ä»¥ç”¨äºç ”ç©¶äººç±»ç¤¾ä¼šçš„åŠ¨æ€ã€å†å²äº‹ä»¶çš„ç»“æœå’Œæœªæ¥çš„ç¤¾ä¼šè½¨è¿¹ã€‚

## finding-the-needle-in-a-haystack--detecting-bug-occurrences-in-gameplay-videos
### Abstract
The presence of bugs in video games can bring significant consequences for
developers. To avoid these consequences, developers can leverage gameplay
videos to identify and fix these bugs. Video hosting websites such as YouTube
provide access to millions of game videos, including videos that depict bug
occurrences, but the large amount of content can make finding bug instances
challenging. We present an automated approach that uses machine learning to
predict whether a segment of a gameplay video contains the depiction of a bug.
We analyzed 4,412 segments of 198 gameplay videos to predict whether a segment
contains an instance of a bug. Additionally, we investigated how our approach
performs when applied across different specific genres of video games and on
videos from the same game. We also analyzed the videos in the dataset to
investigate what characteristics of the visual features might explain the
classifier's prediction. Finally, we conducted a user study to examine the
benefits of our automated approach against a manual analysis. Our findings
indicate that our approach is effective at detecting segments of a video that
contain bugs, achieving a high F1 score of 0.88, outperforming the current
state-of-the-art technique for bug classification of gameplay video segments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆè§†é¢‘ä¸­çš„â€œå¯»é’ˆâ€ï¼šè‡ªåŠ¨æ£€æµ‹æ¸¸æˆä¸­çš„bug

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ¸¸æˆè¡Œä¸šæ˜¯ä¸€ä¸ªä»·å€¼æ•°åäº¿ç¾å…ƒçš„äº§ä¸šï¼Œæ¸¸æˆä¸­çš„bugä¼šå¯¹å¼€å‘è€…é€ æˆé‡å¤§å½±å“ã€‚ä¸ºäº†å‡å°‘è¿™äº›å½±å“ï¼Œå¼€å‘è€…å¯ä»¥åˆ©ç”¨æ¸¸æˆè§†é¢‘æ¥è¯†åˆ«å’Œä¿®å¤è¿™äº›bugã€‚è§†é¢‘æ‰˜ç®¡ç½‘ç«™å¦‚YouTubeæä¾›äº†æ•°ç™¾ä¸‡ä¸ªæ¸¸æˆè§†é¢‘ï¼ŒåŒ…æ‹¬æç»˜bugå‡ºç°çš„è§†é¢‘ï¼Œä½†å¤§é‡å†…å®¹ä½¿å¾—æ‰¾åˆ°bugå®ä¾‹å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ æ¥é¢„æµ‹æ¸¸æˆè§†é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«bugçš„æç»˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–çš„æ–¹æ³•ï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ æ¥é¢„æµ‹æ¸¸æˆè§†é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«bugçš„æç»˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†æè§†é¢‘ç‰‡æ®µçš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡è¿˜è¿›è¡Œäº†åˆæ­¥åˆ†æï¼Œè§£é‡Šäº†è§†é¢‘å¸§å›¾åƒçš„å“ªäº›ç‰¹å¾å¯èƒ½ä¸bugçš„å‡ºç°æœ‰å…³ã€‚é€šè¿‡åˆ†æè§†é¢‘å¸§ï¼Œç ”ç©¶äººå‘˜å¯ä»¥äº†è§£å“ªäº›å…ƒç´ æˆ–è¡Œä¸ºå¯èƒ½å¯¼è‡´bugçš„å‡ºç°ï¼Œä»è€Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°è¿›è¡Œæµ‹è¯•å’Œä¿®å¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«4,412ä¸ªæ¸¸æˆè§†é¢‘ç‰‡æ®µçš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬198ä¸ªä¸åŒçš„æ¸¸æˆè§†é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹åŒ…å«bugçš„è§†é¢‘ç‰‡æ®µæ–¹é¢éå¸¸æœ‰æ•ˆï¼ŒF1åˆ†æ•°è¾¾åˆ°äº†0.88ï¼Œè¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„æ¸¸æˆè§†é¢‘ç‰‡æ®µbugåˆ†ç±»æŠ€æœ¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥å¸®åŠ©æ¸¸æˆå¼€å‘è€…æ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œä¿®å¤æ¸¸æˆä¸­çš„bugã€‚é€šè¿‡ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¼€å‘è€…å¯ä»¥è‡ªåŠ¨åˆ†æå¤§é‡çš„æ¸¸æˆè§†é¢‘ï¼Œå¿«é€Ÿæ‰¾åˆ°åŒ…å«bugçš„ç‰‡æ®µï¼Œä»è€ŒèŠ‚çœæ—¶é—´å’Œç²¾åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„åˆ†æç»“æœè¿˜å¯ä»¥å¸®åŠ©å¼€å‘è€…äº†è§£å“ªäº›ç‰¹å¾å¯èƒ½ä¸bugçš„å‡ºç°æœ‰å…³ï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œæµ‹è¯•å’Œä¿®å¤ã€‚

## pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model
### Abstract
Poker, also known as Texas Hold'em, has always been a typical research target
within imperfect information games (IIGs). IIGs have long served as a measure
of artificial intelligence (AI) development. Representative prior works, such
as DeepStack and Libratus heavily rely on counterfactual regret minimization
(CFR) to tackle heads-up no-limit Poker. However, it is challenging for
subsequent researchers to learn CFR from previous models and apply it to other
real-world applications due to the expensive computational cost of CFR
iterations. Additionally, CFR is difficult to apply to multi-player games due
to the exponential growth of the game tree size. In this work, we introduce
PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number
of players and gaining high win rates, established on a lightweight large
language model (LLM). PokerGPT only requires simple textual information of
Poker games for generating decision-making advice, thus guaranteeing the
convenient interaction between AI and humans. We mainly transform a set of
textual records acquired from real games into prompts, and use them to
fine-tune a lightweight pre-trained LLM using reinforcement learning human
feedback technique. To improve fine-tuning performance, we conduct prompt
engineering on raw data, including filtering useful information, selecting
behaviors of players with high win rates, and further processing them into
textual instruction using multiple prompt engineering techniques. Through the
experiments, we demonstrate that PokerGPT outperforms previous approaches in
terms of win rate, model size, training time, and response speed, indicating
the great potential of LLMs in solving IIGs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PokerGPTï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è½»é‡çº§å¤šç©å®¶å¾·å·æ‰‘å…‹è§£å†³æ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¾·å·æ‰‘å…‹ä½œä¸ºä¸€ç§å…¸å‹çš„éå®Œç¾ä¿¡æ¯æ¸¸æˆï¼ˆIIGï¼‰ï¼Œä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶çš„é‡è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚DeepStackå’ŒLibratusï¼Œä¸»è¦ä¾èµ–äºåäº‹å®åæ‚”æœ€å°åŒ–ï¼ˆCFRï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è®¡ç®—æˆæœ¬å’Œæ‰©å±•æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚CFRç®—æ³•çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åº”ç”¨äºå¤šç©å®¶æ¸¸æˆï¼Œä¸”éš¾ä»¥ä»ç°æœ‰æ¨¡å‹ä¸­å­¦ä¹ å¹¶åº”ç”¨äºå…¶ä»–ç°å®ä¸–ç•Œåº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†PokerGPTï¼Œä¸€ç§åŸºäºè½»é‡çº§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«¯åˆ°ç«¯å¾·å·æ‰‘å…‹è§£å†³æ–¹æ¡ˆã€‚PokerGPTé€šè¿‡ä»¥ä¸‹åˆ›æ–°ç‚¹å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶
PokerGPTé‡‡ç”¨ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ï¼Œé¿å…äº†å¤æ‚çš„ç‰¹å¾å·¥ç¨‹å’Œä¸­é—´æ­¥éª¤ã€‚å®ƒä»…éœ€è¦ç®€å•çš„æ–‡æœ¬ä¿¡æ¯å³å¯ç”Ÿæˆå†³ç­–å»ºè®®ï¼Œå®ç°äº†äººæœºäº¤äº’çš„ä¾¿æ·æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè½»é‡çº§LLM
PokerGPTåŸºäºè½»é‡çº§LLMï¼Œå…·æœ‰æ›´å°‘çš„å‚æ•°å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´ä¹Ÿæ›´çŸ­ï¼Œå®ç°äº†èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé«˜æ•ˆçš„æ•°æ®å¤„ç†
PokerGPTé‡‡ç”¨æ•°æ®æ¸…æ´—å’Œæç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå°†çœŸå®æ¸¸æˆæ•°æ®è½¬æ¢ä¸ºå¯ç†è§£çš„æ–‡æœ¬æç¤ºï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆæŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒPokerGPTåœ¨èƒœç‡ã€æ¨¡å‹å¤§å°ã€è®­ç»ƒæ—¶é—´å’Œå“åº”é€Ÿåº¦ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒPokerGPTèƒ½å¤Ÿå¤„ç†ä»»æ„æ•°é‡çš„ç©å®¶ï¼Œå¹¶å±•ç°å‡ºå‡ºè‰²çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PokerGPTçš„æˆåŠŸè¡¨æ˜ï¼ŒLLMåœ¨è§£å†³IIGæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å…¶ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ã€è½»é‡çº§æ¨¡å‹å’Œé«˜æ•ˆçš„æ•°æ®å¤„ç†æŠ€æœ¯ä¸ºå…¶ä»–IIGç ”ç©¶æä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒã€‚æ­¤å¤–ï¼ŒPokerGPTçš„äº¤äº’å¼ç‰¹æ€§ä½¿å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚

## cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game
### Abstract
Multi-agent collaboration with Large Language Models (LLMs) demonstrates
proficiency in basic tasks, yet its efficiency in more complex scenarios
remains unexplored. In gaming environments, these agents often face situations
without established coordination protocols, requiring them to make intelligent
inferences about teammates from limited data. This problem motivates the area
of ad hoc teamwork, in which an agent may potentially cooperate with a variety
of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork
problem where the agent operates in an environment driven by natural language.
Our findings reveal the potential of LLM agents in team collaboration,
highlighting issues related to hallucinations in communication. To address this
issue, we develop CodeAct, a general agent that equips LLM with enhanced memory
and code-driven reasoning, enabling the repurposing of partial information for
rapid adaptation to new teammates.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸æ–­æå‡ï¼Œå®ƒä»¬åœ¨æ„å»ºè‡ªä¸»ä»£ç†å’Œæ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰é¢„å…ˆè®¾å®šçš„åè°ƒåè®®çš„åŠ¨æ€ç¯å¢ƒä¸­ï¼ŒLLMsçš„åä½œæ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ˜ç¡®å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä¸ä¸åŒçš„é˜Ÿå‹è¿›è¡Œæœ‰æ•ˆåˆä½œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥AvalonPlayåŸºå‡†
æœ¬æ–‡æå‡ºäº†AvalonPlayåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€çš„å¤šæ™ºèƒ½ä½“å¹³å°ï¼Œç”¨äºæ¨¡æ‹ŸåŠ¨æ€ç¯å¢ƒä¸­çš„åä½œä»»åŠ¡ã€‚åœ¨è¿™ä¸ªåŸºå‡†ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨æœ‰é™çš„ä¿¡æ¯å’Œæ²¡æœ‰é¢„å…ˆè®¾å®šçš„å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è§‚å¯Ÿé˜Ÿå‹çš„è¡Œä¸ºæ¥æ¨æ–­ä»–ä»¬çš„è§’è‰²ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å›¢é˜Ÿç­–ç•¥ä»¥å®ç°å…±åŒç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘CodeActæ™ºèƒ½ä½“
ä¸ºäº†è§£å†³LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­åä½œæ—¶å¯èƒ½å‡ºç°çš„è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CodeActæ™ºèƒ½ä½“ã€‚CodeActåˆ©ç”¨LLMsçš„ä»£ç é©±åŠ¨æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å°†å¤æ‚çš„è¯­ä¹‰ä»»åŠ¡è½¬åŒ–ä¸ºçµæ´»çš„ä»£ç ç»“æ„ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4æ¨¡å‹åœ¨AvalonPlayåŸºå‡†ä¸­è¡¨ç°å‡ºæœ€ä½³çš„åä½œèƒ½åŠ›ï¼Œè€ŒCodeActæ™ºèƒ½ä½“åœ¨å›¢é˜Ÿé€‰æ‹©å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–è¯­ä¹‰æ¨ç†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œå¼•å…¥è‡ªç„¶è¯­è¨€é€šä¿¡åè®®å¹¶ä¸æ€»æ˜¯èƒ½æ˜¾è‘—æé«˜LLMsçš„åä½œæ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆã€‚ä¸ºäº†æé«˜LLMsçš„åä½œæ•ˆç‡ï¼Œå¯ä»¥å€Ÿé‰´æœ¬æ–‡æå‡ºçš„CodeActæ™ºèƒ½ä½“çš„è®¾è®¡æ€è·¯ï¼Œåˆ©ç”¨ä»£ç é©±åŠ¨æ¨ç†å’Œè®°å¿†æ£€ç´¢ç³»ç»Ÿæ¥å¢å¼ºæ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›å’Œä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å‡å°‘å¹»è§‰ç”Ÿæˆçš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination
### Abstract
AI agents powered by Large Language Models (LLMs) have made significant
advances, enabling them to assist humans in diverse complex tasks and leading
to a revolution in human-AI coordination. LLM-powered agents typically require
invoking LLM APIs and employing artificially designed complex prompts, which
results in high inference latency. While this paradigm works well in scenarios
with minimal interactive demands, such as code generation, it is unsuitable for
highly interactive and real-time applications, such as gaming. Traditional
gaming AI often employs small models or reactive policies, enabling fast
inference but offering limited task completion and interaction abilities. In
this work, we consider Overcooked as our testbed where players could
communicate with natural language and cooperate to serve orders. We propose a
Hierarchical Language Agent (HLA) for human-AI coordination that provides both
strong reasoning abilities while keeping real-time execution. In particular,
HLA adopts a hierarchical framework and comprises three modules: a proficient
LLM, referred to as Slow Mind, for intention reasoning and language
interaction, a lightweight LLM, referred to as Fast Mind, for generating macro
actions, and a reactive policy, referred to as Executor, for transforming macro
actions into atomic actions. Human studies show that HLA outperforms other
baseline agents, including slow-mind-only agents and fast-mind-only agents,
with stronger cooperation abilities, faster responses, and more consistent
language communications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†å±‚è¯­è¨€ä»£ç†ï¼šå®æ—¶äººæœºåä½œçš„çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼ŒåŸºäºLLMsçš„AIä»£ç†åœ¨è¾…åŠ©äººç±»å®Œæˆå¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¨åŠ¨äº†äººæœºåä½œçš„é©å‘½ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†é€šå¸¸éœ€è¦è°ƒç”¨LLMs APIå¹¶ä½¿ç”¨äººå·¥è®¾è®¡çš„å¤æ‚æç¤ºï¼Œå¯¼è‡´æ¨ç†å»¶è¿Ÿé«˜ã€‚è¿™ç§èŒƒå¼åœ¨äº¤äº’éœ€æ±‚è¾ƒä½çš„åœºæ™¯ï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦å®æ—¶å“åº”å’Œé«˜é¢‘äº¤äº’çš„åº”ç”¨ï¼ˆå¦‚æ¸¸æˆï¼‰ä¸­å¹¶ä¸é€‚ç”¨ã€‚ä¼ ç»Ÿçš„æ¸¸æˆAIé€šå¸¸é‡‡ç”¨å°å‹æ¨¡å‹æˆ–ååº”ç­–ç•¥ï¼Œè™½ç„¶èƒ½å¤Ÿå®ç°å¿«é€Ÿæ¨ç†ï¼Œä½†ä»»åŠ¡å®Œæˆå’Œäº¤äº’èƒ½åŠ›æœ‰é™ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ†å±‚è¯­è¨€ä»£ç†ï¼ˆHLAï¼‰ï¼Œç”¨äºå®æ—¶äººæœºåä½œï¼Œè¯¥ä»£ç†ç»“åˆäº†å¤§å‹æ¨¡å‹çš„å¼ºå¤§æ¨ç†å’Œäº¤äº’èƒ½åŠ›ä»¥åŠå°å‹æ¨¡å‹å’Œååº”ç­–ç•¥çš„å®æ—¶æ¨ç†èƒ½åŠ›ã€‚HLAé‡‡ç”¨åˆ†å±‚æ¡†æ¶ï¼Œç”±ä¸‰ä¸ªæ¨¡å—ç»„æˆï¼š

* **æ…¢æ€ç»´ï¼ˆSlow Mindï¼‰**ï¼šä¸€ä¸ªç†Ÿç»ƒçš„LLMï¼Œç”¨äºæ„å›¾æ¨ç†å’Œè¯­è¨€äº¤äº’ã€‚
* **å¿«æ€ç»´ï¼ˆFast Mindï¼‰**ï¼šä¸€ä¸ªè½»é‡çº§çš„LLMï¼Œç”¨äºç”Ÿæˆå®æ“ä½œã€‚
* **æ‰§è¡Œå™¨ï¼ˆExecutorï¼‰**ï¼šä¸€ä¸ªååº”ç­–ç•¥ï¼Œç”¨äºå°†å®æ“ä½œè½¬æ¢ä¸ºåŸå­æ“ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Overcookedæ¸¸æˆå¹³å°ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHLAåœ¨æ¸¸æˆå¾—åˆ†ã€å“åº”å»¶è¿Ÿå’Œäººç±»åå¥½æ–¹é¢å‡ä¼˜äºå…¶ä»–åŸºçº¿ä»£ç†ï¼ŒåŒ…æ‹¬ä»…ä½¿ç”¨æ…¢æ€ç»´æˆ–å¿«æ€ç»´çš„ä»£ç†ã€‚HLAå±•ç°å‡ºæ›´å¼ºçš„åä½œèƒ½åŠ›ã€æ›´å¿«çš„å“åº”é€Ÿåº¦å’Œæ›´ä¸€è‡´çš„è¯­è¨€é€šä¿¡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **åˆ†å±‚è®¾è®¡**ï¼šHLAçš„åˆ†å±‚è®¾è®¡æœ‰æ•ˆåœ°è§£å†³äº†LLMsæ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œä½¿å…¶é€‚ç”¨äºå®æ—¶äººæœºåä½œåœºæ™¯ã€‚
* **è½»é‡çº§LLM**ï¼šä½¿ç”¨è½»é‡çº§LLMè¿›è¡Œå®æ“ä½œç”Ÿæˆï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶é¿å…äº†ç”Ÿæˆæ¬¡ä¼˜æ“ä½œã€‚
* **ååº”ç­–ç•¥**ï¼šæ‰§è¡Œå™¨æ¨¡å—ç¡®ä¿äº†åŠ¨ä½œçš„å¯è¡Œæ€§å’Œé«˜é¢‘äº¤äº’ï¼Œæé«˜äº†AIä»£ç†çš„å®æ—¶å“åº”èƒ½åŠ›ã€‚

### ğŸŒŸ æ€»ç»“
HLAä¸ºå®æ—¶äººæœºåä½œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶åˆ†å±‚è®¾è®¡æœ‰æ•ˆåœ°è§£å†³äº†LLMsæ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨éœ€è¦å®æ—¶å“åº”å’Œé«˜é¢‘äº¤äº’çš„åœºæ™¯ä¸­å‘æŒ¥ä½œç”¨ã€‚HLAåœ¨äººæœºåä½œé¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œæœ‰æœ›æ¨åŠ¨äººæœºåä½œçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach
### Abstract
StarCraft II is a challenging benchmark for AI agents due to the necessity of
both precise micro level operations and strategic macro awareness. Previous
works, such as Alphastar and SCC, achieve impressive performance on tackling
StarCraft II , however, still exhibit deficiencies in long term strategic
planning and strategy interpretability. Emerging large language model (LLM)
agents, such as Voyage and MetaGPT, presents the immense potential in solving
intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs
on StarCraft II, a highly complex RTS game.To conveniently take full advantage
of LLMs` reasoning abilities, we first develop textual StratCraft II
environment, called TextStarCraft II, which LLM agent can interact. Secondly,
we propose a Chain of Summarization method, including single frame
summarization for processing raw observations and multi frame summarization for
analyzing game information, providing command recommendations, and generating
strategic decisions. Our experiment consists of two parts: first, an evaluation
by human experts, which includes assessing the LLMs`s mastery of StarCraft II
knowledge and the performance of LLM agents in the game; second, the in game
performance of LLM agents, encompassing aspects like win rate and the impact of
Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the
relevant knowledge and complex planning abilities needed to address StarCraft
II scenarios; 2. Human experts consider the performance of LLM agents to be
close to that of an average player who has played StarCraft II for eight years;
3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)
difficulty level. We have open sourced the code and released demo videos of LLM
agent playing StarCraft II.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­çš„è¡¨ç°ï¼šåŸºå‡†æµ‹è¯•ä¸æ‘˜è¦é“¾æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ˜Ÿé™…äº‰éœ¸IIï¼ˆStarCraft IIï¼‰æ˜¯ä¸€æ¬¾æå…·æŒ‘æˆ˜æ€§çš„å®æ—¶æˆ˜ç•¥æ¸¸æˆï¼Œè¦æ±‚ç©å®¶åœ¨å¾®è§‚æ“ä½œå’Œå®è§‚æˆ˜ç•¥è§„åˆ’ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å°½ç®¡ä¹‹å‰çš„AIç ”ç©¶ï¼Œå¦‚AlphaStarå’ŒSCCï¼Œåœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœï¼Œä½†å®ƒä»¬åœ¨é•¿æœŸæˆ˜ç•¥è§„åˆ’å’Œç­–ç•¥å¯è§£é‡Šæ€§æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œæœ¬æ–‡æ—¨åœ¨éªŒè¯LLMåœ¨æ˜Ÿé™…äº‰éœ¸IIä¸­çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šTextStarCraft IIç¯å¢ƒ
ä¸ºäº†å……åˆ†åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªåä¸ºTextStarCraft IIçš„æ–‡æœ¬ç¯å¢ƒï¼ŒLLMä»£ç†å¯ä»¥ä¸ä¹‹äº¤äº’ã€‚è¯¥ç¯å¢ƒå°†æ˜Ÿé™…äº‰éœ¸IIçš„å¤æ‚æ¸¸æˆåŠ¨æ€è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œå…è®¸LLMä»£ç†é€šè¿‡è¯­è¨€å‘½ä»¤æ‰§è¡Œå®è§‚æˆ˜ç•¥è¡ŒåŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ‘˜è¦é“¾ï¼ˆCoSï¼‰æ–¹æ³•
æœ¬æ–‡æå‡ºäº†æ‘˜è¦é“¾ï¼ˆCoSï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å•å¸§æ‘˜è¦å’Œå¤šå¸§æ‘˜è¦ã€‚å•å¸§æ‘˜è¦ç”¨äºå¤„ç†åŸå§‹è§‚å¯Ÿæ•°æ®ï¼Œè€Œå¤šå¸§æ‘˜è¦ç”¨äºåˆ†ææ¸¸æˆä¿¡æ¯ï¼Œæä¾›å‘½ä»¤å»ºè®®å¹¶ç”Ÿæˆæˆ˜ç•¥å†³ç­–ã€‚CoSæ–¹æ³•é€šè¿‡ä¿¡æ¯å‹ç¼©ã€æ¨ç†åŠ é€Ÿå’Œå…¨å±€ç†è§£ï¼Œå¢å¼ºäº†LLMä»£ç†åœ¨å¤„ç†å¤æ‚ä¿¡æ¯å’Œåšå‡ºæˆ˜ç•¥å†³ç­–æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMå…·å¤‡è§£å†³æ˜Ÿé™…äº‰éœ¸IIåœºæ™¯æ‰€éœ€çš„ç›¸å…³çŸ¥è¯†å’Œå¤æ‚è§„åˆ’èƒ½åŠ›ã€‚äººç±»ä¸“å®¶è®¤ä¸ºï¼ŒLLMä»£ç†åœ¨æ¸¸æˆä¸­çš„è¡¨ç°æ¥è¿‘äºç©äº†å…«å¹´æ˜Ÿé™…äº‰éœ¸IIçš„å¹³å‡ç©å®¶ã€‚æ­¤å¤–ï¼ŒLLMä»£ç†èƒ½å¤Ÿåœ¨Harderï¼ˆLv5ï¼‰éš¾åº¦çº§åˆ«ä¸‹å‡»è´¥å†…ç½®AIã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„TextStarCraft IIç¯å¢ƒå’ŒCoSæ–¹æ³•ä¸ºè¯„ä¼°LLMåœ¨å®æ—¶æˆ˜ç•¥å†³ç­–å’Œé•¿æœŸè§„åˆ’æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥åœ¨æ˜Ÿé™…äº‰éœ¸IIå’Œå…¶ä»–å®æ—¶æˆ˜ç•¥æ¸¸æˆä¸­çš„AIç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft
### Abstract
Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Auto MC-Rewardï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°ï¼Œæå‡Minecraftä¸­å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Minecraft ç­‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒé€šå¸¸åªæä¾›ç¨€ç–å¥–åŠ±ï¼Œå³åªæœ‰ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥æ—¶æ‰ä¼šè·å¾—å¥–åŠ±ã€‚è¿™ç§å¥–åŠ±æœºåˆ¶ä½¿å¾—å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨æ¢ç´¢æ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å­¦ä¹ å¤æ‚ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Auto MC-Rewardï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„å…ˆè¿›å­¦ä¹ ç³»ç»Ÿï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAuto MC-Reward ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ã€‚å¥–åŠ±è®¾è®¡å™¨æ ¹æ®ç¯å¢ƒä¿¡æ¯å’Œä»»åŠ¡æè¿°ï¼Œé€šè¿‡ç¼–å†™å¯æ‰§è¡Œçš„ Python å‡½æ•°æ¥è®¾è®¡å¥–åŠ±å‡½æ•°ã€‚å¥–åŠ±è¯„è®ºå®¶è´Ÿè´£éªŒè¯ä»£ç ï¼Œæ£€æŸ¥ä»£ç æ˜¯å¦è‡ªæ´½ä¸”æ²¡æœ‰è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯ã€‚è½¨è¿¹åˆ†æå™¨æ ¹æ®æ”¶é›†çš„è½¨è¿¹æ€»ç»“å¯èƒ½çš„å¤±è´¥åŸå› ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAuto MC-Reward åˆ©ç”¨ LLM çš„ä»»åŠ¡ç†è§£å’Œç»éªŒæ€»ç»“èƒ½åŠ›ï¼Œä¸ºå­¦ä¹ æä¾›è¯¦ç»†å’Œå³æ—¶çš„å¥–åŠ±æŒ‡å¯¼ã€‚å¥–åŠ±è®¾è®¡å™¨é¦–å…ˆæ ¹æ®ç¯å¢ƒå’Œä»»åŠ¡çš„åŸºæœ¬æè¿°ï¼Œä½¿ç”¨ LLM è®¾è®¡ä¸ä»»åŠ¡ç›¸å…³çš„å¯†é›†å¥–åŠ±å‡½æ•°ã€‚ç„¶åï¼Œå¥–åŠ±è¯„è®ºå®¶å¯¹è®¾è®¡çš„å¥–åŠ±å‡½æ•°è¿›è¡Œè‡ªæˆ‘éªŒè¯ã€‚ä¸ºäº†è§£å†³ LLM ç†è§£çš„æ½œåœ¨åå·®æˆ–ç–å¿½ï¼Œè¿˜æå‡ºäº†åŸºäº LLM çš„è½¨è¿¹åˆ†æå™¨ï¼Œç”¨äºåˆ†æå’Œæ€»ç»“è®­ç»ƒä»£ç†çš„è½¨è¿¹ï¼Œå¹¶å¸®åŠ©å¥–åŠ±è®¾è®¡å™¨æ”¹è¿›å¥–åŠ±å‡½æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Auto MC-Reward åœ¨ä¸€ç³»åˆ—ä»£è¡¨æ€§åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬åœ°ä¸‹æ°´å¹³æ¢ç´¢é’»çŸ³å’Œæ¢ç´¢å¹³åŸç”Ÿç‰©ç¾¤è½ä¸­çš„æ ‘æœ¨å’ŒåŠ¨ç‰©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹ç¨€ç–å¥–åŠ±å’Œç°æœ‰å¯†é›†å¥–åŠ±æ–¹æ³•ç›¸æ¯”ï¼ŒAuto MC-Reward åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ›´å¥½çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸Šé«˜æ•ˆå­¦ä¹ çš„å…ˆè¿›èƒ½åŠ›ã€‚é€šè¿‡è¿­ä»£æ”¹è¿›å¥–åŠ±å‡½æ•°çš„è®¾è®¡ï¼ŒAuto MC-Reward ä½¿ä»£ç†èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å¯¹æ–°ä»»åŠ¡æœ‰ç›Šçš„æ–°è¡Œä¸ºï¼Œä¾‹å¦‚é¿å…ç†”å²©ï¼Œä»è€Œå¤§å¤§æé«˜äº†æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward ä»…ä½¿ç”¨åŸå§‹ä¿¡æ¯å°±å®ç°äº†é«˜é’»çŸ³è·å–æˆåŠŸç‡ï¼ˆ36.5%ï¼‰ï¼Œè¯æ˜äº†å…¶è§£å†³é•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Auto MC-Reward ä¸ºè§£å†³ç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸­çš„æ¢ç´¢æ•ˆç‡é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚å…¶åˆ©ç”¨ LLM è‡ªåŠ¨è®¾è®¡å¯†é›†å¥–åŠ±å‡½æ•°çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜å¼ºåŒ–å­¦ä¹ ä»£ç†çš„å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒAuto MC-Reward çš„ä¸‰ä¸ªç»„ä»¶ï¼ˆå¥–åŠ±è®¾è®¡å™¨ã€å¥–åŠ±è¯„è®ºå®¶å’Œè½¨è¿¹åˆ†æå™¨ï¼‰å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä½¿å¾—æ•°æ®åˆ†æå’Œå¥–åŠ±å‡½æ•°æ›´æ–°æ›´åŠ çµæ´»ã€‚

## mp5--a-multi-modal-open-ended-embodied-system-in-minecraft-via-active-perception
### Abstract
It is a long-lasting goal to design an embodied system that can solve
long-horizon open-world tasks in human-like ways. However, existing approaches
usually struggle with compound difficulties caused by the logic-aware
decomposition and context-aware execution of these tasks. To this end, we
introduce MP5, an open-ended multimodal embodied system built upon the
challenging Minecraft simulator, which can decompose feasible sub-objectives,
design sophisticated situation-aware plans, and perform embodied action
control, with frequent communication with a goal-conditioned active perception
scheme. Specifically, MP5 is developed on top of recent advances in Multimodal
Large Language Models (MLLMs), and the system is modulated into functional
modules that can be scheduled and collaborated to ultimately solve pre-defined
context- and process-dependent tasks. Extensive experiments prove that MP5 can
achieve a 22% success rate on difficult process-dependent tasks and a 91%
success rate on tasks that heavily depend on the context. Moreover, MP5
exhibits a remarkable ability to address many open-ended tasks that are
entirely novel.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MP5ï¼šåŸºäºä¸»åŠ¨æ„ŸçŸ¥çš„å¤šæ¨¡æ€å¼€æ”¾å¼å…·èº«ç³»ç»Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿä»¥äººç±»æ–¹å¼è§£å†³å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡çš„å…·èº«ç³»ç»Ÿä¸€ç›´æ˜¯é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸éš¾ä»¥åº”å¯¹è¿™äº›ä»»åŠ¡ä¸­é€»è¾‘æ„ŸçŸ¥åˆ†è§£å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‰§è¡Œæ‰€å¸¦æ¥çš„å¤åˆå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MP5ï¼Œä¸€ä¸ªåŸºäºMinecraftæ¨¡æ‹Ÿå™¨çš„å¼€æ”¾å¼å¤šæ¨¡æ€å…·èº«ç³»ç»Ÿï¼Œå®ƒèƒ½å¤Ÿåˆ†è§£å¯è¡Œçš„å­ç›®æ ‡ï¼Œè®¾è®¡å¤æ‚çš„æƒ…å¢ƒæ„ŸçŸ¥è®¡åˆ’ï¼Œå¹¶æ‰§è¡Œå…·èº«åŠ¨ä½œæ§åˆ¶ï¼ŒåŒæ—¶ä¸ç›®æ ‡æ¡ä»¶ä¸‹çš„ä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆè¿›è¡Œé¢‘ç¹é€šä¿¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMP5åŸºäºæœ€æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºï¼Œå¹¶å°†ç³»ç»Ÿåˆ†è§£ä¸ºå¯è°ƒåº¦å’Œåä½œçš„åŠŸèƒ½æ¨¡å—ï¼Œä»¥è§£å†³é¢„å®šä¹‰çš„ä¸Šä¸‹æ–‡å’Œè¿‡ç¨‹ä¾èµ–ä»»åŠ¡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMP5åŒ…æ‹¬ä¸€ä¸ªä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆï¼Œé€šè¿‡æ„ŸçŸ¥å™¨ä¸å·¡é€»å™¨ä¹‹é—´çš„å¤šè½®äº¤äº’ï¼Œä¸»åŠ¨æ„ŸçŸ¥è§‚å¯Ÿåˆ°çš„å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥è§£å†³ä¸Šä¸‹æ–‡ä¾èµ–ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
MP5åœ¨å›°éš¾çš„è¿‡ç¨‹ä¾èµ–ä»»åŠ¡ä¸Šå®ç°äº†22%çš„æˆåŠŸç‡ï¼Œåœ¨é«˜åº¦ä¾èµ–ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ä¸Šå®ç°äº†91%çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒMP5è¡¨ç°å‡ºè§£å†³è®¸å¤šå®Œå…¨æ–°é¢–çš„å¼€æ”¾å¼ä»»åŠ¡çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MP5çš„è®¾è®¡å’Œå®ç°ä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œé•¿æ—¶ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶ä¸»åŠ¨æ„ŸçŸ¥æ–¹æ¡ˆå’Œæ¨¡å—åŒ–è®¾è®¡å¯¹äºå¼€å‘æ›´æ™ºèƒ½ã€æ›´çµæ´»çš„å…·èº«ç³»ç»Ÿå…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## stay-focused--problem-drift-in-multi-agent-debate
### Abstract
Multi-agent debate - multiple instances of large language models discussing
problems in turn-based interaction - has shown promise for solving knowledge
and reasoning tasks. However, these methods show limitations, particularly when
scaling them to longer reasoning chains. In this study, we unveil a new issue
of multi-agent debate: discussions drift away from the initial problem over
multiple turns. We define this phenomenon as problem drift and quantify its
presence across ten tasks (i.e., three generative, three knowledge, three
reasoning, and one instruction-following task). To identify the reasons for
this issue, we perform a human study with eight experts on discussions
suffering from problem drift, who find the most common issues are a lack of
progress (35% of cases), low-quality feedback (26% of cases), and a lack of
clarity (25% of cases). To systematically address the issue of problem drift,
we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem
drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of
problem drift cases. Our study can be seen as a first step to understanding a
key limitation of multi-agent debate, highlighting pathways for improving their
effectiveness in the future.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„é—®é¢˜æ¼‚ç§»ï¼šè¯†åˆ«ä¸ç¼“è§£

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤šæ™ºèƒ½ä½“è¾©è®ºä½œä¸ºä¸€ç§æ–°å…´çš„AIæŠ€æœ¯ï¼Œé€šè¿‡å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè½®æ¢å¼äº¤äº’ï¼Œå±•ç°å‡ºåœ¨è§£å†³çŸ¥è¯†å’Œæ¨ç†ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œéšç€è¾©è®ºé“¾çš„å»¶é•¿ï¼Œè¿™äº›æ–¹æ³•åœ¨æ‰©å±•æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„ä¸€ä¸ªæ–°é—®é¢˜ï¼šåœ¨å¤šè½®äº¤äº’ä¸­ï¼Œè®¨è®ºé€æ¸åç¦»åˆå§‹é—®é¢˜ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œé—®é¢˜æ¼‚ç§»â€ï¼Œæœ¬æ–‡æ—¨åœ¨é‡åŒ–é—®é¢˜æ¼‚ç§»çš„å­˜åœ¨ï¼Œå¹¶æ¢ç´¢å…¶èƒŒåçš„åŸå› ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé—®é¢˜æ¼‚ç§»çš„é‡åŒ–
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºFOCUSçš„æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å¤šè½®äº¤äº’ä¸­ä»»åŠ¡æ€§èƒ½çš„è¡°å‡ç¨‹åº¦ã€‚é€šè¿‡FOCUSæŒ‡æ ‡ï¼Œå¯ä»¥è¯†åˆ«å‡ºå“ªäº›è®¨è®ºå‡ºç°äº†é—®é¢˜æ¼‚ç§»ï¼Œå¹¶é‡åŒ–å…¶å½±å“ç¨‹åº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé—®é¢˜æ¼‚ç§»çš„æ£€æµ‹ä¸ç¼“è§£
ä¸ºäº†åœ¨æµ‹è¯•æ—¶æ£€æµ‹é—®é¢˜æ¼‚ç§»ï¼Œæœ¬æ–‡æå‡ºäº†DRIFTJudgeæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºLLM-as-a-judgeï¼Œé€šè¿‡è¯„ä¼°è¿ç»­è½®æ¬¡çš„è§£å†³æ–¹æ¡ˆæ¥åˆ¤æ–­æ˜¯å¦å­˜åœ¨é—®é¢˜æ¼‚ç§»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†DRIFTPolicyæ–¹æ³•ï¼Œé€šè¿‡å‘å‚ä¸è¾©è®ºçš„æ™ºèƒ½ä½“æä¾›é’ˆå¯¹æ€§çš„åé¦ˆï¼Œä»¥å‡å°‘é—®é¢˜æ¼‚ç§»çš„å‘ç”Ÿã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨åä¸ªä»»åŠ¡ï¼ˆåŒ…æ‹¬ç”Ÿæˆã€çŸ¥è¯†ã€æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå‘ç°é—®é¢˜æ¼‚ç§»æ™®éå­˜åœ¨ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ã€‚é€šè¿‡äººç±»ä¸“å®¶çš„ç ”ç©¶ï¼Œæœ¬æ–‡ç¡®å®šäº†å¯¼è‡´é—®é¢˜æ¼‚ç§»çš„å…«ä¸ªä¸»è¦åŸå› ï¼ŒåŒ…æ‹¬ç¼ºä¹è¿›å±•ã€ä½è´¨é‡åé¦ˆã€ç¼ºä¹æ¸…æ™°åº¦ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIFTPolicyæ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡å°‘31%çš„é—®é¢˜æ¼‚ç§»æ¡ˆä¾‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æ­ç¤ºäº†å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„ä¸€ä¸ªå…³é”®å±€é™æ€§ï¼Œå¹¶ä¸ºæé«˜å…¶æœ‰æ•ˆæ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ™ºèƒ½ä½“æ¢ç´¢æ–°æƒ³æ³•çš„ä½œç”¨ï¼Œå¹¶æ¯”è¾ƒäººç±»å’Œæ™ºèƒ½ä½“è¾©è®ºä¹‹é—´çš„å†…åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„DRIFTJudgeå’ŒDRIFTPolicyæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæä¾›å‚è€ƒï¼Œä»¥æ£€æµ‹å’Œç¼“è§£é—®é¢˜æ¼‚ç§»ç°è±¡ã€‚

## glitchbench--can-large-multimodal-models-detect-video-game-glitches-
### Abstract
Large multimodal models (LMMs) have evolved from large language models (LLMs)
to integrate multiple input modalities, such as visual inputs. This integration
augments the capacity of LLMs for tasks requiring visual comprehension and
reasoning. However, the extent and limitations of their enhanced abilities are
not fully understood, especially when it comes to real-world tasks. To address
this gap, we introduce GlitchBench, a novel benchmark derived from video game
quality assurance tasks, to test and evaluate the reasoning capabilities of
LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios
from video games and aims to challenge both the visual and linguistic reasoning
powers of LMMs in detecting and interpreting out-of-the-ordinary events. We
evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents
a new challenge for these models. Code and data are available at:
https://glitchbench.github.io/
### ğŸŒŸ è®ºæ–‡è§£è¯» | GlitchBenchï¼šå¤§å‹å¤šæ¨¡æ€æ¨¡å‹èƒ½å¦æ£€æµ‹è§†é¢‘æ¸¸æˆä¸­çš„é”™è¯¯ï¼Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„ä¸æ–­å‘å±•ï¼Œå®ƒä»¬åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å’Œå±€é™æ€§å°šä¸æ˜ç¡®ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†GlitchBenchï¼Œä¸€ä¸ªåŸºäºè§†é¢‘æ¸¸æˆè´¨é‡ä¿è¯ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LMMsåœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šGlitchBenchæ•°æ®é›†
GlitchBenchæ•°æ®é›†ç”±593ä¸ªæ¸¸æˆä¸­çš„å¼‚å¸¸å’Œé”™è¯¯åœºæ™¯ç»„æˆï¼Œæ¶µç›–äº†205æ¬¾ä¸åŒç±»å‹çš„æ¸¸æˆã€‚æ¯ä¸ªåœºæ™¯éƒ½åŒ…å«ä¸€ä¸ªè§†é¢‘ç‰‡æ®µã€ä¸€ä¸ªä»£è¡¨æ€§å¸§ã€ä¸€ä¸ªç®€çŸ­çš„æè¿°ä»¥åŠä¸€ä¸ªæŒ‡å‘Redditä¸Šç›¸å…³è®¨è®ºçš„é“¾æ¥ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†è¿˜åŒ…æ‹¬330ä¸ªæ— é”™è¯¯çš„å›¾åƒä½œä¸ºå¯¹æ¯”ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°æ–¹æ³•
æœ¬æ–‡è¯„ä¼°äº†11ä¸ªæœ€å…ˆè¿›çš„LMMsï¼ŒåŒ…æ‹¬GPT-4Vå’ŒLLaVAï¼Œåœ¨GlitchBenchä¸Šçš„è¡¨ç°ã€‚è¯„ä¼°æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé—®é¢˜ï¼š
1. è¿™å¼ å›¾ç‰‡æœ‰ä»€ä¹ˆä¸å¯»å¸¸çš„åœ°æ–¹ï¼Ÿ
2. è¿™å¼ å›¾ç‰‡æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
3. è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ã€‚
é€šè¿‡æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¸çœŸå®æ ‡ç­¾ï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLMMsåœ¨æ£€æµ‹è¿åç®€å•ç‰©ç†å®šå¾‹çš„é”™è¯¯ï¼ˆå¦‚æ±½è½¦åœ¨ç©ºä¸­é£è¡Œï¼‰æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨æ£€æµ‹æ›´å¾®å¦™çš„é”™è¯¯ï¼ˆå¦‚äººä½“éƒ¨ä½å¤„äºä¸å¯èƒ½çš„å§¿åŠ¿ï¼‰æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚GPT-4Våœ¨GlitchBenchä¸Šè¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡è¾¾åˆ°43.4%ã€‚ç„¶è€Œï¼Œä¸æ— é”™è¯¯å›¾åƒç›¸æ¯”ï¼Œæ¨¡å‹åœ¨æ£€æµ‹é”™è¯¯å›¾åƒæ–¹é¢çš„å‡†ç¡®ç‡æ˜æ˜¾è¾ƒä½ï¼Œè¿™è¡¨æ˜é”™è¯¯å›¾åƒæ›´å…·æŒ‘æˆ˜æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GlitchBenchåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°LMMsåœ¨å®é™…åº”ç”¨ä¸­çš„æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLMMsåœ¨æ£€æµ‹å’Œè§£é‡Šå¼‚å¸¸äº‹ä»¶æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚

## a-framework-for-complementary-companion-character-behavior-in-video-games
### Abstract
We propose a game development framework capable of governing the behavior of
complementary companions in a video game. A "complementary" action is
contrasted with a mimicking action and is defined as any action by a friendly
non-player character that furthers the player's strategy. This is determined
through a combination of both player action and game state prediction processes
while allowing the AI companion to experiment. We determine the location of
interest for companion actions based on a dynamic set of regions customized to
the individual player. A user study shows promising results; a majority of
participants familiar with game design react positively to the companion
behavior, stating that they would consider using the frame-work in future games
themselves.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¸¸æˆAIæ–°æ¡†æ¶ï¼šäº’è¡¥å‹ä¼™ä¼´è¡Œä¸ºåŠ©åŠ›ç©å®¶ç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ¸¸æˆå¤æ‚æ€§çš„æå‡ï¼Œç©å®¶å¯¹æ¸¸æˆä¸­çš„éç©å®¶è§’è‰²ï¼ˆNPCï¼‰çš„æ™ºèƒ½å’Œäº’åŠ¨æ€§è¦æ±‚è¶Šæ¥è¶Šé«˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆAIå¾€å¾€åªèƒ½æ¨¡ä»¿ç©å®¶çš„è¡Œä¸ºï¼Œç¼ºä¹ä¸»åŠ¨æ€§å’Œåˆ›é€ æ€§ï¼Œæ— æ³•çœŸæ­£è¾…åŠ©ç©å®¶è¾¾æˆç›®æ ‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¸¸æˆå¼€å‘æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°äº’è¡¥å‹ä¼™ä¼´è¡Œä¸ºï¼Œå³NPCèƒ½å¤Ÿæ ¹æ®ç©å®¶çš„ç­–ç•¥å’Œæ¸¸æˆçŠ¶æ€ï¼Œä¸»åŠ¨é‡‡å–è¡ŒåŠ¨ï¼Œå¸®åŠ©ç©å®¶æ›´è½»æ¾åœ°è¾¾æˆç›®æ ‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº’è¡¥å‹è¡Œä¸ºå®šä¹‰
æœ¬æ–‡é¦–å…ˆå¯¹â€œäº’è¡¥å‹è¡Œä¸ºâ€è¿›è¡Œäº†æ˜ç¡®å®šä¹‰ï¼Œå³NPCçš„è¡Œä¸ºèƒ½å¤Ÿä¿ƒè¿›ç©å®¶çš„ç­–ç•¥ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿ç©å®¶ã€‚è¿™è¦æ±‚NPCèƒ½å¤Ÿé¢„æµ‹ç©å®¶çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå¹¶é‡‡å–ç›¸åº”çš„è¡ŒåŠ¨æ¥è¾…åŠ©ç©å®¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€å†³ç­–è¿‡ç¨‹
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š
1. é¢„æµ‹ç©å®¶çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨
2. æ£€æŸ¥é¢„æµ‹è¡ŒåŠ¨æ˜¯å¦å¯è¡Œï¼Œè‹¥ä¸å¯è¡Œåˆ™å¯»æ‰¾æ›¿ä»£è¡ŒåŠ¨
3. åˆ¤æ–­æ˜¯å¦èƒ½å¤Ÿè¾…åŠ©ç©å®¶å½“å‰è¡ŒåŠ¨
4. è‹¥æ— æ³•è¾…åŠ©ï¼Œåˆ™éšæœºé€‰æ‹©æ‰§è¡Œé¢„æµ‹è¡ŒåŠ¨æˆ–å¯»æ‰¾æœ€ä½³æ¸¸æˆçŠ¶æ€çš„è¡ŒåŠ¨
5. è‹¥æ‰€æœ‰è¡ŒåŠ¨å‡ä¸å¯è¡Œï¼Œåˆ™å°è¯•æ‰§è¡Œç©å®¶æœªå°è¯•è¿‡çš„è¡ŒåŠ¨æˆ–é‡‡å–é»˜è®¤è¡Œä¸º
6. éšæœºé€‰æ‹©æ‰§è¡Œæœªå°è¯•è¿‡çš„è¡ŒåŠ¨ï¼Œä»¥é¼“åŠ±NPCè¿›è¡Œæ¢ç´¢

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŠ¨æ€åŒºåŸŸç³»ç»Ÿ
ä¸ºäº†æ›´ç²¾ç¡®åœ°ç¡®å®šNPCçš„è¡ŒåŠ¨ä½ç½®ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€åŒºåŸŸç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ ¹æ®ç©å®¶çš„è¡ŒåŠ¨åŠ¨æ€åˆ’åˆ†åœ°å›¾åŒºåŸŸï¼Œä½¿å¾—NPCçš„è¡ŒåŠ¨æ›´åŠ çµæ´»å’Œé’ˆå¯¹æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šç©å®¶è¡ŒåŠ¨é¢„æµ‹
æœ¬æ–‡ä½¿ç”¨åˆ†ç±»å™¨å¯¹ç©å®¶çš„å†å²æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥é¢„æµ‹ç©å®¶çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚åˆ†ç±»å™¨çš„ç±»å‹å’Œè®­ç»ƒæ–¹å¼å¯ä»¥æ ¹æ®æ¸¸æˆå¼€å‘è€…è¿›è¡Œè°ƒæ•´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹5ï¼šæ¸¸æˆçŠ¶æ€é¢„æµ‹
æœ¬æ–‡é€šè¿‡æ¨¡æ‹ŸNPCæ‰§è¡Œæ¯ä¸ªå¯èƒ½çš„è¡ŒåŠ¨ï¼Œå¹¶è¯„ä¼°å…¶ç»“æœï¼Œæ¥é¢„æµ‹æ¸¸æˆçŠ¶æ€çš„å˜åŒ–ã€‚è¿™æœ‰åŠ©äºNPCé€‰æ‹©èƒ½å¤Ÿå¸¦æ¥æœ€ä½³ç»“æœçš„è¡ŒåŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°å‚ä¸è€…å¯¹NPCçš„äº’è¡¥å‹è¡Œä¸ºè¡¨ç¤ºæ»¡æ„ï¼Œå¹¶è®¤ä¸ºè¯¥æ¡†æ¶å¯¹æ¸¸æˆå¼€å‘è€…å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºæ¸¸æˆå¼€å‘è€…æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¯ä»¥å¸®åŠ©ä»–ä»¬åˆ›å»ºæ›´å…·æ™ºèƒ½å’Œäº’åŠ¨æ€§çš„NPCã€‚è¯¥æ¡†æ¶çš„å¯é…ç½®æ€§å’Œçµæ´»æ€§ä½¿å…¶é€‚ç”¨äºå„ç§ç±»å‹çš„æ¸¸æˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŠ¨æ€åŒºåŸŸç³»ç»Ÿå’Œç©å®¶è¡ŒåŠ¨é¢„æµ‹æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿã€‚

## creative-agents--empowering-agents-with-imagination-for-creative-tasks
### Abstract
We study building embodied agents for open-ended creative tasks. While
existing methods build instruction-following agents that can perform diverse
open-ended tasks, none of them demonstrates creativity -- the ability to give
novel and diverse task solutions implicit in the language instructions. This
limitation comes from their inability to convert abstract language instructions
into concrete task goals in the environment and perform long-horizon planning
for such complicated goals. Given the observation that humans perform creative
tasks with the help of imagination, we propose a class of solutions for
creative agents, where the controller is enhanced with an imaginator that
generates detailed imaginations of task outcomes conditioned on language
instructions. We introduce several approaches to implementing the components of
creative agents. We implement the imaginator with either a large language model
for textual imagination or a diffusion model for visual imagination. The
controller can either be a behavior-cloning policy learned from data or a
pre-trained foundation model generating executable codes in the environment. We
benchmark creative tasks with the challenging open-world game Minecraft, where
the agents are asked to create diverse buildings given free-form language
instructions. In addition, we propose novel evaluation metrics for open-ended
creative tasks utilizing GPT-4V, which holds many advantages over existing
metrics. We perform a detailed experimental analysis of creative agents,
showing that creative agents are the first AI agents accomplishing diverse
building creation in the survival mode of Minecraft. Our benchmark and models
are open-source for future research on creative agents
(https://github.com/PKU-RL/Creative-Agents).
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ›æ„æ™ºèƒ½ä½“ï¼šèµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›ä»¥å®Œæˆåˆ›æ„ä»»åŠ¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰çš„æ™ºèƒ½ä½“å¤§å¤šåªèƒ½æ‰§è¡Œé¢„å®šä¹‰çš„ä»»åŠ¡ï¼Œç¼ºä¹å¤„ç†å¼€æ”¾æ€§ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦åˆ›é€ åŠ›çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨Minecraftæ¸¸æˆä¸­ï¼Œç°æœ‰çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œç®€å•çš„æŒ‡ä»¤ï¼Œå¦‚â€œæ”¶é›†çŸ³å¤´â€æˆ–â€œå»ºé€ ä¸€ä¸ªé›ªäººâ€ï¼Œä½†æ— æ³•å®Œæˆæ›´å¤æ‚çš„åˆ›æ„ä»»åŠ¡ï¼Œå¦‚â€œå»ºé€ ä¸€ä¸ªç ‚å²©å®«æ®¿â€ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬æ— æ³•å°†æŠ½è±¡çš„è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºå…·ä½“çš„ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶æ‰§è¡Œé•¿æœŸè§„åˆ’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†â€œåˆ›æ„æ™ºèƒ½ä½“â€çš„æ¦‚å¿µï¼Œé€šè¿‡èµ‹äºˆæ™ºèƒ½ä½“æƒ³è±¡åŠ›æ¥å¤„ç†å¼€æ”¾æ€§åˆ›æ„ä»»åŠ¡ã€‚åˆ›æ„æ™ºèƒ½ä½“ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šæƒ³è±¡å™¨å’Œæ§åˆ¶å™¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæƒ³è±¡å™¨
æƒ³è±¡å™¨è´Ÿè´£æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»»åŠ¡ç»“æœçš„è¯¦ç»†æƒ³è±¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æƒ³è±¡å™¨çš„æ–¹æ³•ï¼š
- **æ–‡æœ¬æƒ³è±¡**ï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4ï¼Œé€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æŠ€æœ¯ç”Ÿæˆæ–‡æœ¬æƒ³è±¡ã€‚
- **è§†è§‰æƒ³è±¡**ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionï¼Œç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸ç¬¦çš„è§†è§‰æƒ³è±¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ§åˆ¶å™¨
æ§åˆ¶å™¨è´Ÿè´£å°†æƒ³è±¡è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç°æ§åˆ¶å™¨çš„æ–¹æ³•ï¼š
- **è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨**ï¼šä»ç¯å¢ƒä¸­å­¦ä¹ è¡Œä¸ºå…‹éš†ç­–ç•¥ï¼Œå°†å›¾åƒæƒ³è±¡è½¬æ¢ä¸ºå»ºç­‘è“å›¾ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„åŠ¨ä½œã€‚
- **åŸºäºGPT-4(V)çš„æ§åˆ¶å™¨**ï¼šåˆ©ç”¨GPT-4(V)çš„è§†è§‰è¯­è¨€ç†è§£å’Œä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œç›´æ¥ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ¥å®Œæˆä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Minecraftæ¸¸æˆä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ›æ„æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªç”±å½¢å¼çš„è¯­è¨€æŒ‡ä»¤åˆ›å»ºå¤šæ ·åŒ–å’Œè§†è§‰ä¸Šå¸å¼•äººçš„å»ºç­‘ã€‚å…¶ä¸­ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†è§‰æƒ³è±¡ï¼Œå¹¶ç»“åˆGPT-4(V)è¿›è¡Œæ§åˆ¶çš„æ™ºèƒ½ä½“è¡¨ç°æœ€ä½³ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åˆ›æ„æ™ºèƒ½ä½“æ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ å’Œåˆ›æ„AIæ™ºèƒ½ä½“ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•æé«˜è¡Œä¸ºå…‹éš†æ§åˆ¶å™¨çš„æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•å¢å¼ºæ™ºèƒ½ä½“çš„åˆ›é€ åŠ›ã€‚

## visual-encoders-for-data-efficient-imitation-learning-in-modern-video-games
### Abstract
Video games have served as useful benchmarks for the decision making
community, but going beyond Atari games towards training agents in modern games
has been prohibitively expensive for the vast majority of the research
community. Recent progress in the research, development and open release of
large vision models has the potential to amortize some of these costs across
the community. However, it is currently unclear which of these models have
learnt representations that retain information critical for sequential decision
making. Towards enabling wider participation in the research of gameplaying
agents in modern games, we present a systematic study of imitation learning
with publicly available visual encoders compared to the typical, task-specific,
end-to-end training approach in Minecraft, Minecraft Dungeons and
Counter-Strike: Global Offensive.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§†è§‰ç¼–ç å™¨åœ¨ç°ä»£è§†é¢‘æ¸¸æˆä¸­çš„é«˜æ•ˆæ¨¡ä»¿å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†é¢‘æ¸¸æˆä¸€ç›´æ˜¯å†³ç­–åˆ¶å®šç¤¾åŒºçš„æœ‰ç”¨åŸºå‡†ï¼Œä½†å°†ç ”ç©¶æ‰©å±•åˆ°ç°ä»£æ¸¸æˆå¯¹äºå¤§å¤šæ•°ç ”ç©¶ç¤¾åŒºæ¥è¯´æˆæœ¬é«˜æ˜‚ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹è§†è§‰æ¨¡å‹çš„ç ”ç©¶ã€å¼€å‘å’Œå…¬å¼€å‘å¸ƒæœ‰å¯èƒ½åœ¨æ•´ä¸ªç¤¾åŒºä¸­åˆ†æ‘Šè¿™äº›æˆæœ¬ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹ä¸­çš„å“ªäº›æ¨¡å‹å·²ç»å­¦ä¹ äº†ä¿ç•™å¯¹é¡ºåºå†³ç­–è‡³å…³é‡è¦çš„ä¿¡æ¯çš„è¡¨ç¤ºã€‚ä¸ºäº†ä½¿æ›´å¹¿æ³›çš„ç¤¾åŒºå‚ä¸ç°ä»£æ¸¸æˆä¸­çš„æ¸¸æˆä»£ç†ç ”ç©¶ï¼Œæœ¬æ–‡å¯¹Minecraftã€Minecraft Dungeonså’ŒCounter-Strike: Global Offensiveä¸­çš„æ¨¡ä»¿å­¦ä¹ è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œå¹¶ä¸å…¸å‹çš„ã€ç‰¹å®šä»»åŠ¡çš„ç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æ¯”è¾ƒäº†ç«¯åˆ°ç«¯è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒç¼–ç å™¨åœ¨æ¨¡ä»¿å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¼–ç å™¨åœ¨ç›¸å¯¹è¾ƒå°çš„å›¾åƒä¸Šè®­ç»ƒï¼Œè€Œé¢„è®­ç»ƒç¼–ç å™¨åˆ™æ˜¯åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œå¯èƒ½æä¾›æœ‰ç”¨ä¸”é€šç”¨çš„è¡¨ç¤ºï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡ç ”ç©¶äº†ä¸åŒæ•°é‡çš„è®­ç»ƒæ•°æ®å¯¹è§†è§‰ç¼–ç å™¨æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒç¼–ç å™¨ä¹Ÿèƒ½è¡¨ç°å‡ºä¸ç‰¹å®šä»»åŠ¡ç¼–ç å™¨ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸‰ä¸ªç°ä»£è§†é¢‘æ¸¸æˆï¼ˆMinecraftã€Minecraft Dungeonså’ŒCounter-Strike: Global Offensiveï¼‰ä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š

1. å°å›¾åƒï¼ˆ128Ã—128ï¼‰è¶³ä»¥è®­ç»ƒç°ä»£è§†é¢‘æ¸¸æˆä¸­çš„ä»£ç†ï¼Œå³ä½¿åœ¨ä½¿ç”¨å°‘é‡é«˜è´¨é‡æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚
2. é¢„è®­ç»ƒç¼–ç å™¨ï¼Œç‰¹åˆ«æ˜¯DINOv2ï¼Œåœ¨æ¨¡ä»¿å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨å°‘é‡æ•°æ®æ—¶ä»ç„¶æœ‰æ•ˆã€‚
3. ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¼–ç å™¨åœ¨å¤„ç†æ›´çœŸå®ä¸–ç•Œçš„å›¾åƒæ—¶è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨ä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨æ—¶éœ€è¦ä»”ç»†è€ƒè™‘å›¾åƒå¤§å°å’Œæ¯”ä¾‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒç¼–ç å™¨åœ¨ç°ä»£è§†é¢‘æ¸¸æˆçš„æ¨¡ä»¿å­¦ä¹ ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨è¿™äº›ç¼–ç å™¨æ¥è®­ç»ƒä»£ç†ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†å›¾åƒå¤§å°å’Œæ¯”ä¾‹å¯¹é¢„è®­ç»ƒç¼–ç å™¨æ€§èƒ½çš„é‡è¦æ€§ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## building-open-ended-embodied-agent-via-language-policy-bidirectional-adaptation
### Abstract
Building embodied agents on integrating Large Language Models (LLMs) and
Reinforcement Learning (RL) have revolutionized human-AI interaction:
researchers can now leverage language instructions to plan decision-making for
open-ended tasks. However, existing research faces challenges in meeting the
requirement of open-endedness. They typically either train LLM/RL models to
adapt to a fixed counterpart, limiting exploration of novel skills and
hindering the efficacy of human-AI interaction. To this end, we present
OpenPAL, a co-training framework comprising two stages: (1) fine-tuning a
pre-trained LLM to translate human instructions into goals for planning, and
goal-conditioned training a policy for decision-making; (2) co-training to
align the LLM and policy, achieving instruction open-endedness. We conducted
experiments using Contra, an open-ended FPS game, demonstrating that an agent
trained with OpenPAL not only comprehends arbitrary instructions but also
exhibits efficient execution. These results suggest that OpenPAL holds the
potential to construct open-ended embodied agents in practical scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | OpenPALï¼šæ„å»ºå¼€æ”¾å¼çš„å…·èº«æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å…´èµ·ï¼Œæ„å»ºèƒ½å¤Ÿä¸äººç±»è¿›è¡Œäº¤äº’çš„å…·èº«æ™ºèƒ½ä½“æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶åœ¨å®ç°å¼€æ”¾æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå®ƒä»¬é€šå¸¸è®­ç»ƒLLM/RLæ¨¡å‹ä»¥é€‚åº”å›ºå®šçš„å¯¹æ‰‹ï¼Œé™åˆ¶äº†æ¢ç´¢æ–°æŠ€èƒ½çš„èƒ½åŠ›ï¼Œå¹¶é˜»ç¢äº†äººæœºäº¤äº’çš„æœ‰æ•ˆæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
OpenPALæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„ååŒè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°åŒå‘é€‚åº”ï¼Œä»è€Œæ„å»ºå¼€æ”¾å¼çš„å…·èº«æ™ºèƒ½ä½“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ­¥å¾®è°ƒ
OpenPALé¦–å…ˆé€šè¿‡å¤šæ­¥å¾®è°ƒé¢„è®­ç»ƒçš„LLMï¼Œä½¿å…¶èƒ½å¤Ÿå°†äººç±»æŒ‡ä»¤ç¿»è¯‘æˆè§„åˆ’ç›®æ ‡ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªç›®æ ‡æ¡ä»¶ç­–ç•¥æ¥è¿›è¡Œå†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šååŒè®­ç»ƒ
åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒOpenPALé€šè¿‡ååŒè®­ç»ƒæ¥å¯¹é½LLMå’Œç­–ç•¥ï¼Œå®ç°æŒ‡ä»¤çš„å¼€æ”¾æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒOpenPALäº¤æ›¿æ‰§è¡Œä»¥ä¸‹ä¸¤ä¸ªè¿‡ç¨‹ï¼š
1. ä½¿ç”¨ä»£ç†åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLAFï¼‰å¯¹LLMè¿›è¡Œè®­ç»ƒï¼Œå¥–åŠ±LLMç”Ÿæˆèƒ½å¤Ÿè¢«ä»£ç†æ‰§è¡Œçš„æŒ‡ä»¤ã€‚
2. ä½¿ç”¨LLMç”Ÿæˆçš„ç›®æ ‡å¯¹ç­–ç•¥è¿›è¡Œç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGCRLï¼‰è®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
OpenPALåœ¨Contraï¼ˆä¸€ä¸ªå¼€æ”¾å¼çš„FPSæ¸¸æˆï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä½¿ç”¨OpenPALè®­ç»ƒçš„æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿç†è§£ä»»æ„æŒ‡ä»¤ï¼Œè€Œä¸”èƒ½å¤Ÿé«˜æ•ˆåœ°æ‰§è¡Œè¿™äº›æŒ‡ä»¤ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
OpenPALä¸ºæ„å»ºå¼€æ”¾å¼çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå…¶å¤šæ­¥å¾®è°ƒå’ŒååŒè®­ç»ƒçš„è®¾è®¡å€¼å¾—å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒOpenPALåœ¨Contraä¸Šçš„æˆåŠŸåº”ç”¨è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹çš„äººæœºäº¤äº’ä¸­å…·æœ‰æ½œåŠ›ã€‚

## deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games
### Abstract
In this study, we explore the application of Large Language Models (LLMs) in
\textit{Jubensha}, a Chinese detective role-playing game and a novel area in
Artificial Intelligence (AI) driven gaming. We introduce the first dataset
specifically for Jubensha, including character scripts and game rules, to
foster AI agent development in this complex narrative environment. Our work
also presents a unique multi-agent interaction framework using LLMs, allowing
AI agents to autonomously engage in this game. To evaluate the gaming
performance of these AI agents, we developed novel methods measuring their
mastery of case information and reasoning skills. Furthermore, we incorporated
the latest advancements in in-context learning to improve the agents'
performance in information gathering, murderer identification, and logical
reasoning. The experimental results validate the effectiveness of our proposed
methods. This work aims to offer a novel perspective on understanding LLM
capabilities and establish a new benchmark for evaluating large language
model-based agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§£ç æ•°å­—ä¾¦æ¢ï¼šç†è§£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ™ºèƒ½ä½“æ¨ç†æ¸¸æˆä¸­çš„è¡Œä¸ºå’Œèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äº’åŠ¨è§’è‰²æ‰®æ¼”æ¸¸æˆï¼ˆIRPGsï¼‰çš„å…¨çƒæµè¡Œï¼Œç‰¹åˆ«æ˜¯ä¸­å›½ä¾¦æ¢è§’è‰²æ‰®æ¼”æ¸¸æˆâ€œå‰§æœ¬æ€â€ï¼ˆJubenshaï¼‰çš„å…´èµ·ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨ä¹Ÿæ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¼ ç»Ÿçš„æ£‹ç±»æ¸¸æˆã€è§†é¢‘æ¸¸æˆç­‰é¢†åŸŸï¼Œå¯¹äºâ€œå‰§æœ¬æ€â€è¿™ç±»éœ€è¦å¤šè½®è¯­è¨€äº¤äº’ã€ä¿¡æ¯æ”¶é›†å’Œé€»è¾‘æ¨ç†çš„æ¸¸æˆï¼ŒAIçš„åº”ç”¨è¿˜å¤„äºèµ·æ­¥é˜¶æ®µã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œä»¥è¡¡é‡LLMåœ¨å¤æ‚å™äº‹ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºäº†é¦–ä¸ªä¸“é—¨é’ˆå¯¹â€œå‰§æœ¬æ€â€æ¸¸æˆçš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬è§’è‰²å‰§æœ¬å’Œé¢„è®¾æ¸¸æˆè§„åˆ™ï¼Œä¸ºAIä»£ç†çš„å¼€å‘æä¾›äº†åŸºç¡€ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†ä¸€ä¸ªç‹¬ç‰¹çš„å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œä½¿ç”¨LLMsä½¿AIä»£ç†èƒ½å¤Ÿè‡ªä¸»å‚ä¸â€œå‰§æœ¬æ€â€æ¸¸æˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸ºäº†è¯„ä¼°AIä»£ç†åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œè®¾è®¡äº†ä¸¤ä¸ªæ–°é¢–çš„ä»»åŠ¡ï¼šä¸€ä¸ªç”¨äºè¯„ä¼°ä»–ä»¬å¯¹æ¡ˆä»¶ä¿¡æ¯çš„æŒæ¡ç¨‹åº¦ï¼Œå¦ä¸€ä¸ªç”¨äºè¯„ä¼°ä»–ä»¬çš„æ¨ç†èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ©ç”¨æœ€æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯ï¼Œè®¾è®¡äº†æ¨¡å—æ¥å¢å¼ºLLMä»£ç†åœ¨ä¿¡æ¯æ”¶é›†ã€å‡¶æ‰‹è¯†åˆ«å’Œé€»è¾‘æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä¿¡æ¯æ”¶é›†ã€å‡¶æ‰‹è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾è‘—æé«˜äº†LLMä»£ç†çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œä¸æ²¡æœ‰è®°å¿†æ£€ç´¢æ¨¡å—çš„ä»£ç†ç›¸æ¯”ï¼Œå…·æœ‰è®°å¿†æ£€ç´¢æ¨¡å—çš„ä»£ç†åœ¨å›ç­”å…³äºå…¶ä»–è§’è‰²çš„é—®é¢˜æ—¶å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œè‡ªå®Œå–„å’Œè‡ªéªŒè¯æ¨¡å—çš„ç»„åˆè¿›ä¸€æ­¥æé«˜äº†ä»£ç†çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜è¿™äº›æ¨¡å—æœ‰æ•ˆåœ°å¢å¼ºäº†ä»£ç†åœ¨â€œå‰§æœ¬æ€â€æ¸¸æˆä¸­çš„æ²Ÿé€šæ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ä¸ºLLMsåœ¨å¤æ‚å™äº‹ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºè¯„ä¼°LLMä»£ç†çš„æ€§èƒ½å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ThinkThriceæ¡†æ¶å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¨¡å—çš„è®¾è®¡ï¼Œä¸ºå¼€å‘æ›´æ™ºèƒ½ã€æ›´å…·æ¨ç†èƒ½åŠ›çš„AIä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars
### Abstract
Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå†å²æˆ˜äº‰ï¼Œæ¢ç´¢å’Œå¹³çš„å¯èƒ½æ€§

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æˆ˜äº‰ä¸å’Œå¹³æ˜¯äººç±»å†å²æ°¸æ’çš„ä¸»é¢˜ï¼Œç†è§£æˆ˜äº‰çš„åŸå› å’Œé¢„é˜²æˆ˜äº‰çš„å‘ç”Ÿä¸€ç›´æ˜¯äººç±»è¿½æ±‚çš„ç›®æ ‡ã€‚ä¼ ç»Ÿçš„æˆ˜äº‰ç ”ç©¶æ–¹æ³•ä¸»è¦ä¾èµ–äºå†å²åˆ†æå’Œæ–‡çŒ®å›é¡¾ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å—é™äºé™æ€è§†è§’å’Œäº‹åè¯¸è‘›äº®çš„åè§ã€‚éšç€äººå·¥æ™ºèƒ½å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬æœ‰æœºä¼šåˆ©ç”¨è¿™äº›å…ˆè¿›æŠ€æœ¯æ¥æ¨¡æ‹Ÿå†å²äº‹ä»¶ï¼Œæ¢ç´¢æˆ˜äº‰ä¸å’Œå¹³çš„åŠ¨æ€è¿‡ç¨‹ï¼Œå¹¶ä¸ºå†²çªè§£å†³å’Œå’Œå¹³ç»´æŠ¤æä¾›æ–°çš„è§†è§’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº† WarAgentï¼Œä¸€ä¸ªåŸºäº LLM çš„å¤šæ™ºèƒ½ä½“ AI ç³»ç»Ÿï¼Œç”¨äºæ¨¡æ‹Ÿå†å²å›½é™…å†²çªï¼ŒåŒ…æ‹¬ç¬¬ä¸€æ¬¡ä¸–ç•Œå¤§æˆ˜ï¼ˆWWIï¼‰ã€ç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ˜ï¼ˆWWIIï¼‰å’Œä¸­å›½å¤ä»£æˆ˜å›½æ—¶æœŸï¼ˆWSPï¼‰ã€‚WarAgent é€šè¿‡æ¨¡æ‹Ÿå‚ä¸å›½å®¶çš„å†³ç­–è¿‡ç¨‹å’Œäº’åŠ¨ï¼Œæ¢ç´¢äº†ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š

* **æ¨¡æ‹Ÿæœ‰æ•ˆæ€§**ï¼šLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦æœ‰æ•ˆåœ°å¤åˆ¶å†å²äº‹ä»¶ä¸­æˆ˜ç•¥è§„åˆ’å’Œå†³ç­–è¿‡ç¨‹çš„æ¼”å˜ï¼Ÿ
* **æˆ˜äº‰èµ·å› **ï¼šå“ªäº›å› ç´ æ˜¯å¯¼è‡´æˆ˜äº‰çˆ†å‘çš„å…³é”®å› ç´ ï¼ŸLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦è¯†åˆ«è¿™äº›å› ç´ ï¼Ÿ
* **æˆ˜äº‰ä¸å¯é¿å…æ€§**ï¼šå†å²ä¸Šçš„æˆ˜äº‰æ˜¯å¦çœŸçš„ä¸å¯é¿å…ï¼ŸLLM åŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¦æ­ç¤ºå¯¼è‡´æˆ˜äº‰ï¼ˆæˆ–å’Œå¹³ï¼‰çš„æ¡ä»¶ï¼Ÿ

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWarAgent èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå†å²äº‹ä»¶ï¼Œå¹¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¤åˆ¶å†å²å†³ç­–è¿‡ç¨‹å’Œäº’åŠ¨ã€‚ä¾‹å¦‚ï¼Œåœ¨ WWI æ¨¡æ‹Ÿä¸­ï¼ŒWarAgent èƒ½å¤Ÿé‡ç°ä¸»è¦å›½å®¶ä¹‹é—´çš„è”ç›Ÿå½¢æˆã€æˆ˜äº‰å®£è¨€å’ŒåŠ¨å‘˜æƒ…å†µï¼Œä¸å†å²äº‹ä»¶å…·æœ‰è¾ƒé«˜çš„å»åˆåº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹å˜è§¦å‘äº‹ä»¶å’Œå›½å®¶çš„åˆå§‹æ¡ä»¶ï¼ŒWarAgent èƒ½å¤Ÿæ¢ç´¢ä¸åŒçš„æˆ˜äº‰èµ·å› å’Œæˆ˜äº‰ä¸å¯é¿å…æ€§ï¼Œä¸ºç†è§£å†å²äº‹ä»¶å’Œé¢„é˜²æœªæ¥å†²çªæä¾›äº†æ–°çš„è§†è§’ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨**ï¼šWarAgent ä¸º LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œä¸ºç†è§£å¤æ‚çš„äººç±»è¡Œä¸ºå’Œç¤¾ä¼šåŠ¨æ€æä¾›äº†æ–°çš„å·¥å…·ã€‚
* **å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å†²çªè§£å†³ä¸­çš„åº”ç”¨**ï¼šWarAgent çš„è®¾è®¡ç†å¿µå¯ä»¥ä¸ºå†²çªè§£å†³å’Œå’Œå¹³ç»´æŠ¤æä¾›æ–°çš„æ€è·¯ï¼Œä¾‹å¦‚é€šè¿‡æ¨¡æ‹Ÿä¸åŒæ”¿ç­–çš„å½±å“æ¥è¯„ä¼°å†²çªè§£å†³ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
* **å†å²æ•™å­¦çš„æ–°æ–¹æ³•**ï¼šWarAgent å¯ä»¥ä½œä¸ºä¸€ç§æ–°çš„å†å²æ•™å­¦æ–¹æ³•ï¼Œå¸®åŠ©å­¦ç”Ÿå’Œæ•™å¸ˆæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯ï¼Œå¹¶ç†è§£å†å²äº‹ä»¶çš„å¤æ‚å› æœå…³ç³»ã€‚

### ğŸŒŸ æœªæ¥å±•æœ›
WarAgent çš„ç ”ç©¶ä¸º LLM åœ¨å†å²æ¨¡æ‹Ÿä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„é“è·¯ï¼Œæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š

* **æ—¶é—´é©±åŠ¨æ¨¡æ‹Ÿ**ï¼šå°† WarAgent çš„å›åˆåˆ¶æ¨¡æ‹Ÿæ‰©å±•ä¸ºæ—¶é—´é©±åŠ¨æ¨¡æ‹Ÿï¼Œä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå†å²äº‹ä»¶çš„æ—¶é—´åŠ¨æ€ã€‚
* **åœæ­¢æ¡ä»¶**ï¼šç ”ç©¶æ›´æœ‰æ•ˆçš„åœæ­¢æ¡ä»¶ï¼Œä»¥æ›´æ¸…æ™°åœ°ç»“æŸæ¨¡æ‹Ÿå¹¶åˆ†æç»“æœã€‚
* **æ–°çš„ç ”ç©¶é—®é¢˜**ï¼šæ¢ç´¢æ›´å¤šä¸å†å²äº‹ä»¶å’Œå†²çªè§£å†³ç›¸å…³çš„ç ”ç©¶é—®é¢˜ï¼Œä¾‹å¦‚å¤–äº¤æ²Ÿé€šä¸å†²çªå¯èƒ½æ€§ä¹‹é—´çš„å…³ç³»ã€éå›½å®¶è¡Œä¸ºä½“å¯¹åœ°ç¼˜æ”¿æ²»çš„å½±å“ç­‰ã€‚

é€šè¿‡ä¸æ–­æ”¹è¿›å’Œæ‰©å±• WarAgentï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£å†å²äº‹ä»¶ï¼Œå¹¶ä¸ºæ„å»ºæ›´åŠ å’Œå¹³çš„æœªæ¥æä¾›æ–°çš„æ€è·¯ã€‚

## see-and-think--embodied-agent-in-virtual-environment
### Abstract
Large language models (LLMs) have achieved impressive pro-gress on several
open-world tasks. Recently, using LLMs to build embodied agents has been a
hotspot. This paper proposes STEVE, a comprehensive and visionary embodied
agent in the Minecraft virtual environment. STEVE comprises three key
components: vision perception, language instruction, and code action. Vision
perception involves interpreting visual information in the environment, which
is then integrated into the LLMs component with agent state and task
instruction. Language instruction is responsible for iterative reasoning and
decomposing complex tasks into manageable guidelines. Code action generates
executable skill actions based on retrieval in skill database, enabling the
agent to interact effectively within the Minecraft environment. We also collect
STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge
question-answering pairs, and 200+ skill-code pairs. We conduct continuous
block search, knowledge question and answering, and tech tree mastery to
evaluate the performance. Extensive experiments show that STEVE achieves at
most 1.5x faster unlocking key tech trees and 2.5x quicker in block search
tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | See and Think: Embodied Agent in Virtual Environment

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œè®¾è®¡èƒ½å¤Ÿè¡¨ç°å‡ºæ™ºèƒ½è¡Œä¸ºå’Œé€‚åº”æ€§çš„æ™ºèƒ½ä½“ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªé•¿æœŸè€Œé‡è¦çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼€å‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå±•ç°å‡ºå…¶ä½œä¸ºå¤šåŠŸèƒ½ã€é€šç”¨å‹åŠ©æ‰‹çš„æ½œåŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œåœ¨è®¸å¤šå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¦‚Minecraftï¼Œå½“ä»£æ™ºèƒ½ä½“ä¸»è¦ä½¿ç”¨LLMsè¿›è¡Œæ–‡æœ¬äº¤äº’ã€‚ç„¶è€Œï¼Œè¿™ç§å¯¹æ–‡æœ¬é€šä¿¡çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨è¿™äº›ä¸–ç•Œä¸­çš„äº¤äº’ï¼ŒåŒ…æ‹¬ä½çº§æ¡ˆä¾‹ã€‚Minecraftè¦æ±‚æ™ºèƒ½ä½“å…·å¤‡å„ç§æŠ€èƒ½ï¼Œä»åˆ¶ä½œåŸºæœ¬ç‰©å“åˆ°æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±LLMsé©±åŠ¨çš„æ™ºèƒ½ä½“å¾€å¾€äº§ç”Ÿä¸å¯é¢„æµ‹çš„è¾“å‡ºã€‚å®ƒä»¬äº¤äº’çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œæ—¨åœ¨å°†LLMçš„ç†è§£ä¸ç¯å¢ƒçš„ä¸Šä¸‹æ–‡å’Œé¢„æœŸç›®æ ‡ç›¸ä¸€è‡´ã€‚è¿™ç§æç¤ºå·¥ç¨‹è¿‡ç¨‹ä¸ä»…è´¹åŠ›ï¼Œè€Œä¸”æ— æ³•å®ç°åŸ¹å…»è‡ªä¸»ã€è‡ªæˆ‘é©±åŠ¨çš„æ™ºèƒ½ä½“çš„ç›®æ ‡ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬é€šä¿¡åœ¨è‡ªç„¶ä¼ è¾¾æŸäº›ä¸–ç•Œæ¦‚å¿µæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚åˆ¶ä½œé…æ–¹ï¼Œè¿™äº›æ¦‚å¿µé€šå¸¸é€šè¿‡è§†è§‰æ›´æœ‰æ•ˆåœ°ä¼ è¾¾ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºSTEVEï¼Œä¸€ä¸ªåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å…·æœ‰è§†è§‰æ„ŸçŸ¥ã€è¯­è¨€æŒ‡ä»¤å’Œä»£ç åŠ¨ä½œçš„æ™ºèƒ½ä½“ï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨è§£é”å…³é”®æŠ€æœ¯æ ‘æ–¹é¢æœ€å¤šå¿«1.5å€ï¼Œåœ¨å—æœç´¢ä»»åŠ¡ä¸­æœ€å¤šå¿«2.3å€ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºSTEVE-7B/13Bï¼Œä¸€ç³»åˆ—é€šè¿‡ä½¿ç”¨Llama-2-7B/13Bçš„MinecraftçŸ¥è¯†é—®ç­”å¯¹è¿›è¡Œå¾®è°ƒè·å¾—çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”¶é›†STEVE-21Kæ•°æ®é›†ï¼ŒåŒ…æ‹¬600å¤šä¸ªè§†è§‰-ç¯å¢ƒå¯¹ã€20Kä¸ªçŸ¥è¯†é—®ç­”å¯¹å’Œ200å¤šä¸ªæŠ€èƒ½-ä»£ç å¯¹ï¼Œä»¥è¯æ˜STEVEçš„æœ‰æ•ˆæ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEVEåœ¨è¿ç»­å—æœç´¢ã€çŸ¥è¯†é—®ç­”å’Œç§‘æŠ€æ ‘æŒæ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸AutoGPTå’ŒVoyagerç­‰åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒSTEVEåœ¨è§£é”å…³é”®æŠ€æœ¯æ ‘æ–¹é¢æœ€å¤šå¿«1.5å€ï¼Œåœ¨å—æœç´¢ä»»åŠ¡ä¸­æœ€å¤šå¿«2.3å€ã€‚æ­¤å¤–ï¼ŒSTEVEåœ¨çŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶æ€§èƒ½ä¼˜äºLlama-2å’ŒGPT-4ç­‰æ›´å¹¿æ³›çš„æ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„STEVEæ¡†æ¶ä¸ºæ„å»ºå…·æœ‰è§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€æŒ‡ä»¤èƒ½åŠ›çš„æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼ŒSTEVE-21Kæ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†è¿›è¡Œå¤šæ¨¡æ€å­¦ä¹ ç ”ç©¶çš„æœ‰ç”¨èµ„æºã€‚

## designgpt--multi-agent-collaboration-in-design
### Abstract
Generative AI faces many challenges when entering the product design
workflow, such as interface usability and interaction patterns. Therefore,
based on design thinking and design process, we developed the DesignGPT
multi-agent collaboration framework, which uses artificial intelligence agents
to simulate the roles of different positions in the design company and allows
human designers to collaborate with them in natural language. Experimental
results show that compared with separate AI tools, DesignGPT improves the
performance of designers, highlighting the potential of applying multi-agent
systems that integrate design domain knowledge to product scheme design.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DesignGPTï¼šè®¾è®¡æµç¨‹ä¸­çš„å¤šæ™ºèƒ½ä½“åä½œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‘å±•ï¼Œç”Ÿæˆå¼AIå·¥å…·åœ¨äº§å“è®¾è®¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå¦‚MidJourneyå’ŒStable Diffusionç­‰å›¾åƒç”Ÿæˆå·¥å…·ï¼Œä»¥åŠChatGPTç­‰æ–‡æœ¬ç”Ÿæˆå·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆå¼AIå·¥å…·åœ¨äº§å“è®¾è®¡çš„å®é™…åº”ç”¨ä¸­é¢ä¸´ç€ç•Œé¢å¯ç”¨æ€§å’Œäº¤äº’æ¨¡å¼ç­‰æŒ‘æˆ˜ï¼Œä¸”è®¾è®¡æ€ç»´ä¸æœºå™¨æ€ç»´ä¹‹é—´å­˜åœ¨å¤©ç„¶é¸¿æ²Ÿã€‚å¦‚ä½•è®©AIæ›´å¥½åœ°ç†è§£è®¾è®¡æ€ç»´ï¼Œæ˜¯è®¾è®¡å¸ˆä¸AIäº¤äº’çš„ä¸€å¤§æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†DesignGPTï¼Œä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„è®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©è®¾è®¡å¸ˆä¸AIæ™ºèƒ½ä½“è¿›è¡Œè‡ªç„¶è¯­è¨€åä½œï¼Œå®Œæˆäº§å“æ–¹æ¡ˆè®¾è®¡ã€‚DesignGPTçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬éœ€æ±‚å¯¼å…¥è¡¨å•ã€è§’è‰²å®šä¹‰ä¸é€‰æ‹©ä»¥åŠä¼šè®®å®¤ã€‚ç³»ç»Ÿåˆå§‹åŒ–äº†å¤šä¸ªæ¨¡æ‹Ÿè®¾è®¡å…¬å¸ä¸åŒèŒä½çš„å‘˜å·¥è§’è‰²ï¼Œå¦‚è™šæ‹Ÿç”¨æˆ·ã€è€æ¿ã€äº§å“ç»ç†ã€è®¾è®¡æ€»ç›‘ã€CMFè®¾è®¡å¸ˆã€è¯„åˆ†è®°å½•å‘˜å’ŒæŠ€æœ¯äººå‘˜ï¼Œæ¯ä¸ªè§’è‰²éƒ½æœ‰æ˜ç¡®çš„èŒè´£å’Œä»»åŠ¡ã€‚è®¾è®¡å¸ˆç”¨æˆ·å¯ä»¥è¾“å…¥è®¾è®¡éœ€æ±‚ï¼Œé€‰æ‹©è§’è‰²å¹¶å¼€å§‹ä¼šè®®ï¼Œä¸AIæ™ºèƒ½ä½“è¿›è¡Œè‡ªç„¶è¯­è¨€äº¤æµï¼Œå…±åŒå®Œæˆéœ€æ±‚åˆ†æã€è®¾è®¡ææ¡ˆã€è¯¦ç»†è®¾è®¡å’ŒæŠ€æœ¯è¿­ä»£ç­‰å·¥ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä¸ºäº†è¯„ä¼°DesignGPTçš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡è¿›è¡Œäº†ä¸€é¡¹åœ¨çº¿å®éªŒï¼Œå°†å‚ä¸è€…éšæœºåˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«ä½¿ç”¨DesignGPTå’Œå•ç‹¬çš„AIå·¥å…·è¿›è¡Œè®¾è®¡ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸å•ç‹¬ä½¿ç”¨AIå·¥å…·ç›¸æ¯”ï¼ŒDesignGPTåœ¨è®¾è®¡çš„åˆ›æ–°æ€§ã€å®Œæ•´æ€§å’Œå¯è¡Œæ€§æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œå‚ä¸è€…å¯¹DesignGPTçš„äº¤äº’å½¢å¼ç»™äºˆäº†é«˜åº¦è¯„ä»·ï¼Œè®¤ä¸ºAIè§’è‰²èƒ½å¤Ÿç†Ÿç»ƒè¿ç”¨è®¾è®¡æµç¨‹å’Œè®¾è®¡è¡¨è¾¾ï¼Œå¹¶ä»å¤šä¸ªè§’åº¦æ¨å¯¼è®¾è®¡æ–¹æ¡ˆï¼Œçªç ´äº†ä»¥å¾€è®¾è®¡æ–¹æ¡ˆæ„æ€ä¸­è§†è§’ç›¸å¯¹å•ä¸€çš„é—®é¢˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DesignGPTçš„ç ”ç©¶ä¸ºç†è§£AIåœ¨è®¾è®¡æµç¨‹ä¸­çš„ä½œç”¨æä¾›äº†é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ã€‚å…¶å¤šæ™ºèƒ½ä½“åä½œçš„è®¾è®¡æ¡†æ¶ä¸ºè®¾è®¡å¸ˆä¸AIçš„åä½œæä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæé«˜è®¾è®¡æ•ˆç‡å’Œè®¾è®¡è´¨é‡ã€‚æ­¤å¤–ï¼ŒDesignGPTçš„å®éªŒç»“æœä¹Ÿè¡¨æ˜ï¼Œå°†è®¾è®¡é¢†åŸŸçŸ¥è¯†ä¸å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç›¸ç»“åˆï¼Œåœ¨äº§å“æ–¹æ¡ˆè®¾è®¡ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## magic--investigation-of-large-language-model-powered-multi-agent-in-cognition--adaptability--rationality-and-collaboration
### Abstract
Large Language Models (LLMs) have significantly advanced natural language
processing, demonstrating exceptional reasoning, tool usage, and memory
capabilities. As their applications expand into multi-agent environments, there
arises a need for a comprehensive evaluation framework that captures LLMs'
reasoning, planning, collaboration, and other social abilities. This work
introduces a novel competition-based benchmark framework specifically designed
to assess LLMs within multi-agent settings, providing quantitative metrics to
evaluate their judgment, reasoning, deception, self-awareness, cooperation,
coordination, and rationality. We utilize two social deduction games alongside
three game-theory scenarios to create diverse environments. Our frame is
fortified with the probabilistic graphic modeling (PGM) method, enhancing the
LLMs' capabilities in navigating complex social and cognitive dimensions. We
evaluate seven LLMs, quantitatively highlighting a significant capability gap
of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.
It also confirms that our PGM enhancement boosts the abilities of all selected
models by an average of 37%. Our data and code can be found here
https://github.com/cathyxl/MAgIC.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MAgICï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¤çŸ¥ã€é€‚åº”æ€§ã€ç†æ€§å’Œåä½œä¸­çš„å¤šæ™ºèƒ½ä½“ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ï¼Œå®ƒä»¬åœ¨æ¨ç†ã€å·¥å…·ä½¿ç”¨å’Œè®°å¿†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“LLMsçš„åº”ç”¨æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“ç¯å¢ƒæ—¶ï¼Œéœ€è¦ä¸€ç§å…¨é¢çš„è¯„ä¼°æ¡†æ¶æ¥æ•æ‰LLMsçš„æ¨ç†ã€è§„åˆ’ã€åä½œå’Œå…¶ä»–ç¤¾äº¤èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç«äº‰çš„åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå¹¶æä¾›å®šé‡æŒ‡æ ‡æ¥è¯„ä¼°å®ƒä»¬çš„åˆ¤æ–­ã€æ¨ç†ã€æ¬ºéª—ã€è‡ªæˆ‘æ„è¯†ã€åˆä½œã€åè°ƒå’Œç†æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«äº‰åŸºå‡†æ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç«äº‰çš„åŸºå‡†æ¡†æ¶ï¼Œé€šè¿‡å°†LLMsç½®äºå¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­çš„ç«äº‰ä¸­ï¼Œè¯„ä¼°å®ƒä»¬åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„çœŸå®èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªç¤¾äº¤æ¨ç†æ¸¸æˆå’Œä¸‰ä¸ªåšå¼ˆè®ºåœºæ™¯ï¼Œä»¥åˆ›å»ºå¤šæ ·åŒ–çš„ç¯å¢ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¦‚ç‡å›¾æ¨¡å‹ï¼ˆPGMï¼‰å¢å¼º
ä¸ºäº†å¢å¼ºLLMsåœ¨å¤æ‚ç¤¾äº¤å’Œè®¤çŸ¥ç»´åº¦ä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œæœ¬æ–‡å°†æ¦‚ç‡å›¾æ¨¡å‹ï¼ˆPGMï¼‰æ–¹æ³•ä¸LLMsç›¸ç»“åˆã€‚PGMèƒ½å¤Ÿæè¿°éšæœºå˜é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œå¸®åŠ©LLMsæ›´å¥½åœ°ç†è§£å…¨å±€ä¿¡æ¯ï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡è¯„ä¼°äº†ä¸ƒä¸ªLLMsï¼ŒåŒ…æ‹¬GPT-o1ã€GPT-4ã€GPT-3.5-turboã€PaLM 2ã€Claude 2ã€Cohereå’ŒLlama-2-70Bã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-o1åœ¨æ‰€æœ‰è¯„ä¼°ç»´åº¦ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒLlama-2-70Bè¡¨ç°æœ€å·®ã€‚æ­¤å¤–ï¼ŒPGMå¢å¼ºæ–¹æ³•å°†æ‰€æœ‰é€‰å®šæ¨¡å‹çš„å¹³å‡èƒ½åŠ›æé«˜äº†37%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ç«äº‰åŸºå‡†æ¡†æ¶å’ŒPGMå¢å¼ºæ–¹æ³•ä¸ºè¯„ä¼°å’Œæå‡LLMsåœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­çš„èƒ½åŠ›æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ºLLMsåœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åº”ç”¨æä¾›äº†é‡è¦çš„å‚è€ƒå’ŒæŒ‡å¯¼ã€‚

## mineland--simulating-large-scale-multi-agent-interactions-with-limited-multimodal-senses-and-physical-needs
### Abstract
While Vision-Language Models (VLMs) hold promise for tasks requiring
extensive collaboration, traditional multi-agent simulators have facilitated
rich explorations of an interactive artificial society that reflects collective
behavior. However, these existing simulators face significant limitations.
Firstly, they struggle with handling large numbers of agents due to high
resource demands. Secondly, they often assume agents possess perfect
information and limitless capabilities, hindering the ecological validity of
simulated social interactions. To bridge this gap, we propose a multi-agent
Minecraft simulator, MineLand, that bridges this gap by introducing three key
features: large-scale scalability, limited multimodal senses, and physical
needs. Our simulator supports 64 or more agents. Agents have limited visual,
auditory, and environmental awareness, forcing them to actively communicate and
collaborate to fulfill physical needs like food and resources. Additionally, we
further introduce an AI agent framework, Alex, inspired by multitasking theory,
enabling agents to handle intricate coordination and scheduling. Our
experiments demonstrate that the simulator, the corresponding benchmark, and
the AI agent framework contribute to more ecological and nuanced collective
behavior.The source code of MineLand and Alex is openly available at
https://github.com/cocacola-lab/MineLand.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MineLandï¼šæ¨¡æ‹Ÿå¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“äº¤äº’çš„Minecraftæ¨¡æ‹Ÿå™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶é¢ä¸´èµ„æºæ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ï¼Œå¹¶ä¸”é€šå¸¸å‡è®¾æ™ºèƒ½ä½“æ‹¥æœ‰å®Œç¾ä¿¡æ¯å’Œæ— é™èƒ½åŠ›ï¼Œè¿™ä¸ç°å®ä¸–ç•Œä¸­çš„äººç±»äº¤äº’å­˜åœ¨è¾ƒå¤§å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MineLandï¼Œä¸€ä¸ªåŸºäºMinecraftçš„å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿæ›´æ¥è¿‘ç°å®ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“äº¤äº’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤§è§„æ¨¡å¯æ‰©å±•æ€§
MineLandé€šè¿‡å°†æ¯ä¸ªMinecraftå®¢æˆ·ç«¯ç®€åŒ–ä¸ºå•ä¸ªçº¿ç¨‹ï¼Œä¼˜åŒ–äº†æ€§èƒ½å¼€é”€ï¼Œä»è€Œæ”¯æŒ64ä¸ªæˆ–æ›´å¤šæ™ºèƒ½ä½“åœ¨ä¸»æµæ¶ˆè´¹çº§æ¡Œé¢PCä¸Šè¿è¡Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ‰é™çš„æ¨¡æ€æ„ŸçŸ¥
MineLandæ¨¡æ‹Ÿäº†äººç±»çš„è§†è§‰å’Œå¬è§‰æœºåˆ¶ï¼Œå¯¹æ™ºèƒ½ä½“çš„æ„ŸçŸ¥èƒ½åŠ›æ–½åŠ äº†é™åˆ¶ï¼ŒåŒ…æ‹¬è·ç¦»è¡°å‡ã€ç¯å¢ƒé®æŒ¡å’Œæ–¹å‘çº¦æŸï¼Œä½¿å…¶æ›´æ¥è¿‘ç°å®ä¸–ç•Œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç‰©ç†éœ€æ±‚
MineLandå°†çœŸå®çš„ç‰©ç†éœ€æ±‚ï¼ˆå¦‚é£Ÿç‰©ã€æ°§æ°”å’Œé¥¥é¥¿ï¼‰é›†æˆåˆ°æ™ºèƒ½ä½“ä¸­ï¼Œä½¿å…¶éœ€è¦ç®¡ç†èµ„æºå¹¶ä¸å…¶ä»–æ™ºèƒ½ä½“ç«äº‰æˆ–åˆä½œï¼Œä»¥ç»´æŒç”Ÿå­˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¤šä»»åŠ¡å¤„ç†æ¡†æ¶Alex
MineLandå¼•å…¥äº†åŸºäºå¤šä»»åŠ¡ç†è®ºçš„AIæ™ºèƒ½ä½“æ¡†æ¶Alexï¼Œå…è®¸æ™ºèƒ½ä½“åŒæ—¶æ‰§è¡Œå¤æ‚çš„åè°ƒå’Œè°ƒåº¦ï¼Œä»¥å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒMineLandåœ¨æ”¯æŒå¤§è§„æ¨¡æ™ºèƒ½ä½“ã€æœ‰é™çš„æ¨¡æ€æ„ŸçŸ¥å’Œç‰©ç†éœ€æ±‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒAlexæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šä»»åŠ¡ï¼Œå¹¶åœ¨åˆä½œæ¨¡å¼ä¸‹æé«˜æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MineLandä¸ºç ”ç©¶å¤šæ™ºèƒ½ä½“äº¤äº’æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ï¼Œå…¶åˆ›æ–°çš„è®¾è®¡å’ŒåŠŸèƒ½å¯ä»¥åº”ç”¨äºäººç±»åŠ¨åŠ›å­¦ã€ç¤¾ä¼šå¿ƒç†å­¦ã€æœºå™¨äººæŠ€æœ¯å’Œæ¸¸æˆè®¾è®¡ç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼ŒAlexæ¡†æ¶çš„å¤šä»»åŠ¡å¤„ç†æœºåˆ¶ä¸ºå¼€å‘æ›´æ™ºèƒ½çš„AIæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚

## adapt--as-needed-decomposition-and-planning-with-language-models
### Abstract
Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ADaPTï¼šæŒ‰éœ€åˆ†è§£ä¸è§„åˆ’ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬ä¹Ÿé€æ¸è¢«åº”ç”¨äºéœ€è¦è§„åˆ’å’Œé€‚åº”ç¯å¢ƒçš„äº¤äº’å¼å†³ç­–ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºLLMsåœ¨æ‰§è¡Œå­ä»»åŠ¡æ—¶å¯èƒ½ä¼šå¤±è´¥ï¼Œä»è€Œå¯¼è‡´æ•´ä¸ªä»»åŠ¡çš„å¤±è´¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ADaPTï¼ˆAs-Needed Decomposition and Planning for complex Tasksï¼‰ï¼Œä¸€ç§æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’å¤æ‚ä»»åŠ¡çš„æ–¹æ³•ã€‚ADaPTçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå°†å…¶åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è¿›è¡Œåˆ†è§£ï¼Œä»¥é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæŒ‰éœ€åˆ†è§£
ADaPTé€šè¿‡é€’å½’åœ°åˆ†è§£å­ä»»åŠ¡ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚å½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå®ƒä¼šè°ƒç”¨LLMä½œä¸ºè§„åˆ’è€…æ¥ç”Ÿæˆæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è°ƒç”¨ADaPTæ¥æ‰§è¡Œè¿™äº›å­ä»»åŠ¡ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šçº§åˆ†è§£
ADaPTæ”¯æŒå¤šçº§åˆ†è§£ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥è¿›ä¸€æ­¥åˆ†è§£å­ä»»åŠ¡ï¼Œç›´åˆ°å®ƒä»¬å˜å¾—è¶³å¤Ÿç®€å•ï¼Œå¯ä»¥è¢«LLMä½œä¸ºæ‰§è¡Œè€…æˆåŠŸæ‰§è¡Œã€‚è¿™ç§å¤šçº§åˆ†è§£çš„èƒ½åŠ›ä½¿å¾—ADaPTèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ALFWorldã€WebShopå’ŒTextCraftä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒADaPTæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ–¹æ³•ï¼Œåœ¨ALFWorldä¸Šæé«˜äº†28.3%çš„æˆåŠŸç‡ï¼Œåœ¨WebShopä¸Šæé«˜äº†27%ï¼Œåœ¨TextCraftä¸Šæé«˜äº†33%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ADaPTæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å¤„ç†LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ‰§è¡Œå¤±è´¥é—®é¢˜ã€‚å®ƒé€šè¿‡æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†ä»»åŠ¡çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒADaPTçš„å¤šçº§åˆ†è§£èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚

## leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models
### Abstract
The automatic evaluation of LLM-based agent intelligence is critical in
developing advanced LLM-based agents. Although considerable effort has been
devoted to developing human-annotated evaluation datasets, such as AlpacaEval,
existing techniques are costly, time-consuming, and lack adaptability. In this
paper, inspired by the popular language game ``Who is Spy'', we propose to use
the word guessing game to assess the intelligence performance of LLMs. Given a
word, the LLM is asked to describe the word and determine its identity (spy or
not) based on its and other players' descriptions. Ideally, an advanced agent
should possess the ability to accurately describe a given word using an
aggressive description while concurrently maximizing confusion in the
conservative description, enhancing its participation in the game. To this end,
we first develop DEEP to evaluate LLMs' expression and disguising abilities.
DEEP requires LLM to describe a word in aggressive and conservative modes. We
then introduce SpyGame, an interactive multi-agent framework designed to assess
LLMs' intelligence through participation in a competitive language-based board
game. Incorporating multi-agent interaction, SpyGame requires the target LLM to
possess linguistic skills and strategic thinking, providing a more
comprehensive evaluation of LLMs' human-like cognitive abilities and
adaptability in complex communication situations. The proposed evaluation
framework is very easy to implement. We collected words from multiple sources,
domains, and languages and used the proposed evaluation framework to conduct
experiments. Extensive experiments demonstrate that the proposed DEEP and
SpyGame effectively evaluate the capabilities of various LLMs, capturing their
ability to adapt to novel situations and engage in strategic communication.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨çŒœè¯æ¸¸æˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTã€GPT-4å’ŒBardç­‰åœ¨å„ä¸ªä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººçš„æ€§èƒ½ï¼Œå¼€å‘åŸºäºLLMsçš„æ™ºèƒ½ä»£ç†å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°LLMsæ™ºèƒ½çš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š1ï¼‰äººå·¥æ ‡æ³¨æˆæœ¬é«˜ï¼Œè€—æ—¶ä¸”ç¼ºä¹å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼›2ï¼‰æ— æ³•å…¨é¢åæ˜ LLMsçš„æ™ºèƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œå³åˆ©ç”¨çŒœè¯æ¸¸æˆæ¥è¯„ä¼°LLMsçš„æ™ºèƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDEEPæ¡†æ¶
æœ¬æ–‡é¦–å…ˆæå‡ºäº†DEEPæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsçš„è¡¨è¾¾å’Œä¼ªè£…èƒ½åŠ›ã€‚DEEPè¦æ±‚LLMsä»¥æ¿€è¿›å’Œä¿å®ˆä¸¤ç§æ¨¡å¼æè¿°ä¸€ä¸ªç»™å®šçš„è¯ï¼Œå¹¶åˆ©ç”¨GPT-4æ¥åˆ¤æ–­è¿™äº›æè¿°æ˜¯å¦å‡†ç¡®ã€‚æ¿€è¿›æ¨¡å¼è¦æ±‚LLMsæä¾›æ¸…æ™°ã€è¯¦ç»†å’Œå‡†ç¡®çš„æè¿°ï¼Œè€Œä¿å®ˆæ¨¡å¼åˆ™è¦æ±‚LLMsæä¾›æ¨¡ç³Šçš„æè¿°ä»¥ä¼ªè£…ç›®æ ‡è¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSpyGameæ¡†æ¶
æœ¬æ–‡è¿˜æå‡ºäº†SpyGameæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªäº¤äº’å¼å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å‚ä¸ç«äº‰æ€§è¯­è¨€æ¸¸æˆâ€œè°æ˜¯å§åº•â€æ¥è¯„ä¼°LLMsçš„æ™ºèƒ½ã€‚SpyGameè¦æ±‚LLMså…·å¤‡è¯­è¨€æŠ€èƒ½å’Œæˆ˜ç•¥æ€ç»´èƒ½åŠ›ï¼Œä»è€Œæ›´å…¨é¢åœ°è¯„ä¼°LLMsåœ¨å¤æ‚æ²Ÿé€šæƒ…å¢ƒä¸­çš„äººç±»è®¤çŸ¥èƒ½åŠ›å’Œé€‚åº”æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¯¹å››ç§å¼€æºLLMså’Œä¸¤ç§é—­æºLLMsè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œé—­æºLLMsï¼ˆå¦‚GPT-4å’ŒGPT-3.5ï¼‰åœ¨æ¿€è¿›å’Œä¿å®ˆæ¨¡å¼ä¸‹çš„è¡¨ç°æ˜æ˜¾ä¼˜äºå¼€æºæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSpyGameæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°LLMsåœ¨å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„èƒ½åŠ›ï¼Œæ•æ‰å®ƒä»¬é€‚åº”æ–°æƒ…å†µå¹¶è¿›è¡Œæˆ˜ç•¥æ²Ÿé€šçš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„DEEPå’ŒSpyGameæ¡†æ¶ä¸ºè¯„ä¼°LLMsçš„æ™ºèƒ½æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. åˆ©ç”¨æ¸¸æˆè¿›è¡Œè¯„ä¼°ï¼Œæ›´å…·äº’åŠ¨æ€§å’Œè¶£å‘³æ€§ã€‚
2. å…³æ³¨LLMsçš„è¡¨è¾¾å’Œä¼ªè£…èƒ½åŠ›ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°å…¶æ™ºèƒ½ã€‚
3. SpyGameæ¡†æ¶æ”¯æŒäººç±»å‚ä¸ï¼Œæ›´è´´è¿‘çœŸå®åœºæ™¯ã€‚
4. é’ˆå¯¹å¤šæ™ºèƒ½ä½“äº¤äº’ä¸­çš„åå·®é—®é¢˜ï¼Œæå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡æå‡ºçš„è¯„ä¼°æ–¹æ³•ä¸ºLLMsçš„æ™ºèƒ½è¯„ä¼°æä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæ¨åŠ¨LLMsçš„å‘å±•å’Œåº”ç”¨ã€‚

## language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game
### Abstract
Agents built with large language models (LLMs) have shown great potential
across a wide range of domains. However, in complex decision-making tasks, pure
LLM-based agents tend to exhibit intrinsic bias in their choice of actions,
which is inherited from the model's training data and results in suboptimal
performance. To develop strategic language agents, i.e., agents that generate
flexible language actions and possess strong decision-making abilities, we
propose a novel framework that powers LLM-based agents with reinforcement
learning (RL). We consider Werewolf, a popular social deduction game, as a
challenging testbed that emphasizes versatile communication and strategic
gameplay. To mitigate the intrinsic bias in language actions, our agents use an
LLM to perform deductive reasoning and generate a diverse set of action
candidates. Then an RL policy trained to optimize the decision-making ability
chooses an action from the candidates to play in the game. Extensive
experiments show that our agents overcome the intrinsic bias and outperform
existing LLM-based agents in the Werewolf game. We also conduct human-agent
experiments and find that our agents achieve human-level performance and
demonstrate strong strategic play.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ™ºèƒ½ä½“åœ¨ç‹¼äººæ€æ¸¸æˆä¸­çš„æˆ˜ç•¥å†³ç­–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ„å»ºæ™ºèƒ½ä½“æ–¹é¢çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºçš„å†…åœ¨åå·®é—®é¢˜é€æ¸å‡¸æ˜¾ã€‚è¿™ç§åå·®æºäºæ¨¡å‹è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´LLM-basedæ™ºèƒ½ä½“åœ¨æˆ˜ç•¥å†³ç­–æ–¹é¢è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå°†LLMä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆï¼Œä»¥æ„å»ºå…·æœ‰çµæ´»è¯­è¨€è¡ŒåŠ¨å’Œå¼ºå¤§å†³ç­–èƒ½åŠ›çš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šéšè—è§’è‰²æ¨ç†
æœ¬æ–‡ä½¿ç”¨LLMå¯¹æ¸¸æˆä¸­çš„ä¿¡æ¯è¿›è¡Œåˆ†ç±»ï¼ŒåŒºåˆ†çœŸä¼ªï¼Œå¹¶æ¨æ–­æ¯ä¸ªç©å®¶çš„éšè—è§’è‰²ï¼Œä¸ºåç»­å†³ç­–æä¾›åŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆ
ä¸ºäº†å…‹æœLLMçš„å†…åœ¨åå·®ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æç¤ºLLMç”Ÿæˆä¸€ç³»åˆ—è¡ŒåŠ¨å€™é€‰è€…ï¼Œä»è€Œé¿å…å›ºå®šæ¨¡å¼å¹¶æé«˜å†³ç­–çš„çµæ´»æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
æœ¬æ–‡é‡‡ç”¨åŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªRLç­–ç•¥æ¥ä¼˜åŒ–è¡ŒåŠ¨å€™é€‰è€…çš„åˆ†å¸ƒï¼Œå¹¶é€šè¿‡ä¸å„ç§æ™ºèƒ½ä½“è¿›è¡Œå¯¹æŠ—æ¥æé«˜ç­–ç•¥çš„é²æ£’æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ç‹¼äººæ€æ¸¸æˆä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„LLM-basedæ™ºèƒ½ä½“ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“èƒ½å¤Ÿå…‹æœå†…åœ¨åå·®ï¼Œå¹¶åœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºæ›´å¼ºçš„æˆ˜ç•¥å†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸äººç±»ç©å®¶çš„å¯¹å±€å®éªŒä¹Ÿè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ™ºèƒ½ä½“èƒ½å¤Ÿè¾¾åˆ°äººç±»æ°´å¹³çš„æ¸¸æˆè¡¨ç°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶ä¸ºæ„å»ºå…·æœ‰å¼ºå¤§å†³ç­–èƒ½åŠ›çš„æˆ˜ç•¥è¯­è¨€æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦çµæ´»è¯­è¨€è¡ŒåŠ¨å’Œæˆ˜ç•¥å†³ç­–çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„å¤šæ ·åŒ–è¡ŒåŠ¨ç”Ÿæˆæ–¹æ³•å’ŒåŸºäºç¾¤ä½“çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ä¹Ÿä¸ºè§£å†³LLMå†…åœ¨åå·®é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚

## llm-based-agent-society-investigation--collaboration-and-confrontation-in-avalon-gameplay
### Abstract
This paper explores the open research problem of understanding the social
behaviors of LLM-based agents. Using Avalon as a testbed, we employ system
prompts to guide LLM agents in gameplay. While previous studies have touched on
gameplay with LLM agents, research on their social behaviors is lacking. We
propose a novel framework, tailored for Avalon, features a multi-agent system
facilitating efficient communication and interaction. We evaluate its
performance based on game success and analyze LLM agents' social behaviors.
Results affirm the framework's effectiveness in creating adaptive agents and
suggest LLM-based agents' potential in navigating dynamic social interactions.
By examining collaboration and confrontation behaviors, we offer insights into
this field's research and applications. Our code is publicly available at
https://github.com/3DAgentWorld/LLM-Game-Agent.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ç¤¾ä¼šè¡Œä¸ºç ”ç©¶ï¼šåœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­çš„åä½œä¸å¯¹æŠ—

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨æ¨¡æ‹Ÿå¤æ‚ç¯å¢ƒä¸­çš„è¡Œä¸ºæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨LLMæ™ºèƒ½ä½“åœ¨æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œè€Œå¯¹å…¶ç¤¾ä¼šè¡Œä¸ºçš„ç†è§£å´ç›¸å¯¹ç¼ºä¹ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMæ™ºèƒ½ä½“åœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­çš„ç¤¾ä¼šè¡Œä¸ºï¼ŒåŒ…æ‹¬åä½œä¸å¯¹æŠ—ï¼Œä»¥æœŸä¸ºLLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šå’Œæˆ˜ç•¥ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›æ–°çš„è§è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºLLMæ™ºèƒ½ä½“åœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­çš„åä½œä¸å¯¹æŠ—ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å¤šä¸ªæ¨¡å—ï¼Œå¦‚è®°å¿†å­˜å‚¨å’Œæ€»ç»“ã€åˆ†æå’Œè§„åˆ’ã€æ¸¸æˆåŠ¨ä½œå’Œå“åº”ç”Ÿæˆã€ç»éªŒå­¦ä¹ ç­‰ï¼Œä»¥æ¨¡æ‹Ÿäººç±»æ€ç»´è¿‡ç¨‹ï¼Œå¸®åŠ©LLMæ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£æ¸¸æˆå¹¶åšå‡ºå†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡é‡‡ç”¨ç³»ç»Ÿæç¤ºæ¥å¼•å¯¼LLMæ™ºèƒ½ä½“è¿›è¡Œæ¸¸æˆï¼Œå¹¶ä½¿ç”¨ChatGPTæ¥åˆ†ææ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸ºã€‚é€šè¿‡è§‚å¯Ÿæ™ºèƒ½ä½“åœ¨æ¸¸æˆä¸­çš„åä½œã€å¯¹æŠ—ã€é¢†å¯¼ã€è¯´æœã€ä¼ªè£…ç­‰è¡Œä¸ºï¼Œæœ¬æ–‡æ­ç¤ºäº†LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„è¡Œä¸ºç‰¹ç‚¹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶åœ¨é˜¿ç“¦éš†æ¸¸æˆä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæ™ºèƒ½ä½“çš„èƒœç‡æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†ææ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸ºï¼Œæœ¬æ–‡å‘ç°LLMæ™ºèƒ½ä½“èƒ½å¤Ÿå±•ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„ç¤¾ä¼šè¡Œä¸ºï¼Œå¦‚é¢†å¯¼ã€è¯´æœã€ä¼ªè£…ç­‰ï¼Œè¿™è¡¨æ˜LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›å·¨å¤§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œå®éªŒæ–¹æ³•ä¸ºç ”ç©¶LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„è¡Œä¸ºæä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿä¸ºLLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šå’Œæˆ˜ç•¥ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„è§è§£ï¼Œæœ‰åŠ©äºæ¨åŠ¨LLMæ™ºèƒ½ä½“åœ¨ç¤¾ä¼šç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

## steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds
### Abstract
Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to "a blindfolded text-based game."
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Steve-Eyeï¼šä¸ºåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“èµ‹äºˆå¼€æ”¾ä¸–ç•Œçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨èµ‹äºˆå…·èº«æ™ºèƒ½ä½“ä¸ä¸–ç•Œäº’åŠ¨çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™æ ‡å¿—ç€é€šç”¨æœºå™¨äººæŠ€æœ¯è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶å¾€å¾€å¿½ç•¥äº†å¼€æ”¾ä¸–ç•Œçš„è§†è§‰ä¸°å¯Œæ€§ï¼Œå¯¼è‡´æ•´ä¸ªäº¤äº’è¿‡ç¨‹ç±»ä¼¼äºâ€œä¸€ä¸ªè’™ç€çœ¼ç›çš„åŸºäºæ–‡æœ¬çš„æ¸¸æˆâ€ã€‚å› æ­¤ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨ç›´è§‚åœ°ç†è§£å‘¨å›´ç¯å¢ƒå’Œç”Ÿæˆæ˜“äºç†è§£çš„å“åº”æ–¹é¢ç»å¸¸é‡åˆ°æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Steve-Eyeï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨èµ‹äºˆåŸºäºLLMçš„å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿›è¡Œè§†è§‰æ„ŸçŸ¥çš„èƒ½åŠ›ã€‚Steve-Eyeå°†LLMä¸è§†è§‰ç¼–ç å™¨ç›¸ç»“åˆï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰-æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆå¤šæ¨¡æ€åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠè‡ªåŠ¨ç­–ç•¥æ”¶é›†äº†ä¸€ä¸ªåŒ…å«850Kå¼€æ”¾ä¸–ç•ŒæŒ‡ä»¤å¯¹çš„å¹¿æ³›æ•°æ®é›†ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ¶µç›–æ™ºèƒ½ä½“çš„ä¸‰ä¸ªåŸºæœ¬åŠŸèƒ½ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸‰ä¸ªå¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ï¼Œç„¶åä»å¹¿æ³›çš„è§†è§’è¿›è¡Œå¤§é‡å®éªŒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒSteve-Eyeåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºåŸºäºLLMçš„æ™ºèƒ½ä½“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹ä¸‰ä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼š
1. ç¯å¢ƒè§†è§‰æè¿°ï¼ˆENV-VCï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æ„ŸçŸ¥å’Œæè¿°å…¶å‘¨å›´ç¯å¢ƒçš„èƒ½åŠ›ã€‚
2. åŸºç¡€çŸ¥è¯†é—®ç­”ï¼ˆFK-QAï¼‰ï¼šè¯„ä¼°æ™ºèƒ½ä½“æŒæ¡å¯¹å†³ç­–è‡³å…³é‡è¦çš„åŸºæœ¬çŸ¥è¯†çš„ç†Ÿç»ƒç¨‹åº¦ã€‚
3. æŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’ï¼ˆSPPï¼‰ï¼šé‡åŒ–æ™ºèƒ½ä½“åœ¨æˆ˜ç•¥è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Steve-Eyeçš„ç ”ç©¶æˆæœä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æœ‰æ•ˆäº’åŠ¨çš„å…·èº«æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚å…¶å¤šæ¨¡æ€æ„ŸçŸ¥ã€åŸºç¡€çŸ¥è¯†åº“å’ŒæŠ€èƒ½é¢„æµ‹ä¸è§„åˆ’åŠŸèƒ½ä¸ºæ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»è¡ŒåŠ¨å’Œè§„åˆ’æä¾›äº†å¼ºå¤§çš„æ”¯æŒã€‚æ­¤å¤–ï¼ŒSteve-Eyeçš„å¼€æ”¾ä¸–ç•Œè¯„ä¼°åŸºå‡†ä¸ºè¯„ä¼°å…·èº«æ™ºèƒ½ä½“çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## sotopia--interactive-evaluation-for-social-intelligence-in-language-agents
### Abstract
Humans are social beings; we pursue social goals in our daily interactions,
which is a crucial aspect of social intelligence. Yet, AI systems' abilities in
this realm remain elusive. We present SOTOPIA, an open-ended environment to
simulate complex social interactions between artificial agents and evaluate
their social intelligence. In our environment, agents role-play and interact
under a wide variety of scenarios; they coordinate, collaborate, exchange, and
compete with each other to achieve complex social goals. We simulate the
role-play interaction between LLM-based agents and humans within this task
space and evaluate their performance with a holistic evaluation framework
called SOTOPIA-Eval. With SOTOPIA, we find significant differences between
these models in terms of their social intelligence, and we identify a subset of
SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.
We find that on this subset, GPT-4 achieves a significantly lower goal
completion rate than humans and struggles to exhibit social commonsense
reasoning and strategic communication skills. These findings demonstrate
SOTOPIA's promise as a general platform for research on evaluating and
improving social intelligence in artificial agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SOTOPIAï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹ç¤¾äº¤æ™ºèƒ½çš„å¼€æ”¾ç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
äººç±»åœ¨ç¤¾ä¼šäº’åŠ¨ä¸­è¿½æ±‚å¤æ‚çš„ç¤¾ä¼šç›®æ ‡ï¼Œè¿™æ˜¯ç¤¾äº¤æ™ºèƒ½çš„å…³é”®æ–¹é¢ã€‚ç„¶è€Œï¼ŒAIç³»ç»Ÿåœ¨ç¤¾äº¤é¢†åŸŸçš„æ™ºèƒ½ä»ç„¶éš¾ä»¥æ‰æ‘¸ã€‚ç°æœ‰çš„ç¤¾äº¤æ™ºèƒ½è¯„ä¼°æ–¹æ³•è¦ä¹ˆæ˜¯éäº¤äº’å¼çš„ï¼Œè¦ä¹ˆç¼ºä¹å¯¹å¤šæ ·åŒ–ç›®æ ‡é©±åŠ¨è¡Œä¸ºçš„å…³æ³¨ï¼Œæˆ–è€…ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ã€‚ä¸ºäº†ç ”ç©¶åŠ¨æ€å’Œç›®æ ‡é©±åŠ¨çš„ç¤¾äº¤æ™ºèƒ½ï¼Œæœ¬æ–‡æå‡ºäº†SOTOPIAï¼Œä¸€ä¸ªå¼€æ”¾å¼çš„é€šç”¨é¢†åŸŸç¯å¢ƒï¼Œç”¨äºæ¨¡æ‹Ÿäººå·¥ä»£ç†ä¹‹é—´çš„å¤æ‚ç¤¾äº¤äº’åŠ¨å¹¶è¯„ä¼°å…¶ç¤¾äº¤æ™ºèƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSOTOPIAç¯å¢ƒ
SOTOPIAæ˜¯ä¸€ä¸ªäº¤äº’å¼ç¯å¢ƒï¼Œæ”¯æŒå¤šè½®æ¨¡æ‹Ÿé€šä¿¡ï¼Œä»£ç†å¯ä»¥ä½¿ç”¨è¯­è¨€å’Œéè¯­è¨€æ²Ÿé€šä»¥åŠç‰©ç†åŠ¨ä½œã€‚å®ƒå…·æœ‰å¤šæ ·åŒ–çš„ä»»åŠ¡ç©ºé—´ï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç”Ÿæˆçš„åœºæ™¯ã€ç›®æ ‡ã€è§’è‰²ã€å…³ç³»å’Œå…¶ä»–ä»£ç†çš„ç­–ç•¥ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªåºå¤§ä¸”å¤šæ ·åŒ–çš„ä»»åŠ¡ç©ºé—´ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSOTOPIA-EVALè¯„ä¼°æ¡†æ¶
SOTOPIA-EVALæ˜¯ä¸€ä¸ªå¤šç»´åº¦çš„è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç¤¾äº¤ç»´åº¦åˆ†æä»£ç†çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ç›®æ ‡å®Œæˆåº¦ã€å¯ä¿¡åº¦ã€çŸ¥è¯†è·å–ã€ç§˜å¯†ä¿æŒã€å…³ç³»ç»´æŠ¤ã€ç¤¾ä¼šè§„åˆ™éµå®ˆå’Œè´¢åŠ¡åŠç‰©è´¨åˆ©ç›Šã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4åœ¨SOTOPIA-EVALçš„æŸäº›ç»´åº¦ä¸Šå¯ä»¥ä½œä¸ºäººç±»åˆ¤æ–­çš„ä»£ç†ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡å®Œæˆåº¦æ–¹é¢ã€‚åœ¨æ¨¡å‹ä¹‹é—´ï¼ŒGPT-4åœ¨å¤§å¤šæ•°ç»´åº¦ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯GPT-3.5ã€Llama-2-70b-chatå’ŒMPT-30b-chatã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨éµå®ˆç¤¾ä¼šè§„åˆ™å’Œä¿æŒç§˜å¯†æ–¹é¢éƒ½å­˜åœ¨é£é™©ã€‚ä¸äººç±»ç›¸æ¯”ï¼ŒGPT-4åœ¨ç›®æ ‡å®Œæˆåº¦æ–¹é¢æ˜¾è‘—ä½äºäººç±»ï¼Œå¹¶ä¸”åœ¨å±•ç¤ºç¤¾äº¤å¸¸è¯†æ¨ç†å’Œæˆ˜ç•¥æ²Ÿé€šæŠ€èƒ½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SOTOPIAä¸ºè¯„ä¼°å’Œæ”¹è¿›äººå·¥ä»£ç†çš„ç¤¾äº¤æ™ºèƒ½æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„å¹³å°ã€‚SOTOPIA-EVALæ¡†æ¶å¯ä»¥ç”¨äºè¯„ä¼°ä»£ç†åœ¨å¤šä¸ªç¤¾äº¤ç»´åº¦ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¸®åŠ©ç ”ç©¶äººå‘˜ç†è§£æ¨¡å‹ä¹‹é—´ä»¥åŠæ¨¡å‹ä¸äººç±»ä¹‹é—´çš„ç¤¾äº¤æ™ºèƒ½å·®å¼‚ã€‚æ­¤å¤–ï¼ŒSOTOPIAè¿˜å¯ä»¥ç”¨äºè®­ç»ƒæ›´å…·ç¤¾äº¤æ™ºèƒ½çš„è¯­è¨€ä»£ç†ã€‚

## character-llm--a-trainable-agent-for-role-playing
### Abstract
Large language models (LLMs) can be used to serve as agents to simulate human
behaviors, given the powerful ability to understand human instructions and
provide high-quality generated texts. Such ability stimulates us to wonder
whether LLMs can simulate a person in a higher form than simple human
behaviors. Therefore, we aim to train an agent with the profile, experience,
and emotional states of a specific person instead of using limited prompts to
instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs
to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,
etc. Our method focuses on editing profiles as experiences of a certain
character and training models to be personal simulacra with these experiences.
To assess the effectiveness of our approach, we build a test playground that
interviews trained agents and evaluates whether the agents \textit{memorize}
their characters and experiences. Experimental results show interesting
observations that help build future simulacra of humankind.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Character-LLMï¼šåŸºäºè§’è‰²æ‰®æ¼”çš„å¯è®­ç»ƒæ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸åªèƒ½æ ¹æ®æœ‰é™çš„æç¤ºæ¥æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œç¼ºä¹å¯¹ç‰¹å®šäººç‰©è§’è‰²ã€ç»éªŒå’Œæƒ…æ„ŸçŠ¶æ€çš„æ·±å…¥ç†è§£ã€‚æœ¬æ–‡æ—¨åœ¨è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹Ÿç‰¹å®šäººç‰©çš„æ™ºèƒ½ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´çœŸå®åœ°æ‰®æ¼”å„ç§è§’è‰²ï¼Œä¾‹å¦‚è´å¤šèŠ¬ã€å…‹è‰å¥¥å¸•ç‰¹æ‹‰å¥³ç‹ã€å‡¯æ’’å¤§å¸ç­‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç»éªŒé‡å»º
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»éªŒé‡å»ºè¿‡ç¨‹ï¼Œé€šè¿‡æ”¶é›†ç‰¹å®šäººç‰©çš„ç”Ÿå¹³ç»å†ï¼Œåˆ©ç”¨LLMsæå–åœºæ™¯ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„äº’åŠ¨ä½“éªŒï¼Œä»è€Œä¸ºè®­ç»ƒæ™ºèƒ½ä½“æä¾›å½¢å¼åŒ–çš„ç»éªŒæ•°æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»éªŒä¸Šä¼ ä¸ä¿æŠ¤æ€§ç»éªŒ
ä¸ºäº†ä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°æ‰®æ¼”ç‰¹å®šè§’è‰²ï¼Œæœ¬æ–‡å¼•å…¥äº†ç»éªŒä¸Šä¼ è¿‡ç¨‹ï¼Œå°†é‡å»ºçš„ç»éªŒæ•°æ®ç”¨äºå¾®è°ƒLLMsï¼Œä½¿å…¶èƒ½å¤Ÿæ¨¡æ‹Ÿç‰¹å®šäººç‰©çš„è¡Œä¸ºå’Œæƒ…æ„Ÿã€‚æ­¤å¤–ï¼Œä¸ºäº†é˜²æ­¢æ™ºèƒ½ä½“äº§ç”Ÿä¸è§’è‰²ä¸ç¬¦çš„å¹»è§‰ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¿æŠ¤æ€§ç»éªŒï¼Œå¸®åŠ©æ™ºèƒ½ä½“å¿˜è®°ä¸è§’è‰²æ— å…³çš„çŸ¥è¯†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡æ„å»ºä¸€ä¸ªæµ‹è¯•å¹³å°ï¼Œå¯¹è®­ç»ƒåçš„æ™ºèƒ½ä½“è¿›è¡Œè®¿è°ˆï¼Œè¯„ä¼°å…¶æ˜¯å¦èƒ½å¤Ÿè®°ä½è§’è‰²å’Œç»å†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCharacter-LLMèƒ½å¤ŸæˆåŠŸåœ°æ¨¡æ‹Ÿç‰¹å®šäººç‰©ï¼Œå¹¶åœ¨è§’è‰²æ‰®æ¼”æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸åŸºäºæç¤ºçš„LLMsç›¸æ¯”ï¼ŒCharacter-LLMåœ¨ä¸ªæ€§ã€è®°å¿†ã€å¹»è§‰å’Œç¨³å®šæ€§æ–¹é¢éƒ½å–å¾—äº†æ›´å¥½çš„æˆç»©ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Character-LLMæ–¹æ³•ä¸ºæ„å»ºæ›´çœŸå®ã€æ›´å…·ä¸ªæ€§åŒ–çš„æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶ç»éªŒé‡å»ºå’Œä¸Šä¼ è¿‡ç¨‹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£ç‰¹å®šäººç‰©çš„è§’è‰²ã€ç»éªŒå’Œæƒ…æ„ŸçŠ¶æ€ï¼Œä»è€Œåœ¨è§’è‰²æ‰®æ¼”æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„å¯ä¿¡åº¦ã€‚æ­¤å¤–ï¼Œä¿æŠ¤æ€§ç»éªŒçš„å¼•å…¥ä¹Ÿæœ‰åŠ©äºé˜²æ­¢æ™ºèƒ½ä½“äº§ç”Ÿä¸è§’è‰²ä¸ç¬¦çš„å¹»è§‰ï¼Œæé«˜è§’è‰²æ‰®æ¼”çš„å‡†ç¡®æ€§ã€‚

## llama-rider--spurring-large-language-models-to-explore-the-open-world
### Abstract
Recently, various studies have leveraged Large Language Models (LLMs) to help
decision-making and planning in environments, and try to align the LLMs'
knowledge with the world conditions. Nonetheless, the capacity of LLMs to
continuously acquire environmental knowledge and adapt in an open world remains
uncertain. In this paper, we propose an approach to spur LLMs to explore the
open world, gather experiences, and learn to improve their task-solving
capabilities. In this approach, a multi-round feedback-revision mechanism is
utilized to encourage LLMs to actively select appropriate revision actions
guided by feedback information from the environment. This facilitates
exploration and enhances the model's performance. Besides, we integrate
sub-task relabeling to assist LLMs in maintaining consistency in sub-task
planning and help the model learn the combinatorial nature between tasks,
enabling it to complete a wider range of tasks through training based on the
acquired exploration experiences. By evaluation in Minecraft, an open-ended
sandbox world, we demonstrate that our approach LLaMA-Rider enhances the
efficiency of the LLM in exploring the environment, and effectively improves
the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k
instances of collected data, showing minimal training costs compared to the
baseline using reinforcement learning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLaMA Riderï¼šæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹æ¢ç´¢å¼€æ”¾ä¸–ç•Œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è®¸å¤šç ”ç©¶å¼€å§‹åˆ©ç”¨LLMsçš„èƒ½åŠ›æ¥å¸®åŠ©æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–ï¼Œå¹¶å‘ç°LLMså…·æœ‰ä¸€å®šçš„è§„åˆ’å’Œå®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMsçš„çŸ¥è¯†æ¥æºäºé¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯­è¨€è¯­æ–™åº“ï¼Œå¯èƒ½ä¸ç‰¹å®šç¯å¢ƒå­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä¸€äº›ç ”ç©¶é€šè¿‡æç¤ºå·¥ç¨‹è®¾è®¡ç‰¹å®šæœºåˆ¶ï¼Œä¸ºLLMsæä¾›ç¯å¢ƒä¿¡æ¯ã€‚ç„¶è€Œï¼ŒLLMsåœ¨ç¯å¢ƒä¸­å¹¶ä¸ä¼šæ”¹è¿›æˆ–è·å–æ–°çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦æ›´å¤æ‚çš„æœºåˆ¶å’Œæç¤ºï¼Œè¿™ä¼šå¯¼è‡´LLMsç”Ÿæˆæˆæœ¬é«˜ï¼Œå¹¶ä¸”ä¾èµ–äºåƒGPT-4è¿™æ ·å…·æœ‰è¶³å¤ŸçŸ¥è¯†çš„å¼ºå¤§æ¨¡å‹ã€‚è¿˜æœ‰ä¸€äº›ç ”ç©¶é€šè¿‡å¾®è°ƒæ¥å°†LLMsä¸å®é™…ç¯å¢ƒç›¸ç»“åˆï¼Œä½†è¿™é€šå¸¸éœ€è¦ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®é›†ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¹Ÿè¢«ç ”ç©¶ï¼Œä½†è¿™äº›æ–¹æ³•å°†LLMsè®­ç»ƒä¸ºç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°RLæ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹æˆ–æ›´å¤æ‚çš„ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLaMA-Riderçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡LLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢æ¥å¢å¼ºå…¶èƒ½åŠ›ã€‚LLaMA-Rideræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æ¢ç´¢é˜¶æ®µå’Œå­¦ä¹ é˜¶æ®µã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢é˜¶æ®µ
åœ¨æ¢ç´¢é˜¶æ®µï¼ŒLLaMA-Rideråˆ©ç”¨åé¦ˆ-ä¿®æ­£æœºåˆ¶æ¥é¼“åŠ±LLMsä¸»åŠ¨é€‰æ‹©é€‚å½“çš„ä¿®æ­£åŠ¨ä½œï¼Œä»¥é€‚åº”ç¯å¢ƒã€‚LLMsåœ¨ç¯å¢ƒä¸­è¿›è¡Œæ¢ç´¢ï¼Œæ”¶é›†ç»éªŒï¼Œå¹¶é€šè¿‡åé¦ˆä¿¡æ¯æ¥æ”¹è¿›å…¶å†³ç­–ã€‚æ­¤å¤–ï¼ŒLLaMA-Riderè¿˜ä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥å¸®åŠ©LLMsä¿æŒå­ä»»åŠ¡è§„åˆ’çš„è¿è´¯æ€§ï¼Œå¹¶å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§è´¨ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå­¦ä¹ é˜¶æ®µ
åœ¨å­¦ä¹ é˜¶æ®µï¼ŒLLaMA-Riderå°†æ”¶é›†åˆ°çš„ç»éªŒå¤„ç†æˆæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è®­ç»ƒLLMsã€‚é™¤äº†ä»æˆåŠŸä»»åŠ¡ä¸­è·å¾—çš„ç»éªŒå¤–ï¼ŒLLaMA-Riderè¿˜æ”¶é›†éƒ¨åˆ†å®Œæˆçš„å­ä»»åŠ¡çš„ç»éªŒï¼Œå› ä¸ºæœ‰äº›ä»»åŠ¡åœ¨æ¢ç´¢é˜¶æ®µå¾ˆéš¾å®Œæˆã€‚å¼€æ”¾ç¯å¢ƒä¸­çš„è®¸å¤šä»»åŠ¡é€šå¸¸å…·æœ‰ç»„åˆæ€§ï¼Œè¿™æ„å‘³ç€è¿‡å»ä»»åŠ¡çš„ç»éªŒå¯ä»¥ç»å¸¸å¸®åŠ©å®Œæˆå…¶ä»–ä»»åŠ¡ã€‚LLaMA-Riderä½¿ç”¨å­ä»»åŠ¡é‡æ ‡è®°æ¥æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å¸®åŠ©LLMså­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç»„åˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Minecraftæ¨¡æ‹Ÿå™¨MineDojoä¸Šè¯„ä¼°äº†LLaMA-Rideræ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMA-Riderèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒï¼Œå¹¶é€šè¿‡å¾®è°ƒä»…ä½¿ç”¨1.3kä¸ªæ”¶é›†åˆ°çš„æ•°æ®å®ä¾‹æ¥æé«˜LLMså®Œæˆä»»åŠ¡çš„èƒ½åŠ›ï¼Œä¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæˆæœ¬æ›´ä½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LLaMA-Rideræ–¹æ³•ä¸ºLLMsåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„æ¢ç´¢å’Œå­¦ä¹ æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å…¶åé¦ˆ-ä¿®æ­£æœºåˆ¶å’Œå­ä»»åŠ¡é‡æ ‡è®°æŠ€æœ¯å¯ä»¥å¸®åŠ©LLMsæ›´å¥½åœ°é€‚åº”ç¯å¢ƒï¼Œå¹¶æé«˜å…¶å®Œæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLLaMA-Rideræ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¼€æ”¾ç¯å¢ƒï¼Œå¹¶å…·æœ‰ç»ˆèº«æ¢ç´¢å’Œå­¦ä¹ çš„æ½œåŠ›ã€‚

## gamegpt--multi-agent-collaborative-framework-for-game-development
### Abstract
The large language model (LLM) based agents have demonstrated their capacity
to automate and expedite software development processes. In this paper, we
focus on game development and propose a multi-agent collaborative framework,
dubbed GameGPT, to automate game development. While many studies have
pinpointed hallucination as a primary roadblock for deploying LLMs in
production, we identify another concern: redundancy. Our framework presents a
series of methods to mitigate both concerns. These methods include dual
collaboration and layered approaches with several in-house lexicons, to
mitigate the hallucination and redundancy in the planning, task identification,
and implementation phases. Furthermore, a decoupling approach is also
introduced to achieve code generation with better precision.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GameGPTï¼šæ¸¸æˆå¼€å‘çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„çªç ´ï¼Œå®ƒä»¬åœ¨è‡ªåŠ¨åŒ–å’ŒåŠ é€Ÿè½¯ä»¶å¼€å‘è¿‡ç¨‹æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨æ¸¸æˆå¼€å‘é¢†åŸŸï¼ŒLLMçš„åº”ç”¨é¢ä¸´ç€ä¸¤å¤§æŒ‘æˆ˜ï¼šå¹»è§‰å’Œå†—ä½™ã€‚å¹»è§‰æŒ‡çš„æ˜¯LLMåœ¨ç”Ÿæˆå†…å®¹æ—¶å¯èƒ½å‡ºç°çš„ä¸å‡†ç¡®æˆ–ä¸ç›¸å…³çš„æƒ…å†µï¼Œè€Œå†—ä½™åˆ™æ˜¯æŒ‡LLMå¯èƒ½ä¼šç”Ÿæˆä¸å¿…è¦çš„ä»»åŠ¡æˆ–ä»£ç ç‰‡æ®µã€‚è¿™äº›æŒ‘æˆ˜é™åˆ¶äº†LLMåœ¨æ¸¸æˆå¼€å‘ä¸­çš„å®é™…åº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶
GameGPTæå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LLMåœ¨æ¸¸æˆå¼€å‘ä¸­çš„å¹»è§‰å’Œå†—ä½™é—®é¢˜ã€‚è¯¥æ¡†æ¶ç”±å¤šä¸ªå…·æœ‰ä¸åŒè§’è‰²çš„æ™ºèƒ½ä½“ç»„æˆï¼ŒåŒ…æ‹¬æ¸¸æˆå†…å®¹è®¾è®¡å¸ˆã€æ¸¸æˆå¼€å‘ç»ç†ã€è®¡åˆ’å®¡æŸ¥å‘˜ã€æ¸¸æˆå¼€å‘å·¥ç¨‹å¸ˆã€ä»»åŠ¡å®¡æŸ¥å‘˜ã€æ¸¸æˆå¼•æ“å·¥ç¨‹å¸ˆã€ä»£ç å®¡æŸ¥å‘˜å’Œæ¸¸æˆå¼•æ“æµ‹è¯•å·¥ç¨‹å¸ˆã€‚è¿™äº›æ™ºèƒ½ä½“ååŒå·¥ä½œï¼Œå…±åŒå®Œæˆæ¸¸æˆå¼€å‘çš„å„ä¸ªé˜¶æ®µï¼Œä»è€Œæé«˜å¼€å‘æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŒé‡åä½œå’Œåˆ†å±‚æ–¹æ³•
ä¸ºäº†è§£å†³å¹»è§‰å’Œå†—ä½™é—®é¢˜ï¼ŒGameGPTé‡‡ç”¨äº†åŒé‡åä½œå’Œåˆ†å±‚æ–¹æ³•ã€‚åŒé‡åä½œåŒ…æ‹¬LLMä¸å°å‹ä¸“å®¶æ·±åº¦å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠæ‰§è¡Œè§’è‰²å’Œå®¡æŸ¥è§’è‰²ä¹‹é—´çš„åä½œã€‚åˆ†å±‚æ–¹æ³•åˆ™é€šè¿‡ä½¿ç”¨å¤šä¸ªå†…éƒ¨è¯å…¸æ¥æŒ‡å¯¼LLMçš„å†³ç­–è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘å¹»è§‰å’Œå†—ä½™çš„å‘ç”Ÿã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»£ç è§£è€¦
ä¸ºäº†æé«˜ä»£ç ç”Ÿæˆçš„ç²¾åº¦ï¼ŒGameGPTå¼•å…¥äº†ä»£ç è§£è€¦æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æ¸¸æˆè®¾è®¡è„šæœ¬åˆ†è§£æˆå¤šä¸ªå¯ç®¡ç†çš„ä»£ç ç‰‡æ®µï¼Œä»è€Œç®€åŒ–LLMçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å‡å°‘å¹»è§‰å’Œå†—ä½™çš„å‘ç”Ÿã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGameGPTæ¡†æ¶åœ¨æ¸¸æˆå¼€å‘è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå†³ç­–å’Œå†³ç­–ä¿®æ­£ï¼Œä»è€Œæé«˜å¼€å‘æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒGameGPTæ¡†æ¶è¿˜å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ç”¨äºå¼€å‘ä¸­åˆ°å¤§å‹çš„æ¸¸æˆé¡¹ç›®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GameGPTæ¡†æ¶ä¸ºæ¸¸æˆå¼€å‘è‡ªåŠ¨åŒ–æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚å…¶å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ã€åŒé‡åä½œå’Œåˆ†å±‚æ–¹æ³•ä»¥åŠä»£ç è§£è€¦æ–¹æ³•ç­‰åˆ›æ–°ç‚¹ï¼Œå¯ä»¥ä¸ºå…¶ä»–é¢†åŸŸçš„è½¯ä»¶å¼€å‘è‡ªåŠ¨åŒ–æä¾›å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒGameGPTæ¡†æ¶è¿˜å¯ä»¥ä¸å…¶ä»–AIæŠ€æœ¯ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜æ¸¸æˆå¼€å‘çš„è‡ªåŠ¨åŒ–æ°´å¹³ã€‚

## groot--learning-to-follow-instructions-by-watching-gameplay-videos
### Abstract
We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GROOTï¼šé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘å­¦ä¹ æŒ‡ä»¤éµå¾ª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ§åˆ¶å™¨é€šå¸¸åªèƒ½å®Œæˆé¢„å®šä¹‰çš„ã€æœ‰é™çš„ç¨‹åºæ€§ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘æ¥å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†ç›®æ ‡æŒ‡å®šä¸ºå‚è€ƒæ¸¸æˆè§†é¢‘ç‰‡æ®µï¼Œä»è€Œæä¾›ä¸°å¯Œçš„ç›®æ ‡è§„èŒƒï¼ŒåŒæ—¶æ¶ˆé™¤å¯¹æ˜‚è´µçš„æ–‡æœ¬-æ¸¸æˆæ³¨é‡Šçš„éœ€æ±‚ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥äº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶äº§ç”Ÿä¸€ä¸ªç›®æ ‡ç©ºé—´å’Œä¸€ä¸ªè§†é¢‘æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ï¼Œä»è€Œå®ç°ä»æ¸¸æˆè§†é¢‘ä¸­å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Minecraft SkillForgeåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGROOTåœ¨æ•´ä½“Eloè¯„åˆ†æ¯”è¾ƒä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå¹¶ä¸”åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„è·å–é’»çŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å­¦ä¹ æ¡†æ¶å’ŒGROOTä»£ç†çš„æ¶æ„è®¾è®¡ä¸ºæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†ç›®æ ‡ç©ºé—´å’Œæ§åˆ¶å™¨ç­–ç•¥çš„æ½œåœ¨åº”ç”¨ï¼Œä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å¤æ‚ä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## octopus--embodied-vision-language-programmer-from-environmental-feedback
### Abstract
Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. When integrated into an embodied agent,
existing embodied VLM works either output detailed action sequences at the
manipulation level or only provide plans at an abstract level, leaving a gap
between high-level planning and real-world manipulation. To bridge this gap, we
introduce Octopus, an embodied vision-language programmer that uses executable
code generation as a medium to connect planning and manipulation. Octopus is
designed to 1) proficiently comprehend an agent's visual and textual task
objectives, 2) formulate intricate action sequences, and 3) generate executable
code. To facilitate Octopus model development, we introduce OctoVerse: a suite
of environments tailored for benchmarking vision-based code generators on a
wide spectrum of tasks, ranging from mundane daily chores in simulators to
sophisticated interactions in complex video games such as Grand Theft Auto
(GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an
explorative agent that generates training data, i.e., action blueprints and
corresponding executable code. We also collect feedback that enables an
enhanced training scheme called Reinforcement Learning with Environmental
Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's
functionality and present compelling results, showing that the proposed RLEF
refines the agent's decision-making. By open-sourcing our simulation
environments, dataset, and model architecture, we aspire to ignite further
innovation and foster collaborative applications within the broader embodied AI
community.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Octopusï¼šåŸºäºç¯å¢ƒåé¦ˆçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°†å®ƒä»¬é›†æˆåˆ°å…·èº«æ™ºèƒ½ä½“ä¸­æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…·èº«VLMå·¥ä½œè¦ä¹ˆåœ¨æ“ä½œå±‚é¢è¾“å‡ºè¯¦ç»†çš„åŠ¨ä½œåºåˆ—ï¼Œè¦ä¹ˆä»…åœ¨æŠ½è±¡å±‚é¢æä¾›è®¡åˆ’ï¼Œå¯¼è‡´é«˜çº§è§„åˆ’å’Œç°å®ä¸–ç•Œæ“ä½œä¹‹é—´å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºOctopusçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹å™¨ï¼Œå®ƒä½¿ç”¨å¯æ‰§è¡Œä»£ç ç”Ÿæˆä½œä¸ºè¿æ¥è§„åˆ’å’Œæ“ä½œçš„åª’ä»‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šOctopusèƒ½å¤Ÿç†Ÿç»ƒåœ°ç†è§£æ™ºèƒ½ä½“çš„è§†è§‰å’Œæ–‡æœ¬ä»»åŠ¡ç›®æ ‡ï¼Œåˆ¶å®šå¤æ‚çš„åŠ¨ä½œåºåˆ—ï¼Œå¹¶ç”Ÿæˆå¯æ‰§è¡Œä»£ç ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸ºäº†ä¿ƒè¿›Octopusæ¨¡å‹çš„å‘å±•ï¼Œæœ¬æ–‡å¼•å…¥äº†OctoVerseï¼Œè¿™æ˜¯ä¸€å¥—ä¸ºåœ¨å„ç§ä»»åŠ¡ä¸Šè¯„ä¼°åŸºäºè§†è§‰çš„ä»£ç ç”Ÿæˆå™¨è€Œé‡èº«å®šåˆ¶çš„ç¯å¢ƒï¼ŒåŒ…æ‹¬ä»æ¨¡æ‹Ÿå™¨ä¸­çš„æ—¥å¸¸å®¶åŠ¡åˆ°å¤æ‚è§†é¢‘æ¸¸æˆï¼ˆå¦‚GTAå’ŒMinecraftï¼‰ä¸­çš„å¤æ‚äº¤äº’ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä¸ºäº†è®­ç»ƒOctopusï¼Œæœ¬æ–‡åˆ©ç”¨GPT-4æ§åˆ¶ä¸€ä¸ªæ¢ç´¢æ€§æ™ºèƒ½ä½“ï¼Œç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå³åŠ¨ä½œè“å›¾å’Œç›¸åº”çš„å¯æ‰§è¡Œä»£ç ã€‚åŒæ—¶ï¼Œæ”¶é›†åé¦ˆï¼Œä»¥å®ç°ä¸€ç§ç§°ä¸ºç¯å¢ƒåé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLEFï¼‰çš„å¢å¼ºè®­ç»ƒæ–¹æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæœ¬æ–‡å±•ç¤ºäº†Octopusçš„åŠŸèƒ½ï¼Œå¹¶å±•ç¤ºäº†ä»¤äººä¿¡æœçš„ç»“æœï¼Œè¡¨æ˜æ‰€æå‡ºçš„RLEFç»†åŒ–äº†æ™ºèƒ½ä½“çš„å†³ç­–ã€‚Octopusåœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ï¼Œåœ¨ä»»åŠ¡è§„åˆ’ã€ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œæ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚RLEFçš„é›†æˆè¿›ä¸€æ­¥å¢å¼ºäº†Octopusçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è¿™ç§è®­ç»ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Octopusæ¨¡å‹å’ŒOctoVerseç¯å¢ƒä¸ºå…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚Octopusæ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒè¿‡ç¨‹å¯ä»¥å€Ÿé‰´åˆ°å…¶ä»–å…·èº«æ™ºèƒ½ä½“ä¸­ï¼Œä»¥æé«˜å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„æ“ä½œèƒ½åŠ›ã€‚OctoVerseç¯å¢ƒå¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒçš„å…·èº«è§†è§‰-è¯­è¨€ç¼–ç¨‹æ¨¡å‹ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•ã€‚

## metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents
### Abstract
Significant advancements have occurred in the application of Large Language
Models (LLMs) for various tasks and social simulations. Despite this, their
capacities to coordinate within task-oriented social contexts are
under-explored. Such capabilities are crucial if LLMs are to effectively mimic
human-like social behavior and produce meaningful results. To bridge this gap,
we introduce collaborative generative agents, endowing LLM-based Agents with
consistent behavior patterns and task-solving abilities. We situate these
agents in a simulated job fair environment as a case study to scrutinize their
coordination skills. We propose a novel framework that equips collaborative
generative agents with human-like reasoning abilities and specialized skills.
Our evaluation demonstrates that these agents show promising performance.
However, we also uncover limitations that hinder their effectiveness in more
complex coordination tasks. Our work provides valuable insights into the role
and evolution of LLMs in task-oriented social simulations.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MetaAgentsï¼šåŸºäºLLMçš„ä»»åŠ¡å¯¼å‘åè°ƒçš„åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºå’Œæ‰§è¡Œä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºã€‚ç„¶è€Œï¼ŒLLMsåœ¨ä»»åŠ¡å¯¼å‘çš„ç¤¾ä¼šç¯å¢ƒä¸­çš„åè°ƒèƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†ä½¿LLMsèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿäººç±»çš„ç¤¾ä¼šè¡Œä¸ºå¹¶äº§ç”Ÿæœ‰æ„ä¹‰çš„ç»“æœï¼Œè¿™ç§èƒ½åŠ›è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼ˆCollaborative Generative Agentsï¼‰ï¼Œä¸ºåŸºäºLLMçš„æ™ºèƒ½ä½“èµ‹äºˆäº†ä¸€è‡´çš„è¡Œä¸ºæ¨¡å¼å’Œä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚ä¸ºäº†ç ”ç©¶è¿™äº›æ™ºèƒ½ä½“çš„åè°ƒèƒ½åŠ›ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ‹›è˜ä¼šç¯å¢ƒï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŒ…å«æ„ŸçŸ¥ã€è®°å¿†ã€æ¨ç†å’Œæ‰§è¡Œæ¨¡å—çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“å…·å¤‡ç±»ä¼¼äººç±»çš„æ¨ç†èƒ½åŠ›å’Œä¸“ä¸šæŠ€èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¨¡æ‹Ÿçš„æ‹›è˜ä¼šç¯å¢ƒä¸­ï¼Œåä½œç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨è¯†åˆ«åˆæ ¼æ±‚èŒè€…ã€è®¾è®¡å·¥ä½œæµç¨‹å’Œåˆ†é…è§’è‰²æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ‹›è˜ä¼šå¤æ‚æ€§çš„å¢åŠ ï¼Œæ™ºèƒ½ä½“åœ¨åè°ƒæ–¹é¢é‡åˆ°äº†æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦å½’å› äºLLMsçš„ç›®æ ‡æˆ–æ„å›¾ä¸åŒ¹é…ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åä½œç”Ÿæˆå¼æ™ºèƒ½ä½“æ¡†æ¶ä¸ºLLMsåœ¨ä»»åŠ¡å¯¼å‘çš„ç¤¾ä¼šæ¨¡æ‹Ÿä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚è¯¥æ¡†æ¶å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ‹›è˜ã€å›¢é˜Ÿåä½œå’Œç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ­ç¤ºäº†LLMsåœ¨åè°ƒä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## humanoid-agents--platform-for-simulating-human-like-generative-agents
### Abstract
Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents
### ğŸŒŸ è®ºæ–‡è§£è¯» | äººç±»åŒ–æ™ºèƒ½ä½“ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“å¹³å°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼ˆGenerative Agentsï¼‰çš„å‡ºç°ï¼Œäººä»¬å¼€å§‹å°è¯•ä½¿ç”¨é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿæ¥æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ä¸»è¦å…³æ³¨é€»è¾‘å’Œè®¡åˆ’ï¼Œç¼ºä¹å¯¹äººç±»ç›´è§‰å’Œå³æ—¶ååº”çš„æ¨¡æ‹Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†äººç±»åŒ–æ™ºèƒ½ä½“ï¼ˆHumanoid Agentsï¼‰å¹³å°ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦ç­‰å…ƒç´ ï¼Œä½¿æ™ºèƒ½ä½“æ›´æ¥è¿‘äººç±»çš„çœŸå®è¡Œä¸ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥ç³»ç»Ÿ1æ€ç»´
æœ¬æ–‡å€Ÿé‰´äº†å¿ƒç†å­¦ä¸­çš„ç³»ç»Ÿ1æ€ç»´ï¼Œå³ç›´è§‰ã€æ— æ„è¯†å’Œå³æ—¶çš„æ€ç»´è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦ç­‰å…ƒç´ ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ ¹æ®è‡ªèº«çŠ¶æ€å’Œç¯å¢ƒå˜åŒ–åšå‡ºæ›´è‡ªç„¶çš„ååº”ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€è°ƒæ•´è¡Œä¸º
äººç±»åŒ–æ™ºèƒ½ä½“å¹³å°å…è®¸æ™ºèƒ½ä½“æ ¹æ®è‡ªèº«çš„åŸºæœ¬éœ€æ±‚ã€æƒ…æ„Ÿå’Œå…³ç³»äº²å¯†åº¦åŠ¨æ€è°ƒæ•´å…¶æ—¥å¸¸æ´»åŠ¨å’Œå¯¹è¯ã€‚ä¾‹å¦‚ï¼Œå½“æ™ºèƒ½ä½“æ„Ÿåˆ°é¥¥é¥¿æ—¶ï¼Œå®ƒä¼šå¯»æ‰¾é£Ÿç‰©ï¼›å½“å®ƒæ„Ÿåˆ°å­¤ç‹¬æ—¶ï¼Œå®ƒä¼šå°è¯•ä¸å…¶ä»–æ™ºèƒ½ä½“è¿›è¡Œäº¤æµã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»åŒ–æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿäººç±»è¡Œä¸ºã€‚ä¸äººç±»æ ‡æ³¨ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹æ´»åŠ¨æ˜¯å¦æ»¡è¶³åŸºæœ¬éœ€æ±‚ã€æ´»åŠ¨è¡¨è¾¾çš„æƒ…æ„Ÿä»¥åŠå¯¹è¯æ˜¯å¦ä½¿æ™ºèƒ½ä½“ä¹‹é—´çš„å…³ç³»æ›´åŠ äº²å¯†ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Humanoid Agentså¹³å°ä¸ºç ”ç©¶äººç±»è¡Œä¸ºæä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚è¯¥å¹³å°å¯ä»¥æ‰©å±•åˆ°å„ç§åœºæ™¯ï¼Œå¹¶æ”¯æŒæ›´å¤šå½±å“äººç±»è¡Œä¸ºçš„å…ƒç´ ï¼Œå¦‚åŒç†å¿ƒã€é“å¾·ä»·å€¼è§‚å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥å¹³å°è¿˜æä¾›äº†Unity WebGLæ¸¸æˆç•Œé¢å’Œäº¤äº’å¼åˆ†æä»ªè¡¨æ¿ï¼Œæ–¹ä¾¿ç”¨æˆ·å¯è§†åŒ–æ™ºèƒ½ä½“çš„çŠ¶æ€å’Œè¡Œä¸ºã€‚

### ğŸŒ å¹³å°è®¿é—®
- å¹³å°ç½‘ç«™ï¼šhttps://www.humanoidagents.com/
- ä»£ç ä»“åº“ï¼šhttps://github.com/HumanoidAgents/HumanoidAgents

## avalonbench--evaluating-llms-playing-the-game-of-avalon
### Abstract
In this paper, we explore the potential of Large Language Models (LLMs)
Agents in playing the strategic social deduction game, Resistance Avalon.
Players in Avalon are challenged not only to make informed decisions based on
dynamically evolving game phases, but also to engage in discussions where they
must deceive, deduce, and negotiate with other players. These characteristics
make Avalon a compelling test-bed to study the decision-making and
language-processing capabilities of LLM Agents. To facilitate research in this
line, we introduce AvalonBench - a comprehensive game environment tailored for
evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game
environment for Avalon, (2) rule-based bots as baseline opponents, and (3)
ReAct-style LLM agents with tailored prompts for each role. Notably, our
evaluations based on AvalonBench highlight a clear capability gap. For
instance, models like ChatGPT playing good-role got a win rate of 22.2% against
rule-based bots playing evil, while good-role bot achieves 38.2% win rate in
the same setting. We envision AvalonBench could be a good test-bed for
developing more advanced LLMs (with self-playing) and agent frameworks that can
effectively model the layered complexities of such game environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AvalonBenchï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç¤¾äº¤æ¨ç†æ¸¸æˆå¦‚ Resistance Avalon å¯¹ç©å®¶çš„æ¨ç†ã€æ²Ÿé€šå’Œå†³ç­–èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚è¿™äº›æ¸¸æˆè¦æ±‚ç©å®¶åœ¨åŠ¨æ€å˜åŒ–çš„æ¸¸æˆé˜¶æ®µåšå‡ºæ˜æ™ºçš„å†³ç­–ï¼Œå¹¶åœ¨è®¨è®ºä¸­æ¬ºéª—ã€æ¨ç†å’Œä¸å…¶ä»–ç©å®¶åå•†ã€‚è¿™äº›ç‰¹ç‚¹ä½¿å¾— Avalon æˆä¸ºç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å†³ç­–å’Œè¯­è¨€å¤„ç†èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„è¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†æ¸¸æˆç¯å¢ƒä¸­çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº† AvalonBenchï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šä»£ç† LLM ä»£ç†çš„æ¸¸æˆç¯å¢ƒã€‚AvalonBench åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š

1. **Avalon æ¸¸æˆç¯å¢ƒ**ï¼šä¸ºä»£ç†æä¾›æ¸¸æˆå¹³å°ï¼Œè®°å½•æ‰€æœ‰ç©å®¶çš„è¡ŒåŠ¨å¹¶æ¨åŠ¨æ¸¸æˆè¿›ç¨‹ã€‚
2. **åŸºäºè§„åˆ™çš„æœºå™¨äºº**ï¼šä½œä¸ºåŸºçº¿å¯¹æ‰‹ï¼Œä¸ºä»£ç†æä¾›å¯æ¯”è¾ƒçš„åŸºå‡†ã€‚
3. **ReAct é£æ ¼çš„ LLM ä»£ç†**ï¼šé’ˆå¯¹æ¯ä¸ªè§’è‰²å®šåˆ¶æç¤ºï¼Œä»¥è¯„ä¼° LLM ä»£ç†åœ¨ä¸åŒè§’è‰²ä¸‹çš„è¡¨ç°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ä½¿ç”¨ AvalonBench å¯¹ ChatGPT-3.5 å’Œ Llama2 æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸åŸºäºè§„åˆ™çš„æœºå™¨äººè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ‰è®¨è®ºçš„æƒ…å†µä¸‹ï¼ŒLLM ä»£ç†çš„è¡¨ç°ä¹Ÿè¿œä½äºåŸºäºè§„åˆ™çš„æœºå™¨äººã€‚ä¾‹å¦‚ï¼ŒChatGPT-3.5 åœ¨æ‰®æ¼”å¥½äººè§’è‰²æ—¶ï¼Œåœ¨ä¸æ‰®æ¼”åäººçš„åŸºäºè§„åˆ™çš„æœºå™¨äººå¯¹æŠ—ä¸­ï¼Œèƒœç‡ä¸º 22.2%ï¼Œè€Œå¥½äººè§’è‰²çš„æœºå™¨äººèƒœç‡ä¸º 38.2%ã€‚è¿™è¡¨æ˜å½“å‰ LLM ä»£ç†åœ¨æ¨ç†ã€è¯´æœã€åå•†å’Œæ¬ºéª—èƒ½åŠ›æ–¹é¢å­˜åœ¨æ˜æ˜¾å·®è·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AvalonBench ä¸ºç ”ç©¶ LLM ä»£ç†åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å…ˆè¿›çš„ LLM ä»£ç†ï¼Œå¹¶æ¢ç´¢å¦‚ä½•å°†å†³ç­–æŠ€æœ¯é›†æˆåˆ° LLM ä¸­ï¼Œä»¥æé«˜å…¶åœ¨å¤æ‚æ¸¸æˆç¯å¢ƒä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒAvalonBench è¿˜å¯ä»¥ç”¨äºè¯„ä¼° LLM ä»£ç†åœ¨å¤šä»£ç†åä½œã€æ²Ÿé€šå’Œç­–ç•¥åˆ¶å®šæ–¹é¢çš„èƒ½åŠ›ã€‚

## beyond-win-rates--a-clustering-based-approach-to-character-balance-analysis-in-team-based-games
### Abstract
Character diversity in competitive games, while enriching gameplay, often
introduces balance challenges that can negatively impact player experience and
strategic depth. Traditional balance assessments rely on aggregate metrics like
win rates and pick rates, which offer limited insight into the intricate
dynamics of team-based games and nuanced character roles. This paper proposes a
novel clustering-based methodology to analyze character balance, leveraging
in-game data from Valorant to account for team composition influences and
reveal latent character roles. By applying hierarchical agglomerative
clustering with Jensen-Shannon Divergence to professional match data from the
Valorant Champions Tour 2022, our approach identifies distinct clusters of
agents exhibiting similar co-occurrence patterns within team compositions. This
method not only complements existing quantitative metrics but also provides a
more holistic and interpretable perspective on character synergies and
potential imbalances, offering game developers a valuable tool for informed and
context-aware balance adjustments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºèšç±»çš„å›¢é˜Ÿæ¸¸æˆè§’è‰²å¹³è¡¡åˆ†ææ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å›¢é˜Ÿç«æŠ€æ¸¸æˆä¸­ï¼Œè§’è‰²å¤šæ ·æ€§è™½ç„¶ä¸°å¯Œäº†æ¸¸æˆç©æ³•ï¼Œä½†ä¹Ÿå¸¦æ¥äº†å¹³è¡¡æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¹³è¡¡è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºèƒœç‡å’Œé€‰æ‹©ç‡ç­‰èšåˆæŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡æ— æ³•å…¨é¢åæ˜ å›¢é˜Ÿæ¸¸æˆçš„å¤æ‚åŠ¨æ€å’Œè§’è‰²ä¹‹é—´çš„å¾®å¦™å…³ç³»ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„è§’è‰²å¹³è¡¡åˆ†ææ–¹æ³•ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°ç†è§£è§’è‰²ä¹‹é—´çš„ååŒä½œç”¨å’Œæ½œåœ¨çš„ä¸å¹³è¡¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šèšç±»åˆ†æ
æœ¬æ–‡ä½¿ç”¨å±‚æ¬¡å‡èšèšç±»ç®—æ³•ï¼Œç»“åˆJensen-Shannonæ•£åº¦ä½œä¸ºè·ç¦»åº¦é‡ï¼Œå¯¹è§’è‰²ä¹‹é—´çš„å…³ç³»è¿›è¡Œåˆ†æã€‚é€šè¿‡åˆ†æè§’è‰²åœ¨å›¢é˜Ÿä¸­çš„å…±ç°æ¨¡å¼ï¼Œå°†è§’è‰²åˆ†ä¸ºå…·æœ‰ç›¸ä¼¼åŠŸèƒ½çš„ç»„ï¼Œä»è€Œæ­ç¤ºæ½œåœ¨çš„è§’è‰²ç±»å‹å’ŒååŒä½œç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šJensen-Shannonæ•£åº¦
Jensen-Shannonæ•£åº¦æ˜¯ä¸€ç§å¯¹ç§°çš„åº¦é‡ï¼Œç”¨äºé‡åŒ–ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œå®ƒè¢«ç”¨æ¥è¡¡é‡è§’è‰²ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œæ›´å‡†ç¡®åœ°åæ˜ è§’è‰²åœ¨å›¢é˜Ÿä¸­çš„åŠŸèƒ½è§’è‰²ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¯¹ã€ŠValorantã€‹2022å¹´å† å†›å·¡å›èµ›çš„ä¸“ä¸šæ¯”èµ›æ•°æ®è¿›è¡Œèšç±»åˆ†æï¼Œæœ¬æ–‡å‘ç°äº†ä¸€äº›å…·æœ‰ç›¸ä¼¼åŠŸèƒ½çš„è§’è‰²ç»„ï¼Œä¾‹å¦‚æ§åˆ¶è€…ã€å“¨å…µã€å†³æ–—è€…å’Œå‘èµ·è€…ã€‚è¿™äº›ç»“æœä¸æ¸¸æˆä¸­çš„è§’è‰²åˆ†ç±»ç›¸å»åˆï¼Œä½†ä¹Ÿæ­ç¤ºäº†ä¸€äº›æ–°çš„è§’è‰²ç±»å‹å’ŒååŒä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºèšç±»çš„è§’è‰²å¹³è¡¡åˆ†ææ–¹æ³•ä¸ºæ¸¸æˆå¼€å‘è€…æä¾›äº†ä¸€ç§æ–°çš„è§†è§’ï¼Œå¯ä»¥å¸®åŠ©ä»–ä»¬æ›´å…¨é¢åœ°ç†è§£è§’è‰²ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åšå‡ºæ›´æ˜æ™ºçš„å¹³è¡¡è°ƒæ•´ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¯„ä¼°è§’è‰²è°ƒæ•´å¯¹æ¸¸æˆå¹³è¡¡çš„å½±å“ï¼Œä»è€Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç»´æŠ¤æ¸¸æˆçš„å…¬å¹³æ€§å’Œå¯ç©æ€§ã€‚

## llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models
### Abstract
The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by
Large Language Models (LLMs) make them promising candidates for developing
coordination agents. In this study, we introduce a new LLM-Coordination
Benchmark aimed at a detailed analysis of LLMs within the context of Pure
Coordination Games, where participating agents need to cooperate for the most
gain. This benchmark evaluates LLMs through two distinct tasks: (1)
\emph{Agentic Coordination}, where LLMs act as proactive participants for
cooperation in 4 pure coordination games; (2) \emph{Coordination Question
Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice
questions from the 4 games for evaluation of three key reasoning abilities:
Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to
enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture
for Coordination (CAC) framework that can easily integrate different LLMs as
plug-and-play modules for pure coordination games. Our findings indicate that
LLM agents equipped with GPT-4-turbo achieve comparable performance to
state-of-the-art reinforcement learning methods in games that require
commonsense actions based on the environment. Besides, zero-shot coordination
experiments reveal that, unlike RL methods, LLM agents are robust to new unseen
partners. However, results on Coordination QA show a large room for improvement
in the Theory of Mind reasoning and joint planning abilities of LLMs. The
analysis also sheds light on how the ability of LLMs to understand their
environment and their partner's beliefs and intentions plays a part in their
ability to plan for coordination. Our code is available at
\url{https://github.com/eric-ai-lab/llm_coordination}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LLM-Coordinationï¼šè¯„ä¼°å’Œåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è®¸å¤šæ—¥å¸¸ä»»åŠ¡å’Œå…³é”®æ“ä½œä¸­ï¼Œå¦‚çƒ¹é¥ªå’Œæ•‘æ´è¡ŒåŠ¨ï¼Œåˆä½œæ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™äº›åœºæ™¯å¯ä»¥è¢«è§†ä¸ºçº¯åè°ƒæ¸¸æˆï¼Œå…¶ä¸­æ‰€æœ‰å‚ä¸æ–¹éƒ½ä»é€‰æ‹©å®Œå…¨ä¸€è‡´çš„æˆ˜ç•¥ä¸­å—ç›Šï¼Œé¿å…ä»»ä½•åˆ©ç›Šå†²çªã€‚è¿™äº›æ¸¸æˆè¦æ±‚ä»£ç†æ¨ç†ä»–ä»¬çš„ç¯å¢ƒå¹¶è®¡åˆ’ï¼ŒåŒæ—¶è€ƒè™‘ä»–ä»¬çš„ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ€è¿‘åœ¨ç‰©ç†å’Œè™šæ‹Ÿç¯å¢ƒä¸­çš„æ¶Œç°è§„åˆ’èƒ½åŠ›ã€ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›å’Œå¯¹å¿ƒæ™ºç†è®ºçš„æš—ç¤ºï¼Œä½¿å®ƒä»¬æˆä¸ºå¼€å‘åè°ƒä»£ç†çš„æœ‰å¸Œæœ›çš„å€™é€‰è€…ã€‚ç„¶è€Œï¼ŒLLMsåœ¨åè°ƒæ¸¸æˆä¸­çš„å¿…è¦æ¡ä»¶ã€ä¼˜åŠ¿å’Œå±€é™æ€§ä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è¿›è¡ŒLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›çš„å…¨é¢è¯„ä¼°å’Œåˆ†ææ¥å¼¥åˆè¿™ä¸€å·®è·ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„LLM-CoordinationåŸºå‡†ï¼Œæ—¨åœ¨å¯¹LLMsåœ¨çº¯åè°ƒæ¸¸æˆä¸­çš„èƒ½åŠ›è¿›è¡Œè¯¦ç»†åˆ†æã€‚è¯¥åŸºå‡†é€šè¿‡ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡è¯„ä¼°LLMsï¼š
1. **ä»£ç†åè°ƒ**ï¼šLLMsä½œä¸ºç§¯æåˆä½œå‚ä¸è€…å‚ä¸4ä¸ªçº¯åè°ƒæ¸¸æˆã€‚
2. **åè°ƒé—®ç­”ï¼ˆQAï¼‰**ï¼šLLMsè¢«æç¤ºå›ç­”æ¥è‡ª4ä¸ªæ¸¸æˆçš„198ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œä»¥è¯„ä¼°ä¸‰ä¸ªå…³é”®æ¨ç†èƒ½åŠ›ï¼šç¯å¢ƒç†è§£ã€å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’ã€‚

æ­¤å¤–ï¼Œä¸ºäº†ä½¿LLMsèƒ½å¤Ÿè¿›è¡Œå¤šæ™ºèƒ½ä½“åè°ƒï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåè°ƒè®¤çŸ¥æ¶æ„ï¼ˆCACï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è½»æ¾åœ°å°†ä¸åŒçš„LLMsä½œä¸ºå³æ’å³ç”¨æ¨¡å—é›†æˆåˆ°çº¯åè°ƒæ¸¸æˆä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œé…å¤‡GPT-4-turboçš„LLMä»£ç†åœ¨éœ€è¦åŸºäºç¯å¢ƒçš„å¸¸è¯†è¡ŒåŠ¨çš„æ¸¸æˆä¸­ï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œé›¶æ ·æœ¬åè°ƒå®éªŒè¡¨æ˜ï¼Œä¸RLæ–¹æ³•ä¸åŒï¼ŒLLMä»£ç†å¯¹æ–°æœªè§ä¼™ä¼´å…·æœ‰é²æ£’æ€§ã€‚ç„¶è€Œï¼Œåè°ƒQAçš„ç»“æœè¡¨æ˜ï¼ŒLLMsçš„å¿ƒæ™ºç†è®ºæ¨ç†å’Œè”åˆè§„åˆ’èƒ½åŠ›è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚åˆ†æè¿˜æ­ç¤ºäº†LLMsç†è§£å…¶ç¯å¢ƒå’Œå…¶ä¼™ä¼´çš„ä¿¡å¿µå’Œæ„å›¾çš„èƒ½åŠ›å¦‚ä½•å½±å“å®ƒä»¬åè°ƒè®¡åˆ’çš„èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„LLM-CoordinationåŸºå‡†å’ŒCACæ¡†æ¶ä¸ºè¯„ä¼°å’Œåˆ†æLLMsçš„å¤šæ™ºèƒ½ä½“åè°ƒèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç»“æœçªå‡ºäº†LLMsåœ¨åè°ƒä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions
### Abstract
Highly autonomous generative agents powered by large language models promise
to simulate intricate social behaviors in virtual societies. However, achieving
real-time interactions with humans at a low computational cost remains
challenging. Here, we introduce Lyfe Agents. They combine low-cost with
real-time responsiveness, all while remaining intelligent and goal-oriented.
Key innovations include: (1) an option-action framework, reducing the cost of
high-level decisions; (2) asynchronous self-monitoring for better
self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing
critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation
and sociability across several multi-agent scenarios in our custom LyfeGame 3D
virtual environment platform. When equipped with our brain-inspired techniques,
Lyfe Agents can exhibit human-like self-motivated social reasoning. For
example, the agents can solve a crime (a murder mystery) through autonomous
collaboration and information exchange. Meanwhile, our techniques enabled Lyfe
Agents to operate at a computational cost 10-100 times lower than existing
alternatives. Our findings underscore the transformative potential of
autonomous generative agents to enrich human social experiences in virtual
worlds.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä½æˆæœ¬å®æ—¶ç¤¾äº¤äº’åŠ¨çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼šLyfe Agents

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨è™šæ‹Ÿç¤¾ä¼šä¸­æ¨¡æ‹Ÿå¤æ‚ç¤¾äº¤è¡Œä¸ºçš„å‰æ™¯ä¹Ÿå˜å¾—å…‰æ˜ã€‚ç„¶è€Œï¼Œå®ç°ä¸äººç±»åœ¨ä½è®¡ç®—æˆæœ¬ä¸‹çš„å®æ—¶äº’åŠ¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨åˆ›å»ºä¸€ç§æ—¢æ™ºèƒ½åˆè‡ªä¸»çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨ä½è®¡ç®—æˆæœ¬ä¸‹å®ç°ä¸äººç±»çš„å®æ—¶äº’åŠ¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€‰é¡¹-åŠ¨ä½œæ¡†æ¶
ä¸ºäº†å‡å°‘é«˜çº§å†³ç­–çš„æˆæœ¬ï¼ŒLyfe Agents é‡‡ç”¨äº†ä¸€ç§é€‰é¡¹-åŠ¨ä½œæ¡†æ¶ã€‚åœ¨è¿™ç§æ¡†æ¶ä¸­ï¼Œæ™ºèƒ½ä½“é¦–å…ˆé€‰æ‹©ä¸€ä¸ªé«˜çº§åŠ¨ä½œï¼ˆæˆ–â€œé€‰é¡¹â€ï¼‰ï¼Œç„¶ååœ¨åç»­æ­¥éª¤ä¸­åœ¨è¯¥é€‰é¡¹å†…é€‰æ‹©ä½çº§åŠ¨ä½œã€‚è¿™ç§è®¾è®¡å…è®¸æ™ºèƒ½ä½“åœ¨æ›´é•¿æ—¶é—´å†…ä¸“æ³¨äºæ‰§è¡Œé€‰é¡¹èƒŒåçš„æ„å›¾ï¼Œä»è€Œé™ä½æˆæœ¬å¹¶æé«˜æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼‚æ­¥è‡ªæˆ‘ç›‘æ§
ä¸ºäº†æé«˜æ™ºèƒ½ä½“çš„æƒ…å¢ƒæ„è¯†å’Œç›®æ ‡åšæŒæ€§ï¼ŒLyfe Agents å¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘ç›‘æ§æ¨¡å—ã€‚è¯¥æ¨¡å—ç»´æŠ¤ä¸€ä¸ªå…³äºæœ€è¿‘äº‹ä»¶çš„å™äº‹é£æ ¼æ‘˜è¦ï¼Œå¹¶å¼ºè°ƒæ–°é¢–å’Œä¸ç›®æ ‡ç›¸å…³çš„å†…å®¹ã€‚è¿™ç§è‡ªæˆ‘ç›‘æ§æ‘˜è¦å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£æƒ…å¢ƒï¼Œå¹¶ä½¿å…¶è¡Œä¸ºæ›´åŠ ä¸€è‡´å’Œç¬¦åˆç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ€»ç»“-é—å¿˜è®°å¿†æœºåˆ¶
ä¸ºäº†æé«˜è®°å¿†å­˜å‚¨å’Œæ£€ç´¢çš„è´¨é‡ï¼ŒLyfe Agents é‡‡ç”¨äº†ä¸€ç§å±‚æ¬¡åŒ–çš„è®°å¿†æ¶æ„å’Œæ€»ç»“-é—å¿˜ï¼ˆSaFï¼‰æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°†è®°å¿†åˆ†ä¸ºçŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†ï¼Œå¹¶é€šè¿‡èšç±»å’Œæ€»ç»“æŠ€æœ¯å°†çŸ­æœŸè®°å¿†ä¸­çš„ä¿¡æ¯è½¬ç§»åˆ°é•¿æœŸè®°å¿†ä¸­ã€‚æ­¤å¤–ï¼Œé—å¿˜ç®—æ³•ä¼šè¯„ä¼°å¹¶åˆ é™¤ä¸ç°æœ‰è®°å¿†é«˜åº¦ç›¸ä¼¼çš„æ—§è®°å¿†ï¼Œä»¥ç¡®ä¿å­˜å‚¨çš„ä¿¡æ¯æ˜¯ç‹¬ç‰¹å’Œç›¸å…³çš„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨è‡ªå®šä¹‰çš„ LyfeGame 3D è™šæ‹Ÿç¯å¢ƒå¹³å°ä¸Šï¼ŒLyfe Agents åœ¨å¤šä¸ªå¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­å±•ç¤ºäº†å…¶è‡ªæˆ‘æ¿€åŠ±å’Œç¤¾ä¼šæ€§ã€‚ä¾‹å¦‚ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è‡ªä¸»åä½œå’Œä¿¡æ¯äº¤æ¢è§£å†³çŠ¯ç½ªï¼ˆè°‹æ€è°œæ¡ˆï¼‰ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLyfe Agents çš„è®¡ç®—æˆæœ¬é™ä½äº† 10-100 å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Lyfe Agents çš„è®¾è®¡åŸåˆ™å’Œæ¶æ„ç»„ä»¶ä¸ºæ„å»ºä½æˆæœ¬ã€å®æ—¶å“åº”çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚å…¶é€‰é¡¹-åŠ¨ä½œæ¡†æ¶ã€å¼‚æ­¥è‡ªæˆ‘ç›‘æ§å’Œæ€»ç»“-é—å¿˜è®°å¿†æœºåˆ¶ç­‰åˆ›æ–°ç‚¹å¯ä»¥åº”ç”¨äºå…¶ä»–ç”Ÿæˆå¼æ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥æé«˜å…¶è‡ªä¸»æ€§å’Œç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLyfeGame è™šæ‹Ÿç¯å¢ƒå¹³å°ä¹Ÿä¸ºç ”ç©¶ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„ç¤¾ä¼šè¡Œä¸ºå’Œç”¨æˆ·äº¤äº’æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚

## adaptive-multi-goal-exploration
### Abstract
We introduce a generic strategy for provably efficient multi-goal
exploration. It relies on AdaGoal, a novel goal selection scheme that leverages
a measure of uncertainty in reaching states to adaptively target goals that are
neither too difficult nor too easy. We show how AdaGoal can be used to tackle
the objective of learning an $\epsilon$-optimal goal-conditioned policy for the
(initially unknown) set of goal states that are reachable within $L$ steps in
expectation from a reference state $s_0$ in a reward-free Markov decision
process. In the tabular case with $S$ states and $A$ actions, our algorithm
requires $\tilde{O}(L^3 S A \epsilon^{-2})$ exploration steps, which is nearly
minimax optimal. We also readily instantiate AdaGoal in linear mixture Markov
decision processes, yielding the first goal-oriented PAC guarantee with linear
function approximation. Beyond its strong theoretical guarantees, we anchor
AdaGoal in goal-conditioned deep reinforcement learning, both conceptually and
empirically, by connecting its idea of selecting "uncertain" goals to
maximizing value ensemble disagreement.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªé€‚åº”å¤šç›®æ ‡æ¢ç´¢ï¼šAdaGoalç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå½“å¤–éƒ¨å¥–åŠ±ä¿¡å·ç¼ºå¤±æˆ–ä¸å…·ä¿¡æ¯æ€§æ—¶ï¼Œæ™ºèƒ½ä½“éœ€è¦é€šè¿‡æ¢ç´¢ç¯å¢ƒæ¥å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æœ€å¤§åŒ–å¥–åŠ±ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“éœ€è¦è‡ªä¸»è®¾å®šç›®æ ‡å¹¶å­¦ä¹ æœ‰æ•ˆåœ°è¾¾åˆ°è¿™äº›ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGC-RLï¼‰æ–¹æ³•å¾€å¾€ç¼ºä¹ç†è®ºæ”¯æŒå’Œä¿è¯ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦å­¦ä¹ ç¯å¢ƒä¸­ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šç›®æ ‡æ¢ç´¢ï¼ˆMGEï¼‰ç›®æ ‡
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„ç­–ç•¥ï¼Œç”¨äºè¯æ˜æœ‰æ•ˆçš„å¤šç›®æ ‡æ¢ç´¢ã€‚è¯¥ç­–ç•¥ä¾èµ–äºAdaGoalï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç›®æ ‡é€‰æ‹©æ–¹æ¡ˆï¼Œå®ƒåˆ©ç”¨åˆ°è¾¾çŠ¶æ€çš„ä¸ç¡®å®šæ€§åº¦é‡æ¥é€‚åº”æ€§åœ°é€‰æ‹©æ—¢ä¸å¤ªéš¾ä¹Ÿä¸å¤ªå®¹æ˜“çš„ç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAdaGoalç›®æ ‡é€‰æ‹©æ–¹æ¡ˆ
AdaGoalé€šè¿‡è§£å†³ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜æ¥é€‚åº”æ€§åœ°é€‰æ‹©ä¸­ç­‰éš¾åº¦çš„ç›®æ ‡çŠ¶æ€ã€‚å®ƒè¿˜æä¾›äº†ä¸€ä¸ªç®—æ³•åœæ­¢è§„åˆ™å’Œä¸€ä¸ªå€™é€‰ç›®æ ‡çŠ¶æ€é›†ï¼Œæ™ºèƒ½ä½“å¯¹è¿™äº›çŠ¶æ€æœ‰ä¿¡å¿ƒå¯ä»¥å¯é åœ°åˆ°è¾¾ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAdaGoal-UCBVIå’ŒAdaGoal-UCRLÂ·VTRç®—æ³•
æœ¬æ–‡è®¾è®¡äº†AdaGoal-UCBVIç®—æ³•ï¼Œç”¨äºåœ¨è¡¨æ ¼MDPä¸­è§£å†³MGEé—®é¢˜ï¼Œå¹¶è¯æ˜äº†å®ƒå‡ ä¹æ˜¯æœ€ä¼˜çš„æ ·æœ¬å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†AdaGoal-UCRLÂ·VTRç®—æ³•ï¼Œç”¨äºçº¿æ€§æ··åˆMDPï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰çº¿æ€§å‡½æ•°è¿‘ä¼¼çš„é¢å‘ç›®æ ‡çš„PACä¿è¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šAdaGoalåœ¨æ·±åº¦GC-RLä¸­çš„åº”ç”¨
æœ¬æ–‡å°†AdaGoalçš„æ¦‚å¿µä¸æ·±åº¦GC-RLä¸­çš„æœ€å¤§åŒ–ä»·å€¼é›†åˆåˆ†æ­§è”ç³»èµ·æ¥ï¼Œä»è€Œåœ¨æ¦‚å¿µå’Œç»éªŒä¸Šå°†å…¶é”šå®šåœ¨ç›®æ ‡æ¡ä»¶æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒä¸­ï¼ŒAdaGoal-UCBVIå’ŒAdaGoal-UCRLÂ·VTRç®—æ³•åœ¨è¡¨æ ¼MDPå’Œçº¿æ€§æ··åˆMDPä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„AdaGoalç­–ç•¥ä¸ºæ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

*   **è‡ªé€‚åº”ç›®æ ‡é€‰æ‹©**ï¼šAdaGoalé€šè¿‡è€ƒè™‘ç›®æ ‡çŠ¶æ€çš„ä¸ç¡®å®šæ€§æ¥é€‰æ‹©ç›®æ ‡ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æ¢ç´¢ç¯å¢ƒã€‚
*   **ç†è®ºä¿è¯**ï¼šAdaGoal-UCBVIå’ŒAdaGoal-UCRLÂ·VTRç®—æ³•å…·æœ‰å‡ ä¹æœ€ä¼˜çš„æ ·æœ¬å¤æ‚åº¦ï¼Œä¸ºæ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æä¾›äº†ç†è®ºæ”¯æŒã€‚
*   **æ·±åº¦å­¦ä¹ åº”ç”¨**ï¼šAdaGoalçš„æ¦‚å¿µå¯ä»¥ä¸æ·±åº¦å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œä»è€Œåœ¨å¤æ‚çš„ä»»åŠ¡ä¸­å®ç°æœ‰æ•ˆçš„ç›®æ ‡æ¢ç´¢ã€‚

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡æå‡ºçš„AdaGoalç­–ç•¥ä¸ºæ— ç›‘ç£ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚AdaGoalç­–ç•¥å…·æœ‰è‡ªé€‚åº”ç›®æ ‡é€‰æ‹©ã€ç†è®ºä¿è¯å’Œæ·±åº¦å­¦ä¹ åº”ç”¨ç­‰ä¼˜ç‚¹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## smartplay--a-benchmark-for-llms-as-intelligent-agents
### Abstract
Recent large language models (LLMs) have demonstrated great potential toward
intelligent agents and next-gen automation, but there currently lacks a
systematic benchmark for evaluating LLMs' abilities as agents. We introduce
SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs
as agents. SmartPlay consists of 6 different games, including
Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique
setting, providing up to 20 evaluation settings and infinite environment
variations. Each game in SmartPlay uniquely challenges a subset of 9 important
capabilities of an intelligent LLM agent, including reasoning with object
dependencies, planning ahead, spatial reasoning, learning from history, and
understanding randomness. The distinction between the set of capabilities each
game test allows us to analyze each capability separately. SmartPlay serves not
only as a rigorous testing ground for evaluating the overall performance of LLM
agents but also as a road-map for identifying gaps in current methodologies. We
release our benchmark at github.com/Microsoft/SmartPlay
### ğŸŒŸ è®ºæ–‡è§£è¯» | SmartPlayï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“èƒ½åŠ›çš„åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ™ºèƒ½ä½“å’Œä¸‹ä¸€ä»£è‡ªåŠ¨åŒ–æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªç³»ç»Ÿæ€§çš„åŸºå‡†æ¥è¯„ä¼°LLMsä½œä¸ºæ™ºèƒ½ä½“çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†SmartPlayï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†å’Œè¯„ä¼°LLMsä½œä¸ºæ™ºèƒ½ä½“çš„æ–¹æ³•è®ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSmartPlayç”±6ä¸ªä¸åŒçš„æ¸¸æˆç»„æˆï¼ŒåŒ…æ‹¬å‰ªåˆ€çŸ³å¤´å¸ƒã€æ±‰è¯ºå¡”ã€Minecraftç­‰ã€‚æ¯ä¸ªæ¸¸æˆéƒ½æœ‰ç‹¬ç‰¹çš„è®¾ç½®ï¼Œæä¾›å¤šè¾¾20ä¸ªè¯„ä¼°è®¾ç½®å’Œæ— é™çš„ç¯å¢ƒå˜åŒ–ã€‚æ¯ä¸ªæ¸¸æˆéƒ½æŒ‘æˆ˜äº†æ™ºèƒ½LLMæ™ºèƒ½ä½“çš„9ä¸ªé‡è¦èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨ç†å¯¹è±¡ä¾èµ–å…³ç³»ã€å‰ç»æ€§è§„åˆ’ã€ç©ºé—´æ¨ç†ã€ä»å†å²ä¸­å­¦ä¹ ä»¥åŠç†è§£éšæœºæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šSmartPlayä¸ä»…æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æµ‹è¯•åœºï¼Œç”¨äºè¯„ä¼°LLMæ™ºèƒ½ä½“çš„æ•´ä½“æ€§èƒ½ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªè·¯çº¿å›¾ï¼Œç”¨äºè¯†åˆ«å½“å‰æ–¹æ³•ä¸­çš„å·®è·ã€‚SmartPlayä¸ºLLMsæä¾›äº†ç»Ÿä¸€å’Œå¯æ‰©å±•çš„APIï¼Œå…·æœ‰æ–‡æœ¬è§‚å¯Ÿå’ŒæŒ‡å¯¼ï¼Œä»¥æ‰§è¡Œå›åˆåˆ¶çš„LLMæ¨ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4å˜ä½“åœ¨æ‰€æœ‰æ¸¸æˆä¸­éƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–LLMsï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸äººç±»åŸºçº¿æ€§èƒ½ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å…¶ä»–ä¸“æœ‰LLMsåœ¨Crafterç­‰ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­éš¾ä»¥ä¸GPT-4ç«äº‰ã€‚å¼€æºLLMsåœ¨ç®€å•ä»»åŠ¡å’Œæ›´å…·æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¸å¦‚GPT-4å˜ä½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SmartPlayä¸ºè¯„ä¼°LLMsä½œä¸ºæ™ºèƒ½ä½“çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ã€‚å®ƒä¸ä»…æ¶µç›–äº†åŸºæœ¬çš„æŒ‡ä»¤éµå¾ªå’Œä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œè¿˜è¯„ä¼°äº†è§„åˆ’ã€ç†è§£éšæœºæ€§ã€2D/3Dç©ºé—´æ¨ç†å’Œé”™è¯¯å¤„ç†ç­‰èƒ½åŠ›ã€‚SmartPlayçš„å‘å¸ƒå°†æ¨åŠ¨LLMsä½œä¸ºæ™ºèƒ½ä½“é¢†åŸŸçš„ç ”ç©¶ï¼Œå¹¶ä¸ºæ„å»ºæ›´å¼ºå¤§å’Œå¯é çš„LLMæ™ºèƒ½ä½“æä¾›æŒ‡å¯¼ã€‚

## avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation
### Abstract
Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½œä¸ºæ™ºèƒ½ä½“ï¼ˆLLM-as-Agentï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶é€šå¸¸å‡è®¾LLMså¤„ç†çš„ä¿¡æ¯å§‹ç»ˆæ˜¯è¯šå®çš„ï¼Œå¿½ç•¥äº†äººç±»ç¤¾ä¼šä¸­æ™®éå­˜åœ¨çš„æ¬ºéª—æ€§æˆ–è¯¯å¯¼æ€§ä¿¡æ¯ä»¥åŠAIç”Ÿæˆå†…å®¹ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚è¿™ç§å‡è®¾ä½¿å¾—LLMså®¹æ˜“å—åˆ°æ¶æ„æ“çºµï¼Œå¯èƒ½å¯¼è‡´ä¸è‰¯åæœã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMsåœ¨æ¬ºéª—æ€§ç¯å¢ƒä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œé€’å½’æ²‰æ€â€ï¼ˆReConï¼‰çš„æ–°æ¡†æ¶ï¼Œä»¥å¢å¼ºLLMsè¯†åˆ«å’Œå¯¹æŠ—æ¬ºéª—æ€§ä¿¡æ¯çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé€’å½’æ²‰æ€æ¡†æ¶ï¼ˆReConï¼‰
ReConæ¡†æ¶ç»“åˆäº†â€œæ„æ€æ²‰æ€â€å’Œâ€œç²¾ç‚¼æ²‰æ€â€ä¸¤ä¸ªè®¤çŸ¥è¿‡ç¨‹ã€‚æ„æ€æ²‰æ€äº§ç”Ÿåˆå§‹æ€è€ƒå’Œè¨€è¯­ï¼Œè€Œç²¾ç‚¼æ²‰æ€åˆ™è¿›ä¸€æ­¥æ”¹è¿›å®ƒä»¬ã€‚æ­¤å¤–ï¼ŒReConè¿˜å¼•å…¥äº†ç¬¬ä¸€é˜¶å’Œç¬¬äºŒé˜¶è§†è§’è½¬æ¢ï¼Œåˆ†åˆ«å¯¹åº”äºè¿™ä¸¤ä¸ªè¿‡ç¨‹ã€‚ç¬¬ä¸€é˜¶è§†è§’è½¬æ¢å…è®¸LLMæ™ºèƒ½ä½“ä»è‡ªå·±çš„è§’åº¦æ¨æ–­ä»–äººçš„å¿ƒç†çŠ¶æ€ï¼Œè€Œç¬¬äºŒé˜¶è§†è§’è½¬æ¢åˆ™æ¶‰åŠç†è§£ä»–äººå¦‚ä½•çœ‹å¾…æ™ºèƒ½ä½“çš„å¿ƒç†çŠ¶æ€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨Avalonæ¸¸æˆä¸­æµ‹è¯•ReCon
æœ¬æ–‡ä½¿ç”¨å¤æ‚çš„Avalonæ¸¸æˆä½œä¸ºæµ‹è¯•å¹³å°ï¼Œè¯¥æ¸¸æˆå……æ»¡è¯¯å¯¼ä¿¡æ¯ï¼Œéœ€è¦å¤æ‚çš„é€»è¾‘æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReConèƒ½å¤Ÿæœ‰æ•ˆåœ°å¸®åŠ©LLMsè¯†åˆ«å’Œåº”å¯¹æ¬ºéª—æ€§ä¿¡æ¯ï¼Œè€Œæ— éœ€é¢å¤–çš„å¾®è°ƒå’Œæ•°æ®ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒReConåœ¨Avalonæ¸¸æˆä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¸®åŠ©LLMsè¯†åˆ«å’Œåº”å¯¹æ¬ºéª—æ€§ä¿¡æ¯ï¼Œè€Œæ— éœ€é¢å¤–çš„å¾®è°ƒå’Œæ•°æ®ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒReConåœ¨å¤šä¸ªç»´åº¦ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼ŒåŒ…æ‹¬éšè”½æ€§ã€é€»è¾‘æ€§ã€è´¡çŒ®åº¦ã€è¯´æœåŠ›ã€ä¿¡æ¯é‡å’Œåˆ›é€ åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ReConæ¡†æ¶ä¸ºLLMsåœ¨æ¬ºéª—æ€§ç¯å¢ƒä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚ReConæ¡†æ¶çš„è®¾è®¡å’Œå®ç°æ–¹æ³•å¯ä»¥å€Ÿé‰´åˆ°å…¶ä»–éœ€è¦è¯†åˆ«å’Œå¯¹æŠ—æ¬ºéª—æ€§ä¿¡æ¯çš„åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†LLMsåœ¨å®‰å…¨æ€§ã€æ¨ç†èƒ½åŠ›ã€è¨€è¯­é£æ ¼å’Œæ ¼å¼ç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models
### Abstract
The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).
### ğŸŒŸ è®ºæ–‡è§£è¯» | RoleLLMï¼šè§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„è§’è‰²æ‰®æ¼”èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œè§’è‰²æ‰®æ¼”ç­‰å¤æ‚ä»»åŠ¡æˆä¸ºå¯èƒ½ï¼Œä¸ºç”¨æˆ·äº¤äº’æä¾›äº†æ›´å¤šå¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰å¼€æºLLMsåœ¨è§’è‰²æ‰®æ¼”æ–¹é¢å­˜åœ¨ä¼˜åŒ–ä¸è¶³çš„é—®é¢˜ï¼Œè€Œæœ€å…ˆè¿›çš„LLMså¦‚GPT-4ç­‰åˆ™å› å…¶é—­æºæ€§è´¨è€Œé™åˆ¶äº†å…¶åœ¨è§’è‰²æ‰®æ¼”æ–¹é¢çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºRoleLLMæ¡†æ¶ï¼Œä»¥æå‡LLMsçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šRole Profile Construction
æ„å»ºäº†100ä¸ªè§’è‰²çš„è¯¦ç»†æ¡£æ¡ˆï¼ŒåŒ…æ‹¬è§’è‰²æè¿°ã€å£å¤´ç¦…ä»¥åŠä»å‰§æœ¬ä¸­æå–çš„å¯¹è¯ç‰‡æ®µï¼Œä¸ºè§’è‰²æ‰®æ¼”æä¾›äº†ä¸°å¯Œçš„èƒŒæ™¯çŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šContext-Based Instruction Generation (Context-Instruct)
åˆ©ç”¨GPTæ¨¡å‹ä»è§’è‰²æ¡£æ¡ˆä¸­ç”Ÿæˆé«˜è´¨é‡çš„é—®ç­”å¯¹ï¼Œä»¥æå–è§’è‰²ç‰¹å®šçš„çŸ¥è¯†å’Œè®°å¿†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šRole Prompting using GPT (RoleGPT)
é€šè¿‡å¯¹è¯å·¥ç¨‹å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œåˆ©ç”¨GPTæ¨¡å‹ç”Ÿæˆç¬¦åˆè§’è‰²è¯´è¯é£æ ¼çš„å›ç­”ï¼Œä»¥æ¨¡ä»¿è§’è‰²çš„è¯´è¯é£æ ¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šRole-Conditioned Instruction Tuning (RoCIT)
åˆ©ç”¨Context-Instructå’ŒRoleGPTç”Ÿæˆçš„æ•°æ®ï¼Œå¯¹å¼€æºLLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥æå‡å…¶è§’è‰²æ‰®æ¼”èƒ½åŠ›ï¼Œå¹¶ç”ŸæˆRoleLLaMAå’ŒRoleGLMç­‰æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒRoleLLMæ¡†æ¶åœ¨è§’è‰²æ‰®æ¼”æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚RoleLLaMAå’ŒRoleGLMåœ¨æ¨¡ä»¿è§’è‰²è¯´è¯é£æ ¼ã€å›ç­”å‡†ç¡®æ€§å’Œè§’è‰²ç‰¹å®šçŸ¥è¯†æŒæ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¸RoleGPTï¼ˆä½¿ç”¨GPT-4ï¼‰ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
RoleLLMæ¡†æ¶ä¸ºLLMsçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›æå‡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…¶åˆ›æ–°ç‚¹åŒ…æ‹¬è§’è‰²æ¡£æ¡ˆæ„å»ºã€åŸºäºä¸Šä¸‹æ–‡çš„æŒ‡ä»¤ç”Ÿæˆã€è§’è‰²æç¤ºå’Œè§’è‰²æ¡ä»¶æŒ‡ä»¤å¾®è°ƒç­‰ã€‚æ­¤å¤–ï¼ŒRoleBenchæ•°æ®é›†çš„æ„å»ºä¹Ÿä¸ºè§’è‰²æ‰®æ¼”èƒ½åŠ›çš„è¯„ä¼°å’Œæå‡æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## adarefiner--refining-decisions-of-language-models-with-adaptive-feedback
### Abstract
Large Language Models (LLMs) have demonstrated significant success across
various domains. However, their application in complex decision-making tasks
frequently necessitates intricate prompt engineering or fine-tuning, leading to
challenges in unseen downstream tasks and heavy demands on computational
resources. Meanwhile, Reinforcement Learning (RL) has been recognized as
effective in decision-making problems but struggles in environments with sparse
rewards, such as open-world games. To overcome these challenges, we introduce
AdaRefiner, a novel framework designed to enhance the synergy between LLMs and
RL feedback. The key component of AdaRefiner is a lightweight Adapter Language
Model (LM), which automatically refines task comprehension based on feedback
from RL agents. This method mitigates the need for intricate prompt engineering
and intensive LLM fine-tuning while maintaining the LLMs' generalization
abilities and enhancing their decision-making capabilities in downstream tasks.
Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world
game Crafter have demonstrated its superior effectiveness, especially in
guiding agents towards higher-level and common-sense skills. Our work makes
contributions to the automatic self-refinement of LLMs with RL feedback,
offering a more adaptable and efficient solution for complex decision-making
problems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AdaRefinerï¼šåˆ©ç”¨è‡ªé€‚åº”åé¦ˆæå‡è¯­è¨€æ¨¡å‹å†³ç­–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„åº”ç”¨å´é¢ä¸´ç€æŒ‘æˆ˜ã€‚LLMs éœ€è¦è¿›è¡Œç¹ççš„æç¤ºå·¥ç¨‹æˆ–å¾®è°ƒæ‰èƒ½é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æœªçŸ¥ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¸¦æ¥äº†å¯¹è®¡ç®—èµ„æºçš„å·¨å¤§éœ€æ±‚ã€‚å¦ä¸€æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å†³ç­–é—®é¢˜ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç¨€ç–å¥–åŠ±çš„ç¯å¢ƒä¸­ï¼ˆå¦‚å¼€æ”¾ä¸–ç•Œæ¸¸æˆï¼‰å´éš¾ä»¥å‘æŒ¥ä½œç”¨ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº† AdaRefinerï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼º LLMs å’Œ RL åé¦ˆä¹‹é—´ååŒä½œç”¨çš„æ–°æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAdaRefiner å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„é€‚é…å™¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œè¯¥æ¨¡å‹æ ¹æ® RL ä»£ç†çš„åé¦ˆè‡ªåŠ¨ç»†åŒ–ä»»åŠ¡ç†è§£ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†ç¹ççš„æç¤ºå·¥ç¨‹å’Œå¯†é›†çš„ LLM å¾®è°ƒçš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº† LLMs çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†å®ƒä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAdaRefiner åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆ Crafter çš„ 22 ä¸ªä¸åŒä»»åŠ¡ä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶åœ¨å¼•å¯¼ä»£ç†å­¦ä¹ é«˜çº§å’Œå¸¸è¯†æŠ€èƒ½æ–¹é¢å…·æœ‰ä¼˜è¶Šçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AdaRefiner åœ¨ Crafter ç¯å¢ƒä¸­çš„ 22 ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚AdaRefiner èƒ½å¤Ÿå¼•å¯¼ä»£ç†å­¦ä¹ é«˜çº§æŠ€èƒ½ï¼Œå¹¶è¡¨ç°å‡ºå¸¸è¯†è¡Œä¸ºã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œé€‚é…å™¨ LM å’Œ RL åé¦ˆå¯¹äº AdaRefiner çš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AdaRefiner ä¸º LLMs åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†ä¸€ç§æ›´çµæ´»å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å…¶è½»é‡çº§çš„é€‚é…å™¨ LM å’Œè‡ªé€‚åº”åé¦ˆæœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°æå‡ LLMs çš„ä»»åŠ¡ç†è§£å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸º LLMs åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆç­‰å¤æ‚ç¯å¢ƒä¸­çš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

## motif--intrinsic-motivation-from-artificial-intelligence-feedback
### Abstract
Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Motifï¼šä»äººå·¥æ™ºèƒ½åé¦ˆä¸­è·å–å†…åœ¨åŠ¨æœº

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œæ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„æ™ºèƒ½ä½“æ¢ç´¢å’Œè¯„ä¼°å…¶è¡Œä¸ºæå…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Motif çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸­çš„å…ˆéªŒçŸ¥è¯†ä¸æ™ºèƒ½ä½“è¿›è¡Œäº¤äº’ï¼Œä»è€Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨æ²¡æœ‰ä¸ç¯å¢ƒçš„ç›´æ¥äº¤äº’çš„æƒ…å†µä¸‹è¿›è¡Œå†³ç­–ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ LLM çš„åå¥½æ„å»ºå†…åœ¨å¥–åŠ±
Motif çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡ LLM å¯¹äº‹ä»¶æ ‡é¢˜çš„åå¥½æ¥æ„å»ºå†…åœ¨å¥–åŠ±å‡½æ•°ï¼Œå¹¶å°†å…¶ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ™ºèƒ½ä½“ã€‚LLM è¡¨è¾¾å¯¹æˆå¯¹äº‹ä»¶æ ‡é¢˜çš„åå¥½ï¼Œè¿™äº›æ ‡é¢˜åªéœ€ç²—ç•¥æè¿°ç¯å¢ƒä¸­å‘ç”Ÿçš„äº‹ä»¶ï¼Œè€Œä¸éœ€è¦ç²¾ç»†çš„é€æ­¥æè¿°ã€‚LLM ä¸éœ€è¦ç†è§£ä½çº§åŠ¨ä½œç©ºé—´ï¼Œè¿™å¯èƒ½æ˜¯å¤åˆçš„æˆ–è¿ç»­çš„ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå†…åœ¨å¥–åŠ±ä¸å¤–åœ¨å¥–åŠ±çš„ç»“åˆ
Motif çš„å†…åœ¨å¥–åŠ±å¯ä»¥å•ç‹¬ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä¸æ¥è‡ªç¯å¢ƒçš„å¥–åŠ±ä¿¡å·ç»“åˆä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å†…åœ¨å¥–åŠ±ä¸å¤–åœ¨å¥–åŠ±ç»“åˆä½¿ç”¨æ—¶ï¼ŒMotif çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ²¡æœ‰æ¼”ç¤ºçš„æƒ…å†µä¸‹å–å¾—äº†è¿›å±•ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Motif åœ¨ NetHack å­¦ä¹ ç¯å¢ƒ (NLE) ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ã€å¼€æ”¾æ€§å’Œç¨‹åºç”Ÿæˆçš„æ¸¸æˆã€‚ç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡å­¦ä¹ æœ€å¤§åŒ–å…¶å†…åœ¨å¥–åŠ±ï¼ŒMotif å°±å–å¾—äº†æ¯”ç›´æ¥è®­ç»ƒä»¥æœ€å¤§åŒ–åˆ†æ•°çš„ç®—æ³•æ›´é«˜çš„æ¸¸æˆåˆ†æ•°ã€‚å½“å°† Motif çš„å†…åœ¨å¥–åŠ±ä¸ç¯å¢ƒçš„å¥–åŠ±ç›¸ç»“åˆæ—¶ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ²¡æœ‰æ¼”ç¤ºçš„æƒ…å†µä¸‹å–å¾—äº†è¿›å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Motif ä¸ºåˆ©ç”¨ LLM çš„å…ˆéªŒçŸ¥è¯†å’Œå¸¸è¯†æ¥åˆ›å»ºæ™ºèƒ½ä½“æä¾›äº†ä¸€ç§é€šç”¨çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å°† LLM çš„é«˜å±‚æ¬¡çŸ¥è¯†ä¸æ™ºèƒ½ä½“æ“ä½œçš„åº•å±‚ä¼ æ„Ÿå™¨è¿åŠ¨ç°å®ä¹‹é—´çš„å·®è·ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†çŸ¥è¯†æç‚¼å‡ºæ¥ã€‚Motif å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ä¸æ›´å¤§è§„æ¨¡çš„ LLM æˆ–ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒ LLM ç»“åˆä½¿ç”¨ï¼Œå¹¶å¯ä»¥é€šè¿‡æç¤ºä¿®æ”¹è½»æ¾åœ°å¼•å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºã€‚

## simulbench--evaluating-language-models-with-creative-simulation-tasks
### Abstract
We introduce SimulBench, a benchmark designed to evaluate large language
models (LLMs) across a diverse collection of creative simulation scenarios,
such as acting as a Linux terminal or playing text games with users. While
these simulation tasks serve as effective measures of an LLM's general
intelligence, they are seldom incorporated into existing benchmarks. A major
challenge is to develop an evaluation framework for testing different LLMs
fairly while preserving the multi-round interactive nature of simulation tasks
between users and AI. To tackle this issue, we suggest using a fixed LLM as a
user agent to engage with an LLM to collect dialogues first under different
tasks. Then, challenging dialogue scripts are extracted for evaluating
different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4
is employed as the evaluator, tasked with reviewing the quality of the final
response generated by the target LLMs given multi-turn dialogue scripts. Our
comprehensive experiments indicate that these simulation tasks continue to pose
a significant challenge with their unique natures and show the gap between
proprietary models and the most advanced open LLMs. For example, GPT-4-turbo
outperforms LLaMA-3-70b-Chat on 18.55\% more cases.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SimulBenchï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨åˆ›æ„æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿå¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å•è½®ã€é™æ€çš„ç”¨æˆ·ä¸LLMsä¹‹é—´çš„äº¤äº’ï¼Œç¼ºä¹å¯¹å¤šè½®äº¤äº’å’Œå¤æ‚æ¨¡æ‹Ÿèƒ½åŠ›çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ä¸äººç±»ç›¸å…³çš„æ¨¡æ‹Ÿä»»åŠ¡ä¸Šï¼Œè€Œå¿½ç•¥äº†éäººç±»ä¸­å¿ƒçš„æ¨¡æ‹Ÿä»»åŠ¡ï¼Œå¦‚Linuxç»ˆç«¯æˆ–æ–‡æœ¬æ¸¸æˆç­‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šSimulBenchåŸºå‡†
æœ¬æ–‡æå‡ºäº†SimulBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMsåœ¨åˆ›æ„æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†ã€‚SimulBenchåŒ…å«109ä¸ªç‹¬ç‰¹çš„æ¨¡æ‹Ÿä»»åŠ¡ï¼Œæ¶µç›–äº†å„ç§æ¥å£ï¼Œå¦‚Linuxç»ˆç«¯ã€SQLæ‰§è¡Œå™¨ã€æ–‡æœ¬æ¸¸æˆç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šè½®è„šæœ¬è¯„ä¼°æ¡†æ¶
ä¸ºäº†å…¬å¹³åœ°è¯„ä¼°ä¸åŒLLMsï¼ŒSimulBenché‡‡ç”¨äº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„è¯„ä¼°æ¡†æ¶ã€‚é¦–å…ˆï¼Œä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„LLMä½œä¸ºç”¨æˆ·ä»£ç†ä¸å¦ä¸€ä¸ªLLMè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œæ”¶é›†å¯¹è¯å†å²ã€‚ç„¶åï¼Œä»è¿™äº›å¯¹è¯å†å²ä¸­æå–å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹è¯è„šæœ¬ï¼Œç”¨äºè¯„ä¼°ä¸åŒçš„ç›®æ ‡LLMsã€‚æœ€åï¼Œä½¿ç”¨GPT-4ä½œä¸ºè¯„ä¼°è€…ï¼Œå¯¹ç›®æ ‡LLMsåœ¨ç»™å®šå¤šè½®å¯¹è¯è„šæœ¬ä¸‹çš„æœ€ç»ˆå“åº”è´¨é‡è¿›è¡Œè¯„ä¼°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒSimulBenchä¸­çš„æ¨¡æ‹Ÿä»»åŠ¡å¯¹LLMsæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸”æ˜¾ç¤ºäº†ä¸“æœ‰æ¨¡å‹å’Œæœ€å…ˆè¿›çš„å¼€æºLLMsä¹‹é—´çš„å·®è·ã€‚ä¾‹å¦‚ï¼ŒGPT-4-turboåœ¨18.55%çš„æƒ…å†µä¸‹ä¼˜äºLLaMA-3-70b-Chatã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SimulBenchåŸºå‡†ä¸ºè¯„ä¼°LLMsåœ¨æ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å…¶å¤šè½®è„šæœ¬è¯„ä¼°æ¡†æ¶å¯ä»¥ç¡®ä¿å…¬å¹³çš„æ¯”è¾ƒï¼Œå¹¶æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMsåœ¨ä¸åŒæ¨¡æ‹Ÿä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒSimulBenchçš„å®éªŒç»“æœä¹Ÿæ­ç¤ºäº†LLMsåœ¨å¤„ç†å¤æ‚æ¨¡æ‹Ÿä»»åŠ¡æ—¶çš„æŒ‘æˆ˜å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚

## suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4
### Abstract
Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€èƒ½åŠ›ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå†³ç­–å¾€å¾€æ˜¯åœ¨ä¿¡æ¯ä¸å®Œæ•´æˆ–ä¸ç¡®å®šçš„æƒ…å†µä¸‹è¿›è¡Œçš„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„AIç®—æ³•éƒ½æ˜¯åœ¨å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è®­ç»ƒçš„ï¼Œå³æ‰€æœ‰ç©å®¶éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰ä¿¡æ¯ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ¥å¤„ç†ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å†³ç­–è¿‡ç¨‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSuspicion-Agentçš„åˆ›æ–°å‹è‡ªä¸»ä»£ç†ï¼Œå®ƒåŸºäºGPT-4ï¼Œå¹¶åˆ©ç”¨å…¶å¼ºå¤§çš„çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›æ¥ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆã€‚Suspicion-Agentçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€ï¼ˆToMï¼‰èƒ½åŠ›ï¼Œå³ç†è§£ä»–äººå¹¶æœ‰æ„å½±å“ä»–äººè¡Œä¸ºçš„èƒ½åŠ›ã€‚è¿™ä½¿å¾—Suspicion-Agentèƒ½å¤Ÿé¢„æµ‹å¯¹æ‰‹çš„è¡Œä¸ºï¼Œå¹¶æ ¹æ®å¯¹æ‰‹çš„è¡Œä¸ºè°ƒæ•´è‡ªå·±çš„ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°†æ¸¸æˆè¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå­æ¨¡å—ï¼Œå¦‚è§‚å¯Ÿè§£é‡Šå™¨ã€æ¸¸æˆæ¨¡å¼åˆ†æå’Œè§„åˆ’æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½ä½¿ç”¨ä¸åŒçš„æç¤ºæ¥å¼•å¯¼GPT-4æ‰§è¡Œç‰¹å®šçš„åŠŸèƒ½ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒä¸­ï¼ŒSuspicion-Agentåœ¨ä¸‰ä¸ªä¸åŒçš„ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­å±•ç¤ºäº†å…¶èƒ½åŠ›ï¼Œå¹¶åœ¨Leduc Hold'emæ¸¸æˆä¸­è¿›è¡Œäº†å®šé‡è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSuspicion-Agentå¯ä»¥æ½œåœ¨åœ°è¶…è¶Šä¼ ç»Ÿç®—æ³•ï¼Œè€Œæ— éœ€ä»»ä½•ä¸“é—¨çš„è®­ç»ƒæˆ–ç¤ºä¾‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Suspicion-Agentæ¡†æ¶ä¸ºåˆ©ç”¨LLMåœ¨ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è¿›è¡Œå†³ç­–æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†LLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸ToMèƒ½åŠ›ç›¸ç»“åˆï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å…¬å¼€äº†æ‰€æœ‰ä¸æ¸¸æˆç›¸å…³çš„æ•°æ®ï¼Œè¿™å°†æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMçš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘æ›´æœ‰æ•ˆçš„æ¨¡å‹ã€‚

## autoagents--a-framework-for-automatic-agent-generation
### Abstract
Large language models (LLMs) have enabled remarkable advances in automated
task-solving with multi-agent systems. However, most existing LLM-based
multi-agent approaches rely on predefined agents to handle simple tasks,
limiting the adaptability of multi-agent collaboration to different scenarios.
Therefore, we introduce AutoAgents, an innovative framework that adaptively
generates and coordinates multiple specialized agents to build an AI team
according to different tasks. Specifically, AutoAgents couples the relationship
between tasks and roles by dynamically generating multiple required agents
based on task content and planning solutions for the current task based on the
generated expert agents. Multiple specialized agents collaborate with each
other to efficiently accomplish tasks. Concurrently, an observer role is
incorporated into the framework to reflect on the designated plans and agents'
responses and improve upon them. Our experiments on various benchmarks
demonstrate that AutoAgents generates more coherent and accurate solutions than
the existing multi-agent methods. This underscores the significance of
assigning different roles to different tasks and of team cooperation, offering
new perspectives for tackling complex tasks. The repository of this project is
available at https://github.com/Link-AGI/AutoAgents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AutoAgentsï¼šè‡ªåŠ¨ç”Ÿæˆæ™ºèƒ½ä½“æ¡†æ¶ï¼ŒåŠ©åŠ›å¤æ‚ä»»åŠ¡è§£å†³

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨ä»»åŠ¡è§£å†³æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„åŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“æ–¹æ³•å¤§å¤šä¾èµ–äºé¢„å®šä¹‰çš„æ™ºèƒ½ä½“æ¥å¤„ç†ç®€å•ä»»åŠ¡ï¼Œé™åˆ¶äº†å¤šæ™ºèƒ½ä½“åä½œåœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AutoAgentsæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ä¸åŒä»»åŠ¡è‡ªé€‚åº”åœ°ç”Ÿæˆå’Œåè°ƒå¤šä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼Œæ„å»ºAIå›¢é˜Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€ç”Ÿæˆæ™ºèƒ½ä½“
AutoAgentsé€šè¿‡åˆ†æä»»åŠ¡å†…å®¹ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šä¸ªæ‰€éœ€çš„æ™ºèƒ½ä½“ï¼Œå¹¶æ ¹æ®ç”Ÿæˆçš„ä¸“å®¶æ™ºèƒ½ä½“è§„åˆ’å½“å‰ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›ä¸“ä¸šæ™ºèƒ½ä½“ç›¸äº’åä½œï¼Œé«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥è§‚å¯Ÿè€…è§’è‰²
AutoAgentsæ¡†æ¶ä¸­å¼•å…¥äº†è§‚å¯Ÿè€…è§’è‰²ï¼Œç”¨äºåæ€æŒ‡å®šçš„è®¡åˆ’å’Œæ™ºèƒ½ä½“çš„å“åº”ï¼Œå¹¶è¿›è¡Œæ”¹è¿›ã€‚è§‚å¯Ÿè€…è§’è‰²æœ‰åŠ©äºæé«˜æ™ºèƒ½ä½“å›¢é˜Ÿçš„æ•´ä½“æ€§èƒ½å’Œé€‚åº”æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»»åŠ¡æ‰§è¡ŒåŠ¨ä½œ
AutoAgentsæ¡†æ¶åŒ…æ‹¬ä¸¤ç§ä»»åŠ¡æ‰§è¡ŒåŠ¨ä½œï¼šå•ä¸ªæ™ºèƒ½ä½“çš„è‡ªæˆ‘æ”¹è¿›å’Œå¤šä¸ªæ™ºèƒ½ä½“çš„åä½œæ”¹è¿›ã€‚è‡ªæˆ‘æ”¹è¿›ä½¿å•ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿæé«˜å…¶åœ¨æ‰§è¡ŒæŸäº›ä¸“ä¸šä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ï¼Œè€Œåä½œæ”¹è¿›åˆ™ä¿ƒè¿›å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„çŸ¥è¯†å…±äº«ï¼Œå®Œæˆéœ€è¦è·¨å­¦ç§‘ä¸“ä¸šçŸ¥è¯†æ‰èƒ½å®Œæˆçš„ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šçŸ¥è¯†å…±äº«æœºåˆ¶
AutoAgentsæ¡†æ¶æä¾›äº†çŸ­æœŸè®°å¿†ã€é•¿æœŸè®°å¿†å’ŒåŠ¨æ€è®°å¿†ä¸‰ç§çŸ¥è¯†å…±äº«æœºåˆ¶ã€‚çŸ­æœŸè®°å¿†è®°å½•å•ä¸ªåŠ¨ä½œçš„è‡ªæˆ‘æ”¹è¿›æˆ–åä½œæ”¹è¿›é˜¶æ®µçš„å†å²ï¼Œé•¿æœŸè®°å¿†è®°å½•å¤šä¸ªåŠ¨ä½œçš„å†å²è½¨è¿¹ï¼ŒåŠ¨æ€è®°å¿†åˆ™ç”¨äºä¸ºéœ€è¦ç‰¹æ®Šå…³æ³¨çš„åŠ¨ä½œæä¾›è¾…åŠ©ä¿¡æ¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AutoAgentsåœ¨å¼€æ”¾æ€§é—®é¢˜å›ç­”å’ŒTriviaåˆ›æ„å†™ä½œç­‰åŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ¯”ç°æœ‰å¤šæ™ºèƒ½ä½“æ–¹æ³•æ›´ä¸€è‡´ã€æ›´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼ŒAutoAgentsåœ¨è½¯ä»¶å¼€å‘ç­‰å¤æ‚ä»»åŠ¡ä¸­çš„åº”ç”¨æ¡ˆä¾‹ä¹Ÿå±•ç¤ºäº†å…¶çµæ´»æ€§å’Œæ½œåœ¨ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AutoAgentsæ¡†æ¶ä¸ºè§£å†³å¤æ‚ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶åŠ¨æ€ç”Ÿæˆæ™ºèƒ½ä½“ã€å¼•å…¥è§‚å¯Ÿè€…è§’è‰²ã€ä»»åŠ¡æ‰§è¡ŒåŠ¨ä½œå’ŒçŸ¥è¯†å…±äº«æœºåˆ¶ç­‰æ–¹é¢çš„åˆ›æ–°ç‚¹å€¼å¾—å€Ÿé‰´ã€‚æ­¤å¤–ï¼ŒAutoAgentsæ¡†æ¶åœ¨è½¯ä»¶å¼€å‘ç­‰é¢†åŸŸçš„åº”ç”¨æ¡ˆä¾‹ä¹Ÿä¸ºå…¶ä»–é¢†åŸŸæä¾›äº†å‚è€ƒã€‚

## true-knowledge-comes-from-practice--aligning-llms-with-embodied-environments-via-reinforcement-learning
### Abstract
Despite the impressive performance across numerous tasks, large language
models (LLMs) often fail in solving simple decision-making tasks due to the
misalignment of the knowledge in LLMs with environments. On the contrary,
reinforcement learning (RL) agents learn policies from scratch, which makes
them always align with environments but difficult to incorporate prior
knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a
novel general online framework that deploys LLMs as decision-making agents to
efficiently interact and align with embodied environments via RL without
requiring any prepared datasets or prior knowledge of the environments.
Firstly, we query the joint probabilities of each valid action with LLMs to
form behavior policies. Then, to enhance the stability and robustness of the
policies, we propose two normalization methods and summarize four prompt design
principles. Finally, we design a novel parameter-efficient training
architecture where the actor and critic share one frozen LLM equipped with
low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to
evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency
and performance compared to the conventional RL method, PPO, and prompt tuning
method, SayCan, in both classical decision-making environment, Overcooked, and
simulated household environment, VirtualHome. ii) Benefiting from LLMs'
open-vocabulary feature, TWOSOME shows superior generalization ability to
unseen tasks. iii) Under our framework, there is no significant loss of the
LLMs' original ability during online PPO finetuning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | çŸ¥è¯†æºäºå®è·µï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸å…·èº«ç¯å¢ƒå¯¹é½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¼—å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨è§£å†³ç®€å•çš„å†³ç­–ä»»åŠ¡æ—¶å¾€å¾€å¤±è´¥ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºLLMsä¸­çš„çŸ¥è¯†ä¸ç¯å¢ƒçš„é”™ä½ã€‚ç›¸åï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ä»é›¶å¼€å§‹å­¦ä¹ ç­–ç•¥ï¼Œè¿™ä½¿å¾—å®ƒä»¬å§‹ç»ˆä¸ç¯å¢ƒä¿æŒä¸€è‡´ï¼Œä½†éš¾ä»¥èå…¥å…ˆéªŒçŸ¥è¯†ä»¥æé«˜æ¢ç´¢æ•ˆç‡ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†TWOSOMEï¼Œä¸€ä¸ªæ–°é¢–çš„é€šç”¨åœ¨çº¿æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†LLMsä½œä¸ºå†³ç­–ä»£ç†ï¼Œé€šè¿‡RLé«˜æ•ˆåœ°ä¸å…·èº«ç¯å¢ƒäº¤äº’å¹¶å¯¹é½ï¼Œè€Œæ— éœ€ä»»ä½•å‡†å¤‡å¥½çš„æ•°æ®é›†æˆ–å¯¹ç¯å¢ƒçš„å…ˆéªŒçŸ¥è¯†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¡Œä¸ºç­–ç•¥ç”Ÿæˆ
TWOSOMEä¸æ˜¯è®©LLMsç›´æ¥ç”ŸæˆåŠ¨ä½œï¼Œè€Œæ˜¯æŸ¥è¯¢LLMsä¸­æ¯ä¸ªæœ‰æ•ˆåŠ¨ä½œçš„è”åˆæ¦‚ç‡ï¼Œä»¥å½¢æˆè¡Œä¸ºç­–ç•¥ã€‚è¿™æ¶ˆé™¤äº†ç”±äºæ— æ•ˆåŠ¨ä½œé€ æˆçš„é”™ä½é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨ä½œæç¤ºå½’ä¸€åŒ–
ä¸ºäº†å¢å¼ºç­–ç•¥çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§å½’ä¸€åŒ–æ–¹æ³•ï¼štokenå½’ä¸€åŒ–å’Œwordå½’ä¸€åŒ–ï¼Œä»¥è§£å†³åŠ¨ä½œåˆ†å¸ƒä¸å¹³è¡¡çš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå‚æ•°é«˜æ•ˆçš„è®­ç»ƒæ¶æ„
æœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å‚æ•°é«˜æ•ˆè®­ç»ƒæ¶æ„ï¼Œå…¶ä¸­actorå’Œcriticå…±äº«ä¸€ä¸ªå†»ç»“çš„LLMï¼Œå¹¶é…å¤‡ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰ï¼Œç”±PPOæ›´æ–°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šæç¤ºè®¾è®¡åŸåˆ™
æœ¬æ–‡æ€»ç»“äº†å››ä¸ªæç¤ºè®¾è®¡åŸåˆ™ï¼Œä»¥å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæé«˜LLMsä¸å…·èº«ç¯å¢ƒä¹‹é—´çš„å¯¹é½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç»å…¸å†³ç­–ç¯å¢ƒOvercookedå’Œæ¨¡æ‹Ÿå®¶åº­ç¯å¢ƒVirtualHomeä¸­ï¼ŒTWOSOMEåœ¨æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„RLæ–¹æ³•PPOå’Œæç¤ºè°ƒæ•´æ–¹æ³•SayCanã€‚æ­¤å¤–ï¼ŒTWOSOMEè¿˜è¡¨ç°å‡ºå¯¹æœªè§ä»»åŠ¡çš„ä¼˜è¶Šæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„TWOSOMEæ¡†æ¶ä¸ºå°†LLMsä¸å…·èº«ç¯å¢ƒå¯¹é½æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¼€å‘é€šç”¨çš„è‡ªä¸»ä»£ç†è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚è¯¥æ¡†æ¶å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

*   åˆ©ç”¨LLMsçš„å…ˆéªŒçŸ¥è¯†æ¥æé«˜RLä»£ç†çš„æ ·æœ¬æ•ˆç‡ã€‚
*   é€šè¿‡å½’ä¸€åŒ–æ–¹æ³•è§£å†³åŠ¨ä½œåˆ†å¸ƒä¸å¹³è¡¡çš„é—®é¢˜ã€‚
*   è®¾è®¡æœ‰æ•ˆçš„æç¤ºä»¥æé«˜LLMsçš„æ¨ç†èƒ½åŠ›ã€‚
*   ä½¿ç”¨å‚æ•°é«˜æ•ˆçš„è®­ç»ƒæ¶æ„æ¥é™ä½è®­ç»ƒæˆæœ¬ã€‚

### ğŸ¯ æœªæ¥å±•æœ›
æœªæ¥å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å°†TWOSOMEæ¡†æ¶åº”ç”¨äºæ›´å¤æ‚çš„å…·èº«ç¯å¢ƒï¼Œå¹¶æ¢ç´¢LLMsä¸RLä»£ç†ä¹‹é—´çš„æ›´æ·±å…¥äº¤äº’ã€‚

## mindagent--emergent-gaming-interaction
### Abstract
Large Language Models (LLMs) have the capacity of performing complex
scheduling in a multi-agent system and can coordinate these agents into
completing sophisticated tasks that require extensive collaboration. However,
despite the introduction of numerous gaming frameworks, the community has
insufficient benchmarks towards building general multi-agents collaboration
infrastructure that encompass both LLM and human-NPCs collaborations. In this
work, we propose a novel infrastructure - MindAgent - to evaluate planning and
coordination emergent capabilities for gaming interaction. In particular, our
infrastructure leverages existing gaming framework, to i) require understanding
of the coordinator for a multi-agent system, ii) collaborate with human players
via un-finetuned proper instructions, and iii) establish an in-context learning
on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new
gaming scenario and related benchmark that dispatch a multi-agent collaboration
efficiency and supervise multiple agents playing the game simultaneously. We
conduct comprehensive evaluations with new auto-metric CoS for calculating the
collaboration efficiency. Finally, our infrastructure can be deployed into
real-world gaming scenarios in a customized VR version of CUISINEWORLD and
adapted in existing broader Minecraft gaming domain. We hope our findings on
LLMs and the new infrastructure for general-purpose scheduling and coordination
can help shed light on how such skills can be obtained by learning from large
language corpora.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MindAgentï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå…¶åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ä¹Ÿé€æ¸å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¸¸æˆæ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•è¿˜ä¸è¶³ä»¥è¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨LLMsä¸äººç±»NPCsåä½œçš„åœºæ™¯ä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMindAgentçš„æ–°å‹åŸºç¡€è®¾æ–½ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„è§„åˆ’ä¸åè°ƒèƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸åŸºå‡†æµ‹è¯•
æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåä¸ºCUISINEWORLDçš„æ¸¸æˆåœºæ™¯ï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªè™šæ‹Ÿå¨æˆ¿ç¯å¢ƒï¼Œå…¶ä¸­å¤šæ™ºèƒ½ä½“ç³»ç»Ÿéœ€è¦åè°ƒå¤šä¸ªä»£ç†ï¼Œå®Œæˆå°½å¯èƒ½å¤šçš„èœè‚´è®¢å•ã€‚CUISINEWORLDæ¸¸æˆåœºæ™¯å…·æœ‰å¤šç§ä»»åŠ¡ç»“æ„å’Œéš¾åº¦ï¼Œæ˜¯è¯„ä¼°LLMsæ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMindAgentåŸºç¡€è®¾æ–½
MindAgentæ˜¯ä¸€ä¸ªç”¨äºLLMsäº¤äº’å¼å¤šæ™ºèƒ½ä½“è§„åˆ’çš„åŸºç¡€è®¾æ–½ï¼Œå®ƒå±•ç¤ºäº†LLMsçš„æ¶Œç°å¼å¤šæ™ºèƒ½ä½“è§„åˆ’èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†å¤šç§æç¤ºæŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›LLMsçš„è§„åˆ’èƒ½åŠ›ï¼ŒåŒ…æ‹¬æä¾›å°‘é‡ç¤ºä¾‹ã€è§„åˆ’ç†ç”±å’Œç¯å¢ƒåé¦ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨CUISINEWORLDæ¸¸æˆåœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š
1. é›¶æ ·æœ¬å¤šæ™ºèƒ½ä½“è§„åˆ’ï¼šå¼ºå¤§çš„é¢„è®­ç»ƒLLMsï¼ˆå¦‚GPT-4ï¼‰èƒ½å¤Ÿé€šè¿‡é˜…è¯»ç®€å•çš„æ¸¸æˆæŒ‡ä»¤å’Œé£Ÿè°±ï¼Œè°ƒåº¦å¤šä¸ªä»£ç†ï¼ˆ2åˆ°4ä¸ªï¼‰å®Œæˆèœè‚´ï¼Œç”šè‡³ä¸äººç±»ç©å®¶åä½œã€‚
2. åŸºäºé«˜çº§æç¤ºçš„è§„åˆ’ï¼šé€šè¿‡åˆ©ç”¨æ¶Œç°å¼ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥æ˜¾è‘—æé«˜LLMsçš„å¤šæ™ºèƒ½ä½“è§„åˆ’æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ·»åŠ å°‘é‡ä¸“å®¶æ¼”ç¤ºã€è§£é‡ŠæŸäº›è¡ŒåŠ¨çš„ç†ç”±ï¼Œä»¥åŠåœ¨è§„åˆ’è¿‡ç¨‹ä¸­æä¾›å®æ—¶åé¦ˆã€‚
3. é€šç”¨æ½œåŠ›ï¼šLLMsè¡¨ç°å‡ºæˆä¸ºé€šç”¨å¤šæ™ºèƒ½ä½“è§„åˆ’å™¨çš„å·¨å¤§æ½œåŠ›ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿé€šè¿‡å°‘é‡ç¤ºä¾‹æ³›åŒ–åˆ°æ›´å¤šä»£ç†ï¼Œå¹¶é€‚åº”æ–°çš„æ¸¸æˆé¢†åŸŸï¼Œå¦‚Minecraftã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„MindAgentåŸºç¡€è®¾æ–½å’ŒCUISINEWORLDæ¸¸æˆåœºæ™¯ä¸ºè¯„ä¼°LLMsåœ¨æ¸¸æˆäº¤äº’ä¸­çš„æ¶Œç°å¼è§„åˆ’ä¸åè°ƒèƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼ŒLLMsåœ¨å¤šæ™ºèƒ½ä½“è§„åˆ’æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæœ‰æœ›åœ¨æœªæ¥æ¨åŠ¨æ¸¸æˆAIçš„å‘å±•ã€‚

## agents--an-open-source-framework-for-autonomous-language-agents
### Abstract
Recent advances on large language models (LLMs) enable researchers and
developers to build autonomous language agents that can automatically solve
various tasks and interact with environments, humans, and other agents using
natural language interfaces. We consider language agents as a promising
direction towards artificial general intelligence and release Agents, an
open-source library with the goal of opening up these advances to a wider
non-specialist audience. Agents is carefully engineered to support important
features including planning, memory, tool usage, multi-agent communication, and
fine-grained symbolic control. Agents is user-friendly as it enables
non-specialists to build, customize, test, tune, and deploy state-of-the-art
autonomous language agents without much coding. The library is also
research-friendly as its modularized design makes it easily extensible for
researchers. Agents is available at https://github.com/aiwaves-cn/agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼€æºæ¡†æ¶ Agentsï¼šæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†çš„åˆ©å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…èƒ½å¤Ÿæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿè‡ªåŠ¨è§£å†³å„ç§ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢ä¸ç¯å¢ƒã€äººç±»å’Œå…¶ä»–ä»£ç†è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­è¨€ä»£ç†æ¡†æ¶å¾€å¾€ç¼ºä¹æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œéš¾ä»¥æ»¡è¶³éä¸“ä¸šäººå£«çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Agentsï¼Œä¸€ä¸ªå¼€æºçš„è‡ªä¸»è¯­è¨€ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨è®©æ›´å¹¿æ³›çš„éä¸“ä¸šäººå£«èƒ½å¤Ÿè½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²æœ€å…ˆè¿›çš„è‡ªä¸»è¯­è¨€ä»£ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ”¯æŒå…³é”®åŠŸèƒ½
Agents æ¡†æ¶ç²¾å¿ƒè®¾è®¡ï¼Œæ”¯æŒè§„åˆ’ã€è®°å¿†ã€å·¥å…·ä½¿ç”¨ã€å¤šä»£ç†é€šä¿¡å’Œç»†ç²’åº¦ç¬¦å·æ§åˆ¶ç­‰å…³é”®åŠŸèƒ½ã€‚è¿™ä½¿å¾—è¯­è¨€ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å„ç§ä»»åŠ¡å’Œç¯å¢ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§
Agents æ¡†æ¶çš„ç”¨æˆ·å‹å¥½æ€§ä½“ç°åœ¨å…¶å…è®¸éä¸“ä¸šäººå£«è½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²è‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè€Œæ— éœ€å¤§é‡ç¼–ç ã€‚åŒæ—¶ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾æ‰©å±•æ¡†æ¶ï¼Œä»¥æ»¡è¶³ä»–ä»¬çš„ç ”ç©¶éœ€æ±‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAgent Hub å¹³å°
Agents æ¡†æ¶å¼•å…¥äº† Agent Hub å¹³å°ï¼Œå…è®¸ç”¨æˆ·åˆ†äº«ä»–ä»¬å¾®è°ƒçš„è¯­è¨€ä»£ç†ï¼Œå¹¶æœç´¢/ä¸‹è½½å…¶ä»–ç”¨æˆ·åˆ†äº«çš„æœ‰ç”¨è¯­è¨€ä»£ç†ã€‚è¿™å¤§å¤§é™ä½äº†ä»å¤´å¼€å§‹è®¾è®¡å’Œè°ƒæ•´è¯­è¨€ä»£ç†çš„éš¾åº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè‡ªåŠ¨åˆ›å»ºä»£ç†ç³»ç»Ÿ
ä¸ºäº†å‡å°‘ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®š SOP çš„ç¹çå·¥ä½œï¼ŒAgents æ¡†æ¶å®ç°äº†ä¸€ä¸ªè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹ã€‚è¯¥æµç¨‹åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)ï¼Œå¯ä»¥è‡ªåŠ¨åˆ›å»ºå…¶ä»–ä»£ç†å’Œå¤šä»£ç†ç³»ç»Ÿã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡å±•ç¤ºäº†ä½¿ç”¨ Agents æ¡†æ¶æ„å»ºçš„å•ä»£ç†ç³»ç»Ÿå’Œå¤šä»£ç†ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒåŒ…æ‹¬é—²èŠæœºå™¨äººã€åŸºäºçŸ¥è¯†åº“å’Œæœç´¢å¼•æ“çš„å®¢æˆ·æœåŠ¡ä»£ç†ã€è´­ç‰©åŠ©æ‰‹ä»£ç†å’Œé”€å”®ä»£ç†ç­‰ã€‚è¿™äº›æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº† Agents æ¡†æ¶çš„æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠæ„å»ºå„ç§ç”¨ä¾‹çš„è¯­è¨€ä»£ç†çš„å¯èƒ½æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Agents æ¡†æ¶ä¸ºæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå…¶æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…çš„ç†æƒ³é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒAgent Hub å¹³å°å’Œè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹è¿›ä¸€æ­¥æé«˜äº†æ¡†æ¶çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚

## exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf
### Abstract
Communication games, which we refer to as incomplete information games that
heavily depend on natural language communication, hold significant research
value in fields such as economics, social science, and artificial intelligence.
In this work, we explore the problem of how to engage large language models
(LLMs) in communication games, and in response, propose a tuning-free
framework. Our approach keeps LLMs frozen, and relies on the retrieval and
reflection on past communications and experiences for improvement. An empirical
study on the representative and widely-studied communication game,
``Werewolf'', demonstrates that our framework can effectively play Werewolf
game without tuning the parameters of the LLMs. More importantly, strategic
behaviors begin to emerge in our experiments, suggesting that it will be a
fruitful journey to engage LLMs in communication games and associated domains.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ²Ÿé€šæ¸¸æˆä¸­çš„åº”ç”¨ï¼šä»¥ç‹¼äººæ€ä¸ºä¾‹çš„å®è¯ç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ²Ÿé€šæ¸¸æˆï¼Œå¦‚ç‹¼äººæ€ï¼Œæ˜¯ä¸€ç§é‡è¦çš„ç ”ç©¶å·¥å…·ï¼Œå¯ä»¥ç”¨æ¥æ¢ç´¢ç»æµå­¦ã€ç¤¾ä¼šç§‘å­¦å’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸä¸­çš„å„ç§é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIä»£ç†åœ¨ç©è¿™ç±»æ¸¸æˆæ—¶ï¼Œè¦ä¹ˆå¯¹è¯­è¨€çš„ä½¿ç”¨æœ‰ä¸¥æ ¼çš„é™åˆ¶ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨æ•°æ®ï¼Œè¿™ä½¿å¾—AIä»£ç†åœ¨è‡ªç„¶åœ°ç©è¿™ç±»æ¸¸æˆæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œç”¨äºç©æ²Ÿé€šæ¸¸æˆï¼Œå¹¶ä»¥ç‹¼äººæ€ä¸ºä¾‹è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå†å²ä¿¡æ¯æ”¶é›†
ä¸ºäº†è§£å†³LLMçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ä¸‰ä¸ªè§’åº¦ï¼ˆæ–°é²œåº¦ã€ä¿¡æ¯é‡å’Œå®Œæ•´æ€§ï¼‰æ”¶é›†å†å²ä¿¡æ¯çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ï¼š
- æ”¶é›†æœ€è¿‘çš„Kæ¡æ¶ˆæ¯ï¼›
- ä½¿ç”¨è§„åˆ™åŒ¹é…å’Œå¯å‘å¼æŒ‡æ ‡é€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„Næ¡æ¶ˆæ¯ï¼›
- é€šè¿‡å›ç­”é—®é¢˜çš„æ–¹å¼ï¼Œä»æ•´ä¸ªå†å²ä¸­æå–æ›´å¤šä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»éªŒå­¦ä¹ 
ä¸ºäº†ä½¿LLMèƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§éå‚æ•°å­¦ä¹ æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ï¼š
- åœ¨æ¯è½®æ¸¸æˆç»“æŸåï¼Œæ”¶é›†æ‰€æœ‰ç©å®¶çš„å“åº”ã€åæ€å’Œå¾—åˆ†ï¼Œå½¢æˆç»éªŒæ± ï¼›
- åœ¨æ–°çš„ä¸€è½®æ¸¸æˆä¸­ï¼Œæ ¹æ®å½“å‰æƒ…å†µä»ç»éªŒæ± ä¸­æ£€ç´¢æœ€ç›¸å…³çš„ç»éªŒï¼Œå¹¶ä»ä¸­æå–å»ºè®®ï¼Œä»¥æŒ‡å¯¼LLMçš„æ¨ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ç©ç‹¼äººæ€æ¸¸æˆï¼Œå¹¶ä¸”èƒ½å¤Ÿä»ç»éªŒä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€å¾®è°ƒLLMçš„å‚æ•°ã€‚æ­¤å¤–ï¼Œå®éªŒä¸­è¿˜è§‚å¯Ÿåˆ°ä¸€äº›ç­–ç•¥æ€§è¡Œä¸ºï¼Œå¦‚ä¿¡ä»»ã€å¯¹æŠ—ã€ä¼ªè£…å’Œé¢†å¯¼ï¼Œè¿™äº›è¡Œä¸ºå¹¶éé¢„å…ˆç¼–ç¨‹ï¼Œè€Œæ˜¯è‡ªå‘åœ°ä»LLMä¸­æ¶Œç°å‡ºæ¥çš„ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºä½¿ç”¨LLMç©æ²Ÿé€šæ¸¸æˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥ç ”ç©¶LLMåœ¨æ²Ÿé€šæ¸¸æˆä¸­çš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„ç»éªŒå­¦ä¹ æœºåˆ¶ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚å¯¹è¯ç³»ç»Ÿå’Œæ¨èç³»ç»Ÿã€‚

## are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis
### Abstract
Since the introduction of ChatGPT and GPT-4, these models have been tested
across a large number of tasks. Their adeptness across domains is evident, but
their aptitude in playing games, and specifically their aptitude in the realm
of poker has remained unexplored. Poker is a game that requires decision making
under uncertainty and incomplete information. In this paper, we put ChatGPT and
GPT-4 through the poker test and evaluate their poker skills. Our findings
reveal that while both models display an advanced understanding of poker,
encompassing concepts like the valuation of starting hands, playing positions
and other intricacies of game theory optimal (GTO) poker, both ChatGPT and
GPT-4 are NOT game theory optimal poker players.
  Profitable strategies in poker are evaluated in expectations over large
samples. Through a series of experiments, we first discover the characteristics
of optimal prompts and model parameters for playing poker with these models.
Our observations then unveil the distinct playing personas of the two models.
We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This
exploration then sheds light on the divergent poker tactics of the two models:
ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker
vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which
means that it has a propensity to only engage with premium hands and folds a
majority of hands. When subjected to the same directive, GPT-4 plays like a
maniac, showcasing a loose and aggressive style of play. Both strategies,
although relatively advanced, are not game theory optimal.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹ä¸­çš„è¡¨ç°ï¼šä¸€åœºå‰ç¿»ç‰Œåˆ†æ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ ChatGPT å’Œ GPT-4 çš„æ¨å‡ºï¼Œè¿™äº›æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¸¸æˆï¼Œå°¤å…¶æ˜¯æ‰‘å…‹æ¸¸æˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ‰‘å…‹æ˜¯ä¸€ç§éœ€è¦åœ¨ä¸å®Œæ•´ä¿¡æ¯å’Œä¸ç¡®å®šæ€§ä¸‹åšå‡ºå†³ç­–çš„æ¸¸æˆï¼Œå› æ­¤æœ¬æ–‡æ—¨åœ¨è¯„ä¼° ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨å‰ç¿»ç‰Œé˜¶æ®µçš„å†³ç­–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œè¯„ä¼°äº† ChatGPT å’Œ GPT-4 åœ¨å¾·å·æ‰‘å…‹å‰ç¿»ç‰Œé˜¶æ®µçš„å†³ç­–èƒ½åŠ›ã€‚å®éªŒä¸­ï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨äº†ä¸åŒçš„æç¤ºå’Œæ¨¡å‹å‚æ•°ï¼Œä»¥æ¢ç´¢ä¸¤ç§æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„æœ€ä½³è¡¨ç°ã€‚ä»–ä»¬è¿˜åˆ†æäº†ä¸¤ç§æ¨¡å‹çš„ç‹¬ç‰¹æ¸¸æˆé£æ ¼ï¼Œå¹¶æ¯”è¾ƒäº†å®ƒä»¬ä¸æ¸¸æˆç†è®ºæœ€ä¼˜ï¼ˆGTOï¼‰ç­–ç•¥çš„å·®å¼‚ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒChatGPT å’Œ GPT-4 éƒ½å¯¹æ‰‘å…‹æ¸¸æˆæœ‰æ·±å…¥çš„ç†è§£ï¼ŒåŒ…æ‹¬èµ·æ‰‹ç‰Œçš„ä¼°å€¼ã€æ¸¸æˆä½ç½®å’Œå…¶ä»–æ¸¸æˆç†è®ºæœ€ä¼˜ç­–ç•¥çš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œä¸¤ç§æ¨¡å‹éƒ½ä¸æ˜¯æ¸¸æˆç†è®ºæœ€ä¼˜çš„æ‰‘å…‹ç©å®¶ã€‚ChatGPT å€¾å‘äºä¿å®ˆçš„æ¸¸æˆé£æ ¼ï¼Œåªå‚ä¸ä¼˜è´¨ç‰Œå±€ï¼Œè€Œ GPT-4 åˆ™è¡¨ç°å‡ºæ›´åŠ æ¿€è¿›çš„æ¸¸æˆé£æ ¼ï¼Œå‚ä¸æ›´å¤šçš„ç‰Œå±€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ ChatGPT å’Œ GPT-4 åœ¨æ‰‘å…‹æ¸¸æˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚è¿™äº›æ¨¡å‹åœ¨ç†è§£æ¸¸æˆç†è®ºæœ€ä¼˜ç­–ç•¥æ–¹é¢å­˜åœ¨åå·®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå®ƒä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰é’ˆå¯¹æ‰‘å…‹æ¸¸æˆè¿›è¡Œä¸“é—¨è®­ç»ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼Œä¸åŒçš„æç¤ºå’Œæ¨¡å‹å‚æ•°å¯¹æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°æœ‰æ˜¾è‘—å½±å“ã€‚å› æ­¤ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•é€šè¿‡ä¼˜åŒ–æç¤ºå’Œæ¨¡å‹å‚æ•°æ¥æé«˜æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°ã€‚

## towards-ontology-construction-with-language-models
### Abstract
We present a method for automatically constructing a concept hierarchy for a
given domain by querying a large language model. We apply this method to
various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can
be of considerable help for constructing concept hierarchies.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºæœ¬ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æœ¬ä½“æ˜¯é¢†åŸŸå†…æ¦‚å¿µåŠå…¶å…³ç³»çš„æ­£å¼è¡¨ç¤ºï¼Œæ˜¯é«˜åº¦ç»“æ„åŒ–çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ„å»ºå’Œç¼–è¾‘æœ¬ä½“æ˜¯ä¸€é¡¹è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„å·¥ç¨‹ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦é¢†åŸŸä¸“å®¶çš„å‚ä¸ï¼Œä½†é¢†åŸŸä¸“å®¶çš„çŸ¥è¯†å’Œæœ¬ä½“å·¥ç¨‹ä¸“ä¸šçŸ¥è¯†å¾€å¾€ä¸åœ¨åŒä¸€äººæ‰‹ä¸­ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸å‡è®¾æœ¬ä½“çš„æ¨¡å¼ï¼ˆå³è¦ä½¿ç”¨çš„æ¦‚å¿µå’Œå±æ€§åç§°çš„é›†åˆï¼‰æ˜¯é¢„å…ˆé€‰æ‹©çš„ï¼Œç„¶åä½œä¸ºæ–¹æ³•çš„è¾“å…¥æä¾›ã€‚ç„¶è€Œï¼Œå½“ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œè¿™ä¼¼ä¹ä¸æ˜¯ä¸€ä¸ªå¥½çš„é€‰æ‹©ï¼Œå› ä¸ºè‡³å°‘æœ‰ä¸¤ä¸ªåŸå› ã€‚é¦–å…ˆï¼Œä¸ºæ•´ä¸ªé¢†åŸŸè®¾è®¡æ¨¡å¼æœ¬èº«å°±æ˜¯ä¸€ä¸ªéå¹³å‡¡çš„ä»»åŠ¡ï¼Œéœ€è¦é¢†åŸŸä¸“å®¶å¹¶æ¶‰åŠè®¸å¤šè®¾è®¡å†³ç­–ã€‚å®é™…ä¸Šï¼Œè®¾è®¡æ¨¡å¼å’Œæ¦‚å¿µå±‚æ¬¡ç»“æ„æ˜¯ç´§å¯†ç›¸è¿çš„ã€‚å…¶æ¬¡ï¼ŒLLMçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºæ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå…³é”®è¯å’ŒçŸ­è¯­ï¼Œå› æ­¤å®ƒä»¬æ˜¯æå‡ºç»™å®šé¢†åŸŸæ¦‚å¿µå’Œå±æ€§åç§°çš„å®Œç¾å·¥å…·ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æœ¬ä½“æ„å»ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æœ¬ä½“æ„å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æŸ¥è¯¢å¤§å‹è¯­è¨€æ¨¡å‹æ¥è‡ªåŠ¨æ„å»ºç»™å®šé¢†åŸŸçš„æ¦‚å¿µå±‚æ¬¡ç»“æ„ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼ŒLLMéšå«åœ°åŒ…å«å¤§é‡çš„çŸ¥è¯†ï¼Œå¹¶ä¸”å¯ä»¥åƒä¸“å®¶ä¸€æ ·å›ç­”é—®é¢˜ï¼Œä»è€Œå¸®åŠ©æ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **ç§å­æ¦‚å¿µé€‰æ‹©**ï¼šé€‰æ‹©ä¸€ä¸ªç§å­æ¦‚å¿µï¼Œä¾‹å¦‚â€œåŠ¨ç‰©â€ï¼Œä½œä¸ºæ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„çš„èµ·ç‚¹ã€‚
2. **æ¦‚å¿µå±‚æ¬¡ç»“æ„æ¢ç´¢**ï¼šé€šè¿‡é‡å¤è¯¢é—®LLMæä¾›å·²ç»å­˜åœ¨äºå±‚æ¬¡ç»“æ„ä¸­çš„æ¦‚å¿µçš„ç›¸å…³å­æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨éå†ç®—æ³•å°†æ–°æ¦‚å¿µæ”¾ç½®åœ¨å±‚æ¬¡ç»“æ„ä¸­ã€‚
3. **æ¦‚å¿µæè¿°**ï¼šè¯¢é—®LLMæä¾›æ¯ä¸ªæ¦‚å¿µçš„æ–‡æœ¬æè¿°ï¼Œä»¥ä¾¿äºäººç±»ç”¨æˆ·ç†è§£å’Œè§£é‡ŠLLMæå‡ºçš„æ¦‚å¿µã€‚
4. **æ¦‚å¿µéªŒè¯**ï¼šé€šè¿‡å‘LLMæå‡ºé¢å¤–çš„æŸ¥è¯¢æ¥éªŒè¯LLMçš„è¾“å‡ºï¼Œä»¥è¿‡æ»¤æ‰é”™è¯¯çš„ç­”æ¡ˆã€‚
5. **æ¦‚å¿µæ’å…¥**ï¼šä½¿ç”¨KRISç®—æ³•å°†æ–°æ¦‚å¿µæ’å…¥åˆ°æ¦‚å¿µå±‚æ¬¡ç»“æ„ä¸­ï¼Œå¹¶å¤„ç†å­æ¦‚å¿µ/è¶…æ¦‚å¿µå…³ç³»å’ŒåŒä¹‰è¯æ£€æµ‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ä½¿ç”¨OpenAIçš„GPT 3.5å¯¹å„ç§é¢†åŸŸï¼ˆå¦‚åŠ¨ç‰©ã€é¥®æ–™ã€éŸ³ä¹å’Œæ¤ç‰©ï¼‰è¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°å¸®åŠ©æ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„ï¼Œå°½ç®¡ä»ç„¶å­˜åœ¨ä¸€äº›å¹»è§‰å’Œé”™è¯¯ã€‚é€šè¿‡éªŒè¯å’Œä»”ç»†çš„æç¤ºå·¥ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘è¿™äº›é”™è¯¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†å¦‚ä½•å°†äººç±»é¢†åŸŸä¸“å®¶çš„äº¤äº’çº³å…¥æœ¬ä½“æ„å»ºè¿‡ç¨‹ï¼Œä»¥åŠå¦‚ä½•è¯„ä¼°æ„å»ºçš„æœ¬ä½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºåˆ©ç”¨LLMæ„å»ºæœ¬ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **åˆ©ç”¨LLMçš„çŸ¥è¯†å’Œç”Ÿæˆèƒ½åŠ›**ï¼šLLMå¯ä»¥åƒä¸“å®¶ä¸€æ ·å›ç­”é—®é¢˜ï¼Œå¹¶æä¾›ç›¸å…³å­æ¦‚å¿µå’Œæ¦‚å¿µæè¿°ï¼Œä»è€Œå¸®åŠ©æ„å»ºæ¦‚å¿µå±‚æ¬¡ç»“æ„ã€‚
* **æ¦‚å¿µéªŒè¯å’Œæç¤ºå·¥ç¨‹**ï¼šé€šè¿‡éªŒè¯å’Œæç¤ºå·¥ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘LLMçš„å¹»è§‰å’Œé”™è¯¯ï¼Œæé«˜æ„å»ºæœ¬ä½“çš„è´¨é‡ã€‚
* **äººç±»é¢†åŸŸä¸“å®¶çš„äº¤äº’**ï¼šå°†äººç±»é¢†åŸŸä¸“å®¶çš„äº¤äº’çº³å…¥æœ¬ä½“æ„å»ºè¿‡ç¨‹ï¼Œå¯ä»¥å¸®åŠ©è§£å†³è®¾è®¡å†³ç­–å’Œå¼•å…¥â€œç¥ç§˜â€æ¦‚å¿µçš„é—®é¢˜ã€‚
* **æœ¬ä½“è¯„ä¼°**ï¼šä½¿ç”¨æ‰‹åŠ¨è¯„ä¼°æˆ–ç°æœ‰åˆ†ç±»æ³•è¿›è¡Œæœ¬ä½“è¯„ä¼°ï¼Œå¯ä»¥å¸®åŠ©è¯„ä¼°æ„å»ºæœ¬ä½“çš„è´¨é‡å’Œå®Œæ•´æ€§ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸ºåˆ©ç”¨LLMæ„å»ºæœ¬ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶å±•ç¤ºäº†LLMåœ¨æ„å»ºæœ¬ä½“æ–¹é¢çš„æ½œåŠ›ã€‚

## agentverse--facilitating-multi-agent-collaboration-and-exploring-emergent-behaviors
### Abstract
Autonomous agents empowered by Large Language Models (LLMs) have undergone
significant improvements, enabling them to generalize across a broad spectrum
of tasks. However, in real-world scenarios, cooperation among individuals is
often required to enhance the efficiency and effectiveness of task
accomplishment. Hence, inspired by human group dynamics, we propose a
multi-agent framework \framework that can collaboratively and dynamically
adjust its composition as a greater-than-the-sum-of-its-parts system. Our
experiments demonstrate that \framework framework can effectively deploy
multi-agent groups that outperform a single agent. Furthermore, we delve into
the emergence of social behaviors among individual agents within a group during
collaborative task accomplishment. In view of these behaviors, we discuss some
possible strategies to leverage positive ones and mitigate negative ones for
improving the collaborative potential of multi-agent groups. Our codes for
\framework will soon be released at
\url{https://github.com/OpenBMB/AgentVerse}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AgentVerseï¼šå¤šæ™ºèƒ½ä½“åä½œä¸æ¶Œç°è¡Œä¸ºçš„æ¢ç´¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªä¸»æ™ºèƒ½ä½“åœ¨ç†è§£å’Œæ‰§è¡Œä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„å¤æ‚ä»»åŠ¡å¾€å¾€éœ€è¦ä¸ªä½“ä¹‹é—´çš„åä½œæ‰èƒ½æ›´æœ‰æ•ˆåœ°å®Œæˆã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»å›¢é˜Ÿåä½œè¿‡ç¨‹ï¼Œæå‡ºäº†ä¸€ç§åä¸º AgentVerse çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œå¹¶æ¢ç´¢æ¶Œç°è¡Œä¸ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šAgentVerse æ¡†æ¶
AgentVerse æ¡†æ¶æ¨¡æ‹Ÿäº†äººç±»å›¢é˜Ÿè§£å†³é—®é¢˜çš„è¿‡ç¨‹ï¼Œå¹¶å°†å…¶åˆ†ä¸ºå››ä¸ªå…³é”®é˜¶æ®µï¼š
1. **ä¸“å®¶æ‹›å‹Ÿ**ï¼šæ ¹æ®å½“å‰é—®é¢˜è§£å†³è¿›åº¦åŠ¨æ€è°ƒæ•´æ™ºèƒ½ä½“å›¢é˜Ÿçš„ç»„æˆã€‚
2. **åä½œå†³ç­–**ï¼šè®©é€‰å®šçš„æ™ºèƒ½ä½“è¿›è¡Œè”åˆè®¨è®ºï¼Œåˆ¶å®šé—®é¢˜è§£å†³ç­–ç•¥ã€‚
3. **è¡ŒåŠ¨æ‰§è¡Œ**ï¼šæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’ï¼Œæ‰§è¡Œåˆ¶å®šçš„è¡ŒåŠ¨ã€‚
4. **è¯„ä¼°**ï¼šè¯„ä¼°å½“å‰çŠ¶æ€ä¸é¢„æœŸç›®æ ‡ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨æ€è°ƒæ•´ä¸æ¶Œç°è¡Œä¸º
AgentVerse å…è®¸æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€è°ƒæ•´æ™ºèƒ½ä½“å›¢é˜Ÿçš„ç»„æˆï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æ­ç¤ºäº†æ™ºèƒ½ä½“åœ¨åä½œè¿‡ç¨‹ä¸­å‡ºç°çš„æ¶Œç°è¡Œä¸ºï¼ŒåŒ…æ‹¬ï¼š
* **å¿—æ„¿è¡Œä¸º**ï¼šæ™ºèƒ½ä½“ä¸»åŠ¨æä¾›å¸®åŠ©ï¼Œæé«˜å›¢é˜Ÿæ•ˆç‡ã€‚
* **ä»ä¼—è¡Œä¸º**ï¼šæ™ºèƒ½ä½“è°ƒæ•´è‡ªèº«è¡Œä¸ºä»¥ç¬¦åˆå…±åŒç›®æ ‡ã€‚
* **ç ´åæ€§è¡Œä¸º**ï¼šæ™ºèƒ½ä½“é‡‡å–å¯èƒ½å¯¼è‡´ä¸è‰¯åæœçš„è¡ŒåŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AgentVerse åœ¨æ–‡æœ¬ç†è§£ã€æ¨ç†ã€ç¼–ç ã€å·¥å…·åˆ©ç”¨å’Œå…·èº« AI ç­‰ä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼š
* AgentVerse èƒ½å¤Ÿæœ‰æ•ˆåœ°æå‡æ™ºèƒ½ä½“çš„ç†è§£ã€æ¨ç†ã€ç¼–ç å’Œå·¥å…·åˆ©ç”¨èƒ½åŠ›ã€‚
* ä¸å•ä¸ªæ™ºèƒ½ä½“ç›¸æ¯”ï¼ŒAgentVerse ç»„æˆçš„å›¢é˜Ÿåœ¨åä½œå†³ç­–å’Œè¡ŒåŠ¨æ‰§è¡Œæ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ã€‚
* æ™ºèƒ½ä½“åœ¨åä½œè¿‡ç¨‹ä¸­å‡ºç°çš„æ¶Œç°è¡Œä¸ºå¯¹å›¢é˜Ÿæ•ˆç‡æœ‰ç§¯æå½±å“ï¼Œä½†ä¹Ÿå­˜åœ¨æ½œåœ¨é£é™©ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AgentVerse æ¡†æ¶ä¸ºå¤šæ™ºèƒ½ä½“åä½œæä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶åŠ¨æ€è°ƒæ•´å’Œæ¶Œç°è¡Œä¸ºåˆ†æå¯¹æ„å»ºæ›´é«˜æ•ˆã€æ›´æ™ºèƒ½çš„è‡ªä¸»ç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´å…ˆè¿›çš„æ„ŸçŸ¥æ™ºèƒ½ä½“ï¼Œå¹¶è®¾è®¡ç­–ç•¥æ¥åˆ©ç”¨ç§¯æè¡Œä¸ºå¹¶å‡è½»æ½œåœ¨é£é™©ã€‚

## gameeval--evaluating-llms-on-conversational-games
### Abstract
The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GameEvalï¼šé€šè¿‡å¯¹è¯æ¸¸æˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°è¯„ä¼°è¿™äº›æ¨¡å‹çš„èƒ½åŠ›æˆä¸ºä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºå‚è€ƒå’ŒåŸºäºåå¥½çš„æ–¹æ³•ã€‚åŸºäºå‚è€ƒçš„æ–¹æ³•éœ€è¦ä¸é¢„å…ˆç¡®å®šçš„ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œè€ŒåŸºäºåå¥½çš„æ–¹æ³•åˆ™ä¾èµ–äºäººç±»æˆ–æ¨¡å‹è¯„ä¼°è€…çš„åå¥½ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚è·å–é«˜è´¨é‡æ ‡æ³¨çš„æˆæœ¬é«˜ã€æ—¶é—´æ¶ˆè€—å¤§ï¼Œä»¥åŠå¼•å…¥è¯„ä¼°è€…çš„åå¥½åå·®ç­‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†GameEvalï¼Œä¸€ç§é€šè¿‡ç›®æ ‡é©±åŠ¨çš„å¯¹è¯æ¸¸æˆæ¥è¯„ä¼°LLMsçš„æ–°æ–¹æ³•ã€‚GameEvalå°†LLMsè§†ä¸ºæ¸¸æˆç©å®¶ï¼Œå¹¶ä¸ºå…¶åˆ†é…å…·æœ‰ç‰¹å®šç›®æ ‡çš„ç‹¬ç‰¹è§’è‰²ï¼Œé€šè¿‡å¯åŠ¨å„ç§å½¢å¼çš„å¯¹è¯ï¼ˆåŒ…æ‹¬è®¨è®ºã€é—®ç­”å’ŒæŠ•ç¥¨ï¼‰æ¥å®ç°è¿™äº›ç›®æ ‡ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸‰ç§ç‹¬ç‰¹çš„æ¸¸æˆï¼ŒåŒ…æ‹¬åˆä½œå’Œå¯¹æŠ—ç›®æ ‡ï¼Œå¹¶é…å¤‡äº†ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥å±•ç¤ºè¿™ç§æ–°èŒƒå¼å¦‚ä½•å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæœ¬æ–‡å±•ç¤ºäº†GameEvalèƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒLLMsçš„èƒ½åŠ›ï¼Œå¹¶æä¾›å¯¹å…¶è§£å†³å¤æ‚é—®é¢˜ç»¼åˆèƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGameEvalåœ¨åŒºåˆ†ChatGPTå’ŒGPT-4ç­‰æ¨¡å‹çš„èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œç°æœ‰æ–¹æ³•åˆ™éš¾ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GameEvalæä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°LLMsçš„æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶å‡å°‘è¯„ä¼°åå·®ã€‚æ­¤å¤–ï¼ŒGameEvalè¿˜å¯ä»¥ç”¨äºè®¾è®¡æ–°çš„æ¸¸æˆï¼Œä»¥è¯„ä¼°LLMsåœ¨ç°å®ä¸–ç•Œå¤æ‚åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚

## autogen--enabling-next-gen-llm-applications-via-multi-agent-conversation
### Abstract
AutoGen is an open-source framework that allows developers to build LLM
applications via multiple agents that can converse with each other to
accomplish tasks. AutoGen agents are customizable, conversable, and can operate
in various modes that employ combinations of LLMs, human inputs, and tools.
Using AutoGen, developers can also flexibly define agent interaction behaviors.
Both natural language and computer code can be used to program flexible
conversation patterns for different applications. AutoGen serves as a generic
infrastructure to build diverse applications of various complexities and LLM
capacities. Empirical studies demonstrate the effectiveness of the framework in
many example applications, with domains ranging from mathematics, coding,
question answering, operations research, online decision-making, entertainment,
etc.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AutoGenï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“å¯¹è¯èµ‹èƒ½ä¸‹ä¸€ä»£LLMåº”ç”¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨LLMsæ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºæˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å•æ™ºèƒ½ä½“æ¨¡å‹å¾€å¾€éš¾ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼Œè€Œå¤šæ™ºèƒ½ä½“æ¨¡å‹åˆ™å¯ä»¥ååŒå·¥ä½œï¼Œå‘æŒ¥å„è‡ªçš„ä¼˜åŠ¿ï¼Œä»è€Œæé«˜ä»»åŠ¡å¤„ç†çš„æ•ˆç‡å’Œæ•ˆæœã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
AutoGenæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œå®ƒå…è®¸å¼€å‘è€…é€šè¿‡å¤šä¸ªå¯ä»¥ç›¸äº’å¯¹è¯çš„æ™ºèƒ½ä½“æ¥æ„å»ºLLMåº”ç”¨ç¨‹åºã€‚AutoGençš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ **å¯å®šåˆ¶å’Œå¯å¯¹è¯çš„æ™ºèƒ½ä½“**ï¼šAutoGençš„æ™ºèƒ½ä½“å¯ä»¥åŸºäºLLMsã€äººç±»è¾“å…¥ã€å·¥å…·æˆ–å®ƒä»¬çš„ç»„åˆæ¥æ„å»ºï¼Œå¹¶ä¸”å¯ä»¥ç›¸äº’å¯¹è¯ï¼Œæ¥æ”¶ã€ååº”å’Œå“åº”æ¶ˆæ¯ã€‚è¿™ä½¿å¾—å¼€å‘è€…å¯ä»¥è½»æ¾åœ°åˆ›å»ºå…·æœ‰ä¸åŒè§’è‰²å’Œèƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼Œä¾‹å¦‚ç¼–å†™ä»£ç ã€æ‰§è¡Œä»£ç ã€è·å–äººç±»åé¦ˆã€éªŒè¯è¾“å‡ºç­‰ã€‚

ğŸ’¡ **å¯¹è¯ç¼–ç¨‹**ï¼šAutoGené‡‡ç”¨äº†ä¸€ç§ä»¥å¯¹è¯ä¸ºä¸­å¿ƒçš„ç¼–ç¨‹èŒƒå¼ï¼Œå°†å¤æ‚çš„LLMåº”ç”¨ç¨‹åºå·¥ä½œæµç¨‹ç®€åŒ–ä¸ºå¤šæ™ºèƒ½ä½“å¯¹è¯ã€‚å¼€å‘è€…å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ç»„å…·æœ‰ç‰¹å®šèƒ½åŠ›å’Œè§’è‰²çš„å¯å¯¹è¯æ™ºèƒ½ä½“ï¼Œå¹¶ç¼–ç¨‹å®ƒä»¬ä¹‹é—´çš„äº¤äº’è¡Œä¸ºæ¥æ„å»ºåº”ç”¨ç¨‹åºã€‚AutoGenæ”¯æŒä½¿ç”¨è‡ªç„¶è¯­è¨€å’Œç¼–ç¨‹è¯­è¨€æ¥ç¼–ç¨‹çµæ´»çš„å¯¹è¯æ¨¡å¼ï¼Œå¹¶æä¾›äº†ä¸€ç³»åˆ—è®¾è®¡æ¨¡å¼æ¥ç®€åŒ–å¯¹è¯ç¼–ç¨‹ï¼Œä¾‹å¦‚ç»Ÿä¸€çš„å¯¹è¯æ¥å£ã€è‡ªåŠ¨å›å¤æœºåˆ¶ã€åŸºäºç¼–ç¨‹å’Œè‡ªç„¶è¯­è¨€çš„æ§åˆ¶æµç®¡ç†ç­‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AutoGenåœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ç¨‹åºä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ•°å­¦é—®é¢˜è§£å†³ã€ä»£ç ç”Ÿæˆã€é—®ç­”ã€å†³ç­–åˆ¶å®šã€åœ¨çº¿å†³ç­–ã€å¨±ä¹ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoGenå¯ä»¥å¸®åŠ©å®ç°è®¸å¤šä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ï¼Œå¹¶å‡å°‘å¼€å‘å·¥ä½œé‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AutoGenä¸ºæ„å»ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“LLMåº”ç”¨ç¨‹åºæä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **æ¨¡å—åŒ–è®¾è®¡**ï¼šAutoGençš„æ™ºèƒ½ä½“å¯ä»¥ç‹¬ç«‹å¼€å‘ã€æµ‹è¯•å’Œç»´æŠ¤ï¼Œä»è€Œç®€åŒ–äº†æ•´ä½“å¼€å‘å’Œä»£ç ç®¡ç†ã€‚
* **çµæ´»çš„å¯¹è¯ç¼–ç¨‹**ï¼šAutoGenæ”¯æŒä½¿ç”¨è‡ªç„¶è¯­è¨€å’Œç¼–ç¨‹è¯­è¨€æ¥ç¼–ç¨‹çµæ´»çš„å¯¹è¯æ¨¡å¼ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥æ ¹æ®ä¸åŒçš„åº”ç”¨éœ€æ±‚è¿›è¡Œå®šåˆ¶ã€‚
* **å¯æ‰©å±•æ€§**ï¼šAutoGençš„æ™ºèƒ½ä½“å¯ä»¥è½»æ¾åœ°æ‰©å±•å’Œå®šåˆ¶ï¼Œä»¥æ»¡è¶³ä¸åŒçš„åº”ç”¨éœ€æ±‚ã€‚

### ğŸŒŸ æ€»ç»“
AutoGenæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…æ„å»ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“LLMåº”ç”¨ç¨‹åºã€‚å®ƒå…·æœ‰æ¨¡å—åŒ–è®¾è®¡ã€çµæ´»çš„å¯¹è¯ç¼–ç¨‹å’Œå¯æ‰©å±•æ€§ç­‰ä¼˜ç‚¹ï¼Œä¸ºLLMåº”ç”¨ç¨‹åºçš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚

## calypso--llms-as-dungeon-masters--assistants
### Abstract
The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to
perform multiple tasks simultaneously. The DM must digest information about the
game setting and monsters, synthesize scenes to present to other players, and
respond to the players' interactions with the scene. Doing all of these tasks
while maintaining consistency within the narrative and story world is no small
feat of human cognition, making the task tiring and unapproachable to new
players. Large language models (LLMs) like GPT-3 and ChatGPT have shown
remarkable abilities to generate coherent natural language text. In this paper,
we conduct a formative evaluation with DMs to establish the use cases of LLMs
in D&D and tabletop gaming generally. We introduce CALYPSO, a system of
LLM-powered interfaces that support DMs with information and inspiration
specific to their own scenario. CALYPSO distills game context into bite-sized
prose and helps brainstorm ideas without distracting the DM from the game. When
given access to CALYPSO, DMs reported that it generated high-fidelity text
suitable for direct presentation to players, and low-fidelity ideas that the DM
could develop further while maintaining their creative agency. We see CALYPSO
as exemplifying a paradigm of AI-augmented tools that provide synchronous
creative assistance within established game worlds, and tabletop gaming more
broadly.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CALYPSOï¼šå¤§å‹è¯­è¨€æ¨¡å‹åŠ©åŠ›åœ°ä¸‹åŸä¸»

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ°ä¸‹åŸä¸é¾™ï¼ˆD&Dï¼‰æ˜¯ä¸€æ¬¾ç»å…¸çš„æ¡Œé¢è§’è‰²æ‰®æ¼”æ¸¸æˆï¼Œå…¶ä¸­åœ°ä¸‹åŸä¸»ï¼ˆDMï¼‰æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚DMéœ€è¦åŒæ—¶å¤„ç†å¤šé¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¶ˆåŒ–æ¸¸æˆèƒŒæ™¯å’Œæ€ªç‰©ä¿¡æ¯ã€æ„å»ºåœºæ™¯ã€å›åº”ç©å®¶äº’åŠ¨ç­‰ã€‚è¿™äº›ä»»åŠ¡å¯¹äººç±»è®¤çŸ¥èƒ½åŠ›è¦æ±‚æé«˜ï¼Œå¯¹äºæ–°ç©å®¶æ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-3å’ŒChatGPTåœ¨ç”Ÿæˆè¿è´¯çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMåœ¨D&Då’Œæ¡Œé¢æ¸¸æˆä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†CALYPSOç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨LLMä¸ºDMæä¾›ä¿¡æ¯å’Œæ”¯æŒï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°è¿›è¡Œæ¸¸æˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCALYPSOç³»ç»Ÿ
CALYPSOæ˜¯ä¸€ä¸ªç”±LLMé©±åŠ¨çš„ç•Œé¢ç³»ç»Ÿï¼Œæ—¨åœ¨æ”¯æŒDMåœ¨æ¸¸æˆä¸­è·å–ä¿¡æ¯å’Œçµæ„Ÿã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦åŠŸèƒ½ï¼š
1. **é­é‡ç†è§£**ï¼šä½¿ç”¨GPT-3å¯¹æ¸¸æˆèƒŒæ™¯å’Œæ€ªç‰©ä¿¡æ¯è¿›è¡Œæ‘˜è¦ï¼Œå¸®åŠ©DMå¿«é€Ÿç†è§£é­é‡ã€‚
2. **èšç„¦å¤´è„‘é£æš´**ï¼šä½¿ç”¨ChatGPTä¸DMè¿›è¡Œå¯¹è¯ï¼Œå¸®åŠ©ä»–ä»¬è¿›ä¸€æ­¥æ¢ç´¢é­é‡ç»†èŠ‚æˆ–ç”Ÿæˆæ–°çš„æƒ³æ³•ã€‚
3. **å¼€æ”¾åŸŸèŠå¤©åŸºçº¿**ï¼šä½¿ç”¨ChatGPTæä¾›ä¸€ä¸ªå¼€æ”¾åŸŸçš„èŠå¤©ç•Œé¢ï¼Œä¾›ç©å®¶å’ŒDMè¿›è¡Œéæ¸¸æˆç›¸å…³çš„äº¤æµã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMçš„åˆ›é€ æ€§è¾…åŠ©
CALYPSOç³»ç»Ÿå±•ç¤ºäº†LLMä½œä¸ºåˆ›é€ æ€§è¾…åŠ©å·¥å…·çš„æ½œåŠ›ã€‚å®ƒä¸ä»…èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„æ–‡æœ¬ï¼Œé€‚åˆç›´æ¥å‘ˆç°ç»™ç©å®¶ï¼Œè¿˜èƒ½å¤Ÿæä¾›ä½ä¿çœŸåº¦çš„æƒ³æ³•ï¼Œä¾›DMè¿›ä¸€æ­¥å‘å±•å’Œå®Œå–„ã€‚è¿™ç§è¾…åŠ©æ–¹å¼ä¿ç•™äº†DMçš„åˆ›é€ æ€§è‡ªä¸»æƒï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿæ›´å¥½åœ°ä¸“æ³¨äºæ¸¸æˆä¸­çš„è®¤çŸ¥ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDMåœ¨ä½¿ç”¨CALYPSOç³»ç»Ÿåï¼Œæ™®éè®¤ä¸ºå®ƒèƒ½å¤Ÿç”Ÿæˆé€‚åˆç›´æ¥å‘ˆç°ç»™ç©å®¶çš„æ–‡æœ¬ï¼Œå¹¶æä¾›æœ‰ä»·å€¼çš„çµæ„Ÿã€‚DMä»¬åˆ©ç”¨CALYPSOç³»ç»Ÿæ¥ç†è§£å¤æ‚çš„æ€ªç‰©ä¿¡æ¯ã€å¤´è„‘é£æš´éç©å®¶è§’è‰²æˆ–æ€ªç‰©ä¹‹é—´çš„äº’åŠ¨ï¼Œå¹¶è·å–å»ºè®®ï¼Œå°†è¿™äº›å»ºè®®èå…¥åˆ°æ•…äº‹ä¸­å‘ˆç°ç»™ç©å®¶ï¼Œè€Œä¸ä¼šå½±å“æ¸¸æˆçš„èŠ‚å¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœå¯¹äºå¼€å‘AIè¾…åŠ©å·¥å…·åœ¨æ¡Œé¢æ¸¸æˆå’Œå…¶ä»–åˆ›æ„é¢†åŸŸä¸­çš„åº”ç”¨å…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚CALYPSOç³»ç»Ÿçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–ç±»ä¼¼é¡¹ç›®æä¾›å‚è€ƒï¼Œä¾‹å¦‚ï¼š
1. **ç†è§£ç”¨æˆ·éœ€æ±‚**ï¼šé€šè¿‡è®¿è°ˆå’Œç”¨æˆ·ç ”ç©¶ï¼Œæ·±å…¥äº†è§£ç”¨æˆ·çš„éœ€æ±‚å’Œç—›ç‚¹ï¼Œä»è€Œè®¾è®¡å‡ºæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„AIè¾…åŠ©å·¥å…·ã€‚
2. **åˆ©ç”¨LLMçš„åˆ›é€ æ€§æ½œåŠ›**ï¼šLLMåœ¨ç”Ÿæˆè¿è´¯æ–‡æœ¬å’Œæä¾›åˆ›é€ æ€§çµæ„Ÿæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä»¥å°†å…¶åº”ç”¨äºå„ç§åˆ›æ„åœºæ™¯ã€‚
3. **ä¿æŒç”¨æˆ·çš„åˆ›é€ æ€§è‡ªä¸»æƒ**ï¼šAIè¾…åŠ©å·¥å…·åº”è¯¥ä½œä¸ºç”¨æˆ·çš„åŠ©æ‰‹ï¼Œè€Œä¸æ˜¯æ›¿ä»£è€…ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°å‘æŒ¥è‡ªå·±çš„åˆ›é€ åŠ›ã€‚

æ€»è€Œè¨€ä¹‹ï¼ŒCALYPSOç³»ç»Ÿå±•ç¤ºäº†LLMåœ¨æ¡Œé¢æ¸¸æˆä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œå¹¶ä¸ºå¼€å‘AIè¾…åŠ©å·¥å…·æä¾›äº†æœ‰ä»·å€¼çš„ç»éªŒå’Œå¯ç¤ºã€‚

## agentsims--an-open-source-sandbox-for-large-language-model-evaluation
### Abstract
With ChatGPT-like large language models (LLM) prevailing in the community,
how to evaluate the ability of LLMs is an open question. Existing evaluation
methods suffer from following shortcomings: (1) constrained evaluation
abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that
task-based evaluation, where LLM agents complete tasks in a simulated
environment, is a one-for-all solution to solve above problems. We present
AgentSims, an easy-to-use infrastructure for researchers from all disciplines
to test the specific capacities they are interested in. Researchers can build
their evaluation tasks by adding agents and buildings on an interactive GUI or
deploy and test new support mechanisms, i.e. memory, planning and tool-use
systems, by a few lines of codes. Our demo is available at
https://agentsims.com .
### ğŸŒŸ è®ºæ–‡è§£è¯» | AgentSimsï¼šå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å¼€æ”¾æºä»£ç æ²™ç›’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¤¾åŒºä¸­çš„æ™®åŠï¼Œå¦‚ä½•è¯„ä¼°LLMçš„èƒ½åŠ›æˆä¸ºä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ä»¥ä¸‹ä¸è¶³ï¼š
1. è¯„ä¼°èƒ½åŠ›å—é™ï¼šå¤§å¤šæ•°ä»»åŠ¡é‡‡ç”¨å•è½®é—®ç­”æ ¼å¼ï¼Œæ— æ³•å…¨é¢è¯„ä¼°LLMçš„å„ç§èƒ½åŠ›ã€‚
2. åŸºå‡†æ˜“å—æ”»å‡»ï¼šç”±äºLLMå…·æœ‰å¤§é‡çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œæµ‹è¯•é›†å®¹æ˜“æ— æ„ä¸­æ··å…¥è®­ç»ƒé›†ã€‚
3. æŒ‡æ ‡ä¸å®¢è§‚ï¼šç°æœ‰çš„å¼€æ”¾å¼é—®ç­”æŒ‡æ ‡æ¶‰åŠè‡ªåŠ¨æŒ‡æ ‡å’Œä¸»è§‚æŒ‡æ ‡ï¼Œæ— æ³•å®¢è§‚è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†åŸºäºä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•ï¼Œå³LLMä»£ç†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å®Œæˆä»»åŠ¡æ¥è¯æ˜å…¶èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†AgentSimsï¼Œä¸€ä¸ªæ˜“äºä½¿ç”¨çš„è¯„ä¼°LLMèƒ½åŠ›çš„å¹³å°ã€‚AgentSimså…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. å¯æ‰©å±•æ€§å’Œå¯ç»„åˆæ€§ï¼šå…è®¸ç”¨æˆ·ç»„åˆä¸åŒçš„è®¡åˆ’ã€è®°å¿†å’Œä½¿ç”¨å·¥å…·ç³»ç»Ÿï¼Œç ”ç©¶å„ç§ç³»ç»Ÿè®¾è®¡çš„å½±å“å’Œæœ‰æ•ˆæ€§ã€‚
2. äº¤äº’å¼ç”¨æˆ·ç•Œé¢ï¼šä¸ºåœ°å›¾è®¾è®¡å’Œä»£ç†åˆ›å»ºæä¾›äº¤äº’å¼UIï¼Œé™ä½éä¸“ä¸šäººå£«çš„å…¥é—¨é—¨æ§›ã€‚
3. æ ‡å‡†åŒ–å®ç°ï¼šç¡®ä¿å®éªŒç»“æœçš„å†ç°æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å±•ç¤ºäº†AgentSimsåœ¨è¯„ä¼°LLMèƒ½åŠ›æ–¹é¢çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ï¼š
1. è¯„ä¼°LLMçš„ç¤¾ä¼šèƒ½åŠ›ï¼Œå¦‚å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰ã€‚
2. è¯„ä¼°LLMçš„é•¿æœŸè§„åˆ’å’Œç»„ç»‡èƒ½åŠ›ï¼Œå¦‚æ‹…ä»»å¸‚é•¿æˆ–å…¬å¸æ€»è£ã€‚
3. ä½œä¸ºæ•°æ®ç”Ÿæˆå¹³å°ï¼Œç”¨äºæ•°æ®æ ‡æ³¨å’Œå¢å¼ºã€‚
4. ä¸ºç¤¾ä¼šç§‘å­¦ç ”ç©¶æä¾›å¯æ§çš„åˆæ­¥å®éªŒç¯å¢ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AgentSimsä¸ºLLMè¯„ä¼°æä¾›äº†ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„æ²™ç›’å¹³å°ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. åŸºäºä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚
2. äº¤äº’å¼ç”¨æˆ·ç•Œé¢ï¼Œé™ä½éä¸“ä¸šäººå£«çš„å…¥é—¨é—¨æ§›ã€‚
3. æ ‡å‡†åŒ–å®ç°ï¼Œç¡®ä¿å®éªŒç»“æœçš„å†ç°æ€§ã€‚
4. å¯æ‰©å±•æ€§å’Œå¯ç»„åˆæ€§ï¼Œæ–¹ä¾¿ç”¨æˆ·ç ”ç©¶å’Œå¼€å‘æ–°çš„æ”¯æŒç³»ç»Ÿã€‚

## metagpt--meta-programming-for-a-multi-agent-collaborative-framework
### Abstract
Remarkable progress has been made on automated problem solving through
societies of agents based on large language models (LLMs). Existing LLM-based
multi-agent systems can already solve simple dialogue tasks. Solutions to more
complex tasks, however, are complicated through logic inconsistencies due to
cascading hallucinations caused by naively chaining LLMs. Here we introduce
MetaGPT, an innovative meta-programming framework incorporating efficient human
workflows into LLM-based multi-agent collaborations. MetaGPT encodes
Standardized Operating Procedures (SOPs) into prompt sequences for more
streamlined workflows, thus allowing agents with human-like domain expertise to
verify intermediate results and reduce errors. MetaGPT utilizes an assembly
line paradigm to assign diverse roles to various agents, efficiently breaking
down complex tasks into subtasks involving many agents working together. On
collaborative software engineering benchmarks, MetaGPT generates more coherent
solutions than previous chat-based multi-agent systems. Our project can be
found at https://github.com/geekan/MetaGPT
### ğŸŒŸ è®ºæ–‡è§£è¯» | MetaGPTï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…ƒç¼–ç¨‹å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäºLLMsçš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨è‡ªåŠ¨åŒ–é—®é¢˜è§£å†³æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ—¶ï¼Œç”±äºç®€å•åœ°å°†LLMsä¸²è”èµ·æ¥å¯¼è‡´çš„çº§è”å¹»è§‰ï¼Œå¸¸å¸¸å‡ºç°é€»è¾‘ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MetaGPTï¼Œä¸€ä¸ªåˆ›æ–°çš„å…ƒç¼–ç¨‹æ¡†æ¶ï¼Œå°†é«˜æ•ˆçš„äººç±»å·¥ä½œæµç¨‹èå…¥åˆ°åŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“åä½œä¸­ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ ‡å‡†åŒ–æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰ç¼–ç åˆ°æç¤ºåºåˆ—ä¸­ï¼Œä»¥å®ç°æ›´æµç•…çš„å·¥ä½œæµç¨‹ã€‚è¿™ä½¿å¾—å…·æœ‰äººç±»æ°´å¹³çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„æ™ºèƒ½ä½“èƒ½å¤ŸéªŒè¯ä¸­é—´ç»“æœå¹¶å‡å°‘é”™è¯¯ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡‡ç”¨æµæ°´çº¿èŒƒå¼ï¼Œä¸ºå„ç§æ™ºèƒ½ä½“åˆ†é…ä¸åŒçš„è§’è‰²ï¼Œæœ‰æ•ˆåœ°å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ¶‰åŠå¤šä¸ªæ™ºèƒ½ä½“åä½œçš„å­ä»»åŠ¡ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¼•å…¥å¯æ‰§è¡Œåé¦ˆæœºåˆ¶ï¼Œåœ¨è¿è¡Œæ—¶è°ƒè¯•å’Œæ‰§è¡Œä»£ç ï¼Œæ˜¾è‘—æé«˜ä»£ç ç”Ÿæˆè´¨é‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åä½œè½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaGPTç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆæ¯”ä¹‹å‰çš„åŸºäºèŠå¤©çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ›´è¿è´¯ã€‚åœ¨HumanEvalå’ŒMBPPåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaGPTå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«è¾¾åˆ°äº†85.9%å’Œ87.7%çš„Pass@1ã€‚åœ¨è‡ªç”Ÿæˆçš„è½¯ä»¶å¼€å‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMetaGPTåœ¨å‡ ä¹æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºChatDevï¼Œå¹¶ä¸”å®ç°äº†100%çš„ä»»åŠ¡å®Œæˆç‡ï¼Œè¯æ˜äº†å…¶é²æ£’æ€§å’Œæ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MetaGPTæ¡†æ¶ä¸ºåŸºäºLLMsçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¼€å‘æä¾›äº†ä¸€ä¸ªçµæ´»ä¸”åŠŸèƒ½å¼ºå¤§çš„å¹³å°ã€‚å…¶å°†SOPsèå…¥è®¾è®¡ä¸­çš„åˆ›æ–°æ–¹æ³•ï¼Œä¸ºè§£å†³å¤æ‚é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œå¯æ‰§è¡Œåé¦ˆæœºåˆ¶ä¹Ÿä¸ºæé«˜ä»£ç ç”Ÿæˆè´¨é‡æä¾›äº†æœ‰æ•ˆé€”å¾„ã€‚MetaGPTçš„æˆåŠŸç»éªŒå¯ä»¥å¯å‘æœªæ¥ç ”ç©¶ï¼Œæ¢ç´¢æ›´å¤šåŸºäºäººç±»å®è·µçš„äººå·¥å¤šæ™ºèƒ½ä½“ç³»ç»ŸæŠ€æœ¯ã€‚

## emollm--multimodal-emotional-understanding-meets-large-language-models
### Abstract
Multi-modal large language models (MLLMs) have achieved remarkable
performance on objective multimodal perception tasks, but their ability to
interpret subjective, emotionally nuanced multimodal content remains largely
unexplored. Thus, it impedes their ability to effectively understand and react
to the intricate emotions expressed by humans through multimodal media. To
bridge this gap, we introduce EmoBench, the first comprehensive benchmark
designed specifically to evaluate the emotional capabilities of MLLMs across
five popular emotional tasks, using a diverse dataset of 287k images and videos
paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a
novel model for multimodal emotional understanding, incorporating with two core
techniques. 1) Multi-perspective Visual Projection, it captures diverse
emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it
guides MLLMs to reason about emotions in the correct direction. Experimental
results demonstrate that EmoLLM significantly elevates multimodal emotional
understanding performance, with an average improvement of 12.1% across multiple
foundation models on EmoBench. Our work contributes to the advancement of MLLMs
by facilitating a deeper and more nuanced comprehension of intricate human
emotions, paving the way for the development of artificial emotional
intelligence capabilities with wide-ranging applications in areas such as
human-computer interaction, mental health support, and empathetic AI systems.
Code, data, and model will be released.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EmoLLMï¼šå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç›®æ ‡å¤šæ¨¡æ€æ„ŸçŸ¥ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†å®ƒä»¬åœ¨è§£é‡Šä¸»è§‚ã€æƒ…æ„Ÿä¸°å¯Œçš„å¤šæ¨¡æ€å†…å®¹æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™é˜»ç¢äº†å®ƒä»¬æœ‰æ•ˆåœ°ç†è§£å’Œååº”äººç±»é€šè¿‡å¤šæ¨¡æ€åª’ä½“è¡¨è¾¾çš„æƒ…æ„Ÿã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†EmoBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°MLLMsåœ¨äº”ä¸ªæµè¡Œæƒ…æ„Ÿä»»åŠ¡ä¸­çš„æƒ…æ„Ÿèƒ½åŠ›çš„å…¨é¢åŸºå‡†ï¼Œä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«287kå›¾åƒå’Œè§†é¢‘ä»¥åŠç›¸åº”æ–‡æœ¬æŒ‡ä»¤çš„å¤šæ ·åŒ–æ•°æ®é›†ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†EmoLLMï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿç†è§£çš„æ–°å‹æ¨¡å‹ï¼Œç»“åˆäº†ä¸¤ç§æ ¸å¿ƒæŠ€æœ¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šè§†è§’è§†è§‰æŠ•å½±
å®ƒä»å¤šä¸ªè§†è§’æ•è·è§†è§‰æ•°æ®ä¸­çš„å¤šæ ·åŒ–æƒ…æ„Ÿçº¿ç´¢ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šEmoPrompt
å®ƒå¼•å¯¼MLLMsåœ¨æ­£ç¡®çš„æ–¹å‘ä¸Šæ¨ç†æƒ…æ„Ÿã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒEmoLLMæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€æƒ…æ„Ÿç†è§£æ€§èƒ½ï¼Œåœ¨EmoBenchä¸Šå¤šä¸ªåŸºç¡€æ¨¡å‹å¹³å‡æé«˜äº†12.1%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„EmoBenchåŸºå‡†å’ŒEmoLLMæ¨¡å‹ä¸ºMLLMsåœ¨æƒ…æ„Ÿç†è§£æ–¹é¢çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨åŠ¨MLLMsåœ¨æƒ…æ„Ÿæ™ºèƒ½é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

## selective-perception--optimizing-state-descriptions-with-reinforcement-learning-for-language-model-actors
### Abstract
Large language models (LLMs) are being applied as actors for sequential
decision making tasks in domains such as robotics and games, utilizing their
general world knowledge and planning abilities. However, previous work does
little to explore what environment state information is provided to LLM actors
via language. Exhaustively describing high-dimensional states can impair
performance and raise inference costs for LLM actors. Previous LLM actors avoid
the issue by relying on hand-engineered, task-specific protocols to determine
which features to communicate about a state and which to leave out. In this
work, we propose Brief Language INputs for DEcision-making Responses (BLINDER),
a method for automatically selecting concise state descriptions by learning a
value function for task-conditioned state descriptions. We evaluate BLINDER on
the challenging video game NetHack and a robotic manipulation task. Our method
improves task success rate, reduces input size and compute costs, and
generalizes between LLM actors.
### ğŸŒŸ è®ºæ–‡è§£è¯» | BLINDERï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¯­è¨€æ¨¡å‹å†³ç­–è€…çš„çŠ¶æ€æè¿°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¸¸æˆå’Œæœºå™¨äººè§„åˆ’ç­‰é¢†åŸŸçš„å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰çš„å·¥ä½œå¾ˆå°‘æ¢è®¨å¦‚ä½•é€šè¿‡è¯­è¨€å‘LLMå†³ç­–è€…æä¾›ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ã€‚è¯¦å°½æè¿°é«˜ç»´çŠ¶æ€å¯èƒ½ä¼šæŸå®³æ€§èƒ½å¹¶å¢åŠ LLMå†³ç­–è€…çš„æ¨ç†æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BLINDERï¼Œä¸€ç§é€šè¿‡å­¦ä¹ ä»»åŠ¡æ¡ä»¶çŠ¶æ€æè¿°çš„ä»·å€¼å‡½æ•°æ¥è‡ªåŠ¨é€‰æ‹©ç®€æ´çŠ¶æ€æè¿°çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šBLINDERé€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨é€‰æ‹©ç®€æ´çš„çŠ¶æ€æè¿°ï¼Œä»è€Œå‡å°‘LLMå†³ç­–è€…çš„è¾“å…¥é•¿åº¦å’Œè®¡ç®—æˆæœ¬ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šBLINDERåœ¨NetHackæ¸¸æˆå’Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶èƒ½å¤Ÿåœ¨ä¸åŒçš„LLMå†³ç­–è€…ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
BLINDERåœ¨NetHackæ¸¸æˆå’Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸è¯¦å°½çš„çŠ¶æ€æè¿°ç›¸æ¯”ï¼ŒBLINDERèƒ½å¤Ÿæ˜¾è‘—æé«˜ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹¶å‡å°‘è¾“å…¥é•¿åº¦å’Œè®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒBLINDERè¿˜èƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒçš„LLMå†³ç­–è€…ï¼Œä»è€Œæé«˜äº†å…¶é€‚ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
BLINDERæå‡ºäº†ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨é€‰æ‹©ç®€æ´çŠ¶æ€æè¿°çš„æ–¹æ³•ï¼Œä¸ºLLMå†³ç­–è€…åœ¨æ¸¸æˆå’Œæœºå™¨äººè§„åˆ’ç­‰é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæé«˜ä»»åŠ¡æˆåŠŸç‡ï¼Œè¿˜èƒ½å¤Ÿå‡å°‘è¾“å…¥é•¿åº¦å’Œè®¡ç®—æˆæœ¬ï¼Œä»è€Œæé«˜äº†LLMå†³ç­–è€…çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒBLINDERè¿˜èƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒçš„LLMå†³ç­–è€…ï¼Œä»è€Œæé«˜äº†å…¶é€‚ç”¨æ€§ã€‚

## sayplan--grounding-large-language-models-using-3d-scene-graphs-for-scalable-robot-task-planning
### Abstract
Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic
search' for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an 'iterative replanning' pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SayPlanï¼šåˆ©ç”¨3Dåœºæ™¯å›¾å®ç°å¤§è§„æ¨¡æœºå™¨äººä»»åŠ¡è§„åˆ’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»»åŠ¡è§„åˆ’é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨å¤„ç†å„ç§ä»»åŠ¡æ—¶å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›è®¡åˆ’åœ¨å¤§å‹ã€å¤šæ¥¼å±‚å’Œå¤šæˆ¿é—´çš„ç¯å¢ƒä¸­è½åœ°ï¼Œå¯¹äºæœºå™¨äººæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€å±€é™äºå°è§„æ¨¡ç¯å¢ƒï¼Œä¸”éš¾ä»¥æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ä¸­ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
SayPlan æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ–¹æ³•ï¼Œåˆ©ç”¨ 3D åœºæ™¯å›¾ï¼ˆ3DSGï¼‰è¡¨ç¤ºæ¥æ”¯æŒåŸºäº LLM çš„å¤§è§„æ¨¡ä»»åŠ¡è§„åˆ’ã€‚ä¸ºäº†ç¡®ä¿æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼ŒSayPlan é‡‡ç”¨äº†ä»¥ä¸‹ä¸‰ä¸ªå…³é”®åˆ›æ–°ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯­ä¹‰æœç´¢
SayPlan åˆ©ç”¨ 3DSG çš„å±‚æ¬¡ç»“æ„ï¼Œå…è®¸ LLM é€šè¿‡è¯­ä¹‰æœç´¢æ¥è¯†åˆ«ä»»åŠ¡ç›¸å…³çš„å­å›¾ã€‚é€šè¿‡å°†å®Œæ•´çš„ 3DSG å‹ç¼©æˆä¸€ä¸ªæ›´å°çš„è¡¨ç¤ºï¼ŒLLM å¯ä»¥ä¸“æ³¨äºä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ã€ä¿¡æ¯ä¸°å¯Œçš„å­å›¾ï¼Œä»è€Œé¿å…è¶…å‡ºå…¶ token é™åˆ¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¿­ä»£é‡è§„åˆ’
ä¸ºäº†å‡å°‘ LLM çš„è§„åˆ’èŒƒå›´ï¼ŒSayPlan é›†æˆäº†ä¸€ä¸ªç»å…¸çš„è·¯å¾„è§„åˆ’å™¨ï¼Œè´Ÿè´£è¿æ¥ LLM ç”Ÿæˆçš„èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼ŒSayPlan å¼•å…¥äº†ä¸€ä¸ªè¿­ä»£é‡è§„åˆ’æµç¨‹ï¼Œä½¿ç”¨åœºæ™¯å›¾æ¨¡æ‹Ÿå™¨çš„åé¦ˆæ¥éªŒè¯å’Œç»†åŒ–åˆå§‹è®¡åˆ’ï¼Œä»è€Œçº æ­£ä¸å¯æ‰§è¡Œçš„åŠ¨ä½œå¹¶é¿å…è§„åˆ’å¤±è´¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
SayPlan åœ¨ä¸¤ä¸ªå¤§å‹ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ‹¥æœ‰ 3 å±‚æ¥¼å’Œ 36 ä¸ªæˆ¿é—´çš„ç¯å¢ƒï¼Œä»¥åŠä¸€ä¸ªæ‹¥æœ‰ 140 ä¸ªèµ„äº§å’Œå¯¹è±¡çš„ç¯å¢ƒã€‚ç»“æœè¡¨æ˜ï¼ŒSayPlan èƒ½å¤Ÿä»æŠ½è±¡çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­ç”Ÿæˆå¯æ‰§è¡Œçš„ã€å¤§è§„æ¨¡çš„ã€é•¿æ—¶ä»»åŠ¡è®¡åˆ’ï¼Œå¹¶èƒ½å¤Ÿåœ¨çœŸå®æœºå™¨äººä¸Šè¿›è¡Œæ‰§è¡Œã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SayPlan ä¸ºå¤§è§„æ¨¡æœºå™¨äººä»»åŠ¡è§„åˆ’æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æ–¹æ³•ã€‚å…¶è¯­ä¹‰æœç´¢å’Œè¿­ä»£é‡è§„åˆ’æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘ LLM çš„ token æ¶ˆè€—ï¼Œå¹¶ç¡®ä¿ç”Ÿæˆçš„è®¡åˆ’ç¬¦åˆç¯å¢ƒçº¦æŸã€‚æ­¤å¤–ï¼ŒSayPlan çš„æ¡†æ¶å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰çš„æœºå™¨äººç³»ç»Ÿä¸­ï¼Œä¸ºæœåŠ¡æœºå™¨äººé¢†åŸŸçš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚

## building-cooperative-embodied-agents-modularly-with-large-language-models
### Abstract
In this work, we address challenging multi-agent cooperation problems with
decentralized control, raw sensory observations, costly communication, and
multi-objective tasks instantiated in various embodied environments. While
previous research either presupposes a cost-free communication channel or
relies on a centralized controller with shared observations, we harness the
commonsense knowledge, reasoning ability, language comprehension, and text
generation prowess of LLMs and seamlessly incorporate them into a
cognitive-inspired modular framework that integrates with perception, memory,
and execution. Thus building a Cooperative Embodied Language Agent CoELA, who
can plan, communicate, and cooperate with others to accomplish long-horizon
tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA
driven by GPT-4 can surpass strong planning-based methods and exhibit emergent
effective communication. Though current Open LMs like LLAMA-2 still
underperform, we fine-tune a CoELA with data collected with our agents and show
how they can achieve promising performance. We also conducted a user study for
human-agent interaction and discovered that CoELA communicating in natural
language can earn more trust and cooperate more effectively with humans. Our
research underscores the potential of LLMs for future research in multi-agent
cooperation. Videos can be found on the project website
https://vis-www.cs.umass.edu/Co-LLM-Agents/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºåä½œå‹å…·èº«æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œäººç±»æ“…é•¿ä¸ä»–äººåˆä½œå’Œæ²Ÿé€šä»¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ„å»ºèƒ½å¤Ÿä¸äººç±»æˆ–å…¶ä»–æ™ºèƒ½ä½“åä½œçš„å…·èº«æ™ºèƒ½ä½“ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å…·å¤‡æ„ŸçŸ¥ã€éƒ¨åˆ†è§‚å¯Ÿã€é•¿æœŸè§„åˆ’ã€è‡ªç„¶è¯­è¨€æ²Ÿé€šç­‰å¤æ‚èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º CoELA çš„åä½œå‹å…·èº«è¯­è¨€æ™ºèƒ½ä½“ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„å¸¸è¯†çŸ¥è¯†ã€æ¨ç†èƒ½åŠ›ã€è¯­è¨€ç†è§£å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶å°†å…¶é›†æˆåˆ°ä¸€ä¸ªè®¤çŸ¥å¯å‘çš„æ¨¡å—åŒ–æ¡†æ¶ä¸­ï¼Œè¯¥æ¡†æ¶ä¸æ„ŸçŸ¥ã€è®°å¿†å’Œæ‰§è¡Œæ¨¡å—ç›¸ç»“åˆã€‚

CoELA æ¡†æ¶åŒ…æ‹¬äº”ä¸ªå…³é”®æ¨¡å—ï¼š
1. **æ„ŸçŸ¥æ¨¡å—**ï¼šç”¨äºæ„ŸçŸ¥ç¯å¢ƒä¸­çš„åŸå§‹æ„Ÿå®˜è§‚å¯Ÿå¹¶æå–æœ‰ç”¨ä¿¡æ¯ã€‚
2. **è®°å¿†æ¨¡å—**ï¼šæ¨¡æ‹Ÿäººç±»çš„é•¿æœŸè®°å¿†ï¼Œå­˜å‚¨æ™ºèƒ½ä½“å¯¹ä¸–ç•Œå’Œå…¶ä»–æ™ºèƒ½ä½“çš„ç†è§£å’Œç»éªŒã€‚
3. **é€šä¿¡æ¨¡å—**ï¼šåˆ©ç”¨ LLM çš„å¼ºå¤§å¯¹è¯ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ï¼Œå†³å®šå‘é€ä»€ä¹ˆä¿¡æ¯ã€‚
4. **è§„åˆ’æ¨¡å—**ï¼šåˆ©ç”¨ LLM çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œæ ¹æ®æ‰€æœ‰å¯ç”¨ä¿¡æ¯åšå‡ºå†³ç­–ï¼Œå¹¶åˆ¶å®šé«˜çº§è®¡åˆ’ã€‚
5. **æ‰§è¡Œæ¨¡å—**ï¼šæ ¹æ®è§„åˆ’æ¨¡å—ç”Ÿæˆçš„è®¡åˆ’ï¼Œç”Ÿæˆå¯æ‰§è¡Œçš„ä½çº§åŠ¨ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ C-WAH å’Œ TDW-MAT ä¸¤ä¸ªå…·èº«ç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCoELA èƒ½å¤Ÿæ„ŸçŸ¥å¤æ‚è§‚å¯Ÿã€æ¨ç†ä¸–ç•Œå’Œå…¶ä»–æ™ºèƒ½ä½“çš„çŠ¶æ€ã€æœ‰æ•ˆæ²Ÿé€šï¼Œå¹¶ç›¸åº”åœ°åˆ¶å®šé•¿æœŸè®¡åˆ’ã€‚CoELA é©±åŠ¨çš„ GPT-4 èƒ½å¤Ÿè¶…è¶ŠåŸºäºè§„åˆ’çš„å¼ºæ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºæœ‰æ•ˆçš„æ²Ÿé€šã€‚å°½ç®¡å½“å‰çš„å¼€æ”¾ LLMï¼ˆå¦‚ LLAMA-2ï¼‰ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œä½†ä½œè€…é€šè¿‡åœ¨å…·èº«ç¯å¢ƒä¸­æ”¶é›†çš„æ•°æ®å¯¹ CoELA è¿›è¡Œå¾®è°ƒï¼Œå¹¶å±•ç¤ºäº†å…¶æœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
1. **æ¨¡å—åŒ–æ¡†æ¶**ï¼šCoELA çš„æ¨¡å—åŒ–æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°å°† LLM çš„èƒ½åŠ›ä¸æ„ŸçŸ¥ã€è®°å¿†å’Œæ‰§è¡Œæ¨¡å—ç›¸ç»“åˆï¼Œä»è€Œæ„å»ºåä½œå‹å…·èº«æ™ºèƒ½ä½“ã€‚
2. **è‡ªç„¶è¯­è¨€æ²Ÿé€š**ï¼šCoELA ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œæ²Ÿé€šï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ä¸äººç±»æˆ–å…¶ä»–æ™ºèƒ½ä½“åä½œã€‚
3. **å¾®è°ƒ LLM**ï¼šä½œè€…é€šè¿‡åœ¨å…·èº«ç¯å¢ƒä¸­æ”¶é›†çš„æ•°æ®å¯¹ CoELA è¿›è¡Œå¾®è°ƒï¼Œå¹¶å±•ç¤ºäº†å…¶æœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œè¿™ä¸ºæ„å»ºæ›´å¥½çš„åä½œå‹å…·èº«æ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚

### ğŸŒˆ æœªæ¥å±•æœ›
CoELA çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLM åœ¨å¤šæ™ºèƒ½ä½“åä½œé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š
1. **å¤šæ¨¡æ€ LLM**ï¼šå¼€å‘èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è§†è§‰æ¨¡æ€å¹¶æµç•…ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„ LLMï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨ 3D ç©ºé—´ä¿¡æ¯ã€‚
2. **ä½çº§æ§åˆ¶**ï¼šå¼€å‘èƒ½å¤Ÿç›´æ¥è¿›è¡Œä½çº§æ§åˆ¶çš„æ™ºèƒ½ä½“ï¼Œä»¥æ›´å¥½åœ°ç†è§£ä½çº§åŠ¨ä½œçš„æ‰§è¡Œã€‚
3. **å¤æ‚æ¨ç†**ï¼šå¼€å‘å…·æœ‰æ›´å¼ºæŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›çš„ LLMï¼Œä»¥æé«˜æ™ºèƒ½ä½“çš„é²æ£’æ€§ã€‚

æ€»è€Œè¨€ä¹‹ï¼ŒCoELA ä¸ºæ„å»ºåä½œå‹å…·èº«æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚

## embodied-task-planning-with-large-language-models
### Abstract
Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…·èº«ä»»åŠ¡è§„åˆ’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæœºå™¨äººè¢«æœŸæœ›èƒ½å¤Ÿåœ¨å„ç§ç¯å¢ƒä¸­å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å®¶åº­æœåŠ¡ã€åŒ»ç–—æŠ¤ç†å’Œå†œä¸šé‡‡æ‘˜ç­‰ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ ·æœ¬æœ‰é™å’Œä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œç›´æ¥è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿåœ¨ä¸åŒéƒ¨ç½²åœºæ™¯ä¸­å·¥ä½œçš„å…·èº«ä»£ç†æ˜¯ä¸å¯è¡Œçš„ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ä»å¤§é‡ç½‘ç»œæ•°æ®ä¸­è·å–ä¸°å¯Œçš„å¸¸è¯†çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯ä»¥æ½œåœ¨åœ°è¢«å…·èº«ä»£ç†åˆ©ç”¨æ¥ç”Ÿæˆç¬¦åˆäººç±»è¦æ±‚çš„è‡ªç„¶è¯­è¨€å‘½ä»¤çš„åŠ¨ä½œè®¡åˆ’ã€‚ç„¶è€Œï¼ŒLLMæ— æ³•æ„ŸçŸ¥å‘¨å›´åœºæ™¯ï¼Œå¹¶ä¸”å¯èƒ½ä¼šç”Ÿæˆä¸å¯æ‰§è¡Œçš„è¡ŒåŠ¨ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ä¸éå­˜åœ¨çš„å¯¹è±¡è¿›è¡Œäº¤äº’ã€‚å› æ­¤ï¼Œå°†LLMç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’ä¸ç‰©ç†ä¸–ç•Œç›¸ç»“åˆæ˜¯æ„å»ºèƒ½å¤Ÿå®Œæˆå¤æ‚ä»»åŠ¡çš„å…·èº«ä»£ç†çš„å¿…è¦æ¡ä»¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTaPAçš„å…·èº«ä»»åŠ¡è§„åˆ’ä»£ç†ï¼Œç”¨äºåœ¨ç‰©ç†åœºæ™¯ä¸­è¿›è¡Œå…·èº«ä»»åŠ¡è§„åˆ’ã€‚TaPAé€šè¿‡å°†LLMä¸è§†è§‰æ„ŸçŸ¥æ¨¡å‹ç›¸ç»“åˆï¼Œæ ¹æ®åœºæ™¯ä¸­å­˜åœ¨çš„å¯¹è±¡ç”Ÿæˆå¯æ‰§è¡Œçš„è®¡åˆ’ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«å®¤å†…åœºæ™¯ã€æŒ‡ä»¤å’ŒåŠ¨ä½œè®¡åˆ’ä¸‰å…ƒç»„çš„å¤šå…ƒæ•°æ®é›†ï¼Œå…¶ä¸­ä¸ºGPT-3.5æä¾›äº†è®¾è®¡çš„æç¤ºå’Œåœºæ™¯ä¸­ç°æœ‰å¯¹è±¡çš„åˆ—è¡¨ï¼Œä»¥ç”Ÿæˆå¤§é‡æŒ‡ä»¤å’Œç›¸åº”çš„è®¡åˆ’åŠ¨ä½œã€‚ç”Ÿæˆçš„æ•°æ®ç”¨äºå¯¹é¢„è®­ç»ƒçš„LLMè¿›è¡ŒåŸºäºç‰©ç†åœºæ™¯çº¦æŸçš„æ¥åœ°è®¡åˆ’å¾®è°ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æ‰©å±•å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨åˆ°ä»ä¸åŒå¯è¾¾ä½ç½®æ”¶é›†çš„å¤šè§†å›¾RGBå›¾åƒï¼Œå‘ç°åœºæ™¯ä¸­çš„å¯¹è±¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸LLaVAå’ŒGPT-3.5ç›¸æ¯”ï¼ŒTaPAæ¡†æ¶ç”Ÿæˆçš„è®¡åˆ’å¯ä»¥å–å¾—æ›´é«˜çš„æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜å…·èº«ä»»åŠ¡è§„åˆ’åœ¨ä¸€èˆ¬å’Œå¤æ‚ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä¸LLaVAå’ŒGPT-3.5ç›¸æ¯”ï¼ŒTaPAæ¡†æ¶ç”Ÿæˆçš„è®¡åˆ’å¯ä»¥å–å¾—æ›´é«˜çš„æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜å…·èº«ä»»åŠ¡è§„åˆ’åœ¨ä¸€èˆ¬å’Œå¤æ‚ç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„TaPAæ¡†æ¶ä¸ºå…·èº«ä»»åŠ¡è§„åˆ’æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡å°†LLMä¸è§†è§‰æ„ŸçŸ¥æ¨¡å‹ç›¸ç»“åˆï¼Œå¯ä»¥ç”Ÿæˆæ›´ç¬¦åˆç‰©ç†åœºæ™¯çº¦æŸçš„å¯æ‰§è¡Œè®¡åˆ’ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤šå…ƒæ•°æ®é›†ä¹Ÿä¸ºå…¶ä»–ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚

## sprint--scalable-policy-pre-training-via-language-instruction-relabeling
### Abstract
Pre-training robot policies with a rich set of skills can substantially
accelerate the learning of downstream tasks. Prior works have defined
pre-training tasks via natural language instructions, but doing so requires
tedious human annotation of hundreds of thousands of instructions. Thus, we
propose SPRINT, a scalable offline policy pre-training approach which
substantially reduces the human effort needed for pre-training a diverse set of
skills. Our method uses two core ideas to automatically expand a base set of
pre-training tasks: instruction relabeling via large language models and
cross-trajectory skill chaining through offline reinforcement learning. As a
result, SPRINT pre-training equips robots with a much richer repertoire of
skills. Experimental results in a household simulator and on a real robot
kitchen manipulation task show that SPRINT leads to substantially faster
learning of new long-horizon tasks than previous pre-training approaches.
Website at https://clvrai.com/sprint.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SPRINTï¼šé€šè¿‡è¯­è¨€æŒ‡ä»¤é‡æ ‡è®°å®ç°å¯æ‰©å±•çš„ç­–ç•¥é¢„è®­ç»ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æœºå™¨äººå­¦ä¹ é¢†åŸŸï¼Œé¢„è®­ç»ƒæœºå™¨äººç­–ç•¥ä»¥æŒæ¡ä¸°å¯Œçš„æŠ€èƒ½é›†å¯ä»¥æ˜¾è‘—åŠ é€Ÿä¸‹æ¸¸ä»»åŠ¡çš„å­¦ä¹ ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„æ–¹æ³•éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¿™ä¸ä»…è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†SPRINTï¼Œä¸€ç§å¯æ‰©å±•çš„ç¦»çº¿ç­–ç•¥é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¤§å¹…å‡å°‘é¢„è®­ç»ƒå¤šæ ·åŒ–æŠ€èƒ½é›†æ‰€éœ€çš„äººå·¥å·¥ä½œé‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè¯­è¨€æ¨¡å‹æŒ‡ä»¤é‡æ ‡è®°
SPRINTåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨å°†è¿ç»­çš„è¯­è¨€æŒ‡ä»¤ç»„åˆæˆæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å°†â€œå°†æ¯å­æ”¾å…¥å’–å•¡æœºâ€å’Œâ€œæŒ‰ä¸‹å†²æ³¡æŒ‰é’®â€åˆå¹¶ä¸ºâ€œåˆ¶ä½œå’–å•¡â€ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ˜¾è‘—æ‰©å±•é¢„è®­ç»ƒä»»åŠ¡é›†ï¼Œè€Œæ— éœ€é¢å¤–çš„äººå·¥æ ‡æ³¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè·¨è½¨è¿¹æŠ€èƒ½é“¾
SPRINTè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºè¯­è¨€çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›®æ ‡ï¼Œé€šè¿‡å°†æ¥è‡ªæ•°æ®é›†çš„ä¸åŒè½¨è¿¹æ®µâ€œç¼åˆâ€åœ¨ä¸€èµ·å½¢æˆæ–°ä»»åŠ¡ï¼Œä»è€Œå®ç°è·¨è½¨è¿¹æŠ€èƒ½é“¾ã€‚è¿™ç§æ–¹æ³•å…è®¸ç­–ç•¥å­¦ä¹ æ›´é•¿çš„æŠ€èƒ½ï¼Œå¹¶è¿›ä¸€æ­¥ä¸°å¯Œé¢„è®­ç»ƒä»»åŠ¡é›†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®¶åº­æ¨¡æ‹Ÿå™¨å’ŒçœŸå®æœºå™¨äººå¨æˆ¿æ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„é¢„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒSPRINTèƒ½å¤Ÿä½¿æœºå™¨äººæ›´å¿«åœ°å­¦ä¹ æ–°çš„é•¿æ—¶ä»»åŠ¡ã€‚SPRINTé¢„è®­ç»ƒçš„æœºå™¨äººå¯ä»¥åˆ©ç”¨å…¶æ›´ä¸°å¯Œçš„æŠ€èƒ½åº“ï¼Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ æ–°çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SPRINTæä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æœºå™¨äººç­–ç•¥é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—å‡å°‘äººå·¥æ ‡æ³¨å·¥ä½œé‡ï¼Œå¹¶æé«˜æœºå™¨äººå­¦ä¹ æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³ï¼Œå³åˆ©ç”¨è¯­è¨€æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤é‡æ ‡è®°å’Œè·¨è½¨è¿¹æŠ€èƒ½é“¾ï¼Œä¸ºæœºå™¨äººå­¦ä¹ é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚

## chessgpt--bridging-policy-learning-and-language-modeling
### Abstract
When solving decision-making tasks, humans typically depend on information
from two key sources: (1) Historical policy data, which provides interaction
replay from the environment, and (2) Analytical insights in natural language
form, exposing the invaluable thought process or strategic considerations.
Despite this, the majority of preceding research focuses on only one source:
they either use historical replay exclusively to directly learn policy or value
functions, or engaged in language model training utilizing mere language
corpus. In this paper, we argue that a powerful autonomous agent should cover
both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning
and language modeling by integrating data from these two sources in Chess
games. Specifically, we build a large-scale game and language dataset related
to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and
ChessGPT, integrating policy learning and language modeling. Finally, we
propose a full evaluation framework for evaluating language model's chess
ability. Experimental results validate our model and dataset's effectiveness.
We open source our code, model, and dataset at
https://github.com/waterhorse1/ChessGPT.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ChessGPTï¼šç­–ç•¥å­¦ä¹ ä¸è¯­è¨€æ¨¡å‹èåˆçš„æ¡¥æ¢

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨è§£å†³å†³ç­–ä»»åŠ¡æ—¶ï¼Œäººç±»é€šå¸¸ä¾èµ–äºä¸¤ç§å…³é”®ä¿¡æ¯æ¥æºï¼šå†å²ç­–ç•¥æ•°æ®å’Œè‡ªç„¶è¯­è¨€å½¢å¼çš„ç­–ç•¥åˆ†æã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶å¤§å¤šåªå…³æ³¨å…¶ä¸­ä¸€ç§æ¥æºï¼Œè¦ä¹ˆæ˜¯ç›´æ¥ä»å†å²å›æ”¾ä¸­å­¦ä¹ ç­–ç•¥æˆ–ä»·å€¼å‡½æ•°ï¼Œè¦ä¹ˆæ˜¯åˆ©ç”¨è¯­è¨€è¯­æ–™åº“è¿›è¡Œè¯­è¨€æ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡è®¤ä¸ºï¼Œä¸€ä¸ªå¼ºå¤§çš„è‡ªä¸»ä»£ç†åº”è¯¥åŒæ—¶åˆ©ç”¨è¿™ä¸¤ç§æ¥æºï¼Œå› æ­¤æå‡ºäº†ChessGPTï¼Œä¸€ä¸ªé€šè¿‡æ•´åˆå›½é™…è±¡æ£‹æ¸¸æˆä¸­çš„æ•°æ®æ¥è¿æ¥ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹çš„GPTæ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡æ¸¸æˆå’Œè¯­è¨€æ•°æ®é›†
æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤§é‡å›½é™…è±¡æ£‹æ¸¸æˆæ•°æ®å’Œè‡ªç„¶è¯­è¨€æ•°æ®çš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…æ‹¬åœ¨çº¿æ¸¸æˆå›æ”¾ã€ä¸“ä¸šæ£‹æ‰‹æ¯”èµ›ã€è®¡ç®—æœºå¼•æ“æ¸¸æˆã€æ£‹ç›˜æ¸¸æˆã€æ£‹ç›˜æ¸¸æˆåˆ†æã€æ£‹ç›˜æ¸¸æˆåšå®¢ã€æ£‹ç›˜æ¸¸æˆä¹¦ç±ã€æ£‹ç›˜æ¸¸æˆè®ºå›ã€æ£‹ç›˜æ¸¸æˆè§†é¢‘ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºChessCLIPå’ŒChessGPTæ¨¡å‹
æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ¨¡å‹ï¼ŒChessCLIPå’ŒChessGPTï¼Œåˆ©ç”¨ä¸Šè¿°æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ChessCLIPé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†ç­–ç•¥å’Œè¯­è¨€æ¨¡æ€è¿æ¥èµ·æ¥ï¼Œè€ŒChessGPTåˆ™é€šè¿‡å› æœè¯­è¨€æ¨¡å‹è¿›è¡Œç­–ç•¥å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºå…¨é¢çš„è¯„ä¼°æ¡†æ¶
æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å›½é™…è±¡æ£‹æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å»ºæ¨¡èƒ½åŠ›ã€ä»·å€¼åˆ¤æ–­èƒ½åŠ›å’Œç­–ç•¥èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒChessGPTæ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­éƒ½ä¼˜äºå…¶ä»–LLMåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†æ¨¡å‹å’Œæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ChessGPTæ¨¡å‹å’Œæ•°æ®é›†ä¸ºç ”ç©¶ç­–ç•¥å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§çš„è‡ªä¸»ä»£ç†æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## omni--open-endedness-via-models-of-human-notions-of-interestingness
### Abstract
Open-ended algorithms aim to learn new, interesting behaviors forever. That
requires a vast environment search space, but there are thus infinitely many
possible tasks. Even after filtering for tasks the current agent can learn
(i.e., learning progress), countless learnable yet uninteresting tasks remain
(e.g., minor variations of previously learned tasks). An Achilles Heel of
open-endedness research is the inability to quantify (and thus prioritize)
tasks that are not just learnable, but also $\textit{interesting}$ (e.g.,
worthwhile and novel). We propose solving this problem by
$\textit{Open-endedness via Models of human Notions of Interestingness}$
(OMNI). The insight is that we can utilize foundation models (FMs) as a model
of interestingness (MoI), because they $\textit{already}$ internalize human
concepts of interestingness from training on vast amounts of human-generated
data, where humans naturally write about what they find interesting or boring.
We show that FM-based MoIs improve open-ended learning by focusing on tasks
that are both learnable $\textit{and interesting}$, outperforming baselines
based on uniform task sampling or learning progress alone. This approach has
the potential to dramatically advance the ability to intelligently select which
tasks to focus on next (i.e., auto-curricula), and could be seen as AI
selecting its own next task to learn, facilitating self-improving AI and
AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/
### ğŸŒŸ è®ºæ–‡è§£è¯» | OMNIï¼šåŸºäºäººç±»å…´è¶£æ¦‚å¿µçš„å¼€æ”¾æ€§å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼€æ”¾æ€§å­¦ä¹ ç®—æ³•æ—¨åœ¨è®©AIåƒäººç±»ä¸€æ ·ï¼Œåœ¨å¤æ‚å¤šå˜çš„ç¯å¢ƒä¸­ä¸æ–­å­¦ä¹ æ–°æŠ€èƒ½ã€‚ç„¶è€Œï¼Œé¢å¯¹æ— é™å¯èƒ½çš„å­¦ä¹ ä»»åŠ¡ï¼Œå¦‚ä½•é€‰æ‹©å“ªäº›ä»»åŠ¡è¿›è¡Œå­¦ä¹ æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚åŸºäºå­¦ä¹ è¿›åº¦çš„è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼Œå®¹æ˜“é™·å…¥é‡å¤æˆ–æ— è¶£çš„ä»»åŠ¡ä¸­ï¼Œæ— æ³•æœ‰æ•ˆå¼•å¯¼AIè¿›è¡Œæœ‰æ„ä¹‰çš„å­¦ä¹ ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šOMNIæ¡†æ¶
OMNIæ¡†æ¶é€šè¿‡ç»“åˆå­¦ä¹ è¿›åº¦æ¨¡å‹å’Œäººç±»å…´è¶£æ¨¡å‹ï¼Œå®ç°äº†å¯¹å­¦ä¹ ä»»åŠ¡çš„æ™ºèƒ½ç­›é€‰ã€‚å­¦ä¹ è¿›åº¦æ¨¡å‹è´Ÿè´£è¯†åˆ«å½“å‰AIèƒ½å¤Ÿå­¦ä¹ çš„ä»»åŠ¡ï¼Œè€Œäººç±»å…´è¶£æ¨¡å‹åˆ™è´Ÿè´£è¯„ä¼°è¿™äº›ä»»åŠ¡æ˜¯å¦æœ‰è¶£ï¼Œä»è€Œç¡®ä¿AIèƒ½å¤Ÿä¸“æ³¨äºæœ‰æ„ä¹‰çš„å­¦ä¹ ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹
OMNIæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT-3å’ŒGPT-4ï¼‰ä½œä¸ºäººç±»å…´è¶£æ¨¡å‹ï¼Œä»è€Œé¿å…äº†æ‰‹åŠ¨å®šä¹‰å…´è¶£æŒ‡æ ‡çš„å›°éš¾ã€‚è¿™äº›åŸºç¡€æ¨¡å‹åœ¨å¤§é‡äººç±»ç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå·²ç»å†…åŒ–äº†äººç±»å¯¹æœ‰è¶£äº‹ç‰©çš„æ¦‚å¿µï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°ä»»åŠ¡çš„æœ‰è¶£ç¨‹åº¦ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
OMNIæ¡†æ¶åœ¨Crafterã€BabyAIå’ŒAI2-THORç­‰ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸åŸºäºå‡åŒ€ä»»åŠ¡é‡‡æ ·æˆ–ä»…åŸºäºå­¦ä¹ è¿›åº¦çš„æ–¹æ³•ç›¸æ¯”ï¼ŒOMNIæ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æé«˜AIçš„å­¦ä¹ æ•ˆç‡ï¼Œå­¦ä¹ æ›´å¤šæœ‰è¶£ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
OMNIæ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ ç®—æ³•æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚ï¼š
* **å¤šæ¨¡æ€æ¨¡å‹**ï¼šå°†è§†è§‰-è¯­è¨€æ¨¡å‹ç­‰èå…¥äººç±»å…´è¶£æ¨¡å‹ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°ä»»åŠ¡çš„æœ‰è¶£ç¨‹åº¦ã€‚
* **è‡ªä¸»å­¦ä¹ **ï¼šè®©äººç±»å…´è¶£æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»åˆ†æä»»åŠ¡æˆåŠŸç‡ç­‰æŒ‡æ ‡ï¼Œå¹¶æ®æ­¤è°ƒæ•´å¯¹æœ‰è¶£ç¨‹åº¦çš„è¯„ä¼°ã€‚
* **å®‰å…¨æ€§ä¸ä»·å€¼å¯¹é½**ï¼šé€šè¿‡å¼•å…¥äººç±»åé¦ˆæˆ–AIåé¦ˆï¼Œç¡®ä¿OMNIæ¡†æ¶èƒ½å¤Ÿé€‰æ‹©ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›çš„ä»»åŠ¡è¿›è¡Œå­¦ä¹ ã€‚

### ğŸŒˆ æœªæ¥å±•æœ›
OMNIæ¡†æ¶ä¸ºå¼€æ”¾æ€§å­¦ä¹ ç®—æ³•çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ï¼Œæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š
* **æ›´å¤æ‚çš„ä»»åŠ¡ç©ºé—´**ï¼šå°†OMNIæ¡†æ¶åº”ç”¨äºæ›´å¤æ‚çš„ä»»åŠ¡ç©ºé—´ï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®ç¯å¢ƒæˆ–çœŸå®ä¸–ç•Œç¯å¢ƒã€‚
* **æ›´æ™ºèƒ½çš„äººç±»å…´è¶£æ¨¡å‹**ï¼šå¼€å‘æ›´æ™ºèƒ½çš„äººç±»å…´è¶£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£äººç±»å¯¹æœ‰è¶£äº‹ç‰©çš„æ¦‚å¿µï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®AIçš„å­¦ä¹ è¿›åº¦åŠ¨æ€è°ƒæ•´è¯„ä¼°æ ‡å‡†ã€‚
* **å®‰å…¨æ€§ä¸ä»·å€¼å¯¹é½**ï¼šæ¢ç´¢æ›´æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç¡®ä¿OMNIæ¡†æ¶èƒ½å¤Ÿé€‰æ‹©ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›çš„ä»»åŠ¡è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œé¿å…æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

## steve-1--a-generative-model-for-text-to-behavior-in-minecraft
### Abstract
Constructing AI models that respond to text instructions is challenging,
especially for sequential decision-making tasks. This work introduces a
methodology, inspired by unCLIP, for instruction-tuning generative models of
behavior without relying on a large dataset of instruction-labeled
trajectories. Using this methodology, we create an instruction-tuned Video
Pretraining (VPT) model called STEVE-1, which can follow short-horizon
open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two
steps: adapting the pretrained VPT model to follow commands in MineCLIP's
latent space, then training a prior to predict latent codes from text. This
allows us to finetune VPT through self-supervised behavioral cloning and
hindsight relabeling, reducing the need for costly human text annotations, and
all for only $60 of compute. By leveraging pretrained models like VPT and
MineCLIP and employing best practices from text-conditioned image generation,
STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with
low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming
previous baselines and robustly completing 12 of 13 tasks in our early-game
evaluation suite. We provide experimental evidence highlighting key factors for
downstream performance, including pretraining, classifier-free guidance, and
data scaling. All resources, including our model weights, training scripts, and
evaluation tools are made available for further research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Minecraft ä¸­çš„æ–‡æœ¬åˆ°è¡Œä¸ºç”Ÿæˆæ¨¡å‹ï¼šSTEVE-1

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ„å»ºèƒ½å¤Ÿå“åº”æ–‡æœ¬æŒ‡ä»¤çš„ AI æ¨¡å‹æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¡ºåºå†³ç­–çš„ä»»åŠ¡ä¸­ã€‚ç°æœ‰çš„æ¨¡å‹å¾€å¾€éœ€è¦å¤§é‡å¸¦æœ‰æŒ‡ä»¤æ ‡ç­¾çš„è½¨è¿¹æ•°æ®é›†ï¼Œè¿™æ—¢æ˜‚è´µåˆéš¾ä»¥è·å–ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€ä¾èµ–å¤§é‡æŒ‡ä»¤æ ‡ç­¾è½¨è¿¹æ•°æ®é›†çš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºèƒ½å¤Ÿå“åº”æ–‡æœ¬æŒ‡ä»¤çš„è¡Œä¸ºç”Ÿæˆæ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå— unCLIP å¯å‘çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§å— unCLIP å¯å‘çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºæ„å»ºè¡Œä¸ºç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªç”¨äºç”Ÿæˆè¡Œä¸ºè½¨è¿¹çš„ç­–ç•¥æ¨¡å‹ï¼Œå¦ä¸€ä¸ªç”¨äºå°†æ–‡æœ¬æŒ‡ä»¤è½¬æ¢ä¸ºè§†è§‰åµŒå…¥çš„å…ˆéªŒæ¨¡å‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥é¿å…ä½¿ç”¨æ˜‚è´µçš„æ–‡æœ¬æŒ‡ä»¤æ ‡ç­¾ï¼Œè€Œæ˜¯åˆ©ç”¨è§†è§‰åµŒå…¥è¿›è¡Œè®­ç»ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäº VPT å’Œ MineCLIP çš„æ¨¡å‹æ„å»º
æœ¬æ–‡åˆ©ç”¨äº†ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼šVPT å’Œ MineCLIPã€‚VPT æ˜¯ä¸€ä¸ªåŸºäº Minecraft æ¸¸æˆè§†é¢‘é¢„è®­ç»ƒçš„è¡Œä¸ºæ¨¡å‹ï¼Œè€Œ MineCLIP æ˜¯ä¸€ä¸ªå°†æ–‡æœ¬å’Œè§†é¢‘ç‰‡æ®µå¯¹é½çš„æ¨¡å‹ã€‚é€šè¿‡å°†è¿™ä¸¤ä¸ªæ¨¡å‹ç»“åˆèµ·æ¥ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå“åº”æ–‡æœ¬æŒ‡ä»¤çš„è¡Œä¸ºç”Ÿæˆæ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºè‡ªç›‘ç£å­¦ä¹ å’Œå›æº¯é‡æ ‡è®°çš„å¾®è°ƒ
æœ¬æ–‡ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ å’Œå›æº¯é‡æ ‡è®°æŠ€æœ¯å¯¹ VPT æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å‡å°‘å¯¹æ˜‚è´µçš„äººç±»æ–‡æœ¬æ³¨é‡Šçš„éœ€æ±‚ï¼Œå¹¶åˆ©ç”¨ç°æœ‰çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEVE-1 åœ¨ Minecraft ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°å“åº”æ–‡æœ¬æŒ‡ä»¤ï¼Œå¹¶å®Œæˆå„ç§ä»»åŠ¡ã€‚ä¸ä¹‹å‰çš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒSTEVE-1 åœ¨ä½çº§æ§åˆ¶ï¼ˆé¼ æ ‡å’Œé”®ç›˜ï¼‰å’ŒåŸå§‹åƒç´ è¾“å…¥æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜è¡¨æ˜ï¼Œé¢„è®­ç»ƒã€åˆ†ç±»å™¨æ— å…³å¼•å¯¼å’Œæ•°æ®ç¼©æ”¾ç­‰å› ç´ å¯¹ä¸‹æ¸¸æ€§èƒ½è‡³å…³é‡è¦ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸå’Œä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€è™šæ‹Ÿç°å®ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†é¢„è®­ç»ƒã€åˆ†ç±»å™¨æ— å…³å¼•å¯¼å’Œæ•°æ®ç¼©æ”¾ç­‰å› ç´ å¯¹ä¸‹æ¸¸æ€§èƒ½çš„é‡è¦æ€§ï¼Œä¸ºæ„å»ºæ›´å¼ºå¤§çš„ AI æ¨¡å‹æä¾›äº†å‚è€ƒã€‚

## the-text-based-adventure-ai-competition
### Abstract
In 2016, 2017, and 2018 at the IEEE Conference on Computational Intelligence
in Games, the authors of this paper ran a competition for agents that can play
classic text-based adventure games. This competition fills a gap in existing
game AI competitions that have typically focussed on traditional card/board
games or modern video games with graphical interfaces. By providing a platform
for evaluating agents in text-based adventures, the competition provides a
novel benchmark for game AI with unique challenges for natural language
understanding and generation. This paper summarises the three competitions ran
in 2016, 2017, and 2018 (including details of open source implementations of
both the competition framework and our competitors) and presents the results of
an improved evaluation of these competitors across 20 games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›ï¼šæ¢ç´¢è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„æŒ‘æˆ˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å›¾å½¢åŒ–ç•Œé¢çš„æ¸¸æˆAIç«èµ›çš„å…´èµ·ï¼Œæ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›å¡«è¡¥äº†ç°æœ‰æ¸¸æˆAIç«èµ›çš„ç©ºç™½ã€‚æ–‡æœ¬å†’é™©æ¸¸æˆä¸ºæ¸¸æˆAIæä¾›äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†2016å¹´è‡³2018å¹´åœ¨IEEEè®¡ç®—æ™ºèƒ½ä¸æ¸¸æˆä¼šè®®ä¸Šä¸¾åŠçš„æ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›ï¼Œå¹¶æ€»ç»“äº†ç«èµ›çš„æˆæœå’Œç»éªŒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«èµ›æ¡†æ¶
ç«èµ›æ¡†æ¶åŸºäºZPlet Javaè§£é‡Šå™¨ï¼Œç”¨äºè¯„ä¼°è½¯ä»¶ä»£ç†åœ¨æ–‡æœ¬å†’é™©æ¸¸æˆä¸­çš„èƒ½åŠ›ã€‚ä»£ç†é€šè¿‡ä¸€ä¸ªæ¥å£ä¸æ¸¸æˆäº¤äº’ï¼Œè¯¥æ¥å£æ¥æ”¶æ¸¸æˆç¯å¢ƒæè¿°å¹¶è¿”å›ä»£ç†æƒ³è¦æ‰§è¡Œçš„åŠ¨ä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç§ä»£ç†ç®—æ³•
ç«èµ›ä¸­æäº¤äº†å¤šç§ä»£ç†ç®—æ³•ï¼ŒåŒ…æ‹¬BYUAgent 2016ã€Golovinã€CARL (BYUAgent 2017) å’Œ NAILã€‚è¿™äº›ä»£ç†ç®—æ³•ä½¿ç”¨äº†ä¸åŒçš„æ–¹æ³•æ¥ç”Ÿæˆå‘½ä»¤å’Œä¸æ¸¸æˆäº¤äº’ï¼Œä¾‹å¦‚åŸºäºword2vecçš„åŠ¨è¯-åè¯åŒ¹é…ã€å‘½ä»¤æ¨¡å¼ç”Ÿæˆå’ŒçŸ¥è¯†å›¾è°±æ„å»ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ç«èµ›ç»“æœè¡¨æ˜ï¼ŒNAILä»£ç†åœ¨2018å¹´ç«èµ›ä¸­è¡¨ç°æœ€ä½³ï¼Œå…¶æ¬¡æ˜¯CARLå’ŒGolovinã€‚ä¸2016å¹´å’Œ2017å¹´ç›¸æ¯”ï¼Œæ‰€æœ‰ä»£ç†çš„æ€§èƒ½éƒ½æœ‰æ‰€æé«˜ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„ä»£ç†ï¼Œä¹Ÿåªèƒ½å®Œæˆæµ‹è¯•æ¸¸æˆä¸­çš„ä¸€å°éƒ¨åˆ†ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæ¸¸æˆä¸­æ— æ³•è·å¾—ä»»ä½•åˆ†æ•°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–‡æœ¬å†’é™©æ¸¸æˆAIç«èµ›ä¸ºæ¸¸æˆAIç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œå¹¶å¼ºè°ƒäº†è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆåœ¨æ¸¸æˆAIä¸­çš„é‡è¦æ€§ã€‚ç«èµ›ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„ä»£ç†ç®—æ³•åœ¨å¤„ç†æ–‡æœ¬å†’é™©æ¸¸æˆçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢æ›´æœ‰æ•ˆçš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•å°†æ–‡æœ¬å†’é™©æ¸¸æˆAIåº”ç”¨äºç°å®ä¸–ç•Œçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚

## alphablock--embodied-finetuning-for-vision-language-reasoning-in-robot-manipulation
### Abstract
We propose a novel framework for learning high-level cognitive capabilities
in robot manipulation tasks, such as making a smiley face using building
blocks. These tasks often involve complex multi-step reasoning, presenting
significant challenges due to the limited paired data connecting human
instructions (e.g., making a smiley face) and robot actions (e.g., end-effector
movement). Existing approaches relieve this challenge by adopting an open-loop
paradigm decomposing high-level instructions into simple sub-task plans, and
executing them step-by-step using low-level control models. However, these
approaches are short of instant observations in multi-step reasoning, leading
to sub-optimal results. To address this issue, we propose to automatically
collect a cognitive robot dataset by Large Language Models (LLMs). The
resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of
multi-step text plans and paired observation sequences. To enable efficient
data acquisition, we employ elaborated multi-round prompt designs that
effectively reduce the burden of extensive human involvement. We further
propose a closed-loop multi-modal embodied planning model that autoregressively
generates plans by taking image observations as input. To facilitate effective
learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and
finetune additional vision adapter and Q-former to enable fine-grained spatial
perception for manipulation tasks. We conduct experiments to verify the
superiority over existing open and closed-loop methods, and achieve a
significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4
based robot tasks. Real-world demos are shown in
https://www.youtube.com/watch?v=ayAzID1_qQk .
### ğŸŒŸ è®ºæ–‡è§£è¯» | AlphaBlockï¼šæœºå™¨äººæ“ä½œä¸­çš„è§†è§‰-è¯­è¨€æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­ï¼Œå¦‚ä½¿ç”¨ç§¯æœ¨åˆ¶ä½œç¬‘è„¸ï¼Œæœºå™¨äººéœ€è¦ç†è§£å’Œæ‰§è¡Œå¤æ‚çš„è¯­è¨€æŒ‡ä»¤ï¼Œè¿™æ¶‰åŠåˆ°æ„ŸçŸ¥ã€æ¨ç†å’Œæ“ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å¼€ç¯èŒƒå¼ï¼Œå°†é«˜çº§æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•çš„å­ä»»åŠ¡è®¡åˆ’ï¼Œå¹¶ä½¿ç”¨ä½çº§æ§åˆ¶æ¨¡å‹é€æ­¥æ‰§è¡Œã€‚è¿™ç§æ–¹æ³•ç¼ºä¹å¤šæ­¥æ¨ç†ä¸­çš„å³æ—¶è§‚å¯Ÿï¼Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªåŠ¨æ”¶é›†è®¤çŸ¥æœºå™¨äººæ•°æ®é›†
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªåŠ¨æ”¶é›†è®¤çŸ¥æœºå™¨äººæ•°æ®é›†çš„æ–¹æ³•ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆå¤šæ­¥æ–‡æœ¬è®¡åˆ’å’Œé…å¯¹çš„è§‚å¯Ÿåºåˆ—ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†äººç±»å‚ä¸çš„è´Ÿæ‹…ï¼Œå¹¶æ”¶é›†äº†35ä¸ªç»¼åˆçš„é«˜æ°´å¹³ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤šæ­¥æ–‡æœ¬è®¡åˆ’å’Œé…å¯¹çš„è§‚å¯Ÿåºåˆ—ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé—­ç¯å¤šæ¨¡æ€å…·èº«è§„åˆ’æ¨¡å‹
æœ¬æ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§é—­ç¯å¤šæ¨¡æ€å…·èº«è§„åˆ’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¾“å…¥å›¾åƒè§‚å¯Ÿæ¥è‡ªå›å½’åœ°ç”Ÿæˆè®¡åˆ’ã€‚ä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆå­¦ä¹ ï¼Œæœ¬æ–‡åˆ©ç”¨äº†MiniGPT-4ï¼ŒåŒ…æ‹¬å†»ç»“çš„è§†è§‰ç¼–ç å™¨å’ŒLLMï¼Œå¹¶å¾®è°ƒäº†é¢å¤–çš„è§†è§‰é€‚é…å™¨å’ŒQ-formerï¼Œä»¥å®ç°æ“ä½œä»»åŠ¡ä¸­çš„ç»†ç²’åº¦ç©ºé—´æ„ŸçŸ¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¼€ç¯å’Œé—­ç¯æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„CogLoopæ¡†æ¶åœ¨æœºå™¨äººä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ChatGPTå’ŒGPT-4ç›¸æ¯”ï¼ŒCogLoopçš„æˆåŠŸç‡åˆ†åˆ«æé«˜äº†21.4%å’Œ14.5%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„CogLoopæ¡†æ¶ä¸ºæœºå™¨äººæ“ä½œä¸­çš„è§†è§‰-è¯­è¨€æ¨ç†æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡è‡ªåŠ¨æ”¶é›†è®¤çŸ¥æœºå™¨äººæ•°æ®é›†å’Œé—­ç¯å¤šæ¨¡æ€å…·èº«è§„åˆ’æ¨¡å‹ï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œé«˜çº§æŒ‡ä»¤ï¼Œä»è€Œå®ç°æ›´æ™ºèƒ½çš„äº¤äº’å’Œæ“ä½œã€‚è¿™ç§æ–¹æ³•åœ¨å®¶åº­æœºå™¨äººã€åˆ¶é€ ä¸šå’ŒåŒ»ç–—ä¿å¥ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## playing-repeated-games-with-large-language-models
### Abstract
Large Language Models (LLMs) are transforming society and permeating into
diverse applications. As a result, LLMs will frequently interact with us and
other agents. It is, therefore, of great societal value to understand how LLMs
behave in interactive social settings. Here, we propose to use behavioral game
theory to study LLM's cooperation and coordination behavior. To do so, we let
different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with
each other and with other, human-like strategies. Our results show that LLMs
generally perform well in such tasks and also uncover persistent behavioral
signatures. In a large set of two players-two strategies games, we find that
LLMs are particularly good at games where valuing their own self-interest pays
off, like the iterated Prisoner's Dilemma family. However, they behave
sub-optimally in games that require coordination. We, therefore, further focus
on two games from these distinct families. In the canonical iterated Prisoner's
Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting
after another agent has defected only once. In the Battle of the Sexes, we find
that GPT-4 cannot match the behavior of the simple convention to alternate
between options. We verify that these behavioral signatures are stable across
robustness checks. Finally, we show how GPT-4's behavior can be modified by
providing further information about the other player as well as by asking it to
predict the other player's actions before making a choice. These results enrich
our understanding of LLM's social behavior and pave the way for a behavioral
game theory for machines.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡å¤åšå¼ˆä¸­çš„è¡Œä¸ºç ”ç©¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šå„é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬ä¸äººç±»å’Œå…¶ä»–æ™ºèƒ½ä½“ä¹‹é—´çš„äº’åŠ¨æ—¥ç›Šé¢‘ç¹ã€‚å› æ­¤ï¼Œç†è§£LLMsåœ¨ç¤¾äº¤äº’åŠ¨ä¸­çš„è¡Œä¸ºå˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡è¡Œä¸ºåšå¼ˆè®ºæ¥ç ”ç©¶LLMsçš„åˆä½œä¸åè°ƒè¡Œä¸ºã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨è¡Œä¸ºåšå¼ˆè®ºç ”ç©¶LLMs
æœ¬æ–‡æå‡ºä½¿ç”¨è¡Œä¸ºåšå¼ˆè®ºæ¥ç ”ç©¶LLMsçš„åˆä½œä¸åè°ƒè¡Œä¸ºã€‚é€šè¿‡è®©ä¸åŒçš„LLMsï¼ˆå¦‚GPT-3ã€GPT-3.5å’ŒGPT-4ï¼‰ä¸å…¶ä»–LLMsæˆ–äººç±»ç­–ç•¥è¿›è¡Œæœ‰é™é‡å¤åšå¼ˆï¼Œåˆ†æå®ƒä»¬çš„è¡Œä¸ºæ¨¡å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç ”ç©¶LLMsåœ¨ä¸åŒåšå¼ˆç±»å‹ä¸­çš„è¡¨ç°
æœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨å¤šç§åšå¼ˆç±»å‹ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬åŒèµ¢æ¸¸æˆã€ä¸å…¬å¹³æ¸¸æˆã€å¾ªç¯æ¸¸æˆã€æœ‰åæ¸¸æˆå’Œå›šå¾’å›°å¢ƒç­‰ã€‚é€šè¿‡åˆ†æLLMsåœ¨ä¸åŒåšå¼ˆç±»å‹ä¸­çš„å¾—åˆ†å’Œç­–ç•¥é€‰æ‹©ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡Œä¸ºç‰¹å¾ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨è¿½æ±‚è‡ªèº«åˆ©ç›Šçš„åšå¼ˆä¸­è¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶æ˜¯åœ¨å›šå¾’å›°å¢ƒç­‰åšå¼ˆä¸­ã€‚ç„¶è€Œï¼Œåœ¨éœ€è¦åè°ƒçš„åšå¼ˆä¸­ï¼ŒLLMsçš„è¡¨ç°è¾ƒå·®ã€‚ä¾‹å¦‚ï¼Œåœ¨ç»å…¸çš„å›šå¾’å›°å¢ƒä¸­ï¼ŒGPT-4è¡¨ç°å‡ºæç«¯çš„ä¸å®½å®¹ï¼Œä¸€æ—¦å¯¹æ–¹èƒŒå›ï¼Œå®ƒå°±ä¼šæŒç»­èƒŒå›ã€‚è€Œåœ¨æ€§åˆ«ä¹‹æˆ˜ä¸­ï¼ŒGPT-4æ— æ³•ä¸äº¤æ›¿é€‰æ‹©çš„äººç±»ç­–ç•¥ç›¸åŒ¹é…ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœæœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£LLMsçš„ç¤¾äº¤è¡Œä¸ºï¼Œå¹¶ä¸ºæœºå™¨è¡Œä¸ºåšå¼ˆè®ºçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸¤ç§æ”¹è¿›LLMsè¡Œä¸ºçš„æ–¹æ³•ï¼šæä¾›å…³äºå…¶ä»–ç©å®¶çš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠè®©LLMsé¢„æµ‹å…¶ä»–ç©å®¶çš„è¡Œä¸ºã€‚è¿™äº›æ–¹æ³•å¯ä»¥å¸®åŠ©LLMsæ›´å¥½åœ°é€‚åº”ç¤¾äº¤ç¯å¢ƒï¼Œå¹¶ä¸äººç±»è¿›è¡Œæ›´æœ‰æ•ˆçš„äº’åŠ¨ã€‚

## voyager--an-open-ended-embodied-agent-with-large-language-models
### Abstract
We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Voyagerï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾å¼å…·èº«ç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ„å»ºèƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­æŒç»­æ¢ç´¢ã€è§„åˆ’å’Œå¼€å‘æ–°æŠ€èƒ½çš„é€šç”¨å…·èº«æ™ºèƒ½ä½“ï¼Œæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨æ¢ç´¢ã€å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“åˆ©ç”¨é¢„è®­ç»ƒLLMä¸­çš„ä¸–ç•ŒçŸ¥è¯†ç”Ÿæˆä¸€è‡´çš„è¡ŒåŠ¨è®¡åˆ’æˆ–å¯æ‰§è¡Œç­–ç•¥ï¼Œä½†å®ƒä»¬å¹¶éç»ˆèº«å­¦ä¹ è€…ï¼Œæ— æ³•åœ¨é•¿æ—¶é—´è·¨åº¦å†…é€æ­¥è·å–ã€æ›´æ–°ã€ç§¯ç´¯å’Œè½¬ç§»çŸ¥è¯†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
Voyager æ˜¯ç¬¬ä¸€ä¸ªç”± LLM é©±åŠ¨çš„å…·èº«ç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿåœ¨ Minecraft ä¸­æŒç»­æ¢ç´¢ä¸–ç•Œã€è·å–å¤šæ ·åŒ–æŠ€èƒ½ï¼Œå¹¶åœ¨æ²¡æœ‰äººç±»å¹²é¢„çš„æƒ…å†µä¸‹è¿›è¡Œæ–°çš„å‘ç°ã€‚Voyager ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªåŠ¨è¯¾ç¨‹
Voyager é€šè¿‡è‡ªåŠ¨è¯¾ç¨‹è¿›è¡Œå¼€æ”¾å¼æ¢ç´¢ï¼Œè¯¥è¯¾ç¨‹ç”± GPT-4 ç”Ÿæˆï¼Œæ—¨åœ¨â€œå‘ç°å°½å¯èƒ½å¤šçš„å¤šæ ·åŒ–äº‹ç‰©â€ã€‚è¯¾ç¨‹ä¼šæ ¹æ®æ¢ç´¢è¿›åº¦å’Œæ™ºèƒ½ä½“çš„çŠ¶æ€æå‡ºè¶Šæ¥è¶Šéš¾çš„ä»»åŠ¡ï¼Œä»è€Œæ¨åŠ¨æ™ºèƒ½ä½“ä¸æ–­å­¦ä¹ æ–°æŠ€èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæŠ€èƒ½åº“
Voyager æ‹¥æœ‰ä¸€ä¸ªä¸æ–­å¢é•¿çš„æŠ€èƒ½åº“ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢å¯æ‰§è¡Œä»£ç ï¼Œä»¥å­˜å‚¨å’Œæ£€ç´¢å¤æ‚çš„è¡Œä¸ºã€‚æ¯ä¸ªæŠ€èƒ½éƒ½ç”±å¯æ‰§è¡Œä»£ç è¡¨ç¤ºï¼Œè¿™äº›ä»£ç å¯ä»¥è‡ªç„¶åœ°è¡¨ç¤ºæ—¶é—´æ‰©å±•å’Œç»„åˆåŠ¨ä½œï¼Œè¿™å¯¹äº Minecraft ä¸­çš„è®¸å¤šé•¿æœŸä»»åŠ¡è‡³å…³é‡è¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿­ä»£æç¤ºæœºåˆ¶
Voyager é€šè¿‡è¿­ä»£æç¤ºæœºåˆ¶ç”Ÿæˆå¯æ‰§è¡Œä»£ç ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨ç¯å¢ƒåé¦ˆã€æ‰§è¡Œé”™è¯¯å’Œè‡ªæˆ‘éªŒè¯æ¥æ”¹è¿›ç¨‹åºã€‚è¯¥æœºåˆ¶é€šè¿‡æ‰§è¡Œç”Ÿæˆçš„ç¨‹åºã€è·å–ç¯å¢ƒåé¦ˆå’Œæ‰§è¡Œé”™è¯¯ï¼Œå¹¶å°†è¿™äº›åé¦ˆçº³å…¥ GPT-4 çš„æç¤ºä¸­ï¼Œä»è€Œè¿›è¡Œä»£ç æ”¹è¿›ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œï¼Œç›´åˆ°è‡ªæˆ‘éªŒè¯æ¨¡å—ç¡®è®¤ä»»åŠ¡å®Œæˆï¼Œæ­¤æ—¶å°†ç¨‹åºæ·»åŠ åˆ°æŠ€èƒ½åº“ä¸­ï¼Œå¹¶æŸ¥è¯¢è‡ªåŠ¨è¯¾ç¨‹ä»¥è·å–ä¸‹ä¸€ä¸ªç›®æ ‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
Voyager åœ¨ MineDojo æ¡†æ¶ä¸­ä¸å…¶ä»– LLM åŸºäºæ™ºèƒ½ä½“æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ Voyager åœ¨å‘ç°æ–°ç‰©å“ã€è§£é” Minecraft æŠ€æœ¯æ ‘ã€ç©¿è¶Šå„ç§åœ°å½¢ä»¥åŠå°†å­¦ä¹ åˆ°çš„æŠ€èƒ½åº“åº”ç”¨äºæ–°ä¸–ç•Œä¸­çš„æœªè§ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚Voyager è·å¾—äº† 3.3 å€çš„æ–°ç‰©å“ï¼Œè§£é”å…³é”®æŠ€æœ¯æ ‘é‡Œç¨‹ç¢‘çš„é€Ÿåº¦æé«˜äº† 15.3 å€ï¼Œç©¿è¶Šçš„è·ç¦»æ˜¯åŸºçº¿çš„ 2.3 å€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Voyager çš„è®¾è®¡ä¸ºå¼€å‘å¼ºå¤§çš„é€šç”¨æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªèµ·ç‚¹ï¼Œæ— éœ€è°ƒæ•´æ¨¡å‹å‚æ•°ã€‚å…¶è‡ªåŠ¨è¯¾ç¨‹ã€æŠ€èƒ½åº“å’Œè¿­ä»£æç¤ºæœºåˆ¶ä¸ºç»ˆèº«å­¦ä¹ æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒVoyager çš„æŠ€èƒ½åº“å¯ä»¥ä½œä¸ºå…¶ä»–æ–¹æ³•çš„å³æ’å³ç”¨èµ„äº§ï¼Œæœ‰æ•ˆåœ°æé«˜æ€§èƒ½ã€‚

## ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory
### Abstract
The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular "ObtainDiamond" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the "ObtainDiamond" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Minecraftä¸­çš„å¹½çµï¼šé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºæ–‡æœ¬çš„çŸ¥è¯†ä¸è®°å¿†ï¼Œåˆ›å»ºå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Minecraftä½œä¸ºä¸€æ¬¾å¼€æ”¾ä¸–ç•Œæ¸¸æˆï¼Œå¸å¼•äº†å¤§é‡ç ”ç©¶å…´è¶£ï¼Œæˆä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„æ™ºèƒ½ä½“çš„ä¸°å¯Œå¹³å°ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šç›®æ ‡ä¸Šï¼Œå¦‚æµè¡Œçš„â€œObtainDiamondâ€ä»»åŠ¡ï¼Œå°šæœªåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæœ‰æ•ˆçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æœ€é«˜æˆåŠŸç‡ä»…ä¸ºçº¦20%ï¼Œçªæ˜¾äº†ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ§åˆ¶å™¨æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Ghost in the Minecraftï¼ˆGITMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ç›¸ç»“åˆï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿåœ¨Minecraftä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“ï¼ˆGCAsï¼‰ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLMåˆ†è§£å™¨
LLMåˆ†è§£å™¨è´Ÿè´£å°†ä»»åŠ¡ç›®æ ‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´æ˜“äºå®ç°çš„å­ç›®æ ‡ã€‚é€šè¿‡è§£å†³æ¯ä¸ªå­ç›®æ ‡ï¼Œå¯ä»¥é€æ­¥å®ç°ä»»åŠ¡ç›®æ ‡ã€‚LLMåˆ†è§£å™¨åˆ©ç”¨ä»äº’è”ç½‘æ”¶é›†çš„æ–‡æœ¬çŸ¥è¯†ï¼Œå°†ç›®æ ‡åˆ†è§£ä¸ºå­ç›®æ ‡æ ‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMè§„åˆ’å™¨
LLMè§„åˆ’å™¨è´Ÿè´£ä¸ºæ¯ä¸ªå­ç›®æ ‡ç”Ÿæˆä¸€ç³»åˆ—ç»“æ„åŒ–æ“ä½œã€‚ç»“æ„åŒ–æ“ä½œå…·æœ‰æ˜ç¡®çš„è¯­ä¹‰å’Œç›¸åº”çš„åé¦ˆï¼Œä½¿LLMsèƒ½å¤Ÿåœ¨è®¤çŸ¥å±‚é¢ç†è§£å‘¨å›´ç¯å¢ƒå¹¶åšå‡ºå†³ç­–ã€‚LLMè§„åˆ’å™¨è¿˜è®°å½•å’Œæ€»ç»“æˆåŠŸçš„æ“ä½œåˆ—è¡¨ï¼Œä»¥å¢å¼ºæœªæ¥çš„è§„åˆ’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šLLMæ¥å£
LLMæ¥å£è´Ÿè´£å°†ç»“æ„åŒ–æ“ä½œè½¬æ¢ä¸ºé”®ç›˜/é¼ æ ‡æ“ä½œï¼Œå¹¶ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚å®ƒè¿˜ä»ç¯å¢ƒä¸­æå–è§‚å¯Ÿç»“æœï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåé¦ˆæ¶ˆæ¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨â€œObtainDiamondâ€ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°äº†47.5%ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ™ºèƒ½ä½“æ˜¯ç¬¬ä¸€ä¸ªåœ¨Minecraft Overworldä¸­è·å–æ‰€æœ‰ç‰©å“çš„æ™ºèƒ½ä½“ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„æŠ€èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GITMæ¡†æ¶ä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œä¸­è¿è¡Œçš„é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡åˆ©ç”¨LLMsçš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠåŸºäºæ–‡æœ¬çš„çŸ¥è¯†å’Œè®°å¿†ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“æœ‰æ•ˆåœ°å¤„ç†å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å„ç§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å…·æœ‰é«˜æ•ˆçš„å­¦ä¹ æ•ˆç‡å’Œè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶åœ¨å¼€å‘é€šç”¨èƒ½åŠ›æ™ºèƒ½ä½“æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## spring--studying-the-paper-and-reasoning-to-play-games
### Abstract
Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent's current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM's answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
"reasoning" induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | SPRINGï¼šé€šè¿‡é˜…è¯»è®ºæ–‡å’Œæ¨ç†æ¥ç©æ¸¸æˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼€æ”¾ä¸–ç•Œç”Ÿå­˜æ¸¸æˆå¯¹AIç®—æ³•æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤šä»»åŠ¡å¤„ç†ã€æ·±åº¦æ¢ç´¢å’Œç›®æ ‡ä¼˜å…ˆçº§æ’åºã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§£å†³æ¸¸æˆé—®é¢˜æ–¹é¢å¾ˆå—æ¬¢è¿ï¼Œä½†å…¶é«˜æ ·æœ¬å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨åƒCrafteræˆ–Minecraftè¿™æ ·çš„å¤æ‚å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­çš„æœ‰æ•ˆæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
SPRINGæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡é˜…è¯»æ¸¸æˆçš„åŸå§‹å­¦æœ¯è®ºæ–‡å¹¶ä½¿ç”¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å­¦ä¹ åˆ°çš„çŸ¥è¯†æ¥æ¨ç†å’Œç©æ¸¸æˆã€‚SPRINGæ¡†æ¶ä½¿ç”¨ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œå…¶ä¸­æ¸¸æˆç›¸å…³é—®é¢˜ä½œä¸ºèŠ‚ç‚¹ï¼Œä¾èµ–å…³ç³»ä½œä¸ºè¾¹ã€‚é€šè¿‡éå†DAGå¹¶æŒ‰æ‹“æ‰‘é¡ºåºè®¡ç®—LLMå¯¹æ¯ä¸ªèŠ‚ç‚¹çš„å“åº”ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šåœ¨ç¯å¢ƒä¸­é‡‡å–çš„æœ€ä½³è¡ŒåŠ¨ï¼ŒLLMå¯¹æœ€ç»ˆèŠ‚ç‚¹çš„ç­”æ¡ˆç›´æ¥è½¬æ¢ä¸ºç¯å¢ƒè¡ŒåŠ¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Crafterå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬çš„å®éªŒç ”ç©¶äº†ä¸åŒå½¢å¼çš„æç¤ºåœ¨ä¸Šä¸‹æ–‡ä¸­å¼•èµ·çš„â€œæ¨ç†â€è´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“LLMè¢«æç¤ºè¿›è¡Œä¸€è‡´çš„æ€ç»´é“¾æ—¶ï¼Œå®ƒä»¬åœ¨å®Œæˆå¤æ‚çš„é«˜çº§è½¨è¿¹æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚å®šé‡åœ°è¯´ï¼ŒSPRINGä¸GPT-4çš„è¡¨ç°ä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„RLåŸºçº¿ï¼Œè¿™äº›åŸºçº¿ç»è¿‡1Mæ­¥çš„è®­ç»ƒï¼Œè€ŒSPRINGæ²¡æœ‰ç»è¿‡ä»»ä½•è®­ç»ƒã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
SPRINGæ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡ä»å­¦æœ¯è®ºæ–‡ä¸­æ˜ç¡®æå–å¤šä¸ªäº¤äº’å’Œç§‘æŠ€æ ‘ä¾èµ–æ¥åº”å¯¹ç«äº‰æ€§RLåŸºå‡†çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­å±•ç¤ºSOTAæ€§èƒ½çš„é›¶æ ·æœ¬LLM-basedï¼ˆGPT-4ï¼‰ç­–ç•¥ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸åŒæç¤ºå¼•èµ·çš„ä¸Šä¸‹æ–‡â€œæ¨ç†â€è´¨é‡ï¼Œå¹¶é€šè¿‡DAGä¸­çš„é—®é¢˜é“¾æå‡ºäº†ä¸€ç§å—æ§çš„æ€ç»´é“¾æç¤ºï¼Œç”¨äºå†³ç­–ã€‚

## improving-factuality-and-reasoning-in-language-models-through-multiagent-debate
### Abstract
Large language models (LLMs) have demonstrated remarkable capabilities in
language generation, understanding, and few-shot learning in recent years. An
extensive body of work has explored how their performance may be further
improved through the tools of prompting, ranging from verification,
self-consistency, or intermediate scratchpads. In this paper, we present a
complementary approach to improve language responses where multiple language
model instances propose and debate their individual responses and reasoning
processes over multiple rounds to arrive at a common final answer. Our findings
indicate that this approach significantly enhances mathematical and strategic
reasoning across a number of tasks. We also demonstrate that our approach
improves the factual validity of generated content, reducing fallacious answers
and hallucinations that contemporary models are prone to. Our approach may be
directly applied to existing black-box models and uses identical procedure and
prompts for all tasks we investigate. Overall, our findings suggest that such
"society of minds" approach has the potential to significantly advance the
capabilities of LLMs and pave the way for further breakthroughs in language
generation and understanding.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºæå‡è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç”Ÿæˆã€ç†è§£å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨äº‹å®æ€§å’Œæ¨ç†æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“äº§ç”Ÿé”™è¯¯çš„äº‹å®å’Œæ¨ç†è·³è·ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºæ¥æå‡LLMsçš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¤šæ™ºèƒ½ä½“è¾©è®º
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºæ¥æå‡LLMsçš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæŸ¥è¯¢ï¼Œå¤šä¸ªLLMå®ä¾‹ï¼ˆæˆ–æ™ºèƒ½ä½“ï¼‰é¦–å…ˆç”Ÿæˆå„è‡ªçš„å€™é€‰ç­”æ¡ˆã€‚ç„¶åï¼Œæ¯ä¸ªæ™ºèƒ½ä½“é˜…è¯»å¹¶æ‰¹è¯„å…¶ä»–æ™ºèƒ½ä½“çš„ç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨è¿™äº›å†…å®¹æ¥æ›´æ–°è‡ªå·±çš„ç­”æ¡ˆã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤è¿›è¡Œå¤šè½®ï¼Œç›´åˆ°æ™ºèƒ½ä½“ä»¬è¾¾æˆä¸€ä¸ªå…±åŒçš„æœ€ç»ˆç­”æ¡ˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›
æœ¬æ–‡å‘ç°ï¼Œå¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•åœ¨å¤šä¸ªæ¨ç†ã€äº‹å®æ€§å’Œé—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡å‹åŸºçº¿ï¼Œå¦‚é›¶æ ·æœ¬æ€ç»´é“¾å’Œåæ€ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æé«˜ç”Ÿæˆå†…å®¹çš„äº‹å®æ€§ï¼Œå‡å°‘å½“ä»£æ¨¡å‹å®¹æ˜“äº§ç”Ÿçš„é”™è¯¯ç­”æ¡ˆå’Œå¹»è§‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å¤šä¸ªæ¨ç†å’Œäº‹å®æ€§ä»»åŠ¡ä¸Šè¯„ä¼°äº†å¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ç®—æœ¯ã€å°å­¦æ•°å­¦ã€å›½é™…è±¡æ£‹ç§»åŠ¨é¢„æµ‹ã€ä¼ è®°ç”Ÿæˆå’ŒMMLUã€‚ç»“æœè¡¨æ˜ï¼Œå¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•åœ¨è¿™äº›ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å¤šæ™ºèƒ½ä½“è¾©è®ºæ–¹æ³•ä¸ºæå‡LLMsçš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œå¯ä»¥åº”ç”¨äºå„ç§ä¸åŒçš„æ¨ç†å’Œäº‹å®æ€§ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ä¸å…¶ä»–æ¨¡å‹ç”Ÿæˆæ”¹è¿›æ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡LLMsçš„æ€§èƒ½ã€‚

## improving-language-model-negotiation-with-self-play-and-in-context-learning-from-ai-feedback
### Abstract
We study whether multiple large language models (LLMs) can autonomously
improve each other in a negotiation game by playing, reflecting, and
criticizing. We are interested in this question because if LLMs were able to
improve each other, it would imply the possibility of creating strong AI agents
with minimal human intervention. We ask two LLMs to negotiate with each other,
playing the roles of a buyer and a seller, respectively. They aim to reach a
deal with the buyer targeting a lower price and the seller a higher one. A
third language model, playing the critic, provides feedback to a player to
improve the player's negotiation strategies. We let the two agents play
multiple rounds, using previous negotiation history and AI feedback as
in-context demonstrations to improve the model's negotiation strategy
iteratively. We use different LLMs (GPT and Claude) for different roles and use
the deal price as the evaluation metric. Our experiments reveal multiple
intriguing findings: (1) Only a subset of the language models we consider can
self-play and improve the deal price from AI feedback, weaker models either do
not understand the game's rules or cannot incorporate AI feedback for further
improvement. (2) Models' abilities to learn from the feedback differ when
playing different roles. For example, it is harder for Claude-instant to
improve as the buyer than as the seller. (3) When unrolling the game to
multiple rounds, stronger agents can consistently improve their performance by
meaningfully using previous experiences and iterative AI feedback, yet have a
higher risk of breaking the deal. We hope our work provides insightful initial
explorations of having models autonomously improve each other with game playing
and AI feedback.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹åœ¨è°ˆåˆ¤æ¸¸æˆä¸­é€šè¿‡è‡ªæˆ‘åšå¼ˆå’ŒAIåé¦ˆè¿›è¡Œè‡ªæˆ‘æå‡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æœ¬æ–‡ç ”ç©¶äº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤Ÿåœ¨è°ˆåˆ¤æ¸¸æˆä¸­é€šè¿‡è‡ªæˆ‘åšå¼ˆã€åæ€å’Œæ‰¹è¯„æ¥è‡ªä¸»åœ°ç›¸äº’æå‡ã€‚è¿™ä¸€ç ”ç©¶é—®é¢˜å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå¦‚æœLLMsèƒ½å¤Ÿç›¸äº’æå‡ï¼Œé‚£ä¹ˆå°±æœ‰å¯èƒ½ä»¥æå°çš„äººç±»å¹²é¢„æ¥åˆ›å»ºå¼ºå¤§çš„AIä»£ç†ã€‚æœ¬æ–‡é€šè¿‡è®©ä¸¤ä¸ªLLMsè¿›è¡Œè°ˆåˆ¤ï¼Œä¸€ä¸ªæ‰®æ¼”ä¹°å®¶ï¼Œä¸€ä¸ªæ‰®æ¼”å–å®¶ï¼Œå¹¶è®©ç¬¬ä¸‰ä¸ªLLMä½œä¸ºæ‰¹è¯„è€…æä¾›åé¦ˆæ¥æ”¹å–„ç©å®¶çš„è°ˆåˆ¤ç­–ç•¥ï¼Œä»è€Œæ¢ç´¢äº†è¿™ä¸€å¯èƒ½æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡æå‡ºäº†â€œä»AIåé¦ˆä¸­è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ â€ï¼ˆICL-AIFï¼‰çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨AIæ‰¹è¯„è€…çš„åé¦ˆä»¥åŠä¹‹å‰çš„è°ˆåˆ¤å†å²ä½œä¸ºä¸Šä¸‹æ–‡æ¼”ç¤ºï¼Œä»¥è¿­ä»£åœ°æ”¹è¿›æ¨¡å‹çš„è°ˆåˆ¤ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡ä½¿ç”¨ä¸åŒçš„LLMsï¼ˆGPTå’ŒClaudeï¼‰æ¥æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œå¹¶ä½¿ç”¨äº¤æ˜“ä»·æ ¼ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡å‘ç°åªæœ‰ä¸€éƒ¨åˆ†LLMsèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘åšå¼ˆå’ŒAIåé¦ˆæ¥æé«˜äº¤æ˜“ä»·æ ¼ï¼Œè€Œè¾ƒå¼±çš„æ¨¡å‹è¦ä¹ˆä¸ç†è§£æ¸¸æˆè§„åˆ™ï¼Œè¦ä¹ˆæ— æ³•å°†AIåé¦ˆçº³å…¥è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œåªæœ‰ä¸€éƒ¨åˆ†LLMsèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘åšå¼ˆå’ŒAIåé¦ˆæ¥æé«˜äº¤æ˜“ä»·æ ¼ï¼Œè€Œè¾ƒå¼±çš„æ¨¡å‹è¦ä¹ˆä¸ç†è§£æ¸¸æˆè§„åˆ™ï¼Œè¦ä¹ˆæ— æ³•å°†AIåé¦ˆçº³å…¥è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨æ‰®æ¼”ä¸åŒè§’è‰²æ—¶ï¼Œä»åé¦ˆä¸­å­¦ä¹ çš„èƒ½åŠ›ä¹Ÿæœ‰æ‰€ä¸åŒã€‚ä¾‹å¦‚ï¼ŒClaude-instantåœ¨æ‰®æ¼”ä¹°å®¶æ—¶æ¯”æ‰®æ¼”å–å®¶æ—¶æ›´éš¾æå‡ã€‚å½“æ¸¸æˆæ‰©å±•åˆ°å¤šè½®æ—¶ï¼Œæ›´å¼ºçš„ä»£ç†å¯ä»¥é€šè¿‡æœ‰æ„ä¹‰åœ°ä½¿ç”¨ä¹‹å‰çš„ç»éªŒå’Œè¿­ä»£AIåé¦ˆæ¥æŒç»­æé«˜å…¶æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨æ›´é«˜çš„äº¤æ˜“ç ´è£‚é£é™©ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨è°ˆåˆ¤æ¸¸æˆä¸­å…·æœ‰è‡ªæˆ‘æå‡çš„æ½œåŠ›ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨ä¸€äº›æŒ‘æˆ˜å’Œé£é™©ã€‚æœªæ¥ç ”ç©¶å¯ä»¥æ¢ç´¢å¦‚ä½•æ›´å¥½åœ°åˆ©ç”¨AIåé¦ˆæ¥æå‡LLMsçš„æ€§èƒ½ï¼Œå¹¶ç¡®ä¿å…¶è¡Œä¸ºç¬¦åˆäººç±»çš„æœŸæœ›å’Œä»·å€¼è§‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–å¤šæ™ºèƒ½ä½“æ¸¸æˆåœºæ™¯ä¸‹çš„AIå­¦ä¹ æä¾›å‚è€ƒã€‚

## tidybot--personalized-robot-assistance-with-large-language-models
### Abstract
For a robot to personalize physical assistance effectively, it must learn
user preferences that can be generally reapplied to future scenarios. In this
work, we investigate personalization of household cleanup with robots that can
tidy up rooms by picking up objects and putting them away. A key challenge is
determining the proper place to put each object, as people's preferences can
vary greatly depending on personal taste or cultural background. For instance,
one person may prefer storing shirts in the drawer, while another may prefer
them on the shelf. We aim to build systems that can learn such preferences from
just a handful of examples via prior interactions with a particular person. We
show that robots can combine language-based planning and perception with the
few-shot summarization capabilities of large language models (LLMs) to infer
generalized user preferences that are broadly applicable to future
interactions. This approach enables fast adaptation and achieves 91.2% accuracy
on unseen objects in our benchmark dataset. We also demonstrate our approach on
a real-world mobile manipulator called TidyBot, which successfully puts away
85.0% of objects in real-world test scenarios.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TidyBotï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–æœºå™¨äººè¾…åŠ©

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæœºå™¨äººè¾…åŠ©å·²ç»æˆä¸ºäººä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ä¸€éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœºå™¨äººè¾…åŠ©ç³»ç»Ÿå¾€å¾€ç¼ºä¹ä¸ªæ€§åŒ–ï¼Œæ— æ³•æ ¹æ®ç”¨æˆ·çš„ä¸ªäººå–œå¥½å’Œä¹ æƒ¯æ¥å®Œæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåœ¨å®¶åº­æ¸…æ´ä»»åŠ¡ä¸­ï¼Œæœºå™¨äººéœ€è¦æ ¹æ®ç”¨æˆ·çš„å–œå¥½æ¥å†³å®šæ¯ä¸ªç‰©å“åº”è¯¥æ”¾ç½®çš„ä½ç½®ï¼Œè€Œç”¨æˆ·çš„å–œå¥½å¯èƒ½ä¼šå› ä¸ºä¸ªäººå“å‘³æˆ–æ–‡åŒ–èƒŒæ™¯è€Œæœ‰æ‰€ä¸åŒã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º TidyBot çš„ä¸ªæ€§åŒ–æœºå™¨äººè¾…åŠ©ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‘˜è¦èƒ½åŠ›æ¥å­¦ä¹ ç”¨æˆ·çš„åå¥½ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœªæ¥çš„åœºæ™¯ä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒTidyBot é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥å®ç°ä¸ªæ€§åŒ–è¾…åŠ©ï¼š

1. **ç”¨æˆ·è¾“å…¥ç¤ºä¾‹**ï¼š ç”¨æˆ·é€šè¿‡æ–‡æœ¬è¾“å…¥æä¾›ä¸€äº›ç¤ºä¾‹ï¼Œè¯´æ˜ä»–ä»¬å¸Œæœ›å¦‚ä½•æ”¾ç½®ç‰¹å®šçš„ç‰©å“ã€‚
2. **LLM æ‘˜è¦**ï¼š LLM å°†è¿™äº›ç¤ºä¾‹æ‘˜è¦æˆä¸€ä¸ªé€šç”¨çš„è§„åˆ™ï¼Œä¾‹å¦‚â€œæµ…è‰²è¡£æœæ”¾åœ¨æŠ½å±‰é‡Œï¼Œæ·±è‰²è¡£æœæ”¾åœ¨è¡£æŸœé‡Œâ€ã€‚
3. **è§„åˆ™åº”ç”¨**ï¼š TidyBot ä½¿ç”¨è¿™äº›è§„åˆ™æ¥ç¡®å®šæ–°ç‰©å“åº”è¯¥æ”¾ç½®çš„ä½ç½®ï¼Œå¹¶å°†å…¶æ”¾å…¥ç›¸åº”çš„å®¹å™¨ä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨æ–‡æœ¬åŸºå‡†æ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººç³»ç»Ÿä¸Šè¯„ä¼°äº† TidyBot çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒTidyBot åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº† 91.2% çš„å‡†ç¡®ç‡ï¼Œåœ¨çœŸå®ä¸–ç•Œçš„æµ‹è¯•åœºæ™¯ä¸­æˆåŠŸåœ°å°† 85.0% çš„ç‰©å“æ”¾å…¥æ­£ç¡®çš„å®¹å™¨ä¸­ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ TidyBot ç³»ç»Ÿä¸ºä¸ªæ€§åŒ–æœºå™¨äººè¾…åŠ©æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡åˆ©ç”¨ LLM çš„æ‘˜è¦èƒ½åŠ›ï¼ŒTidyBot å¯ä»¥å¿«é€Ÿå­¦ä¹ ç”¨æˆ·çš„åå¥½ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœªæ¥çš„åœºæ™¯ä¸­ã€‚è¿™ç§æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š

* **å¿«é€Ÿé€‚åº”**ï¼š TidyBot åªéœ€è¦å°‘é‡ç¤ºä¾‹å°±å¯ä»¥å­¦ä¹ ç”¨æˆ·çš„åå¥½ï¼Œä»è€Œå®ç°å¿«é€Ÿé€‚åº”ã€‚
* **å¯è§£é‡Šæ€§**ï¼š LLM ç”Ÿæˆçš„è§„åˆ™ä»¥æ–‡æœ¬å½¢å¼å‘ˆç°ï¼Œæ˜“äºäººç±»ç†è§£å’Œè§£é‡Šã€‚
* **æ³›åŒ–èƒ½åŠ›**ï¼š TidyBot å¯ä»¥å°†å­¦ä¹ åˆ°çš„è§„åˆ™åº”ç”¨äºæ–°çš„åœºæ™¯ä¸­ï¼Œä»è€Œå®ç°æ³›åŒ–ã€‚

### ğŸŒŸ æ€»ç»“
TidyBot æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–æœºå™¨äººè¾…åŠ©ç³»ç»Ÿï¼Œå®ƒå¯ä»¥å¸®åŠ©æœºå™¨äººæ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„åå¥½ï¼Œå¹¶æä¾›æ›´åŠ ä¸ªæ€§åŒ–çš„è¾…åŠ©æœåŠ¡ã€‚æœ¬æ–‡æå‡ºçš„ TidyBot ç³»ç»Ÿä¸ºä¸ªæ€§åŒ–æœºå™¨äººè¾…åŠ©æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## ark--augmented-reality-with-knowledge-interactive-emergent-ability
### Abstract
Despite the growing adoption of mixed reality and interactive AI agents, it
remains challenging for these systems to generate high quality 2D/3D scenes in
unseen environments. The common practice requires deploying an AI agent to
collect large amounts of data for model training for every new task. This
process is costly, or even impossible, for many domains. In this study, we
develop an infinite agent that learns to transfer knowledge memory from general
foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene
understanding and generation in the physical or virtual world. The heart of our
approach is an emerging mechanism, dubbed Augmented Reality with Knowledge
Inference Interaction (ArK), which leverages knowledge-memory to generate
scenes in unseen physical world and virtual reality environments. The knowledge
interactive emergent ability (Figure 1) is demonstrated as the observation
learns i) micro-action of cross-modality: in multi-modality models to collect a
large amount of relevant knowledge memory data for each interaction task (e.g.,
unseen scene understanding) from the physical reality; and ii) macro-behavior
of reality-agnostic: in mix-reality environments to improve interactions that
tailor to different characterized roles, target variables, collaborative
information, and so on. We validate the effectiveness of ArK on the scene
generation and editing tasks. We show that our ArK approach, combined with
large foundation models, significantly improves the quality of generated 2D/3D
scenes, compared to baselines, demonstrating the potential benefit of
incorporating ArK in generative AI for applications such as metaverse and
gaming simulation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ArKï¼šåŸºäºçŸ¥è¯†çš„å¢å¼ºç°å®äº¤äº’å¼æ¶Œç°èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ··åˆç°å®å’Œäº¤äº’å¼AIä»£ç†çš„æ—¥ç›Šæ™®åŠï¼Œè¿™äº›ç³»ç»Ÿåœ¨ç”Ÿæˆé«˜è´¨é‡2D/3Dåœºæ™¯æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åšæ³•éœ€è¦éƒ¨ç½²AIä»£ç†æ¥æ”¶é›†å¤§é‡æ•°æ®ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¿™å¯¹äºè®¸å¤šé¢†åŸŸæ¥è¯´æ—¢æ˜‚è´µåˆä¸å¯èƒ½ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œé€šè¿‡å¼€å‘ä¸€ä¸ªæ— é™ä»£ç†ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿä»é€šç”¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT4ã€DALLEï¼‰ä¸­å­¦ä¹ ï¼Œå¹¶å°†çŸ¥è¯†è®°å¿†è½¬ç§»åˆ°æ–°é¢†åŸŸæˆ–åœºæ™¯ä¸­ï¼Œä»¥å®ç°ç‰©ç†æˆ–è™šæ‹Ÿä¸–ç•Œä¸­çš„åœºæ™¯ç†è§£å’Œç”Ÿæˆã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šArKæœºåˆ¶
æœ¬æ–‡çš„æ ¸å¿ƒæ˜¯ArKæœºåˆ¶ï¼Œå³â€œå¢å¼ºç°å®ä¸çŸ¥è¯†æ¨ç†äº¤äº’â€ã€‚è¯¥æœºåˆ¶åˆ©ç”¨çŸ¥è¯†è®°å¿†æ¥ç”Ÿæˆæœªè§è¿‡çš„ç‰©ç†ä¸–ç•Œå’Œè™šæ‹Ÿç°å®ç¯å¢ƒä¸­çš„åœºæ™¯ã€‚ArKæœºåˆ¶é€šè¿‡è§‚å¯Ÿå­¦ä¹ å®ç°ï¼š
- è·¨æ¨¡æ€çš„å¾®åŠ¨ä½œï¼šåœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­ï¼Œä¸ºæ¯ä¸ªäº¤äº’ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œæœªè§è¿‡çš„åœºæ™¯ç†è§£ï¼‰ä»ç‰©ç†ç°å®ä¸­æ”¶é›†å¤§é‡ç›¸å…³çš„çŸ¥è¯†è®°å¿†æ•°æ®ã€‚
- ç°å®æ— å…³çš„å®è¡Œä¸ºï¼šåœ¨æ··åˆç°å®ç¯å¢ƒä¸­ï¼Œé€šè¿‡è°ƒæ•´äº¤äº’ä»¥é€‚åº”ä¸åŒçš„è§’è‰²ã€ç›®æ ‡å˜é‡ã€åä½œä¿¡æ¯ç­‰ï¼Œä»è€Œæé«˜äº¤äº’æ•ˆæœã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ— é™çŸ¥è¯†è®°å¿†ä»£ç†
æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªæ— é™çŸ¥è¯†è®°å¿†ä»£ç†ï¼Œç”¨äºç‰©ç†ä¸–ç•Œä¸­çš„åœºæ™¯ç†è§£å’Œç”Ÿæˆã€‚è¯¥ä»£ç†å­¦ä¹ ä»é€šç”¨åŸºç¡€æ¨¡å‹ä¸­è½¬ç§»çŸ¥è¯†è®°å¿†ï¼Œå¹¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆæœªè§è¿‡çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œè¯¥ä»£ç†è¿˜èƒ½å¤Ÿé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ¨¡ä»¿å­¦ä¹ æ¥æé«˜å…¶æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨åœºæ™¯ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸ŠéªŒè¯äº†ArKçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒArKæ–¹æ³•ç»“åˆå¤§å‹åŸºç¡€æ¨¡å‹æ˜¾è‘—æé«˜äº†ç”Ÿæˆçš„2D/3Dåœºæ™¯çš„è´¨é‡ï¼Œè¯æ˜äº†å°†ArKçº³å…¥ç”Ÿæˆå¼AIä¸­çš„æ½œåœ¨ç›Šå¤„ï¼Œä¾‹å¦‚åœ¨å…ƒå®‡å®™å’Œæ¸¸æˆæ¨¡æ‹Ÿä¸­çš„åº”ç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ArKæœºåˆ¶å’Œæ— é™çŸ¥è¯†è®°å¿†ä»£ç†ä¸ºç”Ÿæˆå¼AIé¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚ArKæœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨çŸ¥è¯†è®°å¿†æ¥ç”Ÿæˆé«˜è´¨é‡çš„2D/3Dåœºæ™¯ï¼Œè€Œæ— é™çŸ¥è¯†è®°å¿†ä»£ç†åˆ™èƒ½å¤Ÿé€šè¿‡å­¦ä¹ å’Œæ¨ç†æ¥æé«˜å…¶æ€§èƒ½ã€‚è¿™äº›åˆ›æ–°ç‚¹ä¸ºç”Ÿæˆå¼AIçš„å‘å±•å’Œåº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## generative-agents--interactive-simulacra-of-human-behavior
### Abstract
Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„äº¤äº’å¼æ¨¡æ‹Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œäººä»¬å¯¹äºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„æ™ºèƒ½ä½“äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚è¿™äº›æ™ºèƒ½ä½“å¯ä»¥åº”ç”¨äºå„ç§åœºæ™¯ï¼Œä¾‹å¦‚æ²‰æµ¸å¼ç¯å¢ƒã€äººé™…æ²Ÿé€šæ¼”ç»ƒç©ºé—´ã€åŸå‹è®¾è®¡å·¥å…·ç­‰ã€‚ç„¶è€Œï¼Œè¦åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿé•¿æœŸä¿æŒä¸€è‡´æ€§å’Œå¯ä¿¡åº¦çš„æ™ºèƒ½ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¼æ™ºèƒ½ä½“
æœ¬æ–‡æå‡ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„æ¦‚å¿µï¼Œå³åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¨¡æ‹Ÿå¯ä¿¡çš„äººç±»è¡Œä¸ºã€‚è¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œæ—¥å¸¸æ´»åŠ¨ï¼Œå¦‚èµ·åºŠã€åšé¥­ã€ä¸Šç­ç­‰ï¼Œå¹¶èƒ½å¤Ÿå½¢æˆè‡ªå·±çš„è§‚ç‚¹ã€ä¸ä»–äººäº’åŠ¨ã€å‘èµ·å¯¹è¯ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ™ºèƒ½ä½“æ¶æ„
ä¸ºäº†å®ç°ç”Ÿæˆå¼æ™ºèƒ½ä½“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œè¯¥æ¶æ„æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­˜å‚¨æ™ºèƒ½ä½“çš„ç»éªŒè®°å½•ï¼Œå¹¶å°†è¿™äº›è®°å¿†éšç€æ—¶é—´çš„æ¨ç§»åˆæˆæ›´é«˜å±‚æ¬¡çš„åæ€ï¼Œå¹¶åŠ¨æ€åœ°æ£€ç´¢å®ƒä»¬æ¥è§„åˆ’è¡Œä¸ºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­åˆ›å»ºä¸€ä¸ªç”±25ä¸ªæ™ºèƒ½ä½“ç»„æˆçš„å°é•‡ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ™ºèƒ½ä½“èƒ½å¤Ÿäº§ç”Ÿå¯ä¿¡çš„ä¸ªä½“å’Œç¾¤ä½“è¡Œä¸ºï¼Œä¾‹å¦‚ï¼Œåœ¨ç”¨æˆ·æŒ‡å®šä¸€ä¸ªæ™ºèƒ½ä½“æƒ³è¦ä¸¾åŠæƒ…äººèŠ‚æ´¾å¯¹çš„æƒ…å†µä¸‹ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªä¸»åœ°é‚€è¯·å…¶ä»–æ™ºèƒ½ä½“å‚åŠ æ´¾å¯¹ï¼Œå¹¶åè°ƒåœ¨æ­£ç¡®çš„æ—¶é—´ä¸€èµ·åˆ°è¾¾æ´¾å¯¹åœ°ç‚¹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“æ¶æ„ä¸ºåˆ›å»ºå¯ä¿¡çš„äººç±»è¡Œä¸ºæ¨¡æ‹Ÿæä¾›äº†æ–°çš„æ€è·¯ã€‚è¯¥æ¶æ„å¯ä»¥åº”ç”¨äºå„ç§é¢†åŸŸï¼Œä¾‹å¦‚è§’è‰²æ‰®æ¼”ã€ç¤¾äº¤åŸå‹è®¾è®¡ã€è™šæ‹Ÿä¸–ç•Œå’Œæ¸¸æˆç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†ç”Ÿæˆå¼æ™ºèƒ½ä½“åœ¨äº¤äº’å¼ç³»ç»Ÿä¸­çš„åº”ç”¨æœºä¼šã€ä¼¦ç†å’Œç¤¾ä¼šé£é™©ã€‚

## can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions
### Abstract
Large language models (LLMs) such as ChatGPT and GPT-4 have recently
demonstrated their remarkable abilities of communicating with human users. In
this technical report, we take an initiative to investigate their capacities of
playing text games, in which a player has to understand the environment and
respond to situations by having dialogues with the game world. Our experiments
show that ChatGPT performs competitively compared to all the existing systems
but still exhibits a low level of intelligence. Precisely, ChatGPT can not
construct the world model by playing the game or even reading the game manual;
it may fail to leverage the world knowledge that it already has; it cannot
infer the goal of each step as the game progresses. Our results open up new
research questions at the intersection of artificial intelligence, machine
learning, and natural language processing.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼šç°çŠ¶ä¸å¼€æ”¾æ€§é—®é¢˜

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå’ŒGPT-4åœ¨ç†è§£å’Œå“åº”äººç±»è¯­è¨€æŸ¥è¯¢æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç ”ç©¶ç•Œå¯¹å…¶æ˜¯å¦èƒ½å¤Ÿå®ç°é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„æ½œåŠ›äº§ç”Ÿäº†å¹¿æ³›è®¨è®ºã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å°†LLMsç½®äºæ–‡æœ¬æ¸¸æˆçš„ç¯å¢ƒä¸­ï¼Œè¯„ä¼°å…¶åœ¨ç†è§£ç¯å¢ƒã€åšå‡ºå†³ç­–å’Œä¸æ¸¸æˆä¸–ç•Œè¿›è¡Œäº¤äº’æ–¹é¢çš„æ™ºèƒ½æ°´å¹³ï¼Œä»è€Œä¸ºLLMsçš„èƒ½åŠ›å’Œå±€é™æ€§æä¾›æ–°çš„è§è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨æ–‡æœ¬æ¸¸æˆä½œä¸ºè¯„ä¼°LLMsæ™ºèƒ½æ°´å¹³çš„æµ‹è¯•åºŠã€‚æ–‡æœ¬æ¸¸æˆè¦æ±‚ç©å®¶é€šè¿‡æ–‡æœ¬å‘½ä»¤ä¸æ¸¸æˆä¸–ç•Œè¿›è¡Œäº¤äº’ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªå¯æ§çš„ç¯å¢ƒæ¥è¯„ä¼°LLMsçš„æ™ºèƒ½æ°´å¹³ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€šè¿‡åˆ†æLLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†LLMsåœ¨æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œå¯¼èˆªèƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œè¿›è¡Œæœ‰æ•ˆå¯¼èˆªçš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒChatGPTåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ChatGPTæ— æ³•é€šè¿‡æ¸¸æˆæˆ–é˜…è¯»æ¸¸æˆæ‰‹å†Œæ¥æ„å»ºä¸–ç•Œæ¨¡å‹ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å·²æœ‰çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä¹Ÿæ— æ³•æ¨æ–­æ¸¸æˆè¿›è¡Œè¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ç›®æ ‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½æ–¹é¢ä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ–‡æœ¬æ¸¸æˆå¯ä»¥ä½œä¸ºè¯„ä¼°LLMsæ™ºèƒ½æ°´å¹³çš„æœ‰æ•ˆæµ‹è¯•åºŠã€‚é€šè¿‡åˆ†æLLMsåœ¨æ–‡æœ¬æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œå¯ä»¥æ­ç¤ºLLMsåœ¨æ„å»ºä¸–ç•Œæ¨¡å‹ã€æ¨æ–­ç›®æ ‡å’Œå¯¼èˆªèƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºLLMsçš„æœªæ¥å‘å±•æä¾›æ–°çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿä¸ºLLMsåœ¨æ¸¸æˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œä¾‹å¦‚å¼€å‘åŸºäºLLMsçš„æ™ºèƒ½æ¸¸æˆåŠ©æ‰‹æˆ–æ¸¸æˆè§’è‰²ã€‚

## adapter-based-approaches-to-knowledge-enhanced-language-models----a-survey
### Abstract
Knowledge-enhanced language models (KELMs) have emerged as promising tools to
bridge the gap between large-scale language models and domain-specific
knowledge. KELMs can achieve higher factual accuracy and mitigate
hallucinations by leveraging knowledge graphs (KGs). They are frequently
combined with adapter modules to reduce the computational load and risk of
catastrophic forgetting. In this paper, we conduct a systematic literature
review (SLR) on adapter-based approaches to KELMs. We provide a structured
overview of existing methodologies in the field through quantitative and
qualitative analysis and explore the strengths and potential shortcomings of
individual approaches. We show that general knowledge and domain-specific
approaches have been frequently explored along with various adapter
architectures and downstream tasks. We particularly focused on the popular
biomedical domain, where we provided an insightful performance comparison of
existing KELMs. We outline the main trends and propose promising future
directions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºé€‚é…å™¨çš„çŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹æ–¹æ³•ç»¼è¿°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œå°½ç®¡å®ƒä»¬åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡å’Œç”Ÿæˆæ–°æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹å¯¹ç»“æ„åŒ–çŸ¥è¯†å±‚æ¬¡ï¼ˆå¦‚æ¦‚å¿µä¹‹é—´çš„å…³ç³»å’Œæ¨ç†èƒ½åŠ›ï¼‰çš„è®¤è¯†ã€‚è¿™å¯èƒ½å¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é¢„æµ‹ä¸å‡†ç¡®ï¼Œä»¥åŠåœ¨æ–‡æœ¬ç”Ÿæˆä¸­äº§ç”Ÿæ‰€è°“çš„â€œå¹»è§‰â€ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ä¿å¥æˆ–æ³•å¾‹ç­‰é«˜é£é™©é¢†åŸŸã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡ç»¼è¿°äº†åŸºäºé€‚é…å™¨çš„çŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆKELMsï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰æ¥æé«˜LLMsçš„å¯é æ€§ã€‚é€‚é…å™¨æ¨¡å—è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆä¸”è®¡ç®—æ•ˆç‡é«˜çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å¢å¼ºLLMsçš„ä»»åŠ¡æ€§èƒ½ï¼ŒåŒæ—¶é¿å…ç¾éš¾æ€§é—å¿˜å’Œä»»åŠ¡é—´çš„å¹²æ‰°ã€‚

#### é€‚é…å™¨ç±»å‹
- **Houlsby Adapter**: åœ¨æ¯ä¸ªTransformerå±‚ä¸­æ·»åŠ ä¸¤ä¸ªé€‚é…å™¨æ¨¡å—ï¼Œåˆ†åˆ«ä½äºå¤šå¤´æ³¨æ„åŠ›å±‚å’Œä¸¤ä¸ªå‰é¦ˆå±‚ä¹‹åã€‚
- **Bapna and Firat Adapter**: ä»…åœ¨æ¯ä¸ªTransformerå±‚ä¸­çš„å¤šå¤´æ³¨æ„åŠ›å±‚ä¹‹åæ·»åŠ ä¸€ä¸ªé€‚é…å™¨æ¨¡å—ã€‚
- **Pfeiffer Adapter and AdapterFusion**: ä½¿ç”¨AdapterFusionç®—æ³•æ¥ç»„åˆä¸åŒä»»åŠ¡ä¸Šè®­ç»ƒçš„é€‚é…å™¨ï¼Œä»è€Œå®ç°ä¿¡æ¯å…±äº«ã€‚
- **K-Adapter**: ä½œä¸ºâ€œå¤–éƒ¨æ’ä»¶â€ï¼Œç”±å¤šä¸ªé€‚é…å™¨å±‚ç»„æˆï¼Œå¯ä»¥æ’å…¥åˆ°é¢„è®­ç»ƒæ¨¡å‹çš„ä¸åŒTransformerå±‚ä¸­ã€‚

#### çŸ¥è¯†å¢å¼ºæ–¹æ³•
- **é€šç”¨çŸ¥è¯†**: åˆ©ç”¨ConceptNetå’ŒDBpediaç­‰çŸ¥è¯†å›¾è°±ï¼Œå°†å¸¸è¯†å’Œä¸–ç•ŒçŸ¥è¯†æ³¨å…¥LLMsã€‚
- **è¯­è¨€çŸ¥è¯†**: å°†è¯­è¨€çŸ¥è¯†ï¼ˆå¦‚åŠ¨è¯æ„ä¹‰å’Œè®ºå…ƒç»“æ„ï¼‰æ³¨å…¥é€‚é…å™¨ï¼Œä»¥æé«˜äº‹ä»¶æå–å’Œæœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚
- **é¢†åŸŸç‰¹å®šçŸ¥è¯†**: åˆ©ç”¨é€‚é…å™¨è¿›è¡Œé¢†åŸŸé€‚åº”ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œé€šè¿‡UMLSç­‰çŸ¥è¯†å›¾è°±æ¥å¢å¼ºLLMsã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åŸºäºé€‚é…å™¨çš„KELMsåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½å§‹ç»ˆä¼˜äºåŸºç¡€LLMsã€‚ä¾‹å¦‚ï¼ŒDAKIå’ŒMoPæ¡†æ¶åœ¨PubMedQAä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†çº¦7%å’Œ8%ã€‚æ­¤å¤–ï¼Œé€‚é…å™¨è°ƒä¼˜æ¯”å¸¸è§„å¾®è°ƒæ›´èƒ½ç¼“è§£é—å¿˜é—®é¢˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
- **é€‚é…å™¨æ¶æ„**: ç ”ç©¶äººå‘˜å¯ä»¥æ¢ç´¢æ›´é«˜æ•ˆçš„é€‚é…å™¨æ¶æ„ï¼Œä»¥å…‹æœåºåˆ—æ•°æ®å¤„ç†çš„å»¶è¿Ÿå¹¶å®ç°ç¡¬ä»¶å¹¶è¡ŒåŒ–ã€‚
- **é¢†åŸŸåº”ç”¨**: é™¤äº†åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œé€‚é…å™¨å¢å¼ºçš„LLMsè¿˜å¯ä»¥åº”ç”¨äºå…¶ä»–é«˜åº¦ç»“æ„åŒ–çš„é¢†åŸŸï¼Œå¦‚æ³•å¾‹æˆ–é‡‘èé¢†åŸŸã€‚
- **ä»»åŠ¡ç±»å‹**: æœªæ¥å¯ä»¥æ¢ç´¢å°†çŸ¥è¯†å¢å¼ºåº”ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œä»¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„äº‹å®æ€§å’Œä¿¡æ¯æ€§ã€‚

### ğŸ“š æ€»ç»“
æœ¬æ–‡ç»¼è¿°äº†åŸºäºé€‚é…å™¨çš„çŸ¥è¯†å¢å¼ºè¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œå¹¶åˆ†æäº†ä¸åŒé€‚é…å™¨æ¶æ„å’ŒçŸ¥è¯†å¢å¼ºæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚éšç€ç ”ç©¶çš„ä¸æ–­æ·±å…¥ï¼Œé€‚é…å™¨å¢å¼ºçš„LLMsæœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå’Œä»»åŠ¡ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## camel--communicative-agents-for--mind--exploration-of-large-language-model-society
### Abstract
The rapid advancement of chat-based language models has led to remarkable
progress in complex task-solving. However, their success heavily relies on
human input to guide the conversation, which can be challenging and
time-consuming. This paper explores the potential of building scalable
techniques to facilitate autonomous cooperation among communicative agents, and
provides insight into their "cognitive" processes. To address the challenges of
achieving autonomous cooperation, we propose a novel communicative agent
framework named role-playing. Our approach involves using inception prompting
to guide chat agents toward task completion while maintaining consistency with
human intentions. We showcase how role-playing can be used to generate
conversational data for studying the behaviors and capabilities of a society of
agents, providing a valuable resource for investigating conversational language
models. In particular, we conduct comprehensive studies on
instruction-following cooperation in multi-agent settings. Our contributions
include introducing a novel communicative agent framework, offering a scalable
approach for studying the cooperative behaviors and capabilities of multi-agent
systems, and open-sourcing our library to support research on communicative
agents and beyond: https://github.com/camel-ai/camel.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CAMELï¼šæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ç¤¾ä¼šçš„â€œå¿ƒæ™ºâ€äº¤æµ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€åŸºäºèŠå¤©çš„è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡è§£å†³æ–¹é¢çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒä»¬åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„æˆåŠŸä¸¥é‡ä¾èµ–äºäººç±»è¾“å…¥æ¥å¼•å¯¼å¯¹è¯ï¼Œè¿™å¯èƒ½ä¼šå…·æœ‰æŒ‘æˆ˜æ€§ä¸”è€—æ—¶ã€‚æœ¬æ–‡æ¢è®¨äº†æ„å»ºå¯æ‰©å±•æŠ€æœ¯ä»¥ä¿ƒè¿›äº¤æµä»£ç†ä¹‹é—´çš„è‡ªä¸»åˆä½œï¼Œå¹¶æ·±å…¥äº†è§£å…¶â€œè®¤çŸ¥â€è¿‡ç¨‹çš„æ½œåŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§’è‰²æ‰®æ¼”æ¡†æ¶
ä¸ºäº†è§£å†³å®ç°è‡ªä¸»åˆä½œçš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè§’è‰²æ‰®æ¼”â€çš„æ–°å‹äº¤æµä»£ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¶‰åŠä½¿ç”¨â€œèµ·å§‹æç¤ºâ€æ¥å¼•å¯¼èŠå¤©ä»£ç†å®Œæˆä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒä¸äººç±»æ„å›¾çš„ä¸€è‡´æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šèµ·å§‹æç¤º
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œèµ·å§‹æç¤ºâ€çš„å¯¹è¯LLMè‡ªåŠ¨æç¤ºæ–¹æ³•ï¼Œä½¿ä»£ç†èƒ½å¤Ÿé€šè¿‡è§’è‰²æ‰®æ¼”ç›¸äº’æç¤ºä»¥è§£å†³é—®é¢˜ã€‚AIç”¨æˆ·ä¸æ–­å‘AIåŠ©æ‰‹æä¾›æŒ‡ä»¤ä»¥è§£å†³é—®é¢˜ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿å­˜æŒ‡ä»¤-è§£å†³æ–¹æ¡ˆå¯¹å¹¶åˆ›å»ºå¤šæ ·åŒ–ã€æŒ‡ä»¤æ€§ã€å¯¹è¯æ€§å’Œé¢å‘ä»»åŠ¡çš„è¯­æ–™åº“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é›†ç”Ÿæˆ
æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è§’è‰²æ‰®æ¼”æ¥è®©èŠå¤©ä»£ç†ç›¸äº’äº¤æµä»¥å®Œæˆä»»åŠ¡ï¼Œå¹¶è®°å½•ä»–ä»¬çš„å¯¹è¯ä»¥è¿›è¡Œè¡Œä¸ºåˆ†æå’Œèƒ½åŠ›ç†è§£ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹å¤šä»£ç†è®¾ç½®ä¸­çš„æŒ‡ä»¤éµå¾ªåˆä½œè¿›è¡Œäº†å…¨é¢ç ”ç©¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šå¼€æºåº“
æœ¬æ–‡å¼€æºäº†æˆ‘ä»¬çš„åº“ï¼Œå…¶ä¸­åŒ…å«å„ç§ä»£ç†çš„å®ç°ã€æ•°æ®ç”Ÿæˆç®¡é“ã€æ•°æ®åˆ†æå·¥å…·å’Œæ”¶é›†çš„æ•°æ®é›†ï¼Œä»¥æ”¯æŒå¯¹äº¤æµä»£ç†çš„ç ”ç©¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡å®éªŒè¯„ä¼°äº†CAMELæ¡†æ¶çš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜CAMELè§£å†³æ–¹æ¡ˆåœ¨äººç±»è¯„ä¼°å’ŒGPT4è¯„ä¼°ä¸­å‡ä¼˜äºgpt-3.5-turboå•æ¬¡è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ç ”ç©¶äº†LLMè®­ç»ƒèƒ½åŠ›çš„æ˜¾è‘—å‡ºç°ï¼Œé€šè¿‡åœ¨é€šè¿‡æ¡†æ¶ç”Ÿæˆçš„ä¸æ–­å¢é•¿çš„è¯­æ–™åº“ä¸Šå¾®è°ƒLLaMAæ¨¡å‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„CAMELæ¡†æ¶ä¸ºç ”ç©¶äº¤æµä»£ç†ä¹‹é—´çš„è‡ªä¸»åˆä½œæä¾›äº†å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¹¶æä¾›äº†åº”å¯¹æŒ‘æˆ˜çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼€æºçš„åº“ä¸ºç ”ç©¶äº¤æµä»£ç†å’Œæ›´å¹¿æ³›çš„ç ”ç©¶é¢†åŸŸæä¾›äº†å®è´µçš„èµ„æºã€‚

## league--guided-skill-learning-and-abstraction-for-long-horizon-manipulation
### Abstract
To assist with everyday human activities, robots must solve complex
long-horizon tasks and generalize to new settings. Recent deep reinforcement
learning (RL) methods show promise in fully autonomous learning, but they
struggle to reach long-term goals in large environments. On the other hand,
Task and Motion Planning (TAMP) approaches excel at solving and generalizing
across long-horizon tasks, thanks to their powerful state and action
abstractions. But they assume predefined skill sets, which limits their
real-world applications. In this work, we combine the benefits of these two
paradigms and propose an integrated task planning and skill learning framework
named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the
symbolic interface of a task planner to guide RL-based skill learning and
creates abstract state space to enable skill reuse. More importantly, LEAGUE
learns manipulation skills in-situ of the task planning system, continuously
growing its capability and the set of tasks that it can solve. We evaluate
LEAGUE on four challenging simulated task domains and show that LEAGUE
outperforms baselines by large margins. We also show that the learned skills
can be reused to accelerate learning in new tasks domains and transfer to a
physical robot platform.
### ğŸŒŸ è®ºæ–‡è§£è¯» | LEAGUEï¼šåŸºäºå¼•å¯¼çš„æŠ€èƒ½å­¦ä¹ å’ŒæŠ½è±¡ï¼ŒåŠ©åŠ›æœºå™¨äººè§£å†³é•¿æœŸæ“ä½œä»»åŠ¡

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæœºå™¨äººå·²ç»é€æ¸èµ°è¿›æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œå¹¶åœ¨å„ç§åœºæ™¯ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œè¦è®©æœºå™¨äººçœŸæ­£å®ç°è‡ªä¸»å­¦ä¹ å’Œæ“ä½œï¼Œä»ç„¶é¢ä¸´ç€è®¸å¤šæŒ‘æˆ˜ã€‚å…¶ä¸­ï¼Œé•¿æœŸæ“ä½œä»»åŠ¡ï¼ˆlong-horizon tasksï¼‰çš„è§£å†³å’Œæ³›åŒ–èƒ½åŠ›æ˜¯æœºå™¨äººé¢†åŸŸçš„ä¸€å¤§éš¾é¢˜ã€‚ç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ–¹æ³•åœ¨è‡ªä¸»å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤§å‹ç¯å¢ƒä¸­å®ç°é•¿æœŸç›®æ ‡ä»ç„¶å­˜åœ¨å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ–¹æ³•æ“…é•¿è§£å†³å’Œæ³›åŒ–é•¿æœŸä»»åŠ¡ï¼Œä½†ç”±äºå…¶ä¾èµ–äºé¢„å®šä¹‰çš„æŠ€èƒ½é›†ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†å…‹æœä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†LEAGUEï¼ˆLearning and Abstraction with Guidanceï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†DRLå’ŒTAMPçš„ä¼˜åŠ¿ï¼Œå®ç°äº†é•¿æœŸæ“ä½œä»»åŠ¡çš„è§£å†³å’Œæ³›åŒ–ã€‚LEAGUEçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ä»»åŠ¡è§„åˆ’å™¨çš„ç¬¦å·æ¥å£æŒ‡å¯¼åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ€èƒ½å­¦ä¹ ï¼Œå¹¶åˆ›å»ºæŠ½è±¡çŠ¶æ€ç©ºé—´ä»¥å®ç°æŠ€èƒ½å¤ç”¨ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåœ¨ä»»åŠ¡è§„åˆ’ç³»ç»Ÿä¸­å­¦ä¹ æ“ä½œæŠ€èƒ½ï¼Œä¸æ–­æ‰©å±•å…¶èƒ½åŠ›å’Œå¯è§£å†³çš„ä»»åŠ¡é›†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿä»»åŠ¡é¢†åŸŸå¯¹LEAGUEè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜LEAGUEåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†å­¦ä¹ åˆ°çš„æŠ€èƒ½å¯ä»¥å¤ç”¨äºåŠ é€Ÿæ–°ä»»åŠ¡é¢†åŸŸçš„å­¦ä¹ ï¼Œå¹¶è¿ç§»åˆ°ç‰©ç†æœºå™¨äººå¹³å°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LEAGUEæ¡†æ¶ä¸ºæœºå™¨äººè§£å†³é•¿æœŸæ“ä½œä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³å¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆAIç­‰ã€‚æ­¤å¤–ï¼ŒLEAGUEæ¡†æ¶ä¸­çš„æŠ€èƒ½å­¦ä¹ å’ŒæŠ½è±¡æ–¹æ³•ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›å€Ÿé‰´ã€‚

## palm-e--an-embodied-multimodal-language-model
### Abstract
Large language models excel at a wide range of complex tasks. However,
enabling general inference in the real world, e.g., for robotics problems,
raises the challenge of grounding. We propose embodied language models to
directly incorporate real-world continuous sensor modalities into language
models and thereby establish the link between words and percepts. Input to our
embodied language model are multi-modal sentences that interleave visual,
continuous state estimation, and textual input encodings. We train these
encodings end-to-end, in conjunction with a pre-trained large language model,
for multiple embodied tasks including sequential robotic manipulation planning,
visual question answering, and captioning. Our evaluations show that PaLM-E, a
single large embodied multimodal model, can address a variety of embodied
reasoning tasks, from a variety of observation modalities, on multiple
embodiments, and further, exhibits positive transfer: the model benefits from
diverse joint training across internet-scale language, vision, and
visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,
in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains generalist language
capabilities with increasing scale.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PaLM-Eï¼šå°†çœŸå®ä¸–ç•Œä¼ æ„Ÿå™¨æ•°æ®èå…¥è¯­è¨€æ¨¡å‹ï¼Œå®ç°æ›´æ¥åœ°æ°”çš„æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„æ¨ç†ä¸­ï¼Œä¾‹å¦‚æœºå™¨äººé—®é¢˜ï¼Œå­˜åœ¨ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¥åœ°ï¼ˆgroundingï¼‰ã€‚è™½ç„¶LLMsåœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œå¯èƒ½äº§ç”Ÿä¸ç‰©ç†ä¸–ç•Œç›¸å…³çš„è¡¨ç¤ºï¼Œä½†å°†è¿™äº›è¡¨ç¤ºä¸ç°å®ä¸–ç•Œçš„è§†è§‰å’Œç‰©ç†ä¼ æ„Ÿå™¨æ¨¡æ€è¿æ¥èµ·æ¥å¯¹äºè§£å†³æ›´å¹¿æ³›çš„ç°å®ä¸–ç•Œé—®é¢˜è‡³å…³é‡è¦ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†å…·èº«è¯­è¨€æ¨¡å‹ï¼ˆembodied language modelsï¼‰ï¼Œå°†çœŸå®ä¸–ç•Œè¿ç»­ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ•°æ®ç›´æ¥èå…¥è¯­è¨€æ¨¡å‹ï¼Œä»è€Œå»ºç«‹è¯è¯­ä¸æ„ŸçŸ¥ä¹‹é—´çš„è”ç³»ã€‚å…·èº«è¯­è¨€æ¨¡å‹çš„è¾“å…¥æ˜¯å¤šæ¨¡æ€å¥å­ï¼Œè¿™äº›å¥å­äº¤æ›¿åŒ…å«è§†è§‰ã€è¿ç»­çŠ¶æ€ä¼°è®¡å’Œæ–‡æœ¬è¾“å…¥ç¼–ç ã€‚æˆ‘ä»¬ç«¯åˆ°ç«¯åœ°è®­ç»ƒè¿™äº›ç¼–ç ï¼Œä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸ç»“åˆï¼Œç”¨äºå¤šä¸ªå…·èº«ä»»åŠ¡ï¼ŒåŒ…æ‹¬é¡ºåºæœºå™¨äººæ“ä½œè§„åˆ’ã€è§†è§‰é—®ç­”å’Œå­—å¹•ç”Ÿæˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒPaLM-Eï¼Œä¸€ä¸ªå•ä¸€çš„ã€å¤§å‹å…·èº«å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¯ä»¥è§£å†³å„ç§å…·èº«æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¥è‡ªå¤šç§è§‚å¯Ÿæ¨¡æ€çš„å¤šç§å…·èº«å½¢å¼ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºç§¯æçš„è¿ç§»ï¼šæ¨¡å‹å—ç›Šäºè·¨äº’è”ç½‘è§„æ¨¡çš„è¯­è¨€ã€è§†è§‰å’Œè§†è§‰è¯­è¨€é¢†åŸŸçš„å¤šæ ·åŒ–è”åˆè®­ç»ƒã€‚æˆ‘ä»¬çš„æœ€å¤§æ¨¡å‹PaLM-E-562Bï¼Œé™¤äº†åœ¨æœºå™¨äººä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒå¤–ï¼Œè¿˜æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€é€šæ‰ï¼Œåœ¨OK-VQAä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”éšç€è§„æ¨¡çš„å¢åŠ ï¼Œä¿ç•™äº†é€šç”¨çš„è¯­è¨€èƒ½åŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PaLM-Eçš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å°†å…·èº«æ•°æ®æ··åˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œå¯ä»¥è®­ç»ƒå‡ºä¸€ä¸ªé€šç”¨çš„ã€è¿ç§»å­¦ä¹ çš„ã€å¤šå…·èº«å†³ç­–ä»£ç†ã€‚æ­¤å¤–ï¼ŒPaLM-Eè¿˜å±•ç¤ºäº†åœ¨è§†è§‰é—®ç­”å’Œå­—å¹•ç”Ÿæˆç­‰é€šç”¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„ç«äº‰åŠ›ï¼Œå¹¶ä¸”éšç€è¯­è¨€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå¤šæ¨¡æ€å¾®è°ƒæ—¶çš„ç¾éš¾æ€§é—å¿˜æ˜¾è‘—å‡å°‘ã€‚

## guiding-pretraining-in-reinforcement-learning-with-large-language-models
### Abstract
Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks. Code available at
https://github.com/yuqingd/ellm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„é¢„è®­ç»ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ç¼ºä¹å¯†é›†ã€è‰¯å¥½å½¢çŠ¶çš„å¥–åŠ±å‡½æ•°æ—¶é€šå¸¸ä¼šé‡åˆ°å›°éš¾ã€‚å†…åœ¨åŠ¨æœºæ¢ç´¢æ–¹æ³•é€šè¿‡å¥–åŠ±ä»£ç†è®¿é—®æ–°é¢–çŠ¶æ€æˆ–è½¬æ¢æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œä½†åœ¨å¤§å¤šæ•°å‘ç°çš„æ–°é¢–æ€§å¯¹ä¸‹æ¸¸ä»»åŠ¡æ— å…³ç´§è¦çš„å¤§å‹ç¯å¢ƒä¸­ï¼Œè¿™äº›æ–¹æ³•æä¾›çš„ç›Šå¤„æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ¥è‡ªæ–‡æœ¬è¯­æ–™åº“çš„èƒŒæ™¯çŸ¥è¯†æ¥å¡‘é€ æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºELLMï¼ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¢ç´¢ï¼‰ï¼Œå®ƒå¥–åŠ±ä»£ç†å®ç°ç”±è¯­è¨€æ¨¡å‹æå‡ºçš„ä¸ä»£ç†å½“å‰çŠ¶æ€æè¿°ç›¸å…³çš„ç›®æ ‡ã€‚é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼ŒELLMå¼•å¯¼ä»£ç†æœç€äººç±»æœ‰æ„ä¹‰ä¸”å¯èƒ½æœ‰ç”¨çš„è¡Œä¸ºå‘å±•ï¼Œè€Œæ— éœ€äººå·¥å¹²é¢„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒŒæ™¯çŸ¥è¯†æ¥å¡‘é€ æ¢ç´¢ã€‚LLMæ˜¯æ¦‚ç‡æ–‡æœ¬æ¨¡å‹ï¼Œå…¶é¢„æµ‹ç¼–ç äº†ä¸°å¯Œçš„å…³äºäººç±»å¸¸è¯†çŸ¥è¯†å’Œæ–‡åŒ–ä¹ ä¿—çš„ä¿¡æ¯ã€‚ELLMé€šè¿‡æŸ¥è¯¢LLMæ¥è·å–å¯èƒ½çš„ç›®æ ‡ï¼Œå¹¶å¥–åŠ±ä»£ç†å®ç°è¿™äº›å»ºè®®ï¼Œä»è€Œå¼•å¯¼æ¢ç´¢æœç€å®Œæˆå¤šæ ·åŒ–ã€ä¸Šä¸‹æ–‡æ•æ„Ÿå’Œäººç±»æœ‰æ„ä¹‰çš„ç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨LLMç”Ÿæˆçš„ç›®æ ‡ä½œä¸ºå†…åœ¨å¥–åŠ±å‡½æ•°ã€‚ELLMé€šè¿‡æµ‹é‡LLMç”Ÿæˆçš„ç›®æ ‡ä¸ç¯å¢ƒä¸­ä»£ç†è½¬æ¢çš„æè¿°ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥è®¡ç®—å¥–åŠ±ã€‚å½“è½¬æ¢çš„æè¿°ä¸ç›®æ ‡æè¿°è¶³å¤Ÿæ¥è¿‘æ—¶ï¼Œä»£ç†å°†è·å¾—ä¸ç›¸ä¼¼åº¦æˆæ¯”ä¾‹çš„å¥–åŠ±ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Crafteræ¸¸æˆç¯å¢ƒå’ŒHousekeepæœºå™¨äººæ¨¡æ‹Ÿå™¨ä¸­è¯„ä¼°äº†ELLMã€‚ç»“æœè¡¨æ˜ï¼ŒELLMè®­ç»ƒçš„ä»£ç†åœ¨é¢„è®­ç»ƒæœŸé—´å¯¹å¸¸è¯†è¡Œä¸ºçš„è¦†ç›–èŒƒå›´æ›´å¥½ï¼Œå¹¶ä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½é€šå¸¸ä¸åŸºçº¿ç›¸å½“æˆ–æœ‰æ‰€æé«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥ç”¨äºå¼•å¯¼å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ç¼ºä¹å¤–éƒ¨å®šä¹‰çš„å¥–åŠ±çš„æƒ…å†µä¸‹å­¦ä¹ æœ‰ç”¨çš„è¡Œä¸ºã€‚é€šè¿‡åˆ©ç”¨LLMçš„èƒŒæ™¯çŸ¥è¯†ï¼ŒELLMå¯ä»¥å¼•å¯¼ä»£ç†æœç€äººç±»æœ‰æ„ä¹‰ä¸”å¯èƒ½æœ‰ç”¨çš„è¡Œä¸ºå‘å±•ï¼Œä»è€Œæé«˜å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†LLMæ€§èƒ½å¯¹æç¤ºé€‰æ‹©ã€çŠ¶æ€å’Œè½¬æ¢æè¿°çš„æ•æ„Ÿæ€§ï¼Œå¹¶æå‡ºäº†æ”¹è¿›LLMæ€§èƒ½çš„æ½œåœ¨æ–¹æ³•ã€‚

## mariogpt--open-ended-text2level-generation-through-large-language-models
### Abstract
Procedural Content Generation (PCG) is a technique to generate complex and
diverse environments in an automated way. However, while generating content
with PCG methods is often straightforward, generating meaningful content that
reflects specific intentions and constraints remains challenging. Furthermore,
many PCG algorithms lack the ability to generate content in an open-ended
manner. Recently, Large Language Models (LLMs) have shown to be incredibly
effective in many diverse domains. These trained LLMs can be fine-tuned,
re-using information and accelerating training for new tasks. Here, we
introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game
levels, in our case Super Mario Bros levels. MarioGPT can not only generate
diverse levels, but can be text-prompted for controllable level generation,
addressing one of the key challenges of current PCG techniques. As far as we
know, MarioGPT is the first text-to-level model and combined with novelty
search it enables the generation of diverse levels with varying play-style
dynamics (i.e. player paths) and the open-ended discovery of an increasingly
diverse range of content. Code available at
https://github.com/shyamsn97/mario-gpt.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MarioGPTï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾æ€§æ–‡æœ¬åˆ°å…³å¡ç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ¸¸æˆè¡Œä¸šçš„å‘å±•ï¼Œç©å®¶å¯¹æ¸¸æˆå†…å®¹çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§è¦æ±‚è¶Šæ¥è¶Šé«˜ã€‚ä¼ ç»Ÿçš„å…³å¡è®¾è®¡å¾€å¾€éœ€è¦å¤§é‡äººåŠ›å’Œæ—¶é—´ï¼Œä¸”éš¾ä»¥æ»¡è¶³ä¸ªæ€§åŒ–éœ€æ±‚ã€‚å› æ­¤ï¼Œå¦‚ä½•é«˜æ•ˆã€è‡ªåŠ¨åœ°ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§å’Œå¯ç©æ€§çš„æ¸¸æˆå…³å¡æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šMarioGPTæ¨¡å‹
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMarioGPTçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºGPT-2è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç”¨äºç”Ÿæˆé©¬é‡Œå¥¥æ¸¸æˆå…³å¡ã€‚MarioGPTèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå¤šæ ·åŒ–çš„å…³å¡ï¼Œå¹¶é€šè¿‡æ–‡æœ¬æç¤ºå®ç°å¯æ§çš„å…³å¡ç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“åˆæ–°é¢–æ€§æœç´¢
ä¸ºäº†å®ç°å¼€æ”¾æ€§çš„å…³å¡ç”Ÿæˆï¼Œæœ¬æ–‡å°†MarioGPTä¸æ–°é¢–æ€§æœç´¢ç®—æ³•ç›¸ç»“åˆã€‚æ–°é¢–æ€§æœç´¢ç®—æ³•èƒ½å¤Ÿå¼•å¯¼æ¨¡å‹ä¸æ–­ç”Ÿæˆå…·æœ‰æ–°é¢–æ€§çš„å…³å¡ï¼Œä»è€Œå®ç°æ— é™çš„å†…å®¹æ¢ç´¢ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒMarioGPTåœ¨ç”Ÿæˆé©¬é‡Œå¥¥æ¸¸æˆå…³å¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒMarioGPTåœ¨éç©ºæ°”ç“·ç –é¢„æµ‹å‡†ç¡®ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼ŒMarioGPTç”Ÿæˆçš„å…³å¡ä¸­ï¼Œæœ‰88.4%æ˜¯å¯ç©çš„ï¼Œå¹¶ä¸”ç”Ÿæˆçš„è·¯å¾„ä¸å®é™…ç©å®¶çš„è·¯å¾„ç›¸ä¼¼åº¦è¾ƒé«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„MarioGPTæ¨¡å‹ä¸ºæ¸¸æˆå…³å¡ç”Ÿæˆæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–°é¢–æ€§æœç´¢ç®—æ³•ï¼Œå¯ä»¥å®ç°é«˜æ•ˆã€è‡ªåŠ¨åœ°ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§å’Œå¯ç©æ€§çš„æ¸¸æˆå…³å¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ–‡æœ¬æç¤ºæ–¹æ³•ä¹Ÿä¸ºå…³å¡ç”Ÿæˆæä¾›äº†æ›´å¤šçš„æ§åˆ¶æ€§ã€‚

### ğŸŒŸ æœªæ¥å±•æœ›
æœªæ¥ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š
1. æ‰©å±•MarioGPTæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´å¤æ‚ã€æ›´è¯¦ç»†çš„å…³å¡ã€‚
2. å°†äººç±»åé¦ˆå¼•å…¥å…³å¡ç”Ÿæˆè¿‡ç¨‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ä½¿ç”Ÿæˆçš„å…³å¡æ›´ç¬¦åˆç©å®¶çš„éœ€æ±‚ã€‚
3. æ¢ç´¢æ›´æœ‰æ•ˆçš„æœç´¢æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æé«˜å…³å¡ç”Ÿæˆçš„å¤šæ ·æ€§å’Œæ–°é¢–æ€§ã€‚

## describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents
### Abstract
We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
"$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’å¼è§„åˆ’ï¼ŒåŠ©åŠ›å¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¤šä»»åŠ¡æ™ºèƒ½ä½“é¢ä¸´ç€ä¸¤å¤§æŒ‘æˆ˜ï¼š1ï¼‰æ‰§è¡Œè®¡åˆ’éœ€è¦ç²¾ç¡®çš„å¤šæ­¥æ¨ç†ï¼Œå› ä¸ºä»»åŠ¡å…·æœ‰é•¿æœŸæ€§ï¼›2ï¼‰ä¼ ç»Ÿçš„è§„åˆ’å™¨åœ¨æ’åºå¤æ‚çš„è®¡åˆ’ä¸­çš„å¹¶è¡Œå­ç›®æ ‡æ—¶ï¼Œæ²¡æœ‰è€ƒè™‘å½“å‰æ™ºèƒ½ä½“å®Œæˆç»™å®šå­ä»»åŠ¡çš„éš¾æ˜“ç¨‹åº¦ï¼Œå¯¼è‡´ç”Ÿæˆçš„è®¡åˆ’å¯èƒ½æ•ˆç‡ä½ä¸‹ç”šè‡³ä¸å¯è¡Œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†â€œæè¿°ã€è§£é‡Šã€è§„åˆ’å’Œé€‰æ‹©â€ï¼ˆDEPSï¼‰çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæè¿°ã€è§£é‡Šå’Œè§„åˆ’
DEPS é€šè¿‡é›†æˆè®¡åˆ’æ‰§è¡Œè¿‡ç¨‹çš„æè¿°å’Œæä¾›è‡ªæˆ‘è§£é‡Šçš„åé¦ˆï¼Œæ›´å¥½åœ°çº æ­£åˆå§‹ LLM ç”Ÿæˆçš„è®¡åˆ’ä¸­çš„é”™è¯¯ã€‚å½“é‡åˆ°å¤±è´¥æ—¶ï¼Œæè¿°å™¨ä¼šæ€»ç»“å½“å‰æƒ…å†µå¹¶å‘é€ç»™ LLMï¼ŒLLM ä½œä¸ºè§£é‡Šå™¨å®šä½é”™è¯¯ï¼Œç„¶åæ ¹æ®æè¿°å™¨å’Œè§£é‡Šå™¨çš„ä¿¡æ¯æ›´æ–°è®¡åˆ’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç›®æ ‡é€‰æ‹©å™¨
DEPS åŒ…å«ä¸€ä¸ªå¯è®­ç»ƒçš„ç›®æ ‡é€‰æ‹©å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®å®Œæˆæ¯ä¸ªå¹¶è¡Œå€™é€‰å­ç›®æ ‡çš„ä¼°è®¡æ­¥éª¤å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œä»è€Œç»†åŒ–åˆå§‹è®¡åˆ’ã€‚é€‰æ‹©å™¨ä½¿ç”¨é¢„æµ‹å‰©ä½™æ—¶é—´æ­¥æ•°æ¥å®Œæˆæ¯ä¸ªç›®æ ‡ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©æœ€æ¥è¿‘çš„ç›®æ ‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒDEPS åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ Minecraftï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œèƒ½å¤Ÿç¨³å¥åœ°å®Œæˆ 70 å¤šä¸ªä»»åŠ¡ï¼Œå¹¶ä¸”æ•´ä½“æ€§èƒ½å‡ ä¹ç¿»å€ã€‚æ­¤å¤–ï¼ŒDEPS åœ¨éå¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼ˆå¦‚ ALFWorld å’Œæ¡Œé¢æ“ä½œï¼‰ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DEPS çš„äº¤äº’å¼è§„åˆ’æ–¹æ³•ä¸ºå¼€æ”¾ä¸–ç•Œå¤šä»»åŠ¡æ™ºèƒ½ä½“çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡é›†æˆæè¿°ã€è§£é‡Šå’Œè§„åˆ’ï¼Œä»¥åŠä½¿ç”¨ç›®æ ‡é€‰æ‹©å™¨ï¼ŒDEPS èƒ½å¤Ÿç”Ÿæˆæ›´å¯é å’Œé«˜æ•ˆçš„è®¡åˆ’ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€‚

## do-embodied-agents-dream-of-pixelated-sheep--embodied-decision-making-using-language-guided-world-modelling
### Abstract
Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world. However, if initialized with knowledge of high-level
subgoals and transitions between subgoals, RL agents could utilize this
Abstract World Model (AWM) for planning and exploration. We propose using
few-shot large language models (LLMs) to hypothesize an AWM, that will be
verified through world experience, to improve sample efficiency of RL agents.
Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft
in two phases: (1) the Dream phase where the agent uses an LLM to decompose a
task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase
where the agent learns a modular policy for each subgoal and verifies or
corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and
then verifying the AWM based on agent experience not only increases sample
efficiency over contemporary methods by an order of magnitude but is also
robust to and corrects errors in the LLM, successfully blending noisy
internet-scale information from LLMs with knowledge grounded in environment
dynamics.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºè¯­è¨€å¼•å¯¼çš„ä¸–ç•Œå»ºæ¨¡çš„å…·èº«å†³ç­–ï¼šè®©å¼ºåŒ–å­¦ä¹ æ›´é«˜æ•ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šå¸¸ä»é›¶å¼€å§‹å­¦ä¹ ï¼Œæ²¡æœ‰å…³äºä¸–ç•Œçš„å…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¦‚æœ RL ä»£ç†åœ¨åˆå§‹åŒ–æ—¶å…·æœ‰å…³äºé«˜çº§å­ç›®æ ‡å’Œå­ç›®æ ‡ä¹‹é—´è½¬æ¢çš„çŸ¥è¯†ï¼Œå®ƒä»¬å¯ä»¥åˆ©ç”¨è¿™ç§æŠ½è±¡ä¸–ç•Œæ¨¡å‹ï¼ˆAWMï¼‰è¿›è¡Œè§„åˆ’å’Œæ¢ç´¢ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å°‘é‡æ ·æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å‡è®¾ AWMï¼Œå¹¶é€šè¿‡ä¸–ç•Œç»éªŒè¿›è¡ŒéªŒè¯ï¼Œä»¥æé«˜ RL ä»£ç†çš„æ ·æœ¬æ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨å°‘é‡æ ·æœ¬çš„ LLM æ¥å‡è®¾ AWM
æœ¬æ–‡æå‡ºä½¿ç”¨å°‘é‡æ ·æœ¬çš„ LLM æ¥å‡è®¾ AWMï¼Œå¹¶é€šè¿‡ä¸–ç•Œç»éªŒè¿›è¡ŒéªŒè¯ï¼Œä»¥æé«˜ RL ä»£ç†çš„æ ·æœ¬æ•ˆç‡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ LLM ä¸­çš„å¤§è§„æ¨¡ã€å˜ˆæ‚çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…¶ä¸åŸºäºç¯å¢ƒåŠ¨æ€çš„çŸ¥è¯†ç›¸ç»“åˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°† LLM å¼•å¯¼çš„æ¢ç´¢åº”ç”¨äº Minecraft ä¸­çš„ç‰©å“åˆ¶ä½œ
æœ¬æ–‡æå‡ºçš„ DECKARD ä»£ç†å°† LLM å¼•å¯¼çš„æ¢ç´¢åº”ç”¨äº Minecraft ä¸­çš„ç‰©å“åˆ¶ä½œã€‚DECKARD ä»£ç†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ¢¦æƒ³é˜¶æ®µå’Œæ¸…é†’é˜¶æ®µã€‚åœ¨æ¢¦æƒ³é˜¶æ®µï¼Œä»£ç†ä½¿ç”¨ LLM å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—å­ç›®æ ‡ï¼Œå½¢æˆå‡è®¾çš„ AWMã€‚åœ¨æ¸…é†’é˜¶æ®µï¼Œä»£ç†å­¦ä¹ æ¯ä¸ªå­ç›®æ ‡çš„æ¨¡å—åŒ–ç­–ç•¥ï¼Œå¹¶éªŒè¯æˆ–çº æ­£å‡è®¾çš„ AWMã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ²¡æœ‰ LLM å¼•å¯¼çš„ä»£ç†ç›¸æ¯”ï¼ŒDECKARD ä»£ç†åœ¨å¼€æ”¾æ¢ç´¢å’Œç›®æ ‡é©±åŠ¨ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºæ›´é«˜çš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨å¼€æ”¾æ¢ç´¢ä¸­ï¼ŒDECKARD ä»£ç†èƒ½å¤Ÿæ›´å¿«åœ°å‘ç°æ–°çš„ AWM èŠ‚ç‚¹ã€‚åœ¨ç›®æ ‡é©±åŠ¨ä»»åŠ¡ä¸­ï¼ŒDECKARD ä»£ç†èƒ½å¤Ÿæ›´å¿«åœ°å­¦ä¹ åˆ¶ä½œç‰©å“ï¼Œå¹¶ä¸”å¯¹ LLM è¾“å‡ºä¸­çš„é”™è¯¯å…·æœ‰é²æ£’æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºè¯­è¨€å¼•å¯¼çš„ä¸–ç•Œå»ºæ¨¡çš„å…·èº«å†³ç­–æ–¹æ³•ä¸º RL ä»£ç†åˆ©ç”¨å¤§è§„æ¨¡ã€å˜ˆæ‚çš„å…ˆéªŒçŸ¥è¯†æä¾›äº†æ–°çš„æ€è·¯ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºå„ç§ RL ä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨äººæ§åˆ¶ã€æ¸¸æˆç©æ³•å’Œè‡ªåŠ¨é©¾é©¶ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å—åŒ– RL ç­–ç•¥ä¹Ÿå¯ä»¥ç”¨äºæé«˜ RL ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ã€‚

## open-world-multi-task-control-through-goal-aware-representation-learning-and-adaptive-horizon-prediction
### Abstract
We study the problem of learning goal-conditioned policies in Minecraft, a
popular, widely accessible yet challenging open-ended environment for
developing human-level multi-task agents. We first identify two main challenges
of learning such policies: 1) the indistinguishability of tasks from the state
distribution, due to the vast scene diversity, and 2) the non-stationary nature
of environment dynamics caused by partial observability. To tackle the first
challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage
the emergence of goal-relevant visual state representations. To tackle the
second challenge, the policy is further fueled by an adaptive horizon
prediction module that helps alleviate the learning uncertainty brought by the
non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method
significantly outperforms the best baseline so far; in many of them, we double
the performance. Our ablation and exploratory studies then explain how our
approach beat the counterparts and also unveil the surprising bonus of
zero-shot generalization to new scenes (biomes). We hope our agent could help
shed some light on learning goal-conditioned, multi-task agents in challenging,
open-ended environments like Minecraft.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åœ¨å¼€æ”¾ä¸–ç•Œä¸­å®ç°å¤šä»»åŠ¡æ§åˆ¶ï¼šç›®æ ‡æ„ŸçŸ¥è¡¨ç¤ºå­¦ä¹ å’Œè‡ªé€‚åº”é¢„æµ‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼€æ”¾ä¸–ç•Œç¯å¢ƒï¼Œå¦‚Minecraftï¼Œä¸ºå¼€å‘èƒ½å¤Ÿæ‰§è¡Œå„ç§ä»»åŠ¡çš„æ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å¹³å°ã€‚ç„¶è€Œï¼Œè¿™äº›ç¯å¢ƒä¹Ÿå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ï¼š
1. **çŠ¶æ€åˆ†å¸ƒçš„å¤šæ ·æ€§**ï¼šç”±äºåœºæ™¯çš„å¤šæ ·æ€§ï¼Œä¸åŒä»»åŠ¡çš„çŠ¶æ€éš¾ä»¥åŒºåˆ†ï¼Œè¿™ä½¿å¾—å­¦ä¹ ç›®æ ‡æ¡ä»¶ç­–ç•¥å˜å¾—å›°éš¾ã€‚
2. **ç¯å¢ƒåŠ¨æ€çš„éå¹³ç¨³æ€§**ï¼šç”±äºéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œç¯å¢ƒåŠ¨æ€å…·æœ‰éå¹³ç¨³æ€§ï¼Œå¯¼è‡´å­¦ä¹ çš„ä¸ç¡®å®šæ€§å¢åŠ ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä»¥ä¸‹åˆ›æ–°æ–¹æ³•ï¼š
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼š**ç›®æ ‡æ„ŸçŸ¥éª¨å¹²ç½‘ç»œï¼ˆGSB**ï¼‰
   - GSBé€šè¿‡åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šèåˆç›®æ ‡ä¿¡æ¯ï¼Œé¼“åŠ±å‡ºç°ä¸ç›®æ ‡ç›¸å…³çš„è§†è§‰çŠ¶æ€è¡¨ç¤ºï¼Œä»è€Œè§£å†³çŠ¶æ€åˆ†å¸ƒå¤šæ ·æ€§çš„é—®é¢˜ã€‚
   - GSBç”±å¤šä¸ªç›®æ ‡å·ç§¯å—ï¼ˆg-conv blockï¼‰ç»„æˆï¼Œè¿™äº›å—é€šè¿‡é€šé“è°ƒåˆ¶å°†ç›®æ ‡ä¿¡æ¯ä¸è§†è§‰ç‰¹å¾èåˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼š**è‡ªé€‚åº”é¢„æµ‹æ¨¡å—**
   - ä¸ºäº†åº”å¯¹ç¯å¢ƒåŠ¨æ€çš„éå¹³ç¨³æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†è‡ªé€‚åº”é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹ä»å½“å‰çŠ¶æ€åˆ°ç›®æ ‡çš„å‰©ä½™æ—¶é—´æ­¥æ•°ï¼ˆå³è·ç¦»åˆ°ç›®æ ‡çš„è·ç¦»ï¼‰ã€‚
   - è‡ªé€‚åº”é¢„æµ‹æ¨¡å—é€šè¿‡é¢„æµ‹å‰©ä½™æ—¶é—´æ­¥æ•°ï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£ç›®æ ‡çš„å®Œæˆç¨‹åº¦ï¼Œä»è€Œæé«˜å†³ç­–çš„å‡†ç¡®æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Minecraftçš„20ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåœ¨è®¸å¤šä»»åŠ¡ä¸­æ€§èƒ½ç¿»å€ã€‚æ¶ˆèç ”ç©¶å’Œæ¢ç´¢æ€§ç ”ç©¶è§£é‡Šäº†æœ¬æ–‡æ–¹æ³•å¦‚ä½•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº†ä»¤äººæƒŠè®¶çš„é›¶æ ·æœ¬æ³›åŒ–åˆ°æ–°åœºæ™¯ï¼ˆç”Ÿç‰©ç¾¤è½ï¼‰çš„é¢å¤–ä¼˜åŠ¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„GSBå’Œè‡ªé€‚åº”é¢„æµ‹æ¨¡å—ä¸ºåœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å­¦ä¹ ç›®æ ‡æ¡ä»¶ç­–ç•¥æä¾›äº†æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºå¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œå¤šä»»åŠ¡çš„æ™ºèƒ½ä½“æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## welfare-diplomacy--benchmarking-language-model-cooperation
### Abstract
The growing capabilities and increasingly widespread deployment of AI systems
necessitate robust benchmarks for measuring their cooperative capabilities.
Unfortunately, most multi-agent benchmarks are either zero-sum or purely
cooperative, providing limited opportunities for such measurements. We
introduce a general-sum variant of the zero-sum board game Diplomacy -- called
Welfare Diplomacy -- in which players must balance investing in military
conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both
a clearer assessment of and stronger training incentives for cooperative
capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules
and implementing them via an open-source Diplomacy engine; (2) constructing
baseline agents using zero-shot prompted language models; and (3) conducting
experiments where we find that baselines using state-of-the-art models attain
high social welfare but are exploitable. Our work aims to promote societal
safety by aiding researchers in developing and assessing multi-agent AI
systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is
available at https://github.com/mukobi/welfare-diplomacy.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢AIåˆä½œèƒ½åŠ›ï¼šWelfare DiplomacyåŸºå‡†æµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€AIç³»ç»Ÿèƒ½åŠ›çš„ä¸æ–­å¢å¼ºå’Œåº”ç”¨çš„æ—¥ç›Šå¹¿æ³›ï¼Œè¡¡é‡å…¶åˆä½œèƒ½åŠ›çš„éœ€æ±‚ä¹Ÿæ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•è¦ä¹ˆæ˜¯é›¶å’Œåšå¼ˆï¼Œè¦ä¹ˆæ˜¯çº¯ç²¹çš„åˆä½œåšå¼ˆï¼Œè¿™é™åˆ¶äº†å¯¹å…¶åˆä½œèƒ½åŠ›çš„è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºWelfare Diplomacyçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWelfare Diplomacyè§„åˆ™
Welfare Diplomacyæ˜¯å¯¹ç»å…¸å¤–äº¤æ¸¸æˆï¼ˆDiplomacyï¼‰çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç©å®¶éœ€è¦åœ¨å†›äº‹å¾æœå’Œå›½å†…ç¦åˆ©ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æ¸¸æˆç»“æŸåï¼Œç©å®¶çš„æ€»æ•ˆç”¨ç­‰äºå…¶ç´¯ç§¯çš„ç¦åˆ©ç‚¹æ•°ï¼Œè€Œä¸æ˜¯å é¢†çš„ä¾›åº”ä¸­å¿ƒæ•°é‡ã€‚è¿™ç§è§„åˆ™è®¾è®¡é¼“åŠ±ç©å®¶è¿›è¡Œåˆä½œï¼Œä»¥å®ç°æ›´é«˜çš„ç¤¾ä¼šç¦ç¥‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé›¶æ ·æœ¬è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•
æœ¬æ–‡ä½¿ç”¨é›¶æ ·æœ¬æç¤ºè¯­è¨€æ¨¡å‹æ„å»ºäº†Welfare Diplomacyçš„åŸºçº¿æ™ºèƒ½ä½“ï¼Œå¹¶ä½¿ç”¨GPT-4ç­‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿå®ç°é«˜ç¤¾ä¼šç¦ç¥‰ï¼Œä½†å®¹æ˜“è¢«å…¶ä»–ç©å®¶åˆ©ç”¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWelfare Diplomacyèƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°å’Œè®­ç»ƒAIç³»ç»Ÿçš„åˆä½œèƒ½åŠ›ã€‚ä¸æ ‡å‡†å¤–äº¤æ¸¸æˆç›¸æ¯”ï¼ŒWelfare Diplomacyä¸­çš„ç©å®¶å‚ä¸å†²çªçš„é¢‘ç‡æ›´ä½ï¼Œè¿™è¡¨æ˜Welfare Diplomacyèƒ½å¤Ÿæ›´å¥½åœ°ä¿ƒè¿›åˆä½œè¡Œä¸ºã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Welfare DiplomacyåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å’Œè®­ç»ƒAIç³»ç»Ÿçš„åˆä½œèƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„å¹³å°ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯ä»¥ç”¨äºå¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„AIç³»ç»Ÿï¼Œä»¥åº”å¯¹ç°å®ä¸–ç•Œä¸­çš„å¤æ‚æŒ‘æˆ˜ã€‚

### ğŸŒŸ æ€»ç»“
Welfare Diplomacyæ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œè¯„ä¼°AIç³»ç»Ÿçš„åˆä½œèƒ½åŠ›ã€‚éšç€AIæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒWelfare Diplomacyæœ‰æœ›åœ¨ä¿ƒè¿›AIåˆä½œèƒ½åŠ›æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚

## robotic-skill-acquisition-via-instruction-augmentation-with-vision-language-models
### Abstract
In recent years, much progress has been made in learning robotic manipulation
policies that follow natural language instructions. Such methods typically
learn from corpora of robot-language data that was either collected with
specific tasks in mind or expensively re-labelled by humans with rich language
descriptions in hindsight. Recently, large-scale pretrained vision-language
models (VLMs) like CLIP or ViLD have been applied to robotics for learning
representations and scene descriptors. Can these pretrained models serve as
automatic labelers for robot data, effectively importing Internet-scale
knowledge into existing datasets to make them useful even for tasks that are
not reflected in their ground truth annotations? To accomplish this, we
introduce Data-driven Instruction Augmentation for Language-conditioned control
(DIAL): we utilize semi-supervised language labels leveraging the semantic
understanding of CLIP to propagate knowledge onto large datasets of unlabelled
demonstration data and then train language-conditioned policies on the
augmented datasets. This method enables cheaper acquisition of useful language
descriptions compared to expensive human labels, allowing for more efficient
label coverage of large-scale datasets. We apply DIAL to a challenging
real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations
do not contain crowd-sourced language annotations. DIAL enables imitation
learning policies to acquire new capabilities and generalize to 60 novel
instructions unseen in the original dataset.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºæŒ‡ä»¤ï¼Œå®ç°æœºå™¨äººæŠ€èƒ½è·å–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œæœºå™¨äººæ§åˆ¶ç­–ç•¥å·²ç»èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œå­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§é‡æ ‡æ³¨è¿‡çš„æœºå™¨äººè¯­è¨€æ•°æ®ï¼Œè¿™äº›æ•°æ®è¦ä¹ˆæ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡æ”¶é›†çš„ï¼Œè¦ä¹ˆæ˜¯äº‹åç”±äººç±»è¿›è¡Œæ˜‚è´µæ ‡æ³¨çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDIALï¼ˆData-driven Instruction Augmentation for Language-conditioned controlï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è‡ªåŠ¨ä¸ºæœºå™¨äººæ•°æ®æ·»åŠ æ ‡ç­¾ï¼Œä»è€Œå°†äº’è”ç½‘è§„æ¨¡çš„çŸ¥è¯†å¼•å…¥ç°æœ‰æ•°æ®é›†ï¼Œä½¿å…¶å³ä½¿åœ¨çœŸå®æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ç”¨äºå„ç§ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„VLMè¿›è¡ŒæŒ‡ä»¤å¢å¼º
DIALæ–¹æ³•é¦–å…ˆåœ¨åŒ…å«å°‘é‡äººå·¥æ ‡æ³¨çš„è‡ªç„¶è¯­è¨€æè¿°çš„å°å‹æ•°æ®é›†ä¸Šå¾®è°ƒVLMï¼Œç„¶åä½¿ç”¨å¾®è°ƒåçš„VLMä¸ºæ›´å¤§çš„æœªæ ‡æ³¨æ¼”ç¤ºæ•°æ®é›†ç”Ÿæˆæ›¿ä»£æŒ‡ä»¤ã€‚è¿™äº›æŒ‡ä»¤å¯ä»¥åŒ…å«æ›´ä¸°å¯Œçš„è¯­ä¹‰æ¦‚å¿µï¼Œä¾‹å¦‚ç©ºé—´æ¦‚å¿µæˆ–ä¸åŒçš„è¡¨è¿°æ–¹å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºè¡Œä¸ºå…‹éš†çš„è¯­è¨€æ¡ä»¶ç­–ç•¥è®­ç»ƒ
åœ¨ç”Ÿæˆæ›¿ä»£æŒ‡ä»¤åï¼ŒDIALæ–¹æ³•ä½¿ç”¨è¡Œä¸ºå…‹éš†åœ¨åŸå§‹å’Œé‡æ–°æ ‡æ³¨çš„æ•°æ®é›†ä¸Šè®­ç»ƒè¯­è¨€æ¡ä»¶ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°è¦†ç›–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä½¿æœºå™¨äººèƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œæ–°çš„æŒ‡ä»¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨çœŸå®ä¸–ç•Œçš„æœºå™¨äººæ“ä½œé¢†åŸŸï¼ŒDIALæ–¹æ³•åœ¨80,000ä¸ªæ¼”ç¤ºä¸­ï¼Œåªæœ‰3.5%åŒ…å«ä¼—åŒ…è¯­è¨€æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¤§è§„æ¨¡ç ”ç©¶è¶…è¿‡1,300æ¬¡çœŸå®ä¸–ç•Œè¯„ä¼°ï¼Œå‘ç°DIALä½¿æ¨¡ä»¿å­¦ä¹ ç­–ç•¥èƒ½å¤Ÿè·å¾—æ–°çš„èƒ½åŠ›ï¼Œå¹¶æ¨å¹¿åˆ°60ä¸ªåŸå§‹æ•°æ®é›†ä¸­æœªè§çš„å…¨æ–°æŒ‡ä»¤ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DIALæ–¹æ³•æä¾›äº†ä¸€ç§å»‰ä»·ä¸”è‡ªåŠ¨åŒ–çš„é€‰é¡¹ï¼Œå¯ä»¥ä»ç¦»çº¿æ§åˆ¶æ•°æ®é›†ä¸­æå–é¢å¤–çš„è¯­ä¹‰çŸ¥è¯†ã€‚å®ƒé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„VLMè‡ªåŠ¨ä¸ºæœºå™¨äººæ•°æ®æ·»åŠ æ ‡ç­¾ï¼Œä»è€Œå°†äº’è”ç½‘è§„æ¨¡çš„çŸ¥è¯†å¼•å…¥ç°æœ‰æ•°æ®é›†ï¼Œä½¿å…¶å³ä½¿åœ¨çœŸå®æ ‡ç­¾æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ç”¨äºå„ç§ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°è¦†ç›–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä½¿æœºå™¨äººèƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œæ–°çš„æŒ‡ä»¤ã€‚

## emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task
### Abstract
Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create "latent saliency maps" that can help explain predictions
in human terms.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢åºåˆ—æ¨¡å‹åœ¨åˆæˆä»»åŠ¡ä¸­çš„æ¶Œç°ä¸–ç•Œè¡¨ç¤º

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯­è¨€æ¨¡å‹å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„åŸç†å°šä¸æ˜ç¡®ã€‚è¿™äº›æ¨¡å‹æ˜¯ä»…ä»…è®°å¿†äº†ä¸€ç³»åˆ—è¡¨é¢ç»Ÿè®¡ä¿¡æ¯ï¼Œè¿˜æ˜¯ä¾èµ–äºå¯¹ç”Ÿæˆåºåˆ—è¿‡ç¨‹çš„å†…éƒ¨è¡¨ç¤ºï¼Ÿæœ¬æ–‡é€šè¿‡å°†GPTæ¨¡å‹åº”ç”¨äºé¢„æµ‹ç®€å•æ£‹ç›˜æ¸¸æˆOthelloçš„åˆæ³•ç§»åŠ¨ä»»åŠ¡ï¼Œæ¢è®¨äº†è¿™ä¸ªé—®é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä½¿ç”¨Othelloä½œä¸ºæµ‹è¯•å¹³å°
æœ¬æ–‡é€‰æ‹©Othelloä½œä¸ºæµ‹è¯•å¹³å°ï¼Œå› ä¸ºå®ƒæ¯”å›½é™…è±¡æ£‹ç®€å•ï¼Œä½†æ¸¸æˆæ ‘è¶³å¤Ÿå¤§ï¼Œé¿å…äº†è®°å¿†çš„å¯èƒ½æ€§ã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªGPTå˜ä½“æ¨¡å‹ï¼ˆOthello-GPTï¼‰æ¥é¢„æµ‹Othelloçš„åˆæ³•ç§»åŠ¨ï¼Œå°½ç®¡æ¨¡å‹æ²¡æœ‰å…ˆéªŒçš„æ¸¸æˆçŸ¥è¯†ï¼Œä½†ä»ç„¶èƒ½å¤Ÿä»¥é«˜ç²¾åº¦ç”Ÿæˆåˆæ³•ç§»åŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨æ¢é’ˆæŠ€æœ¯æ¢ç´¢å†…éƒ¨è¡¨ç¤º
ä¸ºäº†ç ”ç©¶Othello-GPTæ˜¯å¦è®¡ç®—äº†æ¸¸æˆçŠ¶æ€çš„å†…éƒ¨è¡¨ç¤ºï¼Œæœ¬æ–‡ä½¿ç”¨äº†æ¢é’ˆæŠ€æœ¯ã€‚æ¢é’ˆæ˜¯ä¸€ç§åˆ†ç±»å™¨æˆ–å›å½’å™¨ï¼Œå…¶è¾“å…¥ç”±ç½‘ç»œçš„å†…éƒ¨æ¿€æ´»ç»„æˆï¼Œå¹¶è®­ç»ƒä»¥é¢„æµ‹æ„Ÿå…´è¶£çš„ç‰¹å¾ã€‚é€šè¿‡è®­ç»ƒæ¢é’ˆæ¥é¢„æµ‹ç½‘ç»œå†…éƒ¨æ¿€æ´»åçš„æ£‹ç›˜çŠ¶æ€ï¼Œå‘ç°éçº¿æ€§æ¢é’ˆèƒ½å¤Ÿä»¥é«˜ç²¾åº¦é¢„æµ‹æ£‹ç›˜çŠ¶æ€ï¼Œè¿™è¡¨æ˜æ¨¡å‹å†…éƒ¨å­˜åœ¨ä¸€ä¸ªéçº¿æ€§çš„æ£‹ç›˜çŠ¶æ€è¡¨ç¤ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šå¹²é¢„å®éªŒéªŒè¯è¡¨ç¤ºçš„å› æœä½œç”¨
ä¸ºäº†ç¡®å®šæ£‹ç›˜çŠ¶æ€ä¿¡æ¯æ˜¯å¦å½±å“æ¨¡å‹çš„é¢„æµ‹ï¼Œæœ¬æ–‡è¿›è¡Œäº†ä¸€ç³»åˆ—å¹²é¢„å®éªŒã€‚é€šè¿‡ä¿®æ”¹Othello-GPTçš„å†…éƒ¨æ¿€æ´»ï¼Œå¹¶æµ‹é‡ç”±æ­¤äº§ç”Ÿçš„æ•ˆæœï¼Œå‘ç°å¹²é¢„åçš„é¢„æµ‹ä¸é¢„æœŸçš„æ£‹ç›˜çŠ¶æ€ç›¸åŒ¹é…ï¼Œè¿™è¡¨æ˜æ¶Œç°çš„ä¸–ç•Œè¡¨ç¤ºå¯¹æ¨¡å‹çš„é¢„æµ‹å…·æœ‰å› æœä½œç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ›å»ºæ½œåœ¨æ˜¾è‘—æ€§å›¾
æœ¬æ–‡è¿˜å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¹²é¢„æŠ€æœ¯åˆ›å»ºæ½œåœ¨æ˜¾è‘—æ€§å›¾ï¼Œè¿™äº›å›¾å¯ä»¥å¸®åŠ©è§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚é€šè¿‡å¹²é¢„æ¯ä¸ªæ£‹ç›˜æ ¼çš„çŠ¶æ€ï¼Œå¹¶è§‚å¯Ÿé¢„æµ‹æ¦‚ç‡çš„å˜åŒ–ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªè¡¨ç¤ºæ¯ä¸ªæ£‹ç›˜æ ¼å¯¹å½“å‰æ£‹ç›˜çŠ¶æ€é¢„æµ‹çš„æ˜¾è‘—æ€§çš„å¯è§†åŒ–å›¾ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒOthello-GPTç¡®å®ç»´æŠ¤äº†ä¸€ä¸ªæ¸¸æˆæ£‹ç›˜çŠ¶æ€çš„è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™ä¸ªè¡¨ç¤ºæ˜¯éçº¿æ€§çš„ã€‚æ­¤å¤–ï¼Œè¿™äº›è¡¨ç¤ºä¸æ¨¡å‹çš„é¢„æµ‹å…·æœ‰å› æœè”ç³»ã€‚æ½œåœ¨æ˜¾è‘—æ€§å›¾æ­ç¤ºäº†Othello-GPTè®­ç»ƒæ•°æ®é›†çš„ä¸åŒç‰ˆæœ¬ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåºåˆ—æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¤æ‚çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™äº›è¡¨ç¤ºå¯ä»¥ç”¨äºè§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚æ­¤å¤–ï¼Œå¹²é¢„å®éªŒå’Œæ½œåœ¨æ˜¾è‘—æ€§å›¾ç­‰æŠ€æœ¯å¯ä»¥ç”¨äºæ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚è¿™äº›å‘ç°å¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯­è¨€æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶å¼€å‘æ›´å¯é çš„è§£é‡Šå·¥å…·ã€‚

## large-language-models-are-pretty-good-zero-shot-video-game-bug-detectors
### Abstract
Video game testing requires game-specific knowledge as well as common sense
reasoning about the events in the game. While AI-driven agents can satisfy the
first requirement, it is not yet possible to meet the second requirement
automatically. Therefore, video game testing often still relies on manual
testing, and human testers are required to play the game thoroughly to detect
bugs. As a result, it is challenging to fully automate game testing. In this
study, we explore the possibility of leveraging the zero-shot capabilities of
large language models for video game bug detection. By formulating the bug
detection problem as a question-answering task, we show that large language
models can identify which event is buggy in a sequence of textual descriptions
of events from a game. To this end, we introduce the GameBugDescriptions
benchmark dataset, which consists of 167 buggy gameplay videos and a total of
334 question-answer pairs across 8 games. We extensively evaluate the
performance of six models across the OPT and InstructGPT large language model
families on our benchmark dataset. Our results show promising results for
employing language models to detect video game bugs. With the proper prompting
technique, we could achieve an accuracy of 70.66%, and on some video games, up
to 78.94%. Our code, evaluation data and the benchmark can be found on
https://asgaardlab.github.io/LLMxBugs
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹ä¸­çš„æ½œåŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è§†é¢‘æ¸¸æˆæµ‹è¯•éœ€è¦æ¸¸æˆç‰¹å®šçš„çŸ¥è¯†å’Œå¯¹æ¸¸æˆäº‹ä»¶çš„å¸¸è¯†æ¨ç†ã€‚è™½ç„¶ AI é©±åŠ¨çš„ä»£ç†å¯ä»¥æ»¡è¶³ç¬¬ä¸€ä¸ªè¦æ±‚ï¼Œä½†è‡ªåŠ¨æ»¡è¶³ç¬¬äºŒä¸ªè¦æ±‚ä»ç„¶ä¸å¯èƒ½ã€‚å› æ­¤ï¼Œè§†é¢‘æ¸¸æˆæµ‹è¯•é€šå¸¸ä»ç„¶ä¾èµ–äºæ‰‹åŠ¨æµ‹è¯•ï¼Œéœ€è¦äººç±»æµ‹è¯•è€…å½»åº•åœ°ç©æ¸¸æˆæ¥æ£€æµ‹æ¼æ´ã€‚è¿™ä½¿å¾—å®Œå…¨è‡ªåŠ¨åŒ–æ¸¸æˆæµ‹è¯•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†æ¼æ´æ£€æµ‹é—®é¢˜è¡¨è¿°ä¸ºé—®ç­”ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›æ¥è¯†åˆ«æ¸¸æˆäº‹ä»¶åºåˆ—ä¸­çš„æ¼æ´äº‹ä»¶ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥ GameBugDescriptions åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å« 167 ä¸ªæœ‰æ¼æ´çš„æ¸¸æˆç©æ³•è§†é¢‘å’Œ 334 ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›– 8 ä¸ªæ¸¸æˆã€‚
ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº† OPT å’Œ InstructGPT å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—çš„å…­ä¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåˆ†æäº†è¯­è¨€æ¨¡å‹å¯¹ä¸åŒäº‹ä»¶æè¿°çš„é²æ£’æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹æ–¹é¢å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚é€šè¿‡é€‚å½“çš„æç¤ºæŠ€æœ¯ï¼Œå¯ä»¥å®ç° 70.66% çš„å‡†ç¡®ç‡ï¼Œåœ¨æŸäº›è§†é¢‘æ¸¸æˆä¸­ç”šè‡³å¯ä»¥è¾¾åˆ° 78.94%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¸¸æˆæ¼æ´æ£€æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè‡ªåŠ¨åŒ–æ¸¸æˆæµ‹è¯•æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡ä¸­æå‡ºçš„ GameBugDescriptions åŸºå‡†æ•°æ®é›†å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒä¸åŒè¯­è¨€æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## social-simulacra--creating-populated-prototypes-for-social-computing-systems
### Abstract
Social computing prototypes probe the social behaviors that may arise in an
envisioned system design. This prototyping practice is currently limited to
recruiting small groups of people. Unfortunately, many challenges do not arise
until a system is populated at a larger scale. Can a designer understand how a
social system might behave when populated, and make adjustments to the design
before the system falls prey to such challenges? We introduce social simulacra,
a prototyping technique that generates a breadth of realistic social
interactions that may emerge when a social computing system is populated.
Social simulacra take as input the designer's description of a community's
design -- goal, rules, and member personas -- and produce as output an instance
of that design with simulated behavior, including posts, replies, and
anti-social behaviors. We demonstrate that social simulacra shift the behaviors
that they generate appropriately in response to design changes, and that they
enable exploration of "what if?" scenarios where community members or
moderators intervene. To power social simulacra, we contribute techniques for
prompting a large language model to generate thousands of distinct community
members and their social interactions with each other; these techniques are
enabled by the observation that large language models' training data already
includes a wide variety of positive and negative behavior on social media
platforms. In evaluations, we show that participants are often unable to
distinguish social simulacra from actual community behavior and that social
computing designers successfully refine their social computing designs when
using social simulacra.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ç¤¾äº¤æ¨¡æ‹Ÿï¼šä¸ºç¤¾äº¤è®¡ç®—ç³»ç»Ÿåˆ›å»ºäººå£åŸå‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç¤¾äº¤è®¡ç®—ç³»ç»Ÿçš„è®¾è®¡å¾€å¾€éœ€è¦è€ƒè™‘å¤§é‡ç”¨æˆ·çš„è¡Œä¸ºå’Œäº’åŠ¨ï¼Œè€Œä¼ ç»Ÿçš„åŸå‹è®¾è®¡æ–¹æ³•é€šå¸¸åªèƒ½æ‹›å‹Ÿå°‘é‡ç”¨æˆ·è¿›è¡Œæµ‹è¯•ã€‚ç„¶è€Œï¼Œè®¸å¤šæŒ‘æˆ˜å’Œé—®é¢˜åªæœ‰åœ¨ç³»ç»Ÿå¤§è§„æ¨¡è¿è¡Œæ—¶æ‰ä¼šå‡ºç°ã€‚å› æ­¤ï¼Œè®¾è®¡å¸ˆå¾ˆéš¾é¢„æµ‹ç¤¾äº¤ç³»ç»Ÿåœ¨äººå£ä¼—å¤šæ—¶çš„è¡Œä¸ºï¼Œä¹Ÿæ— æ³•åœ¨ç³»ç»Ÿå‡ºç°é—®é¢˜ä¹‹å‰è¿›è¡Œè°ƒæ•´ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†â€œç¤¾äº¤æ¨¡æ‹Ÿâ€æŠ€æœ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤§é‡ç”¨æˆ·å’Œä»–ä»¬çš„ç¤¾äº¤äº’åŠ¨ï¼Œå¸®åŠ©è®¾è®¡å¸ˆé¢„æµ‹å’Œè¯„ä¼°ç¤¾äº¤ç³»ç»Ÿåœ¨äººå£ä¼—å¤šæ—¶çš„è¡Œä¸ºã€‚ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”Ÿæˆå¤šæ ·åŒ–çš„ç”¨æˆ·è§’è‰²
è®¾è®¡å¸ˆåªéœ€æä¾›å°‘é‡ç§å­è§’è‰²ï¼Œç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯å°±å¯ä»¥ç”Ÿæˆå¤§é‡éé‡å¤ä½†ä¸»é¢˜ç›¸å…³çš„ç”¨æˆ·è§’è‰²ï¼Œä»è€Œæ¨¡æ‹Ÿä¸åŒç”¨æˆ·ä¹‹é—´çš„äº’åŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç”Ÿæˆç¬¦åˆç¤¾åŒºç›®æ ‡å’Œè§„åˆ™çš„äº’åŠ¨å†…å®¹
ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯å¯ä»¥æ ¹æ®è®¾è®¡å¸ˆæä¾›çš„ç¤¾åŒºç›®æ ‡ã€è§„åˆ™å’Œç”¨æˆ·è§’è‰²ï¼Œç”Ÿæˆç¬¦åˆç¤¾åŒºç›®æ ‡å’Œè§„åˆ™çš„äº’åŠ¨å†…å®¹ï¼ŒåŒ…æ‹¬å¸–å­ã€å›å¤å’Œåç¤¾ä¼šè¡Œä¸ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯
ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯å¯ä»¥å¸®åŠ©è®¾è®¡å¸ˆæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯ï¼Œä¾‹å¦‚ï¼Œå¦‚æœç¤¾åŒºæˆå‘˜æˆ–ç‰ˆä¸»è¿›è¡Œå¹²é¢„ï¼Œç¤¾äº¤ç³»ç»Ÿä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡ä¸¤ä¸ªè¯„ä¼°å®éªŒéªŒè¯äº†ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼š

1. æŠ€æœ¯è¯„ä¼°ï¼šå‚ä¸è€…æ— æ³•åŒºåˆ†çœŸå®ç¤¾åŒºè¡Œä¸ºå’Œç¤¾äº¤æ¨¡æ‹Ÿç”Ÿæˆçš„è¡Œä¸ºï¼Œè¡¨æ˜ç¤¾äº¤æ¨¡æ‹Ÿå¯ä»¥åˆ›å»ºå¯ä¿¡çš„å†…å®¹ã€‚
2. è®¾è®¡å¸ˆè¯„ä¼°ï¼šç¤¾äº¤è®¡ç®—è®¾è®¡å¸ˆä½¿ç”¨ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯æˆåŠŸæ”¹è¿›äº†ä»–ä»¬çš„ç¤¾äº¤ç³»ç»Ÿè®¾è®¡ï¼Œä¾‹å¦‚ï¼Œè¯†åˆ«æœªè€ƒè™‘åˆ°çš„ç§¯æç”¨ä¾‹å’Œè´Ÿé¢è¡Œä¸ºï¼Œå¹¶è°ƒæ•´è§„åˆ™å’Œå¹²é¢„ç­–ç•¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯ä¸ºç¤¾äº¤è®¡ç®—ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†ä¸€ç§æ–°çš„åŸå‹è®¾è®¡æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©è®¾è®¡å¸ˆæ›´å¥½åœ°é¢„æµ‹å’Œè¯„ä¼°ç¤¾äº¤ç³»ç»Ÿåœ¨äººå£ä¼—å¤šæ—¶çš„è¡Œä¸ºï¼Œä»è€Œè®¾è®¡å‡ºæ›´å®‰å…¨ã€æ›´æœ‰æ•ˆçš„ç¤¾äº¤ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç¤¾äº¤æ¨¡æ‹ŸæŠ€æœ¯è¿˜å¯ä»¥ç”¨äºæ¢ç´¢â€œå¦‚æœâ€åœºæ™¯ï¼Œå¸®åŠ©è®¾è®¡å¸ˆæ›´å¥½åœ°ç†è§£ç¤¾äº¤ç³»ç»Ÿçš„åŠ¨æ€å˜åŒ–ã€‚

## inner-monologue--embodied-reasoning-through-planning-with-language-models
### Abstract
Recent works have shown how the reasoning capabilities of Large Language
Models (LLMs) can be applied to domains beyond natural language processing,
such as planning and interaction for robots. These embodied problems require an
agent to understand many semantic aspects of the world: the repertoire of
skills available, how these skills influence the world, and how changes to the
world map back to the language. LLMs planning in embodied environments need to
consider not just what skills to do, but also how and when to do them - answers
that change over time in response to the agent's own choices. In this work, we
investigate to what extent LLMs used in such embodied contexts can reason over
sources of feedback provided through natural language, without any additional
training. We propose that by leveraging environment feedback, LLMs are able to
form an inner monologue that allows them to more richly process and plan in
robotic control scenarios. We investigate a variety of sources of feedback,
such as success detection, scene description, and human interaction. We find
that closed-loop language feedback significantly improves high-level
instruction completion on three domains, including simulated and real table top
rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen
environment in the real world.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å†…å¿ƒç‹¬ç™½ï¼šé€šè¿‡è¯­è¨€æ¨¡å‹è§„åˆ’å®ç°å…·èº«æ¨ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæœºå™¨äººéœ€è¦å…·å¤‡æ›´é«˜çº§çš„æ¨ç†èƒ½åŠ›ï¼Œä»¥ä¾¿åœ¨å¤æ‚çš„ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡ã€‚ä¼ ç»Ÿçš„æœºå™¨äººè§„åˆ’æ–¹æ³•å¾€å¾€ä¾èµ–äºä¼˜åŒ–æˆ–ç¬¦å·æ¨ç†ï¼Œç¼ºä¹å¯¹ä¸–ç•Œè¯­ä¹‰çŸ¥è¯†çš„ç†è§£ã€‚è€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶å±•ç°å‡ºä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢LLMsåœ¨å…·èº«ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç ”ç©¶å¦‚ä½•åˆ©ç”¨ç¯å¢ƒåé¦ˆæ¥æé«˜æœºå™¨äººçš„è§„åˆ’èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶ï¼Œé€šè¿‡å°†ç¯å¢ƒåé¦ˆä¸LLMsç›¸ç»“åˆï¼Œå®ç°æœºå™¨äººå¯¹å¤æ‚ä»»åŠ¡çš„æ¨ç†å’Œè§„åˆ’ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡çš„æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨ç¯å¢ƒåé¦ˆå½¢æˆâ€œå†…å¿ƒç‹¬ç™½â€
é€šè¿‡å°†ç¯å¢ƒåé¦ˆï¼ˆå¦‚æˆåŠŸæ£€æµ‹ã€åœºæ™¯æè¿°ã€äººç±»äº¤äº’ç­‰ï¼‰ä¸æ–­æ³¨å…¥LLMsçš„è§„åˆ’è¯­è¨€æç¤ºä¸­ï¼Œæœºå™¨äººå¯ä»¥å½¢æˆâ€œå†…å¿ƒç‹¬ç™½â€ï¼Œä»è€Œæ›´ä¸°å¯Œåœ°å¤„ç†å’Œè§„åˆ’æœºå™¨äººæ§åˆ¶åœºæ™¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç§åé¦ˆæ¥æº
æœ¬æ–‡ç ”ç©¶äº†å¤šç§åé¦ˆæ¥æºï¼ŒåŒ…æ‹¬æˆåŠŸæ£€æµ‹ã€åœºæ™¯æè¿°å’Œäººç±»äº¤äº’ã€‚é€šè¿‡å°†è¿™äº›åé¦ˆä¸LLMsç›¸ç»“åˆï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£ç¯å¢ƒï¼Œå¹¶æ ¹æ®åé¦ˆè¿›è¡Œé‡æ–°è§„åˆ’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ç¯å¢ƒåé¦ˆçš„â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶å¯ä»¥æ˜¾è‘—æé«˜æœºå™¨äººæ‰§è¡Œé«˜çº§æŒ‡ä»¤çš„èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿçš„æ¡Œé¢æ•´ç†ä»»åŠ¡å’ŒçœŸå®çš„å¨æˆ¿ç§»åŠ¨æ“ä½œä»»åŠ¡ä¸­ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä»»åŠ¡å®Œæˆç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶ä¸ºæœºå™¨äººè§„åˆ’æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

*   åˆ©ç”¨LLMsçš„è¯­ä¹‰çŸ¥è¯†è¿›è¡Œæ¨ç†å’Œè§„åˆ’
*   é€šè¿‡ç¯å¢ƒåé¦ˆå®ç°æœºå™¨äººå¯¹å¤æ‚ä»»åŠ¡çš„é€‚åº”æ€§
*   ç»“åˆå¤šç§åé¦ˆæ¥æºæé«˜æœºå™¨äººçš„è§„åˆ’èƒ½åŠ›

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡æå‡ºçš„â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶ä¸ºæœºå™¨äººè§„åˆ’æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡åˆ©ç”¨LLMsçš„è¯­ä¹‰çŸ¥è¯†å’Œç¯å¢ƒåé¦ˆï¼Œæœºå™¨äººå¯ä»¥æ›´å¥½åœ°ç†è§£ç¯å¢ƒå¹¶æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚æœªæ¥ï¼Œéšç€LLMså’Œæ„ŸçŸ¥æŠ€æœ¯çš„å‘å±•ï¼Œç›¸ä¿¡â€œå†…å¿ƒç‹¬ç™½â€æ¡†æ¶å°†åœ¨æœºå™¨äººé¢†åŸŸå‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚

## video-pretraining-(vpt)--learning-to-act-by-watching-unlabeled-online-videos
### Abstract
Pretraining on noisy, internet-scale datasets has been heavily studied as a
technique for training models with broad, general capabilities for text,
images, and other modalities. However, for many sequential decision domains
such as robotics, video games, and computer use, publicly available data does
not contain the labels required to train behavioral priors in the same way. We
extend the internet-scale pretraining paradigm to sequential decision domains
through semi-supervised imitation learning wherein agents learn to act by
watching online unlabeled videos. Specifically, we show that with a small
amount of labeled data we can train an inverse dynamics model accurate enough
to label a huge unlabeled source of online data -- here, online videos of
people playing Minecraft -- from which we can then train a general behavioral
prior. Despite using the native human interface (mouse and keyboard at 20Hz),
we show that this behavioral prior has nontrivial zero-shot capabilities and
that it can be fine-tuned, with both imitation learning and reinforcement
learning, to hard-exploration tasks that are impossible to learn from scratch
via reinforcement learning. For many tasks our models exhibit human-level
performance, and we are the first to report computer agents that can craft
diamond tools, which can take proficient humans upwards of 20 minutes (24,000
environment actions) of gameplay to accomplish.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è§†é¢‘é¢„è®­ç»ƒï¼ˆVPTï¼‰ï¼šé€šè¿‡è§‚çœ‹æ— æ ‡ç­¾åœ¨çº¿è§†é¢‘å­¦ä¹ è¡ŒåŠ¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸï¼Œé€šè¿‡åœ¨å¤§å‹äº’è”ç½‘æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå·²ç»è¯æ˜äº†è®­ç»ƒå¤§å‹é€šç”¨åŸºç¡€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¯¹äºè®¸å¤šåºåˆ—å†³ç­–é¢†åŸŸï¼Œå¦‚æœºå™¨äººã€è§†é¢‘æ¸¸æˆå’Œè®¡ç®—æœºä½¿ç”¨ï¼Œå…¬å¼€å¯ç”¨çš„æ•°æ®å¹¶ä¸åŒ…å«è®­ç»ƒè¡Œä¸ºå…ˆéªŒæ‰€éœ€çš„æ ‡ç­¾ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ©ç”¨äº’è”ç½‘ä¸Šå¤§é‡æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ï¼Œå°†è¿™äº›é¢„è®­ç»ƒèŒƒå¼æ‰©å±•åˆ°åºåˆ—å†³ç­–é¢†åŸŸã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠç›‘ç£æ¨¡ä»¿å­¦ä¹ 
æœ¬æ–‡æå‡ºäº†ä¸€ç§åŠç›‘ç£æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è§‚çœ‹åœ¨çº¿æœªæ ‡è®°çš„è§†é¢‘ï¼Œä½¿æ™ºèƒ½ä½“å­¦ä¼šè¡ŒåŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®è®­ç»ƒä¸€ä¸ªé€†åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¶³å¤Ÿå‡†ç¡®ï¼Œå¯ä»¥æ ‡è®°å¤§é‡æœªæ ‡è®°çš„åœ¨çº¿æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œäººä»¬ç©Minecraftçš„è§†é¢‘ï¼‰ï¼Œç„¶åä»ä¸­è®­ç»ƒä¸€ä¸ªé€šç”¨çš„è¡Œä¸ºå…ˆéªŒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€†åŠ¨åŠ›å­¦æ¨¡å‹
ä¸è¡Œä¸ºå…‹éš†ç›¸æ¯”ï¼Œé€†åŠ¨åŠ›å­¦å»ºæ¨¡ä»»åŠ¡æ›´ç®€å•ï¼Œå› ä¸ºå®ƒæ˜¯éå› æœçš„ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥æŸ¥çœ‹è¿‡å»å’Œæœªæ¥çš„å¸§æ¥æ¨æ–­åŠ¨ä½œã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç¯å¢ƒæœºåˆ¶æ¯”ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿçš„äººç±»è¡Œä¸ºçš„å¹¿åº¦è¦ç®€å•å¾—å¤šï¼Œè¿™è¡¨æ˜éå› æœé€†åŠ¨åŠ›å­¦æ¨¡å‹å¯èƒ½éœ€è¦æ¯”å› æœè¡Œä¸ºå…‹éš†æ¨¡å‹å°‘å¾—å¤šçš„æ•°æ®æ¥è®­ç»ƒã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ä½¿ç”¨äº†åŸç”Ÿäººç±»ç•Œé¢ï¼ˆ20Hzçš„é¼ æ ‡å’Œé”®ç›˜ï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•çš„è¡Œä¸ºå…ˆéªŒå…·æœ‰éå¹³å‡¡çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ æ— æ³•ä»å¤´å­¦ä¹ çš„å›°éš¾æ¢ç´¢ä»»åŠ¡ã€‚å¯¹äºè®¸å¤šä»»åŠ¡ï¼Œæ¨¡å‹è¡¨ç°å‡ºäººç±»æ°´å¹³çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªæŠ¥å‘Šèƒ½å¤Ÿåˆ¶ä½œé’»çŸ³å·¥å…·çš„è®¡ç®—æœºä»£ç†ï¼Œè¿™éœ€è¦ç†Ÿç»ƒçš„äººç±»ç©å®¶è¶…è¿‡20åˆ†é’Ÿï¼ˆ24,000ä¸ªç¯å¢ƒåŠ¨ä½œï¼‰çš„æ¸¸æˆæ—¶é—´æ‰èƒ½å®Œæˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„è§†é¢‘é¢„è®­ç»ƒï¼ˆVPTï¼‰æ–¹æ³•ä¸ºåˆ©ç”¨äº’è”ç½‘ä¸Šå¤§é‡æœªæ ‡è®°çš„æ•°æ®è¿›è¡Œåºåˆ—å†³ç­–é¢†åŸŸçš„é¢„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å°‘é‡æ ‡è®°æ•°æ®ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ æ— æ³•ä»å¤´å­¦ä¹ çš„å›°éš¾æ¢ç´¢ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åº”ç”¨äºä»»ä½•å…·æœ‰å¤§é‡æœªæ ‡è®°æ•°æ®çš„é¢†åŸŸï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## minedojo--building-open-ended-embodied-agents-with-internet-scale-knowledge
### Abstract
Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite, knowledge bases, algorithm
implementation, and pretrained models (https://minedojo.org) to promote
research towards the goal of generally capable embodied agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MineDojoï¼šæ„å»ºå…·æœ‰äº’è”ç½‘è§„æ¨¡çŸ¥è¯†çš„å¼€æ”¾å¼å…·èº«æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ä¼ ç»Ÿçš„è‡ªä¸»æ™ºèƒ½ä½“åœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚Atariæ¸¸æˆå’Œå›´æ£‹ï¼‰å–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨å­¤ç«‹çš„ç¯å¢ƒä¸­å­¦ä¹ ï¼Œç›®æ ‡æœ‰é™ä¸”æ‰‹åŠ¨è®¾è®¡ï¼Œå› æ­¤æ— æ³•åœ¨å¹¿æ³›çš„ä»»åŠ¡å’Œèƒ½åŠ›ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»åœ¨å¼€æ”¾ä¸–ç•Œä¸­ä¸æ–­å­¦ä¹ å’Œé€‚åº”ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤§é‡æ¥è‡ªè‡ªèº«ç»éªŒå’Œä»–äººçš„å…ˆéªŒçŸ¥è¯†ã€‚æœ¬æ–‡æ—¨åœ¨æ„å»ºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·åœ¨å¼€æ”¾ä¸–ç•Œä¸­å­¦ä¹ å’Œé€‚åº”çš„é€šç”¨æ™ºèƒ½ä½“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼€æ”¾å¼ç¯å¢ƒ
MineDojoåŸºäºæµè¡Œçš„Minecraftæ¸¸æˆï¼Œæä¾›äº†ä¸€ä¸ªå…·æœ‰æ•°åƒä¸ªå¤šæ ·åŒ–å¼€æ”¾å¼ä»»åŠ¡çš„æ¨¡æ‹Ÿå¥—ä»¶ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ç”Ÿå­˜ã€é‡‡é›†ã€æŠ€æœ¯æ ‘å’Œæˆ˜æ–—ç­‰ç±»åˆ«ï¼Œæ¶µç›–äº†ä»ç®€å•åˆ°å¤æ‚çš„å„ç§éš¾åº¦çº§åˆ«ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäº’è”ç½‘è§„æ¨¡çš„å¤šæ¨¡æ€çŸ¥è¯†åº“
MineDojoæ”¶é›†äº†å¤§é‡çš„Minecraftç›¸å…³æ•°æ®ï¼ŒåŒ…æ‹¬YouTubeè§†é¢‘ã€Wikié¡µé¢å’ŒRedditè®¨è®ºã€‚è¿™äº›æ•°æ®æ¶µç›–äº†æ¸¸æˆçš„æ‰€æœ‰æ–¹é¢ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›äº†ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šçµæ´»å¯æ‰©å±•çš„æ™ºèƒ½ä½“æ¶æ„
MineDojoæå‡ºäº†ä¸€ä¸ªåŸºäºTransformeré¢„è®­ç»ƒèŒƒå¼çš„æ™ºèƒ½ä½“å­¦ä¹ ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è§†é¢‘è¯­è¨€æ¨¡å‹ä½œä¸ºå­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œèƒ½å¤Ÿè§£å†³å„ç§å¼€æ”¾å¼ä»»åŠ¡ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è®¾è®¡å¯†é›†çš„å¥–åŠ±å‡½æ•°ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
MineDojoçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶æå‡ºçš„MINECLIPå¥–åŠ±æ¨¡å‹åœ¨ç¨‹åºæ€§ä»»åŠ¡å’Œåˆ›é€ æ€§ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚ä¸æ‰‹åŠ¨è®¾è®¡çš„å¯†é›†å¥–åŠ±å‡½æ•°ç›¸æ¯”ï¼ŒMINECLIPåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå–å¾—äº†ç«äº‰æ€§çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†å®ƒä»¬ã€‚æ­¤å¤–ï¼ŒMINECLIPè¿˜èƒ½å¤Ÿæœ‰æ•ˆåœ°è¯„ä¼°åˆ›é€ æ€§ä»»åŠ¡ï¼Œå…¶è¯„ä¼°ç»“æœä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MineDojoä¸ºå¼€å‘é€šç”¨æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„æ¡†æ¶ã€‚å…¶å¼€æ”¾å¼ç¯å¢ƒã€äº’è”ç½‘è§„æ¨¡çš„çŸ¥è¯†åº“å’Œçµæ´»å¯æ‰©å±•çš„æ™ºèƒ½ä½“æ¶æ„ä¸ºç ”ç©¶äººå‘˜æä¾›äº†æ¢ç´¢å¼€æ”¾å¼æ™ºèƒ½ä½“å­¦ä¹ çš„å¼ºå¤§å·¥å…·ã€‚æ­¤å¤–ï¼ŒMineDojoçš„å¼€æºä»£ç å’Œæ•°æ®é›†å°†ä¿ƒè¿›ç¤¾åŒºå¯¹é€šç”¨æ™ºèƒ½ä½“ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚

