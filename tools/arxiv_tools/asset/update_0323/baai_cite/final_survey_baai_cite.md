# Paper List of baai_cite.md

- [24/08] **Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks**  
[[Paper](http://arxiv.org/pdf/2408.03615v2)] [[Code/Page]()] [[TLDR/Notes](#optimus-1--hybrid-multimodal-memory-empowered-agents-excel-in-long-horizon-tasks)]

- [24/07] **VideoGameBunny: Towards vision assistants for video games**  
[[Paper](http://arxiv.org/pdf/2407.15295v1)] [[Code/Page](https://videogamebunny.github.io/)] [[TLDR/Notes](#videogamebunny--towards-vision-assistants-for-video-games)]

- [24/07] **Autoverse: An Evolvable Game Language for Learning Robust Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2407.04221v2)] [[Code/Page]()] [[TLDR/Notes](#autoverse--an-evolvable-game-language-for-learning-robust-embodied-agents)]

- [24/03] **Cradle: Empowering Foundation Agents Towards General Computer Control**  
[[Paper](http://arxiv.org/pdf/2403.03186v3)] [[Code/Page]()] [[TLDR/Notes](#cradle--empowering-foundation-agents-towards-general-computer-control)]

- [24/06] **Using Game Play to Investigate Multimodal and Conversational Grounding in Large Multimodal Models**  
[[Paper](http://arxiv.org/pdf/2406.14035v3)] [[Code/Page]()] [[TLDR/Notes](#using-game-play-to-investigate-multimodal-and-conversational-grounding-in-large-multimodal-models)]

- [24/05] **Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration**  
[[Paper](http://arxiv.org/pdf/2405.14314v2)] [[Code/Page](https://read-llm.github.io/.)] [[TLDR/Notes](#towards-efficient-llm-grounding-for-embodied-multi-agent-collaboration)]

- [22/10] **A Pilot Study on Teacher-Facing Real-Time Classroom Game Dashboards**  
[[Paper](http://arxiv.org/pdf/2210.09427v1)] [[Code/Page]()] [[TLDR/Notes](#a-pilot-study-on-teacher-facing-real-time-classroom-game-dashboards)]

- [24/01] **Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering**  
[[Paper](http://arxiv.org/pdf/2401.08500v1)] [[Code/Page](https://github.com/Codium-ai/AlphaCodium)] [[TLDR/Notes](#code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering)]

- [24/04] **Self-playing Adversarial Language Game Enhances LLM Reasoning**  
[[Paper](http://arxiv.org/pdf/2404.10642v3)] [[Code/Page](https://github.com/Linear95/SPAG.)] [[TLDR/Notes](#self-playing-adversarial-language-game-enhances-llm-reasoning)]

- [24/02] **Predicting Outcomes in Video Games with Long Short Term Memory Networks**  
[[Paper](http://arxiv.org/pdf/2402.15923v1)] [[Code/Page]()] [[TLDR/Notes](#predicting-outcomes-in-video-games-with-long-short-term-memory-networks)]

- [24/03] **Embodied LLM Agents Learn to Cooperate in Organized Teams**  
[[Paper](http://arxiv.org/pdf/2403.12482v2)] [[Code/Page]()] [[TLDR/Notes](#embodied-llm-agents-learn-to-cooperate-in-organized-teams)]

- [24/03] **Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot**  
[[Paper](http://arxiv.org/pdf/2403.11381v2)] [[Code/Page]()] [[TLDR/Notes](#can-llm-augmented-autonomous-agents-cooperate---an-evaluation-of-their-cooperative-capabilities-through-melting-pot)]

- [24/03] **EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2403.12014v2)] [[Code/Page]()] [[TLDR/Notes](#envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents)]

- [24/03] **MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control**  
[[Paper](http://arxiv.org/pdf/2403.12037v2)] [[Code/Page]()] [[TLDR/Notes](#minedreamer--learning-to-follow-instructions-via-chain-of-imagination-for-simulated-world-control)]

- [24/04] **Scaling Instructable Agents Across Many Simulated Worlds**  
[[Paper](http://arxiv.org/pdf/2404.10179v3)] [[Code/Page]()] [[TLDR/Notes](#scaling-instructable-agents-across-many-simulated-worlds)]

- [24/03] **Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation**  
[[Paper](http://arxiv.org/pdf/2403.08282v2)] [[Code/Page]()] [[TLDR/Notes](#hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation)]

- [24/03] **SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents**  
[[Paper](http://arxiv.org/pdf/2403.08715v3)] [[Code/Page]()] [[TLDR/Notes](#sotopia-$π$--interactive-learning-of-socially-intelligent-language-agents)]

- [24/03] **Will GPT-4 Run DOOM?**  
[[Paper](http://arxiv.org/pdf/2403.05468v1)] [[Code/Page]()] [[TLDR/Notes](#will-gpt-4-run-doom-)]

- [24/02] **Large Multimodal Agents: A Survey**  
[[Paper](http://arxiv.org/pdf/2402.15116v1)] [[Code/Page](https://github.com/jun0wanan/awesome-large-multimodal-agents.)] [[TLDR/Notes](#large-multimodal-agents--a-survey)]

- [24/03] **Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents**  
[[Paper](http://arxiv.org/pdf/2403.00690v1)] [[Code/Page]()] [[TLDR/Notes](#playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents)]

- [23/02] **Q-Cogni: An Integrated Causal Reinforcement Learning Framework**  
[[Paper](http://arxiv.org/pdf/2302.13240v1)] [[Code/Page]()] [[TLDR/Notes](#q-cogni--an-integrated-causal-reinforcement-learning-framework)]

- [24/02] **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**  
[[Paper](http://arxiv.org/pdf/2402.17574v3)] [[Code/Page]()] [[TLDR/Notes](#agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization)]

- [24/02] **PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain**  
[[Paper](http://arxiv.org/pdf/2402.15527v1)] [[Code/Page]()] [[TLDR/Notes](#pca-bench--evaluating-multimodal-large-language-models-in-perception-cognition-action-chain)]

- [24/02] **What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents**  
[[Paper](http://arxiv.org/pdf/2402.13184v5)] [[Code/Page](https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.)] [[TLDR/Notes](#what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents)]

- [14/03] **Software Agents Interaction Algorithms in Virtual Learning Environment**  
[[Paper](http://arxiv.org/pdf/1403.5734v2)] [[Code/Page]()] [[TLDR/Notes](#software-agents-interaction-algorithms-in-virtual-learning-environment)]

- [24/02] **Enhance Reasoning for Large Language Models in the Game Werewolf**  
[[Paper](http://arxiv.org/pdf/2402.02330v2)] [[Code/Page]()] [[TLDR/Notes](#enhance-reasoning-for-large-language-models-in-the-game-werewolf)]

- [24/02] **PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2402.01118v3)] [[Code/Page](https://github.com/git-disl/PokeLLMon.)] [[TLDR/Notes](#pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models)]

- [24/01] **SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models**  
[[Paper](http://arxiv.org/pdf/2401.17749v1)] [[Code/Page]()] [[TLDR/Notes](#swarmbrain--embodied-agent-for-real-time-strategy-game-starcraft-ii-via-large-language-models)]

- [24/01] **CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents**  
[[Paper](http://arxiv.org/pdf/2401.10568v2)] [[Code/Page](https://github.com/bigai-ai/civrealm.)] [[TLDR/Notes](#civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents)]

- [23/11] **Finding the Needle in a Haystack: Detecting Bug Occurrences in Gameplay Videos**  
[[Paper](http://arxiv.org/pdf/2311.10926v1)] [[Code/Page]()] [[TLDR/Notes](#finding-the-needle-in-a-haystack--detecting-bug-occurrences-in-gameplay-videos)]

- [24/01] **PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model**  
[[Paper](http://arxiv.org/pdf/2401.06781v1)] [[Code/Page]()] [[TLDR/Notes](#pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model)]

- [23/12] **Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game**  
[[Paper](http://arxiv.org/pdf/2312.17515v1)] [[Code/Page]()] [[TLDR/Notes](#cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game)]

- [23/12] **LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination**  
[[Paper](http://arxiv.org/pdf/2312.15224v2)] [[Code/Page]()] [[TLDR/Notes](#llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination)]

- [23/12] **Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach**  
[[Paper](http://arxiv.org/pdf/2312.11865v3)] [[Code/Page]()] [[TLDR/Notes](#large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach)]

- [23/12] **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**  
[[Paper](http://arxiv.org/pdf/2312.09238v2)] [[Code/Page]()] [[TLDR/Notes](#auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft)]

- [23/12] **MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception**  
[[Paper](http://arxiv.org/pdf/2312.07472v4)] [[Code/Page]()] [[TLDR/Notes](#mp5--a-multi-modal-open-ended-embodied-system-in-minecraft-via-active-perception)]

- [25/02] **Stay Focused: Problem Drift in Multi-Agent Debate**  
[[Paper](http://arxiv.org/pdf/2502.19559v1)] [[Code/Page]()] [[TLDR/Notes](#stay-focused--problem-drift-in-multi-agent-debate)]

- [23/12] **GlitchBench: Can large multimodal models detect video game glitches?**  
[[Paper](http://arxiv.org/pdf/2312.05291v2)] [[Code/Page](https://glitchbench.github.io/)] [[TLDR/Notes](#glitchbench--can-large-multimodal-models-detect-video-game-glitches-)]

- [18/08] **A Framework for Complementary Companion Character Behavior in Video Games**  
[[Paper](http://arxiv.org/pdf/1808.09079v1)] [[Code/Page]()] [[TLDR/Notes](#a-framework-for-complementary-companion-character-behavior-in-video-games)]

- [23/12] **Creative Agents: Empowering Agents with Imagination for Creative Tasks**  
[[Paper](http://arxiv.org/pdf/2312.02519v1)] [[Code/Page](https://github.com/PKU-RL/Creative-Agents).)] [[TLDR/Notes](#creative-agents--empowering-agents-with-imagination-for-creative-tasks)]

- [23/12] **Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games**  
[[Paper](http://arxiv.org/pdf/2312.02312v1)] [[Code/Page]()] [[TLDR/Notes](#visual-encoders-for-data-efficient-imitation-learning-in-modern-video-games)]

- [24/01] **Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation**  
[[Paper](http://arxiv.org/pdf/2401.00006v3)] [[Code/Page]()] [[TLDR/Notes](#building-open-ended-embodied-agent-via-language-policy-bidirectional-adaptation)]

- [23/12] **Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games**  
[[Paper](http://arxiv.org/pdf/2312.00746v2)] [[Code/Page]()] [[TLDR/Notes](#deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games)]

- [23/11] **War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars**  
[[Paper](http://arxiv.org/pdf/2311.17227v2)] [[Code/Page](https://github.com/agiresearch/WarAgent}.)] [[TLDR/Notes](#war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars)]

- [23/11] **See and Think: Embodied Agent in Virtual Environment**  
[[Paper](http://arxiv.org/pdf/2311.15209v3)] [[Code/Page]()] [[TLDR/Notes](#see-and-think--embodied-agent-in-virtual-environment)]

- [23/11] **DesignGPT: Multi-Agent Collaboration in Design**  
[[Paper](http://arxiv.org/pdf/2311.11591v1)] [[Code/Page]()] [[TLDR/Notes](#designgpt--multi-agent-collaboration-in-design)]

- [23/11] **MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration**  
[[Paper](http://arxiv.org/pdf/2311.08562v3)] [[Code/Page](https://github.com/cathyxl/MAgIC.)] [[TLDR/Notes](#magic--investigation-of-large-language-model-powered-multi-agent-in-cognition--adaptability--rationality-and-collaboration)]

- [24/03] **MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs**  
[[Paper](http://arxiv.org/pdf/2403.19267v2)] [[Code/Page](https://github.com/cocacola-lab/MineLand.)] [[TLDR/Notes](#mineland--simulating-large-scale-multi-agent-interactions-with-limited-multimodal-senses-and-physical-needs)]

- [23/11] **ADaPT: As-Needed Decomposition and Planning with Language Models**  
[[Paper](http://arxiv.org/pdf/2311.05772v2)] [[Code/Page]()] [[TLDR/Notes](#adapt--as-needed-decomposition-and-planning-with-language-models)]

- [23/10] **Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.20499v2)] [[Code/Page]()] [[TLDR/Notes](#leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models)]

- [23/10] **Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game**  
[[Paper](http://arxiv.org/pdf/2310.18940v3)] [[Code/Page]()] [[TLDR/Notes](#language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game)]

- [23/10] **LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay**  
[[Paper](http://arxiv.org/pdf/2310.14985v4)] [[Code/Page](https://github.com/3DAgentWorld/LLM-Game-Agent.)] [[TLDR/Notes](#llm-based-agent-society-investigation--collaboration-and-confrontation-in-avalon-gameplay)]

- [23/10] **Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds**  
[[Paper](http://arxiv.org/pdf/2310.13255v2)] [[Code/Page]()] [[TLDR/Notes](#steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds)]

- [23/10] **SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents**  
[[Paper](http://arxiv.org/pdf/2310.11667v2)] [[Code/Page]()] [[TLDR/Notes](#sotopia--interactive-evaluation-for-social-intelligence-in-language-agents)]

- [23/10] **Character-LLM: A Trainable Agent for Role-Playing**  
[[Paper](http://arxiv.org/pdf/2310.10158v2)] [[Code/Page]()] [[TLDR/Notes](#character-llm--a-trainable-agent-for-role-playing)]

- [23/10] **LLaMA Rider: Spurring Large Language Models to Explore the Open World**  
[[Paper](http://arxiv.org/pdf/2310.08922v1)] [[Code/Page]()] [[TLDR/Notes](#llama-rider--spurring-large-language-models-to-explore-the-open-world)]

- [23/10] **GameGPT: Multi-agent Collaborative Framework for Game Development**  
[[Paper](http://arxiv.org/pdf/2310.08067v1)] [[Code/Page]()] [[TLDR/Notes](#gamegpt--multi-agent-collaborative-framework-for-game-development)]

- [23/10] **GROOT: Learning to Follow Instructions by Watching Gameplay Videos**  
[[Paper](http://arxiv.org/pdf/2310.08235v2)] [[Code/Page](https://craftjarvis-groot.github.io.)] [[TLDR/Notes](#groot--learning-to-follow-instructions-by-watching-gameplay-videos)]

- [23/10] **Octopus: Embodied Vision-Language Programmer from Environmental Feedback**  
[[Paper](http://arxiv.org/pdf/2310.08588v2)] [[Code/Page]()] [[TLDR/Notes](#octopus--embodied-vision-language-programmer-from-environmental-feedback)]

- [23/10] **MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.06500v1)] [[Code/Page]()] [[TLDR/Notes](#metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents)]

- [23/10] **Humanoid Agents: Platform for Simulating Human-like Generative Agents**  
[[Paper](http://arxiv.org/pdf/2310.05418v1)] [[Code/Page](https://www.humanoidagents.com/)] [[TLDR/Notes](#humanoid-agents--platform-for-simulating-human-like-generative-agents)]

- [23/10] **AvalonBench: Evaluating LLMs Playing the Game of Avalon**  
[[Paper](http://arxiv.org/pdf/2310.05036v3)] [[Code/Page]()] [[TLDR/Notes](#avalonbench--evaluating-llms-playing-the-game-of-avalon)]

- [25/02] **Beyond Win Rates: A Clustering-Based Approach to Character Balance Analysis in Team-Based Games**  
[[Paper](http://arxiv.org/pdf/2502.01250v1)] [[Code/Page]()] [[TLDR/Notes](#beyond-win-rates--a-clustering-based-approach-to-character-balance-analysis-in-team-based-games)]

- [23/10] **LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03903v2)] [[Code/Page](https://github.com/eric-ai-lab/llm_coordination}.)] [[TLDR/Notes](#llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models)]

- [23/10] **Lyfe Agents: Generative agents for low-cost real-time social interactions**  
[[Paper](http://arxiv.org/pdf/2310.02172v1)] [[Code/Page]()] [[TLDR/Notes](#lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions)]

- [21/11] **Adaptive Multi-Goal Exploration**  
[[Paper](http://arxiv.org/pdf/2111.12045v2)] [[Code/Page]()] [[TLDR/Notes](#adaptive-multi-goal-exploration)]

- [23/10] **SmartPlay: A Benchmark for LLMs as Intelligent Agents**  
[[Paper](http://arxiv.org/pdf/2310.01557v5)] [[Code/Page]()] [[TLDR/Notes](#smartplay--a-benchmark-for-llms-as-intelligent-agents)]

- [23/10] **Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation**  
[[Paper](http://arxiv.org/pdf/2310.01320v3)] [[Code/Page]()] [[TLDR/Notes](#avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation)]

- [23/10] **RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.00746v3)] [[Code/Page]()] [[TLDR/Notes](#rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models)]

- [23/09] **AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback**  
[[Paper](http://arxiv.org/pdf/2309.17176v3)] [[Code/Page]()] [[TLDR/Notes](#adarefiner--refining-decisions-of-language-models-with-adaptive-feedback)]

- [23/10] **Motif: Intrinsic Motivation from Artificial Intelligence Feedback**  
[[Paper](http://arxiv.org/pdf/2310.00166v1)] [[Code/Page]()] [[TLDR/Notes](#motif--intrinsic-motivation-from-artificial-intelligence-feedback)]

- [24/09] **SimulBench: Evaluating Language Models with Creative Simulation Tasks**  
[[Paper](http://arxiv.org/pdf/2409.07641v1)] [[Code/Page]()] [[TLDR/Notes](#simulbench--evaluating-language-models-with-creative-simulation-tasks)]

- [23/09] **Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4**  
[[Paper](http://arxiv.org/pdf/2309.17277v3)] [[Code/Page]()] [[TLDR/Notes](#suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4)]

- [23/09] **AutoAgents: A Framework for Automatic Agent Generation**  
[[Paper](http://arxiv.org/pdf/2309.17288v3)] [[Code/Page](https://github.com/Link-AGI/AutoAgents.)] [[TLDR/Notes](#autoagents--a-framework-for-automatic-agent-generation)]

- [24/01] **True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2401.14151v2)] [[Code/Page]()] [[TLDR/Notes](#true-knowledge-comes-from-practice--aligning-llms-with-embodied-environments-via-reinforcement-learning)]

- [23/09] **MindAgent: Emergent Gaming Interaction**  
[[Paper](http://arxiv.org/pdf/2309.09971v2)] [[Code/Page]()] [[TLDR/Notes](#mindagent--emergent-gaming-interaction)]

- [23/09] **Agents: An Open-source Framework for Autonomous Language Agents**  
[[Paper](http://arxiv.org/pdf/2309.07870v3)] [[Code/Page](https://github.com/aiwaves-cn/agents.)] [[TLDR/Notes](#agents--an-open-source-framework-for-autonomous-language-agents)]

- [23/09] **Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf**  
[[Paper](http://arxiv.org/pdf/2309.04658v2)] [[Code/Page]()] [[TLDR/Notes](#exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf)]

- [23/08] **Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis**  
[[Paper](http://arxiv.org/pdf/2308.12466v2)] [[Code/Page]()] [[TLDR/Notes](#are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis)]

- [23/09] **Towards Ontology Construction with Language Models**  
[[Paper](http://arxiv.org/pdf/2309.09898v1)] [[Code/Page]()] [[TLDR/Notes](#towards-ontology-construction-with-language-models)]

- [23/08] **AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors**  
[[Paper](http://arxiv.org/pdf/2308.10848v3)] [[Code/Page](https://github.com/OpenBMB/AgentVerse}.)] [[TLDR/Notes](#agentverse--facilitating-multi-agent-collaboration-and-exploring-emergent-behaviors)]

- [23/08] **GameEval: Evaluating LLMs on Conversational Games**  
[[Paper](http://arxiv.org/pdf/2308.10032v1)] [[Code/Page](https://github.com/GameEval/GameEval.)] [[TLDR/Notes](#gameeval--evaluating-llms-on-conversational-games)]

- [23/08] **AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation**  
[[Paper](http://arxiv.org/pdf/2308.08155v2)] [[Code/Page]()] [[TLDR/Notes](#autogen--enabling-next-gen-llm-applications-via-multi-agent-conversation)]

- [23/08] **CALYPSO: LLMs as Dungeon Masters' Assistants**  
[[Paper](http://arxiv.org/pdf/2308.07540v1)] [[Code/Page]()] [[TLDR/Notes](#calypso--llms-as-dungeon-masters--assistants)]

- [23/08] **AgentSims: An Open-Source Sandbox for Large Language Model Evaluation**  
[[Paper](http://arxiv.org/pdf/2308.04026v1)] [[Code/Page](https://agentsims.com)] [[TLDR/Notes](#agentsims--an-open-source-sandbox-for-large-language-model-evaluation)]

- [23/08] **MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework**  
[[Paper](http://arxiv.org/pdf/2308.00352v7)] [[Code/Page](https://github.com/geekan/MetaGPT)] [[TLDR/Notes](#metagpt--meta-programming-for-a-multi-agent-collaborative-framework)]

- [24/06] **EmoLLM: Multimodal Emotional Understanding Meets Large Language Models**  
[[Paper](http://arxiv.org/pdf/2406.16442v2)] [[Code/Page]()] [[TLDR/Notes](#emollm--multimodal-emotional-understanding-meets-large-language-models)]

- [23/07] **Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors**  
[[Paper](http://arxiv.org/pdf/2307.11922v1)] [[Code/Page]()] [[TLDR/Notes](#selective-perception--optimizing-state-descriptions-with-reinforcement-learning-for-language-model-actors)]

- [23/07] **SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning**  
[[Paper](http://arxiv.org/pdf/2307.06135v2)] [[Code/Page](https://sayplan.github.io.)] [[TLDR/Notes](#sayplan--grounding-large-language-models-using-3d-scene-graphs-for-scalable-robot-task-planning)]

- [23/07] **Building Cooperative Embodied Agents Modularly with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2307.02485v2)] [[Code/Page](https://vis-www.cs.umass.edu/Co-LLM-Agents/.)] [[TLDR/Notes](#building-cooperative-embodied-agents-modularly-with-large-language-models)]

- [23/07] **Embodied Task Planning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2307.01848v1)] [[Code/Page]()] [[TLDR/Notes](#embodied-task-planning-with-large-language-models)]

- [23/06] **SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling**  
[[Paper](http://arxiv.org/pdf/2306.11886v3)] [[Code/Page](https://clvrai.com/sprint.)] [[TLDR/Notes](#sprint--scalable-policy-pre-training-via-language-instruction-relabeling)]

- [23/06] **ChessGPT: Bridging Policy Learning and Language Modeling**  
[[Paper](http://arxiv.org/pdf/2306.09200v2)] [[Code/Page](https://github.com/waterhorse1/ChessGPT.)] [[TLDR/Notes](#chessgpt--bridging-policy-learning-and-language-modeling)]

- [23/06] **OMNI: Open-endedness via Models of human Notions of Interestingness**  
[[Paper](http://arxiv.org/pdf/2306.01711v3)] [[Code/Page](https://www.jennyzhangzt.com/omni/)] [[TLDR/Notes](#omni--open-endedness-via-models-of-human-notions-of-interestingness)]

- [23/06] **STEVE-1: A Generative Model for Text-to-Behavior in Minecraft**  
[[Paper](http://arxiv.org/pdf/2306.00937v3)] [[Code/Page]()] [[TLDR/Notes](#steve-1--a-generative-model-for-text-to-behavior-in-minecraft)]

- [18/08] **The Text-Based Adventure AI Competition**  
[[Paper](http://arxiv.org/pdf/1808.01262v4)] [[Code/Page]()] [[TLDR/Notes](#the-text-based-adventure-ai-competition)]

- [23/05] **AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation**  
[[Paper](http://arxiv.org/pdf/2305.18898v1)] [[Code/Page](https://www.youtube.com/watch?v=ayAzID1_qQk)] [[TLDR/Notes](#alphablock--embodied-finetuning-for-vision-language-reasoning-in-robot-manipulation)]

- [23/05] **Playing repeated games with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.16867v1)] [[Code/Page]()] [[TLDR/Notes](#playing-repeated-games-with-large-language-models)]

- [23/05] **Voyager: An Open-Ended Embodied Agent with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.16291v2)] [[Code/Page](https://voyager.minedojo.org/.)] [[TLDR/Notes](#voyager--an-open-ended-embodied-agent-with-large-language-models)]

- [23/05] **Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory**  
[[Paper](http://arxiv.org/pdf/2305.17144v2)] [[Code/Page](https://github.com/OpenGVLab/GITM.)] [[TLDR/Notes](#ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory)]

- [23/05] **SPRING: Studying the Paper and Reasoning to Play Games**  
[[Paper](http://arxiv.org/pdf/2305.15486v3)] [[Code/Page]()] [[TLDR/Notes](#spring--studying-the-paper-and-reasoning-to-play-games)]

- [23/05] **Improving Factuality and Reasoning in Language Models through Multiagent Debate**  
[[Paper](http://arxiv.org/pdf/2305.14325v1)] [[Code/Page]()] [[TLDR/Notes](#improving-factuality-and-reasoning-in-language-models-through-multiagent-debate)]

- [23/05] **Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback**  
[[Paper](http://arxiv.org/pdf/2305.10142v1)] [[Code/Page]()] [[TLDR/Notes](#improving-language-model-negotiation-with-self-play-and-in-context-learning-from-ai-feedback)]

- [23/05] **TidyBot: Personalized Robot Assistance with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2305.05658v2)] [[Code/Page]()] [[TLDR/Notes](#tidybot--personalized-robot-assistance-with-large-language-models)]

- [23/05] **ArK: Augmented Reality with Knowledge Interactive Emergent Ability**  
[[Paper](http://arxiv.org/pdf/2305.00970v1)] [[Code/Page]()] [[TLDR/Notes](#ark--augmented-reality-with-knowledge-interactive-emergent-ability)]

- [23/04] **Generative Agents: Interactive Simulacra of Human Behavior**  
[[Paper](http://arxiv.org/pdf/2304.03442v2)] [[Code/Page]()] [[TLDR/Notes](#generative-agents--interactive-simulacra-of-human-behavior)]

- [23/04] **Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions**  
[[Paper](http://arxiv.org/pdf/2304.02868v1)] [[Code/Page]()] [[TLDR/Notes](#can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions)]

- [24/11] **Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey**  
[[Paper](http://arxiv.org/pdf/2411.16403v1)] [[Code/Page]()] [[TLDR/Notes](#adapter-based-approaches-to-knowledge-enhanced-language-models----a-survey)]

- [23/03] **CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society**  
[[Paper](http://arxiv.org/pdf/2303.17760v2)] [[Code/Page](https://github.com/camel-ai/camel.)] [[TLDR/Notes](#camel--communicative-agents-for--mind--exploration-of-large-language-model-society)]

- [22/10] **LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation**  
[[Paper](http://arxiv.org/pdf/2210.12631v2)] [[Code/Page]()] [[TLDR/Notes](#league--guided-skill-learning-and-abstraction-for-long-horizon-manipulation)]

- [23/03] **PaLM-E: An Embodied Multimodal Language Model**  
[[Paper](http://arxiv.org/pdf/2303.03378v1)] [[Code/Page]()] [[TLDR/Notes](#palm-e--an-embodied-multimodal-language-model)]

- [23/02] **Guiding Pretraining in Reinforcement Learning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2302.06692v2)] [[Code/Page](https://github.com/yuqingd/ellm.)] [[TLDR/Notes](#guiding-pretraining-in-reinforcement-learning-with-large-language-models)]

- [23/02] **MarioGPT: Open-Ended Text2Level Generation through Large Language Models**  
[[Paper](http://arxiv.org/pdf/2302.05981v3)] [[Code/Page](https://github.com/shyamsn97/mario-gpt.)] [[TLDR/Notes](#mariogpt--open-ended-text2level-generation-through-large-language-models)]

- [23/02] **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**  
[[Paper](http://arxiv.org/pdf/2302.01560v3)] [[Code/Page](https://github.com/CraftJarvis/MC-Planner.)] [[TLDR/Notes](#describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents)]

- [23/01] **Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling**  
[[Paper](http://arxiv.org/pdf/2301.12050v2)] [[Code/Page]()] [[TLDR/Notes](#do-embodied-agents-dream-of-pixelated-sheep--embodied-decision-making-using-language-guided-world-modelling)]

- [23/01] **Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction**  
[[Paper](http://arxiv.org/pdf/2301.10034v3)] [[Code/Page]()] [[TLDR/Notes](#open-world-multi-task-control-through-goal-aware-representation-learning-and-adaptive-horizon-prediction)]

- [23/10] **Welfare Diplomacy: Benchmarking Language Model Cooperation**  
[[Paper](http://arxiv.org/pdf/2310.08901v1)] [[Code/Page](https://github.com/mukobi/welfare-diplomacy.)] [[TLDR/Notes](#welfare-diplomacy--benchmarking-language-model-cooperation)]

- [22/11] **Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models**  
[[Paper](http://arxiv.org/pdf/2211.11736v3)] [[Code/Page]()] [[TLDR/Notes](#robotic-skill-acquisition-via-instruction-augmentation-with-vision-language-models)]

- [22/10] **Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task**  
[[Paper](http://arxiv.org/pdf/2210.13382v5)] [[Code/Page]()] [[TLDR/Notes](#emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task)]

- [22/10] **Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors**  
[[Paper](http://arxiv.org/pdf/2210.02506v1)] [[Code/Page](https://asgaardlab.github.io/LLMxBugs)] [[TLDR/Notes](#large-language-models-are-pretty-good-zero-shot-video-game-bug-detectors)]

- [22/08] **Social Simulacra: Creating Populated Prototypes for Social Computing Systems**  
[[Paper](http://arxiv.org/pdf/2208.04024v1)] [[Code/Page]()] [[TLDR/Notes](#social-simulacra--creating-populated-prototypes-for-social-computing-systems)]

- [22/07] **Inner Monologue: Embodied Reasoning through Planning with Language Models**  
[[Paper](http://arxiv.org/pdf/2207.05608v1)] [[Code/Page]()] [[TLDR/Notes](#inner-monologue--embodied-reasoning-through-planning-with-language-models)]

- [22/06] **Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos**  
[[Paper](http://arxiv.org/pdf/2206.11795v1)] [[Code/Page]()] [[TLDR/Notes](#video-pretraining-(vpt)--learning-to-act-by-watching-unlabeled-online-videos)]

- [22/06] **MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**  
[[Paper](http://arxiv.org/pdf/2206.08853v2)] [[Code/Page](https://minedojo.org))] [[TLDR/Notes](#minedojo--building-open-ended-embodied-agents-with-internet-scale-knowledge)]



# TLDR/Notes
## face-recognition-methods-&-applications
### Abstract
Face recognition presents a challenging problem in the field of image
analysis and computer vision. The security of information is becoming very
significant and difficult. Security cameras are presently common in airports,
Offices, University, ATM, Bank and in any locations with a security system.
Face recognition is a biometric system used to identify or verify a person from
a digital image. Face Recognition system is used in security. Face recognition
system should be able to automatically detect a face in an image. This involves
extracts its features and then recognize it, regardless of lighting,
expression, illumination, ageing, transformations (translate, rotate and scale
image) and pose, which is a difficult task. This paper contains three sections.
The first section describes the common methods like holistic matching method,
feature extraction method and hybrid methods. The second section describes
applications with examples and finally third section describes the future
research directions of face recognition.
### 🌟 论文解读 | 《人脸识别方法与应用》：深入探索人脸识别技术的奥秘

### 📌 背景痛点/本文动机
随着信息安全变得越来越重要和复杂，人脸识别作为一种生物识别系统，在图像分析和计算机视觉领域面临着巨大的挑战。本文旨在探讨人脸识别的常见方法、应用实例以及未来的研究方向，以期为相关领域的研究和实践提供参考。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：全面梳理人脸识别方法
本文详细介绍了三种主流的人脸识别方法：整体匹配方法、特征提取方法和混合方法。整体匹配方法如Eigenfaces，通过主成分分析（PCA）提取面部特征；特征提取方法关注局部特征，如眼睛、鼻子和嘴巴；混合方法结合了整体和局部特征，通常使用3D图像进行识别。

💡 创新点2：丰富的人脸识别应用案例
文章不仅介绍了人脸识别的基本方法，还提供了多个实际应用案例，如选民注册系统中的重复身份识别、计算机登录监控、机场安全系统、图像数据库调查等，展示了人脸识别技术在各个领域的广泛应用。

### 📈 实验结果
本文没有详细描述具体的实验结果，但通过文献综述和案例分析，展示了人脸识别技术在多种场景下的有效性和实用性。

### 💬 可借鉴之处
1. **方法多样性**：本文提供了多种人脸识别方法，研究者可以根据具体应用场景选择合适的方法。
2. **实际应用案例**：通过实际案例，展示了人脸识别技术在现实世界中的具体应用，为其他研究者提供了实践参考。
3. **未来研究方向**：文章指出了人脸识别技术的未来发展方向，如2D和3D人脸识别、大规模应用等，为后续研究提供了方向指引。

总之，本文对人脸识别技术进行了全面的梳理和探讨，对于人脸识别领域的研究者和工程师具有很高的参考价值。

## optimus-1--hybrid-multimodal-memory-empowered-agents-excel-in-long-horizon-tasks
### Abstract
Building a general-purpose agent is a long-standing vision in the field of
artificial intelligence. Existing agents have made remarkable progress in many
domains, yet they still struggle to complete long-horizon tasks in an open
world. We attribute this to the lack of necessary world knowledge and
multimodal experience that can guide agents through a variety of long-horizon
tasks. In this paper, we propose a Hybrid Multimodal Memory module to address
the above challenges. It 1) transforms knowledge into Hierarchical Directed
Knowledge Graph that allows agents to explicitly represent and learn world
knowledge, and 2) summarises historical information into Abstracted Multimodal
Experience Pool that provide agents with rich references for in-context
learning. On top of the Hybrid Multimodal Memory module, a multimodal agent,
Optimus-1, is constructed with dedicated Knowledge-guided Planner and
Experience-Driven Reflector, contributing to a better planning and reflection
in the face of long-horizon tasks in Minecraft. Extensive experimental results
show that Optimus-1 significantly outperforms all existing agents on
challenging long-horizon task benchmarks, and exhibits near human-level
performance on many tasks. In addition, we introduce various Multimodal Large
Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show
that Optimus-1 exhibits strong generalization with the help of the Hybrid
Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.
### 🌟 论文解读 | Optimus-1：混合多模态记忆赋能代理在长时任务中表现出色

### 📌 背景痛点/本文动机
构建一个通用代理是人工智能领域长期以来的愿景。尽管现有的代理在许多领域取得了显著进展，但它们仍然难以在开放世界中完成长时任务。本文认为，这主要是因为缺乏必要的世界知识和多模态经验，这些知识和经验可以指导代理完成各种长时任务。

### 🚀 核心方法
💡 创新点1：混合多模态记忆模块
本文提出了一个混合多模态记忆模块，该模块由分层有向知识图（HDKG）和抽象多模态经验池（AMEP）组成。HDKG将知识转化为分层有向图结构，使代理能够显式地表示和学习世界知识。AMEP将历史信息总结为抽象多模态经验池，为代理提供丰富的参考，以便进行上下文学习。

💡 创新点2：Optimus-1代理
在混合多模态记忆模块的基础上，本文构建了一个多模态代理Optimus-1，该代理由知识引导规划器、经验驱动反思器和动作控制器组成。知识引导规划器利用HDKG来捕获完成任务所需的知识，并将其转化为可执行的子目标序列。动作控制器根据子目标和当前观察生成低级动作，与游戏环境交互以更新代理的状态。经验驱动反思器定期激活，从AMEP中检索相关的多模态经验，以评估当前子目标是否可以成功执行。如果不行，它会指示知识引导规划器修改其计划。

### 📈 实验结果
在Minecraft中进行的广泛实验结果表明，Optimus-1在具有挑战性的长时任务基准测试中显著优于所有现有代理，并在许多任务上表现出接近人类水平的性能。此外，本文引入了各种多模态大型语言模型（MLLMs）作为Optimus-1的骨干。实验结果表明，在混合多模态记忆模块的帮助下，Optimus-1在各种任务上表现出强大的泛化能力，优于GPT-4V基线。

### 💬 可借鉴之处
本文提出的混合多模态记忆模块和Optimus-1代理为构建能够完成长时任务的通用代理提供了新的思路和方法。此外，本文提出的非参数学习方法也为代理的学习和进化提供了新的思路。

## videogamebunny--towards-vision-assistants-for-video-games
### Abstract
Large multimodal models (LMMs) hold substantial promise across various
domains, from personal assistance in daily tasks to sophisticated applications
like medical diagnostics. However, their capabilities have limitations in the
video game domain, such as challenges with scene understanding, hallucinations,
and inaccurate descriptions of video game content, especially in open-source
models. This paper describes the development of VideoGameBunny, a LLaVA-style
model based on Bunny, specifically tailored for understanding images from video
games. We release intermediate checkpoints, training logs, and an extensive
dataset comprising 185,259 video game images from 413 titles, along with
389,565 image-instruction pairs that include image captions, question-answer
pairs, and a JSON representation of 16 elements of 136,974 images. Our
experiments show that our high quality game-related data has the potential to
make a relatively small model outperform the much larger state-of-the-art model
LLaVa-1.6-34b (which has more than 4x the number of parameters). Our study
paves the way for future research in video game understanding on tasks such as
playing, commentary, and debugging. Code and data are available at
https://videogamebunny.github.io/
### 🌟 论文解读 | 视频游戏助手：VideoGameBunny

### 📌 背景痛点/本文动机
随着视频游戏产业的蓬勃发展，大型多模态模型（LMMs）在游戏领域的应用潜力巨大。然而，现有的LMMs在理解游戏内容方面存在局限性，例如场景理解困难、幻觉现象以及游戏内容的描述不准确。本文旨在解决这些问题，通过开发一个专门针对游戏内容理解的模型，即VideoGameBunny，来提升LMMs在游戏领域的表现。

### 🚀 核心方法
💡 创新点1：VideoGameBunny模型
VideoGameBunny是一个基于Bunny模型的大型多模态模型，经过专门针对游戏内容的微调，使其能够更好地理解和处理游戏图像。该模型采用了LLaVA-style架构，通过多层感知器（MLP）将视觉嵌入与语言模型相结合，从而实现图像和文本的融合。

💡 创新点2：游戏相关数据集
为了训练VideoGameBunny，本文构建了一个包含185,259张游戏图像和389,565个图像-指令对的数据集。这些图像来自413款不同的游戏，涵盖了各种游戏类型、图形风格和游戏机制。图像-指令对包括图像标题、问答对和图像的JSON表示，为模型提供了丰富的游戏相关数据。

### 📈 实验结果
实验结果表明，VideoGameBunny在游戏内容理解任务上取得了显著的性能提升。与现有的SOTA模型LLaVa-1.6-34b相比，VideoGameBunny在游戏相关问答基准测试中取得了更高的准确率。此外，实验还发现，图像-to-JSON数据集对模型性能的提升最为显著，而短标题数据集则可能对模型性能产生负面影响。

### 💬 可借鉴之处
VideoGameBunny的研究成果为游戏内容理解领域提供了重要的参考。通过构建专门针对游戏内容的数据集和模型，可以有效地提升LMMs在游戏领域的表现。此外，本文还探讨了不同类型的数据集和混合策略对模型性能的影响，为未来研究提供了有价值的指导。

## autoverse--an-evolvable-game-language-for-learning-robust-embodied-agents
### Abstract
We introduce Autoverse, an evolvable, domain-specific language for
single-player 2D grid-based games, and demonstrate its use as a scalable
training ground for Open-Ended Learning (OEL) algorithms. Autoverse uses
cellular-automaton-like rewrite rules to describe game mechanics, allowing it
to express various game environments (e.g. mazes, dungeons, sokoban puzzles)
that are popular testbeds for Reinforcement Learning (RL) agents. Each rewrite
rule can be expressed as a series of simple convolutions, allowing for
environments to be parallelized on the GPU, thereby drastically accelerating RL
training. Using Autoverse, we propose jump-starting open-ended learning by
imitation learning from search. In such an approach, we first evolve Autoverse
environments (their rules and initial map topology) to maximize the number of
iterations required by greedy tree search to discover a new best solution,
producing a curriculum of increasingly complex environments and playtraces. We
then distill these expert playtraces into a neural-network-based policy using
imitation learning. Finally, we use the learned policy as a starting point for
open-ended RL, where new training environments are continually evolved to
maximize the RL player agent's value function error (a proxy for its regret, or
the learnability of generated environments), finding that this approach
improves the performance and generality of resultant player agents.
### 🌟 论文解读 | Autoverse：用于学习鲁棒具身智能体的可进化游戏语言

### 📌 背景痛点/本文动机
在虚拟环境中进行开放式学习（OEL）的目标是训练出能力逐渐增强、行为逐渐复杂的智能体。然而，现有的OEL环境往往缺乏复杂性和多样性，导致学习效果有限。此外，从生成的环境中“冷启动”学习策略的复杂性也是一个挑战。

### 🚀 核心方法
💡 创新点1：Autoverse，一种可进化的领域特定语言（DSL），用于单玩家2D网格游戏。它使用类似细胞自动机的重写规则来描述游戏机制，能够表达各种游戏环境，如迷宫、地牢、推箱子谜题等。

💡 创新点2：利用GPU并行化重写规则，通过卷积操作实现，从而加速强化学习（RL）训练。

💡 创新点3：通过模仿学习从搜索中启动开放式学习。首先，通过进化Autoverse环境（规则和初始地图拓扑）来最大化贪婪树搜索发现新最佳解决方案所需的迭代次数，产生一个越来越复杂的环境和游戏轨迹的教程。然后，使用模仿学习将这些专家游戏轨迹提炼成一个基于神经网络的策略。最后，使用学习到的策略作为开放式RL的起点，其中新的训练环境不断进化以最大化RL玩家智能体的价值函数误差（作为其遗憾或生成环境的可学习性的代理），发现这种方法提高了玩家智能体的性能和通用性。

### 📈 实验结果
实验结果表明，在模仿学习过程中，更大的观察范围和观察环境规则集可以提高智能体的性能。此外，进化过程中产生的环境具有不同的动态特性，包括高度不稳定、部分稳定和平衡稳定/混沌模式的环境。

### 💬 可借鉴之处
Autoverse为开放式学习提供了一个可扩展的测试平台，并展示了如何通过进化策略来搜索具有挑战性的游戏环境。此外，该论文还提出了使用模仿学习来启动开放式学习的想法，并通过实验验证了其有效性。这些方法和思想可以应用于其他开放式学习环境的设计和训练中。

## cradle--empowering-foundation-agents-towards-general-computer-control
### Abstract
Despite the success in specific scenarios, existing foundation agents still
struggle to generalize across various virtual scenarios, mainly due to the
dramatically different encapsulations of environments with manually designed
observation and action spaces. To handle this issue, we propose the General
Computer Control (GCC) setting to restrict foundation agents to interact with
software through the most unified and standardized interface, i.e., using
screenshots as input and keyboard and mouse actions as output. We introduce
Cradle, a modular and flexible LMM-powered framework, as a preliminary attempt
towards GCC. Enhanced by six key modules, Cradle can understand input
screenshots and output executable code for low-level keyboard and mouse control
after high-level planning, so that Cradle can interact with any software and
complete long-horizon complex tasks without relying on any built-in APIs.
Experimental results show that Cradle exhibits remarkable generalizability and
impressive performance across four previously unexplored commercial video
games, five software applications, and a comprehensive benchmark, OSWorld.
Cradle is the first to enable foundation agents to follow the main storyline
and complete 40-minute-long real missions in the complex AAA game Red Dead
Redemption 2 (RDR2). Cradle can also create a city of a thousand people in
Cities: Skylines, farm and harvest parsnips in Stardew Valley, and trade and
bargain with a maximal weekly total profit of 87% in Dealer's Life 2. Cradle
can not only operate daily software, like Chrome, Outlook, and Feishu, but also
edit images and videos using Meitu and CapCut. Cradle greatly extends the reach
of foundation agents by enabling the easy conversion of any software,
especially complex games, into benchmarks to evaluate agents' various abilities
and facilitate further data collection, thus paving the way for generalist
agents.
### 🌟 论文解读 | Cradle：迈向通用计算机控制的基石

### 📌 背景痛点/本文动机
现有的基础智能体在特定场景中取得了成功，但在泛化到各种虚拟场景时仍然面临挑战。这主要是因为环境封装的巨大差异，包括手动设计的观察和动作空间。为了解决这个问题，本文提出了通用计算机控制（GCC）设置，限制基础智能体通过最统一和标准化的接口与软件进行交互，即使用屏幕截图作为输入和键盘鼠标操作作为输出。

### 🚀 核心方法
💡 创新点1：GCC设置
本文提出了GCC设置，旨在让基础智能体通过统一的接口与软件进行交互，从而提高其泛化能力。GCC设置要求智能体仅通过屏幕截图作为输入和键盘鼠标操作作为输出与软件进行交互。

💡 创新点2：Cradle框架
为了实现GCC设置，本文提出了Cradle框架，这是一个模块化和灵活的LMM（大型多模态模型）驱动框架。Cradle框架由六个关键模块组成：信息收集、自我反思、任务推理、技能管理、动作规划和记忆。这些模块协同工作，使Cradle能够理解输入屏幕截图，并在高级规划后输出可执行的代码，以进行低级键盘和鼠标控制。这样，Cradle可以与任何软件进行交互，并完成长期复杂的任务，而无需依赖任何内置API。

### 📈 实验结果
实验结果表明，Cradle在四个以前未探索过的商业视频游戏、五个软件应用程序和一个全面的基准测试OSWorld中表现出显著的泛化能力和令人印象深刻的性能。Cradle是第一个能够使基础智能体遵循复杂AAA游戏《荒野大镖客救赎2》（RDR2）的主线剧情并完成40分钟长的真实任务的框架。此外，Cradle还可以在《城市：天际线》中创建一个拥有千人的城市，在《星露谷物语》中种植和收获欧芹，以及在《经销商生活2》中以87%的最大每周总利润进行交易和讨价还价。Cradle不仅可以操作日常软件，如Chrome、Outlook和飞书，还可以使用美图和剪映编辑图像和视频。

### 💬 可借鉴之处
Cradle框架为通用计算机控制提供了一个有前景的解决方案。其模块化和灵活的设计使其能够适应各种环境和任务。此外，Cradle框架的实验结果表明，它能够在复杂的虚拟环境中表现出色，并具有显著的泛化能力。这些发现为开发更强大的基础智能体和推动通用人工智能（AGI）的发展提供了有价值的见解。

## using-game-play-to-investigate-multimodal-and-conversational-grounding-in-large-multimodal-models
### Abstract
While the situation has improved for text-only models, it again seems to be
the case currently that multimodal (text and image) models develop faster than
ways to evaluate them. In this paper, we bring a recently developed evaluation
paradigm from text models to multimodal models, namely evaluation through the
goal-oriented game (self) play, complementing reference-based and
preference-based evaluation. Specifically, we define games that challenge a
model's capability to represent a situation from visual information and align
such representations through dialogue. We find that the largest closed models
perform rather well on the games that we define, while even the best
open-weight models struggle with them. On further analysis, we find that the
exceptional deep captioning capabilities of the largest models drive some of
the performance. There is still room to grow for both kinds of models, ensuring
the continued relevance of the benchmark.
### 🌟 论文解读 | 游戏化评估：探究大型多模态模型中的多模态和对话式接地

### 📌 背景痛点/本文动机
随着大型多模态模型（LMMs）的快速发展，现有的评估方法主要依赖于参考式评估，难以全面评估模型在复杂场景下的交互能力。本文旨在探索一种新的评估范式，即通过目标导向的游戏（自我）玩法来评估多模态模型，以补充现有的参考式和偏好式评估方法。

### 🚀 核心方法
💡 创新点1：将游戏化评估范式应用于多模态模型
本文借鉴了文本模型中新兴的游戏化评估方法，并将其应用于多模态模型。通过定义三种对话游戏（参考游戏、图像比较游戏和导航游戏），挑战模型从视觉信息中构建情境模型并通过对话进行对齐的能力。

💡 创新点2：构建多模态游戏框架
本文使用 clemgame/clembench 框架来实现游戏化评估。该框架通过自然语言提示模板来定义游戏目标，并通过程序化的游戏大师来控制游戏流程和评分规则。

### 📈 实验结果
实验结果表明，大型闭源模型在本文定义的游戏中表现良好，而即使是最好的开源模型也难以应对这些挑战。进一步分析发现，大型模型在深度图像描述方面的出色能力推动了部分性能提升。这表明，无论是闭源模型还是开源模型，都仍有很大的发展空间。

### 💬 可借鉴之处
本文提出的游戏化评估方法为多模态模型的评估提供了新的思路，可以帮助研究人员更全面地评估模型在复杂场景下的交互能力。此外，本文构建的多模态游戏框架也为其他研究人员提供了可复现的实验平台。

### 📚 参考文献
* Chalamalasetti, V., et al. (2023). clemgame: A framework for evaluating language models through goal-oriented games. arXiv preprint arXiv:2304.01213.
* Chiang, D., et al. (2024). Chatbot Arena: A preference-based evaluation platform for conversational agents. arXiv preprint arXiv:2403.04528.
* Qiao, Y., et al. (2023). Evaluating language models through interactive games. arXiv preprint arXiv:2303.08574.

## towards-efficient-llm-grounding-for-embodied-multi-agent-collaboration
### Abstract
Grounding the reasoning ability of large language models (LLMs) for embodied
tasks is challenging due to the complexity of the physical world. Especially,
LLM planning for multi-agent collaboration requires communication of agents or
credit assignment as the feedback to re-adjust the proposed plans and achieve
effective coordination. However, existing methods that overly rely on physical
verification or self-reflection suffer from excessive and inefficient querying
of LLMs. In this paper, we propose a novel framework for multi-agent
collaboration that introduces Reinforced Advantage feedback (ReAd) for
efficient self-refinement of plans. Specifically, we perform critic regression
to learn a sequential advantage function from LLM-planned data, and then treat
the LLM planner as an optimizer to generate actions that maximize the advantage
function. It endows the LLM with the foresight to discern whether the action
contributes to accomplishing the final task. We provide theoretical analysis by
extending advantage-weighted regression in reinforcement learning to
multi-agent systems. Experiments on Overcooked-AI and a difficult variant of
RoCoBench show that ReAd surpasses baselines in success rate, and also
significantly decreases the interaction steps of agents and query rounds of
LLMs, demonstrating its high efficiency for grounding LLMs. More results are
given at https://read-llm.github.io/.
### 🌟 论文解读 | 提升大型语言模型在具身多智能体协作中的推理能力

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在处理具身任务时，由于物理世界的复杂性，其推理能力难以落地。特别是在多智能体协作中，LLM 规划需要智能体之间的通信或信用分配作为反馈，以调整计划并实现有效协调。然而，现有方法过度依赖物理验证或自我反思，导致对 LLM 的查询过多且效率低下。

### 🚀 核心方法
本文提出了一种名为 ReAd（Reinforced Advantage feedback）的框架，用于多智能体协作，以实现高效的自适应计划。具体来说，ReAd 通过以下步骤实现：

1. **优势函数学习**：使用 LLM 规划数据，通过批评回归学习一个序列优势函数。
2. **LLM 规划器优化**：将 LLM 规划器视为优化器，生成最大化优势函数的动作。
3. **前瞻性推理**：赋予 LLM 前瞻性，使其能够判断动作是否有助于完成任务。

### 📈 实验结果
在 Overcooked-AI 和 RoCoBench 的困难变体上进行实验，结果表明 ReAd 在成功率方面优于基线方法，并且显著减少了智能体的交互步骤和 LLM 的查询轮次，证明了其在落地 LLM 方面的高效性。

### 💬 可借鉴之处
ReAd 框架为 LLM 在具身多智能体协作中的推理能力落地提供了一种高效的方法。其核心思想是将 LLM 规划器视为优化器，并通过优势函数引导其生成更有效的动作。这种方法可以应用于各种需要 LLM 协作的场景，例如机器人协作、虚拟现实等。

## a-pilot-study-on-teacher-facing-real-time-classroom-game-dashboards
### Abstract
Educational games are an increasingly popular teaching tool in modern
classrooms. However, the development of complementary tools for teachers
facilitating classroom gameplay is lacking. We present the results of a
participatory design process for a teacher-facing, real-time game data
dashboard. This two-phase process included a workshop to elicit teachers'
requirements for such a tool, and a pilot study of our dashboard prototype. We
analyze post-gameplay survey and interview data to understand teachers'
experiences with the tool in terms of evidence of co-design, feasibility, and
effectiveness. Our results indicate the participatory design yielded a tool
both useful for and usable by teachers within the context of a real class
gameplay session. We advocate for the continued development of data-driven
teacher tools to improve the effectiveness of games deployed in the classroom.
### 🌟 论文解读 | 教育游戏中的实时课堂游戏仪表板：教师视角的探索

### 📌 背景痛点/本文动机
教育游戏在现代课堂中越来越受欢迎，但缺乏辅助教师进行课堂游戏的教学工具。教师通常只能通过查看每个学生的电脑屏幕来了解他们的游戏情况，这不仅耗时，而且可能无法全面了解游戏细节，从而影响教学效果。

### 🚀 核心方法
本文提出了一种参与式设计方法，通过两个阶段的研究来开发一个面向教师的实时游戏数据仪表板：
1. **参与式设计工作坊**：与教师和其他利益相关者合作，通过设计活动收集他们对仪表板的需求和期望。
2. **原型开发和试点研究**：根据工作坊的结果开发仪表板原型，并在多个课堂环境中进行试点测试，收集教师反馈。

### 📈 实验结果
试点研究表明，参与式设计过程产生了对教师有用且易于使用的工具。教师反馈表明，仪表板帮助他们更好地理解游戏和学生的游戏情况，提高了他们对游戏教学的支持能力。

### 💬 可借鉴之处
本文的研究结果表明，参与式设计是开发教师支持工具的有效方法。未来可以进一步扩展仪表板的功能，包括班级概述和总结数据，以提高其可用性和有效性。此外，还可以探索如何将仪表板与其他教育技术工具集成，以更好地支持游戏教学。

## code-generation-with-alphacodium--from-prompt-engineering-to-flow-engineering
### Abstract
Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
https://github.com/Codium-ai/AlphaCodium
### 🌟 论文解读 | AlphaCodium：从提示工程到流程工程，提升代码生成性能

### 📌 背景痛点/本文动机
代码生成问题与常见的自然语言问题不同，它需要匹配目标语言的精确语法，识别正常路径和边缘情况，关注问题规范中的许多小细节，并解决其他代码特定的问题和要求。因此，许多在自然语言生成中成功的优化和技巧可能对代码任务无效。本文提出了一种新的代码生成方法，称为AlphaCodium，它是一种基于测试的多阶段、代码导向的迭代流程，旨在提高大型语言模型（LLMs）在代码问题上的性能。

### 🚀 核心方法
💡 创新点1：测试导向的迭代流程
AlphaCodium的核心是迭代流程，其中生成的代码会反复运行并针对输入输出测试进行修复。这种方法允许模型逐步改进其解决方案，直到找到正确的答案。

💡 创新点2：多阶段处理
AlphaCodium流程分为两个主要阶段：预处理阶段和代码迭代阶段。在预处理阶段，模型会对问题进行自然语言推理，例如生成问题反思和公共测试推理。在代码迭代阶段，模型会生成代码解决方案，并在公共和AI生成的测试上进行迭代和修复。

💡 创新点3：代码导向的设计概念
AlphaCodium还采用了多种代码导向的设计概念，例如YAML结构化输出、通过项目符号分析进行语义推理、生成模块化代码、软决策和双重验证、鼓励探索以及测试锚点。这些概念有助于提高代码生成的质量和效率。

### 📈 实验结果
在CodeContests数据集上进行的实验表明，AlphaCodium流程显著提高了LLMs在代码问题上的性能。例如，GPT-4在验证集上的准确率（pass@5）从使用单个精心设计的直接提示的19%提高到使用AlphaCodium流程的44%。

### 💬 可借鉴之处
AlphaCodium的许多原则和最佳实践可以广泛应用于一般的代码生成任务。例如，使用结构化输出、生成模块化代码、通过项目符号分析进行语义推理、软决策和双重验证、鼓励探索以及测试锚点等技术都可以帮助提高代码生成的质量和效率。

### 📚 总结
AlphaCodium是一种创新的代码生成方法，它通过测试导向的迭代流程和多阶段处理，显著提高了LLMs在代码问题上的性能。该方法还采用了多种代码导向的设计概念，进一步提高了代码生成的质量和效率。AlphaCodium的许多原则和最佳实践可以广泛应用于一般的代码生成任务，为代码生成领域的研究和应用提供了新的思路和方向。

## self-playing-adversarial-language-game-enhances-llm-reasoning
### Abstract
We explore the potential of self-play training for large language models
(LLMs) in a two-player adversarial language game called Adversarial Taboo. In
this game, an attacker and a defender communicate around a target word only
visible to the attacker. The attacker aims to induce the defender to speak the
target word unconsciously, while the defender tries to infer the target word
from the attacker's utterances. To win the game, both players must have
sufficient knowledge about the target word and high-level reasoning ability to
infer and express in this information-reserved conversation. Hence, we are
curious about whether LLMs' reasoning ability can be further enhanced by
Self-Playing this Adversarial language Game (SPAG). With this goal, we select
several open-source LLMs and let each act as the attacker and play with a copy
of itself as the defender on an extensive range of target words. Through
reinforcement learning on the game outcomes, we observe that the LLMs'
performances uniformly improve on a broad range of reasoning benchmarks.
Furthermore, iteratively adopting this self-play process can continuously
promote LLMs' reasoning abilities. The code is available at
https://github.com/Linear95/SPAG.
### 🌟 论文解读 | 自对抗语言游戏提升大型语言模型推理能力

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在自然语言处理领域取得了显著进展，但在推理能力方面仍面临挑战。现有的推理能力提升方法，如提示工程和工具调用，依赖于额外的数据收集和设计，且容易过拟合到特定任务。本文提出了一种新的训练策略，通过自对抗语言游戏（SPAG）来提升LLMs的推理能力。

### 🚀 核心方法
💡 创新点1：自对抗语言游戏（Adversarial Taboo）
本文选择了一种名为Adversarial Taboo的自对抗语言游戏，其中攻击者和防御者围绕一个目标词进行对话。攻击者试图诱导防御者无意识地说出目标词，而防御者则试图从攻击者的发言中推断出目标词。这种游戏需要玩家具备丰富的知识和高级推理能力，从而为LLMs的推理能力提升提供了良好的训练环境。

💡 创新点2：自玩训练（Self-Play）
本文提出了一种名为SPAG的自玩训练策略，通过让LLMs在Adversarial Taboo游戏中自我对抗，并通过强化学习来优化其策略。具体来说，LLMs首先通过模仿学习来学习游戏规则，然后进行自我对抗，并收集游戏回合。最后，LLMs通过强化学习来优化其策略，从而提升其推理能力。

### 📈 实验结果
实验结果表明，SPAG训练策略能够显著提升LLMs在多个推理基准测试上的性能。与传统的提示工程和工具调用方法相比，SPAG训练策略具有更高的效率和更广泛的应用范围。此外，SPAG训练策略还能够提升LLMs在Adversarial Taboo游戏中的胜率，表明其在游戏技能方面的提升。

### 💬 可借鉴之处
本文提出的SPAG训练策略为LLMs的推理能力提升提供了一种新的思路。通过设计各种自玩环境，LLMs可以不断学习和提升其推理能力，从而在更广泛的任务中取得更好的表现。此外，本文还提醒开发者在使用自玩训练策略时，需要注意潜在的安全风险，并进行充分的安全检查。

## predicting-outcomes-in-video-games-with-long-short-term-memory-networks
### Abstract
Forecasting winners in E-sports with real-time analytics has the potential to
further engage audiences watching major tournament events. However, making such
real-time predictions is challenging due to unpredictable variables within the
game involving diverse player strategies and decision-making. Our work attempts
to enhance audience engagement within video game tournaments by introducing a
real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)
based approach enables efficient predictions of win-lose outcomes by only using
the health indicator of each player as a time series. As a proof of concept, we
evaluate our model's performance within a classic, two-player arcade game,
Super Street Fighter II Turbo. We also benchmark our method against state of
the art methods for time series forecasting; i.e. Transformer models found in
large language models (LLMs). Finally, we open-source our data set and code in
hopes of furthering work in predictive analysis for arcade games.
### 🌟 论文解读 | 利用长短期记忆网络预测电子竞技比赛结果

### 📌 背景痛点/本文动机
随着电子竞技（Esports）的日益流行，观众对于实时比赛结果的预测产生了浓厚的兴趣。然而，由于游戏中的变量众多，包括玩家策略和决策的不确定性，实时预测比赛结果一直是一个挑战。本文旨在通过引入一种实时预测方法来增强观众在电子游戏锦标赛中的参与度。

### 🚀 核心方法
💡 创新点1：使用长短期记忆网络（LSTM）进行实时预测
本文提出了一种基于LSTM的实时预测方法，该方法仅使用每个玩家的健康指示器作为时间序列来预测胜负结果。这种方法能够有效地处理时间序列数据，并捕捉游戏中的动态变化。

💡 创新点2：在经典的双人街机游戏《超级街头霸王II Turbo》中评估模型性能
为了验证模型的有效性，本文在经典的双人街机游戏《超级街头霸王II Turbo》中评估了模型的性能。通过分析玩家健康指示器的变化，模型能够在比赛的不同阶段进行实时预测。

💡 创新点3：与大型语言模型中的Transformer模型进行基准测试
为了进一步验证模型的有效性，本文将LSTM模型与大型语言模型中的Transformer模型进行了基准测试。结果表明，LSTM模型在预测比赛结果方面表现出了更高的准确率。

### 📈 实验结果
实验结果表明，LSTM模型在预测比赛结果方面表现出了较高的准确率。在比赛的不同阶段（25%，75%，95%），LSTM模型的ROC-AUC分数均高于Transformer模型。此外，LSTM模型的训练时间也相对较短，更适合实时预测场景。

### 💬 可借鉴之处
本文提出的基于LSTM的实时预测方法为电子竞技比赛结果的预测提供了一种新的思路。该方法可以应用于其他电子竞技游戏，并有助于提高观众在比赛中的参与度。此外，本文还开源了数据集和代码，为其他研究人员提供了进一步研究的便利。

## embodied-llm-agents-learn-to-cooperate-in-organized-teams
### Abstract
Large Language Models (LLMs) have emerged as integral tools for reasoning,
planning, and decision-making, drawing upon their extensive world knowledge and
proficiency in language-related tasks. LLMs thus hold tremendous potential for
natural language interaction within multi-agent systems to foster cooperation.
However, LLM agents tend to over-report and comply with any instruction, which
may result in information redundancy and confusion in multi-agent cooperation.
Inspired by human organizations, this paper introduces a framework that imposes
prompt-based organization structures on LLM agents to mitigate these problems.
Through a series of experiments with embodied LLM agents and human-agent
collaboration, our results highlight the impact of designated leadership on
team efficiency, shedding light on the leadership qualities displayed by LLM
agents and their spontaneous cooperative behaviors. Further, we harness the
potential of LLMs to propose enhanced organizational prompts, via a
Criticize-Reflect process, resulting in novel organization structures that
reduce communication costs and enhance team efficiency.
### 🌟 论文解读 | 基于大型语言模型的智能体在组织化团队中学习协作

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在推理、规划和决策方面的应用日益广泛，它们在多智能体系统中的协作潜力也逐渐显现。然而，LLM智能体在多智能体协作中存在过度报告和盲目服从指令的问题，这可能导致信息冗余和混乱。为了解决这些问题，本文提出了一种基于提示的组织结构框架，旨在提高LLM智能体在多智能体系统中的协作效率。

### 🚀 核心方法
💡 创新点1：设计了一种新型的多LLM智能体架构，支持≥3个智能体在物理/模拟环境中进行灵活的通信和协作。
💡 创新点2：开发了一种基于LLMs的“批评-反思”框架，用于自动优化组织提示，从而生成更有效的组织结构。

### 📈 实验结果
实验结果表明，具有指定领导者的层次化组织结构能够显著提高团队效率，并且LLM智能体能够通过通信动态地选举和调整领导者。此外，通过“批评-反思”框架，LLM智能体能够自发地形成新颖、有效的团队结构，从而降低通信成本并提高团队效率。

### 💬 可借鉴之处
本文提出的基于提示的组织结构框架和“批评-反思”框架为LLM智能体在多智能体系统中的协作提供了新的思路和方法。这些方法可以应用于各种场景，例如自动驾驶网络、无人机群等，以提高智能体系统的协作效率和性能。

## can-llm-augmented-autonomous-agents-cooperate---an-evaluation-of-their-cooperative-capabilities-through-melting-pot
### Abstract
As the field of AI continues to evolve, a significant dimension of this
progression is the development of Large Language Models and their potential to
enhance multi-agent artificial intelligence systems. This paper explores the
cooperative capabilities of Large Language Model-augmented Autonomous Agents
(LAAs) using the well-known Meltin Pot environments along with reference models
such as GPT4 and GPT3.5. Preliminary results suggest that while these agents
demonstrate a propensity for cooperation, they still struggle with effective
collaboration in given environments, emphasizing the need for more robust
architectures. The study's contributions include an abstraction layer to adapt
Melting Pot game scenarios for LLMs, the implementation of a reusable
architecture for LLM-mediated agent development - which includes short and
long-term memories and different cognitive modules, and the evaluation of
cooperation capabilities using a set of metrics tied to the Melting Pot's
"Commons Harvest" game. The paper closes, by discussing the limitations of the
current architectural framework and the potential of a new set of modules that
fosters better cooperation among LAAs.
### 🌟 论文解读 | 大型语言模型增强的自主智能体能否合作？

### 📌 背景痛点/本文动机
随着人工智能领域的不断发展，大型语言模型（LLMs）在多智能体人工智能系统中的应用潜力日益凸显。然而，当前的研究对于LLM增强的自主智能体（LAAs）的合作能力探讨相对较少。本文旨在评估LAAs在合作方面的能力，并探讨如何提升其合作效果。

### 🚀 核心方法
💡 创新点1：将Melting Pot场景转换为文本表示，以便LLMs可以轻松操作。
💡 创新点2：实现了一个可重用的LAAs开发架构，该架构包括短期和长期记忆以及不同的认知模块，如感知、规划、反思和行动。
💡 创新点3：通过自然语言定义“个性”，使智能体明确是否应该合作。
💡 创新点4：使用Melting Pot的“公共资源收获”游戏来评估LLM中介智能体的合作能力。

### 📈 实验结果
实验结果表明，尽管智能体表现出合作的倾向，但它们在特定环境中仍然难以有效协作。这突显了需要更强大的架构来促进LAAs之间的合作。

### 💬 可借鉴之处
本文的研究结果表明，为了提高LAAs的合作能力，需要更全面的架构，包括增强的理解能力、有效的沟通机制、可信的承诺机制以及明确的社交结构或制度。此外，本文提出的改进架构，包括理解模块、沟通模块、宪法模块和声誉系统，为未来研究提供了有价值的参考。

## envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents
### Abstract
Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller RL agents learn useful skills that they are weak at? We propose EnvGen,
a novel framework to address this question. We first prompt an LLM to generate
training environments by giving it the task description and simulator
objectives that the agents should learn and then asking it to generate a set of
environment configurations (e.g., different terrains, items initially given to
agents, etc.). Next, we train a small RL agent in a mixture of the original and
LLM-generated environments. Then, we enable the LLM to continuously adapt the
generated environments to progressively improve the skills that the agent is
weak at, by providing feedback to the LLM in the form of the agent's
performance. We demonstrate the usefulness of EnvGen with comprehensive
experiments in Crafter and Heist environments. We find that a small RL agent
trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and
learns long-horizon tasks significantly faster. We also show that using an LLM
to adapt environments dynamically outperforms curriculum learning approaches
and how the environments are adapted to help improve RL agents' weaker skills
over time. Additionally, EnvGen is substantially more efficient as it only uses
a small number of LLM calls (e.g., 4 in total), whereas LLM agents require
thousands of calls. Lastly, we present detailed ablation studies for EnvGen
design choices.
### 🌟 论文解读 | EnvGen：利用LLM生成和适应环境，训练具身智能体

### 📌 背景痛点/本文动机
随着具身智能体在开放世界游戏中的兴起，如何让智能体快速学习并掌握各种技能成为了一个挑战。传统的强化学习（RL）方法在处理长时任务时效率低下，而直接使用大型语言模型（LLM）作为智能体虽然性能强大，但调用成本高昂。本文提出了一种新的框架EnvGen，旨在利用LLM的推理能力来生成和适应训练环境，帮助小型RL智能体学习它们不擅长的技能。

### 🚀 核心方法
💡 创新点1：LLM生成环境
EnvGen首先通过向LLM提供任务描述和模拟器目标，让LLM生成一系列环境配置，例如不同的地形、初始物品等。这些环境可以并行训练智能体，使其快速学习不同的技能。

💡 创新点2：LLM适应环境
EnvGen通过将智能体在原始环境中的表现反馈给LLM，让LLM不断调整生成的环境，使其更加专注于智能体不擅长的技能。这种动态适应过程可以帮助智能体逐步提高其技能水平。

### 📈 实验结果
在Crafter和Heist游戏环境中进行的实验表明，使用EnvGen训练的小型RL智能体在性能上超过了包括GPT-4在内的SOTA方法，并且学习长时任务的速度显著提高。此外，EnvGen的效率也远高于直接使用LLM作为智能体的方法，因为它只需要很少的LLM调用次数。

### 💬 可借鉴之处
EnvGen提供了一种利用LLM推理能力来提高RL智能体性能的有效方法。它可以应用于各种开放世界游戏和模拟器，帮助智能体快速学习并掌握各种技能。此外，EnvGen的动态适应机制也为RL智能体的训练提供了一种新的思路。

## minedreamer--learning-to-follow-instructions-via-chain-of-imagination-for-simulated-world-control
### Abstract
It is a long-lasting goal to design a generalist-embodied agent that can
follow diverse instructions in human-like ways. However, existing approaches
often fail to steadily follow instructions due to difficulties in understanding
abstract and sequential natural language instructions. To this end, we
introduce MineDreamer, an open-ended embodied agent built upon the challenging
Minecraft simulator with an innovative paradigm that enhances
instruction-following ability in low-level control signal generation.
Specifically, MineDreamer is developed on top of recent advances in Multimodal
Large Language Models (MLLMs) and diffusion models, and we employ a
Chain-of-Imagination (CoI) mechanism to envision the step-by-step process of
executing instructions and translating imaginations into more precise visual
prompts tailored to the current state; subsequently, the agent generates
keyboard-and-mouse actions to efficiently achieve these imaginations, steadily
following the instructions at each step. Extensive experiments demonstrate that
MineDreamer follows single and multi-step instructions steadily, significantly
outperforming the best generalist agent baseline and nearly doubling its
performance. Moreover, qualitative analysis of the agent's imaginative ability
reveals its generalization and comprehension of the open world.
### 🌟 论文解读 | MineDreamer：基于想象链的模拟世界控制指令跟随

### 📌 背景痛点/本文动机
在人工智能领域，设计一个能够以人类方式理解和执行多样化指令的通用型具身智能体一直是长期目标。然而，现有的方法往往难以稳定地遵循指令，尤其是在理解和执行抽象和顺序的自然语言指令方面存在困难。

### 🚀 核心方法
💡 创新点1：引入“想象链”（Chain-of-Imagination, CoI）机制
MineDreamer 通过 CoI 机制，使智能体能够根据指令和当前状态逐步想象并执行指令。这种方法模拟了人类在解决问题时，根据当前状态逐步想象下一步目标的过程。

💡 创新点2：多模态大型语言模型（MLLM）增强的扩散模型
MineDreamer 使用 MLLM 增强的扩散模型来生成包含物理规则和环境理解的想象图像，这些图像作为更精确的视觉提示，引导智能体生成低级控制信号。

💡 创新点3：目标漂移收集方法
为了训练 Imaginator，MineDreamer 使用了目标漂移收集方法来收集大量具身数据，帮助 Imaginator 理解如何逐步完成指令以及如何重复完成指令。

### 📈 实验结果
MineDreamer 在执行单步和多步指令方面表现出色，显著优于最佳通用型智能体基线，性能几乎翻倍。此外，对智能体想象能力的定性分析表明，它能够理解和适应开放世界的环境。

### 💬 可借鉴之处
MineDreamer 的 CoI 机制为解决指令跟随问题提供了一种新颖的方法，其 MLLM 增强的扩散模型和目标漂移收集方法也为具身智能体的发展提供了新的思路。此外，MineDreamer 的成功也表明，具身智能体在开放世界环境中具有巨大的潜力。

## scaling-instructable-agents-across-many-simulated-worlds
### Abstract
Building embodied AI systems that can follow arbitrary language instructions
in any 3D environment is a key challenge for creating general AI. Accomplishing
this goal requires learning to ground language in perception and embodied
actions, in order to accomplish complex tasks. The Scalable, Instructable,
Multiworld Agent (SIMA) project tackles this by training agents to follow
free-form instructions across a diverse range of virtual 3D environments,
including curated research environments as well as open-ended, commercial video
games. Our goal is to develop an instructable agent that can accomplish
anything a human can do in any simulated 3D environment. Our approach focuses
on language-driven generality while imposing minimal assumptions. Our agents
interact with environments in real-time using a generic, human-like interface:
the inputs are image observations and language instructions and the outputs are
keyboard-and-mouse actions. This general approach is challenging, but it allows
agents to ground language across many visually complex and semantically rich
environments while also allowing us to readily run agents in new environments.
In this paper we describe our motivation and goal, the initial progress we have
made, and promising preliminary results on several diverse research
environments and a variety of commercial video games.
### 🌟 论文解读 | SIMA：跨越多个模拟世界的可指令智能体

### 📌 背景痛点/本文动机
尽管大型语言模型在自然语言处理方面取得了显著进展，但将它们与我们所处的具身世界连接起来仍然是一个巨大的挑战。现代AI在语言能力方面表现出色，但在感知和行动方面却远不及人类。为了克服这一挑战，我们需要开发能够理解语言指令并在复杂环境中执行任务的具身AI系统。

### 🚀 核心方法
SIMA项目旨在构建一个能够遵循任意语言指令并在任何虚拟3D环境中通过键盘和鼠标操作进行行动的系统。该项目的主要创新点包括：

💡 **多环境训练**：SIMA在多种虚拟3D环境中训练智能体，包括定制的研究环境和开放式的商业视频游戏。这种多样化的训练环境有助于智能体学习更通用的技能，并提高其在不同场景下的适应能力。

💡 **语言驱动**：SIMA的智能体通过自然语言指令进行训练，这有助于它们学习更通用的语言表示和抽象，并提高学习效率。

💡 **人类界面**：SIMA的智能体使用与人类相同的键盘和鼠标控制方式与虚拟环境交互，这有助于它们模仿人类行为，并提高其在新环境中的迁移能力。

💡 **数据收集**：SIMA收集了大量由人类专家生成的游戏数据，包括视频、语言指令、记录的动作和各种注释。这些数据被用于训练智能体，并提高其学习效率。

💡 **评估方法**：SIMA使用多种评估方法来评估智能体的性能，包括动作日志概率、静态视觉输入、地面实况评估、光学字符识别和人工评估。这些评估方法有助于全面评估智能体的性能，并确保其能够遵循语言指令。

### 📈 实验结果
SIMA项目取得了初步的成功，智能体能够在多种环境中完成各种任务，包括导航、资源收集、对象管理和战斗等。实验结果表明，SIMA的智能体在环境泛化、零样本迁移和语言条件行为方面表现出色。

### 💬 可借鉴之处
SIMA项目为具身AI研究提供了一个重要的平台，其创新方法和技术可以应用于其他领域，例如机器人、虚拟现实和增强现实等。此外，SIMA项目的研究成果也有助于我们更好地理解语言与感知和行动之间的关系，并为通用人工智能的发展提供新的思路。

## hierarchical-auto-organizing-system-for-open-ended-multi-agent-navigation
### Abstract
Due to the dynamic and unpredictable open-world setting, navigating complex
environments in Minecraft poses significant challenges for multi-agent systems.
Agents must interact with the environment and coordinate their actions with
other agents to achieve common objectives. However, traditional approaches
often struggle to efficiently manage inter-agent communication and task
distribution, crucial for effective multi-agent navigation. Furthermore,
processing and integrating multi-modal information (such as visual, textual,
and auditory data) is essential for agents to comprehend their goals and
navigate the environment successfully and fully. To address this issue, we
design the HAS framework to auto-organize groups of LLM-based agents to
complete navigation tasks. In our approach, we devise a hierarchical
auto-organizing navigation system, which is characterized by 1) a hierarchical
system for multi-agent organization, ensuring centralized planning and
decentralized execution; 2) an auto-organizing and intra-communication
mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal
information platform, facilitating multi-modal perception to perform the three
navigation tasks with one system. To assess organizational behavior, we design
a series of navigation tasks in the Minecraft environment, which includes
searching and exploring. We aim to develop embodied organizations that push the
boundaries of embodied AI, moving it towards a more human-like organizational
structure.
### 🌟 论文解读 | HAS：开放世界多智能体导航的分层自组织系统

### 📌 背景痛点/本文动机
在开放世界的环境中，如Minecraft，多智能体系统面临着复杂的导航挑战。传统的导航方法往往难以有效地管理智能体之间的通信和任务分配，这对于有效的多智能体导航至关重要。此外，处理和整合多模态信息（如视觉、文本和听觉数据）对于智能体理解其目标并在环境中成功导航至关重要。

### 🚀 核心方法
💡 创新点1：分层自组织导航系统
HAS框架设计了一个分层自组织导航系统，该系统具有以下特点：
1. 分层系统：确保集中式规划和分布式执行，提高导航效率。
2. 自组织机制：根据子任务动态调整关键角色和行动组，并保持组间通信，确保高效协作。
3. 多模态信息平台：促进多模态感知，使系统能够处理图像、对象和音频目标，并执行搜索和探索等导航任务。

💡 创新点2：多模态语言模型
HAS框架使用了多模态语言模型（MLM），包括管理者和执行者两种类型的模型。这些模型具有不同的功能，如规划、描述、评估和部署，以实现集中式规划和分布式执行。

💡 创新点3：多模态记忆
HAS框架还引入了多模态记忆机制，用于存储和检索多模态信息，从而提高规划准确性和一致性。通过检索增强生成（RAG）和多模态检索（MMR）技术，HAS能够有效地利用历史交互反馈和经验，生成更准确的计划。

### 📈 实验结果
在Minecraft环境中进行的实验表明，HAS框架在多模态目标搜索、连续块搜索和地图探索等任务中取得了最先进的性能。与基线方法相比，HAS在导航效率、成功率和探索能力方面均有显著提升。

### 💬 可借鉴之处
HAS框架为开放世界多智能体导航提供了一种新的思路和方法。其分层自组织结构、多模态语言模型和多模态记忆机制等创新点，对于提高多智能体系统的自主性、效率和适应性具有重要意义。此外，HAS框架还可以应用于其他需要多智能体协作的场景，如机器人协同、虚拟现实等。

## sotopia-$π$--interactive-learning-of-socially-intelligent-language-agents
### Abstract
Humans learn social skills through both imitation and social interaction.
This social learning process is largely understudied by existing research on
building language agents. Motivated by this gap, we propose an interactive
learning method, SOTOPIA-$\pi$, improving the social intelligence of language
agents. This method leverages behavior cloning and self-reinforcement training
on filtered social interaction data according to large language model (LLM)
ratings. We show that our training method allows a 7B LLM to reach the social
goal completion ability of an expert model (GPT-4-based agent), while improving
the safety of language agents and maintaining general QA ability on the MMLU
benchmark. We also find that this training paradigm uncovers some difficulties
in LLM-based evaluation of social intelligence: LLM-based evaluators
overestimate the abilities of the language agents trained specifically for
social interaction.
### 🌟 论文解读 | SOTOPIA-$π$: 提升语言模型社交智能的交互式学习方法

### 📌 背景痛点/本文动机
人类通过模仿和社会互动学习社交技能，但现有研究在构建语言模型时对此过程关注不足。本文提出了一种交互式学习方法 SOTOPIA-$π$，旨在提升语言模型的社交智能。

### 🚀 核心方法
💡 创新点1：利用行为克隆和自我强化训练
SOTOPIA-$π$ 利用行为克隆和自我强化训练，在经过大型语言模型（LLM）评分过滤的社会互动数据上进行训练。行为克隆从具有强大社交技能的专家模型（如 GPT-4）的行为轨迹中学习，而自我强化则从模型自身的高评分行为中学习。

💡 创新点2：LLM 评分作为训练信号
SOTOPIA-$π$ 使用 GPT-4 对社交互动中的积极行为进行评分，并将这些评分作为训练信号。这种方法无需人工参与，且具有高效和可扩展性。

### 📈 实验结果
实验结果表明，SOTOPIA-$π$ 可以显著提升语言模型的社交目标完成能力，使其接近专家模型（如 GPT-4）的性能。此外，该方法还能提高语言模型的安全性，并保持其在 MMLU 基准测试中的问答能力。

### 💬 可借鉴之处
SOTOPIA-$π$ 为提升语言模型的社交智能提供了一种有效的方法，并揭示了 LLM 评分在评估社交智能方面的局限性。未来研究可以探索在线强化学习、从人类数据中学习、更稳健的评估方法等方向，以进一步提升语言模型的社交智能。

## will-gpt-4-run-doom-
### Abstract
We show that GPT-4's reasoning and planning capabilities extend to the 1993
first-person shooter Doom. This large language model (LLM) is able to run and
play the game with only a few instructions, plus a textual
description--generated by the model itself from screenshots--about the state of
the game being observed. We find that GPT-4 can play the game to a passable
degree: it is able to manipulate doors, combat enemies, and perform pathing.
More complex prompting strategies involving multiple model calls provide better
results. While further work is required to enable the LLM to play the game as
well as its classical, reinforcement learning-based counterparts, we note that
GPT-4 required no training, leaning instead on its own reasoning and
observational capabilities. We hope our work pushes the boundaries on
intelligent, LLM-based agents in video games. We conclude by discussing the
ethical implications of our work.
### 🌟 论文解读 | GPT-4能否玩转经典射击游戏Doom？

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）如GPT-4的出现，它们在理解和执行指令方面的能力得到了显著提升。然而，LLM在复杂环境中的推理和规划能力仍然是一个未解之谜。本文旨在探索LLM在视频游戏中的智能代理应用，特别是评估GPT-4在经典射击游戏Doom中的表现。

### 🚀 核心方法
💡 创新点1：视觉输入与文本描述的结合
本文提出了一种新的方法，将GPT-4的视觉输入能力与文本描述相结合，以理解游戏状态。GPT-4V模型从游戏截图中生成游戏状态的文本描述，而GPT-4模型则根据这些描述和之前的行动历史来做出决策。

💡 创新点2：多层次规划策略
为了提高GPT-4在游戏中的表现，本文提出了多种规划策略，包括简单的指令、分步的关卡攻略、更细粒度的计划生成，以及基于多个专家意见的k-levels策略。这些策略旨在为GPT-4提供更多的上下文信息，以增强其推理和规划能力。

### 📈 实验结果
实验结果表明，GPT-4能够在一定程度上玩转Doom游戏，能够执行开门、战斗敌人、路径规划等基本操作。然而，GPT-4的推理深度有限，缺乏长期规划和记忆能力，例如，当敌人离开视野时，模型会忘记它们的存在。

### 💬 可借鉴之处
本文的研究结果表明，LLM在视频游戏中的应用具有巨大的潜力，但仍面临一些挑战。未来研究可以探索更精细的规划策略，以及如何提高LLM的推理深度和记忆能力。此外，本文也提醒我们，LLM技术的快速发展需要更加谨慎的评估和监管，以防止潜在的滥用风险。

## large-multimodal-agents--a-survey
### Abstract
Large language models (LLMs) have achieved superior performance in powering
text-based AI agents, endowing them with decision-making and reasoning
abilities akin to humans. Concurrently, there is an emerging research trend
focused on extending these LLM-powered AI agents into the multimodal domain.
This extension enables AI agents to interpret and respond to diverse multimodal
user queries, thereby handling more intricate and nuanced tasks. In this paper,
we conduct a systematic review of LLM-driven multimodal agents, which we refer
to as large multimodal agents ( LMAs for short). First, we introduce the
essential components involved in developing LMAs and categorize the current
body of research into four distinct types. Subsequently, we review the
collaborative frameworks integrating multiple LMAs , enhancing collective
efficacy. One of the critical challenges in this field is the diverse
evaluation methods used across existing studies, hindering effective comparison
among different LMAs . Therefore, we compile these evaluation methodologies and
establish a comprehensive framework to bridge the gaps. This framework aims to
standardize evaluations, facilitating more meaningful comparisons. Concluding
our review, we highlight the extensive applications of LMAs and propose
possible future research directions. Our discussion aims to provide valuable
insights and guidelines for future research in this rapidly evolving field. An
up-to-date resource list is available at
https://github.com/jun0wanan/awesome-large-multimodal-agents.
### 🌟 论文解读 | 大型多模态智能体：迈向通用人工智能的桥梁

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在文本处理任务中展现出卓越的性能，研究人员开始探索将这些模型应用于多模态领域，以构建能够理解和响应多模态用户查询的智能体。然而，现有的研究往往孤立地进行，缺乏对现有框架的总结和比较。本文旨在填补这一空白，对LLM驱动的多模态智能体（LMAs）进行系统性的综述。

### 🚀 核心方法
本文首先介绍了LMAs的核心组件，包括感知、规划、行动和记忆，并提出了一个新的分类框架，将现有研究分为四类：

* **类型I：使用闭源LLMs作为规划器，无长期记忆**。这类LMAs主要使用提示技术来引导闭源LLMs进行决策和规划，完成图像编辑、视觉定位和视觉问答等任务。
* **类型II：使用微调的LLMs作为规划器，无长期记忆**。这类LMAs通过收集多模态指令遵循数据或使用自指令来微调开源LLMs，使其具备决策、规划和工具调用的能力。
* **类型III：具有间接长期记忆的规划器**。这类LMAs的LLMs作为中央规划器，并配备长期记忆。规划器通过调用相关工具来访问和检索长期记忆，以增强推理和规划能力。
* **类型IV：具有原生长期记忆的规划器**。这类LMAs的LLMs直接与长期记忆交互，无需工具来访问长期记忆。例如，在Minecraft等开放世界环境中，这类LMAs能够完成超过200个不同的任务。

此外，本文还回顾了多模态智能体的协作框架，并探讨了评估LMAs性能的现有方法，包括主观评估和客观评估。为了解决现有研究中评估方法多样性的问题，本文建立了一个综合的评估框架，旨在标准化评估过程，促进更有意义的比较。

### 📈 实验结果
本文没有提供具体的实验结果，而是对现有研究进行了综述和分析。

### 💬 可借鉴之处
* **LMAs的框架设计**：本文提出的分类框架可以帮助研究人员更好地理解LMAs的不同类型和特点，并为构建新的LMAs提供参考。
* **LMAs的评估方法**：本文提出的综合评估框架可以为LMAs的性能评估提供标准和指导，促进LMAs的进一步发展。
* **LMAs的应用场景**：本文列举了LMAs在GUI自动化、机器人与具身AI、游戏开发、自动驾驶、视频理解、视觉生成与编辑、复杂视觉推理任务、音频编辑与生成等领域的应用，为LMAs的未来发展提供了方向。

### 🌈 未来展望
本文展望了LMAs的未来发展方向，包括：

* **框架设计**：从单个智能体和多个智能体两个角度出发，构建更加统一和协作的LMAs框架。
* **评估方法**：建立系统化和标准化的评估框架，并开发更贴近真实场景的评估数据集。
* **应用场景**：探索LMAs在更多领域的应用，例如人机交互、医疗健康、教育等。

总而言之，本文为LMAs的研究和应用提供了全面的概述和深入的见解，为推动LMAs的发展和应用奠定了基础。

## playing-nethack-with-llms--potential-&-limitations-as-zero-shot-agents
### Abstract
Large Language Models (LLMs) have shown great success as high-level planners
for zero-shot game-playing agents. However, these agents are primarily
evaluated on Minecraft, where long-term planning is relatively straightforward.
In contrast, agents tested in dynamic robot environments face limitations due
to simplistic environments with only a few objects and interactions. To fill
this gap in the literature, we present NetPlay, the first LLM-powered zero-shot
agent for the challenging roguelike NetHack. NetHack is a particularly
challenging environment due to its diverse set of items and monsters, complex
interactions, and many ways to die.
  NetPlay uses an architecture designed for dynamic robot environments,
modified for NetHack. Like previous approaches, it prompts the LLM to choose
from predefined skills and tracks past interactions to enhance decision-making.
Given NetHack's unpredictable nature, NetPlay detects important game events to
interrupt running skills, enabling it to react to unforeseen circumstances.
While NetPlay demonstrates considerable flexibility and proficiency in
interacting with NetHack's mechanics, it struggles with ambiguous task
descriptions and a lack of explicit feedback. Our findings demonstrate that
NetPlay performs best with detailed context information, indicating the
necessity for dynamic methods in supplying context information for complex
games such as NetHack.
### 🌟 论文解读 | LLMs 在 NetHack 中的潜力与局限性：零样本智能体

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在游戏领域展现出强大的规划能力，尤其是在 Minecraft 等游戏中。然而，这些模型在处理复杂、动态的环境时，如机器人环境，往往面临局限性。为了填补这一空白，本文提出了 NetPlay，一个基于 LLM 的零样本智能体，用于挑战性的 Rogue-like 游戏 NetHack。

### 🚀 核心方法
NetPlay 采用了一种专为动态机器人环境设计的架构，并针对 NetHack 进行了修改。它通过提示 LLM 从预定义的技能中选择，并跟踪过去的交互来增强决策。NetPlay 还能够检测重要的游戏事件，以便在出现意外情况时中断正在执行的技能。

### 📈 实验结果
实验结果表明，NetPlay 在与 NetHack 的机制交互方面表现出色，但在处理模糊的任务描述和缺乏明确反馈时存在困难。NetPlay 在提供详细上下文信息的情况下表现最佳，这表明在 NetHack 等复杂游戏中，动态提供上下文信息的方法至关重要。

### 💬 可借鉴之处
NetPlay 的研究结果表明，LLMs 在游戏领域具有巨大的潜力，但仍然存在局限性。未来研究可以探索如何更好地利用 LLMs 的能力，例如通过动态提供上下文信息或使用机器学习来替代手工制作的组件。此外，NetPlay 的架构可以为其他复杂游戏的设计提供参考。

## q-cogni--an-integrated-causal-reinforcement-learning-framework
### Abstract
We present Q-Cogni, an algorithmically integrated causal reinforcement
learning framework that redesigns Q-Learning with an autonomous causal
structure discovery method to improve the learning process with causal
inference. Q-Cogni achieves optimal learning with a pre-learned structural
causal model of the environment that can be queried during the learning process
to infer cause-and-effect relationships embedded in a state-action space. We
leverage on the sample efficient techniques of reinforcement learning, enable
reasoning about a broader set of policies and bring higher degrees of
interpretability to decisions made by the reinforcement learning agent. We
apply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against
state-of-the-art reinforcement learning algorithms. We report results that
demonstrate better policies, improved learning efficiency and superior
interpretability of the agent's decision making. We also compare this approach
with traditional shortest-path search algorithms and demonstrate the benefits
of our causal reinforcement learning framework to high dimensional problems.
Finally, we apply Q-Cogni to derive optimal routing decisions for taxis in New
York City using the Taxi & Limousine Commission trip record data and compare
with shortest-path search, reporting results that show 85% of the cases with an
equal or better policy derived from Q-Cogni in a real-world domain.


## agent-pro--learning-to-evolve-via-policy-level-reflection-and-optimization
### Abstract
Large Language Models (LLMs) exhibit robust problem-solving capabilities for
diverse tasks. However, most LLM-based agents are designed as specific task
solvers with sophisticated prompt engineering, rather than agents capable of
learning and evolving through interactions. These task solvers necessitate
manually crafted prompts to inform task rules and regulate LLM behaviors,
inherently incapacitating to address complex dynamic scenarios e.g., large
interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent
with Policy-level Reflection and Optimization that can learn a wealth of
expertise from interactive experiences and progressively elevate its behavioral
policy. Specifically, it involves a dynamic belief generation and reflection
process for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
### 🌟 论文解读 | Agent-Pro：基于策略级反思和优化的学习进化

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在解决各种任务方面表现出强大的能力，但大多数基于LLMs的智能体都是为特定任务设计的，需要复杂的提示工程来告知任务规则和调节LLMs的行为。这使得它们难以应对复杂动态的场景，例如大型互动游戏。本文提出了一种名为Agent-Pro的LLM-based智能体，它具有策略级反思和优化能力，可以从互动经验中学习大量专业知识，并逐步提升其行为策略。

### 🚀 核心方法
💡 创新点1：策略级反思和优化
Agent-Pro通过策略级反思和优化来学习进化。它不仅反思过去的轨迹和信念，还通过深度优先搜索来优化策略，确保策略收益的持续提升。

💡 创新点2：信念感知决策过程
Agent-Pro采用信念感知决策过程，通过更新自身信念和世界信念来生成更合理的行为。它能够根据信念来预测行动，并在游戏结束后根据结果来反思和调整信念。

### 📈 实验结果
Agent-Pro在两个游戏（Blackjack和Texas Hold'em）中进行了评估，结果表明它能够学习并进化，在复杂和动态的场景中表现出色。与传统的LLMs和专门模型相比，Agent-Pro在游戏中的收益更高。

### 💬 可借鉴之处
Agent-Pro的设计理念和方法为构建能够学习和进化的LLM-based智能体提供了新的思路。其策略级反思和优化机制可以帮助智能体从互动经验中学习，并逐步提升其行为策略。此外，信念感知决策过程可以帮助智能体在不确定的场景中做出更合理的决策。这些方法可以应用于各种复杂的任务，例如商业谈判、安全监控等。

## pca-bench--evaluating-multimodal-large-language-models-in-perception-cognition-action-chain
### Abstract
We present PCA-Bench, a multimodal decision-making benchmark for evaluating
the integrated capabilities of Multimodal Large Language Models (MLLMs).
Departing from previous benchmarks focusing on simplistic tasks and individual
model capability, PCA-Bench introduces three complex scenarios: autonomous
driving, domestic robotics, and open-world games. Given task instructions and
diverse contexts, the model is required to seamlessly integrate multiple
capabilities of Perception, Cognition, and Action in a reasoning chain to make
accurate decisions. Moreover, PCA-Bench features error localization
capabilities, scrutinizing model inaccuracies in areas such as perception,
knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To
balance accuracy and efficiency in evaluation, we propose PCA-Eval, an
automatic evaluation protocol, and assess 10 prevalent MLLMs. The results
reveal significant performance disparities between open-source models and
powerful proprietary models like GPT-4 Vision. To address this, we introduce
Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing
instruction tuning examples in multimodal embodied environments. EIE generates
7,510 training examples in PCA-Bench and enhances the performance of
open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision
accuracy), thereby validating the effectiveness of EIE. Our findings suggest
that robust MLLMs like GPT4-Vision show promise for decision-making in embodied
agents, opening new avenues for MLLM research.
### 🌟 论文解读 | PCA-Bench：评估多模态大语言模型在感知-认知-行动链中的决策能力

### 📌 背景痛点/本文动机
随着多模态大语言模型（MLLMs）在处理复杂任务方面的能力日益增强，现有的评估基准往往只关注单个模型能力的评估，而忽略了模型在感知、认知和行动方面的综合能力。此外，现有的基准缺乏对模型错误进行定位的能力，这使得难以确定模型在哪些方面需要改进。

### 🚀 核心方法
💡 创新点1：PCA-Bench
本文提出了PCA-Bench，这是一个用于评估MLLMs在感知-认知-行动链中决策能力的多模态决策基准。PCA-Bench引入了三个复杂的场景：自动驾驶、家庭机器人和开放世界游戏。在这些场景中，模型需要根据任务指令和不同的上下文，无缝地整合感知、认知和行动的能力，以做出准确的决策。

💡 创新点2：PCA-Eval
为了平衡评估的准确性和效率，本文提出了PCA-Eval，这是一个自动评估协议。PCA-Eval利用LLMs强大的语义解析能力，根据数据注释中的锚点信息，自动进行错误定位。实验结果表明，PCA-Eval与人类评估结果具有高度的一致性，平均Kappa系数达到0.8+。

💡 创新点3：Embodied-Instruction-Evolution (EIE)
为了解决PCA-Bench数据集标注工作量大的问题，本文提出了Embodied-Instruction-Evolution (EIE)框架。EIE利用LLMs自动合成多模态具身环境中的指令调整示例，从而减少了人工劳动，并提高了PCA-Bench的多样性和可扩展性。

### 📈 实验结果
实验结果表明，GPT-4 Vision在感知、认知和行动方面都表现出色，超过了现有的开源MLLMs。EIE方法能够显著提高开源MLLMs的性能，在某些指标上甚至超过了GPT-4 Vision。PCA-Eval能够有效地定位模型错误，从而提高了模型评估的可靠性。

### 💬 可借鉴之处
本文提出的PCA-Bench和PCA-Eval为评估MLLMs的决策能力提供了一个新的基准和评估工具。EIE框架为自动合成多模态具身环境中的指令调整示例提供了一种有效的方法。本文的研究结果表明，强大的MLLMs在具身智能体中的决策能力具有很大的潜力，为MLLMs的研究开辟了新的方向。

## what-if-llms-have-different-world-views--simulating-alien-civilizations-with-llm-based-agents
### Abstract
This study introduces "CosmoAgent," an innovative artificial intelligence
system that utilizes Large Language Models (LLMs) to simulate complex
interactions between human and extraterrestrial civilizations. This paper
introduces a mathematical model for quantifying the levels of civilization
development and further employs a state transition matrix approach to evaluate
their trajectories. Through this methodology, our study quantitatively analyzes
the growth trajectories of civilizations, providing insights into future
decision-making at critical points of growth and saturation. Furthermore, this
paper acknowledges the vast diversity of potential living conditions across the
universe, which could foster unique cosmologies, ethical codes, and worldviews
among different civilizations. Recognizing the Earth-centric bias inherent in
current LLM designs, we propose the novel concept of using LLM agents with
diverse ethical paradigms and simulating interactions between entities with
distinct moral principles. This innovative research not only introduces a novel
method for comprehending potential inter-civilizational dynamics but also holds
practical value in enabling entities with divergent value systems to
strategize, prevent conflicts, and engage in games under conditions of
asymmetric information. The accompanying code is available at
https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.
### 🌟 论文解读 | 用大型语言模型模拟外星文明：探索宇宙中的互动与冲突

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，其在模拟复杂社会动态方面的潜力日益凸显。然而，现有的LLMs设计往往带有地球中心主义的偏见，难以全面模拟外星文明的多样性和独特性。本文旨在通过引入具有不同伦理范式和道德原则的LLM代理，模拟人类与外星文明之间的复杂互动，从而为理解潜在星际动态提供新的视角。

### 🚀 核心方法
💡 创新点1：CosmoAgent多智能体系统
本文提出了CosmoAgent，一个基于LLMs的多智能体系统，用于模拟宇宙中不同文明之间的互动。CosmoAgent通过模拟文明的决策过程，包括选择隐藏、战斗或合作，来探索文明发展的轨迹和潜在冲突。

💡 创新点2：文明发展模型
本文引入了一个数学模型来量化文明的发展水平，并使用状态转移矩阵方法来评估文明的轨迹。该模型考虑了五个关键资源：军事能力、技术发展、生产能力、消费和储存，以及不同文明的世界观（和平主义、军国主义和孤立主义）对决策的影响。

💡 创新点3：信息不对称的模拟
为了模拟宇宙中文明之间的互动，本文考虑了信息不对称的情况，即文明之间的观测数据滞后于实际发展。LLMs代理需要根据过时的信息做出决策，这增加了模拟的复杂性和现实性。

💡 创新点4：道德多样性的模拟
本文提出了使用具有不同伦理范式的LLM代理来模拟具有不同道德原则的实体之间的互动。这有助于理解不同文明如何共存，以及道德框架如何影响星际互动。

### 📈 实验结果
实验结果表明，具有军国主义世界观的文明倾向于对较弱文明发动攻击，而孤立主义文明则更倾向于在观察一段时间后选择性地与其他文明合作。此外，信息不对称会延迟冲突的发生，为较弱文明提供了反击的机会。

### 💬 可借鉴之处
本文的研究不仅为理解潜在星际动态提供了新的视角，还为解决具有不同价值体系的实体之间的冲突提供了策略。此外，本文的研究方法可以应用于其他领域，如模拟古代社会、人类文明模式和社会生态系统。

## software-agents-interaction-algorithms-in-virtual-learning-environment
### Abstract
This paper highlights the multi-agent learning virtual environment and agents
communication algorithms. The researcher proposed three algorithms required
software agents interaction in virtual learning information system environment.
The first proposed algorithm is agents interaction localization algorithm, the
second one is the dynamic agents distribution algorithm (load distribution
algorithm), and the third model is Agent communication algorithm based on using
agents intermediaries. The main objectives of these algorithms are to reduce
the response time for any agents changes in virtual learning environment (VLE)
by increasing the information exchange intensity between software agents and
reduce the overall network load, and to improve the communication between
mobile agents in distributed information system to support effectiveness.
Finally the paper describe the algorithms of information exchange between
mobile agents in VLE based on the expansion of the address structure and the
use of an agent, intermediary agents, matchmaking agents, brokers and their
entrepreneurial functions
### 🌟 论文解读 | 虚拟学习环境中软件代理交互算法的革新

### 📌 背景痛点/本文动机
随着信息技术的飞速发展，虚拟学习环境（VLE）已经成为教育领域的重要工具。然而，现有的VLE系统在处理大量信息交互时，面临着网络负载增加、信息交换效率低下等问题。为了解决这些问题，本文提出了一种基于软件代理的交互算法，旨在提高VLE系统的效率和响应速度。

### 🚀 核心方法
💡 创新点1：代理交互定位算法
该算法通过分析代理之间的通信依赖性，将频繁交互的代理分组到同一主机上，从而将跨主机的交互转化为主机内的交互，减少网络负载并提高信息交换效率。

💡 创新点2：动态代理分配算法（负载分配算法）
该算法通过监控主机负载，将代理分组并动态分配到不同的主机上，以实现负载均衡，避免某些主机过载，从而提高系统的稳定性和性能。

💡 创新点3：基于代理中介的通信模型
该模型利用代理中介（如经纪人代理和配对代理）来促进代理之间的通信。代理中介可以帮助代理查找具有相似兴趣的代理，并提供消息转发和匹配服务，从而提高通信效率和灵活性。

### 📈 实验结果
本文提出的算法在虚拟学习环境中进行了实验验证，结果表明，这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。

### 💬 可借鉴之处
本文提出的算法和模型为虚拟学习环境的设计和优化提供了新的思路和方法。其中，代理交互定位算法和动态代理分配算法可以应用于其他分布式系统中，以提高系统的效率和性能。基于代理中介的通信模型可以应用于其他多代理系统中，以促进代理之间的协作和通信。

### 📚 总结
本文提出的基于软件代理的交互算法为虚拟学习环境的设计和优化提供了新的思路和方法。这些算法能够有效减少网络负载，提高信息交换效率，并提高代理之间的通信效率。本文的研究成果对于虚拟学习环境和其他分布式系统的设计和优化具有重要的参考价值。

## enhance-reasoning-for-large-language-models-in-the-game-werewolf
### Abstract
This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.
### 🌟 论文解读 | 大型语言模型推理能力提升：以狼人杀游戏为案例

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理（NLP）任务上的突破，其在推理、规划和决策等领域的潜力也逐渐显现。然而，LLMs在处理复杂推理任务时仍面临挑战，尤其是在需要领域特定知识和深度逻辑分析的任务中。本文旨在通过引入外部推理模块，即“思考者”（Thinker），来增强LLM的推理能力，使其在特定任务中表现更佳。

### 🚀 核心方法
💡 创新点1：双系统推理框架
本文提出了一个创新的框架，将LLMs与外部Thinker模块相结合，形成了一个推理层次结构。LLMs负责处理直观的System-1任务，如自然语言处理和常识推理，而Thinker则专注于需要复杂逻辑分析和领域特定知识的System-2任务。

💡 创新点2：Thinker模块的设计与训练
Thinker模块直接从数据库中获取知识，并采用各种优化技术进行训练。它通过模仿学习、强化学习和基于群体的训练等方法，学习生成合理的游戏动作和LLM的语音指令。

💡 创新点3：数据集贡献
本文收集了18,800场真实人类游戏会话数据，构建了迄今为止最大的社交推理游戏数据集，为研究提供了宝贵资源。

### 📈 实验结果
实验结果表明，引入Thinker模块显著提高了LLMs的推理和生成能力。在狼人杀游戏中，Thinker模块在推理、语音生成和在线游戏评估方面均表现出色。此外，通过将Thinker与一个较小的LLM模型（6B）进行微调，其性能甚至超过了GPT4。

### 💬 可借鉴之处
本文提出的框架和方法为LLMs在复杂推理任务中的应用提供了新的思路。通过将LLMs与外部推理模块相结合，可以有效提升LLMs在特定领域的推理能力，使其在更多实际应用中发挥更大的作用。此外，本文构建的大规模数据集也为社交推理游戏的研究提供了重要的数据基础。

## pokellmon--a-human-parity-agent-for-pokemon-battles-with-large-language-models
### Abstract
We introduce PokeLLMon, the first LLM-embodied agent that achieves
human-parity performance in tactical battle games, as demonstrated in Pokemon
battles. The design of PokeLLMon incorporates three key strategies: (i)
In-context reinforcement learning that instantly consumes text-based feedback
derived from battles to iteratively refine the policy; (ii) Knowledge-augmented
generation that retrieves external knowledge to counteract hallucination and
enables the agent to act timely and properly; (iii) Consistent action
generation to mitigate the panic switching phenomenon when the agent faces a
powerful opponent and wants to elude the battle. We show that online battles
against human demonstrates PokeLLMon's human-like battle strategies and
just-in-time decision making, achieving 49% of win rate in the Ladder
competitions and 56% of win rate in the invited battles. Our implementation and
playable battle logs are available at: https://github.com/git-disl/PokeLLMon.
### 🌟 论文解读 | PokeLLMon：基于大型语言模型实现人类水平的宝可梦战斗AI

### 📌 背景痛点/本文动机
随着生成式AI和大型语言模型（LLMs）在自然语言处理（NLP）任务上的成功，人们开始探索LLMs如何自主地在物理世界中行动，将生成空间从文本扩展到行动，这被认为是追求通用人工智能（AGI）的关键范式。游戏是开发LLM-based代理与虚拟环境交互的合适测试平台。战术战斗游戏，如宝可梦战斗，因其状态和动作空间离散、回合制格式、战略性和复杂性，成为评估LLMs游戏能力的理想基准。

### 🚀 核心方法
💡 创新点1：上下文强化学习（ICRL）
为了解决LLMs在宝可梦战斗中出现的幻觉问题，论文提出了ICRL策略。ICRL利用战斗中即时生成的文本反馈作为“奖励”，在无需训练的情况下迭代优化动作生成策略。通过分析前一轮的行动和相应的文本反馈，代理能够不断调整其策略，从而更好地应对战斗中的变化。

💡 创新点2：知识增强生成（KAG）
为了进一步减少幻觉，论文引入了KAG策略。KAG通过检索外部知识，如类型优势/劣势关系和技能/能力效果，来增强生成过程。这些知识来源于宝可梦游戏中的宝可梦图鉴（Pokédex），它提供了关于宝可梦类型、技能和能力的详细信息。通过将外部知识添加到状态描述中，代理能够更准确地理解战斗情况，并做出更明智的决策。

💡 创新点3：一致性行动生成
为了解决代理在面对强大对手时出现的恐慌切换现象，论文提出了一致性行动生成策略。该策略通过多次独立生成行动并投票选出最一致的行动，来减少行动的不一致性。这种方法有助于代理在面对压力时保持冷静，避免过度思考和恐慌，从而做出更稳定的决策。

### 📈 实验结果
在线战斗结果表明，PokeLLMon在梯子比赛中取得了49%的胜率，在邀请比赛中取得了56%的胜率，展现出与人类玩家相当的比赛能力和策略。然而，PokeLLMon在面对人类玩家的消耗策略和欺骗技巧时也存在弱点，这表明未来需要进一步改进其长期规划和对手行为预测能力。

### 💬 可借鉴之处
PokeLLMon的设计和实现为LLMs在游戏领域的应用提供了新的思路。ICRL、KAG和一致性行动生成策略可以应用于其他游戏，帮助LLMs更好地理解和应对游戏中的挑战。此外，PokeLLMon的实验结果也揭示了LLMs在游戏中的优势和局限性，为未来研究和开发提供了有价值的参考。

## swarmbrain--embodied-agent-for-real-time-strategy-game-starcraft-ii-via-large-language-models
### Abstract
Large language models (LLMs) have recently garnered significant
accomplishments in various exploratory tasks, even surpassing the performance
of traditional reinforcement learning-based methods that have historically
dominated the agent-based field. The purpose of this paper is to investigate
the efficacy of LLMs in executing real-time strategy war tasks within the
StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an
embodied agent leveraging LLM for real-time strategy implementation in the
StarCraft II game environment. The SwarmBrain comprises two key components: 1)
a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed
to orchestrate macro-level strategies from a high-level perspective. This
matrix emulates the overarching consciousness of the Zerg intelligence brain,
synthesizing strategic foresight with the aim of allocating resources,
directing expansion, and coordinating multi-pronged assaults. 2) a Swarm
ReflexNet, which is agile counterpart to the calculated deliberation of the
Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the
Swarm ReflexNet employs a condition-response state machine framework, enabling
expedited tactical responses for fundamental Zerg unit maneuvers. In the
experimental setup, SwarmBrain is in control of the Zerg race in confrontation
with an Computer-controlled Terran adversary. Experimental results show the
capacity of SwarmBrain to conduct economic augmentation, territorial expansion,
and tactical formulation, and it shows the SwarmBrain is capable of achieving
victory against Computer players set at different difficulty levels.
### 🌟 论文解读 | SwarmBrain：基于大型语言模型的实时策略游戏StarCraft II的智能体

### 📌 背景痛点/本文动机
实时策略（RTS）游戏，如《星际争霸II》，因其复杂的战场环境和快速决策的需求，对人工智能（AI）提出了巨大挑战。传统的基于强化学习（RL）的方法在处理这种复杂环境时遇到了困难，尤其是在将高级目标直接映射到低级键盘和鼠标输入方面。而大型语言模型（LLM）因其对复杂语境的高层次抽象和理解能力，在探索性任务中取得了显著成就，但在实时策略游戏中，LLM的推理延迟限制了其直接应用。

### 🚀 核心方法
本文提出了SwarmBrain，一个基于LLM的智能体，用于在《星际争霸II》中执行实时策略任务。SwarmBrain由两个关键组件组成：

💡 创新点1：Overmind Intelligence Matrix
这是一个由最先进的LLM驱动的矩阵，负责从高层次视角协调宏观策略。它模拟了Zerg智能脑的总体意识，结合战略远见，旨在分配资源、指导扩张和协调多方面的攻击。

💡 创新点2：Swarm ReflexNet
这是Overmind Intelligence Matrix的敏捷对应物，采用条件响应状态机框架，为基本的Zerg单位操作提供快速战术响应。由于LLM推理的固有延迟，Swarm ReflexNet能够快速响应，而无需等待LLM的深入思考。

### 📈 实验结果
SwarmBrain在与不同难度级别的计算机对手的对抗中表现出色。在非常容易、容易、中等和中等困难级别，SwarmBrain的成功率为100%。即使在困难级别，SwarmBrain也能在76%的比赛中取得胜利。

### 💬 可借鉴之处
SwarmBrain的设计展示了LLM在实时策略游戏中的应用潜力。通过结合宏观策略规划和快速战术响应，SwarmBrain能够在复杂的游戏环境中取得成功。此外，SwarmBrain的设计也为我们提供了关于如何将LLM应用于其他需要快速决策的领域的见解。

## civrealm--a-learning-and-reasoning-odyssey-in-civilization-for-decision-making-agents
### Abstract
The generalization of decision-making agents encompasses two fundamental
elements: learning from past experiences and reasoning in novel contexts.
However, the predominant emphasis in most interactive environments is on
learning, often at the expense of complexity in reasoning. In this paper, we
introduce CivRealm, an environment inspired by the Civilization game.
Civilization's profound alignment with human history and society necessitates
sophisticated learning, while its ever-changing situations demand strong
reasoning to generalize. Particularly, CivRealm sets up an
imperfect-information general-sum game with a changing number of players; it
presents a plethora of complex features, challenging the agent to deal with
open-ended stochastic environments that require diplomacy and negotiation
skills. Within CivRealm, we provide interfaces for two typical agent types:
tensor-based agents that focus on learning, and language-based agents that
emphasize reasoning. To catalyze further research, we present initial results
for both paradigms. The canonical RL-based agents exhibit reasonable
performance in mini-games, whereas both RL- and LLM-based agents struggle to
make substantial progress in the full game. Overall, CivRealm stands as a
unique learning and reasoning challenge for decision-making agents. The code is
available at https://github.com/bigai-ai/civrealm.
### 🌟 论文解读 | CivRealm：决策智能体的学习与推理之旅

### 📌 背景痛点/本文动机
传统的决策智能体环境往往过于强调学习，而忽视了推理的重要性。然而，在实际应用中，智能体需要同时具备学习和推理能力，才能更好地适应复杂多变的环境。为了解决这个问题，本文提出了CivRealm，一个基于文明游戏的交互式环境，旨在推动决策智能体学习和推理能力的边界。

### 🚀 核心方法
💡 创新点1：CivRealm环境
CivRealm是一个基于文明游戏的开放环境，具有以下特点：
* **不完全信息**：玩家只能获取自己单位发现的信息，需要推理其他玩家的意图。
* **随机性**：游戏中有随机事件和危机，需要智能体灵活应对。
* **多目标**：有多种胜利路径，需要平衡经济、军事、外交、文化和科技发展。
* **动态空间**：游戏过程中，玩家的状态和动作空间会动态变化。
* **多智能体**：多个玩家可以互动，需要智能体进行合作和竞争。
* **动态玩家数量**：游戏过程中，玩家数量会发生变化，需要智能体适应新的环境。
* **通信**：玩家可以通过外交行动和自然语言聊天进行交流。

💡 创新点2：支持多种智能体类型
CivRealm提供了两种API接口，分别支持基于张量的智能体和基于语言的智能体：
* **基于张量的智能体**：使用深度强化学习等方法，擅长学习和模式识别。
* **基于语言的智能体**：使用大型语言模型等方法，擅长推理和自然语言处理。

💡 创新点3：基准方法和评估指标
本文提出了三种基准方法，包括：
* **基于张量的强化学习**：使用AlphaStar的架构，擅长处理复杂动态和海量信息。
* **BaseLang**：基于AutoGPT的架构，擅长推理和自然语言处理。
* **Mastaba**：基于BaseLang的架构，引入层次结构，提高全局视角和决策能力。

### 📈 实验结果
实验结果表明，基于张量的强化学习在迷你游戏中表现良好，但在完整游戏中仍然存在局限性，例如短视策略和难以处理稀疏奖励。基于语言的智能体在完整游戏中表现更佳，但仍然需要改进推理能力和全局视角。

### 💬 可借鉴之处
CivRealm为决策智能体的学习和推理能力提供了一个独特的挑战平台，可以用于评估和改进智能体的性能。此外，CivRealm还可以用于研究人类社会的动态、历史事件的结果和未来的社会轨迹。

## finding-the-needle-in-a-haystack--detecting-bug-occurrences-in-gameplay-videos
### Abstract
The presence of bugs in video games can bring significant consequences for
developers. To avoid these consequences, developers can leverage gameplay
videos to identify and fix these bugs. Video hosting websites such as YouTube
provide access to millions of game videos, including videos that depict bug
occurrences, but the large amount of content can make finding bug instances
challenging. We present an automated approach that uses machine learning to
predict whether a segment of a gameplay video contains the depiction of a bug.
We analyzed 4,412 segments of 198 gameplay videos to predict whether a segment
contains an instance of a bug. Additionally, we investigated how our approach
performs when applied across different specific genres of video games and on
videos from the same game. We also analyzed the videos in the dataset to
investigate what characteristics of the visual features might explain the
classifier's prediction. Finally, we conducted a user study to examine the
benefits of our automated approach against a manual analysis. Our findings
indicate that our approach is effective at detecting segments of a video that
contain bugs, achieving a high F1 score of 0.88, outperforming the current
state-of-the-art technique for bug classification of gameplay video segments.
### 🌟 论文解读 | 游戏视频中的“寻针”：自动检测游戏中的bug

### 📌 背景痛点/本文动机
游戏行业是一个价值数十亿美元的产业，游戏中的bug会对开发者造成重大影响。为了减少这些影响，开发者可以利用游戏视频来识别和修复这些bug。视频托管网站如YouTube提供了数百万个游戏视频，包括描绘bug出现的视频，但大量内容使得找到bug实例变得具有挑战性。本文提出了一种自动化的方法，使用机器学习来预测游戏视频片段是否包含bug的描绘。

### 🚀 核心方法
💡 创新点1：本文提出了一种自动化的方法，使用机器学习来预测游戏视频片段是否包含bug的描绘。该方法通过分析视频片段的文本和视觉特征，训练神经网络模型进行预测。
💡 创新点2：本文还进行了初步分析，解释了视频帧图像的哪些特征可能与bug的出现有关。通过分析视频帧，研究人员可以了解哪些元素或行为可能导致bug的出现，从而帮助开发者更好地进行测试和修复。

### 📈 实验结果
本文使用了一个包含4,412个游戏视频片段的数据集，其中包括198个不同的游戏视频。实验结果表明，该方法在检测包含bug的视频片段方面非常有效，F1分数达到了0.88，超过了当前最先进的游戏视频片段bug分类技术。

### 💬 可借鉴之处
本文提出的方法可以帮助游戏开发者更有效地识别和修复游戏中的bug。通过使用机器学习模型，开发者可以自动分析大量的游戏视频，快速找到包含bug的片段，从而节省时间和精力。此外，本文的分析结果还可以帮助开发者了解哪些特征可能与bug的出现有关，从而更好地进行测试和修复。

## pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model
### Abstract
Poker, also known as Texas Hold'em, has always been a typical research target
within imperfect information games (IIGs). IIGs have long served as a measure
of artificial intelligence (AI) development. Representative prior works, such
as DeepStack and Libratus heavily rely on counterfactual regret minimization
(CFR) to tackle heads-up no-limit Poker. However, it is challenging for
subsequent researchers to learn CFR from previous models and apply it to other
real-world applications due to the expensive computational cost of CFR
iterations. Additionally, CFR is difficult to apply to multi-player games due
to the exponential growth of the game tree size. In this work, we introduce
PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number
of players and gaining high win rates, established on a lightweight large
language model (LLM). PokerGPT only requires simple textual information of
Poker games for generating decision-making advice, thus guaranteeing the
convenient interaction between AI and humans. We mainly transform a set of
textual records acquired from real games into prompts, and use them to
fine-tune a lightweight pre-trained LLM using reinforcement learning human
feedback technique. To improve fine-tuning performance, we conduct prompt
engineering on raw data, including filtering useful information, selecting
behaviors of players with high win rates, and further processing them into
textual instruction using multiple prompt engineering techniques. Through the
experiments, we demonstrate that PokerGPT outperforms previous approaches in
terms of win rate, model size, training time, and response speed, indicating
the great potential of LLMs in solving IIGs.
### 🌟 论文解读 | PokerGPT：基于大型语言模型的轻量级多玩家德州扑克解决方案

### 📌 背景痛点/本文动机
德州扑克作为一种典型的非完美信息游戏（IIG），一直是人工智能研究的重要目标。然而，现有的解决方案，如DeepStack和Libratus，主要依赖于反事实后悔最小化（CFR）算法，该算法在计算成本和扩展性方面存在局限性。CFR算法的计算成本高昂，难以应用于多玩家游戏，且难以从现有模型中学习并应用于其他现实世界应用。

### 🚀 核心方法
本文提出了PokerGPT，一种基于轻量级大型语言模型（LLM）的端到端德州扑克解决方案。PokerGPT通过以下创新点克服了现有方法的局限性：

💡 创新点1：端到端学习框架
PokerGPT采用端到端学习框架，避免了复杂的特征工程和中间步骤。它仅需要简单的文本信息即可生成决策建议，实现了人机交互的便捷性。

💡 创新点2：轻量级LLM
PokerGPT基于轻量级LLM，具有更少的参数和更快的推理速度，同时训练时间也更短，实现了资源的有效利用。

💡 创新点3：高效的数据处理
PokerGPT采用数据清洗和提示工程技术，将真实游戏数据转换为可理解的文本提示，并使用强化学习人类反馈技术进行微调，提高了模型性能。

### 📈 实验结果
实验结果表明，PokerGPT在胜率、模型大小、训练时间和响应速度等方面均优于现有方法。此外，PokerGPT能够处理任意数量的玩家，并展现出出色的灵活性和适应性。

### 💬 可借鉴之处
PokerGPT的成功表明，LLM在解决IIG方面具有巨大潜力。其端到端学习框架、轻量级模型和高效的数据处理技术为其他IIG研究提供了可借鉴的经验。此外，PokerGPT的交互式特性使其在现实世界应用中具有广阔前景。

## cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game
### Abstract
Multi-agent collaboration with Large Language Models (LLMs) demonstrates
proficiency in basic tasks, yet its efficiency in more complex scenarios
remains unexplored. In gaming environments, these agents often face situations
without established coordination protocols, requiring them to make intelligent
inferences about teammates from limited data. This problem motivates the area
of ad hoc teamwork, in which an agent may potentially cooperate with a variety
of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork
problem where the agent operates in an environment driven by natural language.
Our findings reveal the potential of LLM agents in team collaboration,
highlighting issues related to hallucinations in communication. To address this
issue, we develop CodeAct, a general agent that equips LLM with enhanced memory
and code-driven reasoning, enabling the repurposing of partial information for
rapid adaptation to new teammates.
### 🌟 论文解读 | 探索大型语言模型在动态环境中的协作能力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在推理和泛化能力上的不断提升，它们在构建自主代理和推动人工智能领域的发展方面展现出巨大潜力。然而，在多智能体协作中，特别是在没有预先设定的协调协议的动态环境中，LLMs的协作效率仍然是一个未充分探索的领域。本文旨在研究LLMs在动态环境中的协作能力，特别是在没有明确团队策略的情况下，如何与不同的队友进行有效合作。

### 🚀 核心方法
💡 创新点1：引入AvalonPlay基准
本文提出了AvalonPlay基准，这是一个基于自然语言的多智能体平台，用于模拟动态环境中的协作任务。在这个基准中，智能体需要在有限的信息和没有预先设定的团队策略的情况下，通过观察队友的行为来推断他们的角色，并动态调整团队策略以实现共同目标。

💡 创新点2：开发CodeAct智能体
为了解决LLMs在动态环境中协作时可能出现的记忆遗忘和幻觉生成等问题，本文提出了CodeAct智能体。CodeAct利用LLMs的代码驱动推理能力，通过将复杂的语义任务转化为灵活的代码结构，从而提高智能体在动态环境中的协作效率。

### 📈 实验结果
实验结果表明，GPT-4模型在AvalonPlay基准中表现出最佳的协作能力，而CodeAct智能体在团队选择准确性方面优于其他语义推理方法。此外，实验还发现，引入自然语言通信协议并不总是能显著提高LLMs的协作效率。

### 💬 可借鉴之处
本文的研究结果表明，LLMs在动态环境中的协作能力仍然面临一些挑战，如记忆遗忘和幻觉生成。为了提高LLMs的协作效率，可以借鉴本文提出的CodeAct智能体的设计思路，利用代码驱动推理和记忆检索系统来增强智能体的推理能力和信息处理能力。此外，还可以进一步研究如何减少幻觉生成的影响，并探索LLMs在现实世界场景中的应用。

## llm-powered-hierarchical-language-agent-for-real-time-human-ai-coordination
### Abstract
AI agents powered by Large Language Models (LLMs) have made significant
advances, enabling them to assist humans in diverse complex tasks and leading
to a revolution in human-AI coordination. LLM-powered agents typically require
invoking LLM APIs and employing artificially designed complex prompts, which
results in high inference latency. While this paradigm works well in scenarios
with minimal interactive demands, such as code generation, it is unsuitable for
highly interactive and real-time applications, such as gaming. Traditional
gaming AI often employs small models or reactive policies, enabling fast
inference but offering limited task completion and interaction abilities. In
this work, we consider Overcooked as our testbed where players could
communicate with natural language and cooperate to serve orders. We propose a
Hierarchical Language Agent (HLA) for human-AI coordination that provides both
strong reasoning abilities while keeping real-time execution. In particular,
HLA adopts a hierarchical framework and comprises three modules: a proficient
LLM, referred to as Slow Mind, for intention reasoning and language
interaction, a lightweight LLM, referred to as Fast Mind, for generating macro
actions, and a reactive policy, referred to as Executor, for transforming macro
actions into atomic actions. Human studies show that HLA outperforms other
baseline agents, including slow-mind-only agents and fast-mind-only agents,
with stronger cooperation abilities, faster responses, and more consistent
language communications.
### 🌟 论文解读 | 基于大型语言模型的分层语言代理：实时人机协作的突破

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的兴起，基于LLMs的AI代理在辅助人类完成复杂任务方面取得了显著进展，推动了人机协作的革命。然而，这些代理通常需要调用LLMs API并使用人工设计的复杂提示，导致推理延迟高。这种范式在交互需求较低的场景（如代码生成）中表现良好，但在需要实时响应和高频交互的应用（如游戏）中并不适用。传统的游戏AI通常采用小型模型或反应策略，虽然能够实现快速推理，但任务完成和交互能力有限。

### 🚀 核心方法
本文提出了一个分层语言代理（HLA），用于实时人机协作，该代理结合了大型模型的强大推理和交互能力以及小型模型和反应策略的实时推理能力。HLA采用分层框架，由三个模块组成：

* **慢思维（Slow Mind）**：一个熟练的LLM，用于意图推理和语言交互。
* **快思维（Fast Mind）**：一个轻量级的LLM，用于生成宏操作。
* **执行器（Executor）**：一个反应策略，用于将宏操作转换为原子操作。

### 📈 实验结果
在Overcooked游戏平台上进行的实验表明，HLA在游戏得分、响应延迟和人类偏好方面均优于其他基线代理，包括仅使用慢思维或快思维的代理。HLA展现出更强的协作能力、更快的响应速度和更一致的语言通信。

### 💬 可借鉴之处
* **分层设计**：HLA的分层设计有效地解决了LLMs推理延迟高的问题，使其适用于实时人机协作场景。
* **轻量级LLM**：使用轻量级LLM进行宏操作生成，提高了推理速度，同时避免了生成次优操作。
* **反应策略**：执行器模块确保了动作的可行性和高频交互，提高了AI代理的实时响应能力。

### 🌟 总结
HLA为实时人机协作提供了一种新的解决方案，其分层设计有效地解决了LLMs推理延迟高的问题，使其能够在需要实时响应和高频交互的场景中发挥作用。HLA在人机协作领域的应用前景广阔，有望推动人机协作的进一步发展。

## large-language-models-play-starcraft-ii--benchmarks-and-a-chain-of-summarization-approach
### Abstract
StarCraft II is a challenging benchmark for AI agents due to the necessity of
both precise micro level operations and strategic macro awareness. Previous
works, such as Alphastar and SCC, achieve impressive performance on tackling
StarCraft II , however, still exhibit deficiencies in long term strategic
planning and strategy interpretability. Emerging large language model (LLM)
agents, such as Voyage and MetaGPT, presents the immense potential in solving
intricate tasks. Motivated by this, we aim to validate the capabilities of LLMs
on StarCraft II, a highly complex RTS game.To conveniently take full advantage
of LLMs` reasoning abilities, we first develop textual StratCraft II
environment, called TextStarCraft II, which LLM agent can interact. Secondly,
we propose a Chain of Summarization method, including single frame
summarization for processing raw observations and multi frame summarization for
analyzing game information, providing command recommendations, and generating
strategic decisions. Our experiment consists of two parts: first, an evaluation
by human experts, which includes assessing the LLMs`s mastery of StarCraft II
knowledge and the performance of LLM agents in the game; second, the in game
performance of LLM agents, encompassing aspects like win rate and the impact of
Chain of Summarization.Experiment results demonstrate that: 1. LLMs possess the
relevant knowledge and complex planning abilities needed to address StarCraft
II scenarios; 2. Human experts consider the performance of LLM agents to be
close to that of an average player who has played StarCraft II for eight years;
3. LLM agents are capable of defeating the built in AI at the Harder(Lv5)
difficulty level. We have open sourced the code and released demo videos of LLM
agent playing StarCraft II.
### 🌟 论文解读 | 大型语言模型在星际争霸II中的表现：基准测试与摘要链方法

### 📌 背景痛点/本文动机
星际争霸II（StarCraft II）是一款极具挑战性的实时战略游戏，要求玩家在微观操作和宏观战略规划之间取得平衡。尽管之前的AI研究，如AlphaStar和SCC，在星际争霸II中取得了令人印象深刻的成果，但它们在长期战略规划和策略可解释性方面仍存在不足。随着大型语言模型（LLM）在解决复杂任务方面的潜力日益显现，本文旨在验证LLM在星际争霸II中的能力。

### 🚀 核心方法
💡 创新点1：TextStarCraft II环境
为了充分利用LLM的推理能力，本文开发了一个名为TextStarCraft II的文本环境，LLM代理可以与之交互。该环境将星际争霸II的复杂游戏动态转换为文本格式，允许LLM代理通过语言命令执行宏观战略行动。

💡 创新点2：摘要链（CoS）方法
本文提出了摘要链（CoS）方法，包括单帧摘要和多帧摘要。单帧摘要用于处理原始观察数据，而多帧摘要用于分析游戏信息，提供命令建议并生成战略决策。CoS方法通过信息压缩、推理加速和全局理解，增强了LLM代理在处理复杂信息和做出战略决策方面的能力。

### 📈 实验结果
实验结果表明，LLM具备解决星际争霸II场景所需的相关知识和复杂规划能力。人类专家认为，LLM代理在游戏中的表现接近于玩了八年星际争霸II的平均玩家。此外，LLM代理能够在Harder（Lv5）难度级别下击败内置AI。

### 💬 可借鉴之处
本文提出的TextStarCraft II环境和CoS方法为评估LLM在实时战略决策和长期规划方面的能力提供了一个新的基准。此外，本文的研究结果表明，LLM在解决复杂任务方面具有巨大潜力，并为未来在星际争霸II和其他实时战略游戏中的AI研究提供了有价值的见解。

## auto-mc-reward--automated-dense-reward-design-with-large-language-models-for-minecraft
### Abstract
Many reinforcement learning environments (e.g., Minecraft) provide only
sparse rewards that indicate task completion or failure with binary values. The
challenge in exploration efficiency in such environments makes it difficult for
reinforcement-learning-based agents to learn complex tasks. To address this,
this paper introduces an advanced learning system, named Auto MC-Reward, that
leverages Large Language Models (LLMs) to automatically design dense reward
functions, thereby enhancing the learning efficiency. Auto MC-Reward consists
of three important components: Reward Designer, Reward Critic, and Trajectory
Analyzer. Given the environment information and task descriptions, the Reward
Designer first design the reward function by coding an executable Python
function with predefined observation inputs. Then, our Reward Critic will be
responsible for verifying the code, checking whether the code is
self-consistent and free of syntax and semantic errors. Further, the Trajectory
Analyzer summarizes possible failure causes and provides refinement suggestions
according to collected trajectories. In the next round, Reward Designer will
further refine and iterate the dense reward function based on feedback.
Experiments demonstrate a significant improvement in the success rate and
learning efficiency of our agents in complex tasks in Minecraft, such as
obtaining diamond with the efficient ability to avoid lava, and efficiently
explore trees and animals that are sparse in the plains biome.
### 🌟 论文解读 | Auto MC-Reward：利用大型语言模型自动设计密集奖励函数，提升Minecraft中强化学习的效率

### 📌 背景痛点/本文动机
Minecraft 等强化学习环境通常只提供稀疏奖励，即只有任务完成或失败时才会获得奖励。这种奖励机制使得强化学习代理在探索效率方面面临挑战，难以学习复杂任务。为了解决这个问题，本文提出了 Auto MC-Reward，一个利用大型语言模型 (LLM) 自动设计密集奖励函数的先进学习系统，从而提高学习效率。

### 🚀 核心方法
💡 创新点1：Auto MC-Reward 由三个关键组件组成：奖励设计器、奖励评论家和轨迹分析器。奖励设计器根据环境信息和任务描述，通过编写可执行的 Python 函数来设计奖励函数。奖励评论家负责验证代码，检查代码是否自洽且没有语法和语义错误。轨迹分析器根据收集的轨迹总结可能的失败原因，并提供改进建议。

💡 创新点2：Auto MC-Reward 利用 LLM 的任务理解和经验总结能力，为学习提供详细和即时的奖励指导。奖励设计器首先根据环境和任务的基本描述，使用 LLM 设计与任务相关的密集奖励函数。然后，奖励评论家对设计的奖励函数进行自我验证。为了解决 LLM 理解的潜在偏差或疏忽，还提出了基于 LLM 的轨迹分析器，用于分析和总结训练代理的轨迹，并帮助奖励设计器改进奖励函数。

### 📈 实验结果
Auto MC-Reward 在一系列代表性基准测试中进行了验证，包括地下水平探索钻石和探索平原生物群落中的树木和动物。实验结果表明，与原始稀疏奖励和现有密集奖励方法相比，Auto MC-Reward 在这些任务上取得了显著更好的结果，显示出其在稀疏奖励任务上高效学习的先进能力。通过迭代改进奖励函数的设计，Auto MC-Reward 使代理能够有效地学习对新任务有益的新行为，例如避免熔岩，从而大大提高了成功率。此外，Auto MC-Reward 仅使用原始信息就实现了高钻石获取成功率（36.5%），证明了其解决长期任务的能力。

### 💬 可借鉴之处
Auto MC-Reward 为解决稀疏奖励任务中的探索效率问题提供了一种新的思路。其利用 LLM 自动设计密集奖励函数的方法，可以有效地提高强化学习代理的学习效率。此外，Auto MC-Reward 的三个组件（奖励设计器、奖励评论家和轨迹分析器）可以独立运行，使得数据分析和奖励函数更新更加灵活。

## mp5--a-multi-modal-open-ended-embodied-system-in-minecraft-via-active-perception
### Abstract
It is a long-lasting goal to design an embodied system that can solve
long-horizon open-world tasks in human-like ways. However, existing approaches
usually struggle with compound difficulties caused by the logic-aware
decomposition and context-aware execution of these tasks. To this end, we
introduce MP5, an open-ended multimodal embodied system built upon the
challenging Minecraft simulator, which can decompose feasible sub-objectives,
design sophisticated situation-aware plans, and perform embodied action
control, with frequent communication with a goal-conditioned active perception
scheme. Specifically, MP5 is developed on top of recent advances in Multimodal
Large Language Models (MLLMs), and the system is modulated into functional
modules that can be scheduled and collaborated to ultimately solve pre-defined
context- and process-dependent tasks. Extensive experiments prove that MP5 can
achieve a 22% success rate on difficult process-dependent tasks and a 91%
success rate on tasks that heavily depend on the context. Moreover, MP5
exhibits a remarkable ability to address many open-ended tasks that are
entirely novel.
### 🌟 论文解读 | MP5：基于主动感知的多模态开放式具身系统

### 📌 背景痛点/本文动机
在人工智能领域，设计一个能够以人类方式解决开放世界长时任务的具身系统一直是长期目标。然而，现有的方法通常难以应对这些任务中逻辑感知分解和上下文感知执行所带来的复合困难。为了解决这个问题，本文提出了MP5，一个基于Minecraft模拟器的开放式多模态具身系统，它能够分解可行的子目标，设计复杂的情境感知计划，并执行具身动作控制，同时与目标条件下的主动感知方案进行频繁通信。

### 🚀 核心方法
💡 创新点1：MP5基于最新的多模态大型语言模型（MLLMs）构建，并将系统分解为可调度和协作的功能模块，以解决预定义的上下文和过程依赖任务。
💡 创新点2：MP5包括一个主动感知方案，通过感知器与巡逻器之间的多轮交互，主动感知观察到的图像中的上下文信息，以解决上下文依赖任务。

### 📈 实验结果
MP5在困难的过程依赖任务上实现了22%的成功率，在高度依赖上下文的任务上实现了91%的成功率。此外，MP5表现出解决许多完全新颖的开放式任务的能力。

### 💬 可借鉴之处
MP5的设计和实现为解决开放世界长时任务提供了新的思路和方法，其主动感知方案和模块化设计对于开发更智能、更灵活的具身系统具有重要的参考价值。

## stay-focused--problem-drift-in-multi-agent-debate
### Abstract
Multi-agent debate - multiple instances of large language models discussing
problems in turn-based interaction - has shown promise for solving knowledge
and reasoning tasks. However, these methods show limitations, particularly when
scaling them to longer reasoning chains. In this study, we unveil a new issue
of multi-agent debate: discussions drift away from the initial problem over
multiple turns. We define this phenomenon as problem drift and quantify its
presence across ten tasks (i.e., three generative, three knowledge, three
reasoning, and one instruction-following task). To identify the reasons for
this issue, we perform a human study with eight experts on discussions
suffering from problem drift, who find the most common issues are a lack of
progress (35% of cases), low-quality feedback (26% of cases), and a lack of
clarity (25% of cases). To systematically address the issue of problem drift,
we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem
drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of
problem drift cases. Our study can be seen as a first step to understanding a
key limitation of multi-agent debate, highlighting pathways for improving their
effectiveness in the future.
### 🌟 论文解读 | 多智能体辩论中的问题漂移：识别与缓解

### 📌 背景痛点/本文动机
多智能体辩论作为一种新兴的AI技术，通过多个大型语言模型（LLMs）进行轮换式交互，展现出在解决知识和推理任务方面的潜力。然而，随着辩论链的延长，这些方法在扩展性方面存在局限性。本文揭示了多智能体辩论中的一个新问题：在多轮交互中，讨论逐渐偏离初始问题。这种现象被称为“问题漂移”，本文旨在量化问题漂移的存在，并探索其背后的原因。

### 🚀 核心方法
💡 创新点1：问题漂移的量化
本文提出了一个名为FOCUS的指标，用于衡量多轮交互中任务性能的衰减程度。通过FOCUS指标，可以识别出哪些讨论出现了问题漂移，并量化其影响程度。

💡 创新点2：问题漂移的检测与缓解
为了在测试时检测问题漂移，本文提出了DRIFTJudge方法，该方法基于LLM-as-a-judge，通过评估连续轮次的解决方案来判断是否存在问题漂移。此外，本文还提出了DRIFTPolicy方法，通过向参与辩论的智能体提供针对性的反馈，以减少问题漂移的发生。

### 📈 实验结果
本文在十个任务（包括生成、知识、推理和指令遵循任务）上进行了实验，发现问题漂移普遍存在，尤其是在生成任务中。通过人类专家的研究，本文确定了导致问题漂移的八个主要原因，包括缺乏进展、低质量反馈、缺乏清晰度等。实验结果表明，DRIFTPolicy方法可以有效减少31%的问题漂移案例。

### 💬 可借鉴之处
本文的研究揭示了多智能体辩论中的一个关键局限性，并为提高其有效性提供了新的思路。未来研究可以探索智能体探索新想法的作用，并比较人类和智能体辩论之间的内在差异。此外，本文提出的DRIFTJudge和DRIFTPolicy方法可以为其他多智能体系统提供参考，以检测和缓解问题漂移现象。

## glitchbench--can-large-multimodal-models-detect-video-game-glitches-
### Abstract
Large multimodal models (LMMs) have evolved from large language models (LLMs)
to integrate multiple input modalities, such as visual inputs. This integration
augments the capacity of LLMs for tasks requiring visual comprehension and
reasoning. However, the extent and limitations of their enhanced abilities are
not fully understood, especially when it comes to real-world tasks. To address
this gap, we introduce GlitchBench, a novel benchmark derived from video game
quality assurance tasks, to test and evaluate the reasoning capabilities of
LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios
from video games and aims to challenge both the visual and linguistic reasoning
powers of LMMs in detecting and interpreting out-of-the-ordinary events. We
evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents
a new challenge for these models. Code and data are available at:
https://glitchbench.github.io/
### 🌟 论文解读 | GlitchBench：大型多模态模型能否检测视频游戏中的错误？

### 📌 背景痛点/本文动机
随着大型多模态模型（LMMs）的不断发展，它们在视觉理解和推理方面的能力得到了显著提升。然而，这些模型在实际应用中的表现和局限性尚不明确。为了填补这一空白，本文提出了GlitchBench，一个基于视频游戏质量保证任务的基准测试，旨在评估LMMs在检测和解释异常事件方面的推理能力。

### 🚀 核心方法
💡 创新点1：GlitchBench数据集
GlitchBench数据集由593个游戏中的异常和错误场景组成，涵盖了205款不同类型的游戏。每个场景都包含一个视频片段、一个代表性帧、一个简短的描述以及一个指向Reddit上相关讨论的链接。此外，数据集还包括330个无错误的图像作为对比。

💡 创新点2：评估方法
本文评估了11个最先进的LMMs，包括GPT-4V和LLaVA，在GlitchBench上的表现。评估方法包括三个问题：
1. 这张图片有什么不寻常的地方？
2. 这张图片有什么问题？
3. 详细描述这张图片。
通过比较模型生成的文本与真实标签，评估模型在检测和解释异常事件方面的能力。

### 📈 实验结果
实验结果表明，LMMs在检测违反简单物理定律的错误（如汽车在空中飞行）方面表现较好，但在检测更微妙的错误（如人体部位处于不可能的姿势）方面表现较差。GPT-4V在GlitchBench上表现最佳，准确率达到43.4%。然而，与无错误图像相比，模型在检测错误图像方面的准确率明显较低，这表明错误图像更具挑战性。

### 💬 可借鉴之处
本文提出的GlitchBench基准测试为评估LMMs在实际应用中的推理能力提供了一个有价值的工具。此外，本文的研究结果表明，LMMs在检测和解释异常事件方面仍存在局限性，需要进一步改进。

## a-framework-for-complementary-companion-character-behavior-in-video-games
### Abstract
We propose a game development framework capable of governing the behavior of
complementary companions in a video game. A "complementary" action is
contrasted with a mimicking action and is defined as any action by a friendly
non-player character that furthers the player's strategy. This is determined
through a combination of both player action and game state prediction processes
while allowing the AI companion to experiment. We determine the location of
interest for companion actions based on a dynamic set of regions customized to
the individual player. A user study shows promising results; a majority of
participants familiar with game design react positively to the companion
behavior, stating that they would consider using the frame-work in future games
themselves.
### 🌟 论文解读 | 游戏AI新框架：互补型伙伴行为助力玩家策略

### 📌 背景痛点/本文动机
随着游戏复杂性的提升，玩家对游戏中的非玩家角色（NPC）的智能和互动性要求越来越高。然而，现有的游戏AI往往只能模仿玩家的行为，缺乏主动性和创造性，无法真正辅助玩家达成目标。本文提出了一种新的游戏开发框架，旨在实现互补型伙伴行为，即NPC能够根据玩家的策略和游戏状态，主动采取行动，帮助玩家更轻松地达成目标。

### 🚀 核心方法
💡 创新点1：互补型行为定义
本文首先对“互补型行为”进行了明确定义，即NPC的行为能够促进玩家的策略，而不是简单地模仿玩家。这要求NPC能够预测玩家的下一步行动，并采取相应的行动来辅助玩家。

💡 创新点2：动态决策过程
本文提出了一种动态的决策过程，包括以下步骤：
1. 预测玩家的下一步行动
2. 检查预测行动是否可行，若不可行则寻找替代行动
3. 判断是否能够辅助玩家当前行动
4. 若无法辅助，则随机选择执行预测行动或寻找最佳游戏状态的行动
5. 若所有行动均不可行，则尝试执行玩家未尝试过的行动或采取默认行为
6. 随机选择执行未尝试过的行动，以鼓励NPC进行探索

💡 创新点3：动态区域系统
为了更精确地确定NPC的行动位置，本文提出了一种动态区域系统。该系统根据玩家的行动动态划分地图区域，使得NPC的行动更加灵活和针对性。

💡 创新点4：玩家行动预测
本文使用分类器对玩家的历史数据进行训练，以预测玩家的下一步行动。分类器的类型和训练方式可以根据游戏开发者进行调整。

💡 创新点5：游戏状态预测
本文通过模拟NPC执行每个可能的行动，并评估其结果，来预测游戏状态的变化。这有助于NPC选择能够带来最佳结果的行动。

### 📈 实验结果
本文通过用户研究验证了框架的有效性。结果显示，大多数参与者对NPC的互补型行为表示满意，并认为该框架对游戏开发者具有潜在价值。

### 💬 可借鉴之处
本文提出的框架为游戏开发者提供了一种新的思路，可以帮助他们创建更具智能和互动性的NPC。该框架的可配置性和灵活性使其适用于各种类型的游戏。此外，本文提出的动态区域系统和玩家行动预测方法也可以应用于其他领域，例如机器人控制和智能交通系统。

## creative-agents--empowering-agents-with-imagination-for-creative-tasks
### Abstract
We study building embodied agents for open-ended creative tasks. While
existing methods build instruction-following agents that can perform diverse
open-ended tasks, none of them demonstrates creativity -- the ability to give
novel and diverse task solutions implicit in the language instructions. This
limitation comes from their inability to convert abstract language instructions
into concrete task goals in the environment and perform long-horizon planning
for such complicated goals. Given the observation that humans perform creative
tasks with the help of imagination, we propose a class of solutions for
creative agents, where the controller is enhanced with an imaginator that
generates detailed imaginations of task outcomes conditioned on language
instructions. We introduce several approaches to implementing the components of
creative agents. We implement the imaginator with either a large language model
for textual imagination or a diffusion model for visual imagination. The
controller can either be a behavior-cloning policy learned from data or a
pre-trained foundation model generating executable codes in the environment. We
benchmark creative tasks with the challenging open-world game Minecraft, where
the agents are asked to create diverse buildings given free-form language
instructions. In addition, we propose novel evaluation metrics for open-ended
creative tasks utilizing GPT-4V, which holds many advantages over existing
metrics. We perform a detailed experimental analysis of creative agents,
showing that creative agents are the first AI agents accomplishing diverse
building creation in the survival mode of Minecraft. Our benchmark and models
are open-source for future research on creative agents
(https://github.com/PKU-RL/Creative-Agents).
### 🌟 论文解读 | 创意智能体：赋予智能体想象力以完成创意任务

### 📌 背景痛点/本文动机
现有的智能体大多只能执行预定义的任务，缺乏处理开放性任务的能力，尤其是那些需要创造力的任务。例如，在Minecraft游戏中，现有的智能体可以执行简单的指令，如“收集石头”或“建造一个雪人”，但无法完成更复杂的创意任务，如“建造一个砂岩宫殿”。这是因为它们无法将抽象的语言指令转换为具体的任务目标，并执行长期规划。

### 🚀 核心方法
本文提出了“创意智能体”的概念，通过赋予智能体想象力来处理开放性创意任务。创意智能体由两个主要部分组成：想象器和控制器。

💡 创新点1：想象器
想象器负责根据语言指令生成任务结果的详细想象。本文提出了两种实现想象器的方法：
- **文本想象**：使用大型语言模型（LLM）如GPT-4，通过Chain-of-Thought（CoT）技术生成文本想象。
- **视觉想象**：使用扩散模型如Stable Diffusion，生成与文本描述相符的视觉想象。

💡 创新点2：控制器
控制器负责将想象转换为可执行的计划。本文提出了两种实现控制器的方法：
- **行为克隆控制器**：从环境中学习行为克隆策略，将图像想象转换为建筑蓝图，并执行相应的动作。
- **基于GPT-4(V)的控制器**：利用GPT-4(V)的视觉语言理解和代码生成能力，直接生成可执行代码来完成任务。

### 📈 实验结果
本文在Minecraft游戏中进行了实验，结果表明，创意智能体能够根据自由形式的语言指令创建多样化和视觉上吸引人的建筑。其中，使用扩散模型进行视觉想象，并结合GPT-4(V)进行控制的智能体表现最佳。

### 💬 可借鉴之处
本文提出的创意智能体框架为开放性学习和创意AI智能体研究提供了新的思路和方法。未来可以进一步探索如何提高行为克隆控制器的性能，以及如何增强智能体的创造力。

## visual-encoders-for-data-efficient-imitation-learning-in-modern-video-games
### Abstract
Video games have served as useful benchmarks for the decision making
community, but going beyond Atari games towards training agents in modern games
has been prohibitively expensive for the vast majority of the research
community. Recent progress in the research, development and open release of
large vision models has the potential to amortize some of these costs across
the community. However, it is currently unclear which of these models have
learnt representations that retain information critical for sequential decision
making. Towards enabling wider participation in the research of gameplaying
agents in modern games, we present a systematic study of imitation learning
with publicly available visual encoders compared to the typical, task-specific,
end-to-end training approach in Minecraft, Minecraft Dungeons and
Counter-Strike: Global Offensive.
### 🌟 论文解读 | 视觉编码器在现代视频游戏中的高效模仿学习

### 📌 背景痛点/本文动机
视频游戏一直是决策制定社区的有用基准，但将研究扩展到现代游戏对于大多数研究社区来说成本高昂。近年来，大型视觉模型的研究、开发和公开发布有可能在整个社区中分摊这些成本。然而，目前尚不清楚这些模型中的哪些模型已经学习了保留对顺序决策至关重要的信息的表示。为了使更广泛的社区参与现代游戏中的游戏代理研究，本文对Minecraft、Minecraft Dungeons和Counter-Strike: Global Offensive中的模仿学习进行了系统研究，并与典型的、特定任务的端到端训练方法进行了比较。

### 🚀 核心方法
💡 创新点1：本文比较了端到端训练的视觉编码器和公开可用的预训练编码器在模仿学习中的表现。端到端训练的编码器在相对较小的图像上训练，而预训练编码器则是在大型数据集上训练的，可能提供有用且通用的表示，而无需额外的训练。

💡 创新点2：本文研究了不同数量的训练数据对视觉编码器性能的影响。结果表明，即使在使用少量高质量数据的情况下，预训练编码器也能表现出与特定任务编码器相当或更好的性能。

### 📈 实验结果
本文在三个现代视频游戏（Minecraft、Minecraft Dungeons和Counter-Strike: Global Offensive）中进行了实验，结果表明：

1. 小图像（128×128）足以训练现代视频游戏中的代理，即使在使用少量高质量数据的情况下也能取得良好的性能。
2. 预训练编码器，特别是DINOv2，在模仿学习中表现出色，并且在使用少量数据时仍然有效。
3. 端到端训练的编码器在处理更真实世界的图像时表现更好，但在使用预训练编码器时需要仔细考虑图像大小和比例。

### 💬 可借鉴之处
本文的研究结果表明，预训练编码器在现代视频游戏的模仿学习中具有巨大的潜力。研究人员可以利用这些编码器来训练代理，从而降低成本并提高效率。此外，本文还强调了图像大小和比例对预训练编码器性能的重要性，这为未来的研究提供了有价值的见解。

## building-open-ended-embodied-agent-via-language-policy-bidirectional-adaptation
### Abstract
Building embodied agents on integrating Large Language Models (LLMs) and
Reinforcement Learning (RL) have revolutionized human-AI interaction:
researchers can now leverage language instructions to plan decision-making for
open-ended tasks. However, existing research faces challenges in meeting the
requirement of open-endedness. They typically either train LLM/RL models to
adapt to a fixed counterpart, limiting exploration of novel skills and
hindering the efficacy of human-AI interaction. To this end, we present
OpenPAL, a co-training framework comprising two stages: (1) fine-tuning a
pre-trained LLM to translate human instructions into goals for planning, and
goal-conditioned training a policy for decision-making; (2) co-training to
align the LLM and policy, achieving instruction open-endedness. We conducted
experiments using Contra, an open-ended FPS game, demonstrating that an agent
trained with OpenPAL not only comprehends arbitrary instructions but also
exhibits efficient execution. These results suggest that OpenPAL holds the
potential to construct open-ended embodied agents in practical scenarios.
### 🌟 论文解读 | OpenPAL：构建开放式的具身智能体

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）和强化学习（RL）的兴起，构建能够与人类进行交互的具身智能体成为可能。然而，现有的研究在实现开放性方面面临挑战，它们通常训练LLM/RL模型以适应固定的对手，限制了探索新技能的能力，并阻碍了人机交互的有效性。

### 🚀 核心方法
OpenPAL是一个两阶段的协同训练框架，旨在实现双向适应，从而构建开放式的具身智能体。

💡 创新点1：多步微调
OpenPAL首先通过多步微调预训练的LLM，使其能够将人类指令翻译成规划目标，并训练一个目标条件策略来进行决策。

💡 创新点2：协同训练
在第二阶段，OpenPAL通过协同训练来对齐LLM和策略，实现指令的开放性。具体来说，OpenPAL交替执行以下两个过程：
1. 使用代理反馈强化学习（RLAF）对LLM进行训练，奖励LLM生成能够被代理执行的指令。
2. 使用LLM生成的目标对策略进行目标条件强化学习（GCRL）训练。

### 📈 实验结果
OpenPAL在Contra（一个开放式的FPS游戏）上进行了实验，结果表明，使用OpenPAL训练的智能体不仅能够理解任意指令，而且能够高效地执行这些指令。

### 💬 可借鉴之处
OpenPAL为构建开放式的具身智能体提供了一种有效的方法，其多步微调和协同训练的设计值得借鉴。此外，OpenPAL在Contra上的成功应用表明，该方法在复杂场景下的人机交互中具有潜力。

## deciphering-digital-detectives--understanding-llm-behaviors-and-capabilities-in-multi-agent-mystery-games
### Abstract
In this study, we explore the application of Large Language Models (LLMs) in
\textit{Jubensha}, a Chinese detective role-playing game and a novel area in
Artificial Intelligence (AI) driven gaming. We introduce the first dataset
specifically for Jubensha, including character scripts and game rules, to
foster AI agent development in this complex narrative environment. Our work
also presents a unique multi-agent interaction framework using LLMs, allowing
AI agents to autonomously engage in this game. To evaluate the gaming
performance of these AI agents, we developed novel methods measuring their
mastery of case information and reasoning skills. Furthermore, we incorporated
the latest advancements in in-context learning to improve the agents'
performance in information gathering, murderer identification, and logical
reasoning. The experimental results validate the effectiveness of our proposed
methods. This work aims to offer a novel perspective on understanding LLM
capabilities and establish a new benchmark for evaluating large language
model-based agents.
### 🌟 论文解读 | 解码数字侦探：理解大型语言模型在多智能体推理游戏中的行为和能力

### 📌 背景痛点/本文动机
随着互动角色扮演游戏（IRPGs）的全球流行，特别是中国侦探角色扮演游戏“剧本杀”（Jubensha）的兴起，人工智能（AI）在游戏领域的应用也日益受到关注。然而，现有的AI研究主要集中在传统的棋类游戏、视频游戏等领域，对于“剧本杀”这类需要多轮语言交互、信息收集和逻辑推理的游戏，AI的应用还处于起步阶段。本文旨在探索大型语言模型（LLMs）在“剧本杀”游戏中的应用，并建立一个新的评估基准，以衡量LLM在复杂叙事环境中的能力。

### 🚀 核心方法
💡 创新点1：构建了首个专门针对“剧本杀”游戏的中文数据集，包括角色剧本和预设游戏规则，为AI代理的开发提供了基础。
💡 创新点2：设计了一个独特的多智能体交互框架，使用LLMs使AI代理能够自主参与“剧本杀”游戏。
💡 创新点3：为了评估AI代理在“剧本杀”游戏中的表现，设计了两个新颖的任务：一个用于评估他们对案件信息的掌握程度，另一个用于评估他们的推理能力。
💡 创新点4：利用最新的上下文学习技术，设计了模块来增强LLM代理在信息收集、凶手识别和逻辑推理方面的性能。

### 📈 实验结果
实验结果表明，本文提出的方法在信息收集、凶手识别和推理能力方面显著提高了LLM代理的性能。具体来说，与没有记忆检索模块的代理相比，具有记忆检索模块的代理在回答关于其他角色的问题时准确率显著提高。此外，自完善和自验证模块的组合进一步提高了代理的准确率，表明这些模块有效地增强了代理在“剧本杀”游戏中的沟通效率。

### 💬 可借鉴之处
本文的研究为LLMs在复杂叙事环境中的应用提供了新的视角，并为评估LLM代理的性能建立了新的基准。此外，本文提出的ThinkThrice框架和上下文学习模块的设计，为开发更智能、更具推理能力的AI代理提供了有价值的参考。

## war-and-peace-(waragent)--large-language-model-based-multi-agent-simulation-of-world-wars
### Abstract
Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems' abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.
### 🌟 论文解读 | 利用大型语言模型模拟历史战争，探索和平的可能性

### 📌 背景痛点/本文动机
战争与和平是人类历史永恒的主题，理解战争的原因和预防战争的发生一直是人类追求的目标。传统的战争研究方法主要依赖于历史分析和文献回顾，但这些方法往往受限于静态视角和事后诸葛亮的偏见。随着人工智能和大型语言模型（LLM）的快速发展，我们有机会利用这些先进技术来模拟历史事件，探索战争与和平的动态过程，并为冲突解决和和平维护提供新的视角。

### 🚀 核心方法
本文提出了 WarAgent，一个基于 LLM 的多智能体 AI 系统，用于模拟历史国际冲突，包括第一次世界大战（WWI）、第二次世界大战（WWII）和中国古代战国时期（WSP）。WarAgent 通过模拟参与国家的决策过程和互动，探索了以下关键问题：

* **模拟有效性**：LLM 基于多智能体系统能否有效地复制历史事件中战略规划和决策过程的演变？
* **战争起因**：哪些因素是导致战争爆发的关键因素？LLM 基于多智能体系统能否识别这些因素？
* **战争不可避免性**：历史上的战争是否真的不可避免？LLM 基于多智能体系统能否揭示导致战争（或和平）的条件？

### 📈 实验结果
实验结果表明，WarAgent 能够有效地模拟历史事件，并在一定程度上复制历史决策过程和互动。例如，在 WWI 模拟中，WarAgent 能够重现主要国家之间的联盟形成、战争宣言和动员情况，与历史事件具有较高的吻合度。此外，通过改变触发事件和国家的初始条件，WarAgent 能够探索不同的战争起因和战争不可避免性，为理解历史事件和预防未来冲突提供了新的视角。

### 💬 可借鉴之处
* **LLM 在历史模拟中的应用**：WarAgent 为 LLM 在历史模拟中的应用提供了新的思路，为理解复杂的人类行为和社会动态提供了新的工具。
* **多智能体系统在冲突解决中的应用**：WarAgent 的设计理念可以为冲突解决和和平维护提供新的思路，例如通过模拟不同政策的影响来评估冲突解决策略的有效性。
* **历史教学的新方法**：WarAgent 可以作为一种新的历史教学方法，帮助学生和教师探索“如果”场景，并理解历史事件的复杂因果关系。

### 🌟 未来展望
WarAgent 的研究为 LLM 在历史模拟中的应用开辟了新的道路，未来可以进一步探索以下方向：

* **时间驱动模拟**：将 WarAgent 的回合制模拟扩展为时间驱动模拟，以更准确地模拟历史事件的时间动态。
* **停止条件**：研究更有效的停止条件，以更清晰地结束模拟并分析结果。
* **新的研究问题**：探索更多与历史事件和冲突解决相关的研究问题，例如外交沟通与冲突可能性之间的关系、非国家行为体对地缘政治的影响等。

通过不断改进和扩展 WarAgent，我们可以更好地理解历史事件，并为构建更加和平的未来提供新的思路。

## see-and-think--embodied-agent-in-virtual-environment
### Abstract
Large language models (LLMs) have achieved impressive pro-gress on several
open-world tasks. Recently, using LLMs to build embodied agents has been a
hotspot. This paper proposes STEVE, a comprehensive and visionary embodied
agent in the Minecraft virtual environment. STEVE comprises three key
components: vision perception, language instruction, and code action. Vision
perception involves interpreting visual information in the environment, which
is then integrated into the LLMs component with agent state and task
instruction. Language instruction is responsible for iterative reasoning and
decomposing complex tasks into manageable guidelines. Code action generates
executable skill actions based on retrieval in skill database, enabling the
agent to interact effectively within the Minecraft environment. We also collect
STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge
question-answering pairs, and 200+ skill-code pairs. We conduct continuous
block search, knowledge question and answering, and tech tree mastery to
evaluate the performance. Extensive experiments show that STEVE achieves at
most 1.5x faster unlocking key tech trees and 2.5x quicker in block search
tasks.
### 🌟 论文解读 | See and Think: Embodied Agent in Virtual Environment

### 📌 背景痛点/本文动机
在开放世界环境中，设计能够表现出智能行为和适应性的智能体一直是人工智能领域的一个长期而重要的挑战。然而，近年来，大型语言模型（LLMs）在开发方面取得了显著进展，展现出其作为多功能、通用型助手的潜力。尽管如此，在许多开放世界环境中，如Minecraft，当代智能体主要使用LLMs进行文本交互。然而，这种对文本通信的依赖限制了它们在这些世界中的交互，包括低级案例。Minecraft要求智能体具备各种技能，从制作基本物品到执行复杂任务。然而，由LLMs驱动的智能体往往产生不可预测的输出。它们交互的有效性在很大程度上取决于精心设计的提示，旨在将LLM的理解与环境的上下文和预期目标相一致。这种提示工程过程不仅费力，而且无法实现培养自主、自我驱动的智能体的目标。此外，文本通信在自然传达某些世界概念方面存在局限性，例如制作配方，这些概念通常通过视觉更有效地传达。

### 🚀 核心方法
💡 创新点1：提出STEVE，一个在虚拟环境中具有视觉感知、语言指令和代码动作的智能体，与之前最先进的方法相比，在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。
💡 创新点2：提出STEVE-7B/13B，一系列通过使用Llama-2-7B/13B的Minecraft知识问答对进行微调获得的大型语言模型。
💡 创新点3：收集STEVE-21K数据集，包括600多个视觉-环境对、20K个知识问答对和200多个技能-代码对，以证明STEVE的有效性能。

### 📈 实验结果
实验结果表明，STEVE在连续块搜索、知识问答和科技树掌握方面表现出色。与AutoGPT和Voyager等基线方法相比，STEVE在解锁关键技术树方面最多快1.5倍，在块搜索任务中最多快2.3倍。此外，STEVE在知识问答任务中表现出色，其性能优于Llama-2和GPT-4等更广泛的模型。

### 💬 可借鉴之处
本文提出的STEVE框架为构建具有视觉感知和语言指令能力的智能体提供了一个有价值的参考。此外，STEVE-21K数据集为研究人员提供了进行多模态学习研究的有用资源。

## designgpt--multi-agent-collaboration-in-design
### Abstract
Generative AI faces many challenges when entering the product design
workflow, such as interface usability and interaction patterns. Therefore,
based on design thinking and design process, we developed the DesignGPT
multi-agent collaboration framework, which uses artificial intelligence agents
to simulate the roles of different positions in the design company and allows
human designers to collaborate with them in natural language. Experimental
results show that compared with separate AI tools, DesignGPT improves the
performance of designers, highlighting the potential of applying multi-agent
systems that integrate design domain knowledge to product scheme design.
### 🌟 论文解读 | DesignGPT：设计流程中的多智能体协作

### 📌 背景痛点/本文动机
随着人工智能（AI）的发展，生成式AI工具在产品设计中展现出巨大的潜力，如MidJourney和Stable Diffusion等图像生成工具，以及ChatGPT等文本生成工具。然而，现有的生成式AI工具在产品设计的实际应用中面临着界面可用性和交互模式等挑战，且设计思维与机器思维之间存在天然鸿沟。如何让AI更好地理解设计思维，是设计师与AI交互的一大挑战。

### 🚀 核心方法
本文提出了DesignGPT，一个基于多智能体协作的设计框架，旨在帮助设计师与AI智能体进行自然语言协作，完成产品方案设计。DesignGPT的核心组件包括需求导入表单、角色定义与选择以及会议室。系统初始化了多个模拟设计公司不同职位的员工角色，如虚拟用户、老板、产品经理、设计总监、CMF设计师、评分记录员和技术人员，每个角色都有明确的职责和任务。设计师用户可以输入设计需求，选择角色并开始会议，与AI智能体进行自然语言交流，共同完成需求分析、设计提案、详细设计和技术迭代等工作。

### 📈 实验结果
为了评估DesignGPT的有效性，本文进行了一项在线实验，将参与者随机分为两组，分别使用DesignGPT和单独的AI工具进行设计。结果显示，与单独使用AI工具相比，DesignGPT在设计的创新性、完整性和可行性方面均有显著提升。此外，参与者对DesignGPT的交互形式给予了高度评价，认为AI角色能够熟练运用设计流程和设计表达，并从多个角度推导设计方案，突破了以往设计方案构思中视角相对单一的问题。

### 💬 可借鉴之处
DesignGPT的研究为理解AI在设计流程中的作用提供了重要的理论和实践意义。其多智能体协作的设计框架为设计师与AI的协作提供了新的思路，有助于提高设计效率和设计质量。此外，DesignGPT的实验结果也表明，将设计领域知识与多智能体系统相结合，在产品方案设计中具有巨大的潜力。

## magic--investigation-of-large-language-model-powered-multi-agent-in-cognition--adaptability--rationality-and-collaboration
### Abstract
Large Language Models (LLMs) have significantly advanced natural language
processing, demonstrating exceptional reasoning, tool usage, and memory
capabilities. As their applications expand into multi-agent environments, there
arises a need for a comprehensive evaluation framework that captures LLMs'
reasoning, planning, collaboration, and other social abilities. This work
introduces a novel competition-based benchmark framework specifically designed
to assess LLMs within multi-agent settings, providing quantitative metrics to
evaluate their judgment, reasoning, deception, self-awareness, cooperation,
coordination, and rationality. We utilize two social deduction games alongside
three game-theory scenarios to create diverse environments. Our frame is
fortified with the probabilistic graphic modeling (PGM) method, enhancing the
LLMs' capabilities in navigating complex social and cognitive dimensions. We
evaluate seven LLMs, quantitatively highlighting a significant capability gap
of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.
It also confirms that our PGM enhancement boosts the abilities of all selected
models by an average of 37%. Our data and code can be found here
https://github.com/cathyxl/MAgIC.
### 🌟 论文解读 | MAgIC：大型语言模型在认知、适应性、理性和协作中的多智能体研究

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理领域的显著进步，它们在推理、工具使用和记忆能力方面展现出卓越的能力。然而，当LLMs的应用扩展到多智能体环境时，需要一种全面的评估框架来捕捉LLMs的推理、规划、协作和其他社交能力。本文提出了一个基于竞争的基准框架，旨在评估LLMs在多智能体环境中的表现，并提供定量指标来评估它们的判断、推理、欺骗、自我意识、合作、协调和理性。

### 🚀 核心方法
💡 创新点1：竞争基准框架
本文提出了一个基于竞争的基准框架，通过将LLMs置于多智能体场景中的竞争中，评估它们在多智能体系统中的真实能力。该框架包括两个社交推理游戏和三个博弈论场景，以创建多样化的环境。

💡 创新点2：概率图模型（PGM）增强
为了增强LLMs在复杂社交和认知维度中的导航能力，本文将概率图模型（PGM）方法与LLMs相结合。PGM能够描述随机变量之间的依赖关系，从而帮助LLMs更好地理解全局信息，并做出更明智的决策。

### 📈 实验结果
本文评估了七个LLMs，包括GPT-o1、GPT-4、GPT-3.5-turbo、PaLM 2、Claude 2、Cohere和Llama-2-70B。结果表明，GPT-o1在所有评估维度中表现最佳，而Llama-2-70B表现最差。此外，PGM增强方法将所有选定模型的平均能力提高了37%。

### 💬 可借鉴之处
本文提出的竞争基准框架和PGM增强方法为评估和提升LLMs在多智能体环境中的能力提供了有价值的工具。此外，本文的研究结果为LLMs在多智能体系统中的应用提供了重要的参考和指导。

## mineland--simulating-large-scale-multi-agent-interactions-with-limited-multimodal-senses-and-physical-needs
### Abstract
While Vision-Language Models (VLMs) hold promise for tasks requiring
extensive collaboration, traditional multi-agent simulators have facilitated
rich explorations of an interactive artificial society that reflects collective
behavior. However, these existing simulators face significant limitations.
Firstly, they struggle with handling large numbers of agents due to high
resource demands. Secondly, they often assume agents possess perfect
information and limitless capabilities, hindering the ecological validity of
simulated social interactions. To bridge this gap, we propose a multi-agent
Minecraft simulator, MineLand, that bridges this gap by introducing three key
features: large-scale scalability, limited multimodal senses, and physical
needs. Our simulator supports 64 or more agents. Agents have limited visual,
auditory, and environmental awareness, forcing them to actively communicate and
collaborate to fulfill physical needs like food and resources. Additionally, we
further introduce an AI agent framework, Alex, inspired by multitasking theory,
enabling agents to handle intricate coordination and scheduling. Our
experiments demonstrate that the simulator, the corresponding benchmark, and
the AI agent framework contribute to more ecological and nuanced collective
behavior.The source code of MineLand and Alex is openly available at
https://github.com/cocacola-lab/MineLand.
### 🌟 论文解读 | MineLand：模拟大规模多智能体交互的Minecraft模拟器

### 📌 背景痛点/本文动机
传统的多智能体模拟器在处理大规模场景时面临资源消耗过大的问题，并且通常假设智能体拥有完美信息和无限能力，这与现实世界中的人类交互存在较大差距。为了解决这个问题，本文提出了MineLand，一个基于Minecraft的多智能体模拟器，旨在模拟更接近现实世界的多智能体交互。

### 🚀 核心方法
💡 创新点1：大规模可扩展性
MineLand通过将每个Minecraft客户端简化为单个线程，优化了性能开销，从而支持64个或更多智能体在主流消费级桌面PC上运行。

💡 创新点2：有限的模态感知
MineLand模拟了人类的视觉和听觉机制，对智能体的感知能力施加了限制，包括距离衰减、环境遮挡和方向约束，使其更接近现实世界。

💡 创新点3：物理需求
MineLand将真实的物理需求（如食物、氧气和饥饿）集成到智能体中，使其需要管理资源并与其他智能体竞争或合作，以维持生存。

💡 创新点4：多任务处理框架Alex
MineLand引入了基于多任务理论的AI智能体框架Alex，允许智能体同时执行复杂的协调和调度，以处理多个任务。

### 📈 实验结果
实验结果表明，MineLand在支持大规模智能体、有限的模态感知和物理需求方面表现出色。此外，Alex框架能够有效地处理多任务，并在合作模式下提高效率。

### 💬 可借鉴之处
MineLand为研究多智能体交互提供了一个强大的平台，其创新的设计和功能可以应用于人类动力学、社会心理学、机器人技术和游戏设计等领域。此外，Alex框架的多任务处理机制为开发更智能的AI智能体提供了新的思路。

## adapt--as-needed-decomposition-and-planning-with-language-models
### Abstract
Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.
### 🌟 论文解读 | ADaPT：按需分解与规划，提升大型语言模型在复杂任务中的表现

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们也逐渐被应用于需要规划和适应环境的交互式决策任务中。然而，现有的方法在处理复杂任务时面临着挑战，因为LLMs在执行子任务时可能会失败，从而导致整个任务的失败。

### 🚀 核心方法
为了解决上述问题，本文提出了ADaPT（As-Needed Decomposition and Planning for complex Tasks），一种按需分解和规划复杂任务的方法。ADaPT的核心思想是，当LLM作为执行者无法执行子任务时，将其分解为更小的子任务，并递归地进行分解，以适应任务的复杂性和LLM的能力。

#### 💡 创新点1：按需分解
ADaPT通过递归地分解子任务，动态地适应任务的复杂性和LLM的能力。当LLM作为执行者无法执行子任务时，它会调用LLM作为规划者来生成更小的子任务，并递归地调用ADaPT来执行这些子任务。

#### 💡 创新点2：多级分解
ADaPT支持多级分解，这意味着它可以进一步分解子任务，直到它们变得足够简单，可以被LLM作为执行者成功执行。这种多级分解的能力使得ADaPT能够处理更复杂的任务，并提高任务的成功率。

### 📈 实验结果
在ALFWorld、WebShop和TextCraft三个数据集上进行的实验结果表明，ADaPT显著优于现有的强基线方法，在ALFWorld上提高了28.3%的成功率，在WebShop上提高了27%，在TextCraft上提高了33%。

### 💬 可借鉴之处
ADaPT提供了一种有效的方法来处理LLMs在复杂任务中的执行失败问题。它通过按需分解和规划，动态地适应任务的复杂性和LLM的能力，从而提高了任务的成功率。此外，ADaPT的多级分解能力使其能够处理更复杂的任务，并提高任务的成功率。

## leveraging-word-guessing-games-to-assess-the-intelligence-of-large-language-models
### Abstract
The automatic evaluation of LLM-based agent intelligence is critical in
developing advanced LLM-based agents. Although considerable effort has been
devoted to developing human-annotated evaluation datasets, such as AlpacaEval,
existing techniques are costly, time-consuming, and lack adaptability. In this
paper, inspired by the popular language game ``Who is Spy'', we propose to use
the word guessing game to assess the intelligence performance of LLMs. Given a
word, the LLM is asked to describe the word and determine its identity (spy or
not) based on its and other players' descriptions. Ideally, an advanced agent
should possess the ability to accurately describe a given word using an
aggressive description while concurrently maximizing confusion in the
conservative description, enhancing its participation in the game. To this end,
we first develop DEEP to evaluate LLMs' expression and disguising abilities.
DEEP requires LLM to describe a word in aggressive and conservative modes. We
then introduce SpyGame, an interactive multi-agent framework designed to assess
LLMs' intelligence through participation in a competitive language-based board
game. Incorporating multi-agent interaction, SpyGame requires the target LLM to
possess linguistic skills and strategic thinking, providing a more
comprehensive evaluation of LLMs' human-like cognitive abilities and
adaptability in complex communication situations. The proposed evaluation
framework is very easy to implement. We collected words from multiple sources,
domains, and languages and used the proposed evaluation framework to conduct
experiments. Extensive experiments demonstrate that the proposed DEEP and
SpyGame effectively evaluate the capabilities of various LLMs, capturing their
ability to adapt to novel situations and engage in strategic communication.
### 🌟 论文解读 | 利用猜词游戏评估大型语言模型的智能

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）如ChatGPT、GPT-4和Bard等在各个任务中展现出惊人的性能，开发基于LLMs的智能代理变得越来越重要。然而，现有的评估LLMs智能的方法存在两个主要问题：1）人工标注成本高，耗时且缺乏可扩展性和适应性；2）无法全面反映LLMs的智能。为了解决这些问题，本文提出了一种新的评估方法，即利用猜词游戏来评估LLMs的智能。

### 🚀 核心方法
💡 创新点1：DEEP框架
本文首先提出了DEEP框架，用于评估LLMs的表达和伪装能力。DEEP要求LLMs以激进和保守两种模式描述一个给定的词，并利用GPT-4来判断这些描述是否准确。激进模式要求LLMs提供清晰、详细和准确的描述，而保守模式则要求LLMs提供模糊的描述以伪装目标词。

💡 创新点2：SpyGame框架
本文还提出了SpyGame框架，这是一个交互式多智能体框架，旨在通过参与竞争性语言游戏“谁是卧底”来评估LLMs的智能。SpyGame要求LLMs具备语言技能和战略思维能力，从而更全面地评估LLMs在复杂沟通情境中的人类认知能力和适应性。

### 📈 实验结果
本文对四种开源LLMs和两种闭源LLMs进行了实验，结果表明，闭源LLMs（如GPT-4和GPT-3.5）在激进和保守模式下的表现明显优于开源模型。此外，SpyGame框架能够有效地评估LLMs在多智能体交互中的能力，捕捉它们适应新情况并进行战略沟通的能力。

### 💬 可借鉴之处
本文提出的DEEP和SpyGame框架为评估LLMs的智能提供了一种新的方法，具有以下可借鉴之处：
1. 利用游戏进行评估，更具互动性和趣味性。
2. 关注LLMs的表达和伪装能力，更全面地评估其智能。
3. SpyGame框架支持人类参与，更贴近真实场景。
4. 针对多智能体交互中的偏差问题，提出了有效的解决方案。

总而言之，本文提出的评估方法为LLMs的智能评估提供了新的思路，有助于推动LLMs的发展和应用。

## language-agents-with-reinforcement-learning-for-strategic-play-in-the-werewolf-game
### Abstract
Agents built with large language models (LLMs) have shown great potential
across a wide range of domains. However, in complex decision-making tasks, pure
LLM-based agents tend to exhibit intrinsic bias in their choice of actions,
which is inherited from the model's training data and results in suboptimal
performance. To develop strategic language agents, i.e., agents that generate
flexible language actions and possess strong decision-making abilities, we
propose a novel framework that powers LLM-based agents with reinforcement
learning (RL). We consider Werewolf, a popular social deduction game, as a
challenging testbed that emphasizes versatile communication and strategic
gameplay. To mitigate the intrinsic bias in language actions, our agents use an
LLM to perform deductive reasoning and generate a diverse set of action
candidates. Then an RL policy trained to optimize the decision-making ability
chooses an action from the candidates to play in the game. Extensive
experiments show that our agents overcome the intrinsic bias and outperform
existing LLM-based agents in the Werewolf game. We also conduct human-agent
experiments and find that our agents achieve human-level performance and
demonstrate strong strategic play.
### 🌟 论文解读 | 语言智能体在狼人杀游戏中的战略决策

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在构建智能体方面的广泛应用，其在复杂决策任务中表现出的内在偏差问题逐渐凸显。这种偏差源于模型训练数据，导致LLM-based智能体在战略决策方面表现不佳。为了解决这个问题，本文提出了一种新的框架，将LLM与强化学习（RL）相结合，以构建具有灵活语言行动和强大决策能力的战略语言智能体。

### 🚀 核心方法
💡 创新点1：隐藏角色推理
本文使用LLM对游戏中的信息进行分类，区分真伪，并推断每个玩家的隐藏角色，为后续决策提供基础。

💡 创新点2：多样化行动生成
为了克服LLM的内在偏差，本文提出了一种多样化行动生成方法，通过提示LLM生成一系列行动候选者，从而避免固定模式并提高决策的灵活性。

💡 创新点3：基于群体的强化学习训练
本文采用基于群体的强化学习训练方法，通过训练一个RL策略来优化行动候选者的分布，并通过与各种智能体进行对抗来提高策略的鲁棒性。

### 📈 实验结果
本文在狼人杀游戏中进行了广泛的实验，结果表明，与现有的LLM-based智能体相比，本文提出的战略语言智能体能够克服内在偏差，并在游戏中表现出更强的战略决策能力。此外，与人类玩家的对局实验也表明，本文提出的智能体能够达到人类水平的游戏表现。

### 💬 可借鉴之处
本文提出的框架为构建具有强大决策能力的战略语言智能体提供了一种新的思路，其核心方法可以应用于其他需要灵活语言行动和战略决策的场景。此外，本文提出的多样化行动生成方法和基于群体的强化学习训练方法也为解决LLM内在偏差问题提供了新的思路。

## llm-based-agent-society-investigation--collaboration-and-confrontation-in-avalon-gameplay
### Abstract
This paper explores the open research problem of understanding the social
behaviors of LLM-based agents. Using Avalon as a testbed, we employ system
prompts to guide LLM agents in gameplay. While previous studies have touched on
gameplay with LLM agents, research on their social behaviors is lacking. We
propose a novel framework, tailored for Avalon, features a multi-agent system
facilitating efficient communication and interaction. We evaluate its
performance based on game success and analyze LLM agents' social behaviors.
Results affirm the framework's effectiveness in creating adaptive agents and
suggest LLM-based agents' potential in navigating dynamic social interactions.
By examining collaboration and confrontation behaviors, we offer insights into
this field's research and applications. Our code is publicly available at
https://github.com/3DAgentWorld/LLM-Game-Agent.
### 🌟 论文解读 | 基于大型语言模型的智能体社会行为研究：在阿瓦隆游戏中的协作与对抗

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）的快速发展，基于LLM的智能体在模拟复杂环境中的行为方面展现出巨大潜力。然而，目前的研究主要集中在LLM智能体在游戏中的表现，而对其社会行为的理解却相对缺乏。本文旨在探索LLM智能体在阿瓦隆游戏中的社会行为，包括协作与对抗，以期为LLM智能体在社会和战略环境中的应用提供新的见解。

### 🚀 核心方法
💡 创新点1：本文提出了一种新的框架，用于LLM智能体在阿瓦隆游戏中的协作与对抗。该框架包括多个模块，如记忆存储和总结、分析和规划、游戏动作和响应生成、经验学习等，以模拟人类思维过程，帮助LLM智能体更好地理解游戏并做出决策。

💡 创新点2：本文采用系统提示来引导LLM智能体进行游戏，并使用ChatGPT来分析智能体的社会行为。通过观察智能体在游戏中的协作、对抗、领导、说服、伪装等行为，本文揭示了LLM智能体在社会环境中的行为特点。

### 📈 实验结果
实验结果表明，本文提出的框架在阿瓦隆游戏中取得了优异的性能，与基线方法相比，智能体的胜率显著提高。此外，通过分析智能体的社会行为，本文发现LLM智能体能够展现出与人类相似的社会行为，如领导、说服、伪装等，这表明LLM智能体在社会环境中的应用潜力巨大。

### 💬 可借鉴之处
本文提出的框架和实验方法为研究LLM智能体在社会环境中的行为提供了新的思路。此外，本文的研究结果也为LLM智能体在社会和战略环境中的应用提供了新的见解，有助于推动LLM智能体在社会环境中的应用。

## steve-eye--equipping-llm-based-embodied-agents-with-visual-perception-in-open-worlds
### Abstract
Recent studies have presented compelling evidence that large language models
(LLMs) can equip embodied agents with the self-driven capability to interact
with the world, which marks an initial step toward versatile robotics. However,
these efforts tend to overlook the visual richness of open worlds, rendering
the entire interactive process akin to "a blindfolded text-based game."
Consequently, LLM-based agents frequently encounter challenges in intuitively
comprehending their surroundings and producing responses that are easy to
understand. In this paper, we propose Steve-Eye, an end-to-end trained large
multimodal model designed to address this limitation. Steve-Eye integrates the
LLM with a visual encoder which enables it to process visual-text inputs and
generate multimodal feedback. In addition, we use a semi-automatic strategy to
collect an extensive dataset comprising 850K open-world instruction pairs,
empowering our model to encompass three essential functions for an agent:
multimodal perception, foundational knowledge base, and skill prediction and
planning. Lastly, we develop three open-world evaluation benchmarks, then carry
out extensive experiments from a wide range of perspectives to validate our
model's capability to strategically act and plan. Codes and datasets will be
released.
### 🌟 论文解读 | Steve-Eye：为基于LLM的具身智能体赋予开放世界的视觉感知能力

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在赋予具身智能体与世界互动的能力方面取得了显著进展，这标志着通用机器人技术迈出了第一步。然而，这些研究往往忽略了开放世界的视觉丰富性，导致整个交互过程类似于“一个蒙着眼睛的基于文本的游戏”。因此，基于LLM的智能体在直观地理解周围环境和生成易于理解的响应方面经常遇到挑战。

### 🚀 核心方法
为了解决这一局限性，本文提出了Steve-Eye，这是一个端到端训练的大型多模态模型，旨在赋予基于LLM的具身智能体在开放世界中进行视觉感知的能力。Steve-Eye将LLM与视觉编码器相结合，使其能够处理视觉-文本输入并生成多模态反馈。此外，我们使用半自动策略收集了一个包含850K开放世界指令对的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库和技能预测与规划。最后，我们开发了三个开放世界评估基准，然后从广泛的视角进行大量实验，以验证我们的模型在战略行动和规划方面的能力。

### 📈 实验结果
实验结果表明，Steve-Eye在开放世界场景中显著优于基于LLM的智能体。具体来说，我们在以下三个基准上进行了评估：
1. 环境视觉描述（ENV-VC）：评估智能体感知和描述其周围环境的能力。
2. 基础知识问答（FK-QA）：评估智能体掌握对决策至关重要的基本知识的熟练程度。
3. 技能预测与规划（SPP）：量化智能体在战略行动和规划方面的能力。

### 💬 可借鉴之处
Steve-Eye的研究成果为开发能够在开放世界中有效互动的具身智能体提供了重要的启示。其多模态感知、基础知识库和技能预测与规划功能为智能体在复杂环境中进行自主行动和规划提供了强大的支持。此外，Steve-Eye的开放世界评估基准为评估具身智能体的性能提供了有价值的参考。

## sotopia--interactive-evaluation-for-social-intelligence-in-language-agents
### Abstract
Humans are social beings; we pursue social goals in our daily interactions,
which is a crucial aspect of social intelligence. Yet, AI systems' abilities in
this realm remain elusive. We present SOTOPIA, an open-ended environment to
simulate complex social interactions between artificial agents and evaluate
their social intelligence. In our environment, agents role-play and interact
under a wide variety of scenarios; they coordinate, collaborate, exchange, and
compete with each other to achieve complex social goals. We simulate the
role-play interaction between LLM-based agents and humans within this task
space and evaluate their performance with a holistic evaluation framework
called SOTOPIA-Eval. With SOTOPIA, we find significant differences between
these models in terms of their social intelligence, and we identify a subset of
SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models.
We find that on this subset, GPT-4 achieves a significantly lower goal
completion rate than humans and struggles to exhibit social commonsense
reasoning and strategic communication skills. These findings demonstrate
SOTOPIA's promise as a general platform for research on evaluating and
improving social intelligence in artificial agents.
### 🌟 论文解读 | SOTOPIA：评估语言模型社交智能的开放环境

### 📌 背景痛点/本文动机
人类在社会互动中追求复杂的社会目标，这是社交智能的关键方面。然而，AI系统在社交领域的智能仍然难以捉摸。现有的社交智能评估方法要么是非交互式的，要么缺乏对多样化目标驱动行为的关注，或者专注于特定任务。为了研究动态和目标驱动的社交智能，本文提出了SOTOPIA，一个开放式的通用领域环境，用于模拟人工代理之间的复杂社交互动并评估其社交智能。

### 🚀 核心方法
💡 创新点1：SOTOPIA环境
SOTOPIA是一个交互式环境，支持多轮模拟通信，代理可以使用语言和非语言沟通以及物理动作。它具有多样化的任务空间，包括自动生成的场景、目标、角色、关系和其他代理的策略，从而构建了一个庞大且多样化的任务空间。

💡 创新点2：SOTOPIA-EVAL评估框架
SOTOPIA-EVAL是一个多维度的评估框架，从多个社交维度分析代理的表现，包括目标完成度、可信度、知识获取、秘密保持、关系维护、社会规则遵守和财务及物质利益。

### 📈 实验结果
实验结果表明，GPT-4在SOTOPIA-EVAL的某些维度上可以作为人类判断的代理，尤其是在目标完成度方面。在模型之间，GPT-4在大多数维度上表现最佳，其次是GPT-3.5、Llama-2-70b-chat和MPT-30b-chat。然而，所有模型在遵守社会规则和保持秘密方面都存在风险。与人类相比，GPT-4在目标完成度方面显著低于人类，并且在展示社交常识推理和战略沟通技能方面存在困难。

### 💬 可借鉴之处
SOTOPIA为评估和改进人工代理的社交智能提供了一个有前景的平台。SOTOPIA-EVAL框架可以用于评估代理在多个社交维度上的表现，并帮助研究人员理解模型之间以及模型与人类之间的社交智能差异。此外，SOTOPIA还可以用于训练更具社交智能的语言代理。

## character-llm--a-trainable-agent-for-role-playing
### Abstract
Large language models (LLMs) can be used to serve as agents to simulate human
behaviors, given the powerful ability to understand human instructions and
provide high-quality generated texts. Such ability stimulates us to wonder
whether LLMs can simulate a person in a higher form than simple human
behaviors. Therefore, we aim to train an agent with the profile, experience,
and emotional states of a specific person instead of using limited prompts to
instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs
to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,
etc. Our method focuses on editing profiles as experiences of a certain
character and training models to be personal simulacra with these experiences.
To assess the effectiveness of our approach, we build a test playground that
interviews trained agents and evaluates whether the agents \textit{memorize}
their characters and experiences. Experimental results show interesting
observations that help build future simulacra of humankind.
### 🌟 论文解读 | Character-LLM：基于角色扮演的可训练智能体

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在模拟人类行为方面展现出强大的能力，但它们通常只能根据有限的提示来模拟人类行为，缺乏对特定人物角色、经验和情感状态的深入理解。本文旨在训练一个能够模拟特定人物的智能体，使其能够更真实地扮演各种角色，例如贝多芬、克莉奥帕特拉女王、凯撒大帝等。

### 🚀 核心方法
💡 创新点1：经验重建
本文提出了一种经验重建过程，通过收集特定人物的生平经历，利用LLMs提取场景，并生成详细的互动体验，从而为训练智能体提供形式化的经验数据。

💡 创新点2：经验上传与保护性经验
为了使智能体能够更好地扮演特定角色，本文引入了经验上传过程，将重建的经验数据用于微调LLMs，使其能够模拟特定人物的行为和情感。此外，为了防止智能体产生与角色不符的幻觉，本文还引入了保护性经验，帮助智能体忘记与角色无关的知识。

### 📈 实验结果
本文通过构建一个测试平台，对训练后的智能体进行访谈，评估其是否能够记住角色和经历。实验结果表明，Character-LLM能够成功地模拟特定人物，并在角色扮演方面表现出色。与基于提示的LLMs相比，Character-LLM在个性、记忆、幻觉和稳定性方面都取得了更好的成绩。

### 💬 可借鉴之处
本文提出的Character-LLM方法为构建更真实、更具个性化的智能体提供了新的思路。其经验重建和上传过程可以帮助智能体更好地理解特定人物的角色、经验和情感状态，从而在角色扮演方面表现出更高的可信度。此外，保护性经验的引入也有助于防止智能体产生与角色不符的幻觉，提高角色扮演的准确性。

## llama-rider--spurring-large-language-models-to-explore-the-open-world
### Abstract
Recently, various studies have leveraged Large Language Models (LLMs) to help
decision-making and planning in environments, and try to align the LLMs'
knowledge with the world conditions. Nonetheless, the capacity of LLMs to
continuously acquire environmental knowledge and adapt in an open world remains
uncertain. In this paper, we propose an approach to spur LLMs to explore the
open world, gather experiences, and learn to improve their task-solving
capabilities. In this approach, a multi-round feedback-revision mechanism is
utilized to encourage LLMs to actively select appropriate revision actions
guided by feedback information from the environment. This facilitates
exploration and enhances the model's performance. Besides, we integrate
sub-task relabeling to assist LLMs in maintaining consistency in sub-task
planning and help the model learn the combinatorial nature between tasks,
enabling it to complete a wider range of tasks through training based on the
acquired exploration experiences. By evaluation in Minecraft, an open-ended
sandbox world, we demonstrate that our approach LLaMA-Rider enhances the
efficiency of the LLM in exploring the environment, and effectively improves
the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k
instances of collected data, showing minimal training costs compared to the
baseline using reinforcement learning.
### 🌟 论文解读 | LLaMA Rider：激发大型语言模型探索开放世界

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在模拟人类智能方面取得了显著进展。许多研究开始利用LLMs的能力来帮助智能体在环境中进行决策，并发现LLMs具有一定的规划和完成任务的能力。然而，LLMs的知识来源于预训练时使用的语言语料库，可能与特定环境存在差异。为了将LLMs与实际环境相结合，一些研究通过提示工程设计特定机制，为LLMs提供环境信息。然而，LLMs在环境中并不会改进或获取新知识。此外，对于更复杂的任务，需要更复杂的机制和提示，这会导致LLMs生成成本高，并且依赖于像GPT-4这样具有足够知识的强大模型。还有一些研究通过微调来将LLMs与实际环境相结合，但这通常需要依赖于特定任务的训练数据集。强化学习（RL）方法也被研究，但这些方法将LLMs训练为特定任务的策略，并且我们发现RL方法难以扩展到更大的模型或更复杂的任务。

### 🚀 核心方法
本文提出了一种名为LLaMA-Rider的方法，旨在通过LLMs在开放环境中的探索来增强其能力。LLaMA-Rider是一个两阶段的学习框架，包括探索阶段和学习阶段。

#### 💡 创新点1：探索阶段
在探索阶段，LLaMA-Rider利用反馈-修正机制来鼓励LLMs主动选择适当的修正动作，以适应环境。LLMs在环境中进行探索，收集经验，并通过反馈信息来改进其决策。此外，LLaMA-Rider还使用子任务重标记来帮助LLMs保持子任务规划的连贯性，并学习任务之间的组合性质。

#### 💡 创新点2：学习阶段
在学习阶段，LLaMA-Rider将收集到的经验处理成数据集，并使用监督微调（SFT）来训练LLMs。除了从成功任务中获得的经验外，LLaMA-Rider还收集部分完成的子任务的经验，因为有些任务在探索阶段很难完成。开放环境中的许多任务通常具有组合性，这意味着过去任务的经验可以经常帮助完成其他任务。LLaMA-Rider使用子任务重标记来提高数据利用率，并帮助LLMs学习任务之间的组合性。

### 📈 实验结果
本文在Minecraft模拟器MineDojo上评估了LLaMA-Rider方法。实验结果表明，LLaMA-Rider能够有效地探索环境，并通过微调仅使用1.3k个收集到的数据实例来提高LLMs完成任务的能力，与使用强化学习的方法相比，训练成本更低。

### 💬 可借鉴之处
LLaMA-Rider方法为LLMs在开放环境中的探索和学习提供了一种有效的方法。其反馈-修正机制和子任务重标记技术可以帮助LLMs更好地适应环境，并提高其完成任务的能力。此外，LLaMA-Rider方法还可以扩展到其他开放环境，并具有终身探索和学习的潜力。

## gamegpt--multi-agent-collaborative-framework-for-game-development
### Abstract
The large language model (LLM) based agents have demonstrated their capacity
to automate and expedite software development processes. In this paper, we
focus on game development and propose a multi-agent collaborative framework,
dubbed GameGPT, to automate game development. While many studies have
pinpointed hallucination as a primary roadblock for deploying LLMs in
production, we identify another concern: redundancy. Our framework presents a
series of methods to mitigate both concerns. These methods include dual
collaboration and layered approaches with several in-house lexicons, to
mitigate the hallucination and redundancy in the planning, task identification,
and implementation phases. Furthermore, a decoupling approach is also
introduced to achieve code generation with better precision.
### 🌟 论文解读 | GameGPT：游戏开发的多智能体协作框架

### 📌 背景痛点/本文动机
随着大型语言模型（LLM）在自然语言处理和计算机视觉领域的突破，它们在自动化和加速软件开发过程方面展现出巨大潜力。然而，在游戏开发领域，LLM的应用面临着两大挑战：幻觉和冗余。幻觉指的是LLM在生成内容时可能出现的不准确或不相关的情况，而冗余则是指LLM可能会生成不必要的任务或代码片段。这些挑战限制了LLM在游戏开发中的实际应用。

### 🚀 核心方法
💡 创新点1：多智能体协作框架
GameGPT提出了一种多智能体协作框架，旨在解决LLM在游戏开发中的幻觉和冗余问题。该框架由多个具有不同角色的智能体组成，包括游戏内容设计师、游戏开发经理、计划审查员、游戏开发工程师、任务审查员、游戏引擎工程师、代码审查员和游戏引擎测试工程师。这些智能体协同工作，共同完成游戏开发的各个阶段，从而提高开发效率和准确性。

💡 创新点2：双重协作和分层方法
为了解决幻觉和冗余问题，GameGPT采用了双重协作和分层方法。双重协作包括LLM与小型专家深度学习模型之间的交互，以及执行角色和审查角色之间的协作。分层方法则通过使用多个内部词典来指导LLM的决策过程，从而减少幻觉和冗余的发生。

💡 创新点3：代码解耦
为了提高代码生成的精度，GameGPT引入了代码解耦方法。该方法将游戏设计脚本分解成多个可管理的代码片段，从而简化LLM的推理过程，并减少幻觉和冗余的发生。

### 📈 实验结果
实验结果表明，GameGPT框架在游戏开发过程中能够有效地进行决策和决策修正，从而提高开发效率和准确性。此外，GameGPT框架还具有可扩展性，可以用于开发中到大型的游戏项目。

### 💬 可借鉴之处
GameGPT框架为游戏开发自动化提供了一种新的思路和方法。其多智能体协作框架、双重协作和分层方法以及代码解耦方法等创新点，可以为其他领域的软件开发自动化提供借鉴。此外，GameGPT框架还可以与其他AI技术相结合，进一步提高游戏开发的自动化水平。

## groot--learning-to-follow-instructions-by-watching-gameplay-videos
### Abstract
We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.
### 🌟 论文解读 | GROOT：通过观看游戏视频学习指令遵循

### 📌 背景痛点/本文动机
在开放世界环境中，构建能够遵循开放指令的控制器一直是人工智能领域的长期目标。然而，现有的控制器通常只能完成预定义的、有限的程序性任务，这限制了它们在开放世界环境中的应用。本文旨在解决这个问题，提出了一种新的学习框架，通过观看游戏视频来学习指令遵循控制器。

### 🚀 核心方法
💡 创新点1：将目标指定为参考游戏视频片段，从而提供丰富的目标规范，同时消除对昂贵的文本-游戏注释的需求。
💡 创新点2：引入了一种新的学习框架，该框架同时产生一个目标空间和一个视频指令遵循控制器，从而实现从游戏视频中学习指令遵循控制器。

### 📈 实验结果
在Minecraft SkillForge基准测试中，GROOT在整体Elo评分比较中超过了最先进的基线，并且在解决具有挑战性的获取钻石任务中表现出色。

### 💬 可借鉴之处
本文提出的学习框架和GROOT代理的架构设计为构建能够遵循开放指令的控制器提供了新的思路和方法。此外，本文还展示了目标空间和控制器策略的潜在应用，为解决开放世界环境中的复杂任务提供了新的可能性。

## octopus--embodied-vision-language-programmer-from-environmental-feedback
### Abstract
Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. When integrated into an embodied agent,
existing embodied VLM works either output detailed action sequences at the
manipulation level or only provide plans at an abstract level, leaving a gap
between high-level planning and real-world manipulation. To bridge this gap, we
introduce Octopus, an embodied vision-language programmer that uses executable
code generation as a medium to connect planning and manipulation. Octopus is
designed to 1) proficiently comprehend an agent's visual and textual task
objectives, 2) formulate intricate action sequences, and 3) generate executable
code. To facilitate Octopus model development, we introduce OctoVerse: a suite
of environments tailored for benchmarking vision-based code generators on a
wide spectrum of tasks, ranging from mundane daily chores in simulators to
sophisticated interactions in complex video games such as Grand Theft Auto
(GTA) and Minecraft. To train Octopus, we leverage GPT-4 to control an
explorative agent that generates training data, i.e., action blueprints and
corresponding executable code. We also collect feedback that enables an
enhanced training scheme called Reinforcement Learning with Environmental
Feedback (RLEF). Through a series of experiments, we demonstrate Octopus's
functionality and present compelling results, showing that the proposed RLEF
refines the agent's decision-making. By open-sourcing our simulation
environments, dataset, and model architecture, we aspire to ignite further
innovation and foster collaborative applications within the broader embodied AI
community.
### 🌟 论文解读 | Octopus：基于环境反馈的具身视觉-语言编程器

### 📌 背景痛点/本文动机
随着大型视觉-语言模型（VLMs）在多模态感知和推理方面取得显著进展，将它们集成到具身智能体中成为可能。然而，现有的具身VLM工作要么在操作层面输出详细的动作序列，要么仅在抽象层面提供计划，导致高级规划和现实世界操作之间存在差距。本文旨在解决这个问题，提出了一种名为Octopus的具身视觉-语言编程器，它使用可执行代码生成作为连接规划和操作的媒介。

### 🚀 核心方法
💡 创新点1：Octopus能够熟练地理解智能体的视觉和文本任务目标，制定复杂的动作序列，并生成可执行代码。
💡 创新点2：为了促进Octopus模型的发展，本文引入了OctoVerse，这是一套为在各种任务上评估基于视觉的代码生成器而量身定制的环境，包括从模拟器中的日常家务到复杂视频游戏（如GTA和Minecraft）中的复杂交互。
💡 创新点3：为了训练Octopus，本文利用GPT-4控制一个探索性智能体，生成训练数据，即动作蓝图和相应的可执行代码。同时，收集反馈，以实现一种称为环境反馈强化学习（RLEF）的增强训练方案。

### 📈 实验结果
通过一系列实验，本文展示了Octopus的功能，并展示了令人信服的结果，表明所提出的RLEF细化了智能体的决策。Octopus在各种场景中表现出强大的适应性，在任务规划、代码生成和执行方面优于现有模型。RLEF的集成进一步增强了Octopus的性能，展示了这种训练方法的有效性。

### 💬 可借鉴之处
本文提出的Octopus模型和OctoVerse环境为具身视觉-语言编程领域提供了新的思路和方法。Octopus模型的设计和训练过程可以借鉴到其他具身智能体中，以提高其在现实世界中的操作能力。OctoVerse环境可以用于评估和比较不同的具身视觉-语言编程模型，推动该领域的研究和发展。

## metaagents--simulating-interactions-of-human-behaviors-for-llm-based-task-oriented-coordination-via-collaborative-generative-agents
### Abstract
Significant advancements have occurred in the application of Large Language
Models (LLMs) for various tasks and social simulations. Despite this, their
capacities to coordinate within task-oriented social contexts are
under-explored. Such capabilities are crucial if LLMs are to effectively mimic
human-like social behavior and produce meaningful results. To bridge this gap,
we introduce collaborative generative agents, endowing LLM-based Agents with
consistent behavior patterns and task-solving abilities. We situate these
agents in a simulated job fair environment as a case study to scrutinize their
coordination skills. We propose a novel framework that equips collaborative
generative agents with human-like reasoning abilities and specialized skills.
Our evaluation demonstrates that these agents show promising performance.
However, we also uncover limitations that hinder their effectiveness in more
complex coordination tasks. Our work provides valuable insights into the role
and evolution of LLMs in task-oriented social simulations.
### 🌟 论文解读 | MetaAgents：基于LLM的任务导向协调的协作生成式智能体

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们在模拟人类行为和执行任务方面的能力日益增强。然而，LLMs在任务导向的社会环境中的协调能力尚未得到充分探索。为了使LLMs能够有效地模拟人类的社会行为并产生有意义的结果，这种能力至关重要。

### 🚀 核心方法
本文提出了协作生成式智能体（Collaborative Generative Agents），为基于LLM的智能体赋予了一致的行为模式和任务解决能力。为了研究这些智能体的协调能力，本文构建了一个模拟的招聘会环境，并提出了一个包含感知、记忆、推理和执行模块的框架。该框架使协作生成式智能体具备类似人类的推理能力和专业技能。

### 📈 实验结果
在模拟的招聘会环境中，协作生成式智能体在识别合格求职者、设计工作流程和分配角色方面表现出良好的性能。然而，随着招聘会复杂性的增加，智能体在协调方面遇到了挑战，这主要归因于LLMs的目标或意图不匹配。

### 💬 可借鉴之处
本文提出的协作生成式智能体框架为LLMs在任务导向的社会模拟中的应用提供了有价值的见解。该框架可以应用于各种场景，例如招聘、团队协作和社交网络模拟。此外，本文还揭示了LLMs在协调任务中面临的挑战，为未来的研究提供了方向。

## humanoid-agents--platform-for-simulating-human-like-generative-agents
### Abstract
Just as computational simulations of atoms, molecules and cells have shaped
the way we study the sciences, true-to-life simulations of human-like agents
can be valuable tools for studying human behavior. We propose Humanoid Agents,
a system that guides Generative Agents to behave more like humans by
introducing three elements of System 1 processing: Basic needs (e.g. hunger,
health and energy), Emotion and Closeness in Relationships. Humanoid Agents are
able to use these dynamic elements to adapt their daily activities and
conversations with other agents, as supported with empirical experiments. Our
system is designed to be extensible to various settings, three of which we
demonstrate, as well as to other elements influencing human behavior (e.g.
empathy, moral values and cultural background). Our platform also includes a
Unity WebGL game interface for visualization and an interactive analytics
dashboard to show agent statuses over time. Our platform is available on
https://www.humanoidagents.com/ and code is on
https://github.com/HumanoidAgents/HumanoidAgents
### 🌟 论文解读 | 人类化智能体：模拟人类行为的生成式智能体平台

### 📌 背景痛点/本文动机
随着生成式智能体（Generative Agents）的出现，人们开始尝试使用高级自然语言处理系统来模拟人类行为。然而，现有的生成式智能体主要关注逻辑和计划，缺乏对人类直觉和即时反应的模拟。为了解决这个问题，本文提出了人类化智能体（Humanoid Agents）平台，旨在通过引入基本需求、情感和关系亲密度等元素，使智能体更接近人类的真实行为。

### 🚀 核心方法
💡 创新点1：引入系统1思维
本文借鉴了心理学中的系统1思维，即直觉、无意识和即时的思维过程。通过引入基本需求、情感和关系亲密度等元素，使智能体能够根据自身状态和环境变化做出更自然的反应。

💡 创新点2：动态调整行为
人类化智能体平台允许智能体根据自身的基本需求、情感和关系亲密度动态调整其日常活动和对话。例如，当智能体感到饥饿时，它会寻找食物；当它感到孤独时，它会尝试与其他智能体进行交流。

### 📈 实验结果
实验结果表明，人类化智能体能够有效地模拟人类行为。与人类标注相比，该系统能够准确预测活动是否满足基本需求、活动表达的情感以及对话是否使智能体之间的关系更加亲密。

### 💬 可借鉴之处
本文提出的Humanoid Agents平台为研究人类行为提供了一个有价值的工具。该平台可以扩展到各种场景，并支持更多影响人类行为的元素，如同理心、道德价值观和文化背景。此外，该平台还提供了Unity WebGL游戏界面和交互式分析仪表板，方便用户可视化智能体的状态和行为。

### 🌐 平台访问
- 平台网站：https://www.humanoidagents.com/
- 代码仓库：https://github.com/HumanoidAgents/HumanoidAgents

## avalonbench--evaluating-llms-playing-the-game-of-avalon
### Abstract
In this paper, we explore the potential of Large Language Models (LLMs)
Agents in playing the strategic social deduction game, Resistance Avalon.
Players in Avalon are challenged not only to make informed decisions based on
dynamically evolving game phases, but also to engage in discussions where they
must deceive, deduce, and negotiate with other players. These characteristics
make Avalon a compelling test-bed to study the decision-making and
language-processing capabilities of LLM Agents. To facilitate research in this
line, we introduce AvalonBench - a comprehensive game environment tailored for
evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game
environment for Avalon, (2) rule-based bots as baseline opponents, and (3)
ReAct-style LLM agents with tailored prompts for each role. Notably, our
evaluations based on AvalonBench highlight a clear capability gap. For
instance, models like ChatGPT playing good-role got a win rate of 22.2% against
rule-based bots playing evil, while good-role bot achieves 38.2% win rate in
the same setting. We envision AvalonBench could be a good test-bed for
developing more advanced LLMs (with self-playing) and agent frameworks that can
effectively model the layered complexities of such game environments.
### 🌟 论文解读 | AvalonBench：评估大型语言模型在社交推理游戏中的表现

### 📌 背景痛点/本文动机
社交推理游戏如 Resistance Avalon 对玩家的推理、沟通和决策能力提出了挑战。这些游戏要求玩家在动态变化的游戏阶段做出明智的决策，并在讨论中欺骗、推理和与其他玩家协商。这些特点使得 Avalon 成为研究大型语言模型（LLM）代理的决策和语言处理能力的理想测试平台。然而，目前缺乏一个全面的评估 LLM 代理在多代理游戏环境中的性能的基准测试平台。

### 🚀 核心方法
本文提出了 AvalonBench，一个专门用于评估多代理 LLM 代理的游戏环境。AvalonBench 包含以下三个关键组成部分：

1. **Avalon 游戏环境**：为代理提供游戏平台，记录所有玩家的行动并推动游戏进程。
2. **基于规则的机器人**：作为基线对手，为代理提供可比较的基准。
3. **ReAct 风格的 LLM 代理**：针对每个角色定制提示，以评估 LLM 代理在不同角色下的表现。

### 📈 实验结果
本文使用 AvalonBench 对 ChatGPT-3.5 和 Llama2 模型进行了评估，并与基于规则的机器人进行了比较。结果显示，即使在有讨论的情况下，LLM 代理的表现也远低于基于规则的机器人。例如，ChatGPT-3.5 在扮演好人角色时，在与扮演坏人的基于规则的机器人对抗中，胜率为 22.2%，而好人角色的机器人胜率为 38.2%。这表明当前 LLM 代理在推理、说服、协商和欺骗能力方面存在明显差距。

### 💬 可借鉴之处
AvalonBench 为研究 LLM 代理在社交推理游戏中的表现提供了一个有价值的测试平台。该平台可以帮助研究人员开发更先进的 LLM 代理，并探索如何将决策技术集成到 LLM 中，以提高其在复杂游戏环境中的表现。此外，AvalonBench 还可以用于评估 LLM 代理在多代理协作、沟通和策略制定方面的能力。

## beyond-win-rates--a-clustering-based-approach-to-character-balance-analysis-in-team-based-games
### Abstract
Character diversity in competitive games, while enriching gameplay, often
introduces balance challenges that can negatively impact player experience and
strategic depth. Traditional balance assessments rely on aggregate metrics like
win rates and pick rates, which offer limited insight into the intricate
dynamics of team-based games and nuanced character roles. This paper proposes a
novel clustering-based methodology to analyze character balance, leveraging
in-game data from Valorant to account for team composition influences and
reveal latent character roles. By applying hierarchical agglomerative
clustering with Jensen-Shannon Divergence to professional match data from the
Valorant Champions Tour 2022, our approach identifies distinct clusters of
agents exhibiting similar co-occurrence patterns within team compositions. This
method not only complements existing quantitative metrics but also provides a
more holistic and interpretable perspective on character synergies and
potential imbalances, offering game developers a valuable tool for informed and
context-aware balance adjustments.
### 🌟 论文解读 | 基于聚类的团队游戏角色平衡分析方法

### 📌 背景痛点/本文动机
在团队竞技游戏中，角色多样性虽然丰富了游戏玩法，但也带来了平衡挑战。传统的平衡评估方法主要依赖于胜率和选择率等聚合指标，这些指标无法全面反映团队游戏的复杂动态和角色之间的微妙关系。因此，本文提出了一种基于聚类的角色平衡分析方法，旨在更全面地理解角色之间的协同作用和潜在的不平衡。

### 🚀 核心方法
💡 创新点1：聚类分析
本文使用层次凝聚聚类算法，结合Jensen-Shannon散度作为距离度量，对角色之间的关系进行分析。通过分析角色在团队中的共现模式，将角色分为具有相似功能的组，从而揭示潜在的角色类型和协同作用。

💡 创新点2：Jensen-Shannon散度
Jensen-Shannon散度是一种对称的度量，用于量化两个概率分布之间的差异。在本研究中，它被用来衡量角色之间的相似性，从而更准确地反映角色在团队中的功能角色。

### 📈 实验结果
通过对《Valorant》2022年冠军巡回赛的专业比赛数据进行聚类分析，本文发现了一些具有相似功能的角色组，例如控制者、哨兵、决斗者和发起者。这些结果与游戏中的角色分类相吻合，但也揭示了一些新的角色类型和协同作用。

### 💬 可借鉴之处
本文提出的基于聚类的角色平衡分析方法为游戏开发者提供了一种新的视角，可以帮助他们更全面地理解角色之间的关系，并做出更明智的平衡调整。此外，该方法还可以用于评估角色调整对游戏平衡的影响，从而帮助开发者更好地维护游戏的公平性和可玩性。

## llm-coordination--evaluating-and-analyzing-multi-agent-coordination-abilities-in-large-language-models
### Abstract
The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by
Large Language Models (LLMs) make them promising candidates for developing
coordination agents. In this study, we introduce a new LLM-Coordination
Benchmark aimed at a detailed analysis of LLMs within the context of Pure
Coordination Games, where participating agents need to cooperate for the most
gain. This benchmark evaluates LLMs through two distinct tasks: (1)
\emph{Agentic Coordination}, where LLMs act as proactive participants for
cooperation in 4 pure coordination games; (2) \emph{Coordination Question
Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice
questions from the 4 games for evaluation of three key reasoning abilities:
Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to
enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture
for Coordination (CAC) framework that can easily integrate different LLMs as
plug-and-play modules for pure coordination games. Our findings indicate that
LLM agents equipped with GPT-4-turbo achieve comparable performance to
state-of-the-art reinforcement learning methods in games that require
commonsense actions based on the environment. Besides, zero-shot coordination
experiments reveal that, unlike RL methods, LLM agents are robust to new unseen
partners. However, results on Coordination QA show a large room for improvement
in the Theory of Mind reasoning and joint planning abilities of LLMs. The
analysis also sheds light on how the ability of LLMs to understand their
environment and their partner's beliefs and intentions plays a part in their
ability to plan for coordination. Our code is available at
\url{https://github.com/eric-ai-lab/llm_coordination}.
### 🌟 论文解读 | LLM-Coordination：评估和分析大型语言模型的多智能体协调能力

### 📌 背景痛点/本文动机
在许多日常任务和关键操作中，如烹饪和救援行动，合作是至关重要的。这些场景可以被视为纯协调游戏，其中所有参与方都从选择完全一致的战略中受益，避免任何利益冲突。这些游戏要求代理推理他们的环境并计划，同时考虑他们的伙伴的信念和意图。大型语言模型（LLMs）最近在物理和虚拟环境中的涌现规划能力、令人印象深刻的推理能力和对心智理论的暗示，使它们成为开发协调代理的有希望的候选者。然而，LLMs在协调游戏中的必要条件、优势和局限性仍然不清楚。本文旨在通过进行LLMs的多智能体协调能力的全面评估和分析来弥合这一差距。

### 🚀 核心方法
本文提出了一个新的LLM-Coordination基准，旨在对LLMs在纯协调游戏中的能力进行详细分析。该基准通过两个不同的任务评估LLMs：
1. **代理协调**：LLMs作为积极合作参与者参与4个纯协调游戏。
2. **协调问答（QA）**：LLMs被提示回答来自4个游戏的198个多项选择题，以评估三个关键推理能力：环境理解、心智理论推理和联合规划。

此外，为了使LLMs能够进行多智能体协调，本文引入了一个协调认知架构（CAC）框架，该框架可以轻松地将不同的LLMs作为即插即用模块集成到纯协调游戏中。

### 📈 实验结果
实验结果表明，配备GPT-4-turbo的LLM代理在需要基于环境的常识行动的游戏中，其性能与最先进的强化学习方法相当。此外，零样本协调实验表明，与RL方法不同，LLM代理对新未见伙伴具有鲁棒性。然而，协调QA的结果表明，LLMs的心智理论推理和联合规划能力还有很大的改进空间。分析还揭示了LLMs理解其环境和其伙伴的信念和意图的能力如何影响它们协调计划的能力。

### 💬 可借鉴之处
本文提出的LLM-Coordination基准和CAC框架为评估和分析LLMs的多智能体协调能力提供了一个有价值的工具。此外，本文的结果突出了LLMs在协调任务中的优势和局限性，并为未来研究提供了有价值的见解。

## lyfe-agents--generative-agents-for-low-cost-real-time-social-interactions
### Abstract
Highly autonomous generative agents powered by large language models promise
to simulate intricate social behaviors in virtual societies. However, achieving
real-time interactions with humans at a low computational cost remains
challenging. Here, we introduce Lyfe Agents. They combine low-cost with
real-time responsiveness, all while remaining intelligent and goal-oriented.
Key innovations include: (1) an option-action framework, reducing the cost of
high-level decisions; (2) asynchronous self-monitoring for better
self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing
critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation
and sociability across several multi-agent scenarios in our custom LyfeGame 3D
virtual environment platform. When equipped with our brain-inspired techniques,
Lyfe Agents can exhibit human-like self-motivated social reasoning. For
example, the agents can solve a crime (a murder mystery) through autonomous
collaboration and information exchange. Meanwhile, our techniques enabled Lyfe
Agents to operate at a computational cost 10-100 times lower than existing
alternatives. Our findings underscore the transformative potential of
autonomous generative agents to enrich human social experiences in virtual
worlds.
### 🌟 论文解读 | 低成本实时社交互动的生成式智能体：Lyfe Agents

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在模拟人类行为方面的潜力日益显现，生成式智能体在虚拟社会中模拟复杂社交行为的前景也变得光明。然而，实现与人类在低计算成本下的实时互动仍然是一个挑战。本文旨在创建一种既智能又自主的生成式智能体，能够在低计算成本下实现与人类的实时互动。

### 🚀 核心方法
💡 创新点1：选项-动作框架
为了减少高级决策的成本，Lyfe Agents 采用了一种选项-动作框架。在这种框架中，智能体首先选择一个高级动作（或“选项”），然后在后续步骤中在该选项内选择低级动作。这种设计允许智能体在更长时间内专注于执行选项背后的意图，从而降低成本并提高效率。

💡 创新点2：异步自我监控
为了提高智能体的情境意识和目标坚持性，Lyfe Agents 引入了一个自我监控模块。该模块维护一个关于最近事件的叙事风格摘要，并强调新颖和与目标相关的内容。这种自我监控摘要帮助智能体更好地理解情境，并使其行为更加一致和符合目标。

💡 创新点3：总结-遗忘记忆机制
为了提高记忆存储和检索的质量，Lyfe Agents 采用了一种层次化的记忆架构和总结-遗忘（SaF）方法。这种方法将记忆分为短期记忆和长期记忆，并通过聚类和总结技术将短期记忆中的信息转移到长期记忆中。此外，遗忘算法会评估并删除与现有记忆高度相似的旧记忆，以确保存储的信息是独特和相关的。

### 📈 实验结果
在自定义的 LyfeGame 3D 虚拟环境平台上，Lyfe Agents 在多个多智能体场景中展示了其自我激励和社会性。例如，智能体能够通过自主协作和信息交换解决犯罪（谋杀谜案）。此外，与现有方法相比，Lyfe Agents 的计算成本降低了 10-100 倍。

### 💬 可借鉴之处
Lyfe Agents 的设计原则和架构组件为构建低成本、实时响应的生成式智能体提供了有价值的参考。其选项-动作框架、异步自我监控和总结-遗忘记忆机制等创新点可以应用于其他生成式智能体框架，以提高其自主性和社交推理能力。此外，LyfeGame 虚拟环境平台也为研究生成式智能体的社会行为和用户交互提供了有价值的工具。

## adaptive-multi-goal-exploration
### Abstract
We introduce a generic strategy for provably efficient multi-goal
exploration. It relies on AdaGoal, a novel goal selection scheme that leverages
a measure of uncertainty in reaching states to adaptively target goals that are
neither too difficult nor too easy. We show how AdaGoal can be used to tackle
the objective of learning an $\epsilon$-optimal goal-conditioned policy for the
(initially unknown) set of goal states that are reachable within $L$ steps in
expectation from a reference state $s_0$ in a reward-free Markov decision
process. In the tabular case with $S$ states and $A$ actions, our algorithm
requires $\tilde{O}(L^3 S A \epsilon^{-2})$ exploration steps, which is nearly
minimax optimal. We also readily instantiate AdaGoal in linear mixture Markov
decision processes, yielding the first goal-oriented PAC guarantee with linear
function approximation. Beyond its strong theoretical guarantees, we anchor
AdaGoal in goal-conditioned deep reinforcement learning, both conceptually and
empirically, by connecting its idea of selecting "uncertain" goals to
maximizing value ensemble disagreement.
### 🌟 论文解读 | 自适应多目标探索：AdaGoal策略

### 📌 背景痛点/本文动机
在强化学习中，当外部奖励信号缺失或不具信息性时，智能体需要通过探索环境来学习，而不是简单地最大化奖励。这种情况下，智能体需要自主设定目标并学习有效地达到这些目标。然而，现有的无监督目标条件强化学习（GC-RL）方法往往缺乏理论支持和保证，尤其是在深度学习环境中。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：多目标探索（MGE）目标
本文提出了一个通用的策略，用于证明有效的多目标探索。该策略依赖于AdaGoal，这是一种新颖的目标选择方案，它利用到达状态的不确定性度量来适应性地选择既不太难也不太容易的目标。

💡 创新点2：AdaGoal目标选择方案
AdaGoal通过解决一个简单的优化问题来适应性地选择中等难度的目标状态。它还提供了一个算法停止规则和一个候选目标状态集，智能体对这些状态有信心可以可靠地到达。

💡 创新点3：AdaGoal-UCBVI和AdaGoal-UCRL·VTR算法
本文设计了AdaGoal-UCBVI算法，用于在表格MDP中解决MGE问题，并证明了它几乎是最优的样本复杂度。此外，还设计了AdaGoal-UCRL·VTR算法，用于线性混合MDP，这是第一个具有线性函数近似的面向目标的PAC保证。

💡 创新点4：AdaGoal在深度GC-RL中的应用
本文将AdaGoal的概念与深度GC-RL中的最大化价值集合分歧联系起来，从而在概念和经验上将其锚定在目标条件深度强化学习中。

### 📈 实验结果
在实验中，AdaGoal-UCBVI和AdaGoal-UCRL·VTR算法在表格MDP和线性混合MDP中均表现出良好的性能，证明了其有效性和鲁棒性。

### 💬 可借鉴之处
本文提出的AdaGoal策略为无监督目标条件强化学习提供了一种新的思路，具有以下可借鉴之处：

*   **自适应目标选择**：AdaGoal通过考虑目标状态的不确定性来选择目标，从而更有效地探索环境。
*   **理论保证**：AdaGoal-UCBVI和AdaGoal-UCRL·VTR算法具有几乎最优的样本复杂度，为无监督目标条件强化学习提供了理论支持。
*   **深度学习应用**：AdaGoal的概念可以与深度学习技术相结合，从而在复杂的任务中实现有效的目标探索。

### 🌟 总结
本文提出的AdaGoal策略为无监督目标条件强化学习提供了一种新的思路，并取得了显著的成果。AdaGoal策略具有自适应目标选择、理论保证和深度学习应用等优点，为未来的研究提供了重要的参考价值。

## smartplay--a-benchmark-for-llms-as-intelligent-agents
### Abstract
Recent large language models (LLMs) have demonstrated great potential toward
intelligent agents and next-gen automation, but there currently lacks a
systematic benchmark for evaluating LLMs' abilities as agents. We introduce
SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs
as agents. SmartPlay consists of 6 different games, including
Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique
setting, providing up to 20 evaluation settings and infinite environment
variations. Each game in SmartPlay uniquely challenges a subset of 9 important
capabilities of an intelligent LLM agent, including reasoning with object
dependencies, planning ahead, spatial reasoning, learning from history, and
understanding randomness. The distinction between the set of capabilities each
game test allows us to analyze each capability separately. SmartPlay serves not
only as a rigorous testing ground for evaluating the overall performance of LLM
agents but also as a road-map for identifying gaps in current methodologies. We
release our benchmark at github.com/Microsoft/SmartPlay
### 🌟 论文解读 | SmartPlay：评估大型语言模型智能体能力的基准

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在智能体和下一代自动化方面展现出巨大潜力。然而，目前缺乏一个系统性的基准来评估LLMs作为智能体的能力。本文提出了SmartPlay，这是一个具有挑战性的基准和评估LLMs作为智能体的方法论。

### 🚀 核心方法
💡 创新点1：SmartPlay由6个不同的游戏组成，包括剪刀石头布、汉诺塔、Minecraft等。每个游戏都有独特的设置，提供多达20个评估设置和无限的环境变化。每个游戏都挑战了智能LLM智能体的9个重要能力，包括推理对象依赖关系、前瞻性规划、空间推理、从历史中学习以及理解随机性。

💡 创新点2：SmartPlay不仅是一个严格的测试场，用于评估LLM智能体的整体性能，也是一个路线图，用于识别当前方法中的差距。SmartPlay为LLMs提供了统一和可扩展的API，具有文本观察和指导，以执行回合制的LLM推理。

### 📈 实验结果
实验结果表明，GPT-4变体在所有游戏中都显著优于其他LLMs，但在更具挑战性的基准测试中，与人类基线性能仍存在显著差距。其他专有LLMs在Crafter等综合基准测试中难以与GPT-4竞争。开源LLMs在简单任务和更具挑战性任务上的表现均不如GPT-4变体。

### 💬 可借鉴之处
SmartPlay为评估LLMs作为智能体的能力提供了一个全面的基准。它不仅涵盖了基本的指令遵循和上下文推理能力，还评估了规划、理解随机性、2D/3D空间推理和错误处理等能力。SmartPlay的发布将推动LLMs作为智能体领域的研究，并为构建更强大和可靠的LLM智能体提供指导。

## avalon-s-game-of-thoughts--battle-against-deception-through-recursive-contemplation
### Abstract
Recent breakthroughs in large language models (LLMs) have brought remarkable
success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is
that the information processed by LLMs is consistently honest, neglecting the
pervasive deceptive or misleading information in human society and AI-generated
content. This oversight makes LLMs susceptible to malicious manipulations,
potentially resulting in detrimental outcomes. This study utilizes the
intricate Avalon game as a testbed to explore LLMs' potential in deceptive
environments. Avalon, full of misinformation and requiring sophisticated logic,
manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans'
recursive thinking and perspective-taking in the Avalon game, we introduce a
novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to
identify and counteract deceptive information. ReCon combines formulation and
refinement contemplation processes; formulation contemplation produces initial
thoughts and speech, while refinement contemplation further polishes them.
Additionally, we incorporate first-order and second-order perspective
transitions into these processes respectively. Specifically, the first-order
allows an LLM agent to infer others' mental states, and the second-order
involves understanding how others perceive the agent's mental state. After
integrating ReCon with different LLMs, extensive experiment results from the
Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around
deceptive information without extra fine-tuning and data. Finally, we offer a
possible explanation for the efficacy of ReCon and explore the current
limitations of LLMs in terms of safety, reasoning, speaking style, and format,
potentially furnishing insights for subsequent research.
### 🌟 论文解读 | Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在作为智能体（LLM-as-Agent）方面取得了显著进展。然而，这些研究通常假设LLMs处理的信息始终是诚实的，忽略了人类社会中普遍存在的欺骗性或误导性信息以及AI生成内容中的潜在问题。这种假设使得LLMs容易受到恶意操纵，可能导致不良后果。本文旨在探索LLMs在欺骗性环境中的潜力，并提出了一个名为“递归沉思”（ReCon）的新框架，以增强LLMs识别和对抗欺骗性信息的能力。

### 🚀 核心方法
💡 创新点1：递归沉思框架（ReCon）
ReCon框架结合了“构思沉思”和“精炼沉思”两个认知过程。构思沉思产生初始思考和言语，而精炼沉思则进一步改进它们。此外，ReCon还引入了第一阶和第二阶视角转换，分别对应于这两个过程。第一阶视角转换允许LLM智能体从自己的角度推断他人的心理状态，而第二阶视角转换则涉及理解他人如何看待智能体的心理状态。

💡 创新点2：在Avalon游戏中测试ReCon
本文使用复杂的Avalon游戏作为测试平台，该游戏充满误导信息，需要复杂的逻辑推理。实验结果表明，ReCon能够有效地帮助LLMs识别和应对欺骗性信息，而无需额外的微调和数据。

### 📈 实验结果
实验结果表明，ReCon在Avalon游戏中表现出色，能够帮助LLMs识别和应对欺骗性信息，而无需额外的微调和数据。与基线方法相比，ReCon在多个维度上都有显著提升，包括隐蔽性、逻辑性、贡献度、说服力、信息量和创造力。

### 💬 可借鉴之处
本文提出的ReCon框架为LLMs在欺骗性环境中的应用提供了新的思路。ReCon框架的设计和实现方法可以借鉴到其他需要识别和对抗欺骗性信息的场景中。此外，本文还讨论了LLMs在安全性、推理能力、言语风格和格式等方面的局限性，为未来的研究提供了有价值的见解。

## rolellm--benchmarking--eliciting--and-enhancing-role-playing-abilities-of-large-language-models
### Abstract
The advent of Large Language Models (LLMs) has paved the way for complex
tasks such as role-playing, which enhances user interactions by enabling models
to imitate various characters. However, the closed-source nature of
state-of-the-art LLMs and their general-purpose training limit role-playing
optimization. In this paper, we introduce RoleLLM, a framework to benchmark,
elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four
stages: (1) Role Profile Construction for 100 roles; (2) Context-Based
Instruction Generation (Context-Instruct) for role-specific knowledge
extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style
imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning
open-source models along with role customization. By Context-Instruct and
RoleGPT, we create RoleBench, the first systematic and fine-grained
character-level benchmark dataset for role-playing with 168,093 samples.
Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese),
significantly enhancing role-playing abilities and even achieving comparable
results with RoleGPT (using GPT-4).
### 🌟 论文解读 | RoleLLM：解锁大型语言模型的角色扮演能力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的兴起，角色扮演等复杂任务成为可能，为用户交互提供了更多可能性。然而，现有开源LLMs在角色扮演方面存在优化不足的问题，而最先进的LLMs如GPT-4等则因其闭源性质而限制了其在角色扮演方面的应用。本文旨在解决这些问题，提出RoleLLM框架，以提升LLMs的角色扮演能力。

### 🚀 核心方法
💡 创新点1：Role Profile Construction
构建了100个角色的详细档案，包括角色描述、口头禅以及从剧本中提取的对话片段，为角色扮演提供了丰富的背景知识。

💡 创新点2：Context-Based Instruction Generation (Context-Instruct)
利用GPT模型从角色档案中生成高质量的问答对，以提取角色特定的知识和记忆。

💡 创新点3：Role Prompting using GPT (RoleGPT)
通过对话工程和检索增强技术，利用GPT模型生成符合角色说话风格的回答，以模仿角色的说话风格。

💡 创新点4：Role-Conditioned Instruction Tuning (RoCIT)
利用Context-Instruct和RoleGPT生成的数据，对开源LLMs进行微调，以提升其角色扮演能力，并生成RoleLLaMA和RoleGLM等模型。

### 📈 实验结果
实验结果表明，RoleLLM框架在角色扮演方面取得了显著成果。RoleLLaMA和RoleGLM在模仿角色说话风格、回答准确性和角色特定知识掌握方面表现出色，甚至在某些情况下与RoleGPT（使用GPT-4）相当。

### 💬 可借鉴之处
RoleLLM框架为LLMs的角色扮演能力提升提供了新的思路和方法，其创新点包括角色档案构建、基于上下文的指令生成、角色提示和角色条件指令微调等。此外，RoleBench数据集的构建也为角色扮演能力的评估和提升提供了重要的参考。

## adarefiner--refining-decisions-of-language-models-with-adaptive-feedback
### Abstract
Large Language Models (LLMs) have demonstrated significant success across
various domains. However, their application in complex decision-making tasks
frequently necessitates intricate prompt engineering or fine-tuning, leading to
challenges in unseen downstream tasks and heavy demands on computational
resources. Meanwhile, Reinforcement Learning (RL) has been recognized as
effective in decision-making problems but struggles in environments with sparse
rewards, such as open-world games. To overcome these challenges, we introduce
AdaRefiner, a novel framework designed to enhance the synergy between LLMs and
RL feedback. The key component of AdaRefiner is a lightweight Adapter Language
Model (LM), which automatically refines task comprehension based on feedback
from RL agents. This method mitigates the need for intricate prompt engineering
and intensive LLM fine-tuning while maintaining the LLMs' generalization
abilities and enhancing their decision-making capabilities in downstream tasks.
Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world
game Crafter have demonstrated its superior effectiveness, especially in
guiding agents towards higher-level and common-sense skills. Our work makes
contributions to the automatic self-refinement of LLMs with RL feedback,
offering a more adaptable and efficient solution for complex decision-making
problems.
### 🌟 论文解读 | AdaRefiner：利用自适应反馈提升语言模型决策能力

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在各个领域取得了显著的成功，但在复杂决策任务中的应用却面临着挑战。LLMs 需要进行繁琐的提示工程或微调才能适应特定任务，这限制了其在未知下游任务中的泛化能力，并带来了对计算资源的巨大需求。另一方面，强化学习（RL）在决策问题中表现出色，但在稀疏奖励的环境中（如开放世界游戏）却难以发挥作用。为了克服这些挑战，本文提出了 AdaRefiner，一个旨在增强 LLMs 和 RL 反馈之间协同作用的新框架。

### 🚀 核心方法
💡 创新点1：AdaRefiner 引入了一个轻量级的适配器语言模型（LM），该模型根据 RL 代理的反馈自动细化任务理解。这种方法减少了繁琐的提示工程和密集的 LLM 微调的需求，同时保持了 LLMs 的泛化能力，并增强了它们在下游任务中的决策能力。

💡 创新点2：AdaRefiner 在开放世界游戏 Crafter 的 22 个不同任务中进行了实证评估，结果表明其在引导代理学习高级和常识技能方面具有优越的有效性。

### 📈 实验结果
AdaRefiner 在 Crafter 环境中的 22 个任务上进行了评估，结果表明其性能优于最先进的基线方法。AdaRefiner 能够引导代理学习高级技能，并表现出常识行为。消融研究表明，适配器 LM 和 RL 反馈对于 AdaRefiner 的有效性至关重要。

### 💬 可借鉴之处
AdaRefiner 为 LLMs 在复杂决策任务中的应用提供了一种更灵活和高效的解决方案。其轻量级的适配器 LM 和自适应反馈机制可以有效地提升 LLMs 的任务理解和决策能力，为 LLMs 在开放世界游戏等复杂环境中的应用开辟了新的可能性。

## motif--intrinsic-motivation-from-artificial-intelligence-feedback
### Abstract
Exploring rich environments and evaluating one's actions without prior
knowledge is immensely challenging. In this paper, we propose Motif, a general
method to interface such prior knowledge from a Large Language Model (LLM) with
an agent. Motif is based on the idea of grounding LLMs for decision-making
without requiring them to interact with the environment: it elicits preferences
from an LLM over pairs of captions to construct an intrinsic reward, which is
then used to train agents with reinforcement learning. We evaluate Motif's
performance and behavior on the challenging, open-ended and
procedurally-generated NetHack game. Surprisingly, by only learning to maximize
its intrinsic reward, Motif achieves a higher game score than an algorithm
directly trained to maximize the score itself. When combining Motif's intrinsic
reward with the environment reward, our method significantly outperforms
existing approaches and makes progress on tasks where no advancements have ever
been made without demonstrations. Finally, we show that Motif mostly generates
intuitive human-aligned behaviors which can be steered easily through prompt
modifications, while scaling well with the LLM size and the amount of
information given in the prompt.
### 🌟 论文解读 | Motif：从人工智能反馈中获取内在动机

### 📌 背景痛点/本文动机
在复杂环境中，没有先验知识的智能体探索和评估其行为极具挑战性。本文提出了一种名为 Motif 的新方法，旨在将大型语言模型 (LLM) 中的先验知识与智能体进行交互，从而帮助智能体在没有与环境的直接交互的情况下进行决策。

### 🚀 核心方法
💡 创新点1：利用 LLM 的偏好构建内在奖励
Motif 的核心思想是，通过 LLM 对事件标题的偏好来构建内在奖励函数，并将其用于强化学习训练智能体。LLM 表达对成对事件标题的偏好，这些标题只需粗略描述环境中发生的事件，而不需要精细的逐步描述。LLM 不需要理解低级动作空间，这可能是复合的或连续的。

💡 创新点2：内在奖励与外在奖励的结合
Motif 的内在奖励可以单独使用，也可以与来自环境的奖励信号结合使用。实验表明，当内在奖励与外在奖励结合使用时，Motif 的性能显著优于现有方法，并在没有演示的情况下取得了进展。

### 📈 实验结果
Motif 在 NetHack 学习环境 (NLE) 上进行了评估，这是一个具有挑战性、开放性和程序生成的游戏。结果表明，仅通过学习最大化其内在奖励，Motif 就取得了比直接训练以最大化分数的算法更高的游戏分数。当将 Motif 的内在奖励与环境的奖励相结合时，该方法显著优于现有方法，并在没有演示的情况下取得了进展。

### 💬 可借鉴之处
Motif 为利用 LLM 的先验知识和常识来创建智能体提供了一种通用的方法。它通过将 LLM 的高层次知识与智能体操作的底层传感器运动现实之间的差距，从而有效地将知识提炼出来。Motif 具有可扩展性，可以与更大规模的 LLM 或特定领域的微调 LLM 结合使用，并可以通过提示修改轻松地引导智能体的行为。

## simulbench--evaluating-language-models-with-creative-simulation-tasks
### Abstract
We introduce SimulBench, a benchmark designed to evaluate large language
models (LLMs) across a diverse collection of creative simulation scenarios,
such as acting as a Linux terminal or playing text games with users. While
these simulation tasks serve as effective measures of an LLM's general
intelligence, they are seldom incorporated into existing benchmarks. A major
challenge is to develop an evaluation framework for testing different LLMs
fairly while preserving the multi-round interactive nature of simulation tasks
between users and AI. To tackle this issue, we suggest using a fixed LLM as a
user agent to engage with an LLM to collect dialogues first under different
tasks. Then, challenging dialogue scripts are extracted for evaluating
different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4
is employed as the evaluator, tasked with reviewing the quality of the final
response generated by the target LLMs given multi-turn dialogue scripts. Our
comprehensive experiments indicate that these simulation tasks continue to pose
a significant challenge with their unique natures and show the gap between
proprietary models and the most advanced open LLMs. For example, GPT-4-turbo
outperforms LLaMA-3-70b-Chat on 18.55\% more cases.
### 🌟 论文解读 | SimulBench：评估语言模型在创意模拟任务中的表现

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在模拟复杂任务方面的能力日益增强，评估这些模型在模拟任务中的表现变得至关重要。然而，现有的评估基准主要集中在单轮、静态的用户与LLMs之间的交互，缺乏对多轮交互和复杂模拟能力的评估。此外，现有的基准主要集中在与人类相关的模拟任务上，而忽略了非人类中心的模拟任务，如Linux终端或文本游戏等。

### 🚀 核心方法
💡 创新点1：SimulBench基准
本文提出了SimulBench，一个旨在评估LLMs在创意模拟任务中的表现的基准。SimulBench包含109个独特的模拟任务，涵盖了各种接口，如Linux终端、SQL执行器、文本游戏等。

💡 创新点2：多轮脚本评估框架
为了公平地评估不同LLMs，SimulBench采用了一个三阶段的评估框架。首先，使用一个固定的LLM作为用户代理与另一个LLM进行多轮对话，收集对话历史。然后，从这些对话历史中提取具有挑战性的对话脚本，用于评估不同的目标LLMs。最后，使用GPT-4作为评估者，对目标LLMs在给定多轮对话脚本下的最终响应质量进行评估。

### 📈 实验结果
实验结果表明，SimulBench中的模拟任务对LLMs来说仍然是一个巨大的挑战，并且显示了专有模型和最先进的开源LLMs之间的差距。例如，GPT-4-turbo在18.55%的情况下优于LLaMA-3-70b-Chat。

### 💬 可借鉴之处
SimulBench基准为评估LLMs在模拟任务中的表现提供了一个有价值的工具。其多轮脚本评估框架可以确保公平的比较，并有助于研究人员更好地理解LLMs在不同模拟任务中的表现。此外，SimulBench的实验结果也揭示了LLMs在处理复杂模拟任务时的挑战和局限性，为未来的研究提供了方向。

## suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4
### Abstract
Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.
### 🌟 论文解读 | 利用GPT-4的“心智理论”能力玩不完美信息游戏

### 📌 背景痛点/本文动机
在现实世界中，决策往往是在信息不完整或不确定的情况下进行的。然而，大多数现有的AI算法都是在完美信息游戏中训练的，即所有玩家都能看到所有信息。这限制了它们在现实世界中的应用。本文旨在探索如何利用大型语言模型（LLM）的知识和推理能力来处理不完美信息游戏，从而更好地模拟现实世界的决策过程。

### 🚀 核心方法
本文提出了一个名为Suspicion-Agent的创新型自主代理，它基于GPT-4，并利用其强大的知识检索和推理能力来玩不完美信息游戏。Suspicion-Agent的核心创新点包括：

💡 创新点1：利用GPT-4的“心智理论”（ToM）能力，即理解他人并有意影响他人行为的能力。这使得Suspicion-Agent能够预测对手的行为，并根据对手的行为调整自己的策略。

💡 创新点2：将游戏过程分解为多个子模块，如观察解释器、游戏模式分析和规划模块。每个模块都使用不同的提示来引导GPT-4执行特定的功能，从而实现更有效的决策。

### 📈 实验结果
在实验中，Suspicion-Agent在三个不同的不完美信息游戏中展示了其能力，并在Leduc Hold'em游戏中进行了定量评估。结果表明，Suspicion-Agent可以潜在地超越传统算法，而无需任何专门的训练或示例。

### 💬 可借鉴之处
本文提出的Suspicion-Agent框架为利用LLM在不完美信息游戏中进行决策提供了一个新的思路。其核心思想是将LLM的知识和推理能力与ToM能力相结合，从而实现更有效的决策。此外，本文还公开了所有与游戏相关的数据，这将有助于研究人员更好地理解LLM的能力，并开发更有效的模型。

## autoagents--a-framework-for-automatic-agent-generation
### Abstract
Large language models (LLMs) have enabled remarkable advances in automated
task-solving with multi-agent systems. However, most existing LLM-based
multi-agent approaches rely on predefined agents to handle simple tasks,
limiting the adaptability of multi-agent collaboration to different scenarios.
Therefore, we introduce AutoAgents, an innovative framework that adaptively
generates and coordinates multiple specialized agents to build an AI team
according to different tasks. Specifically, AutoAgents couples the relationship
between tasks and roles by dynamically generating multiple required agents
based on task content and planning solutions for the current task based on the
generated expert agents. Multiple specialized agents collaborate with each
other to efficiently accomplish tasks. Concurrently, an observer role is
incorporated into the framework to reflect on the designated plans and agents'
responses and improve upon them. Our experiments on various benchmarks
demonstrate that AutoAgents generates more coherent and accurate solutions than
the existing multi-agent methods. This underscores the significance of
assigning different roles to different tasks and of team cooperation, offering
new perspectives for tackling complex tasks. The repository of this project is
available at https://github.com/Link-AGI/AutoAgents.
### 🌟 论文解读 | AutoAgents：自动生成智能体框架，助力复杂任务解决

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在自动任务解决方面取得了显著进展，但现有的基于LLMs的多智能体方法大多依赖于预定义的智能体来处理简单任务，限制了多智能体协作在不同场景下的适应性。为了解决这一问题，本文提出了AutoAgents框架，该框架能够根据不同任务自适应地生成和协调多个专业智能体，构建AI团队。

### 🚀 核心方法
💡 创新点1：动态生成智能体
AutoAgents通过分析任务内容，动态生成多个所需的智能体，并根据生成的专家智能体规划当前任务的解决方案。这些专业智能体相互协作，高效地完成任务。

💡 创新点2：引入观察者角色
AutoAgents框架中引入了观察者角色，用于反思指定的计划和智能体的响应，并进行改进。观察者角色有助于提高智能体团队的整体性能和适应性。

💡 创新点3：任务执行动作
AutoAgents框架包括两种任务执行动作：单个智能体的自我改进和多个智能体的协作改进。自我改进使单个智能体能够提高其在执行某些专业任务方面的能力，而协作改进则促进多个智能体之间的知识共享，完成需要跨学科专业知识才能完成的任务。

💡 创新点4：知识共享机制
AutoAgents框架提供了短期记忆、长期记忆和动态记忆三种知识共享机制。短期记忆记录单个动作的自我改进或协作改进阶段的历史，长期记忆记录多个动作的历史轨迹，动态记忆则用于为需要特殊关注的动作提供辅助信息。

### 📈 实验结果
AutoAgents在开放性问题回答和Trivia创意写作等基准任务上的实验结果表明，该框架能够生成比现有多智能体方法更一致、更准确的解决方案。此外，AutoAgents在软件开发等复杂任务中的应用案例也展示了其灵活性和潜在优势。

### 💬 可借鉴之处
AutoAgents框架为解决复杂任务提供了一种新的思路，其动态生成智能体、引入观察者角色、任务执行动作和知识共享机制等方面的创新点值得借鉴。此外，AutoAgents框架在软件开发等领域的应用案例也为其他领域提供了参考。

## true-knowledge-comes-from-practice--aligning-llms-with-embodied-environments-via-reinforcement-learning
### Abstract
Despite the impressive performance across numerous tasks, large language
models (LLMs) often fail in solving simple decision-making tasks due to the
misalignment of the knowledge in LLMs with environments. On the contrary,
reinforcement learning (RL) agents learn policies from scratch, which makes
them always align with environments but difficult to incorporate prior
knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a
novel general online framework that deploys LLMs as decision-making agents to
efficiently interact and align with embodied environments via RL without
requiring any prepared datasets or prior knowledge of the environments.
Firstly, we query the joint probabilities of each valid action with LLMs to
form behavior policies. Then, to enhance the stability and robustness of the
policies, we propose two normalization methods and summarize four prompt design
principles. Finally, we design a novel parameter-efficient training
architecture where the actor and critic share one frozen LLM equipped with
low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to
evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency
and performance compared to the conventional RL method, PPO, and prompt tuning
method, SayCan, in both classical decision-making environment, Overcooked, and
simulated household environment, VirtualHome. ii) Benefiting from LLMs'
open-vocabulary feature, TWOSOME shows superior generalization ability to
unseen tasks. iii) Under our framework, there is no significant loss of the
LLMs' original ability during online PPO finetuning.
### 🌟 论文解读 | 知识源于实践：通过强化学习将大型语言模型与具身环境对齐

### 📌 背景痛点/本文动机
尽管大型语言模型（LLMs）在众多任务中表现出色，但它们在解决简单的决策任务时往往失败，这主要是因为LLMs中的知识与环境的错位。相反，强化学习（RL）代理从零开始学习策略，这使得它们始终与环境保持一致，但难以融入先验知识以提高探索效率。为了缩小这一差距，本文提出了TWOSOME，一个新颖的通用在线框架，该框架将LLMs作为决策代理，通过RL高效地与具身环境交互并对齐，而无需任何准备好的数据集或对环境的先验知识。

### 🚀 核心方法
💡 创新点1：行为策略生成
TWOSOME不是让LLMs直接生成动作，而是查询LLMs中每个有效动作的联合概率，以形成行为策略。这消除了由于无效动作造成的错位问题。

💡 创新点2：动作提示归一化
为了增强策略的稳定性和鲁棒性，本文提出了两种归一化方法：token归一化和word归一化，以解决动作分布不平衡的问题。

💡 创新点3：参数高效的训练架构
本文设计了一种新颖的参数高效训练架构，其中actor和critic共享一个冻结的LLM，并配备低秩适配器（LoRA），由PPO更新。

💡 创新点4：提示设计原则
本文总结了四个提示设计原则，以增强LLMs的推理能力，从而提高LLMs与具身环境之间的对齐。

### 📈 实验结果
在经典决策环境Overcooked和模拟家庭环境VirtualHome中，TWOSOME在样本效率和性能方面显著优于传统的RL方法PPO和提示调整方法SayCan。此外，TWOSOME还表现出对未见任务的优越泛化能力。

### 💬 可借鉴之处
本文提出的TWOSOME框架为将LLMs与具身环境对齐提供了一个有效的解决方案，并为开发通用的自主代理迈出了重要的一步。该框架具有以下可借鉴之处：

*   利用LLMs的先验知识来提高RL代理的样本效率。
*   通过归一化方法解决动作分布不平衡的问题。
*   设计有效的提示以提高LLMs的推理能力。
*   使用参数高效的训练架构来降低训练成本。

### 🎯 未来展望
未来可以进一步研究如何将TWOSOME框架应用于更复杂的具身环境，并探索LLMs与RL代理之间的更深入交互。

## mindagent--emergent-gaming-interaction
### Abstract
Large Language Models (LLMs) have the capacity of performing complex
scheduling in a multi-agent system and can coordinate these agents into
completing sophisticated tasks that require extensive collaboration. However,
despite the introduction of numerous gaming frameworks, the community has
insufficient benchmarks towards building general multi-agents collaboration
infrastructure that encompass both LLM and human-NPCs collaborations. In this
work, we propose a novel infrastructure - MindAgent - to evaluate planning and
coordination emergent capabilities for gaming interaction. In particular, our
infrastructure leverages existing gaming framework, to i) require understanding
of the coordinator for a multi-agent system, ii) collaborate with human players
via un-finetuned proper instructions, and iii) establish an in-context learning
on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new
gaming scenario and related benchmark that dispatch a multi-agent collaboration
efficiency and supervise multiple agents playing the game simultaneously. We
conduct comprehensive evaluations with new auto-metric CoS for calculating the
collaboration efficiency. Finally, our infrastructure can be deployed into
real-world gaming scenarios in a customized VR version of CUISINEWORLD and
adapted in existing broader Minecraft gaming domain. We hope our findings on
LLMs and the new infrastructure for general-purpose scheduling and coordination
can help shed light on how such skills can be obtained by learning from large
language corpora.
### 🌟 论文解读 | MindAgent：大型语言模型在游戏交互中的涌现式规划与协调能力

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在各个领域的应用日益广泛，其在多智能体系统中的规划与协调能力也逐渐受到关注。然而，现有的游戏框架和基准测试还不足以评估LLMs在游戏交互中的涌现式规划与协调能力，尤其是在LLMs与人类NPCs协作的场景下。本文旨在解决这个问题，提出了一种名为MindAgent的新型基础设施，用于评估LLMs在游戏交互中的规划与协调能力。

### 🚀 核心方法
💡 创新点1：CUISINEWORLD游戏场景与基准测试
本文设计了一个名为CUISINEWORLD的游戏场景，模拟了一个虚拟厨房环境，其中多智能体系统需要协调多个代理，完成尽可能多的菜肴订单。CUISINEWORLD游戏场景具有多种任务结构和难度，是评估LLMs涌现式多智能体规划能力的理想测试平台。

💡 创新点2：MindAgent基础设施
MindAgent是一个用于LLMs交互式多智能体规划的基础设施，它展示了LLMs的涌现式多智能体规划能力，并引入了多种提示技术，以促进LLMs的规划能力，包括提供少量示例、规划理由和环境反馈。

### 📈 实验结果
本文在CUISINEWORLD游戏场景中进行了广泛的实验，结果表明：
1. 零样本多智能体规划：强大的预训练LLMs（如GPT-4）能够通过阅读简单的游戏指令和食谱，调度多个代理（2到4个）完成菜肴，甚至与人类玩家协作。
2. 基于高级提示的规划：通过利用涌现式上下文学习能力，可以显著提高LLMs的多智能体规划性能。例如，添加少量专家演示、解释某些行动的理由，以及在规划过程中提供实时反馈。
3. 通用潜力：LLMs表现出成为通用多智能体规划器的巨大潜力，因为它能够通过少量示例泛化到更多代理，并适应新的游戏领域，如Minecraft。

### 💬 可借鉴之处
本文提出的MindAgent基础设施和CUISINEWORLD游戏场景为评估LLMs在游戏交互中的涌现式规划与协调能力提供了新的思路和方法。此外，本文的研究结果也表明，LLMs在多智能体规划方面具有巨大的潜力，有望在未来推动游戏AI的发展。

## agents--an-open-source-framework-for-autonomous-language-agents
### Abstract
Recent advances on large language models (LLMs) enable researchers and
developers to build autonomous language agents that can automatically solve
various tasks and interact with environments, humans, and other agents using
natural language interfaces. We consider language agents as a promising
direction towards artificial general intelligence and release Agents, an
open-source library with the goal of opening up these advances to a wider
non-specialist audience. Agents is carefully engineered to support important
features including planning, memory, tool usage, multi-agent communication, and
fine-grained symbolic control. Agents is user-friendly as it enables
non-specialists to build, customize, test, tune, and deploy state-of-the-art
autonomous language agents without much coding. The library is also
research-friendly as its modularized design makes it easily extensible for
researchers. Agents is available at https://github.com/aiwaves-cn/agents.
### 🌟 论文解读 | 开源框架 Agents：构建自主语言代理的利器

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）的进步使得研究人员和开发者能够构建自主语言代理，这些代理能够自动解决各种任务，并通过自然语言界面与环境、人类和其他代理进行交互。然而，现有的语言代理框架往往缺乏易用性和可扩展性，难以满足非专业人士的需求。为了解决这个问题，本文提出了 Agents，一个开源的自主语言代理框架，旨在让更广泛的非专业人士能够轻松构建、定制、测试、调整和部署最先进的自主语言代理。

### 🚀 核心方法
💡 创新点1：支持关键功能
Agents 框架精心设计，支持规划、记忆、工具使用、多代理通信和细粒度符号控制等关键功能。这使得语言代理能够更好地适应各种任务和环境。

💡 创新点2：易用性和可扩展性
Agents 框架的用户友好性体现在其允许非专业人士轻松构建、定制、测试、调整和部署自主语言代理，而无需大量编码。同时，其模块化设计使得研究人员可以轻松扩展框架，以满足他们的研究需求。

💡 创新点3：Agent Hub 平台
Agents 框架引入了 Agent Hub 平台，允许用户分享他们微调的语言代理，并搜索/下载其他用户分享的有用语言代理。这大大降低了从头开始设计和调整语言代理的难度。

💡 创新点4：自动创建代理系统
为了减少用户手动指定 SOP 的繁琐工作，Agents 框架实现了一个自动 SOP 生成流程。该流程基于检索增强生成 (RAG)，可以自动创建其他代理和多代理系统。

### 📈 实验结果
论文展示了使用 Agents 框架构建的单代理系统和多代理系统的案例研究，包括闲聊机器人、基于知识库和搜索引擎的客户服务代理、购物助手代理和销售代理等。这些案例研究展示了 Agents 框架的易用性和可扩展性，以及构建各种用例的语言代理的可能性。

### 💬 可借鉴之处
Agents 框架为构建自主语言代理提供了一个强大的工具，其易用性和可扩展性使其成为研究人员和开发者的理想选择。此外，Agent Hub 平台和自动 SOP 生成流程进一步提高了框架的实用性和效率。

## exploring-large-language-models-for-communication-games--an-empirical-study-on-werewolf
### Abstract
Communication games, which we refer to as incomplete information games that
heavily depend on natural language communication, hold significant research
value in fields such as economics, social science, and artificial intelligence.
In this work, we explore the problem of how to engage large language models
(LLMs) in communication games, and in response, propose a tuning-free
framework. Our approach keeps LLMs frozen, and relies on the retrieval and
reflection on past communications and experiences for improvement. An empirical
study on the representative and widely-studied communication game,
``Werewolf'', demonstrates that our framework can effectively play Werewolf
game without tuning the parameters of the LLMs. More importantly, strategic
behaviors begin to emerge in our experiments, suggesting that it will be a
fruitful journey to engage LLMs in communication games and associated domains.
### 🌟 论文解读 | 探索大型语言模型在沟通游戏中的应用：以狼人杀为例的实证研究

### 📌 背景痛点/本文动机
沟通游戏，如狼人杀，是一种重要的研究工具，可以用来探索经济学、社会科学和人工智能等领域中的各种问题。然而，现有的AI代理在玩这类游戏时，要么对语言的使用有严格的限制，要么需要大量的人工标注数据，这使得AI代理在自然地玩这类游戏方面仍然面临挑战。

### 🚀 核心方法
本文提出了一种无需微调的大型语言模型（LLM）框架，用于玩沟通游戏，并以狼人杀为例进行了实证研究。该框架的核心方法包括：

💡 创新点1：历史信息收集
为了解决LLM的上下文长度限制问题，本文提出了一种从三个角度（新鲜度、信息量和完整性）收集历史信息的方法。具体来说，该方法包括：
- 收集最近的K条消息；
- 使用规则匹配和启发式指标选择最有信息量的N条消息；
- 通过回答问题的方式，从整个历史中提取更多信息。

💡 创新点2：经验学习
为了使LLM能够从经验中学习，本文提出了一种非参数学习机制。具体来说，该方法包括：
- 在每轮游戏结束后，收集所有玩家的响应、反思和得分，形成经验池；
- 在新的一轮游戏中，根据当前情况从经验池中检索最相关的经验，并从中提取建议，以指导LLM的推理。

### 📈 实验结果
实验结果表明，本文提出的框架能够有效地玩狼人杀游戏，并且能够从经验中学习，而无需微调LLM的参数。此外，实验中还观察到一些策略性行为，如信任、对抗、伪装和领导，这些行为并非预先编程，而是自发地从LLM中涌现出来的。

### 💬 可借鉴之处
本文提出的框架和方法为使用LLM玩沟通游戏提供了一种新的思路，并为进一步研究LLM在沟通游戏中的应用提供了有价值的参考。此外，本文提出的经验学习机制也可以应用于其他领域，例如对话系统和推荐系统。

## are-chatgpt-and-gpt-4-good-poker-players-----a-pre-flop-analysis
### Abstract
Since the introduction of ChatGPT and GPT-4, these models have been tested
across a large number of tasks. Their adeptness across domains is evident, but
their aptitude in playing games, and specifically their aptitude in the realm
of poker has remained unexplored. Poker is a game that requires decision making
under uncertainty and incomplete information. In this paper, we put ChatGPT and
GPT-4 through the poker test and evaluate their poker skills. Our findings
reveal that while both models display an advanced understanding of poker,
encompassing concepts like the valuation of starting hands, playing positions
and other intricacies of game theory optimal (GTO) poker, both ChatGPT and
GPT-4 are NOT game theory optimal poker players.
  Profitable strategies in poker are evaluated in expectations over large
samples. Through a series of experiments, we first discover the characteristics
of optimal prompts and model parameters for playing poker with these models.
Our observations then unveil the distinct playing personas of the two models.
We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This
exploration then sheds light on the divergent poker tactics of the two models:
ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker
vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which
means that it has a propensity to only engage with premium hands and folds a
majority of hands. When subjected to the same directive, GPT-4 plays like a
maniac, showcasing a loose and aggressive style of play. Both strategies,
although relatively advanced, are not game theory optimal.
### 🌟 论文解读 | ChatGPT 和 GPT-4 在德州扑克中的表现：一场前翻牌分析

### 📌 背景痛点/本文动机
随着 ChatGPT 和 GPT-4 的推出，这些模型在各种任务中表现出色，但在游戏，尤其是扑克游戏方面的能力尚未得到充分探索。扑克是一种需要在不完整信息和不确定性下做出决策的游戏，因此本文旨在评估 ChatGPT 和 GPT-4 在德州扑克中的表现，特别是它们在前翻牌阶段的决策能力。

### 🚀 核心方法
本文通过一系列实验，评估了 ChatGPT 和 GPT-4 在德州扑克前翻牌阶段的决策能力。实验中，研究人员使用了不同的提示和模型参数，以探索两种模型在扑克游戏中的最佳表现。他们还分析了两种模型的独特游戏风格，并比较了它们与游戏理论最优（GTO）策略的差异。

### 📈 实验结果
实验结果表明，ChatGPT 和 GPT-4 都对扑克游戏有深入的理解，包括起手牌的估值、游戏位置和其他游戏理论最优策略的细节。然而，两种模型都不是游戏理论最优的扑克玩家。ChatGPT 倾向于保守的游戏风格，只参与优质牌局，而 GPT-4 则表现出更加激进的游戏风格，参与更多的牌局。

### 💬 可借鉴之处
本文的研究结果表明，尽管 ChatGPT 和 GPT-4 在扑克游戏方面表现出色，但它们仍然存在局限性。这些模型在理解游戏理论最优策略方面存在偏差，这可能是由于它们在训练过程中没有针对扑克游戏进行专门训练。此外，本文的研究结果也表明，不同的提示和模型参数对模型在扑克游戏中的表现有显著影响。因此，未来的研究可以探索如何通过优化提示和模型参数来提高模型在扑克游戏中的表现。

## towards-ontology-construction-with-language-models
### Abstract
We present a method for automatically constructing a concept hierarchy for a
given domain by querying a large language model. We apply this method to
various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can
be of considerable help for constructing concept hierarchies.
### 🌟 论文解读 | 利用大型语言模型构建本体

### 📌 背景痛点/本文动机
本体是领域内概念及其关系的正式表示，是高度结构化的知识。然而，手动构建和编辑本体是一项耗时且成本高昂的工程任务。现有的方法通常需要领域专家的参与，但领域专家的知识和本体工程专业知识往往不在同一人手中。此外，现有的方法通常假设本体的模式（即要使用的概念和属性名称的集合）是预先选择的，然后作为方法的输入提供。然而，当使用大型语言模型（LLM）时，这似乎不是一个好的选择，因为至少有两个原因。首先，为整个领域设计模式本身就是一个非平凡的任务，需要领域专家并涉及许多设计决策。实际上，设计模式和概念层次结构是紧密相连的。其次，LLM的主要优势在于根据用户提供的上下文生成关键词和短语，因此它们是提出给定领域概念和属性名称的完美工具。因此，本文提出了一种基于LLM的本体构建方法，旨在解决上述问题。

### 🚀 核心方法
本文提出了一种基于LLM的本体构建方法，该方法通过查询大型语言模型来自动构建给定领域的概念层次结构。该方法的核心思想是，LLM隐含地包含大量的知识，并且可以像专家一样回答问题，从而帮助构建概念层次结构。具体来说，该方法包括以下几个步骤：

1. **种子概念选择**：选择一个种子概念，例如“动物”，作为构建概念层次结构的起点。
2. **概念层次结构探索**：通过重复询问LLM提供已经存在于层次结构中的概念的相关子概念，并使用遍历算法将新概念放置在层次结构中。
3. **概念描述**：询问LLM提供每个概念的文本描述，以便于人类用户理解和解释LLM提出的概念。
4. **概念验证**：通过向LLM提出额外的查询来验证LLM的输出，以过滤掉错误的答案。
5. **概念插入**：使用KRIS算法将新概念插入到概念层次结构中，并处理子概念/超概念关系和同义词检测。

### 📈 实验结果
本文使用OpenAI的GPT 3.5对各种领域（如动物、饮料、音乐和植物）进行了实验。结果表明，LLM可以有效地帮助构建概念层次结构，尽管仍然存在一些幻觉和错误。通过验证和仔细的提示工程，可以显著减少这些错误。此外，本文还讨论了如何将人类领域专家的交互纳入本体构建过程，以及如何评估构建的本体。

### 💬 可借鉴之处
本文提出的方法为利用LLM构建本体提供了一种新的思路，具有以下可借鉴之处：

* **利用LLM的知识和生成能力**：LLM可以像专家一样回答问题，并提供相关子概念和概念描述，从而帮助构建概念层次结构。
* **概念验证和提示工程**：通过验证和提示工程，可以显著减少LLM的幻觉和错误，提高构建本体的质量。
* **人类领域专家的交互**：将人类领域专家的交互纳入本体构建过程，可以帮助解决设计决策和引入“神秘”概念的问题。
* **本体评估**：使用手动评估或现有分类法进行本体评估，可以帮助评估构建本体的质量和完整性。

总而言之，本文提出的方法为利用LLM构建本体提供了一种新的思路，并展示了LLM在构建本体方面的潜力。

## agentverse--facilitating-multi-agent-collaboration-and-exploring-emergent-behaviors
### Abstract
Autonomous agents empowered by Large Language Models (LLMs) have undergone
significant improvements, enabling them to generalize across a broad spectrum
of tasks. However, in real-world scenarios, cooperation among individuals is
often required to enhance the efficiency and effectiveness of task
accomplishment. Hence, inspired by human group dynamics, we propose a
multi-agent framework \framework that can collaboratively and dynamically
adjust its composition as a greater-than-the-sum-of-its-parts system. Our
experiments demonstrate that \framework framework can effectively deploy
multi-agent groups that outperform a single agent. Furthermore, we delve into
the emergence of social behaviors among individual agents within a group during
collaborative task accomplishment. In view of these behaviors, we discuss some
possible strategies to leverage positive ones and mitigate negative ones for
improving the collaborative potential of multi-agent groups. Our codes for
\framework will soon be released at
\url{https://github.com/OpenBMB/AgentVerse}.
### 🌟 论文解读 | AgentVerse：多智能体协作与涌现行为的探索

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，自主智能体在理解和执行任务方面取得了显著进步。然而，现实世界中的复杂任务往往需要个体之间的协作才能更有效地完成。本文旨在解决这一问题，通过模拟人类团队协作过程，提出了一种名为 AgentVerse 的多智能体框架，以促进智能体之间的协作并探索涌现行为。

### 🚀 核心方法
💡 创新点1：AgentVerse 框架
AgentVerse 框架模拟了人类团队解决问题的过程，并将其分为四个关键阶段：
1. **专家招募**：根据当前问题解决进度动态调整智能体团队的组成。
2. **协作决策**：让选定的智能体进行联合讨论，制定问题解决策略。
3. **行动执行**：智能体与环境交互，执行制定的行动。
4. **评估**：评估当前状态与预期目标之间的差异，并根据反馈进行改进。

💡 创新点2：动态调整与涌现行为
AgentVerse 允许根据当前状态动态调整智能体团队的组成，以适应不同的任务需求。此外，该框架还揭示了智能体在协作过程中出现的涌现行为，包括：
* **志愿行为**：智能体主动提供帮助，提高团队效率。
* **从众行为**：智能体调整自身行为以符合共同目标。
* **破坏性行为**：智能体采取可能导致不良后果的行动。

### 📈 实验结果
AgentVerse 在文本理解、推理、编码、工具利用和具身 AI 等任务中进行了广泛的实验，结果表明：
* AgentVerse 能够有效地提升智能体的理解、推理、编码和工具利用能力。
* 与单个智能体相比，AgentVerse 组成的团队在协作决策和行动执行方面表现出更高的效率。
* 智能体在协作过程中出现的涌现行为对团队效率有积极影响，但也存在潜在风险。

### 💬 可借鉴之处
AgentVerse 框架为多智能体协作提供了新的思路，其动态调整和涌现行为分析对构建更高效、更智能的自主系统具有重要意义。未来研究可以探索更先进的感知智能体，并设计策略来利用积极行为并减轻潜在风险。

## gameeval--evaluating-llms-on-conversational-games
### Abstract
The rapid advancements in large language models (LLMs) have presented
challenges in evaluating those models. Existing evaluation methods are either
reference-based or preference based, which inevitably need human intervention
or introduce test bias caused by evaluator models. In this paper, we propose
GameEval, a novel approach to evaluating LLMs through goal-driven
conversational games, overcoming the limitations of previous methods. GameEval
treats LLMs as game players and assigns them distinct roles with specific goals
achieved by launching conversations of various forms, including discussion,
question answering, and voting. We design three unique games with cooperative
or adversarial objectives, accompanied by corresponding evaluation metrics, to
show how this new paradigm comprehensively evaluates model performance.Through
extensive experiments, we show that GameEval can effectively differentiate the
capabilities of various LLMs, providing a comprehensive assessment of their
integrated abilities to solve complex problems. Our public anonymous code is
available at https://github.com/GameEval/GameEval.
### 🌟 论文解读 | GameEval：通过对话游戏评估大型语言模型

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，如何有效地评估这些模型的能力成为一个挑战。现有的评估方法主要分为两类：基于参考和基于偏好的方法。基于参考的方法需要与预先确定的答案进行比较，而基于偏好的方法则依赖于人类或模型评估者的偏好。这两种方法都存在局限性，例如获取高质量标注的成本高、时间消耗大，以及引入评估者的偏好偏差等。

### 🚀 核心方法
本文提出了GameEval，一种通过目标驱动的对话游戏来评估LLMs的新方法。GameEval将LLMs视为游戏玩家，并为其分配具有特定目标的独特角色，通过启动各种形式的对话（包括讨论、问答和投票）来实现这些目标。本文设计了三种独特的游戏，包括合作和对抗目标，并配备了相应的评估指标，以展示这种新范式如何全面评估模型性能。

### 📈 实验结果
通过广泛的实验，本文展示了GameEval能够有效地区分不同LLMs的能力，并提供对其解决复杂问题综合能力的全面评估。实验结果表明，GameEval在区分ChatGPT和GPT-4等模型的能力方面表现出色，而现有方法则难以做到这一点。

### 💬 可借鉴之处
GameEval提供了一种新的评估LLMs的方法，可以更全面地评估模型的能力，并减少评估偏差。此外，GameEval还可以用于设计新的游戏，以评估LLMs在现实世界复杂场景中的能力。

## autogen--enabling-next-gen-llm-applications-via-multi-agent-conversation
### Abstract
AutoGen is an open-source framework that allows developers to build LLM
applications via multiple agents that can converse with each other to
accomplish tasks. AutoGen agents are customizable, conversable, and can operate
in various modes that employ combinations of LLMs, human inputs, and tools.
Using AutoGen, developers can also flexibly define agent interaction behaviors.
Both natural language and computer code can be used to program flexible
conversation patterns for different applications. AutoGen serves as a generic
infrastructure to build diverse applications of various complexities and LLM
capacities. Empirical studies demonstrate the effectiveness of the framework in
many example applications, with domains ranging from mathematics, coding,
question answering, operations research, online decision-making, entertainment,
etc.
### 🌟 论文解读 | AutoGen：通过多智能体对话赋能下一代LLM应用

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在各个领域的应用越来越广泛，如何有效地利用LLMs构建复杂的应用程序成为了一个挑战。传统的单智能体模型往往难以处理复杂的任务，而多智能体模型则可以协同工作，发挥各自的优势，从而提高任务处理的效率和效果。

### 🚀 核心方法
AutoGen是一个开源框架，它允许开发者通过多个可以相互对话的智能体来构建LLM应用程序。AutoGen的核心创新点包括：

💡 **可定制和可对话的智能体**：AutoGen的智能体可以基于LLMs、人类输入、工具或它们的组合来构建，并且可以相互对话，接收、反应和响应消息。这使得开发者可以轻松地创建具有不同角色和能力的智能体，例如编写代码、执行代码、获取人类反馈、验证输出等。

💡 **对话编程**：AutoGen采用了一种以对话为中心的编程范式，将复杂的LLM应用程序工作流程简化为多智能体对话。开发者可以通过定义一组具有特定能力和角色的可对话智能体，并编程它们之间的交互行为来构建应用程序。AutoGen支持使用自然语言和编程语言来编程灵活的对话模式，并提供了一系列设计模式来简化对话编程，例如统一的对话接口、自动回复机制、基于编程和自然语言的控制流管理等。

### 📈 实验结果
AutoGen在多个领域的应用程序中展示了其有效性，包括数学问题解决、代码生成、问答、决策制定、在线决策、娱乐等。实验结果表明，AutoGen可以帮助实现许多任务上的卓越性能，并减少开发工作量。

### 💬 可借鉴之处
AutoGen为构建复杂的多智能体LLM应用程序提供了一个通用的框架，具有以下可借鉴之处：

* **模块化设计**：AutoGen的智能体可以独立开发、测试和维护，从而简化了整体开发和代码管理。
* **灵活的对话编程**：AutoGen支持使用自然语言和编程语言来编程灵活的对话模式，使得开发者可以根据不同的应用需求进行定制。
* **可扩展性**：AutoGen的智能体可以轻松地扩展和定制，以满足不同的应用需求。

### 🌟 总结
AutoGen是一个强大的框架，可以帮助开发者构建复杂的多智能体LLM应用程序。它具有模块化设计、灵活的对话编程和可扩展性等优点，为LLM应用程序的开发提供了新的思路和方法。

## calypso--llms-as-dungeon-masters--assistants
### Abstract
The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to
perform multiple tasks simultaneously. The DM must digest information about the
game setting and monsters, synthesize scenes to present to other players, and
respond to the players' interactions with the scene. Doing all of these tasks
while maintaining consistency within the narrative and story world is no small
feat of human cognition, making the task tiring and unapproachable to new
players. Large language models (LLMs) like GPT-3 and ChatGPT have shown
remarkable abilities to generate coherent natural language text. In this paper,
we conduct a formative evaluation with DMs to establish the use cases of LLMs
in D&D and tabletop gaming generally. We introduce CALYPSO, a system of
LLM-powered interfaces that support DMs with information and inspiration
specific to their own scenario. CALYPSO distills game context into bite-sized
prose and helps brainstorm ideas without distracting the DM from the game. When
given access to CALYPSO, DMs reported that it generated high-fidelity text
suitable for direct presentation to players, and low-fidelity ideas that the DM
could develop further while maintaining their creative agency. We see CALYPSO
as exemplifying a paradigm of AI-augmented tools that provide synchronous
creative assistance within established game worlds, and tabletop gaming more
broadly.
### 🌟 论文解读 | CALYPSO：大型语言模型助力地下城主

### 📌 背景痛点/本文动机
地下城与龙（D&D）是一款经典的桌面角色扮演游戏，其中地下城主（DM）扮演着至关重要的角色。DM需要同时处理多项任务，包括消化游戏背景和怪物信息、构建场景、回应玩家互动等。这些任务对人类认知能力要求极高，对于新玩家来说尤其具有挑战性。大型语言模型（LLM）如GPT-3和ChatGPT在生成连贯的自然语言文本方面表现出色。本文旨在探索LLM在D&D和桌面游戏中的应用，并提出了CALYPSO系统，该系统利用LLM为DM提供信息和支持，帮助他们更好地进行游戏。

### 🚀 核心方法
💡 创新点1：CALYPSO系统
CALYPSO是一个由LLM驱动的界面系统，旨在支持DM在游戏中获取信息和灵感。它包括三个主要功能：
1. **遭遇理解**：使用GPT-3对游戏背景和怪物信息进行摘要，帮助DM快速理解遭遇。
2. **聚焦头脑风暴**：使用ChatGPT与DM进行对话，帮助他们进一步探索遭遇细节或生成新的想法。
3. **开放域聊天基线**：使用ChatGPT提供一个开放域的聊天界面，供玩家和DM进行非游戏相关的交流。

💡 创新点2：LLM的创造性辅助
CALYPSO系统展示了LLM作为创造性辅助工具的潜力。它不仅能够生成高保真度的文本，适合直接呈现给玩家，还能够提供低保真度的想法，供DM进一步发展和完善。这种辅助方式保留了DM的创造性自主权，使他们能够更好地专注于游戏中的认知任务。

### 📈 实验结果
研究结果表明，DM在使用CALYPSO系统后，普遍认为它能够生成适合直接呈现给玩家的文本，并提供有价值的灵感。DM们利用CALYPSO系统来理解复杂的怪物信息、头脑风暴非玩家角色或怪物之间的互动，并获取建议，将这些建议融入到故事中呈现给玩家，而不会影响游戏的节奏。

### 💬 可借鉴之处
本文的研究结果对于开发AI辅助工具在桌面游戏和其他创意领域中的应用具有重要的启示意义。CALYPSO系统的设计理念和方法可以为其他类似项目提供参考，例如：
1. **理解用户需求**：通过访谈和用户研究，深入了解用户的需求和痛点，从而设计出更符合用户需求的AI辅助工具。
2. **利用LLM的创造性潜力**：LLM在生成连贯文本和提供创造性灵感方面具有巨大潜力，可以将其应用于各种创意场景。
3. **保持用户的创造性自主权**：AI辅助工具应该作为用户的助手，而不是替代者，帮助用户更好地发挥自己的创造力。

总而言之，CALYPSO系统展示了LLM在桌面游戏中的应用潜力，并为开发AI辅助工具提供了有价值的经验和启示。

## agentsims--an-open-source-sandbox-for-large-language-model-evaluation
### Abstract
With ChatGPT-like large language models (LLM) prevailing in the community,
how to evaluate the ability of LLMs is an open question. Existing evaluation
methods suffer from following shortcomings: (1) constrained evaluation
abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that
task-based evaluation, where LLM agents complete tasks in a simulated
environment, is a one-for-all solution to solve above problems. We present
AgentSims, an easy-to-use infrastructure for researchers from all disciplines
to test the specific capacities they are interested in. Researchers can build
their evaluation tasks by adding agents and buildings on an interactive GUI or
deploy and test new support mechanisms, i.e. memory, planning and tool-use
systems, by a few lines of codes. Our demo is available at
https://agentsims.com .
### 🌟 论文解读 | AgentSims：大型语言模型评估的开放源代码沙盒

### 📌 背景痛点/本文动机
随着ChatGPT等大型语言模型（LLM）在社区中的普及，如何评估LLM的能力成为一个开放性问题。现有的评估方法存在以下不足：
1. 评估能力受限：大多数任务采用单轮问答格式，无法全面评估LLM的各种能力。
2. 基准易受攻击：由于LLM具有大量的预训练知识，测试集容易无意中混入训练集。
3. 指标不客观：现有的开放式问答指标涉及自动指标和主观指标，无法客观评估LLM的能力。

### 🚀 核心方法
本文提出了基于任务的评估方法，即LLM代理在模拟环境中完成任务来证明其能力。为了解决现有评估方法的不足，本文提出了AgentSims，一个易于使用的评估LLM能力的平台。AgentSims具有以下特点：
1. 可扩展性和可组合性：允许用户组合不同的计划、记忆和使用工具系统，研究各种系统设计的影响和有效性。
2. 交互式用户界面：为地图设计和代理创建提供交互式UI，降低非专业人士的入门门槛。
3. 标准化实现：确保实验结果的再现性。

### 📈 实验结果
本文展示了AgentSims在评估LLM能力方面的应用，包括：
1. 评估LLM的社会能力，如心智理论（ToM）。
2. 评估LLM的长期规划和组织能力，如担任市长或公司总裁。
3. 作为数据生成平台，用于数据标注和增强。
4. 为社会科学研究提供可控的初步实验环境。

### 💬 可借鉴之处
AgentSims为LLM评估提供了一个开放源代码的沙盒平台，具有以下可借鉴之处：
1. 基于任务的评估方法，可以更全面地评估LLM的能力。
2. 交互式用户界面，降低非专业人士的入门门槛。
3. 标准化实现，确保实验结果的再现性。
4. 可扩展性和可组合性，方便用户研究和开发新的支持系统。

## metagpt--meta-programming-for-a-multi-agent-collaborative-framework
### Abstract
Remarkable progress has been made on automated problem solving through
societies of agents based on large language models (LLMs). Existing LLM-based
multi-agent systems can already solve simple dialogue tasks. Solutions to more
complex tasks, however, are complicated through logic inconsistencies due to
cascading hallucinations caused by naively chaining LLMs. Here we introduce
MetaGPT, an innovative meta-programming framework incorporating efficient human
workflows into LLM-based multi-agent collaborations. MetaGPT encodes
Standardized Operating Procedures (SOPs) into prompt sequences for more
streamlined workflows, thus allowing agents with human-like domain expertise to
verify intermediate results and reduce errors. MetaGPT utilizes an assembly
line paradigm to assign diverse roles to various agents, efficiently breaking
down complex tasks into subtasks involving many agents working together. On
collaborative software engineering benchmarks, MetaGPT generates more coherent
solutions than previous chat-based multi-agent systems. Our project can be
found at https://github.com/geekan/MetaGPT
### 🌟 论文解读 | MetaGPT：基于大型语言模型的元编程多智能体协作框架

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）的快速发展，基于LLMs的自主智能体在自动化问题解决方面取得了显著进展。然而，现有的基于LLMs的多智能体系统在解决复杂任务时，由于简单地将LLMs串联起来导致的级联幻觉，常常出现逻辑不一致的问题。为了解决这个问题，本文提出了MetaGPT，一个创新的元编程框架，将高效的人类工作流程融入到基于LLMs的多智能体协作中。

### 🚀 核心方法
💡 创新点1：将标准化操作流程（SOPs）编码到提示序列中，以实现更流畅的工作流程。这使得具有人类水平的领域专业知识的智能体能够验证中间结果并减少错误。
💡 创新点2：采用流水线范式，为各种智能体分配不同的角色，有效地将复杂任务分解为涉及多个智能体协作的子任务。
💡 创新点3：引入可执行反馈机制，在运行时调试和执行代码，显著提高代码生成质量。

### 📈 实验结果
在协作软件工程基准测试中，MetaGPT生成的解决方案比之前的基于聊天的多智能体系统更连贯。在HumanEval和MBPP基准测试中，MetaGPT取得了最先进的性能，分别达到了85.9%和87.7%的Pass@1。在自生成的软件开发基准测试中，MetaGPT在几乎所有指标上都优于ChatDev，并且实现了100%的任务完成率，证明了其鲁棒性和效率。

### 💬 可借鉴之处
MetaGPT框架为基于LLMs的多智能体系统开发提供了一个灵活且功能强大的平台。其将SOPs融入设计中的创新方法，为解决复杂问题提供了新的思路。此外，可执行反馈机制也为提高代码生成质量提供了有效途径。MetaGPT的成功经验可以启发未来研究，探索更多基于人类实践的人工多智能体系统技术。

## emollm--multimodal-emotional-understanding-meets-large-language-models
### Abstract
Multi-modal large language models (MLLMs) have achieved remarkable
performance on objective multimodal perception tasks, but their ability to
interpret subjective, emotionally nuanced multimodal content remains largely
unexplored. Thus, it impedes their ability to effectively understand and react
to the intricate emotions expressed by humans through multimodal media. To
bridge this gap, we introduce EmoBench, the first comprehensive benchmark
designed specifically to evaluate the emotional capabilities of MLLMs across
five popular emotional tasks, using a diverse dataset of 287k images and videos
paired with corresponding textual instructions. Meanwhile, we propose EmoLLM, a
novel model for multimodal emotional understanding, incorporating with two core
techniques. 1) Multi-perspective Visual Projection, it captures diverse
emotional cues from visual data from multiple perspectives. 2) EmoPrompt, it
guides MLLMs to reason about emotions in the correct direction. Experimental
results demonstrate that EmoLLM significantly elevates multimodal emotional
understanding performance, with an average improvement of 12.1% across multiple
foundation models on EmoBench. Our work contributes to the advancement of MLLMs
by facilitating a deeper and more nuanced comprehension of intricate human
emotions, paving the way for the development of artificial emotional
intelligence capabilities with wide-ranging applications in areas such as
human-computer interaction, mental health support, and empathetic AI systems.
Code, data, and model will be released.
### 🌟 论文解读 | EmoLLM：多模态情感理解与大型语言模型的结合

### 📌 背景痛点/本文动机
随着多模态大型语言模型（MLLMs）在目标多模态感知任务上取得了显著成果，但它们在解释主观、情感丰富的多模态内容方面的能力仍然没有得到充分探索。这阻碍了它们有效地理解和反应人类通过多模态媒体表达的情感。为了弥合这一差距，本文提出了EmoBench，这是第一个专门设计用于评估MLLMs在五个流行情感任务中的情感能力的全面基准，使用了一个包含287k图像和视频以及相应文本指令的多样化数据集。同时，本文提出了EmoLLM，这是一种用于多模态情感理解的新型模型，结合了两种核心技术。

### 🚀 核心方法
💡 创新点1：多视角视觉投影
它从多个视角捕获视觉数据中的多样化情感线索。

💡 创新点2：EmoPrompt
它引导MLLMs在正确的方向上推理情感。

### 📈 实验结果
实验结果表明，EmoLLM显著提高了多模态情感理解性能，在EmoBench上多个基础模型平均提高了12.1%。

### 💬 可借鉴之处
本文提出的EmoBench基准和EmoLLM模型为MLLMs在情感理解方面的研究提供了新的思路和方法，有助于推动MLLMs在情感智能领域的进一步发展。

## selective-perception--optimizing-state-descriptions-with-reinforcement-learning-for-language-model-actors
### Abstract
Large language models (LLMs) are being applied as actors for sequential
decision making tasks in domains such as robotics and games, utilizing their
general world knowledge and planning abilities. However, previous work does
little to explore what environment state information is provided to LLM actors
via language. Exhaustively describing high-dimensional states can impair
performance and raise inference costs for LLM actors. Previous LLM actors avoid
the issue by relying on hand-engineered, task-specific protocols to determine
which features to communicate about a state and which to leave out. In this
work, we propose Brief Language INputs for DEcision-making Responses (BLINDER),
a method for automatically selecting concise state descriptions by learning a
value function for task-conditioned state descriptions. We evaluate BLINDER on
the challenging video game NetHack and a robotic manipulation task. Our method
improves task success rate, reduces input size and compute costs, and
generalizes between LLM actors.
### 🌟 论文解读 | BLINDER：通过强化学习优化语言模型决策者的状态描述

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在游戏和机器人规划等领域的决策任务中表现出色，但现有的工作很少探讨如何通过语言向LLM决策者提供环境状态信息。详尽描述高维状态可能会损害性能并增加LLM决策者的推理成本。为了解决这个问题，本文提出了BLINDER，一种通过学习任务条件状态描述的价值函数来自动选择简洁状态描述的方法。

### 🚀 核心方法
💡 创新点1：BLINDER通过强化学习自动选择简洁的状态描述，从而减少LLM决策者的输入长度和计算成本。
💡 创新点2：BLINDER在NetHack游戏和机器人操作任务上进行了评估，结果表明该方法能够提高任务成功率，并能够在不同的LLM决策者之间进行泛化。

### 📈 实验结果
BLINDER在NetHack游戏和机器人操作任务上的实验结果表明，与详尽的状态描述相比，BLINDER能够显著提高任务成功率，并减少输入长度和计算成本。此外，BLINDER还能够泛化到不同的LLM决策者，从而提高了其适用性。

### 💬 可借鉴之处
BLINDER提出了一种通过强化学习自动选择简洁状态描述的方法，为LLM决策者在游戏和机器人规划等领域的应用提供了新的思路。该方法不仅能够提高任务成功率，还能够减少输入长度和计算成本，从而提高了LLM决策者的效率和性能。此外，BLINDER还能够泛化到不同的LLM决策者，从而提高了其适用性。

## sayplan--grounding-large-language-models-using-3d-scene-graphs-for-scalable-robot-task-planning
### Abstract
Large language models (LLMs) have demonstrated impressive results in
developing generalist planning agents for diverse tasks. However, grounding
these plans in expansive, multi-floor, and multi-room environments presents a
significant challenge for robotics. We introduce SayPlan, a scalable approach
to LLM-based, large-scale task planning for robotics using 3D scene graph
(3DSG) representations. To ensure the scalability of our approach, we: (1)
exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic
search' for task-relevant subgraphs from a smaller, collapsed representation of
the full graph; (2) reduce the planning horizon for the LLM by integrating a
classical path planner and (3) introduce an 'iterative replanning' pipeline
that refines the initial plan using feedback from a scene graph simulator,
correcting infeasible actions and avoiding planning failures. We evaluate our
approach on two large-scale environments spanning up to 3 floors and 36 rooms
with 140 assets and objects and show that our approach is capable of grounding
large-scale, long-horizon task plans from abstract, and natural language
instruction for a mobile manipulator robot to execute. We provide real robot
video demonstrations on our project page https://sayplan.github.io.
### 🌟 论文解读 | SayPlan：利用3D场景图实现大规模机器人任务规划

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在任务规划领域的快速发展，它们在处理各种任务时展现出惊人的能力。然而，将这些计划在大型、多楼层和多房间的环境中落地，对于机器人来说仍然是一个巨大的挑战。现有的方法往往局限于小规模环境，且难以扩展到更复杂的场景中。

### 🚀 核心方法
SayPlan 提出了一个可扩展的方法，利用 3D 场景图（3DSG）表示来支持基于 LLM 的大规模任务规划。为了确保方法的可扩展性，SayPlan 采用了以下三个关键创新：

💡 创新点1：语义搜索
SayPlan 利用 3DSG 的层次结构，允许 LLM 通过语义搜索来识别任务相关的子图。通过将完整的 3DSG 压缩成一个更小的表示，LLM 可以专注于一个相对较小的、信息丰富的子图，从而避免超出其 token 限制。

💡 创新点2：迭代重规划
为了减少 LLM 的规划范围，SayPlan 集成了一个经典的路径规划器，负责连接 LLM 生成的节点。此外，SayPlan 引入了一个迭代重规划流程，使用场景图模拟器的反馈来验证和细化初始计划，从而纠正不可执行的动作并避免规划失败。

### 📈 实验结果
SayPlan 在两个大型环境中进行了评估，包括一个拥有 3 层楼和 36 个房间的环境，以及一个拥有 140 个资产和对象的环境。结果表明，SayPlan 能够从抽象的自然语言指令中生成可执行的、大规模的、长时任务计划，并能够在真实机器人上进行执行。

### 💬 可借鉴之处
SayPlan 为大规模机器人任务规划提供了一种可扩展且高效的方法。其语义搜索和迭代重规划机制可以有效地减少 LLM 的 token 消耗，并确保生成的计划符合环境约束。此外，SayPlan 的框架可以轻松地集成到现有的机器人系统中，为服务机器人领域的发展提供了新的思路。

## building-cooperative-embodied-agents-modularly-with-large-language-models
### Abstract
In this work, we address challenging multi-agent cooperation problems with
decentralized control, raw sensory observations, costly communication, and
multi-objective tasks instantiated in various embodied environments. While
previous research either presupposes a cost-free communication channel or
relies on a centralized controller with shared observations, we harness the
commonsense knowledge, reasoning ability, language comprehension, and text
generation prowess of LLMs and seamlessly incorporate them into a
cognitive-inspired modular framework that integrates with perception, memory,
and execution. Thus building a Cooperative Embodied Language Agent CoELA, who
can plan, communicate, and cooperate with others to accomplish long-horizon
tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA
driven by GPT-4 can surpass strong planning-based methods and exhibit emergent
effective communication. Though current Open LMs like LLAMA-2 still
underperform, we fine-tune a CoELA with data collected with our agents and show
how they can achieve promising performance. We also conducted a user study for
human-agent interaction and discovered that CoELA communicating in natural
language can earn more trust and cooperate more effectively with humans. Our
research underscores the potential of LLMs for future research in multi-agent
cooperation. Videos can be found on the project website
https://vis-www.cs.umass.edu/Co-LLM-Agents/.
### 🌟 论文解读 | 基于大型语言模型构建协作型具身智能体

### 📌 背景痛点/本文动机
在现实世界中，人类擅长与他人合作和沟通以解决复杂任务。然而，构建能够与人类或其他智能体协作的具身智能体仍然是一个具有挑战性的任务，因为它们需要具备感知、部分观察、长期规划、自然语言沟通等复杂能力。

### 🚀 核心方法
本文提出了一个名为 CoELA 的协作型具身语言智能体，它利用大型语言模型 (LLM) 的常识知识、推理能力、语言理解和文本生成能力，并将其集成到一个认知启发的模块化框架中，该框架与感知、记忆和执行模块相结合。

CoELA 框架包括五个关键模块：
1. **感知模块**：用于感知环境中的原始感官观察并提取有用信息。
2. **记忆模块**：模拟人类的长期记忆，存储智能体对世界和其他智能体的理解和经验。
3. **通信模块**：利用 LLM 的强大对话生成和理解能力，决定发送什么信息。
4. **规划模块**：利用 LLM 的强大推理能力，根据所有可用信息做出决策，并制定高级计划。
5. **执行模块**：根据规划模块生成的计划，生成可执行的低级动作。

### 📈 实验结果
在 C-WAH 和 TDW-MAT 两个具身环境中进行的实验表明，CoELA 能够感知复杂观察、推理世界和其他智能体的状态、有效沟通，并相应地制定长期计划。CoELA 驱动的 GPT-4 能够超越基于规划的强方法，并表现出有效的沟通。尽管当前的开放 LLM（如 LLAMA-2）仍然表现不佳，但作者通过在具身环境中收集的数据对 CoELA 进行微调，并展示了其有希望的性能。

### 💬 可借鉴之处
1. **模块化框架**：CoELA 的模块化框架可以有效地将 LLM 的能力与感知、记忆和执行模块相结合，从而构建协作型具身智能体。
2. **自然语言沟通**：CoELA 使用自然语言进行沟通，使其能够更好地与人类或其他智能体协作。
3. **微调 LLM**：作者通过在具身环境中收集的数据对 CoELA 进行微调，并展示了其有希望的性能，这为构建更好的协作型具身智能体提供了新的思路。

### 🌈 未来展望
CoELA 的研究结果表明，LLM 在多智能体协作领域具有巨大的潜力。未来研究可以探索以下方向：
1. **多模态 LLM**：开发能够有效处理视觉模态并流畅生成自然语言的 LLM，以更好地利用 3D 空间信息。
2. **低级控制**：开发能够直接进行低级控制的智能体，以更好地理解低级动作的执行。
3. **复杂推理**：开发具有更强指令遵循和推理能力的 LLM，以提高智能体的鲁棒性。

总而言之，CoELA 为构建协作型具身智能体提供了一个有希望的方向，并为未来研究开辟了新的可能性。

## embodied-task-planning-with-large-language-models
### Abstract
Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.
### 🌟 论文解读 | 基于大型语言模型的具身任务规划

### 📌 背景痛点/本文动机
随着人工智能的发展，机器人被期望能够在各种环境中完成复杂的任务，例如家庭服务、医疗护理和农业采摘等。然而，由于训练样本有限和任务的多样性，直接训练一个能够在不同部署场景中工作的具身代理是不可行的。大型语言模型（LLM）可以从大量网络数据中获取丰富的常识知识，这些知识可以潜在地被具身代理利用来生成符合人类要求的自然语言命令的动作计划。然而，LLM无法感知周围场景，并且可能会生成不可执行的行动，因为它们需要与非存在的对象进行交互。因此，将LLM生成的任务计划与物理世界相结合是构建能够完成复杂任务的具身代理的必要条件。

### 🚀 核心方法
本文提出了一种名为TaPA的具身任务规划代理，用于在物理场景中进行具身任务规划。TaPA通过将LLM与视觉感知模型相结合，根据场景中存在的对象生成可执行的计划。具体来说，本文首先构建了一个包含室内场景、指令和动作计划三元组的多元数据集，其中为GPT-3.5提供了设计的提示和场景中现有对象的列表，以生成大量指令和相应的计划动作。生成的数据用于对预训练的LLM进行基于物理场景约束的接地计划微调。在推理过程中，通过扩展开放词汇对象检测器到从不同可达位置收集的多视图RGB图像，发现场景中的对象。实验结果表明，与LLaVA和GPT-3.5相比，TaPA框架生成的计划可以取得更高的成功率，这表明具身任务规划在一般和复杂环境中的实用性。

### 📈 实验结果
实验结果表明，与LLaVA和GPT-3.5相比，TaPA框架生成的计划可以取得更高的成功率，这表明具身任务规划在一般和复杂环境中的实用性。

### 💬 可借鉴之处
本文提出的TaPA框架为具身任务规划提供了一种新的思路，通过将LLM与视觉感知模型相结合，可以生成更符合物理场景约束的可执行计划。此外，本文构建的多元数据集也为其他研究提供了有价值的资源。

## sprint--scalable-policy-pre-training-via-language-instruction-relabeling
### Abstract
Pre-training robot policies with a rich set of skills can substantially
accelerate the learning of downstream tasks. Prior works have defined
pre-training tasks via natural language instructions, but doing so requires
tedious human annotation of hundreds of thousands of instructions. Thus, we
propose SPRINT, a scalable offline policy pre-training approach which
substantially reduces the human effort needed for pre-training a diverse set of
skills. Our method uses two core ideas to automatically expand a base set of
pre-training tasks: instruction relabeling via large language models and
cross-trajectory skill chaining through offline reinforcement learning. As a
result, SPRINT pre-training equips robots with a much richer repertoire of
skills. Experimental results in a household simulator and on a real robot
kitchen manipulation task show that SPRINT leads to substantially faster
learning of new long-horizon tasks than previous pre-training approaches.
Website at https://clvrai.com/sprint.
### 🌟 论文解读 | SPRINT：通过语言指令重标记实现可扩展的策略预训练

### 📌 背景痛点/本文动机
在机器人学习领域，预训练机器人策略以掌握丰富的技能集可以显著加速下游任务的学习。然而，以往的方法需要大量的人工标注自然语言指令，这不仅耗时且成本高昂。本文提出了SPRINT，一种可扩展的离线策略预训练方法，旨在大幅减少预训练多样化技能集所需的人工工作量。

### 🚀 核心方法
💡 创新点1：语言模型指令重标记
SPRINT利用大型语言模型（LLM）自动将连续的语言指令组合成更复杂的任务，例如将“将杯子放入咖啡机”和“按下冲泡按钮”合并为“制作咖啡”。这种方法可以显著扩展预训练任务集，而无需额外的人工标注。

💡 创新点2：跨轨迹技能链
SPRINT还引入了一种基于语言的离线强化学习（RL）目标，通过将来自数据集的不同轨迹段“缝合”在一起形成新任务，从而实现跨轨迹技能链。这种方法允许策略学习更长的技能，并进一步丰富预训练任务集。

### 📈 实验结果
在家庭模拟器和真实机器人厨房操作任务上的实验结果表明，与之前的预训练方法相比，SPRINT能够使机器人更快地学习新的长时任务。SPRINT预训练的机器人可以利用其更丰富的技能库，更有效地学习新的下游任务。

### 💬 可借鉴之处
SPRINT提供了一种高效且可扩展的机器人策略预训练方法，可以显著减少人工标注工作量，并提高机器人学习新任务的能力。该方法的核心思想，即利用语言模型进行指令重标记和跨轨迹技能链，为机器人学习领域提供了新的思路和方向。

## chessgpt--bridging-policy-learning-and-language-modeling
### Abstract
When solving decision-making tasks, humans typically depend on information
from two key sources: (1) Historical policy data, which provides interaction
replay from the environment, and (2) Analytical insights in natural language
form, exposing the invaluable thought process or strategic considerations.
Despite this, the majority of preceding research focuses on only one source:
they either use historical replay exclusively to directly learn policy or value
functions, or engaged in language model training utilizing mere language
corpus. In this paper, we argue that a powerful autonomous agent should cover
both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning
and language modeling by integrating data from these two sources in Chess
games. Specifically, we build a large-scale game and language dataset related
to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and
ChessGPT, integrating policy learning and language modeling. Finally, we
propose a full evaluation framework for evaluating language model's chess
ability. Experimental results validate our model and dataset's effectiveness.
We open source our code, model, and dataset at
https://github.com/waterhorse1/ChessGPT.
### 🌟 论文解读 | ChessGPT：策略学习与语言模型融合的桥梁

### 📌 背景痛点/本文动机
在解决决策任务时，人类通常依赖于两种关键信息来源：历史策略数据和自然语言形式的策略分析。然而，现有的研究大多只关注其中一种来源，要么是直接从历史回放中学习策略或价值函数，要么是利用语言语料库进行语言模型训练。本文认为，一个强大的自主代理应该同时利用这两种来源，因此提出了ChessGPT，一个通过整合国际象棋游戏中的数据来连接策略学习和语言模型的GPT模型。

### 🚀 核心方法
💡 创新点1：构建大规模游戏和语言数据集
本文构建了一个包含大量国际象棋游戏数据和自然语言数据的综合数据集，包括在线游戏回放、专业棋手比赛、计算机引擎游戏、棋盘游戏、棋盘游戏分析、棋盘游戏博客、棋盘游戏书籍、棋盘游戏论坛、棋盘游戏视频等。

💡 创新点2：提出ChessCLIP和ChessGPT模型
本文提出了两种模型，ChessCLIP和ChessGPT，利用上述数据集进行训练。ChessCLIP通过对比学习将策略和语言模态连接起来，而ChessGPT则通过因果语言模型进行策略学习。

💡 创新点3：提出全面的评估框架
本文设计了一个全面的评估框架，用于评估语言模型在国际象棋方面的能力，包括建模能力、价值判断能力和策略能力。

### 📈 实验结果
实验结果表明，ChessGPT模型在所有评估任务中都优于其他LLM基线模型，证明了模型和数据集的有效性。

### 💬 可借鉴之处
本文提出的ChessGPT模型和数据集为研究策略学习和语言模型之间的相互作用提供了新的思路和方法，并为开发更强大的自主代理提供了新的可能性。

## omni--open-endedness-via-models-of-human-notions-of-interestingness
### Abstract
Open-ended algorithms aim to learn new, interesting behaviors forever. That
requires a vast environment search space, but there are thus infinitely many
possible tasks. Even after filtering for tasks the current agent can learn
(i.e., learning progress), countless learnable yet uninteresting tasks remain
(e.g., minor variations of previously learned tasks). An Achilles Heel of
open-endedness research is the inability to quantify (and thus prioritize)
tasks that are not just learnable, but also $\textit{interesting}$ (e.g.,
worthwhile and novel). We propose solving this problem by
$\textit{Open-endedness via Models of human Notions of Interestingness}$
(OMNI). The insight is that we can utilize foundation models (FMs) as a model
of interestingness (MoI), because they $\textit{already}$ internalize human
concepts of interestingness from training on vast amounts of human-generated
data, where humans naturally write about what they find interesting or boring.
We show that FM-based MoIs improve open-ended learning by focusing on tasks
that are both learnable $\textit{and interesting}$, outperforming baselines
based on uniform task sampling or learning progress alone. This approach has
the potential to dramatically advance the ability to intelligently select which
tasks to focus on next (i.e., auto-curricula), and could be seen as AI
selecting its own next task to learn, facilitating self-improving AI and
AI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/
### 🌟 论文解读 | OMNI：基于人类兴趣概念的开放性学习

### 📌 背景痛点/本文动机
开放性学习算法旨在让AI像人类一样，在复杂多变的环境中不断学习新技能。然而，面对无限可能的学习任务，如何选择哪些任务进行学习成为一大挑战。现有的方法，如基于学习进度的自动课程学习，容易陷入重复或无趣的任务中，无法有效引导AI进行有意义的学习。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：OMNI框架
OMNI框架通过结合学习进度模型和人类兴趣模型，实现了对学习任务的智能筛选。学习进度模型负责识别当前AI能够学习的任务，而人类兴趣模型则负责评估这些任务是否有趣，从而确保AI能够专注于有意义的学习。

💡 创新点2：利用预训练基础模型
OMNI框架利用预训练的基础模型（如GPT-3和GPT-4）作为人类兴趣模型，从而避免了手动定义兴趣指标的困难。这些基础模型在大量人类生成的数据上进行训练，已经内化了人类对有趣事物的概念，能够有效地评估任务的有趣程度。

### 📈 实验结果
OMNI框架在Crafter、BabyAI和AI2-THOR等环境中进行了实验，结果表明，与基于均匀任务采样或仅基于学习进度的方法相比，OMNI框架能够显著提高AI的学习效率，学习更多有趣且具有挑战性的任务。

### 💬 可借鉴之处
OMNI框架为开放性学习算法提供了新的思路，其核心思想可以应用于其他领域，例如：
* **多模态模型**：将视觉-语言模型等融入人类兴趣模型，以更全面地评估任务的有趣程度。
* **自主学习**：让人类兴趣模型能够自主分析任务成功率等指标，并据此调整对有趣程度的评估。
* **安全性与价值对齐**：通过引入人类反馈或AI反馈，确保OMNI框架能够选择符合人类价值观和期望的任务进行学习。

### 🌈 未来展望
OMNI框架为开放性学习算法的发展开辟了新的方向，未来可以进一步探索以下方向：
* **更复杂的任务空间**：将OMNI框架应用于更复杂的任务空间，例如虚拟现实环境或真实世界环境。
* **更智能的人类兴趣模型**：开发更智能的人类兴趣模型，使其能够更好地理解人类对有趣事物的概念，并能够根据AI的学习进度动态调整评估标准。
* **安全性与价值对齐**：探索更有效的方法，确保OMNI框架能够选择符合人类价值观和期望的任务进行学习，从而避免潜在的安全风险。

## steve-1--a-generative-model-for-text-to-behavior-in-minecraft
### Abstract
Constructing AI models that respond to text instructions is challenging,
especially for sequential decision-making tasks. This work introduces a
methodology, inspired by unCLIP, for instruction-tuning generative models of
behavior without relying on a large dataset of instruction-labeled
trajectories. Using this methodology, we create an instruction-tuned Video
Pretraining (VPT) model called STEVE-1, which can follow short-horizon
open-ended text and visual instructions in Minecraft. STEVE-1 is trained in two
steps: adapting the pretrained VPT model to follow commands in MineCLIP's
latent space, then training a prior to predict latent codes from text. This
allows us to finetune VPT through self-supervised behavioral cloning and
hindsight relabeling, reducing the need for costly human text annotations, and
all for only $60 of compute. By leveraging pretrained models like VPT and
MineCLIP and employing best practices from text-conditioned image generation,
STEVE-1 sets a new bar for open-ended instruction-following in Minecraft with
low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming
previous baselines and robustly completing 12 of 13 tasks in our early-game
evaluation suite. We provide experimental evidence highlighting key factors for
downstream performance, including pretraining, classifier-free guidance, and
data scaling. All resources, including our model weights, training scripts, and
evaluation tools are made available for further research.
### 🌟 论文解读 | Minecraft 中的文本到行为生成模型：STEVE-1

### 📌 背景痛点/本文动机
构建能够响应文本指令的 AI 模型是一项挑战，尤其是在需要顺序决策的任务中。现有的模型往往需要大量带有指令标签的轨迹数据集，这既昂贵又难以获取。本文旨在解决这一问题，提出了一种无需依赖大量指令标签轨迹数据集的方法，用于构建能够响应文本指令的行为生成模型。

### 🚀 核心方法
💡 创新点1：受 unCLIP 启发的指令微调方法
本文提出了一种受 unCLIP 启发的指令微调方法，用于构建行为生成模型。该方法将问题分解为两个模型：一个用于生成行为轨迹的策略模型，另一个用于将文本指令转换为视觉嵌入的先验模型。通过这种方式，可以避免使用昂贵的文本指令标签，而是利用视觉嵌入进行训练。

💡 创新点2：基于 VPT 和 MineCLIP 的模型构建
本文利用了两个预训练模型：VPT 和 MineCLIP。VPT 是一个基于 Minecraft 游戏视频预训练的行为模型，而 MineCLIP 是一个将文本和视频片段对齐的模型。通过将这两个模型结合起来，可以构建一个能够响应文本指令的行为生成模型。

💡 创新点3：基于自监督学习和回溯重标记的微调
本文使用自监督学习和回溯重标记技术对 VPT 模型进行微调。通过这种方式，可以减少对昂贵的人类文本注释的需求，并利用现有的数据集进行训练。

### 📈 实验结果
实验结果表明，STEVE-1 在 Minecraft 中能够有效地响应文本指令，并完成各种任务。与之前的基线模型相比，STEVE-1 在低级控制（鼠标和键盘）和原始像素输入方面取得了显著的性能提升。此外，实验还表明，预训练、分类器无关引导和数据缩放等因素对下游性能至关重要。

### 💬 可借鉴之处
本文提出的指令微调方法可以应用于其他领域和任务，例如机器人控制、虚拟现实等。此外，本文还强调了预训练、分类器无关引导和数据缩放等因素对下游性能的重要性，为构建更强大的 AI 模型提供了参考。

## the-text-based-adventure-ai-competition
### Abstract
In 2016, 2017, and 2018 at the IEEE Conference on Computational Intelligence
in Games, the authors of this paper ran a competition for agents that can play
classic text-based adventure games. This competition fills a gap in existing
game AI competitions that have typically focussed on traditional card/board
games or modern video games with graphical interfaces. By providing a platform
for evaluating agents in text-based adventures, the competition provides a
novel benchmark for game AI with unique challenges for natural language
understanding and generation. This paper summarises the three competitions ran
in 2016, 2017, and 2018 (including details of open source implementations of
both the competition framework and our competitors) and presents the results of
an improved evaluation of these competitors across 20 games.
### 🌟 论文解读 | 文本冒险游戏AI竞赛：探索自然语言理解和生成的挑战

### 📌 背景痛点/本文动机
随着图形化界面的游戏AI竞赛的兴起，文本冒险游戏AI竞赛填补了现有游戏AI竞赛的空白。文本冒险游戏为游戏AI提供了独特的挑战，特别是在自然语言理解和生成方面。本文介绍了2016年至2018年在IEEE计算智能与游戏会议上举办的文本冒险游戏AI竞赛，并总结了竞赛的成果和经验。

### 🚀 核心方法
💡 创新点1：竞赛框架
竞赛框架基于ZPlet Java解释器，用于评估软件代理在文本冒险游戏中的能力。代理通过一个接口与游戏交互，该接口接收游戏环境描述并返回代理想要执行的动作。

💡 创新点2：多种代理算法
竞赛中提交了多种代理算法，包括BYUAgent 2016、Golovin、CARL (BYUAgent 2017) 和 NAIL。这些代理算法使用了不同的方法来生成命令和与游戏交互，例如基于word2vec的动词-名词匹配、命令模式生成和知识图谱构建。

### 📈 实验结果
竞赛结果表明，NAIL代理在2018年竞赛中表现最佳，其次是CARL和Golovin。与2016年和2017年相比，所有代理的性能都有所提高。然而，即使是表现最好的代理，也只能完成测试游戏中的一小部分，并且在许多游戏中无法获得任何分数。

### 💬 可借鉴之处
本文提出的文本冒险游戏AI竞赛为游戏AI研究提供了一个新的基准，并强调了自然语言理解和生成在游戏AI中的重要性。竞赛结果表明，现有的代理算法在处理文本冒险游戏的复杂性和多样性方面仍然面临挑战。未来的研究可以探索更有效的自然语言处理方法，以及如何将文本冒险游戏AI应用于现实世界的自然语言处理任务。

## alphablock--embodied-finetuning-for-vision-language-reasoning-in-robot-manipulation
### Abstract
We propose a novel framework for learning high-level cognitive capabilities
in robot manipulation tasks, such as making a smiley face using building
blocks. These tasks often involve complex multi-step reasoning, presenting
significant challenges due to the limited paired data connecting human
instructions (e.g., making a smiley face) and robot actions (e.g., end-effector
movement). Existing approaches relieve this challenge by adopting an open-loop
paradigm decomposing high-level instructions into simple sub-task plans, and
executing them step-by-step using low-level control models. However, these
approaches are short of instant observations in multi-step reasoning, leading
to sub-optimal results. To address this issue, we propose to automatically
collect a cognitive robot dataset by Large Language Models (LLMs). The
resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of
multi-step text plans and paired observation sequences. To enable efficient
data acquisition, we employ elaborated multi-round prompt designs that
effectively reduce the burden of extensive human involvement. We further
propose a closed-loop multi-modal embodied planning model that autoregressively
generates plans by taking image observations as input. To facilitate effective
learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and
finetune additional vision adapter and Q-former to enable fine-grained spatial
perception for manipulation tasks. We conduct experiments to verify the
superiority over existing open and closed-loop methods, and achieve a
significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4
based robot tasks. Real-world demos are shown in
https://www.youtube.com/watch?v=ayAzID1_qQk .
### 🌟 论文解读 | AlphaBlock：机器人操作中的视觉-语言推理

### 📌 背景痛点/本文动机
在机器人操作任务中，如使用积木制作笑脸，机器人需要理解和执行复杂的语言指令，这涉及到感知、推理和操作。然而，现有的方法通常采用开环范式，将高级指令分解为简单的子任务计划，并使用低级控制模型逐步执行。这种方法缺乏多步推理中的即时观察，导致结果不理想。

### 🚀 核心方法
💡 创新点1：自动收集认知机器人数据集
为了解决这个问题，本文提出了一种自动收集认知机器人数据集的方法，通过大型语言模型（LLMs）自动生成多步文本计划和配对的观察序列。这种方法有效地减少了人类参与的负担，并收集了35个综合的高水平任务，包括多步文本计划和配对的观察序列。

💡 创新点2：闭环多模态具身规划模型
本文进一步提出了一种闭环多模态具身规划模型，该模型通过输入图像观察来自回归地生成计划。为了促进有效学习，本文利用了MiniGPT-4，包括冻结的视觉编码器和LLM，并微调了额外的视觉适配器和Q-former，以实现操作任务中的细粒度空间感知。

### 📈 实验结果
实验结果表明，与现有的开环和闭环方法相比，本文提出的CogLoop框架在机器人任务中取得了显著的性能提升。与ChatGPT和GPT-4相比，CogLoop的成功率分别提高了21.4%和14.5%。

### 💬 可借鉴之处
本文提出的CogLoop框架为机器人操作中的视觉-语言推理提供了一种新的思路。通过自动收集认知机器人数据集和闭环多模态具身规划模型，机器人可以更好地理解和执行高级指令，从而实现更智能的交互和操作。这种方法在家庭机器人、制造业和医疗保健等领域具有广泛的应用前景。

## playing-repeated-games-with-large-language-models
### Abstract
Large Language Models (LLMs) are transforming society and permeating into
diverse applications. As a result, LLMs will frequently interact with us and
other agents. It is, therefore, of great societal value to understand how LLMs
behave in interactive social settings. Here, we propose to use behavioral game
theory to study LLM's cooperation and coordination behavior. To do so, we let
different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with
each other and with other, human-like strategies. Our results show that LLMs
generally perform well in such tasks and also uncover persistent behavioral
signatures. In a large set of two players-two strategies games, we find that
LLMs are particularly good at games where valuing their own self-interest pays
off, like the iterated Prisoner's Dilemma family. However, they behave
sub-optimally in games that require coordination. We, therefore, further focus
on two games from these distinct families. In the canonical iterated Prisoner's
Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting
after another agent has defected only once. In the Battle of the Sexes, we find
that GPT-4 cannot match the behavior of the simple convention to alternate
between options. We verify that these behavioral signatures are stable across
robustness checks. Finally, we show how GPT-4's behavior can be modified by
providing further information about the other player as well as by asking it to
predict the other player's actions before making a choice. These results enrich
our understanding of LLM's social behavior and pave the way for a behavioral
game theory for machines.
### 🌟 论文解读 | 大型语言模型在重复博弈中的行为研究

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）在社会各领域的广泛应用，它们与人类和其他智能体之间的互动日益频繁。因此，理解LLMs在社交互动中的行为变得至关重要。本文旨在通过行为博弈论来研究LLMs的合作与协调行为。

### 🚀 核心方法
💡 创新点1：使用行为博弈论研究LLMs
本文提出使用行为博弈论来研究LLMs的合作与协调行为。通过让不同的LLMs（如GPT-3、GPT-3.5和GPT-4）与其他LLMs或人类策略进行有限重复博弈，分析它们的行为模式。

💡 创新点2：研究LLMs在不同博弈类型中的表现
本文研究了LLMs在多种博弈类型中的表现，包括双赢游戏、不公平游戏、循环游戏、有偏游戏和囚徒困境等。通过分析LLMs在不同博弈类型中的得分和策略选择，揭示了它们在不同情境下的行为特征。

### 📈 实验结果
实验结果表明，LLMs在追求自身利益的博弈中表现良好，尤其是在囚徒困境等博弈中。然而，在需要协调的博弈中，LLMs的表现较差。例如，在经典的囚徒困境中，GPT-4表现出极端的不宽容，一旦对方背叛，它就会持续背叛。而在性别之战中，GPT-4无法与交替选择的人类策略相匹配。

### 💬 可借鉴之处
本文的研究结果有助于我们更好地理解LLMs的社交行为，并为机器行为博弈论的发展奠定了基础。此外，本文还提出了两种改进LLMs行为的方法：提供关于其他玩家的更多信息，以及让LLMs预测其他玩家的行为。这些方法可以帮助LLMs更好地适应社交环境，并与人类进行更有效的互动。

## voyager--an-open-ended-embodied-agent-with-large-language-models
### Abstract
We introduce Voyager, the first LLM-powered embodied lifelong learning agent
in Minecraft that continuously explores the world, acquires diverse skills, and
makes novel discoveries without human intervention. Voyager consists of three
key components: 1) an automatic curriculum that maximizes exploration, 2) an
ever-growing skill library of executable code for storing and retrieving
complex behaviors, and 3) a new iterative prompting mechanism that incorporates
environment feedback, execution errors, and self-verification for program
improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses
the need for model parameter fine-tuning. The skills developed by Voyager are
temporally extended, interpretable, and compositional, which compounds the
agent's abilities rapidly and alleviates catastrophic forgetting. Empirically,
Voyager shows strong in-context lifelong learning capability and exhibits
exceptional proficiency in playing Minecraft. It obtains 3.3x more unique
items, travels 2.3x longer distances, and unlocks key tech tree milestones up
to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill
library in a new Minecraft world to solve novel tasks from scratch, while other
techniques struggle to generalize. We open-source our full codebase and prompts
at https://voyager.minedojo.org/.
### 🌟 论文解读 | Voyager：基于大型语言模型的开放式具身终身学习智能体

### 📌 背景痛点/本文动机
构建能够在开放世界中持续探索、规划和开发新技能的通用具身智能体，是人工智能领域的一大挑战。传统的强化学习和模仿学习方法在探索、可解释性和泛化方面存在局限性。近年来，基于大型语言模型（LLM）的智能体利用预训练LLM中的世界知识生成一致的行动计划或可执行策略，但它们并非终身学习者，无法在长时间跨度内逐步获取、更新、积累和转移知识。

### 🚀 核心方法
Voyager 是第一个由 LLM 驱动的具身终身学习智能体，能够在 Minecraft 中持续探索世界、获取多样化技能，并在没有人类干预的情况下进行新的发现。Voyager 由三个关键组件组成：

💡 创新点1：自动课程
Voyager 通过自动课程进行开放式探索，该课程由 GPT-4 生成，旨在“发现尽可能多的多样化事物”。课程会根据探索进度和智能体的状态提出越来越难的任务，从而推动智能体不断学习新技能。

💡 创新点2：技能库
Voyager 拥有一个不断增长的技能库，用于存储和检索可执行代码，以存储和检索复杂的行为。每个技能都由可执行代码表示，这些代码可以自然地表示时间扩展和组合动作，这对于 Minecraft 中的许多长期任务至关重要。

💡 创新点3：迭代提示机制
Voyager 通过迭代提示机制生成可执行代码，该机制利用环境反馈、执行错误和自我验证来改进程序。该机制通过执行生成的程序、获取环境反馈和执行错误，并将这些反馈纳入 GPT-4 的提示中，从而进行代码改进。这个过程会重复进行，直到自我验证模块确认任务完成，此时将程序添加到技能库中，并查询自动课程以获取下一个目标。

### 📈 实验结果
Voyager 在 MineDojo 框架中与其他 LLM 基于智能体技术进行了比较，结果表明 Voyager 在发现新物品、解锁 Minecraft 技术树、穿越各种地形以及将学习到的技能库应用于新世界中的未见任务方面表现出色。Voyager 获得了 3.3 倍的新物品，解锁关键技术树里程碑的速度提高了 15.3 倍，穿越的距离是基线的 2.3 倍。

### 💬 可借鉴之处
Voyager 的设计为开发强大的通用智能体提供了一个起点，无需调整模型参数。其自动课程、技能库和迭代提示机制为终身学习智能体的开发提供了新的思路。此外，Voyager 的技能库可以作为其他方法的即插即用资产，有效地提高性能。

## ghost-in-the-minecraft--generally-capable-agents-for-open-world-environments-via-large-language-models-with-text-based-knowledge-and-memory
### Abstract
The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular "ObtainDiamond" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the "ObtainDiamond" task stands at around 20%, highlighting the limitations of
Reinforcement Learning (RL) based controllers used in existing methods. To
tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel
framework integrates Large Language Models (LLMs) with text-based knowledge and
memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These
agents, equipped with the logic and common sense capabilities of LLMs, can
skillfully navigate complex, sparse-reward environments with text-based
interactions. We develop a set of structured actions and leverage LLMs to
generate action plans for the agents to execute. The resulting LLM-based agent
markedly surpasses previous methods, achieving a remarkable improvement of
+47.5% in success rate on the "ObtainDiamond" task, demonstrating superior
robustness compared to traditional RL-based controllers. Notably, our agent is
the first to procure all items in the Minecraft Overworld technology tree,
demonstrating its extensive capabilities. GITM does not need any GPU for
training, but a single CPU node with 32 CPU cores is enough. This research
shows the potential of LLMs in developing capable agents for handling
long-horizon, complex tasks and adapting to uncertainties in open-world
environments. See the project website at https://github.com/OpenGVLab/GITM.
### 🌟 论文解读 | Minecraft中的幽灵：通过大型语言模型和基于文本的知识与记忆，创建开放世界环境中的通用能力智能体

### 📌 背景痛点/本文动机
Minecraft作为一款开放世界游戏，吸引了大量研究兴趣，成为开发能够在开放世界中运行的智能体的丰富平台。然而，目前的研究主要集中在特定目标上，如流行的“ObtainDiamond”任务，尚未在更广泛的任务上展现出有效的泛化能力。此外，现有方法在“ObtainDiamond”任务上的最高成功率仅为约20%，突显了现有基于强化学习（RL）的控制器方法的局限性。为了解决这些挑战，本文提出了Ghost in the Minecraft（GITM）框架，该框架将大型语言模型（LLMs）与基于文本的知识和记忆相结合，旨在创建能够在Minecraft中运行的通用能力智能体（GCAs）。

### 🚀 核心方法
💡 创新点1：LLM分解器
LLM分解器负责将任务目标分解为一系列更易于实现的子目标。通过解决每个子目标，可以逐步实现任务目标。LLM分解器利用从互联网收集的文本知识，将目标分解为子目标树。

💡 创新点2：LLM规划器
LLM规划器负责为每个子目标生成一系列结构化操作。结构化操作具有明确的语义和相应的反馈，使LLMs能够在认知层面理解周围环境并做出决策。LLM规划器还记录和总结成功的操作列表，以增强未来的规划。

💡 创新点3：LLM接口
LLM接口负责将结构化操作转换为键盘/鼠标操作，并与环境进行交互。它还从环境中提取观察结果，并将其转换为反馈消息。

### 📈 实验结果
本文的实验结果表明，基于LLM的智能体在“ObtainDiamond”任务上的成功率显著提高，达到了47.5%，超过了现有的方法。此外，该智能体是第一个在Minecraft Overworld中获取所有物品的智能体，展示了其广泛的技能。

### 💬 可借鉴之处
本文提出的GITM框架为开发能够在开放世界中运行的通用能力智能体提供了一种新的思路。通过利用LLMs的常识和推理能力，以及基于文本的知识和记忆，该框架能够使智能体有效地处理开放世界环境中的各种挑战。此外，该框架还具有高效的学习效率和良好的泛化能力，使其在开发通用能力智能体方面具有巨大的潜力。

## spring--studying-the-paper-and-reasoning-to-play-games
### Abstract
Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent's current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM's answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
"reasoning" induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.
### 🌟 论文解读 | SPRING：通过阅读论文和推理来玩游戏

### 📌 背景痛点/本文动机
开放世界生存游戏对AI算法提出了重大挑战，因为它们需要多任务处理、深度探索和目标优先级排序。尽管强化学习（RL）在解决游戏问题方面很受欢迎，但其高样本复杂度限制了其在像Crafter或Minecraft这样的复杂开放世界游戏中的有效性。

### 🚀 核心方法
SPRING是一种新颖的方法，它通过阅读游戏的原始学术论文并使用从大型语言模型（LLM）中学习到的知识来推理和玩游戏。SPRING框架使用一个有向无环图（DAG），其中游戏相关问题作为节点，依赖关系作为边。通过遍历DAG并按拓扑顺序计算LLM对每个节点的响应，我们可以确定在环境中采取的最佳行动，LLM对最终节点的答案直接转换为环境行动。

### 📈 实验结果
在Crafter开放世界环境中，我们的实验研究了不同形式的提示在上下文中引起的“推理”质量。我们的实验表明，当LLM被提示进行一致的思维链时，它们在完成复杂的高级轨迹方面具有巨大的潜力。定量地说，SPRING与GPT-4的表现优于所有最先进的RL基线，这些基线经过1M步的训练，而SPRING没有经过任何训练。

### 💬 可借鉴之处
SPRING是第一个通过从学术论文中明确提取多个交互和科技树依赖来应对竞争性RL基准的方法。我们是第一个在具有挑战性的开放世界游戏中展示SOTA性能的零样本LLM-based（GPT-4）策略。我们研究了不同提示引起的上下文“推理”质量，并通过DAG中的问题链提出了一种受控的思维链提示，用于决策。

## improving-factuality-and-reasoning-in-language-models-through-multiagent-debate
### Abstract
Large language models (LLMs) have demonstrated remarkable capabilities in
language generation, understanding, and few-shot learning in recent years. An
extensive body of work has explored how their performance may be further
improved through the tools of prompting, ranging from verification,
self-consistency, or intermediate scratchpads. In this paper, we present a
complementary approach to improve language responses where multiple language
model instances propose and debate their individual responses and reasoning
processes over multiple rounds to arrive at a common final answer. Our findings
indicate that this approach significantly enhances mathematical and strategic
reasoning across a number of tasks. We also demonstrate that our approach
improves the factual validity of generated content, reducing fallacious answers
and hallucinations that contemporary models are prone to. Our approach may be
directly applied to existing black-box models and uses identical procedure and
prompts for all tasks we investigate. Overall, our findings suggest that such
"society of minds" approach has the potential to significantly advance the
capabilities of LLMs and pave the way for further breakthroughs in language
generation and understanding.
### 🌟 论文解读 | 通过多智能体辩论提升语言模型的事实性和推理能力

### 📌 背景痛点/本文动机
近年来，大型语言模型（LLMs）在语言生成、理解和少样本学习方面取得了显著进展。然而，这些模型在事实性和推理方面仍存在不足，容易产生错误的事实和推理跳跃。本文提出了一种新的方法，通过多智能体辩论来提升LLMs的事实性和推理能力。

### 🚀 核心方法
💡 创新点1：多智能体辩论
本文提出了一种新的方法，通过多智能体辩论来提升LLMs的事实性和推理能力。具体来说，给定一个查询，多个LLM实例（或智能体）首先生成各自的候选答案。然后，每个智能体阅读并批评其他智能体的答案，并使用这些内容来更新自己的答案。这个过程会重复进行多轮，直到智能体们达成一个共同的最终答案。

💡 创新点2：提升事实性和推理能力
本文发现，多智能体辩论方法在多个推理、事实性和问答任务上显著优于单模型基线，如零样本思维链和反思。此外，该方法还能提高生成内容的事实性，减少当代模型容易产生的错误答案和幻觉。

### 📈 实验结果
本文在多个推理和事实性任务上评估了多智能体辩论方法，包括算术、小学数学、国际象棋移动预测、传记生成和MMLU。结果表明，多智能体辩论方法在这些任务上均取得了显著的性能提升。

### 💬 可借鉴之处
本文提出的多智能体辩论方法为提升LLMs的事实性和推理能力提供了一种新的思路。该方法简单有效，可以应用于各种不同的推理和事实性任务。此外，该方法还可以与其他模型生成改进方法相结合，进一步提升LLMs的性能。

## improving-language-model-negotiation-with-self-play-and-in-context-learning-from-ai-feedback
### Abstract
We study whether multiple large language models (LLMs) can autonomously
improve each other in a negotiation game by playing, reflecting, and
criticizing. We are interested in this question because if LLMs were able to
improve each other, it would imply the possibility of creating strong AI agents
with minimal human intervention. We ask two LLMs to negotiate with each other,
playing the roles of a buyer and a seller, respectively. They aim to reach a
deal with the buyer targeting a lower price and the seller a higher one. A
third language model, playing the critic, provides feedback to a player to
improve the player's negotiation strategies. We let the two agents play
multiple rounds, using previous negotiation history and AI feedback as
in-context demonstrations to improve the model's negotiation strategy
iteratively. We use different LLMs (GPT and Claude) for different roles and use
the deal price as the evaluation metric. Our experiments reveal multiple
intriguing findings: (1) Only a subset of the language models we consider can
self-play and improve the deal price from AI feedback, weaker models either do
not understand the game's rules or cannot incorporate AI feedback for further
improvement. (2) Models' abilities to learn from the feedback differ when
playing different roles. For example, it is harder for Claude-instant to
improve as the buyer than as the seller. (3) When unrolling the game to
multiple rounds, stronger agents can consistently improve their performance by
meaningfully using previous experiences and iterative AI feedback, yet have a
higher risk of breaking the deal. We hope our work provides insightful initial
explorations of having models autonomously improve each other with game playing
and AI feedback.
### 🌟 论文解读 | 语言模型在谈判游戏中通过自我博弈和AI反馈进行自我提升

### 📌 背景痛点/本文动机
本文研究了多个大型语言模型（LLMs）是否能够在谈判游戏中通过自我博弈、反思和批评来自主地相互提升。这一研究问题具有重要意义，因为如果LLMs能够相互提升，那么就有可能以极小的人类干预来创建强大的AI代理。本文通过让两个LLMs进行谈判，一个扮演买家，一个扮演卖家，并让第三个LLM作为批评者提供反馈来改善玩家的谈判策略，从而探索了这一可能性。

### 🚀 核心方法
💡 创新点1：本文提出了“从AI反馈中进行上下文学习”（ICL-AIF）的方法。具体来说，使用AI批评者的反馈以及之前的谈判历史作为上下文演示，以迭代地改进模型的谈判策略。

💡 创新点2：本文使用不同的LLMs（GPT和Claude）来扮演不同的角色，并使用交易价格作为评估指标。通过实验，本文发现只有一部分LLMs能够通过自我博弈和AI反馈来提高交易价格，而较弱的模型要么不理解游戏规则，要么无法将AI反馈纳入进一步的改进。

### 📈 实验结果
实验结果表明，只有一部分LLMs能够通过自我博弈和AI反馈来提高交易价格，而较弱的模型要么不理解游戏规则，要么无法将AI反馈纳入进一步的改进。此外，模型在扮演不同角色时，从反馈中学习的能力也有所不同。例如，Claude-instant在扮演买家时比扮演卖家时更难提升。当游戏扩展到多轮时，更强的代理可以通过有意义地使用之前的经验和迭代AI反馈来持续提高其性能，但同时也存在更高的交易破裂风险。

### 💬 可借鉴之处
本文的研究结果表明，LLMs在谈判游戏中具有自我提升的潜力，但同时也存在一些挑战和风险。未来研究可以探索如何更好地利用AI反馈来提升LLMs的性能，并确保其行为符合人类的期望和价值观。此外，本文的研究方法也可以为其他多智能体游戏场景下的AI学习提供参考。

## tidybot--personalized-robot-assistance-with-large-language-models
### Abstract
For a robot to personalize physical assistance effectively, it must learn
user preferences that can be generally reapplied to future scenarios. In this
work, we investigate personalization of household cleanup with robots that can
tidy up rooms by picking up objects and putting them away. A key challenge is
determining the proper place to put each object, as people's preferences can
vary greatly depending on personal taste or cultural background. For instance,
one person may prefer storing shirts in the drawer, while another may prefer
them on the shelf. We aim to build systems that can learn such preferences from
just a handful of examples via prior interactions with a particular person. We
show that robots can combine language-based planning and perception with the
few-shot summarization capabilities of large language models (LLMs) to infer
generalized user preferences that are broadly applicable to future
interactions. This approach enables fast adaptation and achieves 91.2% accuracy
on unseen objects in our benchmark dataset. We also demonstrate our approach on
a real-world mobile manipulator called TidyBot, which successfully puts away
85.0% of objects in real-world test scenarios.
### 🌟 论文解读 | TidyBot：基于大型语言模型的个性化机器人辅助

### 📌 背景痛点/本文动机
随着人工智能技术的不断发展，机器人辅助已经成为人们日常生活中的一部分。然而，现有的机器人辅助系统往往缺乏个性化，无法根据用户的个人喜好和习惯来完成任务。例如，在家庭清洁任务中，机器人需要根据用户的喜好来决定每个物品应该放置的位置，而用户的喜好可能会因为个人品味或文化背景而有所不同。

### 🚀 核心方法
本文提出了一个名为 TidyBot 的个性化机器人辅助系统，该系统利用大型语言模型（LLM）的摘要能力来学习用户的偏好，并将其应用于未来的场景中。具体来说，TidyBot 通过以下步骤来实现个性化辅助：

1. **用户输入示例**： 用户通过文本输入提供一些示例，说明他们希望如何放置特定的物品。
2. **LLM 摘要**： LLM 将这些示例摘要成一个通用的规则，例如“浅色衣服放在抽屉里，深色衣服放在衣柜里”。
3. **规则应用**： TidyBot 使用这些规则来确定新物品应该放置的位置，并将其放入相应的容器中。

### 📈 实验结果
本文在文本基准数据集和真实世界的机器人系统上评估了 TidyBot 的性能。结果表明，TidyBot 在基准数据集上实现了 91.2% 的准确率，在真实世界的测试场景中成功地将 85.0% 的物品放入正确的容器中。

### 💬 可借鉴之处
本文提出的 TidyBot 系统为个性化机器人辅助提供了一种新的思路。通过利用 LLM 的摘要能力，TidyBot 可以快速学习用户的偏好，并将其应用于未来的场景中。这种方法具有以下优点：

* **快速适应**： TidyBot 只需要少量示例就可以学习用户的偏好，从而实现快速适应。
* **可解释性**： LLM 生成的规则以文本形式呈现，易于人类理解和解释。
* **泛化能力**： TidyBot 可以将学习到的规则应用于新的场景中，从而实现泛化。

### 🌟 总结
TidyBot 是一个基于大型语言模型的个性化机器人辅助系统，它可以帮助机器人更好地理解用户的偏好，并提供更加个性化的辅助服务。本文提出的 TidyBot 系统为个性化机器人辅助提供了一种新的思路，并为未来的研究提供了重要的参考价值。

## ark--augmented-reality-with-knowledge-interactive-emergent-ability
### Abstract
Despite the growing adoption of mixed reality and interactive AI agents, it
remains challenging for these systems to generate high quality 2D/3D scenes in
unseen environments. The common practice requires deploying an AI agent to
collect large amounts of data for model training for every new task. This
process is costly, or even impossible, for many domains. In this study, we
develop an infinite agent that learns to transfer knowledge memory from general
foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene
understanding and generation in the physical or virtual world. The heart of our
approach is an emerging mechanism, dubbed Augmented Reality with Knowledge
Inference Interaction (ArK), which leverages knowledge-memory to generate
scenes in unseen physical world and virtual reality environments. The knowledge
interactive emergent ability (Figure 1) is demonstrated as the observation
learns i) micro-action of cross-modality: in multi-modality models to collect a
large amount of relevant knowledge memory data for each interaction task (e.g.,
unseen scene understanding) from the physical reality; and ii) macro-behavior
of reality-agnostic: in mix-reality environments to improve interactions that
tailor to different characterized roles, target variables, collaborative
information, and so on. We validate the effectiveness of ArK on the scene
generation and editing tasks. We show that our ArK approach, combined with
large foundation models, significantly improves the quality of generated 2D/3D
scenes, compared to baselines, demonstrating the potential benefit of
incorporating ArK in generative AI for applications such as metaverse and
gaming simulation.
### 🌟 论文解读 | ArK：基于知识的增强现实交互式涌现能力

### 📌 背景痛点/本文动机
随着混合现实和交互式AI代理的日益普及，这些系统在生成高质量2D/3D场景方面仍然面临挑战。传统的做法需要部署AI代理来收集大量数据以进行模型训练，这对于许多领域来说既昂贵又不可能。本文旨在解决这一难题，通过开发一个无限代理，该代理能够从通用基础模型（如GPT4、DALLE）中学习，并将知识记忆转移到新领域或场景中，以实现物理或虚拟世界中的场景理解和生成。

### 🚀 核心方法
💡 创新点1：ArK机制
本文的核心是ArK机制，即“增强现实与知识推理交互”。该机制利用知识记忆来生成未见过的物理世界和虚拟现实环境中的场景。ArK机制通过观察学习实现：
- 跨模态的微动作：在多模态模型中，为每个交互任务（例如，未见过的场景理解）从物理现实中收集大量相关的知识记忆数据。
- 现实无关的宏行为：在混合现实环境中，通过调整交互以适应不同的角色、目标变量、协作信息等，从而提高交互效果。

💡 创新点2：无限知识记忆代理
本文开发了一个无限知识记忆代理，用于物理世界中的场景理解和生成。该代理学习从通用基础模型中转移知识记忆，并能够理解和生成未见过的场景。此外，该代理还能够通过强化学习和模仿学习来提高其性能。

### 📈 实验结果
本文在场景生成和编辑任务上验证了ArK的有效性。实验结果表明，与基线相比，ArK方法结合大型基础模型显著提高了生成的2D/3D场景的质量，证明了将ArK纳入生成式AI中的潜在益处，例如在元宇宙和游戏模拟中的应用。

### 💬 可借鉴之处
本文提出的ArK机制和无限知识记忆代理为生成式AI领域提供了新的思路和方法。ArK机制能够有效地利用知识记忆来生成高质量的2D/3D场景，而无限知识记忆代理则能够通过学习和推理来提高其性能。这些创新点为生成式AI的发展和应用提供了新的可能性。

## generative-agents--interactive-simulacra-of-human-behavior
### Abstract
Believable proxies of human behavior can empower interactive applications
ranging from immersive environments to rehearsal spaces for interpersonal
communication to prototyping tools. In this paper, we introduce generative
agents--computational software agents that simulate believable human behavior.
Generative agents wake up, cook breakfast, and head to work; artists paint,
while authors write; they form opinions, notice each other, and initiate
conversations; they remember and reflect on days past as they plan the next
day. To enable generative agents, we describe an architecture that extends a
large language model to store a complete record of the agent's experiences
using natural language, synthesize those memories over time into higher-level
reflections, and retrieve them dynamically to plan behavior. We instantiate
generative agents to populate an interactive sandbox environment inspired by
The Sims, where end users can interact with a small town of twenty five agents
using natural language. In an evaluation, these generative agents produce
believable individual and emergent social behaviors: for example, starting with
only a single user-specified notion that one agent wants to throw a Valentine's
Day party, the agents autonomously spread invitations to the party over the
next two days, make new acquaintances, ask each other out on dates to the
party, and coordinate to show up for the party together at the right time. We
demonstrate through ablation that the components of our agent
architecture--observation, planning, and reflection--each contribute critically
to the believability of agent behavior. By fusing large language models with
computational, interactive agents, this work introduces architectural and
interaction patterns for enabling believable simulations of human behavior.
### 🌟 论文解读 | 生成式智能体：模拟人类行为的交互式模拟

### 📌 背景痛点/本文动机
随着人工智能技术的不断发展，人们对于能够模拟人类行为的智能体产生了浓厚的兴趣。这些智能体可以应用于各种场景，例如沉浸式环境、人际沟通演练空间、原型设计工具等。然而，要创建一个能够长期保持一致性和可信度的智能体仍然是一个挑战。

### 🚀 核心方法（介绍本文的几个创新点）
💡 创新点1：生成式智能体
本文提出了生成式智能体的概念，即利用生成模型模拟可信的人类行为。这些智能体能够进行日常活动，如起床、做饭、上班等，并能够形成自己的观点、与他人互动、发起对话等。

💡 创新点2：智能体架构
为了实现生成式智能体，本文提出了一种新的架构，该架构扩展了大型语言模型，使其能够存储智能体的经验记录，并将这些记忆随着时间的推移合成更高层次的反思，并动态地检索它们来规划行为。

### 📈 实验结果
本文通过在模拟环境中创建一个由25个智能体组成的小镇，展示了生成式智能体的潜力。实验结果表明，这些智能体能够产生可信的个体和群体行为，例如，在用户指定一个智能体想要举办情人节派对的情况下，智能体能够自主地邀请其他智能体参加派对，并协调在正确的时间一起到达派对地点。

### 💬 可借鉴之处
本文提出的生成式智能体架构为创建可信的人类行为模拟提供了新的思路。该架构可以应用于各种领域，例如角色扮演、社交原型设计、虚拟世界和游戏等。此外，本文还讨论了生成式智能体在交互式系统中的应用机会、伦理和社会风险。

## can-large-language-models-play-text-games-well--current-state-of-the-art-and-open-questions
### Abstract
Large language models (LLMs) such as ChatGPT and GPT-4 have recently
demonstrated their remarkable abilities of communicating with human users. In
this technical report, we take an initiative to investigate their capacities of
playing text games, in which a player has to understand the environment and
respond to situations by having dialogues with the game world. Our experiments
show that ChatGPT performs competitively compared to all the existing systems
but still exhibits a low level of intelligence. Precisely, ChatGPT can not
construct the world model by playing the game or even reading the game manual;
it may fail to leverage the world knowledge that it already has; it cannot
infer the goal of each step as the game progresses. Our results open up new
research questions at the intersection of artificial intelligence, machine
learning, and natural language processing.
### 🌟 论文解读 | 大型语言模型在文本游戏中的表现：现状与开放性问题

### 📌 背景痛点/本文动机
随着大型语言模型（LLMs）如ChatGPT和GPT-4在理解和响应人类语言查询方面展现出令人印象深刻的能力，研究界对其是否能够实现通用人工智能（AGI）的潜力产生了广泛讨论。本文旨在通过将LLMs置于文本游戏的环境中，评估其在理解环境、做出决策和与游戏世界进行交互方面的智能水平，从而为LLMs的能力和局限性提供新的见解。

### 🚀 核心方法
💡 创新点1：使用文本游戏作为评估LLMs智能水平的测试床。文本游戏要求玩家通过文本命令与游戏世界进行交互，从而提供了一个可控的环境来评估LLMs的智能水平。
💡 创新点2：通过分析LLMs在文本游戏中的表现，揭示了LLMs在构建世界模型、推断目标和导航能力方面的局限性。研究发现，尽管LLMs在文本游戏中的表现优于现有系统，但它们仍然缺乏构建世界模型、推断目标和进行有效导航的能力。

### 📈 实验结果
实验结果表明，ChatGPT在文本游戏中的表现优于现有系统，但仍然存在一些局限性。ChatGPT无法通过游戏或阅读游戏手册来构建世界模型，无法充分利用已有的世界知识，也无法推断游戏进行过程中每一步的目标。这些结果表明，LLMs在实现人类水平的智能方面仍然存在一些挑战。

### 💬 可借鉴之处
本文的研究结果表明，文本游戏可以作为评估LLMs智能水平的有效测试床。通过分析LLMs在文本游戏中的表现，可以揭示LLMs在构建世界模型、推断目标和导航能力方面的局限性，并为LLMs的未来发展提供新的方向。此外，本文的研究结果也为LLMs在游戏领域的应用提供了新的思路，例如开发基于LLMs的智能游戏助手或游戏角色。

## adapter-based-approaches-to-knowledge-enhanced-language-models----a-survey
### Abstract
Knowledge-enhanced language models (KELMs) have emerged as promising tools to
bridge the gap between large-scale language models and domain-specific
knowledge. KELMs can achieve higher factual accuracy and mitigate
hallucinations by leveraging knowledge graphs (KGs). They are frequently
combined with adapter modules to reduce the computational load and risk of
catastrophic forgetting. In this paper, we conduct a systematic literature
review (SLR) on adapter-based approaches to KELMs. We provide a structured
overview of existing methodologies in the field through quantitative and
qualitative analysis and explore the strengths and potential shortcomings of
individual approaches. We show that general knowledge and domain-specific
approaches have been frequently explored along with various adapter
architectures and downstream tasks. We particularly focused on the popular
biomedical domain, where we provided an insightful performance comparison of
existing KELMs. We outline the main trends and propose promising future
directions.
### 🌟 论文解读 | 基于适配器的知识增强语言模型方法综述

### 📌 背景痛点/本文动机
随着自然语言处理（NLP）领域大型语言模型（LLMs）的兴起，尽管它们在解决复杂推理任务和生成新文本方面表现出色，但它们往往缺乏对结构化知识层次（如概念之间的关系和推理能力）的认识。这可能导致下游任务中的预测不准确，以及在文本生成中产生所谓的“幻觉”，特别是在医疗保健或法律等高风险领域。

### 🚀 核心方法
本文综述了基于适配器的知识增强语言模型（KELMs）方法，旨在通过利用知识图谱（KGs）来提高LLMs的可靠性。适配器模块被证明是一种有效且计算效率高的解决方案，可以增强LLMs的任务性能，同时避免灾难性遗忘和任务间的干扰。

#### 适配器类型
- **Houlsby Adapter**: 在每个Transformer层中添加两个适配器模块，分别位于多头注意力层和两个前馈层之后。
- **Bapna and Firat Adapter**: 仅在每个Transformer层中的多头注意力层之后添加一个适配器模块。
- **Pfeiffer Adapter and AdapterFusion**: 使用AdapterFusion算法来组合不同任务上训练的适配器，从而实现信息共享。
- **K-Adapter**: 作为“外部插件”，由多个适配器层组成，可以插入到预训练模型的不同Transformer层中。

#### 知识增强方法
- **通用知识**: 利用ConceptNet和DBpedia等知识图谱，将常识和世界知识注入LLMs。
- **语言知识**: 将语言知识（如动词意义和论元结构）注入适配器，以提高事件提取和机器翻译等任务的性能。
- **领域特定知识**: 利用适配器进行领域适应，例如在医疗保健领域，通过UMLS等知识图谱来增强LLMs。

### 📈 实验结果
基于适配器的KELMs在下游任务中的性能始终优于基础LLMs。例如，DAKI和MoP框架在PubMedQA任务上的准确率分别提高了约7%和8%。此外，适配器调优比常规微调更能缓解遗忘问题。

### 💬 可借鉴之处
- **适配器架构**: 研究人员可以探索更高效的适配器架构，以克服序列数据处理的延迟并实现硬件并行化。
- **领域应用**: 除了医疗保健领域，适配器增强的LLMs还可以应用于其他高度结构化的领域，如法律或金融领域。
- **任务类型**: 未来可以探索将知识增强应用于生成任务，以提高生成文本的事实性和信息性。

### 📚 总结
本文综述了基于适配器的知识增强语言模型方法，并分析了不同适配器架构和知识增强方法的优缺点。随着研究的不断深入，适配器增强的LLMs有望在更多领域和任务中发挥重要作用。

## camel--communicative-agents-for--mind--exploration-of-large-language-model-society
### Abstract
The rapid advancement of chat-based language models has led to remarkable
progress in complex task-solving. However, their success heavily relies on
human input to guide the conversation, which can be challenging and
time-consuming. This paper explores the potential of building scalable
techniques to facilitate autonomous cooperation among communicative agents, and
provides insight into their "cognitive" processes. To address the challenges of
achieving autonomous cooperation, we propose a novel communicative agent
framework named role-playing. Our approach involves using inception prompting
to guide chat agents toward task completion while maintaining consistency with
human intentions. We showcase how role-playing can be used to generate
conversational data for studying the behaviors and capabilities of a society of
agents, providing a valuable resource for investigating conversational language
models. In particular, we conduct comprehensive studies on
instruction-following cooperation in multi-agent settings. Our contributions
include introducing a novel communicative agent framework, offering a scalable
approach for studying the cooperative behaviors and capabilities of multi-agent
systems, and open-sourcing our library to support research on communicative
agents and beyond: https://github.com/camel-ai/camel.
### 🌟 论文解读 | CAMEL：探索大型语言模型社会的“心智”交流

### 📌 背景痛点/本文动机
随着基于聊天的语言模型在复杂任务解决方面的快速发展，它们在解决复杂任务方面取得了显著进展。然而，这些模型的成功严重依赖于人类输入来引导对话，这可能会具有挑战性且耗时。本文探讨了构建可扩展技术以促进交流代理之间的自主合作，并深入了解其“认知”过程的潜力。

### 🚀 核心方法
💡 创新点1：角色扮演框架
为了解决实现自主合作的挑战，本文提出了一种名为“角色扮演”的新型交流代理框架。该框架涉及使用“起始提示”来引导聊天代理完成任务，同时保持与人类意图的一致性。

💡 创新点2：起始提示
本文提出了一种名为“起始提示”的对话LLM自动提示方法，使代理能够通过角色扮演相互提示以解决问题。AI用户不断向AI助手提供指令以解决问题，这使我们能够保存指令-解决方案对并创建多样化、指令性、对话性和面向任务的语料库。

💡 创新点3：数据集生成
本文展示了如何使用角色扮演来让聊天代理相互交流以完成任务，并记录他们的对话以进行行为分析和能力理解。特别是，我们对多代理设置中的指令遵循合作进行了全面研究。

💡 创新点4：开源库
本文开源了我们的库，其中包含各种代理的实现、数据生成管道、数据分析工具和收集的数据集，以支持对交流代理的研究。

### 📈 实验结果
本文通过实验评估了CAMEL框架的性能，结果表明CAMEL解决方案在人类评估和GPT4评估中均优于gpt-3.5-turbo单次解决方案。此外，本文还研究了LLM训练能力的显著出现，通过在通过框架生成的不断增长的语料库上微调LLaMA模型。

### 💬 可借鉴之处
本文提出的CAMEL框架为研究交流代理之间的自主合作提供了可扩展的方法，并提供了应对挑战的策略。此外，本文开源的库为研究交流代理和更广泛的研究领域提供了宝贵的资源。

## league--guided-skill-learning-and-abstraction-for-long-horizon-manipulation
### Abstract
To assist with everyday human activities, robots must solve complex
long-horizon tasks and generalize to new settings. Recent deep reinforcement
learning (RL) methods show promise in fully autonomous learning, but they
struggle to reach long-term goals in large environments. On the other hand,
Task and Motion Planning (TAMP) approaches excel at solving and generalizing
across long-horizon tasks, thanks to their powerful state and action
abstractions. But they assume predefined skill sets, which limits their
real-world applications. In this work, we combine the benefits of these two
paradigms and propose an integrated task planning and skill learning framework
named LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the
symbolic interface of a task planner to guide RL-based skill learning and
creates abstract state space to enable skill reuse. More importantly, LEAGUE
learns manipulation skills in-situ of the task planning system, continuously
growing its capability and the set of tasks that it can solve. We evaluate
LEAGUE on four challenging simulated task domains and show that LEAGUE
outperforms baselines by large margins. We also show that the learned skills
can be reused to accelerate learning in new tasks domains and transfer to a
physical robot platform.
### 🌟 论文解读 | LEAGUE：基于引导的技能学习和抽象，助力机器人解决长期操作任务

### 📌 背景痛点/本文动机
随着人工智能技术的不断发展，机器人已经逐渐走进我们的日常生活，并在各种场景中发挥着重要作用。然而，要让机器人真正实现自主学习和操作，仍然面临着许多挑战。其中，长期操作任务（long-horizon tasks）的解决和泛化能力是机器人领域的一大难题。现有的深度强化学习（DRL）方法在自主学习方面表现出色，但在大型环境中实现长期目标仍然存在困难。另一方面，任务和运动规划（TAMP）方法擅长解决和泛化长期任务，但由于其依赖于预定义的技能集，限制了其在现实世界中的应用。

### 🚀 核心方法
为了克服上述挑战，本文提出了LEAGUE（Learning and Abstraction with Guidance）框架，该框架结合了DRL和TAMP的优势，实现了长期操作任务的解决和泛化。LEAGUE的核心创新点包括：

💡 创新点1：利用任务规划器的符号接口指导基于强化学习的技能学习，并创建抽象状态空间以实现技能复用。
💡 创新点2：在任务规划系统中学习操作技能，不断扩展其能力和可解决的任务集。

### 📈 实验结果
本文在四个具有挑战性的模拟任务领域对LEAGUE进行了评估，结果表明LEAGUE在性能上显著优于基线方法。此外，本文还展示了学习到的技能可以复用于加速新任务领域的学习，并迁移到物理机器人平台。

### 💬 可借鉴之处
LEAGUE框架为机器人解决长期操作任务提供了一种新的思路，其核心思想可以应用于其他领域，例如自动驾驶、游戏AI等。此外，LEAGUE框架中的技能学习和抽象方法也可以为其他强化学习算法提供借鉴。

## palm-e--an-embodied-multimodal-language-model
### Abstract
Large language models excel at a wide range of complex tasks. However,
enabling general inference in the real world, e.g., for robotics problems,
raises the challenge of grounding. We propose embodied language models to
directly incorporate real-world continuous sensor modalities into language
models and thereby establish the link between words and percepts. Input to our
embodied language model are multi-modal sentences that interleave visual,
continuous state estimation, and textual input encodings. We train these
encodings end-to-end, in conjunction with a pre-trained large language model,
for multiple embodied tasks including sequential robotic manipulation planning,
visual question answering, and captioning. Our evaluations show that PaLM-E, a
single large embodied multimodal model, can address a variety of embodied
reasoning tasks, from a variety of observation modalities, on multiple
embodiments, and further, exhibits positive transfer: the model benefits from
diverse joint training across internet-scale language, vision, and
visual-language domains. Our largest model, PaLM-E-562B with 562B parameters,
in addition to being trained on robotics tasks, is a visual-language generalist
with state-of-the-art performance on OK-VQA, and retains generalist language
capabilities with increasing scale.
### 🌟 论文解读 | PaLM-E：将真实世界传感器数据融入语言模型，实现更接地气的推理

### 📌 背景痛点/本文动机
大型语言模型（LLMs）在各种任务中表现出强大的推理能力，但在现实世界的推理中，例如机器人问题，存在一个关键挑战：接地（grounding）。虽然LLMs在大量文本数据上训练，可能产生与物理世界相关的表示，但将这些表示与现实世界的视觉和物理传感器模态连接起来对于解决更广泛的现实世界问题至关重要。

### 🚀 核心方法
本文提出了具身语言模型（embodied language models），将真实世界连续传感器模态的数据直接融入语言模型，从而建立词语与感知之间的联系。具身语言模型的输入是多模态句子，这些句子交替包含视觉、连续状态估计和文本输入编码。我们端到端地训练这些编码，与预训练的大型语言模型相结合，用于多个具身任务，包括顺序机器人操作规划、视觉问答和字幕生成。

### 📈 实验结果
实验结果表明，PaLM-E，一个单一的、大型具身多模态模型，可以解决各种具身推理任务，包括来自多种观察模态的多种具身形式。此外，该模型表现出积极的迁移：模型受益于跨互联网规模的语言、视觉和视觉语言领域的多样化联合训练。我们的最大模型PaLM-E-562B，除了在机器人任务上进行训练外，还是一个视觉语言通才，在OK-VQA上取得了最先进的性能，并且随着规模的增加，保留了通用的语言能力。

### 💬 可借鉴之处
PaLM-E的研究表明，通过将具身数据混合到多模态大型语言模型的训练中，可以训练出一个通用的、迁移学习的、多具身决策代理。此外，PaLM-E还展示了在视觉问答和字幕生成等通用视觉语言任务上的竞争力，并且随着语言模型规模的增加，多模态微调时的灾难性遗忘显著减少。

## guiding-pretraining-in-reinforcement-learning-with-large-language-models
### Abstract
Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks. Code available at
https://github.com/yuqingd/ellm.
### 🌟 论文解读 | 利用大型语言模型引导强化学习的预训练

### 📌 背景痛点/本文动机
强化学习算法在缺乏密集、良好形状的奖励函数时通常会遇到困难。内在动机探索方法通过奖励代理访问新颖状态或转换来解决这一限制，但在大多数发现的新颖性对下游任务无关紧要的大型环境中，这些方法提供的益处有限。本文提出了一种方法，该方法使用来自文本语料库的背景知识来塑造探索。这种方法称为ELLM（使用大型语言模型进行探索），它奖励代理实现由语言模型提出的与代理当前状态描述相关的目标。通过利用大规模语言模型预训练，ELLM引导代理朝着人类有意义且可能有用的行为发展，而无需人工干预。

### 🚀 核心方法
💡 创新点1：利用大型语言模型（LLM）的背景知识来塑造探索。LLM是概率文本模型，其预测编码了丰富的关于人类常识知识和文化习俗的信息。ELLM通过查询LLM来获取可能的目标，并奖励代理实现这些建议，从而引导探索朝着完成多样化、上下文敏感和人类有意义的目标。

💡 创新点2：使用LLM生成的目标作为内在奖励函数。ELLM通过测量LLM生成的目标与环境中代理转换的描述之间的语义相似性来计算奖励。当转换的描述与目标描述足够接近时，代理将获得与相似度成比例的奖励。

### 📈 实验结果
本文在Crafter游戏环境和Housekeep机器人模拟器中评估了ELLM。结果表明，ELLM训练的代理在预训练期间对常识行为的覆盖范围更好，并且在下游任务上的性能通常与基线相当或有所提高。

### 💬 可借鉴之处
本文提出的方法可以用于引导强化学习代理在缺乏外部定义的奖励的情况下学习有用的行为。通过利用LLM的背景知识，ELLM可以引导代理朝着人类有意义且可能有用的行为发展，从而提高强化学习算法的性能。此外，本文还探讨了LLM性能对提示选择、状态和转换描述的敏感性，并提出了改进LLM性能的潜在方法。

## mariogpt--open-ended-text2level-generation-through-large-language-models
### Abstract
Procedural Content Generation (PCG) is a technique to generate complex and
diverse environments in an automated way. However, while generating content
with PCG methods is often straightforward, generating meaningful content that
reflects specific intentions and constraints remains challenging. Furthermore,
many PCG algorithms lack the ability to generate content in an open-ended
manner. Recently, Large Language Models (LLMs) have shown to be incredibly
effective in many diverse domains. These trained LLMs can be fine-tuned,
re-using information and accelerating training for new tasks. Here, we
introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game
levels, in our case Super Mario Bros levels. MarioGPT can not only generate
diverse levels, but can be text-prompted for controllable level generation,
addressing one of the key challenges of current PCG techniques. As far as we
know, MarioGPT is the first text-to-level model and combined with novelty
search it enables the generation of diverse levels with varying play-style
dynamics (i.e. player paths) and the open-ended discovery of an increasingly
diverse range of content. Code available at
https://github.com/shyamsn97/mario-gpt.
### 🌟 论文解读 | MarioGPT：基于大型语言模型的开放性文本到关卡生成

### 📌 背景痛点/本文动机
随着游戏行业的发展，玩家对游戏内容的多样性和复杂性要求越来越高。传统的关卡设计往往需要大量人力和时间，且难以满足个性化需求。因此，如何高效、自动地生成具有多样性和可玩性的游戏关卡成为了一个重要的研究方向。

### 🚀 核心方法
💡 创新点1：MarioGPT模型
本文提出了一种名为MarioGPT的模型，该模型基于GPT-2语言模型进行微调，用于生成马里奥游戏关卡。MarioGPT能够根据自然语言提示生成多样化的关卡，并通过文本提示实现可控的关卡生成。

💡 创新点2：结合新颖性搜索
为了实现开放性的关卡生成，本文将MarioGPT与新颖性搜索算法相结合。新颖性搜索算法能够引导模型不断生成具有新颖性的关卡，从而实现无限的内容探索。

### 📈 实验结果
实验结果表明，MarioGPT在生成马里奥游戏关卡方面表现出色。与基线模型相比，MarioGPT在非空气瓷砖预测准确率方面取得了显著提升。此外，MarioGPT生成的关卡中，有88.4%是可玩的，并且生成的路径与实际玩家的路径相似度较高。

### 💬 可借鉴之处
本文提出的MarioGPT模型为游戏关卡生成提供了一种新的思路。通过结合大型语言模型和新颖性搜索算法，可以实现高效、自动地生成具有多样性和可玩性的游戏关卡。此外，本文提出的文本提示方法也为关卡生成提供了更多的控制性。

### 🌟 未来展望
未来，可以进一步探索以下方向：
1. 扩展MarioGPT模型，使其能够生成更复杂、更详细的关卡。
2. 将人类反馈引入关卡生成过程，通过强化学习等方法使生成的关卡更符合玩家的需求。
3. 探索更有效的搜索方法，进一步提高关卡生成的多样性和新颖性。

## describe--explain--plan-and-select--interactive-planning-with-large-language-models-enables-open-world-multi-task-agents
### Abstract
We investigate the challenge of task planning for multi-task embodied agents
in open-world environments. Two main difficulties are identified: 1) executing
plans in an open-world environment (e.g., Minecraft) necessitates accurate and
multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla
planners do not consider how easy the current agent can achieve a given
sub-task when ordering parallel sub-goals within a complicated plan, the
resulting plan could be inefficient or even infeasible. To this end, we propose
"$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and
$\underline{S}$elect" ($\textbf{DEPS}$), an interactive planning approach based
on Large Language Models (LLMs). DEPS facilitates better error correction on
initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of
the plan execution process and providing self-$\textit{explanation}$ of
feedback when encountering failures during the extended planning phases.
Furthermore, it includes a goal $\textit{selector}$, which is a trainable
module that ranks parallel candidate sub-goals based on the estimated steps of
completion, consequently refining the initial plan. Our experiments mark the
milestone of the first zero-shot multi-task agent that can robustly accomplish
70+ Minecraft tasks and nearly double the overall performances. Further testing
reveals our method's general effectiveness in popularly adopted non-open-ended
domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and
exploratory studies detail how our design beats the counterparts and provide a
promising update on the $\texttt{ObtainDiamond}$ grand challenge with our
approach. The code is released at https://github.com/CraftJarvis/MC-Planner.
### 🌟 论文解读 | 基于大型语言模型的交互式规划，助力开放世界多任务智能体

### 📌 背景痛点/本文动机
在开放世界环境中，多任务智能体面临着两大挑战：1）执行计划需要精确的多步推理，因为任务具有长期性；2）传统的规划器在排序复杂的计划中的并行子目标时，没有考虑当前智能体完成给定子任务的难易程度，导致生成的计划可能效率低下甚至不可行。

### 🚀 核心方法
本文提出了“描述、解释、规划和选择”（DEPS）的交互式规划方法，基于大型语言模型（LLMs）来解决上述挑战。

💡 创新点1：描述、解释和规划
DEPS 通过集成计划执行过程的描述和提供自我解释的反馈，更好地纠正初始 LLM 生成的计划中的错误。当遇到失败时，描述器会总结当前情况并发送给 LLM，LLM 作为解释器定位错误，然后根据描述器和解释器的信息更新计划。

💡 创新点2：目标选择器
DEPS 包含一个可训练的目标选择器模块，该模块根据完成每个并行候选子目标的估计步骤对它们进行排序，从而细化初始计划。选择器使用预测剩余时间步数来完成每个目标，并根据当前状态选择最接近的目标。

### 📈 实验结果
实验结果表明，DEPS 在开放世界环境（如 Minecraft）中取得了显著的成果，能够稳健地完成 70 多个任务，并且整体性能几乎翻倍。此外，DEPS 在非开放世界环境（如 ALFWorld 和桌面操作）中也表现出良好的效果。

### 💬 可借鉴之处
DEPS 的交互式规划方法为开放世界多任务智能体的开发提供了新的思路。通过集成描述、解释和规划，以及使用目标选择器，DEPS 能够生成更可靠和高效的计划，从而提高智能体在开放世界环境中的任务完成能力。

## do-embodied-agents-dream-of-pixelated-sheep--embodied-decision-making-using-language-guided-world-modelling
### Abstract
Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world. However, if initialized with knowledge of high-level
subgoals and transitions between subgoals, RL agents could utilize this
Abstract World Model (AWM) for planning and exploration. We propose using
few-shot large language models (LLMs) to hypothesize an AWM, that will be
verified through world experience, to improve sample efficiency of RL agents.
Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft
in two phases: (1) the Dream phase where the agent uses an LLM to decompose a
task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase
where the agent learns a modular policy for each subgoal and verifies or
corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and
then verifying the AWM based on agent experience not only increases sample
efficiency over contemporary methods by an order of magnitude but is also
robust to and corrects errors in the LLM, successfully blending noisy
internet-scale information from LLMs with knowledge grounded in environment
dynamics.
### 🌟 论文解读 | 基于语言引导的世界建模的具身决策：让强化学习更高效

### 📌 背景痛点/本文动机
强化学习（RL）通常从零开始学习，没有关于世界的先验知识。然而，如果 RL 代理在初始化时具有关于高级子目标和子目标之间转换的知识，它们可以利用这种抽象世界模型（AWM）进行规划和探索。本文提出使用少量样本的大型语言模型（LLMs）来假设 AWM，并通过世界经验进行验证，以提高 RL 代理的样本效率。

### 🚀 核心方法
💡 创新点1：使用少量样本的 LLM 来假设 AWM
本文提出使用少量样本的 LLM 来假设 AWM，并通过世界经验进行验证，以提高 RL 代理的样本效率。这种方法可以有效地利用 LLM 中的大规模、嘈杂的先验知识，并将其与基于环境动态的知识相结合。

💡 创新点2：将 LLM 引导的探索应用于 Minecraft 中的物品制作
本文提出的 DECKARD 代理将 LLM 引导的探索应用于 Minecraft 中的物品制作。DECKARD 代理分为两个阶段：梦想阶段和清醒阶段。在梦想阶段，代理使用 LLM 将任务分解为一系列子目标，形成假设的 AWM。在清醒阶段，代理学习每个子目标的模块化策略，并验证或纠正假设的 AWM。

### 📈 实验结果
实验结果表明，与没有 LLM 引导的代理相比，DECKARD 代理在开放探索和目标驱动任务中都表现出更高的样本效率。在开放探索中，DECKARD 代理能够更快地发现新的 AWM 节点。在目标驱动任务中，DECKARD 代理能够更快地学习制作物品，并且对 LLM 输出中的错误具有鲁棒性。

### 💬 可借鉴之处
本文提出的基于语言引导的世界建模的具身决策方法为 RL 代理利用大规模、嘈杂的先验知识提供了新的思路。这种方法可以应用于各种 RL 任务，例如机器人控制、游戏玩法和自动驾驶等。此外，本文提出的模块化 RL 策略也可以用于提高 RL 代理的泛化能力。

## open-world-multi-task-control-through-goal-aware-representation-learning-and-adaptive-horizon-prediction
### Abstract
We study the problem of learning goal-conditioned policies in Minecraft, a
popular, widely accessible yet challenging open-ended environment for
developing human-level multi-task agents. We first identify two main challenges
of learning such policies: 1) the indistinguishability of tasks from the state
distribution, due to the vast scene diversity, and 2) the non-stationary nature
of environment dynamics caused by partial observability. To tackle the first
challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage
the emergence of goal-relevant visual state representations. To tackle the
second challenge, the policy is further fueled by an adaptive horizon
prediction module that helps alleviate the learning uncertainty brought by the
non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method
significantly outperforms the best baseline so far; in many of them, we double
the performance. Our ablation and exploratory studies then explain how our
approach beat the counterparts and also unveil the surprising bonus of
zero-shot generalization to new scenes (biomes). We hope our agent could help
shed some light on learning goal-conditioned, multi-task agents in challenging,
open-ended environments like Minecraft.
### 🌟 论文解读 | 在开放世界中实现多任务控制：目标感知表示学习和自适应预测

### 📌 背景痛点/本文动机
开放世界环境，如Minecraft，为开发能够执行各种任务的智能体提供了丰富的平台。然而，这些环境也带来了独特的挑战，包括：
1. **状态分布的多样性**：由于场景的多样性，不同任务的状态难以区分，这使得学习目标条件策略变得困难。
2. **环境动态的非平稳性**：由于部分可观察性，环境动态具有非平稳性，导致学习的不确定性增加。

### 🚀 核心方法
为了解决这些挑战，本文提出了以下创新方法：
💡 创新点1：**目标感知骨干网络（GSB**）
   - GSB通过在多个层次上融合目标信息，鼓励出现与目标相关的视觉状态表示，从而解决状态分布多样性的问题。
   - GSB由多个目标卷积块（g-conv block）组成，这些块通过通道调制将目标信息与视觉特征融合。

💡 创新点2：**自适应预测模块**
   - 为了应对环境动态的非平稳性，本文引入了自适应预测模块，该模块预测从当前状态到目标的剩余时间步数（即距离到目标的距离）。
   - 自适应预测模块通过预测剩余时间步数，帮助智能体更好地理解目标的完成程度，从而提高决策的准确性。

### 📈 实验结果
在Minecraft的20个任务上进行的实验表明，本文提出的方法显著优于现有基线，在许多任务中性能翻倍。消融研究和探索性研究解释了本文方法如何优于现有方法，并揭示了令人惊讶的零样本泛化到新场景（生物群落）的额外优势。

### 💬 可借鉴之处
本文提出的GSB和自适应预测模块为在开放世界环境中学习目标条件策略提供了新的思路，并为开发能够在复杂环境中执行多任务的智能体提供了有价值的参考。

## welfare-diplomacy--benchmarking-language-model-cooperation
### Abstract
The growing capabilities and increasingly widespread deployment of AI systems
necessitate robust benchmarks for measuring their cooperative capabilities.
Unfortunately, most multi-agent benchmarks are either zero-sum or purely
cooperative, providing limited opportunities for such measurements. We
introduce a general-sum variant of the zero-sum board game Diplomacy -- called
Welfare Diplomacy -- in which players must balance investing in military
conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both
a clearer assessment of and stronger training incentives for cooperative
capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules
and implementing them via an open-source Diplomacy engine; (2) constructing
baseline agents using zero-shot prompted language models; and (3) conducting
experiments where we find that baselines using state-of-the-art models attain
high social welfare but are exploitable. Our work aims to promote societal
safety by aiding researchers in developing and assessing multi-agent AI
systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is
available at https://github.com/mukobi/welfare-diplomacy.
### 🌟 论文解读 | 探索AI合作能力：Welfare Diplomacy基准测试

### 📌 背景痛点/本文动机
随着AI系统能力的不断增强和应用的日益广泛，衡量其合作能力的需求也日益增长。然而，现有的多智能体基准测试要么是零和博弈，要么是纯粹的合作博弈，这限制了对其合作能力的评估。本文提出了一个名为Welfare Diplomacy的基准测试，旨在解决这一问题。

### 🚀 核心方法
💡 创新点1：Welfare Diplomacy规则
Welfare Diplomacy是对经典外交游戏（Diplomacy）的改进版本，玩家需要在军事征服和国内福利之间进行权衡。游戏结束后，玩家的总效用等于其累积的福利点数，而不是占领的供应中心数量。这种规则设计鼓励玩家进行合作，以实现更高的社会福祉。

💡 创新点2：零样本语言模型基准测试
本文使用零样本提示语言模型构建了Welfare Diplomacy的基线智能体，并使用GPT-4等最先进的模型进行了基准测试。实验结果表明，这些模型能够实现高社会福祉，但容易被其他玩家利用。

### 📈 实验结果
实验结果表明，Welfare Diplomacy能够有效地评估和训练AI系统的合作能力。与标准外交游戏相比，Welfare Diplomacy中的玩家参与冲突的频率更低，这表明Welfare Diplomacy能够更好地促进合作行为。

### 💬 可借鉴之处
本文提出的Welfare Diplomacy基准测试为评估和训练AI系统的合作能力提供了一个新的平台。该基准测试可以用于开发更安全、更可靠的AI系统，以应对现实世界中的复杂挑战。

### 🌟 总结
Welfare Diplomacy是一个很有前景的基准测试，可以帮助研究人员更好地理解和评估AI系统的合作能力。随着AI技术的不断发展，Welfare Diplomacy有望在促进AI合作能力方面发挥重要作用。

## robotic-skill-acquisition-via-instruction-augmentation-with-vision-language-models
### Abstract
In recent years, much progress has been made in learning robotic manipulation
policies that follow natural language instructions. Such methods typically
learn from corpora of robot-language data that was either collected with
specific tasks in mind or expensively re-labelled by humans with rich language
descriptions in hindsight. Recently, large-scale pretrained vision-language
models (VLMs) like CLIP or ViLD have been applied to robotics for learning
representations and scene descriptors. Can these pretrained models serve as
automatic labelers for robot data, effectively importing Internet-scale
knowledge into existing datasets to make them useful even for tasks that are
not reflected in their ground truth annotations? To accomplish this, we
introduce Data-driven Instruction Augmentation for Language-conditioned control
(DIAL): we utilize semi-supervised language labels leveraging the semantic
understanding of CLIP to propagate knowledge onto large datasets of unlabelled
demonstration data and then train language-conditioned policies on the
augmented datasets. This method enables cheaper acquisition of useful language
descriptions compared to expensive human labels, allowing for more efficient
label coverage of large-scale datasets. We apply DIAL to a challenging
real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations
do not contain crowd-sourced language annotations. DIAL enables imitation
learning policies to acquire new capabilities and generalize to 60 novel
instructions unseen in the original dataset.
### 🌟 论文解读 | 利用视觉语言模型增强指令，实现机器人技能获取

### 📌 背景痛点/本文动机
随着深度学习的发展，机器人控制策略已经能够通过自然语言指令进行学习。然而，这些方法通常依赖于大量标注过的机器人语言数据，这些数据要么是针对特定任务收集的，要么是事后由人类进行昂贵标注的。为了解决这个问题，本文提出了一种名为DIAL（Data-driven Instruction Augmentation for Language-conditioned control）的方法，利用预训练的视觉语言模型（VLM）自动为机器人数据添加标签，从而将互联网规模的知识引入现有数据集，使其即使在真实标签有限的情况下也能用于各种任务。

### 🚀 核心方法
💡 创新点1：利用预训练的VLM进行指令增强
DIAL方法首先在包含少量人工标注的自然语言描述的小型数据集上微调VLM，然后使用微调后的VLM为更大的未标注演示数据集生成替代指令。这些指令可以包含更丰富的语义概念，例如空间概念或不同的表述方式。

💡 创新点2：基于行为克隆的语言条件策略训练
在生成替代指令后，DIAL方法使用行为克隆在原始和重新标注的数据集上训练语言条件策略。这种方法可以更有效地覆盖大规模数据集，并使机器人能够理解和执行新的指令。

### 📈 实验结果
在真实世界的机器人操作领域，DIAL方法在80,000个演示中，只有3.5%包含众包语言注释的情况下，通过大规模研究超过1,300次真实世界评估，发现DIAL使模仿学习策略能够获得新的能力，并推广到60个原始数据集中未见的全新指令。

### 💬 可借鉴之处
DIAL方法提供了一种廉价且自动化的选项，可以从离线控制数据集中提取额外的语义知识。它通过利用预训练的VLM自动为机器人数据添加标签，从而将互联网规模的知识引入现有数据集，使其即使在真实标签有限的情况下也能用于各种任务。这种方法可以更有效地覆盖大规模数据集，并使机器人能够理解和执行新的指令。

## emergent-world-representations--exploring-a-sequence-model-trained-on-a-synthetic-task
### Abstract
Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create "latent saliency maps" that can help explain predictions
in human terms.
### 🌟 论文解读 | 探索序列模型在合成任务中的涌现世界表示

### 📌 背景痛点/本文动机
语言模型展现出惊人的能力，但其背后的原理尚不明确。这些模型是仅仅记忆了一系列表面统计信息，还是依赖于对生成序列过程的内部表示？本文通过将GPT模型应用于预测简单棋盘游戏Othello的合法移动任务，探讨了这个问题。

### 🚀 核心方法
💡 创新点1：使用Othello作为测试平台
本文选择Othello作为测试平台，因为它比国际象棋简单，但游戏树足够大，避免了记忆的可能性。通过训练一个GPT变体模型（Othello-GPT）来预测Othello的合法移动，尽管模型没有先验的游戏知识，但仍然能够以高精度生成合法移动。

💡 创新点2：使用探针技术探索内部表示
为了研究Othello-GPT是否计算了游戏状态的内部表示，本文使用了探针技术。探针是一种分类器或回归器，其输入由网络的内部激活组成，并训练以预测感兴趣的特征。通过训练探针来预测网络内部激活后的棋盘状态，发现非线性探针能够以高精度预测棋盘状态，这表明模型内部存在一个非线性的棋盘状态表示。

💡 创新点3：干预实验验证表示的因果作用
为了确定棋盘状态信息是否影响模型的预测，本文进行了一系列干预实验。通过修改Othello-GPT的内部激活，并测量由此产生的效果，发现干预后的预测与预期的棋盘状态相匹配，这表明涌现的世界表示对模型的预测具有因果作用。

💡 创新点4：创建潜在显著性图
本文还展示了如何使用干预技术创建潜在显著性图，这些图可以帮助解释模型的预测。通过干预每个棋盘格的状态，并观察预测概率的变化，可以生成一个表示每个棋盘格对当前棋盘状态预测的显著性的可视化图。

### 📈 实验结果
实验结果表明，Othello-GPT确实维护了一个游戏棋盘状态的表示，并且这个表示是非线性的。此外，这些表示与模型的预测具有因果联系。潜在显著性图揭示了Othello-GPT训练数据集的不同版本之间的显著差异。

### 💬 可借鉴之处
本文的研究结果表明，序列模型可以学习到复杂的内部表示，并且这些表示可以用于解释模型的预测。此外，干预实验和潜在显著性图等技术可以用于更好地理解模型的内部工作机制。这些发现对于自然语言处理等领域的研究具有重要意义，可以帮助我们更好地理解语言模型的内部表示，并开发更可靠的解释工具。

## large-language-models-are-pretty-good-zero-shot-video-game-bug-detectors
### Abstract
Video game testing requires game-specific knowledge as well as common sense
reasoning about the events in the game. While AI-driven agents can satisfy the
first requirement, it is not yet possible to meet the second requirement
automatically. Therefore, video game testing often still relies on manual
testing, and human testers are required to play the game thoroughly to detect
bugs. As a result, it is challenging to fully automate game testing. In this
study, we explore the possibility of leveraging the zero-shot capabilities of
large language models for video game bug detection. By formulating the bug
detection problem as a question-answering task, we show that large language
models can identify which event is buggy in a sequence of textual descriptions
of events from a game. To this end, we introduce the GameBugDescriptions
benchmark dataset, which consists of 167 buggy gameplay videos and a total of
334 question-answer pairs across 8 games. We extensively evaluate the
performance of six models across the OPT and InstructGPT large language model
families on our benchmark dataset. Our results show promising results for
employing language models to detect video game bugs. With the proper prompting
technique, we could achieve an accuracy of 70.66%, and on some video games, up
to 78.94%. Our code, evaluation data and the benchmark can be found on
https://asgaardlab.github.io/LLMxBugs
### 🌟 论文解读 | 大型语言模型在零样本视频游戏漏洞检测中的潜力

### 📌 背景痛点/本文动机
视频游戏测试需要游戏特定的知识和对游戏事件的常识推理。虽然 AI 驱动的代理可以满足第一个要求，但自动满足第二个要求仍然不可能。因此，视频游戏测试通常仍然依赖于手动测试，需要人类测试者彻底地玩游戏来检测漏洞。这使得完全自动化游戏测试具有挑战性。

### 🚀 核心方法
💡 创新点1：将漏洞检测问题表述为问答任务，利用大型语言模型的零样本能力来识别游戏事件序列中的漏洞事件。
💡 创新点2：引入 GameBugDescriptions 基准数据集，包含 167 个有漏洞的游戏玩法视频和 334 个问答对，涵盖 8 个游戏。
💡 创新点3：在基准数据集上评估了 OPT 和 InstructGPT 大型语言模型家族的六个模型的性能。
💡 创新点4：分析了语言模型对不同事件描述的鲁棒性。

### 📈 实验结果
实验结果表明，大型语言模型在视频游戏漏洞检测方面具有很大的潜力。通过适当的提示技术，可以实现 70.66% 的准确率，在某些视频游戏中甚至可以达到 78.94%。

### 💬 可借鉴之处
这篇论文展示了大型语言模型在视频游戏漏洞检测方面的潜力，为自动化游戏测试提供了新的思路。此外，论文中提出的 GameBugDescriptions 基准数据集可以用于评估和比较不同语言模型在漏洞检测任务上的性能。

## social-simulacra--creating-populated-prototypes-for-social-computing-systems
### Abstract
Social computing prototypes probe the social behaviors that may arise in an
envisioned system design. This prototyping practice is currently limited to
recruiting small groups of people. Unfortunately, many challenges do not arise
until a system is populated at a larger scale. Can a designer understand how a
social system might behave when populated, and make adjustments to the design
before the system falls prey to such challenges? We introduce social simulacra,
a prototyping technique that generates a breadth of realistic social
interactions that may emerge when a social computing system is populated.
Social simulacra take as input the designer's description of a community's
design -- goal, rules, and member personas -- and produce as output an instance
of that design with simulated behavior, including posts, replies, and
anti-social behaviors. We demonstrate that social simulacra shift the behaviors
that they generate appropriately in response to design changes, and that they
enable exploration of "what if?" scenarios where community members or
moderators intervene. To power social simulacra, we contribute techniques for
prompting a large language model to generate thousands of distinct community
members and their social interactions with each other; these techniques are
enabled by the observation that large language models' training data already
includes a wide variety of positive and negative behavior on social media
platforms. In evaluations, we show that participants are often unable to
distinguish social simulacra from actual community behavior and that social
computing designers successfully refine their social computing designs when
using social simulacra.
### 🌟 论文解读 | 社交模拟：为社交计算系统创建人口原型

### 📌 背景痛点/本文动机
社交计算系统的设计往往需要考虑大量用户的行为和互动，而传统的原型设计方法通常只能招募少量用户进行测试。然而，许多挑战和问题只有在系统大规模运行时才会出现。因此，设计师很难预测社交系统在人口众多时的行为，也无法在系统出现问题之前进行调整。

### 🚀 核心方法
本文提出了“社交模拟”技术，利用大型语言模型生成大量用户和他们的社交互动，帮助设计师预测和评估社交系统在人口众多时的行为。社交模拟技术包括以下几个关键步骤：

💡 创新点1：生成多样化的用户角色
设计师只需提供少量种子角色，社交模拟技术就可以生成大量非重复但主题相关的用户角色，从而模拟不同用户之间的互动。

💡 创新点2：生成符合社区目标和规则的互动内容
社交模拟技术可以根据设计师提供的社区目标、规则和用户角色，生成符合社区目标和规则的互动内容，包括帖子、回复和反社会行为。

💡 创新点3：探索“如果”场景
社交模拟技术可以帮助设计师探索“如果”场景，例如，如果社区成员或版主进行干预，社交系统会发生什么变化。

### 📈 实验结果
本文通过两个评估实验验证了社交模拟技术的有效性：

1. 技术评估：参与者无法区分真实社区行为和社交模拟生成的行为，表明社交模拟可以创建可信的内容。
2. 设计师评估：社交计算设计师使用社交模拟技术成功改进了他们的社交系统设计，例如，识别未考虑到的积极用例和负面行为，并调整规则和干预策略。

### 💬 可借鉴之处
社交模拟技术为社交计算系统的设计提供了一种新的原型设计方法，可以帮助设计师更好地预测和评估社交系统在人口众多时的行为，从而设计出更安全、更有效的社交系统。此外，社交模拟技术还可以用于探索“如果”场景，帮助设计师更好地理解社交系统的动态变化。

## inner-monologue--embodied-reasoning-through-planning-with-language-models
### Abstract
Recent works have shown how the reasoning capabilities of Large Language
Models (LLMs) can be applied to domains beyond natural language processing,
such as planning and interaction for robots. These embodied problems require an
agent to understand many semantic aspects of the world: the repertoire of
skills available, how these skills influence the world, and how changes to the
world map back to the language. LLMs planning in embodied environments need to
consider not just what skills to do, but also how and when to do them - answers
that change over time in response to the agent's own choices. In this work, we
investigate to what extent LLMs used in such embodied contexts can reason over
sources of feedback provided through natural language, without any additional
training. We propose that by leveraging environment feedback, LLMs are able to
form an inner monologue that allows them to more richly process and plan in
robotic control scenarios. We investigate a variety of sources of feedback,
such as success detection, scene description, and human interaction. We find
that closed-loop language feedback significantly improves high-level
instruction completion on three domains, including simulated and real table top
rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen
environment in the real world.
### 🌟 论文解读 | 内心独白：通过语言模型规划实现具身推理

### 📌 背景痛点/本文动机
随着人工智能的发展，机器人需要具备更高级的推理能力，以便在复杂的环境中执行任务。传统的机器人规划方法往往依赖于优化或符号推理，缺乏对世界语义知识的理解。而大型语言模型（LLMs）在自然语言处理方面表现出色，并展现出丰富的世界知识。本文旨在探索LLMs在具身环境中的推理能力，并研究如何利用环境反馈来提高机器人的规划能力。

### 🚀 核心方法
本文提出了“内心独白”框架，通过将环境反馈与LLMs相结合，实现机器人对复杂任务的推理和规划。具体来说，本文的核心方法包括：

💡 创新点1：利用环境反馈形成“内心独白”
通过将环境反馈（如成功检测、场景描述、人类交互等）不断注入LLMs的规划语言提示中，机器人可以形成“内心独白”，从而更丰富地处理和规划机器人控制场景。

💡 创新点2：多种反馈来源
本文研究了多种反馈来源，包括成功检测、场景描述和人类交互。通过将这些反馈与LLMs相结合，机器人可以更好地理解环境，并根据反馈进行重新规划。

### 📈 实验结果
本文在模拟和真实世界的机器人平台上进行了实验，结果表明，利用环境反馈的“内心独白”框架可以显著提高机器人执行高级指令的能力。在模拟的桌面整理任务和真实的厨房移动操作任务中，与基线方法相比，本文提出的方法在任务完成率方面取得了显著的提升。

### 💬 可借鉴之处
本文提出的“内心独白”框架为机器人规划提供了一种新的思路，具有以下可借鉴之处：

*   利用LLMs的语义知识进行推理和规划
*   通过环境反馈实现机器人对复杂任务的适应性
*   结合多种反馈来源提高机器人的规划能力

### 🌟 总结
本文提出的“内心独白”框架为机器人规划提供了一种新的思路，通过利用LLMs的语义知识和环境反馈，机器人可以更好地理解环境并执行复杂任务。未来，随着LLMs和感知技术的发展，相信“内心独白”框架将在机器人领域发挥更大的作用。

## video-pretraining-(vpt)--learning-to-act-by-watching-unlabeled-online-videos
### Abstract
Pretraining on noisy, internet-scale datasets has been heavily studied as a
technique for training models with broad, general capabilities for text,
images, and other modalities. However, for many sequential decision domains
such as robotics, video games, and computer use, publicly available data does
not contain the labels required to train behavioral priors in the same way. We
extend the internet-scale pretraining paradigm to sequential decision domains
through semi-supervised imitation learning wherein agents learn to act by
watching online unlabeled videos. Specifically, we show that with a small
amount of labeled data we can train an inverse dynamics model accurate enough
to label a huge unlabeled source of online data -- here, online videos of
people playing Minecraft -- from which we can then train a general behavioral
prior. Despite using the native human interface (mouse and keyboard at 20Hz),
we show that this behavioral prior has nontrivial zero-shot capabilities and
that it can be fine-tuned, with both imitation learning and reinforcement
learning, to hard-exploration tasks that are impossible to learn from scratch
via reinforcement learning. For many tasks our models exhibit human-level
performance, and we are the first to report computer agents that can craft
diamond tools, which can take proficient humans upwards of 20 minutes (24,000
environment actions) of gameplay to accomplish.
### 🌟 论文解读 | 视频预训练（VPT）：通过观看无标签在线视频学习行动

### 📌 背景痛点/本文动机
近年来，在自然语言处理和计算机视觉等领域，通过在大型互联网数据集上进行预训练，已经证明了训练大型通用基础模型的有效性。然而，对于许多序列决策领域，如机器人、视频游戏和计算机使用，公开可用的数据并不包含训练行为先验所需的标签。本文旨在通过利用互联网上大量未标记的视频数据，将这些预训练范式扩展到序列决策领域。

### 🚀 核心方法
💡 创新点1：半监督模仿学习
本文提出了一种半监督模仿学习方法，通过观看在线未标记的视频，使智能体学会行动。具体来说，使用少量标记数据训练一个逆动力学模型，该模型足够准确，可以标记大量未标记的在线数据（例如，人们玩Minecraft的视频），然后从中训练一个通用的行为先验。

💡 创新点2：逆动力学模型
与行为克隆相比，逆动力学建模任务更简单，因为它是非因果的，这意味着它可以查看过去和未来的帧来推断动作。在大多数情况下，环境机制比环境中可能发生的人类行为的广度要简单得多，这表明非因果逆动力学模型可能需要比因果行为克隆模型少得多的数据来训练。

### 📈 实验结果
实验结果表明，尽管使用了原生人类界面（20Hz的鼠标和键盘），但这种方法的行为先验具有非平凡的零样本能力，并且可以通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。对于许多任务，模型表现出人类水平的性能，并且是第一个报告能够制作钻石工具的计算机代理，这需要熟练的人类玩家超过20分钟（24,000个环境动作）的游戏时间才能完成。

### 💬 可借鉴之处
本文提出的视频预训练（VPT）方法为利用互联网上大量未标记的数据进行序列决策领域的预训练提供了一种新的思路。该方法不仅能够有效地利用少量标记数据，而且能够通过模仿学习和强化学习进行微调，以解决强化学习无法从头学习的困难探索任务。此外，该方法还可以应用于任何具有大量未标记数据的领域，具有广泛的应用前景。

## minedojo--building-open-ended-embodied-agents-with-internet-scale-knowledge
### Abstract
Autonomous agents have made great strides in specialist domains like Atari
games and Go. However, they typically learn tabula rasa in isolated
environments with limited and manually conceived objectives, thus failing to
generalize across a wide spectrum of tasks and capabilities. Inspired by how
humans continually learn and adapt in the open world, we advocate a trinity of
ingredients for building generalist agents: 1) an environment that supports a
multitude of tasks and goals, 2) a large-scale database of multimodal
knowledge, and 3) a flexible and scalable agent architecture. We introduce
MineDojo, a new framework built on the popular Minecraft game that features a
simulation suite with thousands of diverse open-ended tasks and an
internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and
forum discussions. Using MineDojo's data, we propose a novel agent learning
algorithm that leverages large pre-trained video-language models as a learned
reward function. Our agent is able to solve a variety of open-ended tasks
specified in free-form language without any manually designed dense shaping
reward. We open-source the simulation suite, knowledge bases, algorithm
implementation, and pretrained models (https://minedojo.org) to promote
research towards the goal of generally capable embodied agents.
### 🌟 论文解读 | MineDojo：构建具有互联网规模知识的开放式具身智能体

### 📌 背景痛点/本文动机
传统的自主智能体在特定领域（如Atari游戏和围棋）取得了巨大进步，但它们通常在孤立的环境中学习，目标有限且手动设计，因此无法在广泛的任务和能力之间进行泛化。相比之下，人类在开放世界中不断学习和适应，能够利用大量来自自身经验和他人的先验知识。本文旨在构建能够像人类一样在开放世界中学习和适应的通用智能体。

### 🚀 核心方法
💡 创新点1：开放式环境
MineDojo基于流行的Minecraft游戏，提供了一个具有数千个多样化开放式任务的模拟套件。这些任务包括生存、采集、技术树和战斗等类别，涵盖了从简单到复杂的各种难度级别。

💡 创新点2：互联网规模的多模态知识库
MineDojo收集了大量的Minecraft相关数据，包括YouTube视频、Wiki页面和Reddit讨论。这些数据涵盖了游戏的所有方面，为智能体提供了丰富的先验知识。

💡 创新点3：灵活可扩展的智能体架构
MineDojo提出了一个基于Transformer预训练范式的智能体学习算法。该算法利用大规模预训练的视频语言模型作为学习奖励函数，能够解决各种开放式任务，而无需手动设计密集的奖励函数。

### 📈 实验结果
MineDojo的实验结果表明，其提出的MINECLIP奖励模型在程序性任务和创造性任务上都取得了良好的性能。与手动设计的密集奖励函数相比，MINECLIP在大多数任务上取得了竞争性的性能，并且在某些情况下甚至超过了它们。此外，MINECLIP还能够有效地评估创造性任务，其评估结果与人类判断高度一致。

### 💬 可借鉴之处
MineDojo为开发通用智能体提供了一个有价值的框架。其开放式环境、互联网规模的知识库和灵活可扩展的智能体架构为研究人员提供了探索开放式智能体学习的强大工具。此外，MineDojo的开源代码和数据集将促进社区对通用智能体研究的进一步发展。

