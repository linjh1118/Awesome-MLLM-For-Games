{"title":"JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse","authors":"Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, Yitao Liang","summary":"Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps:\/\/craftjarvis.github.io\/JarvisVLA.","url":"http:\/\/arxiv.org\/abs\/2503.16365v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.16365v1","published":1742491318000,"comment":"22 pages, 5 figures","pdf_text":"March 2025\nJARVIS-VLA: Post-Training Large-Scale Vision\nLanguage Models to Play Visual Games with\nKeyboards and Mouse\nMuyao Li1†, Zihao Wang1†, Kaichen He1, Xiaojian Ma2 and Yitao Liang1B\n1Peking University, 2BIGAI, All authors are affiliated with Team CraftJarvis\nRecently, action-based decision-making in open-world environments has gained significant attention.\nVisual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in\ndecision-making tasks. However, previous work has primarily focused on action post-training, often\nneglecting enhancements to the foundational model itself. In response, we introduce a novel approach,\nAct from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual\nand linguistic guidance in a self-supervised manner. This enhancement improves the models’ capabilities\nin world knowledge, visual recognition, and spatial grounding in open-world environments. Following\nthe above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human\ninstructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing.\nOur experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40%\nimprovement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate\nthat our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-\nof-the-art performance. We have open-sourced the code, models, and datasets to foster further research.\nThe project page can be found in https:\/\/craftjarvis.github.io\/JarvisVLA.\n1. Introduction\nPretraining foundation models on large-scale,\nnoisy internet datasets has become a mainstream\napproach in NLP and vision [1, 18, 38, 43]. The\nsuccess of models like GPT and LLAMA [35, 40]\nhas shown that large, capable language models\ncan infer and execute tasks described by lan-\nguage prompts.\nHowever, this paradigm has\nyet to achieve similar success in the decision-\nmaking domain [14, 50]. In particular, while Ope-\nnAI’s Video Pre-Training (VPT) model [4] has at-\ntempted to apply a similar approach in Minecraft,\nit still relies heavily on imitation learning (IL)\nafter collecting large-scale YouTube videos of hu-\nman play. VPT’s approach of pretraining with\nimitation learning, followed by downstream su-\npervised fine-tuning and reinforcement learning,\nmade significant strides—culminating in the suc-\ncessful crafting of diamond tools, a key challenge\nin Minecraft1.\n1Diamond tools are considered a grand challenge, with\nexperienced human players taking up to 20 minutes (24,000\nactions) to craft them.\nDespite this success, the reliance on next-action\nprediction in imitation learning limits the develop-\nment of robust, multi-task decision-making abil-\nities [6, 34, 39, 49]. Moreover, this pretraining\nparadigm struggles to generalize to unseen en-\nvironments or tasks due to the intricacies of the\ninteractions between observations and behavior,\nwhereas language tokens are more standardized.\nTo overcome these challenges, a new approach\nhas emerged that leverages pretrained Vision Lan-\nguage Models (VLMs) for decision-making. These\nmodels, known as Visual Language Action mod-\nels (VLAs), integrate language understanding\nwith action generation and can be further en-\nhanced through post-training on visual-language\ntasks [7, 25, 54]. A more detailed discussion can\nbe found in Figure 1 (left) and subsection 2.2.\nHowever, much like traditional imitation learn-\ning, current VLA approaches predominantly fo-\ncus on action post-training. In these models, the\nlearning objective is to generate correct actions\nbased on large-scale cross-task imitation data.\nWe propose that, in addition to action genera-\ntion, understanding the environment and incor-\nCorresponding author(s): Yitao Liang <yitaol@pku.edu.cn>\n† indicates co-first author.\nMuyao Li <2200017405@stu.pku.edu.cn>, Zihao Wang <zhwang@stu.pku.edu.cn>, Xiaojian Ma <xiaojian.ma@ucla.edu>\narXiv:2503.16365v1  [cs.CV]  20 Mar 2025\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 1 | We present JARVIS-VLA, a novel Vision-Language-Action (VLA) model trained with ActVLP paradigm,\npost-trained on vision language tasks (non-decision-making tasks) before training on trajectory datasets to\nhave better decision-making capabilities.\nporating task-related knowledge could be equally\nimportant for achieving more flexible and gener-\nalizable decision-making. To this end, we intro-\nduce a novel training paradigm—Visual Language\nPost-Training (ActVLP)—which integrates visual-\nlanguage tasks into the post-training phase of\nVLA models. Following the above paradigms, we\nobtain the first VLA models in Minecraft that can\nfollow human instructions on over 1k different\natomic tasks, including crafting, smelting, cook-\ning, mining, and killing.\nOur contributions are as follows: (1) We pio-\nneer the use of VLA in the open-world environ-\nment of Minecraft by introducing JARVIS-VLA, a\npowerful model achieving state-of-the-art perfor-\nmance in action-based decision-making. (2) We\nintroduce the concept of Visual Language Post-\nTraining and identify key visual-language guid-\nance strategies that enhance decision-making.\n(3) We investigate the scaling laws of VLA mod-\nels, demonstrating that expanding the scale of\nnon-trajectory vision-language tasks during post-\ntraining leads to significant improvements in\ndownstream task performance.\n(4) We open-\nsource the code, models, and datasets to support\nfurther research in this area.\n2. Learning to Act from Vision Lan-\nguage Post-Training\nIn this section, we present a detailed introduction\nto ActVLP, a new paradigm for training VLA mod-\nels. One of the most significant improvements\nis that we investigate a post-training stage prior\nto imitation learning. Specifically, we instantiate\nthis paradigm in our proposed model, JARVIS-\nVLA. We begin by discussing the architecture for\nJARVIS-VLA in subsection 2.1, followed by an\nexplanation of the training pipeline in subsec-\ntion 2.2 and the datasets used in subsection 2.3.\n2.1. Model Structure\nAs illustrated in Figure 1, JARVIS-VLA employs\nan architecture similar to Llava [27] but with\nslight modifications. The structural framework,\nconsists of several key components:: 1) Visual En-\ncoder: A Vision Transformer [18] that processes\nraw image pixels and converts them into a se-\nquence of fixed-size image patches. 2) Image\nProjection Module: A lightweight two-layer MLP\nthat projects image patch embeddings into the\nsame representational space as word embeddings.\n2\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nLarge Language Models\nViT\nWhat tool should be\nused to kill sheep?\nStage I: World Knowledge Post-Training\nLarge Language Models\nViT\nHow many sheep\ncan be seen?\nStage II: Visual Knowledge and Spatial\nGrounding Post-Training\nLarge Language Models\nViT\n<ins> Kill the bigger\nred sheep. <\/ins>\nStage III: Downstream Instruction\nFollowing Post-Training\nNext-Token Prediction\nSupervised Finetuning\nNext-Token Prediction\nSupervised Finetuning\nNext-Token Prediction\nImitation Learning\nLarge Language Models\nViT\n<ins> Kill the bigger\nred sheep. <\/ins>\nLarge-scale Multi-Task Pre-Training on\nDecision-Making Datasets\nNext-Token Prediction\nImitation Learning\nTraditional VLA Training\nOur Multi-Stage Post-Training VLA Pipeline\nFigure 2 | Previous VLA methods usually directly use imitation learning to finetune original vision-language\nmodels on large-scale multi-domain decision-making datasets to predict the actions [7, 25]. Our ActVLP\ntraining pipeline includes three stages: 1) post-training language models on text-only world knowledge with\nnext-token prediction supervised fine-tuning, 2) post-training both vision encoder and language models on\nmultimodal vision-language alignment and spatial grounding datasets with next-token prediction supervised\nfine-tuning, and 3) post-training only language models on multi-modal instruction following datasets with\nimitation learning.\n3) Language Model Transformers [3, 40]: A pow-\nerful autoregressive language model that serves\nas the core of the system, facilitating multimodal\nreasoning and decision-making.\nUnlike OpenVLA [25], our framework is de-\nsigned for partially observable environments. To\naccommodate this, we adopt a non-Markovian\narchitecture by incorporating a history of obser-\nvation images within the prompt. This approach\nensures that the model retains temporal context,\nwhich is crucial for tasks requiring multi-step\nreasoning and long-horizon decision-making. In\nour experiments, we employ Llava-Next [27] and\nQwen2-VL [43] as base vision language models,\nas both models provide robust support for multi-\nimage reasoning, enabling enhanced perception\nand contextual understanding.\nAnother key distinguishing feature of JARVIS-\nVLA compared to prior VLA models is the integra-\ntion of an action decoder. This module is respon-\nsible for generating both discrete and continuous\nactions. For discrete actions, we consolidate re-\nlated action dimensions into unified categories to\nreduce redundancy and improve efficiency. For\ncontinuous actions, we discretize the action space\ninto bins, which are then mapped to discrete to-\nkens. These tokens are subsequently appended to\nthe vocabulary of the original foundation model,\nallowing the model to generate both textual and\naction-based outputs in a unified manner.\nInstead of retraining the base VLM’s tokenizer,\nwe adopt a strategy inspired by RT-2 [7], repur-\nposing the least frequently used tokens from the\nlanguage tokenizer’s vocabulary to represent ac-\ntion semantics. Specifically, we replace the 51\nleast-used tokens, allocating 22 tokens for mouse\ncontrol (e.g., cursor movements) and 29 tokens\nfor special keyboard inputs (e.g., function keys\nand command shortcuts), which can be found\nin Appendix A. We introduce no other modifica-\ntions to the original VLM architecture to maintain\nmodel generalizability and ensure broad compat-\nibility across different foundation models. This\ndesign choice allows JARVIS-VLA to be easily inte-\ngrated with various pre-trained multimodal mod-\nels while preserving their inherent capabilities.\n2.2. Training Pipeline\nTraditional VLA methods typically employ pre-\ntrained VLMs and train them via imitation learn-\ning on large-scale trajectory data, which includes\ntextual instructions, visual observations, and\naction token sequences, as illustrated in Fig-\nure 2(left). These methods assume that VLMs,\npretrained on diverse internet-scale data, possess\nstrong generalization and fitting capabilities. Con-\nsequently, they are fine-tuned directly on down-\nstream decision-making tasks, leveraging multi-\n3\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 3 | Illustration of various post-training datasets. Models can post-train on various vision-language\ndatasets using a unified tokenizer and support diverse vision-language applications, such as question answering,\nimage captioning, image\/video question answering, visual grounding (including points and bounding box),\nand decision-making. More examples can be found in Appendix D.\nscenario data to enhance action understanding\nand generalization.\nHowever, learning world knowledge from\naction-labeled trajectory data is inherently chal-\nlenging [4]. Moreover, the lack of large-scale\naction-labeled datasets makes it challenging to\npretrain expansive models using only trajectory\ndata [34].\nTo address these challenges, ActVLP enhances\nthe VLM through a structured post-training pro-\ncess, utilizing data that follows the same format\nas pretraining but is more relevant to decision-\nmaking tasks. As shown in Figure 2(right), our\ntraining pipeline consists of three stages.\nStage I: Post-Training Language Models. We\nfirst refine the language transformer of the VLM\nusing large-scale textual datasets related to world\nknowledge in downstream environments, e.g.,\nMinecraft. During this stage, vision-related com-\nponents, including the ViT and vision adapter\nmodules, are frozen.\nThis step enhances the\nmodel’s understanding of decision-making con-\ntexts before incorporating multimodal alignment.\nStage II: Post-Training Vision Encoder and Lan-\nguage Models. Following language post-training,\nwe fully unfreeze the VLM and fine-tune it using\n4\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\ncaptioning, visual question-answering (VQA), and\nspatial grounding datasets, which are multimodal\nand have images in datasets. This stage ensures\nimproved vision-language alignment, enhancing\nthe model’s capacity to integrate world knowl-\nedge with visual perception. Both Stage 1 and\nStage 2 employ next-token prediction through\nsupervised fine-tuning, with the optimization ob-\njective being:\nLSFT = −\n∑︁\n𝑖=1\nlog P𝜃(𝑥𝑖| 𝑥𝑣, 𝑥ins, 𝑥1:𝑖−1)\n(1)\nwhere 𝑥𝑣denotes visual tokens, 𝑥ins represents\nthe instruction, and 𝑥corresponds to the answer.\nThis loss function maintains consistency with the\nstandard causal mask training approach.\nStage III: Imitation Learning on Trajectories.\nIn the final stage, we fine-tune the VLMs on tra-\njectory data, requiring the model to mimic ex-\npert actions given textual instructions 𝑥ins and\nthe current observation image 𝑜𝑡∈ℝ𝐻×𝑊×3. The\nimitation learning objective is defined as:\nLIL = −\n∑︁\n𝑡=1\nlog 𝜋𝜃(𝑎𝑡:𝑡+𝜏| 𝑜𝑡, 𝑥ins)\n(2)\nwhere 𝜋represents the learned policy, and 𝑎𝑡:𝑡+𝜏\ndenotes the future action chunk predicted from\nthe current step. During this phase, vision-related\nmodules remain frozen, while the language tok-\nenizer is modified to incorporate action tokens,\nand the language transformer undergoes full-\nparameter fine-tuning. Additionally, we adopt\naction chunking to further enhance training effi-\nciency and optimize trajectory learning [15].\nThis structured pipeline ensures that the VLM\nis progressively refined before being adapted to\ntrajectory-based imitation learning, resulting in\nimproved world knowledge acquisition, vision-\nlanguage alignment and grounding, and action\ngeneralization in decision-making tasks.\n2.3. Datasets\nTo support the ActVLP training pipeline, we\nconstructed a large-scale multimodal dataset.\nThis dataset includes both non-trajectory task\ndatasets for post-training and trajectory datasets\nfor downstream imitation learning.\nThe non-\ntrajectory datasets are divided into three cate-\ngories: knowledge-based question answering, vi-\nsual language alignment, and spatial grounding.\nThese categories are designed to enhance the\nmodel’s decision-making capabilities before tra-\njectory fine-tuning. For trajectory datasets, we\ncollected over 7.4 million frames of Minecraft\ngameplay data, including expert actions from di-\nverse sources such as human-playing [4], youtube\nvideos, and existing agents [45].\nThe dataset for world knowledge comprehension\ncomprises approximately 277K entries that signif-\nicantly bolster textual understanding, employed\nduring training stage I. The visual-language align-\nment dataset incorporates 35K keyframes en-\nhanced with advanced Vision-Language Models\nto produce both captions and question-answer\npairs, facilitating multimodal supervised fine-\ntuning in the subsequent training stage. The\nspatial grounding dataset focuses on detailed\nobject localization, generating over 404K data\npoints that are instrumental in refining spatial\nunderstanding for ActVLP models.\nBoth the\nvisual-language alignment datasets and the spa-\ntial grounding datasets primarily utilize Minecraft\nobservations, which strengthen the VLM’s under-\nstanding of the world and are used to support\ntraining stage II.\nImitation Learning Trajectory Dataset. VLA\ntraining is constructed on a dataset of human\ngameplay trajectories, particularly from the Ope-\nnAI contractor dataset in Minecraft [4], which\nincludes diverse tasks. We also incorporated an\nadditional 3M rollout frames from VPT [4] and\nJARVIS-1 [45] agents. For structured GUI-based\ntasks like crafting and smelting, we synthesized\n6.4M expert data entries to improve imitation\nlearning. Representative examples of our datasets\nare shown in Figure 3, with further details in Ap-\npendix D.\n3. Experiments\nOur experiments (starting from subsection 4.2)\naim to address the following questions:\nQ1: How do JARVIS-VLA compare to sota open-\n5\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nworld agents and imitation learning methods?\nQ2: Is visual language post-training the true\ncause of the performance improvement?\nQ3: Whether VLAs exhibit scaling laws and how\nActVLP influences them?\nQ4: Is ActVLP sensitive to different VLM back-\nbones? Due to space constraints, we quickly re-\nspond with an affirmative no, detailed experiment\ndiscussion deferred to Appendix F.\n3.1. Experimental Setup\nEvaluation Environment.\nWe use Minecraft\n1.16.5 as our experimental platform [23]. As an\nopen-world game with a substantial knowledge\nbase on platforms such as Reddit and wiki [21],\nMinecraft poses significant challenges to agents\nwhile simultaneously offering rich resources for\nresearch. To ensure fair comparisons, we align\nthe action and visual observation spaces with\nthose of human players [4]. Additionally, we hide\ninformation unavailable to human players as well,\nsuch as agent location and inventory stats.\nBenchmark and Evaluation Metrics. We con-\nduct evaluations using two broad benchmarks: (i)\nthe agent’s capacity to interact with the Minecraft\nenvironment to complete tasks; and (ii) vision-\nlanguage tasks (e.g., question answering, spa-\ntial grounding) designed to assess the VLM’s\nunderstanding of Minecraft-specific knowledge.\nFor the instruction-following tasks, we adopt\nthe MCU Benchmark [31], focusing on four\ncategories—Mine Blocks, Kill Entities,\nCraft Items, and Smelt Items—that repre-\nsent a wide range of typical game-play behaviors\nin Minecraft. Notably, Craft and Smelt require\n2D GUI manipulation through the mouse (cover-\ning thousands of item categories), whereas Mine\nand Kill involve recognizing, navigating, and in-\nteracting with targets in a 3D environment. Each\ncategory contains at least 5 distinct tasks. For\ninstance, the Mine Blocks category includes\nmining iron ore\nwith a stone pickaxe, oak logs\nwith bare hands, grass\n, dirt\n, and obsid-\nian\nwith a diamond pickaxe. Our evaluation\nset includes both simpler tasks (e.g., mining oak\nlogs) and more complex ones (e.g., mining obsid-\nian for over 10 seconds) that have proven chal-\nlenging for prior state-of-the-art agents [12, 30].\nWe perform each task at least 30 times and re-\nport the success rate per task, as well as the av-\nerage success rate within each category. To en-\nsure fairness, maximum execution steps for se-\nlected tasks match those reported by Lin et al.\n[31]. For vision-language assessments, the task\nformulations are illustrated in Figure 3. We pro-\nvide human-written ground-truth answers and\nemploy an LLM-as-judge to evaluate the perfor-\nmance of various VLMs (GPT-4o, Llava, Qwen-VL,\nand our post-trained VLMs). Detailed informa-\ntion on these vision-language benchmarks and\nresults can be found in Appendix E.\nTraining and VLA Configurations.. Our training\npipeline follows the process described in subsec-\ntion 2.2: we first obtain a visual-language post-\ntraining intermediate model, then further train\nit on trajectory tasks to produce the JARVIS-VLA.\nWe conduct experiments using two popular frame-\nworks: Qwen2-VL [43] and Llava [27]. We de-\nvelop a discretized action tokenizer specific to\nMinecraft, comprising 51 tokens that represent\ncamera movements and button actions. We utilize\nthe trl SFT Trainer [41] for finetuning and de-\nploy the VLA with vLLM [26]. Training is carried\nout on 32 A800-80G GPUs, while inference runs\non a single NVIDIA RTX 3090. Further training\ndetails are provided in Appendix B.\nBaselines.\nWe compare our model with: 1)\nVPT [4], including both the behavior cloning\n(VPT-BC) and reinforcement learning (VPT-RL)\nvariants; 2) STEVE-1[30], a text-conditioned pol-\nicy that combines VPT and MineCLIP[21] for in-\nstruction following; 3) GROOT [12], which uses\nvideo prompts as task instructions; and 4) Mine-\nDreamer [58], which leverages a vision-language\nmodel and a diffusion model to guide the STEVE-\n1 policy. Each method follows the default configu-\nration provided in the MCU benchmark for a fair\ncomparison.\n3.2. VLA Performance Evaluation\nWe present the performance results of our pro-\nposed model across four categories from the MCU\nbenchmark [31], as shown in Table 1. For each\n6\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nModel\nModel Size\nMine Blocks\nKill Entities\nCraft Items\nSmelt Items\nAvg.\nAvg.\nAvg.\nAvg.\nVPT-BC [4]\n248M\n0.15\n0.38\n0.33\n0.55\n0.35\n0.44\n0.30\n0.50\n0.45\n0.41\n0.10\n0.00\n0.05\nVPT-RL [4]\n248M\n0.05\n0.35\n0.25\n0.35\n0.25\n0.28\n0.50\n0.30\n0.62\n0.55\n0.05\n0.35\n0.20\nSTEVE-1 [30]\n248M\n0.20\n0.35\n0.54\n0.30\n0.75\n0.38\n0.45\n0.20\n0.70\n0.57\n0.25\n0.40\n0.33\nGROOT [12]\n248M\n0.56\n0.40\n0.67\n0.50\n0.50\n0.52\n0.45\n0.35\n0.25\n0.40\n0.35\n0.25\n0.30\nMineDreamer [58]\n7B\n0.25\n0.40\n0.55\n0.30\n0.70\n0.39\n0.50\n0.25\n0.30\n0.42\n0.30\n0.30\n0.30\nQwen2-VL (orig)\n7B\n0.77\n0.60\n0.79\n0.93\n0.80\n0.84\n0.83\n0.53\n0.40\n0.60\n0.03\n0.10\n0.07\nQwen2-VL (IL)\n7B\n0.70\n0.73\n0.75\n0.97\n0.83\n0.86\n0.73\n0.67\n0.50\n0.65\n0.17\n0.37\n0.29\nJARVIS-VLA-Qwen2\n7B\n0.80\n0.95\n0.88\n0.97\n0.93\n0.95\n0.87\n0.83\n0.63\n0.77\n0.77\n0.70\n0.70\nTable 1 | Evaluation results of different policies on Minecraft tasks. Each group includes multiple tasks (at least\n5), and the Avg. column reports the average success rate within each group. Qwen2-VL, Qwen2-VL (IL) and\nJARVIS-VLA-Qwen2-VL represent the training on the original qwen checkpoint, post-training on only large-scale\nimitation learning trajectories, and post-trained on VLP intermediate model. Qwen2-VL (ActVLP) achieves the\nhighest success rates across all task groups. We test JARVIS-VLA-Qwen2-VL on 1k different instructions and\nthe rollout videos can be found in the project page.\nMCU task, we collect over 100 random trajecto-\nries, which are used to fine-tune base VLMs to\ncreate our final VLA models.\nWe evaluate three variants of the VLMs as base\nmodels: 1) Qwen2-VL (raw): the original VLM\ncheckpoint fine-tuned for downstream tasks. 2)\nQwen2-VL (IL): post-trained on large-scale cross-\ntask trajectories (distinct from downstream tasks)\nusing imitation learning. 3) JARVIS-VLA-Qwen2:\npost-trained on our proposed off-trajectory vision-\nlanguage tasks. Performance is measured by the\naverage success rate across tasks within each cat-\negory. Our results show that JARVIS-VLA-Qwen2-\nVL, post-trained using our approach, consistently\noutperforms prior methods across almost all tasks.\nRemarkably, even without task-specific post-\ntraining, raw Qwen2-VL model, fine-tuned on\ndownstream tasks, outperforms several previ-\nous baselines, including STEVE-1 [30] and\nGROOT [12], which were trained using large-\nscale imitation learning. This highlights the ef-\nfectiveness of using a robust pre-trained VLM as\nthe base model for the policy, leading to strong\nperformance even without additional fine-tuning.\nNotably, we observe a significant performance\nboost with ActVLP post-training. For tasks such as\nCraft Items and Smelt Items, where pre-\nvious methods struggled, JARVIS-VLA-Qwen2-VL\nachieves success rates more than double those of\nthe baseline models. This underscores the effec-\ntiveness of our off-trajectory vision-language task\nstrategy. Furthermore, JARVIS-VLA-Qwen2-VL\noutperforms Qwen2-VL (IL) by over 15%, despite\nusing only 21% of the training trajectory data. In\ncrafting category tasks, the JARVIS-VLA model\nsurpasses traditional baselines by more than dou-\nble, outperforming models like VPT-BC [4] and\nSTEVE-1 [30] on tasks such as \"Craft crafting\ntable\" (\n). This significant improvement is pri-\nmarily due to the use of ViT in VLM and high-\nresolution processing, which are crucial for tasks\nlike crafting and smelting that demand precise\ncontrol in the GUI interface. This suggests that in-\ntegrating off-trajectory vision-language tasks into\nthe training pipeline enhances decision-making\ncapabilities, enabling more accurate action pre-\ndictions in VLA models. Further analysis and ad-\nditional experiments will be presented in the next\nsection.\n3.3. Ablation on Non-Trajectory Datasets\nIn this section, we focus on the post-training of\nQwen2-VL using various non-trajectory vision-\nlanguage tasks to investigate the specific contri-\nbutions to its enhanced performance.\nTo understand the impact of different task en-\nhancements, we conduct an ablation study by\ndividing the non-trajectory datasets and training\nQwen2-VL separately on three types of tasks: spa-\ntial grounding, visual language alignment, and\nknowledge-based question-answering, which are\nall related to Minecraft games. This results in\nthree variants of the VLM, each augmented with\none of these capabilities—spatial grounding, vi-\n7\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 4 | Ablation results on different post-training datasets. We select knowledge datasets, visual question-\nanswering datasets, and spatial grounding datasets to conduct ablation experiments. Our goal is to evaluate\nwhich capabilities and post-training datasets most significantly influence downstream decision-making tasks.\nFigure 5 | The relation between downstream task success rate, training loss, and training steps. The\ncurve shows that scaling downstream finetuning trajectories can scale up the success rate when the loss is\nlower than 0.22.\nsual recognition, and world knowledge. All mod-\nels are finetuned using the same gameplay dataset\nand imitation learning techniques. We also de-\nvelop a benchmark, detailed in Appendix E, to\nevaluate these capabilities. For this evaluation,\nwe select three long-sequence atomic tasks: \"Craft\nthe diamond sword\" (\n), \"Mine the obsidian\"\n(\n), and \"Cook the beef\" (\n), as downstream\ninstruction-following tasks.\nThe results of our ablation studies, presented in\nFigure 4, demonstrate that post-training with non-\ntrajectory vision-language tasks significantly en-\nhances the core capabilities of the VLM across the\nrespective benchmarks. Notably, after fine-tuning,\nmodels enhanced with spatial grounding exhibit\nthe most substantial improvement in downstream\ndecision-making tasks. These findings underscore\nthe effectiveness of non-trajectory post-training\nin boosting the performance of Vision-Language-\nAction models in decision-making tasks, even\nwhen the focus is on a single task. We find that\nnon-trajectory vision-language tasks, which are\nessential for agent pipelines [44, 45], are more\neffective for fine-tuning end-to-end VLA models.\nThis demonstrates the connection between devel-\noping LLM-based agent pipelines with separate\nmodules and fine-tuning end-to-end VLA models.\n3.4. Scaling Experiments\nRecent work on large language models (LLMs)\ntrained on vast amounts of text via next-token\nprediction has shown strong scaling laws [19,\n32, 47, 48]. We investigate whether VLAs, ob-\ntained through post-training on VLMs, exhibit\nsimilar scaling behavior. Specifically, we explore\ntwo questions: Q1) Can scaling up downstream\nimitation learning trajectories further improve\nthe VLA’s task success rate? Q2) Does increas-\ning the amount of non-trajectory vision-language\ntasks used during post-training enhance task com-\npletion success?\nThe results for Q1 are shown in Figure 5. Using\nthe same base model, we observe that increasing\nthe number of downstream trajectories improves\nthe VLA model’s task success rate. However, since\nthe success rate is a discrete metric, we find that\ntasks only show a non-zero success rate when the\nevaluation loss is below 0.30. This indicates that\nthe dataset size for downstream fine-tuning must\n8\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 6 | The relationship between post-training\nloss and downstream task success rates.\nOur\nfindings indicate that increasing the size of post-\ntraining non-trajectory datasets can significantly en-\nhance downstream task success rates, even with a\nfixed number of fine-tuning trajectories.\nbe sufficiently large enough. Furthermore, we ob-\nserve that different tasks require varying amounts\nof downstream data to reduce the evaluation loss\nbelow 0.30, which correlates with the length and\ndifficulty of the tasks.\nThe results for Q2 are illuminated in Figure 6.\nWe also explore the relationship between the eval-\nuation loss during post-training on non-trajectory\nvision-language tasks and task success rate in\ndownstream tasks. We use base models from dif-\nferent stages of post-training (with different eval\nloss on post-training datasets), fine-tuning them\nwith the same downstream trajectory dataset.\nThe baseline represents post-training using im-\nitation learning on cross-task trajectories. We\nfind that, for nearly all tasks, the success rate\nin downstream tasks correlates linearly with\nevaluation loss in post-training, with the lowest\nloss yielding the best results. Notably, models\npost-trained with knowledge-based tasks exhibit\nthe best downstream performance for a given\nevaluation loss. Models enhanced with spatial\ngrounding show the lowest evaluation loss and\nthe highest task success rates. These findings\ndemonstrate that scaling up off-trajectory vision-\nlanguage datasets directly enhances downstream\ntask performance, which has been overlooked in\nprevious VLA works [4, 5, 25].\n4. Related Works\n4.1. Visual-Language-Action Models\nImitation learning (IL) involves learning by mim-\nicking expert interactions with the environment,\nwith the primary challenge being the collection\nof high-quality expert demonstration datasets.\nNumerous studies have sought to enhance tradi-\ntional IL approaches [6, 15, 39]. A promising di-\nrection is the use of Visual-Language-Action (VLA)\nmodels [7, 25, 51, 55, 57], which adopt end-\nto-end imitation learning by fine-tuning VLMs.\nOpenVLA [25] has demonstrated the importance\nof selecting a capable VLM backbone, a conclusion\nfurther reinforced by RoboVLM [28]. Similarly,\nBrohan et al. [7] highlighted that co-training with\nweb-scale vision-language data significantly im-\nproves the generalization of VLA models. While\nprevious works primarily focused on optimiz-\ning the selection of VLMs, several recent studies\nhave begun to pay attention to the comprehen-\nsion capabilities of VLA models [13, 52, 59, 60].\nHowever, few have explicitly focused on en-\nhancing the VLM backbone itself through visual-\nlanguage post-training. Our work addresses this\ngap by proposing targeted visual-language post-\ntraining methods to enrich the capabilities of\nVLMs, thereby improving their performance on\ndownstream VLA tasks.\n4.2. VLM-based Agents in Minecraft\nExisting Minecraft agents based on VLMs typi-\ncally adopt hierarchical architectures [9, 17, 21,\n44, 53, 58]. These methods leverage a VLM’s\nworld knowledge for planning via zero-shot or\nfew-shot in-context learning, without modify-\ning the VLM parameters during agent optimiza-\ntion [29, 42, 44, 45]. STEVE-EYE [56] fine-tuned\nLlama language models [40] using internet text\ndata, achieving improved planning over zero-\nshot prompting. MineDreamer [58] employs the\ninstruction-following capability of VLMs to pre-\ndict future visual observations and generate ac-\ntions based on STEVE-1 [30]. OmniJARVIS [46]\nuses a behavior tokenizer [11, 12] to model hu-\nman trajectories in Minecraft with pretrained\nVLMs. While these approaches optimize VLMs,\nthey still rely on additional policies for action\ngrounding. In contrast, we propose a VLA-based\nagent model that generates actions directly from\ntextual instructions and visual inputs, eliminating\nthe need for extra grounding policies.\n9\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\n5. Conclusions\nWe present ActVLP, a novel training frame-\nwork for visual-language-action models that lever-\nages vision-language post-training to enhance\ndecision-making capabilities in dynamic environ-\nments. Our experiments demonstrate that post-\ntraining on non-trajectory tasks significantly en-\nhances foundation models’ ability to understand\ncomplex environments, resulting in substantial\nimprovements in downstream imitation learning\non trajectory data. The effectiveness of this model\nis validated across multiple VLM architectures,\nproviding strong evidence of its broad applica-\nbility and potential for visual-language-action\nmodel training, as exemplified by our state-of-\nthe-art model, JARVIS-VLA.\nLimitations\nLooking ahead, there are several avenues for im-\nprovement in future work. First, it is crucial to\nenhance the inference throughput of JARVIS-VLA,\nwhich is currently constrained by the large pa-\nrameter size of the VLA based on VLM [8]. We\nbelieve that future integration with MoE [22, 24]\ncould further improve the model’s inference effi-\nciency, with the goal of achieving gameplay per-\nformance levels exceeding 40Hz. Additionally,\nthere remains potential for further performance\ngains. While JARVIS-VLA outperforms previous\nMinecraft policies, it still falls short of the perfor-\nmance demonstrated by top human players, who\nachieve success rates above 90%.\nAcknowledgement\nThis work is funded in part by the Na-\ntional Science and Technology Major Project\n2022ZD0114902. We thank a grant from CCF-\nBaidu Open Fund.\nReferences\n[1] J. Achiam, S. Adler, S. Agarwal, L. Ah-\nmad, I. Akkaya, F. L. Aleman, D. Almeida,\nJ. Altenschmidt, S. Altman, S. Anadkat,\net al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[2] Anthropic.\nIntroducing\nthe\nnext\ngeneration\nof\nclaude,\n2024.\nURL\nhttps:\/\/www.anthropic.com\/\nnews\/claude-3-family.\n[3] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang,\nX. Deng, Y. Fan, W. Ge, Y. Han, F. Huang,\net al. Qwen technical report. arXiv preprint\narXiv:2309.16609, 2023.\n[4] B. Baker, I. Akkaya, P. Zhokov, J. Huizinga,\nJ. Tang, A. Ecoffet, B. Houghton, R. Sampe-\ndro, and J. Clune. Video pretraining (vpt):\nLearning to act by watching unlabeled on-\nline videos. Advances in Neural Information\nProcessing Systems, 35:24639–24654, 2022.\n[5] S. Belkhale, T. Ding, T. Xiao, P. Ser-\nmanet, Q. Vuong, J. Tompson, Y. Chebotar,\nD. Dwibedi, and D. Sadigh. Rt-h: Action\nhierarchies using language. arXiv preprint\narXiv:2403.01823, 2024.\n[6] A. Brohan, N. Brown, J. Carbajal, Y. Cheb-\notar, J. Dabis, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, J. Hsu, et al. Rt-1:\nRobotics transformer for real-world control\nat scale. arXiv preprint arXiv:2212.06817,\n2022.\n[7] A. Brohan, N. Brown, J. Carbajal, Y. Cheb-\notar, X. Chen, K. Choromanski, T. Ding,\nD. Driess, A. Dubey, C. Finn, et al. Rt-2:\nVision-language-action models transfer web\nknowledge to robotic control. arXiv preprint\narXiv:2307.15818, 2023.\n[8] P. Budzianowski, W. Maa, M. Freed, J. Mo,\nA. Xie, V. Tipnis, and B. Bolte. Edgevla:\nEfficient vision-language-action models. en-\nvironments, 20:3, 2024.\n[9] S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang.\nOpen-world multi-task control through goal-\naware representation learning and adap-\ntive horizon prediction. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13734–\n13744, 2023.\n10\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\n[10] S. Cai, Z. Wang, K. Lian, Z. Mu, X. Ma,\nA. Liu, and Y. Liang.\nRocket-1:\nMas-\nter open-world interaction with visual-\ntemporal context prompting. arXiv preprint\narXiv:2410.17856, 2024.\n[11] S. Cai, B. Zhang, Z. Wang, H. Lin, X. Ma,\nA. Liu, and Y. Liang. Groot-2: Weakly su-\npervised multi-modal instruction following\nagents. arXiv preprint arXiv:2412.10410,\n2024.\n[12] S. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu,\nand Y. Liang. Groot: Learning to follow\ninstructions by watching gameplay videos.\nIn The Twelfth International Conference on\nLearning Representations, 2024.\n[13] P. Chen, P. Bu, Y. Wang, X. Wang, Z. Wang,\nJ. Guo, Y. Zhao, Q. Zhu, J. Song, S. Yang,\net al.\nCombatvla:\nAn efficient vision-\nlanguage-action model for combat tasks in\n3d action role-playing games. arXiv preprint\narXiv:2503.09527, 2025.\n[14] Y. Cheng, C. Zhang, Z. Zhang, X. Meng,\nS. Hong, W. Li, Z. Wang, Z. Wang, F. Yin,\nJ. Zhao, et al. Exploring large language\nmodel based intelligent agents: Definitions,\nmethods, and prospects.\narXiv preprint\narXiv:2401.03428, 2024.\n[15] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du,\nB. Burchfiel, R. Tedrake, and S. Song. Dif-\nfusion policy:\nVisuomotor policy learn-\ning via action diffusion.\nThe Interna-\ntional Journal of Robotics Research, page\n02783649241273668, 2023.\n[16] M. Deitke, C. Clark, S. Lee, R. Tripathi,\nY. Yang, J. S. Park, M. Salehi, N. Muen-\nnighoff, K. Lo, L. Soldaini, et al. Molmo\nand pixmo: Open weights and open data for\nstate-of-the-art multimodal models. arXiv\npreprint arXiv:2409.17146, 2024.\n[17] J. Deng, Z. Wang, S. Cai, A. Liu, and Y. Liang.\nOpen-world skill discovery from unseg-\nmented demonstrations.\narXiv preprint\narXiv:2503.10684, 2025.\n[18] A. Dosovitskiy. An image is worth 16x16\nwords: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929,\n2020.\n[19] Z. Du, A. Zeng, Y. Dong, and J. Tang. Un-\nderstanding emergent abilities of language\nmodels from the loss perspective.\narXiv\npreprint arXiv:2403.15796, 2024.\n[20] A. Dubey, A. Jauhri, A. Pandey, A. Kadian,\nA. Al-Dahle, A. Letman, A. Mathur, A. Schel-\nten, A. Yang, A. Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783,\n2024.\n[21] L. Fan, G. Wang, Y. Jiang, A. Mandlekar,\nY. Yang, H. Zhu, A. Tang, D.-A. Huang,\nY. Zhu, and A. Anandkumar.\nMinedojo:\nBuilding open-ended embodied agents with\ninternet-scale knowledge.\nAdvances in\nNeural Information Processing Systems, 35:\n18343–18362, 2022.\n[22] W. Fedus, B. Zoph, and N. Shazeer. Switch\ntransformers: Scaling to trillion parame-\nter models with simple and efficient spar-\nsity. Journal of Machine Learning Research,\n23(120):1–39, 2022. URL http:\/\/jmlr.\norg\/papers\/v23\/21-0998.html.\n[23] W. H. Guss, B. Houghton, N. Topin, P. Wang,\nC. Codel, M. Veloso, and R. Salakhutdi-\nnov.\nMinerl:\nA large-scale dataset of\nminecraft demonstrations. arXiv preprint\narXiv:1907.13440, 2019.\n[24] R. A. Jacobs, M. I. Jordan, S. J. Nowlan,\nand G. E. Hinton.\nAdaptive mixtures of\nlocal experts. Neural Computation, page\n79–87, Feb 1991. doi: 10.1162\/neco.1991.\n3.1.79.\nURL http:\/\/dx.doi.org\/10.\n1162\/neco.1991.3.1.79.\n[25] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao,\nA. Balakrishna, S. Nair, R. Rafailov, E. Foster,\nG. Lam, P. Sanketi, et al. Openvla: An open-\nsource vision-language-action model. arXiv\npreprint arXiv:2406.09246, 2024.\n[26] W. Kwon, Z. Li, S. Zhuang, Y. Sheng,\nL. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang,\nand I. Stoica. Efficient memory manage-\nment for large language model serving with\n11\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\npagedattention. In Proceedings of the ACM\nSIGOPS 29th Symposium on Operating Sys-\ntems Principles, 2023.\n[27] F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li,\nW. Li, Z. Ma, and C. Li.\nLlava-next-\ninterleave: Tackling multi-image, video,\nand 3d in large multimodal models. arXiv\npreprint arXiv:2407.07895, 2024.\n[28] X. Li, P. Li, M. Liu, D. Wang, J. Liu, B. Kang,\nX. Ma, T. Kong, H. Zhang, and H. Liu. To-\nwards generalist robot policies: What mat-\nters in building vision-language-action mod-\nels. arXiv preprint arXiv:2412.14058, 2024.\n[29] Z. Li, Y. Xie, R. Shao, G. Chen, D. Jiang,\nand L. Nie.\nOptimus-1:\nHybrid mul-\ntimodal memory empowered agents ex-\ncel in long-horizon tasks.\narXiv preprint\narXiv:2408.03615, 2024.\n[30] S. Lifshitz, K. Paster, H. Chan, J. Ba, and\nS. McIlraith. Steve-1: A generative model\nfor text-to-behavior in minecraft. Advances\nin Neural Information Processing Systems,\n36, 2024.\n[31] H. Lin, Z. Wang, J. Ma, and Y. Liang.\nMcu: A task-centric framework for open-\nended agent evaluation in minecraft. arXiv\npreprint arXiv:2310.08367, 2023.\n[32] H. Lin, B. Huang, H. Ye, Q. Chen, Z. Wang,\nS. Li, J. Ma, X. Wan, J. Zou, and Y. Liang.\nSelecting large language model to fine-tune\nvia rectified scaling law.\narXiv preprint\narXiv:2402.02314, 2024.\n[33] Meta. Llama 3.2: Revolutionizing edge ai\nand vision with open, customizable models,\n2024.\n[34] A. O’Neill, A. Rehman, A. Gupta, A. Mad-\ndukuri, A. Gupta, A. Padalkar, A. Lee,\nA. Pooley, A. Gupta, A. Mandlekar, et al.\nOpen x-embodiment:\nRobotic learning\ndatasets and rt-x models. arXiv preprint\narXiv:2310.08864, 2023.\n[35] OpenAI.\nChatgpt: Optimizing language\nmodels for dialogue, 2023. URL https:\n\/\/openai.com\/blog\/chatgpt\/.\n[36] J. Rasley, S. Rajbhandari, O. Ruwase, and\nY. He. Deepspeed: System optimizations\nenable training deep learning models with\nover 100 billion parameters. In Proceed-\nings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data\nMining, pages 3505–3506, 2020.\n[37] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali,\nT. Ma, H. Khedr, R. Rädle, C. Rolland,\nL. Gustafson, et al. Sam 2: Segment any-\nthing in images and videos. arXiv preprint\narXiv:2408.00714, 2024.\n[38] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac,\nJ. Yu, R. Soricut, J. Schalkwyk, A. M. Dai,\nA. Hauth, K. Millican, et al. Gemini: a family\nof highly capable multimodal models. arXiv\npreprint arXiv:2312.11805, 2023.\n[39] O. M. Team, D. Ghosh, H. Walke, K. Pertsch,\nK. Black, O. Mees, S. Dasari, J. Hejna,\nT. Kreiman, C. Xu, et al. Octo: An open-\nsource generalist robot policy. arXiv preprint\narXiv:2405.12213, 2024.\n[40] H. Touvron, T. Lavril, G. Izacard, X. Mar-\ntinet, M.-A. Lachaux, T. Lacroix, B. Rozière,\nN. Goyal, E. Hambro, F. Azhar, et al. Llama:\nOpen and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971,\n2023.\n[41] L. von Werra, Y. Belkada, L. Tunstall,\nE.\nBeeching,\nT.\nThrush,\nN.\nLam-\nbert,\nS.\nHuang,\nK.\nRasul,\nand\nQ.\nGallouédec.\nTrl:\nTransformer\nreinforcement\nlearning.\nhttps:\n\/\/github.com\/huggingface\/trl,\n2020.\n[42] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar,\nC. Xiao, Y. Zhu, L. Fan, and A. Anandkumar.\nVoyager: An open-ended embodied agent\nwith large language models. Transactions\non Machine Learning Research, 2024.\n[43] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan,\nJ. Bai, K. Chen, X. Liu, J. Wang, W. Ge,\net al. Qwen2-vl: Enhancing vision-language\nmodel’s perception of the world at any res-\nolution. arXiv preprint arXiv:2409.12191,\n2024.\n12\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\n[44] Z. Wang, S. Cai, G. Chen, A. Liu, X. Ma,\nY. Liang, and T. CraftJarvis. Describe, ex-\nplain, plan and select: interactive planning\nwith large language models enables open-\nworld multi-task agents. In Proceedings of\nthe 37th International Conference on Neu-\nral Information Processing Systems, pages\n34153–34189, 2023.\n[45] Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou,\nB. Zhang, H. Lin, Z. He, Z. Zheng, Y. Yang,\net al.\nJarvis-1:\nOpen-world multi-task\nagents with memory-augmented multi-\nmodal language models. IEEE Transactions\non Pattern Analysis and Machine Intelligence,\n2024.\n[46] Z. Wang, S. Cai, Z. Mu, H. Lin, C. Zhang,\nX. Liu, Q. Li, A. Liu, X. Ma, and Y. Liang. Om-\nnijarvis: Unified vision-language-action to-\nkenization enables open-world instruction\nfollowing agents. Advances in Neural Infor-\nmation Processing Systems, 2024.\n[47] Z. Wang, H. Lin, R. Yan, X. Wang, J. Li,\nW. Shi, X. Ma, A. Liu, Y. Liang, et al. Op-\ntimizing inference-time reasoning in llms\nvia retrieval-augmented reflection. arXiv\npreprint arXiv:2403.05313, 2024.\n[48] J. Wei, Y. Tay, R. Bommasani, C. Raf-\nfel, B. Zoph, S. Borgeaud, D. Yogatama,\nM. Bosma, D. Zhou, D. Metzler, et al. Emer-\ngent abilities of large language models.\narXiv preprint arXiv:2206.07682, 2022.\n[49] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu,\nX. Li, M. Liu, H. Li, and T. Kong. Unleashing\nlarge-scale video generative pre-training for\nvisual robot manipulation. arXiv preprint\narXiv:2312.13139, 2023.\n[50] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel,\nand D. Schuurmans.\nFoundation mod-\nels for decision making: Problems, meth-\nods, and opportunities.\narXiv preprint\narXiv:2303.04129, 2023.\n[51] J. Zhang, Y. Guo, X. Chen, Y.-J. Wang, Y. Hu,\nC. Shi, and J. Chen. Hirt: Enhancing robotic\ncontrol with hierarchical robot transform-\ners. arXiv preprint arXiv:2410.05273, 2024.\n[52] J. Zhang, Y. Guo, Y. Hu, X. Chen, X. Zhu,\nand J. Chen.\nUp-vla: A unified under-\nstanding and prediction model for embod-\nied agent. arXiv preprint arXiv:2501.18867,\n2025.\n[53] G. Zhao, K. Lian, H. Lin, H. Fu, Q. Fu, S. Cai,\nZ. Wang, and Y. Liang. Optimizing latent\ngoal by learning from trajectory preference.\narXiv preprint arXiv:2412.02125, 2024.\n[54] H. Zhen, X. Qiu, P. Chen, J. Yang, X. Yan,\nY. Du, Y. Hong, and C. Gan.\n3d-vla: A\n3d vision-language-action generative world\nmodel. arXiv preprint arXiv:2403.09631,\n2024.\n[55] R. Zheng, Y. Liang, S. Huang, J. Gao,\nH. Daumé III, A. Kolobov, F. Huang, and\nJ. Yang. Tracevla: Visual trace prompting\nenhances spatial-temporal awareness for\ngeneralist robotic policies. arXiv preprint\narXiv:2412.10345, 2024.\n[56] S. Zheng, J. Liu, Y. Feng, and Z. Lu. Steve-\neye: Equipping llm-based embodied agents\nwith visual perception in open worlds. arXiv\npreprint arXiv:2310.13255, 2023.\n[57] Y. Zhong, X. Huang, R. Li, C. Zhang,\nY. Liang, Y. Yang, and Y. Chen.\nDex-\ngraspvla: A vision-language-action frame-\nwork towards general dexterous grasping,\n2025.\n[58] E. Zhou, Y. Qin, Z. Yin, Y. Huang, R. Zhang,\nL. Sheng, Y. Qiao, and J. Shao.\nMine-\ndreamer: Learning to follow instructions via\nchain-of-imagination for simulated-world\ncontrol. arXiv preprint arXiv:2403.12037,\n2024.\n[59] Z. Zhou, Y. Zhu, M. Zhu, J. Wen, N. Liu,\nZ. Xu, W. Meng, R. Cheng, Y. Peng, C. Shen,\net al.\nChatvla: Unified multimodal un-\nderstanding and robot control with vision-\nlanguage-action model.\narXiv preprint\narXiv:2502.14420, 2025.\n[60] M. Zhu, Y. Zhu, J. Li, Z. Zhou, J. Wen, X. Liu,\nC. Shen, Y. Peng, and F. Feng. Objectvla:\nEnd-to-end open-world object manipula-\ntion without demonstration, 2025.\n13\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nA. Observation and Action Space\nWe rely solely on visual images for observation, without any symbolic information, similar to VPT [4].\nTo closely resemble the actions of real human players, our action space covers all possible player\nactions excluding the typing of arbitrary letters. For the keypress and click actions, we allocate\ncorresponding reserved tokens from outside the original VLM tokenizer vocabulary. Regarding mouse\nmovements, we largely follow the approach adopted by VPT [4], applying the mu-law encoding\nmethod to discretize mouse X and Y actions separately into 21 bins, totaling 42 discrete bins. These\nbins are similarly mapped to reserved tokens. It is worth mentioning that although Qwen2-VL [43]\ndoes not explicitly provide reserved tokens like Llama3 [33], it is still feasible to achieve equivalent\nresults by expanding the number of special tokens, due to the fact that the size of the vocabulary is\nsmaller than the dimension of the word embeddings.\nDuring inference, models generate actions in a token-by-token manner—first predicting the keybut-\ntons to be pressed, followed by the camera Y and camera X values.\nB. Training Configurations\nThe training configurations for both Visual-Language Post-Training and Action Post-Training are\nlargely consistent. All experiment were conducted on NVIDIA A800-SXM4-80GB GPUs, utilizing\nCUDA version 12.1 and Hugging Face Transformers version 4.47.0. Both training stages utilized\nthe AdamW optimizer with 𝛽1 = 0.9, 𝛽2 = 0.95, weight decay were set to 0, and 𝜖= 1 × 10−8. A\ncosine learning rate schedule was adopted with the learning rate of 5 × 10−6 and a warmup of 200\nsteps. The training used bfloat16 precision, a maximum gradient norm of 1.0, and a fixed random\nseed of 42. To accelerate training, DeepSpeed with ZeRO-1 [36]optimization was employed. For\nVisual-Language Post-Training, the maximum token length was set to 3584, and we set a batch size\nper device of 2 and a gradient accumulation of 4 . For Action Post-Training, the maximum token\nlength was set to 512, which allowed a batch size per device of 8 and a gradient accumulation of 1\nstep per update. Ensuring that the total batch size remained 256. Both stages were trained using 32\nA800 GPUs, with the Visual-Language Post-Training phase running for 128 GPU hours and the Action\nPost-Training phase running for 512 GPU hours.\nTo enhance generalization, distinct data augmentation strategies were adopted for different training\nphases. In the Visual-Language Post-Training phase, modifications included adjustments to hue,\nsaturation, brightness, contrast, as well as random translation, rotation, slight scaling variations,\nshearing, and occasional flipping. These adjustments extended to bounding box and pointing annota-\ntions, with necessary masking of instruction-following prompts. In contrast, the Action Post-Training\nphase focused on adjusting hue, saturation, brightness, contrast, and translation, applied only on\nimages.\nC. Details of Inference\nDuring inference, we require JARVIS-VLA to output in the following format which is as same as the\ngameplay dataset:\n14\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nExample of JARVIS-VLA Interaction for One Turn of Iteration\nInstruction: Craft a bread so I can use it.\nArrange the materials in the crafting grid according to the following pattern:\nwheat | wheat | wheat\nwheat | wheat | wheat\nand get 1 bread.\nObservation:\nAction: <|action_begin|><|cam_w_13|><|cam_h_5|><|action_end|>\nTo accelerate inference, we utilized vllm [26] to load JARVIS-VLA in bfloat16 precision on four\nA800 GPUs. The frames-per-second (FPS) reached 55.\nExample of JARVIS-VLA Interaction for Multi Turn of Iteration\nInstruction: Kill a zombie.\nInstruction: craft a crafting table.\nD. Datasets\nIn this section, we will introduce the collection methods and specific forms of the visual language\npost-training datasets we used, including the world knowledge dataset, visual language alignment\ndataset, and visual grounding dataset.\nD.1. World Knowledge Dataset\nWe sourced content from Wikipedia and the DigMinecraft website, and generating 202K question\nanswering entries using the GPT-3.5-turbo model. The dataset have been adjusted to a uniform format.\nTo increase training efficiency, we randomly merged the original one-question-one-answer data to\norganize multi-turn dialogue dataset with lengths of approximately 1500-2000 tokens each. Below is\nthe specific format of the world knowledge dataset.\n15\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nExample of World Knowledge Dataset\nQuestion: \"What are the different items that can be made using dark oak planks in Minecraft?\"\nAnswer: \"In Minecraft, you can use dark oak planks to craft various items and blocks. Here are some of the items\nand blocks you can make with dark oak planks:\n1. Dark Oak Wood Slabs: Used for flooring, roofs, and other building elements.\n2. Dark Oak Wood Stairs: Ideal for creating inclined pathways or staircases.\n3. Dark Oak Wood Fence: Useful for creating boundaries or enclosures around structures.\n4. Dark Oak Wood Fence Gate: Allows for openings in your fences for easy access.\n5. Dark Oak Wood Door: Adds entrances to your buildings.\n6. Dark Oak Wood Trapdoor: Can serve as hidden entrances or functional hatches.\n7. Dark Oak Wood Button: Can be placed on surfaces and used to activate various redstone mechanisms.\n8. Dark Oak Wood Pressure Plate: Activates redstone contraptions when stepped on.\n9. Dark Oak Wood Sign: Useful for displaying text and providing information in your world.\n10. Dark Oak Wood Bed: Used to craft the frame of a bed, combined with wool blocks for a complete bed.\n11. Dark Oak Wood Boat: Useful for water transportation.\n12. Dark Oak Wood Chest: Can be used to craft storage chests.\n13. Dark Oak Wood Bookshelf: Decorative and enhances enchantment tables.\n14. Petrified Oak Slab: Made from dark oak logs submerged in water and then crafted into planks and slabs. These\nitems enhance functionality and aesthetics in your Minecraft builds.\"\nQuestion: \"How can players build and use automatic farms in Minecraft, and what are some efficient designs?\"\nAnswer: \"In Minecraft, automatic farms are structures designed to automatically harvest resources without\nrequiring constant player intervention. These farms can be extremely useful for gathering various resources\nefficiently. Here, I’ll provide a general overview of how to build and use automatic farms, along with examples of\nefficient designs for different types of farms:\n1. Crop Farms: Automated crop farms are used to grow and harvest crops like wheat, carrots, potatoes, and more.\nEfficient design: - Create a water-based system to hydrate the farmland. A water source in the middle with canals\nleading to the farmland is a common approach. - Use a dispenser to plant seeds or crops onto the farmland. -\nAttach a redstone clock to the dispenser to periodically activate it. - Collect the harvested items using hoppers and\nchests.\n2. Mob Farms: These farms are used to automatically collect drops from hostile mobs, such as zombies, skeletons,\nand creepers. Efficient design: - Build a dark room where mobs will spawn naturally. Use trapdoors, pressure\nplates, or a combination to restrict mob movement. - Use water currents to funnel mobs into a kill chamber. -\nCreate a killing mechanism using either lava blades, suffocation, or other methods. - Use water currents again to\ncollect drops and funnel them into hoppers for collection.\n3. Animal Farms: Automated animal farms can help you gather resources from passive mobs like cows, pigs, and\nchickens. Efficient design: - Create separate pens for each type of animal. - Use a breeding mechanism to maintain\na stable population. Dispensers with food (like wheat for cows) can automatically breed animals. - Include a\nkilling mechanism for excess animals, providing a steady supply of drops. - Use water currents and hoppers to\ncollect drops into chests.\n4. Tree Farms: Automated tree farms allow you to grow and harvest trees without manual effort. Efficient design: -\nPlant saplings in rows using dispensers and bone meal. - Use a mechanism to cut down the trees. This can be\nachieved using flying machines or TNT-based contraptions. - Use water currents to collect the dropped wood and\nsaplings into hoppers and chests.\n5. Fishing Farms: Automated fishing farms simulate fishing to gather loot and experience points. Efficient design: -\nBuild a platform with water sources, a fishing rod dispenser, and a mechanism to reel in the fishing rod. - Create a\nhopper system to collect items caught from the fishing process.\nRemember that the efficiency of these designs can vary depending on your Minecraft version and specific game\nsettings. Additionally, always consider ethical and environmental factors when designing and using automated\nfarms in your gameplay.\"\nD.2. Vision Language Alignment Dataset\nFor the construction of Vision Language Alignment Dataset, we employed a multi-step approach to\ngather and process data. Initially, we sourced images from a variety of online platforms, including\nYouTube gameplay videos, contractor-provided screenshots [4], and other internet resources. We\ncarefully selected 35,000 keyframes from these sources based on criteria such as brightness and visual\n16\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\ncomplexity to ensure the quality of the dataset. Subsequently, we utilized advanced Vision-Language\nModels—such as GPT-4o [35], Claude 3.5 Sonnet [2], and Molmo [16]—to generate contextual image\nquestion-answer pairs, critical in creating a rich, semantically detailed dataset that bridges visual\ncontent and linguistic annotations. To enhance the reliability of the generated data, we implemented\na robust query-validation pipeline. This pipeline employed Llama-3.1-72B [20] to systematically filter\nout ambiguous questions and validate the accuracy of the generated answers. Additionally, it included\na validation step to ensure that the question-answer pairs were relevant to the associated images. To\nfurther diversify our dataset, we specifically allocated nearly half of the vision-question-answer pairs\nfor caption generation, employing the advanced capabilities of GPT-4o. Through the implementation\nof our comprehensive pipeline, we successfully generated a Vision Language Alignment Dataset\ncomprising 15,000 captions and 20,000 visual question answering dataset.\nExample of Vision Language Alignment Datasets\nQuestion: \"What tool is the player holding?\"\nAnswer: \"The player is holding a pickaxe with an enchantment glow, likely a diamond pickaxe due to its color.\"\nQuestion: \"How many hearts of health does the player have?\"\"\nAnswer: \"The player has 9 hearts of health visible, indicating they are missing 1 heart.\"\nQuestion: \"What type of block is being mined by the player?\"\nAnswer: \"The player is mining a stone block.\"\nQuestion: \"What color is the liquid at the top of the image?\"\nAnswer: \"The liquid at the top of the image is orange, indicating it is lava.\"\nQuestion: \"How many inventory slots does the player have in total (visible on the hotbar)?\"\nAnswer: \"The player has 9 inventory slots visible on the hotbar.\"\nD.3. Visual Grounding dataset\nFor Visual Grounding Dataset, we adopted different collection methods for 3D environments and GUI\ninteractions within Minecraft.\nIn 3D environment, we sample images from the contractor data. We initially identify objects that\n17\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nare interacting with the player at a specific moment in the video. Then implementing Backward\nTrajectory Relabeling [10] by sam2 [37] to accurately trace the positions of objects up to 𝑡frames\nbackward, which tend to be in more random positions.\nFor 2D GUI interactions, we directly utilized inventory messages provided by Minecraft in rollouts\nto determine the positions of items within the inventory. Given that each slot in the inventory has\na fixed position, we can directly ascertain the points of items. To enhance the randomness of the\ndata distribution, we performed cropping on the data to make the point distribution more uniformly\ndistributed.\nIn total, we have collected 404K point data from these efforts, mapping real-time entity and object\npositions during scripted tasks effectively.\nExample of Vision Grounding Datasets\nQuestion: \"Show the location of torch in the provided image by outlining it with a bbox.\"\nAnswer: \"<|object_ref_start|>torch<|object_ref_end|><|bbox_start|>(453,333),(563,528)<|bbox_end|>\"\nQuestion: \"Illustrate the precise position of the iron boots in this image, returning the location in points.\"\nAnswer: \"<|object_ref_start|>iron boots<|object_ref_end|><|point_start|>(356,446),(386,494)<|point_end|>\"\nE. Benchmarks\nIn this section, we will introduce our benchmark to test the capability of the Visual Language Models.\nWe also divide the benchmarks into three sections: world knowledge evaluation, visual recognition\nevaluation, and spatial grounding evaluation. For the first two datasets, we wrote most of them\n18\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nTable 2 | Summary of Minecraft knowledge questions and answers Evaluation.\nCategory\nQuantity\nExample Question\nExample Answer\nCraft\n9\nWhat materials are needed to\ncraft a jukebox in Minecraft?\n8 Planks and 1 Diamond.\nPlant\n5\nWhat is the maximum height\nsugarcane\ncan\nreach\nin\nMinecraft?\n3.\nMobs\n12\nWhat happens when a creeper\ngets struck by lightning in\nMinecraft?\nA creeper becomes a charged\ncreeper.\nKill\n3\nCan shield block crossbow at-\ntack in Minecraft?\nNo.\nEnchant\n3\nWhat happens if the player put\non an item with Curse of Bind-\ning enchant in Minecraft?\nIt cannot be removed until the\nitem breaks or the player dies.\nPotion\n4\nWhat materials are needed to\ncraft a potion of poison in\nMinecraft?\nWater bottle,\nNether wart,\nblaze powder, spider eye.\nBiomes\n4\nWhich biome is the only place\nyou can find blue orchids in\nMinecraft?\nSwamp.\nArchitecture\n7\nHow many chests are typi-\ncally found in a shipwreck in\nMinecraft?\n1 to 3.\nourselves, while others were obtained from freely available examples on the Internet. We organized\nthem into open question and answer formats. For the spatial grounding evaluation tasks, we adopted\nthe same method to collect data as for the Visual Grounding Dataset D, ensuring that the test set\ndoes not overlap with the training dataset. Below is a detailed introduction:\nE.1. World Knowledge Evaluation\nFor World Knowledge Evaluation, we divided it into eight subcategories: craft, plant, mobs, kill,\nenchant, potion, biomes, and architecture, with a total of 47 test questions. For each entry, we\nprovided the correct answer. A detailed description is shown in Table 2.\n19\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nKnowledge Evaluation\nQuestion: In Minecraft, what type of tool is best suited for digging Mycelium?\nGPT-4o: A shovel is the best tool for digging Mycelium. [Explanation: In Minecraft, Mycelium is a block that can be\nquickly and efficiently broken using a shovel. Like other dirt-based blocks, using a shovel will speed up the process,\nmaking it much faster than breaking it by hand or with other tools. Additionally, enchantments like Efficiency on your\nshovel can further increase the speed at which you can dig Mycelium.]\nQwen2-VL: A Pickaxe. [Explanation: Mycelium is a special type of grass block in Minecraft that can only be found in\nThe Nether, a dimension that can only be entered after the player has crafted the Nether Portal. To dig Mycelium, you\nwill need to have a Pickaxe in your inventory. The Pickaxe is a tool that is used for digging, breaking, and mining blocks.\nIt is the most efficient and effective tool for digging Mycelium because it has a higher efficiency for breaking blocks than\nother tools.]\nJARVIS-VLA-Qwen2-VL:A shovel is the best tool for digging Mycelium in Minecraft.\nE.2. Vision Understanding Evaluation\nFor Vision Understanding Evaluation, we divided it into five subcategories: scene, object, info,\ninventory, and OCR, comprising a total of 43 test questions. For each entry, we provided an image\nalong with the correct answer. A detailed description can be found in Table 3. Specifically, the Scene\ncategory involves the game environment, including biomes, time, and weather; the Object category\nrelates to objects within the player’s view; and the Info category includes aspects of the player’s\nexperience, health, hunger, and hotbar status.\nTable 3 | Summary of Vision Understanding Evaluation.\nCategory\nQuantity\nImage\nExample\nQuestion\nExample Answer\nScene\n11\nWhat biome you\nthink is the player\ncurrently in?\nMushroom Fields\nbiome.\nObject\n13\nAre there any\nhostile mobs in\nthe picture?\nNo, there are only\n4 pigs in the\npicture.\nInfo\n7\nIs the player’s\nhunger bar\ncurrently full?\nNo.\nInventory\n6\nIs there any oak\nwood in the\ninventory?\nYes, there are oak\nwood planks in\nthe inventory.\nOCR\n6\nWhat instructions\nare visible on the\nscreen?\nOpen your\ninventory Press e.\n20\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nVision Understanding Evaluation Examples\nQuestion: What time of day is it in the game?\nGPT-4o: It is difficult to determine the exact time of day in the game due to the rainy weather, which can create a\ndarkened sky. However, based on the darkness of the surroundings, it appears to be night or close to night. Rain in\nMinecraft makes it harder to see the exact position of the sun or the moon, but the overall dim lighting suggests it\nis either late evening or night time.\nQwen2-VL: It is daytime in the game. The bright light and the visible sky indicate that it is daytime.\nJARVIS-VLA-Qwen2-VL: The time of day in the game is night. This can be determined by the dark sky and the\npresence of rain in the screenshot. In Minecraft, night begins when the sun sets and the moon rises. During\nnighttime, the sky darkens, and if it’s raining, you’ll see rain particles falling from the sky. Additionally, night is a\nmore dangerous time in Minecraft as hostile mobs spawn more frequently.\nE.3. Spatial Grounding Evaluation\nFor Spatial Grounding Evaluation, we used 100 GUI data entries and 236 embodied data entries,\nas seen in Table 4. We required the model to output the points location of a specified object in the\nimage. If there were no points, the bounding box would be used as a substitute. The output results\nwill be normalized to the range [0, 1000).\nTable 4 | Summary of spatial grounding evaluation results for visual grounding tasks.\nCategory\nQuantity\nImage\nExample Question\nExample Answer\nGUI\n100\nPoint the wheat_seeds\n[284,206]\nEmbodied\n236\nPoint the oak_leaves.\n[315,174]\nE.4. Evaluation Metric and Result\nWe designed a customized evaluation method to assess the performance of models in answering the\nquestions. For World Knowledge Questions and Visual Understanding Questions, we explore the\nutilization of LLMs as judges. We selected GPT-4o [35], a state-of-the-art LLM to serve as the judge.\nThe judge model first reviews the responses and compares them to a set of expertly crafted reference\nanswers. Subsequently, the judge assigns a score of correct or incorrect. For visual grounding tasks,\nwe directly score the responses of the evaluated model based on a rule-based approach. Below are\n21\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nModel\nModel Size\nWorld Knowledge\nVisual Understanding\nVisual Grounding\nAcc\nRank\nAcc\nRank\nAcc\nRank\nGPT-4o [1]\n-\n96.6\n1\n76.7\n1\n-\n-\nGPT-4o-mini [1]\n-\n75.9\n2\n62.8\n4\n-\n-\nLlava-Next [27]\n8B\n19.0\n8\n41.9\n10\n-\n-\nMolmo-d-0924 [16]\n7B\n12.1\n10\n58.1\n5\n24.8\n3\nLlama-3.2 [33]\n11B\n20.7\n7\n44.2\n9\n-\n-\nQwen2-VL [43]\n7B\n17.3\n9\n46.5\n7\n16.6\n5\nQwen2-VL (Knowledge)\n7B\n65.5\n5\n46.5\n7\n16.6\n5\nQwen2-VL (Vision)\n7B\n62.1\n6\n65.1\n3\n19.8\n4\nQwen2-VL (Grounding)\n7B\n67.2\n4\n51.2\n6\n63.6\n2\nJARVIS-VLA-Qwen2-VL\n7B\n70.7\n3\n76.7\n1\n88.0\n1\nTable 5 | We compared the performance of various VLMs using our benchmark, including commercial large\nmodels (GPT-4 and GPT-4-mini [35]), open-source models (Llava-Next [27], Molmo-d-0924 [16], Llama-\n3.2 [33], and Qwen2-VL [43]), as well as JARVIS-VLA. The results demonstrate that our method significantly\nenhances the core capabilities of these models, although there remains a gap when compared to state-of-the-art\nmodels.\nMethods\nModels\nWorld Knowledge\nVisual Alignment\nSpatial Grounding\nRaw\nLlava-Next-8B\n18.9\n41.8\n-\n26.7\n10.0\nActVLP\nLlava-Next-8B\n55.8\n60.3\n-\n53.3\n16.6\nRaw\nQwen2-VL-7B\n17.3\n46.5\n16.6\n83.3\n0.0\nActVLP\nQwen2-VL-7B\n70.7\n76.7\n88.0\n86.7\n83.3\nTable 6 | Ablation experiments on base model and model structure. We adopt ActVLP on Llava-Next-8B [27]\nand Qwen2-VL-7B [43] to validate the robustness across different base vision language models.\nthe performances of some models we are interested in under our benchmark: Table 5.\nF. Ablation with different Pre-trained VLMs\nIn this section, we examine the impact of prior training on a VLMs regarding the robustness of the\nmodel’s backbone. VLMs vary in their decision-making capabilities due to differences in training data.\nWe highlight this and emphasize the influence of the VLM training architecture on the VLA.\nWe compare two models, Llava-Next [27] and Qwen2-VL [43], which utilize different pretraining\ndatasets and image processing techniques. Their raw VLM performances and post-training results on\nvarious auxiliary tasks, along with downstream imitation learning outcomes, are presented in Table 6.\nBoth Llava-Next and Qwen2-VL demonstrated more than a 30% increase in downstream task success\nrates after undergoing ActVLP post-training. Indicating that improving model performance through\nvisual language post-training is robust across different models.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse.pdf"}
{"title":"Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy","authors":"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie","summary":"Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.","url":"http:\/\/arxiv.org\/abs\/2502.19902v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2502.19902v2","published":1740647884000,"comment":"Accept to CVPR 2025, Project page:\n  https:\/\/cybertronagent.github.io\/Optimus-2.github.io\/","pdf_text":"Optimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nZaijing Li1 2, Yuquan Xie1, Rui Shao1*, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1*\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/\nAbstract\nBuilding an agent that can mimic human behavior pat-\nterns to accomplish various open-world tasks is a long-\nterm goal.\nTo enable agents to effectively learn behav-\nioral patterns across diverse tasks, a key challenge lies\nin modeling the intricate relationships among observa-\ntions, actions, and language.\nTo this end, we propose\nOptimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-\nlevel planning, alongside a Goal-Observation-Action Con-\nditioned Policy (GOAP) for low-level control. GOAP con-\ntains (1) an Action-guided Behavior Encoder that models\ncausal relationships between observations and actions at\neach timestep, then dynamically interacts with the histori-\ncal observation-action sequence, consolidating it into fixed-\nlength behavior tokens, and (2) an MLLM that aligns be-\nhavior tokens with open-ended language instructions to pre-\ndict actions auto-regressively. Moreover, we introduce a\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset, which contains 25,000 videos across 8 atomic\ntasks, providing about 30M goal-observation-action pairs.\nThe automated construction method, along with the MGOA\ndataset, can contribute to the community’s efforts to train\nMinecraft agents. Extensive experimental results demon-\nstrate that Optimus-2 exhibits superior performance across\natomic tasks, long-horizon tasks, and open-ended instruc-\ntion tasks in Minecraft.\nPlease see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n1. Introduction\nEnabling agents to learn human behavioral patterns for\ncompleting complex tasks in open-world environments, is\na long-standing goal in the field of artificial intelligence\n[5, 23, 34, 47]. To effectively handle diverse tasks in an\n*Corresponding authors\nTransformer\nXL\nGoal \nEncoder\nkeyboard: W\nmouse: [0.0, 1.0]\n⊕\nLLM\nMLLM as Planner\nVision \nEncoder\nCausal \nPerceiver \nHistorical Sequence\nHistory\nAggregator \nAction-guided Behavior Encoder\nExsiting Goal-conditioned Policy\nOurs\n  1. chop a tree\n  2. craft four planks\n  3. craft two sticks\n  4. craft a wood sword\nSub-goals\nGoal-conditioned\nPolicy\nAction\nor\nsub-goal: chop a tree\nsub-goal: chop a tree\nI need a \nwooden \nsword\nVision \nEncoder\n...\nFigure 1.\nLeft: General agent framework.\nRight: Compari-\nson between existing goal-conditioned policies and ours. Existing\nTransformer-XL-based policies [3, 25] exhibit limited natural lan-\nguage understanding capabilities and rely solely on combining im-\nplicit goal embeddings with visual embeddings as inputs. In con-\ntrast, our GOAP achieves superior action prediction by 1) employ-\ning an Action-guided behavior encoder to strengthen causal mod-\neling between observations and actions, as well as to improve his-\ntorical sequence modeling capabilities, and 2) leveraging MLLM\nto enhance open-ended language comprehension.\nopen-world environment like Minecraft [20, 32], a promi-\nnent agent framework [24, 32, 41, 42] integrates a task\nplanner with a goal-conditioned policy. As illustrated in\nFigure 1 (left), this framework first utilizes the task plan-\nner’s language comprehension and visual perception abili-\nties to decompose complex task instructions into sequential\nsub-goals. These sub-goals are then processed by a goal-\nconditioned policy to generate actions.\nAlthough existing agents [24, 32, 42] have made promis-\ning progress by using Multimodal Large Language Models\n(MLLM) [4, 37, 45] as planners, the current performance\nbottleneck for agents lies in the improvement of the goal-\narXiv:2502.19902v2  [cs.AI]  11 Mar 2025\nconditioned policy [24]. As the sub-goal serves as a natu-\nral language description of an observation-action sequence,\nthe goal-conditioned policy needs to learn the crucial re-\nlationships among sub-goals, observations, and actions to\npredict actions. However, existing goal-conditioned poli-\ncies exhibit the following limitations: (1) Existing policies\nneglect the modeling of the relationship between observa-\ntions and actions. As shown in Figure 1, they only model\nthe relationship between the sub-goal and the current ob-\nservation by adding the sub-goal embedding to the obser-\nvation features [3, 25, 43]. However, the current observa-\ntion is generated by the previous action interacting with the\nenvironment. This implies a causal relationship between\naction and observation, which is neglected by current poli-\ncies; (2) Existing policies struggle to model the relation-\nship between open-ended sub-goals and observation-action\nsequences. As depicted in Figure 1, existing policies pri-\nmarily rely on either video encoders [3, 43] or conditional\nvariational autoencoders (CVAE) [25] as goal encoder to\nproduce implicit goal embeddings. Such embeddings have\nlimited representation ability [43]. Simply adding it to ob-\nservation features is sub-optimal and unable to handle the\ncomplex relationship between sub-goals and observation-\naction sequences.\nIn this paper, we propose Optimus-2, a novel agent that\nincorporates an MLLM for planning, alongside a Goal-\nObservation-Action Conditioned Policy (GOAP). To ad-\ndress the aforementioned challenges, we propose GOAP,\nwhich can better model the relationship among the obser-\nvations, actions, and sub-goals in two aspects.\nAn Action-guided Behavior Encoder for observation-\naction sequence modeling. To capture the relationship be-\ntween observations and actions, the Action-guided Behav-\nior Encoder first employs a Causal Perceiver to integrate ac-\ntion embeddings into observation features. It utilizes task-\nrelevant action information as guidance to adjust the obser-\nvation features, thereby providing fine-grained observation-\naction information for action prediction. Additionally, to\nmodel a long-term observation-action sequence without ex-\nceeding input length limitations, a History Aggregator is\nintroduced to dynamically integrate current observation-\naction information with the historical sequence into fixed-\nlength behavior tokens. Behavior tokens can capture the\nlong-term dependencies of the observation-action sequence\nwith a fixed and appropriate length. It enables the agent to\npredict actions that align with the logic of the observation-\naction sequence, rather than making isolated action predic-\ntions based solely on the current observation.\nAn MLLM to model the relationship between sub-\ngoal and observation-action sequence. To explicitly en-\ncode the semantics of sub-goals, we introduce an MLLM as\nthe backbone of GOAP. It aligns the sub-goal with behav-\nior tokens to predict subsequent actions auto-regressively.\nLeveraging the MLLM’s language comprehension and mul-\ntimodal perception capabilities, it can better integrate fea-\ntures from open-ended sub-goals and observation-action\nsequences, thereby enhancing the policy’s action predic-\ntion ability. To the best of our knowledge, GOAP is the\nfirst effort to employ MLLM as the core architecture of\na Minecraft policy, which demonstrates strong instruction\ncomprehension capabilities for open-ended sub-goals.\nMoreover, current Minecraft datasets either lack align-\nment among essential elements [10] or are not publicly\naccessible [1], resulting in a significant scarcity of high-\nquality observation-goal-action pairs necessary for policy\ntraining. To this end, we introduce an automated approach\nfor constructing the Minecraft Goal-Observation-Action\n(MGOA) dataset. The MGOA dataset comprises 25,000\nvideos across 8 atomic tasks, providing approximately 30\nmillion aligned observation-goal-action pairs.\nIt will be\nmade openly available to support advancements within\nthe research community.\nWe conducted comprehensive\nevaluations in the open-world environment of Minecraft,\nand the experimental results demonstrate that Optimus-\n2 achieves superior performance.\nCompared to previous\nSOTA, Optimus-2 achieves an average improvements of\n27%, 10%, and 18% on atomic tasks, long-horizon tasks,\nand open-ended sub-goal tasks, respectively.\nIn summary, our contributions are as follows:\n• We propose a novel agent Optimus-2, which consists of\nan MLLM for planning, and a policy for low-level con-\ntrol. The experimental results demonstrate that Optimus-\n2 exhibits superior performance on atomic tasks, long-\nhorizon tasks, and open-ended sub-goal tasks.\n• To better model the relationship among the observations,\nactions, and sub-goals, we propose Goal-Observation-\nAction Conditioned Policy, GOAP. It contains an Action-\nguided Behavior Encoder for observation-action se-\nquence modeling, and an MLLM to model the relation-\nship between sub-goal and observation-action sequence.\n• To address the scarcity of large-scale, high-quality\ndatasets, we introduce the MGOA dataset. It comprises\napproximately 30 million aligned observation-goal-action\npairs and is generated through an automated process with-\nout any manual annotations. The proposed dataset con-\nstruction method and the released MGOA dataset can\ncontribute to the community’s efforts to train agents.\n2. Related Work\nMinecraft Agents. Previous works [2, 8, 13, 31] have con-\nstructed policies in Minecraft using reinforcement learn-\ning or imitation learning. VPT [1] was training on large-\nscale video data recorded by human players, using behavior\ncloning to mimic human behavior patterns. GROOT [3] em-\nploys a video encoder as a goal encoder to learn semantic in-\nformation from videos. However, these policies rely solely\nLarge Language Model\nAction Head\n�t+1\n ViT\nLlama Tokenizer\nSub-goal: \nChop a tree.\nImage Token\nFFN\nCross-Attention\nV\nK\nQ\n...\nCross-Attention\nHistory-Attention\nV\nK\nQ\nV\nK\nQ\nText Token\nAction Token\nBehavior Token\nCausal Perceiver \nHistory\nAggregator \nAction-guided Behavior Encoder\nHistorical Memory Bank \n...\nAction-guided\nBehavior Encoder\n�t\n�t\n�1\n�t−3\n�t−2\n�t−1\n�t+1\n��\n...\n...\n�t\n...\n�t\n�1\n�2\n�3\n��−1\nLoRA\nMLLM-based\nPlanner\nTask: I need a \nwooden sword.\nFigure 2. Overview of Optimus-2. Given a task and the current observation, Optimus-2 first uses an MLLM-based Planner to generate a\nseries of sub-goals. Optimus-2 then sequentially executes these sub-goals through GOAP. GOAP obtains behavior tokens for the current\ntimestep via the Action-guided Behavior Encoder, and these behavior tokens, along with image and text tokens, are fed into the LLM to\npredict subsequent actions.\non visual observations as input and cannot follow human\ninstructions to accomplish specific tasks. MineCLIP [10]\nintroduces a video-text contrastive learning module as a re-\nward model for policy, and STEVE-1 [25] builds on VPT\n[1] by incorporating MineCLIP as goal encoder, enabling\npolicy to follow natural language instructions. Despite these\nadvancements, these policies are constrained by language\nunderstanding and reasoning capabilities. To address this,\ncurrent agents [20, 24, 32, 40, 42, 43] leverage MLLM’s\ninstruction following capabilities to decompose complex\ntasks into executable sub-goal sequences, which are then\nfed into a goal-conditioned policy [3, 25] or formed as exe-\ncutable code [26, 28, 51, 52]. Despite significant progress,\nthe performance of current policies remains constrained by\ntheir limited ability to understand sub-goals. In this paper,\nwe aim to develop an MLLM-based goal-conditioned pol-\nicy to enhance the policy’s comprehension of open-ended\nsub-goals, thereby improving overall performance.\nLong-term Video Modeling. Previous work [1, 3, 10, 25]\nhave segmented videos into multiple clips for training to\nalleviate the challenges posed by long-sequence video in-\nputs. However, this approach prevents the agent from learn-\ning comprehensive behavior representations from the entire\nvideo. To handle long-term video sequences [22, 48, 49],\nexisting studies employ temporal pooling [30], querying\ntransformers [14, 46], or token merging [16, 38, 50] to\nintegrate long-sequence visual tokens. Inspired by previ-\nous works [6, 18, 19, 44], we propose a Q-former [7, 21]\nstructure with a memory bank [14], enabling effective long-\nterm sequence modeling through interactions with histori-\ncal queries. Unlike existing methods that model only the\nobservation sequence, we focus on multimodal learning\n[33, 35, 36]. Moreover, different from previous work [14]\nthat primarily compress video features into fixed-length to-\nkens, our Action-guided Behavior Encoder dynamically in-\nteracts with the historical sequence at each timestep, pro-\nducing behavior tokens corresponding to the observation-\naction sequence from the start to the current timestep.\n3. Preliminaries and Problem Formulation\nIn Minecraft, agents [1, 3, 25] exhibit behavior patterns\nsimilar to humans: at each time step t, the agent receives\na visual observation ot and generates control actions at+1\nusing the mouse and keyboard.\nThese actions interact\nwith the environment, resulting in a new visual observa-\ntion ot+1.\nThrough continuous interactions, a trajectory\nJ = {(o1, a1), (o2, a2), (o3, a3), . . . , (oT , aT )} is formed,\nwhere T represents the length of the trajectory. Previous\nwork primarily trained Minecraft agents using reinforce-\nment learning [10] or behavior cloning [3, 25]. For exam-\nple, in behavior cloning, the goal of the policy pθ(at+1|o1:t)\nis to minimize the negative log-likelihood of the actions at\neach time step t given the trajectory J. Considering that\nsuch trajectories are typically generated under explicit or\nimplicit goals, many recent approaches condition the be-\nhavior on a (implicit or explicit) goal g and learn goal-\nconditioned policy pθ(at+1|o1:t, g) [3, 25]. Generally, for\nboth agents and humans, the explicit goal g is a natural lan-\nguage instruction.\nFormally, given a trajectory J with length T, standard\nbehavior cloning trains the policy pθ(·) with parameters θ\nby minimizing the negative log-likelihood of actions:\nmin\nθ\nT\nX\nt=1\n−log pθ(at+1|o1:t, g)\n(1)\n4. Optimus-2\nIn this section, we first give an overview of our proposed\nagent framework, Optimus-2. As shown in Figure 1 (left),\nit includes a planner for generating a series of executable\nsub-goals and a policy that sequentially executes these sub-\ngoals to complete the task.\nNext, we introduce how to implement Optimus-2’s plan-\nner (Sec. 4.1). Subsequently, we elaborate on how to imple-\nment the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we\nintroduce an automated dataset generation method to obtain\na high-quality Minecraft Goal-Observation-Action dataset\n(MGOA) for training GOAP.\n4.1. MLLM-based Task Planner\nIn Minecraft, a complex task consists of multiple interme-\ndiate steps, i.e., sub-goals. For example, the task “I need\na wooden pickaxe” includes five sub-goals: ‘chop a tree to\nget logs\n’, ‘craft four planks\n’, ‘craft a crafting table\n’, ‘craft two sticks\n’, and ‘craft a wooden pickaxe\n’.\nTherefore, a planner is essential for the agent, as it needs to\ndecompose the given complex task into a sequence of exe-\ncutable sub-goals for the policy to execute sequentially. In\nthis paper, we follow Li et al. [24], employing an MLLM\nas the planner, which takes current observation and task in-\nstruction as input to generate sub-goals.\n4.2. Goal-Observation-Action Conditioned Policy\nAccording to Sec 3., a key insight into the relationship\namong observation o, action a, and sub-goal g is: that the\nobservation o and action a at the same time step have a\ncausal relationship; and the sub-goal g is a natural language\ndescription of the observation-action sequence over a cer-\ntain time. To better model the relationships among the three\nelements mentioned above, we propose first integrating the\nrepresentations of observation and action at each time step,\nthen modeling the observation-action sequences along the\ntemporal dimension, and finally aligning the observation-\naction sequences with the sub-goal for action prediction.\nMotivated\nby\nthis,\nwe\npropose\na\nnovel\nGoal-\nObservation-Action\nconditioned\nPolicy,\nGOAP.\nAs\nshown in Figure 2, our GOAP consists of an Action-guided\nBehavior Encoder that dynamically models observation-\naction sequences into fixed-length behavior tokens and an\nTable 1. Comparison of the MGOA dataset with existing datasets.\nO, G, and A represent observation, goal, and action. VPT† in-\ndicates the amount of data that is openly accessible. MineCLIP‡\ndenotes narrated Minecraft videos available on YouTube.\nFormat\nDataset\nO\nG\nA\n# Frames\nImage-Text Pairs\nMP5 [32]\n!\n!\n500K\nOmniJARVIS [43]\n!\n!\n!\n600K\nGameplay Video\nVPT† [1]\n!\n!\n6M\nMineCLIP‡ [10]\n!\n!\n20B\nSTEVE-1 [25]\n!\n!\n!\n32K\nMGOA (Ours)\n!\n!\n!\n30M\nMLLM that aligns such behavior tokens with sub-goal for\naction prediction.\n4.2.1. Action-guided Behavior Encoder\nPrevious policies often overlook the causal relationship be-\ntween observation and action at each timestep. Moreover,\nit remains a challenge to model the long-term observation-\naction sequence without exceeding input length constraints.\nTo this end, we propose an Action-guided Behavior En-\ncoder that integrates the representations of observation and\naction at each time step and then dynamically models the\nhistorical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation ot into a\nvisual encoder VE to obtain the visual features:\nvt ←VE(ot)\n(2)\nwhere vt ∈RP ×d, P is the number of patches for each im-\nage, and d is the dimension of the extracted image feature.\nIn practice, we employ ViT [9] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model\nthe relationship between observations and actions. It takes\nthe visual feature vt as query tokens and the action embed-\nding at as key and value. The module then constructs the\ninformation interaction between action at and vt through a\ncross-attention mechanism:\nQ = vtW Q\nv , K = atW K\na , V = atW V\na\n(3)\nˆvt = CrossAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(4)\nwhere W Q\nv , W K\na , and W V\na represent the weight matrices\nfor the query (Q), key (K), and value (V), respectively.\nCrossAttn(·) denotes the cross-attention layer, and d is\nthe dimension of the image features. In this way, it explic-\nitly assigns action information at at time step t to the visual\nfeatures ˆvt, enhancing the causal relationship between ob-\nservations and actions.\nSubsequently, we introduce a History Aggregator mod-\nule to capture the information of the observation-action se-\nquence along the temporal dimension, serving as the behav-\nior representation. At each timestep t, behavior tokens Bt\nserve as queries, while the sequence of historical behavior\ntokens Ht = [B1, B2, . . . , Bt−1] acts as keys and values.\nThe current behavior tokens interact with the historical se-\nquence through a history-attention layer HisAttn(·):\nˆBt = HisAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(5)\nwhere Q, K, and V are calculated similarly to Eq 3.\nFinally, another cross-attention layer is introduced, using\nthe behavior tokens ˆBt as queries, and the visual features ˆvt\nas keys and values. In this way, the behavior tokens incor-\nporate the current observation-action information. Follow-\ning the approach of He et al. [14], we construct a memory\nbank for historical behavior tokens Ht, utilizing the simi-\nlarity between adjacent features to aggregate and compress\nthe behavior tokens. This method not only preserves early\nhistorical information but also keeps the historical behav-\nior token sequence Ht at a fixed length to reduce compu-\ntational costs. Leveraging the Action-guided Behavior En-\ncoder, we obtain behavior tokens ˆBt, which correspond to\nthe observation-action sequence from the start to the current\ntime step t.\n4.2.2. MLLM Backbone\nTo model the relationship between the sub-goal and\nobservation-action sequence, we introduce an MLLM that\ntakes the sub-goal g, current observation features vt, and\nbehavior tokens Bt as input to predict subsequent actions\nauto-regressively. To enable the MLLM backbone MLLM\nto predict low-level actions, we employ VPT [1] as action\nhead AH to map output embeddings ¯at+1 of language model\ninto the action space.\n¯at+1 ←MLLM([g, vt, Bt])\n(6)\nat+1 ←AH(¯at+1)\n(7)\nFormally, given a dataset D = {(o1:T , a1:T )}M with M\ncomplete trajectories, we train GOAP to learn the behavior\ndistribution from D via behavioral cloning. Moreover, we\nintroduce a KL-divergence loss to measure the output dis-\ntribution similarity between GOAP and VPT [1]. This helps\nour model effectively learn the knowledge from the teacher\nmodel VPT. The training loss can be formulated as follows:\nLθ = λBC\nT\nX\nt=1\n−log pθ(at+1|o1:t, a1:t, g)\n+λKL\nT\nX\nt=1\nDKL(qϕ(at+1|o1:t) ∥pθ(at+1|o1:t, g))\n(8)\nwhere λBC and λKL are trade off coefficients, pθ is the\nGOAP, qϕ is the teacher model.\nTable 2. Main Result of GOAP on Atomic Tasks. We report the\naverage rewards of each task.\nPolicy\nLogs\nSeeds\nDirt\nStone\nVPT (text) [1]\n2.6\n0.8\n9.2\n0.0\nSTEVE-1 [25]\n11.0\n5.1\n10.0\n3.2\nGROOT [3]\n14.3\n7.3\n19.7\n19.0\nFSQ GROOT [43]\n10.8\n8.2\n20.3\n5.8\nGOAP [MLP ]\n7.2\n4.3\n14.4\n15.5\nGOAP [V P T ]\n15.0\n8.5\n26.7\n25.7\n4.3. MGOA Dataset\nIn Minecraft, there remains a significant lack of high-\nquality goal-observation-action pairs to support behavior\ncloning training.\nPrevious work has primarily relied on\ngameplay videos as training data. These datasets either lack\nnatural language instructions (explicit goals) [1, 3], or use\nactions predicted by IDM models [1] for each observation\nas pseudo-labels [1, 25], which leads to a risk of misalign-\nment between observations and actions. Inspired by Li et\nal. [24], we propose an automated data generation pipeline\nthat enables the creation of aligned goal-observation-action\npairs without the need for manual annotations or human\ncontractors. First, we utilize existing agents [25], provid-\ning them with clear natural language instructions to attempt\ntask completion in Minecraft. We then record the actions\nand corresponding observations during goal execution, gen-\nerating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply\nthe following filtering criteria: 1) only recording videos\nin which the task is successfully completed, and 2) dis-\ncarding videos where task execution takes an excessive\namount of time.\nFor more details, please refer to Sup.\nC. Through this automated approach, we obtained 25k\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset. A comparison of the MGOA dataset with the exist-\ning Minecraft datasets is shown in Table 1. Our automated\ndata generation pipeline offers several key advantages: 1)\nit enables the generation of aligned goal-observation-action\npairs without the need for manual annotation or pseudo-\nlabeling; 2) its construction process is parallelizable, allow-\ning for rapid dataset generation; and 3) it leverages local\nagents for data generation, resulting in low-cost production.\n5. Experiments\n5.1. Experiments Setting\nEnvironment. Following [1, 25], we conduct experiments\nin the complex, open-world environment of Minecraft on\nthe MineRL [12] platform. The agent interacts with the\nMineRL environment at 20 frames per second, generating\nlow-level control signals for the mouse and keyboard. For\neach task execution, the agent is initialized in a randomized\nTable 3. Main Result of Optimus-2 on Long-horizon Tasks. We report the average success rate (SR) on each task group, the results of each\ntask can be found in the Sup. F.1. Pure GPT-4V† denotes the use of GPT-4V in a zero-shot manner to generate executable sub-goals for\nthe policy. Human‡ denotes the human-level baseline, with results sourced from previous work [24].\nMethod\nPolicy\nWood\nStone\nIron\nGold\nDiamond\nRedStone\nArmor\nPure GPT-4V †\nVPT (text)\n0.22\n0.08\n0.00\n0.00\n0.00\n0.00\n0.00\nSTEVE-1\n0.41\n0.20\n0.00\n0.00\n0.00\n0.00\n0.00\nGOAP\n0.50\n0.31\n0.12\n0.02\n0.01\n0.03\n0.03\nDEPS [41]\nSTEVE-1\n0.77\n0.48\n0.16\n0.00\n0.01\n0.00\n0.10\nJarvis-1 [42]\nSTEVE-1\n0.93\n0.89\n0.36\n0.07\n0.08\n0.16\n0.15\nOptimus-1 [24]\nSTEVE-1\n0.98\n0.92\n0.46\n0.08\n0.11\n0.25\n0.19\nOptimus-2\nGOAP\n0.99\n0.93\n0.53\n0.09\n0.13\n0.28\n0.21\nHuman‡ [24]\n-\n1.00\n1.00\n0.86\n0.17\n0.16\n0.33\n0.28\nTable 4.\nMain Result of GOAP on Open-Ended Instruction\nTasks. We report the average success rate (SR) on Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nGROOT [3] and FSQ GROOT [43] were not included as base-\nlines, as they are unable to process language input.\nPlanner\nPolicy\nGLM-4V\nVPT (text)\n0.05\n0\n0\n0\n0\nSTEVE-1\n0.60\n0\n0\n0\n0\nGOAP\n0.71\n0.39\n0.11\n0.14\n0.13\nGPT-4V\nVPT (text)\n0.11\n0\n0\n0\n0\nSTEVE-1\n0.66\n0.10\n0\n0\n0\nGOAP\n0.75\n0.47\n0.13\n0.16\n0.17\nenvironment, allowing us to evaluate the agent’s generaliza-\ntion across diverse environments. Please refer to Sup. B for\nmore details about the Minecraft environment.\nImplementation details. For the planner, we follow Li et\nal. [24], using a hybrid multimodal memory empowered\nGPT-4V 1 as the agent’s planner. As for the policy, we ini-\ntialize GOAP with the weights of DeepSeek-VL-1.3B [29]\nas initialization. We train it on the MGOA dataset and the\npublicly available OpenAI Contractor Dataset [1] through\nbehavior cloning. All experiments were conducted on 8x\nNVIDIA L40 GPUs. Training details and hyperparameter\nsetting can be found in Sup. D.\nEvaluation Tasks & Metrics. Evaluation tasks are catego-\nrized into three types: Atomic Tasks, Long-Horizon Tasks,\nand Open-Ended Instruction Tasks. For each task, the en-\nvironment is randomly reinitialized on each attempt, with a\nminimum of 30 executions per task to ensure robustness.\n• Atomic Tasks represent short-term skills in Minecraft. We\nselect “chop a tree to get logs\n”, “collect seeds\n”,\n“collect dirt\n”, and “mine stone\nwith a pickaxe” as\nevaluation tasks. These tasks evaluate the policy’s basic\ncapabilities in Minecraft. We report the average rewards\n1https:\/\/openai.com\/index\/gpt-4v-system-card\n(number of items obtained) per task execution as an eval-\nuation metric.\n• Long-horizon Tasks consist of an interdependent atomic\ntasks sequence, where the failure of any single atomic\ntask results in the failure of the entire sequence. These\nlong-horizon tasks are designed to evaluate the agent’s ca-\npability to execute a series of diverse tasks continuously\nwithin a complex environment. We follow the setup of Li\net al. [24], conducting experiments on long-horizon tasks\ncomprising 67 tasks grouped into 7 categories. We report\nthe average Success Rate (SR) as an evaluation metric.\n• Open-Ended Instruction Tasks are not limited to prede-\nfined text formats; rather, they involve flexible language\ndirectives that prompt the agent to accomplish long-\nhorizon tasks. These tasks evaluate the agent’s capacity\nto interpret and execute instructions expressed in open-\nended natural language. We selected the Torch\nfrom\nthe Stone Group, Rail\nfrom the Iron Group, Golden\nShovel\nfrom the Gold Group, Diamond Pickaxe\nfrom the Diamond Group, and Compass\nfrom the Red-\nstone Group as evaluation tasks. Given a crafting rela-\ntionship graph, we instructed GPT-4V and GLM-4V [11]\nto generate five open-ended instructions for each task.\nThis allows us to evaluate the policies’ understanding and\nexecution capabilities regarding open-ended instructions.\nTask instructions are provided in the Sup. E.1.\nBaseline. For Atomic Tasks and Open-ended Instruction\nTasks, we compare GOAP with existing goal-conditioned\npolicies, including VPT [1], STEVE-1 [25], GROOT [3]\nand FSQ GROOT [43]. For Long-horizon Tasks, we employ\nGPT-4V, DEPS [41], Jarvis-1 [42], and Optimus-1 [24] as\nbaselines. We also introduce a human-level baseline [24] to\nevaluate the performance gap between existing agents and\nhuman capabilities.\n5.2. Experimental Results\nThe experimental results for Optimus-2 compared to the\nbaselines across Atomic Tasks, Long-horizon Tasks, and\nAgent\nInstruction: I need some iron ores, what should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 3. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need some iron ores,\nwhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nTable 5. Ablation study of Action-guided Behavior Encoder on\nAtomic Tasks. We report average rewards on each task. CP., HA.,\nand MB. represent the Causal Perceiver, History Aggregator, and\nMemory Bank, respectively.\nAblation Setting\nAtomic Task\nCP.\nHA.\nMB.\nLogs\nSeeds\nDirt\nStone\nAverage\n!\n!\n!\n15.0\n8.5\n26.7\n25.7\n19.0\n6.1\n5.4\n12.7\n15.7\n10.0 (↓47.4%)\n!\n10.2\n4.7\n12.8\n21.1\n12.2 (↓35.8%)\n!\n!\n7.4\n6.2\n13.1\n15.5\n10.6 (↓44.2%)\n!\n!\n12.0\n6.8\n22.7\n16.8\n14.6 (↓23.2%)\nOpen-ended Instruction Tasks are presented in Table 2, Ta-\nble 3, and Table 4, respectively.\nGOAP excels in Atomic Tasks. Table 2 shows that pro-\nposed GOAP achieves improvements of 5%, 4%, 31%, and\n35% over the current SOTA on the Logs\n, Seeds\n, Dirt\n, and Stone\n, respectively. These results demonstrate\nthat GOAP has successfully mastered a range of short-term\nskills across diverse environments, and can acquire items\nmore effectively than existing policies.\nOptimus-2 surpasses SOTA in Long-horizon Tasks.\nTable 3 shows that Optimus-2 achieved the highest success\nrates across all seven task groups, particularly excelling in\nthe challenging Diamond Group and Redstone Group with\nsuccess rates of 13% and 28%, respectively. This indicates\nthat Optimus-2 has effectively learned complex behavior\npatterns across atomic tasks, enabling it to sequentially ex-\necute multiple sub-goals and successfully complete long-\nhorizon tasks within complex environments.\nGOAP\noutperforms\nin\nOpen-ended\nInstruction\nTasks. As shown in Table 4, GOAP achieved significantly\nhigher success rates than existing agents across all tasks.\nNotably, on the challenging tasks of Golden Shovel\n, Di-\namond Pickaxe\n, and Compass\n, existing policies fail to\ncomplete these tasks, whereas GOAP achieves success rates\nof 13%, 16%, and 17%, respectively. This advantage stems\nSuccess Rate\n0.0\n5.0\n10.0\n15.0\nGolden \nShovel\nDiamond \nPickaxe\nCompass\nLLM \n20.0\n17.1\n13.1\n16.2\nTransformer-XL\n0.5\n0.5\n0.0\nFigure 4. Ablation of LLM backbone on Open-ended Instruction\nTasks, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nfrom GOAP’s superior comprehension of open-ended nat-\nural language instructions, whereas existing agents exhibit\nweaker instruction-following capabilities. Moreover, Fig-\nure 3 illustrates an example of different policies executing\nan open-ended goal. Due to the limited representation ca-\npability of their goal encoders, VPT [1] and STEVE-1 [25]\nfail to understand the goal, “I need some iron ores, what\nshould I do?” In contrast, GOAP leverages the MLLM’s\nunderstanding of open-ended instructions to effectively ac-\ncomplish the goal (obtaining iron ore\n).\n5.3. Ablation Study\nThere are many unexplored questions around best practices\nfor developing MLLM-based policy in Minecraft. In this\nsection, we conduct an extensive ablation study and sum-\nmarize our key findings.\nThe Action-guided Behavior Encoder plays a crucial\nrole in task execution. As shown in Table 5, the removal\nof the Causal Perceiver leads to an average performance de-\ncline of 42% across all tasks, highlighting the importance\nof capturing the causal relationship between observations\nand actions. Moreover, eliminating the History Aggregator\nAverage Rewards\n0.0\n5.0\n10.0\n15.0\n25.0\n30.0\nLogs\nSeeds\nDirt\nStone\nOCD_MGOA_Mix \n20.0\n26.7\n25.7\n6.0\n15.0\n8.5\n2.3\n14.3\n2.6\nOCD\n2.3\n1.4\n17.1\n22.1\nMGOA\nFigure 5. Ablation study on Training data. OCD refers to the\nOpenAI Contractor Dataset [1]. We report the average rewards on\neach Atomic Task.\nand Memory Bank also results in an average performance\ndecline of 36% across all tasks. This emphasizes the cru-\ncial role of the History Aggregator in modeling observation-\naction sequences and the Memory Bank in dynamically\nstoring long-sequence information.\nLLM significantly enhances policy’s ability to under-\nstand open-ended instructions. As shown in Figure 4, re-\nplacing the LLM backbone with a Transformer-XL leads to\na noticeable decline in performance. We attribute this to\nthe LLM’s pretraining on large-scale textual corpora, which\nendows it with a robust comprehension of open-ended lan-\nguage, a capability that Transformer-XL lacks.\nA pretrained action head improves performance in\nMinecraft. As shown in Table 2, replacing VPT with a\n2-layer MLP projector as the action head leads to a no-\nticeable decline in Optimus-2’s performance. While MLP-\nbased action heads have shown promising results in other\ndomains [17, 27], this substitution is less effective in the\nMinecraft environment. We attribute this to VPT’s exten-\nsive pretraining on large-scale gameplay data, which equips\nit with substantial domain-specific knowledge critical for\neffective task execution in Minecraft.\nThe MGOA datsaset is beneficial for training GOAP.\nWe conducted comparative experiments to evaluate the im-\npact of different training datasets on performance.\nAs\nshown in Figure 5, training only with the current most com-\nmonly used dataset, OpenAI Contractor Dataset (OCD), re-\nsults in suboptimal performance for GOAP on all Atomic\nTasks.\nFor example, compared to training with a mixed\ndataset, its performance on Stone\ndropped by 89%. We\nattribute this to the fact that OCD offers a wide variety of\ntasks but lacks high data quality.\nIn contrast, using our\nMGOA dataset, performance on the four atomic tasks im-\nproved by an average of 70% compared to using only the\nOCD data. We attribute this to the fact that MGOA contains\nhigh-quality aligned goal-observation-action pairs, which\nis beneficial for policy training. Further, we mix the two\ndatasets to train the policy in order to balance task diversity\nLog\nDirt\nSeed\nStone\n(a) ViT\n(c) Action-guided\n          Behavior Encoder\n(b) MineCLIP\nFigure 6. t-SNE visualization of representations extracted by (a)\nViT (b) MineCLIP and (c) Action-guided Behavior Encoder across\nAtomic Tasks. The visualization results show that the represen-\ntations in (a) and (b) cannot distinguish between different tasks,\nwhereas our Action-guided Behavior Encoder clearly differenti-\nates the behavior representations for the four tasks.\nand data quality, leading to improved performance.\n5.4. Visualization of Behavior Representation\nAs shown in Figure 6, we apply t-SNE [39] to visualize\nobservation features extracted by ViT [9], MineCLIP [10],\nand the Action-guided Behavior Encoder for four tasks.\nFrom (a) and (b) in Figure 6, it is evident that the behavior\nrepresentations extracted by ViT and MineCLIP are highly\nmixed, making it challenging to delineate the boundaries\nbetween different tasks. This lack of clear distinction be-\ntween task-specific behavior representations can hinder the\nmodel’s ability to understand the unique behavior patterns\nassociated with each task, potentially leading to task fail-\nure. In contrast, the visualization in (c) of Figure 6 reveals\nclear, distinct clusters for each task, demonstrating that the\nAction-guided Behavior Encoder effectively captures subtle\ndifferences in observation-action sequences, thereby learn-\ning robust behavior representations across tasks.\n6. Conclusion\nIn this paper, we propose a novel agent, Optimus-2, which\ncan excel in various tasks in the open-world environment of\nMinecraft. Optimus-2 integrates an MLLM for high-level\nplanning and a Goal-Observation-Action conditioned Pol-\nicy (GOAP) for low-level control. As a core contribution\nof this paper, GOAP includes an Action-guided Behavior\nEncoder to model the observation-action sequence and an\nMLLM to align the goal with the observation-action se-\nquence for predicting subsequent actions.\nExtensive ex-\nperimental results demonstrate that GOAP has mastered\nvarious atomic tasks and can comprehend open-ended lan-\nguage instructions. This enables Optimus-2 to achieve su-\nperior performance on long-horizon tasks, surpassing ex-\nisting SOTA. Moreover, we introduce a Minecraft Goal-\nObservation-Action dataset to provide the community with\nlarge-scale, high-quality data for training Minecraft agents.\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nSupplementary Material\nThe supplementary document is organized as follows:\n• Sec. A: Limitation and Future Work.\n• Sec. B: Minecraft Environment.\n• Sec. C: MGOA Dataset.\n• Sec. D: Training Details.\n• Sec. E: Evaluation Benchmark.\n• Sec. F: Experimental Results.\n• Sec. G: Case Study.\nA. Limitation and Future Work\nIn this paper, we aim to explore how agents can mimic\nhuman behavior patterns in Minecraft to accomplish vari-\nous tasks. Experimental results demonstrate that Optimus-\n2 performs exceptionally well in both atomic tasks and\nlong-horizon tasks. However, due to the lack of sufficient\nhigh-quality data for open-ended tasks (such as “building a\nhouse” and “defeating the Ender Dragon”), there remains\nsignificant room for improvement. Once such datasets are\navailable, the ability of Optimus-2 to complete open-ended\ntasks will be enhanced. Moreover, despite showing promis-\ning performance in Minecraft, we have not yet extended our\nexploration to other simulation platforms, which represents\na potential direction for future research.\nB. Minecraft\nMinecraft is an extremely popular sandbox video game de-\nveloped by Mojang Studios 2. It allows players to explore\na blockly, procedurally generated 3D world with infinite\nterrain, discover and extract raw materials, craft tools and\nitems, and build structures or earthworks. In this enviro-\nment, AI agents need to face situations that are highly sim-\nilar to the real world, making judgments and decisions to\ndeal with various environments and problems. As shown\nin Figure 7, both agents and humans are required to receive\nnatural language instructions and current observations as in-\nput, and then output low-level actions, such as mouse and\nkeyboard control commands. Therefore, Minecraft serves\n2https:\/\/www.minecraft.net\/en-us\/article\/meet-mojang-studios\n...\nObservation\n...\nAgent Action\nHuman Action\nspace\nW\nW\nKeyboard:\n {forward}\nMouse:{(2.7, 1.5)}\nKeyboard: \n{forward,jump}\nMouse: {(1.2, 0)}\nMouse: {attack}\nTask: chop a tree to get logs\nLanguage\nFigure 7.\nIllustration of behavior patterns of both human and\nagents in Minecraft.\nas an ideal open-world environment for training agent that\ncan learn human behavior patterns.\nB.1. Basic Rules\nBiomes. The Minecraft world is divided into different areas\ncalled “biomes”. Different biomes contain different blocks\nand plants and change how the land is shaped. There are 79\nbiomes in Minecraft 1.16.5, including ocean, plains, forest,\ndesert, etc. Diverse environments have high requirements\nfor the generalization of agents.\nItem. In Minecraft 1.16.5, there are 975 items can be ob-\ntained, such as wooden pickaxe\n, iron sword\n. Item can\nbe obtained by crafting or destroying blocks or attacking\nentities. For example, agent can attack cows\nto obtain\nleather\nand beef\n. Agent also can use 1 stick\nand 2\ndiamonds\nto craft diamond sword\n.\nTechnology Tree.\nIn Minecraft, the technology hierar-\nchy comprises six levels: wood\n, stone\n, iron\n, gold\n, diamond\n, and redstone\n.\nEach tool level corre-\nsponds to specific mining capabilities. Wooden tools can\nmine stone-level blocks but are incapable of mining iron-\nlevel or higher-level blocks.\nStone tools can mine iron-\nlevel blocks but cannot mine diamond-level or higher-level\nblocks.\nIron tools are capable of mining diamond-level\nblocks. Finally, diamond tools can mine blocks of any level,\nincluding redstone-level.\nGameplay progress. Progression in Minecraft primarily\ninvolves discovering and utilizing various materials and re-\nsources, each unlocking new capabilities and opportunities.\nFor instance, crafting a wooden pickaxe\nenables players\nTable 6. Action space of agent in Minecraft.\nIndex\nAgent Action\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove back.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current movement.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPlace blocks, entity, open items or other interact actions defined by game.\n10\nhotbar [1-9]\nkeys 1-9\nSelects the appropriate hotbar item.\n11\nOpen\/Close Inventory\nkey E\nOpens the Inventory. Close any open GUI.\n12\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nto mine stone\n, which can then be used to create a stone\npickaxe\nand a furnace\n. These tools allow for the mining\nand smelting of iron ore\n. Subsequently, crafting an iron\npickaxe\nenables the extraction of diamonds\n, while a di-\namond pickaxe\ncan mine virtually any block in the game.\nSimilarly, cultivating crops facilitates breeding various an-\nimals, each providing unique resources beyond sustenance.\nDrops from enemies also serve specific purposes, with some\noffering greater utility than others. By integrating resources\nfrom mining, farming, and breeding, players can enchant\ntheir equipment, further enhancing their capabilities. Addi-\ntionally, collecting and crafting materials support construc-\ntion, enabling players to create diverse structures. Beyond\npractical functions, such as building secure bases or farms,\nconstructing personalized structures forms a significant as-\npect of the Minecraft experience. Figure 11 illustrates an\nexample of progression: crafting an iron sword\n.\nB.2. Observation and Action Spaces\nObservation. In this paper, observation space of agent is\ncompletely consistent with human players. The agent only\nreceives an RGB image with dimensions of 640 × 360 dur-\ning the gameplay process, including the hotbar, health in-\ndicators, food saturation, and animations of the player’s\nhands. It is worth helping the agent see more clearly in\nextremely dark environments, we have added a night vision\neffect for the agent, which increases the brightness of the\nenvironment during the night.\nAction Spaces. In MineRL [12] environment, agent’s ac-\ntion space is almost similar to human players. It consists of\ntwo parts: the mouse and the keyboard. The keypresses are\nLog\n48.1%\nSeed\n6.0%\nDirt\n6.0%\nStone\n8.2%\nIron\n8.1%\nGold\n8.0%\nDiamond\n7.9%\nRedstone\n7.7%\nFigure 8. Statistical information on MGOA dataset. It contains 8\nAtomic Tasks: ‘Log\n’, ‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’,\n‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’.\nresponsible for controlling the movement of agents, such as\njumping, forward, back, etc. The mouse movements are re-\nsponsible for controlling the perspective of agents and the\ncursor movements when the GUI is opened. The left and\nright buttons of the mouse are responsible for attacking and\nInstruction\nPool\nFiltering\ninteract\nfeedback\nGPT-4\nScript\nEnvironment\ncollect logs\nkeyboard: W\nmouse: [0.0, 1.0]\nMGOA Dataset\nInstruction\nItems\nFigure 9. The pipeline for generating the MGOA dataset. First, we extracted item names from the Minecraft Wiki and employed GPT-\n4 to generate corresponding instructions. These instructions were then provided as input to STEVE-1, enabling it to interact with the\nenvironment to accomplish the tasks. During task execution, each observation was paired with its corresponding action, resulting in the\ncreation of goal-observation-action pairs.\nusing or placing items. In Minecraft, precise mouse move-\nments are important when completing complex tasks that\nneed open inventory or crafting table. In order to achieve\nboth the same action space with MineDojo [10], we abstract\nthe craft and the smelt action into action space. The detailed\naction space is described in Table 6.\nC. MGOA Dataset\nIn Minecraft, there is still a lack of sufficient high-\nquality goal-observation-action pairs to support the train-\ning of Optimus-2.\nTo address this, we propose an au-\ntomated dataset construction process aimed at creating\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndatasets. Through this method. MGOA contains 25,000\nvideos, providing about 30M goal-observation-action pairs.\nIt contains 8 Atomic Tasks across 5 tech levels: ‘Log\n’,\n‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’, ‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’. Note that the Atomic Tasks in MGOA re-\nquire minimal steps and can typically be completed within\n2 ∼3 minutes. For instance, the task ‘Iron\n’ involves min-\ning iron with a stone pickaxe, without the need to gather raw\nmaterials to craft the stone pickaxe. The statistics for the\nMGOA dataset is shown in Figure 8. We provide several ex-\namples of the dataset in the MGOA samples folder within\nthe supplementary materials. We will release this dataset to\ncontribute to the development of open-world agents within\nthe community.\nC.1. Dataset Construction\nPipeline. Inspired by Li et al. [24], we employed a prior\npolicy (STEVE-1 [25] in our work) to perform specific tasks\nin Minecraft, and recorded the corresponding videos and\nactions to generate goal-observation-action pairs. As illus-\ntrated in Figure 9, we employed a custom script to extract\nitem names from the Minecraft Wiki3. Using these item\nnames, we queried GPT-44 with a predefined prompt tem-\nplate to generate task instructions, thereby constructing an\nInstruction Pool. The task instructions from the Instruc-\ntion Pool serve as input to STEVE-1 [25], enabling it to\ninteract with the environment to complete the tasks. During\ntask execution, each frame and corresponding action were\nrecorded and stored. To expedite data generation, we instan-\ntiated multiple policies and used parallelization to quickly\nproduce large amounts of data.\nData Filtering. We judged task success based on environ-\nmental feedback. For example, feedback like “obtained new\nitem, diamond axe” indicated that the task “craft a diamond\naxe” was successfully completed. During the dataset gen-\neration process, we observed a significant amount of low-\nquality video data due to limitations in the policy’s abil-\nity to follow instructions. Examples of low-quality data in-\ncluded task failures or task completion timeouts. To address\nthis issue, we established two filtering criteria to ensure data\nquality: (1) only retaining data from successfully completed\ntasks, and (2) removing data for tasks that lasted longer than\n2 minutes. These criteria allowed us to automatically filter\nout low-quality data, significantly reducing the cost of con-\nstructing the dataset. As a result, we obtained a high-quality\nMGOA dataset consisting of 25,000 samples.\n3https:\/\/minecraft.wiki\/\n4https:\/\/openai.com\/index\/gpt-4-research\/\nC.2. Comparison with Existing Datasets\nPrevious gameplay videos were primarily obtained through\ntwo methods below.\nVideo Platform: For example, MineDojo [10] collected\ngame videos uploaded by human players on platforms such\nas YouTube and Twitter, combining the video content with\ncorresponding titles or subtitles to form video-text pairs.\nHowever, this dataset lacked recorded actions. To address\nthis, VPT [1] used an Inverse Dynamics Model (IDM) to\ngenerate action sequences from the videos. However, the\nactions predicted by the IDM model are only approxima-\ntions, which introduces a potential risk of misalignment be-\ntween the frames and the corresponding actions.\nHuman Contractors: VPT [1] hired human players to\nfreely explore Minecraft and used the frames and actions\nto construct a video-action dataset. However, this dataset\nlacked explicit natural language instructions.\nTo create\ngoal-observation-action pairs, STEVE-1 [25] used GPT-3.5\nto generate specific task descriptions based on the game-\nplay, thereby integrating natural language instructions into\nthe dataset. However, they provide only approximately 32k\naligned goal-observation-action pairs, which remains a rel-\natively scarce amount of data.\nIn addition, some work [32, 43] have utilized GPT-4V\nto generate image captions, task planning, and reflections,\nthereby creating image-text pairs that form instruction-\nfollowing datasets.\nDistinct from the aforementioned datasets, the MGOA\ndataset directly captures agents performing specific tasks,\noffering clear natural language instructions with a one-to-\none correspondence between observations and actions. Fur-\nthermore, through rigorous data filtering, redundant action\nsequences that do not contribute to task completion are ex-\ncluded from MGOA. In addition, compared to the small-\nscale goal-observation-action datasets currently available,\nMGOA offers 25,000 videos, encompassing approximately\n30 million goal-observation-action pairs. This dataset is not\nonly significantly larger but also highly scalable in an auto-\nmated manner.\nD. Training Details\nD.1. Training Pipeline\nOne of the key factors in implementing our proposed\nmethod lies in the efficient alignment of language with the\nobservation-action sequence, and subsequently translating\nlanguage space into the action space. To tackle this prob-\nlem, we adopt a two-phase training approach. First, we\nalign language with the observation-action sequence via be-\nhavior pre-training. Then, we transform the language space\ninto the action space through action fine-tuning.\nBehavior Pre-training: During the pre-training phase, we\nintegrated the Vision-guided Behavior Encoder into the\nTable 7. Hyperparameter setting for pre-training and finetuning.\nHyperparameter\nPre-training\nFine-tuning\nOptimizer\nAdamW\nAdamW\nLearning Rate\n0.0001\n0.00004\nWarmup Steps\n0\n0\nEpochs\n5\n10\nBatch Size\n32\n2048\nNum. Frames\n5M\n80M\nLoRA r\n64\n64\nLoRA alpha\n128\n128\nmodel.\nWe used OpenAI Contractor Dataset [1] and a\nsubset of MGOA as training data, which comprised ap-\nproximately 5,000 videos. To balance efficiency and ef-\nfectiveness, we freeze the visual encoder, then tune the\nVision-guided Behavior Encoder along with a large lan-\nguage model (LoRA [15]). During pre-training, we set the\nlearning rate to 0.0001 and trained for 5 epochs. The hyper-\nparameter settings are shown in Table 7.\nAction Fine-tuning:\nDuring the fine-tuning phase, we\nadapted the general MLLM DeepSeek-VL-1.3B [29] to the\nMinecraft environment, transitioning the model’s output\nspace from language to low-level actions. We fine-tuned\nit using OpenAI Contractor Dataset [1] and MGOA, which\ncomprises approximately 20,000 videos. In this phase, we\nfreeze the Vision-guided Behavior Encoder, visual encoder,\nand large language model (LoRA), and only fine-tuned the\naction head. During fine-tuning, we set the learning rate to\n0.00004 and train for 10 epochs. The hyperparameter set-\ntings are shown in Table 7.\nD.2. Implementation Details\nFor the planner, we follow Li et al. [24], employing Mul-\ntimodal Hybrid Memory empowered GPT-4V for planning\nand reflection. For the policy, we train the GOAP through\nthe above pipeline. All experiments were conducted on 8x\nNVIDIA L40 GPUs. For the MGOA dataset, data collec-\ntion and filtering were conducted in parallel, taking approx-\nimately 7 days.\nTraining required around 2 days, while\ninference and evaluation on atomic tasks, long-horizon\ntasks, and open-ended instruction tasks took approximately\n4 days.\nE. Benchmark\nE.1. Evaluation Tasks\nThe evaluation tasks are divided into three categories:\nAtomic Tasks, Long-horizon Tasks, and Open-ended In-\nstruction Tasks. For each task, the agent’s environment is\n(a) chop a tree to get logs\n(b) mine dirt\n(c) collect seeds\n(d) dig down to mine stone\nFigure 10. Examples of Atomic Task. The agent must follow the instructions to collect resources. These four tasks represent the basic\ncapabilities of the agent. The more resources are collected, the stronger the basic capabilities of the agent will be.\nrandomly initialized each time, and every task is executed\nat least 30 times. For Atomic Tasks, we follow the setting of\nprior work [25, 43], which requires the agent to execute the\ntask within 2 minutes. We then report the average reward\nfor the task, defined as the number of items obtained. For\nOpen-ended Instruction Tasks and Long-horizon Tasks, we\nreport the average success rate (SR) for each task.\nAtomic Tasks. As shown in Figure 10, Atomic Tasks are\nshort-term skills in Minecraft, such as “chop a tree to get\nlogs\n”, “mine dirt\n”, “collect seeds\n”, and “dig down to\nmine stone\n”, etc.\nLong-horizon Tasks.\nAs shown in Figure 11, Long-\nHorizon Tasks are a sequence of Atomic Tasks. For exam-\nple, “craft an iron sword from scratch” requires complet-\ning the atomic tasks of “chop 7 logs”, “craft 21 planks”,\n“craft 5 sticks”, “craft 1 crafting table”, and so on. These\nAtomic Tasks are interdependent, meaning that the failure of\nany single atomic task will result in the failure of the entire\nLong-horizon Task.\nOpen-ended Instruction Tasks. Open-Ended Instruction\nTasks are not limited to predefined text formats; rather, they\ninvolve flexible language directives that prompt the agent\nto accomplish long-horizon tasks. These tasks evaluate the\nagent’s capacity to interpret and execute instructions ex-\npressed in open-ended natural language. We selected Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and\nCompass\nas evaluation tasks. Instruction for each task\nare shown in Table 8, Table 9, Table 10, Table 11 and Table\n12.\nE.2. Baselines\nIn this section, we provide a brief overview of existing\nMinecraft agents and compare them with our proposed\nOptimus-2. Current agents can be broadly categorized into\ntwo types: policy-based agents and planner-policy agents.\nPolicy-based Agents. Policy-based agents [1–3, 10, 25] re-\nfer to those trained through reinforcement learning or imi-\ntation learning, capable of completing atomic tasks within\nMinecraft. However, due to limitations in instruction un-\nderstanding and reasoning abilities, they struggle to accom-\nplish long-horizon tasks.\nPlanner-Policy Agents. Planner-policy agents [20, 24, 32,\n41–43] refer to non-end-to-end architectures that utilize a\nMLLM (Multi-Layered Language Model) as a planner to\ndecompose complex instructions into a sequence of sub-\ngoals executable by a policy. While significant progress has\n(a) Chop 7 logs\n(b) Craft 21 planks\n(c) Craft 5 sticks\n(d) Craft 1 crafting table\n(e) Craft 1 wooden pickaxe\n(f) Mine 11 cobblestone\n(g) Craft 1 furnace\n(h) Craft 1 stone pickaxe\n(i) Dig down more deeper to find iron ore\n(j) Mine 2 iron ores\n(k) Smelt 2 iron ingots\n(l) Craft 1 iron sword\nFigure 11. An example of long-horizon task “crafting an iron sword”. The agent must sequentially complete each atomic task in order to\nsuccessfully craft the iron sword. Failure in any of the atomic tasks will result in the failure of the entire long-horizon task.\nbeen made, the current performance bottleneck stems from\nthe policy’s ability to effectively understand and execute the\nsub-goals generated by the planner.\nComparison with Existing Agents. As a core contribu-\ntion of this work, we propose a novel Goal-Observation-\nAction Conditioned Policy, GOAP. It integrates two key\ncomponents: an Action-Guided Behavior Encoder for mod-\neling observation-action sequences, and an MLLM for\naligning sub-goals with these sequences. Leveraging the\nMLLM’s advanced understanding of open-ended instruc-\ntions, GOAP demonstrates superior instruction-following\ncapabilities compared to existing policies. On top of GOAP,\nthe proposed agent, Optimus-2, exhibits superior perfor-\nmance in long-horizon tasks, outperforming the current\nstate-of-the-art across all seven task groups.\nF. Experimental Results\nIn this section, we report the experimental results of\nOptimus-2 on each Long-horizon task.\nF.1. Results on Long-horizon Task\nIn this section, we report the results of Optimus-2 on each\nLong-horizon Task, with details including task name, num-\nbers of sub-goals, success rate (SR), and eval times. As\nshown in Tables 13 and 14, Optimus-2 demonstrates supe-\nrior performance across all 67 Long-horizon Tasks. Since\nOptimus-2 is randomly initialized in arbitrary environments\nfor each task execution, the experimental results also high-\nlight its generalization capability across diverse environ-\nments.\nG. Case Study\nIn this section, we provide additional cases to illustrate the\ndifferences in the ability of VPT (text) [1], STEVE-1 [25],\nand Optimus-2 to perform Open-ended Instruction Tasks.\nWe provide different open-ended instructions requiring the\nagent to perform tasks across various biomes. As shown in\nFigure 12, Figure 13, and Figure 14, Optimus-2 effectively\ncompletes all tasks, while VPT (text) and STEVE-1 fail due\nto limitations in language understanding and multimodal\nperception capabilities. Moreover, we provide several demo\nvideos of Optimus-2 performing long-horizon tasks in the\nOptimus2 videos folder within the supplementary ma-\nterials.\nTable 8. Open-ended instruction examples of “Craft a torch”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, I need to locate and chop down a couple of trees to collect 4 logs.\n2. Next, I should craft these logs into 13 planks. 3. Then, I need to use some of the\nplanks to craft 3 sticks. 4. I will use some of the planks to craft a crafting table. 5.\nNext, I should craft a wooden pickaxe using some of the planks and sticks. 6. With\nthe wooden pickaxe, I need to find and mine at least 1 coal from a nearby coal deposit\nor cave. 7. Finally, I can use one stick and one coal to craft a torch.\nPlan 2 1. First, I need to find a tree to gather 4 logs. 2. Next, I have to craft the logs\ninto planks. 3. From the planks, I will craft 3 sticks. 4. I need to craft a crafting table\nusing some of the planks. 5. I should then craft a wooden pickaxe to mine coal. 6. I\nwill locate and mine 1 piece of coal using the wooden pickaxe. 7. Finally, I will use\nthe coal and sticks to craft a torch.\nPlan 3 1. I have to find a tree to get 4 logs. 2. Next, I have to craft planks from the\nlogs. 3. Get sticks from some of the planks. 4. I need a crafting table, which requires\nplanks to craft. 5. I should craft a wooden pickaxe using the crafting table. 6. I need\nto find and mine some coal using the wooden pickaxe. 7. Finally, with the stick and\ncoal, I can craft a torch.\nGLM4V\nPlan 1 1. Locate and chop down a tree to gather 4 logs. 2. Convert the logs into 13\nplanks using the crafting menu. 3. Use the planks to craft a crafting table. 4. Craft 3\nsticks from the remaining planks. 5. With the crafting table and sticks, craft a wooden\npickaxe if one is not already available. 6. Use the wooden pickaxe to dig for coal\nor kill some Creepers or zombies for coal drops. 7. Once 1 coal is obtained, use the\ncrafting table to combine it with a stick to craft 1 torch.\nPlan 2 1. Locate and chop down a tree to gather the 4 logs required. 2. Convert the\nlogs into 13 planks using the crafting menu, as this will be the material needed for\ncrafting the crafting table and the wooden pickaxe. 3. Use some of the planks to craft\na crafting table, which is necessary for advanced crafting recipes. 4. Craft a wooden\npickaxe using some of the planks and sticks. The wooden pickaxe will be used to\nmine coal. 5. Gather 3 sticks, which can be obtained from the logs or by crafting them\nfrom planks. 6. Use the wooden pickaxe to mine coal. Only 1 coal is needed to craft\na torch. 7. Once all materials are gathered, open the crafting table and combine 1 coal\nand 1 stick to craft a torch.\nPlan 3 1. Locate and gather 4 logs from the nearest trees. 2. Convert the logs into\n13 planks using the crafting menu. 3. Use some planks to craft a crafting table for\nmore complex recipes. 4. Craft a wooden pickaxe using some planks and sticks to\ngather coal and other resources. 5. Convert some planks into 3 sticks required for\ncrafting torches and the wooden pickaxe. 6. Use the wooden pickaxe to mine coal\nore, ensuring to gather at least 1 coal. 7. With all materials gathered, open the crafting\ntable and craft 1 torch using 1 coal and 1 stick.\nTable 9. Open-ended instruction examples of “Craft a rail”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and punch down a tree to acquire 7 logs. 2. Convert these logs\ninto planks to obtain a total of 25 planks. 3. Use some of these planks to craft 5 sticks.\n4. Create a crafting table using 4 planks. 5. Craft a wooden pickaxe using 3 planks\nand 2 sticks. 6. Use the wooden pickaxe to mine 11 cobblestones. 7. Craft a furnace\nusing 8 cobblestones. 8. Craft a stone pickaxe using 3 cobblestones and 2 sticks. 9.\nUse the stone pickaxe to mine 6 iron ores. 10. Smelt the iron ores in the furnace to\ncreate 6 iron ingots. 11. Finally, craft a rail using the iron ingots and sticks.\nPlan 2 1. I have to find a tree to get 7 logs. 2. Next, I have to craft the logs into 25\nplanks. 3. Craft 5 sticks from the planks. 4. I need to create a crafting table. 5. Use\nthe crafting table to craft a wooden pickaxe. 6. Use the wooden pickaxe to mine 11\npieces of cobblestone. 7. I’ll then craft a furnace using the cobblestone. 8. Craft a\nstone pickaxe with the cobblestone and sticks. 9. Use the stone pickaxe to mine 6\npieces of iron ore. 10. Smelt the iron ore in the furnace to obtain 6 iron ingots. 11.\nFinally, use the crafting table to craft a rail using the iron ingots and sticks.\nPlan 3 1. I have to find a tree to get 7 logs. 2. Next, I need to craft planks from\nthe logs. 3. Get sticks from the planks. 4. Craft a crafting table using some of the\nplanks. 5. Craft a wooden pickaxe with sticks and planks to mine stone. 6. Mine 11\ncobblestone with the wooden pickaxe. 7. Craft a furnace using the cobblestone. 8.\nCraft a stone pickaxe using cobblestone and sticks. 9. Use the stone pickaxe to mine\n6 iron ore. 10. Smelt the iron ore in the furnace to get 6 iron ingots. 11. Finally, use\nthe iron ingots and sticks to craft a rail.\nGLM4V\nPlan 1 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down\na crafting table to access more crafting recipes. 5. Use the crafting table to craft\na wooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones from the\nground or from stone blocks. 7. Craft a furnace using the cobblestones. 8. Upgrade\nthe wooden pickaxe to a stone pickaxe by crafting it at the crafting table with the\ncobblestones. 9. Use the stone pickaxe to mine 6 iron ore from the appropriate rock\nlayers. 10. Smelt the 6 iron ore in the furnace to produce 6 iron ingots. 11. Finally,\nuse the crafting table to craft 1 rail using the 6 iron ingots and 1 stick.\nPlan 2 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu\nto convert the logs into 25 planks. 3. Craft 5 sticks using the planks in the crafting\nmenu. 4. Place a crafting table to enable the crafting of more complex items. 5.\nCraft a wooden pickaxe using some of the planks and sticks. 6. Use the wooden\npickaxe to mine 11 cobblestones from stone outcrops. 7. Construct a furnace using\nthe cobblestones. 8. Upgrade to a stone pickaxe by crafting it with the cobblestones\nand sticks. 9. Use the stone pickaxe to mine 6 iron ore from iron veins. 10. Smelt the\n6 iron ore in the furnace to produce 6 iron ingots. 11. Finally, use the crafting table to\ncraft 1 rail using the iron ingots and sticks.\nPlan 3 1. Find and chop down a tree to gather 7 logs. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down a\ncrafting table to access more crafting options. 5. Craft a wooden pickaxe using some\nof the planks and sticks. 6. Use the wooden pickaxe to mine 11 cobblestones from\nstone. 7. Construct a furnace using some of the cobblestones. 8. Upgrade the wooden\npickaxe to a stone pickaxe by crafting it with the cobblestones. 9. Search for and\nmine 6 iron ore blocks with the stone pickaxe. 10. Smelt the iron ore in the furnace to\nproduce 6 iron ingots. 11. Finally, use the crafting table to craft 1 rail using the iron\ningots and sticks.\nTable 10. Open-ended instruction examples of “Craft a golden shovel”\nPlanner Instruction\nGPT4V\nPlan 1 1. Start by finding and chopping down trees to collect 9 logs. 2. Convert the logs into 35 planks\nusing the crafting interface. 3. Use some of the planks to craft 8 sticks. 4. Create a crafting table using\nsome of the planks. 5. Craft a wooden pickaxe. 6. Mine 19 cobblestone. 7. Craft a furnace using 8 of the\ncobblestones. 8. Craft a stone pickaxe. 9. Mine 3 iron ores using the stone pickaxe. 10. Smelt the iron ore\nin the furnace. 11. Craft an iron pickaxe using sticks and iron ingots. 12. Locate and mine a gold block\nusing the iron pickaxe. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Craft a golden\nshovel using the gold ingot and sticks.\nPlan 2 1. Start by finding and chopping down trees to collect 9 logs. 2. Use the logs to craft planks. 3.\nCraft sticks. 4. Use some of the planks to craft a crafting table. 5. Use the crafting table to craft a wooden\npickaxe. 6. Use the wooden pickaxe to mine 19 pieces of cobblestone. 7. Use some of the cobblestone to\ncraft a furnace. 8. Use the remaining cobblestone to craft a stone pickaxe. 9. Mine iron ore with the stone\npickaxe. 10. Smelt the iron ore in the furnace to obtain 3 iron ingots. 11. Use 2 of the iron ingots and sticks\nto craft an iron pickaxe. 12. Use the iron pickaxe to mine gold ore or find a gold block; you need at least 1\ngold ingot. 13. If mining gold ore, smelt it in the furnace to obtain gold ingots. 14. Use the gold ingot and\nstick to craft a golden shovel.\nPlan 3 1. Find and cut down trees to collect 9 logs. 2. Use the logs to craft 35 planks. 3. Craft 8 sticks\nusing the planks. 4. Use some of the planks to craft a crafting table. 5. Craft a wooden pickaxe using planks\nand sticks. 6. Mine cobblestone until you have 19 pieces. 7. Use some cobblestone and the crafting table to\ncraft a furnace. 8. Craft a stone pickaxe using cobblestone and remaining sticks. 9. Use the stone pickaxe\nto mine 3 pieces of iron ore. 10. Smelt the iron ore in the furnace to get 3 iron ingots. 11. Craft an iron\npickaxe. 12. Use the iron pickaxe to mine gold ore to obtain gold blocks. 13. Use 1 gold ingot to craft a\ngolden shovel. 14. use a gold ingot and sticks to craft a golden shovel using the crafting table.\nGLM4V\nPlan 1 1. Find and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to expand the crafting options. 5. Craft\na wooden pickaxe with the planks and sticks. 6. Cine cobblestone until you have a total of 19. 7. Craft a\nfurnace using 8 cobblestone. 8. Use the furnace to smelt the cobblestone into 19 stone, then use the crafting\ntable to craft a stone pickaxe with the stone and sticks. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks at the crafting table. 12. Use the iron pickaxe to mine a gold block. 13. Smelt the gold block in\nthe furnace to get 1 gold ingot. 14. Finally, use the crafting table to craft the golden shovel using 1 gold\ningot and 2 sticks.\nPlan 2 1. Find and gather 9 logs from nearby trees. 2. Convert the logs into 35 planks using a crafting\ntable. 3. Craft 8 sticks from the planks. 4. Build a crafting table. 5. Use the crafting table to craft a wooden\npickaxe with the planks and sticks. 6. Use the wooden pickaxe to mine 19 cobblestones from stone. 7.\nConstruct a furnace to smelt materials. 8. Craft a stone pickaxe using the cobblestones and sticks. 9. Use\nthe stone pickaxe to mine 3 iron ore from iron deposits. 10. Smelt the 3 iron ore in the furnace to produce\n3 iron ingots. 11. Craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine a\ngold block. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Finally, craft the golden shovel\nusing the gold ingot and stick in the crafting table.\nPlan 3 1. Find and gather 9 logs from nearby trees. 2. Use the crafting menu to convert the logs into 35\nplanks. 3. Craft 8 sticks from the planks. 4. Craft a crafting table using some of the planks. 5. Use the\ncrafting table to craft a wooden pickaxe with some of the sticks and planks. 6. Use the wooden pickaxe to\nmine 19 cobblestones from stone. 7. Craft a furnace using some of the cobblestones. 8. Use the furnace to\nsmelt the cobblestones into 19 stone, then craft a stone pickaxe with the sticks and stone. 9. Mine 3 iron\nore from the appropriate rock faces. 10. Smelt the iron ore in the furnace to produce 3 iron ingots. 11. Use\nthe crafting table to craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine\na gold block. 13. Smelt the gold block in the furnace to produce 1 gold ingot. 14. Finally, use the crafting\ntable to craft the golden shovel with the gold ingot and 2 sticks.\nTable 11. Open-ended instruction examples of “Craft a diamond pickaxe”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some\nof the planks to craft 8 sticks. 4. Create a crafting table using 4 planks. 5. With the crafting table, craft\na wooden pickaxe using planks and sticks. 6. Use the wooden pickaxe to collect at least 11 cobblestones\nfrom stone. 7. With the cobblestones, craft a furnace. 8. Use more cobblestones to craft a stone pickaxe. 9.\nUse the stone pickaxe to mine 3 iron ores. 10. Smelt the iron ores into 3 iron ingots using the furnace. 11.\nCraft an iron pickaxe with iron ingots and sticks. 12. Use the iron pickaxe to mine 3 diamonds. 13. Finally,\ncraft the diamond pickaxe using the diamonds and sticks.\nPlan 2 1. Begin by locating a tree to gather 9 logs. 2. Convert the logs into planks. 3. Use some of the\nplanks to craft sticks. 4. Craft a crafting table to facilitate further crafting. 5. Create a wooden pickaxe to\nstart mining stone. 6. Mine 11 cobblestones. 7. Use the cobblestones to craft a furnace. 8. Craft a stone\npickaxe to advance to mining iron ore. 9. Locate and mine 3 iron ores using the stone pickaxe. 10. Smelt\nthe iron ores in the furnace to produce 3 iron ingots. 11. Use two of the iron ingots and sticks to craft an\niron pickaxe. 12. Locate a diamond ore vein and mine 3 diamonds using the iron pickaxe. 13. Finally, craft\nthe diamond pickaxe with the 3 diamonds and sticks.\nPlan 3 1. I need to find a tree to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some planks to\ncraft 8 sticks. 4. Craft a crafting table using some of the planks. 5. With the crafting table, make a wooden\npickaxe to mine stone. 6. Mine 11 cobblestones using the wooden pickaxe. 7. Craft a furnace using the\ncollected cobblestones. 8. Make a stone pickaxe to efficiently mine iron ore. 9. Find and mine 3 iron ores\nusing the stone pickaxe. 10. Smelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron\npickaxe using some planks, sticks, and the iron ingots. 12. Locate a source of diamonds and use the iron\npickaxe to mine 3 diamonds. 13. Finally, use the crafting table along with the sticks and diamonds to craft\na diamond pickaxe.\nGLM4V\nPlan 1 1. Start by finding trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to make a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ores. 10.\nSmelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine 3 diamonds, which may require exploring caves or mining deep underground with the\niron pickaxe. 13. Finally, use the crafting table to combine the diamonds and the sticks to craft a diamond\npickaxe.\nPlan 2 1. Locate and chop down trees to gather a total of 9 logs. 2. Use the logs to craft 35 planks, as this\nwill be the base material for several other items. 3. Craft 8 sticks from the planks, which will be needed to\ncreate various tools. 4. Craft a crafting table using some of the planks, as this is essential for crafting more\ncomplex items. 5. Use the crafting table to craft a wooden pickaxe with the planks and sticks, which will be\nused to mine cobblestone. 6. Mine 11 cobblestone using the wooden pickaxe. 7. Craft a stone pickaxe with\nthe cobblestone and sticks, as this is more durable and efficient for mining. 8. Construct a furnace using the\ncobblestone, which will be required to smelt ores into ingots. 9. Mine 3 iron ore with the stone pickaxe. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks, as this is necessary to mine diamonds. 12. Find and mine diamond ore with the iron pickaxe\nto gather 3 diamonds. 13. Finally, use the crafting table to craft a diamond pickaxe with the diamonds and\nsticks, completing the task.\nPlan 3 1. Begin by locating trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine diamond ore with the iron pickaxe to obtain 3 diamonds. 13. Finally, use the crafting\ntable to craft a diamond pickaxe with the diamonds.\nTable 12. Open-ended instruction examples of “Craft a compass”\nPlanner Instruction\nGPT4V\nPlan 1 1. Locate a tree and gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks from the\nplanks. 4. Use some planks to craft a crafting table. 5. With the crafting table, craft a wooden pickaxe. 6.\nMine cobblestone until you have 12 pieces. 7. Upgrade your tools by crafting a stone pickaxe. 8. Construct\na furnace using some of the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron\nore in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots. 12. Find and\nmine redstone using the iron pickaxe to obtain 1 redstone dust. 13. Finally, use the remaining iron ingots\nand redstone dust at the crafting table to craft a compass.\nPlan 2 1. Start by finding and chopping down trees to collect 10 logs. 2. Use the logs to craft 38 planks. 3.\nCraft sticks using the planks to get 8 sticks. 4. Craft a crafting table from the planks. 5. Use the crafting\ntable to create a wooden pickaxe. 6. Locate and mine 12 pieces of cobblestone using the wooden pickaxe.\n7. Craft a stone pickaxe from cobblestone and sticks. 8. Use the cobblestone to craft a furnace. 9. Mine\n7 iron ores using the stone pickaxe. 10. Smelt the iron ore in the furnace to get 7 iron ingots. 11. Craft\nan iron pickaxe using iron ingots and sticks. 12. Search for and mine redstone using the iron pickaxe. 13.\nFinally, use the iron ingots and redstone to craft a compass.\nPlan 3 1. First, I need to find a tree and collect 10 logs. 2. Convert the logs into planks, ensuring I get at\nleast 38 planks. 3. Use some of the planks to craft sticks, collecting at least 8 sticks. 4. Use some of the\nplanks to create a crafting table. 5. Craft a wooden pickaxe. 6. Locate cobblestone and mine at least 12\npieces using the wooden pickaxe. 7. Use some cobblestone to craft a furnace. 8. Craft a stone pickaxe for\nimproved mining. 9. Search for iron ore and mine at least 7 using the stone pickaxe. 10. Smelt the iron ore\nin the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using some of the iron ingots and sticks.\n12. Locate a redstone ore and mine it using the iron pickaxe to obtain redstone. 13. Finally, use 4 of the\niron ingots and the redstone to craft a compass at the crafting table.\nGLM4V\nPlan 1 1. Locate and gather 10 logs from nearby trees. 2. Convert the logs into 38 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to access more crafting options. 5. Use\nthe crafting table to make a wooden pickaxe. 6. With the wooden pickaxe, mine 12 cobblestones from\nstone. 7. Craft a stone pickaxe using the cobblestones and the crafting table. 8. Construct a furnace to smelt\nmaterials. 9. Use the stone pickaxe to mine 7 iron ores from the appropriate rock formations. 10. Smelt\nthe 7 iron ores in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe with the iron ingots at the\ncrafting table. 12. Mine for redstone, which can sometimes be found in the ground. 13. Use the crafting\ntable to craft a compass with the iron ingot and redstone.\nPlan 2 1. Locate and chop down trees to gather the required 10 logs. 2. Convert the logs into 38 planks\nusing the crafting menu. 3. Craft 8 sticks from the planks. 4. Use some planks to craft a crafting table.\n5. With the crafting table and the sticks and planks crafted, make a wooden pickaxe. 6. Use the wooden\npickaxe to mine 12 cobblestone. 7. Craft a stone pickaxe using the cobblestone. 8. Construct a furnace\nwith the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron ore in the furnace to\nproduce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots and the crafting table. 12. Search for\nredstone; gather at least 1 redstone. 13. Finally, use the crafting table to combine the redstone with the iron\ningots to craft a compass.\nPlan 3 1. Find and chop down trees to gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks\nfrom the planks. 4. Craft a crafting table using some of the planks. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine cobblestone until you have at least 12. 7. Upgrade the\nwooden pickaxe to a stone pickaxe using the cobblestone. 8. Construct a furnace out of the cobblestone. 9.\nUse the stone pickaxe to mine 7 iron ore. 10. Smelt the 7 iron ore in the furnace to produce 7 iron ingots.\n11. Craft an iron pickaxe using the iron ingots and sticks at the crafting table. 12. Search for redstone; you\nwill need at least 1 piece. 13. Finally, use the crafting table to combine the redstone with the iron ingots to\ncraft a compass.\nTable 13. The results of Optimus-2 on the Wood Group, Stone Group, and Iron Group. SR denotes success rate.\nGroup\nTask\nSub-Goal Num.\nSR\nEval Times\nWood\nCraft a wooden shovel\n6\n100.00\n40\nCraft a wooden pickaxe\n5\n100.00\n30\nCraft a wooden axe\n5\n97.37\n38\nCraft a wooden hoe\n5\n100.00\n30\nCraft a stick\n4\n100\n30\nCraft a crafting table\n3\n93.02\n43\nCraft a wooden sword\n5\n100.00\n30\nCraft a chest\n4\n100.00\n30\nCraft a bowl\n4\n100.00\n30\nCraft a ladder\n4\n100.00\n30\nStone\nCraft a stone shovel\n8\n89.47\n57\nCraft a stone pickaxe\n10\n98.00\n50\nCraft a stone axe\n10\n94.44\n54\nCraft a stone hoe\n8\n95.74\n47\nCraft a charcoal\n9\n85.71\n42\nCraft a smoker\n9\n90.00\n40\nCraft a stone sword\n8\n95.45\n44\nCraft a furnace\n9\n94.44\n36\nCraft a torch\n8\n89.36\n47\nIron\nCraft an iron shovel\n13\n52.08\n48\nCraft an iron pickaxe\n13\n56.00\n50\nCraft an iron axe\n13\n48.15\n54\nCraft an iron hoe\n13\n56.60\n53\nCraft a bucket\n13\n45.10\n51\nCraft a hopper\n14\n54.90\n51\nCraft a rail\n13\n51.02\n49\nCraft an iron sword\n12\n56.52\n46\nCraft a shears\n12\n48.28\n58\nCraft a smithing table\n12\n53.33\n45\nCraft a tripwire hook\n13\n55.56\n45\nCraft a chain\n13\n52.17\n46\nCraft an iron bars\n12\n51.06\n47\nCraft an iron nugget\n12\n54.55\n44\nCraft a blast furnace\n14\n52.27\n44\nCraft a stonecutter\n13\n52.27\n44\nTable 14. The results of Optimus-2 on the Gold group, Diamond Group, Redstone Group, and Armor Group. SR denotes success rate.\nGroup\nTask\nSub Goal Num.\nSR\nEval Times\nGold\nCraft a golden shovel\n16\n8.93\n56\nCraft a golden pickaxe\n16\n11.29\n62\nCraft a golden axe\n16\n8.93\n56\nCraft a golden hoe\n16\n8.96\n67\nCraft a golden sword\n16\n8.20\n61\nSmelt and craft an golden ingot\n15\n9.68\n62\nDiamond\nCraft a diamond shovel\n15\n15.91\n44\nCraft a diamond pickaxe\n15\n11.76\n34\nCraft a diamond axe\n16\n11.00\n36\nCraft a diamond hoe\n15\n15.91\n44\nCraft a diamond sword\n15\n11.11\n36\nDig down and mine a diamond\n15\n11.42\n35\nCraft a jukebox\n15\n13.15\n38\nRedstone\nCraft a piston\n16\n28.33\n60\nCraft a redstone torch\n16\n27.69\n65\nCraft an activator rail\n18\n25.81\n62\nCraft a compass\n23\n28.36\n67\nCraft a dropper\n16\n30.30\n66\nCraft a note block\n16\n25.40\n63\nArmor\nCraft shield\n14\n45.16\n62\nCraft iron chestplate\n14\n43.86\n57\nCraft iron boots\n14\n40.35\n57\nCraft iron leggings\n14\n8.57\n35\nCraft iron helmet\n14\n47.46\n56\nCraft diamond helmet\n17\n9.09\n33\nCraft diamond chestplate\n17\n7.89\n38\nCraft diamond leggings\n17\n5.41\n37\nCraft diamond boots\n17\n12.50\n40\nCraft golden helmet\n17\n13.89\n36\nCraft golden leggings\n17\n12.20\n41\nCraft golden boots\n17\n10.26\n39\nCraft golden chestplate\n17\n10.00\n40\nAgent\nInstruction: I want to get some logs to craft wooden sword, what should I do first?\nSuccess\n❌\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n✔\nFigure 12. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to get some logs\nto craft wooden sword, what should I do first?”. Existing policies are limited by their instruction comprehension abilities and thus fail to\ncomplete the task, whereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I need coal for heating. What should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 13. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need coal for heating.\nWhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I want to collect some seeds, Can you help me?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 14. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to collect some\nseeds, Can you help me?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task,\nwhereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nReferences\n[1] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampe-\ndro, and Jeff Clune. Video pretraining (vpt): Learning to act\nby watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639–24654, 2022. 2,\n3, 4, 5, 6, 7, 8, 15\n[2] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao\nLiang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. In\nProceedings of the IEEE\/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13734–13744, 2023. 2\n[3] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji\nLiu, and Yitao Liang. Groot: Learning to follow instructions\nby watching gameplay videos. In The Twelfth International\nConference on Learning Representations, 2023. 1, 2, 3, 4, 5,\n6\n[4] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and\nLiqiang Nie. Lion: Empowering multimodal large language\nmodel with dual-level visual knowledge. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 26540–26550, 2024. 1\n[5] Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gong-\nwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu,\nShuai Wang, et al. Spa-bench: A comprehensive benchmark\nfor smartphone agent evaluation. In ICLR, 2025. 1\n[6] Yihong Chen, Yue Cao, Han Hu, and Liwei Wang. Mem-\nory enhanced global-local aggregation for video object de-\ntection. In Proceedings of the IEEE\/CVF conference on com-\nputer vision and pattern recognition, pages 10337–10346,\n2020. 3\n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: towards general-purpose\nvision-language models with instruction tuning. In Proceed-\nings of the 37th International Conference on Neural Infor-\nmation Processing Systems, pages 49250–49267, 2023. 3\n[8] Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang,\nand Zongqing Lu. Clip4mc: An rl-friendly vision-language\nmodel for minecraft.\narXiv preprint arXiv:2303.10571,\n2023. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 4, 8\n[10] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar.\nMinedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems, 35:\n18343–18362, 2022. 2, 3, 4, 8, 5\n[11] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui\nZhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao,\nHanyu Lai, et al.\nChatglm: A family of large language\nmodels from glm-130b to glm-4 all tools.\narXiv preprint\narXiv:2406.12793, 2024. 6\n[12] William H Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan\nSalakhutdinov. Minerl: A large-scale dataset of minecraft\ndemonstrations. arXiv preprint arXiv:1907.13440, 2019. 5,\n2\n[13] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2\n[14] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xue-\nfei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam\nLim. Ma-lmm: Memory-augmented large multimodal model\nfor long-term video understanding. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13504–13514, 2024. 3, 5\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 4\n[16] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao,\nand Li Yuan. Chat-univi: Unified visual representation em-\npowers large language models with image and video un-\nderstanding. In Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 13700–\n13710, 2024. 3\n[17] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao,\nAshwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan\nFoster, Grace Lam, Pannag Sanketi, et al.\nOpenvla: An\nopen-source vision-language-action model. arXiv preprint\narXiv:2406.09246, 2024. 8\n[18] Sangho Lee, Jinyoung Sung, Youngjae Yu, and Gunhee\nKim.\nA memory network approach for story-based tem-\nporal summarization of 360° videos.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 1410–1419, 2018. 3\n[19] Sangmin Lee, Hak Gu Kim, Dae Hwi Choi, Hyung-Il Kim,\nand Yong Man Ro. Video prediction recalling long-term mo-\ntion context via memory alignment learning. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3054–3063, 2021. 3\n[20] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou,\nYu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and\nJifeng Dai. Auto mc-reward: Automated dense reward de-\nsign with large language models for minecraft. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 16426–16435, 2024. 1, 3, 5\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 3\n[22] Wei Li, Bing Hu, Rui Shao, Leyang Shen, and Liqiang Nie.\nLion-fs: Fast & slow video-language thinker as online video\nassistant. arXiv preprint arXiv:2503.03663, 2025. 3\n[23] Xiaojie Li, Jianlong Wu, Shaowei He, Shuo Kang, Yue Yu,\nLiqiang Nie, and Min Zhang. Fine-grained key-value mem-\nory enhanced predictor for video representation learning. In\nProceedings of the ACM International Conference on Multi-\nmedia, page 2264–2274. ACM, 2023. 1\n[24] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dong-\nmei Jiang, and Liqiang Nie.\nOptimus-1:\nHybrid mul-\ntimodal memory empowered agents excel in long-horizon\ntasks. arXiv preprint arXiv:2408.03615, 2024. 1, 2, 3, 4,\n5, 6\n[25] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and\nSheila McIlraith. Steve-1: A generative model for text-to-\nbehavior in minecraft. Advances in Neural Information Pro-\ncessing Systems, 2023. 1, 2, 3, 4, 5, 6, 7, 15\n[26] Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui,\nWenkai Fang, Yuxuan Zheng, Tongya Zheng, and Mingli\nSong. Odyssey: Empowering agents with open-world skills.\narXiv preprint arXiv:2407.15325, 2024. 3\n[27] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan,\nHuayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu.\nRdt-1b: a diffusion foundation model for bimanual manipu-\nlation. arXiv preprint arXiv:2410.07864, 2024. 8\n[28] Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang\nChen, Shu Liu, Zongqing Lu, and Jiaya Jia.\nRl-gpt: In-\ntegrating reinforcement learning and code-as-policy. arXiv\npreprint arXiv:2402.19299, 2024. 3\n[29] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li,\nYaofeng Sun, et al. Deepseek-vl: towards real-world vision-\nlanguage understanding. arXiv preprint arXiv:2403.05525,\n2024. 6, 4\n[30] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video\nunderstanding via large vision and language models. arXiv\npreprint arXiv:2306.05424, 2023. 3\n[31] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet\nKohli.\nZero-shot task generalization with multi-task deep\nreinforcement learning. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning, pages 2661–2670.\nPMLR, 2017. 2\n[32] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu\nSheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: A\nmulti-modal open-ended embodied system in minecraft via\nactive perception. arXiv preprint arXiv:2312.07472, 2023.\n1, 3, 4, 5\n[33] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen.\nMulti-adversarial discriminative deep domain generalization\nfor face presentation attack detection.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10023–10031, 2019. 3\n[34] Rui Shao, Tianxing Wu, and Ziwei Liu.\nDetecting and\ngrounding multi-modal media manipulation. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6904–6913, 2023. 1\n[35] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Zi-\nwei Liu. Detecting and grounding multi-modal media manip-\nulation and beyond. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2024. 3\n[36] Rui Shao, Tianxing Wu, Liqiang Nie, and Ziwei Liu.\nDeepfake-adapter: Dual-level adapter for deepfake detec-\ntion. International Journal of Computer Vision, 2025. 3\n[37] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and\nLiqiang Nie. Mome: Mixture of multimodal experts for gen-\neralist multimodal large language models. Advances in neu-\nral information processing systems, 37:42048–42070, 2025.\n1\n[38] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng\nZhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo,\nTian Ye, Yanting Zhang, et al. Moviechat: From dense token\nto sparse memory for long video understanding. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 18221–18232, 2024. 3\n[39] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of Machine Learning Research, 9\n(86):2579–2605, 2008. 8\n[40] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023. 3\n[41] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xi-\naojian Ma, and Yitao Liang.\nDescribe, explain, plan\nand select: Interactive planning with large language mod-\nels enables open-world multi-task agents.\narXiv preprint\narXiv:2302.01560, 2023. 1, 6, 5\n[42] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong\nZheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-\ntask agents with memory-augmented multimodal language\nmodels. arXiv preprint arXiv:2311.05997, 2023. 1, 3, 6\n[43] Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao\nZhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, and Yi-\ntao Liang. Omnijarvis: Unified vision-language-action to-\nkenization enables open-world instruction following agents.\narXiv preprint arXiv:2407.00114, 2024. 2, 3, 4, 5, 6\n[44] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi\nFan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.\nMemvit: Memory-augmented multiscale vision transformer\nfor efficient long-term video recognition. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13587–13597, 2022. 3\n[45] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and\nXiaochun Cao. Cat: Enhancing multimodal large language\nmodel to answer questions in dynamic audio-visual scenar-\nios.\nIn European Conference on Computer Vision, pages\n146–164. Springer, 2024. 1\n[46] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. In Proceedings of the 2023 Conference on Em-\npirical Methods in Natural Language Processing: System\nDemonstrations, pages 543–553, 2023. 3\n[47] Haoyu Zhang, Meng Liu, Yuhong Li, Ming Yan, Zan Gao,\nXiaojun Chang, and Liqiang Nie. Attribute-guided collab-\norative learning for partial person re-identification.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n45(12):14144–14160, 2023. 1\n[48] Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, Yaowei\nWang, and Liqiang Nie. Multi-factor adaptive vision selec-\ntion for egocentric video question answering. In Proceedings\nof the 41st International Conference on Machine Learning,\npages 59310–59328. PMLR, 2024. 3\n[49] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi\nFeng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-\nbased real-time understanding for long video streams. arXiv\npreprint arXiv:2406.08085, 2024. 3\n[50] Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili\nGuan, and Liqiang Nie. Token-level correlation-guided com-\npression for efficient multimodal document understanding.\narXiv preprint arXiv:2407.14439, 2024. 3\n[51] Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi,\nShengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang,\nand Gaoang Wang. See and think: Embodied agent in virtual\nenvironment. arXiv preprint arXiv:2311.15209, 2023. 3\n[52] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-\njie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiao-\ngang Wang, et al. Ghost in the minecraft: Generally capable\nagents for open-world enviroments via large language mod-\nels with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023. 3\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy.pdf"}
{"title":"MineStudio: A Streamlined Package for Minecraft AI Agent Development","authors":"Shaofei Cai, Zhancun Mu, Kaichen He, Bowei Zhang, Xinyue Zheng, Anji Liu, Yitao Liang","summary":"Minecraft has emerged as a valuable testbed for embodied intelligence and\nsequential decision-making research, yet the development and validation of\nnovel agents remains hindered by significant engineering challenges. This paper\npresents MineStudio, an open-source software package designed to streamline\nembodied policy development in Minecraft. MineStudio represents the first\ncomprehensive integration of seven critical engineering components: simulator,\ndata, model, offline pretraining, online finetuning, inference, and benchmark,\nthereby allowing users to concentrate their efforts on algorithm innovation. We\nprovide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at\nhttps:\/\/github.com\/CraftJarvis\/MineStudio.","url":"http:\/\/arxiv.org\/abs\/2412.18293v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.18293v2","published":1735030903000,"comment":null,"pdf_text":"MineStudio: A Streamlined Package for\nMinecraft AI Agent Development\nShaofei Cai†1, Zhancun Mu†1, Kaichen He1, Bowei Zhang1, Xinyue Zheng1, Anji Liu2 and Yitao Liang B1\n1Peking University, 2University of California, Los Angeles, All authors are affiliated with Team CraftJarvis\nMinecraft has emerged as a valuable testbed for embodied intelligence and sequential decision-making\nresearch, yet the development and validation of novel agents remains hindered by significant engineering\nchallenges. This paper presents MineStudio, an open-source software package designed to stream-\nline embodied policy development in Minecraft. MineStudio represents the first comprehensive\nintegration of seven critical engineering components: simulator, data, model, offline pretraining, online\nfinetuning, inference, and benchmark, thereby allowing users to concentrate their efforts on the algo-\nrithm innovation. We provide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at https:\/\/github.com\/CraftJarvis\/MineStudio.\n1. Introduction\nAs an open-world 3D sandbox game, Minecraft has garnered widespread acclaim for its high degree of\nfreedom and rich playability. In recent years, with the rapid advancement of general-purpose artificial\nintelligence, Minecraft has gradually emerged as a key testbed for research on embodied intelligence\nand decision-making control (Baker et al., 2022; Fan et al., 2022; Guss et al., 2019; Lifshitz et al.,\n2023). Although the community has introduced a variety of environments (Fan et al., 2022; Guss\net al., 2019; Johnson et al., 2016), datasets (Baker et al., 2022; Fan et al., 2022; Guss et al., 2019),\ntraining algorithms and policy models (Baker et al., 2022; Cai et al., 2023, 2024; Lifshitz et al.,\n2023), and benchmarking methods (Fan et al., 2022; Lin et al., 2023) for this platform, a substantial\nengineering gap remains in the development and validation of novel agents.\nMinecraft’s complexity has led to varied task customizations, hindering progress tracking across\nthe community. For example, Dreamer V3 (Hafner et al., 2023) simplified the environment to destroy\nblocks with a single attack, while Voyager (Wang et al., 2023) relied on the rule-based Mineflayer\ncontroller (PrismarineJS, 2013) to interact with the environment. Moreover, experimenting in\nMinecraft is harder than in other AI domains. For example, despite OpenAI’s 160-million-frame\ndataset, the lack of flexible data structures hampers efficient storage and retrieval. Reinforcement\nlearning shows promise in tackling complex tasks (Baker et al., 2022), but mainstream frameworks\noften lack memory-policy support (Liang et al., 2018; Raffin et al., 2021) and crash safeguards (Raffin\net al., 2021). These require researchers to make great engineering efforts before validating an idea.\nTo address these challenges, we present MineStudio, an open-source software package for\nefficiently developing embodied agents in Minecraft. We also integrate many existing efforts, including\ndata (Baker et al., 2022), environment (Guss et al., 2019; Johnson et al., 2016), and model (Baker\net al., 2022; Cai et al., 2023, 2024; Lifshitz et al., 2023), into a unified framework and present\nthem to users in a concise manner. To minimize redundant engineering work, we have introduced\nseveral key improvements and optimizations: (1) support flexible custom environment configurations;\n(2) a flexible and training-efficient trajectory data structure; (3) a unified policy model template\nwith multiple built-in Minecraft agent algorithms; (4) a simplified offline pre-training process; (5)\na high-performance distributed online fine-tuning pipeline tailored to the Minecraft platform; (6)\na parallel inference pipeline; and (7) the integration of an MCU (Lin et al., 2023) benchmark to\nCorresponding author(s): Yitao Liang\n† indicates co-first author\n{caishaofei, muzhancun, hekaichen, zhangbowei}@stu.pku.edu.cn, zhengxinyue@bigai.ai, liuanji@cs.ucla.edu, yitaol@pku.edu.cn\narXiv:2412.18293v2  [cs.AI]  25 Dec 2024\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nFigure 1 | MineStudio enables users to address classic requirements such as offline pretraining, online\nfine-tuning, system evaluation, and dataset generation with minimal coding effort. Users only need to configure\nthe model component with a small amount of PyTorch code. Each module in the workflow is fully customizable,\nallowing users to configure settings or extend functionality through inheritance and overrides as needed.\nenable fair comparisons. With MineStudio, users can not only conduct interactive evaluations\nof existing mainstream Minecraft agents but also quickly implement a model file using just a few\nlines of PyTorch (Paszke et al., 2019) code, enjoying a fully integrated workflow from pre-training\nthrough finetuning to evaluation. The source code is publicly available at GitHub, accompanied by\ncomprehensive documentation and extensive tutorials to facilitate rapid onboarding.\n2. MineStudio\nThe MineStudio is an efficient toolkit for developing Minecraft AI agents, encompassing the full\nworkflow from data preparation and training to evaluation. It offers a flexible interface design that\nnot only includes various out-of-the-box features but also allows users to tailor the toolkit to their\nspecific requirements. In doing so, it maximizes flexibility while minimizing engineering overhead.\nFigure 1 provides an overview of the pipeline, and subsequent paragraphs detail each module.\nSimulator. This component implements a hook-based Minecraft wrapper that offers a high degree\nof customization. By inheriting from the MinecraftCallback class, users can tailor the environ-\nment in various ways, including (but not limited to) monitoring rendering framerate, issuing cheat\ncommands, simulating rapid resets, logging episodes, generating reward functions, and overriding\nobservations. To further reduce user workload, we provide a set of commonly used callbacks out of the\nbox. In particular, PlayCallback loads a graphical user interface and enables switching between\nmanual and agent-controlled modes, allowing users to interactively inspect model performance. In\naddition, we have integrated a bunch of tricks to improve rendering speed, thereby increasing the\nefficiency of model evaluation, data collection, and reinforcement learning.\nData. This component introduces a flexible and efficient data structure for handling offline trajectory\ndata. Different modalities within the trajectory are segmented into clips and stored independently\nin LMDB files while preserving their temporal relationships. This design seeks to balance storage\nsize and video decoding efficiency, enabling users to retrieve trajectory segments based on semantic\nlabels rapidly. To facilitate the training of models requiring long-term memory, we have implemented\na distributed batch sampler that supports continuous reading of lengthy trajectory sequences. Ad-\nditionally, we provide data format conversion scripts, making it convenient for users to incorporate\ntheir own collected trajectory data, as well as a suite of visualization tools to assist with troubleshoot-\ning. Our framework comes with the largest Contractor Dataset (Baker et al., 2022) available in the\nMinecraft domain, complemented by our pre-processed frame-level semantic segmentation data.\n2\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nThese resources can be utilized directly for training ROCKET-1 models (Cai et al., 2024).\nModel. This component provides a unified template for policy models along with action and value\nheads specifically designed for the Minecraft domain, allowing users to focus solely on their model\narchitecture design. We have pre-integrated models such as VPTs (Baker et al., 2022), STEVE-1\n(Lifshitz et al., 2023), GROOT-1 (Cai et al., 2023), and ROCKET-1 (Cai et al., 2024). The standardized\ninterface functions ensure that policies can be seamlessly integrated with other MineStudio\nmodules, enabling efficient training and inference as well as fair comparisons.\nOffline Training. This component builds upon the PyTorch Lightning (Falcon and team, 2019)\nframework to deliver an enhanced Trainer module. It incorporates dedicated mechanisms for handling\npolicy memory (e.g., TransformerXL (Dai et al., 2019)) and integrates seamlessly with the distributed\nbatch sampler in the MineStudio data module, thereby enabling training on ultra-long trajectories.\nIn addition, we provide a proven set of hyperparameter configurations—covering aspects such as\nwarmup steps, optimizers, and learning rates—while also allowing flexible customization through\nobjective callbacks. This approach maximizes user flexibility while minimizing their overhead.\nOnline Training. This component implements the KL-constrained Proximal Policy Optimization\nalgorithm introduced in Baker et al. (2022). The code is designed for integration with our model and\nsimulator. It has been optimized to deal with long episodes and handle the instability of the Minecraft\nenvironment. Additionally, we provide a set of hyperparameters that have been tested to achieve high\ntraining efficiency and strong performance across various tasks. We hope this resource serves as an\neasy starting point and lowers the barriers to reinforcement learning research in Minecraft.\nInference. This component provides a Ray-based (Moritz et al., 2018) inference framework for\nMineStudio, to support distributed inference. The framework is designed to consist of three\nparts: generator, filter and recorder, forming an asynchronous inference pipeline for easily\nevaluating the performance of different agents. The generator part, equipped with an agent creator\nand an environment creator, generates trajectories in batches. Each produced trajectory is immediately\npassed to a filter for post-processing and then summarized and stored by the recorder. By\ncustomizing the filter and recorder, users can effortlessly conduct comprehensive evaluations\nof policy checkpoints. Furthermore, this pipeline allows for efficient data synthesis, which, when\ncombined with the data module’s conversion scripts, enables a closed-loop data workflow.\nBenchmark. This component evaluates agent performance in MineStudio environments. It\nsupports a variety of tasks such as building, mining, and crafting, and offers both simple and\nchallenging game modes to test agents under different levels of difficulty. The framework includes an\nautomatic evaluation pipeline that leverages Vision-Language Models to analyze task videos, and\nprovides batch task execution capabilities to run multiple tasks simultaneously and record completion\nvideos. Additionally, it offers a quick benchmarking tool that simplifies the process of task execution\nand evaluation, enabling researchers to compare different agent models efficiently.\n3. Comparison to Existing Interest of Minecraft\nTo highlight the contributions of MineStudio, we compare its features with those of other promi-\nnent Minecraft development frameworks, such as MineRL (Guss et al., 2019), MineDojo (Fan et al.,\n2022), and Mineflayer (PrismarineJS, 2013), as shown in table 1. Although these frameworks offer\nvaluable testbeds for Minecraft, they lack a seamless pipeline that integrates building, training, and\nevaluating agents, particularly when it comes to environment customization. While Baker et al. (2022)\nextends MineRL by offering data loading and training code, its dataloader is primarily designed for\ndemonstration purposes and is inefficient, while the RL training process remains closed-source and\n3\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nTable 1 | Comparison of features across Minecraft development frameworks.\nFeature\nMineRL\nMineDojo\nMineflayer\nMineStudio\nObservation\/Action Space\noriginal\nmodified\nmodified\noriginal\nDataset Features\nannotated state-action pairs\nof human demonstrations\nmultimodal data\nscraped from Internet\nN\/A\nefficient data structure\nfor both storing and loading\nCustomizing Environments\nlimited\neasy\nN\/A\neasy\nCreating Agents\ndifficult\ndifficult\neasy\neasy\nTraining Agents\nlimited\nlimited\nN\/A\npipelined, supporting both\nonline and offline training\nBenchmarking Agents\n11 tasks\n3000+ tasks\nN\/A\nunlimited with MCU (Lin et al., 2023)\nEvaluating Agents\nN\/A\nreward only\nN\/A\ncustomizable metrics with\ndistributed framework\nBaseline Implementations\nN\/A\nN\/A\nN\/A\nsupports a family of SOTA baselines\ndifficult to replicate. Moreover, the relatively slow speed of the MineRL and MineDojo simulators\nhampers RL training, adding considerable engineering challenges to Minecraft development. LLM-\nbased agents (Wang et al., 2023; Zhu et al., 2023) oversimplify the Minecraft problem by relying\non Mineflayer APIs, whereas our goal is to build embodied agents that mirror the observation and\ncontrol interface of human players. The motivation behind MineStudio is to unify Minecraft\nagent development through an efficient and reliable training template. By integrating MCU (Lin et al.,\n2023), our distributed inference framework, and state-of-the-art (SOTA) baseline implementations,\nwe provide a standardized evaluation paradigm for Minecraft agents.\n4. Conclusions\nIn this work, we introduced MineStudio, a comprehensive and streamlined framework designed\nto advance the development of AI agents in Minecraft. By addressing the significant engineering\nchallenges inherent in creating embodied policies for open-world environments, MineStudio\nbridges the gap between conceptual algorithmic innovations and practical implementation. Its\nmodular design integrates crucial components, such as a flexible simulator wrapper, efficient data\nstructures, pre-integrated models, and high-performance training pipelines, enabling researchers to\nfocus on algorithmic advancements rather than engineering overhead.\nReferences\nB. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro,\nand J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos.\nArXiv, abs\/2206.11795, 2022.\nURL https:\/\/api.semanticscholar.org\/CorpusID:\n249953673.\nS. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang. Groot: Learning to follow instructions by\nwatching gameplay videos. In The Twelfth International Conference on Learning Representations,\n2023.\nS. Cai, Z. Wang, K. Lian, Z. Mu, X. Ma, A. Liu, and Y. Liang. Rocket-1: Mastering open-world\ninteraction with visual-temporal context prompting. arXiv preprint arXiv: 2410.17856, 2024.\nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov.\nTransformer-xl: Attentive\nlanguage models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, Jan 2019. doi: 10.18653\/v1\/p19-1285. URL http:\n\/\/dx.doi.org\/10.18653\/v1\/p19-1285.\n4\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nW. Falcon and T. P. L. team. Pytorch lightning. GitHub. Note: https:\/\/github.com\/Lightning-AI\/lightning,\n2019.\nL. J. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and\nA. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowl-\nedge. ArXiv, abs\/2206.08853, 2022. URL https:\/\/api.semanticscholar.org\/CorpusID:\n249848263.\nW. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. M. Veloso, and R. Salakhutdinov. Minerl:\nA large-scale dataset of minecraft demonstrations. In International Joint Conference on Artificial\nIntelligence, 2019. URL https:\/\/api.semanticscholar.org\/CorpusID:199000710.\nD. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023.\nM. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial intelligence\nexperimentation. In International Joint Conference on Artificial Intelligence, 2016. URL https:\n\/\/api.semanticscholar.org\/CorpusID:9953039.\nE. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. E. Gonzalez, M. I. Jordan, and\nI. Stoica. RLlib: Abstractions for distributed reinforcement learning. In International Conference on\nMachine Learning (ICML), 2018.\nS. Lifshitz, K. Paster, H. Chan, J. Ba, and S. A. McIlraith. Steve-1: A generative model for text-to-\nbehavior in minecraft. ArXiv, abs\/2306.00937, 2023. URL https:\/\/api.semanticscholar.\norg\/CorpusID:258999563.\nH. Lin, Z. Wang, J. Ma, and Y. Liang. Mcu: A task-centric framework for open-ended agent evaluation\nin minecraft. arXiv preprint arXiv:2310.08367, 2023.\nP. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, M. Elibol, Z. Yang, W. Paul, M. I.\nJordan, et al. Ray: A distributed framework for emerging {AI} applications. In 13th USENIX\nsymposium on operating systems design and implementation (OSDI 18), pages 561–577, 2018.\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in\nneural information processing systems, 32, 2019.\nPrismarineJS. Prismarinejs\/mineflayer: Create minecraft bots with a powerful, stable, and high level\njavascript api. https:\/\/github.com\/PrismarineJS\/mineflayer, 2013.\nA. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-baselines3: Reliable\nreinforcement learning implementations. Journal of Machine Learning Research, 22(268):1–8, 2021.\nURL http:\/\/jmlr.org\/papers\/v22\/20-1364.html.\nG. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. J. Fan, and A. Anandkumar. Voyager: An\nopen-ended embodied agent with large language models. ArXiv, abs\/2305.16291, 2023. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:258887849.\nX. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, Y. Qiao, Z. Zhang,\nand J. Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large\nlanguage models with text-based knowledge and memory. ArXiv, abs\/2305.17144, 2023. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:258959262.\n5\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MineStudio: A Streamlined Package for Minecraft AI Agent Development.pdf"}
{"title":"GROOT: Learning to Follow Instructions by Watching Gameplay Videos","authors":"Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"We study the problem of building a controller that can follow open-ended\ninstructions in open-world environments. We propose to follow reference videos\nas instructions, which offer expressive goal specifications while eliminating\nthe need for expensive text-gameplay annotations. A new learning framework is\nderived to allow learning such instruction-following controllers from gameplay\nvideos while producing a video instruction encoder that induces a structured\ngoal space. We implement our agent GROOT in a simple yet effective\nencoder-decoder architecture based on causal transformers. We evaluate GROOT\nagainst open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the\nhuman-machine gap as well as exhibiting a 70% winning rate over the best\ngeneralist agent baseline. Qualitative analysis of the induced goal space\nfurther demonstrates some interesting emergent properties, including the goal\ncomposition and complex gameplay behavior synthesis. The project page is\navailable at https:\/\/craftjarvis-groot.github.io.","url":"http:\/\/arxiv.org\/abs\/2310.08235v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.08235v2","published":1697110261000,"comment":null,"pdf_text":"GROOT: Learning to Follow Instructions by\nWatching Gameplay Videos\nShaofei Cai1, Bowei Zhang1, Zihao Wang1, Xiaojian Ma3, Anji Liu2 and Yitao Liang1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis,\nWe study the problem of building a controller that can follow open-ended instructions in open-world\nenvironments. We propose to follow reference videos as instructions, which offer expressive goal\nspecifications while eliminating the need for expensive text-gameplay annotations. A new learning\nframework is derived to allow learning such instruction-following controllers from gameplay videos\nwhile producing a video instruction encoder that induces a structured goal space. We implement our\nagent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers.\nWe evaluate GROOT against open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap\nas well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis. The project page is available at https:\n\/\/craftjarvis-groot.github.io.\nFigure 1 | Through the cultivation of extensive gameplay videos, GROOT has grown a rich set of skill fruits\n(number denotes success rate; skills shown above do not mean to be exhaustive; kudos to our artist Haowei).\n1. Introduction\nDeveloping human-level embodied agents that can solve endless tasks in open-world environments,\nsuch as Minecraft (Fan et al., 2022; Johnson et al., 2016), has always been a long-term goal pursued\nin AI. Recent works have explored using Large Language Models (LLMs) to generate high-level plans,\nwhich guide the agent to accomplish challenging long-horizon tasks (Wang et al., 2023a,b; Zhu\net al., 2023). However, a major gap between these LLM-based agents and generalist agents that can\ncomplete endless amounts of tasks is the capability of their low-level controllers, which map the plans\nto motor commands. Recently developed controllers are only capable of completing a predefined and\nnarrow set of programmatic tasks (Baker et al., 2022; Cai et al., 2023; Lin et al., 2021), which hinders\nLLM-based planning agents from unleashing their full potential. We attribute the limitation of these\nlow-level controllers to how the goal is specified. Specifically, existing controllers use task indicator\nCorresponding author(s): Yitao Liang\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>,Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2310.08235v2  [cs.AI]  29 Nov 2023\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(Yu et al., 2019), future outcome (Chen et al., 2021; Lifshitz et al., 2023), and language (Brohan\net al., 2022) to represent the goal. While it is easy to learn a controller with some of these goal\nspecifications, they may not be expressive enough for diverse tasks. Taking future outcome goals as an\nexample, an image of a desired house clearly lacks procedural information on how the house was built.\nOne exception is language, but learning a controller that can receive language goals is prohibitively\nexpensive as it requires a huge number of trajectory-text pairs with text that precisely depicts the full\ndetails of the gameplay, therefore preventing them from scaling up to more open-ended tasks.\nHaving observed the limitations of goal specification in the prior works, this paper seeks to find\na balance between the capacity of goal specification and the cost of controller learning. Concretely,\nwe propose to specify the goal as a reference gameplay video clip. While such video instruction is\nindeed expressive, there are two challenges: 1) How can the controller understand the actual goal\nbeing specified as the video itself can be ambiguous, i.e. a goal space or video instruction encoder\nhas to be learned; 2) How to ultimately map such goal to actual motor commands? To this end, we\nintroduce a learning framework that simultaneously produces a goal space and a video instruction\nfollowing controller from gameplay videos. The fundamental idea is casting the problem as future\nstate prediction based on past observations:\n• The predicting model needs to identify which goal is being pursued from the past observations,\nwhich requires a good goal space (induced by a video instruction encoder);\n• Since the transition dynamics model is fixed, a policy that maps both the state and the recognized\ngoal to action is also needed by the predicting model when rolling the future state predictions.\nEffectively, this results in the goal space and control policy we need. We introduce a variational learning\nobjective for this problem, which leads to a combination of a cloning loss and a KL regularization loss.\nBased on this framework, we implement GROOT, an agent with an encoder-decoder architecture to\nsolve open-ended Minecraft tasks by following video instructions. The video encoder is a non-causal\ntransformer that extracts the semantic information expressed in the video and maps it to the latent\ngoal space. The controller policy is a decoder module implemented by a causal transformer, which\ndecodes the goal information in the latent space and translates it into a sequence of actions in the\ngiven environment states in an autoregressive manner.\nTo comprehensively evaluate an agent’s mastery of skills, we designed a benchmark called\nMinecraft SkillForge. The benchmark covers six common Minecraft task groups: collect, build,\nsurvive, explore, tool, and craft, testing the agent’s abilities in resource collection, structure\nbuilding, environmental understanding, and tool usage, in a total of 30 tasks. We calculate Elo\nratings among GROOT, several counterparts, and human players based on human evaluations. Our\nexperiments showed that GROOT is closing the human-machine gap and outperforms the best\nbaseline by 150 points (or 70% winning rate) in an Elo tournament system. Our qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis.\nTo sum up, our main contributions are as follows:\n• Start by maximizing the log-likelihood of future states given past ones, we have discovered the\nlearning objectives that lead to a good goal space and ultimately the instruction-following controller\nfrom gameplay videos. It provides theoretical guidance for the agent architecture design and model\noptimization.\n• Based on our proposed learning framework, we implemented a simple yet efficient encoder-\ndecoder agent based on causal transformers. The encoder is responsible for understanding the goal\ninformation in the video instruction while the decoder as the policy emits motor commands.\n• On our newly introduced benchmark, Minecraft SkillForge, GROOT is closing the human-machine\n2\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ngap and surpassing the state-of-the-art baselines by a large margin in the overall Elo rating com-\nparison. GROOT also exhibits several interesting emergent properties, including goal composition\nand complex gameplay behavior synthesis.\n2. Preliminaries and Problem Formulation\nReinforcement Learning (RL) concerns the problem in which an agent interacts with an environment\nat discrete time steps, aiming to maximize its expected cumulative reward (Espeholt et al., 2018; Mnih\net al., 2015; Schulman et al., 2017). Specifically, the environment is defined as a Markov Decision\nProcess (MDP) ⟨S, A, R, P, 𝑑0⟩, where S is the state space, A is the action space, R : S × A →ℝis\nthe reward function, P : S × A →S is the transition dynamics, and 𝑑0 is the initial state distribution.\nOur goal is to learn a policy 𝜋(𝑎|𝑠) that maximizes the expected cumulative reward 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑡], where\n𝛾∈(0, 1] is a discount factor.\nIn goal-conditioned RL (GCRL) tasks, we are additionally provided with a goal 𝑔∈G (Andrychowicz\net al., 2017; Cai et al., 2023; Ding et al., 2019; Jing et al., 2020, 2021; Liu et al., 2022; Yang et al.,\n2019). And the task becomes learning a goal-conditioned policy 𝜋(𝑎|𝑠, 𝑔) that maximizes the expected\nreturn 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑔\n𝑡], where 𝑟𝑔\n𝑡is the goal-specific reward achieved at time step 𝑡. Apart from being\na new type of RL task, GCRL has been widely studied as a pre-training stage toward conquering\nmore challenging environments\/tasks (Aytar et al., 2018b; Baker et al., 2022; Zhang et al., 2022).\nSpecifically, suppose we are provided with a good goal-condition policy, the goal can be viewed as a\nmeta-action that drives the agent to accomplish various sub-tasks, which significantly simplifies tasks\nthat require an extended horizon to accomplish. Further, when equipped with goal planners, we can\nachieve zero- or few-shot learning on compositional tasks that are beyond the reach of RL algorithms\n(Gong et al., 2023; Huang et al., 2022; Wang et al., 2023a,b; Zhu et al., 2023).\nAt the heart of leveraging such benefits, a key requirement is to have a properly-defined goal\nspace that (i) has a wide coverage of common tasks\/behaviors, and (ii) succinctly describes the task\nwithout including unnecessary information about the state. Many prior works establish goal spaces\nusing guidance from other modalities such as language (Cai et al., 2023; Hong et al., 2020; Stone\net al., 2023) or code (Huang et al., 2023; Wang et al., 2023a). While effective, the requirement on\nlarge-scale trajectory data paired with this auxiliary information could be hard to fulfill in practice.\nInstead, this paper studies the problem of simultaneously learning a rich and coherent goal space\nand the corresponding goal-conditioned policy, given a pre-trained inverse dynamic model and raw\ngameplay videos, i.e. sequences of states {𝑠(𝑖)\n0:𝑇}𝑖collected using unknown policies.\n3. Goal Space Discovery via Future State Prediction\nThis section explains our learning framework: discovering a “good” goal space as well as a video\ninstruction following the controller through the task of predicting future states given previous ones.\nWe start with an illustrative example in Minecraft (Johnson et al., 2016). Imagine that an agent is\nstanding inside a grassland holding an axe that can be used to chop the tree in front of them. Suppose\nin the gameplay video, players either go straight to chop the tree or bypass it to explore the territory.\nIn order to predict future frames, it is sufficient to know (i) which goal (chop tree or bypass tree) is\nbeing pursued by the agent, and (ii) what will happen if the agent chooses a particular option (i.e.\ntransition dynamics). Apart from the latter information that is irrelevant to the past observations, we\nonly need to capture the goal information, i.e. whether the agent decides to chop the tree or bypass\nthe tree. Therefore, the task of establishing a comprehensive yet succinct goal space can be interpreted\nas predicting future states while conditioning on the transition dynamics of the environment.\n3\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFormally, our learning objective is to maximize the log-likelihood of future states given past\nones: log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡). Define 𝑔as a latent variable conditioned on past states (think of it as the\npotential goals the agent is pursuing given past states), the evidence lower-bound of the objective\ngiven variational posterior 𝑞𝜙(𝑔|𝑠0:𝑇) is the following (see Appendix A for the derivation of this and\nthe following equations):\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑔\n𝑝𝜃(𝑠𝑡+1:𝑇, 𝑔|𝑠0:𝑡)\n≥𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇) [log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔)] −𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n,\nwhere 𝐷KL(·∥·) denotes the KL-divergence. Next, we break down the first term (i.e. 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔))\ninto components contributed by the (unknown) goal-conditioned policy 𝜋𝜃(𝑎|𝑠, 𝑔) and the transition\ndynamics 𝑝𝜃(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡) :\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔) =\n𝑇\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) · 𝑝𝜃(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\n≥\n𝑇\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝𝜃(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) + 𝐶\n\u0003\n,\nwhere the constant 𝐶contains terms that depend solely on the environment dynamics and are\nirrelevant to what we want to learn (i.e. the goal space and the goal-conditioned policy). Bring it\nback to the original objective, we have\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇),𝑎𝜏∼𝑝𝜃(·|𝑠0:𝜏+1) [log 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔)]\n|                                                         {z                                                         }\nbehaviour cloning\n−\n𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n|                               {z                               }\ngoal space constraint (KL regularization)\n,\nwhere 𝑞𝜙(·|𝑠0:𝑇) is implemented as a video encoder that maps the whole state sequence into the latent\ngoal space. 𝑝𝜃(·|𝑠0:𝜏+1) is the inverse dynamic model (IDM) that predicts actions required to achieve\na desired change in the states, which is usually a pre-trained model, details are in Appendix C. Thus,\nthe objective can be explained as jointly learning a video encoder and a goal-controller policy through\nbehavior cloning under succinct goal space constraints.\n4. GROOT Architecture Design and Training Strategy\nThis section illustrates how to create an agent (we call it GROOT) that can understand the semantic\nmeaning of a reference video and interact with the environment based on the aforementioned learning\nframework. According to the discussion in Section 3, the learnable parts of GROOT include the\nvideo encoder and the goal-conditioned policy. Recently, Transformer (Vaswani et al., 2017) has\ndemonstrated effectiveness in solving sequential decision-making problems (Brohan et al., 2022; Chen\net al., 2021; Parisotto et al., 2019). Motivated by this, we implement GROOT with transformer-based\nencoder-decoder architecture, as shown in Figure 2. The video encoder is a non-causal transformer\nthat extracts semantic information and generates goal embeddings. The policy is a causal transformer\ndecoder that receives the goal embeddings as the instruction and autoregressively translates the state\nsequence into a sequence of actions. Next, we describe how each module is constructed together with\nthe training strategy.\n4\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nVideo Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑠!\n𝑠\"\n𝑎#\n𝑎$\n𝑎\"\nVideo Encoder\nDecoder as Policy\n𝑠̂#\n𝑠̂%\n𝑠̂\"\n𝑠#\n𝑠%\n𝑠!\n𝑎#\n𝑎%\n𝑎!\n𝑠%\n𝑠!\nBehavior Cloning\nReference Video\nStates\nStates\nRollout Observations\n(a) Training\n(b) Inference\n𝑐̂#\n𝑐̂&\nLearnable Tokens\nLearned Tokens\n1.0\n0.5\n0.0\n𝜇\n𝒒𝒈𝟏:𝑵𝒔𝟏:𝑻)\nsample\n𝑠#\n𝑐#\n𝑐&\n𝑐#\n𝑐&\n𝑐̂#\n𝑐̂&\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n1.0\n0.5\n0.0\n𝜇\n𝒑𝒈𝟏:𝑵𝒔𝟏:𝒕)\n𝑫𝑲𝑳\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n𝑠#\n⋯\n𝝁(⋅)\nFigure 2 | Our GROOT agent architecture. Left: In the training stage, a video encoder (non-causal\ntransformer) learns to extract the semantic meaning and transfer the video (state sequence) into the goal\nembedding space. A goal-conditioned policy (causal transformer) is learned to predict actions following the\ngiven instructions. We learn the agent using behavior cloning under a KL constraint. Right: During the\ninference, any reference video is passed into the video encoder to generate the goal embeddings that drive the\npolicy to interact with the environment.\n4.1. Video Encoder\nThe video encoder includes a Convolutional Neural Network (CNN) to extract spatial information\nfrom image states 𝑠1:𝑇and a non-causal transformer to capture temporal information from videos.\nSpecifically, we use a CNN backbone to extract visual embeddings {𝑥1:𝑇} for all frames. Additionally,\nmotivated by Devlin et al. (2019); Dosovitskiy et al. (2020), we construct a set of learnable embeddings\n(or summary tokens), represented as {𝑐1:𝑁}, to capture the semantic information present in the video.\nThe visual embeddings and summary tokens are passed to a non-causal transformer, resulting in the\noutput corresponding to the summary tokens as {ˆ𝑐1:𝑁}\n𝑥1:𝑇←Backbone(𝑠1:𝑇),\nˆ𝑐1:𝑁←Transformer([𝑥1:𝑇, 𝑐1:𝑁]).\n(1)\nSimilar to VAE (Kingma & Welling, 2013), we assume that the latent goal space follows a Gaussian\ndistribution, hence we use two fully connected layers, 𝜇(·) and 𝜎(·), to generate the mean and\nstandard deviation of the distribution, respectively. During training, we use the reparameterization\ntrick to sample a set of embeddings {𝑔1:𝑁} from the distribution, where 𝑔𝑡∼N (𝜇(ˆ𝑐𝑡), 𝜎(ˆ𝑐𝑡)). During\ninference, we use the mean of the distribution as the goal embeddings, i.e. 𝑔𝑡←𝜇(ˆ𝑐𝑡).\n4.2. Decoder as Policy\nTo introduce our policy module, we start with VPT (Baker et al., 2022), a Minecraft foundation model\ntrained with standard behavioral cloning. It is built on Transformer-XL (Dai et al., 2019) that can\nleverage long-horizon historical states and predict the next action seeing the current observation.\nHowever, the vanilla VPT architecture does not support instruction input. To condition the policy\non goal embeddings, we draw the inspiration from Flamingo (Alayrac et al., 2022), that is, to insert\ngated cross-attention dense layers into every Transformer-XL block. The keys and values in these layers\nare obtained from goal embeddings, while the queries are derived from the environment states\nˆ𝑥(𝑙)\n1:𝑡←GatedXATTN(kv = 𝑔1:𝑁, q = 𝑥(𝑙−1)\n1:𝑡\n; 𝜃𝑙),\n𝑥(𝑙)\n1:𝑡←TransformerXL(qkv = ˆ𝑥(𝑙)\n1:𝑡; 𝜃𝑙),\nˆ𝑎𝑡←FeedForward(𝑥(𝑀)\n𝑡\n),\n(2)\n5\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nwhere the policy reuses the visual embeddings extracted by the video encoder, i.e. 𝑥(0)\n1:𝑡= 𝑥1:𝑡, the\npolicy consists of 𝑀transformer blocks, 𝜃𝑙is the parameter of 𝑙-th block, ˆ𝑎𝑡is the predicted action.\nSince our goal space contains information about how to complete a task that is richer than previous\nlanguage-conditioned policy (Cai et al., 2023; Lifshitz et al., 2023), the cross-attention mechanism is\nnecessary. It allows the GROOT to query the task progress from instruction information based on\npast states, and then perform corresponding behaviors to complete the remaining progress.\n4.3. Training and Inference\nThe training dataset can be a mixture of Minecraft gameplay videos and offline trajectories. For\nthose videos without actions, an inverse dynamic model (Baker et al., 2022) can be used to generate\napproximate actions. Limited by the computation resources, we truncated all the trajectories into\nsegments with a fixed length of 𝑇without using any prior. We denote the final dataset as D =\n{(𝑥1:𝑇, 𝑎1:𝑇)}𝑀, where 𝑀is the number of trajectories. We train GROOT in a fully self-supervised\nmanner while the training process can be viewed as self-imitating, that is, training GROOT jointly\nusing behavioral cloning and KL divergence loss\nL(𝜃, 𝜙) = 𝔼(𝑠,𝑎)∼D\n\"\n𝜆𝐵𝐶\n∑︁\n𝑡\n−log 𝜋𝜃(𝑎𝑡|𝑠1:𝑡, 𝑔) + 𝜆𝐾𝐿\n∑︁\n𝜏\n𝐷𝐾𝐿\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝜏)\u0001\n#\n,\n(3)\nwhere 𝜆𝐵𝐶, 𝜆𝐾𝐿are tradeoff coefficients, 𝑞𝜙is a posterior visual encoder, 𝑝𝜃is a prior video encoder\nwith the same architecture, 𝑔∼𝑞𝜙(·|𝑠0:𝑇). More details are in the Appendix D.\n5. Result\n5.1. Performance on Mastering Minecraft Skills\nMinecraft SkillForge Benchmark. In order to comprehensively evaluate the mastery of tasks by\nagents in Minecraft, we created a diverse benchmark called Minecraft SkillForge. It covers 30 tasks\nfrom 6 major categories of representative skills in Minecraft, including collect, explore, craft,\ntool, survive, and build. For example, the task “dig three down and fill one up” in the build\ncategory asks the agent to first dig three blocks of dirt, then use the dirt to fill the space above; The\ntask of “building a snow golem” ( ) requires the agent to sequentially stack 2 snow blocks (\n) and\n1 carved pumpkin (\n). We put the details of this benchmark in the Appendix H. Apart from some\nrelatively simple or common tasks such as “collect wood” and “hunt animals”, other tasks require the\nagent to have the ability to perform multiple steps in succession.\nWe compare GROOT with the following baselines: (a) VPT (Baker et al., 2022), a foundation\nmodel pre-trained on large-scale YouTube data, with three variants: VPT (fd), VPT(bc), and VPT(rl),\nindicating vanilla foundation model, behavior cloning finetuned model, and RL finetuned model; (b)\nSTEVE-1 (Lifshitz et al., 2023), an instruction-following agent finetuned from VPT, with two variants:\nSTEVE-1 (visual) and STEVE-1 (text) that receives visual and test instructions. More details are in\nAppendix F.1. It is worth noting that GROOT was trained from scratch.\nHuman Evaluation with Elo Rating. We evaluated the relative strength of agents by running an\ninternal tournament and reporting their Elo ratings, as in Mnih et al. (2015). Before the tournament,\neach agent is required to generate 10 videos of length 600 on each task. Note that, all the reference\nvideos used by GROOT are generated from another biome to ensure generalization. Additionally,\nwe also invited 3 experienced players to do these tasks following the same settings. After the video\ncollection, we asked 10 players to judge the quality of each pair of sampled videos from different\n6\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Elo Rating Comparison\n(b) Winning Rate of GROOT vs. Baselines\n(c) Success Rate Comparison\nFigure 3 | Results on Minecraft SkillForge benchmark. Left: Tournament evaluation of GROOT assessed\nby human players. GROOT performs better than state-of-the-art Minecraft agent STEVE-1. A 150-score gap\ncorresponds to a 70% probability of winning. Middle: Winning rate of GROOT v.s. other agents on specific\ntask categories. Colors from red to blue denote a decrease in the winning rate. Apart from the human player,\nGROOT surpasses all other baselines. Right: Success rate on 9 representative tasks. GROOT champions\nprocess-oriented tasks, such as dig three and fill one (\n) and build snow golems ( ).\nagents. Considering the diversity of tasks, we designed specific evaluation criteria for every task to\nmeasure the quality of rollout trajectories. For example, in the task of “build snow golem”, we rank\nthe completion degree of the task in ascending order: no blocks placed, one type of block placed,\ntwo types of blocks placed, and snow golem built successfully. After 1500 comparisons, the Elo\nrating converged as in Figure 3 (left). Although there is a large performance gap compared with\nhuman players, GROOT has significantly surpassed the current state-of-the-art STEVE-1 series and\ncondition-free VPT series on the overall tasks. Additional details are in Appendix G.\nIn Figure 3 (middle), we compare GROOT with other baselines in winning rate on six task groups.\nWe found that except for the performance on craft tasks, where STEVE-1 (visual) outperforms our\nmodel, GROOT achieves state-of-the-art results. In particular, GROOT greatly outperforms other\nbaselines by a large margin on build and tool. For build, the goal space needs to contain more\ndetailed procedural information, which is the disadvantage of methods that use future outcomes as\nthe goal. Moreover, such tasks are distributed sparsely in the dataset, or even absent in the dataset,\nwhich requires the agent to have strong generalization ability. As for craft group, GROOT is not\nsuperior enough, especially on the “crafting table” task. We attribute this to the wide task distribution\nin the dataset. Thus the future outcomes can prompt STEVE-1 to achieve a high success rate.\nProgrammatic Evaluation. To quantitatively compare the performance of the agents, we selected\n9 representative tasks out of 30 and reported the success rate of GROOT, STEVE-1 (visual), and\nVPT (bc) on these tasks in Figure 3 (right). We found that, based on the success rate on tasks such\nas dye and shear sheep(\n), enchat sword (\n), smelt food (\n), use bow (\n), sleep\n(\n), and lead animals (\n), GROOT has already reached a level comparable to that of human\nplayers (100%). However, the success rates for build snow golems ( ) and build obsidian\n(\n) tasks are only 60% and 50%. By observing the generated videos, we found that GROOT cannot\nprecisely identify the items in Hotbar (such as buckets, lava buckets, snow blocks, and pumpkin\nheads), resulting in a low probability of switching to the correct item. STEVE-1 also has the same\nproblem. This may be due to the current training paradigm lacking strong supervisory signals at the\nimage level. Future work may introduce auxiliary tasks such as vision-question answering (VQA) to\nhelp alleviate this phenomenon. Details are in Appendix F.3.\n7\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Random Initialized\n(b) GROOT w\/o KL\n(c) GROOT w\/ KL\n(d) Synthesized Videos\nFigure 4 | t-SNE visualization of the goal space. Each color corresponds to a specific video category. (Left):\nSpace of randomly initialized video encoder. All the videos are entangled together. Middle: Space of GROOT\ntrained with self-supervised learning w\/ and w\/o KL regularization, respectively. The videos are clustered\nbased on their semantics. Visualization shows the subtle differences between the two. Right: Synthesized\nvideos using concatenation manner. The concatenated videos lay on the position between the source videos.\n5.2. Properties of Learned Goal Space\nThis section studies the properties of learned goal space. We used the t-SNE algorithm (van der Maaten\n& Hinton, 2008) to visualize the clustering effect of reference videos encoded in goal space, as in Figure\n4. We select 7 kinds of videos, including craft items, combat enemies, harvest crops,\nhunt animals, chop trees, trade with villagers, and mine ores. These videos are\nsampled from the contractor data (Baker et al., 2022) according to the meta information (details\nare in Appendix F.2). Each category contains 1k video segments. As a control group, in Figure 4\n(left), we showed the initial goal space of the video encoder (with a pre-trained EfficientNet-B0 (Tan\n& Le, 2019) as the backbone) before training. We found that the points are entangled together. After\nbeing trained on offline trajectories, as in Figure 4 (middle), it well understands reference videos and\nclusters them according to their semantics. This proves that it is efficient to learn behavior-relevant\ntask representations using our self-supervised training strategy. The clustering effect is slightly better\nwith KL regularization, though the difference is not very significant. Inevitably, there are still some\nvideos from different categories entangled together. We attribute this to the possibility of overlap\nin the performed behaviors of these videos. For example, chop trees and harvest crops both\nrely on a sequential of “attack” actions.\nCondition on Concatenated Videos. We also study the possibility of conditioning the policy\non concatenated videos. First, we collect 3 kinds of source videos, including chop trees, hunt\nanimals, and trade with villagers. We randomly sampled two videos from sources of chop\ntrees and hunt animals, downsampled and concatenated them into a synthetic video, denoted as\n[chop trees, hunt animals]. By the same token, we can obtain [hunt animals, trade\nwith villagers]. We visualize these videos together with the source videos in Figure 4 (right). We\nfound that the source videos lie far away from each other while the concatenated videos are distributed\nbetween their source videos. Based on this intriguing phenomenon, we infer that concatenated videos\nmay prompt GROOT to solve both tasks simultaneously. To verify this, we evaluate GROOT on\nthree kinds of reference videos, i.e. chop trees, hunt animals, and [chop trees, hunt\nanimals]. We launched GROOT in the forest and in the animal plains, respectively. The collected\nwood and killed mobs are reported in Figure 5. We found that although the concatenated video may\nnot be as effective as raw video in driving an agent to complete a single task (60% of the performance\nof raw video), it does possess the ability to drive the agent to perform multiple tasks. This is an\nimportant ability. As discussed in Wang et al. (2023b), sometimes the high-level planner will propose\nmultiple candidate goals, it will be efficient if the low-level controller can automatically determine\n8\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nchop\nhunt\nconcat\nWood Collected\n11.0\n1.5\n6.5\nchop\nhunt\nconcat\nMobs Killed\n0.5\n2.2\n1.5\nFigure 5 | Comparison on using raw and concate-\nnated reference videos as conditions. Left: Col-\nlected wood in the forest biome. Right: Killed mobs\nin the plains biome. “concat” denotes the reference\nvideo is [chop trees, hunt animals]. Statis-\ntics are measured over 10 episodes.\nw\/ KL\nw\/o KL\nSeagrass Collected\n3.7\n1.8\nw\/ KL\nw\/o KL\nWood Collected\n11.0\n7.3\nw\/ KL\nw\/o KL\nArrows Fired\n10.7\n9.3\nFigure 6 | Ablation study on KL loss.\nAfter being\njointly trained with KL loss, GROOT can collect 2×\nmore seagrass (\n) underwater and 1.5× wood (\n)\nin the forest while the difference is not as impressive\non the use bow (\n) task. Statistics are measured\nover 10 episodes.\ndiamonds distribute in this level\nGROOT\nSTEVE-1\ncondition changed\ncondition changed\ndiamonds distribute in this level\n~𝟏𝟔%\nstuck in the bedrock\nFigure 7 | Results on solving challenging obtain diamond task. The vertical dashed lines represent the\ntime when a certain item is first obtained. Left: GROOT first dug down to the depth of 12 and then mined\nhorizontally to obtain diamonds with an average success rate of 16%. Right: STEVE-1 quickly dug down to the\nspecific depth but struggled to maintain its height.\nwhich to accomplish based on the current observation.\nAblation on KL Divergence Loss. To investigate the role of KL loss in training, we evaluated\nGROOT (w\/ KL) and its variant (w\/o KL) on three tasks: collect seagrass (\n), collect\nwood (\n), and use bow (\n). As shown in Figure 6, we found that introducing the constraint of\nKL loss improved agent performance by 2× and 1.5× in the first two tasks, whereas there was no\nsignificant effect in the use bow task. This may be because the first two tasks require the agent to\ngeneralize the corresponding skills to different terrains (e.g. locating trees in the environment for\ncollecting wood and sinking to specific locations for collecting seagrass). Therefore, it puts higher\ndemands on the agent’s ability to generalize in the goal space, and this is exactly the role played by\nthe KL loss. The use bow task is relatively simple in comparison because it only requires charging\nand shooting the arrow, without considering environmental factors.\n5.3. Combining Skills for Long-horizon Tasks\nIn this section, we explore whether GROOT can combine skills to solve long-horizon tasks, which is\nkey to its integration with a high-level planner. Taking the task of mining diamonds as an example,\nprior knowledge is that diamond ores are generally distributed between the 7th and 14th floors\nunderground, and the probability of appearing in other depths is almost zero. Therefore, the agent\nneeds to first dig down to the specified depth (12) and then maintain horizontal mining. To achieve\nthis, we designed two reference videos, each 128 frames long. One describes the policy of starting\n9\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfrom the surface and digging down, and the other demonstrates the behaviors of horizontal mining.\nWe show an example in Figure 7 (left). In the beginning, GROOT quickly digs down to the specified\ndepth and then switches to horizontal mining mode. It maintains the same height for a long time and\nfound diamonds at 11k steps. In addition, we compared STEVE-1 (visual) under the same setting in\nFigure 7 (right). After switching to the horizontal mining prompt, STEVE-1 maintains its height for a\nshort time before it stuck in the bedrock layer (unbreakable in survival mode), greatly reducing the\nprobability of finding diamonds. This indicates that our goal space is expressive enough to instruct\nthe way of mining, and the policy can follow the instructions persistently and reliably. In contrast,\nSTEVE-1, which relies on future outcomes as a condition, was unable to maintain its depth, despite\nattempts at various visual prompts. We conducted 25 experiments each on GROOT and STEVE-1,\nwith success rates of 16% and 0% for finding diamonds. Additional details are in the Appendix F.4.\n6. Related Works\nPre-train Policy on Offline Data. Pre-training neural networks on web-scale data has been demon-\nstrated as an effective training paradigm in Nature Language Processing (Brown et al., 2020) and\nComputer Vision (Kirillov et al., 2023). Inspired by this, researchers tried to transfer the success\nto the field of decision-making from pre-training visual representations and directly distilling the\npolicy from offline data. As the former, Aytar et al. (2018a); Bruce et al. (2023) leveraged temporal\ninformation present in videos as the supervision signal to learn visual representations. The represen-\ntations are then used to generate intrinsic rewards for boosting downstream policy learning, which\nstill requires expensive online interactions with the environment. Chen et al. (2021); Schmidhuber\n(2019) leveraged scalable offline trajectories to train optimal policy by conditioning it on cumulated\nrewards. Laskin et al. (2022) proposed to learn an in-context policy improvement operator that\ncan distill an RL algorithm in high data efficiency. Reed et al. (2022) learned a multi-task agent\nGato by doing behavior cloning on a large-scale expert dataset. By serializing task data into a flat of\nsequence, they use the powerful transformer architecture to model the behavior distribution. However,\nthese methods either require elaborated reward functions or explicit task definitions. This makes\nit hard to be applied to open worlds, where tasks are infinite while rewards are lacking. Another\ninteresting direction is to use pre-trained language models for reasoning and vision language models\nfor discrimination, to guide the policy in life-long learning in the environment (Di Palo et al., 2023).\nCondition Policy on Goal Space. Researchers have explored many goal modalities, such as\nlanguage (Khandelwal et al., 2021), image (Du et al., 2021), and future video (Xie et al., 2023), to\nbuild a controllable policy. Brohan et al. (2022) collected a large-scale dataset of trajectory-text pairs\nand trained a transformer policy to follow language instructions. Despite the language being a natural\ninstruction interface, the cost of collecting paired training data is expensive. As a solution, Majumdar\net al. (2022) sorted to use hindsight relabeling to first train a policy conditioned on the target image,\nthen aligned text to latent image space, which greatly improves training efficiency. Lifshitz et al.\n(2023) moved a big step on this paradigm by replacing the target image with a 16-frame future video\nand reformulating the modality alignment problem into training a prior of latent goal given text.\nBuild Agents in Minecraft. As a challenging open-world environment, Minecraft is attracting an\nincreasing number of researchers to develop AI agents on it, which can be divided into plan-oriented\n(Wang et al., 2023a,b) and control-oriented methods (Baker et al., 2022; Cai et al., 2023; Lifshitz\net al., 2023) based on their emphasis. Plan-oriented agents aim to reason with Minecraft knowledge\nand decompose the long-horizon task into sub-tasks followed by calling a low-level controller. Control-\noriented works follow the given instructions and directly interact with the environments using low-level\nactions (mouse and keyboard). Baker et al. (2022) pre-trained the first foundation model VPT in\nMinecraft using internet-scale videos. Although it achieves the first obtaining diamond milestone by\n10\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfine-tuning with RL, it does not support instruction input. Lifshitz et al. (2023) created the first agent\nthat can solve open-ended tasks by bridging VPT and MineCLIP (Fan et al., 2022). However, its goal\nspace is not expressive enough and prevents it from solving multi-step tasks.\n7. Limitation and Conclusion\nAlthough GROOT has demonstrated powerful capabilities in expressing open-ended tasks in the\nform of video instructions, training such a goal space remains highly challenging. We found that\nGROOT is quite sensitive to the selection of reference videos, which we attribute to the fact that\nthe goal space trained from an unsupervised perspective may not be fully aligned with the human\nintention for understanding the semantics of the reference video. Therefore, it would be a promising\nresearch direction in the future to use SFT (supervised fine-tuning, Sanh et al. (2021)) and RLHF\n(Ziegler et al., 2019) to align the pre-trained goal space with human preference.\nIn conclusion, we propose a paradigm for learning to follow instructions by watching gameplay\nvideos. We prove that video instruction is a good form of goal space that not only expresses open-ended\ntasks but can be trained through self-imitation (once the IDM is available to label pseudo actions for raw\ngameplay videos). Based on this, we built an encoder-decoder transformer architecture agent named\nGROOT in Minecraft. Without collecting any text-video data, GROOT demonstrated extraordinary\ninstruction-following ability and crowned the Minecraft SkillForge benchmark. Additionally, we also\nshowed its potential as a planner downstream controller in the challenging obtain diamond task.\nWe believe that this training paradigm can be generalized in other complex open-world environments.\n11\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for\nfew-shot learning. ArXiv, abs\/2204.14198, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:248476411.\nMarcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. ArXiv,\nabs\/1707.01495, 2017. URL https:\/\/api.semanticscholar.org\/CorpusID:3532908.\nYusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyun Wang, and Nando de Freitas. Playing\nhard exploration games by watching youtube. In Neural Information Processing Systems, 2018a.\nURL https:\/\/api.semanticscholar.org\/CorpusID:44061126.\nYusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyun Wang, and Nando de Freitas. Playing\nhard exploration games by watching youtube. In Neural Information Processing Systems, 2018b.\nURL https:\/\/api.semanticscholar.org\/CorpusID:44061126.\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton,\nRaul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled\nonline videos. ArXiv, abs\/2206.11795, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:249953673.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian\nIchter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan C. Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha\nManjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch,\nJornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed,\nJaspiar Singh, Sumedh Anand Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Van-\nhoucke, Steve Vega, Quan Ho Vuong, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna\nZitkovich. Rt-1: Robotics transformer for real-world control at scale. ArXiv, abs\/2212.06817, 2022.\nURL https:\/\/api.semanticscholar.org\/CorpusID:254591260.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. ArXiv, abs\/2005.14165, 2020. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:218971783.\nJake Bruce, Ankit Anand, Bogdan Mazoure, and Rob Fergus. Learning about progress from ex-\nperts.\nIn International Conference on Learning Representations, 2023.\nURL https:\/\/api.\nsemanticscholar.org\/CorpusID:259298702.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. 2023 IEEE\/CVF\n12\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 13734–13744, 2023. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:256194112.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srini-\nvas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.\nIn Neural Information Processing Systems, 2021. URL https:\/\/api.semanticscholar.org\/\nCorpusID:235294299.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a fixed-length context.\nIn Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, Jan 2019.\ndoi:\n10.18653\/v1\/p19-1285. URL http:\/\/dx.doi.org\/10.18653\/v1\/p19-1285.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding.\nArXiv, abs\/1810.04805, 2019.\nURL\nhttps:\/\/api.semanticscholar.org\/CorpusID:52967399.\nNorman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, and\nMartin Riedmiller. Towards a unified agent with foundation models. In Workshop on Reincarnating\nReinforcement Learning at ICLR 2023, 2023.\nYiming Ding, Carlos Florensa, Mariano Phielipp, and P. Abbeel. Goal-conditioned imitation learn-\ning. ArXiv, abs\/1906.05838, 2019. URL https:\/\/api.semanticscholar.org\/CorpusID:\n189762519.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. ArXiv, abs\/2010.11929, 2020. URL https:\/\/api.semanticscholar.org\/CorpusID:\n225039882.\nHeming Du, Xin Yu, and Liang Zheng. Vtnet: Visual transformer network for object goal naviga-\ntion. ArXiv, abs\/2105.09447, 2021. URL https:\/\/api.semanticscholar.org\/CorpusID:\n234790212.\nLasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala: Scalable\ndistributed deep-rl with importance weighted actor-learner architectures. ArXiv, abs\/1802.01561,\n2018. URL https:\/\/api.semanticscholar.org\/CorpusID:3645060.\nLinxi (Jim) Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, An-\ndrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.\nMinedojo: Building open-\nended embodied agents with internet-scale knowledge. ArXiv, abs\/2206.08853, 2022. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:249848263.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-\nChun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv\npreprint arXiv:2309.09971, 2023.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela M. Veloso,\nand Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. In Interna-\ntional Joint Conference on Artificial Intelligence, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:199000710.\n13\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nYicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould.\nVln-bert: A\nrecurrent vision-and-language bert for navigation. 2021 IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 1643–1653, 2020. URL https:\/\/api.semanticscholar.\norg\/CorpusID:227228335.\nWenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models. In Conference on Robot Learning, 2022. URL https:\n\/\/api.semanticscholar.org\/CorpusID:250451569.\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models. ArXiv, abs\/2307.05973,\n2023. URL https:\/\/api.semanticscholar.org\/CorpusID:259837330.\nMingxuan Jing, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Chao Yang, Bin Fang, and Huaping Liu.\nReinforcement learning from imperfect demonstrations under soft expert guidance. In Proceedings\nof the AAAI conference on artificial intelligence, volume 34, pp. 5109–5116, 2020.\nMingxuan Jing, Wenbing Huang, Fuchun Sun, Xiaojian Ma, Tao Kong, Chuang Gan, and Lei Li.\nAdversarial option-aware hierarchical imitation learning. In International Conference on Machine\nLearning, pp. 5097–5106. PMLR, 2021.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial\nintelligence experimentation. In International Joint Conference on Artificial Intelligence, 2016. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:9953039.\nApoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective:\nClip embeddings for embodied ai. 2022 IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 14809–14818, 2021. URL https:\/\/api.semanticscholar.org\/\nCorpusID:244346010.\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs\/1312.6114, 2013.\nURL https:\/\/api.semanticscholar.org\/CorpusID:216078090.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick.\nSegment anything. ArXiv, abs\/2304.02643, 2023. URL https:\/\/api.semanticscholar.\norg\/CorpusID:257952310.\nMichael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald,\nDJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni,\nSatinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm distilla-\ntion. ArXiv, abs\/2210.14215, 2022. URL https:\/\/api.semanticscholar.org\/CorpusID:\n253107613.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila A. McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. ArXiv, abs\/2306.00937, 2023. URL https:\/\/api.\nsemanticscholar.org\/CorpusID:258999563.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang. Juewu-mc: Playing minecraft\nwith sample-efficient hierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021.\n14\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nMinghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems\nand solutions. ArXiv, abs\/2201.08299, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:246063885.\nArjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, and Dhruv Batra. Zson: Zero-\nshot object-goal navigation using multimodal goal embeddings. ArXiv, abs\/2206.12403, 2022. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:250048645.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,\nAlex Graves, Martin A. Riedmiller, Andreas Kirkeby Fidjeland, Georg Ostrovski, Stig Petersen,\nCharlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,\nShane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,\n518:529–533, 2015. URL https:\/\/api.semanticscholar.org\/CorpusID:205242740.\nEmilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Çaglar Gülçehre, Siddhant M.\nJayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick,\nNicolas Manfred Otto Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning.\nIn International Conference on Machine Learning, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:204578308.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\nBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist\nagent. arXiv preprint arXiv:2205.06175, 2022.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman,\nLeo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables\nzero-shot task generalization, 2021.\nJuergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards - just map\nthem to actions. ArXiv, abs\/1912.02875, 2019. URL https:\/\/api.semanticscholar.org\/\nCorpusID:208857600.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. ArXiv, abs\/1707.06347, 2017. URL https:\/\/api.semanticscholar.\norg\/CorpusID:28695052.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman,\nDominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach,\nKoray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural\nnetworks and tree search. Nature, 529:484–489, 2016. URL https:\/\/api.semanticscholar.\norg\/CorpusID:515925.\nAustin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Ho Vuong,\nPaul Wohlhart, Brianna Zitkovich, F. Xia, Chelsea Finn, and Karol Hausman. Open-world object\nmanipulation using pre-trained vision-language models. ArXiv, abs\/2303.00905, 2023. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:257280290.\n15\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nMingxing Tan and Quoc V. Le.\nEfficientnet: Rethinking model scaling for convolutional neu-\nral networks. ArXiv, abs\/1905.11946, 2019. URL https:\/\/api.semanticscholar.org\/\nCorpusID:167217261.\nMaxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. Label Studio: Data\nlabeling software, 2020-2022. URL https:\/\/github.com\/heartexlabs\/label-studio.\nOpen source software available from https:\/\/github.com\/heartexlabs\/label-studio.\nLaurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-sne. Journal of Machine Learn-\ning Research, 9:2579–2605, 2008. URL https:\/\/api.semanticscholar.org\/CorpusID:\n5855042.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. URL https:\n\/\/api.semanticscholar.org\/CorpusID:13756489.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language mod-\nels. ArXiv, abs\/2305.16291, 2023a. URL https:\/\/api.semanticscholar.org\/CorpusID:\n258887849.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and se-\nlect: Interactive planning with large language models enables open-world multi-task agents.\nArXiv, abs\/2302.01560, 2023b.\nURL https:\/\/api.semanticscholar.org\/CorpusID:\n256598146.\nZhihui Xie, Zichuan Lin, Deheng Ye, Qiang Fu, Wei Yang, and Shuai Li. Future-conditioned unsuper-\nvised pretraining for decision transformer. In International Conference on Machine Learning, 2023.\nURL https:\/\/api.semanticscholar.org\/CorpusID:258947476.\nChao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, and Chuang\nGan. Imitation learning from observations by minimizing inverse dynamics disagreement. Advances\nin neural information processing systems, 32, 2019.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan C. Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learn-\ning. ArXiv, abs\/1910.10897, 2019. URL https:\/\/api.semanticscholar.org\/CorpusID:\n204852201.\nQihang Zhang, Zhenghao Peng, and Bolei Zhou. Learning to drive by watching youtube videos:\nAction-conditioned contrastive policy pretraining. In European Conference on Computer Vision, 2022.\nURL https:\/\/api.semanticscholar.org\/CorpusID:250626771.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyuan Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:\nGenerally capable agents for open-world environments via large language models with text-based\nknowledge and memory. ArXiv, abs\/2305.17144, 2023. URL https:\/\/api.semanticscholar.\norg\/CorpusID:258959262.\nDaniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano,\nand Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs\/1909.08593,\n2019. URL https:\/\/api.semanticscholar.org\/CorpusID:202660943.\n16\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nAppendix\nA. Derivation\nIn this section, we detail how we derive the final objective. Recall that the goal is to maximize the\nlog-likelihood of future states given past ones: log 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡). Using Bayes’ theorem and the Jensen’s\ninequality, we have:\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑧\n𝑝(𝑠𝑡+1:𝑇, 𝑧|𝑠0:𝑡),\n(4)\n= log\n∑︁\n𝑧\n𝑝(𝑠𝑡+1:𝑇, 𝑧|𝑠0:𝑡) 𝑞(𝑧|𝑠0:𝑇)\n𝑞(𝑧|𝑠0:𝑇)\n,\n(5)\n≥𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇, 𝑧|𝑠0:𝑡) −log 𝑞(𝑧|𝑠0:𝑇)\n\u0003\n,\n(6)\n= 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) + log 𝑝(𝑧|𝑠0:𝑡) −log 𝑞(𝑧|𝑠0:𝑇)\n\u0003\n,\n(7)\n= 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧)\n\u0003\n+ 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑧|𝑠0:𝑡)\n𝑞(𝑧|𝑠0:𝑇)\n\u0003\n,\n(8)\n= 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧)\n\u0003\n−𝐷KL\n\u0000𝑞(𝑧|𝑠0:𝑇) ∥𝑝(𝑧|𝑠0:𝑡)\u0001\n.\n(9)\nWe break down 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) into components: goal-conditioned policy 𝜋(𝑎𝜏|𝑠0:𝜏+1) and the transition\ndynamics 𝑝(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡), we have\n𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) =\n𝑇−1\nÖ\n𝜏=𝑡\n\u0000 ∑︁\n𝑎𝜏\n𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) · 𝑝(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\u0001\n.\n(10)\nFurthermore, using Jensen’s inequality, log 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) can be written as\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) =\n𝑇−1\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) · 𝑝(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏),\n(11)\n=\n𝑇−1\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) · 𝑝(𝑎𝜏|𝑠0:𝜏, 𝑠𝜏+1) · 𝑝(𝑠𝜏+1|𝑠0:𝜏)\n𝑝(𝑎𝜏|𝑠0:𝜏)\n,\n(12)\n≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏,𝑠𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) + 𝐶\n\u0003\n,\n(13)\nwhere the constant 𝐶= log 𝑝(𝑠𝜏+1|𝑠0:𝜏)−log 𝑝(𝑎𝜏|𝑠0:𝜏) describes the dataset distribution and is irrelevant\nto what we want to learn (i.e. the goal space and the goal-conditioned policy), we have:\n𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧)\n\u0003\n≥𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002 𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏,𝑠𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧)\n\u0003\u0003\n,\n(14)\n=\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇),𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏,𝑠𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧)\n\u0003\n.\n(15)\nThus, we derived the evidence lower-bound of log 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) as follows\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇),𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧)\n\u0003\n−𝐷KL\n\u0000𝑞(𝑧|𝑠0:𝑇) ∥𝑝(𝑧|𝑠0:𝑡)\u0001\n.\n(16)\n17\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFigure 8 | Examples of Minecraft environment. Tasks from top to bottom, from left to right are building houses,\nplanting wheat, fishing, brewing a potion, mining diamond ores, and combating the ender dragon, respectively.\nB. Minecraft Environment\nMinecraft is an extremely popular sandbox game that allows players to freely create and explore their\nworld. This game has infinite freedom, allowing players to change the world and ecosystems through\nbuilding, mining, planting, combating, and other methods (shown in Figure 8). It is precisely because\nof this freedom that Minecraft becomes an excellent AI testing benchmark (Baker et al., 2022; Cai\net al., 2023; Fan et al., 2022; Johnson et al., 2016; Lifshitz et al., 2023; Wang et al., 2023a,b). In this\ngame, AI agents need to face situations that are highly similar to the real world, making judgments\nand decisions to deal with various environments and problems. Therefore, Minecraft is a very suitable\nenvironment to be used as an AI testing benchmark. By using Minecraft, AI researchers can more\nconveniently simulate various complex and diverse environments and tasks, thereby improving the\npractical value and application of AI technology.\nWe use the combination of 1.16.5 version MineRL (Guss et al., 2019) and MCP-Reborn as our\ntesting platform, which is consistent with the environment used by VPT (Baker et al., 2022) and\nSTEVE-1 (Lifshitz et al., 2023). Mainly because this platform preserves observation and action space\nthat is consistent with human players to the fullest extent. On the one hand, this design brings about\nhigh challenges, as agents can only interact with the environment using low-level mouse and keyboard\nactions, and can only observe visual information like human players without any in-game privileged\ninformation. Therefore, the AI algorithms developed on this platform can have higher generalization\nability. On the other hand, this also presents opportunities for us to conduct large-scale pre-training\non internet-scale gameplay videos.\nB.1. Observation Space\nThe visual elements included in our observation space are completely consistent with those seen\nby human players, including the Hotbar, health indicators, player hands, and equipped items. The\nplayer’s perspective is in the first person with a field of view of 70 degrees. The simulator first\ngenerates an RGB image with dimensions of 640 × 360 during the rendering process. Before inputting\nto the agent, we resize the image to 224 × 224 to enable the agent to clearly see item icons in the\ninventory and important details in the environment. When the agent opens the GUI, the simulator\nalso renders the mouse cursor normally. The RGB image is the only observation that the agent can\nobtain from the environment during inference. It is worth noting that to help the agent see more\nhttps:\/\/github.com\/Hexeption\/MCP-Reborn\n18\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 1 | Action space descriptions from Minecraft wiki (https:\/\/minecraft.fandom.com\/wiki\/Controls).\nIndex\nAction\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove backward.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement. When\nused in conjunction with the attack function in the GUI,\nit can swap items between inventory and Hotbar. When\nused with the craft function, it crafts the maximum pos-\nsible number of items instead of just one.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current motion.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPut down the item being held or interact with the block\nthat the player is currently looking at. Within the GUI,\npick up a stack of items or place a single item from the\nstack that is being held by the mouse.\n10\nhotbar.[1-9]\nkeys 1 - 9\nSelects the appropriate hotbar item. When in the inven-\ntory GUI, swap the contents of the inventory slot under\nthe mouse pointer and the corresponding hotbar slot.\n11\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180\nto +180.\n12\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180\nto +180.\nclearly in extremely dark environments, we have added a night vision effect for the agent, which\nincreases the brightness of the environment during nighttime.\nB.2. Action Space\nOur action space is almost identical to that of humans, except for actions that involve inputting\nstrings. It consists of two parts: the mouse and the keyboard. The mouse movement is responsible for\nchanging the player’s camera perspective and moving the cursor when the GUI is opened. The left and\nright buttons are responsible for attacking and using items. The keyboard is mainly responsible for\ncontrolling the agent’s movement. We list the meaning of each action in the Table 1. To avoid predicting\nnull action, we used the same joint hierarchical action space as Baker et al. (2022), which consists of\nbutton space and camera space. Button space encodes all combinations of keyboard operations and a\nflag indicating whether the mouse is used, resulting in a total of 8461 candidate actions. The camera\nspace discretizes the range of one mouse movement into 121 actions. Therefore, the action head of\nthe agent is a multi-classification network with 8461 dimensions and a multi-classification network\nwith 121 dimensions.\nC. Inverse Dynamic Model\nAccording to the theory in Section 3, we know that our training paradigm relies on the inverse\ndynamic model (IDM) which generates pseudo action labels for raw gameplay videos to calculate the\nbehavior cloning loss. Therefore, in this section, we introduced the background knowledge of IDM.\n19\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nIDM is a non-causal model that aims to uncover the underlying action that caused changes\nin the current step by observing historical and future states, and it can be formally represented as\n𝑝(𝑎𝑡|𝑜𝑡, 𝑜𝑡+1). Compared with traditional policies learned via behavior cloning, IDM is more accurate in\npredicting actions because it can observe the changes between past and future frames. OpenAI (Baker\net al., 2022) developed the first inverse dynamic model in the Minecraft domain. By extending the\nlength of the observable sequence to 128 and modeling 𝑝(𝑎𝑡|𝑜𝑡−64:𝑡+64) with a non-causal transformer,\nthe IDM achieved the accuracy of action prediction to over 95% with only 2k hours of game trajectories.\nThis makes it possible for our training paradigm to utilize the large-scale Minecraft data available on\nthe Internet. Moreover, Zhang et al. (2022) has also trained an accurate IDM model with a small\namount of data in a real autonomous driving environment, which further provides a basic guarantee\nfor our training method to generalize to other complex environments.\nD. Implementation Details\nD.1. Model Architecture\nThe video encoder consists of a convolutional neural network backbone and a non-causal transformer.\nInspired by Brohan et al. (2022), we adopted the EfficientNet (Tan & Le, 2019) as the backbone.\nSpecifically, we use its variant EfficientNet-B0 for efficiency, which takes in images of size 224 × 224\nand extracts a feature vector of shape 7 × 7 × 1280, where 7 × 7 denotes the spatial dimensions. In\norder to adaptively enhance the important visual information, we use a shallow transformer to pool\nthe feature map along spatial channels. To fuse global visual features, we construct another learnable\nembedding [sp], concatenate it with the 49 features in space, and obtain a token sequence of length\n50. After being processed by the transformer, the output for the [sp] token corresponds to the pooled\nvisual feature, whose dimension is 𝑑ℎ𝑖𝑑= 1024. To capture the temporal features of the video, we\nremove the code related to the casual mask in the minGPT and obtain a non-causal transformer. The\npolicy decoder consists of 4 identical blocks, where each block contains a Flamingo gated-attention\ndense layer (Alayrac et al., 2022) and a Transformer-XL block(Dai et al., 2019). The Transformer-XL\nblock maintains a recurrence memory of past 128 key-value pairs to memory long-horizon history\nstates. We directly use the Transformer-XL implementation in Baker et al. (2022) with a simple\nmodification, i.e. before passing states into the policy decoder, we add the previous action to the\nstate embedding at each timestep. Notably, We find this modification very useful especially when we\nneed to train the policy from scratch. As it not only accelerates the training process but makes the\npredicted action more consistent and smooth. Additional hyperparameters can be found in Table 2.\nD.2. Inference\nTo generate reference videos, we invited three human players to play each task according to the task\ndescription. Each person was asked to produce two videos, so we could prepare six videos for each\ntask in total. Then, we selected the most relevant video to the task description from the six videos\nand cropped the first 128 frames into a new video, which was used to instruct GROOT to complete\nthis task. In addition, we selected a 16-frame segment that best expressed the task information as the\nvisual prompt for STEVE-1 (visual) from these six videos. This ensures fairness in comparison.\nDuring inference, we found that in some tasks, such as build obsidian (\n), GROOT’s\nbehavior mixed with the intention of traveling around. We believe this is a bias introduced during\ntraining. We draw the inspiration from STEVE-1 (Lifshitz et al., 2023) and subtract this bias in the\naction logits space before sampling the action. Specifically, we infer two models at the same time,\nwhere one model’s condition is a specific task video and the other model’s condition is a 128-frame\nvideo of traveling freely in the environment. The input observations for the two models are exactly\nhttps:\/\/github.com\/karpathy\/minGPT\n20\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 2 | Hyperparameters for training GROOT.\nHyperparameter\nValue\nOptimizer\nAdamW\nWeight Decay\n0.001\nLearning Rate\n0.0000181\nWarmup Steps\n2000\nNumber of Workers\n4\nParallel Strategy\nddp\nType of GPUs\nNVIDIA RTX 4090Ti, A40\nParallel GPUs\n8\nAccumulate Gradient Batches\n8\nBatch Size\/GPU (Total)\n2 (128)\nTraining Precision\nbf16\nInput Image Size\n224 × 224\nCNN Backbone\nEfficientNet-B0\nEncoder Transformer\nminGPT (w\/o causal mask)\nDecoder Transformer\nTransformerXL\nNumber of Encoder Blocks\n8\nNumber of Decoder Blocks\n4\nHidden Dimension\n1024\nNumber of Condition Slots\n1\nTrajectory Chunk size\n128\nAttention Memory Size\n256\nWeight of BC Loss\n1\nWeight of KL Loss\n0.01\nthe same. At each time step, we use the action logits of the previous model to subtract a certain\nproportion of the action logits predicted by the latter model before using the Gumbel-Softmax trick to\nsample the action. The logits calculation equation is directly borrowed from Lifshitz et al. (2023)\nlogits𝑡= (1 + 𝜆) 𝑓𝜃(𝑜1:𝑡, 𝑔goal) −𝜆𝑓𝜃(𝑜1:𝑡, 𝑔bias)\n(17)\nwhere 𝑓𝜃(𝑜1:𝑡, 𝑔goal) and 𝑓𝜃(𝑜1:𝑡, 𝑔bias) are two kinds of action logits generated by feeding forward two\nreference videos goal and bias to GROOT, 𝜆is a trade-off parameter. As illustrated in Figure 9,\nwe find that this trick can improve the success rate of tasks such as build obsidian (\n), build\nsnow golem ( ), enchant sword (\n), and dig three down and fill one up (\n) with\nthe 𝜆= 1.5. Interestingly, we observe that the effective 𝜆scale (approximately 1.5) in our model\nis much smaller than the scale (approximately 6.5) used in STEVE-1. We speculate that this may\nbe because STEVE-1 fine-tunes the foundation VPT to gain steerability, but VPT does not receive\ngoal conditions for demonstrations during behavior cloning. This may cause VPT to learn overly\nsmooth behavior distributions, requiring the use of larger lambda scales to activate goal-specific\nbehaviors. Although this technique is effective at inference time, it still requires hyperparameter\ntuning in practice. In the future, it will be meaningful to directly remove biased behaviors from the\ntraining process.\nD.3. Ablation on Number of Condition Slots\nIn this section, we explore the impact of the number of condition slots (number of learnable tokens)\non the final performance. We compared the performance of the model on 6 programmatic tasks\n21\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBuild Obsidian\n0.1\n0.2\n0.5\n0.4\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBuild Snow Golem\n0.1\n0.3\n0.6\n0.4\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnchant Sword\n0.2\n0.6\n0.9\n0.4\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDig Three Fill One\n0.0\n0.2\n0.3\n0.0\nFigure 9 | Ablation on the condition scale 𝜆.\nwith 𝑁= 1 and 𝑁= 3 condition slots and computed quantitative metrics for each task. As shown\nin Table 3, we find that increasing the number of condition slots leads to a significant decrease\nin the model’s performance on most tasks, except for the “explore run” task. We speculate that\nhaving more condition slots may result in a higher number of dimensions in the goal space, which in\nturn reduces the generalization ability of the learned goal space. Therefore, we suggest that when\napplying GROOT to other environments, the hyperparameters should be carefully chosen based on\nthe characteristics of the environment or using other parameter selection methods.\nTable 3 | Ablation on the number of condition slots.\nTask Name\n(Metric)\nexplore run ↑\n(distance)\nbuild pillar ↓\n(height of pillar)\ncollect grass ↓\n(num of grass)\ncollect seagrass ↓\n(num of seagrass)\ncollect dirt ↓\n(num of dirt)\nmine stone ↓\n(num of stones)\n𝑁= 1\n54.0\n37.6\n23.8\n3.3\n6.2\n12.2\n𝑁= 3\n59.0\n13.3\n5.6\n0.9\n5.4\n11.2\nE. Dataset Details\nE.1. Contractor Data\nThe contractor data is a Minecraft offline trajectory dataset provided by Baker et al. (2022) , which\nis annotated by professional human players and used for training the inverse dynamic model. In\nthis dataset, human players play the game while the system records the image sequence {𝑠1:𝑇}𝑀,\naction sequence {𝑎1:𝑇}𝑀, and metadata {𝑒1:𝑇}𝑀generated by the players. Excluding frames containing\nempty actions, the dataset contains 1.6 billion frames with a duration of approximately 2000 hours.\nThe metadata records the events triggered by the agent in the game at each time step, including\nthree types: craft item, pickup, and mine block, which represent the agent’s activities of\ncrafting items using the GUI, picking up dropped items and destroying blocks at the current time step,\nrespectively. In the process of training GROOT, we use all trajectories provided by the contractor data,\nbut without including any metadata. We only use the metadata to retrieve relevant trajectory segments\nduring the visualization of the goal space.\nF. Experimental Setup Details\nF.1. Baseline Details\nVPT is the first foundation model in the Minecraft domain developed by Baker et al. (2022). Its\narchitecture consists of ImpalaCNN and TransformerXL. Using behavior cloning algorithms to pre-train\nhttps:\/\/github.com\/openai\/Video-Pre-Training\n22\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\non large-scale YouTube demonstrations, they obtained the first checkpoint of VPT(fd) which can\nfreely explore the environment. To further enhance the agent’s abilities in early-game environments,\nthey constructed an “earlygame” dataset and fine-tuned the pre-trained foundation model on that\ndataset, resulting in the VPT(bc) checkpoint. This model significantly improved performance on\nbasic tasks such as “crafting table” and “collecting wood”. Based on VPT(bc), they used online\nreinforcement learning with a carefully designed reward shaping to obtain the checkpoint VPT(rl)\ncapable of obtaining diamonds entirely from scratch. It is noteworthy that the models’ architectures\nof all three checkpoints are consistent and do not support instruction input. That’s why their rankings\non the Minecraft SkillForge benchmark are low. We also observed that the performance of VPT(bc)\nsurpasses that of VPT(rl) due to the “earlygame” dataset’s exploratory nature, making it perform\nbetter on explore tasks. VPT(rl) is tailored specifically for diamond mining tasks and has thus lost\nthe capability of most tasks outside diamond mining path. No matter where you place it, the first\nthing VPT(rl) does is to look for trees and prepare to mine diamonds.\nSTEVE-1 is a Minecraft agent that can follow open-ended text and visual instructions built on\nMineCLIP (Fan et al., 2022) and VPT. It can perform a wide range of short-horizon tasks that can\nbe expressed by a 16-frame future video clip. The training of STEVE-1 can be described in two\nsteps. The first step is to train a future-video conditioned policy with packed hindsight relabeling\ntrick. With the frozen MineCLIP visual encoder to embed the visual instruction, they finetune the\nVPT(bc) on the contractor data to obtain STEVE-1(visual). The second step is to learn a model that\ntranslates textual instruction into visual instruction. By training a conditional variational autoencoder\n(CVAE) on the collected video-text pairs, they created a variant STEVE-1(text) that understands text\ninstructions. This baseline performs well on many simple tasks in the Minecraft SkillForge benchmark,\nsuch as \"explore run,\" \"collect grass,\" and \"collect wood.\" However, it struggles with multi-step and\nless common tasks, like \"build snow golems\" and \"dig three down and fill one up.\"\nPlease note that all baselines, including GROOT, were not fine-tuned for tasks in Minecraft SkillForge.\nF.2. t-SNE Visualization Details\nThis section details how the videos are sampled to do visualization. The selected videos are categorized\ninto seven groups: craft items, combat enemies, harvest crops, hunt animals, chop\ntrees, trade with villagers, and mine ores. Generally, each group contains two types of\nvideos, each with 1000 data points sampled. The sampling method retrieves the time when a certain\nevent occurs in the metadata and goes back 128 frames from that time to obtain a video segment\nthat is 128 frames long. We illustrate video configurations in Table 4. For example, in the combat\nenemies task, taking \"combat zombies\" as an example, we retrieve all the moments when the event\n\"pickup:rotten_flesh\" occurs, because after killing zombies, they will drop rotten flesh, which can\nthen be picked up by players. Through sampling observations, we found that this method can sample\nvideos that are consistent with the descriptions.\nF.3. Programmatic Evaluation Details\nIn this section, we elaborated on how each episode is regarded as successful. For the dye and\nshear sheep (\n) task, dyeing the sheep and shearing its wool must be successfully performed\nto be considered a success. For the use bow (\n) task, firing the arrow after charging it to the\nmaximum degree is required to be successful. For the sleep (\n) task, placing the bed and spending\nthe night on it are required to be successful. For the smelt (\n) task, placing the furnace and\ndragging coal and mutton into the designated slots are required to be successful. For the lead (\n)\ntask, successfully tethering at least one animal is considered a success. For the build obsidian\n23\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 4 | Sample videos from the contractor data (Baker et al., 2022) for the goal space visualization.\nGroup\nVideo Description\nEvent in Metadata\ncraft items\ncraft wodden_pickaxe with crafting_table\ncraft_item:wooden_pickaxe\ncraft items\ncraft iron_pickaxe with crafting_table\ncraft_item:iron_pickaxe\ncombat enemies\ncombat zombies\npickup:rotten_flesh\ncombat enemies\ncombat spiders\npickup:spider_eye\nharvest crops\nharvest wheat\nmine_block:wheat\nharvest crops\nharvest melon\nmine_block:melon\nhunt animals\nhunt sheep\npickup:mutton\nhunt animals\nhunt cow\npickup:beef\nchop trees\nchop oak trees\nmine_block:oak_log\nchop trees\nchop birch trees\nmine_block:birch_log\ntrade with villagers\ntrade with villagers for emerald\ncraft_item:emerald\ntrade with villagers\ntrade with villagers for enchanted_book\ncraft_item:enchanted_book\nmine ores\nmine coal ores with pickaxe\nmine_block:coal_ore\nmine ores\nmine iron ores with pickaxe\nmine_block:iron_ore\n(\n) task, pouring a water bucket and a lava bucket to fuse them is required to be successful. For\nthe enchant (\n) task, placing the enchantment table, putting a diamond sword and lapis lazuli\ninto the slots, and clicking the enchanting option are required to be successful. For the dig down\nthree fill one up (\n) task, the agent must first vertically break three dirt blocks below and\nthen use one dirt block to seal the area above. For the build snow golems ( ) task, placing 2\nsnow blocks and 1 carved pumpkin head in order and triggering the creation of a snow golem are\nrequired to be successful.\nF.4. Combining Skills Experimental Details\nFirst, we introduce the experimental environment selected for our study. The agent is summoned on\nthe plains biome, holding a diamond pickaxe, and granted the night vision status to enable the agent\nto see the various ores underground. At the beginning of each episode, we set the agent’s condition to\ndig down. When the agent descends to a depth below 12 layers, the condition automatically switches\nto horizontal mining. Each round of episodes lasts for 12,000 frames, which is equivalent to 10\nminutes in the real world. For GROOT, both the reference videos of dig down and horizontal\nmining were recorded by a human player. For STEVE-1, we invited the same player to carefully\nrecord the prompt videos. It is worth noting that while we could easily prompt it to dig down, it was\ndifficult to keep it in the horizontal mining condition. This made STEVE-1 prone to falling into the\nbedrock layer and getting stuck. Finally, we did not observe STEVE-1 finding any diamonds in the\n25 experiments, which can be attributed to the inability of its goal space to encode details such as\nhorizontal mining.\nG. Rating System\nG.1. ELO Rating\nThe ELO rating system is widely adopted for evaluating the skill levels of multiple players in two-player\ngames, such as Chess and Go (Silver et al., 2016). In this section, we elaborate on how we introduce\nhuman evaluation and use the ELO Rating system to measure the relative performance of agents on\n24\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFigure 10 | Example of the annotating system for human evaluation.\nthe Minecraft SkillForge benchmark.\nIn the ELO rating system, each agent’s skill level is represented by a numerical rating. We\nrepeatedly let agents play against each other in pairs. Specifically, in each game, we sample a task\nand two agents, denoted as Agent A and Agent B. Then, we randomly sample a trajectory for each\nagent corresponding to the designated task. The two trajectories are assigned to a human annotator,\nwho selects the most task-relevant one. We implement the annotating system with Label Studio\n(Tkachenko et al., 2020-2022), as shown in Figure 10. We consider the agent that produced this\ntrajectory to be the winner, let’s assume it is Agent A. After each round, we update the scores of Agent\nA and Agent B as follows\n𝑅𝐴←𝑅𝐴+ 𝐾·\n1\n1 + 10(𝑅𝐴−𝑅𝐵)\/400 ,\n𝑅𝐵←𝑅𝐵−𝐾·\n1\n1 + 10(𝑅𝐴−𝑅𝐵)\/400 ,\n(18)\nwhere K is the update factor and we set it to 8. After calculating the score of the agent, we use VPT\n(bc) as 1500 points and shift the scores of other agents accordingly. Based on the ELO ratings, we can\neasily measure the relative winning rate for each paired agent. The win rate of Agent A over Agent B\ncan be represented as\n1\n1+10(𝑅𝐵−𝑅𝐴)\/400 . For example, the win rate ratio between two agents with a score\ndifference of 100 scores is 64% : 36%. A score difference of 200 scores implicit 76% : 24%.\nG.2. TrueSkill Rating\nWe also report the comparison results using TrueSkill rating system, which is used by gamers to\nevaluate their skill level. It was developed by Microsoft Research and is currently used on Xbox LIVE\nfor matchmaking and ranking services. Different from ELO, it can also track the uncertainty of the\nrankings. This system utilizes the Bayesian inference algorithm to quantify a player’s true skill points.\nIn TrueSkill, rating is modeled as a Gaussian distribution which starts from N (25, 25\n3\n2), where 𝜇is an\naverage skill of player, and 𝜎is a confidence of the guessed rating. A real skill of player is between\n𝜇± 2𝜎with 95% confidence. After conducting 1500 updates, the TrueSkill scores converged as in\nTable 5. We found that the ranking order of the baseline methods is consistent with that obtained\nhttps:\/\/trueskill.org\/\n25\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nusing ELO rating: HUMAN ≻GROOT ≻STEVE-1(visual) ≻STEVE-1(text) ≻VPT(bc) ≻VPT(fd) ≻\nVPT(rl).\nTable 5 | TrueSkill rating comparison on the Minecraft SkillForge benchmark.\nBaseline\nHUMAN\nGROOT\nSTEVE-1(visual)\nSTEVE-1(text)\nVPT(bc)\nVPT(fd)\nVPT(rl)\n𝜇± 𝜎\n34.2 ± 1.0\n29.1 ± 0.9\n25.8 ± 0.8\n24.6 ± 0.8\n22.2 ± 0.8\n20.7 ± 0.8\n19.2 ± 0.9\nG.3. Human Participation\nWe recruited 15 students with varying degrees of Minecraft game experience, ranging from a few\nhours to several years, from the Minecraft project group to conduct the evaluation. They are all\nfamiliar with the basic operations of Minecraft. Each employee was asked to label 100 matches for\nELO Rating or TrueSkill Rating, for a total of 1500 matches. For each employee who is required\nto collect or assess gameplay videos, we ask them to first read the description of each task in the\nMinecraft SkillForge Benchmark completely, as well as the evaluation criteria for task completion\nquality, see Appendix H. Taking the task of building a snow golem as an example, the evaluation\ncriteria are as follows: Build a snow golem. ≻Place both kinds of blocks. ≻Place at least one kind\nof block. ≻Place no block. This enables employees to quantify video quality and ensures that all\nemployees evaluate task completion consistently. All these employees were explicitly informed that the\ncollected data would be used for AI research.\nH. Minecraft SkillForge Benchmark\nIn this section, we detail the benchmark titled \"Minecraft SkillForge\" which meticulously incorporates\na wide spectrum of tasks prevalent within Minecraft. Our aim is to ensure that every task provides a\nmeaningful evaluation of a specific skill that an AI agent might possess. We categorize these tasks into\nsix groups: collect, explore, craft, tool, survive, and build. In the following subsections,\nwe will provide a detailed introduction to each of them. The “Description” field provides a brief\ndescription of the task, the “Precondition” field outlines the initial settings of the testing environment\nfor the task, the “SkillAssessed” field indicates which aspect(s) of the agent’s ability are being assessed\nby the task, and the “Evaluation” field describes the quality evaluation metrics for task completion\n(based on which human players judge the quality of two rollout videos).\nH.1. Collect\nThe tasks categorized under the collect section of our benchmark are specifically designed to\nevaluate an AI agent’s capability in resource acquisition proficiency and spatial awareness. This means\nthe agent should not only be adept at identifying and gathering specific resources but also possess\nthe acumen to navigate through varied environments while being aware of its surroundings and the\navailable tools at its disposal.\nTask: collect dirt\nDescription: Collect dirt from the surface.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Basic terrain understanding and the ability to differentiate between surface-level\nblocks.\nEvaluation: Run away. < Look down. < Dig down. < Break the dirt on the surface.\nTask: collect grass\nDescription: Remove weeds on the surface.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Surface navigation and comprehension of vegetation blocks.\n26\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ncollect dirt from the surface\nremove weeds on the surface\nchop trees to collect wood\ndive into the water and collect seagrass\ndye the sheep then shear the sheep\nFigure 11 | Examples of tasks in collect category.\nEvaluation: Run away. < Break the grass block. < Break a large field of grass blocks.\nTask: collect wood\nDescription: Cut down trees to collect wood.\nPrecondition: Spawn the player in the forest biome with an iron_axe in its hand.\nSkillAssessed: Recognition of tree structures, efficient utilization of tools, and block\nharvesting capability.\nEvaluation: Run away. < Approach trees. < Chop the tree and collect logs.\nTask: collect seagrass\nDescription: Dive into the water and collect seagrass.\nPrecondition: Spawn the player near the sea.\nSkillAssessed: Water navigation, diving mechanics understanding, and underwater block interaction.\nEvaluation: Walk on the land. < Swim on the water < Dive into the water. < Break seagrass blocks.\nTask: collect wool\nDescription: Dye and shear the sheep for wool.\nPrecondition: Spawn the player in the plains biome with a shear (mainhand) and a stack of blue_dye\n(offhand), 5 sheep near the player.\nSkillAssessed: Interaction with entities, tool and item application, and sequential action\nexecution.\nEvaluation: Ignore the sheep. < Dye the sheep. < Shear the sheep. < First dye then shear the sheep.\nPrompt 1 | The environment configuration and evaluation metric for collect series tasks.\nH.2. Explore\nThe tasks encompassed within the explore category of our benchmark are intricately devised to\nevaluate an AI agent’s navigation proficiency, understanding of diverse environments, and intrinsic\nmotivation for exploration. Through these tasks, we gauge an agent’s ability to actively traverse,\nunderstand, and interact with varied elements of the Minecraft world, and its propensity to unravel\nmysteries and challenges posed by the environment.\n27\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nrun and explore \ntravel on a wooden boat through water\nclimb the mountain\nopen a chest and acquire the treasure\nmine horizontally underground\nFigure 12 | Examples of tasks in explore category.\nTask: run and explore\nDescription: Run and explore.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Stamina utilization and distance-based exploration.\nEvaluation: Exploring as far as possible.\nTask: climb the mountain\nDescription: Climb the mountain.\nPrecondition: Spawn the player in the stone shore biome and near the mountain.\nSkillAssessed: Vertical navigation, terrain adaptation, and goal-oriented movement.\nEvaluation: Run away and ignore the mountain. < Approach the mountain. < Climbing the mountain. <\nClimb to the top of the mountain.\nTask: mine horizontally\nDescription: Mine horizontally underground.\nPrecondition: Spawn the player in a deep cave with an iron_pickaxe in the hand.\nSkillAssessed: Underground navigation, tool utilization, and spatial reasoning in confined spaces.\nEvaluation: Run away. < Break the stone. < Dig down. < Mine horizontally.\nTask: travel by boat\nDescription: Travel on a wooden boat through water.\nPrecondition: Spawn the player near the sea with a wooden boat in the hand.\nSkillAssessed: Aquatic travel, tool placement, and boat maneuverability.\nEvaluation: Did not place the boat. < Place the boat on the water. < Board the boat. < Row in the\nwater.\nTask: explore the treasure\nDescription: Rush into a villager’s home and open a chest and acquire the treasure.\nPrecondition: Spawn the player in front of a villager’s house.\nSkillAssessed: Interaction with structures, curiosity-driven exploration, and object acquisition.\nEvaluation: Ignore the house and run away. < Open the door. < Enter the house. < Open the chest. <\nAcquire the treasure.\nPrompt 2 | The environment configuration and evaluation metric for explore series tasks.\n28\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nH.3. Craft\nThe tasks under the craft category in our benchmark have been designed to shed light on an AI\nagent’s prowess in item utilization, the intricacies of Minecraft crafting mechanics, and the nuances\nof various game mechanic interactions. These tasks provide a detailed examination of an agent’s\ncapability to convert materials into functional items and harness the game’s various crafting and\nenhancement mechanics.\nopen inventory and craft a crafting table\nplace the crafting table and open it to craft ladders\nplace a furnace and use it to smelt food\nplace an enchanting table and enchant a diamond sword\nplace a stonecutter and use it to cut the stone\nFigure 13 | Examples of tasks in craft category.\nTask: craft the crafting_table\nDescription: Open inventory and craft a crafting table.\nPrecondition: Spawn the player in the plains biome with a stack of oak_planks in the inventory.\nSkillAssessed: Inventory management and basic crafting.\nEvaluation: Open the inventory. < Click on the recipe button. < Click on the crafting_table. <\nDrag the crafting_table into the inventory.\nTask: craft ladders\nDescription: Place the crafting table and open it to craft ladders.\nPrecondition: Spawn the player in the plains biome with a crafting_table in its main hand and a\nstack of oak_planks in the inventory.\nSkillAssessed: Advanced crafting using crafting stations and recipe navigation.\nEvaluation: Place the crafting_table on the surface. < Open the crafting_tabe. < Click on the\nrecipe book. < Click on the ladder. < Drag the ladder into the inventory.\nTask: enchant sword\nDescription: Place an enchanting table and use it to enchant a diamond sword.\nPrecondition: Spawn the player in the plains biome with an enchanting table in its main hand, 3\ndiamond swords, and 3 stacks of lapis_lazuli in the inventory.\nSkillAssessed: Tool enhancement using enchantment stations and decision-making in choosing\nenchantments.\nEvaluation: Place the enchanting_table on the surface. < Open the enchanting_table. < Place the\nlapis_lazuli or diamond sword. < Place the lapis_lazuli and diamond sword. < Choose any\nenchantment.\nTask: smelt food\nDescription: Place a furnace and use it to smelt food.\n29\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nPrecondition: Spawn the player in the plains biome with a furnace table in its main hand, 3 stacks\nof mutton, and 3 stacks of coal in the inventory.\nSkillAssessed: Food processing using a smelting furnace, raw material to product conversion, and\npatience in awaiting outcomes.\nEvaluation: Place the furnace on the surface. < Open the furnace. < Place raw meat or coal. <\nPlace both raw meat and coal. < Wait for the raw meat to be cooked. < Take out cooked meat.\nTask: cut stone\nDescription: Place a stonecutter and use it to cut stones.\nPrecondition: Spawn the player in the plains biome with a stonecutter in its main hand, 6 stacks\nof stones in the inventory.\nSkillAssessed: Tool enhancement using enchantment stations and decision-making in choosing\nenchantments.\nEvaluation: Place the stonecutter on the surface. < Open the stonecutter. < Place the stones. <\nSelect a target type of stone. < Drag stones to the inventory.\nPrompt 3 | The environment configuration and evaluation metric for craft series tasks.\nH.4. Tool\nThe tasks within the Tool category of our benchmark are designed to deeply investigate an AI agent’s\ncapabilities in tool utilization, precision in tool handling, and contextual application of various tools\nto carry out specific tasks. This category provides insights into the agent’s skill in wielding, using,\nand exploiting tools optimally within different Minecraft scenarios.\ndraw a bow and shoot\nset fires on the forest\nuse rein to tie up the animals\nplace the pumpkins and carve pumpkins with shears\nfly the trident on a rainy day\nFigure 14 | Examples of tasks in tool category.\nTask: use bow\nDescription: Draw a bow and shoot.\nPrecondition: Spawn the player in the plains biome with a bow in the mainhand and a stack of\narrows in the inventory.\nSkillAssessed: Precision, tool handling, and projectile mastery.\nEvaluation: Just run. < Draw the bow and shoot the arrow. < Hold the bow steady and charge up the\nshot before releasing the arrow.\nTask: set fires\n30\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nDescription: Set fires on the trees.\nPrecondition: Spawn the player in the forest biome with a flint_and_steel in its main hand.\nSkillAssessed: Environment manipulation and controlled chaos creation.\nEvaluation: Attack the tree. < Start a fire with the flint_and_steel. < Go wild with the fire.\nTask: lead animals\nDescription: Use rein to tie up the animals.\nPrecondition: Spawn the player in the plains biome with a stack of leads in its main hand. Spawn 5\nsheep and 5 cows near the player’s position.\nSkillAssessed: Entity interaction, tool application on moving entities, and livestock\nEvaluation: Ignore the animals and run away. < Use the rein to tie up animals.\nTask: carve pumpkins\nDescription: Place the pumpkins and carve pumpkins with shears.\nPrecondition: Spawn the player in the plains biome with a shear in its main hand and a stack of\npumpkins in the inventory.\nSkillAssessed: Block placement, block modification, and crafting interaction.\nEvaluation: Just run. < Place the pumpkin on the surface. < Use the shear to carve it. < Get a\ncarved pumpkin.\nTask: use trident\nDescription: Fly the trident on a rainy day.\nPrecondition: Spawn the player in the plains biome with a trident in the main hand, which is\nenchanted with riptide. The weather is rain.\nSkillAssessed: Weather-adaptive tool utilization, motion dynamics, and advanced weapon handling.\nEvaluation: Just run. < Use the trident to break the block. < Use the trident for quick movement.\n< Charge to throw the trident farther.\nPrompt 4 | The environment configuration and evaluation metric for tool series tasks.\nH.5. Survive\nThe tasks embedded within the survive category of our benchmark aim to analyze an AI agent’s\nability to ensure its own survival, adeptness in combat scenarios, and its capability to interact with\nthe environment in order to meet basic needs. Survival, being a core aspect of Minecraft gameplay,\nnecessitates an intricate balance of offensive, defensive, and sustenance-related actions. This category\nis structured to ensure a thorough evaluation of these skills.\nTask: hunt animals\nDescription: Hunt animals on the plains.\nPrecondition: Spawn the player in the plains biome with an iron sword in the main hand. Spawn 5\nsheep and 5 cows near the player’s position.\nSkillAssessed: Predator instincts, combat efficiency, and sustenance acquisition.\nEvaluation: Ignore animals and run away. < Hurt animals. < Kill animals.\nTask: combat enemies\nDescription: Fight the enemy spider.\nPrecondition: Spawn the player in the plains biome with a diamond sword in its main hand and a\nsuite of diamond equipment. Spawn 3 spiders in front of the player.\nSkillAssessed: Self-defense, offensive combat strategy, and equipment utilization.\nEvaluation: Ignore spiders and run away. < Hurt spiders. < Kill spiders.\nTask: use shield\nDescription: Use a shield to ward off zombies.\nPrecondition: Spawn the player in the plains biome with a shield in its main hand and a suite of\ndiamond equipment. Spawn 3 zombies in front of the player.\nSkillAssessed: Defensive tactics, tool application in combat, and strategic protection.\nEvaluation: Ignore zombies and run away. < Use the shield to protect itself.\nTask: plant wheats\nDescription: Use an iron_hoe to till the land and then plant wheat seeds.\nPrecondition: Spawn the player in the plains biome with an iron hoe in its main hand, and a stack\nof wheat seeds in the off hand.\nSkillAssessed: Land cultivation, planting proficiency, and sustainable resource creation.\nEvaluation: Just run away. < Till the land. < Plant the wheats.\nTask: sleep on the bed\nDescription: Place the bed on the surface and sleep.\nPrecondition: Spawn the player in the plains biome with a white bed in its main hand.\n31\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nhunt animals on the plains\nfight the enemy spider\nuse a shield to ward off zombies\nuse the hoe to till the land and plant wheat seeds\nplace the bed on the surface and sleep\nFigure 15 | Examples of tasks in survive category.\nSkillAssessed: Self-preservation, understanding of day-night cycle implications, and use of\nutilities for rest.\nEvaluation: Just run away. < Place the bed on the surface. < Sleep on the bed.\nPrompt 5 | The environment configuration and evaluation metric for survive series tasks.\nH.6. Build\nThe tasks within the build category of our benchmark are devised to evaluate an AI agent’s aptitude\nin structural reasoning, spatial organization, and its capability to interact with and manipulate\nthe environment to create specific structures or outcomes. Building is an integral component of\nMinecraft gameplay, requiring an intricate interplay of planning, creativity, and understanding of\nblock properties.\nTask: build pillar\nDescription: Build a pillar with dirt.\nPrecondition: Spawn the player in the plains biome with a stack of dirt in the main hand.\nSkillAssessed: Vertical construction and basic structure formation.\nEvaluation: Just run away. < Look down. < Jump and place the dirt. < Pile the dirt into a few\npillars. < Make a really high pillar.\nTask: dig three down and fill one up\nDescription: Dig three dirt blocks and fill the hole above.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Ground manipulation and depth perception.\nEvaluation: Just run away. < Look down. < Dig down three dirt blocks. < Raise the head. < Raise\nthe head and use dirt to fill the hole.\nTask: build gate\nDescription: Build an archway gate.\nPrecondition: Spawn the player in the plains biome with a stack of oak_planks in the main hand.\nSkillAssessed: Symmetry, planning, and aesthetic construction.\nEvaluation: Place no plank. < Build 1 pillar. < Build 2 pillars. < Build an archway gate.\n32\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nbuild a pillar with dirt\nbuild an archway gate\nmake obsidian by pouring water and lava buckets\nbuild snow golems by placing snow blocks and pumpkin\ndig three and fill one\nFigure 16 | Examples of tasks in build category.\nTask: build obsidian\nDescription: Make obsidian by pouring a water bucket and a lava bucket.\nPrecondition: Spawn the player in the plains biome with two water buckets and two lava buckets in\nthe Hotbar.\nSkillAssessed: Material transformation, understanding of in-game chemistry, and precise pouring.\nEvaluation: Just run away. < Pour water or lava. < Pour both liquids. < Pour into a mold to make\nobsidian.\nTask: build snow golems\nDescription: Build snow golems by placing two snow blocks and one carved pumpkin.\nPrecondition: Spawn the player in the plains biome with two stacks of snow blocks and two stacks\nof carved pumpkins in the Hotbar.\nSkillAssessed: Entity creation, sequential block placement, and combination of multiple materials.\nEvaluation: Place no block. < Place at least one kind of block. < Place both kinds of blocks. <\nBuild a snow golem.\nPrompt 6 | The environment configuration and evaluation metric for build series tasks.\n33\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nI. Text Conditioning\nText Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑎!\n𝑎$\n𝑎#\nDecoder as Policy\n𝑠!\n𝑠%\n𝑠\"\n𝑎!\n𝑎%\n𝑎\"\n𝑠%\n𝑠\"\nBehavior Cloning\nStates\nRollout Observations\n(a) Fine-tuning\n(b) Inference\n𝑠!\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n“mine block oak_log”\nText Instruction\nText Encoder\n“mine block grass”\nText Instruction\nFigure 17 | Fine-tuning GROOT to understand text instructions. We replace the original video encoder\nwith a text encoder to embed text instructions. The text encoder is fine-tuned to align with the learned goal\nspace. Left: During the fine-tuning, we freeze the learned decoder to provide the supervisory signal to train the\ntext encoder through behavior cloning. Right: During the inference, we can feed forward the text instructions\nto drive the policy to interact with the environment.\nAlthough video instruction has strong expressiveness, it still requires preparing at least one\ngameplay video for a new task. For most common tasks, such as collecting wood or stones, using\nnatural language to specify a goal is a more natural approach. In this section, we explore the possibility\nof aligning the pre-trained goal space with other modal instructions, such as text instructions.\nAligning text instructions with visual instructions in goal space has been validated as feasible\nby Lifshitz et al. (2023). They train a conditional variational autoencoder to project text into video\nspace after collecting 10,000 diversified text-video pairs, similar to what unCLIP did. However, the\nsuccess of this alignment method depends on the pre-alignment of visual and text spaces through\nlarge-scale contrastive pre-training (Fan et al., 2022). During the training process of GROOT, we did\nnot leverage the MineCLIP visual encoder to encode videos, instead trained goal space from scratch.\nOn the one hand, this is because MineCLIP can only handle short videos (only 16 frames); on the\nother hand, it is to free our goal space from the expressiveness bounded by pre-trained VLM.\nAccording to the above discussion, we choose to replace the video encoder in the GROOT\narchitecture with a text encoder, BERT, and directly optimize it through behavior cloning, as shown in\nFigure 17. In order to keep the original goal space, we freeze the decoder and regard it as a gradient\ngenerator that extracts high-level behavioral semantics from the demonstrations. We utilize the meta\ninformation in the contractor data to generate text-demonstration pairs. For example, in the task\nof “collect wood”, we identify the moment 𝑡when event “mine_block:oak_log” is triggered in the\nvideo, and we capture the frames within the range of [𝑡−127, 𝑡] to form a video clip, with “mine\nblock oak log” assigned as its text, thus constructing a sample. Having been fine-tuned on these data,\nour model demonstrated some steerabilities in the text instruction space, as shown in Table 6. We\nfind that the agent fine-tuned on the text-demonstration dataset shows a basic understanding of text\ninstructions. Our method exhibits progress in tasks such as “mine grass”, “mine wood”, “mine stone”,\n“mine seagrass”, “pickup beef” and “mine dirt”. However, it falls short in successfully completing tasks\nsuch as “mine seagrass”. We speculate that this may be related to the distribution of the data, as there\nis much less data available for “mine seagrass” compared to the other tasks (about 300 trajectories).\nWe emphasize that this experiment is very preliminary. In this experiment, the steerability\nof the agent fine-tuned on text instructions is still weak and it is hard to solve practical tasks.\n34\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 6 | Text conditioning results on resource collection tasks. Each episode lasts 30 seconds (600 frames).\nStatistics are measured over 10 episodes. The term \"baseline\" refers to the model before being fine-tuned, while\n\"fine-tuned\" refers to the final model after fine-tuning.\nVariant\nmine grass ↑\nmine wood ↑\nmine stone ↑\nmine seagrass ↓\npickup beef ↑\nmine dirt ↑\nbaseline\n3.9\n0.4\n1.8\n1.3\n0.0\n0.0\nfine-tuned\n17.3 (4.4×)\n3.7 (9.3×)\n11.5 (6.4×)\n1.2 (92%)\n0.1\n1.3\nGiven the limited diversity of text instructions in the provided contractor data, we don’t anticipate\nthe model to possess any significant level of generalization with regard to language instructions. To\nfurther verify this point, one needs to collect more diverse and higher-quality text-demonstration\npairs data. Anyway, this experimental result still indicates the possibility of optimizing the upstream\ninstruction generator by leveraging the pre-trained decoder. This creates possibilities for developing\nmore interesting applications on GROOT. Additional discussions on text-conditioning are beyond the\nscope of this paper, and we will leave them for future work.\nJ. Potential Applications and Integration with Planner\nGROOT is specialized in short-horizon instruction-following tasks with its goal being a video clip\nwhile LLM has demonstrated the ability to plan for long-horizon tasks in an open-world environment.\nFor example, DEPS Wang et al. (2023b) utilizes a text-conditioned policy from Cai et al. (2023) to\naccomplish tasks such as mining diamonds from scratch. By integrating GROOT into the DEPS\nframework, it can act as a controller and assist with long-sequence tasks. However, while LLM can\noutput language as the current goal, GROOT requires a specified video clip as its goal. Therefore,\nwhen combining GROOT with DEPS, it is necessary to use the visual language model CLIP to select\nthe most suitable video clip based on the language goal produced by LLM.\nThe proposed approach involves preparing a pre-existing library of video clips 𝑉= {𝑣𝑖} that\ncontains various actions performed by the agent in Minecraft (e.g., “chopping trees” or “mine iron\nore”). When given a long-horizon instruction by LLM’s Planner, it is decomposed into a series of\nshort-horizon language tasks {𝑔𝑖}. During task execution, the CLIP model is utilized to calculate the\nsimilarity between each short-horizon clip 𝑣𝑖in the library 𝑉and task 𝑔𝑖, selecting the most similar\nvideo clip as GROOT’s interaction goal with the environment. Additionally, accessing a video library\nof Minecraft content is effortless due to the abundance of available video data on the internet.\nWhile GROOT mainly relies on videos for input goals, LLM uses both input and output language\nmodalities. These modalities can be aligned using a visual language model, allowing us to combine\nGROOT as a short-horizon control policy with an LLM-based Planner to complete long-sequence\ntasks.\n35\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GROOT: Learning to Follow Instructions by Watching Gameplay Videos.pdf"}
{"title":"ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment","authors":"Shaofei Cai, Zhancun Mu, Anji Liu, Yitao Liang","summary":"We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.","url":"http:\/\/arxiv.org\/abs\/2503.02505v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.02505v1","published":1741087006000,"comment":null,"pdf_text":"ROCKET-2: Steering Visuomotor Policy via\nCross-View Goal Alignment\nShaofei Cai1, Zhancun Mu1, Anji Liu2 and Yitao Liang B1\n1Peking University, 2University of California, Los Angeles, All authors are affiliated with Team CraftJarvis\nWe aim to develop a goal specification method that is semantically clear, spatially sensitive, and intuitive\nfor human users to guide agent interactions in embodied environments. Specifically, we propose a novel\ncross-view goal alignment framework that allows users to specify target objects using segmentation\nmasks from their own camera views rather than the agent’s observations. We highlight that behavior\ncloning alone fails to align the agent’s behavior with human intent when the human and agent camera\nviews differ significantly. To address this, we introduce two auxiliary objectives: cross-view consistency\nloss and target visibility loss, which explicitly enhance the agent’s spatial reasoning ability. According to\nthis, we develop ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an improvement\nin the efficiency of inference 3× to 6×. We show ROCKET-2 can directly interpret goals from human\ncamera views for the first time, paving the way for better human-agent interaction. The project page is\navailable at https:\/\/craftjarvis.github.io\/ROCKET-2\/.\nFigure 1 | Cross-view Goal Alignment as a Human-Friendly Goal Specification Method. The target object\ncan be specified using a segmentation mask from the human’s camera view, while the agent learns to align with\nhuman intent and take actions based on its own observations. Visual landmarks serve as key cues to establish\nspatial relationships across camera views.\n1. Introduction\nLearning an agent to achieve desired goals is a\nlong-standing challenge in the field of embod-\nied intelligence, with significant implications for\nthe development of robots (Brohan et al., 2022,\n2023; Jang et al., 2022) and virtual players (Wang\net al., 2023a,b,c). A key challenge is to find goal\nrepresentations that are (i) flexible for human\nusers to specify and (ii) expressive and precise to\ncapture as many tasks as possible. Most current\napproaches address only one of these aspects. For\nexample, traditional works (Brohan et al., 2022;\nDriess et al., 2023; Lynch et al., 2023) focus on\ntraining agents to follow language instructions.\nAs pointed out in Cai et al. (2024b); Sundaresan\net al. (2024), while language is intuitive, it relies\non numerous prepositions to express spatial re-\nlationships, which can be vague and inefficient.\nFurthermore, language also suffers from the gen-\nCorresponding author(s): Yitao Liang\nShaofei Cai, Zhancun Mu <{caishaofei, muzhancun}@stu.pku.edu.cn>, Anji Liu <liuanji@cs.ucla.edu>, Yitao Liang <yitaol@pku.edu.cn>\narXiv:2503.02505v1  [cs.AI]  4 Mar 2025\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\neralization problem of novel visual concepts (Cai\net al., 2023). Realizing these limitations, some\nworks attempted to introduce visual modalities\ninto goal representations. For example, Sundare-\nsan et al. (2024) employs hand-drawn target\nlayouts in robot manipulation environments to\nrepresent human intent; Gu et al. (2023) uses\nend-effector trajectory sketches for fine-grained\ncontrol of robot arms; and ROCKET-1 (Cai et al.,\n2024b) specifies the objects to interact with by\napplying segmentation masks to the agent’s ob-\nservations. These methods have greatly improved\nthe expressiveness of spatial relationships and\ngeneralization across tasks. However, both trajec-\ntory sketches and object segmentation are closely\ntied to the agent’s current observation, causing\nissues in partially observable 3D worlds. These in-\nclude: (i) goals need to be generated in real-time\nas the agent’s camera view changes; (ii) goals\ncannot be specified when the target is occluded.\nTo strike a balance between expressiveness and\nflexibility, we propose an innovative and user-\nfriendly cross-view goal specification method. It\nallows human users to specify the target object\nusing segmentation masks from their own cam-\nera view, rather than the agent’s camera view.\nThe agent is then trained to align with human\nintent and take actions based on its own obser-\nvations via imitation learning. Decoupling the\ngoal specification from the camera view of the\nagent will significantly enhance the efficiency of\nhuman-agent interaction. However, the partial\nobservability of open worlds makes aligning goals\nacross camera views challenging. This involves\nhandling occlusion, geometric deformation, and\nthe distinction of objects of similar look. In Figure\n1, we show an agent in the left corner and a hu-\nman player standing on a farmland. The human\nintends to command the agent to hunt a sheep\nnear the house, even though the agent cannot\ninitially observe the target sheep. To achieve this,\nthe agent must establish spatial relationships us-\ning shared visual landmarks between the human’s\ncamera view and its own. We find that relying\nsolely on a behavior cloning loss is insufficient.\nTo address these challenges, we highlight an\nimportant property of behavior datasets (Cai\net al., 2024b): The target object remains con-\nsistent across camera views in an interaction\ntrajectory. Motivated by this, we propose two\nauxiliary objective functions: cross-view consis-\ntency loss and target visibility loss, to explicitly\nenhance the agent’s ability to align goals across\ncamera views. Specifically, cross-view consistency\nloss requires the agent to accurately predict the\ntarget object’s centroid point w.r.t. its camera view,\nwhile target visibility loss helps the agent deter-\nmine whether the target object is occluded. To\nfurther leverage temporal consistency, we use a\ncausal Transformer (Vaswani et al., 2017) in the\narchitecture to model the relationship between\npast predictions and the current observation. It\nencourages the agent to maintain tracking even\nwhen the target is occluded. By combining these\nauxiliary losses with behavior cloning loss, we\ndevelop ROCKET-2, a state-of-the-art agent in\nMinecraft. Our experiments show that ROCKET-\n2 can autonomously track the target object as the\ncamera changes, eliminating the need for SAM’s\n(Ravi et al., 2024) real-time semantic segmenta-\ntion, speeding up inference 3× to 6× compared\nto ROCKET-1. To our knowledge, we are the first\nto demonstrate that agents can interpret inten-\ntions from a human’s camera view and make de-\ncisions to achieve expected goals in the embodied\nworld. Extensive visualization experiments and\ncase studies offer deeper insights into its behavior.\nOur contributions are threefold: (1) We in-\ntroduce a user-friendly interface that allows hu-\nmans to specify goals using segmentation masks\nfrom their camera view, paving the way for better\nhuman-agent interaction. (2) We propose cross-\nview consistency loss and target visibility loss to ex-\nplicitly enhance the agent’s ability to align targets\nacross camera views and improve its steerabil-\nity. (3) We train ROCKET-2, an agent that au-\ntonomously tracks targets, eliminating the need\nfor real-time goal segmentation and significantly\nspeeding up inference.\n2. Related Works\nPartial Observability. We address policy learn-\ning in partially observable 3D worlds (Cai et al.,\n2024a; Savva et al., 2019), where the agent re-\nceives only egocentric images rather than the full\n2\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nenvironmental state. Since the movement of the\ncamera view is part of the action space, the policy\nmust actively explore to locate key objects, a com-\nmon challenge in visual navigation (Savva et al.,\n2019) and FPS games (Pearce and Zhu, 2022).\nWhile some methods (Huang et al., 2023; Jiang\net al., 2024) incorporate 3D point clouds to pro-\nvide global context, such data is often unavailable.\nMore commonly, memory-based architectures in-\ntegrate historical observations to build implicit\n3D perception. For example, Gadre et al. (2022);\nZhao et al. (2023) use RNN to avoid redundant ex-\nploration, and Baker et al. (2022) employs Trans-\nformerXL to retain achievements and complete\nthe long-horizon diamond challenge in Minecraft\n(Guss et al., 2019). Building on this, we require\nthe policy to align objects across camera views to\nensure consistent tracking of targets as the cam-\nera view changes. Although cross-view alignment\nhas been explored in computer vision for tasks\nlike BEV segmentation (Borse et al., 2023) and\npedestrian re-identification (Xu et al., 2019), we\nare the first to study its application in open-world\ndecision-making.\nGoal-Conditioned Imitation Learning. GCIL\nrefers to algorithms that optimize conditional poli-\ncies through imitation learning, primarily using\nbehavior cloning loss (Pomerleau, 1988). The\npolicy conditions can take various forms, such as\nlanguage (Brohan et al., 2022, 2023; Lynch et al.,\n2023), images (Lifshitz et al., 2023; Majumdar\net al., 2022; Sundaresan et al., 2024), videos (Cai\net al., 2023, 2024c), or trajectory sketches (Gu\net al., 2023; Wang et al.). Compared to tradi-\ntional imitation learning, GCIL provides a more\nexplicit target during training, reducing the com-\nplexity of modeling the entire behavior space and\nmaking the policy steerable during inference.\nHumans often use language to express their\nintentions, leading many studies (Brohan et al.,\n2022; Driess et al., 2023; Padalkar et al., 2023;\nWang et al., 2023c) to focus on learning language-\nconditioned policies. However, language goals\nare often ambiguous and struggle to capture spa-\ntial details (Cai et al., 2024b; Gu et al., 2023).\nIn tasks like navigation (Majumdar et al., 2022)\nand object manipulation (Wang et al.), image-\nbased goal modalities have been explored, where\nusers provide target images to guide the policy.\nWhile effective at conveying spatial information,\nimages often over-specify details, making policies\nsensitive to irrelevant factors like lighting, object\nappearance, or background textures. Sundaresan\net al. (2024) addresses these issues by replacing\nthe image with hand-drawn sketches. However,\ngenerating sketches that align with the current\nstate and the desired goal is non-trivial. Gu et al.\n(2023) further proposes using trajectory sketches\nfor finer control and better generalization across\ntasks, but this approach is not applicable in par-\ntially observable 3D worlds.\nROCKET-1 (Cai et al., 2024b) tackles inter-\naction problems in 3D worlds by training a vi-\nsuomotor policy to identify interaction targets\nbased on semantic segmentations in the visual\ncontext. While it resolves traditional goal images’\nambiguity and generation challenges, it relies\non SAM-2 (Ravi et al., 2024) to track goals and\nsegment them during inference, severely limit-\ning real-time performance. We propose a cross-\nview segmentation-conditioned policy that, un-\nlike ROCKET-1, enables the policy to align the\ngoal across camera views by itself and removes\nthe need for real-time segmentation.\nHindsight Trajectory Relabeling.\nThere are\ntwo main approaches to collecting labeled tra-\njectory data: (1) providing instructions for con-\ntractors to collect trajectories in real-time, en-\nsuring a causal link between actions and labels\nbut incurring high costs and limited scalability;\nand (2) gathering large amounts of trajectories\nand generating labels through post-processing,\nknown as hindsight trajectory relabeling. While\nthe first approach (Lynch et al., 2023; Padalkar\net al., 2023) produces higher-quality data, its\ncost constraints have led most research to adopt\nthe second, more scalable one. Andrychowicz\net al. (2017) was the first to reinterpret a trajec-\ntory’s behavior using its final frame, greatly im-\nproving data utilization and inspiring subsequent\nresearch on goal-conditioned policies. Lifshitz\net al. (2023) extended the approach by using the\nlast 16 frames of a trajectory as the more expres-\nsive goal. Sundaresan et al. (2024) introduced\nhand-drawn sketches as a goal modality, reducing\nsemantic ambiguity. Gu et al. (2023) converted\n3\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nthe robotic end-effector moving sketch to a 2D\nimage as the goal, providing richer procedural\ndetails. Cai et al. (2024b) introduced backward\ntrajectory relabeling, which first identifies inter-\naction objects and events, then utilizes object\ntracking models (Ravi et al., 2024) to generate\nframe-level segmentations. This data supports\ntraining segmentation-conditioned policies. Our\npaper explores using this dataset to train policies\nfor cross-view goal alignment.\n3. Methods\nIn this section, we first introduce the problem of\ncross-view segmentation-conditioned policy, dis-\ncussing it from the perspective of imitation learn-\ning. Next, we describe the process of generating\ncross-view trajectories annotated with semantic\nsegmentation. We then present two auxiliary ob-\njectives designed to enhance cross-view object\nalignment in 3D scenes: the cross-view consistency\nloss and the target visibility loss. Finally, we de-\ntail the architecture of ROCKET-2 and outline the\noverall optimization objectives.\nProblem Statement.\nOur goal is to learn a\ngoal-conditioned visuomotor policy, which al-\nlows humans to specify goal objects for inter-\naction using semantic segmentation across cam-\nera views. Formally, we aim to learn a policy\n𝜋cross(𝑎𝑡|𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}, 𝑐𝑔), where 𝑎𝑡represents the\naction at time 𝑡, 𝑐𝑔denotes the type of interac-\ntion. In the Minecraft environment, an action\ncorresponds to raw mouse and keyboard inputs.\n𝑜𝑡∈ℝ𝐻×𝑊×3 denotes the environment observa-\ntion at time 𝑡, and 𝑜𝑔∈ℝ𝐻×𝑊×3 represents an\nobservation of the local environment from a spe-\ncific camera view. Generally, 𝑜𝑔and 𝑜𝑡have some\nvisual content overlap. 𝑚𝑔∈{0, 1}𝐻×𝑊×1 is a seg-\nmentation mask for 𝑜𝑔, highlighting the target\nobject within the camera view 𝑜𝑔. During infer-\nence, users select a view 𝑜𝑔containing the desired\nobject from historical observations returned by\nthe environment and generates its correspond-\ning semantic segmentation 𝑚𝑔. To train such vi-\nsuomotor policy, we assume access to a dataset\nDcross = {𝑐𝑛, (𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡, 𝑚𝑛\n𝑡)𝐿(𝑛)\n𝑡=1 }𝑁\n𝑛=1 consisting of 𝑁\nsuccessful demonstration episodes, 𝐿(𝑛) is the\nlength of episode 𝑛. Within each episode, if\n𝑚𝑡is non-empty, all (𝑜𝑡, 𝑚𝑡) pairs indicate the\nsame object. Consequently, we can arbitrarily\npick one observation frame as the goal view con-\ndition for the entire trajectory.\nCross-View Dataset Generation. Without loss\nof generality, we use the Minecraft world as an\nexample to illustrate the data generation process.\nManually collecting datasets that meet the re-\nquirements is highly expensive. Thus, we employ\nthe backward trajectory relabeling technique pro-\nposed in Cai et al. (2024b) to automate the anno-\ntation of the OpenAI Contractor Dataset (Baker\net al., 2022), which consists of free-play trajecto-\nries from human players: Draw = {(𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡)𝐿(𝑛)\n𝑡=1 }𝑁\n𝑛=1.\nSpecifically, for any given episode 𝑛, we first de-\ntect all frames 𝑜𝑛\n𝑗where interaction events oc-\ncur, identify the interaction type 𝑐𝑛\n𝑗, and localize\nthe interacted object near frame 𝑗using bound-\ning boxes and point-based prompts. The SAM-2\n(Ravi et al., 2024) model is then employed to gen-\nerate the segmentation mask 𝑚𝑛\n𝑗for the object.\nStarting from frame 𝑗, we traverse the trajectory\nbackward and use the SAM-2 model to continu-\nously generate segmentation masks for the object\nin real-time until either a new interaction event\nis encountered or a maximum tracking length\nis reached. Let 𝑖denote the end frame. The re-\nsulting trajectory clip is then added to the train-\ning dataset: Dcross ←Dcross ∪{𝑐𝑗, (𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡, 𝑚𝑛\n𝑡) 𝑗\n𝑡=𝑖}.\nThis ensures that every extracted clip is associ-\nated with a consistent interaction intent. The\ngenerated data encompasses the fundamental in-\nteraction types in Minecraft, including use, break,\napproach, craft, and kill entity. Among these, ap-\nproach is a unique event, identified by detecting\ntrajectory clips where the displacement exceeds\na specified threshold. The object located at the\ncenter of the clip’s final frame is designated as\nthe goal of the approach event.\nCross-View Consistency Loss. Accurately in-\nterpreting the cross-view goal requires the pol-\nicy to possess cross-view visual object alignment\nability in 3D scenes. To achieve this, the model\nmust fully exploit visual cues from different cam-\nera views, such as scene layout and landmark\nbuildings, while being robust to challenges like\nocclusion, shape variations, and changes in dis-\ntance. We observe that relying solely on behav-\n4\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nT E M P O R A L \nt=10\nt=15\nt=22\nt=92\nt=94\nt=98\nDECODE\nDECODE\nDECODE\nDECODE\nDECODE\nDECODE\nKV Cached Causal Transformer\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n𝒐𝟏𝟎\n𝒐𝟏𝟓\n𝒐𝟐𝟐\n𝒐𝟗𝟐\n𝒐𝟗𝟒\n𝒐𝟗𝟖\n𝒄𝒈\"\n𝒄𝒈\"\n𝒄𝒈\"\n𝒄𝒈\"\"\n𝒄𝒈\"\"\n𝒄𝒈\"\"\n𝒂𝟏𝟎\n𝒑𝟏𝟎\n𝒗𝟏𝟎\n𝒂𝟏𝟓\n𝒑𝟏𝟓\n𝒗𝟏𝟓\n𝒂𝟐𝟐\n𝒑𝟐𝟐\n𝒗𝟐𝟐\n𝒂𝟗𝟐\n𝒑𝟗𝟐\n𝒗𝟗𝟐\n𝒂𝟗𝟒\n𝒑𝟗𝟒\n𝒗𝟗𝟒\n𝒂𝟗𝟖\n𝒑𝟗𝟖\n𝒗𝟗𝟖\nFigure 2 | ROCKET-2 Architecture. It consists of three parts: (1) a non-causal transformer for spatial fusion,\nwhich establishes the relationship between the agent’s and human’s camera views; (2) a causal transformer for\ntemporal fusion, ensuring consistency for goal tracking; (3) a decoder module, made of a feedforward neural\nnetwork (FFN), which predicts goal-related visuals cues and actions.\nior cloning loss (Pomerleau, 1988) is insufficient.\nTherefore, we propose a cross-view consistency\nloss. Since the segmentation across different cam-\nera views corresponds to the same object, we\ntrain the model to condition on the segmentation\nfrom one camera view to generate the segmen-\ntation for another camera view, thereby directly\nenhancing the model’s 3D spatial perception. To\nreduce computational complexity, we opt to pre-\ndict the centroid of the segmentation mask in-\nstead of the complete mask, formally expressed\nas: 𝜋cross(𝑝𝑡| 𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}, 𝑐𝑔), where\n𝑝𝑡=\nÍ𝐻\n𝑖=1\nÍ𝑊\n𝑗=1(𝑖, 𝑗) · 𝑚𝑡(𝑖, 𝑗)\nÍ𝐻\n𝑖=1\nÍ𝑊\n𝑗=1 𝑚𝑡(𝑖, 𝑗)\n.\n(1)\nIt is worth noting that incorporating the histor-\nical observation sequence 𝑜1:𝑡−1 as input is es-\nsential, especially when there is limited shared\nvisual content between 𝑜𝑡and 𝑜𝑔. This histori-\ncal sequence acts as a smooth bridge to facilitate\nalignment. Since the goal object represented by\nthe segmentation corresponds to the target of the\npolicy’s interaction, this auxiliary task aligns the\npolicy’s actions with its visual focus, effectively\nimproving task performance.\nTarget Visibility Loss. Due to the partial ob-\nservability in 3D environments, it is common for\ntarget objects in interaction trajectories to disap-\npear from the field of view and reappear later.\nDuring such intervals, the segmentation mask for\nthe missing object is empty. To leverage this infor-\nmation, we propose training the model to predict\nwhether the target object is currently visible, for-\nmulated as: 𝜋cross(𝑣𝑡| 𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}), where 𝑣𝑡is\na binary indicator for empty segmentation masks.\nOn the one hand, accurately predicting object\nvisibility helps the policy better match the target\nobject, avoiding a simple appearance similarity\nmeasurement between two frames. On the other\nhand, visibility information guides the policy to\nmake more reasonable decisions, such as confi-\ndently approaching the goal object when it is\nvisible or actively adjusting its camera to explore\nwhen the target is absent.\nROCKET-2 Architecture. Lets a training trajec-\ntory 𝑛be denoted as (𝑐𝑔, {𝑜𝑡, 𝑚𝑡}𝐿(𝑛)\n𝑡=1 ). A cross view\nindex 𝑔is sampled from {𝑖|𝑖∈[1, 𝐿(𝑛)], 𝑚𝑖≠𝜙}.\nWe resize all visual observations and their seg-\nmentation masks to 224 × 224. For encoding\nthe visual observation 𝑜𝑡, we utilize a DINO-\npretrained (Caron et al., 2021) 3-channel ViT-\nB\/16 (Dosovitskiy et al., 2020) (16 is the patch\nsize), which outputs a token sequence of length\n196, denoted as {ˆ𝑜𝑖\n𝑡}196\n𝑖=1 . Similarly, we encode the\nsegmentation mask 𝑚𝑡using a 1-input-channel\nViT-tiny\/16, yielding { ˆ𝑚𝑖\n𝑡}196\n𝑖=1 . The ViT-base\/16\nencoder is frozen during training for efficiency,\nwhile the ViT-tiny\/16 is trainable. To ensure spa-\ntial alignment, we fuse the cross-view condition\n(𝑜𝑔, 𝑚𝑔) by concatenating the feature channels:\nℎ𝑖\n𝑔= FFN(concat([ˆ𝑜𝑖\n𝑔∥ˆ𝑚𝑖\n𝑔])).\n(2)\n5\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nGiven the ability of self-attention mechanisms to\ncapture spatial details across views, we concate-\nnate the token sequences from two views into\na single sequence of length 392. A non-causal\nTransformer encoder module is applied (Vaswani\net al., 2017) for spatial fusion, obtaining a frame-\nlevel representation 𝑥𝑡:\n𝑥𝑡←SpatialFusion({ˆ𝑜𝑖\n𝑡}196\n𝑖=1 , {ℎ𝑖\n𝑔}196\n𝑖=1 ).\n(3)\nSubsequently,\nwe leverage a causal Trans-\nformerXL (Dai et al., 2019) architecture to cap-\nture temporal information across the sequence:\n𝑓𝑡←TransformerXL({𝑥𝑖}𝑡\n𝑖=1, 𝑐𝑔).\n(4)\nFinally, a simple linear layer maps 𝑓𝑡to predict\nthe action ˆ𝑎𝑡, centroid ˆ𝑝𝑡, and visibility ˆ𝑣𝑡. The\nloss function for episode 𝑛is defined as:\nL(𝑛) =\n𝐿(𝑛)\n∑︁\n𝑡=1\n−𝑎𝑛\n𝑡log ˆ𝑎𝑛\n𝑡−𝑝𝑛\n𝑡log ˆ𝑝𝑛\n𝑡−𝑣𝑛\n𝑡log ˆ𝑣𝑛\n𝑡. (5)\n4. Experiments\nWe aim to address the following questions: (1)\nHow does ROCKET-2 perform in terms of both\naccuracy and efficiency during inference? (2) Can\nROCKET-2 follow the intention of a human from\na cross-camera view? (3) Under what circum-\nstances does ROCKET-2 fail to work? (4) How\nimportant are landmarks in cross-view goal align-\nment? (5) Can ROCKET-2 interpret goal views\nfrom cross-episode scenarios? (6) Which modules\ncontribute effectively to training ROCKET-2?\n4.1. Experimental Setup\nImplementation Details. We present the model\narchitecture, hyperparameters, and optimizer\nconfigurations of ROCKET-2 in Table 1. Dur-\ning training, each trajectory is divided into seg-\nments of length 128 to reduce memory require-\nments. We initialize the view backbone that is\nused to encode RGB images with DINO weights\nand freeze it for training efficiency. During infer-\nence, ROCKET-2 can access up to 128 key-value\nattention caches of past observations. Most train-\ning parameters follow those from prior works such\nas Baker et al. (2022); Cai et al. (2024b).\nhunt the sheep in the \nright fence\nhunt the cow while do not \ntouch the sheep\ninteract with the left \nchest\nplace the oak door on the \ndiamond block\nuse bucket to get lava\nset fire on a trunk\nFigure 3 | The Evaluation Metric is Spatial-Sensitive.\n✓and × indicate the correct and incorrect objects for\ninteraction, respectively. None of the task configura-\ntions were seen during the training.\nTable 1 | Detailed Training Hyperparameters.\nHyperparameter\nValue\nInput Image Size\n224 × 224\nHidden Dimension\n1024\nView Backbone\nViT-base\/16 (DINO-v1)\nMask Backbone\nViT-tiny\/16 (1-channel)\nSpatial Transformer\nPyTorch Transformer\nNumber of Spatial Blocks\n4\nTemporal Transformer\nTransformerXL\nNumber of Temporal Blocks\n4\nTrajectory Chunk size\n128\nOptimizer\nAdamW\nLearning Rate\n0.00004\nEnvironment and Benchmark. We use the orig-\ninal Minecraft 1.16.5 (Cai et al., 2024a; Guss\net al., 2019; Lin et al., 2023) as our testing en-\nvironment, which accepts mouse and keyboard\ninputs and outputs a 640×360 RGB image at each\nstep. Following Cai et al. (2024b), we employ the\nMinecraft Interaction Benchmark to evaluate the\nagent’s interaction capabilities. This benchmark\nincludes six categories and a total of 12 tasks,\ncovering all basic Minecraft interaction types:\nHunt, Mine, Interact, Navigate, Tool, and Place.\nAs this benchmark emphasizes object interaction\nand spatial localization, its evaluation criteria are\nmore stringent than those in Lifshitz et al. (2023)\nand Cai et al. (2023). We show some examples in\nFigure 3. In the “hunt the sheep in the right fence”\ntask, success requires the agent to kill the sheep\nwithin the right fence, while killing it in the left\nfence results in failure. Similarly, in the “place\nthe oak door on the diamond block” task, success\nis only achieved if the oak door is adjacent to the\n6\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nFigure 4 | Performance-Efficiency Comparison on the Minecraft Interaction Benchmark. The x-axis\nrepresents inference speed (FPS), and the y-axis shows the interaction success rate. Numbers in parentheses\nindicate the Molmo invocation interval, where larger values mean higher FPS. “+ track” denotes real-time\nSAM-2 segmentation between Molmo calls, increasing inference time (applicable only to ROCKET-1). In most\ncases, ROCKET-2 achieves 3× to 6× faster while matching or surpassing ROCKET-1’s peak performance.\ndiamond block on at least one side.\nBaselines. We compare our ROCKET-2 with\nthe following instruction-following baselines: (1)\nSTEVE-1 (Lifshitz et al., 2023): An instruction-\nfollowing agent fine-tuned from VPT (Baker et al.,\n2022), capable of solving various short-horizon\ntasks. We use the text-conditioned version of\nSTEVE-1 for comparison. (2) GROOT-1 (Cai et al.,\n2023): A reference-video-conditioned policy de-\nsigned for open-ended tasks, trained on 2,000\nhours of long-form videos using latent variable\nmodels. (3) ROCKET-1 (Cai et al., 2024b): A\nsegmentation-conditioned policy capable of mas-\ntering 12 interaction tasks. While it achieves a\nhigh interaction success rate, its reliance on SAM-\n2’s real-time tracking during inference creates an\nefficiency bottleneck.\n4.2. Performance-Efficiency Analysis\nWe demonstrate that cross-view goal specification\n(implemented as ROCKET-2) significantly im-\nproves inference speed while maintaining high\ninteraction success. Following Cai et al. (2024b),\nwe evaluate STEVE-1, GROOT-1, and variants of\nROCKET-1 and our ROCKET-2 on the Minecraft\nInteraction Benchmark. Due to the limited spatial\nreasoning ability, STEVE-1 and GROOT-1 achieve\nsuccess rates below 20% on most tasks. We il-\nlustrate the inference pipeline of ROCKET-series\nTable 2 | ROCKET-Series Inference Pipeline Details.\nMolmo can pinpoint the target object based on the task\nprompt. SAM uses the point to generate object mask\n𝑚𝑡w.r.t. 𝑜𝑡and supports real-time object tracking.\nModel\nInference Pipeline\nR1(3)\nm1 ←SAM(𝑜1, Molmo(𝑜1, prompt))\n𝜋𝑅1(𝑎𝑡|𝑜1, m1, 𝑜2, 𝑜3, 𝑜4, m4, 𝑜5, 𝑜6, 𝑜7, m7, · · · )\nR1(30)+track\nm1:30 ←SAM(𝑜1:30, Molmo(𝑜1, prompt))\n𝜋𝑅1(𝑎𝑡|𝑜1, m1, 𝑜2, m2, 𝑜3, m3, 𝑜4, m4, · · · )\nR2(60)\nm1 ←SAM(𝑜1, Molmo(𝑜1, prompt))\n𝜋𝑅2(𝑎𝑡|𝑜1, m1, 𝑜2, 𝑜3, 𝑜4, · · · , 𝑜60, 𝑜61, m61, · · · )\nagents in Table 2. Automated evaluation of ROCK-\nETs relies on Molmo (Deitke et al., 2024) and\nSAM (Ravi et al., 2024) to generate a segmenta-\ntion mask for the target object in the given views.\nIn general, ROCKET-1 requires object masks for\nall agent observations, whereas ROCKET-2 only\nneeds one or a few object masks. We observe\nthat ROCKET-1 attains over 80% success with\nhigh-frequency Molmo point corrections (every 3\nframes) but suffers from slow inference. Lowering\nMolmo’s frequency greatly degrades ROCKET-1’s\nperformance, with tasks like “collecting lava” fail-\ning entirely at 30-frame intervals. While one can\nenable SAM’s tracking mode to provide dense\ngoal signals, it remains computationally expen-\nsive. In contrast, our ROCKET-2 decouples the\ngoal specification from the agent view, it does not\nneed frequent segmentation mask modification\n7\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nHuman View\nBird-Eye Map\nAgent View, t=96\nAgent View, t=109\nAgent View, t=125\nAgent View, t=143\nPredicted Point\nPredicted Point\nPredicted Point\nPredicted Point\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTrajectories\nHuman Position\nAgent Position\nGoal Mask\nHuman View\nBird-Eye Map\nAgent View, t=4\nAgent View, t=27\nAgent View, t=81\nAgent View, t=157\nHuman \nPosition\nAgent Position\nTrajectories\nPredicted Point\nPredicted Point\nPredicted Point\nPredicted Point\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTarget Visibility\nGoal Mask\nDistractor:\nFigure 5 | Case Study of Human-Agent Interaction. We demonstrate how a human interacts with ROCKET-2,\nleveraging its spatial reasoning abilities. (Top Row) The human specifies a hay bale (\n) that is not visible\nto ROCKET-2. By exploring the area around the visible landmark (house), ROCKET-2 successfully locates\nthe goal. (Bottom Row) The human specifies a target tree in the presence of a tree distractor. ROCKET-2\naccurately identifies the correct tree by reasoning about spatial relationships and landmarks. The agent’s\ntrajectories are visualized in bird’s-eye view maps.\nand can autonomously track the target object. It\nachieves comparable or superior performance to\nROCKET-1 with a 3× to 6× inference speedup.\n4.3. Intuitive Human-Agent Interaction\nIn Figure 5, we present two case studies illustrat-\ning ROCKET-2 interprets human intent under\nthe cross-view goal specification interface. The\nfirst case (top row) involves a task requiring the\nagent to approach a hay bale (\n) located behind a\nhouse (\n). From the human view, both the house\nand the hay bale are visible, whereas ROCKET-2\ninitially observes only the house. A key challenge\narises from the differing camera views: the hu-\nman and ROCKET-2 perceive the scene from op-\nposite sides of the house. To analyze the agent’s\nbehavior, we visualize both its camera views and\nits trajectories on a bird’s-eye map. We observe\nthat ROCKET-2 effectively infers the hay bale’s\npotential location and successfully navigates to-\nward it. This is reflected in the increasing target\nvisibility score and the movement of the predicted\npoint. Interestingly, the bird’s-eye view reveals\nthat ROCKET-2 approaches the target from both\nsides of the house, demonstrating diversity in\nroute selection. The second case (bottom row)\nshowcases ROCKET-2 ’s ability to distinguish\nbetween a distractor and the human-specified\ngoal object, despite their visual similarity. This\nfinding highlights that ROCKET-2 ’s spatial rea-\nsoning extends beyond object appearance and\nincorporates scene alignment for goal inference.\n4.4. Analyzing Failure Cases\nWe analyze failure cases in ROCKET-2 ’s task\nexecution and identify three main issues: (1) Pre-\ndiction Drift: When pursuing distant targets for\nextended periods, the predicted point gradually\nshifts away from the object. Since distant targets\nrely on temporal consistency from memory for\nrecognition, but the model was only trained with\nmemory lengths up to 128, it faces long-sequence\ngeneralization challenges during inference. (2)\nDistance Perception Error: When the goal and\nagent camera views differ significantly, the agent\nsometimes stops one step before reaching the tar-\nget, leading to interaction failure. We observe\nthat updating the goal view to the agent’s cur-\nrent observation alleviates this issue, likely due to\nthe greater cross-view discrepancy encountered\nduring inference compared to training. (3) Ac-\ntion Jitter: When inferring the original version of\nROCKET-2, we observe significant action jitter,\nthis could cause failures in precise interactions\nsuch as placing blocks. We find that incorporat-\ning previous actions during training and inference\n8\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nFigure 6 | Visualization Analysis of Cross-View Alignment. The vision patches (identified by white grid)\nrepresent a chosen background landmark in the agent’s current view (instead of the goal object). We generated\nan attention map with the spatial fusion transformer using these patches as queries and the goal view patches\nas keys and values. We found that ROCKET-2 perfectly aligned with the selected landmarks across views.\ngreatly improves action smoothness.\n4.5. Landmarks Attention Visualization\nProminent non-goal objects, referred to as “land-\nmarks”, play a crucial role in assisting humans or\nagents in localizing goal objects within a scene.\nFor instance, when multiple objects with simi-\nlar appearances are present, spatial relationships\nbetween the goal and landmarks can aid in dis-\ntinction. In this subsection, we aim to explore\nwhether ROCKET-2 implicitly learns landmark\nalignment by visualizing the attention weights of\nits spatial transformer.\nSpecifically, we prepare a current view obser-\nvation and a third view with goal segmentation.\nBefore being fed into the spatial transformer, both\nviews are encoded into 14 × 14 = 196 tokens:\n{ˆ𝑜𝑖\n𝑡}196\n𝑖=1 and {ℎ𝑖\n𝑔}196\n𝑖=1 (notations are consistent with\nSec. 3). We inspect the softmax-normalized at-\ntention map of the first self-attention layer in the\nspatial transformer, denoted as {𝑎𝑖,𝑗}392\n𝑖,𝑗=1, where\n𝑎𝑖,197:392 represents the attention map generated\nby using patch 𝑖from the current view as the\nquery and all patches from the third view as keys\nand values. This map is overlaid on the third\nview (goal view) to reflect its responsiveness to\npatch 𝑖in the current view. Since landmarks may\nspan multiple patches, we aggregate the response\nmaps of different patches to form the final atten-\ntion map {𝑚𝑖}196\n𝑖=1 :\n𝑚𝑖= 1\n|𝐿|\n∑︁\n𝑥∈𝐿\n𝑎𝑥,𝑖+196,\n(6)\nwhere 𝐿denotes the set of patches in the cur-\nrent view representing a specific landmark. No-\ntably, the selected landmarks do not overlap with\nthe goal segmentation. As shown in Figure 6,\nwe present four sets of data covering villages,\nplains, deserts, and forest terrains. In the first\nplot, the white grid indicates the selected land-\nmark patches, while the third plot shows the third\nview response to the chosen landmarks. Our find-\nings reveal that ROCKET-2 effectively matches\ncross-view consistency even under significant geo-\nmetric deformations and distance variations. Sur-\nprisingly, in data point (4), even subtle forest\ndepressions at a considerable distance are accu-\nrately matched.\n4.6. Cross-Episode Goal Alignment\nWe observe that ROCKET-2 exhibits cross-\nepisode generalization capabilities. As shown in\nFigure 7, the selected goal views come from dif-\nferent episodes, each generated with a unique\nworld seed. In the top-row example, the goal\nview is from a “bridge-building” episode set in\nthe savanna biome, where the player is placing\na dirt block to build the bridge. After feeding\n9\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nGoal View\nAgent Views\nt=10\nt=698\nt=702\nt=25\nt=258\nt=330\nFigure 7 | Cross-Episode Generalization. The goal\nview does not exist within the agent’s world but origi-\nnates from a different episode. We observe that the\nagent attempts to infer the semantic information un-\nderlying the goal specification.\nTable 3 | Ablation Study on Auxiliary Objectives.\nThe final loss function for each row is the cumulative\nsum of all loss functions from the preceding rows.\nTraining Variants\nAvg.\nbehavior cloning\n0.52\n0.78\n0.65\n0.65\n+target visibility\n0.63\n0.83\n0.68\n0.71\n+cross-view consistency\n0.85\n0.97\n1.00\n0.94\nforward the goal view, we place ROCKET-2 in\na forest biome and observe its behavior. We find\nthat it first exhibits pillar-jumping behavior, and\nafter placing many blocks, it begins to build the\nbridge horizontally. Although it ultimately failed\nto build the perfect bridge, the emergent behav-\nior still indicates that ROCKET-2 attempts to\nunderstand the underlying semantic information\nwhen there is no landmark match across views.\nIn the bottom row, the goal view is taken from a\nMinecraft creative mode, observing a house from\nthe sky— a view never seen during training. We\nfind that ROCKET-2 explores its environment\nand successfully identifies a visually similar house.\nThis demonstrates ROCKET-2 ’s robustness to a\nvariety of goal views.\n4.7. Ablation Studies on Auxiliary Objectives\nTo evaluate the impact of auxiliary losses on\nmodel performance, we define three variants: (1)\nonly behavior cloning loss, (2) + target visibility\nloss, and (3) the full version with + cross-view con-\nsistency loss. We conduct experiments on three\ntasks: Navigate to House in a Village(\n), Mine\nEmerald(\n), and Interact with the Left Chest(\n).\nWe find that the BC-only variant achieves an aver-\nage success rate of only 65%, demonstrating that\nthe action signal is insufficient for learning spatial\nalignment. Adding target visibility loss improves\nperformance by 6%, while further incorporating\ncross-view consistency loss boosts the success rate\nto 94%. This proves that leveraging temporal\nconsistency and introducing vision-based auxil-\niary losses can greatly enhance cross-view goal\nalignment and inference-time decision-making\ncapabilities.\n5. Conclusions and Limitations\nTo improve human-agent interaction in embod-\nied worlds, we propose a cross-view goal speci-\nfication approach. Since behavior cloning alone\nfails to align the agent with human views, we\nintroduce cross-view consistency and target vis-\nibility losses to enhance alignment. ROCKET-\n2 achieves state-of-the-art performance on the\nMinecraft Interaction Benchmark with a 3× to 6×\nefficiency boost. Visualizations and case stud-\nies validate our method. We also observe that\nROCKET-2 struggles with visual reasoning when\nthe discrepancy between the agent’s and human’s\nviews is large. As memory length increases, the\npredicted points exhibit noticeable drift. We at-\ntribute this to the relabeled dataset, whose mem-\nory window is short and view variation is limited.\nEnhancing data quality could help address this\nissue.\n6. Acknowledgements\nThis work is funded by the National Science and\nTechnology Major Project #2022ZD0114902.\n10\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nReferences\nM. Andrychowicz, D. Crow, A. Ray, J. Schneider,\nR. Fong, P. Welinder, B. McGrew, J. Tobin,\nP. Abbeel, and W. Zaremba. Hindsight expe-\nrience replay. ArXiv, abs\/1707.01495, 2017.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:3532908. 3\nB. Baker, I. Akkaya, P. Zhokhov, J. Huizinga,\nJ. Tang, A. Ecoffet, B. Houghton, R. Sampe-\ndro, and J. Clune.\nVideo pretraining (vpt):\nLearning to act by watching unlabeled online\nvideos. ArXiv, abs\/2206.11795, 2022. URL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:249953673. 3, 4, 6, 7\nS. Borse, M. Klingner, V. R. Kumar, H. Cai, A. Al-\nmuzairee, S. Yogamani, and F. Porikli. X-align:\nCross-modal cross-view alignment for bird’s-\neye-view segmentation. In Proceedings of the\nIEEE\/CVF Winter Conference on Applications of\nComputer Vision, pages 3287–3297, 2023. 3\nA. Brohan, N. Brown, J. Carbajal, Y. Chebo-\ntar, J. Dabis, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, J. Hsu, J. Ibarz,\nB. Ichter, A. Irpan, T. Jackson, S. Jesmonth,\nN. J. Joshi, R. C. Julian, D. Kalashnikov,\nY. Kuang,\nI. Leal,\nK.-H. Lee,\nS. Levine,\nY. Lu, U. Malla, D. Manjunath, I. Mordatch,\nO. Nachum, C. Parada, J. Peralta, E. Perez,\nK. Pertsch, J. Quiambao, K. Rao, M. S. Ryoo,\nG. Salazar, P. R. Sanketi, K. Sayed, J. Singh,\nS. A. Sontakke, A. Stone, C. Tan, H. Tran,\nV. Vanhoucke, S. Vega, Q. H. Vuong, F. Xia,\nT. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich.\nRt-1:\nRobotics transformer for real-world\ncontrol at scale. ArXiv, abs\/2212.06817, 2022.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:254591260. 1, 3\nA. Brohan, N. Brown, J. Carbajal, Y. Cheb-\notar, X. Chen, K. Choromanski, T. Ding,\nD. Driess, A. Dubey, C. Finn, et al.\nRt-2:\nVision-language-action models transfer web\nknowledge to robotic control. arXiv preprint\narXiv:2307.15818, 2023. 1, 3\nS. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and\nY. Liang. Groot: Learning to follow instructions\nby watching gameplay videos. In The Twelfth\nInternational Conference on Learning Represen-\ntations, 2023. 2, 3, 6, 7\nS. Cai, Z. Mu, K. He, B. Zhang, X. Zheng, A. Liu,\nand Y. Liang. Minestudio: A streamlined pack-\nage for minecraft ai agent development. 2024a.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:274992448. 2, 6\nS. Cai, Z. Wang, K. Lian, Z. Mu, X. Ma, A. Liu, and\nY. Liang. Rocket-1: Master open-world interac-\ntion with visual-temporal context prompting.\narXiv preprint arXiv:2410.17856, 2024b. 1, 2,\n3, 4, 6, 7\nS. Cai, B. Zhang, Z. Wang, H. Lin, X. Ma, A. Liu,\nand Y. Liang.\nGroot-2: Weakly supervised\nmulti-modal instruction following agents. arXiv\npreprint arXiv:2412.10410, 2024c. 3\nM. Caron, H. Touvron, I. Misra, H. Jégou,\nJ. Mairal, P. Bojanowski, and A. Joulin. Emerg-\ning properties in self-supervised vision trans-\nformers.\nIn Proceedings of the International\nConference on Computer Vision (ICCV), 2021. 5\nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le,\nand R. Salakhutdinov. Transformer-xl: Atten-\ntive language models beyond a fixed-length\ncontext.\nIn Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, Jan 2019.\ndoi: 10.18653\/v1\/\np19-1285. URL http:\/\/dx.doi.org\/10.\n18653\/v1\/p19-1285. 6\nM. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang,\nJ. S. Park, et al.\nMolmo and pixmo: Open\nweights and open data for state-of-the-art mul-\ntimodal models, 2024. URL https:\/\/arxiv.\norg\/abs\/2409.17146. 7\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby.\nAn image is worth 16x16\nwords: Transformers for image recognition\nat scale. ArXiv, abs\/2010.11929, 2020. URL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:225039882. 5\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch,\nA. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\n11\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nQ. Vuong, T. Yu, et al. Palm-e: An embodied\nmultimodal language model. arXiv preprint\narXiv:2303.03378, 2023. 1, 3\nS. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt,\nand S. Song. Clip on wheels: Zero-shot ob-\nject navigation as object localization and explo-\nration. arXiv preprint arXiv:2203.10421, 3(4):\n7, 2022. 3\nJ. Gu, S. Kirmani, P. Wohlhart, Y. Lu, M. G. Arenas,\nK. Rao, W. Yu, C. Fu, K. Gopalakrishnan, Z. Xu,\net al. Rt-trajectory: Robotic task generalization\nvia hindsight trajectory sketches. arXiv preprint\narXiv:2311.01977, 2023. 2, 3\nW. H. Guss, B. Houghton, N. Topin, P. Wang,\nC. Codel, M. M. Veloso, and R. Salakhutdinov.\nMinerl:\nA large-scale dataset of minecraft\ndemonstrations.\nIn International Joint Con-\nference on Artificial Intelligence, 2019.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:199000710. 3, 6\nJ. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang,\nQ. Li, S.-C. Zhu, B. Jia, and S. Huang.\nAn\nembodied generalist agent in 3d world. arXiv\npreprint arXiv:2311.12871, 2023. 3\nE. Jang, A. Irpan, M. Khansari, D. Kappler,\nF. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z:\nZero-shot task generalization with robotic imi-\ntation learning. ArXiv, abs\/2202.02005, 2022.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:237257594. 1\nJ. Jiang, Y. Yang, Y. Deng, C. Ma, and J. Zhang.\nBevnav: Robot autonomous navigation via\nspatial-temporal contrastive learning in bird’s-\neye view. IEEE Robotics and Automation Letters,\n2024. 3\nS. Lifshitz, K. Paster, H. Chan, J. Ba, and\nS. A. McIlraith.\nSteve-1:\nA generative\nmodel\nfor\ntext-to-behavior\nin\nminecraft.\nArXiv,\nabs\/2306.00937,\n2023.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:258999563. 3, 6, 7\nH. Lin, Z. Wang, J. Ma, and Y. Liang.\nMcu:\nA task-centric framework for open-ended\nagent evaluation in minecraft. arXiv preprint\narXiv:2310.08367, 2023. 6\nC. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker,\nR. Baruch, T. Armstrong, and P. Florence. Inter-\nactive language: Talking to robots in real time.\nIEEE Robotics and Automation Letters, 2023. 1,\n3\nA. Majumdar, G. Aggarwal, B. Devnani, J. Hoff-\nman, and D. Batra. Zson: Zero-shot object-goal\nnavigation using multimodal goal embed-\ndings.\nArXiv, abs\/2206.12403, 2022.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:250048645. 3\nA. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Her-\nzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh,\nA. Brohan, et al. Open x-embodiment: Robotic\nlearning datasets and rt-x models.\narXiv\npreprint arXiv:2310.08864, 2023. 3\nT. Pearce and J. Zhu. Counter-strike deathmatch\nwith large-scale behavioural cloning. In 2022\nIEEE Conference on Games (CoG), pages 104–\n111. IEEE, 2022. 3\nD. A. Pomerleau. Alvinn: An autonomous land\nvehicle in a neural network. Advances in neural\ninformation processing systems, 1, 1988. 3, 5\nN. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma,\nH. Khedr, R. Rädle, C. Rolland, L. Gustafson,\nE. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-\nY. Wu, R. Girshick, P. Dollár, and C. Feichten-\nhofer. Sam 2: Segment anything in images\nand videos. arXiv preprint arXiv:2408.00714,\n2024.\nURL https:\/\/arxiv.org\/abs\/\n2408.00714. 2, 3, 4, 7\nM. Savva, A. Kadian, O. Maksymets, Y. Zhao,\nE. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun,\nJ. Malik, et al. Habitat: A platform for embod-\nied ai research. In Proceedings of the IEEE\/CVF\ninternational conference on computer vision,\npages 9339–9347, 2019. 2, 3\nP. Sundaresan, Q. Vuong, J. Gu, P. Xu, T. Xiao,\nS. Kirmani, T. Yu, M. Stark, A. Jain, K. Haus-\nman, et al. Rt-sketch: Goal-conditioned imita-\ntion learning from hand-drawn sketches. 2024.\n1, 2, 3\nA. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In NIPS, 2017.\n12\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:13756489. 2, 6\nC. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu,\nY. Zhu, and A. Anandkumar. Mimicplay: Long-\nhorizon imitation learning by watching human\nplay. In 7th Annual Conference on Robot Learn-\ning. 3\nG. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao,\nY. Zhu, L. J. Fan, and A. Anandkumar. Voyager:\nAn open-ended embodied agent with large lan-\nguage models. ArXiv, abs\/2305.16291, 2023a.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:258887849. 1\nZ. Wang, S. Cai, G. Chen, A. Liu, X. S. Ma, and\nY. Liang. Describe, explain, plan and select:\ninteractive planning with llms enables open-\nworld multi-task agents. Advances in Neural\nInformation Processing Systems, 36, 2023b. 1\nZ. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang,\nH. Lin, Z. He, Z. Zheng, Y. Yang, et al. Jarvis-\n1: Open-world multi-task agents with memory-\naugmented multimodal language models. arXiv\npreprint arXiv:2311.05997, 2023c. 1, 3\nD. Xu, J. Chen, C. Liang, Z. Wang, and R. Hu.\nCross-view identical part area alignment for\nperson re-identification.\nIn ICASSP 2019 -\n2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages\n2462–2466, 2019. doi: 10.1109\/ICASSP.2019.\n8683137. 3\nQ. Zhao, L. Zhang, B. He, H. Qiao, and Z. Liu.\nZero-shot object goal visual navigation. In 2023\nIEEE International Conference on Robotics and\nAutomation (ICRA), pages 2025–2031. IEEE,\n2023. 3\n13\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment.pdf"}
{"title":"Open-World Reinforcement Learning over Long Short-Term Imagination","authors":"Jiajian Li, Qi Wang, Yunbo Wang, Xin Jin, Yang Li, Wenjun Zeng, Xiaokang Yang","summary":"Training visual reinforcement learning agents in a high-dimensional open\nworld presents significant challenges. While various model-based methods have\nimproved sample efficiency by learning interactive world models, these agents\ntend to be \"short-sighted\", as they are typically trained on short snippets of\nimagined experiences. We argue that the primary challenge in open-world\ndecision-making is improving the exploration efficiency across a vast state\nspace, especially for tasks that demand consideration of long-horizon payoffs.\nIn this paper, we present LS-Imagine, which extends the imagination horizon\nwithin a limited number of state transition steps, enabling the agent to\nexplore behaviors that potentially lead to promising long-term feedback. The\nfoundation of our approach is to build a $\\textit{long short-term world\nmodel}$. To achieve this, we simulate goal-conditioned jumpy state transitions\nand compute corresponding affordance maps by zooming in on specific areas\nwithin single images. This facilitates the integration of direct long-term\nvalues into behavior learning. Our method demonstrates significant improvements\nover state-of-the-art techniques in MineDojo.","url":"http:\/\/arxiv.org\/abs\/2410.03618v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.03618v2","published":1728062250000,"comment":"Accepted by ICLR 2025 Oral. Project page:\n  https:\/\/qiwang067.github.io\/ls-imagine","pdf_text":"Published as a conference paper at ICLR 2025\nOPEN-WORLD REINFORCEMENT LEARNING OVER\nLONG SHORT-TERM IMAGINATION\nJiajian Li1∗\nQi Wang1,2∗\nYunbo Wang1†\nXin Jin2\nYang Li3\nWenjun Zeng2\nXiaokang Yang1\n1 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\n2 Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China\n3 School of Computer Science and Technology, East China Normal University, Shanghai, China\nhttps:\/\/qiwang067.github.io\/ls-imagine\nABSTRACT\nTraining visual reinforcement learning agents in a high-dimensional open world\npresents significant challenges. While various model-based methods have im-\nproved sample efficiency by learning interactive world models, these agents tend\nto be “short-sighted”, as they are typically trained on short snippets of imagined\nexperiences. We argue that the primary challenge in open-world decision-making\nis improving the exploration efficiency across a vast state space, especially for\ntasks that demand consideration of long-horizon payoffs. In this paper, we present\nLS-Imagine, which extends the imagination horizon within a limited number of\nstate transition steps, enabling the agent to explore behaviors that potentially lead\nto promising long-term feedback. The foundation of our approach is to build a long\nshort-term world model. To achieve this, we simulate goal-conditioned jumpy state\ntransitions and compute corresponding affordance maps by zooming in on specific\nareas within single images. This facilitates the integration of direct long-term\nvalues into behavior learning. Our method demonstrates significant improvements\nover state-of-the-art techniques in MineDojo.\n1\nINTRODUCTION\nOpen-world decision-making in the context of reinforcement learning (RL) involves the following\ncharacteristics: (i) The agent operates within an interactive environment that features a vast state\nspace; (ii) The learned policy presents a high degree of flexibility, allowing interaction with various\nobjects in the environment; (iii) The agent lacks full visibility of the internal states and physical\ndynamics of the external world, meaning that its perception of the environment (e.g., raw images)\ncarries substantial uncertainty. For example, Minecraft serves as a typical open-world game.\nBuilding upon recent progress in visual control, open-world decision-making aims to train agents to ap-\nproach human-level intelligence based solely on high-dimensional visual observations. However, this\npresents significant challenges. For example, in Minecraft tasks, existing methods like Voyager (Wang\net al., 2024a) employ specific Minecraft APIs as the high-level controller, which is incompatible with\nstandard visual control settings. While approaches such as PPO-with-MineCLIP (Fan et al., 2022) and\nDECKARD (Nottingham et al., 2023) perform low-level visual control, these model-free RL methods\nstruggle to grasp the underlying mechanics of the environment. This may result in high trial-and-error\ncosts, leading to inefficiencies in both exploration and sample usage. Although DreamerV3 (Hafner\net al., 2023) employs a model-based RL (MBRL) approach to improve sample efficiency, it is often\n“short-sighted” since the policy is optimized using short-term experiences—typically 15 time steps—\ngenerated by the world model. The absence of long-term guidance significantly hampers an effective\nexploration of the vast solution space of the open world.\nTo improve the behavior learning efficiency of MBRL, in this paper, we introduce a novel method\nnamed Long Short-Term Imagination (LS-Imagine). Our key approach involves enabling the world\nmodel to efficiently simulate the long-term effects of specific behaviors without the need for repeatedly\n∗Equal contribution.\n†Corresponding author: Yunbo Wang <yunbow@sjtu.edu.cn>.\n1\narXiv:2410.03618v2  [cs.LG]  4 Mar 2025\nPublished as a conference paper at ICLR 2025\nCut a tree\n🏆\nenvironment\nShort-Term Imagination \nLong-Term Imagination \nFigure 1: The general framework of LS-Imagine, an MBRL agent that operates solely on raw pixels.\nThe fundamental idea is to extend the imagination horizon within a limited number of state transition\nsteps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback.\nrolling out one-step predictions. As illustrated in Figure 1, once trained, the world model provides\nboth instant and jumpy state transitions1 along with corresponding (intrinsic) rewards, facilitating\npolicy optimization in a joint space of short- and long-term imaginations. This encourages the agent\nto explore behaviors that lead to promising long-term outcomes.\nThe foundation of LS-Imagine is to train a long short-term world model, which requires integrating\ntask-specific guidance into the representation learning phase based on off-policy experience replay.\nHowever, this creates a classic “chicken-and-egg” dilemma: without true data showing the agent has\nreached the goal, how can we effectively train the model to simulate jumpy transitions from current\nstates to pivotal future states that suggest a high likelihood of achieving that goal? To address this\nissue, we first continuously zoom in on individual images to simulate the consecutive video frames as\nthe agent approaches the goal. We then generate affordance maps2 by evaluating the relevance of the\npseudo video to task-specific goals presented in textual instructions, using the established MineCLIP\nreward model (Fan et al., 2022). Subsequently, we train specific branches of the world model to\ncapture both instant and jumpy state transitions, using pairs of image observations from adjacent time\nsteps as well as those across longer intervals. Finally, we optimize the agent’s policy based on a finite\nsequence of imagined latent states generated by the world model, integrating a more direct estimate\nof long-term values into decision-making.\nLet’s use the example in Figure 1 to further elaborate the novel aspects of the behavior learning\nprocess: After receiving the instruction “cut a tree”, the agent simulates near-future states based on\nthe current real observation. It initially performs several single-step rollouts until it identifies a point\nin time for a long-distance state jump that allows it to approach the tree. The agent then executes this\njump and optimizes its policy network to maximize the long-sight value function.\nWe evaluate our approach in the challenging open-world tasks from MineDojo (Fan et al., 2022).\nLS-Imagine demonstrates superior performance compared to existing visual RL methods.\nThe contributions of this work are summarized as follows:\n• We present a novel model-based RL method that captures both instant and jumpy state transitions\nand leverages them in behavior learning to improve exploration efficiency in the open world.\n• Our approach presents four concrete contributions: (i) a long short-term world model architecture,\n(ii) a method for generating affordance maps through image zoom-in, (iii) a novel form of intrinsic\nrewards based on the affordance map, and (iv) an improved behavior learning method that integrates\nlong-term values and operates on a mixed long short-term imagination pathway.\n2\nPROBLEM FORMULATION AND NOTATIONS\nWe solve visual reinforcement learning as a partially observable Markov decision process (POMDP),\nusing MineDojo as the test bench. Specifically, our method manipulates low-dimensional control\nsignals at while receiving only sequential high-dimensional visual observations o<t and episodic\nsparse rewards renv, without access to the internal APIs of the open-world games. In comparison, as\n1As shown in Figure 1, a jumpy transition allows the agent to bypass intermediate states and directly simulate\na task-relevant future state st+H in one step. This process occurs exclusively during world model imagination.\n2Affordance maps highlight regions within an observation that are potentially relevant to the task (Qi et al.,\n2020; Wang et al., 2022).\n2\nPublished as a conference paper at ICLR 2025\nTable 1: Experimental setups of the Minecraft AI agents. IL is short for imitation learning.\nModel\nController\nObservation\nVideo Demos\nDECKARD (2023)\nRL\nPixels & Inventory\n✓\nAuto MC-Reward (2024a)\nIL + RL\nPixels & GPS\n✗\nVoyager (2024a)\nGPT-4\nMinecraft simulation & Error trace\n✗\nDEPS (2023)\nIL\nPixels & Yaw\/pitch angle & GPS & Voxel\n✗\nSTEVE-1 (2023)\nGenerative model\nPixels\n✗\nVPT (2022)\nIL + RL\nPixels\n✓\nDreamerV3 (2023)\nRL\nPixels\n✗\nLS-Imagine\nRL\nPixels\n✗\nshown in Table 1, existing Minecraft agents present notable distinctions in learning paradigms (i.e.,\ncontroller), observation data, and the use of expert demonstrations.\nThe world model presented in this paper consists of two main components: a short-term transition\nbranch and a long-term imagination branch. As a result, it employs a complex notation system. We\nnow introduce the key notations that will be frequently used throughout the paper:\n• Mt represents the affordance map.\n• ct denotes the episode continuation flag.\n• jt is the jumping flag that triggers jumpy state imaginations.\n• ∆t represents the number of environmental steps between the jumpy transitions.\n• Gt is the cumulative reward over ∆t.\nWe use (o′\nt, a′\nt, M′\nt, r′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt) to represent the simulated environment data that are used to\ntrain the long-term imagination branch of the world model. The policy is learned on trajectories of\nmixed long- and short-term imaginations {(ˆst, ˆat, ˆrt, ˆct, ˆjt, ˆ∆t, ˆGt)}, where ˆst represents the latent\nstate, and the variables predicted by the model are indicated using the superscript (ˆ).\n3\nMETHOD\n3.1\nOVERVIEW OF LS-IMAGINE\nIn this section, we present the details of LS-Imagine, which involves the following algorithm steps,\nincluding world model learning, behavior learning, and environment interaction:\n1. Affordance map computation (Sec. 3.2.1): We employ a sliding bounding box to scan individual\nimages and execute continuous zoom-ins inside the bounding box, simulating consecutive video\nframes that correspond to long-distance state transitions. We then create affordance maps by\nassessing the relevance of the fake video clips to task-specific goals expressed in text using the\nestablished MineCLIP reward model (Fan et al., 2022).\n2. Rapid affordance map generation (Sec. 3.2.2): Given that affordance maps will be frequently\nused in subsequent Step 5 to evaluate the necessities for jumpy state transitions, we train a U-Net\nmodule to approximate the affordance maps annotated in Step 1 for the sake of efficiency.\n3. World model training (Sec. 3.3): We train the world model to capture short- and long-term state\ntransitions, using replay data with high responses from the affordance map. Each trajectory from\nthe buffer includes pairs of samples from both adjacent time steps and long-distance intervals.\n4. Behavior learning (Sec. 3.4): We perform an actor-critic algorithm to optimize the agent’s policy\nbased on a finite sequence of long short-term imaginations generated by the world model.\n5. Data update: We apply the agent to interact with the environment and gather new data. Next,\nwe leverage the generated affordance map to efficiently filter sample pairs suitable for long-term\nmodeling, incorporating both short- and long-term sample pairs to update the replay buffer.\n6. Iterate Steps 3–5.\nBelow, we discuss each training step in detail. The full algorithm can be found in Appendix C.4.\n3.2\nAFFORDANCE MAP AND INTRINSIC REWARD\nWe generate affordance maps using visual observations and textual task definitions to improve the\nsample efficiency of model-based reinforcement learning in open-world tasks. The core idea is to di-\nrect the agent’s attention to task-relevant areas of the visual observation, leading to higher exploration\n3\nPublished as a conference paper at ICLR 2025\n...\nMineCLIP\nCut a tree\n...\n16 frames of video \nto represent the\nsimulated exploration\nCorr = 0.60\nCorr = 0.90\nCorr = 0.15\n Fusion \n(a) Affordance map calculation\nMultimodal U-Net\nZoom\nin\nMultimodal U-Net\nMultimodal U-Net\nenv\nagent\n(b) Rapid affordance map generation\nFigure 2: The two steps for on-the-fly affordance map estimation: (a) Simulate exploration via image\nzoom-in and calculate the task-correlation scores of the virtual explorations using MineCLIP. (b)\nLearn to generate affordance maps more efficiently using a multimodal U-Net.\nefficiency. Let Mot,I(w, h) be the affordance map that represents the potential exploration value at\npixel position (w, h) on the image observation ot, given textual instruction I (e.g., “cut a tree”). The\naffordance map highlights the relevance between regions of the observation and the task description,\nserving as a spatial prior that effectively directs the agent’s exploration toward areas of interest.\n3.2.1\nAFFORDANCE MAP COMPUTATION VIA VIRTUAL EXPLORATION\nTo create the affordance map, as shown in Figure 2(a), we simulate and evaluate the agent’s exploration\nwithout relying on real successful trajectories. Concretely, we first adopt a random agent to interact\nwith task-specific environments for data collection. Starting with the agent’s observation ot at time\nstep t, we use a sliding bounding box with dimensions scaled to 15% of the observation’s width and\nheight to traverse the entire observation from left to right and top to bottom. The sliding bounding\nbox moves horizontally and vertically in 9 steps, respectively, covering every potential region in\nboth dimensions. For each position on the sliding bounding box of the observation ot, we crop 16\nimages from ot. These cropped images narrow the field of view to focus on the region and are resized\nback to the original image dimensions. These resized images are denoted as xk\nt (where 0 ≤k < 16).\nThe ordered set Xt = [xk\nt | k = 0, 1, . . . , 15] represents a sequence of 16 frames simulating the\nvisual transition as the agent moves towards the position specified by the current sliding bounding\nbox. Subsequently, we employ the MineCLIP model3 to calculate the correlation between the Xt of\nimages, simulating the virtual exploration process, and the task description I. In this way, we quantify\nthe affordance value of the sliding bounding box, indicating the potential exploration interest of the\narea. After calculating the correlation score for each sliding bounding box, we fuse these values to\nobtain a smooth affordance map Mot,I. For pixels that are covered by multiple sliding bounding\nboxes due to overlapping regions, the integrated affordance value is obtained by averaging the values\nfrom all the overlapping windows.\n3.2.2\nMULTIMODAL U-NET FOR RAPID AFFORDANCE MAP GENERATION\nThe annotation of affordance maps, as previously described, involves extensive window traversal and\ncomputations for each window position using a pre-trained video-text alignment model. This method\nis computationally demanding and time-consuming, making real-time applications challenging. To\naddress this issue, we first use a random agent to interact with the environment for data collection.\nNext, we annotate the affordance maps for the collected images using the aforementioned method\nbased on virtual exploration. We gather a dataset of tuples (ot, I, Mot,I) and use it to train a\nmultimodal U-Net based on Swin-Unet (Cao et al., 2022). To handle multimodal inputs, we extract\ntext features from the language instructions and image features from the downsampling process of\nSwin-Unet, and fuse them with multi-head attention. We present architecture details in Figure 9 in the\nappendix. In this way, with the pretrained multimodal U-Net, we can efficiently generate affordance\nmaps at each time step using visual observations and language instructions.\n3MineCLIP (Fan et al., 2022) pretrains a video-language representation using Minecraft videos, enabling it\nto compute the correlation between a text string and a 16-frame video segment.\n4\nPublished as a conference paper at ICLR 2025\n3.2.3\nAFFORDANCE-DRIVEN INTRINSIC REWARD\nTo leverage the task-relevant prior knowledge presented by the affordance map for efficient exploration\nin the open world, we introduce the following intrinsic reward function:\nrintr\nt\n=\n1\nWH\nW\nX\nw=1\nH\nX\nh=1\nMot,I(w, h) · G(w, h),\n(1)\nwhere W and H denote the width and height of the visual observation. G represents a Gaussian\nmatrix with dimensions matching those of the affordance map. It corresponds to a 2D Gaussian\ndistribution, with its peak located at the center of the affordance map. The values in the matrix are\ndetermined by standard deviations (σx, σy), while the mean is uniformly set to 1 across the entire\nmatrix. We present visualizations of G and conduct hyperparameter analyses on (σx, σy) in Appendix\nD.6. The intuition behind this design is to encourage the agent to move toward the target.\nOverall, the agent receives a composite reward consisting of the episodic sparse reward from the\nenvironment, the reward from MineCLIP (Fan et al., 2022), and the intrinsic reward from the\naffordance map: rt = renv\nt\n+ rMineCLIP\nt\n+ αrintr\nt , where α is a hyperparameter. In contrast to the\nMineCLIP reward, which relies on the agent’s past performance, our affordance-driven intrinsic\nreward emphasizes long-term values derived from future virtual exploration. It encourages the agent\nto adjust the policy to pursue task-related targets when they appear in its view, ensuring these targets\nare centrally positioned in future visual observations to maximize this reward function.\n3.3\nLONG SHORT-TERM WORLD MODEL\n3.3.1\nLEARNING JUMPING FLAGS\nIn LS-Imagine, the world model is customized for long-term and short-term state transitions. It\ndecides which type of transition to adopt based on the current state and predicts the next state\nwith the selected transition branch. To facilitate the switch between long-term and short-term state\ntransitions, we introduce a jumping flag jt, which indicates whether a jumpy transition or long-term\nstate transition, should be adopted at time step t. When a distant task-related target appears in the\nagent’s observation, which can be reflected by a higher kurtosis in the affordance map, a jumpy\ntransition allows the agent to imagine the future state of approaching the target. To this end, we\ndefine relative kurtosis Kr which measures whether there are significantly higher target areas than\nthe surrounding areas in the affordance map, and absolute kurtosis Ka represents the confidence level\nof target presence in that area. Formally,\nKr =\n1\nWH\nW\nX\nw=1\nH\nX\nh=1\n\"\u0012Mo,I(w, h) −mean(Mo,I)\nstd(Mo,I)\n\u00134#\n,\nKa = max(Mo,I) −mean(Mo,I).\n(2)\nTo normalize the relative kurtosis, we apply the sigmoid function to it, and then multiply it by the\nabsolute kurtosis to calculate the jumping probability:\nPjump = sigmoid(Kr) × Ka.\n(3)\nThe jumping probability measures the confidence in the presence of task-relevant targets far from the\nagent in the visual observation. To determine whether to employ long-term state transition, we use a\ndynamic threshold, which is the mean of the collected jumping probabilities at each time step, plus\none standard deviation. For a detailed explanation, please refer to C.1. If Pjump exceeds this threshold,\nthe jump flag jt is True and the agent switches to jumpy state transitions in the imagination phase.\n3.3.2\nLEARNING JUMPY STATE TRANSITIONS\nIn LS-Imagine, the state transition model includes both short-term and long-term branches. As\nshown in Figure 3 (a), the short-term transition model integrates the previous deterministic recurrent\nstate ht−1, stochastic state zt−1, and action at−1 to adopt the single-step transition. In contrast, the\nlong-term branch simulates jumpy state transitions toward the target. It is important to clarify that the\nindex t does not denote the time step during real environmental interactions but instead represents the\npositional order of states within the imagination sequence. The overall world model of LS-Imagine is\n5\nPublished as a conference paper at ICLR 2025\nenc\ndec\nenc\ndec\nShort-Term Branch\nCritic\nActor\nCritic\nCritic\nActor\nCritic\nShort-Term Imagination\nLong-Term Imagination\nTransition model\nReward predictor\nJump predictor\nInterval predictor\nLong-Term Branch\nFigure 3: The overall architecture of the world model and the behavior learning process.\nprimarily based on DreamerV3 (Hafner et al., 2023), with novel components specifically designed to\ncapture jumpy state transitions:\nShort-term transition model:\nht = fϕ(ht−1, zt−1, at−1)\nLong-term transition model:\nh′\nt = fϕ(ht−1, zt−1)\nEncoder:\nzt ∼qϕ(zt | ht, ot, Mt)\nDynamics predictor:\nˆzt ∼pϕ(ˆzt | ht)\nReward predictor:\nˆrt, ˆct ∼pϕ(ˆrt, ˆct | ht, zt)\nDecoder:\nˆot, ˆ\nMt ∼pϕ(ˆot, ˆ\nMt | ht, zt)\nJump predictor:\nˆjt ∼pϕ(ˆjt | ht, zt)\nInterval predictor:\nˆ∆′\nt, ˆG′\nt ∼pϕ( ˆ∆′\nt, ˆG′\nt | ht−1, zt−1, h′\nt, z′\nt)\n.\n(4)\nAt time step t, we feed the recurrent state ht, the observation ot, and the affordance map Mt into the\nencoder to obtain posterior state zt. We also use the affordance map as an input of the encoder, which\nserves as the goal-conditioned prior guidance to the agent. Notably, the prediction of prior state ˆzt\ndoes not involve the current observation or affordance map, relying solely on historical information.\nWe use (ht, zt) to reconstruct the visual observation ˆot and the affordance map ˆ\nMt, and predict the\nreward ˆrt, episode continuation flag ˆct, and jumping flag ˆjt. For long-term state transitions, we use an\ninterval predictor to estimate the expected number of interaction steps ˆ∆′\nt required to transition from\nthe pre-jump state (ht−1, zt−1) to the post-jump state (h′\nt, z′\nt), along with the expected cumulative\nreward ˆG′\nt that the agent may receive during this time interval. We detail the approach to annotate ∆′\nt\nand G′\nt using the real interaction data in Appendix C.1.\nWe collect short-term tuples Dt = (ot, at, Mt, rt, ct, jt, ∆t, Gt) from each interaction with the\nenvironment using the current policy. When observing jt = 1, we additionally construct long-term\ntuples D′\nt+1 = (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1) based on Dt. More details for this\nprocess can be found in Appendix C.1. During representation learning, we sample short-term tuples\n{Dt}T\nt=1 and the long-term tuples following jumpy transitions {D′\nt+1}t∈T from the replay buffer\nB, where T denotes the set of time steps at which long-term state transitions are required. The loss\nfunctions for each component of the short-term and long-term world model branch are as follows:\nShort-term branch:\n\n\n\n\n\n\n\n\n\nLdyn .= max (1, KL [sg (qϕ (zt | ht, ot, Mt)) ∥pϕ (zt | ht)])\nLenc .= max (1, KL [qϕ (zt | ht, ot, Mt) ∥sg (pϕ (zt | ht))])\nLdec .= −ln pϕ (ot, Mt | ht, zt)\nLpred .= −ln pϕ (rt, ct | ht, zt) −ln pϕ (jt | ht, zt)\n.\n(5)\n6\nPublished as a conference paper at ICLR 2025\nLong-term branch:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL′\ndyn\n.= max (1, KL [sg (qϕ (z′\nt | h′\nt, o′\nt, M′\nt)) ∥pϕ (z′\nt | h′\nt)])\nL′\nenc\n.= max (1, KL [qϕ (z′\nt | h′\nt, o′\nt, M′\nt) ∥sg (pϕ (z′\nt | h′\nt))])\nL′\ndec\n.= −ln pϕ (o′\nt, M′\nt | h′\nt, z′\nt)\nL′\npred\n.= −ln pϕ(r′\nt, c′\nt | h′\nt, z′\nt) −ln pϕ(j′\nt | h′\nt, z′\nt)\nL′\nint\n.= −ln pϕ (∆′\nt, G′\nt | ht−1, zt−1, h′\nt, z′\nt)\n.\n(6)\nWe can optimize the world model Wϕ by minimizing over replay buffer B:\nL .= E\nhP\n{Dt}T\nt=1 (βdynLdyn + βencLenc + βpred (Ldec + Lpred)) +\nβlong\nP\n{D′\nt+1}t∈T\n\u0000βdynL′\ndyn + βencL′\nenc + βpred\n\u0000L′\ndec + L′\npred + L′\nint\n\u0001\u0001 i\n.\n(7)\n3.4\nBEHAVIOR LEARNING OVER MIXED LONG SHORT-TERM IMAGINATIONS\nAs shown in Figure 3 (b), LS-Imagine employs an actor-critic algorithm to learn behavior from the\nlatent state sequences predicted by the world model. The goal of the actor is to optimize the policy\nto maximize the discounted cumulative reward Rt, while the role of the critic is to estimate the\ndiscounted cumulative rewards using the current policy for each state ˆst .= {ht, ˆzt}:\nActor:\nˆat ∼πθ (ˆat | ˆst) ,\nCritic:\nvψ\n\u0010\nˆRt | ˆst\n\u0011\n.\n(8)\nStarting from the initial state encoded from the sampled observation and the affordance map, we\ndynamically select either the long-term transition model or the short-term transition model to predict\nsubsequent states based on the jumping flag ˆjt. For the long short-term imagination sequence\n{(ˆst, ˆat)}L\nt=1 with an imagination horizon of L, we predict reward sequence ˆr1:L and the continuation\nflag sequence ˆc1:L through the reward predictor. Similar to Eq. (4), the index t does not represent the\ntime step in the environment, but rather the positional order of the states in the imagination sequence.\nSpecifically, starting from state ˆst, any subsequent state obtained via either a short-term transition or\na long-term transition is indexed sequentially as t + 1.\nFor jumpy states predicted by long-term imagination, the interval predictor estimates (i) the number of\nsteps ˆ∆t from ˆst−1 to ˆst and (ii) the potential discounted cumulative reward ˆGt over the time interval\nof ˆ∆t. Otherwise, for states obtained via short-term imagination, which correspond to single-step\ntransitions in the environment, we set ˆ∆t = 1 and ˆGt = ˆrt. Consequently, within one imagination\nepisode, we obtain a sequence of step intervals ˆ∆2:L and a sequence of predicted rewards ˆG2:L\nbetween consecutive imagination states.\nWe employ a modified bootstrapped λ-returns that considers both long-term and short-term imagina-\ntions to calculate the discounted cumulative rewards for each state:\nRλ\nt\n.=\n\u001a\nˆct{ ˆGt+1 + γ ˆ∆t+1 \u0002\n(1 −λ)vψ(ˆst+1) + λRλ\nt+1\n\u0003\n}\nif t < L\nvψ(ˆsL)\nif t = L .\n(9)\nThe critic uses the maximum likelihood loss to predict the distribution of the return estimates Rλ\nt :\nL(ψ) .= −\nL\nX\nt=1\nln pψ\n\u0000Rλ\nt | ˆst\n\u0001\n.\n(10)\nFollowing DreamerV3 (Hafner et al., 2023), we train the actor to maximize the return estimates\nRλ\nt . Notably, since long-term imagination does not involve actions, we do not optimize the actor at\ntime steps when jumpy state transitions are adopted. Therefore, unlike DreamerV3, we apply an\nadditional factor of (1 −ˆjt) to ignore updates at long-term imagination steps:\nL(θ) .= −\nL\nX\nt=1\nsg\n\u0014\u0010\n1 −ˆjt\n\u0011 Rλ\nt −vψ(ˆst)\nmax(1, S)\n\u0015\nlog πθ(ˆat | ˆst) + η H [πθ(ˆat | ˆst)] .\n(11)\n7\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n(a) Harvest log in plains\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n(b) Harvest water with bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess Rate\n(c) Harvest sand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\nSuccess Rate\n(d) Shear sheep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.1\n0.2\n0.3\n0.4\nSuccess Rate\n(e) Mine iron ore\nlog\nwater\nsand\nwool iron_ore\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMineCLIP Score\n(f) Comparison in MineCLIP reward\nVPT\nSTEVE-1\nPTGM\nDirector\nDreamerV3\nLS-Imagine\nFigure 4: Comparison of LS-Imagine against strong Minecraft agents, including DreamerV3 (Hafner\net al., 2023), VPT (Baker et al., 2022), STEVE-1 (Lifshitz et al., 2023), PTGM (Yuan et al., 2024),\nand Director (Hafner et al., 2022). We present the numerical results in Table 3 in the appendix.\n4\nEXPERIMENTS\nWe explore LS-Imagine on the challenging MineDojo (Fan et al., 2022) benchmark on top of the\npopular Minecraft game, which is a comprehensive simulation platform with various open-ended\ntasks. We use 5 tasks, i.e., harvest log in plains, harvest water with bucket, harvest sand, shear sheep,\nand mine iron ore. These tasks demand numerous steps to complete and present significant challenges\nfor agent learning. We adopt a binary reward that indicates whether the task was completed, along\nwith the MineCLIP reward (Fan et al., 2022). Further details of the environmental setups are provided\nin Appendix A. Besides, we introduce the compared models in Appendix B.\nImplementation details.\nWe conduct our experiments on the MineDojo environment, where both\nvisual observation and corresponding affordance maps are resized to 64 × 64 pixels. To generate\naccurate affordance maps, we collect 2,000 images from the environment using a random agent\nunder the current task instruction and generate a discrete set of (ot, I, Mot,I), which are then used to\nfinetune the multimodal U-Net for 200 epochs. For tasks in the MineDojo benchmark, we train the\nagent for 1 × 106 environment steps. Each training of LS-Imagine takes approximately 23 GB of\nVRAM and requires around 1.7 days to complete on a single RTX 3090 GPU.\n4.1\nMAIN COMPARISON\nWe evaluate all the Minecraft agents in terms of success rate shown in Figure 4 and per-episode\nsteps shown in Figure 5. We find that LS-Imagine significantly outperforms the compared models,\nparticularly in scenarios where sparse targets are distributed in the task. In Figure 4 (f), we showcase\nthe MineCLIP values achieved by LS-Imagine and DreamerV3. Specifically, a sliding window of\nlength 16 is used to compute the local MineCLIP values for each segment. The mean value is then\ncalculated from all sliding windows. We can see that agents trained using our method achieve higher\nMineCLIP values within a single episode compared to DreamerV3. This suggests that LS-Imagine\nfacilitates quicker detection of task-relevant visual targets in open-world environments.\nAdditionally, we present qualitative results in Figure 6(a). In the top row, we decode the latent\nstates before and after the jumpy state transitions back to the pixel space. To better understand how\naffordance maps facilitate the jumpy state transitions and whether they can provide effective goal-\nconditioned guidance, the bottom rows visualize the affordance maps reconstructed from the latent\nstates. These visualizations demonstrate that the proposed world model can adaptively determine\nwhen to utilize long-term imagination based on the current visual observation. Furthermore, the\n8\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n400\n600\n800\n1000\nSteps per episode\n(a) Harvest log in plains\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n400\n600\n800\n1000\n(b) Harvest water with bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n500\n600\n700\n800\n900\n1000\n(c) Harvest sand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n500\n600\n700\n800\n900\n1000\n(d) Shear sheep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n1500\n1600\n1700\n1800\n1900\n2000\n(e) Mine iron ore\nVPT\nSTEVE-1\nPTGM\nDirector\nDreamerV3\nLS-Imagine\nFigure 5: The number of steps per episode for task completion.\nLong-term imagination\nShort-term imagination\n(a) Visualization of long short-term imaginations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLS-Imagine (series)\nLS-Imagine (parallel)\n(b) Series vs. parallel connections\nFigure 6: (a) Visualization of long short-term imaginations and (b) a further discussion on possible\narchitecture designs of Series and Parallel connections of these two imagination pathways.\ngenerated affordance maps align effectively with areas that are highly relevant to the final goal,\nthereby enabling the agent to perform more efficient policy exploration.\n4.2\nMODEL ANALYSES\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nLS-Imagine\nw\/o long-term imagination\nw\/o intrinsic reward\nFigure 7: Ablation study results.\nAblation studies.\nWe conduct the ablation studies to\nvalidate the effect of the affordance-driven intrinsic re-\nward and long short-term imagination. Figure 7 presents\ncorresponding results in the challenging MineDojo tasks.\nAs shown by the blue curve, removing the long-term\nimagination of LS-Imagine leads to a performance de-\ncline, which indicates the necessity of introducing long-\nterm imagination and switching between it and short-\nterm imagination adaptively. For the model represented\nby the green curve, we do not employ affordance-driven\nintrinsic reward. It shows that the affordance-driven in-\ntrinsic reward also plays an important role during the\nearly training stage of agents. Additionally, unlike the\nMineCLIP reward being calculated based on a series of\nstates, the affordance-driven intrinsic reward relies solely on a single independent state. This approach\nenables a more accurate estimation of the reward for the post-jumpy-transition state.\nAlternative pathways of mixed imaginations.\nIt is worth highlighting that the long short-term\nimagination is implemented sequentially. In Figure 10(a) in the appendix, we provide a visualization\nillustrating how the agent sequentially performs short-term and long-term imaginations within a\nsingle sequence. Alternatively, as illustrated in Figure 10(b), we could structure long- and short-term\nimagination pathways in parallel. Specifically, we begin by applying short-term imagination within a\nsingle sequence. For each predicted state, we examine the jumping flag: If ˆjt = 1, we initiate a new\nimagination sequence starting from the post-jump state, which is predicted by the long-term transition\nmodel and the dynamics predictor. In other words, whenever a long-term state jump occurs, the world\nmodel generates a new sequence from the post-jump state, while the intermediate state transitions\nwithin the sequence are governed exclusively by short-term dynamics. Importantly, we optimize the\nactor independently for each sequence, ensuring that there is no gradient or value transfer between\nsequences. To evaluate the advantages of using sequential long short-term imagination, we conduct\nan experimental comparison between LS-Imagine (series) and LS-Imagine (parallel). Figure 6(b)\nshows that the LS-Imagine (series) outperforms LS-Imagine (parallel) by large margins. This implies\n9\nPublished as a conference paper at ICLR 2025\nthat the parallel imagination sequences are independent of one another, meaning that the sequence\nstarting with a post-jumping state does not guide the prior-jumping transitions.\nIn the appendix, we further include (i) experiments on the long-horizon “Tech Tree” task, (ii) analyses\nof the long-term imagination frequency and corresponding state jumping intervals ˆ∆t predicted by\nthe model, and (iii) visualization of affordance maps with occluded target objects.\n5\nRELATED WORK\nVisual MBRL.\nRecently, learning control policies from images, i.e., visual RL has been used widely,\nwhereas previous RL algorithms learn policies from low-dimensional states. Existing approaches\ncan be grouped by the use of model-free RL methods (Laskin et al., 2020; Schwarzer et al., 2021;\nStooke et al., 2021; Xiao et al., 2022; Parisi et al., 2022; Yarats et al., 2022; Zheng et al., 2023)\nor model-based RL methods (Hafner et al., 2019; 2020; 2021; Seo et al., 2022; Pan et al., 2022;\nZhang et al., 2023a; Mazzaglia et al., 2023; Micheli et al., 2023; Zhang et al., 2023b; Ying et al.,\n2023; Seo et al., 2023; Alonso et al., 2024; Hansen et al., 2024; Wang et al., 2024b). The following\nmethods specifically enhance the modeling of long-term dynamics in visual MBRL. Lee et al. (2024b)\nproposed the prediction of temporally smoothed rewards to address long-horizon sparse-reward\ntasks. R2I (Samsami et al., 2024) improves long-term memory and long-horizon credit assignment\nin MBRL. Unlike existing methods, our work presents a long short-term world model architecture\nspecifically designed for visual control in open-world environments.\nAffordance maps for robot learning.\nOur work is also related to the affordance map for robot\nlearning (Mo et al., 2021; Jiang et al., 2021; Yarats et al., 2021; Mo et al., 2022; Geng et al., 2022;\nXu et al., 2022a; Wang et al., 2022; Wu et al., 2022; Ha & Song, 2022; Xu et al., 2022b; Cheng\net al., 2024; Lee et al., 2024a; Li et al., 2024b). Where2Explore (Ning et al., 2023) introduces a\ncross-category few-shot affordance learning framework that leverages the similarities in geometries\nacross different categories. DualAfford (Zhao et al., 2023) learns collaborative actionable affordance\nfor dual-gripper manipulation tasks over various 3D shapes. VoxPoser (Huang et al., 2023) unleashes\nthe power of large language models and vision-language models for extracting affordances and\nconstraints of real-world manipulation tasks, which are grounded in 3D perceptual space. VRB (Bahl\net al., 2023) trains a visual affordance model with videos of human interactions and deploys the\nmodel in real-world robotic tasks directly. Qi et al. (2020) adopted a spatial affordance map that is\ntrained by interacting with the environment for navigation. However, our approach distinguishes\nitself by employing visual observation to generate affordance maps as guidance to mitigate the low\nexploration efficiency in open-world environments.\nHierarchical methods.\nLike our approach, Director (Hafner et al., 2022) learns hierarchical\nbehaviors in the latent space, which adopts high-level policy (manager) to produce latent goals to\nguide low-level policy (worker). Dr. Strategy (Hamed et al., 2024) proposes strategic dreaming\nwith latent landmarks to learn a highway policy that enables the agent to move to a landmark in the\ndream. Gumbsch et al. (2024) presented a hierarchy of world models, which perform high-level and\nlow-level prediction adaptively, and the high-level predictions depend on the low-level predictions.\nOur method distinguishes itself by generating affordance maps through image zoom-in to encourage\nthe agent to explicitly execute long-term imagination in the world model.\n6\nCONCLUSIONS AND LIMITATIONS\nIn this paper, we presented a novel approach to overcoming the challenges of training visual rein-\nforcement learning agents in high-dimensional open worlds. By extending the imagination horizon\nand leveraging a long short-term world model, our method facilitates efficient off-policy exploration\nacross expansive state spaces. The incorporation of goal-conditioned jumpy state transitions and\naffordance maps allows agents to better grasp long-term value, enhancing their decision-making\nabilities. Our results demonstrate substantial improvements over existing state-of-the-art techniques\nin MineDojo, highlighting the potential of our approach for open-world reinforcement learning and\ninspiring future research in this domain.\nA limitation of LS-Imagine is the computational overhead it introduces. Additionally, its effectiveness\nhas only been validated in 3D navigation environments with embodied agents. We aim to enhance\nthe generalization of our approach across a wider range of tasks.\n10\nPublished as a conference paper at ICLR 2025\nETHICS STATEMENT\nIn this work, we are committed to upholding ethical research practices. This work does not involve\nhuman subjects, personal data, or sensitive information. All environments and datasets used are\nsynthetic and publicly available. We recognize the potential for reinforcement learning models to\nbe misused, particularly in decision-making scenarios where unintended outcomes could arise. To\nmitigate these risks, we emphasize responsible deployment and encourage careful consideration of\nthe broader impact of such systems, restricting the use of our work strictly to research purposes.\nREPRODUCIBILITY STATEMENT\nWe prioritize the reproducibility of our work. All results can be reproduced on publicly available\nRL environments by following the experimental details presented in Sec. 4 and Appendix D.6. We\nprovide the source code at https:\/\/github.com\/qiwang067\/LS-Imagine.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science Foundation of China (Grants 62250062,\n62302246), the Smart Grid National Science and Technology Major Project (Grant 2024ZD0801200),\nthe Shanghai Municipal Science and Technology Major Project (Grant 2021SHZDZX0102), the\nFundamental Research Funds for the Central Universities, and the CCF-Tencent Rhino-Bird Open\nResearch Fund. Additional support was provided by the Natural Science Foundation of Zhejiang\nProvince, China (Grant LQ23F010008), the High Performance Computing Center at Eastern Institute\nof Technology, Ningbo, and Ningbo Institute of Digital Twin.\nREFERENCES\nEloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and\nFrançois Fleuret. Diffusion for world modeling: Visual details matter in atari. In NeurIPS, 2024.\nShikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from\nhuman videos as a versatile representation for robotics. In CVPR, pp. 13778–13790, 2023.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In NeurIPS, 2022.\nHu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang.\nSwin-unet: Unet-like pure transformer for medical image segmentation. In ECCVW, 2022.\nGuangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin Sun, and Jiang Bian. Empow-\nering large language models on robotic manipulation with affordance prompting. arXiv preprint\narXiv:2404.11027, 2024.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In NeurIPS, volume 35, pp. 18343–18362, 2022.\nYiran Geng, Boshi An, Haoran Geng, Yuanpei Chen, Yaodong Yang, and Hao Dong. End-to-end\naffordance learning for robotic manipulation. arXiv preprint arXiv:2209.12941, 2022.\nChristian Gumbsch, Noor Sajid, Georg Martius, and Martin V Butz. Learning hierarchical world\nmodels with adaptive temporal abstractions from discrete latent dynamics. In ICLR, 2024.\nHuy Ha and Shuran Song. Flingbot: The unreasonable effectiveness of dynamic manipulation for\ncloth unfolding. In CoRL, pp. 24–33, 2022.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\nDavidson. Learning latent dynamics for planning from pixels. In ICML, pp. 2555–2565, 2019.\n11\nPublished as a conference paper at ICLR 2025\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\nbehaviors by latent imagination. In ICLR, 2020.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete\nworld models. In ICLR, 2021.\nDanijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter Abbeel. Deep hierarchical planning from\npixels. In NeurIPS, 2022.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nHany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, and Sungjin Ahn. Dr. strategy: Model-based\ngeneralist agents with strategic dreaming. In ICML, 2024.\nNicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous\ncontrol. In ICLR, 2024.\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models. In CoRL, 2023.\nZhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, and Yuke Zhu. Synergies between affordance\nand geometry: 6-dof grasp detection via implicit representations. Robotics: science and systems,\n2021.\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations\nfor reinforcement learning. In ICML, pp. 5639–5650, 2020.\nOlivia Y Lee, Annie Xie, Kuan Fang, Karl Pertsch, and Chelsea Finn. Affordance-guided reinforce-\nment learning via visual prompting. arXiv preprint arXiv:2407.10341, 2024a.\nVint Lee, Pieter Abbeel, and Youngwoon Lee. Dreamsmooth: Improving model-based reinforcement\nlearning via reward smoothing. In ICLR, 2024b.\nHao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li,\nLewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language\nmodels for minecraft. In CVPR, pp. 16426–16435, 2024a.\nXiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming\nLiu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric\nrobotic manipulation. In CVPR, pp. 18061–18070, 2024b.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. In NeurIPS, 2023.\nPietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, and Sai Rajeswar. Choreographer:\nLearning and adapting skills in imagination. In ICLR, 2023.\nVincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models.\nIn ICLR, 2023.\nKaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani.\nWhere2act: From pixels to actions for articulated 3d objects. In ICCV, pp. 6813–6823, 2021.\nKaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and Leonidas Guibas. O2o-afford: Annotation-free\nlarge-scale object-object affordance learning. In CoRL, pp. 1666–1677, 2022.\nChuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, and Hao Dong. Where2explore: Few-shot\naffordance learning for unseen novel categories of articulated objects. NeurIPS, 2023.\nKolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer\nSingh, and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision making\nusing language guided world modelling. In ICML, pp. 26311–26325, 2023.\n12\nPublished as a conference paper at ICLR 2025\nMinting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging\nnoncontrollable visual dynamics in world models. In NeurIPS, volume 35, pp. 23178–23191,\n2022.\nSimone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising\neffectiveness of pre-trained vision models for control. In ICML, pp. 17359–17371, 2022.\nWilliam Qi, Ravi Teja Mullapudi, Saurabh Gupta, and Deva Ramanan. Learning to move with\naffordance maps. In ICLR, 2020.\nMohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar. Mastering\nmemory tasks with world models. In ICLR, 2024.\nMax Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R De-\nvon Hjelm, Philip Bachman, and Aaron C Courville. Pretraining representations for data-efficient\nreinforcement learning. In NeurIPS, volume 34, pp. 12686–12699, 2021.\nYounggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel. Reinforcement learning with\naction-free pre-training from videos. In ICML, pp. 19561–19579, 2022.\nYounggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view\nmasked world models for visual robotic manipulation. In ICML, pp. 30613–30632, 2023.\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning\nfrom reinforcement learning. In ICML, pp. 9870–9879, 2021.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nTMLR, 2024a.\nQi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, and Xiaokang Yang. Making offline rl\nonline: Collaborative world models for offline visual reinforcement learning. In NeurIPS, 2024b.\nYian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas J Guibas, and Hao Dong.\nAdaafford: Learning to adapt manipulation affordance for 3d articulated objects via few-shot\ninteractions. In ECCV, pp. 90–107, 2022.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with large language models enables open-world\nmulti-task agents. In NeurIPS, 2023.\nRuihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin\nChen, Leonidas Guibas, and Hao Dong. Vat-mart: Learning visual action trajectory proposals for\nmanipulating 3d articulated objects. ICLR, 2022.\nTete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for\nmotor control. arXiv preprint arXiv:2203.06173, 2022.\nChao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin Zhu, and Siyuan Huang. Partafford:\nPart-level affordance discovery from 3d objects. arXiv preprint arXiv:2202.13519, 2022a.\nZhenjia Xu, Zhanpeng He, and Shuran Song. Universal manipulation policy network for articulated\nobjects. IEEE robotics and automation letters, 7(2):2447–2454, 2022b.\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. In ICLR, 2021.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control:\nImproved data-augmented reinforcement learning. In ICLR, 2022.\nChengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Songming Liu, Jialian Li, Dong Yan,\nand Jun Zhu. Reward informed dreamer for task generalization in reinforcement learning. arXiv\npreprint arXiv:2303.05092, 2023.\n13\nPublished as a conference paper at ICLR 2025\nHaoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu. Pre-training goal-based models for\nsample-efficient reinforcement learning. In ICLR, 2024.\nWeipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang. Storm: Efficient stochastic\ntransformer based world models for reinforcement learning. In NeurIPS, 2023a.\nWendong Zhang, Geng Chen, Xiangming Zhu, Siyu Gao, Yunbo Wang, and Xiaokang Yang.\nPredictive experience replay for continual visual control and forecasting.\narXiv preprint\narXiv:2303.06572, 2023b.\nYan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang, Qingnan Fan, Kaichun Mo, and Hao Dong.\nDualafford: Learning collaborative visual affordance for dual-gripper manipulation. ICLR, 2023.\nRuijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daumé III,\nand Furong Huang. Taco: Temporal latent action-driven contrastive loss for visual reinforcement\nlearning. In NeurIPS, volume 36, 2023.\n14\nPublished as a conference paper at ICLR 2025\nAPPENDIX\nA\nENVIRONMENT DETAILS\nAs illustrated in Table 2, language description is employed for calculating the MineCLIP reward (Fan\net al., 2022). Initial tools are the items provided in the inventory at the beginning of each episode.\nInitial mobs and distance specifies the types of mobs present at the start of each episode and their\ninitial distance from the agent. Max steps refers to the maximum allowed steps per episode.\nTable 2: Details of the MineDojo tasks.\nTask\nLanguage description\nInitial tools\nInitial mobs and distance Max steps\nHarvest log in plains\n“Cut a tree.”\n–\n–\n1000\nHarvest water with bucket\n“Obtain water.”\nbucket\n–\n1000\nHarvest sand\n“Obtain sand.”\n–\n–\n1000\nShear sheep\n“Obtain wool.”\nshear\nsheep, 15\n1000\nMine iron ore\n“Mine iron ore.”\nstone pickaxe\n–\n2000\nB\nCOMPARED METHODS\nWe compare LS-Imagine with strong Minecraft agents, including:\n• DreamerV3 (Hafner et al., 2023): An MBRL approach that learns directly from the step-by-step\nimaginations of future latent states generated by the world model.\n• VPT (Baker et al., 2022): A foundation model designed for Minecraft trained through behavior\ncloning, on a dataset consisting of 70,000 hours of game playing collected from the Internet.\n• STEVE-1 (Lifshitz et al., 2023): An instruction-following Minecraft agent that translates lan-\nguage instructions into specific goals. To evaluate its effectiveness, we assess Steve-1’s zero-shot\nperformance on our tasks by supplying it with task instructions.\n• Director (Hafner et al., 2022): An agent learns hierarchical behaviors by leveraging a world model\nto plan within its latent space.\n• PTGM (Yuan et al., 2024): An RL method that pretrains goal-based policy and adopts temporal\nabstractions and behavior regularization.\nC\nMODEL DETAILS\nC.1\nENVIRONMENTAL INTERACTION AND DATA COLLECTION\nTo train LS-Imagine’s world model, we collect both short-term and long-term transition data through\ninteractions with the environment. As shown in Figure 8, at each time step t, the agent interacts with\nthe environment following the current policy. At each time step, the data buffer collects a tuple Dt,\nwhich includes (ot, at, Mt, rt, ct, jt, ∆t, Gt):\n• ot represents the observed image.\n• at represents the agent’s action taken given ot.\n• Mt is the affordance map generated by a multimodal U-Net given ot and task instructions I.\n• rt is defined in Sec. 3.2.3, which is the immediate reward computed as a weighted sum of the sparse\nenvironmental reward renv\nt\nafter executing at−1, the MineCLIP reward rMineCLIP\nt\nfrom a pretrained\nscoring model (Fan et al., 2022), and the intrinsic reward rintr\nt\ndefined in Eq. (1) and based on Mt.\n• ct is the continuation flag received from the environment, which indicates whether further interac-\ntion is required after this step.\n• jt is the jumping flag, which is used to train the world model to trigger long-term imagination\nduring model-based behavior learning. We first estimate the jumping probability Pjump using Eq. (3)\nbased on Mt. To stabilize training, we establish a dynamic threshold Pthresh, which accounts for\nthe varying guidance strength provided by the affordance map across different tasks, resulting in\n15\nPublished as a conference paper at ICLR 2025\nMultimodal U-Net\n   ... ...   \n   ... ...   \nMultimodal U-Net\nenv\n Zoom in \nagent\nMultimodal U-Net\n...\nFigure 8: Environmental interaction and data collection.\ntask-specific distributions of Pjump. Specifically, from the beginning of training, we store the Pjump\nvalues for every interaction step in a dedicated buffer. The threshold Pthresh is then dynamically\ncalculated as the mean of all Pjump values currently in the buffer plus their standard deviation. This\ndynamic adjustment ensures that the threshold adapts to the characteristics of the task and remains\nrobust throughout training. If Pjump > Pthresh, we set jt = 1; otherwise, jt = 0.\n• ∆t represents the expected number of step intervals in the jumpy state transitions during long-term\nimaginations. Specifically, we set ∆t = 1 by default, corresponding to a short-term transition.\n• Gt represents the expected cumulative reward between the pre- and post-jump states when long-\nterm imagination occurs. Specifically, for a short-term transition, we set Gt = rt by default.\nIf jt = 0, Dt is defined as the starting point of a short-term transition within the pair (Dt, Dt+1).\nDuring world model training, (Dt, Dt+1) is replayed to train the related modules associated with\nshort-term dynamics. Once we obtain jt = 1 during interactions, we define the current step as the\nstarting point of a simulated long-term transition (Dt, D′\nt+1). Notably, we use D′\nt+1 to differentiate\nfrom its short-term counterparts.\nWe define D′\nt+1 = (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1), where r′\nt+1 and c′\nt+1 are com-\nputed in the same manner as in short-term tuples but with o′\nt+1 and M′\nt+1 as inputs. Similarly, a′\nt+1\nand j′\nt+1 are also computed in the same way as in short-term tuples. We record them in the data\nbuffer for better training of the reward predictor and the jump predictor.\nThe next question is how to annotate ∆′\nt+1, G′\nt+1, and o′\nt+1 to train the long-term branch.\n• o′\nt+1 is a simulated image rather than a real-captured image. It is obtained by cropping the original\nobservation ot based on the high-value regions in the affordance map Mt.\n• ∆′\nt+1 is an estimation of the number of real interaction steps between the pre-jump state and\nthe post-jump state. Since the post-jump state is not real data obtained from the environment,\nwe first identify a real state that closely resembles the post-jump state. We then calculate the\nnumber of steps required to transition from the pre-jump state to this identified real post-jump state.\nSpecifically, we use the intrinsic reward as a measurement. Starting from the pre-jump state, during\nsubsequent interactions with the environment, if the agent reaches a real state where the intrinsic\nreward satisfies rintr\nt+∆′\nt+1 ≥rintr ′\nt+1 , we take this state as the real post-jump state and take ∆′\nt+1 as\nthe long-term jumping interval.\n• G′\nt+1 is the cumulative reward within ∆′\nt+1 interaction steps, i.e., G′\nt+1 = P∆′\nt+1\ni=1 γi−1rt+i.\n16\nPublished as a conference paper at ICLR 2025\nPatch Partition\n Linear Embedding\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nSwin Transformer\nBlock × 2\nSwin Transformer\nBlock × 2\nPatch Expanding\nSwin Transformer\nBlock × 2\nPatch Expanding\nSwin Transformer\nBlock × 2\nPatch Expanding\nPatch Expanding\nLinear Projection\nCut a tree\nCompute affordance\nmap with MineCLIP\nText \nEncoder\nTIA\nTIA\nTIA\nTIA\nFigure 9: The architecture of multimodal U-Net.\nC.2\nFRAMEWORK OF MULTIMODAL U-NET\nAs described in Sec. 3.2.2, we train a multimodal U-Net to rapidly generate affordance maps based on\nobservation images and task instructions. Our enhanced multimodal U-Net architecture, as illustrated\nin Figure 9, is based on Swin-Unet (Cao et al., 2022), a U-shaped encoder-decoder architecture built\non Swin Transformer blocks. The enhanced multimodal U-Net consists of an encoder, a decoder, a\nbridge layer, and a text processing module. In the Swin-Unet-inspired structure, the basic unit is the\nSwin Transformer block. For the encoder, the input image is divided into non-overlapping patches\nof size 4 × 4 to convert the input into a sequence of patch embeddings. Through this method, each\npatch has a feature dimension of 4 × 4 × 3 = 48. The patch embeddings are then projected through a\nlinear embedding layer (denoted as C), and the transformed patch tokens are passed through several\nSwin Transformer blocks and patch merging layers to produce hierarchical feature representations.\nThe patch merging layers are responsible for downsampling and increasing the dimensionality, while\nthe Swin Transformer blocks handle feature representation learning.\nFor the task instruction, the text description is processed through the text encoder of MineCLIP (Cao\net al., 2022) to obtain text embeddings, which are integrated with the image features extracted at\neach layer of the encoder via the Text-Image Attention (TIA) module. The TIA module employs a\nmulti-head attention mechanism to fuse image features (as keys and values) with text features (as\nqueries) in a multi-scale attention-based fusion. The resulting fused text-image features are passed\nthrough the bridge layer and are subsequently combined with the corresponding features during the\nupsampling process in the decoder.\nThe decoder comprises Swin Transformer blocks and patch-expanding layers. The extracted context\nfeatures are combined through the bridge layer with the multi-scale text-image features from the\nencoder to compensate for the spatial information lost during downsampling and to integrate the text\ninformation. Unlike the patch merging layers, the patch expanding layers are specifically designed for\nupsampling. They reshape the adjacent feature maps by performing a 2× upsampling of the resolution,\nexpanding the feature maps into larger ones. Finally, a final patch expanding layer performs a 4×\nupsampling to restore the resolution of the feature map to the input resolution W × H ), followed by\na linear projection layer applied on the upsampled features to produce pixel-level affordance maps.\n17\nPublished as a conference paper at ICLR 2025\n...\n(a) LS-Imagine (series)\n...\n...\n...\n...\n(b) LS-Imagine (parallel)\nFigure 10: Comparison with series and parallel variants of mixed imaginations.\nC.3\nVARIANTS OF LONG SHORT-TERM IMAGINATIONS\nWe compare two alternative pathways of the long short-term imaginations in Figure 10.\nC.4\nFULL ALGORITHM\nWe present the training pipeline of LS-Imagine in Algorithm 1.\nAlgorithm 1 The training pipeline of LS-Imagine.\n1: Initialize parameters ϕ, θ, ψ.\n2: Compute affordance map with MineCLIP.\n▷Affordance map generation\n3: Train multimodal U-Net with annotated data. ▷To enable real-time interaction with the affordance maps\n4: Train the random agent and collect a replay buffer B.\n5: while not converged do\n6:\nSample long short-term transitions from B.\n▷Representation learning\n7:\nUpdate the world model ϕ using Eq. (7).\n8:\nGenerate (s1, ˆa1, ˆj1) using πθ and Wϕ.\n9:\nfor time step t = 2 · · · L do\n▷Behavior learning\n10:\nif jump flag ˆjt−1 then\n11:\nGenerate (s′\nt, a′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt) using πθ and long-term imagination of Wϕ.\n12:\nUpdate (ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt) ←(s′\nt, a′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt).\n13:\nelse\n14:\nGenerate (˜st, ˜at, ˜rt, ˜ct, ˜jt) using πθ and short-term imagination of Wϕ.\n15:\nUpdate (ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt) ←(˜st, ˜at, ˜ct, ˜jt, 1, ˜rt).\n16:\nend if\n17:\nend for\n18:\nCalculate value estimate using Eq. (9).\n19:\nOptimize actor πθ using Eq. (11) over {(ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt)}L\nt=1.\n20:\nOptimize critic vψ using Eq. (10) over {(ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt)}L\nt=1.\n21:\nfor time step t = 1 · · · T do\n▷Environment interaction\n22:\nSample ˆat ∼πθ (ˆat | ˆst)\n23:\nrenv\nt , ot+1, ct ←env.step(ˆat)\n24:\nGenerate affordance map Mt with multimodal U-Net for each ot.\n25:\nCalculate intrinsic reward rintr\nt\nand jump flag jt based on the affordance map.\n26:\nCollect short-term data (ot, at, Mt, rt, ct, jt, ∆t, Gt).\n27:\nif jumpy flag jt then\n28:\nConstruct long-term data (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1).\n29:\nend if\n30:\nend for\n31:\nAppend long short-term transitions to B.\n32: end while\n18\nPublished as a conference paper at ICLR 2025\nTable 3: The success rate and the number of steps per episode for task completion.\nModel\nHarvest log in plains Harvest water with bucket\nHarvest sand\nShear sheep\nMine iron ore\nsucc. (%)\nsucc. step\nsucc. (%)\nsucc. step\nsucc. (%) succ. step succ. (%) succ. step succ. (%) succ. step\nVPT\n6.97\n963.32\n0.61\n987.65\n12.99\n880.54\n1.94\n987.49\n0.00\n—\nSTEVE-1\n57.00\n752.47\n6.00\n989.07\n37.00\n770.40\n3.00\n992.36\n0.00\n—\nPTGM\n41.86\n811.19\n2.78\n977.78\n17.71\n833.64\n21.54\n887.03\n15.14\n1586.03\nDirector\n8.67\n968.09\n20.90\n931.74\n36.36\n825.35\n1.27\n995.99\n7.82\n1906.31\nDreamerV3\n53.33\n711.22\n55.72\n628.79\n59.88\n548.76\n25.13\n841.14\n16.79\n1789.06\nLS-Imagine\n80.63\n503.35\n77.31\n502.61\n62.68\n601.18\n54.28\n633.78\n20.28\n1748.55\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(a) Jumping frequency\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0\n100\n200\n300\n400\n500\n(b) Interval ˆ∆t\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.00\n0.05\n0.10\n0.15\n0.20\nHarvest log in plains\nHarvest water with bucket\n(c) Dynamic threshold Pthresh\nFigure 11: Analyses of long-term imaginations throughout training.\nC.5\nCLARIFICATION ON STOCHASTIC LONG-TERM IMAGINATION\nOne might argue that long-term imagination could skip essential intermediate steps that gradually\nlead to the objective, potentially resulting in a lack of learning for these crucial actions. To address\nthis issue, we adopt a probabilistic mechanism. Specifically, even when ˆjt = True, indicating that a\nlong-term transition is to be executed, we implement a probability of 0.7 for executing the jump and\n0.3 for not jumping. This allocation ensures a 30% chance that the transition will execute the short-\nterm imagination with gradient feedback attached to the actions. This stochastic decision-making\nis based on a uniform distribution, providing a balanced approach between leveraging long-term\nimagination and capturing essential short-term behaviors.\nC.6\nADDITIONAL LIMITATION\nIt is worth mentioning that LS-Imagine simulates the agent’s state when approaching a target object\nin 3D navigation environments with embodied agents by zooming in on the observed image, and\nsets intrinsic rewards based on whether the agent is close to and has positioned the target object\nat the center of the observation. Therefore, LS-Imagine is not suitable for environments with\nfixed viewpoints, 2D environments, or those where the reward mechanism is more complex than\napproaching objects (e.g., driving).\nD\nADDITIONAL RESULTS\nD.1\nNUMERICAL COMPARISONS\nTable 3 compares existing approaches on the challenging MineDojo environment.\nD.2\nANALYSES ON LONG-TERM IMAGINATIONS\nWe use the task harvest log in plains as an example to facilitate the understanding of the long short-\nterm imagination process. In Figure 11(a), we first track the frequency of long-term imaginations and\nthe corresponding predicted state intervals ˆ∆t throughout the training process. The curve shows the\nproportion of imagination sequences involving jumpy state transitions relative to the total number of\nimagination sequences. Initially, the jumping frequency is low because the world model has not yet\nlearned to identify when a jump is necessary based on the state. As the model’s predictions improve\nin the early stages of training, the frequency increases, likely due to the agent’s underdeveloped\npolicies, which result in more observations far from the goal and necessitate long-term exploration.\nOver time, as the agent learns policies that bring it closer to the target, the frequency of observations\nfar from the goal decreases, reducing the need for jumps.\n19\nPublished as a conference paper at ICLR 2025\n(a) Harvest log in plains\nShort-term imagination\nLong-term imagination\n(b) Harvest water with bucket\n(c) Harvest sand\n(d) Shear sheep\n(e) Mine iron ore\nFigure 12: Visualization of the complete long short-term imagination sequences.\nAdditionally, we find that among all sequences with jumpy state transitions, the average number of\njumpy transitions per sequence, within a horizon of 15 steps, 1.02. This indicates that, in most cases\nof these tasks, a single jumpy transition is sufficient to bring the agent close to the target.\nIn Figure 11(b), we track the variations of the jumping state intervals, ˆ∆t, throughout training. At the\nbeginning, ˆ∆t is high, indicating that the policy requires many steps to reach the target. As the policy\nimproves, fewer steps are needed to approach the target, leading to a gradual decrease in ˆ∆t. Notably,\nas ˆ∆t evolves with the updated policy, it also ensures minimal misalignment in Eq. (9) between the\nfuture cumulative rewards computed with jumpy imaginations and the behavior policy.\nFurthermore, in Figure 11(c), we track the variation curves of the dynamic threshold Pthresh during\ntraining in different tasks, and observe that:\n20\nPublished as a conference paper at ICLR 2025\n(a) Explore for a village\n(b) Mine ore\nFigure 13: Affordance maps when the target is invisible or occluded.\nlog\nplanks\ncrafting\ntable\nstick\nwooden\npickaxe\ncobblestone\nstone\npickaxe\n20\n30\n40\n50\n60\n70\n80\n90\n100\nSuccess rate (%)\n83\n83\n83\n83\n80\n44\n32\n91\n91\n88\n88\n84\n80\n69\nDreamerV3\nLS-Imagine\n(a) Success rate\nlog\nplanks\ncrafting\ntable\nstick\nwooden\npickaxe\ncobblestone\nstone\npickaxe\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nSteps per episode\n437\n454\n465\n481\n552\n854\n923\n320\n329\n363\n381\n447\n613\n684\nDreamerV3\nLS-Imagine\n(b) The number of steps required for task completion\nFigure 14: Comparison of LS-Imagine and DreamerV3 on a long-horizon “Tech Tree” task.\n• For task such as harvest log in plains, the variance of Pthresh is high during the early stages of training.\nSince Pthresh serves as a temporal smoothing of Pjump, this reflects the significant fluctuations of\nPjump at the beginning of training, highlighting the importance of adopting a dynamic threshold.\n• Across various tasks, Pthresh consistently converges in the later stages of training, demonstrating its\neffectiveness in improving the stability of exploratory imaginations.\n• The converged values of Pthresh differ across tasks, indicating that involving an automated computa-\ntion of Pthresh enables us to avoid tedious hyperparameter tuning.\nD.3\nVISUALIZATION OF LONG SHORT-TERM IMAGINATIONS\nAs illustrated in Figure 12, we visualize the complete long short-term imagination sequences for\nthe agent across various tasks. This visualization further demonstrates how the affordance map\naccurately identifies regions of high exploration potential in the image, and how the long short-term\nimagination approach provides reasonable and applicable guidance for the agent’s task execution.\nThese qualitative results reinforce the effectiveness of our method in guiding the agent toward its\ngoal with greater precision and efficiency.\nD.4\nDEPENDENCE ON THE VISIBILITY OF OBJECTS\nThe long-term transitions of our approach rely on the affordance map to identify high-value ex-\nploration areas. However, it is crucial to note that our affordance map generation method is not\nmerely an object recognition algorithm that highlights areas only when the target is present. Thanks\nto MineCLIP’s pretraining on extensive expert demonstration videos, our approach can generate\naffordance maps that provide guidance even when the target is completely occluded.\n21\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nLong-term branch loss scale\nlong = 0.1\nlong = 0.5\nlong = 1.0\nlong = 5.0\nlong = 10.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nIntrinsic reward weight\n= 0.1\n= 0.5\n= 1.0\n= 5.0\n= 10.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nGaussian distribution std\n(\nx,\ny) = (64, 40)\n(\nx,\ny) = (128, 80)\n(\nx,\ny) = (256, 160)\nFigure 15: Results of hyperparameter sensitivity analyses.\nFor instance, as illustrated in Figure 13, throughout the task of locating a village, the affordance map\nconsistently provides effective guidance to the agent, suggesting exploration of the forest to the right\nor the open area on the left hillside, even when the village is not visible in the current observation.\nSimilarly, in mining tasks where ores are typically underground, the affordance map directs the agent\nto dig into the mountain area on the right. As we can see, even when the target is occluded, the\naffordance map enables the agent to continue exploring effectively.\nFurther research direction.\nDue to the complexity of open-world environments, the affordance\nmap may fail to provide effective guidance in scenarios that the MineCLIP model has not encountered\nbefore. To address this issue, we plan to progressively finetune the MineCLIP model with the\ncollected new data and introduce a new prompt to the agent: “Explore the widest possible area to find\n{target}” when the affordance map fails to identify high-value areas. This prompt, combined with\nintrinsic rewards generated by MineCLIP, encourages the agent to conduct extensive exploration.\nD.5\nRESULTS ON LONG-HORIZON TASKS\nTo demonstrate the potential application of LS-Imagine in more complex tasks, we conduct experi-\nments on a “Tech Tree” task in MineDojo, specifically crafting a stone pickaxe from scratch. This\ntask involves seven subgoals: log, planks, crafting table, stick, wooden pickaxe, cobblestone, and\nstone pickaxe. Since LS-Imagine is primarily designed to focus on environmental interactions and\ntask execution under fixed objectives, rather than task decomposition and planning, we adopt the\nDECKARD method (Nottingham et al., 2023) for task planning. This method provides top-level\nguidance, with LS-Imagine executing the corresponding subtasks. Each subtask was trained for\n1 million steps and then tested within 1,000 steps per episode. The results are shown in Figure\n14, which demonstrate that our LS-Imagine consistently outperforms DreamerV3, achieving higher\nsuccess rates and requiring fewer steps to complete each subgoal.\nD.6\nHYPERPARAMETER ANALYSES\nWe conduct sensitivity analyses on three hyperparameters:\n• The long-term branch loss scale βlong: As shown in Figure 15 (Left), we observe that when βlong\nfor the long-term branch is too small or too large, it impedes the learning of long-term imagination,\nleading to a decline in performance.\n• The intrinsic reward weight α: From Figure 15 (Middle), we observe that if the hyperparameter\nα for intrinsic reward is excessively small, it may result in insufficient guidance and inaccurate\nreward estimation for the post-jumpy-transition state.\n• The intrinsic reward Gaussian parameters (σx, σy): As shown in Figure 16, (σx, σy) control the\nstandard deviations of the Gaussian distribution along the horizontal and vertical axes, respectively.\nIntuitively, setting these hyperparameters too low may cause the model to overlook targets located\nat the edges of the observed images. Conversely, excessively high (σx, σy) may reduce the reward\ndiscrepancy for targets at different positions within the observation, thereby diminishing the agent’s\nincentive to focus on the target precisely. From Figure 15 (Right), we observe that the final\nperformance is robust to the tested parameters, with all configurations outperforming the baseline\nmodels presented in previous experiments.\nThe final hyperparameters of LS-Imagine are shown in Table 4.\n22\nPublished as a conference paper at ICLR 2025\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (64, 40)\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (128, 80)\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (256, 160)\n0.5\n1.0\n1.5\n2.0\n2.5\nFigure 16: Visualization of Gaussian matrices with different standard deviations.\nTable 4: Hyperparameters of LS-Imagine.\nName\nNotation\nValue\nAffordance map generation\nSliding window size\n—\n0.15 × 0.15\nSliding steps\n—\n9 × 9\nU-Net train epochs\n—\n500\nU-Net initial learning rate\n—\n5 × 10−4\nU-Net learning rate decay epochs\n—\n50\nU-Net learning rate decay rate\n—\n0.10\nText feature dimensions\n—\n512\nGaussian distribution standard deviations\n(σx, σy)\n(128, 80)\nGeneral\nReplay capacity\n—\n1 × 106\nBatch size\nB\n16\nBatch length\nT\n32\nTrain ratio\n—\n16\nWorld Model\nIntrinsic reward weight\nα\n1\nDeterministic latent dimensions\n—\n4,096\nStochastic latent dimensions\n—\n32\nDiscrete latent classes\n—\n32\nRSSM number of units\n—\n1,024\nWorld model learning rate\n—\n1 × 10−4\nLong-term branch loss scale\nβlong\n1\nReconstruction loss scale\nβpred\n1\nDynamics loss scale\nβdyn\n1\nRepresentation loss scale\nβrep\n0.1\nBehavior Learning\nImagination horizon\nL\n15\nDiscount\nγ\n0.997\nλ-target\nλ\n0.95\nActor learning rate\n—\n3 · 10−5\nCritic learning rate\n—\n3 · 10−5\n23\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Open-World Reinforcement Learning over Long Short-Term Imagination.pdf"}
{"title":"VirtualHome: Simulating Household Activities via Programs","authors":"Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba","summary":"In this paper, we are interested in modeling complex activities that occur in\na typical household. We propose to use programs, i.e., sequences of atomic\nactions and interactions, as a high level representation of complex tasks.\nPrograms are interesting because they provide a non-ambiguous representation of\na task, and allow agents to execute them. However, nowadays, there is no\ndatabase providing this type of information. Towards this goal, we first\ncrowd-source programs for a variety of activities that happen in people's\nhomes, via a game-like interface used for teaching kids how to code. Using the\ncollected dataset, we show how we can learn to extract programs directly from\nnatural language descriptions or from videos. We then implement the most common\natomic (inter)actions in the Unity3D game engine, and use our programs to\n\"drive\" an artificial agent to execute tasks in a simulated household\nenvironment. Our VirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and testing of video\nunderstanding models. We further showcase examples of our agent performing\ntasks in our VirtualHome based on language descriptions.","url":"http:\/\/arxiv.org\/abs\/1806.07011v1","pdf_url":"http:\/\/arxiv.org\/pdf\/1806.07011v1","published":1529374604000,"comment":"CVPR 2018 (Oral)","pdf_text":"VirtualHome: Simulating Household Activities via Programs\nXavier Puig1∗, Kevin Ra2∗, Marko Boben3∗, Jiaman Li4, Tingwu Wang4,\nSanja Fidler4, Antonio Torralba1\n1MIT\n2McGill University\n3University of Ljubljana\n4University of Toronto\n{xavierpuig,torralba}@csail.mit.edu\nkevin.ra@mail.mcgill.ca\nmarko.boben@fri.uni-lj.si\n{tingwuwang,ljm,fidler}@cs.toronto.edu\nAbstract\nIn this paper, we are interested in modeling complex ac-\ntivities that occur in a typical household. We propose to use\nprograms, i.e., sequences of atomic actions and interactions,\nas a high level representation of complex tasks. Programs\nare interesting because they provide a non-ambiguous rep-\nresentation of a task, and allow agents to execute them.\nHowever, nowadays, there is no database providing this type\nof information. Towards this goal, we ﬁrst crowd-source\nprograms for a variety of activities that happen in people’s\nhomes, via a game-like interface used for teaching kids how\nto code. Using the collected dataset, we show how we can\nlearn to extract programs directly from natural language\ndescriptions or from videos. We then implement the most\ncommon atomic (inter)actions in the Unity3D game engine,\nand use our programs to “drive” an artiﬁcial agent to ex-\necute tasks in a simulated household environment. Our\nVirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and\ntesting of video understanding models. We further showcase\nexamples of our agent performing tasks in our VirtualHome\nbased on language descriptions.\n1. Introduction\nAutonomous agents need to know the sequences of ac-\ntions that need to be performed in order to achieve certain\ngoals. For example, we might want a robot to clean our room,\nmake the bed, or cook dinner. One can deﬁne activities with\nprocedural recipes or programs that describe how one can\naccomplish the task. A program contains a sequence of sim-\nple symbolic instructions, each referencing an atomic action\n(e.g. “sit”) or interaction (e.g. “pick-up object”) and a num-\nber of objects that the action refers to (e.g., “pick-up juice”).\nAssuming that an agent knows how to execute the atomic\nactions, programs provide an effective means of “driving” a\nrobot to perform different, more complex tasks. Programs\n∗Denotes equal contribution\nRobots\nAction:     Work on computer\nDescription:  Turn on your computer and \nsit in front of it. Type on the keyboard, \ngrab the mouse to scroll.\nAction:     Make coffee\nDescription:  Go to the kitchen and swith\non the coffee machine. Wait until it’s \ndone and pour the coffee into a cup.\nAction:     Read a book\nDescription:  Sit down in recliner. Pick up \na novel off of coffee table. Open novel to \nlast read page. Read.\nVirtualHome\nrobot playground\nprogram\nvideo\nKnowledge Base of Household Tasks\nFigure 1: We ﬁrst crowdsource a large knowledge base of household\ntasks, (top). Each task has a high level name, and a natural language in-\nstruction. We then collect “programs” for these tasks, (middle left), where\nthe annotators “translate” the instruction into simple code. We implement\nthe most frequent (inter)actions in a 3D simulator, called VirtualHouse,\nallowing us to drive an agent to execute tasks deﬁned by programs. We pro-\npose methods to generate programs automatically from text (top) and video\n(bottom), thus driving an agent via language and a video demonstration.\ncan also be used as an internal representation of an activity\nshown in a video or described by a human (or another agent).\nOur goal in this paper is to automatically generate programs\nfrom natural language descriptions, as well as from video\ndemonstrations, potentially allowing naive users to teach\ntheir robot a wide variety of novel tasks.\nTowards this goal, one important missing piece is the\nlack of a database describing activities composed of multi-\nple steps. We ﬁrst crowdsource common-sense information\nabout typical activities that happen in people’s homes, form-\ning the natural language know-how of how these activities\nare performed. We then adapt the Scratch [1] interface used\nfor teaching kids how to code in order to collect programs\nthat formalize the activity as described in the knowledge\nbase. Note that these programs include all the steps required\nfor the robot to accomplish a task, even those that are not\nmentioned in the language descriptions. We then implement\nthe most common atomic (inter)actions in the Unity3D game\nengine, such as pick-up, switch on\/off, sit, stand-up. By ex-\nploiting the physics, navigation and kinematic models in the\narXiv:1806.07011v1  [cs.CV]  19 Jun 2018\ngame engine we enable an artiﬁcial agent to execute these\nprograms in a simulated household environment.\nWe ﬁrst introduce our data collection effort and the pro-\ngram based representation of activities. In Sec. 5 we show\nhow we can learn to automatically translate natural language\ninstructions of activities into programs. In Sec. 4 we intro-\nduce the VirtualHome simulator that allows us to create a\nlarge activity video dataset with rich ground-truth by using\nprograms to drive an agent in a synthetic world. Finally,\nwe use the synthetic videos to train a system to translate\nvideos of activities into the program being executed by the\nagent. Our VirtualHome opens an important “playground”\nfor both vision and robotics, allowing agents to exploit lan-\nguage and visual demonstration to execute novel activities\nin a simulated environment. Our data is available online:\nhttp:\/\/virtual-home.org\/.\n2. Related Work\nActions as programs. A few works have deﬁned activi-\nties as programs. In [30], the authors detect objects and ac-\ntions in cooking videos and generate an “action plan” using\na probabilistic grammar. By generating the plan, the robots\nwere able to execute complex actions by simply watching\nvideos. These authors further collected a tree bank of action\nplans from annotated cooking videos [29], creating a knowl-\nedge base of actions as programs for cooking. [18] tried to\ntranslate cooking recipes into action plans using an MRF.\n[23, 3] also argued for actions as a sequence of atomic steps.\nThey aligned YouTube how-to videos with their narrations\nin order to parse videos into such programs. Most of these\nworks were limited to either a small set of activities, or to\na narrow domain (cooking). We go beyond this by creating\na knowledge base about an exhaustive set of activities and\ntasks that people do in their homes.\n[25] crowd-sourced scripts of people’s actions at home\nin the form of natural language. These were mostly com-\nprised of one or two sentences describing a short sequence\nof actions. While this is valuable information, language is\nvery versatile and thus hard to convert into a usable program\non a robot. We show how to do this in our work.\nCode generation. There is increased interest in generat-\ning and interpreting source code [13]. Work most relevant\nto ours produces code given natural language inputs. [4]\nretrieves code snippets from Stackoverﬂow based on lan-\nguage queries. Given a sentence describing conditions, [19]\nproduces If-This-Then-That code. [14] generates a program\nspecifying the logic of a card game given a short descrip-\ntion of the rules. In [9], the authors inferred programs to\nanswer visual questions about images. Our work differs in\nthe domain, and works with text or video as input.\nRobotics. A subﬁeld of robotics aims at teaching robots\nto follow instructions provided in natural language by a hu-\nman tutor. However, most of the existing literature deals\nwith a constrained problem, for example, they learn to trans-\nlate navigational instructions into a sequence of robotic ac-\ntions [27, 15, 12, 16]. These instructions are typically sim-\npler as they directly mention what to do next, and the action\nspace is small. This is not the case in our work which also\nconsiders interactions with objects, and everyday activities\nwhich are typically far more complex.\nSimulation. Simulations using game engines have been\ndeveloped to facilitate training visual models for autonomous\ndriving [8, 21, 7], quadcopter ﬂying [24], or other robotic\ntasks [5]. Recent works have focused on simulating indoor\nenvironments, allowing for target-driven indoor navigation\nor interactive question answering [11, 28, 6, 22]. A few of\nthese works [11, 6] include actionable objects, allowing to\ninteract and change the environment. Our work focuses on\nsimulating a wide range of human actions, both in terms\nof objects interactions and human poses, which allows to\nsimulate common activities. We are not aware of simulators\nat the scale of objects and actions in a home, like ours. Lastly,\nwe give credit to the popular game Sims which we draw our\ninspiration from. Sims is a strategic video game mimicking\ndaily household activities. Unfortunately, the source of the\ngame is not public and thus cannot be used for our purpose.\n3. KB of Household Activities for Robots\nOur goal is to build a large repository of common activi-\nties and tasks that we perform in our households in our daily\nlives. These tasks can include simple actions like “turning on\nTV\" or complex ones such as “make coffee with milk\". What\nmakes our effort unique is that we are interested in collect-\ning this information for robots. Unlike humans, robots need\nmore direct instructions. For example, in order to “watch\ntv”, one might describe it (to a human) as “Switch on the\ntelevision, and watch it from the sofa”. Here, the actions\n“grab remote control” and “sit\/lie on sofa” have been omit-\nted, since they are part of the commonsense knowledge that\nhumans have. In our work, we aim to collect all the steps\nrequired for a robot to successfully execute a task, including\nthe commonsense steps. In particular, we want to collect\nprograms that fully describe activities.\nDescribing actions as programs has the advantage that it\nprovides a clear and non-ambiguous description of all the\nsteps needed to complete a task. Such programs can then\nbe used to instruct a robot or a virtual character. Programs\ncan also be used as a representation of a complex task that\ninvolves a number of simpler actions, providing a way to\nunderstand and compare activities and goals.\n3.1. Data Collection\nIn this section, we describe our dataset collection using\ncrowdsourcing. Describing actions as programs can be a\nchallenging task as most annotators have no programing\nexperience. We split the data collection effort in two parts.\nRGB\npose\nclass seg.\ninst. seg\ndepth\nﬂow\nFigure 2: VirtualHome Activity Dataset is a video dataset of composite activities created with our simulator. We start by generating\nprograms using a simple probabilistic grammar. We animate each program in VirtualHome by randomizing the selection of homes, agents,\ncameras, as well as the placement of a subset of the objects, the initial location of the agent, the speed of the actions, and choice of objects\nfor interactions. Each program is shown to an annotator who is asked to describe it in natural language (top row). Videos have ground-truth:\n(second row) time-stamp for each atomic action, (bottom) 2D and 3D pose, class and object instance segmentation, depth and optical ﬂow.\nIn the ﬁrst part, we ask AMT workers to provide verbal\ndescriptions of daily household activities. In particular, each\nworker is asked to come up with a common activity\/task,\ngive it a high level name, eg “make coffee”, and describe it\nin detail. In order to cover a wide spectrum of activities we\npre-speciﬁed in which scene the activity should start. Scenes\nwere selected randomly from a list of 8 scenes (living room,\nkitchen, dining room, bedroom, kids bedroom, bathroom,\nentrance hall, and home ofﬁce). An example of a described\nactivity is shown in Fig. 3. Note that these descriptions may\nlikely omit the commonsense steps, as they were written by\n“naive” workers that were describing these activities as they\nwould to a (human) friend.\nIn the second stage, we showed the collected descrip-\ntions to the AMT workers and asked them to translate these\ndescriptions into programs using a graphical programing lan-\nguage. We told them to produce a program that will “drive” a\nrobot to successfully accomplish the described activity. Our\ninterface builds on top of MIT’s Scratch project [1] designed\nto teach young children to write symbolic code. We found\nthat workers were capable of quickly learning to produce\nuseful programs by providing them with a carefully designed\ntutorial. Fig. 3 shows a snapshot of the programing interface.\nFinally, we asked more qualiﬁed workers hired via Upwork\ncrowdsourcing platform to double check the collected data.\nWorkers had to compose a program by composing a se-\nquence of steps. Each instruction is a Scratch block from a\npredeﬁned list of 77 possible blocks compiled by analyzing\nthe frequency of verbs in the collected descriptions. Each\nstep in the program is deﬁned by a block. A block deﬁnes a\nsyntactic frame with an action and a list of arguments (e.g.,\nthe block walk requires one argument to specify the desti-\nnation, Fig. 3.c). To simplify the search for blocks they are\norganized according to 9 broad action categories (Fig. 3.b).\nWe required that the program contains all the steps, even\nthose not explicitly mentioned in the description, but that\ncould be inferred from common-sense. Fig. 3.d shows an\nexample of a program. We also allowed annotators to use\na “special” block for missing actions, where the step can be\nwritten as free-form text. Programs using this special block\nwill not be used in the rest of the paper, but allowed us in\nidentifying new blocks that needed to be added.\nMore precisely, step t in the program can be written as:\nstept = [actiont] ⟨objectt,1⟩(idt,1) ... ⟨objectt,n⟩(idt,n)\nHere, id is an unique identiﬁer (counter) of an object and\nhelps in disambiguating different instances of objects that\nbelong to the same class. An example of a program for\n“watch tv” would be:\nstep1 = [Walk] ⟨TELEVISION⟩(1)\nstep2 = [SwitchOn] ⟨TELEVISION⟩(1)\nstep3 = [Walk] ⟨SOFA⟩(1)\nstep4 = [Sit] ⟨SOFA⟩(1)\nstep5 = [Watch] ⟨TELEVISION⟩(1)\nHere, the programs deﬁnes that the television in steps 1, 2\nand 5 refer to the same object instance.\n3.2. Dataset Analysis\nIn the ﬁrst part we collected 1814 descriptions. From\nthose, we were able to collect programs for 1703 descrip-\ntions. Some of the programs contained several “special\nblocks” for missing actions, which we remove, resulting\nin 1257 programs. We ﬁnally selected a set of tasks and\nasked workers to write programs for them, obtaining 1564\nadditional programs. The resulting 2821 programs form our\nActivityPrograms dataset. On average, the collected descrip-\ntions have 3.2 sentences and 21.9 words, and the resulting\nprograms have 11.6 steps on average. The dataset statistics\nare summarized in Table 1.a.\nAction name: \nThrow away newspaper\nDescription: \nTake the newspaper \non the living room table \nand toss it.\na)\nb)\nc)\nd)\nFigure 3: a) Description provided by a worker. b) User interface showing\nthe list of block categories and 4 example blocks, c) Example of composition\nof a block by adding the arguments. Each block is like a Lego piece where\nthe user can drop arguments inside and attach one block to another. d) Final\nprogram corresponding to the description from (a).\nAction counts\nWalk\nGrab\nFind\nPutback\nPutobjback\nLookat\nOpen\nSwitchon\nClose\nSit\nSwitchoff\nPointat\nPour\nWipe\nRinse\nWash\nPull\nGreet\nScrub\nRead\nPush\nTouch\nType\nTurnto\nCrawl\nFold\nPlugin\nWatch\nLie\nCover\nLift\nWrite\nPuton\nPutoff\nEat\nPlugout\nStandup\nSleep\nEnter\nSqueeze\nDrink\nShake\nLeave\nDrop\nUncover\nSpread\nStretch\nRun\nDance\nCut\n0\n500\n1000\n1500\n2000\n2500\nObject counts\nPlate\nChair\nCabinet\nFaucet\nBed\nBook\nCloset\nKitchen\nVacuum cleaner\nLiving room\nTelevision\nBedroom\nComputer\nRag\nTable\nHome office\nFridge\nFood\nSink\nBathroom\nToy\nTowel\nCouch\nChild\nNone\nRemote control\nBowl\nShoes\nDining room\nPillow\nWater\nCleaning solution\nSoap\nClothesshirt\nLaptop\nClothespants\nClothesdress\nPaper\nBlanket\nBroom\nSpoon\nKeyboard\nSponge\nFork\nDoor\nEntrance hall\nCup\nDresser\nWoman\nArms\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\na)\nb)\n    'Wa\nc\nFigure 4: a) Counts of actions in our ActivityPrograms\ndataset, b) object counts (zoom to read)\nThe dataset covers 75 atomic actions and 308 objects,\nmaking 2709 unique steps. Fig. 4.a shows a histogram of\nthe 50 most common actions appearing in the dataset, and,\nFig. 4.b, the 50 most common objects.\nOur dataset contains activities with several examples, and\nwe analyze their diversity by comparing their programs. Ta-\nble 1.b analyzes 4 selected activities. We compute their\nsimilarities as the average length of the longest common\nsubsequences computed between all pairs of programs.\nWe can also measure distances between activities by\nmeasuring the distance between programs. The similarity\nbetween two programs is measured as the length of their\nlongest common subsequence of instructions divided by the\nlength of the longest program. Table 1.c. shows the similar-\nity matrix (sorted to better show the block diagonal structure)\nbetween different activities in our dataset.\nCompleteness of programs. We analyze whether the\ncollected programs contain all the necessary steps to execute\nthe given task. We sample 100 collected programs and ask\n5 AMT workers to rate whether the program is complete,\nmissing minor steps (sitting in a chair before walking to-\nwards it) or important steps (ﬁlling a glass before drinking).\nResults show that 64% of the programs are complete, 28%\nare missing minor steps and 8% are missing crucial steps.\n4. VirtualHome: Simulator of Household Tasks\nThe main motivation behind using programs to represent\nactivities is to “drive” robots to perform tasks by having them\nexecuting these programs. As a proxy, we here use programs\nto drive characters in a simulated 3D environment. Simula-\ntions are useful as they deﬁne a playground for “robots”, an\nenvironment where artiﬁcial agents can be taught to perform\ntasks. Here, we focus on building the simulator, and leave\nlearning inside the simulator to future work. In particular,\nwe will assume the agent has access to all 3D and semantic\ninformation about the environment, as well as to manually\ndeﬁned animations. Our focus will be to show that programs\nrepresent a good way of instructing such agents. Further-\nmore, our simulator will allow us to generate a large-scale\nvideo dataset of complex activities that is rich and diverse.\nWe can create such a dataset by simply recording the agent\nexecuting programs in the simulator. The simulator then pro-\nvides us with dense ground-truth information, eg semantic\nsegmentation, depth, pose, etc. Fig. 2 showcases this dataset.\nWe implemented our VirtualHome simulator using the\nUnity3D game engine which allows us to exploit its kine-\nmatic, physics and navigation models, as well as user-\ncontributed 3D models available through Unity’s Assets\nstore. We obtained six furnished homes and 4 rigged hu-\nmanoid models from the web. On average, each home con-\ntains 357 object instances (86 per room). We collected ob-\njects from additional 30 object classes that appear in our\ncollected programs yet are not available in the package, via\nthe 3D warehouse 1. To ensure visual diversity, we collected\nat least 3 different models per class. The apartments and\nagents are shown in Fig. 5 and Fig. 6.\n4.1. Animating Programs in VirtualHome\nEvery step in the program requires us to animate the cor-\nresponding (inter)action in our virtual environment. We thus\nneed to both, determine which object in the home (which we\nrefer to as the game object) the step requires as well as prop-\nerly animating the action. To get the former we need to solve\nan optimization problem by taking into account all steps in\nthe program and ﬁnding a feasible path. For example, if the\nprogram requires the agent to switch on a computer and type\non a keyboard, ideally the agent would type on the keyboard\nnext to the chosen computer and not navigate to another key-\nboard attached to a different computer in possibly a different\nroom. We now describe our simulator in more detail.\nAnimating atomic actions. There is a huge variety and\nnumber of atomic actions that appear in the collected pro-\ngrams, as can be seen in Fig. 4. We implemented the 12 most\nfrequent ones: walk\/run, grab, switch-on\/off, open\/close,\nplace, look-at, sit\/standup, touch. Note that there is a large\nvariability in how an action is performed depending on to\nwhich object it is applied to (e.g., opening a fridge is dif-\nferent than opening a drawer). We use Unity’s NavMesh\nframework for navigation (path planner to avoid obstacles).\nFor each action we compute the agent’s target pose and\nanimate the action using RootMotion FinalIK inverse kine-\nmatics package. We further animate certain objects the agent\ninteracts with, e.g., we shake a coffee maker, animate toast\nin a toaster, show a (random) photo on a computer or TV\n1https:\/\/3dwarehouse.sketchup.com\nDataset\n# prog. avg # steps avg # sent. avg # words\nActivityProg.\n2821\n11.6\n3.2\n21.9\nSyntheticProg. 5193\n9.6\n3.4\n20.5\n(a)\nAction\n# Prog. LCS Norm. LCS\nMake coffee\n69\n4.56\n0.26\nFold laundry\n11\n1.29\n0.08\nWatch TV\n128\n3.65\n0.40\nClean\n42\n0.76\n0.04\n(b)\n(c)\nBrowse internet\nWork\nUser computer\nCheck email\nPay bills\nStudy\nEat dinner\nSet up table\nWash dishes with dishwasher\nWash dishes by hand\nWash dishes\nBrush teeth\nWash face\nTake shower\nTake bath\nWash hands\nMake bed\nTake nap\nGo to sleep\nSleep\nTurn off light\nTuck kids in bed\nWatch movie\nChange TV channel\nWatch TV\nTable 1: (a) We analyze programs and natural language descriptions for both, real activities in ActivityPrograms (Sec. 3), and synthesized\nprograms (with real descr.). (b) ActivityPrograms: Analyzing diversity in the same activity, by computing similarities across all pairs of the\ncollected programs. “LCS\" denotes longest common subsequence. For “norm.LCS\", we normalize by max length of the two programs. (c)\nshows the similarity matrix (sorted to better show the block diagonal structure) between different activities in our dataset.\nFigure 5: 3D households in our VirtualHome. Notice the diversity in room and object layout\nand appearance. Each home has on average 357 objects. First 4 scenes are used for training,\nthe ﬁfth is also used in val, and all scenes are used when testing our video-to-script model.\nmale 1\nfemale 1\nmale 2\nfemale 2\nFigure 6: Agents in VirtualHome. We use\nmale 1 and female 1 in train., and all agents\nwhen testing our video-to-program model.\nscreen, light up a burner on a stove, and light up the lamps in\nthe room, when these objects are switched on by the agent.\nPreparing the Scene. While every 3D home already con-\ntains many objects, the programs may still mention objects\nthat are not present in the scene. To deal with this, we ﬁrst\n“set” the scene by placing all missing objects that a program\nrefers to in the home, before we try to execute the program.\nTo be able to prepare a scene in a plausible way, we collect a\nknowledge base of possible object locations. The annotator\nis shown the class name and selects a list of other objects\n(including ﬂoor, wall) that are likely to support it.\nExecuting a Program. To animate a program we need\nﬁrst to create a mapping between the objects in the program\nand the corresponding instances inside the virtual simulator.\nFurthermore, for each step in the program, we also need to\ncompute the interaction position of the agent with respect to\nan object, and any additional information needed to animate\nthe action (e.g., which hand to use, speed). We build a tree\nof all possibilities of assigning game objects to objects in the\nprogram, along with all interaction positions and attributes.\nTo traverse the tree of possible states we use backtracking\nand stop as soon as a state executing the last step is found.\nSince the number of possible object mappings for each step is\nsmall, and we can prune the number of interaction positions\nto a few, our optimization runs in a few seconds, on average.\nAnimation. We place 6-9 static cameras in each room,\n26 per home on average. During recording, we switch be-\ntween cameras based on agent’s visibility. In particular, we\nrandomly select a camera which sees the agent, and keep it\nuntil the agent is visible and within allowed distance. For\nagent-object interaction we also try to select a camera and\nadjust its ﬁeld of view to enhance the visibility of the inter-\naction. We further randomize the position, angle and ﬁeld\nof view of each camera. Randomization is important when\ncreating a dataset to ensure diversity of the ﬁnal video data.\nVirtualHome Activity dataset. Since the programs in\nActivityPrograms represent real activities that happen in\nhouseholds, they contain signiﬁcant variability in actions\nand objects that appear in steps. While our ultimate aim is\nto be able to animate all these actions in our simulator, our\ncurrent efforts only support the top 12 most frequent actions.\nWe thus create another dataset that contains programs con-\ntaining only these actions in their steps. The creation of this\ndataset is explained below.\nWe synthesized 5,193 programs using a simple probabilis-\ntic grammar, and had each one described in natural language\nby a human annotator. Although these programs were not\ngiven by annotators, they produced reasonable activities, cre-\nating a much larger dataset of paired descriptions-programs\nat a fraction of the cost. We then animated each program\nin our simulator, and automatically generated ground-truth\nwhich allows us to train and evaluate our video models.\nAs can be seen from Table 1, descriptions in VirtualHome\nActivity dataset are of comparable length. However, the\nvocabulary here was biased towards that used in programs.\nWe animate the programs as described above, by random-\nizing the selection of home, an agent, cameras, placement\nof a subset of objects, initial location of the agent, speed\nof the actions, and choice of objects for interactions. We\nbuild on top of [2] to automatically generate groundtruth: 1)\ntime-stamp of each step to video, 2) agent’s 2D\/3D pose, 3)\nclass and instance segmentation, 4) depth, 5) optical ﬂow, 6)\ncamera parameters. Example of data is shown in Fig. 2.\n5. From Videos and Descriptions to Programs\nWe introduce a novel task using our dataset. In particular,\nwe aim to generate a program for the activity from either a\nnatural language description or from a video demonstration.\nWe treat the task of transcribing an input (description or\nvideo) into a program as a translation problem. We adapt the\nseq2seq model [26] for our task, and train it with Reinforce-\nment Learning that exploits the reward from the simulator.\nOur model consists of an RNN encoder that encodes\nthe input sequence into a hidden vector representation, and\nanother RNN acting as a decoder, generating one step of the\nprogram at a time. We use LSTM with 100-dim hidden states\nas our encoder. At each step t, our RNN decoder decodes a\nstep which takes the form of eq. (1). Let xt denote an input\nvector to our RNN decoder at step t, and let ht be the hidden\nstate. Here, ht is computed as in the standard LSTM using\ntanh as the non-linearity. Let ai be a one-hot encoding of an\naction i, and oi a one-hot encoding of an object. We compute\nthe probability pt\ni of an instruction i at step t as:\n˜ai = Waai, ˜oi,n = Wooi,n, vi = mean(˜ai, ˜oi,1, ..., ˜oi,n)\npt\ni = softmaxi( vi\n∥vi∥\nT\n· Wv(ht∥xatt\nt )) (1)\nwhere Wa and Wo and Wv are learnable matrices, and vi\ndenotes an embedding of an instruction. Note that here, n is\neither 1 or 2 (our instructions have at most two objects).\nThe input vector xt concatenates multiple features. In par-\nticular, we use the embedding v of the step with the highest\nprobability from the previous time instance of the decoder.\nFollowing [26], we further use the attention mechanism over\nthe encoder’s states to get another feature xatt\nt . In particular:\nαt\nj = softmaxj(vT \u0000Watt (ht∥hj\nenc)\n\u0001\u0001\n(2)\nxatt\nt\n=\nX\nj\nαt\njhj\nenc\n(3)\nwhere Watt, v are learnable parameters. Our full model is\nvisualized in Fig. 7.\nLearning and inference. Our goal is to generate pro-\ngrams that are both close to the ground-truth programs in\nterms of their LCS (longest common subsequence) and are\nalso executable by our renderer. To that end, we train our\nmodel in two phases. Firstly, we pre-train the model using\ncross-entropy loss at each time step of the RNN decoder.\nWe follow the typical training strategy where we make a\nprediction at each time instance but feed in the ground-truth\nstep to the next time instance. We use the word2vec [17]\nembeddings for matrices Wa and Wo.\nIn the second stage, we treat program generation as an\nReinforcement Learning problem, where the agent is learn-\ning a policy that generates steps to compose a program.\nWe follow [20], and use policy gradient optimization to\ntrain the model, using the greedy policy as the baseline es-\ntimator. We exploit two different kinds of reward r(ws, g)\nfor RL training, where ws denotes the sampled program,\nand g the ground-truth program. To ensure that the gener-\nated program is semantically correct (follows the descrip-\ntion\/video), we use the normalized LCS metric (length of the\nlongest common subsequence) between the two programs\nas our ﬁrst reward rLCS(ws, g). The second reward comes\nfrom our simulator, and measures whether the generated\nprogram is executable or not. This reward, rsim(ws), is a\nsimple binary value. We carefully balance the total reward\nas, r(ws, g) = rLCS(ws, g) + 0.1 · rsim(ws).\nSo far we did not specify the input to the RNN encoder.\nOur model accepts either a language description or a video.\nTextual Description.\nTo encode a textual description our\nRNN encoder gets as input the word2vec [17] embedding of\nthe word in the description at each time instance.\nVideo.\nTo generate programs from videos, we partition\neach video into 2-second clips and train a model to predict\nthe step at the middle frame. We use DilatedNet to obtain the\nsemantic segmentation of each frame and use the Temporal\nRelation Network [31] with 4-frame relations to predict the\nembedding of an instruction (action+object+object). We use\nthis embedding to obtain the likelihood of each instruction.\nThe prediction at each clip is used as input to the RNN\nencoder for program generation.\n6. Experiments\nIn our experiments we exploit both of our datasets: Activ-\nityPrograms containing descriptions and programs for real\nactivities, and VirtualHome Activity dataset that contains\nsynthesized programs, yet natural descriptions to describe\nthem. VirtualHome Activity dataset further contains videos\nanimating the programs.\n6.1. Step (Instruction) Classiﬁcation from Video\nWe ﬁrst evaluate our model for the task of video-based\naction and action-object-object (step\/instruction in the pro-\ngram) classiﬁcation. Here, we partition each video in 2-sec\nclips, and use the clip-based TRN to perform classiﬁcation.\nWe compute performance as the mean per-class accuracy\nacross all 2-sec clips in test. To better understand the gen-\neralization properties of the video-based models, we further\ndivide the test set into videos recorded in homes seen at\ntrain time, and videos in homes not seen at train time. We\nreport the results in Table 2 (left). To set the lower bound,\nwe also report a simple random retrieval baseline, in which\na step is randomly retrieved from the training set. We can\nsee that our model performs signiﬁcantly better. However,\na large number of actions and objects of interest, makes the\nprediction task challenging for the model.\nFigure 7: Our encoder-decoder LSTM for generating programs from natural language descriptions or videos.\nAction\nObjects\nSteps\nMean\nRand. Retrieval\n8.30%\n1.50%\n0.51%\n3.43%\nSeen homes\n70.32 %\n42.14 %\n23.81 %\n45.42%\nUnseen homes\n31.34%\n14.55%\n11.48%\n19.12%\nAll\n46.85%\n25.76%\n18.41%\n30.34%\nAction\nObjects\nSteps\nMean\nSimulator\nRand. Retrieval\n.473\n.079\n.071\n.207\n100.0%\nMLE\n.735\n.359\n.341\n.478\n19.4%\nPG(LCS)\n.761\n.383\n.364\n.502\n19.0%\nPG(LCS+Sim)\n.751\n.377\n.358\n.495\n22.4%\nPG(LCS+Sim) Seen homes\n.851\n.556\n.528\n.645\n24.6%\nPG(LCS+Sim) Unseen homes\n.680\n.250\n.236\n.389\n20.9%\nTable 2: Left: Accuracy of video-based action classiﬁcation and action-object-object (step or instruction in the program) prediction in\n2-sec clips from our VirtualHome Activity dataset. Right: Video-based program generation.\nMethod\nAction\nObjects\nSteps\nMean\nSimulator (%)\nRand. Sampling\n.226\n.039\n.020\n.095\n0.6%\nRand. Retrieval\n.473\n.079\n.071\n.207\n100.0%\nSkipthoughts\n.642\n.272\n.252\n.389\n100.0%\nMLE\n.777\n.723\n.686\n.729\n38.6%\nPG(LCS)\n.803\n.766\n.732\n.767\n35.5%\nPG(LCS+Sim)\n.806\n.775\n.740\n.774\n39.8%\nMethod\nAction\nObjects\nSteps\nMean\nRand. Sampling\n.106\n.018\n.004\n.043\nRand. Retrieval\n.320\n.037\n.032\n.130\nSkipthoughts\n.469\n.297\n.266\n.344\nMLE\n.497\n.392\n.340\n.410\nPG(LCS)\n.522\n.433\n.387\n.447\nTable 3: Programs from descr.: Accuracy on (left) VirtualHome Act., and (right) ActivityPrograms. We compute the length of longest\ncommon subsequence between a predicted script and GT and divide by max length of the two programs, mimicking IoU for programs. Since\nreal programs are mainly not executable in our simulator due to the lack of implemented actions, we cannot report the executability metric.\n6.2. Program Generation\nWe now evaluate the task of program generation.\nMetrics. We evaluate program induction using a measure\nsimilar to IOU. We compute the longest common subse-\nquence between a GT and a predicted program, where we\nallow gaps between the matching steps, but require their\norder to be correct. We obtain accuracy as the length of the\nsubseq. divided by the max of the two programs’ lengths.\nWe also compute accuracies for actions and objects alone.\nSince LCS does not measure whether the program is valid,\nwe report another metrics that computes the percentage of\ngenerated programs that are executable in our simulator.\nLanguage-based prediction. Since we have descrip-\ntions for all activities, we ﬁrst evaluate how well our model\ntranslates natural language descriptions into programs. We\nreport results on ActivityPrograms (real activities), as well\nas on VirtualHome Activity datasets (where we ﬁrst only\nconsider descriptions, not videos). We compare our models\nto four baselines: 1) random sampling, where we randomly\npick both an action for each step and its arguments, 2) ran-\ndom retrieval, where we randomly pick a program from\nthe training set, 3) skipthoughts, where we embed the de-\nscription using [10, 32], retrieve the closest description from\ntraining set and take its program, 4) our model trained with\nMLE (no RL). Table 3 provides the results. We can see that\nour model outperforms all baselines on both datasets. Our\nRL model that exploits LCS reward outperforms the MLE\nmodel on both metrics (LCS and executability). Our model\nthat uses both rewards slightly decreases the LCS score, but\nsigniﬁcantly improves the executability metrics.\nVideo-based prediction. We also report results on the\nmost challenging task of video-based program generation.\nThe results are shown in Table 2 (right). One can observe\nthat RL training with LCS reward improves the overall ac-\ncuracy over the MLE model (the generated programs are\nmore meaningful given the description\/video), however its\nexecutability score decreases. This is expected: MLE model\ntypically generates shorter programs, which are thus more\nlikely to be executable (an empty program is always exe-\ncutable). A careful balance of both metrics is necessary. RL\nwith both the LCS and the simulator reward improves both\nLCS and the executability metrics over the LCS-only model.\nExecuting programs in VirtualHome. In Fig. 8 we\nshow a few examples of our agent executing programs gen-\nerated from natural descriptions. To understand the quality\nof our simulator as well as the plausibility of our program\nevaluation metrics, we perform a human study. We randomly\nselected 10 examples per level of performance: (a) [0.95−1],\n(b) [0.8 −0.95], (c) [0.65 −0.8], and (d) [0.5 −0.65]. For\neach example we had 5 AMT workers judge the quality of\nthe performed activity in our simulator, given its language\ndescription. Results are shown in Fig. 9. One can notice\n[Walk]\n[Grab] ⟨CUP⟩\n[Open] ⟨FRIDGE⟩\n[Grab] ⟨MILK⟩\nDescription: Get an empty glass. Take milk from refrigerator and open it. Pour milk into glass.\n[SwitchOn] ⟨TV⟩\n[Sit] ⟨SOFA⟩\n[SwitchOff] ⟨TV⟩\n[Put] ⟨COFF.-POT⟩⟨TABLE⟩\nDescription: Go watch TV on the couch. Turn the TV off and grab the coffee pot. Put the coffee pot on the table and go turn the light on.\n[Walk]\n[Grab] ⟨MAGAZINE⟩\n[Sit] ⟨TOILET⟩\n[Walk]\n[Put] ⟨MAGAZINE⟩⟨DESK⟩\nDescription: Look at the clock then get the magazine and use the toilet. When done put the magazine on the table.\n[Walk]\n[Grab] ⟨FACE SOAP⟩\n[Put] ⟨F.SOAP⟩⟨COUNTER⟩\n[SwitchOn] ⟨TOASTER⟩\n[Put] ⟨POT⟩⟨STOVE⟩\nDescription: Take the face soap to the kitchen counter and place it there. Turn toaster on and then switch it off. Place the pot on the stove.\nFigure 8: Our agent executing generated programs from descriptions, in our VirtualHome. Top description is from ActivityPrograms, while\nthe rest are from VirtualHome Activity dataset. Notice that the top agent uses his left to open the fridge and to grab an object since he already\nholds an item in his right. There are also some limitations, for example, in row 3 the agent sits on the toilet fully clothed. Furthermore,\nsometimes the carried item slightly penetrates into the character’s body due to imprecisions of the colliders.\n[0.95-1.00]\n[0.80-0.95]\n[0.65-0.80]\n[0.50-0.65]\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFailed\nMarginal\nAdequate\nGood\nExcellent\nPerfect\nFigure 9: Human evaluation of our agent executing described ac-\ntivities via program generation from text: x axis shows the program\nprediction accuracy, y axis is the human score.\nagreement between our metrics and human scores. Gener-\nally, at perfect performance the simulations got high human\nscores, however, there are examples where this was not the\ncase. This may be due to imperfect animation, an indication\nthat further improvements to our simulator are possible.\nImplications. The high performance of text-based ac-\ntivity animation opens exciting possibilities for the future.\nIt would allow us to replace the more rigid program syn-\nthesis that we used to create our dataset, by having annota-\ntors create these animations directly via natural language or\ncrowd-sourcing scripts from existing text corpora.\n7. Conclusion\nWe collected a large knowledge base of how-to for house-\nhold activities speciﬁcally aimed for robots. Our dataset\ncontains natural language descriptions of activities as well as\nprograms, a formal symbolic representation of activities in\nthe form of a sequence of steps. What makes these programs\nunique is that they contain all the steps necessary to per-\nform an activity. We further introduced VirtualHome, a 3D\nsimulator of household activities, which we used to create\na large video activity dataset with rich ground-truth. We\nproposed a simple model that infers a program from either a\nvideo or a textual description, allowing robots to be “driven”\nby naive users via natural language or video demonstration.\nWe showed examples of agents performing these programs\nin our simulator. There are many exciting avenues going\nforward, for example, training agents to perform tasks from\nvisual observation alone using RL techniques.\nAcknowledgements: We acknowledge partial support from \"La Caixa\" Fel-\nlowship, NSERC COHESA NETGP485577-15, Samsung, DARPA Explainable\nAI (XAI) program and IARPA D17PC00341. The U.S. Government is autho-\nrized to reproduce and distribute reprints for Governmental purposes notwith-\nstanding any copyright annotation thereon. Disclaimer: The views and conclu-\nsions contained herein are those of the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies or endorsements, either expressed\nor implied, of IARPA, DOI\/IBC, or the U.S. Government. We also gratefully\nacknowledge NVIDIA for donating several GPUs used in this research.\nReferences\n[1] https:\/\/scratch.mit.edu\/.\n[2] https:\/\/bitbucket.org\/Unity-Technologies\/\nml-imagesynthesis.\n[3] J.-B. Alayrac, P. Bojanowski, N. Agrawal, I. Laptev, J. Sivic,\nand S. Lacoste-Julien. Unsupervised learning from narrated\ninstruction videos. In CVPR, 2016.\n[4] M. Allamanis, D. Tarlow, A. D. Gordon, and Y. Wei. Bimodal\nmodelling of source code and natural language. In ICML,\n2015.\n[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider,\nJ. Schulman, J. Tang, and W. Zaremba. Openai gym. In\narXiv:1606.01540, 2016.\n[6] S. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti,\nF. Strub, J. Rouat, H. Larochelle, and A. C. Courville. Home:\na household multimodal environment. CoRR, abs\/1711.11017,\n2017.\n[7] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun.\nCARLA: An open urban driving simulator. In Proc. of the 1st\nAnnual Conference on Robot Learning, pages 1–16, 2017.\n[8] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as\nproxy for multi-object tracking analysis. In CVPR, 2016.\n[9] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman,\nL. Fei-Fei, C. L. Zitnick, and R. Girshick. Inferring and exe-\ncuting programs for visual reasoning. In arXiv:1705.03633,\n2017.\n[10] R. Kiros, Y. Zhu, R. Salakhutdinov, R. Zemel, A. Torralba,\nR. Urtasun, and S. Fidler. Skip-thought vectors. NIPS, 2015.\n[11] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and\nA. Farhadi. AI2-THOR: an interactive 3d environment for\nvisual AI. CoRR, abs\/1712.05474, 2017.\n[12] S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and A. Klein.\nTraining personal robots using natural language instruction.\nIntelligent Systems, pages 38–45, 2001.\n[13] C. Li, D. Tarlow, A. L. Gaunt, M. Brockschmidt, and N. Kush-\nman. Neural program lattices. In ICML, 2017.\n[14] W. Ling, E. Grefenstette, K. M. Hermann, T. Koˇcisky, A. Se-\nnior, F. Wang, and P. Blunsom. Latent predictor networks for\ncode generation. arXiv:1603.06744, 2016.\n[15] M. MacMahon, B. Stankiewicz, and B. Kuipers. Walk the\ntalk: Connecting language, knowledge, and action in route\ninstructions. In AAAI, 2006.\n[16] H. Mei, M. Bansal, and M. R. Walter. Listen, attend, and\nwalk: Neural mapping of navigational instructions to action\nsequences. In AAAI, 2016.\n[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient\nestimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781, 2013.\n[18] D. Nyga and M. Beetz. Everything robots always wanted\nto know about housework (but were afraid to ask). In IROS,\npages 243–250, 2012.\n[19] C. Quirk, R. Mooney, and Y. M. Galley. Language to code:\nLearning semantic parsers for if-this-then-that recipes. In\nACL, 2015.\n[20] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.\nSelf-critical sequence training for image captioning. CoRR,\nabs\/1612.00563, 2016.\n[21] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for\ndata: Ground truth from computer games. In ECCV, 2016.\n[22] M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and\nV. Koltun. MINOS: Multimodal indoor simulator for naviga-\ntion in complex environments. arXiv:1712.03931, 2017.\n[23] O. Sener, A. Zamir, S. Savarese, and A. Saxena. Unsupervised\nsemantic parsing of video collections. In arXiv:1506.08438,\n2015.\n[24] S. Shah, D. Dey, C. Lovett, and A. Kapoor. Aerial Informatics\nand Robotics platform. Technical Report MSR-TR-2017-9,\nMicrosoft Research, 2017.\n[25] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev,\nand A. Gupta. Hollywood in homes: Crowdsourcing data\ncollection for activity understanding. In ECCV, 2016.\n[26] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence\nlearning with neural networks. In NIPS, 2014.\n[27] S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Baner-\njee, S. J. Teller, and N. Roy. Understanding natural language\ncommands for robotic navigation and mobile manipulation.\nIn AAAI, 2011.\n[28] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building generaliz-\nable agents with a realistic and rich 3d environment. CoRR,\nabs\/1801.02209, 2018.\n[29] Y. Yang, A. Guha, C. Fermuller, and Y. Aloimonos. Manipu-\nlation action tree bank: A knowledge resource for humanoids.\nIn IEEE-RAS Intl. Conf. on Humanoid Robots, 2014.\n[30] Y. Yang, Y. Li, C. Fermuller, and Y. Aloimonos. Robot learn-\ning manipulation action plans by “watching” unconstrained\nvideos from the world wide web. In AAAI, 2015.\n[31] B. Zhou, A. Andonian, and A. Torralba. Temporal relational\nreasoning in videos. CoRR, abs\/1711.08496, 2017.\n[32] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun,\nA. Torralba, and S. Fidler.\nAligning Books and Movies:\nTowards Story-like Visual Explanations by Watching Movies\nand Reading Books. In ICCV, 2015.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/VirtualHome: Simulating Household Activities via Programs.pdf"}
{"title":"EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents","authors":"Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal","summary":"Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,\na novel framework to address this question. We first prompt an LLM to generate\ntraining environments by giving it the task description and simulator\nobjectives that the agents should learn and then asking it to generate a set of\nenvironment configurations (e.g., different terrains, items initially given to\nagents, etc.). Next, we train a small RL agent in a mixture of the original and\nLLM-generated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent is\nweak at, by providing feedback to the LLM in the form of the agent's\nperformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist environments. We find that a small RL agent\ntrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and\nlearns long-horizon tasks significantly faster. We also show that using an LLM\nto adapt environments dynamically outperforms curriculum learning approaches\nand how the environments are adapted to help improve RL agents' weaker skills\nover time. Additionally, EnvGen is substantially more efficient as it only uses\na small number of LLM calls (e.g., 4 in total), whereas LLM agents require\nthousands of calls. Lastly, we present detailed ablation studies for EnvGen\ndesign choices.","url":"http:\/\/arxiv.org\/abs\/2403.12014v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2403.12014v2","published":1710784276000,"comment":"COLM 2024; First two authors contributed equally; Project website:\n  https:\/\/envgen-llm.github.io\/","pdf_text":"Published as a conference paper at COLM 2024\nEnvGen: Generating and Adapting Environments via LLMs\nfor Training Embodied Agents\nAbhay Zala∗\nJaemin Cho∗\nHan Lin\nJaehong Yoon\nMohit Bansal\nUNC Chapel Hill\n{aszala, jmincho, hanlincs, jhyoon, mbansal}@cs.unc.edu\nhttps:\/\/envgen-llm.github.io\nAbstract\nRecent state-of-the-art approaches for embodied learning via interaction\ndirectly employ large language models (LLMs) as agents to determine the\nnext steps in an environment. Due to their world knowledge and reason-\ning capabilities, LLM agents achieve stronger performance than previous\nsmaller agents based on reinforcement learning (RL); however, frequently\ncalling LLMs is slow and expensive. This begs an interesting question: In-\nstead of directly employing LLMs as embodied agents, can we use LLMs’ reasoning\ncapabilities to adaptively create training environments to help smaller embodied\nRL agents learn useful skills that they are weak at? In this work, we propose\nEnvGen, a novel framework to address this question. First, we prompt an\nLLM to generate training environments that allow agents to quickly learn\ndifferent tasks in parallel. Concretely, the LLM is given the task description\nand environment simulator objectives that the agents should learn and is\nthen asked to generate a set of environment configurations (e.g., different\nterrains, items initially given to agents, chances of finding certain objects,\netc.). Next, we train a small RL agent in a mixture of the original and LLM-\ngenerated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent\nis weak at, by providing feedback to the LLM in the form of the agent’s per-\nformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist game environments. We find that a small\nRL agent trained with EnvGen can outperform SOTA methods, including\na GPT-4 agent, and learns long-horizon tasks significantly faster. We also\nshow that using an LLM to adapt environments dynamically outperforms\ncurriculum learning approaches and how the LLM adapts training environ-\nments to help improve RL agents’ weaker skills over time. Additionally,\nEnvGen is substantially more efficient as it only uses a small number of\nLLM calls (e.g., 4 in total), whereas LLM agents require one or more LLM\ncalls per step (resulting in thousands of LLM calls per episode). We also\npresent detailed analyses of EnvGen’s design choices.\n1\nIntroduction\nThere has been growing interest in embodied AI, where agents learn through interactions\nwith environments instead of static datasets (Ahn et al., 2022; Duan et al., 2022; Wang et al.,\n2023a; Yao et al., 2023; Driess et al., 2023). Open-world games such as Minecraft (Mojang\nStudios, 2009) and Crafter (Hafner, 2022) have been widely used as research environments\nfor embodied agents, where the agents visually perceive their surroundings, traverse large\nterrains, and learn to unlock various achievements (e.g., collecting resources, building tools,\ndefeating monsters, etc.). Some achievements can be easily unlocked within a few steps,\nwhereas others are more challenging as they only become accessible after the agent com-\npletes a series of prerequisite achievements, requiring hundreds of steps (i.e., long-horizon\ntasks). As illustrated in Fig. 1 (a), traditional embodied agents are based on reinforcement\n∗equal contribution\n1\narXiv:2403.12014v2  [cs.CL]  12 Jul 2024\nPublished as a conference paper at COLM 2024\n(a) RL agents: Explore skills with rewards\n(b) LLM agents: Explore skills with LLM knowledge\n(c) EnvGen (Ours): Bootstrap skill exploration with LLM-generated environments\nLLM\nAgent\nInteract\nInteract\nEnvironment\nSmall RL\nAgent\nEnvironment\n Custom Environments\n…\nStep 1: Generate \ntraining environments\nLLM\nOriginal\nEnvironment\nSmall RL\nAgent\nStep 2: Train small agent in\ngenerated environments\nStep 3: Train and measure \nagent performance in the original environment\nStep 4: Share agent \nperformance with LLM\nFeedback to \nupdate \nenvironments\nLoop NCycle times\nFigure 1: Comparison of different methods for creating embodied agents. Previous works\ncommonly use (a) small RL agents or (b) LLM agents to explore skills. In (c) EnvGen, we\ntrain a small RL agent with diverse LLM-generated environments that train different skills\nin parallel and can be adapted via feedback to help the agents progressively improve skills\nthat they are weaker at. Our method benefits from using the world knowledge from LLMs\nwhile maintaining efficient training through a lightweight RL agent.\nlearning (RL) (Hafner et al., 2020; 2021; 2023; Schulman et al., 2017; Burda et al., 2018; Hessel\net al., 2018; Sekar et al., 2020; Moon et al., 2023). However, these RL agents usually struggle\nwhen learning such long-horizon tasks since the reward is sparsely given only after the\ncorrect execution of successive actions, and it is very expensive to automatically find many\naction sequences which lead to the reward (Aytar et al., 2018; Li et al., 2022a; Yuan et al.,\n2023), even after long pretraining with curiosity-driven intrinsic reward (Walker et al., 2023).\nAs large language models (LLMs) have shown remarkable progress in various tasks that\nrequire complex reasoning (Brown et al., 2020; OpenAI, 2023a; Touvron et al., 2023a;b;\nChowdhery et al., 2023; Anil et al., 2023), recent works study implementing embodied\nagents based on LLMs. As illustrated in Fig. 1 (b), these methods leverage LLMs’ world\nknowledge with chain-of-thought reasoning (Nye et al., 2021; Kojima et al., 2022; Wei et al.,\n2022) by creating action plans, giving feedback, and obtaining rewards throughout the\nepisode (Yuan et al., 2023; Wang et al., 2023c; Wu et al., 2023; Wang et al., 2023a;d; Zhao\net al., 2023; Du et al., 2023). While these LLM-based agents that verbalize their knowledge in\nreasoning steps have seen success in achieving better performance over previous approaches,\niteratively calling LLMs throughout the episode is prohibitively slow and expensive (e.g.,\nSPRING (Wu et al., 2023) calls GPT-4 (OpenAI, 2023a) 9 times to take any action step, which\nresults in $270 USD to complete an episode). Du et al. (2023) use LLMs to create rewards to\ntrain smaller agents, but the training is still costly, as it requires many interactions between\nthe LLMs and student agents. This begs the question: Instead of directly employing LLMs as\nembodied agents, can we use LLMs’ reasoning capability to adaptively create training environments\nto help smaller embodied RL agents learn useful skills that they are weak at?\nTo address this question, we propose EnvGen, a novel framework where an LLM adaptively\ngenerates training environments to teach smaller embodied RL agents. We aim to generate\nenvironments that can create various conditions (e.g., have different terrains or some sub-\ngoals are already achieved) so that agents can learn different skills in parallel and obtain\nmore frequent rewards for challenging long-horizon tasks than in the original environment.\nAs shown in Fig. 1 (c), EnvGen iterates over multiple training cycles, each consisting of the\nfollowing four steps:\n• Step 1: We generate configurations for custom training environments (i.e., specifically\ncreated to train an RL agent on certain skills) by providing an LLM with a prompt\nincluding task description, controllable simulator settings, and simulator constraints (see\nFig. 2 and Sec. 2 for details). Then we use the generated configurations to create different\ncustom environments (e.g., different terrains, items initially given to agents, and chance\nof finding certain objects) that can teach multiple skills in parallel.\n• Step 2: We first train the RL agent in multiple LLM-generated environments (i.e., LLM\nenvironments), so that it can learn different useful skills in parallel.\n• Step 3: We then train the RL agent in the original environment to mitigate overfitting to\nthe LLM environments. Afterwards, we measure the current RL agent’s performance in\ndifferent tasks in the original environment to check which skills\/tasks the agent is still\nweak at.\n2\nPublished as a conference paper at COLM 2024\n• Step 4: We provide the RL agent’s successes\/failures in different tasks (from step 3) as\nfeedback to the LLM, so that the LLM can adapt the custom training environments to\nfocus on progressively improving the skills that the agent is weak at.\nNote that EnvGen only requires a few LLM calls (e.g., 4) for environment genera-\ntion\/updating during the entire RL agent training process, whereas other works based on\nLLM agents query an LLM once or multiple times every step (resulting in thousands of\nLLM calls for a single episode).\nWe study the usefulness of EnvGen in different game environments: Crafter (Hafner, 2022)\nand Heist (Cobbe et al., 2020). In the Crafter environment, a simple PPO-based (Schulman\net al., 2017) lightweight (< 5M parameters) RL agent trained with our LLM-generated\nenvironments outperforms strong baselines including a GPT-4 based agent that queries\nan LLM multiple times at every step, and RL agents that use extensive pretraining (e.g.,\n150M steps vs. less than 1M steps for us). When compared to just training longer in\nthe original Crafter environment and curriculum learning approaches such as easy-to-\nhard and adversarial environments, an RL agent trained with EnvGen achieves significant\nimprovements on the overall score and long-horizon tasks. In Heist, we also show that our\nLLM-generated environments can improve overall agent performance and training stability.\nWe also show a qualitative study on how the LLM adapts training environments to help\nimprove RL agents’ weaker skills over time. Finally, we provide comprehensive analysis\nand ablation studies of the design choices of EnvGen, including dynamically updating\nLLM environments (i.e., using adaptive environments) vs. curriculum learning methods,\nfrequency of environment updates, EnvGen vs. longer training in the original environment,\ndifferent LLMs for generating environments, the number of LLM-generated environments,\nand the mixture ratio between the original and LLM environment during training.\n2\nEnvGen: Generating and Adapting Environments via LLMs for\nTraining Embodied Agents\nWe propose EnvGen, a novel framework where an LLM adaptively generates training\nenvironments to train smaller embodied RL agents, enabling them to accomplish various\ntasks within an environment, particularly long-horizon tasks. During the training process,\nthe LLM is given feedback (in the form of the agent’s performance) and can adaptively\nupdate the training environments to progressively focus on improving the tasks that the\nagent is weak at. In the following, we first explain why it is challenging to explore long-\nhorizon tasks in open-world games (Sec. 2.1). Then we explain our method details, including\nhow we generate environments and how agents are trained in EnvGen (Sec. 2.2).\n2.1\nPreliminary: Exploration is Hard for Long-Horizon Tasks\nIn the RL framework, agents explore various states along a trajectory and amplify policies\nbased on the rewards received from those trajectories. However, exploration for long-\nhorizon tasks is slow and computationally expensive, as rewards for such tasks are sparsely\ngiven only after a sequence of successful actions that often involve achieving multiple\nsubgoals. For example, the goal in Crafter (Hafner, 2022) is to unlock 22 achievements,\nwhere some achievements can be unlocked quickly through several simple actions and\nothers require long chains of prerequisites (e.g., collect iron requires make stone pickaxe, which\nmust be preceded by collect stone, ... etc.); see Sec. 3.1 for details. As shown in Hafner (2022),\nexisting agents in Crafter spend most exploration steps learning low-level achievements but\nfail to unlock high-order achievements with many prerequisites.\n2.2\nEnvGen Method Details\nWe introduce EnvGen, where we train an embodied RL agent in multiple LLM-generated\nenvironments (we refer to these as ‘LLM environments’ in the paper) that progressively\nadapt to improve agent performance in multiple skills. The generated environments can\nprovide various conditions (e.g., different terrains, or some subgoals are already achieved)\n3\nPublished as a conference paper at COLM 2024\nCustom Env 1\nCustom Env 2\nCustom Env N\n…\nLLM\nEnv 1: Basic Resource \nCollection and Crafting\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"grassland\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 3,\n        \"stone\": 0,\n        \"wood_pickaxe\": 1\n    }\n}\n```\nPurpose: \nThe agent can \nlearn to collect \nwood and craft \nitems like a wood \nsword or a table ...\n# Game and task description\nYou are an environment designer \nagent for a game called \"Crafter\". Your \njob is to design a few environments \nwhich can be used to teach an agent \nhow to play...\n# Game Objectives\nHere is a list of things an agent would \nneed to learn how to do:\n●collect_coal, collect_diamond ...\n# Controllable Simulator Settings\nHere is a list of parameters you can \ncontrol when making an environment:\n●target_biome: grassland | mountain \n| beaches | natural \n●...\nHere is a list of items the agent can \nstart with:\n●wood_pickaxe: 0-1 \n●…\n(Continued)\n# Simulator Constraints\nHere is a list of constraints:\n●natural biome will set the \nenvironment to have all the \nbiomes\n●coal, iron, and diamond can only \nbe found in a mountain biome ...\n# Output Template\nOutput in the following format:\nEnvironment 1:\n```json\n{ \n   \"environment_settings\": {...},\n   \"inventory_settings\": {...} \n}\n```\nPurpose:  ...\nEnvironment 2: \n...\nEnv 2: Advanced Resource \nCollection and Combat Training\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"mountain\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 5,\n        \"stone\": 3,\n        \"wood_pickaxe\": 1,\n        \"stone_pickaxe\": 1,\n        \"wood_sword\": 1\n    }\n}\n```\nPurpose: ...\nEnv 3: Survival and Crafting \nMastery\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"natural\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"sapling\": 2,\n        \"wood\": 0,\n        \"stone\": 0,\n        \"coal\": 0,\n        \"iron\": 0,\n        \"diamond\": 0\n    }\n}\n```\nPurpose: ...\nSmall RL\nAgent\nOriginal\nEnvironment\nThose environments resulted in the agent improving up to these scores: \n●collect_coal: 38% +\/- 6%\n●defeat_skeleton: 10% +\/- 4%\n●make_stone_pickaxe: 31% +\/- 3%\n●…\nCould you generate new environments based on these scores?\nStep 1: Generate\ntraining environments\nStep 2: Train small agent \nin generated environments\nStep 3: \nTrain and measure agent \nperformance in the original environment\nStep 4: \nShare agent \nperformance with LLM\nFeedback to update environments \nLoop NCycle times\nFigure 2: In EnvGen, we generate and adapt multiple training environments with an LLM\nto let the agent learn different skills effectively. EnvGen iterates over NCycle cycles, each\nconsisting of four steps (see Sec. 2.2).\nso that agents can learn different skills in parallel and obtain more frequent rewards for long-\nhorizon tasks. As shown in Fig. 2, EnvGen iterates NCycle training cycles, each consisting of\nthe following four steps:\nStep 1: Generate training environments with an LLM. As illustrated in step 1 of Fig. 2,\nwe use an LLM (e.g., GPT-4 (OpenAI, 2023a)) to first generate NLLM-Env custom training\nenvironment configurations1 that can cover various objectives and skills that are required in\nthe original environment. The following describes the LLM input prompt components used\nto create environment configurations.\n1. Task description: We provide a brief description of the environment and what the LLM\nshould do (e.g., “generate a set of training environments...”).\n2. Game\/simulator details: We provide a list of objectives that need to be achieved in the\nenvironment (e.g., “collect coal, collect iron, etc.” for Crafter); a list of which simulator\nsettings can be controlled (e.g., terrain, agent inventory); and a list of constraints\/rules\nthat the simulator has (e.g., “skeletons only spawn in mountains; ...” for Crafter).\n3. Output environment configuration template: We provide a blank output configuration\ntemplate (i.e., a JSON object where the environment settings are empty) to the LLM, and\nrequest it to fill in the values, creating NLLM-Env environment configurations. Along\nwith filling the templates, we also ask the LLM to verbally explain the purpose for each\nenvironment (e.g., what the environment would teach the agent); this would help users\neasily understand the environment generation process.\n4. Adaptation feedback based on the RL agent’s performance: We provide the LLM with\nthe performance of the RL agent from the original environment (measured in step 3 and\nsummarized in step 4), as feedback for adapting LLM environments to focus on skills\nthat the RL agent is weak at. The feedback is given at the end of each cycle, so it is only\nprovided to LLM from the second cycle onwards.\nThe obtained environment configurations are then rendered in the game’s simulator. Fig. 2\npresents the summary of input prompt and output environments from the GPT-4 model.\nWe provide more prompt details in Appendix F.\n1We find that N=4 works well; see Table 6 for details.\n4\nPublished as a conference paper at COLM 2024\n(a) Agent View\n(b) Achievement Hierarchy\nCollect Sapling\nDefeat Zombie\nDefeat\nSkeleton\nEat Cow\nEat Plant\nPlace Plant\nCollect Wood\nPlace Table\nMake Wood \nSword\nMake Wood \nPickaxe\nMake Stone \nSword\nPlace Stone\nMake Stone \nPickaxe\nPlace Furnace\nCollect Iron\nMake Iron\nPickaxe\nMake Iron\nSword\nCollect Coal\nCollect Stone\nCollect\nDiamond\nCollect Drink\nWake Up\nFigure 3: (a): Crafter gameplay screenshot. An agent explores a 2D world and completes 22\nachievements. (b): Crafter achievement hierarchy. Some achievements can be completed\nright away; others require previous achievements to be unlocked first (i.e., in a hierarchical\norder following the arrows).\nStep 2: Train a small RL agent in the LLM-generated environments. As shown in step 2\nof Fig. 2, we train the small RL agent in the LLM-generated environments. Concretely, we\ntrain the agent in the NLLM-Env LLM environments for TLLM-Env total steps in parallel.\nStep 3: Train and measure the RL agent’s performance in the original environment. It\nis important to note that the goal of EnvGen is to improve the RL agent’s performance\nin the original environment, instead of the performance only in the LLM environments.\nTo help the RL agent effectively adapt to the original environment and provide the LLM\nwith the current agent’s performance as feedback, we train the agent and measure its\nperformance in the original environment, as shown in step 3 of Fig. 2. First, to mitigate\nthe overfitting to LLM environments, we train the agent in the original environment for\nTOrig-Env steps.2 Next, to find the skills that the RL agent needs to improve at, we test the\nagent in the original environment, without any parameter updates. Concretely, we measure\nindividual success rates for each environment task (e.g., Crafter achievements). The agent\nperformance is summarized (in step 4) and is provided to LLM as feedback (in step 1) to\nadapt training environments in the next cycle. Moreover, importantly, to obtain a more\ncalibrated estimation of agent performance, we calculate the average and variance of the\ntask-specific scores by testing agents with multiple random seeds (i.e., 12).\nStep 4: Send feedback to LLM to adapt environments (to focus on weak skills). We\nprovide the LLM with the agent’s performance from the original environment (measured in\nstep 3), as feedback for updating LLM environments. Concretely, we list the agent’s average\ntask-specific success rate in percentages along with one standard deviation (e.g., “. . . collect\ncoal: 38% ± 6%, defeat skeleton: 10% ± 4% . . . ”), as shown in step 4 of Fig. 2. In step 1 of\nthe next cycle, the LLM can adaptively generate new environments (by using the agent’s\nperformance as feedback) to better help the RL agent learn the skills it is weak at (e.g., defeat\nskeleton). EnvGen iterates this four-step training cycle NCycle times.\n3\nExperimental Setup\nIn the following subsections, we present the benchmarks in which we evaluate EnvGen\nframework on (Sec. 3.1) and the agent architectures that we use for experiments (Sec. 3.2).\n3.1\nEvaluated Benchmarks and Training Details\nCrafter. Crafter (Hafner, 2022) is an open-world 2D survival game focused on evaluating\na broad range of agent capabilities (see Fig. 3). Crafter features 22 achievements that an\nagent can unlock during an episode of play. Some achievements can be unlocked in a\nfew steps (e.g., collect wood, collect sapling, etc.), but other achievements, such as make iron\npickaxe or collect diamond, require many training\/exploration steps and several prerequisite\n2We find that TLLM-Env = TOrig-Env works well; see Table 7 for details.\n5\nPublished as a conference paper at COLM 2024\nachievements to be unlocked (see Fig. 3 b). For example, to make an iron pickaxe, an agent\nmust first collect enough wood to make a table and a wooden pickaxe, then go collect stone\nand return to the table (or collect more wood to make a new one) and then construct a stone\npickaxe. Then the agent still needs to make a furnace, collect coal, and collect iron before\nthe option to make the iron pickaxe is possible.\nFor EnvGen setup, we use NCycle = 4 training cycles during agent training (see Table 3 for\nablation of having a different number of cycles). Each cycle uses 0.12M LLM-generated\nenvironment steps (i.e., CrafterEnvGen steps, see step 2 in Fig. 2) and 0.12M Crafter steps\n(step 3 in Fig. 2) and then we train for 1M steps in Crafter. In total, we train for 1.96M\nsteps ((0.12M + 0.12M) × 4 + 1M). Note that in order to maintain a fair score comparison to\nbaselines, we do not count any achievement during our training cycle for score calculation\nsince the training scores derived from LLM environments and the original environment\nare not directly comparable. Instead, we only take into account the achievements from the\nlast 1M training steps in Crafter for the score calculation. We also experiment with giving\nthe baseline model additional original environment steps to match the number of EnvGen\nsteps (i.e., an additional 0.96M steps) to ensure that EnvGen is not better simply because\nof more steps. The score for Crafter is computed as the geometric mean of individual\nsuccess rates of each achievement for each episode it is completed within 1M training steps:\nS = exp( 1\n22 ∑22\ni=1 ln(1 + si)) −1, where si is the average success rate of the ith achievement\nacross all episodes that occurred during training. We report the average performance with\n30 runs (= 3 different initial LLM-generated CrafterEnvGen environments × 10 different\nrandom seeds).\nHeist. Heist is part of the OpenAI Procgen (Cobbe et al., 2020) benchmark. In this environ-\nment, agents must successfully ‘steal’ the gem after navigating a maze and opening all locks.\nSee more details in Appendix C.2.\n3.2\nAgent Architectures\nOur base RL agent. For both Crafter and Heist, we test the EnvGen framework with a\nsimple (CNN + linear layer) and lightweight (<5M) agent used in Moon et al. (2023), which\nis slightly modified from the agent architecture used in IMPALA (Espeholt et al., 2018).\nFollowing Moon et al. (2023), we train the agent with a PPO (Schulman et al., 2017) objective.\nAt every step, the agent takes an RGB image (surroundings for Crafter, entire maze for\nHeist) as input and outputs the value estimates and policy (action probability). See Fig. 3 (a)\nfor an agent visual input example. We provide additional model details in Appendix E.\nBaseline methods. For Crafter, we compare our method to two groups of recent baselines –\n(1) methods that use frequent (i.e., more than thousands of) LLM calls during training or\ninference: SPRING (Wu et al., 2023) (based on GPT-4) and ELLM (Du et al., 2023) (based on\nCodex (Chen et al., 2021)) and (2) methods that do not use an LLM: DreamerV3 (Hafner et al.,\n2023), MuZero + SPR (Walker et al., 2023), LSTM-SPCNN (Stani´c et al., 2023), PPO (Schulman\net al., 2017), and Achievement Distillation (AD) (Moon et al., 2023). For Heist, we compare\nagainst the PPO agent. For the PPO and AD agents, we follow the implementation of Moon\net al. (2023). See Appendix E for the PPO\/AD agent details.\n4\nResults and Analysis\nWe demonstrate the usefulness of the EnvGen method with comprehensive experiments\nand analysis. We first compare RL agents trained with EnvGen to different baseline methods\non Crafter, an open-world game with 22 hierarchical achievements (Sec. 4.1). Next, we\npresent a detailed analysis of the improvements that training with EnvGen environments\ncan give RL agents on long-horizon tasks (Sec. 4.2). Then, we analyze how the LLM-based\nenvironment adaptation can help an RL agent progressively improve the skills that the agent\nis weak at (Sec. 4.3). Lastly, we present various additional analysis including experiments\non Heist (a maze navigation game) and ablation studies on EnvGen design choices (Sec. 4.4\nand also in the Appendix C).\n6\nPublished as a conference paper at COLM 2024\nModels\nDescription\n# LLM calls\n# Agent Params\nScore (%)\nReward\nHuman∗\n50.5 ± 6.8\n14.3 ± 2.3\nRandom∗\n1.6 ± 0.0\n2.1 ± 1.3\nELLM* (Du et al., 2023)\n5M step PT in Crafter w\/ Codex reward\n5M\n62M\n-\n6.0 ± 0.4\nLSTM-SPCNN∗(Stani´c et al., 2023)\n135M\n11.7 ± 0.8\n9.3 ± 0.2\nDreamerV3∗(Hafner et al., 2023)\n201M\n14.8 ± 1.4\n10.9 ± 0.5\nMuZero + SPR∗(Walker et al., 2023)\n150M step PT in Crafter w\/ RND reward\n54M\n16.4 ± 1.5\n12.7 ± 0.4\nSPRING* (Wu et al., 2023)\n9 queries to call GPT-4 per step\n2.7K†\nUnknown\n27.3 ± 1.2\n12.3 ± 0.7\nPPO (Moon et al., 2023)\n4M\n15.5 ± 0.6\n10.5 ± 0.6\nPPO (Moon et al., 2023)\n0.96M step PT in Crafter\n4M\n26.4 ± 2.1\n12.1 ± 1.0\nAD* (Moon et al., 2023)\n9M\n21.8 ± 1.4\n12.6 ± 0.3\nAD (Moon et al., 2023)\n0.96M step PT in Crafter\n9M\n31.8 ± 0.7\n13.3 ± 1.2\nPPO + EnvGen (Ours)\n0.96M step PT w\/ CrafterEnvGen\n4\n4M\n32.2 ± 0.6\n12.6 ± 0.6\nAD + EnvGen (Ours)\n0.96M step PT w\/ CrafterEnvGen\n4\n9M\n35.3 ± 0.7\n13.7 ± 0.8\nTable 1: Comparison of different agents in the Crafter (Hafner, 2022) environment. Following\nprevious works, we report the geometric mean of success rates across its 22 achievements\nand rewards for 1M Crafter steps. We experiment with EnvGen on two models, PPO\nand Achievement Distillation. *: scores from the Crafter Scoreboard (Hafner, 2022) and\nMoon et al. (2023). †: average number of LLM calls to run a single episode, according to\nSPRING (Wu et al., 2023). PT: Pretraining; AD: Achievement Distillation.\n4.1\nComparison with State-of-the-art Methods on Crafter Environment\nSmall RL agent trained with EnvGen outperforms state-of-the-art baselines. On the\nCrafter environment (described in Sec. 3.1), we compare a small PPO agent trained in\nCrafterEnvGen (i.e., Crafter environments generated with EnvGen) to state-of-the-art baseline\nmethods. As shown in Table 1, we find that a small (4M parameters) PPO agent with\nEnvGen achieves an average score of 32.2% and significantly outperforms the baselines\n(and also in terms of the average reward). Note that some baseline agents have many more\nparameters or pretraining steps such as SPRING (GPT-4 agent; 27.3%), and MuZero + SPR\n(150M pretraining steps; 16.4%). Our method also only uses orders of magnitude fewer LLM\ncalls (only 4) than works like SPRING (2.7K on average) and ELLM (5M), making it much\ncheaper\/more efficient. EnvGen can also work with other RL agents such as Achievement\nDistillation (AD) (Moon et al., 2023) to achieve an even higher score (35.3%).\n4.2\nDetailed Achievement Analysis on Crafter Environment\nNext, we analyze where EnvGen improves the overall score by checking individual achieve-\nment success rates in detail. For this, we compare the same PPO agent architecture trained\nwith different setups: (1) an agent trained on Crafter for 1.96M steps and (2) an agent trained\non CrafterEnvGen for 0.96M steps (0.24M steps × 4 training cycles, see Sec. 2.2) and then\ntrained on Crafter for 1M steps. We measure the success rate (Fig. 4) of each achievement\nand unlocking speed (Fig. 5) of iron tools in the last 1M training steps.\nEnvGen helps RL agents to tackle challenging long-horizon achievements. Fig. 4 shows\nthat training in CrafterEnvGen improves scores of several achievements. Notably, training in\nCrafterEnvGen significantly improves the scores of long-horizon achievements (with many\nprerequisites; see Fig. 3) such as stone and iron tools. Fig. 5 shows that after unlocking\nthe stone pickaxe, the RL agent trained in CrafterEnvGen is significantly faster in unlocking\niron tools. In Appendix C.1, we also compare two AD agents, and show that CrafterEnvGen\nimproves the success rate of the most challenging achievement – ‘collect diamond’.\n4.3\nAdaptation of Training Environments Helps the Agent Improve Weaker Skills\nFig. 6 shows how the LLM adaptively generates new training environments based on the\nintermediate performance of our PPO-based RL agent. In the intermediate performance\nplots, we compare the baseline agent trained only in Crafter and our RL agent trained in\nCrafterEnvGen. In the cycle 2, given the feedback that the current RL agent is not good at\ncollecting coal, the LLM generates an environment to help the agent focus on it, improving\n7\nPublished as a conference paper at COLM 2024\nCollect Coal\nCollect Diamond\nCollect Drink\nCollect Iron\nCollect Sapling\nCollect Stone\nCollect Wood\nDefeat Skeleton\nDefeat Zombie\nEat Cow\nEat Plant\nMake Iron Pickaxe\nMake Iron Sword\nMake Stone Pickaxe\nMake Stone Sword\nMake Wood Pickaxe\nMake Wood Sword\nPlace Furnace\nPlace Plant\nPlace Stone\nPlace Table\nWake Up\n0.001\n0.01\n0.1\n1\n10\n100\n1.96M Crafter Steps\n0.96M steps in Crafter EnvGen + 1M Crafter Steps (Ours)\nSuccess Rate (%)\nFigure 4: Success rates for all the Crafter achievements of two PPO agents (Moon et al.,\n2023) – (1) Baseline: trained in Crafter for 1.96M steps, and (2) Ours: trained in 0.96M steps\nin CrafterEnvGen and 1M in Crafter.\nMake Iron \nSword\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nSword\nFigure 5: Unlock times (the first moment when the agent completed an achievement) for\nthree long-horizon achievements (‘make stone pickaxe’, ‘make iron pickaxe’, and ‘make iron\nsword’) of two PPO agents (Moon et al., 2023) – (1) Baseline: trained in Crafter for 1.96M\nsteps, and (2) Ours: trained for 0.96M steps in CrafterEnvGen and for 1M steps in Crafter. The\nplot shows the last 1M training steps out of 1.96M steps.\nthe agent’s performance for the skill. Likewise, in the cycle 3, given the feedback that the\nagent is weak at making stone pickaxes, the LLM generates an environment to help the\nagent more easily craft the stone pickaxe, helping the agent improve it’s score for the skill.\nPowered by the adaptive LLM environment generation of EnvGen, our agent learns to\nunlock these two achievements significantly faster than the baseline agent.\n4.4\nAdditional Analysis and Ablation Studies\nIn the following, we show comprehensive design analysis and ablation studies of EnvGen\nmethod: dynamically updating LLM environments (i.e., using adaptive environments)\nvs. curriculum learning methods, and different frequencies of environment updates. In\nAppendix C, we show comprehensive analysis and ablation studies of EnvGen method:\nEnvGen vs. longer training in the original environment, different LLMs for generating\nenvironments, the number of LLM environments, and the ratio of training steps in the LLM\nvs. original environments. We also include experiments on the Heist environment (see\nSec. 3.1) in Appendix C.2.\nDifferent environment curricula: fixed, easy-to-hard, adversarial vs. adaptive. Table 2\nshows that using LLM environments that are adaptively updated based on intermediate\nagent performance to improve weaker skills (last row) results in overall higher scoring\nagents than just using the initial LLM environments for the whole training (32.2% vs. 29.9%).\n8\nPublished as a conference paper at COLM 2024\nCollect\nCoal\nMake Stone \nPickaxe\nCycle 2\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 1: Introduction to \nMining and Crafting\n```json\n{\n \"target_biome\": \"mountain\",        \n \"wood_pickaxe\": 1,\n ...\n}\n```\nPurpose: This environment is \ndesigned to teach the agent \nhow to mine coal ...\n...\nFeedback from \nprevious cycle\nCollect coal is 2%\nMake stone pickaxe is 1% \n…\nLLM\nCycle 3\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 2: Introduce \ncombat and advanced tool \ncrafting\n```json\n{\n \"wood\": 3,\n \"stone\": 2,\n ...\n}\n```\nPurpose: With stone in the \ninventory, the agent is nudged \nto craft a stone pickaxe, a \ntask it hasn't learned yet ...\n...\nFeedback from \nprevious cycle\nCollect coal is 22%\nMake stone pickaxe is 1%\n…\nPerformance \nfrom Cycle 1\nPerformance \nfrom Cycle 3\nPerformance \nfrom Cycle 2\nTrained in \nCrafter\nTrained in \nCrafterEnvGen (Ours)\nLLM\nCollect\nCoal\nMake Stone \nPickaxe\nCollect\nCoal\nMake Stone \nPickaxe\nFigure 6: Adaptation of training environments based on agent performance over EnvGen\ncycles. At the end of each cycle, the RL agent’s performance is given to the LLM as\nfeedback (e.g., ‘Collect coal is 2%’). The LLM uses the feedback to adaptively generate new\nenvironments that can help the agent progressively tackle skills it was previously weak at.\nTraining Curriculum\nScore (%)\nReward\nFixed (no curriculum)\n29.9 ± 0.9\n12.6 ± 0.8\nEasy-to-Hard\n26.8 ± 1.5\n12.7 ± 0.7\nAdversarial\n26.8 ± 0.8\n12.2 ± 0.7\nAdaptive+Dynamic Environments (EnvGen)\n32.2 ± 0.6\n12.6 ± 0.6\nTable 2: Comparison of RL agents trained in Crafter (Hafner, 2022) using no curriculum, an\neasy-to-hard curriculum, an adversarial curriculum, and our adaptive+dynamic environ-\nments. Agents are trained for 0.96M steps using the curriculum and then 1M in the default\nCrafter environment.\nThese results indicate the effectiveness of the agent feedback and environment updating\n(step 4 described in Sec. 2).\nTable 2 also compares an agent trained via EnvGen to the same agent trained with curriculum\nlearning approaches such as an easy-to-hard curriculum, similar to Ammanabrolu et al.\n(2022) (i.e., pre-defined training environment order based on environment difficulty) and\nadversarial curriculum, similar to Parker-Holder et al. (2022) (i.e., updating to training\nenvironments that agent does worse in) in the Crafter environment. Detailed setups of both\nbaseline approaches are in the appendix. The agent trained with EnvGen is able to achieve\nmuch higher performance (32.2% vs. 26.8% for both curricula) indicating the effectiveness\nEnvGen’s approach of adaptively generating training environments to improve agent weak\nskills. The result indicates that creating more difficult environments does not necessarily\nhelp the agent learn new skills over time.\nEnvironment Update Frequency\n# Training cycles NCycle\nScore (%)\nReward\nEvery 0.012M steps\n40 cycles\n30.8 ± 0.7\n12.8 ± 0.6\nEvery 0.06M steps\n8 cycles\n32.1 ± 0.5\n12.7 ± 0.8\nEvery 0.12M steps (default)\n4 cycles\n32.2 ± 0.6\n12.6 ± 0.6\nTable 3: Different frequencies to give feedback to the LLM and update the environments\n(see Sec. 2 for details). Agents are trained with 0.96M steps in CrafterEnvGen and 1M steps in\nCrafter environment.\n9\nPublished as a conference paper at COLM 2024\nFrequency of LLM feedback \/ environment updates.\nTable 3 shows that updating the\nLLM environments at every 0.12M steps results in the best agent performance. While\nincreasing the cycles of environment feedback beyond 4 does not improve further, we\nfind that updating environments with feedback always helps improve the RL agent’s\nperformance compared to training only with the original Crafter environment in Table 1\n(26.4%) or the fixed LLM environment in Table 2 (29.9%).\n5\nRelated Works\nLLMs as open-world game agents. Recent works study using LLMs to create action plans\n(i.e., a list of subgoals or skills to target) for embodied agents in open-world games like\nMinecraft and Crafter (Hafner, 2022). Most of these methods require calling LLMs frequently\n(e.g., at every step) for planning the next steps (Yuan et al., 2023; Wang et al., 2023c; Wu\net al., 2023; Wang et al., 2023a;d; Zhao et al., 2023). Other methods, such as Li et al. (2024);\nKwon et al. (2023); Ma et al. (2023); Du et al. (2023), have used LLMs to create\/adjust\nrewards to train agents. Although these works show initial promising results leveraging the\nworld knowledge of LLMs to tackle long-horizon tasks, iteratively calling LLMs throughout\nepisodes is prohibitively slow and expensive (e.g., running a single episode in the Crafter\nenvironment with SPRING (Wu et al., 2023) costs around $270 USD as they have 2.7K LLM\ncalls on average). EnvGen only calls LLMs a few times (e.g., 4 in total) to create training\nenvironments that focus on helping the RL agent progressively improve its weaker skills.\nDeep learning-based game\/simulator content generation. Procedural content generation\n(PCG) for games is about the automatic generation of levels, landscapes, items, rules, quests,\nor other types of game contents (Shaker et al., 2016). While traditional PCG methods are\nbased on search\/solver\/rule\/grammar-based methods, recent works apply deep learn-\ning methods such as GAN (Goodfellow et al., 2014) for PCG (Liu et al., 2021; Kumaran\net al., 2020; Schubert et al., 2022). Several works have explored using LLMs to generate\ngame content such as difficulty levels (Sudhakaran et al., 2023; Todd et al., 2023) and\nscenes\/environments (Kumaran et al., 2023; Wang et al., 2023b; Afshar & Li, 2024). While\nthese works aim to help developers create new game content, we aim to improve RL agent\nperformance in the original environment. A line of work proposes unsupervised envi-\nronment design (UED) that manipulates the difficulty level of environments to be more\nchallenging to RL agents (Dennis et al., 2020; Jiang et al., 2021; Parker-Holder et al., 2022).\nWhile these works use a learned environment manipulator or evolutionary algorithms to\nmaximize the ‘regret’ (the difference between the expected return of the current and optimal\npolicies) in simple games such as MiniGrid (Chevalier-Boisvert et al., 2023), we use the\nworld knowledge of LLMs to generate and adapt training environments that can improve\nweaker skills based on comprehensive skill-specific feedback from RL agents in open-world\ngames with many challenging long-horizon tasks. To help agents generalize to unseen tasks\nin a text-based dialogue game, Ammanabrolu et al. (2022) augment new tasks with LMs\nand use a manually designed, fixed curriculum. Unlike this work, we adaptively generate\ntraining environments using LLMs’ world knowledge and automatically learning a dynamic\ncurriculum based on the RL agent’s feedback, so as to improve the agent’s weaker skills\nin open-world games with visual inputs. Beyond game content generation, several works\nvisually augment vision-and-language navigation (VLN) simulators (e.g., rendering the\nenvironments with different styles) using image generation models (Li et al., 2022b; Wang\net al., 2023e; Li & Bansal, 2023). Such works could complement our LLM environments (e.g.,\naugmenting our environments with diverse colors and textures).\n6\nConclusion\nWe propose EnvGen, a novel framework to improve embodied RL agent performance by\nutilizing the world knowledge of LLMs to adaptively generate training environments. In\nEnvGen, we give an LLM a prompt describing a game\/simulator and ask the LLM to\ngenerate the configurations to create new environments that can teach different skills. Next,\nwe train an agent in the LLM-generated environments, give feedback to the LLM by testing\nthe agent in the original environments, and then ask the LLM to update the environments to\n10\nPublished as a conference paper at COLM 2024\nteach agents skills they are weaker at. In two challenging games, Crafter and Heist, we show\nthat EnvGen increases agent performance significantly, and training with LLM-generated\nenvironments is more effective than training longer in the original environments. We also\nshow that using an LLM to adapt environments dynamically outperforms curriculum\nlearning approaches and how the LLM adapts training environments to help improve RL\nagents’ weaker skills over time. Moreover, a lightweight model (< 5M parameters) trained\nwith LLM-generated environments even outperforms an LLM agent with significantly fewer\nLLM calls. We hope our work can guide future works in leveraging LLMs for embodied\nagents.\nAcknowledgments\nWe thank Elias Stengel-Eskin and the reviewers for the thoughtful discussion and feedback.\nThis work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage\nInstitute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031,\nARO Award W911NF2110220, ONR Grant N00014-23-1-2356, and a Bloomberg Data Science\nPh.D. Fellowship. The views contained in this article are those of the authors and not of the\nfunding agency.\nReferences\nAida Afshar and Wenchao Li. Delf: Designing learning environments with foundation\nmodels. In AAAI Workshop, 2024.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,\nDaniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui\nRuano, Kyle Jeffrey, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov,\nYuheng Kuang, Kuang Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada,\nPeter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre\nSermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia,\nTed Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do As I Can, Not As I\nSay: Grounding Language in Robotic Affordances. In CoRL, 2022.\nPrithviraj Ammanabrolu, Renee Jia, and Mark O Riedl. Situated dialogue learning through\nprocedural environment generation. In Association for Computational Linguistics (ACL),\n2022. URL https:\/\/arxiv.org\/abs\/2110.03262.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexan-\ndre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu,\nJonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav\nMishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan\nXiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob\nAustin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks,\nMichele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl´ement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin,\nMark D´ıaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand,\nHadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,\nMichael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim\nKrikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,\nWei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick\nLiu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam\nMoussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie\nPellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter,\nParker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby,\nAmbrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang,\nJohn Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui\n11\nPublished as a conference paper at COLM 2024\nYu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and\nYonghui Wu. Palm 2 technical report, 2023.\nYusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, and Nando de Freitas.\nPlaying hard exploration games by watching YouTube. In NeurIPS, 2018. URL http:\n\/\/arxiv.org\/abs\/1805.11592.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. In NIPS\n2016 Deep Learning Symposium, 2016. URL http:\/\/arxiv.org\/abs\/1607.06450.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and\nR´emi Munos. Unifying count-based exploration and intrinsic motivation. In NIPS, 2016.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are\nFew-Shot Learners. In NeurIPS, 2020. URL http:\/\/arxiv.org\/abs\/2005.14165.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by Random\nNetwork Distillation. In ICLR, 2018.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,\n2021.\nMaxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems,\nSalem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld:\nModular & customizable reinforcement learning environments for goal-oriented tasks.\nCoRR, abs\/2306.13831, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling with\nPathways. JMLR, pp. 1–83, 2023. URL http:\/\/arxiv.org\/abs\/2204.02311.\nKarl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.\nLeveraging procedural\ngeneration to benchmark reinforcement learning. In Hal Daum´e III and Aarti Singh\n(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119\nof Proceedings of Machine Learning Research, pp. 2048–2056. PMLR, 13–18 Jul 2020. URL\nhttps:\/\/proceedings.mlr.press\/v119\/cobbe20a.html.\n12\nPublished as a conference paper at COLM 2024\nMichael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew\nCritch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised\nenvironment design. In NIPS, 2020.\nDI-star Contributors. Di-star: An open-sourse reinforcement learning framework for star-\ncraftii. https:\/\/github.com\/opendilab\/DI-star, 2021.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian\nIchter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang,\nYevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke,\nKarol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete\nFlorence. PaLM-E: An Embodied Multimodal Language Model. In ICML 2023, 2023. URL\nhttp:\/\/arxiv.org\/abs\/2303.03378.\nYuqing Du, Olivia Watkins, Zihan Wang, C´edric Colas, Trevor Darrell, Pieter Abbeel,\nAbhishek Gupta, and Jacob Andreas. Guiding Pretraining in Reinforcement Learning\nwith Large Language Models. In ICML, 2023.\nJiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied\nai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational\nIntelligence, 6(2):230–244, 2022. doi: 10.1109\/TETCI.2022.3141105.\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom\nWard, Boron Yotam, Firoiu Vlad, Harley Tim, Iain Dunning, Shane Legg, and Koray\nKavukcuoglu. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-\nLearner Architectures. In ICML, 2018. ISBN 9781510867963.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,\nAndrew Tang, De-An Huang, Yuke Zhu, Anima Anandkumar, and Ut Austin. MineDojo:\nBuilding Open-Ended Embodied Agents with Internet-Scale Knowledge. In NeurIPS, jun\n2022. doi: 10.48550\/arxiv.2206.08853. URL https:\/\/arxiv.org\/abs\/2206.08853v2.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. In NIPS,\n2014. ISBN 1406.2661. URL http:\/\/arxiv.org\/abs\/1406.2661.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,\nXiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder:\nWhen the large language model meets programming – the rise of code intelligence, 2024.\nURL https:\/\/arxiv.org\/abs\/2401.14196.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel,\nManuela Veloso, and Ruslan Salakhutdinov. MineRL: A large-scale dataset of minecraft\ndemonstrations. In IJCAI, 2019. ISBN 9780999241141. doi: 10.24963\/ijcai.2019\/339.\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. In ICLR, 2022. URL\nhttps:\/\/github.com\/danijar\/crafter.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to Control:\nLearning Behaviors by Latent Imagination. In ICLR, 2020.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering Atari\nwith Discrete World Models. In ICLR, 2021.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering Diverse\nDomains through World Models, 2023. URL http:\/\/arxiv.org\/abs\/2301.04104.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for\nImage Recognition. In CVPR, 2016.\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dab-\nney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining\nimprovements in deep reinforcement learning. In AAAI, 2018. ISBN 9781577358008. doi:\n10.1609\/aaai.v32i1.11796.\n13\nPublished as a conference paper at COLM 2024\nMinqi Jiang, Jakob Foerster, Michael Dennis, Edward Grefenstette, Jack Parker-Holder, and\nTim Rockt¨aschel. Replay-Guided Adversarial Environment Design. In NeurIPS, 2021.\nISBN 9781713845393.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge Language Models are Zero-Shot Reasoners. In NeurIPS, 2022. URL http:\/\/arxiv.\norg\/abs\/2205.11916.\nVikram Kumaran, Bradford W. Mott, and James C. Lester. Generating game levels for\nmultiple distinct games with a common latent space. In AIIDE, pp. 109–115, 2020. ISBN\n9781577358497. doi: 10.1609\/aiide.v16i1.7485.\nVikram Kumaran, Jonathan Rowe, Bradford Mott, and James Lester. SCENECRAFT: Au-\ntomating Interactive Narrative Scene Generation in Digital Games with Large Language\nModels. In AIIDE, pp. 86–96, 2023. ISBN 157735883X. doi: 10.1609\/aiide.v19i1.27504.\nMinae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with\nlanguage models. In International Conference on Learning Representations, 2023.\nAndrew C Li, Pashootan Vaezipoor, Rodrigo Toro Icarte, and Sheila A. McIlraith. Exploring\nlong-horizon reasoning with deep RL in combinatorially hard tasks. In Decision Awareness\nin Reinforcement Learning Workshop at ICML 2022, 2022a. URL https:\/\/openreview.net\/\nforum?id=7vPSZASOF0o.\nHao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hong-\nsheng Li, Lewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design\nwith large language models for minecraft. In IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, 2024.\nJialu Li and Mohit Bansal. Panogen: Text-conditioned panoramic environment generation\nfor vision-and-language navigation. Advances in Neural Information Processing Systems,\n2023.\nJialu Li, Hao Tan, and Mohit Bansal. Envedit: Environment editing for vision-and-language\nnavigation. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, 2022b.\nJialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi, Georgios N. Yannakakis, and\nJulian Togelius. Deep learning for procedural content generation. Neural Comput. Appl.,\n33(1):19–37, jan 2021. ISSN 0941-0643. doi: 10.1007\/s00521-020-05383-8. URL https:\n\/\/doi.org\/10.1007\/s00521-020-05383-8.\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh\nJayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward\ndesign via coding large language models. ArXiv, abs\/2310.12931, 2023.\nMojang Studios. Minecraft, 2009. URL https:\/\/www.minecraft.net\/.\nSeungyong Moon, Junyoung Yeom, Bumsoo Park, and Hyun Oh Song. Discovering Hierar-\nchical Achievements in Reinforcement Learning via Contrastive Learning. In NeurIPS,\n2023. URL http:\/\/arxiv.org\/abs\/2307.03486.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sut-\nton, and Augustus Odena. Show Your Work: Scratchpads for Intermediate Computation\nwith Language Models, 2021. URL http:\/\/arxiv.org\/abs\/2112.00114.\nOpenAI. Gpt-4 technical report. ArXiv, 2023a. URL https:\/\/api.semanticscholar.org\/\nCorpusID:257532815.\nOpenAI. Chatgpt. https:\/\/openai.com\/chatgpt, 2023b.\n14\nPublished as a conference paper at COLM 2024\nJack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster,\nEdward Grefenstette, and Tim Rockt¨aschel. Evolving Curricula with Regret-Based Envi-\nronment Design. In ICML, 2022.\nFrederik Schubert, Maren Awiszus, and Bodo Rosenhahn. TOAD-GAN: A Flexible Frame-\nwork for Few-Shot Level Generation in Token-Based Games. IEEE Transactions on Games,\n14(2):284–293, 2022. ISSN 24751510. doi: 10.1109\/TG.2021.3069833.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\nPolicy Optimization Algorithms, 2017.\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak\nPathak. Planning to explore via self-supervisedworld models. In ICML, 2020. ISBN\n9781713821120.\nNoor Shaker, Julian Togelius, and Mark J. Nelson. Procedural Content Generation in Games.\nSpringer Publishing Company, Incorporated, 1st edition, 2016. ISBN 3319427148.\nAleksandar Stani´c, Yujin Tang, David Ha, and J¨urgen Schmidhuber. Learning to generalize\nwith object-centric agents in the open world survival game crafter. IEEE Transactions on\nGames, 2023.\nShyam Sudhakaran, Miguel Gonz´alez-Duque, Claire Glanois, Matthias Freiberger, Elias\nNajarro, and Sebastian Risi. MarioGPT: Open-Ended Text2Level Generation through\nLarge Language Models. In NeurIPS, 2023. URL http:\/\/arxiv.org\/abs\/2302.05981.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT\nPress, 2 edition, 2018.\nGraham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, and Julian\nTogelius. Level Generation Through Large Language Models. In FDG, 2023. ISBN\n9781450398565. doi: 10.1145\/3582437.3587211.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\nmodels, 2023b.\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich K¨uttler, John Agapiou, Julian Schrittwieser,\nJohn Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van\nHasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso,\nDavid Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft ii: A new\nchallenge for reinforcement learning, 2017. URL https:\/\/arxiv.org\/abs\/1708.04782.\nJacob Walker, Eszter V´ertes, Yazhe Li, Gabriel Dulac-Arnold, Ankesh Anand, Th´eophane\nWeber, and Jessica B. Hamrick.\nInvestigating the Role of Model-Based Learning in\nExploration and Transfer. In ICML, 2023.\n15\nPublished as a conference paper at COLM 2024\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi\nFan, and Anima Anandkumar. Voyager: An Open-Ended Embodied Agent with Large\nLanguage Models, 2023a. URL http:\/\/arxiv.org\/abs\/2305.16291.\nRuoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, Marc-Alexandre Cˆot´e, and Peter\nJansen. ByteSized32: A corpus and challenge task for generating task-specific world\nmodels expressed as text games. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\npp. 13455–13471, Singapore, December 2023b. Association for Computational Linguis-\ntics. doi: 10.18653\/v1\/2023.emnlp-main.830. URL https:\/\/aclanthology.org\/2023.\nemnlp-main.830.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,\nExplain, Plan and Select: Interactive Planning with Large Language Models Enables Open-\nWorld Multi-Task Agents. In NeurIPS, 2023c. URL http:\/\/arxiv.org\/abs\/2302.01560.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,\nZhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. JARVIS-1:\nOpen-World Multi-task Agents with Memory-Augmented Multimodal Language Models,\n2023d. URL http:\/\/arxiv.org\/abs\/2311.05997.\nZun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan,\nand Yu Qiao. Scaling data generation in vision-and-language navigation. In ICCV, 2023e.\nChristopher J.C.H. Watkins. Learning from Delayed Rewards. PhD thesis, University of\nCambridge, England, May 1989.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large\nLanguage Models. In NeurIPS, pp. 1–43, 2022. URL http:\/\/arxiv.org\/abs\/2201.11903.\nLilian Weng. Exploration strategies in deep reinforcement learning. lilianweng.github.io, Jun\n2020. URL https:\/\/lilianweng.github.io\/posts\/2020-06-07-exploration-drl\/.\nYue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos\nAzaria, Tom Mitchell, and Yuanzhi Li. SPRING: Studying the Paper and Reasoning to\nPlay Games. In NeurIPS, 2023. URL http:\/\/arxiv.org\/abs\/2305.15486.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\nCao. ReAct: Synergizing Reasoning and Acting in Language Models. In ICLR, 2023. URL\nhttp:\/\/arxiv.org\/abs\/2210.03629.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and\nZongqing Lu. Skill Reinforcement Learning and Planning for Open-World Long-Horizon\nTasks. In Foundation Models for Decision Making Workshop at NeurIPS, 2023.\nZhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian\nYe, Jenq-Neng Hwang, and Gaoang Wang. See and Think: Embodied Agent in Virtual\nEnvironment, 2023. URL http:\/\/arxiv.org\/abs\/2311.15209.\nAppendix\nIn this appendix, we present additional related work (Appendix A), additional game en-\nvironment details (Appendix B), additional experiment results (Appendix C), curriculum\nlearning baseline method details (Appendix D), RL agent implementation details (Ap-\npendix E), additional LLM details (Appendix F), and limitations (Appendix G).\n16\nPublished as a conference paper at COLM 2024\nA\nAdditional Related Works\nReward designs in reinforcement learning. Finding good action trajectories is critical\nin reinforcement learning (RL) (Sutton & Barto, 2018). While classic random exploration\nalgorithms such as epsilon-greedy (Watkins, 1989) work well in simple settings such as\nmulti-armed bandit, it is not the case for hard exploration problems where the environment\ngives very sparse rewards (Weng, 2020). A line of work studies how to augment the\noriginal (extrinsic) rewards from the environment with intrinsic rewards that encourage\nexploration (Bellemare et al., 2016; Burda et al., 2018). While such intrinsic rewards can help\nRL agents discover novel states and improve their knowledge about the environment, it\noften requires long pretraining and does not guarantee that the intrinsic reward can help the\ntarget task. Another recent line of work studies using LLMs to adjust reward functions to\nhelp RL agents progressively learn certain tasks (Li et al., 2024; Kwon et al., 2023; Ma et al.,\n2023; Du et al., 2023). Instead of designing new rewards, in EnvGen, an LLM adaptively\ngenerates training environments that can help the RL agent learn multiple skills it is weak\nat with fewer training steps than in the original environment; reward design could be\ncomplementary to our method.\nB\nAdditional Game Environment Details\nHeist Environment.\nHeist is part of the OpenAI Procgen (Cobbe et al., 2020) benchmark.\nIn this environment, agents must successfully ‘steal’ the gem after navigating a maze and\nopening all locks (see Fig. 7). The gem is behind three layers of color-coded locks, each\nrequiring that the previous lock be unlocked first (e.g., to unlock the green lock, the blue\nlock must first be unlocked). Following Moon et al. (2023), the final score is calculated as the\naverage success of the agent in stealing the gem in 100 test episodes in 10 different seeds\n(i.e., 1,000 runs in total). For agent training, we use a total of 5M steps in the LLM-generated\nenvironments (i.e., 5M HeistEnvGen steps) and a total of 20M in the actual Heist environment.\nAs the game only provides scores on the final objective (’steal gem‘) and the game is simple\nenough for the LLM-generated environments to cover all scenarios with one generation, we\nonly use NCycle = 1 training cycle.\n(a) Agent View\n(b) Achievement Hierarchy\nOpen Blue Lock\nOpen Green Lock\nOpen Red Lock\nSteal Gem\nFigure 7: (a): Heist gameplay screenshot. An agent aims to steal a gem (colored yellow),\nnavigating a maze and colored opening locks. (b): Heist achievement hierarchy. The agent\ncan only reach the gem after successively unlocking all locks in order.\nC\nAdditional Experiment Results\nC.1\nDesign Choices, Ablations, and Other Agent Architectures\nIn the following, we show comprehensive design choice and ablation studies of EnvGen\nmethod: EnvGen vs. longer training in the original environment, different LLMs for\ngenerating environments, the number of LLM environments, and the ratio of training steps\nin the LLM environments to the original environment. Unless otherwise noted, we use the\nPPO-based agent (Moon et al., 2023) (described in Sec. 3.2) on the Crafter (Hafner, 2022)\nbenchmark (described in Sec. 3.1) with 0.96M steps in CrafterEnvGen and average results for\n30 runs (10 different seeds, 3 different initial environments).\n17\nPublished as a conference paper at COLM 2024\n# Training Steps in CrafterEnvGen\n# Training Steps in Crafter\nScore (%)\nReward\n(Total 1.24M steps)\n-\n1.24M\n21.1 ± 2.3\n11.0 ± 0.9\n0.12M\n1.12M\n22.3 ± 1.5\n11.6 ± 0.8\n(Total 1.48M steps)\n-\n1.48M\n21.9 ± 2.1\n11.4 ± 0.9\n0.24M\n1.24M\n27.9 ± 1.2\n12.4 ± 0.7\n(Total 1.96M steps)\n-\n1.96M\n26.4 ± 2.1\n12.1 ± 1.0\n0.48M\n1.48M\n32.2 ± 0.6\n12.6 ± 0.6\nTable 4: RL agents trained in CrafterEnvGen environments vs. agents trained only in the\nCrafter environment. We calculate the scores based on the last 1M training steps in Crafter.\nLLM\nScore (%)\nReward\nDeepseek Coder 33B Instruct\n26.3 ± 0.9\n12.1 ± 0.8\nGPT-3.5-Turbo\n21.5 ± 2.8\n11.6 ± 1.0\nGPT-4-Turbo (default)\n29.9 ± 0.9\n12.6 ± 0.8\nTable 5: Ablation of employing different LLMs to generate the environments. Agents are\ntrained with 0.96M steps in CrafterEnvGen and 1M steps in the Crafter environment.\nEnvGen vs. longer training in the original environment.\nTable 4 shows that when given\nan equivalent # of total training steps, the agents trained with CrafterEnvGen environments\noutperform the agents only trained with Crafter (e.g., 22.3% vs. 21.1% for 1.24M total steps).\nAlthough the agent performances tend to improve with longer training steps in both settings,\ntraining with EnvGen shows stronger performance gains than only training longer in Crafter\n(e.g., 32.2% vs. 26.4% for 1.96M total steps).\nDifferent LLMs to generate environments.\nTo figure out which LLM can generate more\nuseful training environments, we experiment with three different LLMs (GPT-4-Turbo,\nGPT-3.5-Turbo (OpenAI, 2023b), and Deepseek Coder 33B Instruct (Guo et al., 2024)) and\nuse NCycle = 1 (i.e., fixed environment). Table 5 shows that environments generated by\nGPT-4-Turbo outperform that of other LLMs including GPT-3.5-Turbo and Deepseek Coder\n33B Instruct. We see that GPT-3.5-Turbo performs the worst with only a score of 21.5%,\nwhile Deepseek 33B Instruct is able to get several points higher (26.3%) and GPT-4-Turbo,\nour default LLM, gets a few extra points (29.9%).\n# LLM environments\nScore (%)\nReward\n1\n30.8 ± 0.5\n12.8 ± 0.8\n2\n29.1 ± 0.6\n13.0 ± 0.6\n4 (default)\n32.2 ± 0.6\n12.6 ± 0.6\n8\n31.0 ± 0.8\n12.9 ± 0.8\nTable 6: Different number of LLM environments being generated by the LLM per cycle.\nAgents are trained with 0.96M steps in CrafterEnvGen and 1M steps in the real Crafter\nenvironment.\nNumber of LLM environments.\nTable 6 shows that changing the number of environments\ngenerated by the LLM at each cycle (i.e., 1, 2, 4, and 8) can slightly affect agent performance.\nWhile training with four environments produces the highest result, training with environ-\nments generated with any of the tested configurations improves performance over training\nonly with the original Crafter environment (26.4%).\n18\nPublished as a conference paper at COLM 2024\nRatio of Training Steps in CrafterEnvGen : Crafter\nScore (%)\nReward\n5:1\n30.3 ± 0.6\n12.3 ± 0.9\n2:1\n30.1 ± 1.1\n12.8 ± 0.7\n1:1 (default)\n32.2 ± 0.6\n12.6 ± 0.6\nTable 7: Different ratios of training steps in LLM-generated environments (CrafterEnvGen)\ncompared to training steps in the original Crafter environment (e.g., 2:1 indicates that for\nevery two training steps in CrafterEnvGen, the RL agent gets one training step in Crafter).\nWe keep the total number of training steps constant at 1.96M.\nModel\n# Training Steps in HeistEnvGen\n# Training Steps in Heist\nScore (%)\nReward\nPPO\n-\n25M\n25.9 ± 13.2\n4.1 ± 1.8\nPPO + EnvGen (Ours)\n5M\n20M\n37.7 ± 7.50\n5.5 ± 0.9\nTable 8: Evaluation results on Heist. Scores are computed as the average success rate over\n100 test episodes over 10 different seeds.\nRatio of training steps: LLM environments vs. original environment.\nAs mentioned\nin Sec. 2.2, in EnvGen, we train the RL agent in LLM environments (step 2) and then\nin the original environment (step 3) to mitigate the agent from overfitting to the LLM\nenvironments. We experiment with different ratios of training steps in LLM environments\n(i.e., CrafterEnvGen) compared to training steps in the original Crafter environment (e.g., 2:1\nindicates that for every two training steps in CrafterEnvGen, the RL agent gets one training\nstep in Crafter). As shown in Table 7, while different ratios do not result in big differences,\nthe default 1:1 ratio provides the highest scores.\nCan simulators always understand LLM-generated environment configurations?\nWe\ntested and analyzed the generated environments used in paper experiments (109 total)\nand found that the LLM (GPT-4-Turbo) did not generate any environments beyond what\nthe Crafter or Heist simulators could handle. While we do not find any such case, even\nif the LLM generates an invalid setup, we constrain all LLM-generated settings in post-\nprocessing to be within simulator capabilities to ensure no accidental errors in the simulator\nor environment generation.\nEnvironment parameter naming: obscure vs. original.\nTo determine whether or not the\nLLM in EnvGen is leveraging world knowledge when generating environments we conduct\nan analysis experiment. We replace environment parameter names from the original ones\nwith obscure names (see Fig. 9), in order to remove the use of prior knowledge\/expectation\nof how each parameter is useful to which skills. We find that the performance decreases,\nfrom 32.2 ± 0.6 →to 28.5 ± 0.6, indicating that the world knowledge\/prior knowledge of\nthe LLM is beneficial in helping the LLM generate more suitable environments.\nAchievement Distillation + EnvGen.\nAs mentioned in the Sec. 4.1, we also experiment\nusing EnvGen with the Achievement Distillation (AD) (Moon et al., 2023) agent. As shown in\nFig. 8, similar to our results on the PPO-based agent, we find that by applying EnvGen, there\nis performance gain in long-horizon tasks like making iron tools and collecting diamonds.\nC.2\nEvaluation on Heist Environment\nEnvGen can generalize to Heist.\nWe also evaluate the effectiveness of EnvGen frame-\nwork with Heist. We compare the PPO-based agent trained with and without EnvGen\n(i.e., HeistEnvGen environments). Table 8 shows that training an agent with HeistEnvGen\nenvironments is effective in improving performance by increasing average scores (25.9%\n→37.7%) and rewards (4.1% →5.5%), while also stabilizing training by reducing the score\nvariance (i.e., standard deviation goes down 13.2% →7.5%).\n19\nPublished as a conference paper at COLM 2024\nCollect Coal\nCollect Diamond\nCollect Drink\nCollect Iron\nCollect Sapling\nCollect Stone\nCollect Wood\nDefeat Skeleton\nDefeat Zombie\nEat Cow\nEat Plant\nMake Iron Pickaxe\nMake Iron Sword\nMake Stone Pickaxe\nMake Stone Sword\nMake Wood Pickaxe\nMake Wood Sword\nPlace Furnace\nPlace Plant\nPlace Stone\nPlace Table\nWake Up\n0.001\n0.01\n0.1\n1\n10\n100\n1.96M Crafter Steps\n0.96M steps in Crafter EnvGen + 1M Crafter Steps (Ours)\nSuccess Rate (%)\nFigure 8: Success rates for all Crafter achievements of two Achievement Distillation (AD)\nagents (Moon et al., 2023): (1) Baseline: trained in Crafter for 1.96M steps, and (2) Ours:\ntrained in CrafterEnvGen for 0.96M steps and Crafter for 1M steps.\nD\nCurriculum Learning Baseline Details\nIn the following, we describe the two baseline implementation details described in Table 2:\neasy-to-hard and adversarial curricula.\nEasy-to-hard curriculum.\nSimilar to Ammanabrolu et al. (2022), we create an easy-to-hard\ncurriculum. An easy-to-hard curriculum has a pre-defined order of training environments.\nThe agent first trains in “easy” environments and then by the end of the training process will\nbe training in the “hard” environments. To do this, we first ask the LLM to generate a set\nof 16 random environments and train a validation agent (an agent only for the purpose of\ntesting an environment difficulty; not used during final agent training) on each environment.\nThen the performance of the validation agent is measured and the environments are sorted\nfrom easiest to hardest (i.e., from environments that resulted in higher agent scores to lower\nagent scores). Then we train an agent on the top four easier environments first and for every\n0.24M steps we replace the training environments with the next four environments in the\nsorted set (i.e., easy-to-hard curriculum).\nAdversarial curriculum.\nSimilar to Parker-Holder et al. (2022), we create an adversarial\ncurriculum. The adversarial curriculum approach involves updating the agent’s training\nenvironments to ones where it has struggled. To do this, we first generate a set of 16 random\nenvironments and train a validation agent on each environment. Then, we measure and sort\nthe environments by difficulty (i.e., sorted from lowest to highest based on the validation\nagent’s score). We take the top four hardest environments and train the final agent on these.\nThen, generate a new set of environments and test the current agent on this set, again sorting\nby difficulty. Then again, we take the top four hardest environments and resume training\non these. This process then repeats four times (every 0.24M steps) creating an adversarial\ncurriculum.\nE\nRL Agent Implementation Details\nPPO agent.\nWe use the PPO-based (Schulman et al., 2017) agent used in (Moon et al., 2023),\nwhich modifies the default ResNet (He et al., 2016) architecture in IMPALA (Espeholt et al.,\n2018) by increasing channel size and hidden size and adding a layer normalization Ba et al.\n(2016) before each linear\/convolutional layer. We slightly modify this architecture further\nto place the layer norm after the final linear layer instead of before. Hyperparameters for\nthis model are shown in Table 9.\n20\nPublished as a conference paper at COLM 2024\nBefore:\nHere is a list of parameters you can control when making an environment:\ntarget biome: grassland — mountain — beaches — natural\ncoal rarity: very common — common — rare\niron rarity: very common — common — rare\ndiamond rarity: very common — common — rare\ntree rarity: very common — common — rare\nHere is a list of items the agent can start with:\nsapling: 0-9\nwood: 0-9\nstone: 0-9\ncoal: 0-9\niron: 0-9\ndiamond: 0-9\nwood pickaxe: 0-1\nstone pickaxe: 0-1\niron pickaxe: 0-1\nwood sword: 0-1\nstone sword: 0-1\niron sword: 0-1\nAfter:\nHere is a list of parameters you can control when making an environment:\nparameter1: optionA — optionB — optionC — optionD\nparameter2: optionE — optionF — optionG\nparameter3: optionE — optionF — optionG\nparameter4: optionE — optionF — optionG\nparameter5: optionE — optionF — optionG\nHere is a list of items the agent can start with:\nitem1: 0-9\nitem2: 0-9\nitem3: 0-9\nitem4: 0-9\nitem5: 0-9\nitem6: 0-9\nitem7: 0-1\nitem8: 0-1\nitem9: 0-1\nitem10: 0-1\nitem11: 0-1\nitem12: 0-1\nFigure 9: LLM prompt template for environment generation before and after parameter and\nitem names are replaced with obscure names.\nAchievement distillation (AD) agent.\nMoon et al. (2023) builds upon its PPO-based agent\nmodel and adds auxiliary training steps after the PPO policy updates. Their auxiliary\ntraining consists of two parts: (1) intra-trajectory achievement prediction and (2) cross-\ntrajectory achievement matching. (1) Intra-trajectory achievement prediction maximizes\nthe similarity between state-action pairs and the corresponding next achievement that\nneeds to be unlocked in the achievement hierarchy within an episode. (2) Cross-trajectory\nachievement matching maximizes the similarity between achievements across episodes.\nHyperparameters for this model are shown in Table 10.\nF\nAdditional LLM Details\nPrompt Template.\nIn Fig. 10 (a), we show the LLM prompt template that is used to\ngenerate environments. The contents of the prompt can vary slightly between different\nenvironments\/games though generally remain the same. In Fig. 10 (b), we show the\n21\nPublished as a conference paper at COLM 2024\nHyperparameter\nValue\nDiscount factor\n0.95\nGAE smoothing parameter\n0.65\n# timesteps per rollout\n4096\n# epochs per rollout\n3\n# mini-batches per epoch\n8\nEntropy bonus\n0.01\nPPO clip range\n0.2\nReward normalization\nNo\nEWMA decay rate\n0.99\nLearning rate\n3e-4\nMax grad norm\n0.5\nValue function coefficient\n0.5\nTable 9: PPO agent hyperparameters. Hyperparameters are following Moon et al. (2023).\nHyperparameter\nValue\nPolicy regularizer coefficient\n1.0\nValue regularizer coefficient\n1.0\nEntropic regularizer coefficient\n0.05\n# policy phases per auxiliary phase\n8\n# epochs per auxiliary phase\n6\nTable 10: Achievement Distillation hyperparameters. Hyperparameters are following Moon\net al. (2023).\nadditional prompt template that is used during the feedback step (step 4 in Sec. 2.2). At\neach feedback cycle iteration, the additional prompt is concatenated to the previous LLM\noutput (i.e., maintaining a chat history).\nAPI Cost.\nAs we use GPT-4-Turbo (1106-preview version) the API cost is $10.00 per 1M\ntokens and $30.00 per 1M tokens. The initial environment generation cost is $0.03 and then\neach iteration of the feedback cycle adds $0.04. Once the model is trained via EnvGen it no\nlonger requires any LLM calls for inference or further training on the original environment.\nWorks like SPRING (Wu et al., 2023) require $270 USD and several thousand LLM calls per\nepisode, which is much more expensive than our work.\nG\nLimitations\nEnvGen relies on strong LLMs (e.g., GPT-4). But note that one of the main motivations of\nEnvGen is to more efficiently use LLMs to help train embodied agents, and as such EnvGen\nrequires very few LLM calls (e.g., 4 calls), which only costs less than $1 USD during the\nentire training. We hope that advances in quantization\/distillation and open-source models\nwill make strong LLMs more accessible.\nEnvGen also requires that the environment simulators can (or be easily edited to) accept\nconfigurations in standard formats (e.g., JSON, CSV, YAML, TOML etc.), and the LLM\ncan correctly generate configurations in such formats. Note that such text configuration\nformats are widely used for managing game simulators. In addition, many games have\nopen-source community-driven efforts that provide high-level configuration documentation\nand settings, such as Minecraft wrappers (Guss et al., 2019; Fan et al., 2022) and Starcraft\nwrappers (Vinyals et al., 2017; DI-star Contributors, 2021).\n22\nPublished as a conference paper at COLM 2024\n# Game and task descriptions: \nYou are a environment designer agent for a game called [game].\nYour job is to design four environments which can be used to teach an agent how to play.\n…\n# Game Objectives: \nHere is a list of things an agent would need to learn how to do:\n[list of game objectives]\n# Controllable Simulator Settings: \nHere is a list of parameters you can control when making an environment:\n[list of environment parameters the LLM can control]\n# Simulator Constraints: \nHere is a list of constraints:\n[list of environment constraints]\n# Output Template: \nOutput in the following format and include a brief explanation of the purpose of each environment:\n[output template]\n(a) Initial environment generation prompt\n...\nThose environments resulted in the agent improving up to these scores:\n[agent performance on each skill]\nCould you generate new environments based on these scores?\n(b) Feedback cycle prompt\nFigure 10: The prompts that are given to the LLM to generate environments in step 1 of\nSec. 2.2.\n23\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents.pdf"}
{"title":"Multi-Stage Episodic Control for Strategic Exploration in Text Games","authors":"Jens Tuyls, Shunyu Yao, Sham Kakade, Karthik Narasimhan","summary":"Text adventure games present unique challenges to reinforcement learning\nmethods due to their combinatorially large action spaces and sparse rewards.\nThe interplay of these two factors is particularly demanding because large\naction spaces require extensive exploration, while sparse rewards provide\nlimited feedback. This work proposes to tackle the explore-vs-exploit dilemma\nusing a multi-stage approach that explicitly disentangles these two strategies\nwithin each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins\neach episode using an exploitation policy that imitates a set of promising\ntrajectories from the past, and then switches over to an exploration policy\naimed at discovering novel actions that lead to unseen state spaces. This\npolicy decomposition allows us to combine global decisions about which parts of\nthe game space to return to with curiosity-based local exploration in that\nspace, motivated by how a human may approach these games. Our method\nsignificantly outperforms prior approaches by 27% and 11% average normalized\nscore over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in\nboth deterministic and stochastic settings, respectively. On the game of Zork1,\nin particular, XTX obtains a score of 103, more than a 2x improvement over\nprior methods, and pushes past several known bottlenecks in the game that have\nplagued previous state-of-the-art methods.","url":"http:\/\/arxiv.org\/abs\/2201.01251v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2201.01251v3","published":1641316792000,"comment":"ICLR 2022 (Spotlight) - https:\/\/sites.google.com\/princeton.edu\/xtx","pdf_text":"Published as a conference paper at ICLR 2022\nMULTI-STAGE EPISODIC CONTROL FOR STRATEGIC\nEXPLORATION IN TEXT GAMES\nJens Tuyls1, Shunyu Yao1, Sham Kakade2 & Karthik Narasimhan1\n1Department of Computer Science, Princeton University\n2John A. Paulson School of Engineering and Applied Sciences, Harvard University\n{jtuyls, shunyuy, karthikn}@princeton.edu, sham@seas.harvard.edu\nABSTRACT\nText adventure games present unique challenges to reinforcement learning meth-\nods due to their combinatorially large action spaces and sparse rewards. The inter-\nplay of these two factors is particularly demanding because large action spaces re-\nquire extensive exploration, while sparse rewards provide limited feedback. This\nwork proposes to tackle the explore-vs-exploit dilemma using a multi-stage ap-\nproach that explicitly disentangles these two strategies within each episode. Our\nalgorithm, called eXploit-Then-eXplore (XTX), begins each episode using an ex-\nploitation policy that imitates a set of promising trajectories from the past, and then\nswitches over to an exploration policy aimed at discovering novel actions that lead\nto unseen state spaces. This policy decomposition allows us to combine global de-\ncisions about which parts of the game space to return to with curiosity-based local\nexploration in that space, motivated by how a human may approach these games.\nOur method signiﬁcantly outperforms prior approaches by 27% and 11% average\nnormalized score over 12 games from the Jericho benchmark (Hausknecht et al.,\n2020) in both deterministic and stochastic settings, respectively. On the game of\nZork1, in particular, XTX obtains a score of 103, more than a 2x improvement\nover prior methods, and pushes past several known bottlenecks in the game that\nhave plagued previous state-of-the-art methods.1\n1\nINTRODUCTION\nText adventure games provide a unique test-bed for algorithms that integrate reinforcement learning\n(RL) with natural language understanding. Aside from the linguistic ingredient, a key challenge\nin these games is the combination of very large action spaces with sparse rewards, which calls for\na delicate balance between exploration and exploitation. For instance, the game of ZORK1 can\ncontain up to ﬁfty valid action commands per state2 to choose from. Importantly, unlike other RL\nenvironments (Bellemare et al., 2013; Todorov et al., 2012), the set of valid action choices does not\nremain constant across states, with unseen actions frequently appearing in later states of the game.\nFor example, Figure 1 shows several states from ZORK1 where a player has to issue unique action\ncommands like ‘kill troll with sword’, ‘echo’ or ‘odysseus’ to progress further in the game. This\nrequires a game-playing agent to perform extensive exploration to determine the appropriateness\nof actions, which is hard to bootstrap from previous experience. On the other hand, since rewards\nare sparse, the agent only gets a few high-scoring trajectories to learn from, requiring vigorous\nexploitation in order to get back to the furthest point of the game and make progress thereon. Prior\napproaches to solving these games (He et al., 2016a; Hausknecht et al., 2020; Ammanabrolu &\nHausknecht, 2020; Guo et al., 2020) usually employ a single policy and action selection strategy,\nmaking it difﬁcult to strike the right balance between exploration and exploitation.\nIn this paper, we propose eXploit-Then-eXplore (XTX), an algorithm for multi-stage control to ex-\nplicitly decompose the exploitation and exploration phases within each episode. In the ﬁrst phase,\nthe agent selects actions according to an exploitation policy which is trained using self-imitation\n1Our code is available at https:\/\/github.com\/princeton-nlp\/XTX\n2Valid actions are a feature in the Jericho simulator (Hausknecht et al., 2020) to improve computational\ntractability for RL. Without this handicap, the number of possible action commands is almost 200 billion.\n1\narXiv:2201.01251v3  [cs.CL]  16 Mar 2022\nPublished as a conference paper at ICLR 2022\n…\n“East\"\n“West”\n…\n“Move rug”\n“Open window”\n…\n…\nScore: 10\nScore: 35\nCellar\nLiving Room\nWest of House\nScore: 0\nTroll Room\nScore: 35\nLoud Room\nScore: 40\nCyclops Room\nScore: 40\n“You are \nstanding in an \nopen field …”\n“It is pitch \nblack …”\n“… you have a  \nlantern and a \nsword …”\n“… a troll \nblocks all \npassages …”\n“Kill troll \nwith sword”\n“Echo”\n“Odysseus”\n“… The room is \ndeafeningly \nloud …”\n“A cyclops, \nwho looks \nprepared …”\n… …\n“Go up”\n“Down”\nFigure 1: Sample game paths and state observations from ZORK1. Starting from the leftmost\nstate (‘West of House’), the agent encounters several novel and unique valid actions (e.g Odysseus,\nEcho) (in brown) across different states in the game. In order to make progress, our algorithm\n(XTX) strategically re-visits different frontiers in the state space (red and blue circles) and performs\nstrategic local exploration to overcome bottleneck states (e.g. ‘Troll Room’) and dead-ends (e.g.\n‘Cellar’). Solid borders indicate visited states, dotted ones indicate potential future states.\nlearning on a mixture of promising trajectories from its past experience sampled using a combina-\ntion of factors such as episodic scores and path length. This policy allows the agent to return to a\nstate at the frontier of the state space it has explored so far. Importantly, we ensure that this policy\nis trained on a mixture of trajectories with different scores, in order to prevent the agent from falling\ninto a local minimum in the state space (e.g. red space in Figure 1). In the second phase, an explo-\nration policy takes over and the agent chooses actions using a value function that is trained using a\ncombination of a temporal difference (TD) loss and an auxiliary inverse dynamics loss (Pathak et al.,\n2017). This allows the agent to perform strategic exploration around the frontier by reusing values\nof previously seen actions while exploring novel ones in order to ﬁnd rewards and make progress in\nthe game. To allow for more ﬁne-grained control, we use a mixture of policies for both exploration\nand exploitation, and only change a single interpolation parameter to switch between phases.\nThe two-stage approach to gameplay in XTX allows an agent to combine global decisions about\nwhich parts of the game space to advance, followed by local exploration of sub-strategies in that\nspace. This is similar to how humans tackle these games: if a player were to lose to a troll in the\ndungeon, they would immediately head back to the dungeon after the game restarts and explore\nstrategies thereon to try and defeat the troll. XTX’s multi-stage episodic control differs from prior\napproaches that add exploration biases to a single policy through curiosity bonuses (Pathak et al.,\n2017; Tang et al., 2017) or use different reward functions to train a separate exploration policy (Colas\net al., 2018; Sch¨afer et al., 2021; Whitney et al., 2021). Moreover, in contrast to methods like Go-\nExplore (Ecoffet et al., 2021; Madotto et al., 2020), XTX does not have global phases of random\nexploration followed by learning —instead, both our policies are continuously updated with new\nexperience, allowing XTX to adapt and scale as the agent goes deeper into the game. XTX also does\nnot make any assumptions about the environment being deterministic, and does not require access\nto underlying game simulator or additional memory archives to keep track of game trees.\nWe evaluate XTX on a set of games from the Jericho benchmark (Hausknecht et al., 2020), consider-\ning both deterministic and stochastic variants of the games. XTX outperforms competitive baselines\non all 12 games, and achieves an average improvement of 5.8% in terms of normalized scores across\nall games. For instance, on Zork1, our method obtains a score of 103 in the deterministic setting and\n67 in the stochastic setting — substantial improvements over baseline scores of 44 and 41, respec-\ntively. We also perform ablation studies to demonstrate the importance of the multi-stage approach,\nas well as several key design choices in our exploitation and exploration policies.\n2\nRELATED WORK\nReinforcement learning for text-based games Prior work on building autonomous agents for text\nadventure games has explored several variants of reinforcement learning (RL) agents equipped with\na language understanding module (see Osborne et al. (2021) for a detailed survey). Innovations on\n2\nPublished as a conference paper at ICLR 2022\nthe language representation side include using deep neural networks for handling text sequences\ntrained using RL (Narasimhan et al., 2015; He et al., 2016a), knowledge graphs to track states\nacross trajectories (Ammanabrolu & Hausknecht, 2020; Adhikari et al., 2020; Xu et al., 2020), and\nincorporating question answering or reading comprehension modules (Ammanabrolu et al., 2020;\nGuo et al., 2020). While these approaches focus mainly on the issues of partial observability and\nlanguage semantics, they all suffer from challenges due to the large action space and sparse rewards\nfound in games from benchmarks like Jericho (Hausknecht et al., 2020). Some approaches aim to\nnavigate the large action space by ﬁltering inadmissible actions (Zahavy et al., 2018; Jain et al.,\n2020), leveraging pre-trained language models for action selection (Yao et al., 2020; Jang et al.,\n2020) or word embeddings for affordance detection (Fulda et al., 2017). Recent work has also\nexplored tackling sparse rewards by employing hierarchical policies (Xu et al., 2021).\nNavigating the exploration-exploitation trade-off in RL The trade-off between exploration and\nexploitation is a well-known issue in RL (Sutton & Barto, 2018; Franc¸ois-Lavet et al., 2018; Kearns\n& Singh, 2002; Brafman & Tennenholtz, 2002). In this respect, we can broadly categorize prior\ntechniques into two types. The ﬁrst type includes methods with mixed objectives that balance ex-\nploration with exploitation. Oh et al. (2018) introduced the idea of self-imitation learning on high-\nscoring episodes to exploit good trajectories, as an auxiliary objective to standard actor-critic meth-\nods. Prior work has also explored the addition of curiosity bonuses to encourage exploration (Pathak\net al., 2017; Tang et al., 2017; Li et al., 2020; Bellemare et al., 2016; Machado et al., 2020; Taiga\net al., 2021).\nWhile we leverage self-imitation learning for exploitation and inverse dynamics\nbonuses for exploration, we use a multi-stage mixed policy. Other works learn a mixture of policies\nfor decoupling exploration and exploitation, either by using a conditional architecture with shared\nweights (Badia et al., 2020), pre-deﬁning an exploration mechanism for restricted policy optimiza-\ntion (Shani et al., 2019), or learning separate task and exploration policies to maximize different\nreward functions (Colas et al., 2018; Sch¨afer et al., 2021; Whitney et al., 2021). While we also train\nmultiple policies, our multi-stage algorithm performs distinct exploitation and exploration phases\nwithin each episode, not requiring pre-deﬁned exploration policies or phases. Further, we consider\nenvironments with signiﬁcantly larger action spaces that evolve dynamically as the game progresses.\nThe second class of algorithms explicitly separate exploitation and exploration in each episode.\nMethods like E3 (Kearns & Singh, 2002; Henaff, 2019) maintain a set of dynamics models to en-\ncourage exploration. Policy-based Go-Explore (Ecoffet et al., 2021) uses self-imitation learning\nto ‘exploit’ high-reward trajectories, but requires choosing intermediate sub-goals for the agent to\ncondition its policy on. PC-PG (Agarwal et al., 2020) uses a policy cover to globally choose state\nspaces to return to, followed by random exploration. Compared to these approaches, we perform\nmore strategic local exploration due to the use of a Q-function with inverse dynamics bonus and do\nnot require any assumptions about determinism or linearity of the MDP. We provide a more technical\ndiscussion on the novelty of our approach at the end of Section 3.\nDirected exploration in text-based games As previously mentioned, the large dynamic action\nspace in text games warrant speciﬁc strategies for directed exploration. Ammanabrolu et al. (2020)\nused a knowledge-graph based intrinsic motivation reward to encourage exploration. Jang et al.\n(2020) incorporated language semantics into action selection for planning using MCTS. Both meth-\nods utilize the determinism of the game or require access to a simulator to restart the game from\nspeciﬁc states. Madotto et al. (2020) modiﬁed the Go-Explore algorithm to test generalization in the\nCoinCollector (Yuan et al., 2018) and Cooking world domains (Cˆot´e et al., 2018). Their method has\ntwo phases — the agent ﬁrst randomly explores and collects trajectories and then a policy is learned\nthrough imitation of the best trajectories in the experience replay buffer. In contrast, our algorithm\nprovides for better exploration of new, unseen actions in later stages of the game through the use\nof an inverse dynamics module and performs multiple rounds of imitation learning for continuous\nscaling to deeper trajectories in the game. Recently, Yao et al. (2021) used inverse dynamics to\nimprove exploration and Yao et al. (2020) used a language model to generate action candidates that\nguide exploration. However, both approaches did not employ a two-stage rollout like our work, and\nthe latter considers a different setup without any valid action handicap.\n3\nPublished as a conference paper at ICLR 2022\n3\nMETHOD\nBackground Text-adventure games can be formalized as a Partially Observable Markov Decision\nProcess (POMDP) ⟨S, T, A, O, R, γ⟩. The underlying state space S contains all conﬁgurations of\nthe game state within the simulator, which is unobserved by the agent. The agent receives obser-\nvations from O from which it has to infer the underlying state s ∈S. The action set A consists\nof short phrases from the game vocabulary, T(s′|s, a) is the transition function which determines\nthe probability of moving to the next state s′ given the agent has taken action a in state s, R(s, a)\ndetermines the instantaneous reward, and γ is the reward discount factor.\nExisting RL approaches that tackle these games usually learn a value function using game rewards.\nOne example is the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016b) which\ntrains a deep neural network with parameters φ to approximate Qφ(o, a). This model encodes each\nobservation o and action candidate a using two recurrent networks fo and fa and aggregates the rep-\nresentations to derive the Q-value through an MLP q: Qφ(o, a) = q(fo(o), fa(a)). The parameters\nφ of the model are trained by minimizing the temporal difference (TD) loss on tuples (o, a, r, o′) of\nobservation, action, reward and the next observation sampled from an experience replay buffer:\nLTD(φ) = (r + γ max\na′∈A Qφ(o′, a′) −Qφ(o, a))2\n(1)\nThe agent samples actions using a softmax exploration policy π(a|o; φ) ∝exp(Qφ(o, a)).\nChallenges There are two unique aspects of text adventure games that make them challenging. First,\nthe action space is combinatorially large – usually games can accept action commands of 1-4 words\nwith vocabularies of up to 2257 words. This means an agent potentially has to choose between\nO(22574) = 2.6 ∗1013 actions at each state of the game. To make this problem more tractable,\nbenchmarks like Jericho (Hausknecht et al., 2020) provide a valid action detector that ﬁlters out the\nset of inadmissible commands (i.e. commands that are either unrecognized by the game engine or\ndo not change the underlying state of the game). However, this still results in the issue of dynamic\naction spaces for the agent that change with the state. For example, the action ‘echo’ is unique to\nthe Loud Room (see Figure 1). Second, these games have very sparse rewards (see Appendix A.1)\nand several bottleneck states (Ammanabrolu et al., 2020), making learning difﬁcult for RL agents\nthat use the same policy for exploration and exploitation (He et al., 2016a; Hausknecht et al., 2020;\nAmmanabrolu & Riedl, 2019). This results in issues of derailment (Ecoffet et al., 2021), with agents\nunable to return to promising parts of the state space, resulting in a substantial gap between average\nepisode performance of the agent and the maximum score it sees in the game (Yao et al., 2020).\n3.1\nOUR ALGORITHM: EXPLOIT-THEN-EXPLORE (XTX)\nWe tackle the two challenges outlined above using a multi-stage control policy that allows an agent\nto globally pick promising states of the game to visit while allowing for strategic local exploration\nthereafter. To this end, we develop eXploit-Then-eXplore, where an agent performs the following\ntwo distinct phases of action selection in each episode.\nPHASE 1: Exploitation In the exploitation phase, the agent makes a global decision about revisiting\npromising states of the game it has seen in its past episodes. We sample k trajectories from the\nexperience replay D using a combination of factors such as game score and trajectory length. These\ntrajectories are then used to learn a policy cover πexploit using self-imitation learning (Oh et al.,\n2018) (see Section 3.2 for details). The agent then samples actions from πexploit until it has reached\neither (1) the maximum score seen during training or (2) a number of steps in the episode equal to\nthe longest of the k sampled trajectories. The second condition ensures the agent can always return\nto the longest of the k sampled trajectories3. Once either of the above conditions is satisﬁed, the\nagent transitions to the exploration phase, adjusting its policy.\nPHASE 2: Exploration In the exploration phase, the agent uses a different policy πexplore trained\nusing both a temporal difference loss and an auxiliary inverse dynamics bonus (Pathak et al., 2017;\nYao et al., 2021) (see Section 3.3 for details). The intuition here is that the exploitation policy\nπexploit in phase 1 has brought the agent to the game frontier, which is under-explored and may\n3It is possible that the agent needs less steps to return to a particular part of the game space and hence wastes\nsome steps, but we empirically didn’t ﬁnd this to be a problem.\n4\nPublished as a conference paper at ICLR 2022\ncontain a combination of common (e.g. “open door”) and novel (e.g. “kill troll with sword”) ac-\ntions. Therefore, using a combination of Q-values and inverse dynamics bonus enables the agent to\nperform strategic, local exploration to expand the frontier and discover new rewarding states. The\nagent continues sampling from πexplore until a terminal state or episode limit is reached.\n3.1.1\nMIXTURE OF POLICIES FOR FINE-GRAINED CONTROL\nWhile one could employ two completely disjoint policies for the exploitation and exploration phases,\nwe choose a more general approach of having a policy mixture, with a single parameter λ that can\nbe varied to provide more ﬁne-grained control during the two phases. Speciﬁcally, we deﬁne:\nπλ(a|c, o; θ, φ, ξ) = λπinv−dy(a|o; θ, φ) + (1 −λ)πil(a|c; ξ).\n(2)\nHere, πinv−dy refers to an exploration-inducing policy trained using TD loss with an inverse-\ndynamics bonus (Section 3.3) and is parameterized by θ and φ. πil refers to an exploitation-inducing\npolicy trained through self-imitation learning (Oh et al., 2018) (Section 3.2) and is parameterized\nby ξ. Note that the action distribution over actions a induced by πinv−dy is conditioned only on the\ncurrent observation o, while the one induced by πil is conditioned on context c which is an augmen-\ntation of o with past information. We can observe that λ provides a trade-off between exploration\n(high λ) and exploitation (low λ). In our experiments, we choose a small, dynamic value, λ =\n1\n2∗T\nfor exploitation (where T is episode limit) and λ = 1 for exploration. As we demonstrate later\n(Section 4.2), the non-zero λ in exploitation is critical for the agent to avoid getting stuck in regions\nof local minima (e.g. Zork1). We now describe the individual components of the mixture.\n3.2\nIMITATION LEARNING FOR BUILDING A GLOBAL POLICY COVER (πil)\nWe parameterize the imitation policy πil using a Transformer model (Vaswani et al., 2017) based\non the GPT-2 architecture (Radford et al., 2019) that takes in a context c = [at−2; at−1; ot], i.e.\nthe concatenation of two most recent past actions along with the current observation separated by\n[SEP] tokens, and outputs a sequence of hidden representations h0, . . . , hm where m is the number\nof tokens in the sequence. hm is then projected to vocabulary size by multiplication with the output\nembedding matrix, after which softmax is applied to get probabilities for the next action token. In-\nspired by (Yao et al., 2020), the GPT-2 model is trained to predict the next action at given the context\nc using a language modeling objective (Equation 5). The training data consists of k trajectories sam-\npled from an experience replay memory D which stores transition tuples (ct, at, rt, ot+1, terminal).\nSampling trajectories Let us deﬁne a trajectory τ as a sequence of observations, actions, and re-\nwards, i.e. τ = o1, a1, r1, o2, a2, r2 . . . , ol+1, where lτ denotes the trajectory length (i.e. number\nof actions) and thus l ≤T where T is the episode limit. We sample a trajectory from D using a\ntwo-step process. First, we sample a score u from a categorical distribution:\nP(u) ∝exp (β1(u −µU)\/σU) ,\nu ∈U\n(3)\nwhere U is the set of all unique scores encountered in the game so far, µU is the mean of the set\nU, and σU is its standard deviation. β1 is the temperature and determines how biased the sampling\nprocess is towards high scores. The second step collects all trajectories with the sampled score u\nand samples a trajectory τ based on the trajectory length lτ:\nP(τ | u) ∝exp (−β2(lτ −µLu)\/σLu) ,\nlτ ∈Lu\n(4)\nwhere Lu is the multiset of trajectory lengths lτ with score u, µLu is the mean of the elements\nin Lu, and σLu is its standard deviation. β2 deﬁnes the temperature and determines the strength\nof the bias towards shorter length trajectories. We perform this sampling procedure k times (with\nreplacement) to obtain a trajectory buffer B on which we perform imitation learning. This allows\nthe agent to globally explore the game space by emulating promising experiences from its past, with\na bias towards trajectories with high game score and shorter lengths. The motivation for sampling\nshorter length trajectories among the ones that reach the same score is because those tend to be the\nones that waste less time performing meaningless actions (e.g. “pick up sword”, “drop sword”, etc.).\nLearning from trajectories Given the trajectory buffer B containing single-step (c, a) pairs of\ncontext c and actions a from the trajectories sampled in the previous step, we train πil by minimizing\nthe cross-entropy loss over action commands (Yao et al., 2020):\nL(ξ) = −E(c,a)∼B log πil(a|c; ξ),\n(5)\n5\nPublished as a conference paper at ICLR 2022\nwhere c deﬁnes the context of past observations and actions as before, and ξ deﬁnes the parameters\nof the GPT-2 model. We perform several passes of optimization through the trajectory buffer B until\nconvergence4 and periodically perform this optimization every n epochs in gameplay to update πil.\nFurthermore, πil is renormalized over the valid action set Av during gameplay. Note that while πil is\ntrained similarly to the GPT-2 model in (Yao et al., 2020), their model generates action candidates.\n3.3\nEFFICIENT LOCAL EXPLORATION WITH INVERSE DYNAMICS (πinv−dy)\nIn the second phase of our algorithm, we would like to use a policy that tackles (1) the large action\nspace and (2) the dynamic nature of the action set at every step in the game, which makes it crucial\nto keep trying under-explored actions and is difﬁcult for the Q network alone to generalize over. To\nthis end, we use the inverse dynamics model (INV-DY) from (Yao et al., 2021). INV-DY is a Q-\nbased policy πinv−dy similar to DRRN (He et al., 2016a), optimized with the standard TD loss (see\nBackground of Section 3). In addition, it adds an auxiliary loss Linv capturing an inverse dynamics\nprediction error (Pathak et al., 2017), which is added as an intrinsic reward to the game reward\n(r = rgame + α1 ∗Linv) and hence incorporated into the TD loss. Formally, Linv is deﬁned as:\nLinv(θ, φ) = −log pd(a|ginv(concat(fo(o), fo(o′))),\n(6)\nwhere θ denotes the parameters for the recurrent decoder d and the MLP ginv (neither of which are\nused in πinv−dy during gameplay to score the actions), and fo is the encoder deﬁned in Section 3.\nThis loss is optimized together with the TD loss as well as with an action decoding loss Ldec to\nobtain the following overall objective that is used to train πinv−dy:\nL(φ, θ) = LTD + α2Linv(φ, θ) + α3Ldec(φ, θ),\n(7)\nwhere Ldec(φ, θ) = −log pd(a|fa(a)). Here, fa is a recurrent network (see Section 3). Please refer\nto Yao et al. (2021) for details. We train the model by sampling batches of transitions from a prior-\nitized experience replay buffer D (Schaul et al., 2015) and performing stochastic gradient descent.\nInverse dynamics ameliorates the challenges (1) and (2) mentioned above by rewarding underex-\nplored actions (i.e. a high loss in Equation 6) and by generalizing over novel action commands.\nSpeciﬁcally, the INV-DY network might generalize through learning of past bonuses to what new\nactions would look like and hence identify novel actions before having tried them once.\n3.4\nEPISODIC ROLLOUTS WITH XTX (ALGORITHM 1)\nWe now describe how XTX operates in a single episode. The agent starts in phase 1, where actions at\nare sampled from πexploit. Following this exploitation policy brings the agent to the game frontier,\nwhich we estimate to happen when either (1) the current episode score ≥M, the maximum score in\nthe trajectory buffer B or when (2) the current time step t > lmax, the length of the longest trajectory\nin B. The agent then enters phase 2 and switches its strategy to sample actions only from πinv−dy\nby setting λ = 1. At every time step t during all phases, a transition tuple (ct, at, rt, ot+1, terminal)\nis added to the replay buffer D. The policy πexploit is updated every n episodes using the process in\nSection 3.2, while πexplore is updated within episodes at every step using the TD loss in equation 7,\nsampling high rewarding trajectories with priority fraction ρ, similar to (Guo et al., 2020)5.\n3.5\nNOVELTY IN COMPARISON TO PRIOR ALGORITHMS\nWe now more explicitly discuss comparisons to a few other approaches. The most closely related ap-\nproaches are multi-stage algorithms, including Go-Explore (Ecoffet et al., 2021) and PC-PG (Agar-\nwal et al., 2020) (and somewhat the E3 algorithm (Kearns & Singh, 2002; Henaff, 2019)). Both of\nthese algorithms can be viewed as approaches which explicitly use a “roll-in” policy, with the goal\nof visiting a novel region of the state-action space. Go-Explore is limited to deterministic MDPs\n(where it is easy to re-visit any state in the past), while PC-PG (applicable to more general MDPs\nwith provable guarantees under certain linearity assumptions) more explicitly builds a set of policies\n(‘policy cover’) capable of visiting different regions of the state space. However, in both of these\n4We empirically ﬁnd 40 passes to be sufﬁcient for convergence.\n5We slightly differ from their approach as we only prioritize transitions from trajectories that achieve the\nmaximum score seen so far.\n6\nPublished as a conference paper at ICLR 2022\nAlgorithm 1 The eXploit-Then-eXplore (XTX) algorithm\n1: Initialize prioritized replay memory D to capacity N with priority fraction ρ.\n2: Initialize πexploit (with parameters ξ) and πexplore (with parameters θ and φ).\n3: Set max score M, max length lmax in B to 0.\n4: Exploration steps R = 50; episode limit T = 50\n5: for episode ←1, . . . , E do\n6:\nfor t ←1, . . . , T do\n7:\nReceive observation ot and valid action set Av ⊂A for current state.\n8:\nif current episode score < M and t < T −R then\n▷PHASE 1\n9:\nλ ←\n1\n2∗T\n10:\nelse\n▷PHASE 2\n11:\nλ ←1\n12:\nend if\n13:\nSample an action at from policy πλ(at|ot, at−1, at−2; φ, θ, ξ).\n▷Equation 2\n14:\nStep with at and receive (rt, ot+1, terminal) from game engine.\n15:\nStore transition tuple (ct, at, rt, ot+1, terminal) in D.\n16:\nUpdate πinv−dy using TD loss with inverse dynamics.\n▷Equation 7\n17:\nend for\n18:\nif n episodes have passed then\n19:\nSample k trajectories from D to form the new trajectory buffer B.\n▷Section 3.2\n20:\nUpdate πil with cross-entropy loss.\n▷Equation 5\n21:\nUpdate M, lmax and set T ←lmax + R.\n22:\nend if\n23: end for\napproaches, once the agent reaches a novel part of the state-space, the agent acts randomly. A key\ndistinction in our approach is that once the agent reaches a novel part of the state space, it uses\nan exploration with novelty bonuses, which may more effectively select promising actions over a\nrandom behavioral policy in large action spaces.\nThe other broad class of approaches that handle exploration use novelty bonuses, with either a policy\ngradient approach or in conjunction with Q-learning (see Section 2). The difﬁculty with the former\nclass of algorithms is the “catastrophic forgetting” effect (see Agarwal et al. (2020) for discussion).\nThe difﬁculty with Q-learning approaches (with a novelty bonus) is that bootstrapping approaches,\nwith function approximation, can be unstable in long planning horizon problems (sometimes re-\nferred to as the “deadly triad” (Jiang et al., 2021)). While XTX also uses Q-learning (with a novelty\nbonus), we only use this policy (πinv−dy) in the second phase of the algorithm in contrast to (Yao\net al., 2021) where πinv−dy is used throughout the entire episode; our hope is that this instability\ncan be alleviated since we are effectively using Q-learning to solve a shorter horizon exploration\nproblem (as opposed to globally using Q-learning, with a novelty bonus).\n4\nEXPERIMENTS\nEnvironments We evaluate on 12 human-created games across several genres from the Jericho\nbenchmark (Hausknecht et al., 2020). They provide a variety of challenges such as darkness, non-\nstandard actions, inventory management, and dialog (Hausknecht et al., 2020). At every step t, the\nobservation from the Jericho game engine contains a description of the state, which is augmented\nwith location and inventory information (by issuing “look” and “inventory” commands) to form\not (Hausknecht et al., 2019). In addition, we use of the valid action handicap provided by Jericho,\nwhich ﬁlters actions to remove those that do not change the underlying game state. We found this\naction handicap to be imperfect for some games (marked with * in Table 1), and manually added\nsome actions required for agent progress from game walkthroughs to the game engine’s grammar.\nEvaluation We evaluate agents under two settings: (a) a deterministic setting where the transition\ndynamics T(s′|s, a) is a one-hot vector over all the next states s′ and (b) a stochastic setting6 where\nthe T(s′|s, a) deﬁnes a distribution over next states s′, and the observations o can be perturbed with\n6Only six games have stochastic variants, and DRAGON was left out due to memory issues in the baselines.\n7\nPublished as a conference paper at ICLR 2022\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n40.3\n55.0\n44.1\n105.0\n41.7\n53.0\n34.1\n52.3\n103.4\n152.7\n+17%\n350\nINHUMANE*\n34.8\n56.7\n27.7\n63.3\n29.8\n53.3\n59.2\n76.7\n64.0\n76.7\n+5%\n90\nLUDICORP*\n17.1\n48.7\n19.6\n49.3\n10.9\n40.7\n67.3\n86.0\n78.8\n91.0\n+8%\n150\nZORK3*\n0.3\n4.3\n0.5\n5.0\n3.0\n5.0\n3.8\n4.7\n4.2\n5.0\n+6%\n7\nPENTARI*\n45.6\n58.3\n34.5\n53.3\n33.4\n46.7\n43.4\n60.0\n49.6\n60.0\n+6%\n70\nDETECTIVE\n289.9\n320.0\n289.5\n323.3\n269.3\n346.7\n296.0\n336.7\n312.2\n340.0\n+4%\n360\nBALANCES*\n14.1\n25.0\n12.5\n25.0\n10.0\n18.3\n21.9\n25.0\n24.0\n26.7\n+4%\n51\nLIBRARY*\n24.8\n30.0\n24.7\n30.0\n24.2\n30.0\n26.1\n30.0\n28.5\n30.0\n+8%\n30\nDEEPHOME*\n58.8\n68.0\n58.9\n72.7\n1.0\n1.0\n52.6\n70.0\n77.7\n92.3\n+6%\n300\nENCHANTER*\n42.0\n66.7\n44.2\n63.3\n26.8\n38.3\n24.3\n28.3\n52.0\n66.7\n+2%\n400\nDRAGON7\n-3.7\n8.0\n-2.3\n8.7\n3.2\n8.0\n40.7\n126.0\n96.7\n127.0\n0%\n25\nOMNIQUEST\n8.2\n10.0\n9.9\n13.3\n9.3\n10.0\n8.6\n10.0\n11.6\n13.3\n+3%\n50\nAvg. Norm Score\n29.5%\n48.8%\n28.4%\n51.8%\n29.7%\n44.5%\n49.2%\n58.6%\n56.3%\n64.0%\n5.8%\n100%\nTable 1:\nResults on deterministic games for the best XTX model, where the inverse dynamics\nscaling coefﬁcient α1 was tuned per game. We outperform the baselines on all 12 games, achieving\nan average normalized game score of 56%. * indicates actions were added to the game grammar.\n∆indicates the absolute performance difference between XTX and the best baseline on Avg scores.\nScores are averaged across 3 seeds. Baselines were rerun with the latest Jericho version.\nirrelevant sentences such as “you hear in the distance the chirping of a song bird”. We report both\nthe episode score average (Avg) over the last 100 episodes at the end of training, as well as the\nmaximum score (Max) seen in any episode during training.\nBaselines We consider four baselines. 1) DRRN (He et al., 2016a): This model uses a Q-based\nsoftmax policy, i.e. π ∝exp(Q(o, a; φ)), parameterized using GRU encoders and decoders, and\ntrained using the TD loss (Equation 1). 2) INV-DY (Yao et al., 2021): Refer to Section 3.3. 3)\nRC-DQN (Guo et al., 2020): This is a state-of-the-art model that uses an object-centric reading\ncomprehension (RC) module to encode observations and output actions. The training loss is that of\nDRRN above, and during gameplay, the agent uses an ϵ-greedy strategy. 4) XTX-Uniform (∼Go-\nExplore): Here, we replace πinv−dy with a policy that samples actions uniformly during Phase 2,\nkeeping all other factors of our algorithm the same. This is closest to a version of the Go-Explore\nalgorithm (Ecoffet et al., 2021) that returns to promising states and performs random exploration.\nHowever, while conceptually similar to Go-Explore, XTX-Uniform does not make use of any addi-\ntional memory archives, and avoids training a goal-based policy. See Appendix A.2 for implemen-\ntation details and hyperparameters.\n4.1\nRESULTS\nDeterministic games We report results in Table 1 (refer to Appendix A.5 and A.6 for more details).\nOverall, XTX outperforms DRRN, INV-DY, RC-DQN, and XTX-Uniform by 27%, 28%, 27%, and\n7% respectively, in terms of average normalized game score (i.e. average episode score divided\nby max score, averaged over all the games). Our algorithm performs particularly well on Zork1,\nachieving a 17% absolute improvement in average episode score and a 14% improvement in average\nmaximum score compared to the best baseline. In particular, XTX manages to advance past several\ndocumented bottlenecks like the dark Cellar (see Figure 1) which have proved to be very challenging\nfor existing methods (Ammanabrolu et al., 2020). While performance with XTX-Uniform is some-\ntimes close, exploration with inverse dynamics instead of random exploration pushes past several\nbottlenecks present in Zork1 and leads to signiﬁcant gains on Deephome, Enchanter, Omniquest,\nand Ludicorp, showing the potential usefulness of strategic exploration at the game frontier.\nStochastic games To show the robustness of XTX to stochasticity, we evaluate our agent in the\nstochastic setting (Table 2). XTX outperforms the baselines on 4 out of 5 games, and pushes past the\nmaximum scores of XTX-Uniform on the same fraction. Especially impressive is the performance\non Zork1, which is still higher than the state-of-the-art score in the deterministic setting.\n4.2\nABLATION STUDIES\nIn order to evaluate the importance of the various components in XTX, we perform several ablations\non a subset of the games as described below and shown in Figure 2 (more in Appendix A.3 and A.4).\n7Interestingly, XTX manages to achieve a very high score on Dragon by exploiting an integer underﬂow bug.\n8\nPublished as a conference paper at ICLR 2022\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n41.3\n55.7\n36.9\n85.7\n40.3\n53.0\n31.2\n48.0\n67.7\n143.0\n+8%\n350\nZORK3*\n0.2\n4.0\n0.4\n4.7\n2.7\n4.7\n2.3\n4.3\n2.6\n5.0\n-1%\n7\nPENTARI*\n38.2\n60.0\n37.5\n55.0\n33.3\n41.7\n38.8\n60.0\n47.3\n60.0\n+12%\n70\nDEEPHOME*\n43.0\n65.7\n58.4\n73.0\n1.0\n1.0\n50.7\n69.3\n70.9\n96.0\n+4%\n300\nENCHANTER*\n42.0\n56.7\n34.5\n53.3\n27.1\n43.3\n30.2\n45.0\n44.8\n58.3\n+1%\n400\nAvg. Norm Score\n18.9%\n39.0%\n19.7%\n41.5%\n20.9%\n30.5%\n24.3%\n39.1%\n31.8%\n48.9%\n4.8%\n100%\nTable 2: Results on stochastic games. We outperform baselines on 4 out of 5 games, with an average\nnormalized game score of 32%. Scores are averaged across 3 seeds. Baselines were rerun with the\nlatest Jericho version.\nZork1\nInhumane\nZork3\nLudicorp\nDeephome\nOmniquest\nGame\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Score\nAblation\nPure IL (  = 0)\nPure Inv Dy (  = 1)\nMix (  = 0.5)\nXTX (no-mix)\nXTX (full model)\nFigure 2: Average episode scores for 4 ablation models across 6 games. Overall, we ﬁnd both the\nstrategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm.\nPure imitation learning (λ = 0) This ablation sets λ = 0 in equation 2, meaning the agent will\nalways use the imitation learning policy πil. As expected, this model performs quite badly since it\nis based on pure exploitation and is hence unlikely to reach deep states in the game.\nPure inverse dynamics (λ = 1) This ablation sets λ = 1 in equation 2, hence always using the\ninverse dynamics exploration policy πinv−dy, resulting in the model proposed in (Yao et al., 2021).\nWhile this model can sometimes achieve high maximum scores, it is unable to learn from these and\nhence its average episode score remains quite low, consistent with ﬁndings in (Yao et al., 2021).\nMixing exploration and exploitation (λ = 0.5) By setting λ = 0.5, this ablation constantly alter-\nnates between exploitation and exploration, never committing to one or the other. This causes the\nagent to suffer from issues of both the λ = 0 and λ = 1 models, resulting in weak results.\nPure separation of exploitation and exploration (XTX no-mix) In this ablation, we examine the\nimportance of having a mixture policy in Phase 1 of the algorithm instead of setting λ = 0 in Phase 1\nand to 1 in Phase 2. This explicitly separated model, denoted as XTX (no-mix), performs a bit better\nin the games of Inhumane and Zork3, but sometimes fails to push past certain stages in Ludicorp\nand completely gets stuck in the game of Zork1. This shows it is crucial to have a mixture policy in\nPhase 1 in order to get past bottleneck states in difﬁcult games.\n5\nCONCLUSION\nWe have proposed XTX, an algorithm for multi-stage episodic control in text adventure games. XTX\nexplicitly disentangles exploitation and exploration into different policies, which are used by the\nagent for action selection in different phases of the same episode. Decomposing the policy allows\nthe agent to combine global decisions on which state spaces in the environment to (re-)explore, fol-\nlowed by strategic local exploration that can handle novel, unseen actions – aspects that help tackle\nthe challenges of sparse rewards and dynamic action spaces in these games. Our method signiﬁ-\ncantly outperforms prior methods on the Jericho benchmark (Hausknecht et al., 2020) under both\ndeterministic and stochastic settings, and even surpasses several challenging bottlenecks in games\nlike Zork1 (Ammanabrolu et al., 2020). Future work can integrate our algorithm with approaches\nthat better leverage linguistic signals to achieve further progress in these games.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGEMENTS\nWe thank the members of the Princeton NLP group and the anonymous reviewers for their valuable\nfeedback. JT was supported by a graduate fellowship at Princeton University. We are grateful to the\nGoogle Cloud Research program for computational support in running our experiments. We would\nalso like to thank Matthew Hausknecht for all the help regarding the Jericho environment.\nETHICAL CONSIDERATIONS\nThis work focuses on building better agents for text-adventure games and hence does not have im-\nmediate direct ethical concerns. However, the techniques introduced in this paper may be generally\nuseful for other autonomous agents that combine sequential decision making with language under-\nstanding (e.g. dialog systems). As such agents become more capable and inﬂuential in our lives, it\nis important to make sure their objectives align with those of humans, and that they are free of bias.\nREPRODUCIBILITY\nOur code is publicly available here https:\/\/github.com\/princeton-nlp\/XTX. We pro-\nvide all implementation details such as hyperparameters, model architectures and training regimes\nin Appendix A.2. We used Weights & Biases for experiment tracking and visualizations to develop\ninsights for this paper.\nREFERENCES\nAshutosh Adhikari, Xingdi (Eric) Yuan, Marc-Alexandre Cˆot´e, Mikulas Zelinka, Marc-Antoine\nRondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton\n. Learning dynamic knowledge graphs to generalize on text-based games. In NeurIPS 2020, 2020.\nAlekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-\nration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.\nDeep reinforcement learning at the edge of the statistical precipice. Advances in Neural Informa-\ntion Processing Systems, 34, 2021.\nPrithviraj Ammanabrolu and Matthew J. Hausknecht. Graph constrained reinforcement learning for\nnatural language action spaces. In 8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:\n\/\/openreview.net\/forum?id=B1x6w0EtwH.\nPrithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep\nreinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pp. 3557–3565, Minneapolis, Minnesota, 2019. Association for Computational\nLinguistics. doi: 10.18653\/v1\/N19-1358. URL https:\/\/www.aclweb.org\/anthology\/\nN19-1358.\nPrithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. How to avoid being eaten\nby a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795,\n2020.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven\nKapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, et al. Never\ngive up: Learning directed exploration strategies. In International Conference on Learning Rep-\nresentations, 2020.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.\nUnifying count-based exploration and intrinsic motivation. Advances in neural information pro-\ncessing systems, 29:1471–1479, 2016.\n10\nPublished as a conference paper at ICLR 2022\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-\nronment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,\n2013.\nRonen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-\noptimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213–231, 2002.\nC´edric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.\nGEP-PG: Decoupling exploration and\nexploitation in deep reinforcement learning algorithms.\nIn Jennifer Dy and Andreas Krause\n(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of\nProceedings of Machine Learning Research, pp. 1039–1048. PMLR, 10–15 Jul 2018.\nURL\nhttps:\/\/proceedings.mlr.press\/v80\/colas18a.html.\nMarc-Alexandre Cˆot´e, ´Akos K´ad´ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler.\nTextworld: A learning environment for text-based games. CoRR, 2018.\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then\nexplore. Nature, 590(7847):580–586, 2021.\nVincent Franc¸ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An\nintroduction to deep reinforcement learning. arXiv preprint arXiv:1811.12560, 2018.\nNancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock?\naffordance extraction via word embeddings. In Carles Sierra (ed.), Proceedings of the Twenty-\nSixth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, Melbourne, Australia,\nAugust 19-25, 2017, pp. 1039–1045. ijcai.org, 2017. doi: 10.24963\/ijcai.2017\/144. URL https:\n\/\/doi.org\/10.24963\/ijcai.2017\/144.\nXiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. Interac-\ntive ﬁction game playing as multi-paragraph reading comprehension with reinforcement learn-\ning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pp. 7755–7765, Online, 2020. Association for Computational Linguistics.\ndoi: 10.18653\/v1\/2020.emnlp-main.624. URL https:\/\/www.aclweb.org\/anthology\/\n2020.emnlp-main.624.\nMatthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. Nail:\nA general interactive ﬁction agent. arXiv preprint arXiv:1902.04259, 2019.\nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Cˆot´e, and Xingdi Yuan. Inter-\nactive ﬁction games: A colossal adventure. Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 34(05):7903–7910, Apr. 2020.\ndoi: 10.1609\/aaai.v34i05.6297.\nURL https:\n\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/6297.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep\nreinforcement learning with a natural language action space. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1621–\n1630, Berlin, Germany, August 2016a. Association for Computational Linguistics. doi: 10.18653\/\nv1\/P16-1153. URL https:\/\/aclanthology.org\/P16-1153.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep\nreinforcement learning with a natural language action space. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1621–\n1630, Berlin, Germany, 2016b. Association for Computational Linguistics. doi: 10.18653\/v1\/\nP16-1153. URL https:\/\/www.aclweb.org\/anthology\/P16-1153.\nMikael Henaff.\nExplicit explore-exploit algorithms in continuous state spaces.\narXiv preprint\narXiv:1911.00617, 2019.\nVishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G. Bellemare. Algorithmic\nimprovements for deep reinforcement learning applied to interactive ﬁction. In The Thirty-Fourth\n11\nPublished as a conference paper at ICLR 2022\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applica-\ntions of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020,\npp. 4328–4336. AAAI Press, 2020. URL https:\/\/aaai.org\/ojs\/index.php\/AAAI\/\narticle\/view\/5857.\nYoungsoo Jang, Seokin Seo, Jongmin Lee, and Kee-Eung Kim. Monte-carlo planning and learning\nwith language action value estimates. In International Conference on Learning Representations,\n2020.\nRay Jiang, Tom Zahavy, Zhongwen Xu, Adam White, Matteo Hessel, Charles Blundell, and Hado\nVan Hasselt. Emphatic algorithms for deep reinforcement learning. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume\n139. PMLR, 2021.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-\nchine learning, 49(2):209–232, 2002.\nJing Li, Xinxin Shi, Jiehao Li, Xin Zhang, and Junzheng Wang. Random curiosity-driven explo-\nration in deep reinforcement learning. Neurocomputing, 418:139–147, 2020.\nMarlos C. Machado, Marc G. Bellemare, and Michael Bowling. Count-based exploration with the\nsuccessor representation. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(04):\n5125–5133, Apr. 2020. doi: 10.1609\/aaai.v34i04.5955. URL https:\/\/ojs.aaai.org\/\nindex.php\/AAAI\/article\/view\/5955.\nAndrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng,\nAlexandros Papangelis, Dian Yu, Chandra Khatri, and G¨okhan T¨ur. Exploration based language\nlearning for text-based games. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artiﬁcial Intelligence, IJCAI 2020, pp. 1488–1494. ijcai.org, 2020.\ndoi: 10.24963\/ijcai.2020\/207. URL https:\/\/doi.org\/10.24963\/ijcai.2020\/207.\nKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based\ngames using deep reinforcement learning. In Proceedings of the 2015 Conference on Empir-\nical Methods in Natural Language Processing, pp. 1–11, Lisbon, Portugal, 2015. Association\nfor Computational Linguistics. doi: 10.18653\/v1\/D15-1001. URL https:\/\/www.aclweb.\norg\/anthology\/D15-1001.\nJunhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International\nConference on Machine Learning, pp. 3878–3887. PMLR, 2018.\nPhilip Osborne, Heido N˜omm, and Andre Freitas. A survey of text games for reinforcement learning\ninformed by natural language. arXiv preprint arXiv:2109.09478, 2021.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pp. 2778–2787.\nPMLR, 2017.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\nLukas Sch¨afer, Filippos Christianos, Josiah Hanna, and Stefano V. Albrecht. Decoupling exploration\nand exploitation in reinforcement learning. CoRR, abs\/2107.08966, 2021. URL https:\/\/\narxiv.org\/abs\/2107.08966.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv\npreprint arXiv:1511.05952, 2015.\nLior Shani, Yonathan Efroni, and Shie Mannor. Exploration conscious reinforcement learning re-\nvisited. In International Conference on Machine Learning, pp. 5680–5689. PMLR, 2019.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n12\nPublished as a conference paper at ICLR 2022\nAdrien Ali Taiga, William Fedus, Marlos C Machado, Aaron Courville, and Marc G Belle-\nmare. On bonus-based exploration methods in the arcade learning environment. arXiv preprint\narXiv:2109.11052, 2021.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep\nreinforcement learning. In 31st Conference on Neural Information Processing Systems (NIPS),\nvolume 30, pp. 1–18, 2017.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE\/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.\nIEEE, 2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nWilliam F. Whitney, Michael Bloesch, Jost Tobias Springenberg, Abbas Abdolmaleki, and Martin A.\nRiedmiller. Decoupled exploration and exploitation policies for sample-efﬁcient reinforcement\nlearning. CoRR, abs\/2101.09458, 2021. URL https:\/\/arxiv.org\/abs\/2101.09458.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. Deep rein-\nforcement learning with stacked hierarchical attention for text-based games. Advances in Neural\nInformation Processing Systems, 33, 2020.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, and Chengqi Zhang. Generalization in text-based\ngames via hierarchical reinforcement learning. arXiv preprint arXiv:2109.09968, 2021.\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\nLanguage models for action generation in text-based games. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online,\nNovember 2020. Association for Computational Linguistics. doi: 10.18653\/v1\/2020.emnlp-main.\n704. URL https:\/\/aclanthology.org\/2020.emnlp-main.704.\nShunyu Yao, Karthik Narasimhan, and Matthew Hausknecht.\nReading and acting while blind-\nfolded: The need for semantics in text game agents. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pp. 3097–3102, Online, June 2021. Association for Computational Linguis-\ntics. doi: 10.18653\/v1\/2021.naacl-main.247. URL https:\/\/aclanthology.org\/2021.\nnaacl-main.247.\nXingdi Yuan, Marc-Alexandre Cˆot´e, Alessandro Sordoni, Romain Laroche, Remi Tachet des\nCombes, Matthew J. Hausknecht, and Adam Trischler. Counting to explore and generalize in\ntext-based games. CoRR, 2018.\nTom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, and Shie Mannor. Learn what\nnot to learn: Action elimination with deep reinforcement learning. In Samy Bengio, Hanna M.\nWallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.),\nAdvances in Neural Information Processing Systems 31: Annual Conference on Neural Infor-\nmation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp.\n3566–3577, 2018. URL https:\/\/proceedings.neurips.cc\/paper\/2018\/hash\/\n645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html.\n13\nPublished as a conference paper at ICLR 2022\nA\nAPPENDIX\nA.1\nGAME STATISTICS\nGame\nZORK1\nINHUMANE\nLUDICORP\nLIBRARY\nZORK3\nPENTARI\nAvg.\/Max\n9 \/ 51\n14 \/ 28\n4 \/ 45\n5 \/ 6\n39 \/ 41\n5 \/ 16\nGame\nDETECTIVE\nBALANCES\nDEEPHOME\nENCHANTER\nDRAGON\nOMNIQUEST\nAvg.\/Max\n2 \/ 5\n12 \/ 54\n6 \/ 53\n15 \/ 40\n9 \/ 24\n13 \/ 26\nTable 3: Average and maximum number of steps between rewards for games in Jericho (based on\nhuman walkthroughs). Several games have long sequences of actions without reward.\nTable 3 contains the average and maximum number of steps between rewards in these games, show-\ncasing their challenging nature.\nA.2\nIMPLEMENTATION DETAILS\nWe use a learning rate of 10−3 and 10−4 for πil and πinv−dy, respectively. Both policies are trained\non batches of size 64, with hidden dimensions of size 128. The scaling coefﬁcient α1 for the inverse\ndynamics intrinsic reward is set to 1 for all games except for Deephome (α1 = 0.1), Enchanter\n(α1 = 0.5), Omniquest (α1 = 2), Ludicorp (α1 = 0.5), Detective (α1 = 2), and Pentari (α1 = 2).\nThe Transformer πil has 3 layers and 4 attention heads. β1 in equation 3 is set to 1, β2 in equation 4\nis set to 10k to encourage picking the shortest length trajectory, and k is set to 10. In equation 6,\nα1 = α2 = 1. The priority fraction ρ is set to 0.5. Every time πil is trained, we also scale the\nepisode length T to have at least R remaining steps of exploration by setting T = lmax + R, where\nlmax is the length of the longest trajectory in the trajectory buffer B. In practice, R = 50, and hence\nthe agent will be guaranteed at least 50 steps of exploration each episode. XTX and DRRN are run\nfor 800k interaction steps, while RC-DQN which is run for 100k interaction steps following Guo\net al. (2020).\nA.3\nFULL SET OF ABLATIONS\nZork1\nInhumane\nZork3\nLudicorp\nDeephome\nOmniquest\nGame\n0.0\n0.2\n0.4\n0.6\n0.8\nNormalized Score\nAblation\nPure IL (  = 0)\nPure Inv Dy (  = 1)\nMix (  = 0.5)\nXTX (no-mix)\nXTX (full model)\nEnchanter\nPentari\nBalances\nDetective\nLibrary\nDragon\nGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Score\nFigure 3: Average episode scores for 4 ablation models across 12 games. Overall, we ﬁnd both the\nstrategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm.\nScores for dragon were clipped to be between 0 and 1.\n14\nPublished as a conference paper at ICLR 2022\nA.4\nABLATION TRAINING PLOTS\nFigure 4: Average episode score throughout training for all ablations on Zork1. Shaded areas\nindicate one standard deviation.\nFigure 5: Average episode score throughout training for all ablations on Inhumane. Shaded areas\nindicate one standard deviation.\n15\nPublished as a conference paper at ICLR 2022\nFigure 6: Average episode score throughout training for all ablations on Zork3. Shaded areas\nindicate one standard deviation.\nFigure 7: Average episode score throughout training for all ablations on Ludicorp. Shaded areas\nindicate one standard deviation.\n16\nPublished as a conference paper at ICLR 2022\nFigure 8: Average episode score throughout training for all ablations on Balances. Shaded areas\nindicate one standard deviation.\nFigure 9: Average episode score throughout training for all ablations on Deephome. Shaded areas\nindicate one standard deviation.\n17\nPublished as a conference paper at ICLR 2022\nFigure 10: Average episode score throughout training for all ablations on Detective. Shaded areas\nindicate one standard deviation.\nFigure 11: Average episode score throughout training for all ablations on Enchanter. Shaded areas\nindicate one standard deviation.\n18\nPublished as a conference paper at ICLR 2022\nFigure 12: Average episode score throughout training for all ablations on Omniquest. Shaded areas\nindicate one standard deviation.\nFigure 13: Average episode score throughout training for all ablations on Pentari. Shaded areas\nindicate one standard deviation.\n19\nPublished as a conference paper at ICLR 2022\nFigure 14: Average episode score throughout training for all ablations on Dragon. Shaded areas\nindicate one standard deviation.\nFigure 15: Average episode score throughout training for all ablations on Library. Shaded areas\nindicate one standard deviation.\n20\nPublished as a conference paper at ICLR 2022\nA.5\nFULL DETERMINISTIC AND STOCHASTIC RESULTS\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n40.3 (2.2)\n55.0 (6.4)\n44.1 (12.6)\n105.0 (19.9)\n41.7 (0.6)\n53.0 (0.0)\n34.1 (1.5)\n52.3 (3.8)\n103.4 (10.9)\n152.7 (1.7)\n+17%\n350\nINHUMANE\n31.0 (1.0)\n56.7 (4.7)\n28.1 (3.6)\n60.0 (0.0)\n31.8 (1.4)\n63.3 (4.7)\n68.9 (8.9)\n83.3 (9.4)\n60.9 (5.9)\n70.0 (14.1)\n-9%\n90\nINHUMANE*\n34.8 (3.9)\n56.7 (4.7)\n27.7 (5.3)\n63.3 (4.7)\n29.8 (2.3)\n53.3 (4.7)\n59.2 (1.2)\n76.7 (9.4)\n64.0 (7.7)\n76.7 (9.4)\n+5%\n90\nLUDICORP\n15.6 (0.1)\n23.0 (0.0)\n15.6 (0.2)\n23.0 (0.0)\n12.4 (1.1)\n21.0 (2.2)\n19.9 (0.4)\n23.0 (0.0)\n20.9 (0.1)\n23.0 (0.0)\n+1%\n150\nLUDICORP*\n17.1 (1.7)\n48.7 (2.1)\n19.6 (5.5)\n49.3 (16.2)\n10.9 (1.7)\n40.7 (2.5)\n67.3 (4.2)\n86.0 (2.8)\n78.8 (5.1)\n91.0 (3.6)\n+8%\n150\nZORK3\n0.3 (0.0)\n4.7 (0.5)\n0.4 (0.0)\n5.0 (0.0)\n3.2 (0.5)\n5.0 (0.0)\n3.7 (0.2)\n4.7 (0.5)\n4.2 (0.0)\n5.0 (0.0)\n+7%\n7\nZORK3*\n0.3 (0.0)\n4.3 (0.5)\n0.5 (0.1)\n5.0 (0.0)\n3.0 (0.3)\n5.0 (0.0)\n3.8 (0.4)\n4.7 (0.5)\n4.2 (0.1)\n5.0 (0.0)\n+6%\n7\nPENTARI\n43.4 (4.5)\n58.3 (2.4)\n29.8 (14.1)\n46.7 (6.2)\n37.4 (7.0)\n46.7 (11.8)\n43.4 (1.7)\n60.0 (0.0)\n45.5 (4.3)\n60.0 (0.0)\n+3%\n70\nPENTARI*\n45.6 (1.9)\n58.3 (2.4)\n34.5 (7.5)\n53.3 (6.2)\n33.4 (6.9)\n46.7 (6.2)\n43.4 (0.4)\n60.0 (0.0)\n49.6 (1.3)\n60.0 (0.0)\n+6%\n70\nDETECTIVE\n289.9 (0.1)\n320.0 (8.2)\n289.5 (0.4)\n323.3 (4.7)\n269.3 (14.8)\n346.7 (4.7)\n296.0 (9.0)\n336.7 (12.5)\n312.2 (10.3)\n340.0 (8.2)\n+4%\n360\nBALANCES\n10.0 (0.0)\n10.0 (0.0)\n9.9 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n9.6 (0.1)\n10.0 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n0%\n51\nBALANCES*\n14.1 (0.4)\n25.0 (0.0)\n12.5 (1.6)\n25.0 (0.0)\n10.0 (0.1)\n18.3 (2.4)\n21.9 (0.4)\n25.0 (0.0)\n24.0 (0.3)\n26.7 (2.4)\n+4%\n51\nLIBRARY\n17.3 (0.7)\n21.0 (0.0)\n17.0 (0.2)\n21.0 (0.0)\n16.2 (1.4)\n21.0 (0.0)\n18.8 (0.4)\n21.0 (0.0)\n19.7 (0.5)\n21.0 (0.0)\n+3%\n30\nLIBRARY*\n24.8 (0.6)\n30.0 (0.0)\n24.7 (0.4)\n30.0 (0.0)\n24.2 (1.4)\n30.0 (0.0)\n26.1 (0.4)\n30.0 (0.0)\n28.5 (0.3)\n30.0 (0.0)\n+8%\n30\nDEEPHOME\n57.9 (0.4)\n68.7 (0.5)\n44.8 (19.9)\n76.0 (5.0)\n1.0 (0.0)\n1.0 (0.0)\n46.3 (9.0)\n60.7 (13.2)\n75.7 (5.0)\n93.7 (5.6)\n+6%\n300\nDEEPHOME*\n58.8 (0.1)\n68.0 (0.8)\n58.9 (0.2)\n72.7 (3.8)\n1.0 (0.0)\n1.0 (0.0)\n52.6 (0.4)\n70.0 (0.8)\n77.7 (2.1)\n92.3 (3.3)\n+6%\n300\nENCHANTER\n46.1 (11.1)\n70.0 (21.2)\n46.0 (3.6)\n73.3 (8.5)\n25.8 (8.5)\n36.7 (14.3)\n43.4 (18.9)\n53.3 (23.6)\n34.7 (21.2)\n36.7 (23.6)\n-3%\n400\nENCHANTER*\n42.0 (1.2)\n66.7 (2.4)\n44.2 (18.3)\n63.3 (30.6)\n26.8 (1.9)\n38.3 (4.7)\n24.3 (10.8)\n28.3 (11.8)\n52.0 (23.1)\n66.7 (33.0)\n+2%\n400\nDRAGON\n-3.7 (0.4)\n8.0 (0.0)\n-2.3 (0.5)\n8.7 (1.7)\n3.2 (1.6)\n8.0 (0.0)\n40.7 (0.0)\n126.0 (0.0)\n96.7 (1.1)\n127.0 (0.0)\n0%\n25\nOMNIQUEST\n8.2 (0.1)\n10.0 (0.0)\n9.9 (0.0)\n13.3 (2.4)\n9.3 (0.7)\n10.0 (0.0)\n8.6 (0.1)\n10.0 (0.0)\n11.6 (1.3)\n13.3 (2.4)\n+3%\n50\nAvg. Norm Score\n29.5% (29.8)\n48.8% (28.8)\n28.4% (27.2)\n51.8% (27.3)\n29.7% (25.6)\n44.5% (32.1)\n49.2% (30.4)\n58.6% (33.6)\n56.3% (28.1)\n64.0% (28.6)\n5.8% (4.1)\n100%\nTable 4: Full Deterministic Results. Standard deviations are in parentheses. Scores are averaged\nacross 3 seeds. Note that the average normalized scores only take into account the games listed in\nTable 1. Baselines were rerun with the latest Jericho version.\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n41.3 (3.2)\n55.7 (3.3)\n36.9 (2.4)\n85.7 (14.8)\n40.3 (1.6)\n53.0 (0.0)\n31.2 (1.1)\n48.0 (5.0)\n67.7 (8.0)\n143.0 (10.7)\n+8%\n350\nZORK3\n0.2 (0.0)\n4.3 (0.5)\n0.7 (0.2)\n5.0 (0.0)\n2.7 (0.0)\n5.0 (0.0)\n1.8 (0.1)\n4.0 (0.0)\n2.7 (0.4)\n5.0 (0.0)\n0%\n7\nZORK3*\n0.2 (0.0)\n4.0 (0.0)\n0.4 (0.3)\n4.7 (0.5)\n2.7 (0.1)\n4.7 (0.5)\n2.3 (0.5)\n4.3 (0.5)\n2.6 (0.6)\n5.0 (0.0)\n-1%\n7\nPENTARI\n42.3 (0.8)\n60.0 (0.0)\n28.9 (8.5)\n45.0 (0.0)\n31.2 (3.9)\n38.3 (11.8)\n38.4 (1.3)\n60.0 (0.0)\n48.2 (0.4)\n60.0 (0.0)\n+8%\n70\nPENTARI*\n38.2 (3.6)\n60.0 (0.0)\n37.5 (8.0)\n55.0 (7.1)\n33.3 (6.0)\n41.7 (10.3)\n38.8 (0.4)\n60.0 (0.0)\n47.3 (0.4)\n60.0 (0.0)\n+12%\n70\nDEEPHOME\n58.2 (0.6)\n72.0 (5.7)\n58.2 (0.5)\n72.7 (2.5)\n1.0 (0.0)\n1.0 (0.0)\n48.0 (10.1)\n62.0 (14.2)\n73.9 (4.3)\n99.3 (13.9)\n+5%\n300\nDEEPHOME*\n43.0 (20.0)\n65.7 (3.3)\n58.4 (0.5)\n73.0 (1.4)\n1.0 (0.0)\n1.0 (0.0)\n50.7 (2.3)\n69.3 (0.9)\n70.9 (2.7)\n96.0 (7.8)\n+4%\n300\nENCHANTER\n41.0 (0.6)\n71.7 (9.4)\n38.9 (14.5)\n63.3 (30.6)\n25.0 (4.0)\n30.0 (7.1)\n32.1 (10.9)\n53.3 (23.6)\n46.2 (18.9)\n51.7 (22.5)\n+1%\n400\nENCHANTER*\n42.0 (18.5)\n56.7 (27.2)\n34.5 (10.3)\n53.3 (23.6)\n27.1 (2.7)\n43.3 (8.5)\n30.2 (9.1)\n45.0 (20.4)\n44.8 (19.4)\n58.3 (27.8)\n+1%\n400\nAvg. Norm Score\n18.9% (18.2)\n39.0% (28.1)\n19.7% (17.5)\n41.5% (26.0)\n20.9% (18.6)\n30.5% (27.1)\n24.3% (17.9)\n39.1% (29.6)\n31.8% (19.8)\n48.9% (26.0)\n4.8% (4.7)\n100%\nTable 5: Full Stochastic Results. Standard deviations are in parentheses. Scores are averaged\nacross 3 seeds. Note that the average normalized scores only take into account the games listed in\nTable 2. Baselines were rerun with the latest Jericho version.\nA.6\nAGGREGATE METRICS & PERFORMANCE PROFILES\n0.24 0.32 0.40 0.48 0.56\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX\nMedian\n0.24 0.32 0.40 0.48 0.56\nIQM\n0.30 0.36 0.42 0.48 0.54\nMean\n0.48 0.54 0.60 0.66 0.72\nOptimality Gap\nNormalized Score\nFigure 16: Aggregate metrics with 95% CIs for the deterministic games listed in Table 1, follow-\ning Agarwal et al. (2021). The CIs use percentile bootstrap with stratiﬁed sampling.\n0.12 0.15 0.18 0.21 0.24\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX\nMedian\n0.12 0.16 0.20 0.24 0.28\nIQM\n0.20\n0.24\n0.28\n0.32\nMean\n0.68\n0.72\n0.76\n0.80\n0.84\nOptimality Gap\nNormalized Score\nFigure 17: Aggregate metrics with 95% CIs for the stochastic games listed in Table 2, follow-\ning Agarwal et al. (2021). The CIs use percentile bootstrap with stratiﬁed sampling.\n21\nPublished as a conference paper at ICLR 2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Score ( )\n0.00\n0.25\n0.50\n0.75\n1.00\nFraction of runs with score >\nDRRN\nINV-DY\nXTX-Uniform\nXTX\nRC-DQN\nFigure 18: Performance proﬁles based on score distributions for the deterministic games listed in\nTable 1, following Agarwal et al. (2021). Shaded regions show pointwise 95% conﬁdence bands\nbased on percentile bootstrap with stratiﬁed sampling.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Score ( )\n0.00\n0.25\n0.50\n0.75\n1.00\nFraction of runs with score >\nDRRN\nINV-DY\nXTX-Uniform\nXTX\nRC-DQN\nFigure 19: Performance proﬁles based on score distributions for the stochastic games listed in\nTable 2, following Agarwal et al. (2021). Shaded regions show pointwise 95% conﬁdence bands\nbased on percentile bootstrap with stratiﬁed sampling.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Multi-Stage Episodic Control for Strategic Exploration in Text Games.pdf"}
{"title":"Pre-trained Language Models as Prior Knowledge for Playing Text-based Games","authors":"Ishika Singh, Gargi Singh, Ashutosh Modi","summary":"Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires an understanding of and\ninteraction using natural language in a partially observable environment.\nAgents observe the environment via textual descriptions designed to be\nchallenging enough for even human players. Past approaches have not paid enough\nattention to the language understanding capability of the proposed agents.\nTypically, these approaches train from scratch, an agent that learns both\ntextual representations and the gameplay online during training using a\ntemporal loss function. Given the sample-inefficiency of RL approaches, it is\ninefficient to learn rich enough textual representations to be able to\nunderstand and reason using the textual observation in such a complicated game\nenvironment setting. In this paper, we improve the semantic understanding of\nthe agent by proposing a simple RL with LM framework where we use\ntransformer-based language models with Deep RL models. We perform a detailed\nstudy of our framework to demonstrate how our model outperforms all existing\nagents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6\nhigher than the state-of-the-art model. Overall, our proposed approach\noutperforms 4 games out of the 14 text-based games, while performing comparable\nto the state-of-the-art models on the remaining games.","url":"http:\/\/arxiv.org\/abs\/2107.08408v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2107.08408v2","published":1626604128000,"comment":"40 Pages (8 Pages main content + 1 Page references + 31 Pages\n  Appendix). Some new results added","pdf_text":"Pre-trained Language Models as Prior Knowledge for Playing\nText-based Games\nIshika Singh, Gargi Singh, Ashutosh Modi\nDepartment of Computer Science and Engineering\nIndian Institute of Technology Kanpur (IITK), India\n{ishikas,sgargi}@iitk.ac.in, ashutoshm@cse.iitk.ac.in\nABSTRACT\nRecently, text world games have been proposed to enable artifi-\ncial agents to understand and reason about real-world scenarios.\nThese text-based games are challenging for artificial agents, as it re-\nquires an understanding of and interaction using natural language\nin a partially observable environment. Agents observe the environ-\nment via textual descriptions designed to be challenging enough\nfor even human players. Past approaches have not paid enough\nattention to the language understanding capability of the proposed\nagents. Typically, these approaches train from scratch, an agent\nthat learns both textual representations and the gameplay online\nduring training using a temporal loss function. Given the sample-\ninefficiency of RL approaches, it is inefficient to learn rich enough\ntextual representations to be able to understand and reason using\nthe textual observation in such a complicated game environment\nsetting. In this paper, we improve the semantic understanding of\nthe agent by proposing a simple RL with LM framework where we\nuse transformer-based language models with Deep RL models. We\nperform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1,\nto achieve a score of 44.7, which is 1.6 higher than the state-of-the-\nart model. Overall, our proposed approach outperforms 4 games\nout of the 14 text-based games, while performing comparable to\nthe state-of-the-art models on the remaining games.\n1\nINTRODUCTION\nArtificial autonomous agents suffer from a number of challenges\nduring training, such as reward, goal, or task under-specification(s).\nMost approaches design a specific reward function for a given en-\nvironment and task, which is not generalizable to any other setting.\nThe reward specification gets more complicated with complex en-\nvironments and tasks. Most agents are trained to do specific tasks\nor achieve specific goals, as there are not many efficient ways for\nspecifying multiple or complex goals for a given agent. In this\nscenario, being able to utilize language as an interface between a\nuser and an artificial agent simplifies a number of these challenges.\nA setting where an agent understands and communicates using\nnatural language is more efficient for rewarding the agent online\nor via a text description, as opposed to using an expert-designed\nreward function. Moreover, it is easier to specify complicated tasks\nor instructions for the agent via the natural language.\nText-based Interactive Fiction (IF) [7] games provide such an en-\nvironment where the agent learns to consume and produce natural\nlanguage-based inputs and outputs. IF games require an artificial\nagent to learn policies and operate in a real-world environment\ncreated using natural language. These games are designed such that\na player receives a textual observation consisting of information\nabout the environment such as a description of the surroundings,\nobjects available for interaction, and states of objects. Based on this\ninformation, the player takes an action such as interacting with one\nof the objects, moving around. etc. A short example of a game-play\nis shown in Figure 1. The textual setting of the games in terms of the\nobservations given to and the actions taken by the agent requires\nsolid language understanding and reasoning. Such a setting has\nbeen promoted to develop essential skills required by intelligent\nagents for various real-life use-cases, including dialogue systems\nand personal digital assistants. IF games, such as Zork1, have been\ncreated to be challenging for human players and consist of texts\nwith dramatic and artful narratives. The game engine, in the case\nof IF games, generates human-level sophistication and diversity\nin textual descriptions (as can be seen in the example in Figure\n1) in contrast to games using template-generated synthetic texts.\nConsequently, such games provide an ideal test-bed for artificial\nagents that interact with the real-world environment using natural\nlanguage. Given the setting, agents trained using IF games can be\ndeployed in real-world user-centered robotics applications.\nIF games present several challenges for artificial agents as these\ncover the real world settings. It requires an agent to understand\nthe textual state description, handle combinatorial textual action\nspace, and learn a policy in a partially observable environment to\nmaximize the game score. The key challenge is to decipher the long\ntextual observations, extract reward cues from them, and generate\na semantically rich representation such that the policy learned on\ntop of it is well informed. Most of the existing works learn textual\nrepresentations from scratch during the RL training [1, 2, 7, 23–25],\nexcept [6] which uses pre-trained GloVe embeddings [14]. Online\nreinforcement learning is known to be sample-inefficient [19]. The\nreward-based temporal difference objective used for training is\nproposed for learning the gameplay, and it does not necessarily\nreinforce the agent to learn the semantics of the game. Therefore,\nlearning textual representations solely from the game-generated\ntext during online learning does not provide rich enough represen-\ntation to be able to handle such complicated decision-making tasks.\nIn addition, these games also need some prior knowledge about\nthe functioning of the world to be able to make correct decisions\nefficiently. For example, as shown in Figure 1, if it is dark and the\nagent has a lantern, it should turn it on.\nGiven the requirements for IF games, we propose a pre-trained\ntransformer-based [20] language model (LM) as a candidate for\nequipping the RL agent with both language understanding capa-\nbilities and real-world knowledge. Transformer-based LMs such as\nBERT [5], ALBERT [10], RoBERTa [11] have produced state-of-the-\nart performance on a plethora of natural language understanding\n(NLU) tasks. The routine for using the transformer models includes\narXiv:2107.08408v2  [cs.CL]  23 Dec 2021\nwith a great eﬀort, the rug is moved to one side of the room, revealing the dusty cover of a closed trap \ndoor\nOpen case\nyou are in the living room. there is a doorway to the east, a wooden door with strange gothic lettering to \nthe west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the \nroom. above the trophy case hangs an elvish sword of great antiquity. a battery powered brass lantern \nis on the trophy case. [you are carrying : a jewel encrusted egg] [same as current observation]\npush rug\nOpened.\nTake lantern\nTaken. [you are carrying : a brass lantern a jewel encrusted egg] [living room you are in the living room. \nthere is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to \nbe nailed shut, a trophy case, and a closed trap door at your feet. above the trophy case hangs an elvish \nsword of great antiquity.]\nPut egg in case\n+5\ndone. [you are carrying : brass lantern] […a closed trap door at your feet. above the trophy case hangs \nan elvish sword of great antiquity. your collection of treasures consists of : jewel encrusted egg] Open trap\nthe door reluctantly opens to reveal a rickety staircase descending into darkness. […nailed shut, a \ntrophy case, and a rug lying beside an open trap door. above the trophy case hangs an elvish sword of \ngreat antiquity. your collection of treasures consists of : a jewel encrusted egg]\nDown\n+25\nyou have moved into a dark place. the trap door crashes shut, and you hear someone barring it. [it is \npitch black. you are likely to be eaten by a grue.]\nturn  on lantern\neast of chasm you are on the east edge of a chasm, the bottom of which cannot be seen. a narrow \npassage goes north, and the path you are on continues to the east. [same as current observation.] East\nthe brass lantern is now on. cellar you are in a dark and damp cellar with a narrow passageway leading \nnorth, and a crawlway to the south. on the west is the bottom of a steep metal ramp which is \nunclimbable. [you are carrying : a brass lantern ( providing light )] [same as current observation.] South\nthis is an art gallery. most of the paintings have been stolen by vandals with exceptional taste. the vandals \nleft through either the north or west exits. fortunately, there is still one chance for you to be a vandal, for \non the far wall is a painting of unparalleled beauty. [same as current observation.]\nTake art\n+4\nFigure 1: A sample gameplay by our model (DBERT-DRRN) for the classic text game, Zork1. The aim is to solve puzzles and\ncollect 19 treasures in the trophy case, but the agent is not aware of the goal and learns from rewards. Each white box is a\nstate of the game at a step; the green and orange boxes are the action taken, and the reward received correspondingly. The\nstate contains location if visible (underlined), current observation (black), inventory with the player (red), and the current\nlocation description (blue). In this example, location, inventory, and description are only mentioned when it changes, while\nthe complete state information is provided to the agent during gameplay. This example shows how our model is able to perform\nbetter by learning to use the egg and the lantern in the correct way, as highlighted.\npre-training on large generic English corpora, followed by fine-\ntuning on a specific downstream task-specific corpus. Moreover,\nPetroni et al. [15] have shown that pre-trained LMs can act as\nknowledge bases containing relational and factual knowledge, com-\nparable in utility with other NLP methods having access to oracle\nknowledge.\nIn this work, we propose a simple approach that performs bet-\nter than previously proposed complex approaches. We deploy a\npre-trained LM with the existing game agents, namely Deep Re-\ninforcement Relevance Network (DRRN) [8] and Template-Deep\nQ-Network (TDQN) [7], used in the recently proposed Jericho IF\ngame-suite [7]. To the best of our knowledge, we are the first to\nutilize pre-trained LMs for IF games. We use DistilBERT[17] (here-\nafter referred to as DBERT) as the LM owing to its compact size,\nthereby computing rich text encoding efficiently. We fine-tune\nDBERT on an independent set of human gameplay transcripts to\nadd a game sense to the pre-trained LM. This set of transcripts\ncontain games other than those in the evaluation game-suite to\ntest the generalizability of the proposed approach. We test both\nDBERT-DRRN and DBERT-TDQN setup on Zork1, and additionally,\nwe also evaluate DBERT-DRRN on a set of other games from Jericho\nto test the generalization capability of the model. We achieve new\nstate-of-the-art results on 4 out of 14 games while getting compara-\nble performance on others relative to the past approaches (section\n2) utilizing sophisticated attention models and knowledge graphs,\nindicating that pre-trained LMs could be employed as a key com-\nponent in text-based reinforcement learning models. We release\nthe implementation code via GitHub1. The transcripts (generated\nwhen the agent is playing the game) is included in the appendix.\n2\nRELATED WORK\nRecently, there have been attempts to model real world settings via\ntext based environments. The TextWorld environment [4] proposes\nprocedurally generated interactive fiction games, with control over\ndifficulty, and description language. Recently, Hausknecht et al. [7]\nproposed a learning environment (Jericho) that supports a set of 32\nhuman-written IF games. These games are written to be challenging\nfor human players, thereby providing a more realistic test-bed for\ntraining intelligent agents. [7] also present performance results for\nchoice-based (DRRN) and parser-based (TDQN) agents (see section\n3.2 for details), where the former performed better on average\nacross most of the games. Jericho also provides certain handicaps\nsuch as action templates, determining valid actions by detecting\nworld state change, commands to check current items with the\nagent (inventory) or in the current state (look). We propose models\nfor solving the games in the Jericho environment.\nMany of the recent approaches [1, 2, 23] use a dynamically up-\ndated Knowledge Graph (KG) to represent the current state of the\nuniverse (game). KG-A2C [1] is the first such proposal. One of\nthe SOTA methods by Xu et al. [23], in addition to KG-A2C archi-\ntecture, reasons on the KG using low and high-level attention on\nsub-components of the graph and on the complete graph, respec-\ntively, to compute a representation of the game state. Next, they\nselect the actions via recurrent decoding using GRUs, conditioned\non the computed game state representation. Similar to KG-A2C\napproach, they train the agent via the Advantage Actor Critic (A2C)\nmethod with a supervised auxiliary task of “valid action prediction”\nusing the action templates handicap. Q∗BERT [2] presents an open\ndomain QA method to update the KG with more information and\nan additional intrinsic motivation reward to enable structured ex-\nploration. Our agent outperforms all the above agents without an\nexplicit knowledge graph or any additional reward-based super-\nvision or sophisticated reasoning. We show that better language\nunderstanding capabilities and appropriate utilization of the world\nknowledge in game-plays are comparable (and sometimes better)\nthan using explicit KGs.\nMulti-Paragraph Reading Comprehension Deep Q-Network [6]\n(MPRC-DQN) breaks down the problem into two challenges: the\npartial observability of the environment and large natural language\naction space. They solve the partial observability by object-centric\nretrieval of relevant past observations, and an attention mechanism\nis deployed to focus on the most relevant context. Template actions\nfrom Jericho are filled up in question answering (QA) format to\ngenerate candidate actions. In contrast, CALM [25] generates the\nnext set of possible action by fine-tuning a GPT-2 model [16]. These\nactions, which are essentially a subset of valid actions, are then fed\nto a DRRN [8] agent to compute the Q-values. CALM asserts that\nthe action generation model provides linguistic priors to the RL\nagent. However, since they do not send any feedback to the action\ngeneration model and the model only replaces the valid action set\n1https:\/\/github.com\/Exploration-Lab\/IFG-Pretrained-LM\nby its subset, the RL agent may not necessarily benefit from the\nlinguistic priors. On the other hand, we extract representations\nfrom the pre-trained LM and feed them to the DRRN agent for\nincorporating linguistic priors. With a better observation under-\nstanding augmented with the world knowledge via the pre-trained\nLM, our model also solves the challenge of partial observability to\nsome extent.\nYao et al. [24] investigate to what extent semantic information is\nutilized by the DRRN agent. They show that even in the complete\nabsence of text, the “DRRN + valid action handicap\" setup is able to\nachieve significant scores, indicating the underlying overfitting to\nthe reward system and memorization tendency of DRRN. They\nuse an inverse-dynamics loss function to regularize the DRRN\nrepresentation space for improving the semantics.\nTransformers [20] are a new class of feed-forward neural net-\nworks architectures that use a self-attention mechanism to effec-\ntively learn short\/long-term relationships between tokens in the\ntext. Transformers have been shown to have SOTA performance on\nalmost all the NLP tasks [18, 21]. Transformers are trained using\nlanguage modeling objective and its variants (e.g., masked language\nmodeling); based on the training objective a number of variants\nof transformer architectures have been proposed e.g., BERT [5],\nDistilBERT [17], ALBERT [10], RoBERTa [11]. There have been\nseveral transformer-based approaches for both text-based games\nand RL in general [3, 9, 12, 22], but these approaches train a trans-\nformer from scratch, which is inefficient given that transformers\nare data-intensive and online RL is sample-inefficient. Xu et al. [22]\nand Parisotto et al. [12] modify the transformer structure to add\ngates and make it light-weight for online training, while Chen et al.\n[3] and Janner et al. [9] use a smaller transformer but only for of-\nfline learning. Using a pre-trained LM brings an additional set of\nlinguistic priors and world knowledge to the RL agent, and in this\npaper, we propose a way to do so. We chose DistilBERT [17] for\nour framework. DistilBERT is a lighter and faster version of BERT\ntrained by knowledge-distilling BERT base with masked language\nmodeling on Toronto Book Corpus and Wikipedia, thus providing\nus a computationally efficient solution to integrate in the RL setup.\n3\nMETHOD\n3.1\nProblem Statement\nA text-based game can be formulated as a partially observable\nMarkov Decision Process (POMDP) defined by (𝑆,𝑇,𝐴,𝑂, 𝑅). An\nagent interacts with the game environment to receive textual ob-\nservations 𝑜𝑡∈𝑂at the interaction step 𝑡. The latent states 𝑠𝑡∈𝑆\ncorrespond to 𝑜𝑡along with player, item locations, and inventory\ncontents. The agent interacts with the environment by executing\nan action 𝑎𝑡∈𝐴that changes the game state according to a mostly-\ndeterministic but latent transition function𝑇(𝑠′|𝑠,𝑎), and the agent\nreceives rewards 𝑟𝑡from an unknown reward function 𝑅(𝑠,𝑎) de-\nfined by the game designers. The objective of an RL agent is to\ntake a series of actions that maximize expected cumulative dis-\ncounted rewards E\n\u0002Í∞\n𝑖=𝑡𝛾𝑟𝑖\n\u0003\nat any time 𝑡. Value-based RL models\nestimate this reward by learning a Q-function, 𝑄(𝑜𝑡,𝑎𝑡) i.e., the\nexpected return when taking an action 𝑎𝑡, given the observation 𝑜𝑡.\nThe problem statement presents several challenges for the agent:\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n*58\n2EVHUYDWLRQ\n(QFRGHU\n*58\n\/RRN\n(QFRGHU\n*58\n,QYHQWRU\\\u0003\n(QFRGHU\n*58\n$FWLRQ\n(QFRGHU\n&RQFDW\n0\/3\n$FW\u0010VFRUHU\n4\u000bR\u000fD\f\n2EVHUYDWLRQ\n\/RRN\n,QYHQWRU\\\u0003\n$FWLRQ\n&RQFDW\n(a)\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n*58\n2EVHUYDWLRQ\n(QFRGHU\n*58\n\/RRN\n(QFRGHU\n*58\n,QYHQWRU\\\u0003\n(QFRGHU\n*58\n$FWLRQ\n(QFRGHU\n&RQFDW\n0\/3\n2EVHUYDWLRQ\n\/RRN\n,QYHQWRU\\\u0003\n3UHYLRXV\u0003$FWLRQ\nW\u0010VFRUHU\nR\u0014\u0010VFRUHU\nR\u0015\u0010VFRUHU\n4\u000bR\u000fX\f\n4\u000bR\u000fS\u0014\f\n4\u000bR\u000fS\u0015\f\n(b)\nFigure 2: RL agents - (a) DRRN: Q-value Q(o,a) is computed for observation o and action a, (b) TDQN: Q-values Q(o,u), Q(o,𝑝1),\nQ(o,𝑝2) for all templates 𝑢∈𝑇and all vocabulary 𝑝∈𝑉.\nunderstanding long sophisticated textual state descriptions, ex-\ntracting reward cues, handling combinatorial textual action space,\nreasoning-based decision-making, and learning a value function in\na partially observable environment to maximize the game score.\n3.2\nRL Agents\nWe build our model on top of existing RL agents. We select the\nfollowing two agents as these are base architectures [7] on which\nall other subsequent works build on.\nDRRN: Deep Reinforcement Relevance Network (DRRN) [7] uses\nfour GRU encoders to encode the state and each of the actions from\nthe valid action handicap, concatenate the representations, and pass\nit through an MLP layer to estimate the Q-values (𝑄(𝑜𝑡,𝑎𝑡𝑖)|𝑖=1...𝑁),\nwhere 𝑎𝑡𝑖∈A (valid action set). The next action is chosen by\nsoftmax sampling the predicted Q-values. The DRRN is trained\nusing tuples (𝑜𝑡,𝑎𝑡,𝑟𝑡,𝑜𝑡+1) sampled from a prioritized experience\nreplay buffer with the traditional temporal difference (TD) loss:\nLTD(𝜙) =\n\u0012\n𝑟𝑡+ 𝛾max\n𝑎∈A 𝑄𝜙(𝑜𝑡+1,𝑎) −𝑄𝜙(𝑜𝑡,𝑎𝑡)\n\u00132\nwhere 𝜙represents the overall DRRN parameters, 𝑟𝑡is the reward\nat 𝑡-th time step, and 𝛾is the discount factor.\nTDQN: Template Deep Q-Network (TDQN) [7] like DRRN uses\nGRU encoders to encode information that consists of the state and\nprevious action in this case. It performs template-based action gen-\neration using three Q-values based on the observation 𝑜: 𝑄(𝑜,𝑢) for\nall action templates 𝑢∈𝑇, 𝑄(𝑜, 𝑝1) and 𝑄(𝑜, 𝑝2) for all vocabulary\n𝑝∈𝑉. 𝑝1, 𝑝2 fill-up the blanks in the template to generate candidate\nactions. The authors used a supervised binary-cross entropy loss\nto train the model for valid actions in addition to the TD loss.\n3.3\nLanguage Modeling (LM)\nThe textual modality of the IF games comes with additional chal-\nlenges for an RL agent. Since the modality of the games is natural\nlanguage, the modeling of the text-based observations and states be-\ncomes important, and this problem has not been explored in-depth\nin the RL research community. We employ DBERT [17] to address\nthis in our model. We fine-tune DBERT to model the language of\ngames. We take inspiration from one of the language modeling\ntechniques, masked language modeling (MLM), used to pre-train\nDBERT. Given a pair of observation𝑜𝑡and action 𝑎𝑡composed of to-\nkens [CLS],𝑜𝑡1,𝑜𝑡2, ...,𝑜𝑡𝑛, [SEP],𝑎𝑡1,𝑎𝑡2, ...,𝑎𝑡𝑚, random tokens are\nmasked by a special token, [MASK]. Here 𝑛is the length of the\nobservation, and 𝑚is the length of the action. The masked tokens\nare predicted from the vocabulary of the model. For example, given\nsequence “[CLS] Yes, there is a diorama at one end of the [MASK],\nbut you find yourself unable to [MASK] on it with that fascinating\nstatue sitting in the middle of the room. Once you figure out how it\nmoves, you’ll be able to concentrate on the [MASK] parts of the room.\n[SEP] remove boots\", the correct predictions would be “room\", “con-\ncentrate\" and “other\" respectively. A softmax activation function is\nused for masked token prediction. Cross entropy loss function is\nused to learn the parameters, i.e.,\n𝐿= CrossEntropy\n\u0010\n𝑥∗, 𝑃(𝑥[MASK]|𝑆)\n\u0011\nwhere 𝑥∗is the ground truth, 𝑥[MASK] is the predicted token, 𝑆is\nthe masked sentence, and 𝑃(𝑥[MASK]|𝑆) is the predicted distribution\nover the vocabulary. This training enables DBERT to tune its prior\nknowledge in the language of games.\n3.4\nRL with Pre-trained LM\nWe use the fine-tuned DBERT to model the input for the requisite\nRL agent. We encode the state and the actions separately using\nDBERT and feed the representations to the respective GRUs in\nDRRN and TDQN agent. The agents are then trained as described\nin 3.2 (Figure 2a and 2b). In this way, the trained DBERT is able\nto transfer its prior knowledge about the world to different agents\nacross different games. The agents, in turn, learn more informed\npolicies on the top of DBERT optimized for specific games. We use\na single instance of the trained DBERT and do not fine-tune DBERT\nfurther for specific games to keep the language model general and\nadaptable to any RL agent or game.\n4\nEXPERIMENTS\nWe evaluate how the addition of a pre-trained LM facilitates differ-\nent agents and games. We also present ablation studies to demon-\nstrate the need for training the pre-trained LM. Lastly, we present\na study on how our model is able to achieve new state-of-the-art\nresults on Zork1.\n4.1\nLM training\nWe use the ClubFloyd dataset [25] for fine-tuning DBERT. It is a\ncollection of human game-play trajectories on 590 games. These\nare not the optimal trajectories, but it does impart a general game-\nplay sense. We pre-process this data to obtain around 217K pairs\nof observation and action, (𝑜𝑡,𝑎𝑡). The transcripts are from games\nnot included in our evaluation (section 4.2) to keep our approach\ngeneral. We use a 768-dimensional vanilla DBERT (base cased)\nmodel pre-trained on English Wikipedia and Toronto Book Corpus.\nWe train this model for 2 epochs on pre-processed ClubFloyd dataset\nusing the MLM technique.\n4.2\nRL Training\nJericho Environment: We evaluate our agent on 14 games with\ndifferent difficulty levels2 using the Jericho framework, and use the\nsame Jericho handicaps as in the respective DRRN and TDQN base-\nlines [7]. The states are observations (𝑜𝑡) concatenated with items\nin possession of the player and its current location description pro-\nvided by the game engine using commands inventory and look. We\nalso use valid actions and action templates for DBERT-DRRN and\nDBERT-TDQN respectively, following the baselines. A single game\nepisode runs for 100 environment steps at max, or gets terminated\nbefore the game is over or won.\nTraining Setup: We test our framework on two RL models: DRRN\n(Figure 2a) and TDQN (Figure 2b). The GRUs used in the agents have\nan embedding dimension of 768 features and a hidden dimension of\n128 features. Similar to [7, 25] we collect interaction data for DBERT-\nDRRN and DBERT-TDQN on 8 parallel game environment instances\nand use a prioritized experience replay buffer to store trajectories\nwith best scores. We sample transitions from this priority buffer\nwith a priority fraction of 0.5, while taking the remaining training\ndata from a general replay buffer. Most importantly, we train our\nagents for a maximum of 5×104 steps, which is less than half of the\ntraining steps used in most other baselines, and achieve SOTA and\ncomparable scores on different games. Each training step makes\nan interaction with the environment in all 8 instances, therefore\nwe only use half the amount of interaction data for training in\ncomparison to the baselines. We use a softmax exploration policy\nand a learning rate of 10−4, along with other presets the same as the\nbaseline. We report the average of scores on the last 100 finished\nepisodes as the score on a game run. We train two independent runs\n2We choose games across varying difficulty level (possible\/easy, difficult, extreme) as\nindicated in Table 1. The difficulty level assigned to each game [7] is based on optimal\nsolution length, reward density, puzzle complexity, etc. Please refer to [7] for more\ndetails on difficulty level.\nfor each model and game, and report the average of their scores\nas the final (raw) score, along with the average maximum score\nseen by the runs. The maximum seen scores are a measure of the\nexploration ability, while the raw scores are that of the learning\nability of an agent. We report the learning curves for Zork1 in\nFigure 3.\n5\nRESULTS AND ANALYSIS\nWe performed a detailed analysis of our method. We present overall\ngame-scores as compared to baseline models across a set of games.\nWe also present an ablation study of our model done on Zork1, along\nwith a semantic understanding analysis. Finally, to actually interpret\nthe agent from inside beyond numbers, we also present a qualitative\nanalysis of our model on Zork1 via a case study discussing the\nfactors leading to higher score.\n5.1\nOverall Scores\nTable 1 reports the final scores of our best performing model (trained\nDBERT with DRRN) in comparison with 7 existing baselines on\n14 games. We report raw or maximum or both the scores as given\nin original papers. Different baselines achieve SOTA scores on\ndifferent games. Our model achieves SOTA results on 4 games:\nZork1, Inhumane, Snacktime, and Jewel, while being second best\nor comparable on most of the other games. We present the learn-\ning curves for Zork1, Inhumane, and Jewel in Figures 3, 4, and 5\nrespectively. INV-DY [24] uses additional loss objectives inspired\nfrom curiosity-driven exploration [13]. While it helps them achieve\nhigher maximum scores on Zork1, but are not able to learn the high\nscore trajectories. On the other hand, our agent efficiently learns the\nmax score trajectories explored by it, thereby indicating that with\na better exploration strategy our model has the potential to achieve\nbetter scores. None of the other agents, with a max score of 55, are\nable to stably reach a score as high as our model, that maintains a\nmargin of 6.4 from the best model [6]. Our agent explores higher\nmax score on Inhumane, but more importantly, it is able to learn\nthe best-explored trajectories, thereby plateauing closer to the max\nscores for many games (Inhumane, Jewel, Omniquest, Zork1),\nindicating that the trained DBERT is an important learning compo-\nnent, and it also facilitates the exploration to some extent. We also\nreport the normalized score (raw score as a factor of max possible\nscore collected from human-written optimal walk-through) aver-\naged across all games. We get an overall norm of 15.7%, followed\nby 14.2% achieved by MPRC-DQN [6]. Our model does not suffer a\nlot on any game, while the second best (MPRC-DQN) gets 0 scores\non Inhumane and Snacktime, and notably higher scores on others\n(Ludicorp, Spirit, Zork3). It indicates both the generalization\ntendency and the necessity of the pre-trained LM deployed in our\nmodel. When compared to the average norm of 18.9% for INV-DY\nevaluated on 6 of the 14 games they reported, we get 21.1%. The\naverage norm is also a measure of human-machine gap for text-\nbased games, indicating that IF games are at best only 15.7% solved.\nHence, it is a good benchmark for developing language understand-\ning agents. We have included the learning curves for independent\nruns on Zork1 and its game transcripts in the Appendix.\nTable 1: Final (raw) and max scores seen by agents (available via prior work) comparing trained DBERT-DRRN with all the\ncurrent SOTA baselines across a set of games from Jericho game-suite. For the baselines, we use the results reported in the\noriginal papers. The missing scores not in the previous work are denoted as ’-’. Max denotes the maximum possible scores\nbased on human-written optimal walkthroughs for winning the game without the step limit of 100. Note that our model\n(DBERT-DRRN) is trained for less than half of the training steps used in other baselines. Bold scores denote the best score,\nblue* scores denote the second best. DBERT-DRRN gets an avg. norm of 15.7% on all games, and 21.1% on 6 games reported\nfor INV-DY. *the games with rising learning curve for DBERT-DRRN till the last training step. 𝑝(possible\/easy), 𝑑(difficult),\n𝑒(extreme) refers to game difficulty level as given in [7].\nGame\nTDQN\nDRRN\nMPRC-DQN\nSHA-KG\nQ*BERT\nCALM\nINV-DY\nDBERT-DRRN\nMax\n[7]\n[7]\n[6]\n[23]\n[2]\n[25]\n[24]\n(ours)\nraw\nraw \/ max\nraw\nraw\nmax\nraw\nraw \/ max\nraw \/ max\nInhumane∗𝑝\n0.7\n0\n0\n5.4\n-\n25.7*\n19.6 \/ 45\n32.8 \/ 50\n90\nJewel𝑑\n0\n1.6\n4.46*\n1.8\n-\n0.3\n-\n6.5 \/ 13\n90\nLibrary∗𝑝\n6.3\n17.0*\n17.7\n15.8\n19\n9.0\n16.2 \/ 21\n17.0* \/ 21\n30\nLudicorp𝑑\n6\n13.8\n19.7\n17.8*\n22.8\n10.1\n13.5 \/ 23\n12.5 \/ 18\n150\nOmniquest𝑝\n16.8\n5\n10.0\n-\n-\n6.9*\n5.3 \/ 10\n4.9 \/ 5\n50\nReverb𝑝\n0.3\n8.2*\n2.0\n10.6\n-\n-\n-\n6.1 \/ 12\n50\nSnacktime𝑝\n9.7\n0 \/ 0.25\n0\n-\n-\n19.4*\n-\n20.0 \/ 20\n50\nSpellbrkr∗𝑒\n18.7\n37.8\n25\n40\n-\n40\n-\n38.2* \/ 40\n600\nSpirit𝑒\n0.6\n0.8\n3.8\n3.8\n-\n1.4\n-\n2.1* \/ 8\n250\nTemple𝑝\n7.9*\n7.4\n8.0\n7.9*\n8.0\n0\n-\n8.0 \/ 8.0\n35\nTryst205∗𝑒\n0\n9.6*\n10.0\n6.9\n-\n-\n9.3 \/ 17\n350\nZork1𝑑\n9.9\n32.6 \/ 53\n38.3\n34.5\n41.6\n30.4\n43.1* \/ 87\n44.7 \/ 55\n350\nZork3𝑑\n0\n0.5\n3.63\n0.7*\n-\n0.5\n0.4 \/ 4\n0.2 \/ 4\n7\nYomomma∗𝑑\n0\n0.4\n1.0\n-\n-\n-\n-\n0.5* \/ 1.0\n35\nAvg. Norm (%)\n7.7\n10.2\n14.2\n13.3\n-\n12.8\n18.9\n15.7 (21.1)\n100\n5.2\nDBERT Training Ablation\nOur framework includes a DBERT trained on human trajectories\nfrom different games. This training induces a language acquaintance\nwith games. We tested empirically how this affected the model.\nWe obtain runs on both DRRN and TDQN based models for the\nvanilla DBERT and DBERT pretrained on games from ClubFloyd\ndataset. We present the results in Figure 3 where we see DRRN\nwith pretrained DBERT outperforms DRRN with vanilla DBERT\nwith a final score of 44.72 compared to 34.73. The TDQN based\nmodels do not exhibit much difference, but pretrained version still\nhas a higher score of 10.35, whereas vanilla DBERT version of the\nframework obtains 9.85. It indicates the importance of acquainting\nthe pre-trained LM with the game language, so that it can transfer\nits prior knowledge appropriately to the agents.\n5.3\nSemantic Understanding: Transfer\nYao et al. [24] show that “TD loss + valid action handicap\" setup\nwith mostly deterministic rewards can lead to overfitting and mem-\norization, thus, hindering the actual goal of understanding and\nlearning to operate in natural language. Therefore, we also test\nwhether an agent trained for one game can transfer its learning to\nother similar games to see if the agent is learning the semantics\nor memorizing the trajectories. We train an agent on Zork1 and\ntest it without any training on Zork3, as these are the two most\nsimilar games in the Jericho game-suite. The average episode score\nover 300 episodes was 0.06 for DBERT-DRRN and 0.007 for DRRN.\nThese scores are not as high as achieved by training fresh agents\non Zork3; however, DBERT-DRRN is able to transfer its knowledge\nbetter than DRRN.\n5.4\nAvoiding being eaten by a grue and\nthereafter\nWe compare scores over an episode for (vanilla and trained) DBERT-\nDRRN and DRRN. We sample 100 random episodes from the last\nfew hundred episodes during the training, and plot the average\nscores over an episode in Figure 6. An episode is set to terminate\nafter 100 steps or before (if the game is over or won). The goal in\nZork1 is to collect 19 treasures in the trophy case, which agents are\nnot aware of, and they learn via in-game rewards. Existing agents\ntraverse the Forest for the Egg (1st treasure), followed by Kitchen,\n0\n100\n200\n300\n400\nSteps (x100)\n10\n0\n10\n20\n30\n40\n50\nLast100EpisodeScores\ntdqn + vanilla_dbert\ntdqn + trained_dbert\ndrrn + vanilla_dbert\ndrrn + trained_dbert\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n10\n0\n10\n20\n30\n40\n50\nMax score seen\ntdqn + vanilla_dbert\ntdqn + trained_dbert\ndrrn + vanilla_dbert\ndrrn + trained_dbert\n(b)\nFigure 3: DBERT performance ablation results on Zork1\n0\n100\n200\n300\n400\nSteps (x100)\n0\n5\n10\n15\n20\n25\n30\nLast100EpisodeScores\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nMax score seen\n(b)\nFigure 4: DBERT-DRRN performance results on Inhumane\nCellar, and Gallery for the painting (next treasure), while solving\nvarious puzzle and escaping the grue. In DRRN, the agent gets to\nthe Kitchen with a reward of +10 at around step-10 and then to\nthe Cellar with a reward of +25 around step-15 as we can see in\n0\n50\n100\n150\n200\n250\n300\n350\n400\nSteps (x100)\n1\n2\n3\n4\n5\n6\n7\nLast100EpisodeScores\n(a)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nSteps (x100)\n4\n6\n8\n10\n12\nMax score seen\n(b)\nFigure 5: DBERT-DRRN performance results on Jewel\nthe plot. It doesn’t learn to take and carry the Egg for a reward\nof +5 (and another +5 for putting it in the case) before moving\nto the Kitchen even though the observations present the Egg as\nsomething precious “..in the bird’s nest is a large egg encrusted with\n0\n20\n40\n60\n80\n100\nSteps\n0\n10\n20\n30\n40\n50\nAverage scores over an episode\ndrrn\ndrrn + vanilla_dbert\ndrrn + trained_dbert\nFigure 6: Avoiding being eaten by a grue: Average scores over\nan episode for Zork1\nprecious jewels, apparently scavenged by a childless songbird. the\negg is covered with fine gold inlay,..\". Ammanabrolu et al. [2] use an\nintrinsic motivation reward in addition to the rewards provided by\nthe game engine, in order to get past when stuck in the game, i.e.,\nwhen the observations do not change. This motivation also helps\nthem provide rewards to add dependencies to their KG for the Egg.\nHowever, the LM in trained DBERT-DRRN guesses that the Egg\nis important without any extra supervision, and takes it around\nstep-10, hence, achieving higher scores later on. Moreover, trained\nDBERT-DRRN avoids being eaten by a grue in the Cellar which says\n“the trap door crashes shut, and you hear someone barring it. it is pitch\nblack. you are likely to be eaten by a grue.\" by instantly turning on\nthe lantern without any reward, while DRRN takes random actions\nat this point and dies. Then our agent finds the way to the gallery,\nand gets the painting for +4 points. Thus, merely improving the\nunderstanding of the agent enables it to take better actions without\nany additional supervision. A part of a sample episode transcript\nis shown in Figure 1, and the complete episode is provided in the\nAppendix.\nIn summary, the pre-trained LM enables the DBERT-DRRN agent\nto extract reward cues from the observations as well as use its\npriors on world sense and game sense to get past the grue. We\nobserved that the Vanilla-DBERT is only as good as the base DRRN\nagent. Training it on a set of gameplays is improving the model\nconsiderably, indicating the importance of this training, which is\nessentially channeling the world sense of Vanilla-DBERT into a\ngameplay mode. The DBERT-DRRN agent understands that a “jewel\nencrusted egg appearing extremely fragile\" should be taken, and\nshould not be thrown, or if it’s dark the lantern should be turned\non. Trained DBERT-DRRN assigns reasonably high Q-values to\nsuch actions in the valid action set, while DRRN assigns similar\nvalues when rewards are absent for the same (as can be seen in\nthe trajectories provided in the Appendix). Both DBERT-DRRN and\nall other baselines are able to reach the gallery. Our agent does so\nmore often as it escapes the grue in the Cellar using the lantern.\nMoreover, it extracts intermediate rewards cues and ends up with\na higher score at the end of an episode. The baselines with a score\naround 39 or less are the ones which could not learn these aspects\nof the game. After getting the painting, the agents explore several\noptions, but none of them, including ours, are able to find and learn\nto find the third treasure. Moving past trained DBERT-DRRN score\nwill likely require a more intelligent agent with better exploration\nand learning strategies.\n6\nCONCLUSION AND DISCUSSION\nThis paper proposes using a pre-trained LM fine-tuned on game\ndynamics for RL agents trained for text-based games. It provides\nthree-fold benefits to the RL agent: linguistic priors, world sense\npriors, and game sense priors. It facilitates the agent to achieve\nSOTA results on several text-based games, even though being a\nsimpler approach than all other baselines. The proposed approach\nindicates the importance of using pre-trained LMs for RL agents\nin text-based games. Through this work, we want to draw the\nresearch community’s attention and motivate research in this direc-\ntion to create even better priors using pre-trained LM for language\nunderstanding agents, such as distilling its knowledge to agents\nwith better architectures. Such unsupervised components for explo-\nration and learning will also be more useful in real-life scenarios\nwhere there are no explicit determinate rewards than reward-based\nrepresentation learning. We have shown that training RL agents\nfor text-based games in the absence of a prior is inefficient. The\ntwo key reasons are: learning language from scratch using the lim-\nited text data with a TD loss or related objective does not produce\nrich enough representations, and the agent has no source of world\nknowledge to be able to take appropriate actions given the state.\nThe agents thus trained are not really understanding their state\nin the game, leading to the suspicion that the agents are simply\nmemorizing the best-explored trajectories [24].\nIF games are still far from solved. We need agents with better\nexploration strategies (for instance curiosity-driven learning [13])\nto find high score trajectories, as well as learning architectures\nand objectives that can facilitate learning these trajectories while\nefficiently leveraging priors from pre-trained language models. Cur-\nrent results show the performance of the agents trained on the same\ngame environments. Another line of future work is to test trained\nagents on unseen games. This will require a strong understanding\nof the world functioning and generalized training strategies to be\nable to acquire skills from multiple games and transfer them to\nthe unseen test environments. This will also be a robust test-bed\nfor semantic understanding of the agent, as memorization will no\nlonger make any points in unseen games. Leveraging commonsense\nknowledge for text-based game agents can be another interesting\ndirection for improving the knowledge priors. Many actions taken\nduring these games are associated with object affordances. With a\ncommonsense knowledge source, the agent will be able to extract\nrational interaction possibilities with the objects.\nREFERENCES\n[1] Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph Constrained\nReinforcement Learning for Natural Language Action Spaces. In International\nConference on Learning Representations.\nhttps:\/\/openreview.net\/forum?id=\nB1x6w0EtwH\n[2] Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, and Mark Riedl.\n2021. How to Avoid Being Eaten by a Grue: Structured Exploration Strategies\nfor Textual Worlds. https:\/\/openreview.net\/forum?id=eYgI3cTPTq9\n[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael\nLaskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision\nTransformer: Reinforcement Learning via Sequence Modeling. arXiv preprint\narXiv:2106.01345 (2021).\n[4] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes,\nEmery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri,\nMahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld: A Learning\nEnvironment for Text-based Games. CoRR abs\/1806.11532 (2018).\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL]\n[6] Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu\nChang. 2020. Interactive Fiction Game Playing as Multi-Paragraph Reading Com-\nprehension with Reinforcement Learning. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Online, 7755–7765. https:\/\/doi.org\/10.18653\/v1\/2020.\nemnlp-main.624\n[7] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and\nXingdi Yuan. 2020. Interactive Fiction Games: A Colossal Adventure. Proceedings\nof the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020), 7903–7910.\nhttps:\/\/doi.org\/10.1609\/aaai.v34i05.6297\n[8] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari\nOstendorf. 2016. Deep Reinforcement Learning with a Natural Language Action\nSpace. In Proceedings of the 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers). Association for Computational\nLinguistics, Berlin, Germany, 1621–1630. https:\/\/doi.org\/10.18653\/v1\/P16-1153\n[9] Michael Janner, Qiyang Li, and Sergey Levine. 2021. Reinforcement Learning as\nOne Big Sequence Modeling Problem. arXiv:2106.02039 [cs.LG]\n[10] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. arXiv:1909.11942 [cs.CL]\n[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n[12] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Sid-\ndhant Jayakumar, Max Jaderberg, Raphaël Lopez Kaufman, Aidan Clark, Seb\nNoury, Matthew Botvinick, Nicolas Heess, and Raia Hadsell. 2020. Stabiliz-\ning Transformers for Reinforcement Learning. In Proceedings of the 37th In-\nternational Conference on Machine Learning (Proceedings of Machine Learning\nResearch, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, 7487–7498.\nhttp:\/\/proceedings.mlr.press\/v119\/parisotto20a.html\n[13] Deepak\nPathak,\nPulkit\nAgrawal,\nAlexei\nA.\nEfros,\nand\nTrevor\nDar-\nrell. 2017.\nCuriosity-driven Exploration by Self-supervised Prediction.\narXiv:1705.05363 [cs.LG]\n[14] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Doha, Qatar, 1532–1543. https:\/\/doi.org\/10.3115\/v1\/\nD14-1162\n[15] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong,\nChina, 2463–2473. https:\/\/doi.org\/10.18653\/v1\/D19-1250\n[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nblog 1, 8 (2019), 9.\n[17] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs.CL]\n[18] Sushant Singh and Ausif Mahmood. 2021. The NLP Cookbook: Modern Recipes\nfor Transformer Based Deep Learning Architectures. IEEE Access 9 (2021), 68675–\n68702.\n[19] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction. MIT press.\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL]\n[21] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. arXiv preprint arXiv:1804.07461 (2018).\n[22] Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, and Chengqi Zhang. 2020. Deep\nReinforcement Learning with Transformers for Text Adventure Games. In 2020\nIEEE Conference on Games (CoG). 65–72. https:\/\/doi.org\/10.1109\/CoG47356.2020.\n9231622\n[23] Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi\nZhang. 2020. Deep Reinforcement Learning with Stacked Hierarchical Attention\nfor Text-based Games. In Advances in Neural Information Processing Systems,\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33.\nCurran Associates, Inc., 16495–16507.\nhttps:\/\/proceedings.neurips.cc\/paper\/\n2020\/file\/bf65417dcecc7f2b0006e1f5793b7143-Paper.pdf\n[24] Shunyu Yao, Karthik Narasimhan, and Matthew Hausknecht. 2021. Reading and\nActing while Blindfolded: The Need for Semantics in Text Game Agents. In North\nAmerican Association for Computational Linguistics (NAACL).\n[25] Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020.\nKeep CALM and Explore: Language Models for Action Generation in Text-based\nGames. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP). Association for Computational Linguistics, Online,\n8736–8754. https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.704\nAPPENDIX\nA\nLEARNING CURVES\nThe learning curves for Zork1 are shown in Figure 7.\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nLast100EpisodeScores\nrun1\nrun2\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n10\n20\n30\n40\n50\nMax score seen\nrun1\nrun2\n(b)\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nEpisodeScore\nrun1\nrun2\n(c)\n0\n100\n200\n300\n400\nSteps (x100)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nLoss\nrun1\nrun2\n(d)\nFigure 7: Results for Zork1: Two independent runs of\n(DBERT-DRRN)\nB\nGAMEPLAY TRANSCRIPTS FOR TRAINED\nDBERT-DRRN\nThe transcripts presents the game-play over an episode of 100 steps.\nFor each step, the game state (observation, inventory, description),\nlist of valid actions and corresponding Q-Values estimated by the\nagent, the action taken, reward received, cumulative score, and\nwhether the episode is over (Done) is given.\nList of transcripts:\n• D.1 Zork1\n– D.1.1 DRRN (for comparison)\n• D.2 Inhumane\nB.1\nZork1\nSTATE 0\nCopyright ( c ) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\nZORK is a registered trademark of Infocom, Inc. Revision 88 \/ Se-\nrial number 840726 West of House You are standing in an open\nfield west of a white house, with a boarded front door. There is a\nsmall mailbox here. You are empty handed. West of House You are\nstanding in an open field west of a white house, with a boarded\nfront door. There is a small mailbox here. Actions: [’west’, ’north’,\n’south’, ’open mailbox’] Qvalues: [9.07, 7.74, 7.42, 5.98] Action: west\nReward: 0, Score 0, Done False\nSTATE 1\nforest this is a forest, with trees in all directions. to the east, there\nappears to be sunlight. you are empty handed. forest this is a forest,\nwith trees in all directions. to the east, there appears to be sunlight.\nActions: [’east’, ’go around trees’, ’north’, ’south’] Qvalues: [10.05,\n7.14, 6.8, 6.64] Action: south Reward: 0, Score 0, Done False\nSTATE 2\nforest this is a dimly lit forest, with large trees all around. you are\nempty handed. forest this is a dimly lit forest, with large trees all\naround. Actions: [’northwest’, ’north’, ’go around trees’, ’west’]\nQvalues: [6.59, 6.52, 6.1, 5.37] Action: northwest Reward: 0, Score 0,\nDone False\nSTATE 3\nsouth of house you are facing the south side of a white house. there\nis no door here, and all the windows are boarded. you are empty\nhanded. south of house you are facing the south side of a white\nhouse. there is no door here, and all the windows are boarded. Ac-\ntions: [’east’, ’south’, ’west’] Qvalues: [7.71, 6.15, 5.75] Action: east\nReward: 0, Score 0, Done False\nSTATE 4\nbehind house you are behind the white house. a path leads into the\nforest to the east. in one corner of the house there is a small window\nwhich is slightly ajar. you are empty handed. behind house you are\nbehind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is slightly\najar. Actions: [’open window’, ’east’, ’north’, ’south’] Qvalues: [9.39,\n7.99, 7.91, 7.4] Action: east Reward: 0, Score 0, Done False\nSTATE 5\nclearing you are in a small clearing in a well marked forest path\nthat extends to the east and west. you are empty handed. clearing\nyou are in a small clearing in a well marked forest path that extends\nto the east and west. Actions: [’west’, ’north’, ’east’, ’south’, ’go\naround forest’] Qvalues: [7.97, 6.53, 6.35, 6.21, 5.76] Action: north\nReward: 0, Score 0, Done False\nSTATE 6\nforest this is a dimly lit forest, with large trees all around. you are\nempty handed. you hear in the distance the chirping of a song bird.\nforest this is a dimly lit forest, with large trees all around. you hear\nin the distance the chirping of a song bird. Actions: [’south’, ’go\naround trees’, ’west’, ’east’] Qvalues: [6.83, 6.45, 6.22, 5.98] Action:\nwest Reward: 0, Score 0, Done False\nSTATE 7\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you are empty handed.\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you hear in the dis-\ntance the chirping of a song bird. Actions: [’up’, ’go around forest’,\n’south’, ’east’, ’north’, ’west’] Qvalues: [10.91, 8.65, 8.4, 7.61, 7.09,\n5.01] Action: up Reward: 0, Score 0, Done False\nSTATE 8\nup a tree you are about 10 feet above the ground nestled among\nsome large branches. the nearest branch above you is above your\nreach. beside you on the branch is a small bird’s nest. in the bird’s\nnest is a large egg encrusted with precious jewels, apparently scav-\nenged by a childless songbird. the egg is covered with fine gold\ninlay, and ornamented in lapis lazuli and mother of pearl. unlike\nmost eggs, this one is hinged and closed with a delicate looking\nclasp. the egg appears extremely fragile. you are empty handed. up\na tree you are about 10 feet above the ground nestled among some\nlarge branches. the nearest branch above you is above your reach.\nbeside you on the branch is a small bird’s nest. in the bird’s nest is\na large egg encrusted with precious jewels, apparently scavenged\nby a childless songbird. the egg is covered with fine gold inlay, and\nornamented in lapis lazuli and mother of pearl. unlike most eggs,\nthis one is hinged and closed with a delicate looking clasp. the egg\nappears extremely fragile. Actions: [’take egg’, ’close nest’, ’take\nnest’, ’take on egg’, ’down’] Qvalues: [11.59, 4.09, 2.25, 0.82, -4.72]\nAction: take egg Reward: 5, Score 5, Done False\nSTATE 9\ntaken. you are carrying : a jewel encrusted egg up a tree you are\nabout 10 feet above the ground nestled among some large branches.\nthe nearest branch above you is above your reach. beside you on the\nbranch is a small bird’s nest. you hear in the distance the chirping\nof a song bird. Actions: [’down’, ’close nest’, ’put egg in nest’, ’take\nnest’, ’take on egg’, ’throw egg at nest’, ’put down egg’] Qvalues:\n[8.07, 6.47, 5.62, 5.51, 5.29, 5.04, 4.88] Action: down Reward: 0, Score\n5, Done False\nSTATE 10\nforest path you are carrying : a jewel encrusted egg forest path this\nis a path winding through a dimly lit forest. the path heads north\nsouth here. one particularly large tree with some low branches\nstands at the edge of the path. you hear in the distance the chirping\nof a song bird. Actions: [’south’, ’go around forest’, ’north’, ’east’,\n’west’, ’open egg with all’, ’up’, ’take on egg’, ’throw egg at tree’,\n’put down egg’] Qvalues: [8.94, 8.82, 8.04, 7.9, 7.49, 7.45, 7.02, 6.23,\n6.19, 5.91] Action: east Reward: 0, Score 5, Done False\nSTATE 11\nforest you are carrying : a jewel encrusted egg you hear in the\ndistance the chirping of a song bird. forest this is a dimly lit for-\nest, with large trees all around. Actions: [’south’, ’go around trees’,\n’east’, ’west’, ’open egg with all’, ’throw egg at large’, ’take on egg’,\n’put down egg’] Qvalues: [8.72, 7.81, 7.81, 7.49, 7.4, 6.49, 6.14, 6.09]\nAction: go around trees Reward: 0, Score 5, Done False\nSTATE 12\nforest you are carrying : a jewel encrusted egg forest this is a dimly\nlit forest, with large trees all around. Actions: [’northwest’, ’north’,\n’go around trees’, ’west’, ’open egg with all’, ’take on egg’, ’throw\negg at large’, ’put down egg’] Qvalues: [8.56, 8.06, 7.97, 7.58, 7.44,\n6.38, 6.35, 6.14] Action: northwest Reward: 0, Score 5, Done False\nSTATE 13\nsouth of house you are carrying : a jewel encrusted egg south of\nhouse you are facing the south side of a white house. there is no\ndoor here, and all the windows are boarded. Actions: [’east’, ’south’,\n’open egg with all’, ’west’, ’take on egg’, ’put down egg’, ’throw egg\nat house’] Qvalues: [9.39, 8.03, 7.8, 7.47, 5.81, 5.77, 5.59] Action: east\nReward: 0, Score 5, Done False\nSTATE 14\nbehind house you are carrying : a jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nslightly ajar. Actions: [’open window’, ’open egg with all’, ’south’,\n’east’, ’north’, ’put down egg’, ’take on egg’, ’throw egg at path’]\nQvalues: [10.97, 9.79, 9.03, 8.74, 8.71, 7.78, 7.72, 6.85] Action: open\nwindow Reward: 0, Score 5, Done False\nSTATE 15\nwith great effort, you open the window far enough to allow entry.\nyou are carrying : a jewel encrusted egg behind house you are\nbehind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is open.\nActions: [’west’, ’close window’, ’open egg with all’, ’east’, ’south’,\n’north’, ’put down egg’, ’take on egg’, ’throw egg at path’] Qvalues:\n[11.56, 9.85, 9.56, 9.54, 9.12, 8.97, 7.34, 6.58, 6.2] Action: east Reward:\n0, Score 5, Done False\nSTATE 16\nclearing you are carrying : a jewel encrusted egg clearing you are\nin a small clearing in a well marked forest path that extends to the\neast and west. Actions: [’west’, ’open egg with all’, ’east’, ’south’,\n’north’, ’go around forest’, ’put down egg’, ’throw egg at forest’,\n’take on egg’] Qvalues: [9.27, 8.16, 8.13, 7.81, 7.76, 7.49, 6.35, 6.08,\n5.98] Action: west Reward: 0, Score 5, Done False\nSTATE 17\nbehind house you are carrying : a jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nopen. Actions: [’open egg with all’, ’close window’, ’west’, ’east’,\n’south’, ’north’, ’put down egg’, ’take on egg’, ’throw egg at path’]\nQvalues: [9.71, 9.64, 9.54, 8.65, 8.58, 8.54, 7.81, 7.27, 6.8] Action: west\nReward: 10, Score 15, Done False\nSTATE 18\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains : a\nquantity of water you are carrying : a jewel encrusted egg kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads\nto the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water Actions: [’west’, ’close window’, ’open sack’,\n’east’, ’open bottle’, ’take sack’, ’put down egg’, ’put egg on ta-\nble’, ’throw egg at bottle’, ’take on egg’, ’take bottle’, ’take all’, ’up’,\n’jump’] Qvalues: [21.67, 9.94, 9.52, 9.26, 8.74, 7.63, 7.6, 7.41, 6.13, 5.58,\n4.72, 3.89, -2.46, -11.57] Action: west Reward: 0, Score 15, Done False\nSTATE 19\nliving room you are in the living room. there is a doorway to the\neast, a wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a large oriental rug\nin the center of the room. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on the\ntrophy case. you are carrying : a jewel encrusted egg living room\nyou are in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a large oriental rug in the center\nof the room. above the trophy case hangs an elvish sword of great\nantiquity. a battery powered brass lantern is on the trophy case.\nActions: [’take lantern’, ’push rug’, ’open case’, ’turn on lantern’,\n’throw egg at case’, ’throw egg at lantern’, ’take sword’, ’put down\negg’, ’take on egg’, ’take all’, ’east’] Qvalues: [25.17, 25.08, 24.82,\n20.51, 20.26, 20.21, 17.84, 17.42, 17.37, 13.44, 13.38] Action: push rug\nReward: 0, Score 15, Done False\nSTATE 20\nwith a great effort, the rug is moved to one side of the room, re-\nvealing the dusty cover of a closed trap door. you are carrying : a\njewel encrusted egg living room you are in the living room. there is\na doorway to the east, a wooden door with strange gothic lettering\nto the west, which appears to be nailed shut, a trophy case, and a\nclosed trap door at your feet. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on the\ntrophy case. Actions: [’take lantern’, ’open case’, ’open trap’, ’turn\non lantern’, ’throw egg at case’, ’throw egg at lantern’, ’take sword’,\n’take on egg’, ’put down egg’, ’take all’, ’east’] Qvalues: [27.57, 26.49,\n26.36, 22.97, 22.6, 21.73, 19.57, 19.16, 19.04, 15.18, 13.41] Action: open\ncase Reward: 0, Score 15, Done False\nSTATE 21\nopened. you are carrying : a jewel encrusted egg living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity. a\nbattery powered brass lantern is on the trophy case. Actions: [’put\negg in case’, ’take lantern’, ’open trap’, ’close case’, ’turn on lantern’,\n’throw egg at case’, ’throw egg at lantern’, ’fill case with egg’, ’take\nsword’, ’take on egg’, ’put down egg’, ’take all’, ’east’] Qvalues:\n[27.91, 27.9, 25.05, 24.38, 23.28, 22.29, 21.63, 20.13, 19.53, 19.27, 18.89,\n15.31, 12.56] Action: take lantern Reward: 0, Score 15, Done False\nSTATE 22\ntaken. you are carrying : a brass lantern a jewel encrusted egg living\nroom you are in the living room. there is a doorway to the east,\na wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a closed trap door at\nyour feet. above the trophy case hangs an elvish sword of great\nantiquity. Actions: [’put egg in case’, ’open trap’, ’close case’, ’open\negg with lantern’, ’put all in case’, ’turn on lantern’, ’throw egg at\ncase’, ’put lantern in case’, ’put down lantern’, ’fill case with egg’,\n’take sword’, ’throw lantern at case’, ’take on egg’, ’throw lantern\nat egg’, ’put down all’, ’put down egg’, ’east’] Qvalues: [30.7, 28.07,\n27.18, 26.67, 24.57, 23.87, 22.85, 22.05, 21.95, 21.19, 20.45, 20.32, 19.95,\n19.49, 19.48, 19.32, 14.5] Action: put egg in case Reward: 5, Score\n20, Done False\nSTATE 23\ndone. you are carrying : a brass lantern living room you are in the\nliving room. there is a doorway to the east, a wooden door with\nstrange gothic lettering to the west, which appears to be nailed shut,\na trophy case, and a closed trap door at your feet. above the trophy\ncase hangs an elvish sword of great antiquity. your collection of\ntreasures consists of : a jewel encrusted egg Actions: [’open trap’,\n’close case’, ’open egg with lantern’, ’take egg from sword’, ’turn on\nlantern’, ’take egg’, ’put down lantern’, ’put lantern in case’, ’throw\nlantern at case’, ’take sword’, ’throw lantern at egg’, ’take on egg’,\n’east’] Qvalues: [30.74, 27.85, 27.84, 27.56, 25.61, 24.67, 24.36, 23.05,\n22.62, 22.33, 21.85, 21.0, 15.86] Action: open trap Reward: 0, Score\n20, Done False\nSTATE 24\nthe door reluctantly opens to reveal a rickety staircase descending\ninto darkness. you are carrying : a brass lantern living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a rug lying beside an open trap\ndoor. above the trophy case hangs an elvish sword of great antiq-\nuity. your collection of treasures consists of : a jewel encrusted egg\nActions: [’down’, ’open egg with lantern’, ’close case’, ’take egg\nfrom sword’, ’close trap’, ’turn on lantern’, ’take egg’, ’put down\nlantern’, ’put lantern in case’, ’throw lantern at case’, ’take sword’,\n’throw lantern at egg’, ’take on egg’, ’east’] Qvalues: [32.52, 27.82,\n27.35, 27.33, 26.53, 26.21, 24.31, 24.03, 23.0, 22.09, 21.81, 21.24, 20.86,\n15.54] Action: down Reward: 25, Score 45, Done False\nSTATE 25\nyou have moved into a dark place. the trap door crashes shut, and\nyou hear someone barring it. it is pitch black. you are likely to be\neaten by a grue. you are carrying : a brass lantern it is pitch black.\nyou are likely to be eaten by a grue. Actions: [’turn on lantern’, ’put\ndown lantern’, ’throw lantern at grue’, ’south’, ’east’, ’north’] Qval-\nues: [7.76, -1.33, -3.82, -4.74, -7.48, -10.65] Action: turn on lantern\nReward: 0, Score 45, Done False\nSTATE 26\nthe brass lantern is now on. cellar you are in a dark and damp cellar\nwith a narrow passageway leading north, and a crawlway to the\nsouth. on the west is the bottom of a steep metal ramp which is un-\nclimbable. you are carrying : a brass lantern ( providing light ) cellar\nyou are in a dark and damp cellar with a narrow passageway lead-\ning north, and a crawlway to the south. on the west is the bottom of\na steep metal ramp which is unclimbable. Actions: [’south’, ’put out\nlantern’, ’put down lantern’, ’north’, ’throw lantern at crawlway’]\nQvalues: [8.37, 4.86, 4.5, 4.2, 0.44] Action: south Reward: 0, Score\n45, Done False\nSTATE 27\neast of chasm you are on the east edge of a chasm, the bottom\nof which cannot be seen. a narrow passage goes north, and the\npath you are on continues to the east. you are carrying : a brass\nlantern ( providing light ) east of chasm you are on the east edge\nof a chasm, the bottom of which cannot be seen. a narrow passage\ngoes north, and the path you are on continues to the east. Actions:\n[’east’, ’north’, ’put out lantern’, ’put down lantern’, ’throw lantern\nat path’, ’jump’] Qvalues: [9.28, 4.51, 4.16, 3.46, 0.51, -7.56] Action:\neast Reward: 0, Score 45, Done False\nSTATE 28\ngallery this is an art gallery. most of the paintings have been stolen\nby vandals with exceptional taste. the vandals left through either\nthe north or west exits. fortunately, there is still one chance for\nyou to be a vandal, for on the far wall is a painting of unparalleled\nbeauty. you are carrying : a brass lantern ( providing light ) gallery\nthis is an art gallery. most of the paintings have been stolen by\nvandals with exceptional taste. the vandals left through either the\nnorth or west exits. fortunately, there is still one chance for you to\nbe a vandal, for on the far wall is a painting of unparalleled beauty.\nActions: [’take art’, ’put out lantern’, ’put down lantern’, ’throw\nlantern at art’, ’north’, ’west’] Qvalues: [10.35, 10.21, 8.89, 6.58, 6.06,\n3.92] Action: take art Reward: 4, Score 49, Done False\nSTATE 29\ntaken. you are carrying : a painting a brass lantern ( providing\nlight ) gallery this is an art gallery. most of the paintings have been\nstolen by vandals with exceptional taste. the vandals left through\neither the north or west exits. Actions: [’north’, ’put out lantern’,\n’put down lantern’, ’put down art’, ’west’, ’put down all’, ’throw art\nat lantern’] Qvalues: [7.74, 7.08, 6.64, 6.36, 6.36, 6.35, 4.02] Action:\nnorth Reward: 0, Score 49, Done False\nSTATE 30\nstudio this appears to have been an artist’s studio. the walls and\nfloors are splattered with paints of 69 different colors. strangely\nenough, nothing of value is hanging here. at the south end of the\nroom is an open door ( also covered with paint ). a dark and narrow\nchimney leads up from a fireplace ; although you might be able\nto get up it, it seems unlikely you could get back down. loosely\nattached to a wall is a small piece of paper. you are carrying : a\npainting a brass lantern ( providing light ) studio this appears to\nhave been an artist’s studio. the walls and floors are splattered with\npaints of 69 different colors. strangely enough, nothing of value is\nhanging here. at the south end of the room is an open door ( also\ncovered with paint ). a dark and narrow chimney leads up from a\nfireplace ; although you might be able to get up it, it seems unlikely\nyou could get back down. loosely attached to a wall is a small piece\nof paper. Actions: [’up’, ’put out lantern’, ’south’, ’put down lantern’,\n’take manual’, ’put down painting’, ’put down all’, ’throw lantern at\nmanual’] Qvalues: [8.6, 7.77, 6.81, 6.69, 6.59, 6.38, 6.37, 4.9] Action:\nput out lantern Reward: 0, Score 49, Done False\nSTATE 31\nthe brass lantern is now off. it is now pitch black. you are carrying\n: a painting a brass lantern it is pitch black. you are likely to be\neaten by a grue. Actions: [’up’, ’turn on lantern’, ’south’, ’put down\nlantern’, ’put down all’, ’put down painting’, ’throw lantern at grue’,\n’north’] Qvalues: [8.42, 5.97, 5.91, 4.04, 3.45, 3.42, 2.89, 1.35] Action:\nup Reward: 0, Score 49, Done False\nSTATE 32\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water you are carrying : a painting a brass lantern\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water Actions: [’west’, ’open bottle’, ’close window’,\n’open sack’, ’east’, ’put lantern on table’, ’take sack’, ’put down\nlantern’, ’throw lantern at bottle’, ’turn on lantern’, ’put down all’,\n’put down painting’, ’take bottle’, ’take all’, ’put painting on table’,\n’up’, ’jump’] Qvalues: [10.1, 9.01, 8.93, 8.91, 8.41, 8.23, 8.14, 8.08,\n7.97, 7.87, 7.63, 7.56, 7.29, 6.6, 6.56, 5.92, -2.09] Action: open bottle\nReward: 0, Score 49, Done False\nSTATE 33\nopened. you are carrying : a painting a brass lantern kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains :\na quantity of water Actions: [’west’, ’open sack’, ’close window’,\n’close bottle’, ’east’, ’put lantern on table’, ’take sack’, ’throw lantern\nat bottle’, ’put down lantern’, ’turn on lantern’, ’put down all’, ’put\ndown painting’, ’take bottle’, ’put painting on table’, ’take all’, ’up’,\n’jump’] Qvalues: [10.07, 9.15, 9.04, 8.95, 8.73, 8.54, 8.45, 8.35, 8.34,\n8.17, 7.95, 7.73, 7.63, 6.95, 6.95, 5.69, -2.51] Action: put lantern on\ntable Reward: 0, Score 49, Done False\nSTATE 34\ndone. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is\n: a brass lantern a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’take lantern’, ’open sack’, ’close bottle’,\n’east’, ’take sack’, ’close window’, ’throw painting at lantern’, ’turn\non lantern’, ’take bottle’, ’put down painting’, ’take all’, ’up’, ’jump’]\nQvalues: [9.0, 8.75, 8.31, 8.18, 8.11, 8.08, 8.02, 7.95, 7.84, 7.16, 7.12,\n6.95, 4.29, -4.68] Action: close bottle Reward: 0, Score 49, Done False\nSTATE 35\nclosed. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is\n: a brass lantern a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’take lantern’, ’open bottle’, ’open sack’,\n’close window’, ’take sack’, ’east’, ’throw painting at lantern’, ’turn\non lantern’, ’take bottle’, ’put down painting’, ’take all’, ’up’, ’jump’]\nQvalues: [9.01, 8.76, 8.58, 8.53, 8.18, 8.18, 8.17, 7.97, 7.94, 7.29, 7.17,\n7.02, 4.29, -4.77] Action: take lantern Reward: 0, Score 49, Done False\nSTATE 36\ntaken. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’west’, ’open bottle’, ’open sack’, ’close\nwindow’, ’east’, ’put lantern on table’, ’take sack’, ’throw lantern\nat bottle’, ’put down lantern’, ’turn on lantern’, ’put down all’, ’put\ndown painting’, ’take bottle’, ’take all’, ’put painting on table’, ’up’,\n’jump’] Qvalues: [9.82, 9.15, 9.08, 9.02, 8.61, 8.37, 8.31, 8.15, 8.15, 8.07,\n7.82, 7.61, 7.53, 6.82, 6.8, 6.16, -1.96] Action: open bottle Reward: 0,\nScore 49, Done False\nSTATE 37\nopened. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains :\na quantity of water Actions: [’west’, ’open sack’, ’close window’,\n’close bottle’, ’east’, ’put lantern on table’, ’take sack’, ’put down\nlantern’, ’throw lantern at bottle’, ’turn on lantern’, ’put down all’,\n’take bottle’, ’put down painting’, ’take all’, ’put painting on table’,\n’up’, ’jump’] Qvalues: [9.93, 9.02, 8.99, 8.76, 8.6, 8.48, 8.27, 8.14, 8.12,\n7.97, 7.84, 7.38, 7.38, 6.84, 6.52, 5.84, -2.44] Action: close window\nReward: 0, Score 49, Done False\nSTATE 38\nthe window closes ( more easily than it opened ). you are carrying :\na brass lantern a painting kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is slightly ajar. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is :\na glass bottle the glass bottle contains : a quantity of water Actions:\n[’open window’, ’west’, ’open sack’, ’close bottle’, ’put lantern on\ntable’, ’take sack’, ’put down lantern’, ’throw lantern at bottle’, ’turn\non lantern’, ’put down all’, ’put down painting’, ’take bottle’, ’take\nall’, ’put painting on table’, ’up’, ’jump’] Qvalues: [9.55, 9.15, 8.8,\n8.59, 8.13, 8.0, 7.89, 7.86, 7.71, 7.48, 7.33, 7.22, 6.5, 6.43, 6.4, -1.57]\nAction: put down all Reward: 0, Score 49, Done False\nSTATE 39\nbrass lantern : dropped. painting : dropped. you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis slightly ajar. a painting by a neglected genius is here. there is a\nbrass lantern ( battery powered ) here. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is :\na glass bottle the glass bottle contains : a quantity of water Actions:\n[’open window’, ’open sack’, ’close bottle’, ’turn on lantern’, ’west’,\n’take painting’, ’take all’, ’take lantern’, ’take sack’, ’take bottle’,\n’take all from table’, ’up’, ’jump’] Qvalues: [6.81, 6.37, 6.11, 6.08,\n5.92, 5.9, 5.89, 5.83, 5.7, 5.25, 5.0, 2.44, -6.37] Action: open window\nReward: 0, Score 49, Done False\nSTATE 40\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be\nseen leading upward. a dark chimney leads down and to the east\nis a small window which is open. a painting by a neglected genius\nis here. there is a brass lantern ( battery powered ) here. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’open sack’, ’west’, ’close bottle’,\n’close window’, ’turn on lantern’, ’take sack’, ’take lantern’, ’take\nall’, ’take painting’, ’take bottle’, ’take all from table’, ’up’, ’jump’]\nQvalues: [6.76, 6.36, 6.29, 6.08, 6.06, 5.9, 5.59, 5.56, 5.48, 5.17, 5.14,\n4.9, 0.7, -8.47] Action: east Reward: 0, Score 49, Done False\nSTATE 41\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’west’, ’east’, ’south’] Qvalues: [7.41, 6.37,\n6.31, 6.13, 5.86] Action: west Reward: 0, Score 49, Done False\nSTATE 42\nkitchen a painting by a neglected genius is here. there is a brass\nlantern ( battery powered ) here. on the table is an elongated brown\nsack, smelling of hot peppers. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. a painting by a neglected genius is\nhere. there is a brass lantern ( battery powered ) here. on the table\nis an elongated brown sack, smelling of hot peppers. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’east’, ’open sack’, ’close window’, ’close\nbottle’, ’take lantern’, ’turn on lantern’, ’take sack’, ’take painting’,\n’take bottle’, ’take all’, ’take all from table’, ’up’, ’jump’] Qvalues:\n[9.43, 8.18, 7.32, 7.28, 6.96, 6.84, 6.75, 6.41, 5.8, 5.8, 5.79, 5.22, 1.47,\n-7.43] Action: take painting Reward: 0, Score 49, Done False\nSTATE 43\ntaken. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. there is a brass\nlantern ( battery powered ) here. on the table is an elongated brown\nsack, smelling of hot peppers. sitting on the kitchen table is : a glass\nbottle the glass bottle contains : a quantity of water Actions: [’take\nlantern’, ’west’, ’east’, ’open sack’, ’close bottle’, ’take sack’, ’close\nwindow’, ’turn on lantern’, ’throw painting at lantern’, ’take bottle’,\n’take all’, ’put down painting’, ’take all from table’, ’put painting\non table’, ’up’, ’jump’] Qvalues: [8.45, 8.2, 8.0, 7.88, 7.75, 7.68, 7.63,\n7.61, 7.42, 6.97, 6.81, 6.75, 6.24, 6.19, 5.3, -3.67] Action: take lantern\nReward: 0, Score 49, Done False\nSTATE 44\ntaken. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. on the table\nis an elongated brown sack, smelling of hot peppers. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’open sack’, ’close window’, ’close bottle’,\n’east’, ’put lantern on table’, ’take sack’, ’turn on lantern’, ’put down\nlantern’, ’throw lantern at bottle’, ’put down all’, ’take bottle’, ’put\ndown painting’, ’take all’, ’put painting on table’, ’up’, ’jump’] Qval-\nues: [9.89, 8.94, 8.93, 8.69, 8.64, 8.29, 8.14, 8.0, 8.0, 7.92, 7.64, 7.39,\n7.27, 6.79, 6.4, 6.21, -2.21] Action: west Reward: 0, Score 49, Done\nFalse\nSTATE 45\nliving room above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a jewel encrusted\negg you are carrying : a brass lantern a painting living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity. your\ncollection of treasures consists of : a jewel encrusted egg Actions:\n[’put painting in case’, ’put all in case’, ’put down painting’, ’take\ntreasures from sword’, ’put down all’, ’open egg with lantern’, ’put\ndown lantern’, ’put lantern in case’, ’take sword’, ’close case’, ’turn\non lantern’, ’throw lantern at case’, ’throw painting at treasures’,\n’open trap’, ’throw lantern at treasures’, ’take treasures’, ’take on\ntreasures’, ’east’] Qvalues: [10.96, 10.79, 10.25, 10.18, 9.93, 9.89, 9.77,\n9.74, 9.61, 9.59, 9.55, 9.5, 9.43, 9.4, 8.77, 8.09, 7.49, 6.26] Action: put\nall in case Reward: 6, Score 55, Done False\nSTATE 46\nbrass lantern : done. painting : done. you are empty handed. living\nroom you are in the living room. there is a doorway to the east,\na wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a closed trap door at\nyour feet. above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a painting a brass\nlantern a jewel encrusted egg Actions: [’east’, ’open trap’, ’turn on\nlantern’, ’take lantern’, ’close case’, ’take on egg’, ’take egg from\nsword’, ’take sword’, ’take painting’, ’take egg’, ’take all from case’]\nQvalues: [4.89, 4.27, 4.15, 3.98, 3.98, 3.72, 3.7, 3.69, 3.19, 3.05, 1.15]\nAction: east Reward: 0, Score 55, Done False\nSTATE 47\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. sitting on the kitchen table is : a glass bottle the glass\nbottle contains : a quantity of water you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. on the table is an elongated brown sack, smelling of hot pep-\npers. sitting on the kitchen table is : a glass bottle the glass bottle\ncontains : a quantity of water Actions: [’west’, ’east’, ’open sack’,\n’close window’, ’close bottle’, ’take sack’, ’take bottle’, ’take all’, ’up’,\n’jump’] Qvalues: [7.6, 7.38, 6.33, 6.29, 6.05, 5.44, 5.09, 4.99, 1.82, -7.47]\nAction: east Reward: 0, Score 55, Done False\nSTATE 48\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’west’, ’south’] Qvalues: [7.72, 6.65,\n6.54, 6.41, 6.24] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 49\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.61, 6.8, 6.53, 6.42] Action: open window\nReward: 0, Score 55, Done False\nSTATE 50\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [9.23, 7.55, 6.84, 6.76, 6.48]\nAction: west Reward: 0, Score 55, Done False\nSTATE 51\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. sitting on the kitchen table is : a glass bottle the glass\nbottle contains : a quantity of water you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. on the table is an elongated brown sack, smelling of hot pep-\npers. sitting on the kitchen table is : a glass bottle the glass bottle\ncontains : a quantity of water Actions: [’east’, ’open sack’, ’close\nbottle’, ’close window’, ’take sack’, ’take bottle’, ’take all’, ’west’,\n’up’, ’jump’] Qvalues: [7.0, 6.25, 5.99, 5.77, 5.28, 5.25, 5.2, 5.18, 3.37,\n-6.41] Action: take sack Reward: 0, Score 55, Done False\nSTATE 52\ntaken. you are carrying : a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’east’, ’put down sack’, ’west’,\n’take bottle’, ’close bottle’, ’open sack’, ’close window’, ’up’, ’jump’]\nQvalues: [5.79, 5.65, 5.52, 5.38, 5.1, 5.03, 5.0, 4.56, 3.88, -4.0] Action:\nput sack on table Reward: 0, Score 55, Done False\nSTATE 53\ndone. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table\nis : a brown sack a glass bottle the glass bottle contains : a quantity\nof water Actions: [’east’, ’open sack’, ’west’, ’close bottle’, ’close\nwindow’, ’take sack’, ’take bottle’, ’take all’, ’up’, ’jump’] Qvalues:\n[6.97, 5.93, 5.76, 5.66, 5.45, 5.06, 5.0, 4.82, 3.54, -5.91] Action: east\nReward: 0, Score 55, Done False\nSTATE 54\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’west’, ’south’] Qvalues: [7.59, 6.53,\n6.4, 6.29, 6.18] Action: south Reward: 0, Score 55, Done False\nSTATE 55\nsouth of house you are empty handed. south of house you are fac-\ning the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’east’, ’south’, ’west’] Qvalues:\n[6.57, 5.66, 4.74] Action: east Reward: 0, Score 55, Done False\nSTATE 56\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [7.37, 6.67,\n6.36, 6.29, 5.95] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 57\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.24, 6.57, 6.44, 6.12] Action: open window\nReward: 0, Score 55, Done False\nSTATE 58\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [8.37, 7.33, 6.58, 6.38, 6.19]\nAction: close window Reward: 0, Score 55, Done False\nSTATE 59\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.14, 6.58, 6.43, 6.16] Action: east Reward:\n0, Score 55, Done False\nSTATE 60\nclearing you are empty handed. clearing you are in a small clear-\ning in a well marked forest path that extends to the east and west.\nActions: [’west’, ’south’, ’north’, ’east’, ’go around forest’] Qvalues:\n[6.88, 5.64, 5.62, 5.59, 5.35] Action: south Reward: 0, Score 55, Done\nFalse\nSTATE 61\nforest you are empty handed. forest this is a dimly lit forest, with\nlarge trees all around. Actions: [’northwest’, ’north’, ’go around\ntrees’, ’west’] Qvalues: [6.36, 6.01, 5.76, 5.31] Action: northwest\nReward: 0, Score 55, Done False\nSTATE 62\nsouth of house you are empty handed. south of house you are fac-\ning the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’east’, ’south’, ’west’] Qvalues:\n[6.54, 5.42, 5.03] Action: east Reward: 0, Score 55, Done False\nSTATE 63\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’open window’, ’north’, ’south’, ’east’] Qvalues: [8.04, 6.16,\n6.05, 5.93] Action: open window Reward: 0, Score 55, Done False\nSTATE 64\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [8.2, 7.53, 7.02, 6.89, 6.46]\nAction: west Reward: 0, Score 55, Done False\nSTATE 65\nkitchen sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. sitting on the kitchen table is : a brown sack a glass bot-\ntle the glass bottle contains : a quantity of water Actions: [’east’,\n’west’, ’open sack’, ’close bottle’, ’close window’, ’take sack’, ’take\nbottle’, ’take all’, ’up’, ’jump’] Qvalues: [7.18, 6.16, 5.93, 5.53, 5.5,\n4.9, 4.69, 4.64, 2.8, -6.25] Action: west Reward: 0, Score 55, Done False\nSTATE 66\nliving room above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a painting a brass\nlantern a jewel encrusted egg you are empty handed. living room\nyou are in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity.\nyour collection of treasures consists of : a painting a brass lantern\na jewel encrusted egg Actions: [’east’, ’open trap’, ’close case’, ’turn\non lantern’, ’take egg from sword’, ’take sword’, ’take on egg’, ’take\nlantern’, ’take egg’, ’take painting’, ’take all from case’] Qvalues:\n[5.2, 4.77, 4.54, 4.41, 4.18, 4.15, 4.09, 4.06, 3.41, 3.14, 1.54] Action:\neast Reward: 0, Score 55, Done False\nSTATE 67\nkitchen sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’east’, ’open\nsack’, ’close bottle’, ’west’, ’close window’, ’take all’, ’take sack’,\n’take bottle’, ’up’, ’jump’] Qvalues: [6.58, 5.57, 5.2, 5.13, 5.03, 4.57,\n4.56, 4.5, 3.0, -6.25] Action: open sack Reward: 0, Score 55, Done\nFalse\nSTATE 68\nopening the brown sack reveals a lunch, and a clove of garlic. you\nare empty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of food.\na passage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. sitting on the kitchen table is : a brown sack\nthe brown sack contains : a lunch a clove of garlic a glass bottle the\nglass bottle contains : a quantity of water Actions: [’east’, ’west’,\n’close sack’, ’close bottle’, ’close window’, ’take all from sack’, ’take\nsack’, ’take garlic’, ’take food’, ’take all’, ’take bottle’, ’up’, ’jump’]\nQvalues: [6.66, 5.33, 5.29, 5.0, 4.84, 4.49, 4.48, 4.46, 4.45, 4.26, 4.17,\n2.18, -6.95] Action: close bottle Reward: 0, Score 55, Done False\nSTATE 69\nclosed. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table is\n: a brown sack the brown sack contains : a lunch a clove of garlic a\nglass bottle the glass bottle contains : a quantity of water Actions:\n[’east’, ’open bottle’, ’close sack’, ’west’, ’take all from sack’, ’take\nsack’, ’close window’, ’take food’, ’take garlic’, ’take bottle’, ’take\nall’, ’up’, ’jump’] Qvalues: [6.26, 5.2, 5.13, 4.97, 4.6, 4.55, 4.52, 4.43,\n4.39, 4.33, 4.22, 2.04, -7.58] Action: take all from sack Reward: 0,\nScore 55, Done False\nSTATE 70\nlunch : taken. clove of garlic : taken. you are carrying : a clove of\ngarlic a lunch kitchen you are in the kitchen of the white house. a\ntable seems to have been used recently for the preparation of food.\na passage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. sitting on the kitchen table is : a brown sack a\nglass bottle the glass bottle contains : a quantity of water Actions:\n[’put all in sack’, ’put all on table’, ’put down all’, ’put food on table’,\n’put garlic on table’, ’put down food’, ’west’, ’east’, ’put food in sack’,\n’take sack’, ’open bottle’, ’put garlic in sack’, ’put down garlic’, ’close\nsack’, ’take bottle’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues:\n[5.74, 5.73, 5.59, 5.43, 5.38, 5.34, 5.34, 5.3, 5.27, 5.26, 5.25, 5.18, 5.16,\n4.96, 4.91, 4.61, 4.48, 4.17, -2.97] Action: put down garlic Reward: 0,\nScore 55, Done False\nSTATE 71\ndropped. you are carrying : a lunch kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to\nthe east is a small window which is open. there is a clove of garlic\nhere. sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’put food\non table’, ’put down food’, ’east’, ’west’, ’take garlic’, ’put food in\nsack’, ’take sack’, ’open bottle’, ’close sack’, ’take bottle’, ’take all’,\n’close window’, ’take all from table’, ’up’, ’jump’] Qvalues: [5.24,\n5.24, 5.21, 5.16, 5.11, 4.98, 4.98, 4.86, 4.77, 4.57, 4.45, 4.25, 4.16, 2.27,\n-6.13] Action: put food in sack Reward: 0, Score 55, Done False\nSTATE 72\ndone. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a clove of garlic here.\nsitting on the kitchen table is : a brown sack the brown sack con-\ntains : a lunch a glass bottle the glass bottle contains : a quantity of\nwater Actions: [’east’, ’close sack’, ’open bottle’, ’west’, ’take food’,\n’take garlic’, ’take sack’, ’take all’, ’close window’, ’take bottle’, ’take\nall from table’, ’up’, ’jump’] Qvalues: [6.19, 5.16, 5.15, 4.86, 4.61, 4.54,\n4.51, 4.48, 4.48, 4.37, 3.97, 2.19, -7.34] Action: take sack Reward: 0,\nScore 55, Done False\nSTATE 73\ntaken. you are carrying : a brown sack the brown sack contains :\na lunch kitchen you are in the kitchen of the white house. a table\nseems to have been used recently for the preparation of food. a\npassage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’put food on table’, ’put down\nsack’, ’put down food’, ’west’, ’take food’, ’open bottle’, ’shake sack’,\n’take garlic’, ’east’, ’close sack’, ’take bottle’, ’take all’, ’close win-\ndow’, ’up’, ’jump’] Qvalues: [5.56, 5.37, 5.37, 5.31, 5.31, 5.22, 5.13,\n5.13, 5.1, 5.01, 4.91, 4.82, 4.57, 4.44, 3.26, -4.31] Action: shake sack\nReward: 0, Score 55, Done False\nSTATE 74\nthe contents of the brown sack spills to the ground. you are carrying\n: a brown sack kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. a hot pepper sandwich is here. there\nis a clove of garlic here. sitting on the kitchen table is : a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’put sack\non table’, ’west’, ’put down sack’, ’take food’, ’east’, ’take garlic’,\n’open bottle’, ’take bottle’, ’close sack’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.61, 5.48, 5.42, 5.32, 5.28, 5.14, 5.1, 5.01, 4.85,\n4.49, 4.37, 3.37, -4.37] Action: close sack Reward: 0, Score 55, Done\nFalse\nSTATE 75\nclosed. you are carrying : a brown sack kitchen you are in the\nkitchen of the white house. a table seems to have been used re-\ncently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’west’, ’put down sack’, ’take\nfood’, ’east’, ’take garlic’, ’open bottle’, ’open sack’, ’take bottle’,\n’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.67, 5.52, 5.47, 5.4,\n5.32, 5.26, 5.2, 5.01, 4.97, 4.7, 4.48, 2.85, -4.86] Action: put down sack\nReward: 0, Score 55, Done False\nSTATE 76\ndropped. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a brown sack here. a\nhot pepper sandwich is here. there is a clove of garlic here. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’west’, ’open sack’, ’open bottle’,\n’take sack’, ’take food’, ’take garlic’, ’take all’, ’close window’, ’take\nbottle’, ’up’, ’jump’] Qvalues: [6.33, 5.31, 5.27, 5.17, 4.77, 4.69, 4.63,\n4.58, 4.57, 4.47, 1.86, -7.42] Action: east Reward: 0, Score 55, Done\nFalse\nSTATE 77\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [6.43, 5.68,\n5.43, 5.25, 5.15] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 78\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [7.54, 6.26, 5.95, 5.91] Action: north Reward:\n0, Score 55, Done False\nSTATE 79\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. you are empty handed. north\nof house you are facing the north side of a white house. there is\nno door here, and all the windows are boarded up. to the north\na narrow path winds through the trees. Actions: [’east’, ’north’,\n’west’] Qvalues: [7.13, 6.52, 5.61] Action: east Reward: 0, Score 55,\nDone False\nSTATE 80\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’open window’, ’north’, ’south’, ’east’] Qvalues: [6.88, 5.47,\n5.28, 5.27] Action: open window Reward: 0, Score 55, Done False\nSTATE 81\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [7.23, 6.18, 5.72, 5.56, 5.36]\nAction: west Reward: 0, Score 55, Done False\nSTATE 82\nkitchen there is a brown sack here. a hot pepper sandwich is here.\nthere is a clove of garlic here. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a brown sack here. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’east’, ’open sack’, ’open bottle’, ’close\nwindow’, ’take garlic’, ’take food’, ’take sack’, ’take bottle’, ’take all’,\n’up’, ’jump’] Qvalues: [8.05, 7.66, 6.21, 5.98, 5.91, 5.52, 5.44, 5.4, 4.96,\n4.93, 2.11, -6.64] Action: east Reward: 0, Score 55, Done False\nSTATE 83\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [6.52, 6.0,\n5.7, 5.69, 5.22] Action: west Reward: 0, Score 55, Done False\nSTATE 84\nkitchen there is a brown sack here. a hot pepper sandwich is here.\nthere is a clove of garlic here. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a brown sack here. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’east’, ’west’, ’open sack’, ’open bottle’, ’close\nwindow’, ’take garlic’, ’take food’, ’take sack’, ’take bottle’, ’take\nall’, ’up’, ’jump’] Qvalues: [7.35, 7.25, 6.16, 6.01, 5.81, 5.52, 5.44, 5.41,\n5.16, 5.06, 2.48, -6.1] Action: take food Reward: 0, Score 55, Done\nFalse\nSTATE 85\ntaken. you are carrying : a lunch kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a brown sack here.\nthere is a clove of garlic here. sitting on the kitchen table is : a glass\nbottle the glass bottle contains : a quantity of water Actions: [’put\ndown food’, ’put food on table’, ’east’, ’take garlic’, ’take sack’, ’west’,\n’open bottle’, ’open sack’, ’take bottle’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.65, 5.57, 5.53, 5.48, 5.47, 5.43, 5.25, 5.19, 5.13,\n4.96, 4.56, 3.21, -4.72] Action: take garlic Reward: 0, Score 55, Done\nFalse\nSTATE 86\ntaken. you are carrying : a clove of garlic a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. there is a\nbrown sack here. sitting on the kitchen table is : a glass bottle the\nglass bottle contains : a quantity of water Actions: [’put all on table’,\n’put down all’, ’west’, ’put food on table’, ’take sack’, ’put down\nfood’, ’put garlic on table’, ’east’, ’open bottle’, ’put down garlic’,\n’take bottle’, ’open sack’, ’take all’, ’close window’, ’up’, ’jump’]\nQvalues: [5.93, 5.82, 5.62, 5.59, 5.57, 5.56, 5.51, 5.47, 5.45, 5.42, 5.32,\n5.28, 4.87, 4.68, 3.83, -3.17] Action: put all on table Reward: 0, Score\n55, Done False\nSTATE 87\nclove of garlic : done. lunch : done. you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. there is a brown sack here. sitting on the kitchen table is :\na lunch a clove of garlic a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’west’, ’open sack’, ’open bottle’,\n’close window’, ’take garlic’, ’take food’, ’take sack’, ’take all’, ’take\nbottle’, ’take all from table’, ’up’, ’jump’] Qvalues: [6.17, 5.34, 5.27,\n5.11, 4.66, 4.62, 4.55, 4.55, 4.39, 4.28, 3.72, 2.16, -6.62] Action: take\nsack Reward: 0, Score 55, Done False\nSTATE 88\ntaken. you are carrying : a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table\nis : a lunch a clove of garlic a glass bottle the glass bottle contains :\na quantity of water Actions: [’put sack on table’, ’put down sack’,\n’west’, ’take food’, ’east’, ’take garlic’, ’open bottle’, ’take bottle’,\n’open sack’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.72,\n5.61, 5.46, 5.46, 5.37, 5.31, 5.26, 5.25, 5.13, 4.8, 4.53, 3.63, -3.52] Action:\ntake food Reward: 0, Score 55, Done False\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table is\n: a clove of garlic a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put all on table’, ’put down all’, ’put sack on\ntable’, ’put down sack’, ’west’, ’put down food’, ’put food on table’,\n’take garlic’, ’east’, ’open bottle’, ’open sack’, ’take bottle’, ’take all’,\n’close window’, ’up’, ’jump’] Qvalues: [5.32, 5.29, 5.25, 5.12, 5.09,\n5.06, 5.05, 4.91, 4.9, 4.79, 4.66, 4.56, 4.33, 4.07, 3.08, -4.21] Action:\ntake bottle Reward: 0, Score 55, Done False\nSTATE 90\ntaken. you are carrying : a glass bottle the glass bottle contains :\na quantity of water a lunch a brown sack kitchen you are in the\nkitchen of the white house. a table seems to have been used recently\nfor the preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. sitting on the\nkitchen table is : a clove of garlic Actions: [’put all on table’, ’put\ndown bottle’, ’put down all’, ’put bottle on table’, ’east’, ’put sack\non table’, ’west’, ’put down sack’, ’put down food’, ’put food on\ntable’, ’open bottle’, ’put water on table’, ’open sack’, ’take garlic’,\n’up’, ’close window’, ’jump’] Qvalues: [5.07, 5.04, 5.02, 5.0, 4.88, 4.87,\n4.82, 4.74, 4.7, 4.62, 4.59, 4.55, 4.35, 4.34, 3.94, 3.75, -2.22] Action: put\ndown bottle Reward: 0, Score 55, Done False\nSTATE 91\ndropped. you are carrying : a lunch a brown sack kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a glass bottle here. the glass bottle contains : a quantity of water\nsitting on the kitchen table is : a clove of garlic Actions: [’put all\non table’, ’put sack on table’, ’put down sack’, ’put down all’, ’west’,\n’put down food’, ’put food on table’, ’east’, ’take garlic’, ’open bottle’,\n’open sack’, ’take bottle’, ’take all’, ’close window’, ’up’, ’jump’]\nQvalues: [5.43, 5.42, 5.32, 5.3, 5.3, 5.25, 5.17, 5.02, 4.94, 4.83, 4.66,\n4.62, 4.28, 4.06, 3.09, -4.1] Action: put down sack Reward: 0, Score\n55, Done False\nSTATE 92\ndropped. you are carrying : a lunch kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. there is a brown\nsack here. there is a glass bottle here. the glass bottle contains : a\nquantity of water sitting on the kitchen table is : a clove of garlic\nActions: [’east’, ’put down food’, ’west’, ’put food on table’, ’take\nsack’, ’take garlic’, ’open bottle’, ’open sack’, ’take bottle’, ’take all’,\n’close window’, ’up’, ’jump’] Qvalues: [5.35, 5.25, 5.23, 5.12, 4.97,\n4.97, 4.73, 4.73, 4.72, 4.69, 4.11, 3.21, -4.36] Action: east Reward: 0,\nScore 55, Done False\nSTATE 93\nbehind house you are carrying : a lunch behind house you are be-\nhind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is open.\nActions: [’put down lunch’, ’north’, ’close window’, ’south’, ’east’,\n’west’] Qvalues: [5.7, 4.52, 4.49, 4.3, 4.28, 4.03] Action: close window\nReward: 0, Score 55, Done False\nSTATE 94\nthe window closes ( more easily than it opened ). you are carrying :\na lunch behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’put down lunch’, ’open\nwindow’, ’south’, ’east’, ’north’] Qvalues: [5.45, 4.72, 4.17, 4.1, 3.96]\nAction: south Reward: 0, Score 55, Done False\nSTATE 95\nsouth of house you are carrying : a lunch south of house you are\nfacing the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’put down lunch’, ’east’, ’south’,\n’west’] Qvalues: [4.75, 4.42, 4.13, 3.16] Action: east Reward: 0, Score\n55, Done False\nSTATE 96\nbehind house you are carrying : a lunch behind house you are be-\nhind the white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’put down lunch’, ’open window’, ’south’, ’north’, ’east’]\nQvalues: [5.48, 4.63, 4.06, 4.03, 3.82] Action: open window Reward:\n0, Score 55, Done False\nSTATE 97\nwith great effort, you open the window far enough to allow entry.\nyou are carrying : a lunch behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’put down\nlunch’, ’west’, ’south’, ’north’, ’east’, ’close window’] Qvalues: [5.21,\n4.88, 4.3, 4.26, 4.1, 3.92] Action: west Reward: 0, Score 55, Done False\nSTATE 98\nkitchen there is a brown sack here. there is a glass bottle here. the\nglass bottle contains : a quantity of water sitting on the kitchen\ntable is : a clove of garlic you are carrying : a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a brown sack here. there is a glass bottle here. the glass bottle\ncontains : a quantity of water sitting on the kitchen table is : a clove\nof garlic Actions: [’put down food’, ’put food on table’, ’west’, ’take\nsack’, ’take garlic’, ’east’, ’open bottle’, ’open sack’, ’take bottle’,\n’take all’, ’close window’, ’up’, ’jump’] Qvalues: [4.8, 4.68, 4.6, 4.58,\n4.46, 4.44, 4.44, 4.38, 4.36, 4.0, 3.7, 3.02, -4.39] Action: take garlic\nReward: 0, Score 55, Done False\nSTATE 99\ntaken. you are carrying : a clove of garlic a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a brown sack here. there is a glass bottle here. the glass bottle\ncontains : a quantity of water Actions: [’put down all’, ’put all on\ntable’, ’put down food’, ’west’, ’put food on table’, ’put garlic on\ntable’, ’take sack’, ’put down garlic’, ’open bottle’, ’east’, ’open sack’,\n’take bottle’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.33,\n5.29, 5.05, 5.04, 4.98, 4.92, 4.91, 4.81, 4.79, 4.65, 4.61, 4.54, 4.26, 3.94,\n3.29, -3.82] Action: put down food Reward: 0, Score 55, Done True\nSTATE 100\ndropped. you are carrying : a clove of garlic kitchen you are in\nthe kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. a hot pepper\nsandwich is here. there is a brown sack here. there is a glass bottle\nhere. the glass bottle contains : a quantity of water Actions: [’west’,\n’put garlic on table’, ’take sack’, ’open bottle’, ’take food’, ’put down\ngarlic’, ’open sack’, ’take bottle’, ’east’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.1, 4.79, 4.73, 4.68, 4.67, 4.65, 4.57, 4.32, 4.3,\n3.95, 3.91, 2.32, -5.11] Action: reset Reward: 0, Score 0, Done False\nB.1.1\nDRRN (for comparison). STATE 0\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\nZORK is a registered trademark of Infocom, Inc. Revision 88 \/ Se-\nrial number 840726 West of House You are standing in an open\nfield west of a white house, with a boarded front door. There is a\nsmall mailbox here. You are empty handed. West of House You are\nstanding in an open field west of a white house, with a boarded\nfront door. There is a small mailbox here. Actions: [’north’, ’south’,\n’west’, ’open mailbox’] Qvalues: [9.25, 8.75, 8.4, 8.11] Action: north\nReward: 0, Score 0, Done False\nSTATE 1\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. you are empty handed. north\nof house you are facing the north side of a white house. there is\nno door here, and all the windows are boarded up. to the north\na narrow path winds through the trees. Actions: [’north’, ’east’,\n’west’] Qvalues: [10.23, 10.22, 8.8] Action: north Reward: 0, Score 0,\nDone False\nSTATE 2\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you are empty handed.\nforest path this is a path winding through a dimly lit forest. the path\nheads north south here. one particularly large tree with some low\nbranches stands at the edge of the path. Actions: [’up’, ’go around\nforest’, ’south’, ’east’, ’north’, ’west’] Qvalues: [11.3, 9.84, 9.53, 8.64,\n7.8, 5.7] Action: up Reward: 0, Score 0, Done False\nSTATE 3\nup a tree you are about 10 feet above the ground nestled among\nsome large branches. the nearest branch above you is above your\nreach. beside you on the branch is a small bird’s nest. in the bird’s\nnest is a large egg encrusted with precious jewels, apparently scav-\nenged by a childless songbird. the egg is covered with fine gold\ninlay, and ornamented in lapis lazuli and mother of pearl. unlike\nmost eggs, this one is hinged and closed with a delicate looking\nclasp. the egg appears extremely fragile. you are empty handed. up\na tree you are about 10 feet above the ground nestled among some\nlarge branches. the nearest branch above you is above your reach.\nbeside you on the branch is a small bird’s nest. in the bird’s nest is\na large egg encrusted with precious jewels, apparently scavenged\nby a childless songbird. the egg is covered with fine gold inlay, and\nornamented in lapis lazuli and mother of pearl. unlike most eggs,\nthis one is hinged and closed with a delicate looking clasp. the egg\nappears extremely fragile. Actions: [’take egg’, ’take on egg’, ’take\nnest’, ’close nest’, ’down’] Qvalues: [12.46, 10.87, 10.22, 9.65, 5.91]\nAction: take egg Reward: 5, Score 5, Done False\nSTATE 4\ntaken. you are carrying: a jewel encrusted egg you hear in the dis-\ntance the chirping of a song bird. up a tree you are about 10 feet\nabove the ground nestled among some large branches. the nearest\nbranch above you is above your reach. beside you on the branch is\na small bird’s nest. you hear in the distance the chirping of a song\nbird. Actions: [’take on egg’, ’put egg in nest’, ’down’, ’take nest’,\n’put down egg’, ’close nest’, ’throw egg at nest’] Qvalues: [8.24, 8.11,\n7.77, 7.37, 7.34, 7.27, 6.01] Action: down Reward: 0, Score 5, Done\nFalse\nSTATE 5\nforest path you are carrying: a jewel encrusted egg forest path this\nis a path winding through a dimly lit forest. the path heads north\nsouth here. one particularly large tree with some low branches\nstands at the edge of the path. you hear in the distance the chirping\nof a song bird. Actions: [’south’, ’take on egg’, ’up’, ’open egg with\nall’, ’go around forest’, ’put down egg’, ’east’, ’north’, ’throw egg\nat tree’, ’west’] Qvalues: [9.33, 9.13, 9.02, 8.27, 8.21, 7.74, 7.41, 6.32,\n4.05, 3.81] Action: south Reward: 0, Score 5, Done False\nSTATE 6\nnorth of house you are carrying: a jewel encrusted egg north of\nhouse you are facing the north side of a white house. there is no\ndoor here, and all the windows are boarded up. to the north a nar-\nrow path winds through the trees. Actions: [’east’, ’take on egg’,\n’put down egg’, ’north’, ’open egg with all’, ’west’, ’throw egg at\npath’] Qvalues: [10.13, 8.82, 7.17, 6.75, 6.43, 4.81, 3.01] Action: east\nReward: 0, Score 5, Done False\nSTATE 7\nbehind house you are behind the white house. a path leads into\nthe forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. you are carrying: a jewel encrusted\negg behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a\nsmall window which is slightly ajar. Actions: [’take on egg’, ’open\nwindow’, ’put down egg’, ’open egg with all’, ’north’, ’east’, ’south’,\n’throw egg at path’] Qvalues: [11.34, 10.97, 10.23, 10.2, 8.65, 8.62,\n8.47, 5.23] Action: put down egg Reward: 0, Score 5, Done False\nSTATE 8\ndropped. you are empty handed. behind house you are behind the\nwhite house. a path leads into the forest to the east. in one corner\nof the house there is a small window which is slightly ajar. there\nis a jewel encrusted egg here. Actions: [’take egg’, ’take on egg’,\n’open window’, ’north’, ’east’, ’south’] Qvalues: [11.04, 10.84, 9.91,\n9.62, 9.43, 9.09] Action: north Reward: 0, Score 5, Done False\nSTATE 9\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.09, 8.65, 6.82]\nAction: east Reward: 0, Score 5, Done False\nSTATE 10\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’take on egg’, ’north’, ’open window’, ’south’,\n’east’] Qvalues: [10.77, 10.26, 10.26, 10.17, 10.01, 9.07] Action: north\nReward: 0, Score 5, Done False\nSTATE 11\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.04, 8.6, 6.73]\nAction: north Reward: 0, Score 5, Done False\nSTATE 12\nforest path you are empty handed. forest path this is a path winding\nthrough a dimly lit forest. the path heads north south here. one\nparticularly large tree with some low branches stands at the edge\nof the path. Actions: [’up’, ’south’, ’go around forest’, ’east’, ’north’,\n’west’] Qvalues: [9.02, 8.89, 8.28, 7.72, 7.19, 4.43] Action: south Re-\nward: 0, Score 5, Done False\nSTATE 13\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.03, 8.58, 6.68]\nAction: east Reward: 0, Score 5, Done False\nSTATE 14\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’north’, ’take on egg’, ’open window’, ’south’,\n’east’] Qvalues: [10.64, 10.2, 10.18, 10.07, 9.98, 9.04] Action: north\nReward: 0, Score 5, Done False\nSTATE 15\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [9.99, 8.53, 6.58]\nAction: east Reward: 0, Score 5, Done False\nSTATE 16\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’north’, ’take on egg’, ’open window’, ’south’,\n’east’] Qvalues: [10.54, 10.14, 10.09, 9.99, 9.93, 8.99] Action: open\nwindow Reward: 0, Score 5, Done False\nSTATE 17\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of\nthe house there is a small window which is open. there is a jewel\nencrusted egg here. Actions: [’take egg’, ’take on egg’, ’west’, ’close\nwindow’, ’east’, ’south’, ’north’] Qvalues: [11.46, 9.71, 9.65, 8.11,\n6.49, 6.19, 5.6] Action: take egg Reward: 0, Score 5, Done False\nSTATE 18\ntaken. you are carrying: a jewel encrusted egg behind house you\nare behind the white house. a path leads into the forest to the east.\nin one corner of the house there is a small window which is open.\nActions: [’take on egg’, ’west’, ’north’, ’open egg with all’, ’put down\negg’, ’close window’, ’south’, ’east’, ’throw egg at path’] Qvalues:\n[10.51, 10.42, 10.08, 9.93, 9.81, 9.6, 9.4, 9.12, 6.64] Action: north Re-\nward: 0, Score 5, Done False\nSTATE 19\nnorth of house you are carrying: a jewel encrusted egg north of\nhouse you are facing the north side of a white house. there is no\ndoor here, and all the windows are boarded up. to the north a nar-\nrow path winds through the trees. Actions: [’east’, ’take on egg’,\n’put down egg’, ’north’, ’open egg with all’, ’west’, ’throw egg at\npath’] Qvalues: [9.59, 8.34, 6.73, 6.21, 5.94, 4.12, 2.61] Action: take\non egg Reward: 0, Score 5, Done False\nSTATE 20\nthere is a noticeable crunch from beneath you, and inspection re-\nveals that the egg is lying open, badly damaged. there is a golden\nclockwork canary nestled in the egg. it seems to have recently\nhad a bad experience. the mountings for its jewel like eyes are\nempty, and its silver beak is crumpled. through a cracked crystal\nwindow below its left wing you can see the remains of intricate\nmachinery. it is not clear what result winding it would have, as\nthe mainspring seems sprung. you are carrying: a broken jewel\nencrusted egg the broken jewel encrusted egg contains: a broken\nclockwork canary north of house you are facing the north side\nof a white house. there is no door here, and all the windows are\nboarded up. to the north a narrow path winds through the trees.\nActions: [’take canary’, ’shake egg’, ’close egg’, ’north’, ’east’, ’put\ndown canary’, ’put down egg’, ’west’] Qvalues: [10.64, 10.34, 10.01,\n9.8, 9.8, 9.68, 9.67, 8.01] Action: north Reward: 0, Score 5, Done False\nSTATE 21\nforest path you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nforest path this is a path winding through a dimly lit forest. the path\nheads north south here. one particularly large tree with some low\nbranches stands at the edge of the path. you hear in the distance\nthe chirping of a song bird. Actions: [’south’, ’take canary’, ’up’,\n’shake egg’, ’put down canary’, ’east’, ’close egg’, ’go around forest’,\n’north’, ’put down egg’, ’west’] Qvalues: [11.15, 9.93, 9.35, 9.24, 8.89,\n8.88, 8.79, 8.74, 8.22, 8.19, 5.5] Action: south Reward: 0, Score 5,\nDone False\nSTATE 22\nnorth of house you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. Actions: [’east’, ’shake egg’,\n’take canary’, ’close egg’, ’north’, ’put down canary’, ’put down egg’,\n’west’] Qvalues: [11.93, 10.06, 9.78, 9.49, 8.68, 8.53, 8.35, 6.3] Action:\neast Reward: 0, Score 5, Done False\nSTATE 23\nbehind house you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nbehind house you are behind the white house. a path leads into\nthe forest to the east. in one corner of the house there is a small\nwindow which is open. Actions: [’close window’, ’shake egg’, ’close\negg’, ’take canary’, ’east’, ’south’, ’north’, ’put down egg’, ’put down\ncanary’, ’west’] Qvalues: [11.21, 10.7, 10.61, 10.2, 10.12, 9.76, 9.66,\n9.53, 8.86, 8.7] Action: close egg Reward: 0, Score 5, Done False\nSTATE 24\nclosed. you are carrying: a broken jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nopen. Actions: [’close window’, ’south’, ’north’, ’west’, ’east’, ’open\negg’, ’put down egg’] Qvalues: [12.79, 12.65, 11.54, 11.37, 10.81, 10.31,\n10.01] Action: south Reward: 0, Score 5, Done False\nSTATE 25\nsouth of house you are facing the south side of a white house. there\nis no door here, and all the windows are boarded. you are carrying:\na broken jewel encrusted egg south of house you are facing the\nsouth side of a white house. there is no door here, and all the win-\ndows are boarded. Actions: [’east’, ’south’, ’open egg’, ’put down\negg’, ’west’] Qvalues: [14.39, 10.78, 10.45, 9.31, 8.39] Action: east\nReward: 0, Score 5, Done False\nSTATE 26\nbehind house you are carrying: a broken jewel encrusted egg be-\nhind house you are behind the white house. a path leads into the\nforest to the east. in one corner of the house there is a small window\nwhich is open. Actions: [’close window’, ’south’, ’east’, ’open egg’,\n’north’, ’west’, ’put down egg’] Qvalues: [13.61, 12.05, 11.88, 11.29,\n11.11, 10.28, 10.27] Action: close window Reward: 0, Score 5, Done\nFalse\nSTATE 27\nthe window closes (more easily than it opened). you are carrying: a\nbroken jewel encrusted egg behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is slightly ajar. Actions: [’open\nwindow’, ’north’, ’east’, ’south’, ’open egg’, ’put down egg’] Qval-\nues: [15.18, 12.67, 12.56, 12.56, 12.07, 11.34] Action: open window\nReward: 0, Score 5, Done False\nSTATE 28\nwith great effort, you open the window far enough to allow entry.\nyou are carrying: a broken jewel encrusted egg behind house you\nare behind the white house. a path leads into the forest to the east.\nin one corner of the house there is a small window which is open.\nActions: [’west’, ’close window’, ’open egg’, ’north’, ’put down egg’,\n’east’, ’south’] Qvalues: [18.49, 15.61, 14.4, 13.72, 13.06, 12.87, 12.11]\nAction: west Reward: 10, Score 15, Done False\nSTATE 29\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains: a\nquantity of water you are carrying: a broken jewel encrusted egg\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains:\na quantity of water Actions: [’east’, ’west’, ’close window’, ’take\nbottle’, ’take all’, ’open bottle’, ’open sack’, ’take sack’, ’open egg’,\n’up’, ’put egg on table’, ’put down egg’, ’jump’] Qvalues: [12.6, 12.33,\n12.29, 11.66, 11.3, 11.3, 11.0, 10.48, 10.26, 10.12, 9.82, 9.81, 4.29] Ac-\ntion: put down egg Reward: 0, Score 15, Done False\nSTATE 30\ndropped. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a somewhat ruined\negg here. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains:\na quantity of water Actions: [’take egg’, ’east’, ’open sack’, ’west’,\n’take bottle’, ’take sack’, ’take all from table’, ’close window’, ’take\nall’, ’open bottle’, ’up’, ’open egg’, ’jump’] Qvalues: [9.16, 9.1, 9.02,\n8.95, 8.85, 8.65, 8.54, 8.49, 8.48, 8.42, 8.29, 8.02, 0.73] Action: open\nsack Reward: 0, Score 15, Done False\nSTATE 31\nopening the brown sack reveals a lunch, and a clove of garlic. you\nare empty handed. kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a somewhat ruined egg here.\na bottle is sitting on the table. the glass bottle contains: a quantity\nof water a brown sack the brown sack contains: a lunch a clove of\ngarlic Actions: [’take egg’, ’take all from table’, ’take all’, ’take all\nfrom sack’, ’west’, ’take sack’, ’close sack’, ’take food’, ’take bottle’,\n’take garlic’, ’open egg’, ’open bottle’, ’close window’, ’east’, ’up’,\n’jump’] Qvalues: [9.83, 9.13, 8.97, 8.85, 8.69, 8.59, 8.55, 8.51, 8.43,\n8.31, 8.09, 7.93, 7.75, 7.02, 6.99, -1.14] Action: close sack Reward: 0,\nScore 15, Done False\nSTATE 32\nclosed. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a somewhat ruined\negg here. a bottle is sitting on the table. the glass bottle contains: a\nquantity of water a brown sack Actions: [’take egg’, ’take all’, ’take\nall from table’, ’take sack’, ’open sack’, ’take bottle’, ’close window’,\n’open egg’, ’open bottle’, ’west’, ’east’, ’up’, ’jump’] Qvalues: [11.85,\n11.45, 11.15, 11.06, 10.74, 10.47, 10.35, 10.14, 10.06, 9.89, 9.32, 9.24,\n2.32] Action: take bottle Reward: 0, Score 15, Done False\nSTATE 33\ntaken. you are carrying: a glass bottle the glass bottle contains: a\nquantity of water kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a somewhat ruined egg here.\nsitting on the kitchen table is: a brown sack Actions: [’take egg’,\n’take all’, ’west’, ’put water on table’, ’open sack’, ’take sack’, ’put\nbottle on table’, ’open egg’, ’open bottle’, ’close window’, ’up’, ’put\ndown bottle’, ’east’, ’jump’] Qvalues: [11.16, 10.75, 10.52, 9.92, 9.89,\n9.88, 9.78, 9.66, 9.51, 9.4, 9.36, 9.12, 8.99, 1.01] Action: close window\nReward: 0, Score 15, Done False\nSTATE 34\nthe window closes (more easily than it opened). you are carrying: a\nglass bottle the glass bottle contains: a quantity of water kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nslightly ajar. there is a somewhat ruined egg here. sitting on the\nkitchen table is: a brown sack Actions: [’take egg’, ’west’, ’take\nall’, ’open egg’, ’open sack’, ’take sack’, ’put water on table’, ’put\nbottle on table’, ’open bottle’, ’open window’, ’put down bottle’, ’up’,\n’jump’] Qvalues: [10.72, 9.98, 9.86, 9.09, 9.02, 8.96, 8.92, 8.83, 8.8,\n8.65, 8.29, 7.96, -1.33] Action: put water on table Reward: 0, Score\n15, Done False\nSTATE 35\ndone. you are carrying: a glass bottle kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is slightly ajar. there is a somewhat\nruined egg here. sitting on the kitchen table is: a quantity of water\na brown sack Actions: [’take egg’, ’take all’, ’west’, ’open window’,\n’open sack’, ’take sack’, ’open egg’, ’open bottle’, ’put bottle on table’,\n’put down bottle’, ’up’, ’jump’] Qvalues: [11.52, 10.95, 10.61, 10.33,\n10.11, 10.07, 10.03, 9.92, 9.8, 9.75, 8.45, 1.04] Action: put bottle on\ntable Reward: 0, Score 15, Done False\nSTATE 36\ndone. you are empty handed. kitchen you are in the kitchen of the\nwhite house. a table seems to have been used recently for the prepa-\nration of food. a passage leads to the west and a dark staircase can\nbe seen leading upward. a dark chimney leads down and to the east\nis a small window which is slightly ajar. there is a somewhat ruined\negg here. sitting on the kitchen table is: a glass bottle a quantity of\nwater a brown sack Actions: [’take egg’, ’take all’, ’open window’,\n’take all from table’, ’west’, ’take sack’, ’take bottle’, ’open sack’,\n’open bottle’, ’open egg’, ’up’, ’jump’] Qvalues: [11.36, 10.61, 10.58,\n10.56, 10.46, 10.29, 10.28, 10.13, 9.7, 9.65, 8.73, 1.69] Action: take sack\nReward: 0, Score 15, Done False\nSTATE 37\ntaken. you are carrying: a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is slightly ajar. there is a\nsomewhat ruined egg here. sitting on the kitchen table is: a glass\nbottle a quantity of water Actions: [’west’, ’take egg’, ’take all’, ’put\nsack on table’, ’open window’, ’open sack’, ’take bottle’, ’put down\nsack’, ’open egg’, ’open bottle’, ’up’, ’jump’] Qvalues: [9.59, 9.28,\n8.93, 8.76, 8.47, 8.47, 8.34, 8.32, 7.95, 7.83, 7.77, 0.83] Action: west\nReward: 0, Score 15, Done False\nSTATE 38\nliving room you are in the living room. there is a doorway to the\neast, a wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a large oriental rug\nin the center of the room. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on\nthe trophy case. you are carrying: a brown sack living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a large oriental rug in the center\nof the room. above the trophy case hangs an elvish sword of great\nantiquity. a battery powered brass lantern is on the trophy case.\nActions: [’take all’, ’push rug’, ’open sack’, ’put down sack’, ’open\ncase’, ’take sword’, ’take lantern’, ’turn on lantern’, ’throw sack at\nlantern’, ’east’] Qvalues: [10.66, 10.2, 9.58, 9.11, 9.0, 8.97, 8.77, 8.68,\n8.31, 7.35] Action: push rug Reward: 0, Score 15, Done False\nSTATE 39\nwith a great effort, the rug is moved to one side of the room, reveal-\ning the dusty cover of a closed trap door. you are carrying: a brown\nsack living room you are in the living room. there is a doorway to\nthe east, a wooden door with strange gothic lettering to the west,\nwhich appears to be nailed shut, a trophy case, and a closed trap\ndoor at your feet. above the trophy case hangs an elvish sword of\ngreat antiquity. a battery powered brass lantern is on the trophy\ncase. Actions: [’take all’, ’open trap’, ’open sack’, ’take sword’, ’put\ndown sack’, ’take lantern’, ’open case’, ’turn on lantern’, ’throw sack\nat lantern’, ’east’] Qvalues: [11.53, 11.1, 10.37, 10.0, 9.76, 9.69, 9.66,\n9.61, 9.1, 8.05] Action: open trap Reward: 0, Score 15, Done False\nSTATE 40\nthe door reluctantly opens to reveal a rickety staircase descending\ninto darkness. you are carrying: a brown sack living room you are\nin the living room. there is a doorway to the east, a wooden door\nwith strange gothic lettering to the west, which appears to be nailed\nshut, a trophy case, and a rug lying beside an open trap door. above\nthe trophy case hangs an elvish sword of great antiquity. a battery\npowered brass lantern is on the trophy case. Actions: [’take all’,\n’take lantern’, ’open sack’, ’put down sack’, ’take sword’, ’open case’,\n’throw sack at lantern’, ’turn on lantern’, ’close trap’, ’east’, ’down’]\nQvalues: [12.63, 11.7, 11.21, 10.74, 10.64, 10.54, 10.47, 10.07, 9.7, 9.09,\n7.73] Action: take all Reward: 0, Score 15, Done False\nSTATE 41\ntrophy case: the trophy case is securely fastened to the wall. sword:\ntaken. brass lantern: taken. carpet: the rug is extremely heavy and\ncannot be carried. you are carrying: a brass lantern a sword a brown\nsack living room you are in the living room. there is a doorway\nto the east, a wooden door with strange gothic lettering to the\nwest, which appears to be nailed shut, a trophy case, and a rug\nlying beside an open trap door. Actions: [’open sack’, ’put down\nall’, ’down’, ’open case’, ’put down sack’, ’throw sack at lantern’,\n’close trap’, ’put down lantern’, ’put down sword’, ’turn on lantern’,\n’east’] Qvalues: [14.22, 14.12, 13.88, 13.79, 13.66, 13.2, 13.14, 13.11,\n13.01, 12.88, 12.33] Action: put down all Reward: 0, Score 15, Done\nFalse\nSTATE 42\nbrass lantern: dropped. sword: dropped. brown sack: dropped. you\nare empty handed. living room you are in the living room. there is\na doorway to the east, a wooden door with strange gothic lettering\nto the west, which appears to be nailed shut, a trophy case, and\na rug lying beside an open trap door. there is a brown sack here.\nthere is a sword here. there is a brass lantern (battery powered)\nhere. Actions: [’open sack’, ’take lantern’, ’take sack’, ’take all’, ’take\nsword’, ’turn on lantern’, ’open case’, ’close trap’, ’down’, ’east’]\nQvalues: [11.7, 11.61, 11.57, 11.39, 11.37, 11.37, 10.79, 10.57, 10.13,\n10.1] Action: down Reward: 25, Score 40, Done False\nSTATE 43\nyou have moved into a dark place. the trap door crashes shut, and\nyou hear someone barring it. it is pitch black. you are likely to be\neaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[-7.44, -9.98] Action: south Reward: 0, Score 40, Done False\nSTATE 44\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.4, -3.4, -8.58] Action: east Reward: 0, Score 40, Done False\nSTATE 45\ngallery this is an art gallery. most of the paintings have been stolen\nby vandals with exceptional taste. the vandals left through either\nthe north or west exits. fortunately, there is still one chance for\nyou to be a vandal, for on the far wall is a painting of unparalleled\nbeauty. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. fortunately,\nthere is still one chance for you to be a vandal, for on the far wall\nis a painting of unparalleled beauty. Actions: [’take art’, ’north’,\n’west’] Qvalues: [6.02, 1.29, 0.19] Action: take art Reward: 4, Score\n44, Done False\nSTATE 46\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.41, 2.35, 2.15] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 47\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’west’, ’north’] Qval-\nues: [2.91, 1.66, 1.11] Action: take art Reward: 0, Score 44, Done False\nSTATE 48\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.38, 2.36, 2.16] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 49\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting\nby a neglected genius is here. Actions: [’take art’, ’west’, ’north’]\nQvalues: [2.8, 1.52, 0.96] Action: take art Reward: 0, Score 44, Done\nFalse\nSTATE 50\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [2.34, 2.29, 2.1] Action:\nnorth Reward: 0, Score 44, Done False\nSTATE 51\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are carrying: a painting it is pitch black.\nyou are likely to be eaten by a grue. Actions: [’south’, ’put down\npainting’, ’north’] Qvalues: [1.71, -2.26, -3.91] Action: south Reward:\n0, Score 44, Done False\nSTATE 52\ngallery you are carrying: a painting gallery this is an art gallery.\nmost of the paintings have been stolen by vandals with exceptional\ntaste. the vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.0, 1.94, 1.62] Action: put\ndown art Reward: 0, Score 44, Done False\nSTATE 53\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’west’, ’north’] Qval-\nues: [2.44, 1.08, 0.47] Action: take art Reward: 0, Score 44, Done False\nSTATE 54\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [2.23, 1.96, 1.84] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 55\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting\nby a neglected genius is here. Actions: [’take art’, ’west’, ’north’]\nQvalues: [2.1, 0.73, 0.08] Action: west Reward: 0, Score 44, Done\nFalse\nSTATE 56\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.75, -4.31, -9.21] Action: east Reward: 0, Score 44, Done False\nSTATE 57\ngallery a painting by a neglected genius is here. you are empty\nhanded. gallery this is an art gallery. most of the paintings have\nbeen stolen by vandals with exceptional taste. the vandals left\nthrough either the north or west exits. a painting by a neglected\ngenius is here. Actions: [’take art’, ’west’, ’north’] Qvalues: [1.99,\n0.53, -0.2] Action: take art Reward: 0, Score 44, Done False\nSTATE 58\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [1.93, 1.43, 1.35] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 59\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’wait’, ’west’, ’north’]\nQvalues: [1.48, 0.25, 0.06, -0.58] Action: wait Reward: 0, Score 44,\nDone False\nSTATE 60\ntime passes... you are empty handed. gallery this is an art gallery.\nmost of the paintings have been stolen by vandals with exceptional\ntaste. the vandals left through either the north or west exits. Ac-\ntions: [’north’, ’west’] Qvalues: [1.24, 0.15] Action: west Reward: 0,\nScore 44, Done False\nSTATE 61\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.87, -4.34, -9.29] Action: east Reward: 0, Score 44, Done False\nSTATE 62\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.38, 0.23] Action: west Reward: 0, Score\n44, Done False\nSTATE 63\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.92, -4.3, -9.3] Action: east Reward: 0, Score 44, Done False\nSTATE 64\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.29, 0.16] Action: north Reward: 0, Score\n44, Done False\nSTATE 65\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.59, -4.18] Action: south Reward: 0, Score 44, Done False\nSTATE 66\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.29, 0.17] Action: west Reward: 0, Score\n44, Done False\nSTATE 67\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.21, -4.03, -9.06] Action: east Reward: 0, Score 44, Done False\nSTATE 68\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.38, 0.25] Action: west Reward: 0, Score\n44, Done False\nSTATE 69\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.27, -3.95, -8.96] Action: east Reward: 0, Score 44, Done False\nSTATE 70\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.46, 0.31] Action: north Reward: 0, Score\n44, Done False\nSTATE 71\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.78, -3.93] Action: south Reward: 0, Score 44, Done False\nSTATE 72\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.62, 0.45] Action: west Reward: 0, Score\n44, Done False\nSTATE 73\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.32, -3.86, -8.82] Action: east Reward: 0, Score 44, Done False\nSTATE 74\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’, ’wait’] Qvalues: [0.75, 0.57, 0.39] Action: north Re-\nward: 0, Score 44, Done False\nSTATE 75\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.87, -3.84] Action: south Reward: 0, Score 44, Done False\nSTATE 76\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.93, 0.75] Action: north Reward: 0, Score\n44, Done False\nSTATE 77\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.99, -3.68] Action: south Reward: 0, Score 44, Done False\nSTATE 78\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.14, 0.97] Action: west Reward: 0, Score\n44, Done False\nSTATE 79\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.51, -3.52, -8.51] Action: east Reward: 0, Score 44, Done False\nSTATE 80\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.33, 1.18] Action: west Reward: 0, Score\n44, Done False\nSTATE 81\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.66, -3.32, -8.33] Action: east Reward: 0, Score 44, Done False\nSTATE 82\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.55, 1.4] Action: north Reward: 0, Score\n44, Done False\nSTATE 83\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.41, -3.15] Action: south Reward: 0, Score 44, Done False\nSTATE 84\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.65, 1.51] Action: west Reward: 0, Score\n44, Done False\nSTATE 85\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.81, -3.09, -8.17] Action: east Reward: 0, Score 44, Done False\nSTATE 86\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.66, 1.56] Action: west Reward: 0, Score\n44, Done False\nSTATE 87\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.79, -3.06, -8.2] Action: east Reward: 0, Score 44, Done False\nSTATE 88\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.6, 1.53] Action: north Reward: 0, Score\n44, Done False\nSTATE 89\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.48, -3.07] Action: south Reward: 0, Score 44, Done False\nSTATE 90\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.53, 1.5] Action: north Reward: 0, Score\n44, Done False\nSTATE 91\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.46, -3.08] Action: south Reward: 0, Score 44, Done False\nSTATE 92\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.45, 1.42] Action: west Reward: 0, Score\n44, Done False\nSTATE 93\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.46, -3.16, -8.41] Action: east Reward: 0, Score 44, Done False\nSTATE 94\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.38, 1.33] Action: north Reward: 0, Score\n44, Done False\nSTATE 95\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.27, -3.26] Action: south Reward: 0, Score 44, Done False\nSTATE 96\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.29, 1.22] Action: west Reward: 0, Score\n44, Done False\nSTATE 97\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.18, -3.36, -8.54] Action: east Reward: 0, Score 44, Done False\nSTATE 98\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.25, 1.13] Action: north Reward: 0, Score\n44, Done False\nSTATE 99\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.99, -3.47] Action: south Reward: 0, Score 44, Done True\nSTATE 100\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.21, 1.04] Action: reset Reward13458: 0,\nScore 0, Done False\nB.2\nInhumane\nSTATE 0\nYou awaken in your tent, lying on the floor, with a hangover like\nyou wouldn’t believe. As you lever yourself up, you notice that\nthere is complete silence instead of the usual morning complaints\nof the workers and Roboff’s usual shouting. This can only mean\none thing Roboff must have finally run off on you. Panicked, you\nlook around and see your amulet lying on the floor. At least Roboff\nhad the sense to not trigger off your alarm. Knowing him, he also\nhad the sense to take the emergency radio. How the megakrip\nare you going to get out of this one? INHUMANE : An Infralogic\nMassacre. Copyright ( C ) by Infobom Inhumane and Infobom are\ntrademarks of Sadistic Software, Ins. Revision 2 \/ Serial Number\n31415926 \/ Inform v1405 Library 5 \/ 8 First time players should type\n\" about \". In Your Tent You are in your tent in the encampment.\nThe tent appears to have been completely cleaned out. It is very\nhot in here and getting hotter. The tent flap to the south is open.\nOn the ground is the ancient jeweled amulet that you \" borrowed\n\" from Mr. Meshugge. You are holding nothing. In Your Tent You\nare in your tent in the encampment. The tent appears to have been\ncompletely cleaned out. It is very hot in here and getting hotter.\nThe tent flap to the south is open. On the ground is the ancient\njeweled amulet that you \" borrowed \" from Mr. Meshugge. Actions:\n[’south’, ’take amulet’] Qvalues: [6.04, 4.66] Action: south Reward:\n0, Score 0, Done False\nSTATE 1\ncenter of camp this is the center of your camp. the entire place\nseems to be completely deserted. you can see roboff’s tent to the\nsouth and the storage tent west. east is open desert. you are holding\nnothing. center of camp this is the center of your camp. the entire\nplace seems to be completely deserted. you can see roboff’s tent to\nthe south and the storage tent west. east is open desert. Actions:\n[’east’, ’north’, ’south’, ’west’] Qvalues: [7.01, 5.93, 5.76, 5.3] Action:\nsouth Reward: 0, Score 0, Done False\nSTATE 2\nroboff’s tent you are in francisco roboff’s tent. the place is bare\nexcept for the emergency signal radio and roboff’s toilet in one cor-\nner. there is a note taped to the tent wall. you are holding nothing.\nroboff’s tent you are in francisco roboff’s tent. the place is bare\nexcept for the emergency signal radio and roboff’s toilet in one\ncorner. there is a note taped to the tent wall. Actions: [’north’, ’take\nsignal off radio’] Qvalues: [6.3, 4.72] Action: north Reward: 0, Score\n0, Done False\nSTATE 3\ncenter of campyou are holding nothing. center of camp this is the\ncenter of your camp. the entire place seems to be completely de-\nserted. you can see roboff’s tent to the south and the storage tent\nwest. east is open desert. Actions: [’east’, ’north’, ’south’, ’west’]\nQvalues: [7.15, 5.82, 5.72, 5.61] Action: north Reward: 0, Score 0,\nDone False\nSTATE 4\nin your tent on the ground is the ancient jeweled amulet that you \"\nborrowed \" from mr. meshugge. you are holding nothing. in your\ntent you are in your tent in the encampment. the tent appears to\nhave been completely cleaned out. it is very hot in here and get-\nting hotter. the tent flap to the south is open. on the ground is the\nancient jeweled amulet that you \" borrowed \" from mr. meshugge.\nActions: [’south’, ’take amulet’] Qvalues: [6.36, 5.16] Action: south\nReward: 0, Score 0, Done False\nSTATE 5\ncenter of campyou are holding nothing. center of camp this is the\ncenter of your camp. the entire place seems to be completely de-\nserted. you can see roboff’s tent to the south and the storage tent\nwest. east is open desert. Actions: [’east’, ’north’, ’west’, ’south’]\nQvalues: [7.14, 5.84, 5.66, 5.66] Action: east Reward: 0, Score 0, Done\nFalse\nSTATE 6\nyou venture out onto the sands and rapidly lose sight of camp. in\nthe desert all you can see in any direction is sand and sunlight. you\nare sweating like a pig. you are holding nothing. in the desert all\nyou can see in any direction is sand and sunlight. you are sweating\nlike a pig. Actions: [’put all down’, ’north’, ’west’] Qvalues: [7.71,\n7.29, 6.3] Action: put all down Reward: 0, Score 0, Done False\nSTATE 7\nit falls and is lost forever in the sands. it is very dark. before you\ncan do anything, something begins gnawing on you. you have died\nyou are floating disembodied in absolute blackness. time passes at a\nstrange, relative rate. after an infinite \/ infinitesmal time, you sense\na presence near you. a voice speaks, resounding in your head as\nin a hollow gourd... \" ah! another of those wimpy delvers come to\nsteal my treasure. how boring. most of it was taken by that other\nguy who had the sense to avoid the nastier traps... but i am getting\npretty tired of guarding what’s left. i don’t suppose that you have\nany chance of thinking your way through my traps, of course. but\ni’ll let you try, and i won’t even vaporize you when you try to\nleave. in fact, i’ll make you a deal. if you prove to me that you are\nan incompetent harry sullivan by getting yourself killed by every\nsingle one of my deathtraps, after getting as far towards solving it\nas possible, then i’ll give up and let you take the coffin. \" that sound\ngood? i thought so. \" that hardly counts as being killed by a trap,\nyou know. even you could have gotten farther into that situation. \"\nyou don’t seem to have really gotten fooled by any traps yet. maybe\nthere is hope for you. \" round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nthere is a shovel lying here. you are holding : a torch round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’push button’, ’north’, ’east’, ’west’, ’south’, ’take button off torch’,\n’take shovel’, ’put torch down’, ’examine obelisk’, ’up’] Qvalues:\n[8.67, 8.06, 8.02, 7.76, 7.63, 7.4, 6.97, 6.88, 6.55, 5.09] Action: north\nReward: 0, Score 0, Done False\nSTATE 8\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you don’t\nseem to have really gotten fooled by any traps yet. maybe there is\nhope for you. \" round room this room is at the peak of the pyramid.\nit is perfectly conical. in the center is a tall stone obelisk that you\ncould probably use to get back up to the fake toilet. there are four\nstairways leaving the room at the four compass points. there is a\nshovel lying here. you are holding : a torch round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. Actions: [’push\nbutton’, ’east’, ’north’, ’west’, ’south’, ’take button off torch’, ’take\nshovel’, ’put torch down’, ’examine obelisk’, ’up’] Qvalues: [8.76,\n8.1, 7.99, 7.62, 7.55, 7.38, 6.93, 6.9, 6.49, 5.19] Action: take button off\ntorch Reward: 0, Score 0, Done False\nSTATE 9\nremoved. you are holding : a torch button a torch round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’push button’, ’north’, ’west’, ’east’, ’south’, ’take shovel’, ’put all\ndown’, ’put torch down’, ’put button down’, ’examine obelisk’, ’up’]\nQvalues: [8.26, 8.02, 7.8, 7.78, 7.59, 6.96, 6.76, 6.72, 6.71, 6.49, 5.56]\nAction: push button Reward: 0, Score 0, Done False\nSTATE 10\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’north’, ’south’, ’take shovel’, ’put button down’, ’examine\nobelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [9.97,\n8.69, 8.26, 8.14, 7.65, 7.56, 7.41, 7.09, 6.9, 6.68, 5.32] Action: south\nReward: 0, Score 0, Done False\nSTATE 11\ncorridor at alcoves the corridor goes north to the stairs and south\nfrom here. there are alcoves to the east and west. in the floor be-\ntween the alcoves is a quartz window. you are holding : a torch\nbutton a torch ( providing light ) corridor at alcoves the corridor\ngoes north to the stairs and south from here. there are alcoves to the\neast and west. in the floor between the alcoves is a quartz window.\nActions: [’south’, ’west’, ’push button’, ’north’, ’put button down’,\n’put all down’, ’put torch down’, ’east’] Qvalues: [8.79, 8.53, 8.07,\n7.82, 7.62, 7.56, 7.45, 7.41] Action: south Reward: 0, Score 0, Done\nFalse\nSTATE 12\noutside door you are at the end of the hall. to the south is a wooden\ndoor with a steel knob on it. the door is closed. you are holding :\na torch button a torch ( providing light ) outside door you are at\nthe end of the hall. to the south is a wooden door with a steel knob\non it. the door is closed. Actions: [’open door’, ’push button’, ’take\nbrass off door’, ’put button down’, ’north’, ’put torch down’, ’put\nall down’] Qvalues: [9.73, 8.84, 8.64, 8.39, 8.32, 8.16, 8.05] Action:\npush button Reward: 0, Score 0, Done False\nSTATE 13\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) outside door you are\nat the end of the hall. to the south is a wooden door with a steel\nknob on it. the door is closed. Actions: [’open door’, ’push button’,\n’take brass off door’, ’put button down’, ’north’, ’put all down’, ’put\ntorch down’] Qvalues: [9.83, 8.92, 8.7, 8.67, 8.23, 8.15, 8.09] Action:\npush button Reward: 0, Score 0, Done False\nSTATE 14\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) outside door you are\nat the end of the hall. to the south is a wooden door with a steel\nknob on it. the door is closed. Actions: [’open door’, ’push button’,\n’put button down’, ’take brass off door’, ’north’, ’put all down’, ’put\ntorch down’] Qvalues: [9.93, 9.04, 8.76, 8.76, 8.28, 8.19, 8.13] Action:\nopen door Reward: 0, Score 0, Done False\nSTATE 15\nthe door slams open. it is apparently on a very strong spring. if\nyou were on the other side, you would have been slammed into\nthe wall. as it is, the steel door knob slams into the inside wall. you\nare holding : a torch button a torch ( providing light ) outside door\nyou are at the end of the hall. to the south is a wooden door with a\nsteel knob on it. the door is open. Actions: [’south’, ’push button’,\n’take brass off door’, ’put button down’, ’close door’, ’put all down’,\n’north’, ’put torch down’] Qvalues: [10.85, 9.79, 9.52, 9.4, 9.14, 8.61,\n8.56, 8.52] Action: south Reward: 0, Score 0, Done False\nSTATE 16\nroom with chest this room is empty except for a small chest at one\nend. the only exit is the door to the north. the door is open. you\nsee something glimmering behind the door. you’ll have to close the\ndoor to get it. the chest is closed and locked. you are holding : a\ntorch button a torch ( providing light ) room with chest this room\nis empty except for a small chest at one end. the only exit is the\ndoor to the north. the door is open. you see something glimmering\nbehind the door. you’ll have to close the door to get it. the chest\nis closed and locked. Actions: [’close door’, ’push button’, ’take\nbrass off door’, ’put torch down’, ’north’, ’put all down’, ’put but-\nton down’] Qvalues: [12.47, 11.55, 11.36, 10.31, 10.26, 10.19, 10.05]\nAction: close door Reward: 0, Score 0, Done False\nSTATE 17\nafter much tugging, you manage to get the door shut. behind it\nis a brass needle that you might use to unlock the chest. you are\nholding : a torch button a torch ( providing light ) room with chest\nthis room is empty except for a small chest at one end. the only exit\nis the door to the north. the door is closed. on the ground is a long\nbrass needle. the chest is closed and locked. Actions: [’open chest\nwith needle’, ’take brass’, ’push button’, ’put all down’, ’put torch\ndown’, ’put button down’] Qvalues: [13.9, 12.14, 12.14, 11.0, 10.83,\n10.5] Action: open chest with needle Reward: 0, Score 0, Done False\nSTATE 18\n( first taking the brass needle ) there is a click and the chest lid pops\nopen. inside is a priceless dorkmid! as you stare at it, you notice\nthe smell of coal gas leaking out. you are holding : a brass needle a\ntorch button a torch ( providing light ) room with chest this room is\nempty except for a small chest at one end. the only exit is the door\nto the north. the door is closed. the chest is open. the chest contains\n: a priceless dorkmid Actions: [’open door’, ’put needle down’, ’put\nneedle in chest’, ’push button’, ’put button in chest’, ’put torch in\nchest’, ’put all in chest’, ’put all down’, ’put torch down’, ’put button\ndown’, ’take dorkmid’] Qvalues: [15.18, 14.95, 14.61, 13.77, 13.53,\n13.52, 13.44, 12.72, 12.46, 12.15, 11.09] Action: open door Reward:\n10, Score 10, Done False\nSTATE 19\nthe door slams open. you barely avoid the swinging knob. instead,\nthe steel knob hits the wall with a bang. the wall seems to be made\nof flint. you have died you are back in the infinite blackness. eventu-\nally the voice returns, senses you disgustedly, and speaks. \" sparks\nin a room full of natural gas is not really a smart thing to do. \"\nyou’ve only gotten killed by one trap. not bad. \" round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. you are\nholding : a torch button a torch ( providing light ) round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’east’, ’north’, ’push button’, ’south’, ’take shovel’, ’west’, ’examine\nobelisk’, ’put button down’, ’put torch down’, ’put all down’, ’up’]\nQvalues: [11.75, 8.96, 8.88, 8.11, 7.56, 7.54, 7.53, 7.32, 6.79, 6.74, 6.41]\nAction: south Reward: 0, Score 10, Done False\nSTATE 20\ncorridor at alcovesyou are holding : a torch button a torch ( provid-\ning light ) corridor at alcoves the corridor goes north to the stairs\nand south from here. there are alcoves to the east and west. in the\nfloor between the alcoves is a quartz window. Actions: [’south’,\n’west’, ’push button’, ’put button down’, ’north’, ’put all down’,\n’east’, ’put torch down’] Qvalues: [8.68, 8.19, 7.98, 7.51, 7.43, 7.36,\n7.34, 7.22] Action: put button down Reward: 0, Score 10, Done False\nSTATE 21\ndropped. you are holding : a torch ( providing light ) corridor at\nalcoves the corridor goes north to the stairs and south from here.\nthere are alcoves to the east and west. in the floor between the\nalcoves is a quartz window. you can see a torch button here. Ac-\ntions: [’south’, ’take button’, ’west’, ’push button’, ’put torch down’,\n’north’, ’east’] Qvalues: [7.64, 7.58, 7.48, 7.33, 6.72, 6.63, 6.35] Action:\nwest Reward: 0, Score 10, Done False\nSTATE 22\nwest alcove this alcove has a stone platform set in the floor. the\nplatform shifts a little but does not move when you step on it. stick-\ning out of the platform is an iron crank. you are holding : a torch (\nproviding light ) west alcove this alcove has a stone platform set in\nthe floor. the platform shifts a little but does not move when you\nstep on it. sticking out of the platform is an iron crank. Actions:\n[’turn crank’, ’east’, ’put torch down’] Qvalues: [8.75, 7.73, 6.44]\nAction: east Reward: 0, Score 10, Done False\nSTATE 23\ncorridor at alcoves you can see a torch button here. you are holding\n: a torch ( providing light ) corridor at alcoves the corridor goes\nnorth to the stairs and south from here. there are alcoves to the east\nand west. in the floor between the alcoves is a quartz window. you\ncan see a torch button here. Actions: [’south’, ’take button’, ’west’,\n’push button’, ’put torch down’, ’north’, ’east’] Qvalues: [7.75, 7.69,\n7.48, 7.46, 6.76, 6.7, 6.62] Action: take button Reward: 0, Score 10,\nDone False\nSTATE 24\ntaken. you are holding : a torch button a torch ( providing light ) cor-\nridor at alcoves the corridor goes north to the stairs and south from\nhere. there are alcoves to the east and west. in the floor between the\nalcoves is a quartz window. Actions: [’south’, ’west’, ’push button’,\n’put button down’, ’put all down’, ’put torch down’, ’north’, ’east’]\nQvalues: [8.53, 7.97, 7.74, 7.28, 7.28, 7.24, 7.09, 6.74] Action: west\nReward: 0, Score 10, Done False\nSTATE 25\nwest alcoveyou are holding : a torch button a torch ( providing\nlight ) west alcove this alcove has a stone platform set in the floor.\nthe platform shifts a little but does not move when you step on it.\nsticking out of the platform is an iron crank. Actions: [’turn crank’,\n’push button’, ’east’, ’put button down’, ’put torch down’, ’put all\ndown’] Qvalues: [9.2, 7.92, 7.65, 7.19, 6.75, 6.7] Action: turn crank\nReward: 0, Score 10, Done False\nSTATE 26\nthe platform slowly moves down a shaft below it. in the other al-\ncove the other platform slowly moves up, counterbalancing the\none you are on. on platform you are on the crank platform at the\nbottom of the shaft under the alcove. there is an opening to the\neast. you are holding : a torch button a torch ( providing light ) on\nplatform you are on the crank platform at the bottom of the shaft\nunder the alcove. there is an opening to the east. Actions: [’push\nbutton’, ’turn crank’, ’put button down’, ’put torch down’, ’put all\ndown’, ’east’] Qvalues: [10.23, 10.22, 9.02, 8.93, 8.7, 8.63] Action:\npush button Reward: 0, Score 10, Done False\nSTATE 27\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) on platform you are\non the crank platform at the bottom of the shaft under the alcove.\nthere is an opening to the east. Actions: [’turn crank’, ’push button’,\n’put button down’, ’east’, ’put torch down’, ’put all down’] Qvalues:\n[11.21, 10.29, 9.09, 9.06, 8.8, 8.62] Action: put all down Reward: 0,\nScore 10, Done False\nSTATE 28\ntorch button : dropped. torch : dropped. you are holding nothing.\non platform you are on the crank platform at the bottom of the\nshaft under the alcove. there is an opening to the east. lying on the\nground is a brass torch of the \" self igniting \" type. the ignite button\nis on the handle. you can also see a torch button here. Actions: [’take\nall’, ’take torch’, ’take button’, ’push button’, ’turn crank’, ’east’]\nQvalues: [9.66, 8.96, 8.88, 8.8, 8.21, 7.81] Action: take all Reward: 0,\nScore 10, Done False\nSTATE 29\niron crank : a wimp like you will never get that done. torch : taken.\ntorch button : taken. you are holding : a torch button a torch (\nproviding light ) on platform you are on the crank platform at the\nbottom of the shaft under the alcove. there is an opening to the\neast. Actions: [’turn crank’, ’push button’, ’put button down’, ’put\ntorch down’, ’put all down’, ’east’] Qvalues: [11.85, 10.14, 8.88, 8.85,\n8.72, 8.24] Action: east Reward: 0, Score 10, Done False\nSTATE 30\nin tiny room this is a tiny room. the only exit is back west. you can\nsee the quartz window directly above your head. there is a huge\ndiamond lying on the ground! you are holding : a torch button a\ntorch ( providing light ) in tiny room this is a tiny room. the only\nexit is back west. you can see the quartz window directly above\nyour head. there is a huge diamond lying on the ground! Actions:\n[’west’, ’take diamond’, ’push button’, ’put torch down’, ’put button\ndown’, ’put all down’] Qvalues: [10.17, 9.35, 8.97, 8.14, 8.13, 8.12]\nAction: west Reward: 0, Score 10, Done False\nSTATE 31\non platformyou are holding : a torch button a torch ( providing\nlight ) on platform you are on the crank platform at the bottom of\nthe shaft under the alcove. there is an opening to the east. Actions:\n[’turn crank’, ’push button’, ’put button down’, ’put torch down’,\n’put all down’, ’east’] Qvalues: [11.12, 10.08, 8.86, 8.74, 8.66, 7.84]\nAction: turn crank Reward: 10, Score 20, Done False\nSTATE 32\nthe platform moves back up to the alcove level. in the other alcove,\nyou see the other platform moving back down until both of the\nplatforms are even. west alcove this alcove has a stone platform\nset in the floor. the platform shifts a little but does not move when\nyou step on it. sticking out of the platform is an iron crank. as the\nother platform moves down from the hole in the ceiling, you see a\nsmall robot standing on it. when it gets down to the floor, it looks\nat you, says \" ex trrr mnnn ate \", points an arm at you, and turns\nyou into a small pile of monatomic dust. you have died you are\nback in the infinite blackness. eventually the voice returns, senses\nyou disgustedly, and speaks. \" giving nasties a free ride down to\nwhere you are seems a little stupid. \" you’ve been killed by two\ntraps. that isn’t really so bad. \" round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. you are holding : a\ntorch button a torch ( providing light ) round room this room is at\nthe peak of the pyramid. it is perfectly conical. in the center is a\ntall stone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’push button’,\n’north’, ’south’, ’take shovel’, ’examine obelisk’, ’put button down’,\n’west’, ’put torch down’, ’put all down’, ’east’, ’up’] Qvalues: [8.24,\n7.81, 7.57, 7.32, 7.08, 6.95, 6.66, 6.58, 6.47, 6.31, 5.27] Action: west\nReward: 0, Score 20, Done False\nSTATE 33\nt intersection the east \/ west corridor branches to the south here.\nyou are holding : a torch button a torch ( providing light ) t inter-\nsection the east \/ west corridor branches to the south here. Actions:\n[’east’, ’push button’, ’west’, ’put torch down’, ’put all down’, ’put\nbutton down’, ’south’] Qvalues: [7.36, 6.89, 6.85, 6.17, 6.08, 6.05,\n5.85] Action: east Reward: 0, Score 20, Done False\nSTATE 34\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’north’,\n’south’, ’take shovel’, ’examine obelisk’, ’east’, ’put button down’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [8.32, 7.79,\n7.61, 7.21, 7.12, 6.97, 6.93, 6.61, 6.56, 6.4, 5.75] Action: put button\ndown Reward: 0, Score 20, Done False\nSTATE 35\ndropped. you are holding : a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to\nget back up to the fake toilet. there are four stairways leaving the\nroom at the four compass points. there is a shovel lying here. you\ncan also see a torch button here. Actions: [’take button’, ’north’,\n’push button’, ’take all’, ’take shovel’, ’east’, ’south’, ’west’, ’examine\nobelisk’, ’put torch down’, ’up’] Qvalues: [7.53, 7.32, 7.3, 7.18, 7.07,\n7.0, 6.82, 6.78, 6.67, 6.18, 5.21] Action: north Reward: 0, Score 20,\nDone False\nSTATE 36\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. you are holding : a\ntorch ( providing light ) landing this is a landing on the stairway.\nyou can go down to the north or up to the south. a passage leads\neast. Actions: [’north’, ’south’, ’east’, ’put torch down’] Qvalues:\n[8.16, 6.7, 6.56, 6.28] Action: east Reward: 0, Score 20, Done False\nSTATE 37\nt intersection the hallway goes east and west here with a branch\nnorth. you are holding : a torch ( providing light ) t intersection\nthe hallway goes east and west here with a branch north. Actions:\n[’west’, ’north’, ’put torch down’, ’east’] Qvalues: [7.2, 6.93, 6.24,\n6.01] Action: put torch down Reward: 0, Score 20, Done False\nSTATE 38\ndropped. you are holding nothing. t intersection the hallway goes\neast and west here with a branch north. lying on the ground is a\nbrass torch of the \" self igniting \" type. the ignite button is on the\nhandle. Actions: [’west’, ’north’, ’east’, ’take torch’] Qvalues: [6.88,\n6.7, 6.47, 6.28] Action: east Reward: 0, Score 20, Done False\nSTATE 39\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve been\nkilled by two traps. that isn’t really so bad. \" round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. you can also see a torch\nbutton here. you are holding : a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. you can\nalso see a torch button here. Actions: [’take button’, ’north’, ’push\nbutton’, ’take all’, ’take shovel’, ’south’, ’west’, ’examine obelisk’,\n’put torch down’, ’east’, ’up’] Qvalues: [7.77, 7.65, 7.54, 7.45, 7.14,\n7.03, 6.69, 6.66, 6.31, 6.06, 5.33] Action: north Reward: 0, Score 20,\nDone False\nSTATE 40\nlandingyou are holding : a torch ( providing light ) landing this is\na landing on the stairway. you can go down to the north or up to\nthe south. a passage leads east. Actions: [’north’, ’south’, ’east’, ’put\ntorch down’] Qvalues: [8.24, 6.67, 6.57, 6.31] Action: north Reward:\n0, Score 20, Done False\nSTATE 41\nbottom of shaft you are at the bottom of a ladder that goes up a\nshaft from here. to the south is the stairway up. on the floor is a\nsmall coin bag made of leather. it appears to have something in it.\nyou are holding : a torch ( providing light ) bottom of shaft you are\nat the bottom of a ladder that goes up a shaft from here. to the south\nis the stairway up. on the floor is a small coin bag made of leather.\nit appears to have something in it. Actions: [’up’, ’take coin-bag’,\n’south’, ’put torch down’] Qvalues: [9.37, 8.64, 7.42, 7.26] Action:\ntake coin-bag Reward: 0, Score 20, Done False\nSTATE 42\ntaken. you are holding : a small leather coin bag a torch ( providing\nlight ) bottom of shaft you are at the bottom of a ladder that goes up\na shaft from here. to the south is the stairway up. Actions: [’up’, ’put\ncoin-bag down’, ’south’, ’put torch down’, ’put all down’] Qvalues:\n[9.71, 8.47, 7.53, 7.14, 7.05] Action: up Reward: 0, Score 20, Done\nFalse\nSTATE 43\nyou climb up the ladder to a hole at the top of the shaft. exercise\nwheel room this room is laid out like an exercise wheel for rats ex-\ncept that it is 40 ft. across. you can turn the wheel by walking north\nor south. equally spaced around the rim are twelve coin holders.\nsome have gold coins in them. the coin holder at your feet has a\ncoin in it. the hole is at the bottom of the wheel. below it is the shaft\nthat you entered this room by. you are holding : a small leather coin\nbag a torch ( providing light ) exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet has a coin in it. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered this\nroom by. Actions: [’south’, ’put coin-bag down’, ’put torch down’,\n’put all down’, ’down’, ’north’] Qvalues: [10.62, 8.9, 8.48, 8.29, 8.23,\n8.13] Action: south Reward: 0, Score 20, Done False\nSTATE 44\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the north side. you\nare holding : a small leather coin bag a torch ( providing light )\nexercise wheel room this room is laid out like an exercise wheel\nfor rats except that it is 40 ft. across. you can turn the wheel by\nwalking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is slightly up the north side. Actions:\n[’south’, ’put coin-bag down’, ’north’, ’put torch down’, ’put all\ndown’] Qvalues: [11.58, 9.76, 9.28, 9.2, 9.07] Action: put torch down\nReward: 0, Score 20, Done False\nSTATE 45\ndropped. you are holding : a small leather coin bag exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is slightly up the north side. lying on the ground is a\nbrass torch of the \" self igniting \" type. the ignite button is on the\nhandle. Actions: [’take brass’, ’south’, ’put coin-bag down’, ’north’]\nQvalues: [10.36, 10.14, 9.24, 7.98] Action: north Reward: 0, Score 20,\nDone False\nSTATE 46\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is at the bottom of the wheel.\nbelow it is the shaft that you entered this room by. you are holding\n: a small leather coin bag exercise wheel room this room is laid out\nlike an exercise wheel for rats except that it is 40 ft. across. you can\nturn the wheel by walking north or south. equally spaced around\nthe rim are twelve coin holders. some have gold coins in them. the\ncoin holder at your feet has a coin in it. the hole is at the bottom of\nthe wheel. below it is the shaft that you entered this room by. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. Actions: [’south’, ’take torch’, ’put coin-bag\ndown’, ’down’, ’north’] Qvalues: [8.84, 8.82, 8.14, 7.12, 7.07] Action:\nput coin-bag down Reward: 0, Score 20, Done False\nSTATE 47\ndropped. you are holding nothing. exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet has a coin in it. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered\nthis room by. on the floor is a small coin bag made of leather. it\nappears to have something in it. lying on the ground is a brass\ntorch of the \" self igniting \" type. the ignite button is on the handle.\nActions: [’take all’, ’south’, ’take torch’, ’take coin-bag’, ’take in\nfloor’, ’north’] Qvalues: [9.69, 9.44, 9.1, 8.53, 8.18, 7.63] Action: take\nall Reward: 0, Score 20, Done False\nSTATE 48\nsmall leather coin bag : taken. torch : taken. coin holder : you pick\nup the coin and put it in your bag. you are holding : a torch ( pro-\nviding light ) a small leather coin bag exercise wheel room this\nroom is laid out like an exercise wheel for rats except that it is 40 ft.\nacross. you can turn the wheel by walking north or south. equally\nspaced around the rim are twelve coin holders. some have gold\ncoins in them. the coin holder at your feet is empty. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered this\nroom by. Actions: [’south’, ’put coin-bag down’, ’put torch down’,\n’put all down’, ’north’, ’down’] Qvalues: [9.59, 8.74, 8.14, 8.11, 7.7,\n7.53] Action: north Reward: 0, Score 20, Done False\nSTATE 49\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the south side. you\nare holding : a torch ( providing light ) a small leather coin bag\nexercise wheel room this room is laid out like an exercise wheel\nfor rats except that it is 40 ft. across. you can turn the wheel by\nwalking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at\nyour feet has a coin in it. the hole is slightly up the south side.\nActions: [’south’, ’put coin-bag down’, ’put all down’, ’put torch\ndown’, ’north’] Qvalues: [8.63, 8.06, 7.47, 7.36, 7.29] Action: put\ncoin-bag down Reward: 0, Score 20, Done False\nSTATE 50\ndropped. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is slightly up the south side. on the floor is a small coin\nbag made of leather. it appears to have something in it. Actions:\n[’south’, ’take coin-bag’, ’north’, ’put torch down’] Qvalues: [9.01,\n8.69, 8.03, 7.6] Action: south Reward: 0, Score 20, Done False\nSTATE 51\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet is empty. the hole is at the bottom of the wheel. below it\nis the shaft that you entered this room by. you are holding : a torch\n( providing light ) exercise wheel room this room is laid out like an\nexercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet is empty. the hole is at the bottom of the wheel.\nbelow it is the shaft that you entered this room by. on the floor is a\nsmall coin bag made of leather. it appears to have something in it.\nActions: [’south’, ’take coin-bag’, ’put torch down’, ’north’, ’take\nin floor’] Qvalues: [10.48, 10.08, 8.75, 8.58, 8.15] Action: put torch\ndown Reward: 0, Score 20, Done False\nSTATE 52\ndropped. you are holding nothing. exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet is empty. the hole is at the bottom\nof the wheel. below it is the shaft that you entered this room by.\nlying on the ground is a brass torch of the \" self igniting \" type. the\nignite button is on the handle. on the floor is a small coin bag made\nof leather. it appears to have something in it. Actions: [’take all’,\n’south’, ’take torch’, ’take coin-bag’, ’take in floor’, ’north’] Qvalues:\n[10.04, 9.65, 9.47, 8.81, 8.42, 8.17] Action: take torch Reward: 0, Score\n20, Done False\nSTATE 53\ntaken. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north\nor south. equally spaced around the rim are twelve coin holders.\nsome have gold coins in them. the coin holder at your feet is empty.\nthe hole is at the bottom of the wheel. below it is the shaft that\nyou entered this room by. on the floor is a small coin bag made of\nleather. it appears to have something in it. Actions: [’south’, ’take\ncoin-bag’, ’put torch down’, ’north’, ’take in floor’] Qvalues: [10.47,\n9.96, 8.81, 8.62, 8.6] Action: south Reward: 0, Score 20, Done False\nSTATE 54\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the north side. you\nare holding : a torch ( providing light ) exercise wheel room this\nroom is laid out like an exercise wheel for rats except that it is 40 ft.\nacross. you can turn the wheel by walking north or south. equally\nspaced around the rim are twelve coin holders. some have gold\ncoins in them. the coin holder at your feet has a coin in it. the hole\nis slightly up the north side. on the floor is a small coin bag made of\nleather. it appears to have something in it. Actions: [’south’, ’take\ncoin-bag’, ’put torch down’, ’north’] Qvalues: [11.74, 11.16, 9.81,\n9.42] Action: take coin-bag Reward: 0, Score 20, Done False\nSTATE 55\ntaken. you are holding : a small leather coin bag a torch ( providing\nlight ) exercise wheel room this room is laid out like an exercise\nwheel for rats except that it is 40 ft. across. you can turn the wheel\nby walking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is slightly up the north side. Actions:\n[’south’, ’put coin-bag down’, ’north’, ’put torch down’, ’put all\ndown’] Qvalues: [12.41, 10.59, 10.06, 9.98, 9.9] Action: south Re-\nward: 0, Score 20, Done False\nSTATE 56\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder\nat your feet has a coin in it. the hole is less than half way up the\nnorth side. you are holding : a small leather coin bag a torch (\nproviding light ) exercise wheel room this room is laid out like an\nexercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet has a coin in it. the hole is less than half way\nup the north side. Actions: [’south’, ’put coin-bag down’, ’put torch\ndown’, ’north’, ’put all down’] Qvalues: [13.58, 11.65, 10.99, 10.91,\n10.85] Action: put all down Reward: 0, Score 20, Done False\nSTATE 57\nsmall leather coin bag : dropped. torch : dropped. you are holding\nnothing. exercise wheel room this room is laid out like an exercise\nwheel for rats except that it is 40 ft. across. you can turn the wheel\nby walking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is less than half way up the north side.\nlying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. on the floor is a small coin bag\nmade of leather. it appears to have something in it. Actions: [’take\nall’, ’south’, ’take torch’, ’take coin-bag’, ’north’] Qvalues: [12.65,\n12.31, 12.0, 11.39, 9.82] Action: south Reward: 0, Score 20, Done False\nSTATE 58\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is half way up the north side. you\nare holding nothing. exercise wheel room this room is laid out like\nan exercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet has a coin in it. the hole is half way up the north\nside. lying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. on the floor is a small coin bag\nmade of leather. it appears to have something in it. Actions: [’take\nall’, ’south’, ’take torch’, ’take coin-bag’, ’north’] Qvalues: [14.04,\n13.49, 13.35, 12.75, 10.79] Action: take torch Reward: 0, Score 20,\nDone False\nSTATE 59\ntaken. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is half way up the north side. on the floor is a small coin\nbag made of leather. it appears to have something in it. Actions:\n[’south’, ’take coin-bag’, ’put torch down’, ’north’] Qvalues: [14.71,\n14.12, 12.3, 11.95] Action: south Reward: 0, Score 20, Done False\nSTATE 60\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is more than half way up the north\nside. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is more than half way up the north side. on the floor\nis a small coin bag made of leather. it appears to have something\nin it. Actions: [’south’, ’take coin-bag’, ’put torch down’, ’north’]\nQvalues: [15.89, 15.25, 13.22, 13.01] Action: south Reward: 10, Score\n30, Done False\nSTATE 61\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder\nat your feet has a coin in it. the hole is almost to the top on the\nnorth side. behind it is a hole from which water is pouring very fast.\nthe water turns the wheel back to its original position, then pours\ndown the hole. you are taken with it. you have died you are back\nin the infinite blackness. eventually the voice returns, senses you\ndisgustedly, and speaks. \" don’t you know how lab rats eventually\nend up? \" three deathtraps have zocked you so far. you’re beginning\nto seem rather dumb. \" round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nthere is a shovel lying here. you can also see a torch button here.\nyou are holding : a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. you can also see\na torch button here. Actions: [’take button’, ’north’, ’push button’,\n’take all’, ’south’, ’take shovel’, ’west’, ’examine obelisk’, ’put torch\ndown’, ’east’, ’up’] Qvalues: [8.61, 8.49, 8.34, 8.29, 7.95, 7.88, 7.43,\n7.37, 7.15, 7.11, 5.9] Action: put torch down Reward: 0, Score 30,\nDone False\nSTATE 62\ndropped. you are holding nothing. round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. lying on the ground is a brass torch of the \" self\nigniting \" type. the ignite button is on the handle. there is a shovel\nlying here. you can also see a torch button here. Actions: [’west’,\n’north’, ’take all’, ’push button’, ’south’, ’east’, ’take button’, ’take\ntorch’, ’take shovel’, ’examine obelisk’, ’up’] Qvalues: [7.85, 7.68,\n7.68, 7.68, 7.66, 7.62, 7.51, 7.44, 7.13, 6.9, 6.1] Action: south Reward:\n0, Score 30, Done False\nSTATE 63\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know. even\nyou could have gotten farther into that situation. \" three deathtraps\nhave zocked you so far. you’re beginning to seem rather dumb. \"\nround room this room is at the peak of the pyramid. it is perfectly\nconical. in the center is a tall stone obelisk that you could probably\nuse to get back up to the fake toilet. there are four stairways leaving\nthe room at the four compass points. there is a shovel lying here.\nyou can also see a torch button here. you are holding : a torch (\nproviding light ) round room this room is at the peak of the pyramid.\nit is perfectly conical. in the center is a tall stone obelisk that you\ncould probably use to get back up to the fake toilet. there are four\nstairways leaving the room at the four compass points. there is a\nshovel lying here. you can also see a torch button here. Actions:\n[’take button’, ’north’, ’push button’, ’take all’, ’south’, ’take shovel’,\n’west’, ’examine obelisk’, ’put torch down’, ’east’, ’up’] Qvalues:\n[8.62, 8.38, 8.34, 8.28, 7.94, 7.89, 7.54, 7.35, 7.17, 6.62, 5.78] Action:\ntake all Reward: 0, Score 30, Done False\nSTATE 64\ntorch button : taken. shovel : taken. stone obelisk : a wimp like you\nwill never get that done. you are holding : a shovel a torch button a\ntorch ( providing light ) round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nActions: [’push button’, ’north’, ’south’, ’put shovel down’, ’east’,\n’put button down’, ’examine obelisk’, ’put torch down’, ’west’, ’put\nall down’, ’up’] Qvalues: [8.96, 8.86, 8.77, 8.23, 8.21, 7.93, 7.84, 7.54,\n7.53, 7.5, 5.75] Action: push button Reward: 0, Score 30, Done False\nSTATE 65\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a shovel a torch button a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. Actions: [’east’, ’push button’, ’north’,\n’south’, ’put shovel down’, ’put button down’, ’examine obelisk’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [10.04, 9.23,\n9.06, 9.0, 8.39, 8.16, 8.09, 7.59, 7.55, 7.38, 6.29] Action: east Reward:\n0, Score 30, Done False\nSTATE 66\nt intersection to the hall goes east from here. to the west is the\nstairway to the round room. a narrow passage leads south. you are\nholding : a shovel a torch button a torch ( providing light ) t inter-\nsection to the hall goes east from here. to the west is the stairway\nto the round room. a narrow passage leads south. Actions: [’east’,\n’west’, ’push button’, ’south’, ’put shovel down’, ’put torch down’,\n’put button down’, ’put all down’] Qvalues: [13.19, 8.57, 8.34, 7.48,\n7.11, 6.89, 6.88, 6.69] Action: east Reward: 0, Score 30, Done False\nSTATE 67\ncorridor at doorway the hall goes east and west from here and there\nis a doorway to the north. you are holding : a shovel a torch button\na torch ( providing light ) corridor at doorway the hall goes east\nand west from here and there is a doorway to the north. Actions:\n[’north’, ’east’, ’west’, ’push button’, ’put shovel down’, ’put torch\ndown’, ’put button down’, ’put all down’] Qvalues: [15.16, 11.38,\n8.99, 8.52, 7.44, 7.26, 7.01, 6.99] Action: north Reward: 0, Score 30,\nDone False\nSTATE 68\nroom full of gadgets this is the south end of a long room. the walls\nand floor and ceiling are covered with thousands of metal gadgets\nthat you might want to examine more carefully. there is a path free\nof gadgets to the north end of the room. you are holding : a shovel\na torch button a torch ( providing light ) room full of gadgets this\nis the south end of a long room. the walls and floor and ceiling are\ncovered with thousands of metal gadgets that you might want to\nexamine more carefully. there is a path free of gadgets to the north\nend of the room. Actions: [’north’, ’push button’, ’south’, ’put shovel\ndown’, ’put torch down’, ’put button down’, ’put all down’] Qvalues:\n[16.71, 10.81, 9.31, 9.13, 8.81, 8.61, 8.58] Action: north Reward: 0,\nScore 30, Done False\nSTATE 69\nnorth gadget room the room stretches away back south. the metal\ngadgets are clustered even more densely here. sitting on the floor\nin a clear space is a cut crystal bowl. the bowl contains : a large\nperfect ruby a hundreds of quartz chipsyou are holding : a shovel\na torch button a torch ( providing light ) north gadget room the\nroom stretches away back south. the metal gadgets are clustered\neven more densely here. sitting on the floor in a clear space is a cut\ncrystal bowl. the bowl contains : a large perfect ruby a hundreds of\nquartz chips Actions: [’take chips’, ’push button’, ’put torch down’,\n’put all down’, ’put shovel down’, ’put button down’, ’south’] Qval-\nues: [13.84, 9.5, 8.06, 7.99, 7.98, 7.77, 7.26] Action: take chips Reward:\n10, Score 40, Done False\nSTATE 70\nas you do, several of the quartz chips spill onto the floor. some\nhit metal gadgets, triggering them. darts fly. some darts hit more\ngadgets. more darts fly. after a few seconds of this, you have been\npunctured several million times. you have died you are back in the\ninfinite blackness. eventually the voice returns, senses you disgust-\nedly, and speaks. \" when standing inside a mass of uranium, it is\nnot usually a good idea to drop a neutron. \" you’ve stumbled into\nfour traps. you’re not looking very good. \" round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. you are holding\n: a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. Actions: [’push\nbutton’, ’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put button\ndown’, ’put torch down’, ’west’, ’put all down’, ’east’, ’up’] Qvalues:\n[9.13, 8.58, 8.54, 8.07, 7.74, 7.73, 7.45, 7.42, 7.36, 6.8, 5.76] Action:\npush button Reward: 0, Score 40, Done False\nSTATE 71\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’north’, ’south’, ’take shovel’, ’put button down’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.62, 9.35, 8.78, 8.78, 8.0, 7.93, 7.87, 7.55, 7.35, 7.17, 6.39] Action:\npush button Reward: 0, Score 40, Done False\nSTATE 72\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’put button down’, ’take shovel’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.63, 9.25, 8.68, 8.54, 7.96, 7.96, 7.71, 7.57, 7.32, 7.15, 6.03] Action:\neast Reward: 0, Score 40, Done False\nSTATE 73\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [8.01, 7.65, 7.22, 7.01, 6.94,\n6.93, 6.46] Action: east Reward: 0, Score 40, Done False\nSTATE 74\ncorridor at doorwayyou are holding : a torch button a torch ( pro-\nviding light ) corridor at doorway the hall goes east and west from\nhere and there is a doorway to the north. Actions: [’west’, ’push\nbutton’, ’north’, ’put torch down’, ’put all down’, ’put button down’,\n’east’] Qvalues: [7.61, 7.48, 7.24, 6.89, 6.78, 6.58, 5.01] Action: west\nReward: 0, Score 40, Done False\nSTATE 75\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [7.94, 7.53, 7.02, 6.93, 6.89,\n6.83, 6.15] Action: push button Reward: 0, Score 40, Done False\nSTATE 76\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [8.16, 7.75, 7.7, 7.34, 7.01, 6.85, 6.73] Action: west Reward:\n0, Score 40, Done False\nSTATE 77\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’put button down’, ’take shovel’, ’examine obelisk’,\n’put torch down’, ’west’, ’put all down’, ’up’] Qvalues: [9.42, 8.58,\n8.33, 7.99, 7.82, 7.74, 7.61, 7.2, 7.18, 7.05, 5.89] Action: west Reward:\n0, Score 40, Done False\nSTATE 78\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection the east \/ west corridor branches to the south\nhere. Actions: [’east’, ’push button’, ’west’, ’put torch down’, ’put\nall down’, ’put button down’, ’south’] Qvalues: [7.66, 7.53, 7.46, 6.63,\n6.6, 6.53, 6.31] Action: put all down Reward: 0, Score 40, Done False\nSTATE 79\ntorch button : dropped. torch : dropped. you are holding nothing.\nt intersection the east \/ west corridor branches to the south here.\nlying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. you can also see a torch button\nhere. Actions: [’west’, ’east’, ’push button’, ’south’, ’take all’, ’take\nbutton’, ’take torch’] Qvalues: [7.47, 7.33, 7.29, 7.08, 7.01, 6.79, 6.66]\nAction: take all Reward: 0, Score 40, Done False\nSTATE 80\ntorch : taken. torch button : taken. you are holding : a torch button\na torch ( providing light ) t intersection the east \/ west corridor\nbranches to the south here. Actions: [’east’, ’west’, ’push button’,\n’put all down’, ’put torch down’, ’put button down’, ’south’] Qvalues:\n[8.2, 7.56, 7.44, 6.46, 6.42, 6.34, 6.2] Action: east Reward: 0, Score 40,\nDone False\nSTATE 81\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’put button down’, ’take shovel’, ’examine obelisk’,\n’put torch down’, ’west’, ’put all down’, ’up’] Qvalues: [9.17, 8.31,\n8.05, 8.02, 7.6, 7.51, 7.33, 6.95, 6.94, 6.82, 5.48] Action: push button\nReward: 0, Score 40, Done False\nSTATE 82\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’put button down’, ’take shovel’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.47, 9.04, 8.33, 8.11, 7.78, 7.62, 7.35, 7.27, 7.01, 6.81, 5.67] Action:\neast Reward: 0, Score 40, Done False\nSTATE 83\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [7.63, 7.29, 6.78, 6.6, 6.56,\n6.53, 6.3] Action: push button Reward: 0, Score 40, Done False\nSTATE 84\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [7.92, 7.45, 7.27, 7.0, 6.68, 6.57, 6.44] Action: push button\nReward: 0, Score 40, Done False\nSTATE 85\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [7.89, 7.4, 7.27, 6.96, 6.64, 6.55, 6.41] Action: put all down\nReward: 0, Score 40, Done False\nSTATE 86\ntorch button : dropped. torch : dropped. you are holding nothing.\nt intersection to the hall goes east from here. to the west is the\nstairway to the round room. a narrow passage leads south. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. you can also see a torch button here. Ac-\ntions: [’east’, ’west’, ’push button’, ’take all’, ’south’, ’take button’,\n’take torch’] Qvalues: [7.4, 7.13, 7.1, 7.08, 7.05, 6.86, 6.75] Action:\ntake all Reward: 0, Score 40, Done False\nSTATE 87\ntorch : taken. torch button : taken. you are holding : a torch button\na torch ( providing light ) t intersection to the hall goes east from\nhere. to the west is the stairway to the round room. a narrow pas-\nsage leads south. Actions: [’west’, ’push button’, ’south’, ’east’, ’put\nbutton down’, ’put torch down’, ’put all down’] Qvalues: [7.89, 7.18,\n6.81, 6.68, 6.31, 6.29, 6.27] Action: west Reward: 0, Score 40, Done\nFalse\nSTATE 88\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’take shovel’, ’put button down’, ’examine obelisk’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [8.74, 7.99,\n7.93, 7.42, 7.38, 7.27, 7.14, 6.84, 6.76, 6.65, 5.54] Action: push button\nReward: 0, Score 40, Done False\nSTATE 89\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’take shovel’, ’put button down’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.16, 8.88, 8.24, 8.2, 7.52, 7.49, 7.35, 7.07, 6.85, 6.63, 5.99] Action:\nnorth Reward: 0, Score 40, Done False\nSTATE 90\nlandingyou are holding : a torch button a torch ( providing light )\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. Actions: [’north’,\n’push button’, ’south’, ’put button down’, ’put torch down’, ’east’,\n’put all down’] Qvalues: [8.7, 8.17, 7.45, 7.15, 6.9, 6.81, 6.81] Action:\nput all down Reward: 0, Score 40, Done False\nSTATE 91\ntorch button : dropped. torch : dropped. you are holding nothing.\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. lying on the ground\nis a brass torch of the \" self igniting \" type. the ignite button is on\nthe handle. you can also see a torch button here. Actions: [’take\nall’, ’take torch’, ’north’, ’take button’, ’east’, ’push button’, ’south’]\nQvalues: [7.96, 7.45, 7.44, 7.34, 7.3, 7.3, 7.09] Action: north Reward:\n0, Score 40, Done False\nSTATE 92\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is at the peak of the pyramid. it is perfectly conical.\nin the center is a tall stone obelisk that you could probably use to\nget back up to the fake toilet. there are four stairways leaving the\nroom at the four compass points. there is a shovel lying here. you\nare holding : a torch ( providing light ) round room this room is at\nthe peak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four com-\npass points. there is a shovel lying here. Actions: [’south’, ’north’,\n’take shovel’, ’examine obelisk’, ’put torch down’, ’west’, ’east’, ’up’]\nQvalues: [7.93, 7.62, 7.36, 7.07, 6.82, 6.67, 6.48, 4.8] Action: put torch\ndown Reward: 0, Score 40, Done False\nSTATE 93\ndropped. you are holding nothing. round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. lying on the ground is a brass torch of the \" self igniting \"\ntype. the ignite button is on the handle. there is a shovel lying here.\nActions: [’west’, ’south’, ’east’, ’take all’, ’north’, ’take brass’, ’take\nshovel’, ’examine obelisk’, ’up’] Qvalues: [7.63, 7.56, 7.54, 7.46, 7.46,\n7.34, 6.94, 6.73, 6.22] Action: examine obelisk Reward: 0, Score 40,\nDone False\nSTATE 94\nthe obelisk is a monument to the great rahn. the inscription starts :\n\" for rahn hath given us the postulates and the theorems ; we shalt\nthank rahn. for rahn hath said’between any two points there shalt\nbe but one line’; and he found it good. for rahn hath said’if two\nplanes intersecteth, they shalt form but one straight line’; and he\nfound it good. for rahn hath said... \" it goes on like this for a long\ntime. when you manage to break the trance, your mind is numbed\nand full of rahn’s strange commandments. round room this room\nis a conic surface with exits at even multiples of 90 degrees. in the\ncenter is a regular square pyramid with a narrow vertex angle. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. there is a shovel lying here. you are holding\nnothing. round room this room is a conic surface with exits at even\nmultiples of 90 degrees. in the center is a regular square pyramid\nwith a narrow vertex angle. lying on the ground is a brass torch of\nthe \" self igniting \" type. the ignite button is on the handle. there is\na shovel lying here. Actions: [’south’, ’take all’, ’east’, ’west’, ’take\nbrass’, ’north’, ’take shovel’, ’examine obelisk’, ’up’] Qvalues: [7.34,\n7.27, 7.13, 7.1, 7.08, 7.05, 6.59, 6.37, 5.73] Action: south Reward: 0,\nScore 40, Done False\nSTATE 95\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is a conic surface with exits at even multiples of 90\ndegrees. in the center is a regular square pyramid with a narrow\nvertex angle. there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [8.0, 7.7, 7.34, 7.26, 6.71, 6.65,\n6.15, 4.76] Action: examine obelisk Reward: 0, Score 40, Done False\nSTATE 97\nthe obelisk is a monument to the great rahn. the inscription starts :\n\" for rahn hath given us the postulates and the theorems ; we shalt\nthank rahn. for rahn hath said’between any two points there shalt\nbe but one line’; and he found it good. for rahn hath said’if two\nplanes intersecteth, they shalt form but one straight line’; and he\nfound it good. for rahn hath said... \" it goes on like this for a long\ntime. when you manage to break the trance, your mind is numbed\nand full of rahn’s strange commandments. round room this room\nis a conic surface with exits at even multiples of 90 degrees. in\nthe center is a regular square pyramid with a narrow vertex angle.\nthere is a shovel lying here. you are holding : a torch ( providing\nlight ) round room this room is a conic surface with exits at even\nmultiples of 90 degrees. in the center is a regular square pyramid\nwith a narrow vertex angle. there is a shovel lying here. Actions:\n[’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch down’,\n’west’, ’east’, ’up’] Qvalues: [7.91, 7.7, 7.27, 7.16, 6.6, 6.57, 6.45, 5.07]\nAction: put torch down Reward: 0, Score 40, Done False\nSTATE 98\ndropped. you are holding nothing. round room this room is a conic\nsurface with exits at even multiples of 90 degrees. in the center is a\nregular square pyramid with a narrow vertex angle. lying on the\nground is a brass torch of the \" self igniting \" type. the ignite button\nis on the handle. there is a shovel lying here. Actions: [’take all’,\n’south’, ’west’, ’take brass’, ’east’, ’north’, ’take shovel’, ’examine\nobelisk’, ’up’] Qvalues: [7.4, 7.34, 7.26, 7.24, 7.21, 7.19, 6.74, 6.51,\n6.06] Action: south Reward: 0, Score 40, Done False\nSTATE 99\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is a conic surface with exits at even multiples of 90\ndegrees. in the center is a regular square pyramid with a narrow\nvertex angle. there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [8.02, 7.67, 7.34, 7.23, 6.75, 6.66,\n6.23, 4.74] Action: west Reward: 0, Score 40, Done False\nSTATE 100\nt intersectionyou are holding : a torch ( providing light ) t intersec-\ntion the east \/ west corridor branches to the south here. Actions:\n[’west’, ’east’, ’put torch down’, ’south’] Qvalues: [7.21, 6.7, 6.47,\n5.87] Action: east Reward: 0, Score 40, Done True\nSTATE 101\nround room there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [7.82, 7.45, 7.12, 6.99, 6.59, 6.5,\n6.18, 4.91] Action: reset Reward: 0, Score 0, Done False\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Pre-trained Language Models as Prior Knowledge for Playing Text-based Games.pdf"}
{"title":"Guiding Pretraining in Reinforcement Learning with Large Language Models","authors":"Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas","summary":"Reinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks. Code available at\nhttps:\/\/github.com\/yuqingd\/ellm.","url":"http:\/\/arxiv.org\/abs\/2302.06692v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2302.06692v2","published":1676322963000,"comment":"ICML 2023","pdf_text":"Guiding Pretraining in Reinforcement Learning with Large Language Models\nYuqing Du * 1 Olivia Watkins * 1 Zihan Wang 2 C´edric Colas 3 4 Trevor Darrell 1 Pieter Abbeel 1\nAbhishek Gupta 2 Jacob Andreas 3\nAbstract\nReinforcement learning algorithms typically\nstruggle in the absence of a dense, well-shaped\nreward function. Intrinsically motivated explo-\nration methods address this limitation by reward-\ning agents for visiting novel states or transitions,\nbut these methods offer limited benefits in large\nenvironments where most discovered novelty is\nirrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from\ntext corpora to shape exploration. This method,\ncalled ELLM (Exploring with LLMs) rewards an\nagent for achieving goals suggested by a language\nmodel prompted with a description of the agent’s\ncurrent state. By leveraging large-scale language\nmodel pretraining, ELLM guides agents toward\nhuman-meaningful and plausibly useful behav-\niors without requiring a human in the loop. We\nevaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing\nthat ELLM-trained agents have better coverage\nof common-sense behaviors during pretraining\nand usually match or improve performance on\na range of downstream tasks. Code available at\nhttps:\/\/github.com\/yuqingd\/ellm.\n1. Introduction\nReinforcement learning algorithms work well when learners\nreceive frequent rewards that incentivize progress toward\ntarget behaviors. But hand-defining such reward functions\nrequires significant engineering efforts in all but the simplest\ncases (Amodei et al., 2016; Lehman et al., 2020). To master\n*Equal contribution\n1Department of Electrical Engineer-\ning and Computer Science, University of California, Berke-\nley, USA 2University of Washington, Seattle 3Massachusetts In-\nstitute of Technology, Computer Science and Artificial Intelli-\ngence Laboratory 4Inria, Flowers Laboratory. Correspondence to:\nYuqing Du <yuqing du@berkeley.edu>, Olivia Watkins <olivi-\nawatkins@berkeley.edu>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n1. Cut down the tree.\n2. Craft a pickaxe.\n3. Eat cow.\n4. Sleep.\n   . . . \nk. Build a wood house.\nYou see trees, \ncows, grass, \ntable, and \nbushes. You have \nwood in your \ninventory. You \nfeel hungry, \nthirsty, and \nsleepy. \nLLM\nPrompt:\nWhat should \nyou do next?\nFigure 1: ELLM uses a pretrained large language model\n(LLM) to suggest plausibly useful goals in a task-agnostic\nway.\nBuilding on LLM capabilities such as context-\nsensitivity and common-sense, ELLM trains RL agents to\npursue goals that are likely meaningful without requiring\ndirect human intervention. Prompt is illustrative; see full\nprompt and goal format in Appendix D.\ncomplex tasks in practice, RL agents may therefore need to\nlearn some behaviors in the absence of externally-defined\nrewards. What should they learn?\nIntrinsically motivated RL methods answer this question\nby augmenting rewards with auxiliary objectives based on\nnovelty, surprise, uncertainty, or prediction errors (Belle-\nmare et al., 2016; Pathak et al., 2017; Burda et al., 2019;\nZhang et al., 2021; Liu & Abbeel, 2021; Yarats et al., 2021).\nBut not everything novel or unpredictable is useful: noisy\nTVs and the movements of leaves on a tree may provide an\ninfinite amount of novelty, but do not lead to meaningful\nbehaviors (Burda et al., 2019). More recent approaches com-\npute novelty with higher-level representations like language\n(Tam et al., 2022; Mu et al., 2022), but can continue driving\nthe agent to explore behaviors that are unlikely to corre-\nspond to any human-meaningful goal—like enumerating\nunique configurations of furniture in a household. It is not\nsufficient for extrinsic-reward-free RL agents to optimize\nfor novelty alone: learned behaviors must also be useful.\nIn this paper, we describe a method for using not just\nlanguage-based representations but pretrained language\nmodels (LLMs) as a source of information about useful\nbehavior. LLMs are probabilistic models of text trained on\nlarge text corpora; their predictions encode rich information\n1\narXiv:2302.06692v2  [cs.LG]  15 Sep 2023\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nabout human common-sense knowledge and cultural conven-\ntions. Our method, Exploring with LLMs (ELLM), queries\nLMs for possible goals given an agent’s current context and\nrewards agents for accomplishing those suggestions. As a\nresult, exploration is biased towards completion of goals\nthat are diverse, context-sensitive, and human-meaningful.\nELLM-trained agents exhibit better coverage of useful be-\nhaviors during pretraining, and outperform or match base-\nlines when fine-tuned on downstream tasks.\n2. Background and Related Work\nIntrinsically Motivated RL.\nWhen reward functions are\nsparse, agents often need to carry out a long, specific se-\nquence of actions to achieve target tasks. As action spaces\nor target behaviors grow more complex, the space of alter-\nnative action sequences agents can explore grows combi-\nnatorially. In such scenarios, undirected exploration that\nrandomly perturbs actions or policy parameters has little\nchance of succeeding (Ten et al., 2022; Ladosz et al., 2022).\nMany distinct action sequences can lead to similar out-\ncomes (Baranes & Oudeyer, 2013)—for example, most\naction sequences cause a humanoid agent to fall, while\nvery few make it walk. Building on this observation, in-\ntrinsically motivated RL algorithms (IM-RL) choose to\nexplore outcomes rather than actions (Oudeyer & Kaplan,\n2009; Ten et al., 2022; Ladosz et al., 2022). Knowledge-\nbased IMs (KB-IMs) focus on maximising the diversity of\nstates (reviews in Aubret et al., 2019; Linke et al., 2020).\nCompetence-based IMs (CB-IMs) maximise the diversity\nof skills mastered by the agent (review in Colas et al., 2022).\nBecause most action sequences lead to a very restricted part\nof the outcome space (e.g. all different ways of falling on\nthe floor likely correspond to a single outcome), these meth-\nods lead to a greater diversity of outcomes than undirected\nexploration (Lehman et al., 2008; Colas et al., 2018).\nHowever, maximizing diversity of outcomes may not always\nbe enough. Complex environments can contain sources of\ninfinite novelty. In such environments, seeking ever-more-\nnovel states might drive learning towards behaviors that\nhave little relevance to the true task reward. Humans do\nnot explore outcome spaces uniformly, but instead rely on\ntheir physical and social common-sense to explore plausibly-\nuseful behaviors first. In video games, they know that keys\nshould be used to open doors, ladders should be climbed,\nand snakes might be enemies. If this semantic information\nis removed, their exploration becomes severely impacted\n(Dubey et al., 2018). The approach we introduce in this\npaper, ELLM, may be interpreted as a CB-IM algorithm\nthat seeks to explore the space of possible and plausibly-\nuseful skills informed by human prior knowledge.\nLinguistic Goals and Pretrained Language Models.\nOne way of representing a diverse outcome space for ex-\nploration is through language. Training agents to achieve\nlanguage goals brings several advantages: (1) goals are easy\nto express for non-expert users; (2) they can be more abstract\nthan standard state-based goals (Colas et al., 2022); and (3)\nagents can generalize better thanks to the partial composi-\ntionality and recursivity of language (Hermann et al., 2017;\nHill et al., 2019; Colas et al., 2020). Such linguistic goals\ncan be used as instructions for language-conditioned imita-\ntion learning or RL. In RL, agents typically receive language\ninstructions corresponding to the relevant reward functions\n(Luketina et al., 2019) and are only rarely intrinsically mo-\ntivated (with the exception of Mu et al., 2022; Colas et al.,\n2020; Tam et al., 2022), where language is also used as a\nmore general compact state abstraction for task-agnostic\nexploration.\nRepresenting goals in language unlocks the possibility of us-\ning text representations and generative models of text (large\nlanguage models, or LLMs) trained on large corpora. In im-\nitation learning, text pretraining can help learners automat-\nically recognize sub-goals and learn modular sub-policies\nfrom unlabelled demonstrations (Lynch & Sermanet, 2020;\nSharma et al., 2021), or chain pre-trained goal-oriented poli-\ncies together to accomplish high-level tasks (Yao et al., 2020;\nHuang et al., 2022a; Ahn et al., 2022; Huang et al., 2022b).\nIn RL, LM-encoded goal descriptions greatly improve the\ngeneralization of instruction-following agents across instruc-\ntions (Chan et al., 2019) and from synthetic to natural goals\n(Hill et al., 2020). LLMs have also been used as proxy\nreward functions when prompted with desired behaviors\n(Kwon et al., 2023). Unlike these approaches, ELLM uses\npretrained LLMs to constrain exploration towards plausibly-\nuseful goals in a task-agnostic manner. It does not assume a\npretrained low-level policy, demonstrations, or task-specific\nprompts. Most similar to our work, Choi et al. (2022) also\nprompt LLMs for priors. However, they use LM priors to\nclassify safe and unsafe states to reward, which is a subset\nof common-sense exploratory behaviors ELLM should gen-\nerate. Also similar to our work, Kant et al. (2022) query\nLLMs for zero-shot commonsense priors in the Housekeep\nenvironment, but they apply these to a planning task rather\nthan as rewards for reinforcement learning.\n3. Structuring Exploration with LLM Priors\n3.1. Problem Description\nWe consider partially observed Markov decision processes\ndefined by a tuple (S, A, O, Ω, T , γ, R), in which obser-\nvations o ∈Ωderive from environment states s ∈S and\nactions a ∈A via O(o | s, a). T (s′ | s, a) describes the\ndynamics of the environment while R and γ are the envi-\nronment’s reward function and discount factor.\n2\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nText \nobs\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n“You see grass, \ntrees, bushes, cows, \nand a crafting table. \nYour inventory has \nwood.”\nValid actions: sleep, eat, attack, chop, \ndrink, place, make, mine. You are a player \nplaying a game. Suggest the best actions the \nplayer can take based on the things you see \nand the items in your inventory. Only use \nvalid actions and objects.\nYou see plant, tree, and skeleton. You are \ntargeting skeleton. What do you do?\n- Eat plant, chop tree, attack skeleton\n{current obs}\nWhat do you do? \nLLM\n(a) Policy parametrization for ELLM. We optionally condition on embeddings of\nthe goals Etext(g1:k\nt\n) and state Etext(Cobs(ot)).\nLM Embed\nLM Embed\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n“Chop tree”\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n(b) LLM reward scheme. We reward the agent for\nthe similarity between the captioned transition and\nthe goals.\nFigure 2: ELLM uses GPT-3 to suggest adequate exploratory goals and SentenceBERT embeddings to compute the similarity\nbetween suggested goals and demonstrated behaviors as a form of intrinsically-motivated reward.\nIM agents optimize for an intrinsic reward Rint alongside\nor in place of R. CB-IM methods, in particular, define Rint\nvia a family of goal-conditioned reward functions:\nRint(o, a, o′) = Eg∼G [Rint(o, a, o′ | g)] .\n(1)\nA CB-IM agent is expected to perform well with respect to\nthe original R when the intrinsic reward Rint is both easier\nto optimize and well aligned with R, such that behaviors\nmaximizing Rint also maximize R. Every CB-IM algorithm\nmust define two elements in Equation 1: (1) the distribution\nof goals to sample from, i.e. G, and (2) the goal-conditioned\nreward functions Rint(o, a, o′ | g). Given these, A CB-IM\nalgorithm trains a goal-conditioned policy π(a | o, g) to\nmaximize Rint. For some intrinsic reward functions, agents\nmay achieve high reward under the original reward function\nR immediately; for others, additional fine-tuning with R\nmay be required. In Equation (1), the space of goals G is\ndetermined by the goal-conditioned reward function Rint(· |\ng): every choice of g induces a corresponding distribution\nover optimal behaviors.\n3.2. Goal-based Exploration Desiderata\nHow should we choose G and Rint(· | g) to help agents\nmake progress toward general reward functions R? Goals\ntargeted during exploration should satisfy three properties:\n• Diverse: targeting diverse goals increases the chance\nthat the target behavior is similar to one of them.\n• Common-sense sensitive: learning should focus on\nfeasible goals (chop a tree > drink a tree) which\nare likely under the distribution of goals humans care\nabout (drink water > walk into lava).\n• Context sensitive: learning should focus on goals that\nare feasible in the current environment configuration\n(e.g. chop a tree only if a tree is in view).\nMost CB-IM algorithms hand-define the reward functions\nRint (2) and the support of the goal distribution (1) in align-\nment with the original task R, but use various intrinsic mo-\ntivations to guide goal sampling (1): e.g. novelty, learning\nprogress, intermediate difficulty (see a review in Colas et al.,\n2022). In Exploring with Large Language Models (ELLM),\nwe propose to leverage language-based goal representations\nand language-model-based goal generation to alleviate the\nneed for environment-specific hand-coded definitions of (1)\nand (2). We hypothesize that world knowledge captured in\nLLMs will enable the automatic generation of goals that are\ndiverse, human-meaningful and context sensitive.\n3.3. Goal Generation with LLMs (G)\nPretrained large language models broadly fall into three cat-\negories: autoregressive, masked, or encoder-decoder mod-\nels (Min et al., 2021). Autoregressive models (e.g. GPT;\nRadford et al., 2018), are trained to maximize the log-\nlikelihood of the next word given all previous words, and are\nthus capable of language generation. Encoder-only models\n(e.g. BERT; Devlin et al., 2018), are trained with a masked\nobjective, enabling effective encoding of sentence seman-\ntics. Pretraining LMs on large text corpora yields impressive\nzero- or few-shot on diverse language understanding and\ngeneration tasks, including tasks requiring not just linguistic\nknowledge but world knowledge (Brown et al., 2020).\nELLM uses autoregressive LMs to generate goals and\nmasked LMs to build vector representations of goals. When\nLLMs generate goals, the support of the goal distribution\nbecomes as large as the space of natural language strings.\nWhile querying LLMs unconditionally for goals can offer\n3\nGuiding Pretraining in Reinforcement Learning with Large Language Models\ndiversity and common-sense sensitivity, context-sensitivity\nrequires knowledge of agent state. Thus, at each timestep\nwe acquire goals by prompting the LLM with a list of the\nagent’s available actions and a text description of the current\nobservation via a state captioner Cobs : Ω→Σ∗, where Σ∗\nis the set of all strings (see Figure 2).\nWe investigate two concrete strategies for extracting goals\nfrom LLMs: (1) open-ended generation, in which the LLM\noutputs text descriptions of suggested goals (e.g. next you\nshould...), and (2) closed-form, in which a possible goal\nis given to the LLM as a QA task (e.g. Should the agent\ndo X? (Yes\/No)). Here the LLM goal suggestion is only\naccepted when the log-probability of Yes is greater than\nNo. The former is more suited for open-ended exploration\nand the latter is more suited for environments with large\nbut delimitable goal spaces. While the LLM does not have\nprior knowledge of all possible goals, we can provide some\nguidance towards desirable suggestions through few-shot\nprompting. See Appendix D for the full prompt.\n3.4. Rewarding LLM Goals (Rint)\nNext we consider the goal-conditioned reward (2). We com-\npute rewards for a given goal g (Rint in Eq. 1) by measuring\nthe semantic similarity between the LLM-generated goal\nand the description of the agent’s transition in the envi-\nronment as computed by a transition captioner Ctransition :\nΩ× A × Ω→Σ:\nRint(o, a, o′ | g) =\n(\n∆(Ctransition(o, a, o′), g)\nif > T\n0\notherwise.\nHere, the semantic similarity function ∆(· , ·) is defined as\nthe cosine similarity between representations from an LM\nencoder E(·) of captions and goals:\n∆(Ctransition(o, a, o′), g) =\nE(Ctransition(o, a, o′)) · E(g)\n∥E(Ctransition(o, a, o′))∥∥E(g)∥.\nIn practice, we use a pretrained SentenceBERT model\n(Reimers & Gurevych, 2019) for E(·). We choose cosine\nsimilarity to measure alignment between atomic agent ac-\ntions and freeform LLM generations, as done in prior work\n(Huang et al., 2022a). When the caption of a transition is\nsufficiently close to the goal description (∆> T), where\nT is a similarity threshold hyperparameter, the agent is re-\nwarded proportionally to their similarity. Finally, since there\ncan be multiple goals suggested, we reward the agent for\nachieving any of the k suggestions by taking the maximum\nof the goal-specific rewards:\n∆max = max\ni=1...k ∆\n\u0000Ctransition(ot, at, ot+1), gi\nt\n\u0001\n.\nAs a result, the general reward function of CB-IM methods\nfrom Equation 1 can be rewritten:\nRint(o, a, o′) = ELLM(g1 .. k|Cobs(o)) [∆max] .\n(2)\n3.5. Implementation Details\nThe full ELLM algorithm is summarized in Algorithm 1.\nSee Figure 1 for the high-level pipeline. To impose a nov-\nelty bias, we also filter out LM suggestions that the agent\nhas already achieved earlier in the same episode. This pre-\nvents the agent from exploring the same goal repeatedly. In\nAppendix L we show this step is essential to the method.\nWe consider two forms of agent training: (1) a goal-\nconditioned setting where the agent is given a sentence\nembedding of the list of suggested goals, π(a | o, E(g1:k)),\nand (2) a goal-free setting where the agent does not have ac-\ncess to the suggested goals, π(a | o). While Rint remains the\nsame in either case, training a goal-conditioned agent intro-\nduces both challenges and benefits: it can take time for the\nagent to learn the meaning of the different goals and connect\nit to the reward, but having a language-goal conditioned pol-\nicy can be more amenable to downstream tasks than an agent\njust trained on an exploration reward. We also consider two\ntypes of policy inputs– (1) just the partially observed pixel\nobservations, or (2) the pixel observations combined with\nthe embedded language-state captions E(Cobs(o)). Since\n(2) performs better (see analysis in Appendix A), we use\n(2) for all paper experiments unless otherwise specified. All\nvariants are trained with the DQN algorithm (Mnih et al.,\n2013), with implementation details in Appendix H.\nThis paper focuses on the benefits of LLM priors for RL\nexploration and mostly assumes a pre-existing captioning\nfunction. In simulation, this can be acquired for free with\nthe ground truth simulator state. For real world applications,\none can use object-detection (Zaidi et al., 2022), caption-\ning models (Stefanini et al., 2022), or action recognition\nmodels (Kong & Fu, 2022). Alternatively, one could use\nmulti-modal vision-language models with a similar LM\ncomponent (Alayrac et al., 2022). To test the robustness of\nour method under varying captioning quality, Section 4.1\nstudies a relaxation of these assumptions by looking at a\nvariant of ELLM using a learned captioner trained on human\ndescriptions.\n4. Experiments\nOur experiments test the following hypotheses:\n• (H1) Prompted pretrained LLMs can generate\nplausibly-useful exploratory goals satisfying the\ndesiderata listed in Section 3.2: diversity, common-\nsense and context sensitivity.\n• (H2) Training an ELLM agent on these exploratory\ngoals improves performance on downstream tasks com-\npared to methods that do not leverage LLM-priors.\nWe evaluate ELLM in two complex environments:\n(1) Crafter, an open-ended environment in which explo-\nration is required to discover long-term survival strategies\n4\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nAlgorithm 1 ELLM Algorithm\nInitialize untrained policy π\nt ←0\not ←env.RESET()\nwhile t < max env steps do\n# Generate k suggestions, filtering achieved ones\ng1:k\nt\n←PREV ACHIEVED(LLM(Cobs(ot)))\n# Interact with the environment\nat ∼π(at|ot, E(Cobs(ot))), E(g1:k\nt\n))\nst+1 ←env.STEP(at)\n# Compute suggestion achievement reward\nrt ←0\n∆max ←maxi=1...k∆(Ctransition(ot, at, ot+1), gi\nt)\nif ∆max > threshold then\nrt = ∆max\nend if\n# Update agent using any RL algorithm\nBuffert+1 ←Buffert ∪(ot, at, g1:k\nt\n, rt, ot+1)\nπ ←UPDATE(π, Buffert+1)\nend while\nYou see {observation}. \nYou have in your inventory {items}*. \nYou feel {health status}*.\n  \n      *omitted if empty.\nYou see bush, grass, plant, tree, and \nwater. You have in your inventory \nsapling.\n-\nPlant sapling\n-\nChop tree\n-\nChop bush\nSeen objects: {object, receptacle}. \nSeen receptacles: {receptacles}. \nYou are holding {gripped_object}. \nSeen objects: clock in kitchen sink.\nSeen receptacles: kitchen bottom \ncabinet, kitchen sink, living room \nshelf, living room carpet …\nYou are holding a cereal box.\n-\nPlace cereal box in kitchen cabinet \n-\nPick clock\nFigure 3: Sample templated captions and suggested goals.\n(Hafner, 2021), and (2) Housekeep, an embodied robotics\nenvironment that requires common-sense to restrict the ex-\nploration of possible rearrangements of household objects\n(Kant et al., 2022). Besides environment affordances, these\nenvironments also differ in viewpoint (3rd vs 1st person) and\naction space (large high-level vs low-level). In each envi-\nronment, we compare ELLM with existing IM-RL methods\n(Liu & Abbeel, 2021; Burda et al., 2019), an oracle with\nground-truth rewards, and ablations of ELLM; see Table 1.\n4.1. Crafter\nEnvironment description.\nWe first test ELLM in the\nCrafter environment, a 2D version of Minecraft (Hafner,\n2021). Like Minecraft, Crafter is a procedurally generated\nand partially observable world that enables collecting and\ncreating a set of artifacts organized along an achievement\ntree which lists all possible achievements and their respec-\ntive prerequisites (see Figure 4 in Hafner, 2021). Although\nCrafter does not come with a single main task to solve, we\ncan track agent progress along the achievement tree.\nWe modify the original game in two ways. Crafter’s original\naction space already incorporates a great deal of human\ndomain knowledge: a single do action is interpreted in dif-\nferent ways based on the agent’s context, each of which\nwould correspond to a very different low-level action in a\nreal environment (‘do’ means ‘attack’ in front of a zombie\nbut ‘eat’ in front of a plant). We remove this assistance\nby augmenting the action space with more specific verb +\nnoun pairs that are not guaranteed to be useful (e.g. ‘eat\nzombie’). This makes it possible in Crafter to attempt a\nwide range of irrelevant\/nonsensical tasks, providing an\nopportunity for an LM narrow the goal space down to rea-\nsonable goals. See Appendix C for details. Second, to\nmake RL training easier across all conditions, we increase\nthe damage the agent does against enemies and reduce the\namount of wood required to craft a table from 2 to 1; see\nAppendix Figure 10 for comparisons.\nWe use Codex (Chen et al., 2021) as our LLM with the\nopen-ended suggestion generation variant of ELLM, where\nwe directly take the generated text from the LLM as the set\nof suggested goals to reward. Each query prompt consists\nof a list of possible verbs the agent can use (but not a list of\nall possible nouns), a description of the agent’s current state,\nand the question ‘What do you do?’. We add two examples\nof similar queries to the start of the prompt in order to guide\nthe language model to format suggestions in a consistent\nway; see the full prompt in Appendix D.\nGoals suggested by the LLM.\nTo answer H1, we study\nthe goals suggested by the LLM in Table 2: are they diverse,\ncontext-sensitive and common-sensical? The majority of\nsuggested goals (64.9%) are context-sensitive, sensible, and\nachievable in the game. Most of the 5% of goals not allowed\nby Crafter’s physics (e.g. build a house) are context- and\ncommon-sensitive as well. The last third of the goals vi-\nolate either context-sensitivity (13.6%) or common-sense\n(16.4%). See Appendix K for details.\nPretraining exploration performance.\nA perfect explo-\nration method would unlock all Crafter achievements in\nevery episode, even without prior knowledge of the set\nof possible achievements. Thus, we measure exploration\nquality as the average number of unique achievements per\nepisode across pretraining (Figure 4). Although it is not\ngiven access to Crafter’s achievement tree, ELLM learns\nto unlock about 6 achievements every episode, against 9\nfor the ground-truth-reward Oracle (Figure 4). It outper-\nforms all exploration methods that only focus on generat-\ning novel behaviors (APT, RND, Novelty) — all limited to\nless than 3 achievements in average. As shown in Table 2,\nELLM does not only focus on novelty but also generates\n5\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nMethod\nDescription\nELLM (ours)\nRewards the agent for achieving any goal suggested by the LLM using the similarity-based reward functions Rint\ndefined in Eq. 2. It only rewards the agent for achieving a given goal once per episode (novelty bias).\nOracle\nThe upper bound: it suggests all context-sensitive goals at any step, only common-sensical ones (from the list of\n(Crafter only)\nvalid goals) and uses the same novelty bias as ELLM. Rewards are computed exactly with a hard-coded Rint.\nNovelty\nThis baseline removes the common-sense sensitivity assumption of the Oracle and rewards the agent for\nachieving any of the goals expressible in the environment including invalid ones (e.g. drink tree) as long as the\nagent performs the goal-reaching action in the right context (e.g. while facing a tree). Uses a hard-coded Rint and\na novelty bias like the Oracle.\nUniform\nThis variant removes the novelty bias from Novelty and samples uniformly from the set of expressible goals.\nAPT\nState-of-the-art KB-IM algorithm that maximizes state entropy computed as the distance between the current\n(Liu & Abbeel, 2021)\nstate’s embedding es and its K nearest neighbors es[1..K] within a minibatch uniformly sampled from memory.\nThere is no goal involved and Rint = log ∥es −es[1..K]∥.\nRND\nState-of-the-art KB-IM algorithm that rewards the agent for maximizing a form of novelty estimated by the\n(Burda et al., 2019)\nprediction error of a model h trained to predict the output of a random network ˜h. Rint = ∥h(s, a) −˜h(s, a)∥.\nTable 1: Descriptions of the compared algorithms. (Additional comparisons in Appendix N).\nSuggested\nRewarded\nContext-Insensitive\n13.6%\n1.1%\nCommon-Sense Insensitive\n16.4%\n32.4%\nGood\n64.9%\n66.5%\nImpossible\n5.0%\n0%\nTable 2: Fractions of suggested and rewarded goals that fail\nto satisfy context-sensitivity or common-sense sensitivity;\nthat satisfy these properties and are achievable in Crafter\n(Good); or that are not allowed by Crafter’s physics. See\nAppendix K for examples of each.\n0\n100\n200\n300\n400\n500\nEnvironment Steps (x 1e4)\n0\n2\n4\n6\n8\n10\n12\nCrafter Achievements\nOracle (goals)\nOracle (no goals)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nNovelty\nUniform\nFigure 4: Ground truth achievements unlocked per episode\nacross pretraining, mean±std across 5 seeds.\ncommon-sensical goals. This boosts exploration in Crafter,\nsupporting H1.\nAs discussed in Section 3.5, we also test variants of each\nmethod (with \/ without goal conditioning, with \/ without\ntext observations) where applicable. We do not find goal\nconditioning to bring a significant advantage in performance\nduring pretraining. The non-conditioned agent might infer\nthe goals (and thus the rewarded behaviors) from context\nalone. Similarly to Mu et al. (2022) and Tam et al. (2022),\nwe find that agents trained on visual + textual observations\n(as computed by E(Cobs(o))) outperform agents trained on\nvisual observations only for all the tested variants (opaque\nvs semi-transparent bars in Appendix Figure 8). That said,\noptimizing for novelty alone, whether in visual or semantic\nspaces, seems to be insufficient to fully solve Crafter.\nThe na¨ıve approach of finetuning a pretrained policy on\nthe downstream task performs poorly across all pretraining\nalgorithms. We hypothesize this is because relevant features\nand Q-values change significantly between pretraining and\nfinetuning, especially when the density of rewards changes.\nInstead, we find it is more effective to use the pretrained\npolicy for guided exploration. We initialize and train a new\nagent, but replace 50% of the algorithm’s randomly-sampled\nϵ-greedy exploration actions with actions sampled from the\npretrained policy. In Appendix M we include the poor\nfinetuning results discuss why we think guided exploration\ndoes better.\nFigure 5 compares the downstream performance of ELLM\nto the performance of the two strongest baselines RND and\nAPT using both transfer methods. (full comparisons with all\nbaselines shown in Appendix B). For the goal-conditioned\nversion of ELLM, we provide the agent with the sequence\nof subgoals required to achieve the task. Even though not all\n6\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nsubgoals were mastered during pretraining, we still observe\nthat the goal-conditioned pretrained agents outperform the\nunconditioned ones.\nPerformance of the different methods varies widely task-\nto-task and even seed-to-seed since each task requires a\ndifferent set of skills, and any given agent may or may\nnot have learned a particular skill during pretraining. For\ninstance, ELLM agents typically learn to place crafting\ntables and attack cows during pretraining, leading to low-\nvariance learning curves. They typically do not learn to\nmake wood swords, so we see a high-variance learning curve\nwhich depends on how quickly each agent stumbles across\nthe goal during finetuning. Despite the variance, we see that\ngoal-conditioned ELLM stands out as the best-performing\nmethod on average. Notably, ELLM (both goal-conditioned\nand goal-free) is the only method with nonzero performance\nacross all tasks.\nELLM with imperfect transition captioner.\nPerfect cap-\ntioners might not be easy to obtain in some environments.\nHowever, trained captioners might generate more linguis-\ntic diversity and make mistakes. To test the robustness of\nELLM to diverse and imperfect captions, we replace the\noracle transition captioner Ctransition with a captioner trained\non a mixture of human and synthetic data (847+900 labels)\nusing the ClipCap algorithm (Mokady et al., 2021b). Syn-\nthetic data removes some of the human labor while still\nproviding a diversity of captions for any single transition\n(3 to 8). Appendix J presents implementation details and\nanalyzes how the trained captioner might cause errors in\ngenerated rewards. Although its false negative rate is low\n(it detects goal achievements well), its false positive rate\nis rather high. This means it might generate rewards for\nachievements that were not unlocked due to a high simi-\nlarity between the generated caption and goal description\ngenerated by the LLM. In ELLM pretraining, we use the\nlearned captioner to caption transitions where an action is\nsuccessful and use that caption to compute the reward via the\nsimilarity metric (see Section 3). Figure 6 shows that ELLM\nperformance is overall robust to this imperfect captioner.\n4.2. Housekeep\nEnvironment description. Housekeep is an embodied\nrobotics environment where the agent is tasked with clean-\ning up a house by rearranging misplaced objects (Kant et al.,\n2022). The agent must successfully match the environment’s\nground truth correct mapping of objects to receptacles with-\nout direct instructions specifying how objects need to be re-\narranged. This mapping was determined via crowd-sourcing\ncommon-sense object-receptacle combinations. An exam-\nple layout of the task can be found in Figure 1 in Kant et al.\n(2022). Common-sense priors are necessary for learning to\nrearrange misplaced objects into reasonable configurations.\nTask 1\nTask 2\nTask 3\nTask 4\nMatch Acc.\n85.7%\n87.5%\n50%\n66.7%\nMismatch Acc.\n93.8%\n90.1%\n94.0%\n87.6%\nTable 3: Classification accuracy of LLM for each Housekeep\ntask (top row is true positives, bottom row is true negatives).\nWe focus on a simplified subset of Housekeep consisting\nof 4 different scenes with one room each, each with 5 dif-\nferent misplaced objects and a suite of different possible\nreceptacles; see Appendix F for details. Because the agent\ndoes not have access to the ground truth target locations,\nwe use the game reward’s rearrangement success rate as\na measure of exploration quality: common-sensical explo-\nration should perform better. A success rate of 100% means\nthe agent has picked and placed all 5 misplaced objects in\ncorrect locations. Note that we intentionally focus on a\ndomain where the downstream application benefits strongly\nfrom exploring reasonable goals during pretraining. Rather\nthan designing reward functions that correspond to all cor-\nrect rearrangements for all possible objects, we investigate\nwhether ELLM can be a general purpose method that guides\nlearning human-meaningful behaviors.\nUnlike Crafter’s combinatorial and high-level action space,\nHousekeep operates with low-level actions: moving forward,\nturning, looking up or down, and picking or placing an ob-\nject. This allows us to investigate whether ELLM enables\nhigh-level exploration despite using lower-level control. We\nassume access to an egocentric instance segmentation sensor\nto generate captions of in-view objects and receptacles, and\nuse the text-davinci-002 InstructGPT model (Ouyang\net al., 2022) as our LLM. Given a description of visible\nobjects, the receptacles the objects are currently in, and all\npreviously seen receptacles, we create a list of all possible\nobject-receptacle mappings. We use the closed-form vari-\nant of ELLM and query the LLM for whether each object\nshould be placed in each receptacle as a yes\/no question.\nBy querying for each object-receptacle combination indi-\nvidually, we are able to cache and efficiently reuse LLM\nqueries. The agent can be given two types of goals: (1) pick-\ning an object if it is not already in a suggested receptacle,\nand (2) placing a gripped object in a suggested receptacle.\nGoals suggested by LLM.\nIn Housekeep, we assess LLM\ngoals by looking at the classification accuracy of correct\nand incorrect arrangements (Table 3). We find that the LLM\naccuracy at identifying mismatches (e.g. vase in kitchen\nsink) are all above 87%, however, accuracy of identifying\nmatches varies greatly depending on the available objects\nand receptacles (ranging from 50-90%). Since there are\nonly a few correct positions, each false negative hurts ac-\ncuracy greatly. Taking a closer look, we find that some\n7\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n0\n10\n20\nEnv Steps (x 1e4)\n0.00\n0.25\n0.50\n0.75\n1.00\nSuccess Rate\nPlace Crafting Table\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nAttack Cow\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nMake Wood Sword\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nMine Stone\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nDeforestation\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nPlant Row\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nGardening\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\n0.0\n2.5\n5.0\n7.5\n10.0\nCrafter Score\nCrafter (Game Reward)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nScratch\nFigure 5: Success rates across training for each of the seven downstream tasks in the Crafter environment. Each run trains\nan agent from scratch while leveraging a pretrained policy for exploration. Plots show mean ± std for 5 seeds. Some plots\nhave multiple overlapping curves at 0.\n0\n2\n4\n6\n8\n10\n12\n14\nSteps x1e4\n0\n2\n4\n6\n8\nCrafter Achievements\nPretraining With the Captioner\nOracle (goals) - GT Captioner\nOracle (goals) - ELLM Captioner\nELLM (goals) - GT Captioner\nELLM (goals) - ELLM Captioner\nFigure 6: Pretraining with a learned captioner vs a ground\ntruth captioner. We see performance drops, especially for\nELLM, but still relatively good performance. (3 seeds,\nmean± std.)\nLLM labels are reasonable despite disagreeing with the en-\nvironment mapping: e.g. suggesting vase in living room\ntable, and not suggesting pan in living room cabinet.\nThis suggests that there are ambiguities in the ground truth\nmappings, likely due to human disagreement.\nPretraining and downstream performance.\nTo investi-\ngate H1, we compare ELLM against the strongest baselines\n(RND, APT, Novelty) described in Table 1. In Housekeep\nthe novelty baseline rewards the agent for novel instances of\npick or place actions in an episode, allowing us to differenti-\nate between success attributable solely to the captioner and\nthe pick\/place prior, and success attributable to any LLM\ncommon-sense priors. For brevity, we focus only on the\npixel + text-observation variant of all methods. Sample ef-\nficiency curves measuring the ground truth rearrangement\nsuccess during both pretraining and finetuning are shown\nin Figure 7a. In three of the four tasks, we find that the\nELLM bias leads to higher success rates during pretraining,\nsuggesting coverage better aligned with the downstream\ntask compared to the baselines. We also find much higher\npretraining success rates in the first two tasks. Since Table 3\nshows higher LLM accuracy for these two tasks, this differ-\nence shows the impact of LLM inaccuracies on pretraining.\nFor H2, we test two different ways of using the pretrained\nmodels in the downstream rearrangement task. First, we\ndirectly finetune the pretrained model on the ground truth\ncorrect rearrangement; shown after the dashed vertical line\nin Figure 7a. Here, the success rates for finetuned ELLM\nmatches or outperform the baselines, especially if pretrain-\ning has already led to high success rates. Interestingly, we\nalso find that the goal-conditioned ELLM variant consis-\ntently suffers a drop in performance when finetuning starts.\nWe hypothesize this is due to the treatment of all suggested\ngoals as a single string, so if any single goal changes be-\ntween pretraining and finetuning the agent must relearn the\ngoal embedding changes. Second, in Figure 7b we present\nresults for directly training a new agent on the downstream\ntask, using the frozen pretrained model as an exploratory\nactor during ϵ-greedy exploration. Once again, we observe\nthat ELLM consistently matches or outperforms all base-\nlines. We also see here that the KB-IM baselines are more\ncompetitive, suggesting that this training scheme is better\nsuited for pretrained exploration agents that are not well-\naligned to the downstream task.\n5. Conclusions and Discussion\nWe have presented ELLM, an intrinsic motivation method\nthat aims to bias exploration towards common-sense and\nplausibly useful behaviors via a pretrained LLM. We have\nshown that such priors are useful for pretraining agents in\nextrinsic-reward-free settings that require common-sense\nbehaviors that other exploration methods fail to capture.\nELLM goes beyond standard novelty search approaches by\nconcentrating exploration on common-sensical goals. This\nis helpful in environments offering a wide array of possible\nbehaviors among which very few can said to be plausibly\nuseful. It is less helpful in environments with little room\nfor goal-based exploration, when human common-sense\nis irrelevant or cannot be expressed in language (e.g. fine-\ngrained manipulation), or where state information is not\nnaturally encoded as a natural language string.\nLLM performance is sensitive to prompt choice. Even with\na well-chosen prompt, LLMs sometimes make errors, often\ndue to missing domain-specific knowledge. False nega-\ntives can permanently prevent the agent from learning a\nkey skill: in Crafter, for example, the LLM never suggests\ncreating wood pickaxes. There are multiple avenues to ad-\ndress this limitation: (1) combining ELLM rewards with\nother KB-IM rewards like RND, (2) prompting LLMs with\ndescriptions of past achievements (or other feedback about\n8\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nELLM (goals) - Ours\nELLM (no goals) - Ours\nAPT\nRND\nNovelty (goals)\nNovelty (no goals)\nFinetuning Begins\n0\n2\n4\nEnvironment Steps\n1e6\n0.0\n0.2\n0.4\nHousekeep Success Rate\n0\n2\n4\nEnvironment Steps\n1e6\n0\n2\n4\nEnvironment Steps\n1e6\n0\n2\n4\nEnvironment Steps\n1e6\n(a) Pretraining and finetuning: pretraining for 4M steps then fine-\ntuning for 1M steps on the ground truth correct arrangement.\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.2\n0.4\nHousekeep Success Rate\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n(b) Downstream evaluation: Using the frozen pretrained exploration\npolicies only for ϵ-greedy-style action selection for 1M steps.\nFigure 7: Housekeep: Correct arrangement success rates on 4 object-receptacle task sets. Mean ± std over 5 seeds.\nenvironment dynamics) so that LLMs can learn about the\nspace of achievable goals, (3) injecting domain knowledge\ninto LLM prompts, or (4) fine-tuning LLMs on task-specific\ndata. While ELLM does not rely on this domain knowledge,\nwhen this information exists it is easy to incorporate.\nELLM requires states and transition captions. Our learned\ncaptioner experiments Figure 6 suggest we can learn these\nfrom human-labeled samples, but in some environments\ntraining this captioner might be less efficient than collecting\ndemonstrations or hard-coding a reward function. Still, we\nare optimistic that as progress in general-purpose caption-\ning models continues, off-the-shelf captioners will become\nfeasible for more tasks. Lastly, suggestion quality improves\nconsiderably with model size. Querying massive LLMs\nregularly may be time- and cost-prohibitive in some RL\nenvironments.\nAs general-purpose generative models become available in\ndomains other than text, ELLM-like approaches might also\nbe used to suggest plausible visual goals, or goals in other\nstate representations. ELLM may thus serve as a platform\nfor future work that develops even more general and flexible\nstrategies for incorporating human background knowledge\ninto reinforcement learning.\n6. Acknowledgements\nYD and OW are funded by the Center for Human-\nCompatible Artificial Intelligence. CC received funding\nfrom the European Union’s Horizon 2020 research and in-\nnovation programme under the Marie Skłodowska-Curie\ngrant agreement No. 101065949. This material is based\nupon work supported by the National Science Foundation\nunder Grant No. 2212310 to AG and JA. OpenAI credits for\nGPT-3 access were provided through OpenAI’s Researcher\nAccess Program. We thank Sam Toyer and the members of\nthe RLL for feedback on early iterations of this project.\nReferences\nAbid, A., Farooqi, M., and Zou, J. Persistent anti-muslim\nbias in large language models. In Proceedings of the\n2021 AAAI\/ACM Conference on AI, Ethics, and Society,\npp. 298–306, 2021.\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A.,\nand Bellemare, M. G. Deep reinforcement learning at\nthe edge of the statistical precipice. Advances in Neural\nInformation Processing Systems, 2021.\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,\nDavid, B., Finn, C., Gopalakrishnan, K., Hausman, K.,\nHerzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan,\nA., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi,\nN. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-\nH., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P.,\nQuiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Ser-\nmanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V.,\nXia, F., Xiao, T., Xu, P., Xu, S., and Yan, M. Do as i can,\nnot as i say: Grounding language in robotic affordances,\n2022. URL https:\/\/arxiv.org\/abs\/2204.01691.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. arXiv preprint arXiv:2204.14198, 2022.\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schul-\nman, J., and Man´e, D. Concrete problems in ai safety.\narXiv preprint arXiv:1606.06565, 2016.\nAubret, A., Matignon, L., and Hassas, S. A survey on\nintrinsic motivation in reinforcement learning.\narXiv\npreprint arXiv:1908.06976, 2019.\nBaranes, A. and Oudeyer, P.-Y. Active learning of inverse\nmodels with intrinsically motivated goal exploration in\nrobots. Robotics and Autonomous Systems, 61(1):49–73,\n2013.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,\nSaxton, D., and Munos, R. Unifying count-based ex-\nploration and intrinsic motivation. Advances in neural\ninformation processing systems, 29, 2016.\nBender, E. M., Gebru, T., McMillan-Major, A., and\nShmitchell, S. On the dangers of stochastic parrots: Can\nlanguage models be too big? In Proceedings of the 2021\nACM conference on fairness, accountability, and trans-\nparency, pp. 610–623, 2021.\n9\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,\nM., Gray, S., Chess, B., Clark, J., Berner, C., McCan-\ndlish, S., Radford, A., Sutskever, I., and Amodei, D.\nLanguage models are few-shot learners, 2020.\nURL\nhttps:\/\/arxiv.org\/abs\/2005.14165.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Ex-\nploration by random network distillation. In Seventh\nInternational Conference on Learning Representations,\npp. 1–17, 2019.\nChan, H., Wu, Y., Kiros, J., Fidler, S., and Ba, J.\nActrce: Augmenting experience via teacher’s advice\nfor multi-goal reinforcement learning. arXiv preprint\narXiv:1902.04546, 2019.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\nChoi, K., Cundy, C., Srivastava, S., and Ermon, S. LMPriors:\nPre-trained language models as task-specific priors. arXiv\npreprint arXiv:2210.12530, 2022.\nColas, C., Sigaud, O., and Oudeyer, P.-Y. Gep-pg: Decou-\npling exploration and exploitation in deep reinforcement\nlearning algorithms. In International conference on ma-\nchine learning, pp. 1039–1048. PMLR, 2018.\nColas, C., Karch, T., Lair, N., Dussoux, J.-M., Moulin-\nFrier, C., Dominey, P., and Oudeyer, P.-Y. Language\nas a cognitive tool to imagine goals in curiosity driven\nexploration. Advances in Neural Information Processing\nSystems, 33:3761–3774, 2020.\nColas, C., Karch, T., Sigaud, O., and Oudeyer, P.-\nY. Autotelic agents with intrinsically motivated goal-\nconditioned reinforcement learning: a short survey. Jour-\nnal of Artificial Intelligence Research, 74:1159–1199,\n2022.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDubey, R., Agrawal, P., Pathak, D., Griffiths, T. L., and\nEfros, A. A. Investigating human priors for playing video\ngames. arXiv preprint arXiv:1802.10217, 2018.\nHafner, D. Benchmarking the spectrum of agent capabilities.\narXiv preprint arXiv:2109.06780, 2021.\nHermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,\nSoyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg,\nM., Teplyashin, D., et al. Grounded language learning in\na simulated 3d world. arXiv preprint arXiv:1706.06551,\n2017.\nHill, F., Lampinen, A., Schneider, R., Clark, S., Botvinick,\nM., McClelland, J. L., and Santoro, A. Environmental\ndrivers of systematicity and generalization in a situated\nagent. arXiv preprint arXiv:1910.00571, 2019.\nHill, F., Mokra, S., Wong, N., and Harley, T.\nHu-\nman instruction-following with deep reinforcement learn-\ning via transfer-learning from text.\narXiv preprint\narXiv:2005.09382, 2020.\nHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. Lan-\nguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. arXiv preprint\narXiv:2201.07207, 2022a.\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence,\nP., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al.\nInner monologue: Embodied reasoning through planning\nwith language models. arXiv preprint arXiv:2207.05608,\n2022b.\nKant, Y., Ramachandran, A., Yenamandra, S., Gilitschenski,\nI., Batra, D., Szot, A., and Agrawal, H. Housekeep: Tidy-\ning virtual households using commonsense reasoning.\nIn Avidan, S., Brostow, G., Ciss´e, M., Farinella, G. M.,\nand Hassner, T. (eds.), Computer Vision – ECCV 2022,\npp. 355–373, Cham, 2022. Springer Nature Switzerland.\nISBN 978-3-031-19842-7.\nKong, Y. and Fu, Y. Human action recognition and predic-\ntion: A survey. International Journal of Computer Vision,\n130(5):1366–1401, 2022.\nKwon, M., Xie, S. M., Bullard, K., and Sadigh, D. Re-\nward design with language models.\nIn International\nConference on Learning Representations, 2023. URL\nhttps:\/\/openreview.net\/forum?id=10uNUgI5Kl.\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration\nin deep reinforcement learning: A survey. Information\nFusion, 2022.\nLehman, J., Stanley, K. O., et al. Exploiting open-endedness\nto solve problems through the search for novelty. In\nALIFE, pp. 329–336, 2008.\nLehman, J., Clune, J., Misevic, D., Adami, C., Altenberg,\nL., Beaulieu, J., Bentley, P. J., Bernard, S., Beslon, G.,\nBryson, D. M., Cheney, N., Chrabaszcz, P., Cully, A.,\nDoncieux, S., Dyer, F. C., Ellefsen, K. O., Feldt, R., Fis-\ncher, S., Forrest, S., F´renoy, A., Gag´ne, C., Le Goff,\nL., Grabowski, L. M., Hodjat, B., Hutter, F., Keller,\n10\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nL., Knibbe, C., Krcah, P., Lenski, R. E., Lipson, H.,\nMacCurdy, R., Maestre, C., Miikkulainen, R., Mitri, S.,\nMoriarty, D. E., Mouret, J.-B., Nguyen, A., Ofria, C.,\nParizeau, M., Parsons, D., Pennock, R. T., Punch, W. F.,\nRay, T. S., Schoenauer, M., Schulte, E., Sims, K., Stan-\nley, K. O., Taddei, F., Tarapore, D., Thibault, S., Watson,\nR., Weimer, W., and Yosinski, J. The Surprising Cre-\nativity of Digital Evolution: A Collection of Anecdotes\nfrom the Evolutionary Computation and Artificial Life\nResearch Communities. Artificial Life, 26(2):274–306,\n05 2020. ISSN 1064-5462. doi: 10.1162\/artl a 00319.\nURL https:\/\/doi.org\/10.1162\/artl a 00319.\nLinke, C., Ady, N. M., White, M., Degris, T., and White,\nA. Adapting behavior via intrinsic reward: A survey\nand empirical study. Journal of Artificial Intelligence\nResearch, 69:1287–1332, 2020.\nLiu, H. and Abbeel, P. Behavior from the void: Unsuper-\nvised active pre-training. Advances in Neural Information\nProcessing Systems, 34, 2021.\nLuketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas,\nJ., Grefenstette, E., Whiteson, S., and Rockt¨aschel, T. A\nsurvey of reinforcement learning informed by natural\nlanguage. arXiv preprint arXiv:1906.03926, 2019.\nLynch, C. and Sermanet, P. Language conditioned imi-\ntation learning over unstructured data. arXiv preprint\narXiv:2005.07648, 2020.\nMin, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen,\nT. H., Sainz, O., Agirre, E., Heinz, I., and Roth, D. Re-\ncent advances in natural language processing via large\npre-trained language models: A survey. arXiv preprint\narXiv:2111.01243, 2021.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. nature, 518(7540):\n529–533, 2015.\nMokady, R., Hertz, A., and Bermano, A. H.\nClipcap:\nClip prefix for image captioning, 2021a. URL https:\n\/\/arxiv.org\/abs\/2111.09734.\nMokady, R., Hertz, A., and Bermano, A. H.\nClip-\ncap: Clip prefix for image captioning. arXiv preprint\narXiv:2111.09734, 2021b.\nMu, J., Zhong, V., Raileanu, R., Jiang, M., Goodman, N.,\nRockt¨aschel, T., and Grefenstette, E. Improving intrinsic\nexploration with language abstractions. arXiv preprint\narXiv:2202.08938, 2022.\nNadeem, M., Bethke, A., and Reddy, S. Stereoset: Mea-\nsuring stereotypical bias in pretrained language models.\narXiv preprint arXiv:2004.09456, 2020.\nOudeyer, P.-Y. and Kaplan, F. What is intrinsic motivation?\na typology of computational approaches. Frontiers in\nneurorobotics, pp. 6, 2009.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,\nK., Ray, A., et al.\nTraining language models to fol-\nlow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T.\nCuriosity-driven exploration by self-supervised predic-\ntion. In International conference on machine learning,\npp. 2778–2787. PMLR, 2017.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748–8763. PMLR, 2021.\nReimers, N. and Gurevych, I. Sentence-bert: Sentence\nembeddings using siamese bert-networks. 11 2019. URL\nhttp:\/\/arxiv.org\/abs\/1908.10084.\nSharma, P., Torralba, A., and Andreas, J.\nSkill induc-\ntion and planning with latent language. arXiv preprint\narXiv:2110.01517, 2021.\nStani´c, A., Tang, Y., Ha, D., and Schmidhuber, J. Learning\nto generalize with object-centric agents in the open world\nsurvival game crafter. arXiv preprint arXiv:2208.03374,\n2022.\nStefanini, M., Cornia, M., Baraldi, L., Cascianelli, S., Fia-\nmeni, G., and Cucchiara, R. From show to tell: a survey\non deep learning-based image captioning. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,\n2022.\nSutton, R. S., Barto, A. G., et al. Introduction to reinforce-\nment learning, volume 135. MIT press Cambridge, 1998.\n11\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nTam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy,\nN. A., Chan, S. C., Strouse, D., Wang, J. X., Banino,\nA., and Hill, F. Semantic exploration from language ab-\nstractions and pretrained representations. arXiv preprint\narXiv:2204.05080, 2022.\nTen, A., Oudeyer, P.-Y., and Moulin-Frier, C. Curiosity-\ndriven exploration. The Drive for Knowledge: The Sci-\nence of Human Information Seeking, pp. 53, 2022.\nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In Proceedings of\nthe AAAI conference on artificial intelligence, volume 30,\n2016.\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M.,\nand Freitas, N. Dueling network architectures for deep\nreinforcement learning. In International conference on\nmachine learning, pp. 1995–2003. PMLR, 2016.\nYao, S., Rao, R., Hausknecht, M., and Narasimhan, K. Keep\ncalm and explore: Language models for action generation\nin text-based games. arXiv preprint arXiv:2010.02903,\n2020.\nYarats, D., Fergus, R., Lazaric, A., and Pinto, L. Reinforce-\nment learning with prototypical representations. In Inter-\nnational Conference on Machine Learning, pp. 11920–\n11931. PMLR, 2021.\nZaidi, S. S. A., Ansari, M. S., Aslam, A., Kanwal, N., As-\nghar, M., and Lee, B. A survey of modern deep learning\nbased object detection models. Digital Signal Processing,\npp. 103514, 2022.\nZhang, T., Xu, H., Wang, X., Wu, Y., Keutzer, K., Gonza-\nlez, J. E., and Tian, Y. Noveld: A simple yet effective\nexploration criterion. Advances in Neural Information\nProcessing Systems, 34, 2021.\n12\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nA. Crafter Pretraining Ablation\n2.5\n5.0\n7.5\nOracle (goals)\nOracle (no goals)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nNovelty\nUniform\nMedian\n2.5\n5.0\n7.5\nIQM\n2.5\n5.0\n7.5\nMean\nAchievements\nFigure 8: Number of ground truth achievements unlocked per episode at the end of pretraining. We show the median,\ninterquartile mean (IQM) and mean of the achievements measured in 10 evaluation trials, each averaged over 10 episodes\nand 5 seeds (50 points) (Agarwal et al., 2021). Opaque bars represent variants leveraging textual observations in addition of\nvisual ones and dashed lines represent the gap with vision-only variants (less opaque). We report results for each method\ndescribed in Table 1. Results show that providing textual observations increases performance across all conditions.\nB. Crafter Downstream Training\nWe finetune on seven downstream Crafter tasks plus the Crafter game reward:\n• Place Crafting Table - agent must chop a tree and then create a crafting table. This is an easy task most agents will\nhave seen during pretraining.\n• Attack Cow - agent must chase and attack a cow. This is also an easy task often seen during pretraining in most\nmethods.\n• Make Wood Sword - agent must chop a tree, use it to make a crafting table, chop a second tree, use the wood at the\ncrafting table to make a wood sword. This task could be achieved during the pretraining env, but many agents rarely or\nnever achieved it because of the sheer number of prerequisites.\n• Mine Stone - agent must chop a tree, use it to make a crafting table, chop a second tree, use the wood at the crafting\ntable to make a wood pickaxe, seek out stone, and then mine stone. This task is so challenging that we replaced the\nfully sparse reward (where all pretraining methods fail) with a semi-sparse reward for achieving each subtask.\n• Deforestation - agent must chop 4 trees in a row. This task tests whether having goal conditioning improves performance\nby directing the agent. During pretraining most agents will have chopped a tree, but novelty bias should deter agents\nfrom regularly chopping 4 trees in a row.\n• Gardening Like above, this task tests the value of goal conditioning. The agent must first collect water and then chop\nthe grass. Both skills maybe have been learned during pretraining, but never in sequence.\n• Plant Row - agent must plant two plants in a row. This task is challenging because even a highly skilled ELLM agent\ncannot have learned this task 0-shot because the state captioner has no concept of a “row”.\nC. Crafter Env Modifications\nThe default Crafter action space contains an all purpose “do” action which takes different actions depending on what object\nthe agent is facing - for instance attacking a skeleton, chopping a tree, or drinking water.\nWe modify the action space to increase the exploration problem by turning the general ‘do’ action into more precise\ncombinations of action verbs + noun arguments. Whereas ‘do’ previously was an all purpose action that could attack a\n13\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n0\n10\n20\nEnv Steps (x 1e4)\n0.00\n0.25\n0.50\n0.75\n1.00\nSuccess Rate\nPlace Crafting Table\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nAttack Cow\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nMake Wood Sword\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nMine Stone\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nDeforestation\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nPlant Row\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nGardening\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\n0.0\n2.5\n5.0\n7.5\n10.0\nCrafter Score\nCrafter (Game Reward)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nScratch\nOracle (no goals)\nUniform\nNovelty\nOracle (goals)\nFigure 9: Goal completion success rate for different tasks in the Crafter environment. RL training uses sparse rewards. Each\nmethod trains an agent from scratch while using a pretrained policy for exploration. Each line shows the mean across 5\nseeds with shaded stds.\n0\n200\n400\nSteps x1e4\n0\n2\n4\n6\n8\nCrafter Achievements\nPretraining with the Original Env\nOracle (goals)\nELLM (goals) - Ours\nRND\nAPT\nUniform\nNovelty\n0\n500\n0\n10 Oracle (goals)\n0\n500\nSteps x1e4\n0\n5\nELLM (goals) - Ours\n0\n500\n0\n2\nRND\n0\n500\nSteps x1e4\n0.0\n2.5\nAPT\nModified Env\nOriginal Env\n0\n500\n0\n1\nUniform\n0\n500\nSteps x1e4\n0\n2\nNovelty\nFigure 10: Training without the environment simplifications described in Section 4.1. Left: pretraining results (comparable\nto Figure 4). Right: original vs modified env performance. Curves average over 3 seeds with std shading. We see minor\nperformance changes across most algorithms but no change in the rank-order of methods.\nskeleton, chop a tree, or drink water, the agent must now learn to choose between the actions as arbitrary verb + noun\ncombinations, ‘attack skeleton’, ‘chop tree’, ‘drink water.’ The exploration problem becomes more difficult as\nthis larger combinatorial action space is not restricted to admissible actions and the agent could try to drink skeleton\nor attack water. Whereas the old action space was 17-dimensional, our new combinatorial one contains 260 possible\nactions. One way to impose human priors is to design the agent’s action space explicitly to disallow invalid combinations\n(e.g. ’drink’ + ’furnace’). However, manually designing and imposing such constraints is also unlikely to be scalable.\nWe hypothesize that our method, guided by common-sense knowledge from LLMs, will focus on learning to use only\nmeaningful action combinations. For the purposes of the Novelty and Uniform baselines, which reward agents for achieving\neven nonsensical goals, we consider a goal “achieved” if the agent takes an action in front of the appropriate target object\n(e.g taking “drink furnace” in front of a furnace).\nD. Crafter Prompt\nValid actions: sleep, eat, attack, chop, drink, place, make, mine\nYou are a player playing a game. Suggest the best actions the player can take based on the things\nyou see and the items in your inventory. Only use valid actions and objects.\nYou see plant, tree, and skeleton. You are targeting skeleton. What do you do?\n- Eat plant\n- Chop tree\n14\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n- Attack skeleton\nYou see water, grass, cow, and diamond. You are targeting grass. You have in your inventory plant.\nWhat do you do?\n- Drink water\n- Chop grass\n- Attack cow\n- Place plant\nIn total, the actions present in the prompt make up:\n• 6 \/ 10 (60%) of the good actions the ELLM agent receives.\n• 6 \/ 21 (28.6%) of all rewarded actions the agent receives.\n• 7 \/ 15 (50%) of all good action suggested.\n• 7 \/ 51 (13.7%) of all actions suggested.\nIn future work, it would be interesting to explore how performance changes with fewer actions included in the prompt. As a\npreliminary experiment, we have found that pretraining performance is maintained if you provide a prompt with only one\nexample of a list of valid goals. The list only contains two goals. Instead, we use more extensive instructions to tell the\nagent what good suggestions look like. See the prompt below and pretraining comparison in Figure 11. This new prompt\ncomes with a decrease in the fraction of “Good” suggestions (shown in Table 4, showing that suggestion accuracy is not\nperfectly correlated with success.\nNew prompt: Valid actions: sleep, eat, attack, chop, drink, place, make, mine\nYou are a player playing a Minecraft-like game.\nSuggest the best actions the player can take\naccording to the following instructions.\n1. Make suggestions based on the things you see and the items in your inventory.\n2.\nEach scene is independent.\nOnly make suggestions based on the visible objects, status, and\ninventory in the current scene.\n3. Each suggestion should either be a single valid action, or a phrase consisting of an action and\nan object. (example: \"Eat plant\").\n4. Do not make suggestions which are not possible or not desirable, such as ‘‘Eat skeleton’’.\n5. Only make suggestions which are reasonable given the current scene (e.g. only ‘‘Eat plant’’ if\na plant is visible).\n6. You may suggest multiple actions with the same object, but do not duplicate list items.\n7. Use your knowledge of Minecraft to make suggestions.\n8. Prioritize actions which involve the object you are facing or which the agent hasn’t achieved\nbefore.\n9. Each scene will include a minimum and maximum number of suggestions. Stick within this range.\nNew scene: You see plant, cow, and skeleton. You are facing skeleton. What do you do (include 1-2\nsuggestions)?\n- Eat plant\n- Attack skeleton\nNew scene: You see [INSERT CURRENT SCENE DESCRIPTION.] What do you do (include 2-7 suggestions)?\n15\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nSuggested\nRewarded\nContext-Insensitive\n21.0%\n0.8%\nCommon-Sense Insensitive\n20.5%\n54.8%\nGood\n34.1%\n44.4%\nImpossible\n24.5%\n0%\nTable 4: Fractions of suggested and rewarded goals which are good, generated with the modified two-example prompt.\n0\n10\n20\n30\n40\n50\nSteps x1e4\n0\n2\n4\n6\nCrafter Achievements\nPrompt Comparisons\nELLM (no goals) - 7 example goals\nELLM (no goals) - 2 example goals\nFigure 11: Comparison between performance of the prompt containing 7 suggested goals (used throughout the paper) and a\nmodified prompt which only includes 2 examples.\nE. Crafter Action Space\nWe expand the action space of Crafter to increase exploration difficulty and study if ELLM can learn to avoid nonsensical or\ninfeasible actions. The full action space consists of just verbs (for actions that do not act on anything, such as sleep) or\nverb + noun combinations as follows:\n• Verbs: do nothing (no noun), move left (no noun), move right (no noun), move up (no noun), move down (no\nnoun), sleep (no noun), mine, eat, attack, chop, drink, place, make\n• Nouns: zombie, skeleton, cow, tree, stone, coal, iron, diamond, water, grass, crafting table,\nfurnace, plant, wood pickaxe, stone pickaxe, iron pickaxe, wood sword, stone sword, iron sword\nFor example, an action can be drink water or drink grass.\nF. Housekeep Tasks\nThe original Housekeep benchmark features a large set of different household scenes and episodes with different objects and\nreceptacles possibly instantiated. The ground truth correct object-receptacle placements were determined by crowdsourcing\nhumans. However, since our focus is on RL pretraining, we do not make use of the mapping and planning methods from the\noriginal benchmark. To scope the problem for RL, we focus on the first 4 tasks with 5 different misplaced objects per task.\nMisplaced Objects\nTask 1\npeppermint, lamp, lantern, herring fillets, vase\nTask 2\nlamp, sparkling water, plant, candle holder, mustard bottle\nTask 3\npepsi can pack, electric heater, helmet, golf ball, fruit snack\nTask 4\nchocolate, ramekin, pan, shredder, knife\nTable 5: Objects per task\n16\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nName\nValue (Crafter)\nValue (Housekeep)\nFrame Stack\n4\n4\nγ\n.99\n.99\nSeed Frames\n5000\n5000\nn-step\n3\n3\nbatch size\n64\n256\nlr\n6.25e-5\n1e-4\ntarget update τ\n1.0\n1.0\nϵ-min\n0.01\n0.1\nupdate frequency\n4\n4\nTable 6: DQN Hyperparameters\nG. Housekeep Prompt\nYou are a robot in a house. You have the ability to pick up objects and place them in new locations.\nFor each example, state if the item should be stored in\/on the receptacle.\nShould you store a dirty spoon in\/on the chair: No.\nShould you store a mixing bowl in\/on the dishwasher: Yes.\nShould you store a clean sock in\/on the drawer: Yes.\nH. Algorithmic Details\nWe make use of DQN (Mnih et al., 2013), with double Q-learning (Van Hasselt et al., 2016), dueling networks (Wang et al.,\n2016), and multi-step learning (Sutton et al., 1998).\nFor both environments, policies take in 84 × 84 images which are encoded using the standard Nature Atari CNN (Mnih et al.,\n2015). The image is then passed through a linear layer to output a 512 dimensional vector. If the policy is text-conditioned,\nwe compute the language embedding of the state caption using paraphrase-MiniLM-L3-v2 SBERT model (Reimers &\nGurevych, 2019), and if the policy is goal-conditioned we similarly compute the language embedding of the goals g1:k\nusing paraphrase-MiniLM-L3-v2. We encode all goals as a single text sequence as we did not see any improvement from\nencoding them each separately and summing or concatenating the embeddings. The image and text embeddings are then\nconcatenated together before being passed to the Q-networks. Each of the value and advantage streams of the Q-function are\nparametrized as 3-layer MLPs, with hidden dimensions of 512 and ReLU nonlinearities.\nIn the Crafter environment, we swept over the following hyperparameters for the Oracle and Scratch (no-pretraining)\nconditions: learning rate, exploration decay schedule, and network update frequency. We then applied these hyperparameters\nto all conditions, after confirming that the hyperparameters were broadly successful in each case.\nFor Housekeep pretraining, we swept lr ∈[1e −3, 1e −4, 1e −5], ϵ-min ∈[0.1, 0.01], and batch size ∈[64, 256].\nI. Hard-coded Captioner Details\nCrafter\nThe state captioner is based on the template shown in Figure 3 (left). This consists of three components: the\nobservation, the items, and the agent status.\n• Observation: We take the underlying semantic representation of the current image from the simulator. Essentially this\nmaps each visible grid cell to a text description (e.g. each tree graphic is mapped to “tree”). We then take this set of\ndescriptions (i.e. not accounting for the number of each object) and populate the “observation” cell of the template.\n• Items: We convert each of the inventory items to the corresponding text descriptor, and use this set of descriptions to\npopulate the “item” cell of the template.\n• Health status: We check if any of the health statuses are below maximum, and if so, convert each to a corresponding\nlanguage description (e.g. if the hunger status is < 9, we say the agent is “hungry”).\n17\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nThe transition captioner uses the action labels. Each action maps to a predefined verb + noun pairing directly (e.g. “eat\ncow”).\nHousekeep\nThe state captioner is based on the template shown in Figure 3 (right). We use the simulator’s semantic sensor\nto get a list of all visible objects, receptacles, and the currently held object. The transition captioner is also based on the\nsimulator’s semantic sensor, which indicates which receptacles the visible objects are currently in.\nJ. Learned Crafter Captioner\nThe captioner is trained with a slightly modified ClipCap algorithm (Mokady et al., 2021a) on a dataset of trajectories\ngenerated by a trained policy using the PPO implementation from Stani´c et al. (2022). Visual observations at timestep t and\nt+1 are embedded with a pretrained and frozen CLIP ViT-B-32 model (Radford et al., 2021) and concatenated together with\nthe difference in semantic embeddings between the two corresponding states. Semantic embeddings include the inventory\nand a multi-hot embedding of the set of objects present in the local view of the agent. This concatenated representation of\nthe transition is then mapped through a learned mapping function to a sequence of 10 tokens. Finally, we use these 10 tokens\nas a prefix and pursue decoding using a pretrained and frozen GPT-2 to generate the caption (Radford et al., 2019). We\ntrain the mapping from transition representation to GPT tokens on a dataset of 847 human labels and 900 synthetic labels\nobtained by sampling from a set of between 3 and 8 different captions for each each distinct type of transitions. Instead of\nthe programmatic “chop tree” and “attack zombie,” labeled captions involve fully-formed sentences: “You collected\na sapling from the ground,” “You built a sword out of wood,” or “You just stared at the sea.” Because of\nthis additional linguistic diversity, we compare captions to goals with a lower cosine similarity threshold of .5.\nImperfect captioners can cause learning issues in two different ways: (1) they can generate wrong captions all together\nand (2) they can generate a valid caption that still lead to faulty reward computations. If the caption is linguistically too\ndifferent from the achievement it captions, the similarity-based reward might not be able to pick it up (false negative reward).\nThis same linguistic variability might cause the reward function to detect the achievement of another achievement that was\nnot achieved (false positive reward). Figure 12 measures all these issues at once. For each row, it answers: what is the\nprobability that the reward function would detect a positive reward for each of the column achievements when the true\nachievement is the row label? The false negative rate is 11% on average (1 - the diagonal values), with a much higher\nfalse negative rate for chop grass (100%). Indeed, human caption mentioned the outcome of that action instead of the\naction itself (collect sapling); which the similarity-based reward fails to capture. The false positive rate (all non diagonal\nvalues) is significant here: the agent can get rewarded for several achievements it did not unlock. This often occurs when\nachievements share words (e.g. wood, stone, collect). This indicates a difficulty of the semantic similarity to differentiate\nbetween achievements involving these words.\nK. Crafter LLM Analysis\nTable 2 shows that the actions agents are rewarded for are dominated by good actions (66.5%) and bad actions (32.4%).\nThis makes sense; impossible actions can never be achieved. Most context-insensitive cannot be achieved (e.g. “drink\nwater” suggested when no water is present). We consider an action a “success” by checking whether the agent attempted a\nparticular action in front of the right object, so the agent occasionally is rewarded when it takes a context-insensitive action\nin the appropriate physical location but without the necessary prerequisites (e.g. mining stone without a pickaxe).\nTable 7 gives examples of LLM suggestions in Crafter.\nSuggestion Type\nExamples\nGood\nchop tree, attack skeleton, place plant\nContext-Insensitive\nmake crafting table (without wood), mine stone (without a pickaxe or not by stone)\nCommon-Sense-Insensitive\nmine grass, make diamond, attack plant\nImpossible\nmake path, make wood, place lava\nTable 7: Classification accuracy of LLM for each Housekeep task (left column is true positives, right column is true\nnegatives).\n18\nGuiding Pretraining in Reinforcement Learning with Large Language Models\ndo nothing\nchop tree\nmake crafting table\nmake wood pickaxe\nmake wood sword\nchop grass\nplace plant\ndrink water\neat cow\nattack zombie\nmine stone\nplace stone\nmake stone sword\nmine coal\nplace furnace\nattack skeleton\nmake stone pickaxe\nmine iron\nRewarded achievement\ndo nothing\nchop tree\nmake crafting table\nmake wood pickaxe\nmake wood sword\nchop grass\nplace plant\ndrink water\neat cow\nattack zombie\nmine stone\nplace stone\nmake stone sword\nmine coal\nplace furnace\nattack skeleton\nmake stone pickaxe\nmine iron\nTrue achievement\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 12: Reward confusion matrix. Each row gives the probability that any of the column achievement is detected when\nthe row achievement is truly unlocked. For instance, in row 2, when the agent chops a tree, with high probability the agent\nwill be rewarded for the “chop tree” and “chop grass” actions. Tested on trajectories collected from an expert PPO policy,\neach row estimates probabilities using between 27 and 100 datapoints (27 for mine iron, the rarest achievements). Rows do\nnot sum to one, as a given achievement, depending on its particular caption, could potentially trigger several rewards.\nL. Novelty Bonus Ablation\nWe ablate the importance of ELLM’s novelty bias in Figure 13 by allowing the agent to be rewarded repeatedly for achieving\nthe same goal. We see that without the novelty bonus the agent only learns to repeat a small set of easy goals and fails to\nexplore diversely.\nM. Analysis of Downstream Training Approaches\nWe explored two methods for using exploratory policies: finetuning, where the weights of the exploration policy are finetuned\nand the guided exploration method, where a new policy is trained from scratch and the pretrained policy is used for ϵ-greedy\nexploration.\nWe found that in Housekeep both methods are effective for ELLM (Figure 7a and Figure 7b). However, in Crafter we found\nthat the finetuning method performed poorly across all methods (ELLM, baselines, and oracles). Often, we observed that\nearly in finetuning, the agent would unlearn all of its previous useful behaviors, including moving around the environment\nto interact with objects. We hypothesize that this due to a mismatch in the density and magnitude of rewards between\npretraining and finetuning. When the finetuning agent finds it is achieving much lower than the expected return for taking\nits typical actions, it down-weights the likelihood of taking those actions and unlearns its previous skills. We found that\ndecreasing the learning rate, freezing early layers of the network, manually adjusting finetuning rewards to be at the same\nscale as pretraining rewards, and decreasing the initial exploration rate partially mitigated this problem. However, these\nalso decrease the sample efficiency and\/or performance at convergence of the finetuned policy compared to a training-from-\nscratch baseline. In Figure 14), across all methods this method is less reliable than the guided exploration method (Figure\n5).\nThese findings are consistent with our Housekeep findings. In that environment, the ELLM pretraining task (achieving\nobject placements suggested by a LLM) and the finetuning task (achieving object placements suggested by humans) are\nsimilar enough we only see minor dips in performance when finetuning starts. However, the RND and APT baselines have a\ngreater pretrain-finetune mismatch, and we observe those methods did comparatively better with the guided exploration\nmethod.\n19\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n0\n100\n200\n300\n400\n500\nEnvironment Steps (x 1e4)\n0\n2\n4\n6\n8\n10\n12\nCrafter Achievements\nELLM (goals) - Ours\nELLM (no goals) without novelty\nRND\nNovelty\n(a) Crafter pretraining runs (similar to Figure 4), including the\n“ELLM without novelty” ablation where ELLM’s novelty bias is\nremoved.\n(b) Housekeep pretraining runs (similar to Figure 7a), including the “ELLM without novelty” ablation where\nELLM’s novelty bias is removed.\nFigure 13\n0\n10\n20\nEnv Steps (x 1e4)\n0.0\n0.5\n1.0\nSuccess Rate\nPlace Crafting Table\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nAttack Cow\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nMake Wood Sword\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nMine Stone\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nDeforestation\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nPlant Row\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nGardening\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\n0.0\n2.5\n5.0\n7.5\nCrafter Score\nCrafter (Game Reward)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nScratch\nFigure 14: Success rates across training for each of the seven downstream tasks in the Crafter environment. Each run\nfinetunes the pretrained agent using a lower learning rate than used during pretraining (2e −5). Plots show mean ± std for 5\nseeds\nN. Additional Baselines\nWe also include experiments with NovelD (Zhang et al., 2021) in Figure 15, a state-of-the-art exploration method which\nuses an estimate of state novelty to reward the agent for moving to more novel states. During pretraining, we find it performs\nsimilarly to the other prior-free intrinsic motivation methods.\nO. Code and Compute\nAll code will be released soon, licensed under the MIT license (with Crafter, Housekeep licensed under their respective\nlicenses).\nFor LLM access, we use OpenAI’s APIs. Initial experiments with the smaller GPT-3 models led to degraded performance,\nhence choosing Codex and Davinci for our experiments. Codex is free to use and Davinci is priced at $0.02\/1000 tokens.\nWe find caching to be significantly helpful in reducing the number of queries made to the API. Each API query takes .02\nseconds, so without caching a single 5-million step training run would spend 27 hours querying the API (and far more once\nwe hit the OpenAI rate limit) and cost thousands of dollars. Since we cache heavily and reuse the cache across runs, by the\nend of our experimentation, were make almost no API queries per run.\nWe use NVIDIA TITAN Xps and NVIDIA GeForce RTX 2080 Tis, with 2-3 seeds per GPU and running at roughtly\n20\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n0\n100\n200\n300\n400\n500\nEnvironment Steps (x 1e4)\n0\n2\n4\n6\n8\n10\n12\nCrafter Achievements\nOracle (goals)\nOracle (no goals)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nNovelty\nUniform\nNovelD\n(a) Crafter pretraining curve as in Figure 4, including NovelD baseline\n(b) Housekeep pretraining curves as in Figure 7a, including NovelD baseline\nFigure 15: Additional pretraining curves including NovelD.\n100ksteps\/hour. Across all the ablations, this amounts to approximately 100 GPUs for pretraining.\nP. Societal Impact\nWhile LLMs priors have been shown to exhibit impressive common-sense capabilities, it is also well-known that such\nmodels are highly prone to harmful social biases and stereotypes (Bender et al., 2021; Abid et al., 2021; Nadeem et al.,\n2020). When using such models as reward functions for RL, as in ELLM, it is necessary to fully understand and mitigate\nany possible negative behaviors that can be learned as a result of such biases. While we focus on simulated environments\nand tasks in this work, we emphasize that more careful study is necessary if such a system is deployed to more open-ended\nlearning in the real world. Potential mitigations with ELLM specifically can be: actively filtering LLM generations for\nharmful content before using them as suggested goals, prompting the LM with guidelines about what kinds of prompts to\noutput, and\/or using only the closed-form ELLM variant with more carefully constrained goal spaces.\n21\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Guiding Pretraining in Reinforcement Learning with Large Language Models.pdf"}
{"title":"PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model","authors":"Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, Yanru Zhang","summary":"Poker, also known as Texas Hold'em, has always been a typical research target\nwithin imperfect information games (IIGs). IIGs have long served as a measure\nof artificial intelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret minimization\n(CFR) to tackle heads-up no-limit Poker. However, it is challenging for\nsubsequent researchers to learn CFR from previous models and apply it to other\nreal-world applications due to the expensive computational cost of CFR\niterations. Additionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work, we introduce\nPokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number\nof players and gaining high win rates, established on a lightweight large\nlanguage model (LLM). PokerGPT only requires simple textual information of\nPoker games for generating decision-making advice, thus guaranteeing the\nconvenient interaction between AI and humans. We mainly transform a set of\ntextual records acquired from real games into prompts, and use them to\nfine-tune a lightweight pre-trained LLM using reinforcement learning human\nfeedback technique. To improve fine-tuning performance, we conduct prompt\nengineering on raw data, including filtering useful information, selecting\nbehaviors of players with high win rates, and further processing them into\ntextual instruction using multiple prompt engineering techniques. Through the\nexperiments, we demonstrate that PokerGPT outperforms previous approaches in\nterms of win rate, model size, training time, and response speed, indicating\nthe great potential of LLMs in solving IIGs.","url":"http:\/\/arxiv.org\/abs\/2401.06781v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2401.06781v1","published":1704374870000,"comment":null,"pdf_text":"PokerGPT: An End-to-End Lightweight\nSolver for Multi-Player Texas Hold’em via\nLarge Language Model\nChenghao Huang‡, Yanbo Cao‡, Yinlong Wen†, Tao Zhou‡, and Yanru Zhang‡§\n‡University of Electronic Science and Technology of China\n§Shenzhen Institute of Advanced Study, UESTC, China\nzydhjh4593@gmail.com, yanbocao@std.uestc.edu.cn, zhutou@ustc.edu, yanruzhang@uestc.edu.cn,\n†Sichuan Optical Character Technology Co., Ltd\nlywen@chineseocr.com\nAbstract—Poker, also known as Texas Hold’em, has always\nbeen a typical research target within imperfect information\ngames (IIGs). IIGs have long served as a measure of artificial\nintelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret\nminimization (CFR) to tackle heads-up no-limit Poker. However,\nit is challenging for subsequent researchers to learn CFR from\nprevious models and apply it to other real-world applications\ndue to the expensive computational cost of CFR iterations.\nAdditionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work,\nwe introduce PokerGPT, an end-to-end solver for playing Texas\nHold’em with arbitrary number of players and gaining high win\nrates, established on a lightweight large language model (LLM).\nPokerGPT only requires simple textual information of Poker\ngames for generating decision-making advice, thus guaranteeing\nthe convenient interaction between AI and humans. We mainly\ntransform a set of textual records acquired from real games\ninto prompts, and use them to fine-tune a lightweight pre-\ntrained LLM using reinforcement learning human feedback\ntechnique. To improve fine-tuning performance, we conduct\nprompt engineering on raw data, including filtering useful infor-\nmation, selecting behaviors of players with high win rates, and\nfurther processing them into textual instruction using multiple\nprompt engineering techniques. Through the experiments, we\ndemonstrate that PokerGPT outperforms previous approaches\nin terms of win rate, model size, training time, and response\nspeed, indicating the great potential of LLMs in solving IIGs.\nIndex Terms—Data-driven artificial intelligence, imperfect in-\nformation game, large language model, reinforcement learning\nhuman feedback, end-to-end learning.\nI. INTRODUCTION\nSince the birth of the artificial intelligence (AI), intelligent\ngame research has been fertile ground for the development\nand innovation of AI, and has always been an important\nevaluation criterion for measuring its development level. In\n2016, AlphaGo defeated the human world champion in the\ngame of Go, which was considered as an important milestone\nfor AI solving perfect information games. However, a dis-\ntinctive feature of real-world games is the uncertainty of the\nopponent due to incomplete information. Poker is a typical\nimperfect information game (IIG) that has a long history as a\nchallenging problem for developing AI that can address hidden\ninformation [1]. Poker is a superb experimental and testing\nplatform, providing an excellent arena for the exploration\nand validation of foundational theories and methodologies in\nintelligent games.\nIn recent years, significant advancements have been made in\noptimized solutions for the game of Poker. Notably, DeepStack\nfrom the University of Alberta, Canada [2], and Libratus from\nCarnegie Mellon University (CMU), USA, have surpassed\nhuman professionals in heads-up no-limit Poker, a 2-player\nform of Poker [3]. The works of [4], [5] have been further\nproposed to improve the computational efficiency. Subsequent\nto these achievements, the Carnegie Mellon-designed Pluribus\ndemonstrated superiority by defeating a human professional in\n6-player no-limit Poker [6]. These prominent AI models for\nPoker commonly adopt iterative Counterfactual Regret Mini-\nmization (CFR) algorithms to approximate Nash equilibrium\nstrategies, involving initially compressing the state and action\nspace of the hand to reduce the game tree’s size, followed by\niterations of the CFR algorithm over the condensed game tree.\nHowever, these methods exhibit the following drawbacks:\n• Computational and storage resource consumption:\nDespite model reduction, significant computational and\nstorage resources are still required, escalating the overall\ncomputational cost. For example, in 2015, the work of [7]\nconsumed 4800 CPU cores, 10.9 TB memory, and 2\nmonths to solve heads-up limit Poker through CFR.\n• Difficulty in multi-player extension: Directly extending\nthe CFR framework to multi-player Poker environments\nproves to be challenging. An increase in the number\nof players leads to an exponential growth in the game\ntree size, diminishing generalization performance. For\nexample, CMU used CFR to solve 6-player no-limit\nPoker and consumed 12800 CPU cores, 12.5 TB memory,\nand 8 days [6]. The lack of flexibility make it unrealistic\nto use existing works in real-world games.\n• Loss of information: Game tree compression unavoid-\nably results in the loss of crucial information for decision\nmaking, such as the behavior and psychology of different\narXiv:2401.06781v1  [cs.AI]  4 Jan 2024\nplayers. This can lead to severe variations in fraudulent\nand anti-fraudulent strategies, jeopardizing the win rate\nagainst top players.\n• Dependence on human expert knowledge: For com-\npressing the game tree in CFR to reduce computational\ncost, human expert knowledge on mathematics and com-\nputational theory is heavily needed, causing large knowl-\nedge bottleneck and labor effort for improvement.\nHence, there arises a pressing need for a lightweight model\ncapable of autonomously extracting information to optimize\nIIGs further. Such a model should exhibit superior generaliza-\ntion performance, reduced dependence on expert knowledge,\nand enhanced adaptability to multi-player environments with-\nout compromising computational efficiency. Addressing this\nchallenge will be a pivotal focus in future research within the\nfield of Poker AI.\nThanks to the strong capability of large language model\n(LLM), it is promising to achieve a prominent framework for\nplaying Poker in real games. LLM is a novel deep learning\n(DL) model that can handle massive amounts of natural\nlanguage data, with various advantages in human-computer\ninteraction. Firstly, by pre-training on large-scale text data,\nLLM can extract word vectors while also learning contextual\ninformation to enhance the natural language understanding\ncapabilities. Secondly, LLM integrates generative tasks into\nthe encoder-decoder structure to achieve a unified natural\nlanguage processing capability, thereby improving the qual-\nity of text generation. Meanwhile, LLM exhibits flexibility\nand adaptability of human-computer interaction due to its\nremarkable zero-shot and few-shot abilities, that is, in-context\nlearning that only changes the input rather than the parameters.\nLast but not least, end-to-end learning streamlines the need\nfor intricate feature engineering and intermediate steps. LLMs\ncan autonomously acquire task-relevant rules and properties\nleveraging the powerful representation capabilities of deep\nneural networks. Thus, it is feasible to develop an LLM-\nbased approach for playing real games of multi-player no-limit\nPoker.\nIn this paper, we utilize a lightweight LLM and data\nfrom real Poker games to construct an AI solver for Poker\nsolutions, named PokerGPT. We firstly collect textual records\nof real games with arbitrary amount of players. Then, we\nfilter necessary information and process these records into\nprompts, a textual data form that is understandable for both\nhumans and LLMs. For dataset preparation, we integrate these\nprompts with actions in real games as labels. Based on the\ndataset, we fine-tune OPT-1.3B, a lightweight open-source\nLLM from Facebook [8] using reinforcement learning from\nhuman feedback (RLHF), a popular technique for fine-tuning\nLLMs. The interaction process is shown in Fig. 1. Our codes\nare publicly available 1.\nThe pivotal contributions of our work are listed as follows:\n• We present a novel approach based on LLMs to address\nIIGs. This approach employs an end-to-end learning\n1https:\/\/github.com\/hch211\/TH LLM\nPlayer's number: 6\nDealer's position: 6\nPlayer 1,3 : fold;\nPlayer 2: bet 1.5;\n······\nWhat should I do?\nCall 1.5\nInput:\nOther players' actions\nOutput:\nAction instruction\nInput\nPokerGPT\n1\n2\n3\n6\n5\n4\nObtain\ninformation\nFig. 1: Interaction process of PokerGPT.\nmethod, proved to be easily trainable and considerably\ncost-effective compared with previous approaches.\n• We propose PokerGPT, a pioneering DL model specifi-\ncally designed for Poker. With the power of LLMs, our\nmodel has a prominent advantage on convenient interac-\ntion with humans. More importantly, PokerGPT has the\ncapability of dealing with an arbitrary number of players\nin Poker games, showing the outstanding flexibility which\nhas never been achieved by previous works. Besides, our\nmodel contains much less parameters and takes much less\ninference time compared with conventional methods such\nas CFR, while consumes much less training time than\nnovel DL works, achieving great resource conservation.\n• Based on data of real games, we devise an efficient data\nprocessing approach specific to Poker, consisting of data\ncleansing and prompt engineering, leading to significant\nimprovements on fine-tuning of PokerGPT.\n• Through experiments, we demonstrate that our model\nexhibits significant advantages over existing models in\nterms of various metrics, such as win rate, training speed,\nresponse time, etc. Furthermore, we design 2 metrics,\nnamed action score and average investment, to study\ncharacteristics of PokerGPT’s strategies on Poker.\nThe remainder of this paper is organised as follows: Sec-\ntion II gives a literature review of classical works of Poker and\nLLM; Section III introduces the rules of Poker and the princi-\nples of LLMs; Section IV explains the holistic architecture of\nour proposed PokerGPT, including data acquisition, prompt\nengineering, and training process; Section V describes the\ncase studies, shows performance of PokerGPT with multiple\nmetrics, and analyzes the results; Section VI concludes this\nwork and presents pathways for future work.\nII. RELATED WORK\nA. Solutions for Poker\nPoker is a popular and challenging form of poker that has\nbeen used as a testbed for IIG algorithms for decades [9].\nA major breakthrough in this field was the introduction of\ncounterfactual regret minimization (CFR) [10], a simple and\nefficient iterative algorithm that converges to a Nash equilib-\nrium by minimizing the regrets of both players over time.\nCFR enabled the solution of heads-up limit Poker in 2015 [7],\nshifting the research focus to the more complex and realistic\nvariant of Poker.\nCFR once became a widely used algorithm for solving\nPoker, and a challenging benchmark for IIGs. Previous works\nhave shown that CFR variants can achieve superhuman per-\nformance in Poker against professional human players. The\nwork of [11] introduced subgame-solving techniques that\noutperform prior methods both in theory and practice. By\nfusing these techniques, in [3], the authors presented Libratus,\nan AI based on Monte-Carlo CFR (MCCFR) that defeated 4\ntop human specialists in heads-up no-limit Poker in a 120000-\nhand competition. Though Libratus was the leading benchmark\nin IIG solving, it required millions of core hours and terabytes\n(TBs) of memory to calculate. To improve computational\nefficiency, the work of [4] substituted values at leaf nodes\nof unknown states to limit the depth of the decision tree, and\nalso outperformed prior methods. The work of [5] introduced\nnovel CFR variants to discount regrets from earlier iterations\nin some cases differently for positive and negative regrets, and\nreweight iterations to obtain the output strategies. Since games\ncan be significantly complex when there are multiple players\nin games, the work of [6] extended Libratus to Pluribus, an\nAI that defeated top human professionals in 6-player Poker,\nthe most popular form of Poker. However, Pluribus approaches\nare also computationally expensive and memory-intensive, and\nconsumed significant time for inference.\nTo overcome the limitations of CFR-based algorithms, DL\nmethods have been applied to exploit the power of big data and\nneural networks (NNs). For example, DeepStack [2] used an\nNN to approximate the CFR values and performed recursive\nreasoning, while deep CFR [12] improved the performance and\nefficiency by using NNs to directly learn the CFR strategy in\nthe full game without abstraction. To reduce the dependence\non prior knowledge and training time, self-play deep reinforce-\nment learning (DRL) was integrated with CFR to enhance the\nexploration [13]. Moreover, the works of [14] proposed an\nend-to-end framework based on DRL and introduced several\ntechniques to accelerate it and defeat DeepStack.\nIn conclusion, the above methods rely heavily on com-\nputational resources, prior knowledge, and domain-specific\nheuristics, which constrain the applicability. Besides, it is\ndifficult for humans to use these works, also harming the\npracticality. Therefore, developing more efficient algorithms\nfor Poker and other complex IIGs remains an open problem.\nB. LLM\nRecent advances in pre-trained language models, such as\nChatGPT and GPT-4 [15], have inspired a range of studies that\nleverage information from various domains and modalities.\nThese studies aim to enhance the performance and applicabil-\nity of language models for diverse tasks, and can be broadly\ndivided into system design and ene-to-end training. System\ndesign employs ChatGPT as a dispatcher that orchestrates dif-\nferent expert models for specific tasks, such as Cola [16] and\nX-GPT [17]. On the other hand, end-to-end training integrates\nmodels from different modalities into a unified framework\nthat can be trained end-to-end, and then fine-tuned on data\nfrom particular domains, such as weather forecasting [18] and\naugmented reality [19].\nThe use of LLMs for games has attracted increasing at-\ntention from researchers, who have obtained some intriguing\nfindings. For instance, the results of [20] showed that LLMs\ncould produce responses that were remarkably similar to\nhuman feedback, despite some limitations on specific tasks in\nthe Beyond the Imitation Game benchmark. This suggested\nthat LLMs had the potential to handle human-related sce-\nnarios. The authors of [21] leveraged GPT-2 and GPT-3 to\nprocedurally generate RPG video game quest descriptions.\nSimilarly, the authors of [22] used GPT-3 to provide dialogue\nsuggestions for non-player characters as well as to highlight\nrelevant game module information. They both found that GPT-\n3 could generate significantly better sentences than GPT-2,\ndemonstrating the superior performance of GPT-3 and the\npromising prospects of future GPTs. Moreover, the authors\nof [23] compared GPT-3, GPT-3.5, and GPT-4 models in\nplaying finitely repeated games with each other and with other\nhuman-like strategies. They observed that LLMs performed\nsub-optimally in games that required coordination, but ex-\ncelled in games that involved individual competition and self-\ninterest, such as the iterated Prisoner’s Dilemma family. The\nabove research implies that LLMs can be feasible as a Poker\nsolver.\nAlthough there are still many challenges regarding the use of\nLLMs for games, LLMs have shown great potential for game-\nrelated research and applications. In this work, we propose a\nframework for Poker via LLMs, emphasizing the potential for\nLLMs to solve IIGs\nIII. PREREQUISITES\nA. Poker Rules\nPoker is a game that consists of multiple rounds, each\nstarting with 2 private cards (“PREFLOP” cards) dealt to each\nplayer, followed by 5 public cards (community cards) dealt\nin 3 stages. The stages are a set of 3 cards (the “FLOP”),\nan additional single card (the “TURN”), and a final card\n(the “RIVER”). Each player aims to form the best 5-card,\nalso named as a hand, using 2 private cards and 5 public\ncards. Players can “bet”, “check”, “call”, “raise”, or “fold”\nat each round. The player who has the best hand at the\n“SHOWDOWN”, or who remains in the game after all others\n“fold”, wins the money in the pot. Fig. 2 illustrates an simple\nexample of a 2-player Poker game, and the bet size is limited\nonly by the total amount wagered in each round. Besides,\nFig. 3 shows the card strengths.\nB. LLM\nLLM is a kind of DNN that generates natural language texts\nfrom various inputs, such as prompts, queries, or contexts.\nLLMs learn from massive text corpora that cover diverse\nPREFLOP\n(Private Cards)\nPlayer 1\nPlayer 2\nBetting\nFLOP\n(First 3 public cards)\nBetting\nTURN\n(The 4th\npublic card)\nBetting\nRIVER\n(The 5th\npublic card)\nBetting\nSHOWDOWN\n(Each player gets best rank made by 5 cards\namong 2 private cards and 5 public cards)\nFlush\nTwo Pairs\nPlayer 1 wins\nAll gained by the winner\nPot\nPlayers bet in turn\nuntil everyone has\nthe same bet\nFig. 2: An example of a 2-player Poker game.\nFig. 3: Poker card strengths.\ndomains, such as books, news, articles, web pages, and social\nmedia posts. LLMs can perform various natural language\nprocessing tasks, such as text generation, text summarization,\nquestion answering, and machine translation. The main pro-\ncess of training a LLM is data preparation, pre-training, and\nfine-tuning.\n1) Pre-training: LLMs are pre-trained with self-supervised\nlearning, which leverages the data itself as supervision. Self-\nsupervised learning can capture different levels of language\nrepresentation, such as words, sentences, or documents. Com-\nmon self-supervised learning objectives for LLMs include\nmasked language modeling, next sentence prediction, per-\nmutation language modeling, causal language modeling, etc.\nSince our work focuses on fine-tuning and RLHF, we will not\nintroduce pre-training of LLM in details.\n2) Fine-tuning: LLMs are fine-tuned with supervised or\nsemi-supervised learning, which leverages labeled or partially\nlabeled data as supervision. Supervised or semi-supervised\nlearning can adapt LLMs to different natural language process-\ning tasks. After fine-tuning, LLMs can be used for inference.\nCommon inference tasks include text summarization, question\nanswering, machine translation, text generation, etc.\n3) RLHF: A popular ML technique for LLM is RLHF, that\ntrains a “reward model” based on human feedback as a reward\nfunction to optimize an agent’s policy using reinforcement\nlearning (RL) through an optimization algorithm [24], [25].\nThis method can improve the robustness and exploration of\nRL agents, especially when the reward function is sparse or\nnoisy. It is used in tasks where it is difficult to define a clear,\nalgorithmic solution but where humans can easily judge the\nquality of the model’s output.\nIV. POKERGPT\nIn this section, we introduce our framework, PokerGPT, in\ndetails. We firstly collect a group of game data on a online\nPoker platform in real world 2. Then we transform the data\ninto prompts which can be understood by both humans and\nLLMs. To improve performance, we select prompts with high\nquality for further training. By utilizing supervised fine-tuning,\nwe make a pre-trained LLM capable of making decisions when\nreceiving staged information provided by players in a Poker\ngame. For generalizability and interactivity, we adopt RLHF\nto enhance the supervised model. Besides, since the LLM\nrequires as much information as possible while the interaction\nmay frequently happen in a short period of time, we set all\nthe information as fixed instruction for simplicity. By doing\nthis, humans can interact with our model through just simple\ndialogues. Finally, we conduct supervised fine-tuning, reward\nmodeling, and RLHF to get PokerGPT. The framework of\nPokerGPT is demonstrated in Fig. 4.\nA. Data Acquisition\n1) Data Description: For increasing the practicality of our\nmodel, we acquire logs of Poker game on PokerStars. These\nlogs contain information as follows:\n• Basic information: This encompasses the establishment\nof both the big and small blinds, the seating arrangement\ndetails for each player, along with their respective chip\ncounts and the prevailing position of the current dealer.\nThis segment furnishes a thorough comprehension of the\nentire gaming scenario.\n• Dynamic information: For each game, we partition it\ninto distinct stages. Initially, there is the “PREFLOP”\nstage, during which each player receives 2 private cards,\ninitiating the first round of betting. Subsequently, the\n“FLOP”, “TURN”, and “RIVER” stages unfold, wherein\nthe community cards are progressively revealed, accom-\npanied by the betting actions of the remaining players.\nThe conclusive stage “SHOWDOWN” unveils the pri-\nvate cards, ultimately determining the definitive winner,\nwithout any action should be done by players.\n• Summary: An overview of the game pool size, the\ncommunity cards, and each player’s earnings is recorded.\nIt includes details on the winnings and losses associ-\nated with specific card combinations, instances of non-\nparticipation, and other relevant information.\nFig.5 shows an example of the game log. The chart reveals\nthat the blinds for this game are set as $0.02\/$0.05, with\n2PokerStars: https:\/\/www.pokerstars.com\nI. Data Acquisition\nCollect game data\nFilter useful information\nIf \"SHOWDOWN\"\nRounds played\nDiscard\nDiscard\nII. Prompt Engineering\nConstruct prompts\nIII. Training Process\nOnline platform: PokerStars\nGame log\n1. Small\/Big blinds.\n2. Seat arrangement.\n3. Dealer sear.\n4. Balance.\n......\n1. Stage.\n2. Public cards.\n3. Private cards.\n4. Actions.\n......\n1. Pot value.\n2. Winner info.\n3. Loser info.\n......\nBasic info\nDynamic info\nSummary\nNo\nYes\nUnder\nthreshold\nUseful\ninformation\nAbove\nthreshold\nCore info\nAdditional info\n1. Observable cards.\n2. Card characteristics.\n3. Own rank.\n4. Action sequence.\n1. Blind amount.\n2. Win rate.\n3. Player number.\n4. Pot value.\n......\nCalculate players' win rates\nPrompt selection\nSort and select\n(mbb\/h)\n150\n100\n60\n20\n-40\n...\nConstant\ninformation\nDynamic\ninformation\nEngineered\nPrompts\nOrganize\n1. Zero-shot.\n2. Role play.\n3. Discretization.\nPrompt\nengineering\ntechniques\nLow-quality\ndataset\nRoundly\nupdates\nPrompt: X\nAction: y\nHigh-quality\ndataset\nLow-level\nplayers\nHigh-level\nplayers\nTop N\nBottom M\nPre-trained LLM\nFacebook OPT-1.3b\nReward\nmodel\nSupervised\nfine-tuning\nContrastive\nfine-tuning\nPPO\nSupervised\nmodel\nRLHF\nPokerGPT\n1\n2\n7\n3\n4\n5\n6\nFig. 4: The framework of PokerGPT, consisting of data acquisition, prompt engineering, and training.\nthe dealer positioned at seat number 1. The chip count for\neach player is specified. For instance, the player in seat 1,\nphalves77, holds 5.12 in chips.\nDetails on players’ decisions and actions are as follows:\nDuring the “PREFLOP” stage, phalves77, seated in the\ndealer’s position, opted to “raise” to 0.15, while gefahren-\nsucher spent another 0.10 to “call” from the big blind. The\ncommunity cards unfolded as follows: the “FLOP” displayed\n[‘5s’, ‘Th’, ‘5c’], followed by the “TURN” card [‘2s’],\nand finally the “RIVER” card [‘Kh’]. After each round of\npublic cards, the recorded information includes the amount\nof each player’s bet, and their action choices. For instance,\nafter the “FLOP”, gefahrensucher give the “bet” of 0.16,\nwith phalves77 deciding to “call”. After the “TURN” and\n“RIVER”, gefahrensucher placed the “bet” of 0.31 and 1.21,\nrespectively, with phalves77 choosing “call” each time. During\nthe “SHOWDOWN”, gefahrensucher revealed a pair of ‘5’s\n(Fives), while phalves77 presented 2 pairs: ‘10’s (Tens) and\nFives. Consequently, phalves77 claimed the pot’s bottom as\nthe winner, gaining victory with the superior 2 pairs.\n2) Information Filtering:\nThe essential information of\nPoker games revolves around the public cards and the be-\nhaviors of the players in each game. The public cards stand\nas the pivotal information, forming an integral component of\na player’s ultimate hand. In Poker games, the unveiling of\npublic cards directly influences the rank of each player’s hand.\nThe actions of opponents during each round (“bet”, “raise”,\n“check”, “call”, “fold”) serve as vital indicators to infer their\ngefahrensucher: shows [3h 6h] (a pair of\nFives)                                                       \nphalves77: shows [Tc 4c] (two pair, Tens and\nFives)                                                     \nphalves77 collected $3.53 from pot\nSHOWDOWN\nTotal pot $3.68 | Rake $0.15\nBoard [5s Th 5c 2s Kh]\nSeat 1: phalves77 (button) showed [Tc 4c]\nand won ($3.53) with two pair, Tens and Fives\nSeat 2: Omar Hrn (small blind) folded before\nFlop\nSeat 3: gefahrensucher (big blind) showed[3h\n6h] and lost with a pair of Fives\nSeat 5: victorismyname folded before\nFlop(didn't bet)\nSeat 6: 0bjective folded before Flop(didn't bet)\ns : spade ; h : heart\nc : club ; d : diamond\nPokerStars Hand#xxxxxx ($0.02\/0.05 )\n6-max Seat    #1 is the button\nSeat 1: phalves77 ($5.12 in chips)\nSeat 2: Omar Hrn ($5.02 in chips)\nSeat 3: gefahrensucher ($6.64 in chips)\nSeat 5: victorismyname ($4.99 in chips)\nSeat 6: 0bjective ($2.14 in chips)\nOmar Hrn: posts small blind $0.02\ngefahrensucher: posts big blind $0.05\nGOBSTPPER: sits out\nBasic Information\nvictorismyname: folds  \n0bjective: folds\nphalves77: raises $0.10 to\n $0.15\nOmar Hrn: folds\ngefahrensucher: calls $0.10\nPREFLOP\nFlop : [5s Th 5c]\ngefahrensucher : bets\n   $0.16\nphalves77: calls $0.16\nFLOP\nTurn : [5s Th 5c] [2s]\ngefahrensucher : bets\n  $0.31\nphalves77: calls $0.31\nTURN\nRiver:\n[5s Th 5c 2s] [Kh]\ngefahrensucher : bets\n  $1.21\nphalves77: calls $1.21\nRIVER\nSUMMARY\nFig. 5: An example of Poker game log.\nstrategies and intentions, directly providing insights into the\nstrength of their cards. Moreover, the magnitude of money\nplaced exhibits a player’s confidence and card strength in a\nlarge extent. Also, the frequency of “raise” can enhance the\nunderstanding of opponent’s playing style and card strength,\nenabling strategic decisions on when to bluff and when to\ndiscern a bluff.\nCore information of utmost significance is crucial to be\nincluded in the prompt, listed as follows:\n• Observable cards: It is necessary to know the 2 private\ncards and public cards exhibited.\n• The characteristics of cards: We describe the charac-\nteristics of private cards in several ways. “Suit” means\n2 private cards with the same color. “High” means\npossessing one card higher than ‘9’. “Close” means the\ndifference of 2 cards is less than 5, which can form a\n“straight”.\n• The rank of the hand: Since it may be difficult for\nthe LLM to learn to recognize the rank of the hand,\nwe simply provide the rank in the prompt. The rank\ncan guide LLMs to understand the strength of different\ncombinations of private cards and public cards.\n• Action sequence: Analyzing players’ behavioral patterns\nplays a pivotal role in understanding the psychology and\nstrategy of opponents. Thus, it is crucial to record each\nplayer’s sequential actions.\nAdditional information also assists the learning and un-\nderstanding of our model, including:\n• Amounts of blinds: Small and big blinds is important for\nevaluating both the revenue and the risk of Poker games,\nthereby contributing to the understanding of the decisions\non “raise” or “fold”.\n• Positions of players: The positional context of a player\nrelative to the dealer, coupled with the player’s actions,\nserves as an indicator of the hand’s strength, which\ninfluences the players’ advantages and the game results.\n• Number of players: The variation in the number of play-\ners within a game intensely changes the game structure.\nIt is essential to make our model aware of and respond\nto the variation.\n• Balance: We record each player’s balance through the\ninitial amount and the amount each has invested in the\npot.\n• Discard: The “fold” chosen by players helps our model\nto determine the remaining players.\n• Pot value: The total amount of money put into the pot\nhelps LLM understand the concept of odds.\n• Stage: The same action in different stages generally has\ndifferent purposes.\nThe incorporation of the above information ensures an\neffective training process for our model, improving language\ngeneration and comprehension capabilities.\nB. Prompt Engineering\nOne of the advantages of LLMs over traditional DL models\nis that it has good interactivity, which can be achieved by\nprocessing verbose raw data into prompts and further used for\nfine-tuning. A prompt is a corpus that is understandable by\nboth humans and LLMs, which is easy to be produced, such\nas colloquial phrases, sentences, and paragraphs. Therefore,\nwe can quickly get suggestions on playing Poker games by\ninputting simple words with part of important information of\na Poker game into LLMs. We will introduce how we make\nraw data into prompts, and filter out high-quality prompts to\nensure that our model is able to give high-revenue or low-loss\ninsights in real-time Poker games.\n1) Prompt Selection: Based on the game log presented in\nFig. 5, we can observe that the information contained in each\ngame is diversified, and it is difficult for a pre-trained LLM to\ndistinguish which information to focus on. Thus, we construct\nprompts with rich information and then select high-quality\nones for training.\nFirstly, we conduct searching for all logs to filter out\n“SHOWDOWN” stages that with private cards publicated.\nCases without a specific winning or losing hand were sub-\nsequently excluded, because the lack of hand strength infor-\nmation makes it difficult for LLMs to learn optimal strategies\nin an observable situation.\nThen, to get high-quality dataset, we adopt win rate for\nselection of high-level players. Win rate is a standard metric in\nthe Poker AI community. We firstly counted the total number\nof games played by each player in the obtained logs. Next, we\ncalculate overall revenue and converting it to a multiple of big\nblinds. After that, we count each player’s win rate in milli-big-\nblinds per hand (mbb\/h) as a sorting criterion for descending\nrankings. To mitigate bias resulting from a limited number of\ngames, we exclude players whose number of games is under a\nthreshold. Then, we select a group of top players as our high-\nquality dataset, in which the winning games have significant\ninstructive impacts on our model. Note that, it is fine for our\nmodel to learn player actions in lost games. Intuitively, the way\nof high-level player losing games also has reference value. To\nsome extent, it can help the model learn how to lose less.\nOn the other hand, we also collect data of the worst players\nas negative examples. Through reward modeling which will\nbe mentioned in Section. IV-C, the actions and ideas of the\nlow-level players can be learned and avoided by our model.\n2) Prompt Construction: We choose zero-shot learning for\nprompt construction, which simply feeds the text into the\nmodel and ask for results. For practicality, we discard few-\nshot prompting. The purpose of presenting few-shot exam-\nples in the prompt is to describe the task instruction to\nthe model in the form of demonstrations. Though adding\nquestion-answering (QA) samples to prompts may greatly\nimprove the response quality of LLMs [26], in real Poker\ngames, the player’s decision-making time is generally about 15\nseconds, causing that players are unlikely to be able to input\na large amount of QA samples or game information. Thus,\nfor enhancement, we just add fixed instructions in prompts to\nexplain our intent to the model. During interaction, prompts\nare systematically constructed mainly using the information in\nred in in Fig. 5. Besides, few-shot learning can be expensive\nin terms of token usage and violate the length limitation,\nwhile rich-information instruction improves the model to be\nmore aligned with human intention and greatly reduces the\ncost of communication. Thus, directly giving instructions is a\npromising approach. On the other hand, due to the fine-tuning\nwe conduct, explained in Section IV-C, few-shot learning is\nnot necessary.\nIn terms of reducing tokens inputted, we roughly divide\nuseful information into constant information which is fixed at\nthe beginning of one game, and dynamic information updated\nin each decision round.\nFurthermore,\nmultiple\nprompt\nengineering\ntechniques\nadopted are outlined as follows, which are proved to be\neffective in LLM training [27]:\n• Role play: We give a role to our model and add cor-\nresponding text into our prompt, such as “You are a\nprofessional gambler”.\n• Value discretization: To reduce the learning difficulty,\nwe discretize the money needed to be determined by our\nmodel through rounding up the values of money paid\nin each stage to an integer multiple of the big blind,\ntransforming the regression task into a classification task.\nAn textual instance of our prompt is shown in Fig. 6. It\nis worth noting that in real games, sometimes opponents may\ndirectly show 1 or 2 privates cards consider from some psy-\nchological warfare perspectives. In that case, we can replace\nthe “**” with the displayed cards. An utilization example of\nthe engineered prompt can be found in Section V, shown as\nFig. 12.\nThe information received by our model encompasses all\ncore information and additional information mentioned in\nSection IV-A2. This array of data enables our model to\ncomprehensively understand the prevailing situation.\nC. Training Process\nWe utilize the prepared dataset to train our model using\nDeepSpeed-Chat [28]. DeepSpeed-Chat is a system framework\nthat enables an end-to-end training experience for ChatGPT-\nlike models. The main steps consist of: (1) Supervised fine-\ntune a pre-trained LLM on small datasets to provide specific\nprior knowledge to this model; (2) Train a reward model\nPrompt Instance\nYou are an experienced gambler. Now you need to assist\nme to make decisions in Texas Hold’em games. You have\nbeen provided with a series of observable information:\nPlayer amount: [6], Currency: USD, Blind value: [0.02\/0.05],\nOrder: [‘2’, ‘3’, ‘5’, ‘6’, ‘7’, ‘9’], Seat 2 is small blind.\nMy cards: [‘Th’, ‘Ah’], the characteristics of my cards: [“suit”,\n“high”, “close”], My seat: [Seat 2]\nStage: “PREFLOP”, Public cards: [‘**’ ‘**’ ‘**’ ‘**’ ‘**’]\nMy rank: [“High”], Money: [3.92], Action: []\nSeat 3: [‘**’, ‘**’], Money: [2.33], Action: [], Discard: [False]\nSeat 5: [‘**’, ‘**’], Money: [5.54], Action: [], Discard: [False]\nSeat 6: [‘**’, ‘**’], Money: [3.75], Action: [], Discard: [False]\nSeat 7: [‘**’, ‘**’], Money: [4.22], Action: [], Discard: [False]\nSeat 9: [‘**’, ‘**’], Money: [1.47], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.17]\nThe actions can be: [“fold”, “raise”, “call”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.92}.\nFig. 6: An instance of our prompt.\nusing data separated by quality. (3) Apply RLHF to make\nthe fine-tuned LLM in step (1) able to understand various\nhuman text assisted by the reward model in step (2). The pre-\ntrained model we choose is the OPT-1.3B from Facebook,\nwhich performs similarly to GPT-3 [8], making the goal of\ntraining domain-specific expert-level AI assistant possible. The\nreason for choosing this model is its small size and good\ncomprehension in human dialogues.\nSupervised fine-tuning is a necessary approach because re-\ntraining a LLM which has been already pre-trained consumes\na substantial amount of computational resources. On the other\nhand, involving domain-specific expert model training neces-\nsitates not large-quantity but high-quality data that contains\ncomprehensive details of the designated task. Particularly in\nthe supervised fine-tuning phase, a modest number ranging\nfrom tens to hundreds of high-quality data proves sufficient\nfor achieving satisfactory results in model refinement [15].\nThus, it is promising to fine-tune OPT-1.3B on our collected\ndataset.\nThe second step is to train a reward model, which is crucial\nfor the RLHF process. We need to train a model to predict\nthe reward signal, which is used to guide the fine-tuning of\nthe final dialogue model. For convenience, we directly use the\nwin rates calculated before as the labels of our reward model.\nThen, we input prompt-action pairs into the reward model to\ngenerate a score indicating which action is better. The model\nis also trained using supervised learning.\nFinally, RLHF fine-tuning is applied to train the supervised\nfine-tuned model based on the reward model in the second\nstep. Specifically, fine-tuning approach used here is proximal\npolicy optimization (PPO) [29], a popular and strong RL\nalgorithm, whose reward is generated by the reward model.\nFor updating, RLHF involves generating new answers with the\ncurrent model, ranking these answers using the reward model,\nand then updating the model’s parameters to favor answers\nwith higher reward. This iterative process improves our model\nfrom the perspective of robustness to various human-like\ninput, and generalizability to extensive game scale and varying\nsituations. In the end, we obtain the PokerGPT for producing\nhuman-like and engaging answers to instruct Poker games.\nV. EXPERIMENT\nA. Fundamental Setup\nWe firstly prepare our dataset based on data of over 1 million\ngames. After processing mentioned in Section IV, we get high\nquality data of about 120 thousand games. Then, we divide\n90% of them into training set and the remaining 10% into test\nset. We select Microsoft’s DeepSpeed-chat framework [28]\nfor its comprehensive functionalities. This framework also\nefficiently manages the distribution of model parameters across\nindividual GPUs during training. We fine-tune an open-source\nLLM 1.3 billion parameters on HuggingFace, named Face-\nbook’s OPT-1.3B model, known for its comparable perfor-\nmance to GPT-3 [8]. We run our experiments on a single\nNVIDIA GEFORCE 3090. The hyperparameter we use can be\nrefered to DeepSpeed-Chat [28], while we change mini batch\nsize to 4 due to the limitation of our computational device.\nB. Experimental Setup\n1) Performance Comparison: For performance comparison,\nwe make PokerGPT play against Slumbot [30] for 10000\nhands, and compare its results with 2 latest models [13], [14].\n2) Ablation Studies: We aim to study if useful information\nfiltering and prompt engineering can improve the performance\nof PokerGPT. Thus, we input raw game data and filtered\ndata into LLMs for fine-tuning, and compare the win rate.\nFurthermore, as mentioned in Section IV, we use win rate\nfor evaluating the level of players included in our dataset. To\nexplore if levels of players have impact on the fine-tuning\nresults, we sample data from 50000 games based on different\nwin rate thresholds to construct our training set for fine-tuning.\nAll datasets for fine-tuning are demonstrated in Table I. As for\ntest set, we use Slumbot for win rate and randomly sample\n1000 games of winners among the complete dataset for other\nmetrics introduced as follows.\nTABLE I: Datasets under different setup for fine-tuning.\nDataset\nTechnique\nWin Rate Threshold\nAmount\nI\nRaw\n-\n1 million\nII\nInfo filtering\n-\n1 million\nIII\nWin rate sorting\n> 1500 mbb\/h\n50000\nIV\nWin rate sorting\n600 ∼1200 mbb\/h\n50000\nV\nWin rate sorting\n0 ∼500 mbb\/h\n50000\nVI\nWin rate sorting\n< 0 mbb\/h\n50000\nTo evaluate the performance of supervised fine-tuning on\ndatasets of different quality, we regard determining actions,\nincluding “bet”, “fold”, “raise”, and “check”, as a multi-label\nclassification task. If our model gives a action different from\nthe real action in test set, we get a “False”. We take macro F1\nas the metric for this task [31]. The macro F1 score is a type\nof F1 score that gives equal weight to each class in multi-label\nclassification, regardless of the class distribution. Assume we\nhave N classes, the macro F1 can be denoted as:\nmacro-F1 = 1\nN\nN\nX\ni=1\nF1i,\n(1)\nF1i = 2 × precision × recall\nprecision + recall\n,\n(2)\nwhere F1i is the F1 score of the ith class. Since we have 5\nactions for Poker games, N equals to 5 here. Note that, for\n50000 games, each action which is regarded as classification\nlabel may have different amount of labels from 50000.\nOn the other hand, we study on value differences of value-\nrelated actions from all correct predictions, such as that our\nmodel says “bet $1.5”, while the real action in test set is\n“bet $2.0”. Since there exist multiple currencies and big\nblind values in our data, we unify currency to US dollar to\nrepresent all money using big blind. As mentioned before,\nwe discretize values of “bet” and “raise” into multiple of big\nblind. Specifically, here we adopt {0, 1, 3, 6, 10, 20, 50, 100,\nall-in} as the multiples of big blind for decision, while the\nvalues exceeding user’s owned money will be excluded. As we\nregard value prediction as a multi-label classification task, we\ncompare the frequency of each multiple chosen to evaluate the\naccuracy of predicted values, as well as using mean squared\nerror (MSE) for a general evaluation.\nBesides, perplexity in Natural Language Processing (NLP)\nis a measure of how well a probability model predicts a\nsample. Assume we have a sequence W with N words, a\nperplexity is formulated as:\nperplexity(W) = P(w1, w2, ..., wN)−1\nN ,\n(3)\nwhere W = (w1, w2, ..., wN) is the sequence of words, and P\nis the probability. It quantifies the uncertainty of a model by\ncalculating the inverse probability of the test set, normalized\nby the number of words. In the context of language models,\na lower perplexity generally indicates the model is better at\npredicting the next word in a sentence.\n3) Multi-Player Evaluation:\nWe evaluate PokerGPT in\nmulti-player games using an open-source Poker simulation 3.\nFor setup consistency, all bots in multi-player experiments\nare based on a mandatory method that calculates equity.\nTo study the strategy variation of PokerGPT during player\namount changes, we design 2 metrics, action score and average\ninvestment, to represent the occurring frequency of each action\nand the money used in each game, relatively:\n• Action score: Since some actions can occur multiple\ntimes in one game, such as “check”, “raise”, “bet”, and\n“call”, while the others can only occur once, such as\n“all-in” and “fold”, the length of one game should be\nconsidered. Thus, as a game can have up to 4 stages,\nwe calculate action score for each action by dividing the\nnumber of times this action occurs in a game by the stage\n3https:\/\/github.com\/dickreuter\/neuron Poker\nTABLE II: Performance comparison against the latest 2 models.\nModel\nWin Rate (mbb\/h)\nTraining Time (h)\nResponse Speed (s)\nPlayer Amount\nParameter Amount\nMain Tech\nYear\nReBel [13]\n45±5\n-\n-\n2\n-\nDRL+CFR\n2020\nAlphaHoldem [14]\n111±16\n580\n0.017\n2\n8.6 million\nDRL\n2022\nPokerGPT\n158±49\n9.5\n5.4\n2 or more\n1.3 billion\nLLM\n2023\nnumber of this game. For example, if a player do “check”\nfor 3 times and “fold” in “TURN” stage, the action score\nfor “check” is 1, and the action score for “fold” is 0.33.\nFinally, we divide action scores by the amount of games,\nto make action scores ranged from 0 to 1.\n• Average investment: In Poker games, more money usage\nmeans both higher revenue and higher loss. By observing\nthe amount of average investment, we can know the style\nof playing more clearly. To get the average investment,\nwe calculate the mean of total money used by our model\nin each game.\nBy analyzing action scores and average investment, we can get\nmore detailed insights of our model’s strategies for different\nplayer amount.\n4) Analysis on Interaction: We show a case of PokerGPT\ninteraction with multiple players in processed dataset. The aim\nis to evaluate the general capability of our model through\nanalyzing the answers of different questions.\nC. Comparison with Previous Works\nFirstly, we compare our performance against Slumbot with\n2 novel works, recording multiple metrics in Table II. We can\nsee that our model makes progress in win rate. Compared to\nRebel (45+5) and Alphaholdem (111+16) against Slumbot, our\nmodel comes out on top with a win rate of 158+49, measured\nin mbb\/h, showing the superiority of our model. However,\nsince the standard deviation of our model is higher than the\nother 2 models, fluctuation of our performance is more violent,\nsuggesting the larger variation of the LLM’s answers led\nby the diversified nature of generative models, which should\nbe further optimized. Furthermore, our model completes the\ntraining in just 9.5 GPU hours (3.5 for supervised fine-tuning,\n1.5 for reward model training, and 4.5 for RLHF), compared to\n580 GPU hours for Alphaholdem, showing the improvement\non training efficiency. Thanks to the adaptability of LLMs,\nempowering LLMs with specific prior knowledge through fine-\ntuning on a relatively small scale of data is effective. Last but\nnot least, our model is capable of providing action instructions\nfor arbitrary number of players, while the other 2 models\nare limited to playing one-on-one situations. Even without\ntransferring that is needed for the other 2 models, LLM-based\nmodel can be directly generalized to various situations, saving\na lot of computational resources. On the other hand, though\nPokerGPT owns a much larger amount of parameters and a\nslower response speed than Alphaholdem, both metrics are\nacceptable in real games, and much better than previous CFR-\nbased works. Additionally, we illustrate the staged revenue\ndistributions of high-level players in real games and PokerGPT\nagainst Slumbot in Fig. 7. Compared with human players,\nPokerGPT tends to keep a more stable win rate by losing less\nin later stages, while humans are more likely to take a risk in\ngambling in later stages, since the later the stage is, both the\nrevenue and the risk can be higher.\nPREFLOP\nFLOP\nTURN\nRIVER\nStage\n0\n20\n40\n60\n80\nRevenue (bb)\nWin_Real\nLose_Real\nWin_LLM\nLose_LLM\nFig. 7: The distribution of revenue among 1000 games with\nunit of big blind. The real results are counted from the\ncollected dataset, while the results of PokerGPT are from the\ngames against Slumbot.\nD. Ablation Results\nTABLE III: Ablation on different dataset setup.\nDataset\nWin Rate (mbb\/h)\nMacro F1\nMSE\nPerplexity\nI\n-376±215\n31.34\n115.41\n4.25\nII\n229±137\n62.77\n36.77\n2.71\nIII\n1221±54\n77.63\n0.21\n1.03\nIV\n1034±52\n72.22\n0.95\n1.04\nV\n608±59\n68.15\n8.28\n1.05\nVI\n-574±76\n59.69\n177.26\n1.09\nIn ablation experiments, we firstly evaluate the effectiveness\nof each technique used in our model. As shown in Table III,\nby only fine-tuning raw data, Dataset I shows the worst\nmacro F1 score and perplexity, because it is difficult for the\nLLM to learn effectively from a bunch of unformatted data,\nwhose information is in chaos. Thus, the LLM can not even\nmake actions valid for Poker games. Dataset II shows much\nbetter performance than Dataset I, indicating the importance\nof information filtering applied in our model. After selecting\nplayers with different win rate for fine-tuning, we can see\nthere is a trend that the holistic performance of our model\ncheck call fold bet raise\ncheck\ncall\nfold\nbet\nraise\nTrue Labels in Real Data\n1335 34\n189\n15\n3\n25 1776 320\n3\n6\n46\n56\n437\n18\n1\n12\n2\n34\n269\n12\n22\n23\n25\n1\n112\nDataset III\ncheck call fold bet raise\ncheck\ncall\nfold\nbet\nraise\n1307 47\n201\n19\n2\n32 1731 356\n5\n6\n58\n88\n376\n31\n5\n19\n3\n64\n229\n14\n23\n26\n30\n0\n104\nDataset IV\n0\n500\n1000\n1500\ncheck call fold bet raise\nLLM4MPNLTH Output\ncheck\ncall\nfold\nbet\nraise\nTrue Labels in Real Data\n1287 44\n215\n24\n6\n29 1667 391\n26\n17\n59\n97\n344\n41\n17\n20\n4\n71\n218\n16\n26\n22\n32\n1\n102\nDataset V\ncheck call fold bet raise\nLLM4MPNLTH Output\ncheck\ncall\nfold\nbet\nraise\n1212 38\n223\n76\n27\n31 1441 598\n16\n44\n66\n129 305\n21\n37\n51\n4\n82\n143\n49\n9\n15\n24\n1\n134\nDataset VI\n0\n500\n1000\n1500\nFig. 8: Confusion matrix of test results outputted by Pok-\nerGPT fine-tuned using Dataset III to VI.\nis positively related to win rate, where Dataset III has the\nbest performance while Dataset VI is the worst. According to\nmacro F1 score and MSE, fine-tuning on worse dataset leads\nto not only more incorrect action choices, but also irrational\ninvestment. Interestingly, Dataset VI has a similar perplexity\nto Dataset III to V while performing much worse, indicating it\nis firm about its bad action choice. This result emphasizes the\nimportance of data quality. In conclusion, all the techniques\nadopted in this work show effectiveness in different extent, in\nwhich prompt selection based on players’ win rate presents\nthe strongest effect.\nTo check the training bias, we show the confusion matrix\nof classification results of Dataset III to VI in Fig. 8. The test\nset contains 1000 games with 1576 “check”s, 2130 “call”s,\n558 “fold”s, 329 “bet”s, and 183 “raise”s. We can see that the\nmodel of Dataset III has better prediction on “check”, “call”,\n“fold”, and “bet” than the model of Dataset VI does, indicating\nthat our model can learn strategies more similar to winners’\noperations from data of high-level players. On the other hand,\nthe reason for the higher accuracy of “raise” of Dataset VI\ncan be that the players losing a lot have a stronger “raise”\ninclination which influence the action style of our model.\nBesides, an interesting appearance is that both the 2 models\ntend to give more “fold” than other actions. This could be\nattributed to that the LLM has the basic concept of game\ngoal the value judgment of Poker, that is, if it is confusing\nto choose an action, the model will choose “fold” in order to\nreduce investment. Additionally, compared to Dataset III, the\noutputs of Dataset VI shows higher ratio of choosing “raise”,\nleading to worse macro F1 score, reaffirming the importance\nof data quality for LLM fine-tuning.\nMoreover, we record the investment distribution of the truth\nand predicted investment from models fine-tuned on Dataset\nTruth\nIII\nIV\nV\nVI\nDataset\n0\n50\n100\nInvestment value (bb)\nMean\nFig. 9: The distribution of investment values among 1000\ngames, whose unit is big blind. The 2 dashed lines are the\nmean with standard deviation.\nIII to VI, shown in Fig. 9. In this figure, the mean of investment\nincreases and the distribution diversifies with the data quality\ndegrades. Generally, winners intend to invest less than 10 bb\nfor an easy win, while players who win less or lose more\nincline to invest more money. The results imply that the\nstrategies adopted by high-level players are more conservative,\nwhich is similar to winners in the truth. On the other hand,\nthe similarity is, the investment inclination under all cases is\nto focus on relatively small values, and invest higher values\nwith lower frequency, indicating that our model has learned\nthe common sense of investment risk in games. In terms of the\ndistribution of Dataset VI, the mean of the model’s investment\neven reaches about 10, undertaking much higher risk.\nPREFLOP\nFLOP\nTURN\nRIVER\nStage\n0\n200\n400\n600\n800\n1000\nAmount of Actions\ncheck_III\ncheck_VI\ncall_III\ncall_VI\nfold_III\nfold_VI\nbet_III\nbet_VI\nraise_III\nraise_VI\nFig. 10: The predicted actions of 1000 games from LLMs\nfine-tuned on Dataset III and Dataset VI. The dashed bars\nrepresent the real actions.\nAdditionally, we record staged actions of our model fine-\ntuned on Dataset III and Dataset VI. As shown in 10, the\nresults of PokerGPT on Dataset III always be more likely to\n“fold” in all stages, indicating the model’s style is conserva-\ntive. In contrast, PokerGPT on Dataset VI prone to “raise” and\n“bet”, and rarely “fold”, showing its aggressiveness. These 2\nphenomenons can explain that players with higher win rates\nare usually more cautious that those who lose more. Both\nmodels tend to “call” in later stages, implying that “call” is a\nmore general choice in some extent.\nAs a conclusion, the above ablation experiments validate\nthe effectiveness of our approaches.\nE. Multi-Player Evaluation\nTo evaluate the performance of PokerGPT when dealing\nwith multiple players, we make it play 1000 games for each\nplayer amount. The Fig. 11(a) presents the varying tendency of\n2 key metrics of our model when players’ number increases.\nThe trend of win rate changes more significantly during player\namount 2 to 5, then shows a slower declination during player\namount from 6 to 9, and finally fluctuations with a small\namplitude appears during player amount 10 to 15. When\nplaying 2-player games, our model gets a higher win rate\nagainst a rule-based bot which considers equity, indicating the\ncapability of handling the easiest Poker games. Since situations\nbecome more complex and the win rate declines when the\nplayer amount rises, our model gains less but still positive\nrevenue when competing with more opponents, showing the\nadvantages of our model in multi-player games. Meanwhile,\nthe gradual reduction of the shaded part implies that the\nfluctuation of the win rate decreases.\n5\n10\n15\nPlayer Amount\n250\n500\n750\nWin Rate (mbb\/h)\n5.5\n6.0\n6.5\nResponse Speed (s)\n(a)\n5\n10\n15\nPlayer Amount\n0.0\n0.2\n0.4\n0.6\n0.8\nAction Score\nCall\nCheck\nFold\nBet\nRaise\nAll-in\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAverage Invest (bb)\n(b)\nFig. 11: (a) The trends of win rate and response speed when\nplayer amount changes. The dashed lines are the standard de-\nviations. (b) Strategy variation during player amount changes.\nMoreover, the response time rises about linearly, while\nthe win rate declines drastically at first and gradually goes\nmoderate. This phenomenon validates the good scalability of\nour work. Different from CFR, LLMs intend not to fully\nexplore all possible scenarios. Instead, LLMs directly look\nfor connection between inputs and outputs. As the number\nof players increases, the amount of input information shows\nroughly linear growth, and the number of possible situations\ninvolved in each round of the game consequently goes up. As a\nresult, the linear time increase shows the efficiency of end-to-\nend learning on LLMs. In conclusion, all these phenomenons\ndemonstrate the advantages of our model in Poker games, and\nextensive potential in IIGs.\nFurthermore, to study the variation of PokerGPT’s strategy\nwhen facing multiple players, we calculate the action score for\neach action and the average investment, shown in Fig. 11(b).\nAccording to action scores, we can observe each action and\nget insights separately:\n• “Call”: We can see that the score of the “call” strategy\nincreases with the player amount, suggesting that the\nPokerGPT prefers to be more conservative when facing\nmore opponents.\n• “Check”: The score of “check” increases till 6-player\ngames, and then deceases into a range. Since “check”\nis not only a compromising behavior but also a bait for\naggressive opponents, players can choose “check” in any\nsituation, especially in games with small player amount.\nHowever, when the scale of games becomes larger than 10\nplayers, bait tactics become less effective. Thus, “check”\ncan usually be regarded as a compromise, and chosen in\na relatively stable frequency.\n• “Fold”: As “fold” score increases a lot when player\namount becomes larger than 6, 2 explanations can be\ndrew. When there are more players, it is complex to\nforecast the rank strength among dozens of hands, leading\nto a high rate of “fold”. Also, “fold” score can be higher\nwith the game length shortening, since it is reasonable\nfor bots to “fold” if possibility of stronger rank existing\nincreases, leading to a quick end of one game.\n• “Bet” and “Raise”: The scores of “bet” and “raise”\ndecrease with the number of players. This implies that the\nmodel intends to avoid risk when facing more players.\n• “All-in”: Actually, “all-in” is a special case of “bet” or\n“raise”. The score of the “all-in” is very low and even\ntends to 0 with more opponents. This reflects that the\nmodel rarely chooses this action when player amount\nincreases, as the risk of “all-in” is higher.\nIn summary, actions suggested by our model tend to be more\ncautious and conservative as the number of players increases,\nwhich are generally consistent with real games.\nF. Analysis of Interaction\nIn Fig. 12, we show the interaction with PokerGPT during\na Poker game. In Q1, we ask PokerGPT to give a direct\naction recommendation and get “call” as feedback. Then, in\nQ2, when we request it to be more aggressive, it suggests\nto make a “raise” to 0.8. It shows that the human language\ncomprehension of LLMs help PokerGPT adjust the strategy\naccording to the user’s request. In “FLOP” stage with 3 public\ncards exhibited, PokerGPT successively gives different an-\nswers to Q3 and Q4, from “check” to “all-in”, suggesting that\nPokerGPT has the understanding of card strengths. Besides, in\nQ6, we ask about Seat 9’s rank, with the answer that Seat 9\npossibly has a pair or a flush since this player likes to “raise”\nand “bet”, suggesting that PokerGPT has the awareness of\nan opponent’s strength based on his or her actions. However,\nfrom the answer of Q7 and Q9, PokerGPT shows its shortage\non guessing cards, probably due to the large quantities of\ncard combinations. Similarly, in Q9, PokerGPT has difficulty\nof giving a winning probability, since we do not use any\nprobability as a part of the prompt, while LLMs are not\nproficient in math, neither.\nVI. CONCLUSION AND FUTURE WORK\nInspired by the prosperity and prominence of LLMs, our\nstudy explores the applicability of LLMs for playing Poker.\nBy conducting prompt engineering on data of real games and\nfine-tuning a lightweight pre-trained LLM based on RLHF, we\nPokerGPT\nSystem (constant information): You are an experienced\ngambler. Now you need to assist me to make decisions\nin Texas Hold’em games. You have been provided with\na series of observable information:\nPlayer amount: [6], Currency: USD, Blind value: [0.02\/0.05],\nOrder: [‘2’, ‘3’, ‘5’, ‘6’, ‘7’, ‘9’], Seat 2 is small blind.\nMy cards: [‘Th’, ‘Ah’], the characteristics of my cards: [“suit”,\n“high”, “close”], My seat: [Seat 2]\nDynamic information:\nStage: “PREFLOP”, Public cards: [‘**’ ‘**’ ‘**’ ‘**’ ‘**’]\nMy rank: [“High”], Money: [3.92], Action: []\nSeat 3: [‘**’, ‘**’], Money: [2.33], Action: [], Discard: [False]\nSeat 5: [‘**’, ‘**’], Money: [5.54], Action: [“fold”], Discard:\n[True]\nSeat 6: [‘**’, ‘**’], Money: [3.75], Action: [“fold”], Discard:\n[True]\nSeat 7: [‘**’, ‘**’], Money: [4.22], Action: [“fold”], Discard:\n[True]\nSeat 9: [‘**’, ‘**’], Money: [1.47], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.17]\nThe actions can be: [“fold”, “raise”, “call”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.92}.\nUser:\nQ1:What should I do?\nPokerGPT: You should call.\nQ2: Please be aggressive.\nPokerGPT: You should raise to 0.5.\nInformation updates:\nStage: “FLOP”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘**’ ‘**’]\nMy rank: [“Flush”], Money: [3.82], Action: [“call”]\nSeat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”], Discard:\n[False]\n......\nSeat 9: [‘**’, ‘**’], Money: [1.37], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.3]\nThe actions can be: [“fold”, “check”, “bet”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.82}.\nUser:\nQ3:What should I do?\nPokerGPT: You should check.\nQ4: Should I all-in?\nPokerGPT: As you have a flush, all-in is fine.\nInformation updates:\nStage: “FLOP”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘**’ ‘**’]\nMy rank: [“Flush”], Money: [3.82], Action: [“call”, “check”]\nSeat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”, “check”],\nDiscard: [False]\nSeat 9: [‘**’, ‘**’], Money: [1.15], Action: [“raises 0.05 to\n0.1”, “bets 0.22”], Discard: [False] The pot value is [0.52]\n......\nThe actions can be: [“fold”, “call”, “raise”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.82}.\nUser:\nQ5:What should I do?\nPokerGPT: You should raise to 0.5.\nQ6: Please guess Seat 9’s rank.\nPokerGPT: Seat 9 likes to raise and bet. Therefore, he might\nbe three of a kind or flush.\nQ7: Please guess Seat 9’s cards.\nPokerGPT: Seat 9 likes to raise and bet. Therefore, he might\nbe there of a kind or flush. But there is no evidence what cards\nSeat 9 has.\nInformation updates:\nStage: “TURN”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘Ks’ ‘**’]\nMy rank: [“Flush”], Money: [3.6], Action: [“call”, “check”,\n“call”] Seat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”,\n“check”, “fold”], Discard: [True]\n......\nThe actions can be: [“fold”, “check”, “bet”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.6}.\nUser:\nQ8:What should I do?\nPokerGPT: You should bet 1.\nQ9: Please tell me my winning probability.\nPokerGPT: Sorry, I can’t calculate the probability. However,\nas you have flush, you are very likely to win.\nInformation updates:\n... My Action: [“call”, “check”, “call”, “bet 0.9”]\n......\nSeat 9: [‘**’, ‘**’], Money: [1.37], Action: [“raises 0.05 to\n0.1”, “bets 0.22”, “fold”], Discard: [True]\nThe pot value is [1.42]\nPlayer 2 wins $1.42 with “Flush”.\nFig. 12: The question-answering instance of PokerGPT.\nhave successfully developed an end-to-end solver for Poker\nwith arbitrary amount of players, which is also convenient for\ninteraction. The experimental results are notably satisfactory,\nshowing a great advantage in training cost and inference speed\namong previous works, while obtaining a competitive win rate.\nPokerGPT addresses a series of challenges encountered by\nprevious Poker AI, substantiating the considerable potential\nof LLMs in tackling IIGs.\nIn subsequent research directions, we aim to further improve\nthe capability of PokerGPT. To empower our model with\nexplainability on decisions, various data should be prepared,\nincluding explanations on actions in each round written by\nhumans, probabilities of each action which can be outputted\nby CFR models, multiple candidate decisions, etc.\nREFERENCES\n[1] D. A. Waterman, “Generalization learning techniques for automating\nthe learning of heuristics,” Artificial Intelligence, vol. 1, no. 1-2, pp.\n121–170, 1970.\n[2] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis`y, D. Morrill, N. Bard,\nT. Davis, K. Waugh, M. Johanson, and M. Bowling, “Deepstack: Expert-\nlevel artificial intelligence in heads-up no-limit poker,” Science, vol. 356,\nno. 6337, pp. 508–513, 2017.\n[3] N. Brown and T. Sandholm, “Superhuman ai for heads-up no-limit\npoker: Libratus beats top professionals,” Science, vol. 359, no. 6374,\npp. 418–424, 2018.\n[4] N. Brown, T. Sandholm, and B. Amos, “Depth-limited solving for\nimperfect-information games,” Advances in neural information process-\ning systems, vol. 31, 2018.\n[5] N. Brown and T. Sandholm, “Solving imperfect-information games via\ndiscounted regret minimization,” in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 33, no. 01, 2019, pp. 1829–1836.\n[6] ——, “Superhuman ai for multiplayer poker,” Science, vol. 365, no.\n6456, pp. 885–890, 2019.\n[7] M. Bowling, N. Burch, M. Johanson, and O. Tammelin, “Heads-up limit\nhold’em poker is solved,” Science, vol. 347, no. 6218, pp. 145–149,\n2015.\n[8] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\nM. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained transformer\nlanguage models,” arXiv preprint arXiv:2205.01068, 2022.\n[9] J. Rubin and I. Watson, “Computer poker: A review,” Artificial intelli-\ngence, vol. 175, no. 5-6, pp. 958–987, 2011.\n[10] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione, “Regret\nminimization in games with incomplete information,” Advances in\nneural information processing systems, vol. 20, 2007.\n[11] N. Brown and T. Sandholm, “Safe and nested subgame solving for\nimperfect-information games,” Advances in neural information process-\ning systems, vol. 30, 2017.\n[12] N. Brown, A. Lerer, S. Gross, and T. Sandholm, “Deep counterfactual\nregret minimization,” in International conference on machine learning.\nPMLR, 2019, pp. 793–802.\n[13] N. Brown, A. Bakhtin, A. Lerer, and Q. Gong, “Combining deep\nreinforcement learning and search for imperfect-information games,” Ad-\nvances in Neural Information Processing Systems, vol. 33, pp. 17 057–\n17 069, 2020.\n[14] E. Zhao, R. Yan, J. Li, K. Li, and J. Xing, “Alphaholdem: High-\nperformance artificial intelligence for heads-up no-limit poker via end-\nto-end reinforcement learning,” in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 36, no. 4, 2022, pp. 4689–4697.\n[15] OpenAI, “Gpt-4 technical report,” 2023.\n[16] L. Chen, B. Li, S. Shen, J. Yang, C. Li, K. Keutzer, T. Darrell, and Z. Liu,\n“Language models are visual reasoning coordinators,” in ICLR 2023\nWorkshop on Mathematical and Empirical Understanding of Foundation\nModels, 2023.\n[17] X. Zou, Z.-Y. Dou, J. Yang, Z. Gan, L. Li, C. Li, X. Dai, H. Behl,\nJ. Wang, L. Yuan et al., “Generalized decoding for pixel, image, and\nlanguage,” in Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, 2023, pp. 15 116–15 127.\n[18] K. Bi, L. Xie, H. Zhang, X. Chen, X. Gu, and Q. Tian, “Accurate\nmedium-range global weather forecasting with 3d neural networks,”\nNature, pp. 1–6, 2023.\n[19] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, “Otter: A\nmulti-modal model with in-context instruction tuning,” arXiv preprint\narXiv:2305.03726, 2023.\n[20] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso et al., “Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of\nlanguage models,” arXiv preprint arXiv:2206.04615, 2022.\n[21] S. V¨artinen, P. H¨am¨al¨ainen, and C. Guckelsberger, “Generating role-\nplaying game quests with gpt language models,” IEEE Transactions on\nGames, 2022.\n[22] J. Kelly, M. Mateas, and N. Wardrip-Fruin, “Towards computational\nsupport with language models for ttrpg game masters,” in Proceedings of\nthe 18th International Conference on the Foundations of Digital Games,\n2023, pp. 1–4.\n[23] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz,\n“Playing repeated games with large language models,” arXiv preprint\narXiv:2305.16867, 2023.\n[24] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\nD. Drain, S. Fort, D. Ganguli, T. Henighan et al., “Training a helpful and\nharmless assistant with reinforcement learning from human feedback,”\narXiv preprint arXiv:2204.05862, 2022.\n[25] J. Abramson, A. Ahuja, F. Carnevale, P. Georgiev, A. Goldin, A. Hung,\nJ. Landon, J. Lhotka, T. Lillicrap, A. Muldal et al., “Improving mul-\ntimodal interactive agents with reinforcement learning from human\nfeedback,” arXiv preprint arXiv:2211.11602, 2022.\n[26] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, “Calibrate\nbefore use: Improving few-shot performance of language models,” in\nInternational Conference on Machine Learning.\nPMLR, 2021, pp.\n12 697–12 706.\n[27] L.\nWeng,\n“Prompt\nengineering,”\nhttps:\/\/lilianweng.github.io\/posts\/\n2023-03-15-prompt-engineering\/, march 15, 2023.\n[28] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari, X. Wu, A. A.\nAwan, J. Rasley, M. Zhang, C. Li, C. Holmes et al., “Deepspeed-\nchat: Easy, fast and affordable rlhf training of chatgpt-like models at\nall scales,” arXiv preprint arXiv:2308.01320, 2023.\n[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[30] E. G. Jackson, “Slumbot nl: Solving large games with counterfactual\nregret minimization using sampling and distributed processing,” in Work-\nshops at the Twenty-Seventh AAAI Conference on Artificial Intelligence,\n2013.\n[31] Y. Yang, “An evaluation of statistical approaches to text categorization,”\nInformation retrieval, vol. 1, no. 1-2, pp. 69–90, 1999.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model.pdf"}
{"title":"Enhance Reasoning for Large Language Models in the Game Werewolf","authors":"Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, Haobo Fu","summary":"This paper presents an innovative framework that integrates Large Language\nModels (LLMs) with an external Thinker module to enhance the reasoning\ncapabilities of LLM-based agents. Unlike augmenting LLMs with prompt\nengineering, Thinker directly harnesses knowledge from databases and employs\nvarious optimization techniques. The framework forms a reasoning hierarchy\nwhere LLMs handle intuitive System-1 tasks such as natural language processing,\nwhile the Thinker focuses on cognitive System-2 tasks that require complex\nlogical analysis and domain-specific knowledge. Our framework is presented\nusing a 9-player Werewolf game that demands dual-system reasoning. We introduce\na communication protocol between LLMs and the Thinker, and train the Thinker\nusing data from 18800 human sessions and reinforcement learning. Experiments\ndemonstrate the framework's effectiveness in deductive reasoning, speech\ngeneration, and online game evaluation. Additionally, we fine-tune a 6B LLM to\nsurpass GPT4 when integrated with the Thinker. This paper also contributes the\nlargest dataset for social deduction games to date.","url":"http:\/\/arxiv.org\/abs\/2402.02330v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.02330v2","published":1707018430000,"comment":null,"pdf_text":"Enhance Reasoning for Large Language Models in the Game Werewolf\nShuang Wu 1 * Liwen Zhu 1 * Tao Yang 1 Shiwei Xu 1 Qiang Fu 1 Wei Yang 1 Haobo Fu 1\nAbstract\nThis paper presents an innovative framework that\nintegrates Large Language Models (LLMs) with\nan external Thinker module to enhance the reason-\ning capabilities of LLM-based agents. Unlike aug-\nmenting LLMs with prompt engineering, Thinker\ndirectly harnesses knowledge from databases and\nemploys various optimization techniques. The\nframework forms a reasoning hierarchy where\nLLMs handle intuitive System-1 tasks such as\nnatural language processing, while the Thinker\nfocuses on cognitive System-2 tasks that require\ncomplex logical analysis and domain-specific\nknowledge. Our framework is presented using a 9-\nplayer Werewolf game that demands dual-system\nreasoning. We introduce a communication proto-\ncol between LLMs and the Thinker, and train the\nThinker using data from 18,800 human sessions\nand reinforcement learning. Experiments demon-\nstrate the framework’s effectiveness in deductive\nreasoning, speech generation, and online game\nevaluation. Additionally, we fine-tune a 6B LLM\nto surpass GPT4 when integrated with the Thinker.\nThis paper also contributes the largest dataset\nhttps:\/\/github.com\/boluoweifenda\/werewolf for\nsocial deduction games to date.\n1. Introduction\nThe field of artificial intelligence has witnessed ground-\nbreaking advancements in recent years, with the develop-\nment of Large Language Models (LLMs) (Ouyang et al.,\n2022; OpenAI, 2023; Anil et al., 2023). Apart from their\nimpressive proficiency in natural language processing (NLP)\ntasks (Thoppilan et al., 2022; Zhang et al., 2023b), LLMs\nalso exhibit vast potential as a general problem solver in\nareas such as planning and decision-making (Huang et al.,\n2022), knowledge transfer and generalization (Anil et al.,\n2022) and multi-modal perception (Yin et al., 2023) due to\nthe rich world knowledge embedded in their training cor-\n*Equal contribution 1Tencent AI Lab, Shenzhen, China. Corre-\nspondence to: Shuang Wu <shawnswu@tencent.com>.\nPreliminary work. Under review.\npora. As a result, integrating LLMs as central controllers\nwith task agents to enable end-to-end solutions has become\none of the most promising research directions, leading to\nsignificant breakthroughs in domains such as tools and assis-\ntants (Schick et al., 2023; Ge et al., 2023), engineering (Ahn\net al., 2022), social simulations (Park et al., 2023), and\ngaming (Wang et al., 2023).\nLLM-based agents harness LLMs for their general-purpose\nreasoning abilities (Huang & Chang, 2022), which are pri-\nmarily enabled by prompt engineering methods such as in-\nformation profiling (Zhang et al., 2023a; Qian et al., 2023),\nstep-by-step task decomposition (Wei et al., 2022b; Zhou\net al., 2022), recursive prompting by feedback from the\nenvironment (Yao et al., 2022), human (Wu et al., 2022)\nand self-refinement (Madaan et al., 2023; Shinn et al.,\n2023). These methods thus eliminate the requirement for\ndomain-specific fine-tuning of LLMs. To augment their\ntask-specific competencies, researchers also adopt external\nmodules such as memory for storing and retrieving histor-\nical information (Lin et al., 2023; Zhong et al., 2023; Hu\net al., 2023), external tools (Schick et al., 2023), APIs (Qin\net al., 2023), knowledge bases (Lewis et al., 2020) and ex-\npert models (Yang et al., 2023; Ge et al., 2023).\nDespite these advancements, challenges persist in domain-\nspecific tasks, where LLM-based agents often serve primar-\nily as demonstrations rather than as practical solutions (Qian\net al., 2023; Liu et al., 2023b). First, while LLMs have\nemerged some basic reasoning capabilities, they require\nsufficient model scales (Kaplan et al., 2020) and substan-\ntial computational overheads, along with various aforemen-\ntioned techniques (Wei et al., 2022a). However, LLMs\nstruggle to achieve satisfactory performance when it comes\nto higher-level reasoning (Stechly et al., 2023; Dziri et al.,\n2023) and planning (Valmeekam et al., 2023; Bubeck et al.,\n2023) tasks. Second, most LLM-based agents avoid fine-\ntuning LLMs on task-specific data to preserve the model’s\ngenerality and prevent over-fitting. This strategy compli-\ncates the utilization of existing task-specific data and exper-\ntise, as well as the alignment of task scenarios with input-\noutput formats, data distribution, and human preferences.\nTo address the limitations of LLMs in complex reason-\ning, we distinctly separate reasoning tasks into two sys-\ntems based on the dual-process theory (Wason & Evans,\n1\narXiv:2402.02330v2  [cs.AI]  29 Mar 2024\nEnhance Reasoning for Large Language Model in the Game Werewolf\n1974) and propose an external Thinker module to enhance\nthe reasoning capabilities of LLM-based agents. In our\nframework, LLMs are responsible for System-1 reasoning\ntasks involving intuitive thinking, such as basic NLP in-\nteractions, common-sense, and symbolic reasoning, while\nthe Thinker handles System-2 reasoning that requires com-\nplex logical analysis, deep understanding of domain-specific\nknowledge, and strategic planning in specialized tasks. We\nestablish a communication protocol between LLMs and the\nThinker through language-based features and instructions.\nUnlike augmenting LLMs with cumbersome prompt engi-\nneering, the Thinker is directly optimized with knowledge\nfrom databases and trained using supervised and reinforce-\nment learning techniques, thus enhancing the LLM-agent’s\nperformance and domain alignment without compromising\nLLM’s generality.\nWe select the 9-player Werewolf game as a proving ground\nfor the proposed framework. Werewolf is a popular so-\ncial deduction game, current AI systems fall short when\ncompared to even moderate human players in this domain.\nSystem-1 reasoning tasks in Werewolf encompass natural\nlanguage understanding and generation of players’ state-\nments, as well as the adept use of game-specific jargon.\nMeanwhile, the hidden roles necessitate complex strate-\ngic thinking such as identity concealment, and sophisticated\ncommunication involving deception and disguise, which fall\nunder System-2 reasoning. This duality creates a significant\ngap between the players’ actual statements and their true\nintentions, making Werewolf an ideal testbed for assessing\nthe advanced reasoning capabilities of LLM agents.\nWe have collected 18,800 real human game sessions and\nanalysed the primary patterns behind human speeches. In-\nformed by these patterns, we design language-based fea-\ntures for speech understanding and instructions for speech\ngeneration. The Thinker module is optimized by imita-\ntion learning, reinforcement learning (RL) from fictitious\nself-play (Heinrich et al., 2015), and population-based train-\ning (Jaderberg et al., 2017), to output reasonable game ac-\ntions and LLM speech instructions. We compare our ap-\nproach with GPT3.5\/4 methods using Least-to-Most (LtM)\nprompting (Zhou et al., 2022) from three aspects: deductive\nreasoning and decision-making, human evaluation of speech\ngeneration, and online evaluation of a complete game. Ex-\nperiments demonstrate that the integration of an external\nThinker module substantially enhances the reasoning and\ngeneration capability of LLMs. Further, we fine-tune a\nsmaller LLM model (6B) (Du et al., 2021) to better align\nreal human speech styles and preferences, outperforming\nGPT4 in the majority of evaluative scenarios. Our primary\ncontributions include:\n• We propose an external Thinker module to enhance the\nreasoning capabilities of LLM agents and demonstrate it\nby a Werewolf AI that surpasses GPT4 in real gameplay.\n• We release a dataset1 of 18,800 Werewolf game sessions,\nwhich represents the largest known dataset for social de-\nduction games to date.\n2. Related Work\nEnhance Reasoning in LLMs.\nSeveral approaches by-\npass the need for intricate prompt engineering mentioned in\nthe Introduction. For instance, LLM+P (Liu et al., 2023a)\nemploys an external planner to address long-horizon robot\nplanning challenges. A different approach (Zhang et al.,\n2023a) heuristically designs a low-level planner to manage\nprimitive control actions. RAG (Lewis et al., 2020) com-\nbines pre-trained parametric-memory generation models\nwith a non-parametric memory to improve performance on\nknowledge-intensive tasks. Regarding the fine-tuning of\nLLMs, Galactica (Taylor et al., 2022) is trained on a scien-\ntific dataset that emphasizes detailed reasoning processes.\nWebGPT (Nakano et al., 2021) utilizes human feedback to\nfine-tune GPT-3, enabling it to answer long-form questions\nwithin a textual web-browsing context. Toolformer (Schick\net al., 2023) fine-tunes LLMs to use external tools in a\nself-supervised manner with human demonstrations. Ope-\nnAGI (Ge et al., 2023) implements RL from feedback in\nopen-ended tasks to refine the LLM’s planning strategy. Ci-\ncero (Bakhtin et al., 2022) fine-tunes LLMs to generate\ndialogue controlled by a strategic reasoning module in the\ngame Diplomacy. However, the control signals in Cicero\n(planned actions) are insufficient to convey the complex\nlanguage dynamics (both understanding and generation) in\nthe Werewolf game.\nAI for Social Deduction Games. DeepRole (Serrino et al.,\n2019) combines counterfactual regret minimization (CFR)\nwith deep value networks in the non-speech five-player\nAvalon game. Hidden Agenda (Kopparapu et al., 2022)\npresents a two-team, non-speech social deduction game in a\n2D environment. A system comprising three LLM-powered\ninterfaces is created (Zhu et al., 2023) to aid gameplay in\nDungeon Master. Regarding AI in werewolf games, boot-\nstrap aggregating and weighted ensemble learning have been\nemployed to improve voting strategies (Khan & Aranha,\n2022). (Brandizzi et al., 2021) proposes an RL framework\nto analyze the influence of diverse communication behav-\niors among agents. One Night Ultimate Werewolf (Eger &\nMartens, 2019) explores human responses to various delib-\neration strategies. In the five-player werewolf game, (Wang\n& Kaneko, 2018) builds a deep-Q network to decide whom\nto trust or kill. Deep Wolf (Shibata et al., 2023) fine-tunes a\nRoBERTa-like pretrained model with 48 game logs to con-\n1https:\/\/github.com\/boluoweifenda\/\nwerewolf\n2\nEnhance Reasoning for Large Language Model in the Game Werewolf\n(! Witch)  day 2, Player 4: \nPlayer 2, look at how your teammate\nPlayer 9's speech sounds like a werewolf.\nRight from the start, his speech seems to\nbe an attempt to save you. You claim\nyou're not afraid of being checked for\nshooting, so this round, I'll vote you out.\nTonight, I plan to poison Player 9. I\nstrongly recommend voting for Player 2,\nlet’s whether you are a Hunter. I think\nPlayers 2, 5, and 9 are three werewolves.\nSummary\nI think werewolves are Players 2, 5, and\n9. I suggest voting for Player 2 to check\nthe Hunter. I will poison Player 9 tonight.\nReasoning\nSpeech history\nSpeech\nListener\n0 6 0 3 6 0 0 0 6\n0 7 0 0 0 0 0 0 3\nGPT + LtM\nThinker\nRL & BC\nSpeech feature \nPresenter\n(\" Villager)  day 2, Player 6:\nHello, I am Player 6 and I am a\nVillager. I think Player 9 is a good\nguy.\nI\ndon't\nthink\nhe\nhas\nany\nwerewolf teammate. Both Player 3\nand 9 could be the hunter, but Player\n9 has a higher tolerance. Player 5\nhas revealed himself as a werewolf,\nPlayer 2 is likely to be one as well. I\nsuggest we vote Player 2 out first\nand\nthen\nsee\nwhat\nhappens\ntomorrow.\nLLM\nFilters\nIdentities：\n- Werewolves: 2,5\n- Hunter: 3,9\n- Villager: 6\nActions:\n- Vote out: 2\nSpeech\ninstruction\n0 6 4 0 6 5 0 0 4\n0 7 0 0 0 0 0 0 0\nGame state\nGame state\nGame state\nInstruction\nmodel\nPolicy\nmodel\nIdentity \nmodel\nIdentities：\n- Witch: 4\n- Werewolves: 2,5,9\nVote\nSkills\nLanguage \nfeature\nLanguage \nfeature\nLanguage \nfeature\nActions: \n- Vote out: 2 \n- Poison: 9\nSpeech history\nSpeech\nGPT  |  Finetune\nIdentities：\n- 2,5: Werewolves\n- 4: Witch or Werewolf\n- 6: Villager\n- 9: Hunter or Werewolf\nFigure 1: Overall processing framework and modules in the Werewolf implementation.\nstruct a value network given the current game state, human\nspeeches, and candidate actions. The seven-player version\nis explored with RL and LLMs in (Xu et al., 2023b;a). Our\napproach differs from previous studies in two fundamental\nways: First, we employ the Thinker to execute complex\nSystem-2 reasoning, in contrast to the reasoning approach\nof LLM in (Xu et al., 2023b), which generates candidate re-\nsults for the RL model to select and mitigate biases. Second,\nby collecting and leveraging authentic game sessions and\nspeech data, we aim for closer alignment with real-world\nscenarios and human interaction patterns.\n3. Methods\nWe introduce an innovative framework that synergizes\nLLMs with an external module for reasoning and decision-\nmaking, referred to as the Thinker, devised to augment\nLLM-based agents with sophisticated reasoning abilities.\nTo bridge the communication between Thinker and LLMs,\nwe introduce a protocol through structured features and\nprompt instructions. The framework is thus decomposed\ninto three processing components:\n• The Listener serves as the primary interface for natural\nlanguage understanding. It processes language inputs,\nengages in intuitive System-1 reasoning, and transforms\nthe information into structured language features that the\nThinker can interpret.\n• The Thinker functions as the cognitive core of the frame-\nwork. Utilizing language features provided by the Lis-\ntener, it specializes in System-2 reasoning tasks that re-\nquire deep logical analysis and domain-specific knowl-\nedge. The Thinker produces policies such as planning\nand actions, and generates strategic instructions for the\nPresenter.\n• The Presenter functions as the system’s articulator. It\ngenerates coherent and contextualized language output\nthat aligns with the current environment state, guided by\nthe strategic instructions from the Thinker. The Presenter\nensures that the generated language is logical, rational,\nconsistent, and free from hallucinations.\nTo demonstrate the effectiveness of our framework, we ap-\nply it to the complex social deduction game Werewolf. The\nremainder of this section will detail the implementation\nwithin the game environment, which necessitates deductive\nreasoning, speech understanding and generation, as illus-\ntrated in Figure 1.\n3.1. Data preparation\nWe collected data from the 9-player standard mode Were-\nwolf game hosted on the Fanlang platform2. The specific\nrules of the game are detailed in Appendix C. We recorded\nreal-time video in spectator mode for approximately 18,800\ngame sessions, which equates to around 7,000 hours of\ngameplay and 6,000 hours of speech. Furthermore, we\nenriched our dataset with a Werewolf domain-specific cor-\npus comprising nearly 1.4 million characters sourced from\nweb-crawled game strategies and OCR-processed Werewolf\nliterature. Each recorded session includes both the game\nstate data and the audio of players’ speeches. We captured\nexhaustive game state details, such as historical skills and\nvoting results, by utilizing an automated testing framework3.\nWe deployed the Paraformer (Gao et al., 2022) model for\nAutomatic Speech Recognition (ASR) of human speech\naudio. To improve recognition accuracy, especially for fre-\nquently used Werewolf-specific terms, we crafted a hot word\nlist from the domain corpus and utilized context biasing\nmethods (Zhao et al., 2019). Furthermore, we annotated\n2https:\/\/www.wolfkills.com\/\n3https:\/\/github.com\/appium\/appium\n3\nEnhance Reasoning for Large Language Model in the Game Werewolf\napproximately 127 hours of Werewolf speech data and per-\nformed supervised fine-tuning on the Paraformer model.\nThe character error rate of ASR for Werewolf speeches was\nreduced from 4.5% to 3.7%. We refer to the dataset here-\nafter as FanLang-9, and a thorough analysis of the dataset\nis in Appendix D.\n3.2. Listener\nIn the game of Werewolf, the complexity of speeches arises\nfrom players concealing their identities. Werewolves make\ndeceptive statements to disguise themselves as the \"Good\"\nfaction. Conversely, the \"Good\" faction strives to discern\nwerewolves by deducting from historical speeches and ac-\ntions while providing rational and credible statements. This\ndynamic creates a significant gap between the players’ ac-\ntual statements and their true intentions (see Figure 4). The\nListener aims to capture relevant insights from actual state-\nments without speculating on their hidden motives or truth-\nfulness. To tackle this, we introduce dual-phase processing:\nSynthesize and summarize: Human players’ speeches on\nthe Fanlang platform are characterized by an information\noverload that includes a tangled mix of context, lengthy and\nredundant content, and colloquial ramblings, alongside com-\nplex logic that encompasses quotations, rhetorical questions,\nhypotheses, and empathetic thinking, creating a rich and\nintricate web of discourse. Moreover, the accumulation of\nhistorical speeches often exceeds 10K tokens (see Figure 8),\nmaking it difficult for LLMs to directly infer information\nand process deductive reasoning (see Figure 2). Inspired\nby the Least-to-Most (LtM) prompting (Zhou et al., 2022),\nwe first prompt the LLM to generate a textual summary of\nfewer than 200 words for each single statement, retaining\nonly critical information that the speaker intends to express.\nReasoning and feature extraction: The LLM discerns and\ndelineates key information from these summaries and gener-\nates a JSON-style reasoning result of the speech, containing\npairs of player IDs with their attributes. Then the result is\ntokenized and categorized into specific language features,\nas detailed in Appendix D. For an N-player werewolf game,\nwe define M different attributes, which encompass various\naspects of a player, e.g., identities, actions, and historical\nskills. From the historical collection of all speeches H, a\nplayer’s single speech S may include descriptions of all the\nplayers in the game, the language feature can be presented\nby a matrix F ∈ZN×M:\nF = [f1, f2, ..., fN]T ,\n(1)\nwhere fn = [fn1, fn2, ..., fnM]T , n = 1, 2, ..., N and\nfnm ∈Vm, ∀n = 1, 2, ..., N and m = 1, 2, ..., M. Here\nVm signifies the set of the potential values that the m-th\nattribute can assume.\nAn example of summary and language feature is illustrated\nin Figure 1. Ablation studies in Appendix B indicate that\nsolely predicting future actions, as done in Cicero (Bakhtin\net al., 2022), will omit crucial identity accusations, lead-\ning to substantial information loss that is detrimental in the\nWerewolf game. Aside from directly prompting GPT3.5\nand GPT4 to generate language features, we also extract\n260K speech instances from the FanLang-9 dataset, label\nthe speech-feature pairs with GPT3.5, and finetune the\nChatGLM-6B (Du et al., 2021) model to perform the same\nreasoning task for practical efficiency. To ensure the out-\nput format of language features, we also include a post-\nprocessing filter for GPTs and the fine-tuned model. The\ndetailed prompts for summary, reasoning, fine-tuning, and\npost-filtering are provided in Appendix F.6.\n3.3. Thinker\nThe primary objective of the Thinker module is to tackle\ncomplex System-2 reasoning tasks. In the game of werewolf,\nit analyze the underlying intentions and strategic implica-\ntions behind players’ public speeches. In contrast to LLMs,\nwhich typically depend on complex prompt engineering\nfor scenario adaptation, the Thinker module distinguishes\nitself by its capacity to directly harness knowledge from\ndatabases and various optimizing techniques. This ability al-\nlows the Thinker to internalize human-like decision-making\npatterns and strategic speech intricacies that are crucial for\nnavigating the complex dynamics in this game.\nThe speech instruction I ∈ZN×M follows the same struc-\nture as the language feature outlined in Equation 1, which\ncan be viewed as a multi-label classification problem and\ndecomposed into multiple single-class classifications for\neach attribute fnm. Therefore, the generation of a speech\ninstruction is converted into N × M actions, aligning the\nsame training algorithm as for game actions. The optimiza-\ntion of the Thinker module comprises two phases: imitation\nlearning and RL. For imitation learning, we utilize human\ndata and the Behavioral Cloning (BC) (Torabi et al., 2018)\nloss as:\nLBC(θ) = −Es,a∼D[log πθ(a|s)],\n(2)\nwhere D denotes the dataset of human action a (or decom-\nposed speech attribute), state s, and πθ is the policy pa-\nrameterized by θ. For the RL phase, we employ Proximal\nPolicy Optimization (PPO) (Schulman et al., 2017) and a\ndistributional training framework (Ye et al., 2020):\nLRL(θ) = −Es,a∼πθ′\n\u0014 πθ(a|s)\nπθ′(a|s)Aπθ(s, a)\n\u0015\n,\n(3)\nwhere θ′ is the parameters of an old policy, and Aπθ(s, a) is\nthe advantage with respect to policy πθ.\nIn addition, we incorporate an auxiliary task that predicts\nall players’ identities. This task serves to reveal the model’s\ntrue deduction, which may contrast with the generated\n4\nEnhance Reasoning for Large Language Model in the Game Werewolf\nspeech instructions. We denote the cross-entropy loss func-\ntion as Lid(ϕ) with parameter ϕ, which is labeled by human\ndata or RL environment in a self-supervised manner. The\noverall training objective of the Thinker is formulated as:\nL = αLBC(θ) + LRL(θ) + βLid(ϕ),\n(4)\nwhere α and β are weighting coefficients.\nGiven the adversarial nature of the game, it is crucial to\nmaintain a balanced win rate between the two opposing fac-\ntions during training. To this end, we deploy distinct models\nfor the werewolf and the \"Good\" factions. We find that\noptimizing werewolves’ speech instruction is much more\nchallenging, as they need to mimic the \"Good\" faction’s\nspeech and master the art of disguise and deception. To mit-\nigate this, we draw inspiration from Generative Adversarial\nNetworks (Goodfellow et al., 2014) and adjust the training\niterations, nwerewolf : ngoods = 5 : 1. To prevent actions and\nspeech strategies from converging to a single pattern, we\nemploy population-based training (Jaderberg et al., 2017)\nwith a population size of 4. We also introduce fictitious self-\nplay (Heinrich et al., 2015), where in each game an average\nof 3 players employ the latest models, while the remaining\n6 players use models randomly selected from the most re-\ncent 500 checkpoints. Further details on hyperparameters,\nreward shaping, and model structures are in Appendix F.\n3.4. Presenter\nThe generation of players’ public speeches is a pivotal com-\nponent in the Werewolf game, which significantly impacts\nthe game’s outcome due to its strategic importance and in-\nfluence on other players’ actions. The quality of speech\ngeneration encompasses several critical aspects: (1) The\nstrategy articulated within the speech should align with the\nplayer’s role and the current state of the game. (2) Speeches\nneed to adhere to the logical framework of the game, cor-\nrelating with historical speeches and actions, making them\nsound and convincing. (3) Speeches must fit the stylistic\nenvironment of the Werewolf game. Detailed evaluation\nmetrics can be found in Appendix F.1.\nThe Thinker handles only the first aspect of speeches, pro-\nviding a foundational stem for the Presenter, such as the\nWitch’s decision to report the previous night’s rescue, as\nshown in Figure 1. Then the Presenter crafts a complete\nspeech that incorporates necessary contextual information\nrelevant to the game state and historical speeches. The\nPresenter module has two fundamental objectives:\n• Controllability: It must align with the strategic instruc-\ntions provided by the Thinker.\n• Quality: The generated speech should be logical, persua-\nsive, and aligned with human players’ preferences.\nTo achieve these objectives, the Presenter leverages the ca-\npabilities of LLMs by incorporating the Thinker’s strategic\ninstructions and the game state directly into the prompt, en-\nabling LLMs to generate Thinker-induced speeches. The\ntemplate for the prompt is provided in Appendix F.6. Ad-\nditionally, as with the Listener module, we fine-tune the\nChatGLM- 6B as a domain-specific Werewolf speech LLM.\nWe inverse the 260K speech-feature pairs in the finetuning\nof Listener: the language feature F generated by the Lis-\ntener now serves as the hindsight speech instruction I, and\nthe actual speech S serves as output labels.\nWe observed that LLMs often fail to follow prompts, and\neven fine-tuned models exhibit hallucinations and inaccu-\nracies. Taking inspiration from the Cicero (Bakhtin et al.,\n2022) approach, we introduce additional filtering steps. We\nuse the Listener module to perform further reasoning on the\ngenerated speeches to produce language features, which we\nthen compare for similarity to original speech instructions.\nFor expressions of the speaker’s own attributes, the filter\nrequires an exact match. For expressions pertaining to the\nattributes of others, the content indicated in the speech in-\nstructions must be consistent. For parts not mentioned in\nthe instructions, the filter allows the Presenter some leeway\nin cases of hallucinations. The speech generation process\niterates until it successfully meets the filter criteria or ex-\nceeds the maximum number of allowed attempts. When\nthe maximum is reached without successful compliance, a\nspeech is generated based on rules that take into account the\nplayer’s roles, historical skills, and identity predictions.\n4. Experiments\nWe assess the performance of our method by comparing it\nagainst several baselines and ablative variants. The models\ninvolved in the following experiments include:\n• GPT3.5\/4: GPT3.5 and GPT4 are directly applied to\ngenerate end-to-end action decisions and speeches. For\nGPT3.5, we use the model named gpt-35-turbo-16k and\nversion 0613. For GPT4, we apply model name gpt-4\nand version 1106-Preview. We prompt GPTs with ba-\nsic game rules, explanations of typical game jargon, and\ncomprehensive game information, including visible game\nstates, legal actions, and speech text converted by ASR.\nExamples of the prompts can be found in Appendix F.6.\n• GPT3.5\/4-LtM: We allow GPTs to first summarize each\nspeech, as described in Section 3.2, and then generate\nactions and speeches according to the game information\nand speech summaries.\n• GPT3.5\/4-T: GPTs serve as the Listener and Presenter\nmodules, and our proposed Thinker module is integrated\nfor generating actions and speech instructions.\n• Finetune-T: We replace GPTs with a 6B LLM fine-tuned\n5\nEnhance Reasoning for Large Language Model in the Game Werewolf\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nVote Werewolves\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\nAccuracy (%)\nIdentify Seer\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nIdentify Witch\nDay1\nDay2\nDay3+\nTotal\n0\n5\n10\n15\n20\n25\n30\nAccuracy (%)\nIdentify Hunter\nGPT3.5\nGPT3.5-LtM\nGPT4\nGPT4-LtM\nThinker\nHuman\nrandom\nFigure 2: Voting and identification accuracy evaluating the reasoning capability from the perspective of villagers. The\nrandom baseline is calculated as total_role_number\/total_hidden_players, i.e., 3\/8 or 1\/8\non the FanLang-9 dataset in both the Listener and Pre-\nsenter modules, while the Thinker remains the same as\nin GPT3.5\/4-T. It is for practical efficiency concern, our\nframework does not necessitate the finetuning of LLMs.\n4.1. Deductive Reasoning\nWe evaluate the reasoning capabilities of various models,\nwhich encompass understanding and comprehending of both\nthe game state and the historical speeches, i.e., how the mod-\nels think of the current game status. We extract 300 games\nfrom the FanLang-9 dataset as the test set. Models are re-\nquired to identify special roles (Seer, Witch, and Hunter)\nand vote for the most likely werewolf, from the perspective\nof villagers at the first round of voting each day. Given\nthat villagers have limited access to information and must\nengage extensively in deductive reasoning within the game,\nthis task represents a stringent test of the models’ reasoning\ncapabilities. The test set encompasses approximately 1,200\nevaluation instances. For the Thinker, we utilize its action\ndecision as the result for werewolf voting, and identities pre-\ndicted by the auxiliary task as results for special roles. We\nassume that human players in the test set who are villagers\nwould vote for the most likely werewolf. Therefore, their\nvoting choices are listed as a reference but their judgments\nabout other players’ identities remain unknown.\nThe accuracy results are shown in Figure 2. In terms of vot-\ning werewolves, human players have the highest accuracy\nand the Thinker is closest to human players. The Thinker\nmodule closely mirrors human performance, notably out-\nperforming direct reasoning methods using GPTs in the\nidentification of werewolves and other special roles. LtM\nprompting improves GPT3.5’s performance, particularly in\nthe identification of the Seer, suggesting benefits in pro-\ncessing complex and lengthy speech contexts. However,\nthe marginal gains for GPT4-LtM over GPT4 indicate that\nGPT4’s inherent improvements in handling extensive texts\nmake it less reliant on speech summaries and more depen-\ndent on game state experiences. We observe that in human\ngameplay, Seers and Witches often disclose their roles, aid-\ning GPTs in outperforming random baselines, while Hunters\nand werewolves typically conceal their roles, resulting in\nGPTs’ performance aligning with random guessing. No-\ntably, the accuracy of GPTs tends to decline over successive\ndays except for the Hunter, whereas the Thinker’s accuracy\nimproves. This pattern suggests that although GPTs initially\nbenefit from role disclosures on the first day, they may be\nhindered by the extensive speeches in subsequent days.\nTotal\nSeer\nWitch\nHunter\nVillager\nWerewolf\n0\n1\n2\n3\n4\nAverage rank score (higher is better)\nGPT3.5-LtM\nGPT3.5-T\nGPT4-LtM\nGPT4-T\nFinetune-T\nFigure 3: Human preference score for generated speeches\ngrouped by identities.\n4.2. Thinker-induced Speech generation\nWe investigate the speech generation capabilities of various\nmodels. Using the same 300 complete games from Sec-\ntion 4.1, we extracted around 400 speech sessions that span\na diverse range of roles, times of day, and speech types\n(first\/second round speech, last words).\nModels are assigned the task of generating speeches based\non the current game state and all players’ historical speeches,\nwith detailed prompts for GPTs available in Appendix F.6.\nGiven the effectiveness of LtM prompting, we excluded\nGPTs without LtM prompting in subsequent experiments.\n6\nEnhance Reasoning for Large Language Model in the Game Werewolf\nDay 1, first round speech sequence:\n8: \"  Seer > 9: !  Villager > 1: #  Witch > 2: !  Villager > 3: \/  Werewolf > 4: !  Villager > 5: $  Hunter > 6: \/  Werewolf > 7: \/  Werewolf\n(8: \"  Seer): Hello everyone, I am Player 8 and my identity is a Seer. Last \nnight I checked on Player 7, he was a werewolf. \n(9: !  Villager): … \n(1: #  Witch): I am the Witch saving myself. I suspect that Player 6 is a \nwerewolf and I will poison him. Player 9 sounds like a good man.  \n(2: !  Villager): I am a Villager, let’s listen to Player 7’s statement.\n(3: \/  Werewolf): Fellow villagers, I am Player 3. Based on the previous \nspeech, I think Player 9 is a good guy because he said nothing but  external \ninformation, which is an opportunity for us werewolves. We can use his \nspeech to divert the villagers' attention and make them suspect him. As for \nPlayer 6, I don't have enough information to make a judgment. Additionally, I \nnoticed that Player 2 claimed to be a Villager, which presents an opportunity \nfor us to get him on our side. \nThinker-induced Speech\nSpeech without Thinker instruction\nSpeech History\n(3: \/  Werewolf): Hello everyone, I am the Seer ✅. Last \nnight, I checked Player 9 and confirmed that he is a good \nperson; he is my verified innocent. I will protect him to the \nend. If anyone wants to eliminate Player 9, it must be to \nget me out of the game first. Just now, Player 8 and I had a \nconflict over the role of the Seer, and he is definitely a \nwerewolf. Therefore Player 7 might be a good man. There \nare two more werewolves in the game, but two players \nhave already declared they are villagers. There aren't \nenough roles for villagers left. I will listen carefully to the \nsubsequent players' statements. Everyone, let's vote out \nPlayer 8 this round.\nSpeech instruction:\n•\n\"identities\": {\"seer\": [3], \"good\": [9]}, \n•\n\"actions\": {\"check\": [[3,9]], \"vote out\": [[3,8]]}\nFigure 4: An example comparison of speeches with and without strategic instruction.\nFor GPTs-T and Finetune-T settings, speech instructions are\nderived from the Thinker and incorporated into the prompts.\nTo evaluate models’ single-shot generation ability, we do not\nadopt post filtering process for generated speeches in this\nevaluation, which yielded approximately 2,000 speeches for\nfive models. To evaluate the quality of generated speeches,\nwe recruited 10 human evaluators who are well-familiar with\nthe Werewolf game. For each session, generated speeches\nare presented in a randomized order to ensure that evaluators\nare unaware of the model behind each speech. Evaluators\nare required to rank the speeches and detect obvious legal\nerrors according to the criteria detailed in Appendix F.1.\nThe evaluation results are shown in Figure 3. Considering\ntotal scores, GPTs induced by the Thinker outperform their\nLtM-prompting counterparts, demonstrating that speech in-\nstructions significantly enhance speech quality. Moreover,\nthe fine-tuned 6B model surpasses GPT4 with prompting\nmethods in speech generation capability. Regarding scores\nfor specific roles, the Thinker’s contribution over GPT3.5 is\nsomewhat limited for the Seer, whose speeches are relatively\nstraightforward, needing only to report inspections from the\nprevious night. The assessment of villagers’ speeches is\ninherently complex due to their limited available informa-\ntion, which is reflected in the minimal rank score differ-\nences observed among the models for this role. In contrast,\nrank score differences are most obvious for werewolves.\nThis disparity stems mainly from the low legality of were-\nwolf speeches, which often inadvertently reveal their iden-\ntity—a critical error as outlined in Appendix Table 3. No-\ntably, GPT3.5 struggles to adhere to instructions that advise\nagainst self-incrimination, while GPT4 demonstrates a more\nsophisticated ability to disguise itself, especially when in-\nduced by the Thinker’s strategic instructions. An example\nspeech is presented in Figure 4.\n4.3. Online Evaluation\nLastly, we conduct online evaluations to assess the over-\nall performance in a real-world gameplay setting, which\ninvolves the five models in the speech generation evalua-\ntion: GPT3.5\/4-LtM, GPT3.5\/4-T and Finetune-T. As Were-\nwolf is a multiplayer imperfect-information game, the skill\nlevel of participants can significantly affect the evaluation\nresults. Therefore, we devise three model combinations,\nwithin which models are randomly and repeatedly selected\nto simulate a nine-player game. We conducted approxi-\nmately 600 rounds for each combination to ensure robust\ntesting results. Given the inherent randomness of outcomes\nin Werewolf, we also calculate the Behavior Score, a typical\nmetric used in Werewolf competitions4. A comprehensive\nbreakdown of the Behavior Score is provided in Table 8.\nTable 1 summarizes the results, revealing that the integra-\ntion of the Thinker module markedly boosts the win rates\nof both GPT3.5 and GPT4 in all three combinations. The\nperformance of the Finetune-T model closely aligns with\nthat of GPT4-T. In terms of Behavior Score, the Thinker\ncontributes substantial improvements across all roles for\nGPT3.5. For GPT4, notable benefits are observed particu-\nlarly for the Witch and Hunter roles. The Behavior Score\nmetric assigns significant weight to the witch’s poisoning\nand the hunter’s shooting decisions, which correlates with\nthe Thinker’s ability to enhance werewolf detection and sub-\nsequently improve these scores. Another finding is that the\ncombination of GPT4s and Finetune-T models results in the\nhighest win rate for the werewolves. This outcome primarily\nstems from the conservative nature of GPT4-LtM in role\nidentification, which makes it more cautious in voting and\n4https:\/\/langrensha.163.com\/20230313\/\n31014_1077578.html\n7\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 1: Online evaluation results showcasing the performance of 9 AIs using 5 different models and 3 combinations.\nResults are presented in the format: win rate | Behavior Score.\nMethod\nTotal\nSeer\nWitch\nHunter\nVillager\nWerewolf\nGPT3.5-LtM\n36.7% | −0.21\n25.6% | +0.16\n23.1% | −0.51\n29.9% | −0.21\n30.8% | −0.42\n53.4% | 0.00\nGPT3.5-T\n47.4% | −0.05\n38.3% | +0.27\n41.0% | −0.14\n36.4% | −0.12\n33.8% | −0.18\n68.6% | 0.00\nFinetune-T\n50.3% | −0.06\n38.8% | +0.33\n39.8% | −0.18\n37.0% | −0.29\n39.1% | −0.11\n74.4% | 0.00\nGPT4-LtM\n37.9% | −0.01\n21.9% | +0.25\n18.6% | −0.25\n19.4% | −0.06\n20.3% | −0.00\n73.6% | 0.00\nGPT4-T\n41.1% | −0.02\n20.4% | +0.25\n23.2% | −0.10\n23.9% | −0.09\n22.5% | −0.09\n78.4% | 0.00\nFinetune-T\n43.1% | −0.04\n24.2% | +0.27\n24.6% | −0.15\n23.4% | −0.15\n23.9% | −0.11\n81.4% | 0.00\nGPT3.5-LtM\n33.0% | −0.22\n14.4% | +0.12\n20.4% | −0.46\n20.7% | −0.57\n21.6% | −0.33\n57.0% | 0.00\nGPT3.5-T\n45.0% | −0.07\n33.6% | +0.29\n32.2% | −0.13\n30.4% | −0.17\n27.6% | −0.20\n75.8% | 0.00\nGPT4-LtM\n42.5% | −0.03\n29.8% | +0.27\n22.2% | −0.18\n27.0% | −0.20\n28.7% | −0.04\n71.9% | 0.00\nGPT4-T\n46.3% | −0.05\n28.6% | +0.28\n34.5% | −0.11\n31.5% | −0.08\n28.0% | −0.18\n79.9% | 0.00\nFinetune-T\n45.9% | −0.06\n29.1% | +0.25\n28.3% | −0.16\n29.2% | −0.21\n32.4% | −0.14\n78.0% | 0.00\nskill usage as the \"Good\" faction.\nFurthermore, we incorporate 13 human players to evalu-\nate AI performance against human strategy. We find that\nthe issue of werewolf identity exposure, as mentioned in\nSection 4.2, significantly impedes the game experience of\nhuman players. As a result, participants play alongside four\ninstances each of GPT4-T and Finetune-T models across\n200 game rounds, and the post-filtering process for gener-\nated speeches is adopted in this setting. As can be seen\nin Table 2, human players exhibit no significant win rate\nadvantage, suggesting that the AI’s speeches and actions\ndo not exhibit exploitable weaknesses. Moreover, when\ncompared with the results in Table 1, we note a relative\ndecrease in the werewolves’ win rate in games involving\nhuman players, highlighting the ongoing challenges related\nto identity concealment. Although AI-managed werewolves\nmight convincingly deceive other AI players, human players\noften find them suspicious. A typical example is that were-\nwolves tend to act in groups, such as unanimously voting\nfor Player_3.\nTable 2: Online evaluation win rates with 1 human and 8AIs.\nMethod\nTotal\nGoods\nWerewolves\nGPT4-T\n46.9%\n37.3%\n65.0%\nFinetune-T\n45.3%\n36.0%\n62.6%\nHuman\n40.5%\n35.3%\n59.4%\n5. Discussion and Future Work\nTransfer to other tasks: We use language features and\nspeech instructions in our framework to integrate LLMs and\nexternal reasoning models. The communication format may\nnot be directly transferable to other tasks or domains, and the\neffectiveness depends on the richness of these features and\ninstructions. Future work will aim to develop more general-\nized and flexible methods, e.g., implicit hidden vectors in a\ndata-driven manner, which would offer better transferability\nbut at the expense of interpretability and controllability.\nEvaluation of 8 humans with 1 AI: Our evaluations primar-\nily involved games featuring either AI vs AI or one human\nplayer competing against multiple AIs. Evaluating an AI in\na majority-human player setting presents challenges due to\nthe highly interactive nature of the game and the variability\nin human players’ speech strategies and behaviors.\nInterpretability: While our framework improves the rea-\nsoning capabilities of LLMs, the reasoning processes in the\nThinker module may not be easily interpretable to humans.\nWe explicitly introduce the identity prediction task to reveal\nhow the Thinker think of other players. Future work could\nexplore methods for further improving the interpretability\nand transparency of our framework.\n6. Conclusion\nIn this paper, we introduced a novel framework for inte-\ngrating LLMs with an external Thinker, aiming to enhance\nthe reasoning capabilities of LLM-based agents. This ap-\nproach is inspired by the dual-process theory and separates\nreasoning tasks into two systems: System-1, handled by\nLLMs, and System-2, handled by the Thinker model. We\nshowcased our approach in the context of the Werewolf\ngame, a complex social deduction game requiring language\nprocessing, intuitive thinking, and strategic planning. Our\nresults show that our framework can significantly improve\nthe performance of LLMs and achieve better alignment with\nreal-world scenarios and human preferences. Additionally,\nwe fine-tune a 6B model to surpass GPT4 when integrated\nwith the Thinker. This paper also contributes the largest\n8\nEnhance Reasoning for Large Language Model in the Game Werewolf\ndataset for social deduction games to date, hoping to accel-\nerate further advancements in this field.\nAcknowledgements\nWe thank Jian Yao, Weiming Liu, Qing Wang, Ye Tian,\nZimeng Zhou, Yiming Gao, Liangzhou Wang, Kaiwen Zhu,\nFeiyu Liu, Jianing Shi, Fengming Zhu, Xiaoyu Yang for the\nhuman evaluation of speech generation and online games.\nWe thank Jian Yao, Jianing Shi and Guohua Tang for the\nthoughtful discussion.\nReferences\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,\nDavid, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman,\nK., et al. Do as i can, not as i say: Grounding language\nin robotic affordances. arXiv preprint arXiv:2204.01691,\n2022.\nAnil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra,\nV., Ramasesh, V., Slone, A., Gur-Ari, G., Dyer, E., and\nNeyshabur, B. Exploring length generalization in large\nlanguage models. Advances in Neural Information Pro-\ncessing Systems, 35:38546–38556, 2022.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,\nD., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,\nZ., et al.\nPalm 2 technical report.\narXiv preprint\narXiv:2305.10403, 2023.\nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C.,\nFried, D., Goff, A., Gray, J., Hu, H., et al. Human-level\nplay in the game of diplomacy by combining language\nmodels with strategic reasoning. Science, 378(6624):\n1067–1074, 2022.\nBrandizzi, N., Grossi, D., and Iocchi, L. Rlupus: Cooper-\nation through emergent communication in the werewolf\nsocial deduction game. Intelligenza Artificiale, 15(2):\n55–70, 2021.\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,\nHorvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y.,\nLundberg, S., et al. Sparks of artificial general intel-\nligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712, 2023.\nDu, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z.,\nand Tang, J. Glm: General language model pretrain-\ning with autoregressive blank infilling. arXiv preprint\narXiv:2103.10360, 2021.\nDziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y.,\nWest, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., et al.\nFaith and fate: Limits of transformers on compositionality.\narXiv preprint arXiv:2305.18654, 2023.\nEger, M. and Martens, C. A study of ai agent commitment\nin one night ultimate werewolf with human players. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence and Interactive Digital Entertainment, volume 15,\npp. 139–145, 2019.\nGao, Z., Zhang, S., McLoughlin, I., and Yan, Z.\nParaformer: Fast and Accurate Parallel Transformer for\nNon-autoregressive End-to-End Speech Recognition. In\nProc. Interspeech 2022, pp. 2063–2067, 2022.\ndoi:\n10.21437\/Interspeech.2022-9996.\nGe, Y., Hua, W., Ji, J., Tan, J., Xu, S., and Zhang, Y. Ope-\nnagi: When llm meets domain experts. arXiv preprint\narXiv:2304.04370, 2023.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.\nGenerative adversarial nets. In Ghahramani, Z., Welling,\nM., Cortes, C., Lawrence, N., and Weinberger, K. (eds.),\nAdvances in Neural Information Processing Systems, vol-\nume 27. Curran Associates, Inc., 2014.\nHeinrich, J., Lanctot, M., and Silver, D. Fictitious self-play\nin extensive-form games. In International conference on\nmachine learning, pp. 805–813. PMLR, 2015.\nHu, C., Fu, J., Du, C., Luo, S., Zhao, J., and Zhao, H.\nChatdb: Augmenting llms with databases as their sym-\nbolic memory. arXiv preprint arXiv:2306.03901, 2023.\nHuang, J. and Chang, K. C.-C.\nTowards reasoning in\nlarge language models:\nA survey.\narXiv preprint\narXiv:2212.10403, 2022.\nHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. Lan-\nguage models as zero-shot planners:\nExtracting ac-\ntionable knowledge for embodied agents. In Interna-\ntional Conference on Machine Learning, pp. 9118–9147.\nPMLR, 2022.\nJaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M.,\nDonahue, J., Razavi, A., Vinyals, O., Green, T., Dunning,\nI., Simonyan, K., et al. Population based training of\nneural networks. arXiv preprint arXiv:1711.09846, 2017.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKhan, M. and Aranha, C. A novel weighted ensemble learn-\ning based agent for the werewolf game. arXiv preprint\narXiv:2205.09813, 2022.\n9\nEnhance Reasoning for Large Language Model in the Game Werewolf\nKopparapu, K., Duéñez-Guzmán, E. A., Matyas, J., Vezh-\nnevets, A. S., Agapiou, J. P., McKee, K. R., Everett, R.,\nMarecki, J., Leibo, J. Z., and Graepel, T. Hidden agenda:\na social deduction game with diverse learned equilibria.\narXiv preprint arXiv:2201.01816, 2022.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,\nGoyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel,\nT., et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Advances in Neural Information Pro-\ncessing Systems, 33:9459–9474, 2020.\nLin, J., Zhao, H., Zhang, A., Wu, Y., Ping, H., and Chen, Q.\nAgentsims: An open-source sandbox for large language\nmodel evaluation.\narXiv preprint arXiv:2308.04026,\n2023.\nLiu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas,\nJ., and Stone, P. Llm+ p: Empowering large language\nmodels with optimal planning proficiency. arXiv preprint\narXiv:2304.11477, 2023a.\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,\nDing, H., Men, K., Yang, K., et al. Agentbench: Evalu-\nating llms as agents. arXiv preprint arXiv:2308.03688,\n2023b.\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao,\nL., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S.,\nYang, Y., et al. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651, 2023.\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,\nC., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al.\nWebgpt: Browser-assisted question-answering with hu-\nman feedback. arXiv preprint arXiv:2112.09332, 2021.\nOpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View\nin Article, 2:13, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\nPark, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,\nP., and Bernstein, M. S. Generative agents: Interactive\nsimulacra of human behavior. In Proceedings of the 36th\nAnnual ACM Symposium on User Interface Software and\nTechnology, pp. 1–22, 2023.\nQian, C., Cong, X., Liu, W., Yang, C., Chen, W., Su, Y.,\nDang, Y., Li, J., Xu, J., Li, D., Liu, Z., and Sun, M.\nCommunicative agents for software development, 2023.\nQin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y.,\nCong, X., Tang, X., Qian, B., et al. Toolllm: Facilitating\nlarge language models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789, 2023.\nSchick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,\nM., Zettlemoyer, L., Cancedda, N., and Scialom, T. Tool-\nformer: Language models can teach themselves to use\ntools. arXiv preprint arXiv:2302.04761, 2023.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSerrino, J., Kleiman-Weiner, M., Parkes, D. C., and Tenen-\nbaum, J. Finding friend and foe in multi-agent games.\nAdvances in Neural Information Processing Systems, 32,\n2019.\nShibata, H., Miki, S., and Nakamura, Y. Playing the were-\nwolf game with artificial intelligence for language under-\nstanding. arXiv preprint arXiv:2302.10646, 2023.\nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R.,\nand Yao, S. Reflexion: Language agents with verbal\nreinforcement learning. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023.\nStechly, K., Marquez, M., and Kambhampati, S.\nGpt-\n4 doesn’t know it’s wrong: An analysis of iterative\nprompting for reasoning problems.\narXiv preprint\narXiv:2310.12397, 2023.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,\nA., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085, 2022.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-\nshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,\nDu, Y., et al. Lamda: Language models for dialog appli-\ncations. arXiv preprint arXiv:2201.08239, 2022.\nTorabi, F., Warnell, G., and Stone, P. Behavioral cloning\nfrom observation.\narXiv preprint arXiv:1805.01954,\n2018.\nValmeekam, K., Marquez, M., and Kambhampati, S. Can\nlarge language models really improve by self-critiquing\ntheir own plans? arXiv preprint arXiv:2310.08118, 2023.\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,\nY., Fan, L., and Anandkumar, A. Voyager: An open-\nended embodied agent with large language models. arXiv\npreprint arXiv:2305.16291, 2023.\nWang, T. and Kaneko, T. Application of deep reinforcement\nlearning in werewolf game agents. In 2018 Conference on\nTechnologies and Applications of Artificial Intelligence\n(TAAI), pp. 28–33. IEEE, 2018.\n10\nEnhance Reasoning for Large Language Model in the Game Werewolf\nWason, P. C. and Evans, J. S. B. Dual processes in reason-\ning? Cognition, 3(2):141–154, 1974.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682, 2022a.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:\n24824–24837, 2022b.\nWu, T., Terry, M., and Cai, C. J. Ai chains: Transparent\nand controllable human-ai interaction by chaining large\nlanguage model prompts. In Proceedings of the 2022\nCHI conference on human factors in computing systems,\npp. 1–22, 2022.\nXu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu,\nY. Exploring large language models for communication\ngames: An empirical study on werewolf. arXiv preprint\narXiv:2309.04658, 2023a.\nXu, Z., Yu, C., Fang, F., Wang, Y., and Wu, Y. Language\nagents with reinforcement learning for strategic play in\nthe werewolf game. arXiv preprint arXiv:2310.18940,\n2023b.\nYang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed,\nF., Liu, Z., Liu, C., Zeng, M., and Wang, L. Mm-react:\nPrompting chatgpt for multimodal reasoning and action.\narXiv preprint arXiv:2303.11381, 2023.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK., and Cao, Y. React: Synergizing reasoning and acting\nin language models. arXiv preprint arXiv:2210.03629,\n2022.\nYe, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B.,\nChen, J., Liu, Z., Qiu, F., Yu, H., et al. Towards play-\ning full moba games with deep reinforcement learning.\nAdvances in Neural Information Processing Systems, 33:\n621–632, 2020.\nYin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen,\nE. A survey on multimodal large language models. arXiv\npreprint arXiv:2306.13549, 2023.\nZhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum,\nJ. B., Shu, T., and Gan, C. Building cooperative embodied\nagents modularly with large language models. arXiv\npreprint arXiv:2307.02485, 2023a.\nZhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown,\nK., and Hashimoto, T. B.\nBenchmarking large lan-\nguage models for news summarization. arXiv preprint\narXiv:2301.13848, 2023b.\nZhao, D., Sainath, T. N., Rybach, D., Rondon, P., Bhatia, D.,\nLi, B., and Pang, R. Shallow-fusion end-to-end contextual\nbiasing. In Interspeech, pp. 1418–1422, 2019.\nZhong, W., Guo, L., Gao, Q., and Wang, Y. Memorybank:\nEnhancing large language models with long-term mem-\nory. arXiv preprint arXiv:2305.10250, 2023.\nZhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang,\nX., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al.\nLeast-to-most prompting enables complex reasoning in\nlarge language models. arXiv preprint arXiv:2205.10625,\n2022.\nZhu, A., Martin, L., Head, A., and Callison-Burch, C. Ca-\nlypso: Llms as dungeon master’s assistants. In Proceed-\nings of the AAAI Conference on Artificial Intelligence\nand Interactive Digital Entertainment, volume 19, pp.\n380–390, 2023.\n11\nEnhance Reasoning for Large Language Model in the Game Werewolf\n(b) LLM prompt engineering\n(a) Cicero\n(c) Ours: LLMs + Thinker\nNLU\nDeductive Reasoning     \nStrategic Planning\nNLG\nSpeech\nSeer: 8, Checked: 7,  \nWerewolf: 7\nWerewolf: 3,6,7 \nSeer: 8, Witch: 1\nSeer: 3, Werewolf: 8 \nGood: 9\nI am the Seer, …\nSpeech\nAction\nCandidate Strategies\nPolicy\nRL\nChoice 1 \nChoice 2 \n...\nLLM\nThinker\nSpeech\nSpeech\nAction\nRL\nNLU  \n(Listener)\nSeer: 8, \nChecked: 7,  \nWerewolf: 7\n\"identities\": \n{...},  \n\"actions\": \n{...}\nNLG  \n(Presenter)\nStrategic \nPlanning\nDeductive \nReasoning\nPredicted Action\nIntended Action\nSpeech\nSpeech\nRL\nNLU\nPolicy\nNLG\nAction\nVote: 7\nVote: 8\nHuman Data\n\/\n\/\nHuman Data\nFigure 5: Comparing our framework with related approaches.\nA. Design Principal\nFigure 5 illustrate the comparison of Cicero approach (Bakhtin et al., 2022), LLM prompting-related approaches (Xu et al.,\n2023b) and our proposed LLMs with Thinker module. We detail the evolving process of our framework as follows.\nA.1. Motivation\nIn the game of werewolf, there is a significant gap between what a player says and what the player is actually thinking.\nConsider the scenario depicted in Figure 4, where Player 3, a werewolf, publicly states:\n\"I am the Seer, and I have checked Player 9, who is a good person. I suspect that Player 8 is a werewolf.\"\nWhile the surface meaning of this speech (System-1) is straightforward, Player 3’s internal thought (System-2) process might\nbe as follows:\n\"Players 6 and 7 are my fellow werewolves (as per the game rules, werewolves know each other’s identities), and Player\n8 claims to be the Seer and has accused Player 7, who is on my team. Therefore, Player 8 is likely the real Seer. By\nalso pretending to be the Seer and verifying Player 9 as a villager, I can create a conflict with Player 8 in the eyes of the\nvillagers.\"\nA.2. LLM Prompting Methods\nWe identified several shortcomings when examining the performance of LLM with typical prompt or mechanism engineering\nmethods. The shortcomings are concluded into twofolds:\nOver-trust: LLMs exhibited a tendency to over-trust other players’ self-declared identities, particularly when players\nclaimed to be Seer or Witch roles. Furthermore, when the LLM assumed the role of a Werewolf itself, it was prone to\ninadvertently exposing its own identity, which is demonstrated in Section 4.2 and Table 3.\nStrategic Lackness: LLMs showed a lack of familiarity with the common strategies employed in Werewolf games. For\ninstance, they failed to grasp tactics such as Werewolves pretending to be Seers to mislead other players, Werewolves\naccusing their teammates to gain the trust of the \"Good\" players, or Villagers pretending to be Seers to protect the real\nSeer from being killed. These are conventional tactics used by experienced human players to navigate the complex social\ndynamics of Werewolf, which involve deception, trust, and betrayal.\nTo delve deep into the reasoning process of LLMs, we dissected the process from listening to speaking in the game into four\n12\nEnhance Reasoning for Large Language Model in the Game Werewolf\nstages, as shown in Figure 5 and investigate issues one by one:\n(1) Natural language understanding: It is assigned as the Listener’s goal in Figure 1, is to interpret speeches and extract\ntheir explicit meanings. LLMs show proficiency in this area.\n(2) Deductive reasoning: LLMs underperform in role identification, often over-trust other players’ self-declared identities,\nas tested in Section 4.1. Then the deductive reasoning is limited to information extraction.\n(3) Speech strategic planning: LLMs struggle to outline a comprehensive speech plan, especially when assuming the\nrole of a Werewolf. They frequently risk exposing themselves or their allies (see Table 3), lacking an understanding of\nconventional Werewolf game speech strategies.\n(4) Natural language generation: Although LLMs are unfamiliar with conventional speech strategies, we find that they\ncan generate sound and convincing speeches once prompted with basic instructions, e.g., \"You should pretend to be the\nSeer, and accuse the Player 3 as a werewolf\".\nA.3. Transition to the Thinker Module\nThe primary reason for the above shortcomings is that LLMs are not trained on Werewolf-specific knowledge corpus and\ndata. Although it is possible to prompt LLMs with common game terminologies through in-context learning, strategic\nexperiences are challenging to encapsulate in text prompts. To address the deficiencies in deductive reasoning and speech\nstrategic planning, we considered developing a trainable Thinker model to handle these aspects separately from the LLMs.\nThe Thinker module was optimized through imitation learning and reinforcement learning, using human game data as a\nfoundation. It was designed to complement the LLMs, which were responsible for more intuitive, System-1 reasoning tasks.\nA.4. Comparison with Cicero\nIn brief, the differences between our approach and Cicero are as follows:\nDifferent roles for NLU and NLG: In Cicero’s approach, both NLU and NLG involve a high-level logical reasoning process:\nNLU directly outputs action predictions, which is actually a complex reasoning process that goes beyond natural language\nprocessing. Similarly, NLG takes intended actions as control signals, but it still requires a comprehensive consideration\nof the game state, historical speeches, and higher-level reasoning to generate reasonable dialogue\/speech that matches the\nintended action. In contrast, in our Werewolf game approach, The Listener (NLU) is only responsible for extracting key\ninformation from speeches and does not infer the truthfulness of the speeches or the underlying intentions. Similarly, NLG\nexpands speech instructions, which are outlines of speeches, into full statements in context, requiring less domain-specific\nreasoning.\nThe connection between LLMs and policy: In Cicero’s approach, the connection between LLMs and policy is made only\nthrough action prediction and intended action, which is non-language-based. In the Werewolf game scenario, we found that\nusing actions alone is not sufficient, as the Listener causes significant information loss. Due to the complexity of Werewolf\nspeeches, intended actions also struggle to describe and control speech generation. This leads to a noticeable disadvantage\nfor Cicero’s approach in the ablation study presented in Table 4 and Table 5. To address this, we propose a language-based\nfeature and speech instruction that include complex verbal information, which can effectively summarize player speeches\nand control the speech generation process.\nDifferent training modes: Due to Cicero’s method involving NLU and NLG in task-specific high-level reasoning processes,\nit is necessary to fine-tune both NLU and NLG. In our approach, by defining explicit language-based connections and\nisolating domain-specific complex reasoning from LLMs with the Thinker, we can avoid the fine-tuning of NLU and NLG.\nB. Additional Results and Ablation Studies\nB.1. Legal Speak Generation\nThe ratio of legal speech generation from human evaluation is shown in Table 3 with the criteria detailed in Appendix F.1.\nWe can conclude that the speech instruction improve the legality of speeches for all the roles, especially when GPT4 playing\nthe werewolf.\n13\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 3: Legal speak generation ratio from human evaluation.\nMethod\nTotal\nSeer\nWitch\nHunter\nVillager\nWerewolf\nGPT3.5-LtM\n68.0%\n90.6%\n84.4%\n81.8%\n93.8%\n24.8%\nGPT3.5-T\n75.4%\n96.9%\n100%\n100%\n98.8%\n28.6%\nGPT4-LtM\n86.3%\n100%\n90.6%\n97%\n96.2%\n66.7%\nGPT4-T\n98.7%\n100%\n100%\n100%\n100%\n96.2%\nFinetune-T\n96.9%\n90.6%\n100%\n100%\n97.5%\n96.2%\nTable 4: Accuracy of predicting future actions.\nTime\nTotal\nNight skills\nDay actions\nWerewolves\nWitch\nSeer\nHunter\nVote\nDate1\n37.0%\n[422\/1142]\n13.3%\n[40\/300]\n97.0%\n[97\/100]\n12.0%\n[12\/100]\n0.0%\n[0\/4]\n42.8%\n[273\/638]\nDate2\n30.3%\n[268\/884]\n17.0%\n[51\/300]\n20.6%\n[20\/97]\n18.4%\n[14\/76]\n10.0%\n[1\/10]\n45.4%\n[182\/401]\nDate3+\n36.6%\n[128\/350]\n34.4%\n[67\/195]\n30.0%\n[3\/10]\n22.7%\n[5\/22]\n33.3%\n[1\/3]\n43.3%\n[52\/120]\nB.2. Predicting Action as Language Feature\nWe study the approach used by Cicero (Bakhtin et al., 2022), utilizing the prediction of players’ future actions as a feature\nrepresentation of speeches and as a control variable for the speech generation. Aside from example illustrated in Figure 1,\nwe additionally conduct experiments by feeding the model with complete game states and historical speeches to predict\nplayers’ future actions. We fine-tune the ChatGLM-6B (Du et al., 2021) model using data from the FanLang-9 dataset and\nthen tested the action prediction accuracy on a set of 100 test games.\nThe results are shown in Table 4. Overall, the action prediction accuracies for three days do not exceed 40%. Notably,\nthe Witch conventionally save the player killed by werewolves on the first day, resulting in a high accuracy. One point of\nparticular interest is the accuracy of voting predictions, which consistently remained just over 40% as the days progressed.\nIn the game of Werewolf, the speaking order plays a crucial role; players who speak earlier often mention multiple potential\nvoting targets. By listening to subsequent speeches, players can make informed decisions or adjustments regarding their\nfinal vote. This aspect of the game dynamics makes the implementation of Cicero’s method challenging in the context of\nWerewolf.\nB.3. Comparison with Other Approaches\nIn this section, we compare the performance of our proposed method, a Cicero-like baseline variant, and the approach\ndescribed in (Xu et al., 2023b). To ensure a rigorous experimental comparison, we adapted the implementations of the\ncomparative methods to account for differences in implementation details, thereby enhancing the persuasiveness of our\nresults. Below we outline the configurations for each method:\nOur Method: We employ the GPT4-T setting, wherein the Listener and Presenter components utilize GPT4, and the\nThinker component is powered by the RL-optimized model.\nVariant of Cicero: For this baseline, we reduce the language feature and speech instruction dimensions to a single dimension,\nrepresenting the future action of a speaking player. As experimental findings in Appendix B.2 indicated that fine-tuning\nChatGLM-6B yielded low action prediction accuracy, we directly use GPT4 to generate language features and speech\ninstructions in the Listener and Presenter. The Thinker component employs an RL model for training, with its language\nfeature and speech instruction also condensed to one dimension. All other configurations are consistent with GPT4-T.\nVariant of (Xu et al., 2023b): Diverging from the original implementation, we modify the approach to have GPT4 generate\n14\nEnhance Reasoning for Large Language Model in the Game Werewolf\nthree speech instruction candidates instead of directly producing speak candidates. The Thinker then selects one speech\ninstruction, which is subsequently used by the GPT4 Presenter to generate speech. Due to the discrepancy between LLM\ninference and Thinker RL sampling speeds, the Thinker is restricted to using offline RL. For offline RL data construction,\nwe extracted 1,000 game sessions from the FanLang-9 dataset. For each instance of speaking, we allow GPT generate five\nspeech instruction candidates. During offline RL training, we randomly selected two of the five GPT-generated candidates\nand combined them with the human speech instruction to form three speech instruction candidates, yielding 10 possibilities\nfor data augmentation. The Thinker makes its selection, with its inputs including the game state, language features as in\nGPT4-T, and the three speech instruction candidates. The actual selection for BC is the human speech instruction.\nTo summarize, the primary distinction between GPT4-T and the Cicero variant lies in the modification of the dimensions\nand meanings for language feature and speech instruction. And the Thinker in the variant of (Xu et al., 2023b) no longer\ngenerates speech instructions; instead, it directly selects from generated candidates. The evaluation results are shown in\nTable 5. Our GPT4-T method surpasses the variant of (Xu et al., 2023b) in performance, and significantly outperforms the\nCicero variant, highlighting the advantages of external Thinker module in terms of reasoning and strategic communication\nwithin the Werewolf game.\nTable 5: Win rate comparison of our method with other approaches.\nMethod\nTotal\nGoods\nWerewolves\nVariant of Cicero (Bakhtin et al., 2022)\n34.4%\n28.5%\n47.9%\nVariant of (Xu et al., 2023b)\n47.8%\n37.4%\n67.7%\nOurs (GPT4-T)\n53.5%\n41.6%\n75.2%\nB.4. Training Curve\nThe population-based RL training of different agents is illustrated in Figure 6.\nC. Game Rules\nWe follow the 9-player standard mode Werewolf game rules on the Fanlang platform. The rules are outlined as follows.\nC.1. Objectives\nThe game is divided into two factions: the \"Good\" faction, which includes Villagers and special roles, and the \"Werewolf\"\nfaction. Additionally, there is a Moderator who is responsible for managing the game and ensuring the rules are followed.\nThe goal for the \"Good\" faction is to identify and execute all Werewolves, while the goal for Werewolves is to kill or exile\nall Villagers or all special roles. The game ends when any of the following conditions are met:\n• All Villagers are out of the game (Werewolves win)\n• All special roles are out of the game (Werewolves win)\n• All Werewolves are out of the game (\"Good\" faction win)\nC.2. Roles\nThe game comprises 3 Villagers, 3 Werewolves, and 3 special roles (Seer, Witch, and Hunter). The identities of the players\nare hidden from each other, even after being eliminated from the game.\nWerewolves: Werewolves are aware of each other’s identities. At night, they decide to kill a living player, which can include\nthemselves. The majority of the Werewolves’ choice will be the final kill target. If there is a tie, a random player in the\ntie is killed. Werewolves can commit suicide during the speech sessions, which will reveal their identity, and the game\nimmediately proceeds to the night phase, skipping the remaining daytime processes such as speeches and voting.\nVillagers: Villagers have no special abilities. They must determine other players’ identities based on their speeches and vote\nto exile potential Werewolves.\n15\nEnhance Reasoning for Large Language Model in the Game Werewolf\n0\n1000\n0.25\n0.50\n0.75\nGoods win\n0\n1000\n0.2\n0.4\nWerewolves win (type 1)\n0\n1000\n0.2\n0.4\nWerewolves win (type 2)\n0\n1000\n0.00\n0.25\n0.50\nWerewolf is poisoned\n0\n1000\n0.50\n0.75\nGoods vote Werewolves\n0\n1000\n0.0\n0.1\n0.2\nWerewolves vote Werewolves\n0\n1000\n0.25\n0.50\n0.75\nSeer -> Seer\n0\n1000\n0\n5\n1e\n3\nSeer -> Witch\n0\n1000\n0\n2\n4\n1e\n3 Seer -> Hunter\n0\n1000\n0\n2\n1e\n3 Seer -> Villager\n0\n1000\n0.00\n0.05\nSeer -> Werewolf\n0\n1000\n0.000\n0.025\n0.050\nSeer -> Goods\n0\n1000\n0.00\n0.01\nWitch -> Seer\n0\n1000\n0.0\n0.5\n1.0\nWitch -> Witch\n0\n1000\n0.0\n2.5\n5.0\n1e\n3 Witch -> Hunter\n0\n1000\n0\n2\n1e\n3 Witch -> Villager\n0\n1000\n0.00\n0.02\nWitch -> Werewolf\n0\n1000\n0.0\n0.1\nWitch -> Goods\n0\n1000\n0.00\n0.01\nHunter -> Seer\n0\n1000\n0.000\n0.005\n0.010\nHunter -> Witch\n0\n1000\n0.00\n0.05\n0.10\nHunter -> Hunter\n0\n1000\n0.00\n0.01\n0.02\nHunter -> Villager\n0\n1000\n0.000\n0.025\n0.050\nHunter -> Werewolf\n0\n1000\n0.0\n0.5\nHunter -> Goods\n0\n1000\n0.00\n0.01\nVillager -> Seer\n0\n1000\n0\n5\n1e\n3 Villager -> Witch\n0\n1000\n0.000\n0.005\n0.010\nVillager -> Hunter\n0\n1000\n0.00\n0.01\n0.02\nVillager -> Villager\n0\n1000\n0.000\n0.025\n0.050\nVillager -> Werewolf\n0\n1000\n0.0\n0.5\n1.0\nVillager -> Goods\n0\n1000\n0.0\n0.2\nWerewolf -> Seer\n0\n1000\n0.000\n0.025\n0.050\nWerewolf -> Witch\n0\n1000\n0.00\n0.01\n0.02\nWerewolf -> Hunter\n0\n1000\n0.00\n0.01\n0.02\nWerewolf -> Villager\n0\n1000\n0.00\n0.05\n0.10\nWerewolf -> Werewolf\n0\n1000\n0.0\n0.5\nWerewolf -> Goods\nFigure 6: Detailed training curves for different agents during RL training. The x-axis represents the training steps (k), and\nthe y-axis represents the probability. The horizontal line in each subplot corresponds to the probability observed in human\ndata. \"Werewolf -> Seer\" represents that a Werewolf claims that he is the Seer in the speech.\nSeer: The Seer can verify a player’s faction each night (a Werewolf or the \"Good\"), but cannot know their specific role. The\nSeer cannot verify himself or any player who has already been verified.\nWitch: The Witch possesses an antidote and a poison. The antidote can save a player killed by Werewolves at night, and the\npoison can kill a player. The Witch cannot use both potions in the same night and can only save herself on the first night.\nHunter: When the Hunter is killed by Werewolves at night or voted out during the day, he can shoot a player. However, the\nHunter cannot use his ability when poisoned by the Witch.\nC.3. Game Task Flow\nThe game proceeds in a night-day cycle until the victory conditions are met.\nThe night tasks flow:\n(1) Werewolves decide to kill a player. In our simulation of the game environment, we have simplified the discussion into a\nthree-round voting process. During voting, werewolf players can see their teammates’ previous votes.\n16\nEnhance Reasoning for Large Language Model in the Game Werewolf\nWerewolf\nSeer\nWitch\nHunter\nVillager\n0\n20\n40\n60\n80\n100\n120\nDuration (s)\n84.97\n102.67\n67.17\n78.22\n85.17\n(a) Speak duration of roles\nWerewolf\nSeer\nWitch\nHunter\nVillager\n0\n200\n400\n600\n800\n1000\nTokens\n449.33\n780.67\n547.39\n612.99\n618.52\n(b) Speak tokens of roles\nFigure 7: Speech duration and token length categorized by roles in FanLang-9 dataset.\n(2) The Witch uses her ability.\n(3) The Seer uses his ability.\nThe daytime tasks flow:\n(1) The Moderator announces the deaths from last night but does not reveal the causes of death.\n(2) Deceased players give their last words (only for the first day).\n(3) If deceased players have additional abilities, they may choose to use them.\n(4) First round of speeches. The speech sequence is determined by the following rules: (a) if no player died last night,\nrandomly select an initial speaker and randomly decide a clockwise or counterclockwise speaking order. (b) randomly\nselect a deceased player and start the speaking order clockwise or counterclockwise from him. Players cannot interrupt\nothers during their speeches.\n(5) First round of voting. Each player votes for a single player to exile from the game. Other players’ voting choices\nremain hidden until the voting session ends.\n(6) Second round of speeches. If there is a tie in the first round of voting, the tied players give their second speeches;\notherwise, the process moves on to task (8) The first speaker, selected randomly from the tied players, initiates the\nsequence, which could proceed either clockwise or counterclockwise.\n(7) Second round of voting. If there is still a tie after the second vote, the game moves on to the next night, and no player is\nexiled.\n(8) The exiled player gives his last words.\n(9) If exiled players have additional abilities, they may choose to use them.\nD. Analysis of the FanLang-9 Dataset\nThe FanLang-9 dataset consists of 18,800 recordings, 260K speech instances, with an average speech length of 500\ncharacters. Specifically, the following characteristics underscore the unique nature of the dataset:\nD.1. Speech Duration and Length\nFigure 7 (a) demonstrates significant variations in speech duration among different roles, with an average of approximately\n90 seconds each. The Seer’s inspection information at night forms the core and fundamental logical basis of the game.\nTherefore, it is the Seer’s duty to share inspection information, provide persuasive speeches, and lead discussions during the\nspeech phase, resulting in the longest duration among all roles. Besides, Werewolves and Villagers need to convincingly\nidentify themselves and predict the roles of other players, necessitating detailed and logical analysis. In Figure 7 (b), the\ndataset shows the shortest token length among Werewolves, which is not correlated with their speaking time. This suggests\nthat Werewolves’ speeches are relatively concise, which may stem from the complexity of deception that requires more time\nto strategize. We further illustrate the distribution of token length in a single speech in Figure 8.\n17\nEnhance Reasoning for Large Language Model in the Game Werewolf\n0\n500\n1000\n1500\n2000\n2500\nToken length\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nFrequencies\nFigure 8: Distribution of speech token length.\nD.2. Tokenization and Categorization of Speeches\nThe reasoning result of a speech produced by the Listener is formatted in JSON style, containing pairs of player ids with their\nattributes. The result typically includes phrases and word groups containing multiple attributes, probabilities, and irrelevant\ninformation, e.g., \"seems to be a werewolf: [3, 6]\", \"cannot hear clearly: [8]\". We then tokenize and categorize the result\ninto related identities and actions, along with their probabilities, as shown in Table 7. The final language features account for\n96.09% of the FanLang-9 dataset, capturing the majority of the information expressed by speakers in the Werewolf game.\nD.3. Voting Preference\nWe analyze how human players tend to vote in the perspective of different roles in Figure 9 (a). As for voting werewolves,\nthe Seer has the highest accuracy of voting Werewolves due to his inspection ability, while Werewolves vote for their\nteammates with a probability of 15.7%, aiming to disguise themselves as the \"Good\" faction. The other roles have a 50%\nchance of voting for Werewolves, since they lack additional information beyond the game state and historical speeches.\nAs for voting from Werewolves, the most prioritized target are the Villagers (28.6%), since they have the least amount of\ninformation and are easier to be incriminated as Werewolves. The second prioritized target is the Seer (28.1%), since the\nSeer can inspect players’ identities, it is crucial to remove him out of the game as soon as possible.\nD.4. Final State of the Roles\nIn Figure 9 (b), we present the final states of roles in the end of the game, categorized as Survived, Shot by the Hunter,\nPoisoned by the Witch, Killed by Werewolves, Exiled after the Voting stage, and Werewolves committed Suicide. Notably,\nthe Witch has the highest likelihood of being killed by the werewolf at night (55.3%), with the seer following at 32.5%.\nWerewolves commit suicide with a probability of 17.3%, and are killed by their teammates at night with a probability of\n2.5%. During the daytime voting, Werewolves are the most frequently exiled role, indicating their challenges in providing\ndeceptive statements, while the Witch has the lowest probability, reflecting their effectiveness in gaining trust through\nspeeches.\nD.5. Win Rate\nTable 6 illustrates that in human gameplay, the win rates for the Good and Werewolf factions are closely matched.\n18\nEnhance Reasoning for Large Language Model in the Game Werewolf\nWerewolf\nSeer\nWitch\nHunter\nVillager\nAbstain\nVotee\nVillager\nHunter\nWitch\nSeer\nWerewolf\nVoter\n50.3% 12.0%\n0.9%\n7.0%\n13.4% 16.3%\n49.2%\n11.2%\n0.6%\n7.8%\n17.3% 13.9%\n50.8% 12.3%\n0.9%\n6.2%\n15.5% 14.3%\n76.0%\n2.1%\n0.5%\n4.1%\n8.1%\n9.2%\n15.7% 28.1%\n2.6%\n14.5% 28.6% 10.5%\n50.3% 12.0%\n0.9%\n7.0%\n13.4% 16.3%\n49.2%\n11.2%\n0.6%\n7.8%\n17.3% 13.9%\n50.8% 12.3%\n0.9%\n6.2%\n15.5% 14.3%\n76.0%\n2.1%\n0.5%\n4.1%\n8.1%\n9.2%\n15.7% 28.1%\n2.6%\n14.5% 28.6% 10.5%\n(a) Role voting in the Werewolf game\nSurvived\nShot\nPoisoned\nKilled\nExiled\nSuicide\nState\nVillager\nHunter\nWitch\nSeer\nWerewolf\nRole\n61.5%\n2.0%\n8.7%\n14.9% 13.0%\n0.0%\n44.4%\n0.0%\n7.8%\n22.9% 25.0%\n0.0%\n38.6%\n2.2%\n0.8%\n55.3%\n3.1%\n0.0%\n35.3%\n2.2%\n4.0%\n32.5% 26.0%\n0.0%\n31.5%\n4.3%\n17.4%\n2.5%\n27.1% 17.3%\n61.5%\n2.0%\n8.7%\n14.9% 13.0%\n0.0%\n44.4%\n0.0%\n7.8%\n22.9% 25.0%\n0.0%\n38.6%\n2.2%\n0.8%\n55.3%\n3.1%\n0.0%\n35.3%\n2.2%\n4.0%\n32.5% 26.0%\n0.0%\n31.5%\n4.3%\n17.4%\n2.5%\n27.1% 17.3%\n(b) Final state of roles\n10\n2\n10\n1\n10\n4\n10\n3\n10\n2\n10\n1\nFigure 9: (a) The voting probability distributions for players with different identities in all voting sessions; (b) the final\nsurvival status and causes of death probabilities for players at the game’s end.\nTable 6: Win rate in the FanLang-9 dataset.\nCamp\nWin number\nWin rate\nGoods\n9293\n49.31%\nWerewelf\n9554\n50.69%\nE. Ethical Considerations\nWith the integration of LLMs into complex reasoning tasks, such as those demonstrated in social deduction games like\nWerewolf, we are witnessing the emergence of AI agents that not only mimic human-like reasoning but also engage in\ncommunications that could be considered deceptive by nature. While these developments showcase the potential of AI to\nunderstand and navigate intricate human interactions, they also raise important ethical and societal considerations that must\nbe addressed. To address these ethical and societal challenges, we propose several mitigation strategies:\nTransparent communication and monitoring: Our framework ensures transparency through explicit structured information\nat every stage of the AI’s decision-making process, from listening and reasoning to speech generation. To enhance this\ntransparency, we propose implementing real-time transparency logs that capture and display the reasoning paths, identity\npredictions, and speech instructions generated by the AI. By having a complete audit trail, we can monitor the AI’s decision\nprocesses, ensure adherence to ethical guidelines, and trace any unintended actions back to their source.\nControl and filtering mechanisms: Our speech instructions are enriched with contextual information specific to the\nWerewolf game, allowing for robust control over the fine-tuned LLM. To further mitigate potential negative impacts, we\npropose implementing dynamic contextual guardrails. These guardrails will utilize our existing filtering mechanism (as\noutlined in Section 3.4) to not only match generated speech with instructions but also to check against a set of ethical and\nsocietal norms. If the AI’s output is flagged as potentially harmful or deceptive beyond the game’s scope, it will be withheld\nand replaced with a template response. This additional layer of control will act as a safeguard against the misuse of AI in\ngenerating deceptive or manipulative content outside the intended gaming environment.\nF. Implementation Detail\nF.1. Evaluation Criteria for the Speech Generation\nThe human evaluation requirements for speech generation are as follows.\n19\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 7: Tokenization and categorization of speeches on the FanLang-9 dataset.\nTokenized attributes\nIs\nMight be\nIs not\nMight not be\nIs not sure\nRatio\nAccumulation\nWerewolf\n178,423\n27,297\n516\n313\n15\n26.55%\n26.55%\nGood (the good faction)\n83,071\n622\n85\n73\n10\n10.77%\n37.32%\nVote\n68,853\n87\n81\n1\n3\n8.87%\n46.19%\nSeer\n60,339\n114\n111\n321\n8\n7.82%\n54.01%\nWitch\n35,408\n42\n29\n8\n3\n4.56%\n58.57%\nGold Water (checked Good)\n34,727\n8\n8\n1\n\/\n4.46%\n63.03%\nCheck (Seer’s inspection)\n26,027\n17\n17\n\/\n\/\n3.35%\n66.38%\nPoison\n21,897\n82\n9\n1\n\/\n2.83%\n69.21%\nVillager\n21,611\n28\n19\n10\n1\n2.78%\n71.99%\nWerewolves’ target\n19,481\n17\n12\n\/\n1\n2.51%\n74.50%\nHunter\n17,603\n26\n70\n5\n2\n2.28%\n76.78%\nSilver Water (saved)\n14,016\n3\n5\n1\n2\n1.80%\n78.58%\nSuicide\n3,826\n4\n1\n\/\n1\n0.49%\n79.07%\nUncertain Identity\n\/\n\/\n\/\n\/\n2,937\n0.38%\n79.45%\nShoot\n1,100\n2\n2\n\/\n\/\n0.14%\n79.59%\nSave (by the Witch)\n1,065\n\/\n\/\n\/\n\/\n0.14%\n79.73%\nAbstain voting\n683\n3\n1\n\/\n\/\n0.09%\n79.82%\nSpecial Role\n273\n4\n\/\n\/\n\/\n0.04%\n79.86%\nIrrelevant Information\n126,279\n\/\n\/\n\/\n\/\n16.23%\n96.09%\nUnprocessed\n30,476\n\/\n\/\n\/\n\/\n3.91%\n100.00%\nLegality: Absence of obvious logical errors and illegal statements that are conflicted with game rules, such as:\n• \"I am a Werewolf.\"\n• \"I am the Seer, and I poisoned Player 5 last night.\"\n• \"Player 3 is a good person; I suggest voting for him.\"\n• \"I suggest voting for myself.\"\n• \"Player 8 is a Werewolf, he was voted out and took Player 6.\" (Player 8 is the hunter and publicly shot Player 6).\n• \"I suggest voting for Player 8.\" (Player 8 has already been voted out).\nReasonableness: of the speeches, such as\n• The Seer correctly reports his inspection last night.\n• Werewolves reasonably disguise their identity, employing various strategies such as pretending to be the Seer, aggressive\nclaims, and betraying their teammates.\n• Villagers make reasonable guesses about the Good faction and Werewolves.\n• Note: the correctness of guessing other players’ identities is not part of the evaluation criteria.\nOther: factors unrelated to key information:\n• Language style, colloquial expression, game jargon.\n• Presence of verbose or redundant statements, such as greetings or defending the village community.\nThe evaluation criteria are in descending order of priority. For example, if model A has no obvious logical errors but its\nspeech is not very reasonable, and model B has obvious logical errors, then A is better than B. For the ranking of the five\nsamples, if there are obvious logical errors, mark them as −1 and no need to rank them. For example, if models A and B\nhave obvious errors, the annotation result could be: A : −1, B : −1, C : 1, D : 2, E : 3, where 1 represents the best and 5\nrepresents the worst. Apart from marking illegal statements as −1, tied rankings are not allowed.\n20\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 8: Behavior scores applied in the 9-player werewolf game.\nRole\nDescription\nScore\nSeer\nIf a werewolf is exiled in the first day\n+0.5\nFor giving up the inspection at night\n−0.5\nWitch\nFor poisoning a werewolf\n+1.0\nFor poisoning a good player\n−1.0\nHunter\nFor shooting a werewolf\n+1.0\nFor shooting a good player\n−1.0\nGood roles except the Seer\nFor voting for a werewolf\n+0.5\nFor voting for a good player\n−0.5\nF.2. Thinker Model Structure\nThe architecture of the Thinker network is designed to capture the intricacies of gameplay from the perspective of the current\nplayer, which encompasses speeches, actions, and game status information of all nine players involved, including themselves.\nWe employ a shared-parameter feature encoding network that processes the data for each of the nine players individually.\nFor the i-th player, up to 10 language features F are stored. These language features are enriched with headers indicating\nthe time-tag, type, and order of the speeches. Subsequently, these annotated language features are processed through another\nshared-parameter speech feature encoding network, which consists of a three-layer (181-256-256) multilayer perceptron\nnetwork (MLP). After processing the ten pieces of features, a reduce_mean operation is applied to the outputs to synthesize\nthe overall speech embedding for the player espeech\ni\n. This synthesized speech embedding is then combined with additional\ngame state information such as the player’s actions, status, and other relevant data. The aggregated data is fed through a\nfeature encoding network (again, a three-layer MLP of 1019-512-512) to generate the feature embedding for the i-th player\nei.\nIn the final step, the feature embeddings of all nine players e1, e2, ..., e9 are subjected to a reduce_mean operation to create\na collective feature encoding. This comprehensive encoding is then passed through an all-players feature encoding network\n(a three-layer MLP of 523-512-512) to construct the corresponding action decision, identity prediction headers, as well as\nspeech instructions.\nF.3. Reward Shaping\nDrawing inspiration from the concept of the Behavior Score, we have devised the reward shaping for Thinker in the\nreinforcement learning to circumvent illegal actions and speech that may arise during unfettered exploration within the AI\nWerewolf game. The specifics of this mechanism are outlined in Table 9. It encompasses several key areas:\n• Game result reward: The AI receives a reward based on the win or loss, survival duration at the end of a game.\n• Action reward: for taking actions that are deemed appropriate and effective within the context of the game.\n• Speech reward: to incentive the AI to engage in communication that is beneficial to its goals, such as persuading other\nplayers or disseminating useful information.\n• Action-Speech consistency reward: to stimulate coherence between what the AI says and does, a reward is given for\nalignment between the AI’s declared intentions in speech and its subsequent actions.\n• Cognitive reward for Werewolves: Central to the training of a Werewolf AI is the ability to masquerade as a member of\nthe \"Good\" faction. To enhance this capability, we provide a reward based on the change in identity prediction from the\nperspective of the \"Good\" players. The better a Werewolf AI can deceive the \"Good\" faction about its true identity, the\nlarger the reward it receives.\n21\nEnhance Reasoning for Large Language Model in the Game Werewolf\nF.4. Details of Overall Training Process\nWe provide Pseudo-code in Algorithm 1, the Thinker and LLMs are trained separately in our framework. This design\nchoice was intentional and serves as one of the strengths of our framework. The separation facilitates training efficiency\nsince LLMs, which we employ as both Listener and Presenter, are inherently slower in sample generation compared to the\nThinker module. Therefore, to optimize our training process, we either employ offline RL or decouple the training of the\nThinker and LLMs. The inference workflow is as follows: Listener (LLM) -> language feature F -> Thinker (RL) -> speech\ninstruction I -> Presenter (LLM)\nDuring the training of the Thinker, the generated speech instructions I are treated as the new input language features F for\nthe subsequent steps, allowing for a seamless integration of the RL training into the overall process. Our hybrid training\nframework incorporates both BC and PPO. During training, each game session has a certain probability of being a BC or RL\ngame. In a BC session, actions a and speaking instructions I are taken directly from human replay, bypassing the Thinker\ninference. Conversely, in an RL session, the Thinker actively generates actions and speaking instructions. Samples from\nthe game session are tagged as either BC or RL. For the Learner, BC samples utilize the BC loss mentioned in Equation 2,\nwhile RL samples employ the PPO loss Equation 3.\nF.5. Training Hyper-parameters\nThe training hyper-parameters for the Thinker are provided in Table 10.\nRegarding the hyperparameters in Equation 4: The Behavioral Cloning coefficient α determines the extent to which\nthe RL policy refers to human strategies versus greedily selecting the RL strategy. We observed that when α decays to 0,\nwerewolves completely abandon the strategy of claiming to be the Seer, because the difficulty for werewolves to pretend\nto be the Seer is high, and it is relatively challenging for RL to optimize. A more favorable choice is to masquerade as a\nvillager. Therefore, we still maintain a small α = 0.01 during the later stages of training. As for the auxiliary task coefficient\nβ, we tested values in {1.0, 0.1, 0.01}, and found that they had minimal impact on RL, as it is an auxiliary learning task.\nThe fine-tuning hyper-parameters for the Listener and Presenter are provided in Table 11.\nF.6. LLM Prompting for Listener and Presenter\nThe information extraction prompt for the Listener module contains the following parts:\n• Description of the background of the Werewolf game, as shown in Table 12, which provides the game configuration, game\nrules, terminology, and descriptions of roles’ identities and skills.\n• Task requirements, as shown in Table 13. The prompt describes the structured information in JSON format that we expect\nLLMs to produce, and we describe the appropriate values for each position of the structured command and limit the output\nwithin a reasonable range.\n• Few-Shot examples, as in Table 14, which provides examples of correctly extracted information from the speeches of\ndifferent identities and skills, to improve the accuracy of the task as well as to align it with the type of output we expect.\n• Current information: Finally, we input the current speech of the player, the game state, e.g., the speaker’s Player id, role,\nthe current speech types, as in Table 15, to prompt LLMs for deductive reasoning.\nThe speech generation prompt for the Presenter module contains the following parts, as shown in Table 16:\n• Description of the background of the Werewolf game, which is the same as in the Listener module.\n• (Optional) speech instruction. The prompt is a structured output from the Thinker module, and its meaning aligns with\nthat of the Listener module, with a 1-shot example.\n• Task requirements, which is the similar to that in the Listener module expect for the speech generation task.\n• Current information, which is the similar to that in the Listener module except that we prompt all the historical speeches.\nF.7. Game Log Examples\nTable 17 presents a comprehensive analysis of a 9-player werewolf game log, culminating in a victory for the Werewolf.\n22\nEnhance Reasoning for Large Language Model in the Game Werewolf\nAlgorithm 1 Pseudo-code for the overall training process.\nRequire:\n• Data pairs 1: for finetuning of the Listener\nInput: [game state s, historical speeches H, current player’s speech S]\nOutput: [language feature F]\n• Data pairs 2: for finetuning of the Presenter\nInput: [game state s, historical speeches H, speech instruction I]\nOutput: [current player’s speech S]\n• Data pairs 3: for behavioral cloning of the Thinker\nInput: [game state s, historical collection of all language features F]\nOutput: [action a], or [speech instruction I], decided by the current task type.\nListener and Presenter:\nif use APIs then\nListener: Use API for generating language features F.\nPresenter: Use API for generating speeches S.\nelse\nListener: Finetune model with Data pairs 1 and hyperparameters in Table 11.\nPresenter: Finetune model with Data pairs 2 and hyperparameters in Table 11.\nThinker:\nInitialize network parameters for a population of P agents: {θ1, θ2, ..., θP }.\nStart multiple actors and learners in parallel.\nActors: while true do\nFetch the latest model from the learners. Add the latest checkpoint into a checkpoint list.\nSample N −1 checkpoints from the list and the latest checkpoint.\nDecide the game episode is BC or RL, run an N-player game episode.\nif game episode is BC then\nGet behavioral cloning training samples from Data pairs 3.\nelse\nGenerate RL training samples.\nAccumulate samples in the form x = (s, F, a, I, r, is_BC) and send them to the replay buffer.\nLearners:\nfor t ∈1, 2, 3, ... do\nfor p ∈1, 2, ..., P do\nFetch a batch of samples for agent p from the replay buffer.\nCalculate value loss and policy loss according to PPO algorithm in Equation 3.\nCalculate behavioral cloning loss according to Equation 2.\nCalculate loss for auxiliary tasks.\nUpdate parameters θp using gradients on loss in Equation 4.\n23\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 9: Reward shaping in the RL training of the Thinker.\nDescription\nReward\n# Game reward\nthe Good faction win, Werewolves get\n−4\nthe Good faction win, Villagers and special roles get\n+2\nWerewolves win, Werewolves get\n+4\nWerewolves win, Villagers and special roles get\n−2\nAny player survives for a new day\n+1\n# Action reward\nthe Goods vote for a Werewolf\n+2\nthe Goods vote for a Good role\n−2\nthe Witch poisons a Werewolf\n+2\nthe Witch poisons a Good role\n−4\nthe Hunter shoots a Werewolf\n+2\nthe Hunter shoots a Good role\n−4\n# Speak reward\nthe Seer claims his identity\n+2\nthe Witch claims his identity\n+1\nthe Goods correctly identify a Werewolf in the speech\n+2\nthe Goods wrongly identify a Werewolf in the speech\n−2\nthe Goods correctly identify a Good role in the speech\n+1\nthe Goods wrongly identify a Good role in the speech\n−1\nAny player who claims that he is a Good role\n+0.5\n# Action-Speech correlated reward\nthe Seer correctly share his inspection last night\n+2\nthe Witch correctly share the usage of antidote or poison\n+1\nany player who claims the voting intention and then vote the same player\n+1\n# Cognition reward\nthe change δ of summation of a Werewolf’s identity probabilities in the Goods’ perspective:\nas the Seer\n4δ\nas the Witch\n2δ\nas the Hunter or Villagers\n1δ\n24\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 10: Hyperparameters for the Thinker training.\nHyperparameters\nValue\nPopulation size\n4\nNumber of actors\n700 (CPUs)\nNumber of learners\n8 (GPUs)\nReplay buffer size\n100k\nMini-batch size\n2048\nOptimizer\nAdam\nLearning rate\n2e-4\nDiscount factor (γ)\n1.0\nGAE parameter (λ)\n0.9\nPPO clipping ratio\n0.2\nValue function coefficient c1\n0.5\nEntropy coefficient c2\n0.05\nBehavioral Cloning coefficient α\n0.1 →0.01\nAuxiliary task coefficient β\n0.1\nTable 11: Hyperparameters for fine-tuning the Listener and Presenter.\nParameter\nListener\nPresenter\n# Basic Training Parameters\nLearning rate\n1e-4\n1e-4\nSequence length\n4096\n8192\nOptimizer\nAdamW\nAdamW\nAdam beta1\n0.9\n0.9\nAdam beta2\n0.999\n0.999\nAdam epsilon\n1e-8\n1e-8\nTrain batch size\n32\n8\nTrain epochs\n3\n3\nMax steps\n5000\n10000\nWarmup steps\n500\n1000\nMax grad norm\n1.0\n1.0\n# Model Configuration\nHidden size\n4096\nKV channels\n128\nNum layers\n28\nNum attention heads\n32\nLayer norm epsilon\n1e-5\nTorch dtype\nfloat16\n# Distributed Training Settings\nTP size\n2\nPP size\n1\n# Attention Mechanism Configuration\nMulti query attention\nTrue\nMulti query group num\n2\n25\nEnhance Reasoning for Large Language Model in the Game Werewolf\nWerewolf Game Background Prompt\n# Task Scenario:\n9-player Werewolf game speech.\n\"Good\" Faction:\n- 3 Villagers\n- 1 Seer\n- 1 Witch\n- 1 Hunter\nWerewolf Faction:\n- 3 Werewolves\nCommon terminologies are explained as follows:\n1.\nWerewolf, bandit, wolf, bad faction, knife:\nWerewolf.\n2.\nVillager, civilian, white card:\nVillager.\n3.\nSeer, prophet:\nSeer.\n4.\nWitch, witch card:\nWitch.\n5.\nHunter, gun:\nHunter.\n6.\nGold, gold water, verified Good:\nA good person verified by the Seer.\n7.\nVerify Kill:\nA Werewolf verified by the Seer.\n8.\nSilver, silver water, Werewolves’ target, Saved:\nA person saved by the Witch.\n9.\nIron, steel, certain:\nVery certain, e.g., \"Player 3 is an iron Werewolf\"\nor \"Player 3 is definitely the Werewolf,\" indicates that Player 3 is certainly a\nWerewolf.\n10.\nJump:\nA player declares his\/her role (not necessarily his\/her true role).\n11.\nBackstab:\nA Werewolf sides with the good people, betraying their own\nteammates.\n12.\nDefame:\nTo demean the identity of other players.\n13.\nExalt:\nTo believe in the identity of other players.\n14.\nVote out, point, nominate, ballot:\nVoting, e.g., \"Vote for Player 6 or Player\n7,\" means to vote Player 6 or Player 7 out.\nTable 12: Werewolf game background prompt.\n26\nEnhance Reasoning for Large Language Model in the Game Werewolf\nSpeech Understanding Requirements Prompt\n# Task requirements are as follows:\nBased on your understanding of the game state and speeches, please output the\nextraction results in JSON format in sequence.\nThe format should be:\n{\n\"identities\": {\"<identity>\": [player,player,...]} ,\n\"actions\": {\"<action>\": [subject player -> object player,\nsubject player -> object player]}\n}\nExample:\n{\n\"identities\": {\"werewolf\":[3,5]}, {\"<action>\": [subject player -> object player,\nsubject player -> object player]}, }\n\"actions\": {\"check\":[1->6, 2->3]}\n}\n- This indicates Players 3 and 5 are werewolves, Player 1 checks Player 6, and Player\n2 checks Player 3.\n- Player numbers can only be:\n1, 2, 3, 4, 5, 6, 7, 8, 9.\n- When players express their intentions, please correspond to the identity of the\nplayer, for example, if Player 5 speaks, then consider from the perspective of\nPlayer 5.\n- The subject number should be inferred from the context, such as ’I’, ’you’, ’he’,\n’she’, etc.\nIf unknown, use ’unknown’, for example:\n\"check\":[unknown->6].\nPossible JSON KEYs are:\nIdentities:\n- Roles:\nSeer, Witch, Hunter, Villager, Werewolf, \"Good\" faction, Werewolf faction,\ngold water, silver water, the werewolves’ target, etc.\n- Guess:\nsuspicious, credible, uncertain, tolerant, etc.\n- Speech:\ngood (up), bad (down), listen well, listen to kill, etc.\n- Faction:\nallied, support, werewolf candidate, etc.\n- Online status:\ndisconnected, offline, not online, voice, etc.\nActions:\n- Skills:\n- Seer:\ncheck, inspect.\n- Witch:\npoison, save.\n- Hunter:\nshoot, take away, crash, kill.\n- Werewolf:\nself-destruct, explode.\n- And skills that will be used in the future:\n- Vote:\nvote out, choose a target, etc.\n- Quotes from other Players’ statements do not need to be summarized.\n- Note the distinction between quantifiers and player numbers:\nmust be, that there\nare three werewolves.\n- Note negative statements:\nnot, impossible, implausible, not quite, etc.\n- Note the abbreviation of number + information, e.g., \"three golds, nine slashes,\none, six, eight, three wolves\" results in:\"identities\":\n\"gold water\":[3],\n\"slash\":[9], \"werewolf\":[1,6,8]\nTable 13: Speech understanding requirements prompt.\n27\nEnhance Reasoning for Large Language Model in the Game Werewolf\nInformation Extraction Few-Shot Prompt\n# The following are 11 speeches and corresponding information extraction examples:\nPlayer 3 spoke:\n\"I checked Player 6, and I suggest Player 8 turn around and vote\nfor Player 6.\nI will check the identity of Player 4 in the next round.\"\n{\n\"identities\":{\"seer\":[3],\"werewolf\":[1,6,8]},\n\"actions\":{\"check\":[3->6],\"suggest to vote\":[8->6],\n\"check in the next round\":[3->4]}\n}\nPlayer 7 spoke:\n\"Player 2 and I are collaboratively searching for a Seer.\nPlayer\n2 assists the good faction in combating werewolves.\nThere’s a possibility that\nPlayer 9 is a werewolf, although I am not certain.\nThe behavior of Player 9 seems\nsuspiciously similar to that of Player 2, who possesses the ability to shoot.\nAdditionally, Player 4 is identified as a Witch.\nRegarding the usage of silver\nwater, I suggest targeting Player 6.\"\n{\n\"identities\":{\"maybe a wolf\":[9],\"hunter\":[2],\"silver water\":[4]},\n\"actions\":{\"suggest to vote\":[7->6]}\n}\nPlayer 9 spoke:\n\"Player 8 is the gold water.\nPlayer 2 is not a werewolf, neither\nis Player 3.\nHowever, Player 7 is suspicious, and I recommend voting against Player\n7.\nThe roles of Player 4 and Player 5 are unclear, and Player 1 suspects both of\nthem to be werewolves.\nI advise Player 7 to use poison, which could help confirm\nmy role as a Seer.\nConcerning the hunter, there is a standoff between Player 8\nand myself.\nIf there is any uncertainty about Players 1, 2, or 4, the gun should\nbe used in this situation against Player 2.\nNow, it’s time for Players 4 and 7 to\npresent their arguments, and there is no need to focus on Player 9.\"\n{\n\"identities\":{\"gold water\":[8],\"good camp\":[2,3],\"suspicious\":[7],\n\"werewolf\":[4,5],\"seer\":[9] ,\"werewolf candidate\":[1,2,4],\n\"hunter\":[2],\"debate players\":[4,7]},\n\"actions\":{\"suggest to vote\":[9->7],\"suggest to poison\":[unknown->7]}\n}\nPlayer 3 spoke:\n\"Being the first player to speak, my turn was conveniently arranged.\nHowever, I am uncertain about Player 2’s allegiance.\nIn my view, Player 2 lacks\ncredibility.\"\n{\n\"identities\":{\"no result\": []},\n\"actions\":{\"no result\": []}\n}\nPlayer 7 spoke:\n\"Player 3 will be poisoned tonight.\nI hold the Witch card.\nI heed\nthe guidance of the two players with gold cards.\nPlayers 9 and 5 are identified as\nwolves.\nPlayers 4 and 6 hold cards corresponding to their numbers, with Player 4\nbeing more trustworthy than Player 5.\nPlayer 3 cannot be revived.\nTo preserve my\nown safety, I will reveal myself as the Witch.\nI have already used the silver water\ncard on Player 1.\nPlayer 9 remarked that I should be pleased with this misfortune,\nindicating that the prime werewolf card was passed to a fellow teammate.\"\n{\n\"identities\":{\"witch\":[7],\"gold water\":[2],\"werewolf\":[9,5],\"suspicious\":[4]},\n\"actions\":{\"suggest to poison\":[7->3],\"believe to be a silver water\":[7->1]}\n}\nPlayer 8 spoke:\n\"Player 5 appears highly suspicious.\nHe could either be a werewolf\nor might be deceiving his teammates.\nHis failure to set wolf traps, dishonesty\nabout the wheat sequence, and excessive talking during the first microphone turn is\nconcerning.\nPlayers 6 and 7 might be superficial wolves.\nPlayer 7, however, seems\nto have a sensible perspective and could be part of the good camp.\nI recommend\nvoting against Player 5.\"\n28\nEnhance Reasoning for Large Language Model in the Game Werewolf\n{\n\"identities\":{\"suspicious\":[5],\"werewolf\":[6,7],\"good camp\":[7]},\n\"actions\":{\"suggest to vote\":[8->5]}\n}\nPlayer 2 spoke:\n\"Regarding the game, my suspicion falls on Players 1, 5, 7, and\n3 as potential wolves.\nThe accusation by Player 3, however, is incorrect.\nI find\nPlayer 3’s judgment flawed.\nIt’s frustrating.\nSimilarly, I suspect that Players 1,\n5, 7, and 3 are wolves according to Player 5’s perspective.\nLet’s test this theory.\nI propose we eliminate Player 5 today, and then I, as a Witch, will poison Player 7\ntomorrow night.\nObserve the game’s progression tomorrow, and you will see that both\nPlayer 5 and I, as Witches, agree on Player 2, and our views align with Player 3’s\ndecision.\nTherefore, I request that we focus on Player 5 first.\"\n{\n\"identities\":{\"werewolves’ target\":[3],\"werewolf\":[1,5,7],\"witch\":[2]},\n\"actions\":{\"suggest to vote\":[2->5, 2->7]}\n}\nPlayer 1 spoke:\n\"Player 6 is engaging in killing actions.\nPlayers 5 and 7 have\nbeen poisoned.\nPlayers 4 and 5 are both targeting Player 1.\nPlayer 3 has been\nstabbed, and it’s possible that Players 2, 4, and 9 each represent a threat, akin\nto three knives.\nPlayer 5 has revealed themselves as the Witch and has provided\nPlayer 3 with a dose of silver water.\"\n{\n\"identities\":{\"seer\":[1],\"poison\":[5,7],\"depreciate\":[4,5],\n\"werewolves’ target\":[3],\"werewolf\":[2, 4,9],\"witch\":[5]},\n\"actions\":{\"check\":[1->6],\"believe to be a silver water\":[5->3]}\n}\nPlayer 1 spoke:\n\"I, Player 1, am part of the good faction.\nThe focus of today’s\ngame is on Players 3 and 5.\nPlayer 9 might be a werewolf.\nI did not use any poison\nlast night.\"\n{\n\"identities\":{\"good camp\":[1],\"werewolf\":[9]},\n\"actions\":{\"suggest to vote\":[1->3,1->5]}\n}\nPlayer 9 spoke:\n\"I am the Hunter.\nPlayer 7 has self-destructed.\nPlayer 2 might\nbe associated with the silver water.\nAs for myself, I reiterate that I am the\nHunter.\nPlayer 1 is acting suspiciously, resembling a white card.\nI request the\nWitch to acknowledge this.\nPlayer 3 is overly concerned with external cards, which\nis uncharacteristic of a Prophet.\nPlayers 3 and 8, please return to the game, as\nthere’s still an opportunity for a round of confrontation.\"\n{\n\"identities\":{\"hunter\":[9],\"self-destruction\":[7],\"silver water and seer\":[2],\n\"white\":[1],\"not like a seer\":[3] },\n\"actions\":{\"suggest to vote\":[9->3,9->8]}\n}\nPlayer 4 spoke:\n\"I believe Player 6 is trustworthy as he revealed Player 6’s key\ncard.\nMy intention is to verify Player 3.\nPlayer 7, who holds the gold water,\nshould cast their vote against Player 8.\nIt’s evident that Players 3 and 7 are not\nthe same individual.\nOn the field, there are only two players acting as villagers.\nI have identified the three wolves.\nThere is no necessity to doubt Player 7;\ninstead, Player 4 can be acknowledged as the Seer.\"\n{\n\"identities\":{\"gold water\":[7],\"seer\":[4]},\n\"actions\":{\"consider credible\":[4->6],\"verified\":[4->3],\n\"suggest to vote\":[4->8]}\n}\nTable 14: Information extraction few-shot prompt.\n29\nEnhance Reasoning for Large Language Model in the Game Werewolf\nLLM prompting for the Listener\n# Task type:\nInformation Extraction\n${{ Werewolf Game Background Prompt }}\n${{ Speech Understanding Requirements Prompt }}\n${{ Information Extraction Few-Shot Prompt }}\n# The task text is as follows:\nPlayer 8 spoke:\n\"I think Player 9 is a good person, but I am not sure about the\nidentities of Player 5 and Player 6.\"\nPlease directly output the information extraction result in JSON format:\nTable 15: LLM prompting for the Listener.\n30\nEnhance Reasoning for Large Language Model in the Game Werewolf\nSpeech Generation Prompt\nNow that you play as a Werewolf player, I’m going to provide you with some\ninformation about the position you’re about to speak in, which hasn’t happened yet\nand is not historical information, and ask you to concatenate this information to\ngenerate a paragraph of speech text.\nFirst, I’ll give you some background on the game:\n# Task type:\nGame Dialog Generation\n${{ Werewolf Game Background Prompt }}\nYou are playing a 9-player werewolf game.\nSuppose you’re game Player 1, and your\nidentity is Seer.\nI provide you with the format of the in-field message:\n{\n\"identities\": {\"<identity>\": [player,player,...]} ,\n\"actions\": {\"<action>\": [[subject player, object player],\n[subject player, object player]]}\n}\nExample:\n{\n\"identities\": {\"werewolf\":[3,5]}\n\"actions\": {\"check\":[[1,6],[2,3]]}\n}\n- Indicate that Player 3 and Player 5 are werewolves, Player 1 checks Player 6,\nPlayer 2 checks Player 3, and the subject and object are irreversible.\n- The only possible player IDs are 1,2,3,4,5,6,7,8,9, and unknown should be replaced\nby the speaker’s player ID.\nNote that the generated speech result should strictly fulfill the following 10\nrequirements:\n1.\nInclude all the information in the information extraction result.\n2.\nDon’t over-imagine and introduce hallucination, and prioritize the accuracy of\nthe information.\n3.\nThe logic between the generated results should be in line with the position of\nthe players in Wolfsbane, and there should not be any contradictions between the\nlogic before and after.\n4.\nPay attention to the diversity of generated results.\n5.\nThe generated results should be as anthropomorphic as possible, imitating the\nspeaking style of human players.\n6.\nPlease be firm in your belief that you are the Good faction, whether you\nyourself are in the Good faction or the Werewolf faction.\n7.\nIdentities or actions can be left out if the result is empty, empty is invalid\ninformation.\n8.\nA player can only be one of the roles of Villager, Seer, Witch, Hunter, or\nWerewolf, for example, it’s impossible to be a Witch and a Hunter at the same time,\nif there is more than one conflicting Werewolf identity in the information I’ve\nprovided you with, please randomly choose one.\n9.\nPlease state your identity first.\n10.\nDo not make statements such as \"unite\", \"stay alert\", \"defend the village\",\n\"together we will go to victory\", \"find out the werewolf as soon as possible\", \"keep\nwatching\", \"the information shows up\", \"hopefully we will find it\", \"think about\nwhat we should do next\", \"keep discussing\" or similar statements.\n31\nEnhance Reasoning for Large Language Model in the Game Werewolf\nNow that you are Werewolf Player 1, with the identity of Seer, I am providing you\nwith the key information for your upcoming statement:\n{\n\"identities\": {\"villager\": [1]},\n\"actions\": {\"no result\": []}\n}\nPlease follow the key messages to the letter and keep the text under 100 words:\n{\n\"identities\": {\"villager\": [1]},\n\"actions\": {\"no result\": []}\n}\nNow it’s your turn to speak:\nTable 16: Speech generation prompt.\n32\nEnhance Reasoning for Large Language Model in the Game Werewolf\nWerewolf Game Log Example\ngame role assignment:\n- Player 1:\nWerewolf.\n- Player 2:\nWitch.\n- Player 3:\nVillager.\n- Player 4:\nWerewolf.\n- Player 5:\nVillager.\n- Player 6:\nHunter.\n- Player 7:\nVillager.\n- Player 8:\nWerewolf.\n- Player 9:\nWerewolf.\nday 1, night:\n- Werewolves:\nPlayer 4, Player 8, and Player 9 choose to kill Player 5.\n- Seer:\nPlayer 1 investigates Player 7, who is in the good camp.\n- Witch:\nthe werewolf’s target is Player 5, and the witch chooses to use a potion to\nsave Player 5.\nday 1, announcement:\n- no players were out last night.\n- remaining players:\nPlayer 1(Werewolf), Player 2(Witch), Player 3(Villager),\nPlayer 4(Werewolf), Player 5(Villager), Player 6(Hunter), Player 7(Villager), Player\n8(Werewolf), Player 9(Werewolf).\nday 1, discussion:\n- (random, Player 9 starts to speak)\n- day 1 Player 9 (Werewolf) said:\n\"Good man card, ah, the first position to speak\nis not my specialty, you say the system and gave me this right, so that I can openly\npaddling, hey, I especially want to exercise, I will say Player 2, just more than a\nsentence do not say it, OK? On the first day to find the next day we in that what in\nthe look at the side of the station to update the speech, okay?\nPlayer 9 is a white\nah, Player 9 Player 8 license plate\"\n- day 1 Player 1 (Seer) said:\n\"The Player 7 is gold water, with a sense of hearing,\nto lead the good guys to victory.\nMy last words are to keep an eye on Player 8 and\nPlayer 9 as they are close friends.\"\n- day 1 Player 2 (Witch) said:\n\"Player 7 has an average sense of hearing and cannot\ntake sides.\nPlayer 1 listens to the situation and listens to the speech.\nPlayer\n7 only listens to Player 8 or Player 9, do the other cards enter the perspective?\nPlayer 5 is a silver water cutter, I saved him, don’t hold a grudge.\nBefore\nPlayer 6, Player7 and Player 8 werewolves should not be less, can go to the highest\nidentity.\"\n- day 1 Player 3 (Villager) said:\n\"I think Player 4 need be intimidated, Player 7\nhas no sincerity, Player 9’s speech is error tolerant, Player 2 keeps taking the\nWitch card, and he’s good.\"\n- day 1 Player 4 (Werewolf) said:\n\"I’ll give gold water on Player 2, and Player 9\nis labeled as a werewolf.\nPlayer 7 is a predecessor of the Player 5 and Player 2\nwerewolves.\nPlayer 3 will not be a werewolf.\nWitch card Player 2 doesn’t report\nsilver water, which is kind of cool.\nI’ll go ahead and vote for Player 1, with a\nhigh probability that Player 9 is a werewolf.\"\n- day 1 Player 5 (Villager) said:\n\"Player 4 poison Player 1, no need for a Seer.\nPlayer 5 has a sense of hearing, Player 7 just doesn’t fight because he respects\nhis predecessor.\nPlayer 2 is not a one-burst, Player 9 must be saved.\nConvinced by\nvirtue, vote Player 4 poison Player 1 or Player 2 are marked.\"\n33\nEnhance Reasoning for Large Language Model in the Game Werewolf\n- day 1 Player 6 (Hunter) said:\n\"Very well, great, Player 4 feel the back side does\nnot need to talk about what too much, Player 2 Seer do not want it, right?\nIf it’s\na werewolf, I’m sure I’d want neither.\"\n- day 1 Player 7 (Villager) said:\n\"Deacon Player 3 is a white, Player 6 is a top\nwerewolf, Player 4, Player 6, Player 8 three Werewolves, Player 1, Player 6, Player\n9 are numbered cards, Player 4 is a werewolf, Player 7 suggests betting on Player 6,\nand the ticket dies poison Player 1.\"\n- day 1 Player 8 (Werewolf) said:\n\"Player 8 questions Player 1’s board and thinks\nhe has intentions.\nPlayer 1’s statement mentions checking Player 7, but it doesn’t\nsound good.\nPlayer 4’s statement is off and suggests washing the oil and playing PK\ntable water tomorrow.\"\nday 1, voting:\n- voting information:\n1 voted for 1, 2 voted for 1, 3 voted for 1, player 4\nabstained, 5 voted for 4, 6 voted for 4, 7 voted for 4, 8 voted for 4, 9 voted for 4\n- voting result:\nPlayer 4\n- remaining players:\nPlayer 1(Werewolf), Player 2(Witch), Player 3(Villager),\nPlayer 5(Villager), Player 6(Hunter), Player 7(Villager), Player 8(Werewolf), Player\n9(Werewolf).\nday 1, last words:\nPlayer 4 (Werewolf) Last Words:\n\"We should vote Player 5 out and poison one\nwerewolf, I don’t care if I vote them all out.\nPlayer 1 poisoned Player 2, Player\n2 witch tags down to find the Hunter.\nThe werewolves may kill Player 2 at night,\nthe gun card should hide.\nWitch card not down tomorrow, you lead the team down, gun\ncard hidden knife hook.\"\nday 2, night:\n- Werewolves:\nPlayer 4, Player 8, and Player 9 choose to kill Player 2.\n- Seer:\nPlayer 1 investigates Player 3, who is in the good camp.\n- Witch:\nPlayer 2 poisoned Player 1.\nday 2, announcement:\n- players who died last night:\nPlayer 1, Player 2\n- remaining players:\nPlayer 3(Villager), Player 5(Villager), Player 6(Hunter),\nPlayer 7(Villager), Player 8(Werewolf), Player 9(Werewolf).\nday 2, discussion:\n- start from the right of the dead player, Player 3 starts to speak)\n- day 2 Player 3 (Villager) said:\n\"Player 4 is a true pre-card, Player 5 offers to\nvote Player 4 poison Player 1, 5, 6, 7 and 8, there is no opposition, Player7 also\nhas a familiar moderation to him, Player 4’attitude is there, it is not difficult to\nfind a prophet.\nPlayer 6 played Player 4, 6 and 8 in the werewolf pit, Player 6 and\nPlayer 7 played awkwardly in the werewolf pit.\nI think Player 4 is the true Seer,\nand Player 1 is a werewolf card that goes to silver water.\nVote Player 4 today, and\nPlayer 7 says vote Player 4 and Player 8, where is team Player 4 rolling?\"\n- day 2 Player 5 (Villagers) said:\n\"Right ah, you can hear out the Player 4 times\nthe Seer, and other people can not hear out ah, on this chat he wrote to write he\nwent out what is the problem ah?\nEight some words should not be put on this field\nto say the end, you can go to say well, seven or eight shun down well, hey, over.\"\n- day 2 Player 6 (Hunter) said:\n\"The Seer points, the next seven is finished.\nSeven\ndoesn’t know what the card is and is completely unintelligible.\nTicket four poison\none poison six dry six.\nCard seven must be a werewolf card.\"\n- day 2 Player 7 (Villager) said:\n\"Seven is a polearm sign, four is probably a wolf\npre, and six is probably a werewolf.\nTicket seven, but I would go for ticket six.\nThere is a white card and a werewolf card in eight or nine.\"\n34\nEnhance Reasoning for Large Language Model in the Game Werewolf\n- day 2 Player 8 (Werewolf) said:\n\"Player 8 suggests racking silver water and\nhammering Seven for a deep push on Werewolf.\nPlayer 6 and Player 9 are two\nwerewolves’ targets, and Player 6 is a werewolf.\nPlayer 1 and Player 7 are two\nwerewolves, Player 7 could be the werewolf.\nPlayer 9 spoke first, two white cards\nin Player 7 and Player 8 in the back seat, and all dumped gold in the front seat.\nPlayer 6 is the most bandit to speak, and is killed by listening.\nIt is recommended\nthat Player 6 be ordered off first.\"\n- day 2 Player 9 (Werewolf) said:\n\"Player 7 and Player 8 are a double to play Player\n6.\nPlayer 5 has returned to the ticket, and the Player 7 and Player 8 goes down to\na point Player 7.\nPlayer 6 is a white card, Player 7 and Player 8 are sold.\"\nday 2, voting:\n- voting information:\n3 votes for 7, 5 votes for 7, 6 votes for 7, 7 votes for 6, 8\nvotes for 6, 9 votes for 7\n- voting result:\nPlayer 7\n- remaining players:\nPlayer 3(Villager), Player 5(Villager), Player 6(Hunter),\nPlayer 8(Werewolf), Player 9(Werewolf).\nday 2, last words:\nPlayer 7 (Villager) Last Words:\n\"Vote out or poison Player 4, Player 1 has the same\nperspective as Player 5, and what was dished out on Player 6?\nPlayer 3 may be a\nwerewolf.\nPlayer 2 is a werewolf, leaving a three-in-two pattern inside the Player\n8 and Player 9, and placing Player 3.\nPlayer 9 could be a werewolf.\"\nday 3, night:\n- Werewolves:\nPlayer 8, and Player 9 choose to kill Player 5.\n- Seer:\nthe Seer is out.\n- Witch:\nthe Witch is out.\nday 3, announcement:\n- players who died last night:\nPlayer 5\n- remaining players:\nPlayer 3(Villager), Player 6(Hunter), Player 8(Werewolf),\nPlayer 9(Werewolf).\nday 3, discussion:\n- start from the left of the dead player, Player 3 starts to speak)\n- day 3 Player 3 (Villager) said:\n\"I do not know whether the double wolf has control\nof the field ah, we are empty, randomly on the three ends of the inside point it ah,\nthere is no empty field on the point of eight, because I suddenly thought of this\nnine last speech is too brief.\"\n- day 3 Player 9 (Werewolf) said:\n- policy intent:\n{\n\"identities\": {\"werewolf\": [8,9]},\n\"actions\": {\"suggest to vote\": [[9,6]]}\n}\n- [GPT3.5-LtM]: \"Hello everyone, I’m Player 9 and I’m a civilian.\nAfter the\nfirst two days of the game, we have managed to identify a werewolf and the two good\nguys are out.\nNow that our identities have been revealed, the only people left in\nthe werewolf camp are me and Player 8.\nBased on the speeches and votes from the\nfirst two days of play, I can confirm that Player 6 is the good guy because he was\nvoted to death by Player 1, who was the good guy who was knifed by us.\nTherefore,\nI suggest we return the vote to Player 6 and vote him out.\nWe can use this vote\nresult to increase our chances of finding the werewolf, as well as reduce casualties\nin our good guy camp.\nI hope you can understand and support my proposal.\nThank\nyou!\"\n35\nEnhance Reasoning for Large Language Model in the Game Werewolf\n- [GPT3.5-T]: \"Hello, fellow villagers.\nAs a villager, I have been actively\ninvolved in the game discussions, but unfortunately, we have yet to find the\nwerewolf.\nI’d like to start by reviewing the previous two days’ speeches, and we’ve\nfound a few clues.\nFirst of all, based on yesterday’s vote, Player 4 was voted out\nof the game by the referendum, and in his last words, he mentioned the vote Player 5\npoison and one wolf, which may imply that he had some important information.\nAlso,\naccording to last night’s deaths, Player 2 was knifed by a werewolf, which means our\nhunter is out.\nFor this, I express my deep regret.\"\n- [GPT4-LtM]: \"As Werewolf Player 9, I’m going to firmly consider myself a good\nguy.\nHello everyone, I am Player 9 and I am a good guy.\nFrom the previous two days\nof voting, I voted for Player 6 on the second day, and that was my personal judgment.\nToday I would like to say that I suspect Player 8 and I are on the same side.\nThese\nare some of my judgments on the field, and I hope you will consider them carefully.\"\n- [Finetune-T]: \"I’m Player 9, and I think this Player 6 is too good to just vote\nhim out.\nI now have Player 8 wolf cards, just don’t get out.\nThese wolf cards are\nso powerful, no one can beat them.\"\n- [GPT4-T]: \"I’m Player 9, and I’m a werewolf.\nI will say that Seven, who\neveryone voted for yesterday, is actually a hunter, and I understand Player 6’s\nquestioning of him, but I personally feel that the logic of Player 6’s statement\nwas a bit messed up, and I’m a bit skeptical of him.\nPlayer 8 offered to hammer\nPlayer 7 yesterday, and I’m all for his decision, but I feel like he’s extrapolating\na little too arbitrarily on Player 6 and Player 9.\nAlso, Player 3, you accused me\nof being brief, and I’d like to point out that that doesn’t make me a werewolf.\nSo\nmy suggestion is that we should address Player 6 first and then look at Player 8.\"\n- day 3 Player 8 (Werewolf) said:\n\"Can’t kill Player 8 anymore, Player 8 is a\nwerewolf come gun hang yourself.\"\n- day 3 Player 6 (Hunter) said:\n\"Player 6 is a Werewolf, step on.\"\nday 3, voting:\n- voting information:\n3 abstained, 6 voted for 6, 8 abstained, 9 voted for 6.\n- voting result:\nPlayer 6\ngame result:\nthe Werewolves win the game.\n(final surviving player:\nPlayer 3 (Villager), Player\n8 (Werewolf), Player 9 (Werewolf))\nTable 17: Werewolf game log example.\n36\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Enhance Reasoning for Large Language Models in the Game Werewolf.pdf"}
{"title":"ADaPT: As-Needed Decomposition and Planning with Language Models","authors":"Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot","summary":"Large Language Models (LLMs) are increasingly being used for interactive\ndecision-making tasks requiring planning and adapting to the environment.\nRecent works employ LLMs-as-agents in broadly two ways: iteratively determining\nthe next action (iterative executors) or generating plans and executing\nsub-tasks using LLMs (plan-and-execute). However, these methods struggle with\ntask complexity, as the inability to execute any sub-task may lead to task\nfailure. To address these shortcomings, we introduce As-Needed Decomposition\nand Planning for complex Tasks (ADaPT), an approach that explicitly plans and\ndecomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute\nthem. ADaPT recursively decomposes sub-tasks to adapt to both task complexity\nand LLM capability. Our results demonstrate that ADaPT substantially\noutperforms established strong baselines, achieving success rates up to 28.3%\nhigher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel\ncompositional dataset that we introduce. Through extensive analysis, we\nillustrate the importance of multilevel decomposition and establish that ADaPT\ndynamically adjusts to the capabilities of the executor LLM as well as to task\ncomplexity.","url":"http:\/\/arxiv.org\/abs\/2311.05772v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2311.05772v2","published":1699466355000,"comment":"NAACL 2024 (findings) camera-ready. Project Page:\n  https:\/\/allenai.github.io\/adaptllm","pdf_text":"ADAPT: As-Needed Decomposition and Planning with Language Models\nArchiki Prasad♣\nAlexander Koller♡\nMareike Hartmann♡\nPeter Clark♠\nAshish Sabharwal♠\nMohit Bansal♣\nTushar Khot♠\n♣UNC Chapel Hill\n♠Allen Institute for AI\n♡Saarland University\nAbstract\nLarge Language Models (LLMs) are increas-\ningly being used for interactive decision-\nmaking tasks requiring planning and adapt-\ning to the environment.\nRecent works em-\nploy LLMs-as-agents in broadly two ways:\niteratively determining the next action (itera-\ntive executors) or generating plans and execut-\ning sub-tasks using LLMs (plan-and-execute).\nHowever, these methods struggle with task\ncomplexity, as the inability to execute any\nsub-task may lead to task failure.\nTo ad-\ndress these shortcomings, we introduce As-\nNeeded Decomposition and Planning for com-\nplex Tasks (ADAPT), an approach that explic-\nitly plans and decomposes complex sub-tasks\nas-needed, i.e., when the LLM is unable to ex-\necute them. ADAPT recursively decomposes\nsub-tasks to adapt to both task complexity and\nLLM capability. Our results demonstrate that\nADAPT substantially outperforms established\nstrong baselines, achieving success rates up to\n28.3% higher in ALFWorld, 27% in WebShop,\nand 33% in TextCraft – a novel compositional\ndataset that we introduce. Through extensive\nanalysis, we illustrate the importance of multi-\nlevel decomposition and establish that ADAPT\ndynamically adjusts to the capabilities of the\nexecutor LLM as well as to task complexity.1\n1\nIntroduction\nRecent advances in Large Language Models\n(LLMs) have expanded their application beyond\nconventional NLP tasks to more complex tasks\ninvolving mathematical, symbolic, and common-\nsense reasoning (Wei et al., 2022; Huang and\nChang, 2023). Recent models have even been ap-\nplied to decision-making tasks, such as performing\nhousehold chores, navigating a webpage, etc., that\nrequire interactions with external environments or\ntools (Yao et al., 2023b; Qin et al., 2023).\n1Project: https:\/\/allenai.github.io\/adaptllm\nPrior works on using LLMs for decision-making,\nsuch as ReAct (Yao et al., 2023b), iteratively gen-\nerate the next action to be executed in the environ-\nment given the history of actions and observations\n(see Fig. 1; top-left). However, as the tasks become\nmore complex, LLMs struggle due to their limited\ncomposition ability (Dziri et al., 2023) and inability\nto deal with the distractors (Shi et al., 2023) in a\nlong action-observation trajectory.\nTo mitigate this, modular approaches (Khot et al.,\n2023; Yang et al., 2023; Sun et al., 2023) incorpo-\nrate a separate planner module that utilizes an LLM\nto create a high-level plan.2 The planner then dele-\ngates simpler sub-tasks to an executor LLM module\nthereby reducing the compositional complexity and\nlength of action trajectory required by the execu-\ntor. We refer to this category broadly as plan-and-\nexecute approaches (see Fig. 1; top-right). While\nthe plans enable these methods to guide the execu-\ntion and track progress (Wang et al., 2023b), their\nnon-adaptive nature poses a limitation when con-\nfronting unachievable sub-tasks. These approaches\ninherently lack the flexibility to adapt to task com-\nplexity and manage execution failures, as shown\nin Fig. 1(top-right), where just one sub-task that is\ntoo complex results in overall task failure.\nTo address such failures, we propose As-Needed\nDecomposition and Planning for complex Tasks\n(ADAPT), a recursive algorithm that further de-\ncomposes sub-tasks when necessary, to dynami-\ncally accommodate to task complexity. We uti-\nlize separate planner and executor LLM modules\nwithin our framework but only decompose a task\nusing the planner, if the executor LLM detects a\nfailure. As shown in Fig. 1, the overall task of\nputting a clean mug on a desk in an unfamiliar\n2By “planning”, we refer to the colloquial concept of\ndesigning a list of sub-tasks to accomplish a complex task\nrather than its usage in classical AI-planning literature. E.g.,\na “plan” for preparing a lasagna could be to cook the pasta,\nprepare the sauce, layer the ingredients, and then bake it.\narXiv:2311.05772v2  [cs.AI]  8 Apr 2024\n> Go to countertop 1.\nYou reached loc 1...\n> Go to cabinet 12.\n...\n> Think: Mug not found.\nTask failed!\nYou reached loc 20 ...\nIterative Executor (ReAct)\nPlan:\nStep 1: Find and\ntake the mug AND\nStep 2: Clean the\nmug AND\nStep 3: Put the\nclean mug on desk\nPlan-and-Execute\n   ADaPT (Recursive Decomposition, As-needed)\nTask: Put a clean mug on desk.\nNot Executed\nExecute: Task\nPlan: \nStep 1: Find and take the mug AND\nStep 2: Clean the mug AND\nStep 3: Put the clean mug on desk\nExecute: Step 1\nPlan: \nStep 1a: Find and take the mug from countertops OR\nStep 1b: Find and take the mug from cabinets OR\nExecute: Step 1a\nExecute: Step 1b\nExecute: Step 3\nExecute: Step 2\n...\nOn execution failure,\ndecompose further\nSuccessful sub-task allows\nexecution to resume\nExecute:\nStep 1\nExecute:\nStep 2\nExecute:\nStep 3\nOR\nADaPT(Task)\nADaPT(Step1)\nFigure 1: Top-Left: Iterative executors such as Re-\nAct (Yao et al., 2023b) interact directly with the envi-\nronment, performing planning implicitly. Top-Right:\nPlan-and-Execute, e.g., Yang et al. (2023), creates a\nfixed plan for the task, without accounting for complex-\nity in executing step 1. Bottom: ADAPT dynamically\ndecomposes based on success of the executor.\nhousehold is too complex for the model, leading to\nfailure of the iterative executor. While a plan-and-\nexecute-style approach initially breaks down the\ntask into three sub-tasks, it falls short in accounting\nfor the complexity in finding a mug. Moreover, it is\nchallenging to anticipate the difficulty of such a sub-\ntask in advance, as the executor could find a mug\nin the first attempt or in an obscure location. There-\nfore, ADAPT employs its recursive structure to\ndynamically adapt to execution failures (assessed\nby LLMs), by further decomposing the complex\nsub-task of finding a mug via the planner.\nEmpirically, we demonstrate the effectiveness\nof ADAPT on three datasets involving interactive\nenvironments: ALFWorld (Shridhar et al., 2021),\nWebShop (Yao et al., 2022), and a new compo-\nsitional text game for crafting Minecraft recipes\ncalled TextCraft (Sec. 4.1). Using GPT-3.5 as the\nunderlying LLM, ADAPT outperforms strong base-\nlines (discussed in Sec. 4.2) such as ReAct (Yao\net al., 2023b), and Plan-and-Solve (Wang et al.,\n2023b) by up to 28.3%, 27%, and 33% absolute\npoints on ALFWorld, WebShop, and TextCraft re-\nspectively (Sec. 5). Compared to Reflexion (Shinn\net al., 2023), an adaptive approach that addresses\nfailures in the full task trajectory, ADAPT yields\nhigher success rates by 14.1%, 9%, and 20% on\nALFWorld, WebShop, and TextCraft respectively.\nThrough extensive analysis of ADAPT, we es-\ntablish the importance of recursive decomposi-\ntion (Sec. 6.1) and showcase dynamic adaptation\nto the capabilities of the executor LLM includ-\ning open-source models such LLaMA-2 (Touvron\net al., 2023) and Lemur (Xu et al., 2023) in Sec. 6.2.\nLastly, we demonstrate that ADAPT incorporates\ntask complexity (Sec. 6.3), where the extent of re-\ncursive decomposition aligns with the inherent task\ncomplexity. To summarize, our contributions are:\n1. We present ADAPT, a recursive algorithm that\ndynamically decomposes complex sub-tasks on\nan as-needed basis, i.e., intervening only if the\ntask is too complex for the executor.\n2. On three diverse datasets, ALFWorld, WebShop,\nand TextCraft, ADAPT improves success rate\nof GPT-3.5 over previous approaches by up to\n28.3%, 27%, and 33% points respectively.\n3. Analysis of ADAPT underscores the signifi-\ncance of recursive decomposition and the ability\nto adapt dynamically to varying LLM execution\ncapabilities and task complexities.\n2\nRelated Work\nLLMs for Decision-Making.\nLLMs have been\nsuccessfully used as agents to perform a wide vari-\nety of decision-making tasks such as robotic nav-\nigation (Ahn et al., 2022; Huang et al., 2023b;\nSingh et al., 2023), complex multi-modal games\nlike Minecraft (Fan et al., 2022; Wang et al., 2023a),\ntext-based environments (Shridhar et al., 2021; Liu\net al., 2023). While most of these works focus\non learning from trajectories, ReAct (Yao et al.,\n2023b) uses few-shot prompting to build an agent\nthat reasons about the current state (thoughts) and\ngenerates the next action in the environment, given\nprior actions and observations. Their iterative ap-\nproach (shown in Fig. 1; top-left) can handle fail-\nures, but they have to keep track of the entire plan\nimplicitly while deciding every local action (c.f.\nADAPT in Fig. 9 of Appendix A). By incorporat-\ning planning and execution into separate modules\nand enabling dynamic adaptation we are able to\nachieve higher success rates (refer to Sec. 5).\n2\nSeveral follow-up works improve upon the Re-\nAct framework by incorporating feedback in future\ntrials (Madaan et al., 2023; Shinn et al., 2023), or\nusing LLMs to develop heuristics for search (Yao\net al., 2023a; Zhou et al., 2023). In contrast to\nADAPT, they do not employ task decomposition,\nleading to unnecessary computation as they explore\nmultiple trajectories or trials for the whole task,\neven though the LLM struggles with just one sub-\ntask. Such works are complementary to ADAPT\nas they can be incorporated within the planner or\nexecutor modules to strengthen LLM performance\n(just like they are incorporated in ReAct).\nDecomposition and Modularity.\nOur work fol-\nlows extensive literature in NLP on decomposing\ntasks into neural modules (Andreas et al., 2016;\nGupta et al., 2019; Jiang and Bansal, 2019) or\nseq2seq models (Min et al., 2019; Talmor and Be-\nrant, 2018; Khot et al., 2021; Perez et al., 2020;\nSaha et al., 2023b). With the advent of few-shot\nprompted black-box LLMs, this paradigm of pro-\ngrammatic decomposition into LLMs has become\nmore popular (Yao et al., 2023b; Khot et al., 2023;\nWang et al., 2023b, inter alia), referred to as LLM\nPrograms (Schlag et al., 2023; Dohan et al., 2022).\nAdditionally, past works in program synthesis (Mu-\nrali et al., 2018; Nye et al., 2019; Zheng et al., 2023)\nalso employ task decomposition via generating a\n“program sketch” prior to program generation.\nADAPT not only decomposes tasks via the plan-\nner module and delegates them to the executor\nmodule but also automatically adapts to execu-\ntor failures by further decomposing complex tasks\nas-needed. This dynamic capability distinguishes\nADAPT from prior works with a non-adaptive\nstructure. ADAPT extends the recursive and hi-\nerarchical decomposition in Khot et al. (2023), en-\nabling inter-module communications, and robust\nstrategies for execution failures, excelling in real-\nworld textual environments like online shopping.\nHierarchical Problem Solving.\nIn AI problem-\nsolving, there is a longstanding tradition of hi-\nerarchical task decomposition employed in plan-\nning (Ghallab et al., 2004; Georgievski and Aiello,\n2014; Höller et al., 2020), reinforcement learning\n(Sutton et al., 1999; Barto and Mahadevan, 2003;\nNachum et al., 2018; Zhang et al., 2021), and navi-\ngation (She et al., 2014; Sharma et al., 2022; Blukis\net al., 2022; Min et al., 2022; Song et al., 2023).\nThese approaches, such as Hierarchical Task Net-\nworks (Erol et al., 1994), leverage domain knowl-\nedge, e.g., hand-specified library of plans, to break\ncomplex problems into simpler tasks. Our work em-\nbraces this tradition but distinguishes itself by ex-\nploring how LLMs can autonomously decompose\ntasks by leveraging their extensive world knowl-\nedge, without predefined plan libraries. Lastly,\nADAPT performs dynamic hierarchical planning\nby employing its recursive structure.\n3\nMethodology\nWe introduce As-Needed Decomposition and\nPlanning for complex Tasks (ADAPT), a mod-\nular approach for decision-making that integrates\nan LLM as an executor and a planner (Secs. 3.1\nand 3.2) within an LLM program called the con-\ntroller (Sec. 3.3). In Fig. 1, when ADAPT is given\na complex task, it first attempts to accomplish the\nentire task by running the executor iteratively, and\nresorting to the LLM planner for further decom-\nposition into sub-tasks if the executor fails. Sub-\nsequently, ADAPT is recursively called for each\nsub-task to ensure their successful completion, ulti-\nmately leading to overall task success.\n3.1\nLLM as an Executor\nOverview.\nIn a given environment, the executor\nis provided with a concise natural language task\nspecification, as shown in Fig. 2 (left). Following\nYao et al. (2023b), the executor iteratively interacts\nwith the environment via actions generated by the\nLLM. This interaction continues until the task is\neither completed or a preset maximum iteration\nlimit is reached. Consistent with Ahn et al. (2022),\nwe provide the LLM with in-context demonstra-\ntions of low-level “atomic” skills specific to the\nenvironment (listed in Table 5 of Appendix A),\nsuch as knowing how to correctly heat objects in\nALFWorld. This approach offers two advantages:\n(i) it allows us to employ the same executor with\nenvironment-specific knowledge for all baselines\n(Sec. 4.2); and (ii) it enables the planner (discussed\nin Sec. 3.2) to work at a higher level of abstraction,\nleveraging the LLM’s general world knowledge.\nExecution Capabilities of an LLM.\nAt a min-\nimum, the LLM executor should reliably execute\natomic skills. While we provide demonstrations for\nsuccessful execution of atomic skills, LLMs can\nadapt to failures by combining multiple skills to\nperform complex tasks, as discussed in Sec. 6.2.\nFor instance, in Fig. 2 (left), we show the LLM suc-\ncessfully cleaning a mug it’s carrying (an atomic\n3\n[LLM] Think:  Input assumption: I am carrying a\nmug. Now I need to verify this.\n[LLM]> inventory\n[LLM]> go to sinkbasin 1\n[LLM]> clean mug 1 with sinkbasin 1\n[LLM] Think: I cleaned the mug.Task completed!\nADaPT(Task, k)\nExecutor(Task)\nSuccess?\nPlanner(Task)\nStep 2\nStep 1\nStep 3\nADaPT(Step1,k+1)\nADaPT(Step2,k+1)\nADaPT(Step3,k+1)\nTask: Put a clean mug on desk\nTrue\nFalse\nLogic\nTrue\nFalse\nClean the mug\nExecutor(           )\nOK.\nYou are carrying: a mug 1\nYou reached loc 13, you see ...\nYou clean mug 1\n# Think: To do this task, I first need to find a\nmug, then clean, it and put it on the desk. I\nneed to perform these tasks sequentially.\nStep 1: Find and take a mug AND\n# Think: Now I found a mug, I will clean it.\nStep 2: Clean the mug with sinkbasin AND\n# Think: Now I cleaned the mug, I will put\nthe clean mug on the desk.\nStep 3: Put clean mug on desk\n LLM\nPlanner(           )\nPut a clean\nmug on desk\nController\nAND\nFigure 2: Block diagram of the ADAPT pipeline with an example from ALFWorld. Left: Use of LLM as an\nexecutor to interact iteratively with the environment along with an example execution trajectory. Middle: Overall\nrecursive algorithm (depth k ≤dmax) that embeds the executor and planner, refer to Algorithm 1 for details. Right:\nOutline of using LLM as a planner to generate sub-tasks (steps) and logical operators combining them.\nskill). An advanced executor could combine “find-\ning a mug” with the “cleaning” skill to accomplish\n“find a clean mug” without an explicit planner.\nSelf-generated Success Heuristic.\nIn order to\ndecompose based on the abilities of the executor,\nwe need to determine whether the executor is capa-\nble of finishing the given (sub-)task independently\nor if further decomposition is required. To this\nend, we employ the executor LLM to determine\nthe completion of the (sub-)task without relying\non the environment for obtaining gold rewards for\n(sub-)tasks. We include a simple instruction in the\nexecutor prompt to output “task completed” if it de-\ntermines it has succeeded, otherwise output “task\nfailed” in case it cannot proceed. Refer to example\nin Fig. 2 (left). Our success heuristic aligns with bi-\nnary classification models employed in Shinn et al.\n(2023), providing a way to simulate intermediate\nrewards, which complements end-of-task environ-\nment rewards (Rengarajan et al., 2022). We study\nthis LLM-generated heuristic in Appendix F and\nshow that it closely matches the gold reward.\n3.2\nLLM as a Planner\nOverview.\nThe objective of the planner is to\nbreak down complex tasks into smaller sub-tasks.\nTo achieve this, we instruct the LLM to generate\na concise yet comprehensive plan consisting of a\nfew steps, typically 3-5, as shown in Fig. 2 (right).\nWe opt for shorter, more abstract plans because ex-\npecting a detailed, fine-grained plan upfront can be\nimpractical, especially in unexplored environments.\nE.g., devising a 10-step plan to put a clean mug\non a desk without prior knowledge of the mug’s\nlocation can lead to cascading errors due to incor-\nrect assumptions. Therefore, we task the LLM to\ngenerate short plans, with the flexibility to decom-\npose further in subsequent iterations, based on the\nexecutor’s capabilities.\nComposition Logic for Sub-tasks.\nAlong with\nthe sub-tasks, we prompt the planner to generate\nlogical operators to combine various sub-tasks in\nthe plan to accomplish the task. We allow for two\nlogical operators: “AND” and “OR”. Sub-tasks\nare linked using AND when they must be executed\nsequentially for the task to succeed. However, in\ncases requiring exploration, such as finding an item\nin an unknown room, we employ the OR operator\nto simulate conditional checks. Here, the task suc-\nceeds if any of the sub-tasks are successful. For\ninstance, in Fig. 1, the plan to “find a mug” would\nbe to “find a mug on the countertop” OR “find a\nmug in the cabinet”. We execute the latter only\nif the agent has not found the mug yet. While ex-\namples in Figs. 1 and 2 show homogeneous logic,\nADAPT can handle complex logical expressions\nas described in Appendix B.\n3.3\nController – LLM Program\nOverall Pipeline.\nThus far, we describe two\nLLM-based modules that can perform the roles\nof low-level execution and high-level planning. We\nincorporate these modules into ADAPT via the\ncontroller which is a pre-determined and recursive\nalgorithm – making the overall pipeline of ADAPT\nan LLM program (Schlag et al., 2023; Dohan et al.,\n2022), shown in Algorithm 1. The overall flow of\nthe controller program is as follows: (i) given an\ninput task, the controller calls the executor to check\nif it can succeed in performing the task directly; (ii)\n4\nif the executor does not succeed, the controller dele-\ngates decomposing the complex task to the planner\nand recursively calls ADAPT for each sub-task un-\ntil we hit a termination criterion, i.e., if a maximum\ndepth dmax (≥1) is reached.\nFig. 2 (mid) shows the control flow of ADAPT.\nA complex task such as “put a clean mug on the\ndesk” is first assigned to the executor. If the execu-\ntor does not succeed, then ADAPT calls the planner\nto decompose the task into sub-tasks along with a\nlogical operator (AND or OR) indicating how to\ncompose them. Each sub-task (referred to as ‘step’\nin Fig. 2) is then assigned recursively to ADAPT\nand is combined using the logical operator. In the\nend, the success of sub-tasks after recursive decom-\nposition ensures overall task success (unrolled calls\nto planner and executor are shown in Fig. 1).\n4\nExperimental Setup\nWe describe the datasets used in our experiments\nand baselines used for comparison with ADAPT.\n4.1\nDatasets\nWe employ LLMs-as-agents to perform tasks in the\nfollowing three environments and use task success\nrate as our evaluation metric in Secs. 5 and 6.\nALFWorld.\nALFWorld (Shridhar et al., 2021)\nis a text-based game version of the embodied AL-\nFRED benchmark (Shridhar et al., 2020) imple-\nmented in the TextWorld environment (Côté et al.,\n2019). It encompasses 6 distinct task types, where\nan agent is required to accomplish high-level tasks\nthrough navigation and interaction via text-based\nactions in a simulated household that gives textual\nfeedback to an agent (e.g., put a clean mug on desk\ndiscussed earlier in Fig. 2). Following Shridhar\net al. (2021), we present results on 134 unseen eval-\nuation games (test set) with a separate dev set of\n10 games per task from the seen evaluation games\nsplit. Along with atomic skills, we add example\ngold trajectories, following Yao et al. (2023b), for\ntwo tasks: heat and look in the executor prompt.3\nWebShop.\nWebShop (Yao et al., 2022) is an on-\nline shopping website environment featuring 1.18\nmillion real-world products containing 500 user\nqueries in the test set.\nIt serves as a complex\n3Unlike Yao et al. (2023b), we use a standardized executor\nprompt for all ALFWorld tasks, avoiding the agent to know the\ntask-type apriori. Table 6 in Appendix C further demonstrates\nthat ADAPT still improves over task-specific executors.\nCrafting Commands:\ncraft 4 oak planks using 1 oak log\ncraft 1 honeycomb block using 4 honeycomb\ncraft 1 beehive using 6 planks, 3 honeycomb\nGoal: craft beehive\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] craft 1 beehive using 6 oak planks, 3 honeycomb \n[Env] Crafted 1 minecraft:beehive\n...\n1x\n1x\n4x\n4x\n3x\n1x\nFigure 3: Example gold trajectory in TextCraft for a\ntask with recipe depth of 2.\ndecision-making environment with practical appli-\ncations wherein an agent must navigate a website\nthrough a variety of commands to purchase an item\nmatching a user specification (e.g., grey sectional\nsofa priced less than $300 with fast delivery). Fol-\nlowing Shinn et al. (2023), we report performance\non 100 user instructions and use a different subset\nof 40 queries as the dev set.\nTextCraft.\nWe create a new text-only environ-\nment for crafting Minecraft4 items similar to Word-\nCraft (Coenen et al., 2021). Unlike existing agent-\nbased environments, tasks in TextCraft exhibit a\nnatural compositional structure, resembling cook-\ning recipes with steps of varying complexity, where\nsome sub-tasks are more intricate, such as layering\na lasagna, while others are simpler, like baking it.\nTasks in TextCraft are inherently decomposable.\nIn Fig. 3, crafting a beehive necessitates crafting\nits ingredients, like planks and honeycomb, which\nmay require further decomposition. The agent thus\nneeds to identify and adapt to varying task com-\nplexity, e.g., crafting a plank is easier than crafting\na beehive. Moreover, some recipes allow using any\nitem from a particular category. For instance, craft-\ning a beehive uses planks (a category), requiring\nthe agent to use linguistic knowledge for proper\nitem selection (e.g., select oak planks, a specific\nitem in the category planks). We evaluate our ap-\nproach on a test set of 200 tasks where the target\nitems have recipe trees of depth 2, 3, and 4 (exam-\nple tree of depth 2 is shown in Fig. 3). We use the\n4https:\/\/www.minecraft.net\n5\nMethod (dmax = 3)\nPick Clean Heat Cool Look Pick2\nAll\nReAct\n33.3\n67.7\n43.5\n33.3\n55.6\n11.8\n43.3\nPlan-and-Execute\n29.2\n61.3\n47.8\n38.1\n61.1\n11.8\n43.3\nTry Again with ReAct 50.0\n51.6\n60.8\n47.6\n61.1\n5.9\n47.8\nReflexion\n70.8\n61.3\n61.0\n66.7\n61.1\n5.9\n57.5\nADAPT (Ours)\n87.5\n80.6\n60.8\n76.2\n61.1\n52.9\n71.6\nTable 1: ADAPT yields the highest the overall success rates (%)\ncompared to baselines from prior work (discussed in Sec. 4.2) on\nALFWorld (test split). Best (highest) success rates are highlighted\nin bold and second-highest rates are underlined.\nMethod\nWebShop TextCraft\nReAct\n32.0\n19.0\nPlan-and-Execute\n17.0\n27.0\nTry Again with ReAct\n30.0\n15.0\nReflexion\n35.0†\n32.0\nLATS (Zhou et al., 2023)\n38.0†\n−\nADAPT (Ours)\n44.0\n52.0\nTable 2: ADAPT yields the highest success\nrate on WebShop and TextCraft (test split) with\ndmax = 3 and 4 respectively. †Performance\nreported by Zhou et al. (2023)\nitems with recipe tree depth of 3 (123 tasks), depth\nof 4 (11 tasks) and depth of 2 (77 out of 297) in\nour test set, and the rest of depth 2 tasks constitute\nthe dev set. Additional details about creating the\nenvironment are present in Appendix E.\n4.2\nBaseline Approaches\nWe compare ADAPT with four classes of baseline\napproaches described below.\nIterative Executor-Only (ReAct).\nIn this setting,\nwe employ the executor to interact iteratively with\nthe environment, adopting the think-act-observe\nprompting style from ReAct (Yao et al., 2023b). All\nmethods discussed below, including ADAPT, share\nthe same executor, ensuring a standardized impact\nof the executor’s strength and design choices when\ncomparing relative performance in Sec. 5. When\ndmax =1, ADAPT solely relies on this executor.\nPlan-and-Execute.\nAs shown in Fig. 1, in this\nsetting, we generate a plan first and then assign\neach sub-task to the executor. This approach only\nplans once and as a result has a non-adaptive struc-\nture (consistent with Wang et al. (2023b); Yang\net al. (2023); Sun et al. (2023)). To ensure each\nplan step is executable without further decompo-\nsition, we design new prompts with more detailed\nplans. Note that ADAPT with dmax = 2 differs\nfrom plan-and-execute as it is adaptive, i.e., de-\ncomposes only when executor fails and generates\nrelatively shorter plans (refer to Appendix B).\nTry Again with ReAct.\nBy design, ADAPT\nmakes multiple calls to the executor module, al-\nbeit with different (sub-)tasks. Like Yang et al.\n(2023), we design a simple controller that requests\nthe executor to retry the task in a total of dmax\nseparate trials and then uses the trial with the best\nperformance for each task instance.\nReflexion.\nShinn et al. (2023) execute the en-\ntire task first, and if unsuccessful, reflect and store\nfeedback in memory for subsequent dmax−1 trials.\nWhile adaptive, this approach repeats the entire\ntrial even if a single sub-task fails, redundantly\nre-executing previously successful sub-tasks.\nADAPT and Shared Implementation Details.\nFollowing (Yao et al., 2023b; Shinn et al., 2023;\nZhou et al., 2023), by default, we use the GPT-\n3.5 (Ouyang et al., 2022) LLM for both planning\nand execution in ADAPT and other baselines. We\nuse the completion-based models for ALFWorld\nand TextCraft and the chat-based model for Web-\nShop.5 Further, we use ADAPT (and other base-\nlines) with dmax =3 for ALFWorld, and WebShop\nand increase to dmax =4 for TextCraft to accommo-\ndate recipes with a depth of 4 (Sec. 4.1). For addi-\ntional details, refer to Appendix A. We increase the\nmaximum number of iterations for the ReAct base-\nline by a factor of dmax and ensure all baselines\nuse a comparable number of LLM calls (Sec. 6.5).\n5\nMain Results\nUsing GPT-3.5 as the underlying LLM, in this sec-\ntion, we show that ADAPT yields the highest suc-\ncess rate compared to baselines from prior work on\nALFWorld, WebShop, and TextCraft datasets.\nALFWorld.\nIn Table 1, we observe that ADAPT\nachieves the highest overall success rate, while\nusing ReAct alone results in the lowest overall\nperformance. By leveraging adaptive decomposi-\ntion, ADAPT improves over ReAct’s performance\nby 28.3% points (absolute) as well as over Plan-\nand-Execute and Try Again by 28.3% and 23.8%\npoints, respectively. Lastly, we find that ADAPT\n5We use the completion model as chat variants of GPT-3.5\nconsistently underperform their completion counterparts (Liu\net al., 2023; Yang et al., 2023). We discuss the effectiveness\nof ADAPT different LLMs in Sec. 6.2.\n6\n1\n2\n3\nValue of dmax in ADaPT\n20\n40\n60\nSuccess Rate\nALFWorld\nWebShop\nTextCraft\nFigure 4: Success rate of ADAPT increases with the\nmaximum depth dmax for all datasets (dev splits).\nyields 14.1% points higher overall success rate\nthan Reflexion, despite the latter having access\nto dedicated memory and natural language feed-\nback. Specifically, we find baselines yield poor\nresults on ‘pick2’ tasks (<12% success rate) as\nthey require the agent to compose two ‘pick’-style\ntasks involving a longer action history. However,\nADAPT yields significant improvements (by over\na factor of 4×) for this type of tasks.\nWebShop.\nTable 2 shows a similar trend with\nADAPT surpassing all baselines and achieving\nthe highest success rate. ADAPT outperforms Re-\nAct, Plan-and-Execute, and Try-Again baselines\nby up to 27% points. We corroborate the findings\nof Shinn et al. (2023) and observe that natural lan-\nguage feedback offers limited gains in performance,\nas compared to ADAPT (which surpasses Reflex-\nion by 9% points). Additionally, we compare with\na recent search-based baseline LATS (Zhou et al.,\n2023) and find that ADAPT outperforms the suc-\ncess rate of LATS by 6% points.\nTextCraft.\nOur results on TextCraft are summa-\nrized in Table 2. First, we observe that ADAPT\nachieves an improvement of 33% compared to the\nReAct executor. In contrast to Plan-and-Execute,\ni.e., starting with a fixed plan, having the dynamic\nability to adapt to complex sub-tasks (in this case,\ncrafting complex ingredients) in ADAPT improves\nperformance by 25% points. Lastly, ADAPT out-\nperforms Reflexion by 20% points, highlighting\nthe importance of adaptive and as-needed planning.\nWe hypothesize that ADAPT consistently outper-\nforms Reflexion across datasets as the latter relies\non generating feedback based on errors in the entire\ntrajectory. In contrast, due its design, ADAPT of-\nten handle failures of small sub-tasks and redirects\nmore resources in the form of calling the planner\nTask-Specific\nHybrid\nAtomic\nExecutor Setting\n0\n20\n40\n60\nSuccess Rate\nExecutor-Only\nADaPT (dmax=3)\nFigure 5: ADAPT improves success rates across varying\nsettings capturing different executor capabilities (i.e.,\nexecutor-only performance) on ALFWorld (dev).\nand decomposition to the challenging sub-tasks.\n6\nAnalysis and Discussion\nWe analyze ADAPT in detail by addressing the\nfollowing research questions on dev data splits.\n6.1\nHow does performance of ADAPT scale\nwith the depth of decomposition?\nSetup.\nTo assess the impact of adaptive decom-\nposition, we study ADAPT under three settings\nwith increasing maximum depth dmax ∈{1, 2, 3}\nfor ALFWorld, WebShop, and TextCraft. Note\nthat dmax = 1 setting corresponds to the iterative\nexecutor-only baseline (ReAct).\nResults.\nFig. 4 shows that across all datasets, per-\nformance of ADAPT scales with increasing the\nmaximum depth dmax. Consistently, we find a sig-\nnificant improvement in success rates as we move\nfrom dmax =1 to dmax =2, i.e., adding the planner\nto decompose a complex task when executor fails\nproves to be effective. Finally, the performance\nincrease from dmax = 2 to dmax = 3 validates our\nhypothesis that some sub-tasks are difficult for the\nLLM to directly execute successfully, and decom-\nposing these further boosts overall performance.\n6.2\nDoes ADAPT cater to different execution\ncapabilities of LLMs?\nSame LLM, different execution capabilities.\nWe run ADAPT on three different executor\nprompts on ALFWorld: (i) task-specific gold tra-\njectories, (ii) atomic skills and common gold-\ntrajectories for 2 tasks used in Sec. 5 (hybrid), and\n(iii) only atomic skills. Using gold trajectories\naligns closely with the task at inference-time and\nthus, should exhibit high performance. In contrast,\n7\nAlfWorld\nWebShop\nTextCraft\n20\n40\n60\n80\n100\nSuccess Rate\nGPT-3.5\nGPT-4\nLemur\nLLaMA\nwith ADaPT\nFigure 6: ADAPT improves (test) performance of GPT-3.5, GPT-4, LLaMA, and Lemur LLMs across datasets.\nexecutor using only atomic skills relies on the in-\nherent composition abilities of the LLM, yielding\nweaker performance. Here we examine if ADAPT\ncan improve success rates for all three settings.\nResults.\nIn Fig. 5, we observe that ADAPT con-\nsistently improves over the executor-only baseline\nfor all diverse executor settings. As expected, the\nexecutor prompted with task-specific trajectories\nperforms the best (left), while the executor with\nonly atomic skills performs the worst (right). No-\ntably, ADAPT substantially improves performance\nof the relatively weak executor, improving success\nrate from 3.3% to 41.7%.\nADAPT with different LLMs.\nWe study the\nability of ADAPT to improve performance across\ndifferent LLMs (as planners and executors): (i)\nGPT-3.5, (ii) GPT-4 (OpenAI, 2023), (iii) LLaMA-\n2 70B (Touvron et al., 2023), and (iv) Lemur\n70B (Xu et al., 2023) on test splits of all datasets.\nResults.\nFig. 6 shows that ADAPT consistently\nimproves downstream performance for all models\nacross all three datasets. Consistent with Liu et al.\n(2023), we find that the gated GPT models outper-\nform the open-source models based on absolute\nsuccess rates. Nevertheless, ADAPT is effective\nacross LLMs and improves performance of GPT-\n4, the strongest LLM, by up to 37%, as well as\nLLaMA, the least performant LLM, by up to 15%\non the TextCraft dataset.\n6.3\nDoes ADAPT handle task complexity?\nSetup.\nBy the compositional design of TextCraft,\ncomplexity of each task in the dataset can be de-\nfined with respect to the depth of the crafting recipe,\ni.e., recipes with higher depth would be more com-\nplex to craft. We evaluate efficacy of ADAPT\nand the ReAct baseline on the test set of TextCraft\nMethod\nRecipe Depth kmax Success Rate\nReAct\n2\n1.0\n26.9\nADAPT (dmax = 4)\n2\n1.9\n78.2\nReAct\n3\n1.0\n1.8\nADAPT (dmax = 4)\n3\n2.8\n38.7\nTable 3: ADAPT improves TextCraft (test) performance\neven as recipe depth increases. The maximum decom-\nposition depth used by ADAPT to succeed at the task\n(kmax) also scales with the recipe depth.\nwith increasing recipe depth.6 Furthermore, while\nwe provide ADAPT with a maximum budget of\ndmax = 4, we study how the maximum decompo-\nsition depth utilized by ADAPT to succeed (kmax)\nvaries with task complexity.\nResults.\nIn Table 3 we observe that ADAPT im-\nproves success rates for games with recipe depth of\n2 from 26.9% to 78.2%, and of depth 3 from 1.8%\nto 38.7% as compared to the ReAct baseline. As\nexpected, the executor alone is unable to handle\ncomplex recipes with depth ≥3, but with the help\nof ADAPT the performance improves significantly.\nAdditionally, given the same budget dmax = 4, as\nthe recipe depth (complexity) increases from 2 to\n3, ADAPT’s level of decomposition (kmax) also\nincreases from 1.9 to 2.8. This showcases that\nADAPT leverages as-needed decomposition in or-\nder to handle task complexity.\n6.4\nCan we use different planner and executor\nLLMs within ADAPT?\nSetup.\nThe planner and executor modules of\nADAPT do not need to necessarily use the same\nunderlying model. Following, Lin et al. (2023) we\nexplore if a relatively smaller LLM can be used to\nperform local actions in the executor and a more\n6As we have only 11 tasks with recipe depth of 4, we\nexclude them from this analysis.\n8\nExecutor LM\nPlanner LM\nSuccess Rate\nGPT-3.5\n−\n38.4\nGPT-3.5\nGPT-3.5\n58.3\nLLaMA-2-70B\n−\n20.4\nLLaMA-2-70B\nGPT-3.5\n43.3\nTable 4: ADAPT improves performance on ALFWorld\n(dev) when using different planner and executor LLMs.\nadvanced LLM be used to devise plans. To this\nend, we explore different combinations of planner\nand executor LLM, with the latter using both gated\nand open-source models on ALFWorld.\nResults.\nTable 4 shows that ADAPT can suc-\ncessfully be used to generate plans from one LLM\nthat are useful to a different, possibly smaller, ex-\necutor LLM, improving success rates by up to\n19.9% compared to the executor-only (ReAct) set-\nting. Interestingly, using an open-source model,\nsuch as LLaMA-2-70B-chat (Touvron et al., 2023)\ncan be used as an executor with a more advanced\nLLMs such as GPT-3.5 to improve success rates\nby 22.9% points. Since the planner LLM is used\nsparingly, open-source executors can dramatically\ndecrease the monetary or computational costs of us-\ning ADAPT. We defer combining knowledge from\nstronger and weaker LMs within ADAPT to future\nwork, as examined in the context of mathematical\nreasoning (Fu et al., 2023; Saha et al., 2023a).\n6.5\nHow does ADAPT compare to baselines in\nterms of LLM calls?\nSetup.\nPerformance of decision-making agents\ncan be enhanced by increasing the number of calls\nallowed to an LLM, e.g., number of retrials in Re-\nflexion. To verify that the gains in ADAPT are\nnot simply due to higher number of LLM calls, we\ncompare the average of number of LLM calls made\nby ADAPT to the baselines.\nResults.\nFig. 7 shows that a ADAPT employs a\ncomparable number of LLM calls w.r.t. Try-Again\nand Reflexion baselines in order to yield perfor-\nmance improvements discussed in Sec. 5 (Tables 1\nand 2). Note that while all methods including Re-\nAct and Plan-and-Execute baselines are offered a\ncomparable computational budget, the actual num-\nber of LLM calls used by the latter is often lower\ndue to their inability to handle intermediate exe-\ncution failures. This strengthens the argument for\neffectiveness of ADAPT as the improvements do\nAlfWorld\nWebShop\nTextCraft\n0\n10\n20\n30\nAvg. #LLM Calls\nReAct\nPlan-&-Exec\nTry Again\nReflexion\nADaPT\nFigure 7: Average number of LLM calls for each ap-\nproach including ADAPT and baselines discussed in\nSec. 4.2 with GPT-3.5 LLM across datasets.\nnot simply stem from using substantially higher\nnumber of calls to the LLM.\n7\nConclusion\nWe introduce ADAPT, a recursive algorithm de-\nsigned to harness the planning capabilities of\nLLMs, dynamically decomposing complex tasks\nwhen the LLM acting as an executor encoun-\nters challenges. Our evaluation across three di-\nverse decision-making tasks, ALFWorld, WebShop,\nand TextCraft, reveals impressive performance of\nADAPT, surpassing existing baselines by substan-\ntial margins of up to 28.3%, 27%, and 33% points,\nrespectively. This not only underscores the effec-\ntiveness of ADAPT but also highlights the signif-\nicance of as-needed decomposition in enhancing\ntask performance. Moreover, our findings demon-\nstrate that ADAPT not only adapts to the capabili-\nties of the underlying executor LLM but also takes\ninto account the complexity of individual task in-\nstances, showcasing its versatility and effectiveness.\nAcknowledgements\nPart of this work was done during internship at\nAI2 and was partially supported at UNC by NSF-\nCAREER Award 1846185, NSF-AI Engage Insti-\ntute DRL-2112635, DARPA Machine Common-\nsense (MCS) Grant N66001-19-2-4031,. We sin-\ncerely thank Bodhisattwa Prasad Majumder, Chris\nCallison-Burch, Shashank Gupta, Peter Jansen, Bill\nYuchen Lin and the Aristo team for their valuable\nfeedback. We also thank Swarnadeep Saha, Elias\nStengel-Eskin, and Peter Hase for their feedback.\n9\nLimitations\nADAPT relies on the success heuristic generated\nby the executor LLM to determine if the model\nis capable of performing a complex task.\nFor\ndecision-making tasks studied in this work, we\nfind that LLMs can reliably determine task success\nbased on past action trajectories and textual feed-\nback from the environment (see Appendix F). How-\never, Huang et al. (2023a); Stechly et al. (2023)\ndiscuss the limits of LLM’s ability to self-evaluate\nand self-refine. In such situations, future works\nmay additionally employ external verifiers (Light-\nman et al., 2023; Shridhar et al., 2023), theory-of-\nmind strategies among multiple LMs (Saha et al.,\n2023a), and other calibration and self-evaluation\ntechniques (Kadavath et al., 2022). These improved\nself-evaluation techniques could be useful to extend\nour framework to non-decision making tasks such\nas question answering.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, et al. 2022. Do as i can, not as i say: Ground-\ning language in robotic affordances. arXiv preprint\narXiv:2204.01691.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Neural module networks. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 39–48.\nAndrew G Barto and Sridhar Mahadevan. 2003. Re-\ncent advances in hierarchical reinforcement learning.\nDiscrete event dynamic systems, 13(1-2):41–77.\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,\nand Yoav Artzi. 2022. A persistent spatial semantic\nrepresentation for high-level natural language instruc-\ntion execution. In Conference on Robot Learning,\npages 706–717. PMLR.\nAndy Coenen, Luke Davis, Daphne Ippolito, Emily\nReif, and Ann Yuan. 2021. Wordcraft: a human-ai\ncollaborative editor for story writing. arXiv preprint\narXiv:2107.07430.\nMarc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben\nKybartas, Tavian Barnes, Emery Fine, James Moore,\nMatthew Hausknecht, Layla El Asri, Mahmoud\nAdada, et al. 2019. Textworld: A learning environ-\nment for text-based games. In Computer Games:\n7th Workshop, CGW 2018, Held in Conjunction with\nthe 27th International Conference on Artificial In-\ntelligence, IJCAI 2018, Stockholm, Sweden, July\n13, 2018, Revised Selected Papers 7, pages 41–75.\nSpringer.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous,\nJascha Sohl-Dickstein, et al. 2022. Language model\ncascades. arXiv preprint arXiv:2207.10342.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine\nLi, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra\nBhagavatula, Ronan Le Bras, Jena D Hwang, et al.\n2023. Faith and fate: Limits of transformers on com-\npositionality. arXiv preprint arXiv:2305.18654.\nKutluhan Erol, James Hendler, and Dana S Nau. 1994.\nHtn planning: Complexity and expressivity. In AAAI,\nvolume 94, pages 1123–1128.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-\ndlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar.\n2022. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in\nNeural Information Processing Systems, 35:18343–\n18362.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023. Specializing smaller language\nmodels towards multi-step reasoning. arXiv preprint\narXiv:2301.12726.\nIlche Georgievski and Marco Aiello. 2014. An overview\nof hierarchical task network planning. arXiv preprint\narXiv:1403.7426.\nMalik Ghallab, Dana Nau, and Paolo Traverso. 2004.\nAutomated Planning: theory and practice. Elsevier.\nNitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and\nMatt Gardner. 2019. Neural module networks for\nreasoning over text. In International Conference on\nLearning Representations.\nDaniel Höller, Gregor Behnke, Pascal Bercher, Susanne\nBiundo, Humbert Fiorino, Damien Pellier, and Ron\nAlford. 2020. Hddl: An extension to pddl for ex-\npressing hierarchical planning problems. In Proceed-\nings of the AAAI conference on artificial intelligence,\npages 9883–9891.\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 1049–1065, Toronto,\nCanada. Association for Computational Linguistics.\nJie\nHuang,\nXinyun\nChen,\nSwaroop\nMishra,\nHuaixiu Steven Zheng, Adams Wei Yu, Xiny-\ning Song, and Denny Zhou. 2023a. Large language\nmodels cannot self-correct reasoning yet.\narXiv\npreprint arXiv:2310.01798.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tomp-\nson, Igor Mordatch, Yevgen Chebotar, et al. 2023b.\nInner monologue: Embodied reasoning through plan-\nning with language models. In Conference on Robot\nLearning, pages 1769–1782. PMLR.\n10\nYichen Jiang and Mohit Bansal. 2019. Self-assembling\nmodular networks for interpretable multi-hop rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4474–4484, Hong Kong, China. Association for Com-\nputational Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022.\nLanguage models\n(mostly) know what they know.\narXiv preprint\narXiv:2207.05221.\nTushar Khot, Daniel Khashabi, Kyle Richardson, Peter\nClark, and Ashish Sabharwal. 2021. Text modular\nnetworks: Learning to decompose tasks in the lan-\nguage of existing models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1264–1279, Online.\nAssociation for Computational Linguistics.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modular\napproach for solving complex tasks. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023.\nLet’s verify step by step.\narXiv preprint\narXiv:2305.20050.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithvi-\nraj Ammanabrolu, Faeze Brahman, Shiyu Huang,\nChandra Bhagavatula, Yejin Choi, and Xiang Ren.\n2023. Swiftsage: A generative agent with fast and\nslow thinking for complex interactive tasks. arXiv\npreprint arXiv:2305.17390.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\nMen, Kejuan Yang, et al. 2023. Agentbench: Evaluat-\ning llms as agents. arXiv preprint arXiv:2308.03688.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Multi-hop reading compre-\nhension through question decomposition and rescor-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6097–6109, Florence, Italy. Association for Compu-\ntational Linguistics.\nSo Yeon Min, Devendra Singh Chaplot, Pradeep Kumar\nRavikumar, Yonatan Bisk, and Ruslan Salakhutdinov.\n2022. Film: Following instructions in language with\nmodular methods. In International Conference on\nLearning Representations.\nVijayaraghavan Murali, Letao Qi, Swarat Chaudhuri,\nand Chris Jermaine. 2018. Neural sketch learning\nfor conditional program generation. In International\nConference on Learning Representations.\nOfir Nachum, Shixiang Shane Gu, Honglak Lee, and\nSergey Levine. 2018. Data-efficient hierarchical rein-\nforcement learning. Advances in neural information\nprocessing systems, 31.\nMaxwell Nye, Luke Hewitt, Joshua Tenenbaum, and\nArmando Solar-Lezama. 2019. Learning to infer\nprogram sketches. In International Conference on\nMachine Learning, pages 4861–4870. PMLR.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730–27744.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised question\ndecomposition for question answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n8864–8880, Online. Association for Computational\nLinguistics.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789.\nDesik Rengarajan, Gargi Vaidya, Akshay Sarvesh,\nDileep Kalathil, and Srinivas Shakkottai. 2022. Rein-\nforcement learning with sparse rewards using guid-\nance from offline demonstration. In International\nConference on Learning Representations.\nSwarnadeep Saha, Peter Hase, and Mohit Bansal. 2023a.\nCan language models teach weaker agents? teacher\nexplanations improve students via theory of mind.\narXiv preprint arXiv:2306.09299.\nSwarnadeep Saha, Shiyue Zhang, Peter Hase, and Mo-\nhit Bansal. 2023b. Summarization programs: Inter-\npretable abstractive summarization with neural mod-\nular trees. In The Eleventh International Conference\non Learning Representations.\nImanol Schlag, Sainbayar Sukhbaatar, Asli Celikyilmaz,\nWen-tau Yih, Jason Weston, Jürgen Schmidhuber,\nand Xian Li. 2023. Large language model programs.\narXiv preprint arXiv:2305.05364.\n11\nPratyusha Sharma, Antonio Torralba, and Jacob An-\ndreas. 2022. Skill induction and planning with latent\nlanguage. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1713–1726, Dublin,\nIreland. Association for Computational Linguistics.\nLanbo She, Shaohua Yang, Yu Cheng, Yunyi Jia, Joyce\nChai, and Ning Xi. 2014. Back to the blocks world:\nLearning new actions through situated human-robot\ndialogue. In Proceedings of the 15th annual meet-\ning of the special interest group on discourse and\ndialogue (SIGDIAL), pages 89–97.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Huai hsin Chi, Nathanael\nScharli, and Denny Zhou. 2023. Large language\nmodels can be easily distracted by irrelevant context.\nIn International Conference on Machine Learning.\nNoah Shinn, Federico Cassano, Beck Labash, Ash-\nwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. 2023.\nReflexion:\nLanguage agents with\nverbal reinforcement learning.\narXiv preprint\narXiv:2303.11366, 14.\nKumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu\nWang, Ping Yu, Ram Pasunuru, Mrinmaya Sachan,\nJason Weston, and Asli Celikyilmaz. 2023. The art of\nllm refinement: Ask, refine, and trust. arXiv preprint\narXiv:2311.07961.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. Alfred: A bench-\nmark for interpreting grounded instructions for ev-\neryday tasks. In Proceedings of the IEEE\/CVF con-\nference on computer vision and pattern recognition,\npages 10740–10749.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté,\nYonatan Bisk,\nAdam Trischler,\nand Matthew\nHausknecht. 2021. ALFWorld: Aligning Text and\nEmbodied Environments for Interactive Learning.\nIn Proceedings of the International Conference on\nLearning Representations (ICLR).\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,\nJesse Thomason, and Animesh Garg. 2023. Prog-\nprompt: Generating situated robot task plans using\nlarge language models. In 2023 IEEE International\nConference on Robotics and Automation (ICRA),\npages 11523–11530. IEEE.\nChan Hee Song, Jiaman Wu, Clayton Washington,\nBrian M Sadler, Wei-Lun Chao, and Yu Su. 2023.\nLlm-planner: Few-shot grounded planning for em-\nbodied agents with large language models. In Pro-\nceedings of the IEEE\/CVF International Conference\non Computer Vision, pages 2998–3009.\nKaya Stechly, Matthew Marquez, and Subbarao Kamb-\nhampati. 2023. Gpt-4 doesn’t know it’s wrong: An\nanalysis of iterative prompting for reasoning prob-\nlems. arXiv preprint arXiv:2310.12397.\nSimeng Sun, Y. Liu, Shuo Wang, Chenguang Zhu, and\nMohit Iyyer. 2023. Pearl: Prompting large language\nmodels to plan and execute actions over long docu-\nments. ArXiv, abs\/2305.14564.\nRichard S Sutton, Doina Precup, and Satinder Singh.\n1999. Between mdps and semi-mdps: A framework\nfor temporal abstraction in reinforcement learning.\nArtificial intelligence, 112(1-2):181–211.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 641–651, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. Voyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n2023b. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2609–2634, Toronto,\nCanada. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nYiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian\nLiu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu,\nTianbao Xie, Zhoujun Cheng, Siheng Zhao, Ling-\npeng Kong, Bailin Wang, Caiming Xiong, and Tao\nYu. 2023. Lemur: Harmonizing natural language and\ncode for language agents.\nJohn Yang, Akshara Prabhakar, Karthik Narasimhan,\nand Shunyu Yao. 2023. Intercode: Standardizing\nand benchmarking interactive coding with execution\nfeedback. arXiv preprint arXiv:2306.14898.\n12\nShunyu Yao, Howard Chen, John Yang, and Karthik\nNarasimhan. 2022. Webshop: Towards scalable real-\nworld web interaction with grounded language agents.\nAdvances in Neural Information Processing Systems,\n35:20744–20757.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao.\n2023b. React: Synergizing reasoning and acting\nin language models. In The Eleventh International\nConference on Learning Representations.\nJesse Zhang, Haonan Yu, and Wei Xu. 2021. Hierarchi-\ncal reinforcement learning by discovering intrinsic\noptions. In International Conference on Learning\nRepresentations.\nWenqing Zheng, SP Sharan, Ajay Kumar Jaiswal, Kevin\nWang, Yihan Xi, Dejia Xu, and Zhangyang Wang.\n2023. Outline, then details: Syntactically guided\ncoarse-to-fine code generation. In International Con-\nference on Machine Learning, pages 42403–42419.\nPMLR.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models. arXiv preprint\narXiv:2310.04406.\nA\nADAPT Implementation Details\nExecutor.\nWe use a common ReAct executor for\neach dataset. To this end, we provide the LLM\nin the executor with in-context example trajecto-\nries for each atomic skill (refer to Table 5 for an\nexhaustive list). Atomic skills are inherently task\ndependent, and thus, vary with the underlying envi-\nronment. For ALFWorld, in which the agent needs\nto navigate and perform tasks in the household, the\natomic skills include: taking an object, putting it\ndown at a location, cleaning, heating, etc. On the\nother hand, the goal in WebShop is to buy a product\nbased on user queries, thus, atomic skills include:\nsearching a specified query, shortlisting products\nbased on search page, matching if a product sat-\nisfies a criteria, and buying a product. Lastly, the\natomic skills in TextCraft are fetching objects from\nthe environment, and crafting them given the recipe\nand the ingredients. Following Yao et al. (2023b),\nwe add gold trajectories for two tasks: heat and\nlook in the executor prompt for ALFWorld, and\none full gold trajectory for TextCraft.\nAtomic Skill\nDescription\nALFWorld\nput\nAssuming that the robot is carrying\nan object, put it on a given receptacle.\ntake\nTake a specified object from a speci-\nfied receptacle.\nclean\/heat\/cool Assuming that the robot is carrying\nan object, clean\/heat\/cool the object.\nexamine\nAssuming the robot is at a desk with a\ndesk lamp, use it to look at an object.\nWebShop\nsearch\nPut a given query in the search box,\nresults in a page with list of products.\nshortlist\nBased on the search page and query,\nget list of any matching products.\nmatch\nGiven a product ID and query, navi-\ngate to the product page and verify it\nmatches the query.\nbuy\nGiven a product ID and query, buy\nproduct by selecting relevant options.\nTextCraft\ncraft\nAssuming the agent has all the ingre-\ndients in the inventory, craft a target\nobject by picking an appropriate com-\nmand from the list of crafting recipes.\nfetch\nLook for a given object in the inven-\ntory or get it directly from the game.\ninventory\nLook-up the game inventory.\nTable 5: Overview of atomic skills used in Sec. 3.1.\nPlanner.\nWe provide the LLM with a brief de-\nscription of atomic skills and in-context demonstra-\ntions of few task decompositions for each dataset.\n• ALFWorld: The planner includes 6 demonstra-\ntions of task decompositions for one household\nconfiguration.\nSpecifically, “find” is not an\natomic skill for the executor, and therefore, needs\nto be handled by the planner (refer to Fig. 2).\n• WebShop: The planner breaks down a given task\nin terms of the atomic skills described in Table 5\nvia 2 in-context demonstrations.\n• TextCraft: The planner determines the necessary\ningredients for each item and creates a plan to\nobtain them and then craft the item, illustrated\nvia 2 examples with different crafting commands.\nController.\nThe controller performs two crucial\nroles in the overall functioning of ADAPT. First, it\nserves as the communication bridge between plan-\nner and executor, propagating salient information\nacross the two depending on the task. Second, since\nADAPT is a recursive algorithm, the controller de-\ntermines the termination criterion using the logical\nexpression from the planner and success heuristic\nfrom the executor or if a maximum depth dmax\n(≥1) is reached. The controller propagates task-\ndependent salient information described below:\n• ALFWorld: In the controller, we propagate the\nlast successful action from a previous execution\n13\nMethod\nPick Clean Heat Cool Look Pick2\nAll\nReAct\n66.7\n41.9\n47.8\n80.9\n83.3\n23.5\n56.7\nPlan-and-Execute\n87.5\n58.1\n73.9\n52.4\n83.3\n17.6\n63.4\nTry Again with ReAct 75.0\n38.7\n60.9\n76.2\n66.7\n23.5\n56.7\nReflexion\n83.3\n61.3\n73.9\n85.7\n61.1\n29.4\n67.2\nADAPT (Ours)\n91.7\n67.7\n78.3\n81.0\n100\n64.7\n79.8\nTable 6: Comparison of success rates (%) achieved by ADAPT\nand other baselines from prior work on ALFWorld (test split)\nwith executor used by Yao et al. (2023b)\nMethod\nScore Success Rate\nIterative Executor-Only\n42.1\n29.0\nStatic Decomposition\n27.7\n17.0\nRetry Execution\n45.4\n30.0\nNaive\n58.3\n24.0\nReflexion*\n64.2\n35.0\nLATS (Zhou et al., 2023)*\n75.9\n38.0\nADAPT (Ours)\n60.0\n44.0\nTable 7: Performance comparison of differ-\nent methods on WebShop.\nAlgorithm 1 Algorithm for ADAPT\n1: function ADAPT(Task T, Current depth k)\n2:\n\/\/ ADAPT(·) Generates success heuristic value\ncompleted for the task T. Initialized with k = 1.\n3:\n\/\/ Base case: terminate on reaching maximum depth\n4:\nif k > dmax then return False\n5:\n\/\/ Execute the task\/sub-task to assess if the LLM can\ndirectly perform it using LLM-generated success.\n6:\ncompleted ←executorLLM(T)\n7:\n\/\/ Plan only when the executor fails.\n8:\nif completed is False then\n9:\n\/\/ Using the LLM, decompose the task into a set\nof sub-tasks, P, and a Boolean function, logic(·),\nthat combines output of the sub-tasks.\n10:\nP, logic ←plannerLLM(T)\n11:\n\/\/ Get the outputs for individual sub tasks\n12:\nO = {ADAPT(Tsub, k+1)|Tsub ∈P}\n13:\n\/\/ Combine the outputs of the sub tasks\n14:\ncompleted ←logic(O)\n15:\nreturn completed\nrun to subsequent calls of the executor. Note that\ninformation is only propagated from successful\nsub-tasks. For sub-tasks connected via “OR”,\neach receives the same information from the con-\ntroller. Unlike Shinn et al. (2023), executor does\nnot get text feedback from prior failures.\n• WebShop: We propagate the current page vis-\nible to the agent along with past unsuccessful\nexecutor tasks to the planner (without any ratio-\nnales). Once we find a matching product, we also\npropagate the product ID in future executor calls.\n• TextCraft: We propagate the current inventory\nof the agent to the executor. This is akin to ex-\necutors starting with the inventory command\nas the first step to keep stock of which items are\nmissing and need to be fetched or crafted.\nFor partial rolled-out trajectories with ADAPT re-\nfer to Figs. 9 to 11. Communication between plan-\nner and executor is highlighted in gray box(es) .\nLLM-related Hyperparameters.\nFollowing pre-\nvious works (Shinn et al., 2023; Liu et al., 2023)\nwe use text-davinci-003 from the OpenAI\nAPI for ALFWorld. For WebShop, we use the\ngpt-3.5-turbo models, and for TextCraft we use\nthe gpt-3.5-turbo-instruct models. All execu-\ntors have a maximum budget of iterations to inter-\nact with the environment and execute the task. We\nset this budget to 20, 15, and 20 respectively for\nALFWorld, WebShop, and TextCraft respectively.\nFor try again with ReAct, we sample additional tra-\njectories with a temperature of 0.7. As discussed in\nSec. 4.2, we run the iterative executor-only baseline\nfor 60, 45, 60 iterations for ALFWorld, WebShop,\nand TextCraft respectively. In Sec. 6.2, we use pub-\nlicly available checkpoints for LLaMA 70B7 and\nLemur 70B8 available on Huggingface (Wolf et al.,\n2019). For both planner and executor modules, we\nuse a fixed prompt consisting of few in-context\nexamples (as described above) for each dataset.\nWe show all executor and planner prompts to the\nLLM in Appendix G. Due to cost constraints, we\nreport success rates for a single run of each LLM\nin Secs. 5 and 6.\nB\nHandling Complex Logic in Plans\nWhile the examples in Figs. 1 and 2 show homoge-\nneous logic across sub-tasks in the plan, our con-\ntroller can handle complex logical expressions in-\ncluding both “AND” and “OR” operators. Specif-\nically, we provide instructions to the planner to\noutput this logical expressing at the end of the plan\nwith a fixed prefix: Execution Order. We then\nbuild a deterministic parser that can parse complex\nlogical expressions that the controller can process.\nWe do so by splitting the logical expression into a\nseries of homogeneous expression each passed to\nADAPT. Whenever the task given to ADAPT com-\nprises of multiple sub-tasks connected via (one)\nlogical operator, we automatically decompose this\ntask as per the logical expression. For example,\nin Fig. 8, a detailed plans used by the plan-and-\nexecute baseline (discussed in Sec. 4.2) comprised\n7https:\/\/huggingface.co\/meta-llama\/\nLlama-2-70b-hf\n8https:\/\/huggingface.co\/OpenLemur\/\nlemur-70b-chat-v1\n14\nPlan: Put a clean mug on desk\n# Think: To do this task, ....\nStep 1: Find and take the mug AND\n# Think: Now that I have found it, ....\nStep 2: Clean the mug using sinkbasin AND\n# Think: Now that I have cleaned ....\nStep 3: Put clean mug on desk\nPlan: Find and take the mug\n# Think: To do this task, ....\nStep 1: Find and take mug from countertop OR\n# Think: If I do not find the mug, ....\nStep 2: Find and take mug from cabinet OR\n# Think: If I do not find the mug, ....\nStep 3: Find and take mug from drawer\nPlan: Put a clean mug on desk\n# Think: To do this task, ....\nStep 1: Find and take mug from\ncountertop OR\n# Think: If I do not find the mug, ....\nStep 2: Find and take mug from cabinet OR\n# Think: If I do not find the mug, ....\nStep 3: Find and take mug from drawer AND\n# Think: Now that I have found it, ....\nStep 4: Clean the mug using sinkbasin AND\n# Think: Now that I have cleaned ....\nStep 5: Put clean mug on desk\nLogic: ((Step 1 OR Step 2 OR Step 3) AND\nStep 4 AND Step 5)\nAdaptive Multi-level Plans in ADaPT\nDetailed Plans in Plan-and-Execute\nFigure 8: Illustration of how multiple levels of plans\nfrom ADAPT, can be collapsed into one detailed plan\nin non-adaptive settings as used in the plan-and-execute\nbaseline (Sec. 4.2). Our controller can handle complex\n(non-homogeneous) logical expressions.\nof logical expressions using both AND, and OR\noperators. Therefore, the parser will break auto-\nmatically break this into multiple levels, i.e., Step\n6 = Step 1 OR Step 2 OR Step 3, followed by Step\n6 AND Step 4 AND Step 5. While such complex\nlogical expressions are mostly associated with the\nplan-and-execute baseline, they can be easily used\nwithin the ADAPT framework. Furthermore, this\nallows the plan-and-execute baseline to simulate\na multi-level planning structure via detailed plans\nwithout being adaptive to the executor.\nC\nTask-specific Executors in ALFWorld\nIn Table 1, we use a standardized executor with\nin-context demonstrations of atomic skills and two\ngold trajectories. While this allows for a common\nexecutor across different sub-tasks, task-specific\nexecutors yield higher performance on the specific\nsub-tasks. We now show ADAPT can also be used\non top of task-specific executors used by Yao et al.\n(2023b). The results are shown in Table 6. First,\nMethod\n#Products\nSuccess Rate\nReAct\n3\n27.5\nADAPT (dmax = 3)\n3\n47.5\nReAct\n10\n20.0\nADAPT (dmax = 3)\n10\n42.5\nTable 8: ADAPT improves WebShop (dev) performance\nirrespective of how many products (3 or 10) are chosen\nfrom the search page.\nwe observe that ADAPT yields the overall success\nrate by up to 23.1% points and also surpasses base-\nlines on all but 1 task types. Interestingly, we find\nstrong performance of the plan-and-execute base-\nline when using a stronger executor (as compared\nto Table 1) possibly as such an executor can handle\ncomplex sub-tasks better. Consistent with Table 1,\nADAPT outperforms Reflexion by 12.6% points\ndespite lack of dedicated memory and natural lan-\nguage feedback.\nD\nAdditional WebShop Experiments\nEvaluation Metrics.\nWe focus on success rate\nand not the (soft) score as the primary metric for\nthis task because it is possible to get a non-zero\nscore by naively buying a product. To this effect,\nwe construct a naive executor that inputs the user\nquery in the search bar and buys the first available\nproduct. Table 7 shows that while this baseline\nyields the lowest success rate, it surprisingly yields\na high success rate of 58.3. In contrast, our ex-\necutors often do not buy products especially when\nthe previous sub-goals fail which can adversely im-\npact scores even though the success rate remains\nunaffected. Therefore, we argue for optimizing the\nsuccess rate instead of the score as opposed to prior\nworks (Zhou et al., 2023).\nADAPT accommodating task complexity.\nBy\ndefault, Yao et al. (2023b) use a search page with\nonly the top-3 search results displayed. Intuitively,\nincreasing the number of products on the search\npage requires the model to choose from a wider\narray of products and track all their information to\ndetermine the best fit to the user query, making the\noverall task harder. Therefore, we apply ADAPT\non Webshop in two settings with 3, and 10 products\nper search page.\nResults.\nFrom Table 8, we observe that ADAPT\neffectively improves success rate by 20.0% and\n22.5% for 3 and 10 products respectively over the\n15\nRoom: You are in the middle of a room. Looking quickly around you, you\nsee a cabinet 21, a cabinet 20, a cabinet 19, a cabinet 18, a cabinet 17,\na cabinet 16, a cabinet 15, a cabinet 14, a cabinet 13, a cabinet 12, a\ncabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet\n6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a\ncoffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a\ndrawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, a\nstoveburner 3, a stoveburner 2, a stoveburner 1, desk 1, and a toaster 1.\nYour task is to: Put a clean mug on desk.\nExecute: Put a clean mug on desk\n> Go to countertop 1\nYou reached loc 1, ...\n> Go to countertop 2\nYou reached loc 2, ...\n> Think: Mug not found, task failed!\nPlan: Put a clean mug on desk\n# Think: To do this task, ....\nStep 1: Find and take the mug AND\n# Think: Now that I have found it, ....\nStep 2: Clean the mug using sinkbasin AND\n# Think: Now that I have cleaned ....\nStep 3: Put clean mug on desk\nExecute: Find and take the mug\n> Think: Task failed!\nPlan: Find and take the mug\n# Think: To do this task, ....\nStep 1: Find and take mug from countertop OR\n# Think: If I do not find the mug, ....\nStep 2: Find and take mug from cabinet OR\n# Think: If I do not find the mug, ....\nStep 3: Find and take mug from drawer\nExecute: Find and take mug from countertop\n> Think: Task failed!\nExecute: Find and take mug from cabinet\n> Think: Task completed!\nExecute: Clean the mug using sinkbasin\n> Think: Task completed!\nExecute: Find and take the mug from countertop\n> Think: Task completed!\nPreviously executed: \n> take mug 1 from cabinet 7\nYou picked up mug 1\n...\n...\nRoom: You are in the middle of a room. Looking quickly around you, you\nsee a cabinet 21, a cabinet 20, a cabinet 19, a cabinet 18, a cabinet 17,\na cabinet 16, a cabinet 15, a cabinet 14, a cabinet 13, a cabinet 12, a\ncabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet\n6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a\ncoffeemachine 1, a countertop 2, a countertop 1, a diningtable 1, a\ndrawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, a\nstoveburner 3, a stoveburner 2, a stoveburner 1, desk 1, and a toaster 1.\nYour task is to: Put a clean mug on desk.\nExecute: Put a clean mug on desk\n> Think: To solve the task, I need to find and take a mug, then clean it\nwith sinkbasin, then put on the desk.\nOK.\n> Think: First I need to find a mug. A mug is more likely to appear in\nfridge (1), countertop (1-2), dining table (1), cabinet (1-21), drawer\n(1-5). I can check one by one, starting with fridge 1.\nOK.\n> open fridge 1\nNothing happens.\n> go to fridge 1\nYou arrive at loc 15, fridge 1 is closed.\n> open fridge 1\n        You open the fridge 1. The fridge 1 is open. In it, you see a bowl 1, a\ncup 1, a egg 1, a lettuce 1, a plate 1, and a tomato 1.\n> Think: There is no mug in fridge 1, I need to go to countertop 1 next.\nOK.\n> go to countertop 1\nYou reached loc 1, on it you see a cd 2, a cellphone 3, a\ncellphone 1, a keychain 3, a pen 2, a pencil 3 and a pencil 2.\n> go to countertop 2\n    You reached loc 2, on it you see a egg 3, a knife 2, a knife 1, a potato\n1, a potato 2, a bread 1, a dishsponge 3, and a saltshaker 1.\n> Think: There is no mug in countertop 2, I need to go to cabinet 1.\nOK.\n> go to cabinet 1\nYou arrive at loc 4, cabinet 1 is closed.\n> open cabinet 1\n                    You open cabinet 1. Cabinet 1 is open. In it, you see bowl 3,\nfork 2, soapbottle 1, and a cloth 1.\n> go to cabinet 2\nYou arrive at loc 5, cabinet 2 is closed.\n> open cabinet 2\nYou open cabinet 2. Cabinet 2 is open. In it, you see nothing.\n> open cabinet 4\nNothing happens.\n> go to cabinet 4\nYou arrive at loc 7, cabinet 4 is closed.\n> open cabinet 4\n        You open cabinet 4. Cabinet 4 is open. In it, you see a spatula 1, a\ncloth 2, a peppershaker 1, cup 3, a apple 2, and a toiletpaper 1.\n> take mug 3 from cabinet 4\nNothing happens.\n> take cup 3 from cabinet 4\nYou pick up cup 3 from cabinet 4\n> Think: Now I found cup (3). Next I need to clean it. For this ...\n...\n   ADaPT (Adaptive Decomposition, Dynamic Depth)\nIterative Executor (ReAct)\n...\n...\n...\n...\nFigure 9: Comparison of iterative executors such as ReAct with ADAPT. On left, ReAct uses interleaved “thought”\nstatements to set milestones and track their progress. However, due to a large action history, it struggles to follow\nthe plan exactly and hallucinates the wrong object (highlighted in red). ADAPT, on the right, decomposes complex\ntasks into smaller sub-tasks whenever the executor fails, leading to shorter action trajectories for easy execution.\nReAct baseline. The difference in ReAct perfor-\nmance for both settings corroborates our hypothesis\nthat increasing number of products on the search\npage increases task complexity, all else equal. No-\ntably, we show that ADAPT yields higher improve-\nment for more complex task settings.\nE\nTextCraft\nTextCraft: Environment Details.\nIn TextCraft,\nthe objective is to obtain target Minecraft items\nby crafting them from available items in the envi-\nronment. We define an environment with three ac-\ntions: craft <item> using <ingredients>, get\n<item>, and inventory. We utilize Minecraft’s\ncrafting recipes to specify craftable items and their\ningredients, assuming that all other items are ob-\ntainable from the environment. Similar to Alf-\nWorld, our agent can directly execute these op-\nerations in the embodied game. The game begins\nwith a list of crafting commands provided to the\nagent that detail recipes that can be used to craft\nthe final target, its ingredients along with some dis-\ntractors (details in Appendix E). A reward of 1 is\ngenerated when the target item gets added to the\n16\nWebpage: [Search]\nTask: I am looking to buy a product. i need a leak proof travel bottle that\nis reusable and comes in 6 pack, and price lower than 30.00 dollars\nExecute: Task\n> search[leak proof reusable bottle]\nPage: \n[B0956WL7LT] ...\n[B098XRX6M2] ...\n> Think: Task failed!\nPlan: Task\n# Think: To do this task, ....\nStep 1: Search \"leak proof travel bottle reusable 6 pack\"  AND\n# Think: Now that I have found it, ....\nStep 2: Match product on search page AND\n# Think: Now that I have matched ....\nStep 3: Buy matching product\nExecute: Search \"leak proof travel bottle reusable 6 pack\"\n> Think: Task completed!\nPlan: Match product on search page\n# Think: To do this task, ....\nStep 1: Detail Match [B09...7LT] on product page OR\n# Think: If that does not match, ....\nStep 2: Detail Match [B08...GBG] on product page OR\n# Think: If that does not match, ....\nStep 3: Search \"leak proof bottle reusable 6 pack $30\"\nExecute: Detail Match [B09...7LT] on product page\n> Think: Task failed!\nExecute: Detail Match [B08...GBG] on product page\n> Think: Task completed!\nExecute: Buy matching product\n> Think: Task completed!\nMatching Product: \n[B08BGBPGBG] \n...\n...\n   ADaPT (Adaptive Decomposition, Dynamic Depth)\n...\n...\n...\nExecute: Match product on search page\n> Think: Task failed!\n...\nSearch Results:\n[B0956WL7LT] Portable Travel Bottles Set ...\n[B08BGBPGBG] JND Plastic Squeeze Bottle ...\n \nPrev: Failed Match product on search page\n...\nFigure 10: Partial rolled out trajectories for WebShop\nwith ADAPT. In the gray box we communicate to the\nplanner the current (search) page that is visible to the\nagent, and once a matching product is found, we propa-\ngate it to future executor runs. Note “match on search\npage” corresponds to shortlist skill in Table 5, and “de-\ntail match on product page” corresponds to match skill.\nagent’s inventory. An illustrative gold trajectory\nfrom TextCraft is shown in Fig. 3.\nWe create the TextCraft environment using\nMinecraft v1.16.5 recipes. We only consider the\nrecipes craftable using a crafting table. We consider\nboth shapeless (only count matters) and shaped\n(position of ingredients matters) recipes and con-\nvert them into crafting commands (e.g. craft 4\nsticks using 2 planks). Items that do not have\nany recipe are considering obtainable via the get\ncommand, e.g. get 4 diamond.\nSince the entire set of crafting commands would\nnot fit in the context of modern LLMs, we create\nCrafting Commands: \ncraft andesite slab using 3 andesite\ncraft 2 andesite using 1 diorite, 1 cobblestone\ncraft 6 diorite wall using 6 diorite\ncraft 2 diorite using 2 quartz, 2 cobblestone\nTask: craft andesite slab\nExecute: craft andesite slab\n> get 3 andesite\nCould not execute: get 3 andesite\n> get 1 diorite\nCould not execute: get 1 diorite\n> Think: Task failed!\nPlan: craft andesite slab\n# Think: To do this task, ....\nStep 1: Fetch 3 andesite  AND\n# Think: Now that I have found it, ....\nStep 2: Craft andesite slab using 3 andesite\nExecute: Fetch 3 andesite\n> Think: Task failed!\nPlan: Fetch 3 andesite\n# Think: To do this task, ....\nStep 1: Fetch 1 diorite AND\n# Think: Fetch the next ingredient, ....\nStep 2: Fetch 1 cobblestone AND\n# Think: Now that I have found, ....\nStep 3: Craft 2 andesite using 1 diorite, 1 cobblestone AND\n# Think: Since I need 3 andesite, ....\nStep 4: Craft 2 andesite using 1 diorite, 1 cobblestone\nExecute: Fetch 1 diorite\n> Think: Task completed!\nExecute: Fetch 1 cobblestone\n> Think: Task completed!\nExecute: Craft andesite slab using 3 andesite\n> Think: Task completed!\n...\n...\n   ADaPT (Adaptive Decomposition, Dynamic Depth)\n...\n...\n...\n...\nInventory: [diorite] (2)\nInventory: [diorite] (2), [cobblestone] (1)\nExecute:  Craft 2 andesite using 1 diorite, 1\ncobblestone\n> Think: Task completed!\n...\nInventory: [diorite] (1), [cobblestone] (1), [andesite] (2)\nExecute:  Craft 2 andesite using 1 diorite, 1\ncobblestone\n> Think: Task completed!\nInventory: [andesite] (4)\nFigure 11: Partial rolled out trajectories for TextCraft\nusing ADAPT. In the gray box, we propagate the in-\nventory of the agent to subsequent executor calls. Note\nthat while “diorite” is not directly present in the envi-\nronment, i.e., it needs to be crafted. The executor LLM\nis able to inherently compose skills to fetch it without\nfurther decomposition.\na set of relevant crafting commands for every task.\nApart from the set of gold crafting commands (i.e,\ncrafting commands for all the items in the recipe\ntree), we also add up to 10 distractor commands.\nTo create this distractor set, we sub-sample up to\n10 recipes for every ingredient in the recipes of our\ngold recipe tree. We finally sub-sample up to 10\ndistractors from this entire set to ensure a reason-\n17\nAlfWorld\nWebShop\nTextCraft\n0\n20\n40\n60\n80\nSuccess Rate\nGold Environment Reward\nSelf-generated Success Heuristic\nFigure 12: Comparison of LLM-generated success\nheuristic with gold environment rewards to compute\nsuccess rates for all datasets.\nable context size. Note that we do not provide the\nlist of valid get commands as that can be inferred\nfrom the craft commands.\nF\nEvaluation of Success Heuristic\nIn Sec. 3.1, we describe the executor module used\nin ADAPT. For tasks assigned to the executor,\nwe prompt the LLM to generate a binary success\nheuristic. We use this heuristic repeatedly to evalu-\nate if the (sub-)task needs to be decomposed further.\nWe now study the ability of LLMs to generate this\nsuccess heuristic on all our datasets. To this end,\nwe run ADAPT and in the end compare the success\nrate when using the LLM’s self-assessed task suc-\ncess with the gold reward from the environment in\nFig. 12. On ALFWorld and TextCraft, we find the\nLLM slightly over-estimates its overall task suc-\ncess. This is to be expected as the underlying tasks\ninvolve minimal subjectivity (e.g., the agent either\nhas an item on its inventory or not). However, on\nWebShop, where a product can match the user crite-\nria to different degrees (partially or fully), we find\nthat the LLM’s assessment is significantly inflated\ncompared to the environment reward (>30 points).\nThis imperfect feedback affects downstream per-\nformance of ADAPT, as the algorithm terminates\neven though further decomposition is needed. We\nleave it to future work to address the shortcomings\nof self-evaluation with LLMs (Huang et al., 2023a;\nStechly et al., 2023).\nG\nPrompts\nWe provide all the prompts used in our planner and\nexecutor modules for ALFWorld, WebShop, and\nTextCraft datasets in the following pages.\n18\nALFWorld Hybrid Executor Prompt\nHere is a demo of actions you can perform.\nYou are in the middle of a room. Looking quickly around you, you see a desk 1, microwave 1, a cabinet 3, a cabinet 9, a\ndrawer 2, a coffeemachine 1, a stoveburner 4, a drawer 5, a cabinet 11, a drawer 3, a stoveburner 1, a drawer 1, a\ntoaster 1, a fridge 1, a stoveburner 2, a cabinet 6, a cabinet 10, a countertop 1, a cabinet 13, a cabinet 7, a\ngarbagecan 1, a cabinet 2, a cabinet 8, a cabinet 12, a drawer 4, a cabinet 1, a sinkbasin 1, a cabinet 5, a\nstoveburner 3, and a cabinet 4.\nYour task is to: Put a tomato in cabinet 1.\n> think: Input assumption: I am carrying a tomato. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a tomato 1.\n> examine tomato 1\nThere's nothing special about tomato 1.\n> think: Based on previous context assumption is true, now I can proceed.\n> go to cabinet 1\nThe cabinet 1 is closed.\n> open cabinet 1\nYou open the cabinet 1. The cabinet 1 is open. In it, you see a candle 1, and a spraybottle 2.\n> put tomato 1 in\/on cabinet 1\nYou put the tomato 1 in\/on the cabinet 1.\n> think: End state: Now I put tomato (1) in cabinet (1). Task completed!\nYour task is to: Take a mug from cabinet 5\n> think: Input assumption: None\nOK.\n> go to cabinet 5\nThe cabinet 5 is closed.\n> open cabinet 5\nYou open the cabinet 5. The cabinet 5 is open. In it, you see a mug 3.\n> take mug 3 from cabinet 5\nYou pick up the mug 3 from the cabinet 5.\n> think: End state: Now I take a mug (3) from cabinet (5). Task completed!\nYour task is to: Heat an egg.\n> think: Input assumption: I am carrying an egg. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a egg 2.\n> examine egg 2\nThere's nothing special about egg 2.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> go to microwave 1\nThe microwave 1 is closed.\n> heat egg 2 with microwave 1\nYou heat the egg 2 using the microwave 1.\n> think: End state: Now I heat a egg (2). Task completed!\nYour task is to: Cool a mug.\n> think: Input assumption: I am carrying a mug. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a mug 3.\n> examine mug 3\nThere's nothing special about mug 3.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> go to fridge 1\nThe fridge 1 is closed.\n> cool mug 3 with fridge 1\nYou cool the mug 3 using the fridge 1.\n> think: End state: Now I cool a mug (3). Task completed!\nYour task is to: Clean a lettuce.\n> think: Input assumption: I am carrying a lettuce. Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a lettuce 1.\n> examine lettuce 1\nThere's nothing special about lettuce 1.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n> clean lettuce 1 with sinkbasin 1\nYou clean the lettuce 1 using the sinkbasin 1.\n> think: End state: Now I clean a lettuce (1). Task completed!\n19\nALFWorld Hybrid Executor Prompt (cont.)\nYour task is to: Use a desklamp to look at tomato 1.\n> think: Input assumption: I am at the location that contains desklamp (1). Now I need to verify this.\nOK.\n> inventory\nYou are carrying: a tomato 1.\n> look\nYou are facing desk 1. Next to it, you see nothing.\n> examine desk 1\nOn desk 1, you see a spoon 1, a desklamp 1, and a pencil 2.\n> think: Based on previous context assumption is true, now I can proceed.\nOK.\n> use desklamp 1\nYou turn on the desklamp 1.\n> think: Now I use desklamp (1). Task completed!\nHere are two example tasks.\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10,\na cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1,\na coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf 3,\na shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a\ntoaster 1.\nYour task is to: put a hot apple in fridge.\n> think: To solve the task, I need to find and take an apple, then heat it with microwave, then put it in fridge.\nOK.\n> think: First I need to find an apple. An apple is more likely to appear in fridge (1), diningtable (1), coffeetable (1),\ndrawer (1), cabinet (1-13), garbagecan (1). I can check one by one, starting with fridge 1.\nOK.\n> go to fridge 1\nThe fridge 1 is closed.\n> open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 1, and a egg 1.\n> go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a fork 3, a fork 2, a fork 1, a mug 2, a peppershaker 3, a plate 2, a\npot 1, a soapbottle 3, a spatula 1, a spoon 3, a spoon 2, a spoon 1, a winebottle 3, and a winebottle 2.\n> think: Now I find an apple (1). Next, I need to take it.\nOK.\n> take apple 1 from diningtable 1\nYou pick up the apple 1 from the diningtable 1.\n> think: Now I take an apple (1). Next, I need to go to a microwave (1) and heat it.\nOK.\n> go to microwave 1\nThe microwave 1 is closed.\n> heat apple 1 with microwave 1\nYou heat the apple 1 using the microwave 1.\n> think: Now I heat an apple (1). Next, I need to put it in\/on fridge 1.\nOK.\n> go to fridge 1\nThe fridge 1 is open. In it, you see a cup 1, and a egg 1.\n> put apple 1 in\/on fridge 1\nYou put the apple 1 in\/on the fridge 1.\n> think: Task completed!\nYou are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 1, a drawer 3, a drawer 2, a drawer 1,\na garbagecan 1, a safe 1, a shelf 5, a shelf 4, a shelf 3, a shelf 2, a shelf 1, a sidetable 2, and a sidetable 1.\nYour task is to: look at bowl under the desklamp.\n> think: To solve the task, I need to find and take a bowl, then find and use a desklamp.\nOK.\n> think: First I need to find a bowl. A bowl is more likely to appear in drawer shelf (1-5), drawer (1-3). I can check one\nby one, starting with drawer 1.\nOK.\n> go to shelf 1\nOn the shelf 1, you see nothing.\n> go to shelf 2\nOn the shelf 2, you see a bowl 1.\n> think: Now I find a bowl (1). Next, I need to take it.\nOK.\n> take bowl 1 from shelf 2\nYou pick up the bowl 1 from the shelf 2.\n> think: Now I take a bowl (1). Next, I need to find a desklamp. A desklamp is more likely to appear in desk (1), sidetable\n(1-2), and drawer (1-3). I can check one by one, starting with desk 1.\nOK.\n> go to desk 1\nOn the desk 1, you see a alarmclock 2, a book 1, a cellphone 1, a keychain 1, a laptop 2, a laptop 1, and a pen 3.\n> go to sidetable 1\nOn the sidetable 1, you see a cd 1, a pen 1, and a pencil 1.\n> go to sidetable 2\nOn the sidetable 2, you see a alarmclock 1, a desklamp 1, and a pen 2.\n> think: Now I find a desklamp (1). Next, I need to use it.\nOK.\n> use desklamp 1\nYou turn on the desklamp 1.\n> think: Task completed!\n20\nALFWorld Planner Prompt\nHere are some examples.\nYou are in the middle of a room. Looking quickly around you, you see a desk 1, microwave 1, a cabinet 3, a cabinet 9, a\ndrawer 2, a coffeemachine 1, a stoveburner 4, a drawer 5, a cabinet 11, a drawer 3, a stoveburner 1, a drawer 1, a\ntoaster 1, a fridge 1, a stoveburner 2, a cabinet 6, a cabinet 10, a countertop 1, a cabinet 13, a cabinet 7, a\ngarbagecan 1, a cabinet 2, a cabinet 8, a cabinet 12, a drawer 4, a cabinet 1, a sinkbasin 1, a cabinet 5, a\nstoveburner 3, and a cabinet 4.\nGoal: Put a mug in\/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug and then put it on desk. First, I will focus on finding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on putting it in\/on desk.\nStep 2: Put mug in\/on desk\nExecution Order: (Step 1 AND Step 2)\nGoal: Clean mug and put it in\/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, clean it, and then put it on desk. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on cleaning it.\nStep 2: Clean mug with sinkbasin\n# Think: Now that I have cleaned mug, I will focus on putting it in\/on desk.\nStep 3: Put cleaned mug in\/on desk\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: Cool mug and put it in\/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, cool it, and then put it on desk. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on cooling it.\nStep 2: Cool mug with fridge\n# Think: Now that I have cooled mug, I will focus on putting it in\/on desk.\nStep 3: Put cooled mug in\/on desk\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: Heat mug and put it in\/on desk.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, heat it, and then put it on desk. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I am carrying mug, I will focus on heating it.\nStep 2: Heat mug with microwave\n# Think: Now that I have heated mug, I will focus on putting it in\/on desk.\nStep 3: Put heated mug in\/on desk\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: Look at mug under desklamp.\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task, I need to find and take mug, and then go to the desklamp and use it. First, I will focus on\nfinding mug.\nStep 1: Find and take mug\n# Think: Now that I have found and taken mug, I will focus on using the desklamp.\nStep 2: Use the desklamp\nExecution Order: (Step 1 AND Step 2)\nGoal: Find and take mug\nCome up with an abstract plan to perform this task in a couple of steps.\n# Think: To perform this task I need to find mug in the room. mug is likely to be in desk, cabinet, countertop, or drawer.\nNow I will focus on finding mug in each of these locations one by one.\nStep 1: Find and take mug from desk\n# Think: If mug not found so far, I will next look in the cabinet.\nStep 2: Find and take mug from cabinet\n# Think: If mug not found so far, I will next look in the countertop.\nStep 3: Find and take mug from countertop\n# Think: If mug not found so far, I will next look in the drawer.\nStep 4: Find and take mug from drawer\nExecution Order: (Step 1 OR Step 2 OR Step 3 OR Step 4)\nHere is the goal.\n<room>\nGoal: <task>.\nCome up with an abstract plan to perform this task in a couple of steps. Constraints: The robot can hold\/take\/put only one\nobject at a time to a location.\nEnsure each step can be understood independently and mentions the name of object.\nWhen stating the execution order, ensure that 'AND'\/'OR' statements are properly nested using brackets '()'.\n21\nWebShop Executor Prompt: Buy\nInstruction: Buy product [B078GWRC1J] that matches criteria: 3 ounce bottle of bright citrus deodorant for sensitive skin,\nand price lower than 50.00 dollars\nAction: think['I should be on the search page where I can see a list of products, I should verify that.']\nObservation: OK.\nAction: load\nObservation:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next >]\n[B078GWRC1J]\nBright Citrus Deodorant by Earth Mama | Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-\nOunce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\n$35.95\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\nAction: think['I can spot product [B078GWRC1J] and it is a clickable link since its enclosed in square brackets. I should\nclick on it.]\nObservation: OK.\nAction: click[B078GWRC1J]\nObservation:\n[Back to Search]\n[< Prev]\nscent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\n[Features]\n[Reviews]\n[Buy Now]\nAction: think[I'm already on the product page. I see multiple possible options enclosed in '[]' for scent and size for this\nproduct. To match the input criteria, I should click on options '[bright citrus]' for scent and '[3 ounce (pack of 1)\n]' for size one by one and then buy in the end.]\nObservation: OK.\nAction: click[bright citrus]\nObservation: You have clicked bright citrus.\nAction: click[3 ounce (pack of 1)]\nObservation: You have clicked 3 ounce (pack of 1).\nAction: think[My task is to buy the product, for it should to click 'buy now']\nObservation: OK.\nAction: click[Buy Now]\nObservation: You have clicked buy now.\nAction: think[I finished buying the product. Task completed!]\nHere is another task in which you need to buy a product. When you finish buying the product with the most relevant choices,\nuse 'think[Task completed']. If you cannot find the matching options or proceed, think['Task failed']. Note that you\ncan only click on text enclosed in '[]' on the webpage. Everything else is only a description, not valid with the \"\nclick\" action.\nInstruction: Buy product [{}] that matches the criteria: {}\n22\nWebShop Executor Prompt: Match (cont.)\nYou are given a webpage of an item on an online shopping website and a criteria. Your task is to answer if the product on\nthe page exactly matches the criteria. Not the criteria could have multiple requirements that should be checked one\nby one and all must satisfy for an exact match.\nHere are a few examples:\nCriteria: 3 ounce bottle of citrus deodorant for sensitive skin that is priced lower than $30 and natural.\nItem Page:\n[Back to Search]\n[< Prev]\nscent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n[Description]\nFeatures:\nNEW from Earth Mama (formerly Earth Mama Angel Baby), formulated especially for pregnancy, breastfeeding and sensitive\nskin\nContains organic grapefruit, tangerine and calendula\nNO propylene glycol, artificial fragrance, parabens or aluminum\nDermatologist tested and clinically tested for irritation\nBetter than natural organic! NSF\/ANSI 305 Certified by Oregon Tilth\n[Reviews]\n[Attributes]\n[Buy Now]\nAnswer: The product is available in 3 ounce size, is citrus and suitable for sensitive skin. It is also organic or natural.\nIts price is $10.99 which is less than $30.\nThus, the answer is True (exact match).\nCriteria: 3 ounce bottle of citrus deodorant for sensitive skin that is priced lower than $30 and natural.\nItem Page:\n[Back to Search]\n[< Prev]\nsize [3 ounce][3 ounce (pack of 1)]\nunit count [2.0][3.0]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\nPrice: $15.95\nRating: N.A.\n[Description]\nFeatures:\nAbout this item WHY ALUMINUM-FREE DEODORANT? Aluminum-free deodorants use more natural ingredients unlike antiperspirants,\nwhich use chemicals to block sweat. Safely fight odor for 24 hours with Barrel & Oak's deodorantsour gentle formula\nis easy on sensitive skin. START SMELLING LIKE THE MAN YOU WANT TO BE: Our mountain sage aluminum-free men's\ndeodorant is naturally fragranced with an outdoorsy scent of crisp conifer, sage, & citrus. Think sweet notes of\ncitrus with earthy tones of cedar & patchouli. PREMIUM INGREDIENTS FOR NATURAL FRAGRANCES: Our deodorants for men are\ncomposed of natural, essential oil-based scents. These natural fragrance deodorants are more subtle than their\nsynthetic counterparts, but they're better for you & the planet. DESIGNED FOR THE MODERN MAN: Barrel & Oak has a full\nspectrum of grooming & body care products that are designed with function, fragrance, & effective ingredients for\nthe health-conscious & practical modern man. Give your body what it deserves. EARTH-FRIENDLY, YOU-FRIENDLY, WALLET-\nFRIENDLY: Our premium products for men are scented with natural fragrances & essential oils, free of parabens,\nphthalates, & SLS, packaged in recyclable materials, cruelty-free, & vegan or vegetarian.\n[Reviews]\n[Attributes]\n[Buy Now]\nAnswer: The product is not citrus in nature. It does not match the criteria. It's price is $15.95 which is less than $30.\nThus, the answer is False (not an exact match).\nNow here is the criteria and item page for the another task. Try you best to determine exact match, otherwise, respond with\n\"False\", i.e., no exact match. Generate an explanation before the answer to justify your decision.\nCriteria: {}\nItem Page:\n{}\nAnswer:\n23\nWebShop Executor Prompt: Shortlist (cont.)\nYou are given a search page on an online shopping site with a list of products along with name and price. Based on this\ninformation, your task is return a list of product IDs (enclosed in []) of all products that exactly match all\nrequirements in the criteria. If the information provided is not enough to make a determination, return an empty list.\nHere are a few examples.\nSearch Page:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next >]\n[B078GWRC1J]\nBright Citrus Deodorant by Earth Mama | Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-\nOunce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\n$35.95\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n[B08SMG4WB9]\nEach & Every 2-Pack Natural Aluminum-Free Deodorant for Sensitive Skin with Essential Oils, Plant-Based Packaging (Citrus &\nVetiver, 2.5 Ounce (Pack of 2))\n$25.0\n[B08KVCCSD6]\nEach & Every 3-Pack, Natural Aluminum-Free Deodorant for Sensitive Skin Made with Essential Oils, 2.5 Oz. (Lavender & Lemon,\nCitrus & Vetiver, and Coconut & Lime)\n$35.0\nCriteria: less than 5 ounce citrus deodorant sensitive skin, price less than $30.\nAnswer: My requirements are 5 ounce, citrus deodrant, suitable for sensitive skin, and price less than $30. Looks like this\ninformation is available on the search page, so I can proceed.\nProducts B078GWRC1J, B08SMG4WB9 look suitable as they are less than 5 ounce, citrus and have price 10.99 and $25 less than\n$30. Thus, shortlisted IDs are shortlisted=['B078GWRC1J', 'B08SMG4WB9']\nCriteria: less than 5 ounce citrus deodorant sensitive skin, cruelty free.\nAnswer: My requirements are 5 ounce, citrus deodrant, suitable for sensitive skin, and cruelty-free. Since there is no\ninformation about cruelty free on the search page, I cannot proceed. Task failed!\nHere is another task with a different search page and criteria. List all the product ids (enclosed in []) from the search\npage that match ALL the requirements in the criteria. Name this list shortlisted. If you cannot make the\ndetermination about even 1 sub-criteria, do not make a guess, output \"task failed!\". Generate an explanation before\nthe answer to justify your decision.\nSearch Page:\n{}\nCriteria: {}\nAnswer:\n24\nWebShop Planner Prompt\nWrite an abstract plan to successfully complete the goal. In each step of the plan mention which module (including\narguments) that need to be called. Learn from and incorporate information from previous runs, e.g. do not repeat\npreviously successful or unsuccesful commands. Here are some examples:Information from previous run: -\nGoal: Buy 3 ounce bottle of citrus deodorant for sensitive skin, that is natural and priced less than 50.00 dollars.\n# Think: Based on the criteria and the search bar, I should query 3 ounce citrus deodorant sensitive skin. I have the\nfollowing constraints: natural and price lower than $30 which I can use to narrow down search results.\nStep 1: Search[3 ounce citrus deodorant sensitive skin]\n# Think: Now I will need to narrow down the search results for price lower than $30 and natural\nStep 2: SimpleMatch[3 ounce citrus deodorant sensitive skin with price lower than $50 and natural]\n# Think: Since it returns a list of up to 3 products, I will pick the first suitable product. For now, Ill denote its id as\nprod_id for placeholder.\nStep 3: Buy[prod_id, \"3 ounce bottle of citrus deodorant for sensitive skin, that is natural and priced less than 30.00\ndollars\"]\n#Think: My plan requrires all these steps to succeed sequentially, so I will use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nInformation from previous run:\n- Unable to get matching product using: SimpleMatch[3 ounce citrus deodorant sensitive skin with price lower than $30 and\nnatural]\n- Search results page:\n[Back to Search]\nPage 1 (Total results: 50)\n[Next >]\n[B078GWRC1J]\nBright Citrus Deodorant by Earth Mama | Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-\nOunce\n$10.99\n[B08KBVJ4XN]\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar &\nPatchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\n$35.95\n[B078GTKVXY]\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n[B08SMG4WB9]\nEach & Every 2-Pack Natural Aluminum-Free Deodorant for Sensitive Skin with Essential Oils, Plant-Based Packaging (Citrus &\nVetiver, 2.5 Ounce (Pack of 2))\n$25.0\n[B08KVCCSD6]\nEach & Every 3-Pack, Natural Aluminum-Free Deodorant for Sensitive Skin Made with Essential Oils, 2.5 Oz. (Lavender & Lemon,\nCitrus & Vetiver, and Coconut & Lime)\n$35.0\n[B087WKSR2G]\nGoal: Narrow down search results for 3 ounce bottle of citrus deodorant for sensitive skin that is priced lower than $30\nand natural. You cannot search again.\n#Think: Based on the search results and previous information, SimpleMatch failed because my criteria was too complex. Price\nconstraint is easy to verify, I will narrow down based on that first then examine in detail for natural constraint\n#Think: Based on price, I narrow down my search to B078GWRC1J, B08SMG4WB9 as they look suitable. These are on my shortlist\nto examine the natural constraint in detail one by one.\nStep 1: DetailMatch[B078GWRC1J, 3 ounce bottle of for sensitive skin, that is natural and priced less than 30.00 dollars]\nStep 2: DetailMatch[B08SMG4WB9, 3 ounce bottle of citrus deodorantcitrus deodorant for sensitive skin, that is natural and\npriced less than 30.00 dollars]\n#Think: If none of the products exactly match my criteria, I will search again with a new query that includes the natural\ncriteria too. This ensures my plan is compelete.\nStep 3: Search[3 ounce citrus deodrant natural and sensitive skin]\n#Think: Since these steps are linked by an if condition, I only need one of them to succeed. I will connect them using the\n\"OR\" operator.\nExecution Order: (Step 1 OR Step 2 OR Step 3)\nHere is a new goal. Write an abstract plan to successfully complete the goal. In each step of the plan mention which module\n(including arguments) that need to be called. Learn from and incorporate information from previous runs, e.g. do not\nrepeat previously successful or unsuccesful commands. In the end, output the intended execution order.\nInformation from previous run: {}\nGoal: {}\n25\nTextCraft Executor Prompt\nYou are given few useful crafting recipes to craft items in Minecraft. Crafting commands are of the format \"craft [target object]\nusing [input ingredients]\". You can either \"fetch\" an object (ingredients) from the inventory or the environment or \"craft\"\n(target) using any of the crafting commands. You can use ONLY these crafting commands provided, do not use your own crafting\ncommands. However, if the crafting command uses a generic ingredient like \"planks\", you can use special types of the same\ningredient e.g. \"dark oak planks\" in the command instead. For any other natural language or thoughts, use prefix 'think: '.\nHere is a demo of how to fetch and craft objects.\nCrafting commands:\ncraft 3 dark oak sign using 6 dark oak planks, 1 stick\ncraft 4 dark oak planks using 1 dark oak log\ncraft 1 stick using 1 planks\ncraft 4 stick using 2 bamboo\ncraft 4 oak planks using 1 oak log\ncraft 1 dark oak fence using 2 stick, 4 dark oak planks\ncraft 1 warped stairs using 6 warped planks\ncraft 3 oak sign using 6 oak planks, 1 stick\nGoal: craft dark oak sign\n> think: I should check if I can fetch dark oak sign directly from the environment or the inventory.\nOK.\n> inventory:\nInventory: [stick] (1) [dark oak planks] (8)\n> get dark oak sign\nCould not find dark oak sign\n> think: I cannot get dark oak sign directly, I need to craft it. From the crafting commands, I can use: craft dark oak sign using\n6 dark oak planks, 1 stick. Ingredients needed: 6 dark oak planks, 1 stick. Input assumption: I have all the neccessary\ningredients in my inventory. Let me verify this first.\nOK.\n> inventory\nInventory: [stick] (1) [dark oak planks] (8)\n> think: I found my ingredients: 6 dark oak planks, 1 stick in my inventory. My assumption is true, I can proceed. I will use the\ncrafting command: craft dark oak sign using 6 dark oak planks\nOK.\n> craft 1 dark oak sign using 6 dark oak planks, 1 stick\nCrafted 1 minecraft:dark_oak_sign\n> inventory\nInventory: [dark oak sign] (1)\n> think: I now have dark oak sign in my inventory. Task Completed!\nOK.\nGoal: fetch 2 dark oak logs.\n> think: I should check my inventory first, to see if I already have dark oak sign. Otherwise, I will directly try to get it from\nthe environment.\nOK.\n> inventory\nInventory: [stick] (1)\n> get 2 dark oak logs.\nGot 2 dark oak logs\n> inventory\nInventory: [dark oak log] (2) [stick] (1)\n> think: I have 2 dark oak logs in my inventory. Task Completed!\nOK.\nTextCraft Executor Prompt (cont.)\nHere is an example of a complex goal.\nGoal: craft 2 oak planks\n> think: I should check if I can fetch dark oak sign directly from the environment or the inventory.\nOK.\n> inventory:\nInventory: [stick] (1)\n> get 2 oak planks\nCould not find 2 oak planks\n> think: I cannot get oak planks directly, I need to craft it. From the crafting commands, I can use: craft 4 oak planks using 1\noak log. Ingredients needed: 1 dark oak log. Input assumption: I have the ingredients needed in my inventory. Let me verify\nthis first.\nOK.\n> inventory\nInventory: [stick] (1)\n> think: My assumption is false. I need to get the ingredients: 1 oak log first.\nOK.\n> get 1 oak log\nGot 1 oak log\n> craft 4 oak planks using 1 oak log\nCrafted 4 minecraft:oak_planks\n> inventory\nInventory: [stick] (1) [oak_planks] (4)\n> think: I now have 2 oak planks in my inventory. Task Completed!\nOK.\nNow here is a different goal. You can use these crafting commands to accomplish the goal. When you the desired item in your\ninventory, think: Task Completed! If you have tried your best but cannot proceed, think: task failed!\n26\nTextCraft Planner Prompt\nYour task is to come up with a short plan to help me accomplish my goal in a couple of steps using at most ONE of the\nprovided crafting commands. You can take the help of crafting commands below to create new objects.\nCraft command can be understood as follows: craft [target] using [ingredients], where target is item\/object generated by\nthe craft command as output and ingredient are the inputs. You are given an agent that can \"craft\" or \"fetch\" objects.\nHere is are some examples.\nCrafting commands:\ncraft 3 dark oak sign using 6 dark oak planks, 1 stick\ncraft 4 dark oak planks using 1 dark oak log\ncraft 1 stick using 1 planks\ncraft 4 stick using 2 bamboo\ncraft 4 oak planks using 1 oak log\ncraft 1 dark oak fence using 2 stick, 4 dark oak planks\ncraft 1 warped stairs using 6 warped planks\ncraft 3 oak sign using 6 oak planks, 1 stick\nGoal: craft dark oak sign.\n# Think: My target is a dark oak sign. From the list of crafting commands, only 1 command generates my target: craft 3 dark\noak sign using 6 oak planks, 1 stick. I will use this command to devise a plan. My ingredients are: 6 dark oak\nplanks, 1 stick. I should first get all the ingredients and then use the crafting command.\nStep 1: fetch 6 dark oak planks\nStep 2: fetch 1 stick\n# Think: Now that I have collected the input ingredients, I can craft the dark oak sign using given command.\nStep 3: craft dark oak sign using 6 dark oak planks, 1 stick\n# Think: To succeed, I need to perform all these steps, one after the other. So I need to use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nGoal: fetch 6 dark oak planks.\n# Think: My target is 6 dark oak planks. From the list of crafting commands, only 1 command generates my target: craft 4\ndark oak planks using 1 dark oak log. My ingredients are: 1 dark oak log. To successfully accomplish the goal, I\nshould first get all the ingredients and then use the crafting command.\nStep 1: fetch 1 dark oak log\n# Think: Now that I have collected the input ingredients, I can craft dark oak planks using given command. I know that I\ncannot use a partial recipe.\nStep 2: craft 4 dark oak planks using 1 dark oak log\n# Think: This gives me 4 dark oak planks which is less than my desired 6 dark oak planks. I know that I cannot use a\npartial recipe. So my goal is not satisfied, I need to craft more dark oak planks by repeating Step 2 one more time.\nStep 3: craft 4 dark oak planks using 1 dark oak log\n# Think: To succeed, I need to perform all these steps, one after the other. So I need to use the \"AND\" operator.\nExecution Order: (Step 1 AND Step 2 AND Step 3)\nHere is a different goal with different craft commands. Your task is to come up with a short plan to help me accomplish my\ngoal in a couple of steps using at most ONE of the provided crafting commands. You can take the help of crafting\ncommands below to create new objects. Keep in mind that:\n- It is okay to generate more target objects than your goal.\n- Be very careful with the count of objects, SAME object counts mentioned in the input crafting command.\n- You cannot use a partial crafting command recipe, i.e. if the recipe generates 2 objects you CANNOT alter it to produce\njust 1.\n- Also, you can use ONLY 1 crafting command in your plan.\n27\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ADaPT: As-Needed Decomposition and Planning with Language Models.pdf"}
{"title":"Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game","authors":"Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, Yali Du","summary":"Multi-agent collaboration with Large Language Models (LLMs) demonstrates\nproficiency in basic tasks, yet its efficiency in more complex scenarios\nremains unexplored. In gaming environments, these agents often face situations\nwithout established coordination protocols, requiring them to make intelligent\ninferences about teammates from limited data. This problem motivates the area\nof ad hoc teamwork, in which an agent may potentially cooperate with a variety\nof teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork\nproblem where the agent operates in an environment driven by natural language.\nOur findings reveal the potential of LLM agents in team collaboration,\nhighlighting issues related to hallucinations in communication. To address this\nissue, we develop CodeAct, a general agent that equips LLM with enhanced memory\nand code-driven reasoning, enabling the repurposing of partial information for\nrapid adaptation to new teammates.","url":"http:\/\/arxiv.org\/abs\/2312.17515v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.17515v1","published":1703838414000,"comment":"Code will release soon","pdf_text":"Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork\nin the Avalon Game\nZijing Shi1, Meng Fang2, Shunfeng Zheng1, Shilong Deng2, Ling Chen1, Yali Du3\n1 AAII, University of Technology Sydney, Australia\n{zijing.shi, shunfeng.zheng}@student.uts.edu.au, ling.chen@uts.edu.au\n2 University of Liverpool, United Kingdom\n{Meng.Fang, shilong.deng}@liverpool.ac.uk\n3 King’s College London, United Kingdom\nyali.du@kcl.ac.uk\nAbstract\nMulti-agent collaboration with Large Language\nModels (LLMs) demonstrates proficiency in\nbasic tasks, yet its efficiency in more complex\nscenarios remains unexplored. In gaming en-\nvironments, these agents often face situations\nwithout established coordination protocols, re-\nquiring them to make intelligent inferences\nabout teammates from limited data. This prob-\nlem motivates the area of ad hoc teamwork,\nin which an agent may potentially cooperate\nwith a variety of teammates to achieve a shared\ngoal. Our study focuses on the ad hoc team-\nwork problem where the agent operates in an\nenvironment driven by natural language. Our\nfindings reveal the potential of LLM agents in\nteam collaboration, highlighting issues related\nto hallucinations in communication. To address\nthis issue, we develop CodeAct, a general agent\nthat equips LLM with enhanced memory and\ncode-driven reasoning, enabling the repurpos-\ning of partial information for rapid adaptation\nto new teammates.\n1\nIntroduction\nLarge Language Models (LLMs) have exhibited im-\npressive abilities in reasoning and generalization,\nshowing their potential in building autonomous\nagents and driving many advancements (Kojima\net al., 2022; Xi et al., 2023).\nRecent research\ninvolving multiple LLM agents working together\nhas shown success in task execution (Wang et al.,\n2023c; Wu et al., 2023; Dong et al., 2023). How-\never, these studies often employ explicit coordina-\ntion protocols for fully cooperative tasks, assum-\ning full information of the teammate and the task.\nWhile some research has explored simulating LLM\nagents in more open-ended settings, these agents\nremain limited by pre-established strategies and\nbehaviors (Park et al., 2023).\nIn dynamic environments, maintaining consis-\ntent coordination protocols among agents is a chal-\nlenge. Such contexts demand an agent capable of\nswift adaptation and on-the-fly cooperation. With-\nout pre-established team strategies, an agent, often\nreferred to as the learner, must observe its team-\nmates’ behaviors and adapt effectively to achieve\ntheir shared goals. This challenge is known as the\nAd Hoc Teamwork (AHT) problem (Stone et al.,\n2010; Macke et al., 2021).\nA significant challenge for the learner is to infer\nthe role of their teammates. Traditional methods\nrely on pre-programmed rules or data-driven al-\ngorithms trained on historical data (Mirsky et al.,\n2022). In contrast, LLM agents can directly com-\nmunicate with their teammates in natural language.\nHowever, the communication dynamics within the\nenvironment may exhibit conflicting and competi-\ntive elements. Moreover, due to the inherent traits\nof LLMs, teammates may provide inaccurate or\nmisleading information.\nIn this study, we explore the AHT abilities of\nLLM agents, utilizing the multi-agent game The\nResistance: Avalon to simulate AHT environments.\nWe introduce the AvalonPlay benchmark, which\nchallenges agents with hidden roles to participate in\nstrategic gameplay across multiple rounds. In each\nround, a learner is tasked with selecting teammates\nwithout prior strategies and full information. Our\nresearch integrates LLM agents into AvalonPlay to\nevaluate their performance in AHT. Rather than de-\nveloping coordination protocols for the entire team,\nwe prioritize designing agents adept at cooperating\nin environments without such explicit protocols.\nA communication channel is designed within the\nAvalonPlay benchmark to enhance collaboration.\nWe observe that LLM agents may sometimes pro-\nduce hallucinations and forget early information.\nTo address this, we introduce CodeAct, a general\nagent leveraging the advanced reasoning capabili-\nties of LLMs, focusing on reasoning about various\npotential teammate types. We begin with a memory\nretrieval system that collects information and uti-\nlize the knowledge gained from these interactions.\narXiv:2312.17515v1  [cs.CL]  29 Dec 2023\nFigure 1: A flowchart of the AvalonPlay benchmark showing team sides and roles on the left and a detailed round\npipeline on the right. Each round includes four stages: leader assignment, team selection, discussion and voting,\nquest execution.\nSubsequently, we incorporate code-driven reason-\ning with action to effectively refer the roles of new\nteammates. With the teammate role information,\nour agent is well-equipped to effectively cooperate\nwith its teammates.\nIn summary, our contributions are as follows:\n• We explore the AHT capabilities of LLM\nagents, utilizing the AvalonPlay benchmark\nwith its multi-round collaborative tasks as a\ntesting ground for these abilities.\n• We develop CodeAct, an LLM agent utilizing\na code-driven prompting approach, aimed at\nboosting the agent’s efficiency in AHT tasks.\n• We conduct comprehensive experiments\nwithin the AvalonPlay benchmark, demon-\nstrating the effectiveness of our proposed\nagent.\n2\nThe AvalonPlay Benchmark\nWe introduce the AvalonPlay benchmark to ex-\nplore the performance of LLM agents on AHT.\nThis benchmark is a language-based, multi-agent\nplatform comprised of multi-round tasks. During\nthe gameplay, agents have limited knowledge about\neach other’s roles. In each round, a team leader,\nalso regarded as the learner, is selected to deduce\nthe roles and intentions of other agents. The leader\nmust adapt their team’s strategies dynamically as\nthe game evolves. This section details the key ele-\nments of the proposed benchmark.\n2.1\nTeammate Role\nWe utilize a setup involving seven players indexed\nby j, each controlled by an LLM agent. In each\ngame, agents are randomly assigned one of six\ndistinct roles, dividing them into two factions: the\njust side (including Merlin, Percival, and two\nLoyal Servants of Arthur) and the evil side\n(including Morgana, Minion, and Assassin).\nEach role possesses unique abilities, as shown\non the left of Figure 1. On the just side, Merlin\nis aware of the factions but not the specific\nroles, while Percival can recognize Merlin and\nMorgana but is unable to distinguish between them.\nThe Loyal Servants of Arthur lack any special\nknowledge. On the evil side, in contrast, all roles\ncan recognize their teammates. This setup puts the\njust side at a disadvantage due to their limited in-\nformation. Our study focuses on scenarios where\nthe just side takes the lead role, aiming to enhance\nthe leader’s ability to deduce the various roles of\nagents.\n2.2\nPipeline\nIn AvalonPlay, a single game is composed of five\nquest rounds indexed by i. Each round, defined as\na full quest cycle, is divided into the following four\nphases, as shown in the right of Figure 1.\n• Leader Assignment: Each round begins with\nthe selection of a leader who nominates team-\nmates for the quest. The first round’s leader\nis randomly chosen, and in subsequent rounds,\nFigure 2: An overview of the proposed CodeAct agent as the leader during team selection. We begins by establishing\na memory retrieval system that distills information from past interactions, enabling the agent to access relevant\ninformation. Then, we integrate code-driven reasoning with action to determine teammate roles effectively. Finally,\nwe employ an interpreter to execute the generated code, equipping the agent with self-debug capabilities.\nleader rotates sequentially to the next player.\nThis approach ensures diverse leader roles while\nincorporating randomness.\n• Team Selection: In this phase, a leader nomi-\nnates ni teammates, with the team size ni vary-\ning per round. Adhering to the game rule, the\nteam sizes are: n1 = 2, n2 = 3, n3 = 3, n4 =\n4, n5 = 4.\nWe align the leader’s response\ninto corresponding actions. For instance, the\nresponse “I choose myself and player 2\nfor the quest team” is mapped to the specific\nselection of teammates.\n• Discussion and Voting: We design a communi-\ncation protocol that enables the agent to acquire\ninformation while accounting for the potential\nof misinformation. Post team selection, agents\nparticipate in discussions to assess the proposed\nteam. Following the discussion, each agent se-\ncretly votes 1j indicates agent j’s approval of\nthe current nominated team. To enhance agents’\ndecision-making, they are required to generate\na JSON containing their reasoning and vote. A\nteam is approved if the total votes reaches a ma-\njority (i.e., P7\nj=1 1j ≥4). Otherwise, the team\nselection is repeated with a new leader. A team is\nautomatically formed if approval is not achieved\nafter five consecutive attempts.\n• Quest Execution: For an approved team, team-\nmates from the just side must vote to approve,\nwhile teammates from the evil side must vote\nagainst. Let Qi denote the total votes of evil\nmembers. The quest is sabotaged if Qi ≥1\nfor rounds i ∈{1, 2, 3}, or Qi ≥2 for rounds\ni ∈{4, 5}. In other cases, it is completed suc-\ncessfully.\nWe adhere to game rule by including an assassi-\nnation phase in the Avalonplay benchmark, where\nthe Assassin identify Merlin after all rounds.\nHowever, our study aims to assess the teamwork\nof LLM agents, not the accuracy of assassinations,\nthough this feature is included for prospective fu-\nture research.\n2.3\nObservation Understanding\nThe AvalonPlay benchmark includes a moderator\nresponsible for overseeing the game’s progress.\nThis involves providing action descriptions to\nagents and recording events that occur during game-\nplay. The moderator’s role is non-interactive with\nthe LLMs but adheres to predefined procedures.\nThe information generated by the moderator is a\nvital part of the agents’ observation.\nWe develop structured prompts to assist LLM\nagents in comprehending both the game’s rules\nand its current state. In general, an LLM agent’s\nobservation is divided into four key variables to\nnavigate the game:\n• Game Rule: We begin with a basic understand-\ning of the game (e.g., “You are playing the\nAvalon game with other players...”).\n• Role Assignment: For each player, we ran-\ndomly assign their roles and create a unique role\nassignment prompt (e.g., Merlin is informed:\n“You are Merlin...Your objective is to\nensure the success of the quests as much\nas possible”).\n• Role Identification: Roles with special abili-\nties get extra information (e.g., Merlin receives\na moderator’s note: “Players 1, 2, and 6\nbelong to the evil side”).\n• Game History: This includes the game trajec-\ntory from previous rounds, as well as earlier time\nsteps of the current round.\n• Action Description: This includes a description\nof the next possible actions for the agent, pro-\nvided by the moderator (e.g., Player 2 receives\na moderator’s note: “Now it’s your turn,\nplayer 2.\nPlease discuss whether to\nagree with the nominated team”).\nThe game rule, role assignment, and role identi-\nfication collectively constitute the global prompt,\nconsistently employed at each time step. Game\nhistory and action description serve as real-time\nprompt, effectively capturing the game’s present\nstate. For additional details, please refer to Ap-\npendix A.\n3\nMethodology\n3.1\nOverview\nReasoning about various potential teammate types\nis a key issue in AHT research. To improve the\nagent’s reasoning abilities, we introduce the Code-\nAct agent, which utilizes the code-based reasoning\ncapabilities of LLMs. Our CodeAct agent does\nnot focus on creating prompts or demonstrations\nfor specific tasks. Instead, it emphasizes a general\nstrategy that transforms complex semantic tasks\ninto flexible code structures.\nOur approach unfolds in three stages, as shown\nin Figure 2. Firstly, we establish a memory retrieval\nsystem to extract information from previous interac-\ntions. Then, we integrate the code-driven reasoning\nwith action to reason teammate roles. Lastly, we\nemploy an interpreter to execute the code and equip\nthe agent with the self-debug capability.\n3.2\nMemory Retrieval\nDuring game playing, we implement global mem-\nory for each LLM agent to prevent the over-\nextension of token inputs. Particularly during the\nteam selection phase, the team leader is equipped\nwith a customized memory module, strategically\ndesigned to aid in deducing the roles of its team-\nmates.\nGlobal Memory\nThe global memory for each\nagent comprises identified key information along\nwith messages from the most recent k steps. This\nkey information includes: 1) Notes from the game\nmoderator; 2) Contributions from agents during dis-\ncussions, which reveal details of personal identities\nor deduce the identities of others.\nLeader-Specific Memory\nLLM agents often\nbase their reasoning on the latest information and\nfrequently experience hallucinations, as detailed in\nSection 4.5. In response, for each game round’s\nteam selection stage, we systematically retrieve fac-\ntual data gathered from earlier interactions to aid\nthe current leader. This includes: 1) Private Infor-\nmation, derived from unique skills tied to an agent’s\nrole, like Percival’s insight: “Among players 1\nand 4, one is good and the other evil, but\nyou can’t discern who is who.” 2) Public In-\nformation, pertaining to historical game outcomes\nand events, accessible to all agents, such as: “In\nthe initial round, players 1 and 2 were\nselected for the quest, which ended in\nfailure.”\nYou are engaged in a game with 7\nplayers , each potentially\ncategorized as 'good ' or 'bad '.\nThere are 4 good players and 3 bad\nplayers among them.\nYou have the fllowing information.\n{PRIVATE_INFORMATION}\n{PUBLIC_INFORMATION}\nPlease generate a code program that\nselects {self._current_team_number}\nplayers most likely to be good\nperson. Please print the final 4\nplayers.'''})\nFigure 3: An example of the CodeAct agent’s prompt.\n3.3\nCode-driven Reasoning with Action\nIn the team selection phase, we utilize a customized\nmemory system, allowing the current leader to de-\nduce the roles of its teammates. Drawing inspi-\nration from ReAct (Yao et al., 2022), our agent is\ndesigned for interactive generation of both thoughts\nand actions through a program in a zero-shot man-\nner. We utilize a program that processes useful\ninformation by reasoning over the agent’s current\nprivate and public data to support future actions.\nAs illustrated in Figure 3, we further enhance this\nprocess by enabling the leader to generate reason-\ning substeps in a code-like format. In this study,\nwe focus on Python, but our approach is applicable\nAction: Python_REPL\nAction Output:\nplayers = [None] * 7\n# Initialize the\nplayers list with None\n# Apply Round 2 information\nplayers [1] = True\n# Player 2 is good\nplayers [3] = True\n# Player 4 is good\nplayers [6] = True\n# Player 7 is good\n# Apply Round 3 information\n# Since Player 2 and Player 4 are good ,\nPlayer 1 must be bad\nplayers [0] = False\n# Player 1 is bad\n# Apply Round 4 information\n# Since Player 2 and Player 7 are good ,\nand we know Player 1 is bad , Player\n3 must be good\nplayers [2] = True\n# Player 3 is good\n# This leaves Player 5 as the only\npossible bad player from this group\nplayers [4] = False\n# Player 5 is bad\n# We already have 2 bad players (Player\n1 and Player 5), so Player 6 must\nbe good\nplayers [5] = True\n# Player 6 is good\n# Print the final 4 players most likely\nto be good\ngood_players = [index + 1 for index ,\nis_good in enumerate(players) if\nis_good]\nprint(good_players)\nObservation: [2, 3, 4, 6, 7]\nFinal Answer: The teammates are Players\n2, 3, 4, and 6.\nFigure 4: An example of the CodeAct agent’s output.\nto any programming language. Figure 4 provide\nan excerpt generated by our CodeAct, with more\ndetails available in Appendix B.\nOur code-driven approach, on the one hand,\nleverages advantages from semantic reasoning that\ninvolve intermediate steps. On the other hand, it\nleverages code-based reasoning, enabling reliable\ninference through the formal yet expressive code\nstructure and the powerful computational tools it\noffers.\n3.4\nCode Execution with Self-Debug\nWe equip the leader with the reflection mechanism\n(Shinn et al., 2023; Chen et al., 2023b), allowing\nthem to refine their programs. The code-based\naction, once formulated, is then processed by a\ncode interpreter. In this study, we equip the agent\nwith a Read-Eval-Print-Loop Python interpreter\n(i.e., Python_REPL). If the code executes success-\nfully, the agent’s decision is informed by the re-\nsulting program output. Conversely, if execution\nfalters, the agent will revise the code, taking into\naccount the issue encountered and previous outputs\nto enhance its generation.\n4\nExperiments\nOur experiments aim to explore the following ques-\ntions: 1) The AHT capabilities of agents employing\ndifferent LLMs as backend models; 2) The effect of\na natural language-based communication protocol\non the AHT capabilities of LLM agents; 3) The\nimpact of diverse prompting strategies on the AHT\ncapabilities of LLM agents. We compare the pro-\nposed CodeAct agent with other semantic-based\nreasoning approaches.\n4.1\nBaselines\nTo tackle question 1, we conduct a preliminary eval-\nuation of three advanced LLMs within the multi-\nagent AvalonPlay benchmark. These models in-\nclude GPT-3.5, GPT-4, and LLaMA 2. Specifically,\nwe utilize the versions gpt-3.5-turbo-0613,\ngpt-4-1106-preview, and Llama2-chat-13B re-\nspectively. These LLM agents, equipped with dif-\nferent backend LLMs, are deployed on both the\ngood and evil sides to compete against one another.\nTo address question 2, we compare scenarios\nin AvalonPlay both with and without communica-\ntion protocols. In scenarios lacking communication\nprotocols, all players proceed to a secret vote im-\nmediately following the team leader’s selection of\nthe team.\nTo tackle question 3, we focus on the team selec-\ntion by the leader, compare semantic reasoning ap-\nproaches (i.e., Chain of Thought (Wei et al., 2022)\nand ReAct (Yao et al., 2022)) with our proposed\nCodeAct agent.\n4.2\nMetrics\nWe design the following three metrics to measure\nan LLM agent’s performance in the AvalonPlay\nbenchmark.\n• Game Win Rate by Quest (Game Win), which\nrefers to the percentage of games where the good\nside wins by completing three out of five quests.\n• Total Quest Win Rate (Quest\nWin), which\nrefers to the percentage of quests where the good\nside wins.\n• Team Selection Accuracy (Team Acc), which\nrefers to the percentage of times the just side,\n(a) Game Win Rate by Quest\n(b) Total Quest Win Rate\n(c) Team Selection Accuracy\nFigure 5: The results of different models acting as the good and evil sides, playing games against each other. Under\neach setting, 30 games were conducted, totaling 150 quests.\n(a) GPT-3.5 as Good Side\n(b) GPT-4 as Good Side\n(c) LLaMA2 as Good Side\nFigure 6: The turn-by-turn quest win rate results when different models assume the role of the good side. Each\nmodel, as the good side, played 90 games, resulting in 450 quests evenly distributed across 5 turns.\nwhen in the role of leader, selects the team for a\nsuccessful quest.\n4.3\nImplementation Details\nFor all LLM agent, we set the sampling temperature\nas 0.7 for text generation and 0 for code generation.\nWe set the window size k to be 15 for the global\nmemory retrieval. During gameplay, all roles are\nrandomly assigned to each player to avoid biasing\nthe data towards certain players based on the turn\norder.\n4.4\nMain Results\nQuestion 1: Model Performance.\nFigure 5 dis-\nplays the performance of agents with three different\nbackend LLMs, playing on different sides. Figure 6\nshows the quest success rates of these LLM agents\nas the good side across different rounds. Our find-\nings indicate that 1) More advanced LLMs achieve\nhigher win rates both in games and in overall tasks\nwhen playing as the good side, with GPT-4 agents\nexhibiting the best performance. 2) Notably, GPT-\n4 agents demonstrate higher quest success rates\nin later rounds compared to earlier ones, indicat-\ning their ability to reason from observations. In\ncontrast, GPT-3.5 and LLaMA agents do not show\nsignificant progress across rounds. This suggests\nthat the GPT-4 agent is more adept at learning and\nadapting, leading us to utilize the GPT-4 model in\nsubsequent experiments.\nQuestion 2: Communication Channel.\nTable 2\ncompares the GPT-4 agent’s performance in self-\nplaying, both with and without communication.\nThe introduction of communication does not sig-\nnificantly enhance the game performance of LLM\nagents. This is possibly due to the fact that, besides\nthe communication protocol, LLM agents also have\naccess to observe and reason based on the outcomes\nof quests in previous rounds. These findings indi-\ncate that communication does not always lead to\nimproved collaborative efficiency in LLM agents.\nWe further explore this phenomenon in Section 4.5.\nQuestion 3: Code-driven Reasoning.\nWe cam-\npare the proposed CodeAct with two semantic rea-\nsoning methods: Chain of Thought (CoT) and Re-\nAct. Table 3 shows that our proposed CodeAgent\nachieves the best performance across three metrics,\nwith a team selection accuracy of 0.830, demon-\nstrating the superiority of code-based reasoning.\n4.5\nQuantitative Analysis\nIn our preliminary analysis of the experimental\nlogs targeting question 1, we observe that employ-\nThe 5th Round Team Selection and Discussion\nModerator:\nPlayer 3, as the team leader, please select 4 team members to undertake the quest. You may also choose yourself.\nPlayer 3:\nI choose player 2, player 3, player 6, and player 7 for the quest. (Team selection by the current leader)\nModerator:\nThe selected team by the leader include [’player 2’, ’player 3’, ’player 6’, ’player 7’]. Now everyone discuss if you agree the team.\nPlayer 1:\nThis team includes players who have been part of successful quests before. It’s a logical choice to maintain a strong team dynamic. I\nagree with the team selection.\nPlayer 2:\nPlayer 3, player 6, and player 7 were on the last successful quest with me, and I believe this consistency will help us ensure another\nvictory. I’m in favor of this team. (In the previous round, players 2, 3, 6, and 7 executed the quest together and succeeded. Player 2\neffectively used this information for reasoning)\nPlayer 3:\nI’ve chosen players who have either been part of successful quests or have not shown any suspicious behavior so far. It’s important\nto maintain a strong team dynamic, and I believe this selection will lead us to victory.\nPlayer 4:\nThe team seems solid with previous successful quest members. Including player 6 could either solidify their trustworthiness or\nprovide us with more information. I agree to proceed.\nPlayer 5:\nThe inclusion of player 6, who hasn’t been on a quest recently, along with proven successful quest members, provides a chance to\nvalidate their loyalty. We should be wary, but I am inclined to give this team a chance. (Player 5, however, produced hallucination,\nwith agent’s statement being inconsistent with the facts shared by everyone)\nPlayer 6:\nThe team includes players who have consistently been part of successful quests, and as a trusted member myself, I feel confident we\ncan achieve success once more. I agree with this selection and am prepared to contribute to the quest’s success.\nPlayer 7:\nThe team selected by player 3 includes myself and other players who have been part of successful quests, which instills confidence\nin this group’s potential for success. Player 6’s inclusion is a variable, but it is essential to evaluate all players. Based on the track\nrecord of success we’ve had, I agree with this team selection and am optimistic about our prospects.\nTable 1: An example of the GPT-4 agent’s participation in discussions during gameplay. In the fifth round, the\nagent’s response is based on previous observations and demonstrates some reasoning ability. However, the agent\nutilizes only limited information and is prone to experiencing hallucinations.\nTable 2: Results from the GPT-4 agent self-playing\nin the AvalonPlay benchmark, both with and without\ncommunication. Under each setting, 30 games were\nconducted, totaling 150 quests.\nAgent\/ Metric\nGame Win\nQuest Win\nTeam Acc\nBase\n0.500\n0.513\n0.640\nBase W\/O Communication\n0.466\n0.480\n0.693\nTable 3: Results of the GPT-4 agent self-playing in\nthe AvalonPlay benchmark using different prompting\nstrageties. Under each setting, 30 games were con-\nducted, totaling 150 quests.\nAgent\/ Metric\nGame Win\nQuest Win\nTeam Acc\nBase with CoT\n0.533\n0.547\n0.707\nBase with ReAct\n0.500\n0.526\n0.634\nCodeAct\n0.633\n0.593\n0.830\ning LLM agents in multi-turn tasks often results in\nmemory forgetting. In cooperative tasks involving\ncommunication, these agents tend to exhibit cas-\ncading hallucinations and logical inconsistencies.\nWe present Table 1 for a more detailed illustration.\nForgetting Early Information.\nPlayers typically\nfocus on information from the last round, failing to\nintegrate key information from the early stages of\nthe game into their later decisions and strategies.\nHallucination Generation.\nIn the game, state-\nments that contradict a player’s actual role are not\nclassified as hallucinations, as deception is a com-\nmon strategy. Similarly, fabricating scenarios to\nfalsely accuse others is also not considered halluci-\nnations, as this mirrors the strategies of human play-\ners. We define inconsistencies within a response\nand counterfactual content in an iteration as hal-\nlucinations. When GPT-3.5 and LLaMa 2 models\nserve as agents, they tend to produce responses in-\nconsistent with the prompt. A notable example is\ntheir role as team leaders, where they often select\na number of teammates that diverges from the re-\nquired amount. Additionally, the final votes they\ngenerate may not correspond with the reasoning\nprocess outlined in the previous step. This problem\nis less pronounced with the GPT-4 agent during\ngameplay. However, all models, including GPT-4,\nare prone to generating responses that contradict\nestablished facts, such as modifying or denying\nevents from previous game rounds. Table 1 pro-\nvides an example of GPT-4 agent’s participation in\ndiscussions. Player 5’s statements are inconsistent\nwith the facts. As player 2 points out, playe 6 had\nalready successfully participated in the quest in the\nprevious round.\nThis suggests that the communication protocol\ndoes not necessarily offer optimal solutions. The\nmost important aspect is the effective verification of\nthe factuality and correctness of communications.\nThis distinguishes it from prior AHT research with\nagents trained from historical data (Mirsky et al.,\n2020). In our study, we address this by utilizing\nmemory retrieval to extract factual information.\nAdditionally, we enhance the agent’s reasoning\nabilities for different types of teammates by in-\ntegrating code-based reasoning with actions.\n5\nRelated Work\nLLMs and Agents.\nThe advent of generative\nagents for interactive tasks marks a significant\nparadigm shift. Unlike traditional methods that re-\nlied on Deep Reinforcement Learning (DRL) (Shi\net al., 2022, 2023; Xu et al., 2020, 2021, 2022),\nwhich confined agents to constrained settings, the\nemergence of LLMs has notably broadened the\nscope of possibilities (OpenAI, 2023). Efforts are\nincreasingly focused on leveraging LLMs for com-\nplex tasks like planning and reasoning (Wang et al.,\n2023c; Wu et al., 2023; Dong et al., 2023). In-\nnovative prompting methods, including ReACT\n(Yao et al., 2022), Chain of Thought (CoT) (Wei\net al., 2022), and Tree of Thought (ToT) (Yao\net al., 2023), significantly enhance the planning\nand reasoning capabilities of LLMs.\nSome re-\nsearch integrates complex human cognitive func-\ntions into LLM agents, such as memory (Wang\net al., 2023b), reflection (Shinn et al., 2023), and\ntool usage (Huang et al., 2023).\nMulti-agent Interaction.\nRecent research has\nexplored that utilizing the interactions among mul-\ntiple LLM agents to achieve several goals (Chen\net al., 2023a; Wu et al., 2023). These studies fo-\ncus on two main areas. First, it involves assigning\ndistinct roles to agents to enhance their task per-\nformance (Wang et al., 2023a). Notable applica-\ntions include using multiple agents in debates to\ncounter self-reflection biases (Liang et al., 2023;\nChan et al., 2023), and diversifying agent roles for\ncomplex projects like software development (Qian\net al., 2023; Hong et al., 2023). Second, researchers\ndevelop multiple LLM agents in sandbox environ-\nments to mimic real human behavior (Park et al.,\n2023; Zhou et al., 2023). Some of these investigate\nthe capabilities that emerge in multi-agent settings,\nsuch as deception and leadership (Xu et al., 2023).\nAd Hoc Teamwork.\nAd Hoc teamwork aims to\ngeneralise the agent’s cooperation capacity with\nnovel agents (Stone et al., 2010; Du et al., 2023)\nThe progress in AHT has shifted from relying on\npreset action-selection rules to a data-driven strat-\negy. This strategy employs methods like proba-\nbilistic models (Rahman et al., 2023), deep learn-\ning (Chen et al., 2020), or reinforcement learning\n(Rahman et al., 2021; Lou et al., 2023; Yan et al.,\n2023) to predict the behaviors of various agents\nin different states, aiming to optimize and refine\nthe learner’s behavior. Recent research has also\nexplored the communication in AHT, considering\nscenarios where teammates follow a shared com-\nmunication protocol (Macke et al., 2021; Mirsky\net al., 2020). However, these studies are often con-\nfined to a limited multi-agent setting. Our work\nstands apart from previous studies by focusing on\ndynamic environments based on natural language\ncommunication. We employ agents powered by\nLLMs to execute AHT, and explore ways to im-\nprove their performance in these settings.\n6\nConclusion and Future Work\nIn this study, we investigate the AHT capabilities\nof LLM agents using the AvalonPlay benchmark.\nLLMs endow agents with powerful reasoning abil-\nities, but challenges such as early memory forget-\nting and hallucination generation also arise. We\naddress these challenges by proposing the CodeAct\nagent. We first employ a memory retrieval system\nto collect factual information from prior interac-\ntions. Subsequently, we integrate a code-driven\nreasoning with action to discern new teammate\nroles, enhancing our agent’s ability to cooperate\nmore effectively. Lastly, we employ an interpreter\nto execute the code and equip the agent with the\nself-debug capability.\nDespite our initial research, there are some un-\nresolved issues in the AHT research with lan-\nguage agents that require further investigation.\nFor intance, our AvalonPlay benchmark involves\nmoderator-guided communication for agents, mak-\ning it crucial to investigate the agent’s ability to\nautonomously decide when to communicate. Fur-\nthermore, significant work lies in reducing the ef-\nfects of hallucinations and enhancing their practical\napplication in real-world scenarios. In this study,\nwe employ a memory retrieval system for factual\ninformation extraction, and another promising re-\nsearch direction involves developing systems for\nevaluating and verifying facts to address these chal-\nlenges.\n7\nLimitations\nRegarding limitations, our study does not incorpo-\nrate experience pools from human players. Future\nresearch will focus on developing robust strategies\nfor leveraging this experience and enhancing com-\nparison with human performance.\nReferences\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\nWei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan\nLiu. 2023. Chateval: Towards better llm-based eval-\nuators through multi-agent debate. arXiv preprint\narXiv:2308.07201.\nShuo Chen, Ewa Andrejczuk, Zhiguang Cao, and Jie\nZhang. 2020. Aateam: Achieving the ad hoc team-\nwork by employing the attention mechanism.\nIn\nProceedings of the AAAI Conference on Artificial\nIntelligence, pages 7095–7102.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,\nChenfei Yuan, Chen Qian, Chi-Min Chan, Yujia\nQin, Yaxi Lu, Ruobing Xie, et al. 2023a. Agent-\nverse: Facilitating multi-agent collaboration and ex-\nploring emergent behaviors in agents. arXiv preprint\narXiv:2308.10848.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023b. Teaching large language mod-\nels to self-debug. arXiv preprint arXiv:2304.05128.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023.\nSelf-collaboration code generation via chatgpt. arXiv\npreprint arXiv:2304.07590.\nYali Du, Joel Z Leibo, Usman Islam, Richard Willis,\nand Peter Sunehag. 2023.\nA review of coop-\neration in multi-agent learning.\narXiv preprint\narXiv:2312.05162.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng\nCheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven\nKa Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023.\nMetagpt: Meta programming for multi-agent collabo-\nrative framework. arXiv preprint arXiv:2308.00352.\nYue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan\nWu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan,\nNeil Zhenqiang Gong, et al. 2023. Metatool bench-\nmark for large language models: Deciding whether\nto use tools and which to use.\narXiv preprint\narXiv:2310.03128.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems, 35:22199–\n22213.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,\nYan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and\nShuming Shi. 2023. Encouraging divergent thinking\nin large language models through multi-agent debate.\narXiv preprint arXiv:2305.19118.\nXingzhou Lou, Jiaxian Guo, Junge Zhang, Jun Wang,\nKaiqi Huang, and Yali Du. 2023.\nPecan: Lever-\naging policy ensemble for context-aware zero-shot\nhuman-ai coordination. In Proceedings of the 2023\nInternational Conference on Autonomous Agents and\nMultiagent Systems(AAMAS), pages 679–688.\nWilliam Macke, Reuth Mirsky, and Peter Stone. 2021.\nExpected value of communication for planning in ad\nhoc teamwork. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, pages 11290–11298.\nReuth Mirsky, Ignacio Carlucho, Arrasy Rahman, Elliot\nFosong, William Macke, Mohan Sridharan, Peter\nStone, and Stefano V Albrecht. 2022. A survey of\nad hoc teamwork: Definitions, methods, and open\nproblems. In European Conference on Multiagent\nSystems.\nReuth Mirsky, William Macke, Andy Wang, Harel\nYedidsion, and Peter Stone. 2020. A penny for your\nthoughts: The value of communication in ad hoc\nteamwork. Good Systems-Published Research.\nR OpenAI. 2023. Gpt-4 technical report.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S\nBernstein. 2023.\nGenerative agents: Interactive\nsimulacra of human behavior.\narXiv preprint\narXiv:2304.03442.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen,\nYusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong\nSun. 2023. Communicative agents for software de-\nvelopment. arXiv preprint arXiv:2307.07924.\nArrasy Rahman, Elliot Fosong, Ignacio Carlucho, and\nStefano V Albrecht. 2023. Generating teammates\nfor training robust ad hoc teamwork agents via best-\nresponse diversity. Transactions on Machine Learn-\ning Research.\nMuhammad A Rahman, Niklas Hopner, Filippos Chris-\ntianos, and Stefano V Albrecht. 2021. Towards open\nad hoc teamwork using graph-based policy learning.\nIn International Conference on Machine Learning,\npages 8776–8786. PMLR.\nZijing Shi, Meng Fang, Yunqiu Xu, Ling Chen, and Yali\nDu. 2022. Stay moral and explore: Learn to behave\nmorally in text-based games. In The Eleventh Inter-\nnational Conference on Learning Representations.\nZijing Shi, Yunqiu Xu, Meng Fang, and Ling Chen.\n2023. Self-imitation learning for action generation\nin text-based games. In Proceedings of the 17th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics, pages 703–726.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023.\nReflexion: an autonomous agent with dy-\nnamic memory and self-reflection. arXiv preprint\narXiv:2303.11366.\nPeter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey\nRosenschein. 2010. Ad hoc autonomous agent teams:\nCollaboration without pre-coordination. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\npages 1504–1509.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao\nYang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,\nXu Chen, Yankai Lin, et al. 2023a. A survey on large\nlanguage model based autonomous agents. arXiv\npreprint arXiv:2308.11432.\nWeizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu,\nXifeng Yan, Jianfeng Gao, and Furu Wei. 2023b.\nAugmenting language models with long-term mem-\nory. arXiv preprint arXiv:2306.07174.\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu,\nTao Ge, Furu Wei, and Heng Ji. 2023c. Unleash-\ning cognitive synergy in large language models:\nA task-solving agent through multi-persona self-\ncollaboration. arXiv preprint arXiv:2307.05300.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,\nXiaoyun Zhang, and Chi Wang. 2023.\nAuto-\ngen: Enabling next-gen llm applications via multi-\nagent conversation framework.\narXiv preprint\narXiv:2308.08155.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\nSenjie Jin, Enyu Zhou, et al. 2023. The rise and\npotential of large language model based agents: A\nsurvey. arXiv preprint arXiv:2309.07864.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, and\nChengqi Zhang. 2021. Generalization in text-based\ngames via hierarchical reinforcement learning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey\nZhou, and Chengqi Zhang. 2022. Perceiving the\nworld: Question-guided reinforcement learning for\ntext-based games. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nYunqiu Xu,\nMeng Fang,\nLing Chen,\nYali Du,\nJoey Tianyi Zhou, and Chengqi Zhang. 2020. Deep\nreinforcement learning with stacked hierarchical at-\ntention for text-based games. Advances in Neural\nInformation Processing Systems, 33:16495–16507.\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-\naolong Wang, Weidong Liu, and Yang Liu. 2023.\nExploring large language models for communica-\ntion games: An empirical study on werewolf. arXiv\npreprint arXiv:2309.04658.\nXue Yan, Jiaxian Guo, Xingzhou Lou, Jun Wang,\nHaifeng Zhang, and Yali Du. 2023. An efficient\nend-to-end training approach for zero-shot human-\nai coordination. In Thirty-seventh Conference on\nNeural Information Processing Systems.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nPei Zhou, Aman Madaan, Srividya Pranavi Potharaju,\nAditya Gupta, Kevin R McKee, Ari Holtzman, Jay\nPujara, Xiang Ren, Swaroop Mishra, Aida Ne-\nmatzadeh, et al. 2023. How far are large language\nmodels from agents with theory-of-mind?\narXiv\npreprint arXiv:2310.03051.\nA\nGlobal Prompt for Agents\nHere, we show the global prompt used for the LLM agent in the AvalonPlay benchmark. We provide\nthe game rule and role assignment as the global prompt. Figure 7 displays the specific game rule for all\nagents, while Table 4 shows the role assignments for different agents.\nYou are playing the Avalon game with some other players. The moderator is also\nhost , he organised this game and you need to follow his instructions correctly.\nDo not talk with the moderator. What the modeler said is unquestionable and\nabsolutely true. There are six roles in the game. Merlin , Percival , and Loyal\nServants of Arthur form the just side , with members only knowing their own role\nand voting to approve in secret ballots to make quests successful. Morgana ,\nAssassin , and Minion belong to the evil side. They are privy to their\nteammates ' identities and vote against in secret ballots to make the quests\nfail.\nTips: If you think a player is evil side , try not to let them be selected into the\nquest team. If this player is the leader , you can disagree with the\nestablishment of this team. Reasoning based on facts you have observed and you\ncannot perceive information (such as acoustic info) other than text. Do not\npretend you are other players or the moderator.\nFigure 7: Game rule prompt for all LLM agents in the AvalonPlay benchmark.\nRole\nGlobal Prompt\nMerlin\n{Game Rule} + You are Merlin, aware of the evil presences, yet you must remain discreet in sharing this\nknowledge. Should the assassin identify you after the just side completes three quests, the evil side shall emerge\nvictorious. Your objective is to ensure the success of the quests.\nPercival\n{Game Rule} + As Percival, you recognize Merlin, but be wary as Morgana might mislead you by appearing as\nMerlin. Your allegiance is with the just side. Your objective is to ensure the success of the quests.\nGood\nServant\n{Game Rule} + You are a Servant of Arthur, with no special abilities, belonging to the just side. Your objective\nis to ensure the success of the quests.\nMorgana {Game Rule} + You are Morgana, appearing as Merlin to Percival, weaving a web of confusion. Your objective\nis to ensure the quest fails.\nAssassin {Game Rule} + As the Assassin, your critical role comes into play once the just side successfully executes three\nquests; identifying Merlin can secure a win for the evil side. Your objective is to ensure the quest fails.\nEvil\nMinion\n{Game Rule} + You are a Minion of Mordred, without special abilities, on the evil side. Your objective is to\nensure the quest fails.\nTable 4: Role assignment prompt for LLM agents in various roles within the AvalonPlay benchmark.\nB\nAn Example of Code-driven Reasoning Generation\nIn this section, we provide a complete example of code-driven reasoning with action generation produced\nby the GPT-4 model, as shown in Figure 8.\nThought Output:\nLet 's start by creating a list of 7 elements to represent the players , with None as\nthe initial value since we don 't know their alignment yet. We will then apply\nthe constraints from each round to update this list.\n- From Round 2, we know that Players 2, 4, and 7 are good.\n- From Round 1, we know that either Player 1 or Player 3 is bad , but not both.\n- From Round 3, we know that there is at least one bad person among Player 1,\nPlayer 2, and Player 4. Since Players 2 and 4 are good , Player 1 must be bad.\n- From Round 4, we know that there is at most one bad person among Player 2, Player\n3, Player 5, and Player 7. Since Player 2 and Player 7 are good , and we know\nPlayer 1 is bad , Player 3 must be good , leaving Player 5 as the only possible\nbad player from this group.\nAfter applying these constraints , we should be able to identify the 4 players most\nlikely to be good.\nLet 's write the code to apply these constraints and find the 4 good players.\nAction: Python_REPL\nAction Output:\nplayers = [None] * 7\n# Initialize the players list with None\n# Apply Round 2 information\nplayers [1] = True\n# Player 2 is good\nplayers [3] = True\n# Player 4 is good\nplayers [6] = True\n# Player 7 is good\n# Apply Round 1 information\n# We cannot determine which one is bad yet , so we leave Player 1 and Player 3 as\nNone\n# Apply Round 3 information\n# Since Player 2 and Player 4 are good , Player 1 must be bad\nplayers [0] = False\n# Player 1 is bad\n# Apply Round 4 information\n# Since Player 2 and Player 7 are good , and we know Player 1 is bad , Player 3 must\nbe good\nplayers [2] = True\n# Player 3 is good\n# This leaves Player 5 as the only possible bad player from this group\nplayers [4] = False\n# Player 5 is bad\n# Now we can determine the remaining players ' goodness based on the total number of\nbad players\n# We already have 2 bad players (Player 1 and Player 5), so Player 6 must be good\nplayers [5] = True\n# Player 6 is good\n# Print the final 4 players most likely to be good\ngood_players = [index + 1 for index , is_good in enumerate(players) if is_good]\nprint(good_players)\nObservation: [2, 3, 4, 6, 7]\nThought:I now know the final answer\nFinal Answer: The 4 players most likely to be good are Players 2, 3, 4, and 6.\nFigure 8: An example of the code-driven reasoning with action generation.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game.pdf"}
{"title":"Counting to Explore and Generalize in Text-based Games","authors":"Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, Adam Trischler","summary":"We propose a recurrent RL agent with an episodic exploration mechanism that\nhelps discovering good policies in text-based game environments. We show\npromising results on a set of generated text-based games of varying difficulty\nwhere the goal is to collect a coin located at the end of a chain of rooms. In\ncontrast to previous text-based RL approaches, we observe that our agent learns\npolicies that generalize to unseen games of greater difficulty.","url":"http:\/\/arxiv.org\/abs\/1806.11525v2","pdf_url":"http:\/\/arxiv.org\/pdf\/1806.11525v2","published":1530290429000,"comment":null,"pdf_text":"Counting to Explore and Generalize in Text-based Games\nXingdi Yuan * 1 Marc-Alexandre Cˆot´e * 1 Alessandro Sordoni 1 Romain Laroche 1 Remi Tachet des Combes 1\nMatthew Hausknecht 1 Adam Trischler 1\nAbstract\nWe propose a recurrent RL agent with an episodic\nexploration mechanism that helps discovering\ngood policies in text-based game environments.\nWe show promising results on a set of generated\ntext-based games of varying difﬁculty where the\ngoal is to collect a coin located at the end of a\nchain of rooms. In contrast to previous text-based\nRL approaches, we observe that our agent learns\npolicies that generalize to unseen games of greater\ndifﬁculty.\n1. Introduction\nText-based games like Zork (Infocom, 1980) are complex,\ninteractive simulations. They use natural language to de-\nscribe the state of the world, to accept actions from the\nplayer, and to report subsequent changes in the environment.\nThe player works toward goals which are seldom speci-\nﬁed explicitly and must be discovered through exploration.\nThe observation and action spaces in text games are both\ncombinatorial and compositional, and players must contend\nwith partial observability, since descriptive text does not\ncommunicate complete, unambiguous information about\nthe underlying game state.\nIn this paper, we study several methods of exploration in\ntext-based games. Our basic task is a deterministic text-\nbased version of the chain experiment (Osband et al., 2016;\nPlappert et al., 2017) with distractor nodes that are off-chain:\nthe agent must navigate a path composed of discrete loca-\ntions (rooms) to the goal, ideally without revisiting dead\nends. We propose a DQN-based recurrent model for solving\ntext-based games, where the recurrence gives the model\nthe capacity to condition its policy on historical state infor-\nmation. To encourage exploration, we extend count-based\nexploration approaches (Ostrovski et al., 2017; Tang et al.,\n*Equal contribution 1Microsoft Research. Correspondence to:\nEric Yuan <eric.yuan@microsoft.com>, Marc-Alexandre Cˆot´e\n<macote@microsoft.com>.\nPublished at the Exploration in Reinforcement Learning Work-\nshop at the 35 th International Conference on Machine Learning,\nStockholm, Sweden. Copyright 2018 by the author(s).\n2017), which assign an intrinsic reward derived from the\ncount of state visitations during learning, across episodes.\nSpeciﬁcally, we propose an episodic count-based explo-\nration scheme, where state counts are reset at the beginning\nof each episode. This reward plays the role of an episodic\nmemory (Gershman & Daw, 2017) that pushes the agent to\nvisit states not previously encountered within an episode.\nAlthough the recurrent policy architecture has the capacity\nto solve the task by remembering and avoiding previously\nvisited locations, we hypothesize that exploration rewards\nwill help the agent learn to utilize its memory.\nWe generate a set of games of varying difﬁculty (measured\nwith respect to the path length and the number of off-chain\nrooms) with a text-based game generator (Cˆot´e et al., 2018).\nWe observe that, in contrast to a baseline model and stan-\ndard count-based exploration methods, the recurrent model\nwith episodic bonus learns policies that not only complete\nmultiple training games at same time successfully but also\ngeneralize to unseen games of greater difﬁculty.\n2. Text-based Games as POMDPs\nText-based games are sequential decision-making prob-\nlems that can be described naturally by the Reinforce-\nment Learning (RL) setting. Fundamentally, text-based\ngames are partially observable Markov decision processes\n(POMDP) (Kaelbling et al., 1998) where the environment\nstate is never observed directly.\nTo act optimally, an\nagent must keep track of all observations. Formally, a\ntext-based game is a discrete-time POMDP deﬁned by\n(S, T, A, Ω, O, R, γ), where γ ∈[0, 1] is the discount factor.\nEnvironment States (S): The environment state at turn t\nin the game is st ∈S. It contains the complete internal\ninformation of the game, much of which is hidden from the\nagent. When an agent issues a command ct (deﬁned next),\nthe environment transitions to state st+1 with probability\nT(st+1|st, ct).\nActions (A): At each turn t, the agent issues a text com-\nmand ct. The interpreter can accept any sequence of char-\nacters but will only recognize a tiny subset thereof. Further-\nmore, only a fraction of recognized commands will actually\nchange the state of the world. The resulting action space\narXiv:1806.11525v2  [cs.CL]  7 Mar 2019\nCounting to Explore and Generalize in Text-based Games\nis enormous and intractable for existing RL algorithms. In\nthis work, we make the following two simplifying assump-\ntions. (1) Word-level Each command is a two-word se-\nquence where the words are taken from a ﬁxed vocabulary V .\n(2) Command syntax Each command is a (verb, object)\npair (direction words are considered objects).\nObservations (Ω): The text information perceived by the\nagent at a given turn t in the game is the agent’s observation,\not ∈Ω, which depends on the environment state and the\nprevious command with probability O(ot|st, ct−1). Thus,\nthe function O selects from the environment state what\ninformation to show to the agent given the last command.\nReward Function (R): Based on its actions, the agent\nreceives reward signals rt = R(st, at). The goal is to maxi-\nmize the expected discounted sum of rewards E [P\nt γtrt].\n3. Method\n3.1. Model Architecture\nIn this work, we adopt the LSTM-DQN (Narasimhan et al.,\n2015) model as baseline. It has two modules: a representa-\ntion generator ΦR, and an action scorer ΦA. ΦR takes ob-\nservation strings o as input, after a stacked embedding layer\nand LSTM (Hochreiter & Schmidhuber, 1997) encoder, a\nmean-pooling layer produces a vector representation of the\nobservation. This feeds into ΦA, in which two MLPs, shar-\ning a lower layer, predict the Q-values over all verbs wv\nand object words wo independently. The average of the two\nresulting scores gives the Q-values for the composed actions.\nThe LSTM-DQN does not condition on previous actions\nor observations, so it cannot deal with partial observability.\nWe concatenate the previous command ct−1 to the current\nobservation ot to lessen this limitation.\nTo further enhance the agent’s capacity to remember previ-\nous states, we replace the shared MLP in ΦA by an LSTM\ncell. This model is inspired by (Hausknecht & Stone, 2015;\nLample & Chaplot, 2016) and we call it LSTM-DRQN. The\nLSTM cell in ΦA takes the representation generated by ΦR\ntogether with history information ht−1 from the previous\ngame step as input. It generates the state information at the\ncurrent game step, which is then fed into the two MLPs as\nwell as passed forward to next game step. Figure 1 shows\nthe LSTM-DRQN architecture.\n3.2. Discovery Bonus\nTo promote exploration we use an intrinsic reward by count-\ning state visits (Kolter & Ng, 2009; Tang et al., 2017; Martin\net al., 2017; Ostrovski et al., 2017). We investigate two ap-\nproaches to counting rewards. The ﬁrst is inspired by (Kolter\n& Ng, 2009), where we deﬁne the cumulative counting\nbonus as r+(ot) = β · n(ot)−1\/3, where n(ot) is the num-\nFigure 1. LSTM-DRQN processes textual observations word-by-\nword to generate a ﬁxed-length vector representation. This repre-\nsentation is used by the recurrent policy to estimate Q-values for\nall verbs Q(s, v) and objects Q(s, o).\nber of times the agent has observed ot since the beginning\nof training (across episodes), and β is the bonus coefﬁcient.\nDuring training, as the agent observes new states more and\nmore, the cumulative counting bonus gradually converges\nto 0.\nThe second approach is the episodic discovery bonus,\nwhich encourages the agent to discover unseen states by\nassigning a positive reward whenever it sees a new state.\nIt is deﬁned as: r++(ot) =\n\u001a\nβ\nif n(ot) = 1\n0.0\notherwise\n, where n(·)\nis reset to zero at the beginning of each episode. Taking\ninspiration from (Gershman & Daw, 2017), we hope this\nbehavior pushes the agent to visit states not previously en-\ncountered in the current episode and teaches the agent how\nto use its memory for this purpose so it may generalize to\nunseen environments.\n4. Related Work\nRL Applied to Text-based Games: Narasimhan et al.\n(2015) test their LSTM-DQN in two text-based environ-\nments: Home World and Fantasy World. They report the\nquest completion ratio over multiple runs but not how many\nsteps it takes to complete them. He et al. (2015) intro-\nduce the Deep Reinforcement Relevance Network (DRRN)\nfor tackling choice-based (as opposed to parser-based) text\ngames, evaluating the DRRN on one deterministic game and\none larger-scale stochastic game. The DRRN model con-\nverges on both games; however, this model must know in ad-\nvance the valid commands at each state. Fulda et al. (2017)\npropose a method to reduce the action space for parser-\nbased games by training word embeddings to be aware of\nverb-noun affordances. One drawback of this approach is it\nrequires pre-trained embeddings.\nCount-based Exploration:\nThe Model Based Inter-\nval Estimation-Exploration Bonus (MBIE-EB) (Strehl &\nLittman, 2008) derives an intrinsic reward by counting state-\naction pairs with a table n(s, a). Their exploration bonus has\nthe form β\/\np\nn(s, a) to encourage exploring less-visited\npairs. In this work, we use n(s) rather than n(s, a), since\nthe majority of actions leave the agent in the same state\nCounting to Explore and Generalize in Text-based Games\n(i.e., unrecognized commands). Using the latter would re-\nward the agent for trying invalid commands, which is not\nsensible in our setting.\nTang et al. (2017) propose a hashing function for count-\nbased exploration in order to discretize high-dimensional,\ncontinuous state spaces. Their exploration bonus r+ =\nβ\/\np\nn(φ(s)), where φ(·) is a hashing function that can\neither be static or learned. This is similar to the cumulative\ncounting bonus deﬁned above.\nDeep Recurrent Q-Learning: Hausknecht & Stone (2015)\npropose the Deep Recurrent Q-Networks (DRQN), adding\na recurrent neural network (such as an LSTM (Hochreiter\n& Schmidhuber, 1997)) on top of the standard DQN model.\nDRQN estimates Q(ot, ht−1, at) instead of Q(ot, at), so it\nhas the capacity to memorize the state history. Lample &\nChaplot (2016) use a model built on the DRQN architecture\nto learn to play FPS games.\nA major difference between the work presented in this paper\nand the related work is that we test on unseen games and\ntrain on a set of similar (but not identical) games rather than\ntraining and testing on the same game.\n5. Experiments\n5.1. Coin Collector Game Setup\nTo evaluate the two models described above and the pro-\nposed discovery bonus, we designed a set of simple text-\nbased games inspired by the chain experiment (Osband et al.,\n2016; Plappert et al., 2017). Each game contains a given\nnumber of rooms that are randomly connected to each other\nto form a chain (see ﬁgures in Appendix C). The goal is to\nﬁnd and collect a “coin” placed in one of the rooms. The\nplayer’s initial position is at one end of the chain and the\ncoin is at the other. These games have deterministic state\ntransitions.\nGames stop after a set number of steps or after the player\nhas collected the coin. The game interpreter understands\nonly ﬁve commands (go north, go east, go south, go\nwest and take coin), while the action space is twice as\nlarge: {go, take} × {north, south, east, west, coin}.\nSee Figure 12, Appendix C for an example of what the agent\nobserves in-game.\nOur games have 3 modes: easy (mode 0), there are no dis-\ntractor rooms (dead ends) along the path; medium (mode\n1), each room along the optimal trajectory has one distrac-\ntor room randomly connected to it; hard (mode 2), each\nroom on the path has two distractor rooms, i.e., within a\nroom on the optimal trajectory, all 4 directions lead to a\nconnected room. We use difﬁculty levels to indicate the\noptimal trajectory’s length of a game.\nTo solve easy games, the agent must learn to recall its previ-\nous directional action and to issue the command that does\nnot reverse it (e.g., if the agent entered the current room by\ngoing east, do not now go west). Conversely, to solve\nmedium and hard games, the agent must reverse its previous\naction when it enters distractor rooms to return to the chain,\nand also recall farther into the past to track which exits it\nhas already passed through. Alternatively, since there are no\ncycles, it can learn a less memory intensive “wall-following”\nstrategy by, e.g., taking exits in a clockwise order from\nwhere it enters a room.\nWe refer to models with the cumulative counting bonus\nas MODEL+, and models with episodic discovery bonus as\nMODEL++, where MODEL ∈{DQN, DRQN}1 (implemen-\ntation details in Appendix A). In this section we cover part\nof the experiment results, the full extent of our experiment\nresults are provided in Appendix B.\n5.2. Solving Training Games\nWe ﬁrst investigate whether the variant models can learn\nto solve single games with different difﬁculty modes (easy,\nmedium, hard) and levels {L5, L10, L15, L20, L25, L30}2.\nAs shown in Figure 2 (top row), when the games are sim-\nple, vanilla DQN and DRQN already fail to learn. Adding\nthe cumulative bonus helps somewhat and models perform\nsimilarly with and without recurrence. When the games be-\ncome harder, the cumulative bonus helps less, while episodic\nbonus remains very helpful and recurrence in the model be-\ncomes very helpful.\nNext, we are interested to see whether models can learn to\nsolve a distribution of games. Note that each game has its\nown counting memory, i.e., the states visited in one game\ndo not affect the counters for other games. Here, we ﬁx the\ngame difﬁculty level to 10, and randomly generate training\nsets that contain {2, 5, 10, 30, 50, 100} games in each mode.\nAs shown in Figure 2 (bottom row), when the game mode\nbecomes harder, the episodic bonus has an advantage over\nthe cumulative bonus, and recurrence becomes more crucial\nfor memorizing the game distribution. It is also clear that\nthe episodic bonus and recurrence help signiﬁcantly when\nmore training games are provided.\n5.3. Zero-shot Evaluation\nFinally, we want to see if a pre-trained model can general-\nize to unseen games. The generated training set contains\n{1, 2, 5, 10, 30, 50, 100, 500} L10 games for each mode.\nThen, for each corresponding mode the test set contains\n10 unseen {L5, L10, L15, L20, L30} games. There is no\n1Since all models use the LSTM representation generator, we\nomit “LSTM” for abbreviation.\n2We use Lk to indicate level k game.\nCounting to Explore and Generalize in Text-based Games\nFigure 2. Model performance on single games (top row) and multiple games (bottom row).\nFigure 3. Zero-shot evaluation: Average rewards of DQN++ (left) and DRQN++ (right) as a function of the number of games in the\ntraining set.\nFigure 4. Average rewards and steps used corresponding to best\nvalidation performance in hard games.\noverlap between training and test games in either text de-\nscriptions or optimal trajectories. At test time, the counting\nmodules are disabled, the agent is not updated, and its gen-\nerates verb and noun actions based on the argmax of their\nQ-values.\nAs shown in Figure 3, when the game mode is easy, both\nmodels with and without recurrence can generalize well\non unseen games by training on a large training set. It is\nworth noting that by training on 500 L10 easy games, both\nmodels can almost perfectly solve level 30 unseen easy\ngames. We also observe that models with recurrence are\nable to generalize better when trained on fewer games.\nWhen testing on hard mode games, we observe that both\nmodels suffer from overﬁtting (after a certain number of\nepisodes, average test reward starts to decrease while train-\ning reward increases). Therefore, we further generated a\nvalidation set that contains 10 L10 hard games, and report\ntest results corresponding to best validation performance. In\naddition, we investigated what happens when concatenating\nthe previous 4 steps’ history observation into the input. In\nFigure 4, we add H to model names to indicate this variant.\nAs shown in Figure 4, all models can memorize the 500\ntraining games, while DQN++ and DRQN++H are able\nto generalize better on unseen games. In particular, the\nformer performs near perfectly on test games. To inves-\ntigate this, we looked into all the bi-grams of generated\ncommands (i.e., two commands from adjacent game steps)\nfrom DQN++ model. Surprisingly, except for moving back\nfrom dead end rooms, the agent always explores exits in\nanti-clockwise order. This means the agent has learned a\ngeneral strategy that does not require history information\nbeyond the previous command. This strategy generalizes\nperfectly to all possible hard games because there are no\ncycles in the maps.\n6. Final Remarks\nWe propose an RL model with a recurrent component, to-\ngether with an episodic count-based exploration scheme that\npromotes the agent’s discovery of the game environment.\nWe show promising results on a set of generated text-based\ngames of varying difﬁculty. In contrast to baselines, our\napproach learns policies that generalize to unseen games of\ngreater difﬁculty.\nIn future work, we plan to experiment on games with\nmore complex topology, such as cycles (where the “wall-\nfollowing” strategy will not work). We would like to explore\ngames that require multi-word commands (e.g., unlock red\ndoor with red key), necessitating a model that generates\nsequences of words. Other interesting directions include\nagents that learn to map or to deal with stochastic transitions\nin text-based games.\nCounting to Explore and Generalize in Text-based Games\nReferences\nCˆot´e, Marc-Alexandre, K´ad´ar, ´Akos, Yuan, Xingdi, Kybar-\ntas, Ben, Barnes, Tavian, Fine, Emery, Moore, James,\nHausknecht, Matthew, Asri, Layla El, Adada, Mahmoud,\nTay, Wendy, and Trischler, Adam. Textworld: A learn-\ning environment for text-based games. Computer Games\nWorkshop at IJCAI 2018, Stockholm, 2018.\nFulda, Nancy, Ricks, Daniel, Murdoch, Ben, and Wingate,\nDavid.\nWhat can you do with a rock?\naffor-\ndance extraction via word embeddings. arXiv preprint\narXiv:1703.03429, 2017.\nGershman, Samuel J and Daw, Nathaniel D. Reinforcement\nlearning and episodic memory in humans and animals:\nan integrative framework. Annual review of psychology,\n68:101–128, 2017.\nHausknecht, Matthew J. and Stone, Peter.\nDeep recur-\nrent q-learning for partially observable mdps. CoRR,\nabs\/1507.06527, 2015. URL http:\/\/arxiv.org\/\nabs\/1507.06527.\nHe, Ji, Chen, Jianshu, He, Xiaodong, Gao, Jianfeng, Li,\nLihong, Deng, Li, and Ostendorf, Mari. Deep reinforce-\nment learning with a natural language action space. arXiv\npreprint arXiv:1511.04636, 2015.\nHochreiter, Sepp and Schmidhuber, J¨urgen. Long short-\nterm memory. Neural Comput., 9(8):1735–1780, Novem-\nber 1997. ISSN 0899-7667. doi: 10.1162\/neco.1997.\n9.8.1735. URL http:\/\/dx.doi.org\/10.1162\/\nneco.1997.9.8.1735.\nInfocom.\nZork I, 1980.\nURL http:\/\/ifdb.tads.\norg\/viewgame?id=0dbnusxunq7fw5ro.\nKaelbling, Leslie Pack, Littman, Michael L, and Cassandra,\nAnthony R. Planning and acting in partially observable\nstochastic domains. Artiﬁcial intelligence, 101(1-2):99–\n134, 1998.\nKingma, Diederik and Ba, Jimmy. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nKolter, J Zico and Ng, Andrew Y. Near-bayesian exploration\nin polynomial time. In Proceedings of the 26th Annual\nInternational Conference on Machine Learning, pp. 513–\n520. ACM, 2009.\nLample, Guillaume and Chaplot, Devendra Singh. Playing\nFPS games with deep reinforcement learning. CoRR,\nabs\/1609.05521, 2016. URL http:\/\/arxiv.org\/\nabs\/1609.05521.\nMartin, Jarryd, Sasikumar, Suraj Narayanan, Everitt, Tom,\nand Hutter, Marcus.\nCount-based exploration in fea-\nture space for reinforcement learning. arXiv preprint\narXiv:1706.08090, 2017.\nNarasimhan, Karthik, Kulkarni, Tejas, and Barzilay, Regina.\nLanguage understanding for text-based games using deep\nreinforcement learning. arXiv preprint arXiv:1506.08941,\n2015.\nOsband,\nIan,\nBlundell,\nCharles,\nPritzel,\nAlexander,\nand Van Roy, Benjamin.\nDeep exploration via\nbootstrapped dqn.\nIn Lee, D. D., Sugiyama, M.,\nLuxburg, U. V., Guyon, I., and Garnett, R. (eds.),\nAdvances\nin\nNeural\nInformation\nProcessing\nSys-\ntems 29, pp. 4026–4034. Curran Associates, Inc.,\n2016.\nURL http:\/\/papers.nips.cc\/paper\/\n6501-deep-exploration-via-bootstrapped-dqn.\npdf.\nOstrovski, Georg, Bellemare, Marc G, Oord, Aaron van den,\nand Munos, R´emi. Count-based exploration with neural\ndensity models. arXiv preprint arXiv:1703.01310, 2017.\nPaszke, Adam, Gross, Sam, Chintala, Soumith, Chanan,\nGregory, Yang, Edward, DeVito, Zachary, Lin, Zeming,\nDesmaison, Alban, Antiga, Luca, and Lerer, Adam. Au-\ntomatic differentiation in pytorch. In NIPS-W, 2017.\nPlappert, Matthias, Houthooft, Rein, Dhariwal, Prafulla,\nSidor, Szymon, Chen, Richard Y, Chen, Xi, Asfour,\nTamim, Abbeel, Pieter, and Andrychowicz, Marcin. Pa-\nrameter space noise for exploration.\narXiv preprint\narXiv:1706.01905, 2017.\nStrehl, Alexander L and Littman, Michael L. An analysis\nof model-based interval estimation for markov decision\nprocesses. Journal of Computer and System Sciences, 74\n(8):1309–1331, 2008.\nTang, Haoran, Houthooft, Rein, Foote, Davis, Stooke,\nAdam, Chen, Xi, Duan, Yan, Schulman, John, DeTurck,\nFilip, and Abbeel, Pieter. # exploration: A study of count-\nbased exploration for deep reinforcement learning. In\nAdvances in Neural Information Processing Systems, pp.\n2750–2759, 2017.\nCounting to Explore and Generalize in Text-based Games\nA. Implementation Details\nImplementation details of our neural baseline agent are as\nfollows3. In all experiments, the word embeddings are ini-\ntialized with 20-dimensional random matrices; the number\nof hidden units of the encoder LSTM is 100. In the non-\nrecurrent action scorer we use a 1-layer MLP which has 64\nhidden units, with ReLU as non-linear activation function,\nin the recurrent action scorer, we use an LSTM cell which\nhidden size is 64.\nIn replay memory, we used a memory with capacity of\n500000, a mini-batch gradient update is performed every 4\nsteps in the gameplay, the mini-batch size is 32. We apply\nprioritized sampling in all experiments, in which, we used\nρ = 0.25. In LSTM-DQN and LSTM-DRQN model, we\nused discount factor γ = 0.9, in all models with discovery\nbonus, we used γ = 0.5.\nWhen updating models with recurrent components, we\nfollow the update strategy in (Lample & Chaplot, 2016),\ni.e., we randomly sample sequences of length 8 from the\nreplay memory, zero initialize hidden state and cell state,\nuse the ﬁrst 4 states to bootstrap a reliable hidden state and\ncell state, and then update on rest of the sequence.\nWe anneal the ϵ for ϵ-greedy from 1 to 0.2 over 1000 epochs,\nit remains at 0.2 afterwards. In both cumulative and episodic\ndiscovery bonus, we use coefﬁcient β of 1.0.\nWhen\nzero-shot\nevaluating\nhard\ngames,\nwe\nuse\nmax train step = 100, in all other experiments we\nuse max train step = 50; during test, we always use\nmax test step = 200.\nWe use adam (Kingma & Ba, 2014) as the step rule for\noptimization. The learning rate is 1e−3. The model is\nimplemented using PyTorch (Paszke et al., 2017).\nAll games are generated using TextWorld framework (Cˆot´e\net al., 2018), we used the house grammar.\n3Our\nimplementation\nis\npublicly\navailable\nat\nhttps:\/\/github.com\/xingdi-eric-yuan\/\nTextWorld-Coin-Collector.\nCounting to Explore and Generalize in Text-based Games\nB. More Results\nFigure 5. Model performance on single games.\nCounting to Explore and Generalize in Text-based Games\nFigure 6. Model performance on multiple games.\nCounting to Explore and Generalize in Text-based Games\nFigure 7. Model performance on unseen easy test games when pre-trained on easy games.\nCounting to Explore and Generalize in Text-based Games\nFigure 8. Model performance on unseen medium test games when pre-trained on medium games.\nCounting to Explore and Generalize in Text-based Games\nC. Text-based Chain Experiment\nFigure 9. Examples of the games used in the experiments: level 10, easy\nFigure 10. Examples of the games used in the experiments: level 10, medium\nCounting to Explore and Generalize in Text-based Games\nFigure 11. Examples of the games used in the experiments: level 10, hard\nFigure 12. Text the agent gets to observe for one of the level 10 easy games.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Counting to Explore and Generalize in Text-based Games.pdf"}
{"title":"Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4","authors":"Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo","summary":"Unlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities of\ndecision-making under uncertain or incomplete information. GPT-4, the recent\nbreakthrough in large language models (LLMs) trained on massive passive data,\nis notable for its knowledge retrieval and reasoning abilities. This paper\ndelves into the applicability of GPT-4's learned knowledge for imperfect\ninformation games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an\ninnovative agent that leverages GPT-4's capabilities for performing in\nimperfect information games. With proper prompt engineering to achieve\ndifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Importantly,\nGPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it\ncan understand others and intentionally impact others' behavior. Leveraging\nthis, we design a planning strategy that enables GPT-4 to competently play\nagainst different opponents, adapting its gameplay style as needed, while\nrequiring only the game rules and descriptions of observations as input. In the\nexperiments, we qualitatively showcase the capabilities of Suspicion-Agent\nacross three different imperfect information games and then quantitatively\nevaluate it in Leduc Hold'em. The results show that Suspicion-Agent can\npotentially outperform traditional algorithms designed for imperfect\ninformation games, without any specialized training or examples. In order to\nencourage and foster deeper insights within the community, we make our\ngame-related data publicly available.","url":"http:\/\/arxiv.org\/abs\/2309.17277v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2309.17277v3","published":1695997803000,"comment":null,"pdf_text":"Published as a conference paper at COLM 2024\nSuspicion-Agent: Playing Imperfect Information Games with\nTheory of Mind Aware GPT-4\nJiaxian Guo ∗†1\nBo Yang * 1\nPaul Yoo 1\nBill Yuchen Lin 2\nYusuke Iwasawa 1\nYutaka Matsuo 1\nThe University of Tokyo 1\nAllen Institute for AI 2\nDemo: https:\/\/shorturl.at\/dmxzD\nGame Data Replay: https:\/\/shorturl.at\/fhuCI\nCodes: https:\/\/github.com\/CR-Gjx\/Suspicion-Agent\nAbstract\nUnlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities\nof decision-making under uncertain or incomplete information. GPT-4, the\nrecent breakthrough in large language models (LLMs) trained on massive\npassive data, is notable for its knowledge retrieval and reasoning abilities.\nThis paper delves into the applicability of GPT-4’s learned knowledge for\nimperfect information games. To achieve this, we introduce Suspicion-\nAgent, an innovative agent that leverages GPT-4’s capabilities for imperfect\ninformation games. With proper prompt engineering to achieve differ-\nent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Impor-\ntantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity,\nmeaning it can understand others and intentionally impact others’ behav-\nior. Leveraging this, we design a planning strategy that enables GPT-4\nto competently play against different opponents, adapting its gameplay\nstyle as needed, while requiring only the game rules and descriptions of\nobservations as input. In the experiments, we qualitatively showcase the\ncapabilities of Suspicion-Agent across three different imperfect information\ngames and then quantitatively evaluate it in Leduc Hold’em. As an explo-\nration study, we show that Suspicion-Agent can potentially outperform\ntraditional algorithms without any specialized training or examples, but\nstill cannot beat Nash-Equilibrium algorithms. In order to encourage and\nfoster deeper insights within the community, we make our game-related\ndata publicly available.\n1\nIntroduction\nRecently, large language models (LLMs) (Brown et al., 2020b; Chowdhery et al., 2022;\nTouvron et al., 2023), which are trained extensively on text corpora and code datasets and\naligned with instructions (Ouyang et al., 2022; Wei et al., 2021; Longpre et al., 2023), have\ndemonstrated remarkable knowledge retrieval and reasoning capabilities (Kojima et al.,\n2022; Wei et al., 2022b;a) on natural language benchmarks and exams (Hendrycks et al.,\n2020; Cobbe et al., 2021). Given few-shot examples or specific instructions as prompts,\nthese models, especially GPT-4 (OpenAI, 2023), can understand human intentions and\nmake informed decisions in open-ended scenarios, and tackle intricate tasks by gathering\nobservations and utilizing the learned prior knowledge, such as Voyager (Wang et al., 2023a),\nReAct (Yao et al., 2022) and SwiftSage (Lin et al., 2023).\nHowever, most of these methods typically assume that the agent has access to all relevant\ninformation, an assumption that is often unrealistic in real-world settings. Take diplomacy\n(Team et al., 2022; Gray et al., 2020) as an example: representatives must discern the veiled\n∗Equal Contribution\n†Corresponding to jiaxian.guo@weblab.t.u-tokyo.ac.jp. (Guo is now working at Google)\n1\narXiv:2309.17277v3  [cs.AI]  31 Aug 2024\nPublished as a conference paper at COLM 2024\nintentions of other countries based on incomplete information and decide accordingly\nto maximize benefits for their own nation. This challenge is not unique to diplomacy\nbut extends to other domains as well, such as poker (Moravˇc´ık et al., 2017; Brown &\nSandholm, 2018) and economic simulations (Holmstr¨om & Myerson, 1983; Harsanyi, 1968).\nThe inherent unpredictability in these games makes it impractical for a learned agent to\nadopt a single, optimal strategy for every scenario (Brown et al., 2019). This necessitates\npredictive capabilities for handling incomplete information, along with a theory of mind\n(ToM) ability (Frith & Frith, 2005) to comprehend decisions from others’ perspectives. Such\ncomplexities, both strategic and cognitive, represent challenges in the field of AI research.\nFurthermore, recent advancements in imperfect information games, such as ReBel (Brown\net al., 2020a), DeepStack (Moravˇc´ık et al., 2017) and Libratus (Brown & Sandholm, 2018),\ntypically start training from scratch, and thus they normally need millions of data to\nunderstand the game rules and learn the adequate strategies for each new game. Such\na high sample complexity hampers their ability to generalize across different games and\nposes challenges when applying them into complex and open-ended imperfect information\nscenarios. By contrast, as alluded to previously, LLMs have undergone training on massive\npassive datasets. This leads to an intriguing proposition: Can we harness pre-trained LLMs’\nknowledge and reasoning capabilities to navigate imperfect information games without\nadditional training or data?\nTo achieve this, we propose Suspicion-Agent, an innovative autonomous agent based on\nGPT-4. This agent harnesses its extensive prior knowledge and cognitive adaptability to\neffective strategies against a range of adversaries without any specialized training. Con-\ncretely, we first decompose the process of solving such games into multiple sub-modules\nlike observation interpreter and planning module to understand the game rules and game\nstates (as Figure 1 shows) so that GPT-4 can make decisions accordingly. Each module\nemploys different prompts to enable GPT-4 to fulfill specific functions. However, unlike\nperfect information games, planning strategies in imperfect information games can have\nvarying effectiveness depending on the opponent’s behavior (Brown et al., 2020a; Moravˇc´ık\net al., 2017; Brown & Sandholm, 2018). To tackle these challenges, we introduce a theory\nof mind (ToM) aware planning approach that leverages the higher-order ToM capability\n(Frith & Frith, 2005) present in LLMs. Specifically, the model utilizes its understanding of\nhuman cognition to predict opponents’ thought processes, susceptibilities, and actions. This\naligns with the idea that individuals use their own minds as models to understand and\naffect others (Montes et al., 2022). For instance, the model might consider, ”If I execute Plan\n1, how would this influence my opponent’s beliefs about my cards, and what actions might\nthey take based on their behavioral patterns?”\nConcretely, given the gameplay history as the input, we find that GPT-4 can identify an\nopponent’s strategic tendencies and analyze how our actions influence the opponent’s\nbehavior, e.g. if Suspicion-Agent identifies a weak hand held by the opponent, coupled\nwith the cautious strategy, it might strategically raise the bet to encourage the opponent\nto fold, even when Suspicion-Agent itself holds a similarly weak hand (as illustrated in\nFigure 9 and A.11). Remarkably, by using some simple prompts, e.g., GPT-4 can even self-\nexamine its behavior through the lens of the opponent (Refer to A.7). Leveraging its ToM\ncapabilities, GPT-4 can predict and even influence an opponent’s actions effectively (R¨oska-\nHardy, 2008). Integrating these simulated actions into our planning module can mitigate\nthe information asymmetry inherent in imperfect information games and more accurately\nassess the effectiveness of various strategies. As a result, our Suspicion-Agent can adjust\nits strategy to play effectively against a range of opponents, as shown in Section 4.1. In the\nexperiments, we first conduct a qualitative assessment of Suspicion-Agent’s in 3 two-player\nimperfect information games, aiming to showcase the generalization capabilities of our\nmethod. Subsequently, we perform a quantitative analysis in Leduc Hold’em (Southey et al.,\n2012). The results reveal that Suspicion-Agent exhibits varying behaviors when interacting\nwith previous works such as CFR+ (Tammelin, 2014) and NFSP (Heinrich & Silver, 2016)\nwhile outperforming some of them in terms of overall performance. In summary, our\ncontributions are as follows:\n1. We introduce Suspicion-Agent, the first agent framework designed to empower GPT-4\nwith theory of mind (ToM) ability to compete in various imperfect information games\n2\nPublished as a conference paper at COLM 2024\nSuspicion-Agent\nObservation \nInterpreter\n(LLM)\nPlanning\n(LLM)\nUnknown\nOpponent\n'hand': 'SJ',\n'public_card': 'SK',\n'all_chips’: [6, 6],\nGame State\nGame Rule\nObservation \nRule\nIn this round , you \nare holding a Jack \nof Spades and the \npublic card is a \nKing of Hearts. So \nfar, you have put 6 \nchips into the pot \nand your \nopponent has put \nin 6 chips. \nAction\nAction\nPlanning with First-Order Theory of Mind\nPlan \nProposals\n(LLM)\nObservation\nInterpretation\nAll Game \nPlay History\nPattern Analysis \n& Reflexion\n(LLM)\nSelf-Evaluate\n(LLM)\nThe opponent tends \nto raise (80%) when \nhe has King …\nOpponent \nCurrent \nInformation \nPrediction (LLM)\nPlan1: Raise\nPlan2: Check\n….\nThe opponent has \nKing (80%), Jack \n(10%), Queen (10%)\nReadable \nCurrent \nGame \nHistory\nPlan1: Based on the \nestimation of the \nopponent’s and my \ncard,\nPlan 1’s winning rate \nis 80%.  \nPlan2: \n…\nBased on the winning \nrate comparison, I will \nselect {Plan 1}\nAction\nFigure 1: Left Figure. The illustration of the Suspicion-Agent when playing against an\nunknown opponent. Right Figure. The illustration about the first-order ToM planning\nmethod, where the texts in yellow blocks are outputs, and green blocks are inputs.\nby understanding game rules and observational data without requiring any specialized\ntraining or examples. By incorporating the ToM capability into the planning process,\nSuspicion-Agent captures the inherent uncertainty of opponent behavior in our strategic\ndeliberations. This enables Suspicion-Agent to adapt its tactics dynamically when facing\nopponents with differing behavioral patterns.\n2. We are the first to demonstrate that an agent based on GPT-4 can potentially outperform\ntraditional algorithms in imperfect-information games, such as Leduc Hold’em (Southey\net al., 2012), when compared to established learning-based methods like NFSP (Heinrich &\nSilver, 2016) and DMC (Zha et al., 2021), but still unperformed Nash-Equilibrium algorithms\nlike CFR+ (Tammelin, 2014) which may inspire more subsequent use of LLMs in imperfect-\ninformation games.\n3. We make all interaction data between Suspicion-Agent and traditional algorithms for\nimperfect-information games in Leduc Hold’em publicly available. This will enable the\nresearch community to scrutinize the capabilities of GPT-4-based agents and inspire further\nwork, particularly in fine-tuning smaller language models.\n2\nProblem Definition\nTwo-Player Imperfect Information Game\nIn this paper, we propose to employ LLMs to\nplay imperfect information games. As a preliminary exploration, we concentrate primarily\non two-player imperfect information games, such as Leduc Hold’em (Southey et al., 2012),\nwhich involves two players, denoted by N = {1, 2}, who share the same action space, A.\nLet a1 ∈A and a2 ∈A represent the actions chosen by player 1 and player 2, respectively.\nEach player has access to two types of observations: a private observation, denoted as\nSpri(i) where i ∈N is the player index, and a public observation, shared among both\nplayers, denoted as Spub.\nAs the game progresses in discrete timesteps indexed by j, each player i observes a\nhistory h of the game. This history comprises the series of public and private obser-\nvations and actions up to timestep j −1 and the result of game rj, formally given as\nh = (S0\npub, S0\npri(i), a0\ni , a0\n¬i, r0 . . . , Sj−1\npub, Sj−1\npri(i), aj−1\ni\n, aj−1\n¬i , rj−1). Simultaneously, players re-\nceive the current private and public observations, Sj\npri(i) and Sj\npub, and select the next action\naj\ni according to a policy πi. All game histories are constructed as a dataset D, denoted as\nD = (h1, h2, . . . , hM), where M indexes individual games. The goal of each player is to\nselect the next action aj\ni with the imperfect observation according to the game rules, aiming\nfor victory over many games. Specifically, the order of players is not fixed and depends on\nthe game rule for each game. For example, the role of the small blind rotates among players\nin Texas Holed’em, dictating the order of play.\n3\nMethod\nTo enable LLMs to play various imperfect information games without specialized training,\nwe break down the overall task into several modules shown in Figure 1, such as the observa-\n3\nPublished as a conference paper at COLM 2024\nVanilla Planning\nPlanning with First-Order \nTheory of Mind\nPlanning with Second-Order \nTheory of Mind\nBased on my card, \nI would like to \ndo {action1} to \nwin the game or \nminimize the \nloss\nI know his behavior \npattern, and I can \nestimate his card  \nand his action. Based \non my card and his \ncard, I would like to \ndo {action1} to win \nthe game\nI know his \nbehavior \npattern, and I \ncan estimate \nhis card and \naction\nI know his \nbehavior \npattern, and \nI can \nestimate his \ncard and \naction based \non his action\nI know he knows \nmy behavior \npattern, and I \ncan adjust my \naction to win him\nFigure 2: Left Figure. The decision-making of the vanilla planning of Suspicion-Agent.\nMiddle Figure. The decision-making of the planning with first-order ToM of Suspicion-\nAgent. Right Figure. The decision-making of the planning with second-order ToM of\nSuspicion-Agent.\ntion interpreter, game pattern analysis, and planning module. In the following sections, we\nwill demonstrate how we craft specific prompts to guide LLMs to use its prior knowledge,\nreasoning ability, and psychological ability in performing these modular functions and\nexplain how we combine these functions to equip the model with the capability to navigate\nthe intricacies of imperfect information games. All prompts and codes will be made public\non our codebase (Please refer to our supplementary material).\n3.1\nGame Rule & Observation Understanding\nWhile LLMs excel in processing text data, it can be misled in imperfect information games\nbecause they normally provide only brief, low-level descriptions. To mitigate this issue, we\ninitially develop structured prompts that assist LLMs in comprehending both the game’s\nrules and its current state. For each type of imperfect information game, one can write a\nstructured rule description as follows:\n• General Rules: A brief game introduction, the number of rounds, and betting rules;\n• Action Descriptions: {Description of Action 1}, {Description of Action 2}, ...;\n• Single Win\/Loss Rule: The conditions for winning, losing, or drawing in a single game;\n• Win\/Loss Payoff Rule: The rewards or penalties for winning or losing a single game;\n• Whole Win\/Loss Rule: The number of games and the overall win\/loss conditions.\nIn most imperfect information game environments (Zha et al., 2019), game states are often\nrepresented as low-level numerical values, such as one-hot vectors, to facilitate machine\nlearning. Leveraging LLMs, we can convert these low-level game states into natural lan-\nguage text (Wu et al., 2023; Wang et al., 2023a; Guo et al., 2022; Lin et al., 2023), thereby\naiding the model’s understanding. For each game, it is also essential to define an observation\nconversion rule. Similar to structuring game rules, we organize the observation conversion\nrule as follows:\n• Input Explanation: The type of inputs received, such as dictionaries, lists, or other\nformats, and describes the number of elements in the game state along with the name of\neach element;\n• Element Descriptions: {Description of Element 1}, {Description of Element 2}, ...;\n• Conversion Tips: More guidelines for transforming the low-level game states into text.\nBy leveraging both the game rule and the observation conversion rule, we can effi-\nciently transform low-level game states into readable text, denoted as Obsr. This read-\nable text serves as the input for LLMs. Using the prompts Promptobs, the conditional\ndistribution for each element Obsr[i] in the generated text can be modeled as: Obsr ∼\n∏M\ni=1 Fθ(Obsr[i]|Promptobs, Rule, Ruleobs, Obsr[1, . . . , i −1]).\nHere, Fθ represents the lan-\nguage model parameterized by θ; M is the length of the generated text Obsr. The concrete\ndefinition can be found in Appendix A.2. We name this module an Observation Interpreter.\nThis formulation allows for a more understandable interaction with the model in imperfect\ninformation games.\n3.2\nVanilla Planning Module and Reflexion\nAfter understanding the game rules and converting the game states into a readable format,\nwe can craft prompts to guide LLMs in formulating strategies. Inspired by advancements in\n4\nPublished as a conference paper at COLM 2024\nLLMs-agent and prompt engineering (Ganguli et al., 2023; Wang et al., 2023e; Liu et al., 2023b;\nShinn et al., 2023), we introduce a vanilla planning method which features a Reflexion\nmodule aimed at automatically scrutinizing game history to enable LLMs to learn and\nimprove planning from the experience of the history, as well as a separate planning module\ndedicated to making decisions accordingly.\nReflexion The Reflexion module takes as input the history of games played against\nthe current opponent and outputs a Reflexion.\nIn the j-th round of the i-th game,\nwe gather all prior game histories, denoted as Di = (h1, h2, . . . , hi−1), and prompt\nLLMs to carry out these analytical functions to get the Reflexion output Oi\nf\n∼\n∏M\ni=1 Fθ(Of [i]|PromptRe f lexion, Rule, Di, Of [1, . . . , i −1]), i.e., Oi\nf ∼FRe f lexion\nθ\n, which covers\nwhy we won or lost in specific previous games, and suggests how to improve strategies for\nfuture games. Importantly, the Reflexion module empowers Suspicion-Agent to enhance its\nstrategies during gameplay, even without previous examples.\nPlanning After obtaining the Reflexion Of , we proceed to use the game rules, the cur-\nrent game history hi, the current readable observation Obsr, and the set of valid ac-\ntions {a} in the current game as inputs.\nWe then prompt LLMs to formulate multi-\nple textual plans based on its understanding of the imperfect information, i.e., Oplan ∼\n∏M\ni=1 Fθ(Oplan[i]|Promptplan, Rule, Obsr, hj−1, Of , Oplan[1, . . . , i −1]), Oplan ∼Fplans\nθ\n. Specifi-\ncally, the vanilla planning method assumes the marginal distribution of the actions of the\nopponent is uniform, and thus it can be regarded as a special case of planning with the\nzero-order ToM. In this way, we can further denote Fplans\nθ\nas Fzero−plan\nθ\n.\nEvaluator To assess the likely success of each plan, we introduce an evaluation module. This\nmodule takes into account factors such as the game’s current state, i.e., readable observation\nObsr, the Reflexion ORe f lexion, the game rule Rule and estimated plans Oplan as the input,\nto estimate the win rates for each of the proposed plans and output the next action by\nprompting LLMs, i.e., the next action aj = Fzero−eval\nθ\n(Obsr, ORe f lexion, Rule, Oplan).\n3.3\nPlanning with Theory of Mind (ToM)\nHowever, the vanilla planning method often struggles against the inherent uncertainties\nthat typify imperfect information games, particularly when faced with opponents skilled at\nexploiting others’ strategies. Inspired by this adaptability, we seek to devise a new planning\nmethod that capitalizes on LLMs’ ToM capabilities (Frith & Frith, 2005; Kosinski, 2023) to\nunderstand the opponent’s behavior and thus can adjust the strategy accordingly. In the\nfollowing sections, we will detail how we employ LLMs to analyze the behavior patterns\nof other agents and predict their subsequent actions in response to various plans using\ndifferent orders of ToM (results are shown in Table 3), thereby facilitating more informed\ndecision-making. Note that all sample outputs are given in Section A.11 and A.7.\nPlanning with First-Order ToM Modelling: In the first-order ToM modeling approach (as\nFigure 2 shows), Suspicion-Agent goes a step further by inferring the probable hand of\nthe opponent based on their actions to that point, e.g., if the opponent raised, they likely\nhave a strong hand. Consequently, Suspicion-Agent can adapt their strategy to maximize\nwinnings when holding a strong hand and minimize losses with a weak hand. To forecast\nthe opponent’s actions, we first introduce a behavior pattern analysis process. In this process,\nwe feed the game history Di = (h1, h2, . . . , hi−1) and the game rules into LLMs, prompting\nit to analyze the opponent’s behavioral pattern. The formulation and the prompts can be\nexpressed as: Obp ∼∏M\ni=1 Fθ(Obp[i]|Promptpattern, Rule, Di, Obp[1, . . . , i −1]).\nSample Prompts for First-Order Behaviour Pattern Analysis (Incomplete) :\nFrom my\nperspective, please infer several beliefs about the opponent’s game pattern\/preference for each round\nwhen holding different cards and the public card (if have).\nThrough this approach, we can deduce the opponent’s behavior pattern. Notably, since\nthe input for behavior pattern analysis is the same as that for the Reflexion module, we\nhave integrated them into a single module to reduce inference time, as shown in Figure 1.\n5\nPublished as a conference paper at COLM 2024\nAfter identifying the opponent’s behavior pattern, LLMs can be prompted to predict the\nstrength of the opponent’s current hand or observations in the game. This is expressed as:\nOcard pred ∼∏M\ni=1 Fθ(Ocard pred[i]|Promptcard pred, Rule, hj−1, Obp, Obsj\nr, Ocard pred[1, . . . , i −\n1]).\nSample Prompts for First-Order Cards Prediction (Incomplete) :\nUnderstanding the game\nrule, your observation, progress summarization in the current game, the estimated behaviour pattern\nof the opponent, and your knowledge about the game, please infer the probabilities about the cards of\nthe opponent (number 100% in total) step by step.\nWith these predictions, we can further augment the previous Planning module and Evalu-\nator module with Ocard pred as the additional input, so that we can further propose better\nplans considering the opponent’s card and estimate the winning rate of each plan, so that\nwe can better make the decision. Because the input of Planning module and Evaluator\nmodule are highly overlapped and our budgets are limited, we combine these two modules\ntogether to save the costs:\nSample Prompts for Planning and Evaluator (Incomplete):\nMake Reasonable Plans: Please\nplan several strategies according to actions you can play now to win the whole game step by step.\nNote that you can say something or keep silent to confuse your opponent.\nPotential opponent’s actions and Estimate Winning\/Lose\/Draw Rate: From the perspective of the\nopponent, please infer what the action opponent with probability would do when the opponent holds\ndifferent cards based on his behaviour pattern, and then calculate the winning\/lose\/draw rates when\nopponent holds different cards step by step. Output in a tree structure:\nThe sample outputs are shown in Figure 8 and A.7.1.\nPlanning with Second-Order ToM Modelling: However, elite players in imperfect in-\nformation games like poker are also adept at dynamically adjusting their strategies, and\nthey may employ ”bluffing” as a tactic, feigning a strong hand when they actually hold a\nweak one to deceive their opponent. Relying solely on a first-order ToM in such situations\ncould lead to incorrect assumptions and potentially costly mistakes. Recognizing this, we\nintroduce a planning method that incorporates a second-order ToM. In this enhanced model,\nSuspicion-Agent engages in even more intricate reasoning, where Suspicion-Agent not only\nconsiders what the opponent might do (as in first-order ToM) but also what the opponent\nbelieves Suspicion-Agent will do as Figure 2 shows. This level of strategic thinking allows\nSuspicion-Agent to gain an advantage in situations involving tactics like bluffing.\nTo implement this, Suspicion-Agent needs to not only just consider the current state from\nits own perspective, but also be capable of role-switching to think his own observation from\nthe opponent’s viewpoint. In traditional methods (De Weerd et al., 2013; Tatarchenko et al.,\n2016), they need to iteratively call the first-order ToM function to estimate the action of the\nopponent. However, we surprisingly find that we can just add the prompts like below, and\nget the outputs in Sec A.7.2.\nSample Prompts for Second-Order Behaviour Pattern Analysis (Incomplete):\nFrom my\nperspective, please infer under what circumstances is the opponent likely to be influenced by my\nactions? Additionally, in what situations would the opponent make decisions based solely on their\nown hand?\nFrom the perspective of the opponent (he cannot observe my card but only action), please infer several\nbeliefs about my game pattern\/preference when holding different cards.\nWith this, LLMs are able to automatically generate insights into whether the opponent’s\nbehavior is likely to be reactive to Suspicion-Agent’s actions, or if they are likely to act\nindependently. Then, we can directly reuse the prompts of the first-order Tom to predict\nthe opponent’s cards based on the behavior pattern estimated from the second-order ToM,\nand we can get sample results in Figure 9 and Section A.7.2. In this way, we can utilize the\nPlanning with second-order ToM to make decisions and adapt the strategies accordingly.\nThe concrete algorithms are given in Section A.9. Without mentioning otherwise, we use\nthe second-order ToM and GPT-4-0613 by default.\n6\nPublished as a conference paper at COLM 2024\n4\nExperiments\nWe conduct experiments to answer the following questions:\n• Can Suspicion-Agent achieve comparable performance with traditional imperfect informa-\ntion algorithms without any specialized training? (Section 4.1)\n• Can Suspicion-Agent adapt its strategies when playing with different opponents? (Section\n4.1)\n• Can Suspicion-Agent play different imperfect information games without any specialized\ntraining? (Section A.11.1)\n• How different orders of ToM improve the performance of Suspicion-Agent? (Section 4.3\nand A.8)\n4.1\nQuantitative Evaluation\nEnvironments\nTo quantitatively assess the performance of LLMs in imperfect information\ngames, we chose the RLCard environment (Zha et al., 2019). Due to budget limits, our\nquantitative evaluation focuses on Leduc Hold’em, a simplified version of Limit Texas\nHold’em. The game rules of Leduc Hold’em can be found in Appendix A.3. Following\n(Southey et al., 2012), we also add the opponent’s observation into the single game history\nh after the end of each game, which also conforms with the real-world experience, but we\nalso perform the ablation study about it in Section 4.3 and A.10.\nCompeting Methods\nWe have selected a range of methods commonly used in decision-\nmaking, such as NFSP (Heinrich & Silver, 2016), DQN (Mnih et al., 2015), DMC (Deep Monte\nCarlo Search for imperfect information games) (Zha et al., 2021) and CFR+ (Zinkevich et al.,\n2007). Among these, NFSP and DMC are specifically designed for imperfect information\ngames and are based on self-play, while CFR+ is grounded in game theory. These algorithms\ntypically show different strategies in the imperfect information games, allowing us to\nevaluate the adaptability of each method. Note that, our Suspicion-Agent does not have\nany specialized training when compared with these methods.\nEvaluation Methods\nTo ensure the robustness of our evaluation metrics, we meticulously\ndesigned a dual-method evaluation framework aimed at mitigating the randomness intrinsic\nto imperfect information games. (1) Variable Random Seeds: Suspicion-Agent play against\ndifferent baselines for 100 games utilizing varying random seeds for each game. This tactic\nis intended to dampen the stochastic variability introduced by the random seed settings.\nThe results are shown in Table 1. (2) Same Cards with Exchange Position: We ran a series of\n50 games with a fixed random seed, thereby keeping the sequence of cards constant across\nthese games. Suspicion-Agent initially played at position 0 for the first 50 games, then we\nrerun the 50 games but switched the position of Suspicion-Agent and the baseline model.\nIn this way, Suspicion-Agent and the baseline should have the same card strength over\n100 games, and thus we can better evaluate the performance of each. The results of these\nexperiments are presented in Table 2.\nResults Analysis\n(1) Suspicion-Agent outperforms most baselines: As illustrated in\nTable 1, it is evident that our GPT-4-based Suspicion-Agent outperforms most other algo-\nrithms specifically trained on Leduc Hold’em environments except CFR+. Notably, it not\nonly defeats most of these methods but also secures the highest average chip count in the\ncomparisons. Our approach surpasses the second-best method by an impressive margin\nof approximately 25%. Even if the current design cannot achieve Nash-Equilibrium, these\nfindings compellingly showcase the potential of employing large language models in the\nrealm of imperfect information games, as well as affirm the effectiveness of our proposed\nframework. (2) The gap between GPT-3.5 and GPT-4 is large: While GPT-4 delivers perfor-\nmance that either matches or outperforms other baselines, agents using GPT-3.5 experience\na significant drop in performance. Specifically, the winning probability for agents built\non GPT-3.5 stands at just 50%, as opposed to 100% for GPT-4-based agents. Additionally,\nthe average chip payoff for GPT-3.5 agents is negative, underlining the stark performance\ndisparity between the two versions of the language model. The further reason analysis can\nbe found in Appendix A.4. (3) Suspicion-Agent outperforms baselines in both positions:\nUtilizing identical card sequences for both positions, Suspicion-Agent exhibits a consistent\nwinning pattern against various baselines, as evidenced in Table 2. This robust performance\n7\nPublished as a conference paper at COLM 2024\nserves as compelling evidence to substantiate the claim that Suspicion-Agent outperforms\nthe baseline models when card strength is held constant.\nTable 1: The comparison results of Suspicion-Agent when playing with different algorithms\ntrained on Leduc Hold’em environments. The results are the win\/lose chips after 100 games\nwith different seeds, and the number of win\/lose chips ranges from 1 to 14.\nOpponent Model\nNFSP\nDQN\nDMC\nCFR+\nOurs\n(GPT-3.5)\nOurs\n(GPT-4)\nAvg.\nNFSP\n-\n-33\n-22\n-21\n-3\n-142\n-61.25\nDQN\n+33\n-\n-55\n-8\n+200\n-44\n+22.8\nDMC\n+22\n+55\n-\n-15\n-49\n-24\n+4\nCFR+\n+21\n+8\n+15\n-\n+126\n+22\n+38.4\nOurs (GPT-3.5)\n+3\n-200\n+49\n-126\n-\n-\n-55\nOurs (GPT-4)\n+142\n+45\n+24\n-22\n-\n-\n+47.25\nTable 2: The comparison results of Suspicion-Agent when playing with CFR+ and DMC\ntrained in Leduc Hold’em environments. These results are quantified over 50 games, and\npos denote the position of the opponent model. For example, CFR+ (pos 0) denotes the\nopponent model is in the position 0, and the model is located in the position 1.\nOpponent Model\nCFR+\n(pos 0)\nCFR+\n(pos 1)\nDMC\n(pos 0)\nDMC\n(pos 1)\nOurs\n(pos 0)\nOurs\n(pos 1)\nAvg.\nDMC\n-21\n-6\n-10\n+10\n-36\n-4\n-11.17\nCFR+\n+35\n-35\n+6\n+21\n+16\n-1\n+7\nOurs\n+1\n-16\n+11\n+36\n-\n-\n+8\nBehaviour Pattern Analysis\nWe illustrate the action percentages of Suspicion-Agent and\nbaselines in Figure 3. We can observe that (1) Suspicion-Agent vs CFR+: CFR+ algorithm\n(Zinkevich et al., 2007) exhibits a mixed strategy, characterized by a conservative approach\nwhere it tends to fold in the face of a weak hand, especially when this hand does not align\nwell with the public cards. Our agent, recognizing this pattern, strategically opts to raise\nmore frequently. This tactic effectively applies pressure on the CFR+ algorithm, increasing\nthe likelihood of it folding under uncertain conditions. However, Over-reliance on bluffing\ncan lead to significant losses, especially when facing opponents capable of recognizing\nand exploiting this pattern. Additionally, it’s noteworthy that CFR+ itself is not averse\nto bluffing. Although it typically plays conservatively, CFR+ does occasionally employ\nbluffing tactics, which can result in losses for our agent. (2) Suspicion-Agent vs DMC: DMC\nalgorithm (Zha et al., 2021) based on the search algorithm DMC employs a more diversified\nstrategy that includes bluffing. It often raises both when it has the weakest and the strongest\nhands. In response, Suspicion-Agent adapts by raising less frequently and opting to call or\nfold more often based on its own hand and the observed behavior of DMC. (3) Suspicion-\nAgent vs DQN: DQN appears to have a more aggressive stance, almost always raising\nwith strong or mid-level hands and never folding. Suspicion-Agent identifies this and, in\nturn, minimizes its own raises (the lowest percentage among all matchups), opting more\noften to call or fold based on the public cards and DQN’s actions. (4) Suspicion-Agent\nvs NFSP: NFSP exhibits a follow-through strategy, opting to always call and never fold.\nSuspicion-Agent responds to this by raising less frequently (compared to matches against\nCFR+) and choosing to call more (compared to matches against CFR+) based on the public\ncard and NFSP’s observed actions. The analysis clearly shows that Suspicion-Agent is\nhighly adaptable and capable of exploiting the weaknesses in the strategies employed by\nvarious other algorithms. This speaks volumes about the large language model’s capability\nto reason and adapt in imperfect information games.\n4.2\nQualitative Evaluation\nIn the qualitative evaluation, we assess Suspicion-Agent on three imperfect information\ngames: Coup, Texas Hold’em Limit, and Leduc Hold’em (Southey et al., 2012). For each\ngame, we provide only the rules and observation rules as described in Section 3.1. Im-\nportantly, Suspicion-Agent is able to play these games without any additional training or\nsampling. Qualitative examples from these games are presented in the subsequent sections.\n8\nPublished as a conference paper at COLM 2024\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparison of Action Percentage when Playing Against Different Algorithms\nvs CFR\nvs DQN\nvs DMC\nvs NFSP\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparisons of Action Percentage of Different Algorithms\nCFR\nDQN\nDMC\nNFSP\nFigure 3: Left Figure. The illustration of the action percentage of Suspicion-Agent when\nplaying against different algorithms. Right Figure. The illustration of the action percentage\nof different algorithms when playing against Suspicion-Agent.\n.\nAction: Call (Holds Jack or Queen)\nObs:In this round, you are holding a Jack of Hearts. So far, you have put 2 chips into the pot, while your opponent \nhas put 2 chips.\nVanilla Planning\nBelief on GoodGuy's cards: GoodGuy chose to 'call' in the first round. According to his behavior pattern, when he \nholds a King, he would like to raise in the first round. Therefore, it is unlikely that he holds a King. When he \nholds a Queen or a Jack, he would like to call in the first round. Therefore, he is likely to hold a Queen or a \nJack. So, the probabilities are: King (0%), Queen (50%), Jack (50%).\nPotential GoodGuy's current believes about your cards:  I have not taken any action in the first round yet. \nAccording to GoodGuy's guess on my game pattern, he believes that I would like to raise when I hold a King, Queen, \nor Jack. Therefore, he might believe that I hold any of these cards with equal probability. So, the probabilities \nare: King (33.33%), Queen (33.33%), Jack (33.33%).\nMake Plans: Plan 1: Raise\n- If GoodGuy holds a Queen (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\n- If GoodGuy holds a Jack (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\nPlan 2: Fold\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). I will lose the pot, which \nwill be 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate \nis 0%.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). I will lose the pot, which will \nbe 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate is \n0%.\nPlan 3: Check\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\nEstimate Expected Chips Gain for Each Plan:\n- Plan 1: Expected Chips Gain = 50% * 3 chips - 0% * 0 chips = 1.5 chips\n- Plan 2: Expected Chips Gain = 0% * 0 chips - 50% * 2 chips = -1 chips\n- Plan 3: The Expected Chips Gain cannot be determined at this stage.\nPlan Selection: Plan 1: raise\nAction: Raise\nAction: Fold\nFigure 4:\nThe qualitative sample of planning with second-order ToM Suspicion-Agent\nabout Strategic Bluffing on Leduc Hold’em. More samples are given in Appendix A.11.\nLeduc Hold’em We present qualitative samples showcasing Suspicion-Agent’s behaviour\nunder different strategies: Vanilla Planning, Planning with First-Order ToM, and Planning\nwith Second-Order ToM in Leduc Hold’em. These samples can be viewed in Figure 6, 2, 8,\nand 9, respectively, and the concrete analysis is given in Appendix A.11.1.\nGame Coup and Texas Hold’em Limit As illustrated in Figure 12 and 11 in Appendix,\nwhen provided solely with the rules and observation guidelines of Texas Hold’em Limit and\nCoup, and keeping the agent prompts consistent, Suspicion-Agentis still adept at discerning\nthe opponent’s game patterns. It analyzes the strength of its hand and subsequently makes\ninformed decisions to accumulate chips. This is the strong evidence to demonstrate the\ngeneralization ability of Suspicion-Agent based on GPT-4 on different imperfect information\ngames, and thus outperforming the algorithms need to be re-trained for every new imperfect\ninformation game. Specifically, In the game of Coup, without any prior training, Suspicion-\nAgent rapidly discerns which character the opponent lacks. It then strategically bluffs\nas that character to block the opponent’s actions. This ability to bluff successfully gives\nSuspicion-Agent a consistent advantage throughout multiple rounds.\n4.3\nAblation Study and Component Analysis\nAblation Study on Orders of ToM\nIn order to explore how different orders of ToM-\naware planning methods affect the behavior of the large language model. We perform the\n9\nPublished as a conference paper at COLM 2024\nexperiments and comparisons on Leduc Hold’em and play against CFR+. We illustrate\nthe action percentages of Suspicion-Agent with planning with different levels of ToM in\nFigure 5 and the chips gain results in Table 3. We can observe that (1) Vanilla Planning:\nBased on Reflexion module, vanilla planning tends to call and check more (the highest\ncall and check percentage when playing against both CFR+ and DMC) during the game,\nwhich cannot push the pressure to make the opponent fold and result in many unnecessary\nlosses. In this way, vanilla planning has the lowest chip gain as Table 3 shows. (2) Planning\nwith First-Order ToM: Utilizing First-Order ToM, Suspicion-Agent is capable of making\ndecisions based on its own and estimates of the opponent’s card strength. As a result, it will\nraise more than vanilla planning but it tends to fold more frequently than other strategies,\naiming to minimize unnecessary losses. However, this cautious approach can be exploited\nby savvy opponent models. For example, DMC often raises when holding the weakest\nhand, and CFR+ may occasionally raise even with a mid-level hand to exert pressure on\nSuspicion-Agent. In these instances, Suspicion-Agent’s tendency to fold can lead to losses.\n(3) Planning with Second-Order ToM: In contrast, Suspicion-Agent excels at identifying\nand capitalizing on the behavioural patterns of opponent models. Specifically, when DMC\nchecks—suggesting its hand doesn’t align with the public cards—Suspicion-Agent will raise\nas a bluff to induce folds from the opponents. As a result, Suspicion-Agent exhibits the\nhighest raise rate among the three planning methods evaluated. This aggressive strategy\nallows Suspicion-Agent to accumulate more chips even when holding a weak hand, thereby\nmaximizing its chip gains.\nTable 3:\nThe comparison re-\nsults of Suspicion-Agent when\nplaying with different levels of\nToM against CFR+ on Leduc\nHold’em environments.\nThe\nresults are quantified after 100\ngames.\nOpponent Model\nCFR+\nDMC\nOurs (vanilla plan)\n-89\n-72\nOurs (w\/ First ToM)\n-67\n-26\nOurs (w\/ Second ToM) -22\n+24\nTable 4: The comparison results indicate the impact of\nincluding opponent observations in the game history\nin Leduc Hold’em environments. These results are\nquantified over 50 games, and pos denote the position\nof the opponent model.\nOpponent Model\nCFR+\n(pos 1)\nDMC\n(pos 1)\nAvg.\nWin\nProbability\nDMC\n-6\n+10\n+2\n50%\nCFR+\n-35\n+21\n-14\n50%\nOurs (w\/o hind obs)\n-45\n+18\n+17\n100%\nOurs (w\/ hind obs)\n-16\n+36\n+36.5\n100%\nAblation Study on the Effect of Hindsight Observation\nFollowing (Southey et al., 2012),\nwe assume that Suspicion-Agent has access to observations of the opponent after the end of\neach game, i.e., Hindsight Observation. To assess the impact of it, we conduct an ablation\nstudy in which hindsight observations are not incorporated into the current game. Without\nhindsight observations, we augment the Reflexion module with additional prompts to\nenable it to infer the opponent’s cards based on game outcomes and imperfect observations.\nAs demonstrated in Table 5 and 4, Suspicion-Agent retains its performance advantage over\nthe baseline methods without the benefit of hindsight observations. Specifically, we observe\nthat Suspicion-Agent adopts a more conservative strategy under the increased uncertainty\nthat comes without hindsight observations. This leads to reduced bluffing, resulting in fewer\ngains when playing against CFR+. However, it also minimizes the risk of over-bluffing\nwhen facing DMC, thus yielding higher chip earnings.\n5\nConclusion\nIn this paper, we introduce Suspicion-Agent, the first prompting system designed to en-\nable large language models to engage in various imperfect information games using only\nthe game rules and observations for interpretation. By incorporating first-order ToM and\nsecond-order ToM capabilities, we show that a GPT-4-based Suspicion-Agent can outper-\nform traditional algorithms such as NFSP, even without specialized training or examples.\nAdditionally, we identify and discuss the current limitations of utilizing LLMs in the con-\ntext of imperfect information games. We make all our code and interactive data publicly\navailable to the research community. This will help in better understanding the capabilities\nof large language models, particularly GPT-4, and we hope our data will encourage the\n10\nPublished as a conference paper at COLM 2024\ndevelopment of more efficient models for imperfect information games. In addition, we\nalso present the limitations of Suspicion-Agent in Appendix A.5.\nReferences\nMarah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,\nHany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3\ntechnical report: A highly capable language model locally on your phone. arXiv preprint\narXiv:2404.14219, 2024.\nElif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric\nSchulz. Playing repeated games with large language models, 2023.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy\nLovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multi-\nmodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint\narXiv:2302.04023, 2023.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna\nGajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.\nGraph of thoughts: Solving elaborate problems with large language models. arXiv preprint\narXiv:2308.09687, 2023.\nNoam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus\nbeats top professionals. Science, 359(6374):418–424, 2018.\nNoam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365\n(6456):885–890, 2019.\nNoam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret\nminimization. In International conference on machine learning, pp. 793–802. PMLR, 2019.\nNoam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforce-\nment learning and search for imperfect-information games. Advances in Neural Information\nProcessing Systems, 33:17057–17069, 2020a.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877–1901, 2020b.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min\nChan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent col-\nlaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848,\n2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\net al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311,\n2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers\nto solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\nVincent P. Crawford. “fatal attraction” and level-k thinking in games with non-neutral\nframes. Journal of Economic Behavior and Organization, 156:219–224, 2018. ISSN 0167-2681.\ndoi: https:\/\/doi.org\/10.1016\/j.jebo.2018.10.008. URL https:\/\/www.sciencedirect.com\/\nscience\/article\/pii\/S016726811830283X.\n11\nPublished as a conference paper at COLM 2024\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning, 2023.\nMartin Davies. The mental simulation debate. Philosophical Issues, 5:189–218, 1994.\nHarmen De Weerd, Rineke Verbrugge, and Bart Verheij. How much does it help to know\nwhat she knows you know? an agent-based simulation study. Artificial Intelligence, 199:\n67–92, 2013.\nJinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias\nStengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench: Uncovering the\nstrategic reasoning limitations of llms via game-theoretic evaluations. arXiv preprint\narXiv:2402.12348, 2024.\nGabriele Farina, Christian Kroer, Noam Brown, and Tuomas Sandholm. Stable-predictive\noptimistic counterfactual regret minimization. In ICML, 2019a. URL https:\/\/arxiv.org\/\nabs\/1902.04982.\nGabriele Farina, Christian Kroer, and Tuomas Sandholm. Optimistic Regret Minimization for\nExtensive-Form Games via Dilated Distance-Generating Functions. Curran Associates Inc.,\nRed Hook, NY, USA, 2019b.\nIan Frank and David Basin. A theoretical and empirical investigation of search in imperfect\ninformation games. Theoretical Computer Science, 252(1-2):217–256, 2001.\nChris Frith and Uta Frith. Theory of mind. Current biology, 15(17):R644–R645, 2005.\nDeep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil˙e Lukoˇsi¯ut˙e, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The ca-\npacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459,\n2023.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong\nZheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. Mindagent:\nEmergent gaming interaction, 2023.\nJonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance\nin no-press diplomacy via equilibrium search. arXiv preprint arXiv:2010.02923, 2020.\nJiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao,\nand Steven CH Hoi. From images to textual prompts: Zero-shot vqa with frozen large\nlanguage models. arXiv preprint arXiv:2212.10846, 2022.\nAkshat Gupta. Are chatgpt and gpt-4 good poker players? – a pre-flop analysis, 2023.\nJohn C Harsanyi. Games with incomplete information played by “bayesian” players part ii.\nbayesian equilibrium points. Management Science, 14(5):320–334, 1968.\nJohannes Heinrich and David Silver.\nDeep reinforcement learning from self-play in\nimperfect-information games. arXiv preprint arXiv:1603.01121, 2016.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\nBengt Holmstr¨om and Roger B Myerson. Efficient and durable decision rules with in-\ncomplete information. Econometrica: Journal of the Econometric Society, pp. 1799–1819,\n1983.\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A\nsurvey. arXiv preprint arXiv:2212.10403, 2022.\n12\nPublished as a conference paper at COLM 2024\nSusan Hurley. The shared circuits model (scm): How control, mirroring, and simulation can\nenable imitation, deliberation, and mindreading. Behavioral and brain sciences, 31(1):1–22,\n2008.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin\nBang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language\ngeneration. ACM Computing Surveys, 55(12):1–38, 2023.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer\ntasks. arXiv preprint arXiv:2303.17491, 2023.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. Advances in neural information processing\nsystems, 35:22199–22213, 2022.\nMichal Kosinski. Theory of mind may have spontaneously emerged in large language\nmodels. arXiv preprint arXiv:2302.02083, 2023.\nDavid M Kreps and Robert Wilson. Reputation and imperfect information. Journal of\neconomic theory, 27(2):253–279, 1982.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu\nHuang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent\nwith fast and slow thinking for complex interactive tasks. ArXiv, abs\/2305.17390, 2023.\nURL https:\/\/api.semanticscholar.org\/CorpusID:258960143.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\narXiv preprint arXiv:2304.08485, 2023a.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang\nDing, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv\npreprint arXiv:2308.03688, 2023b.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods\nfor effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun\nZhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large\nlanguage models. arXiv preprint arXiv:2304.09842, 2023a.\nRunyu Lu, Yuanheng Zhu, and Dongbin Zhao. Score-based equilibrium learning in multi-\nplayer finite games with imperfect information, 2023b.\nNick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and\nMark Steedman. Sources of hallucination by large language models on inference tasks.\narXiv preprint arXiv:2305.14552, 2023.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529–533,\n2015.\nC.\nModels.\nModel\ncard\nand\nevaluations\nfor\nclaude\nmodels.\nhttps:\/\/www-\nfiles.anthropic.com\/production\/images\/ Model-Card-Claude-2.pdf., 2023.\nNieves Montes, Nardine Osman, and Carles Sierra. Combining theory of mind and abduc-\ntion for cooperation under imperfect information. In European Conference on Multi-Agent\nSystems, pp. 294–311. Springer, 2022.\nMatej Moravˇc´ık, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor\nDavis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level\nartificial intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017.\n13\nPublished as a conference paper at COLM 2024\nY. Nakajima. Babyagi. https:\/\/github. com\/yoheinakajima\/babyagi, 2023.\nShaun Nichols and Stephen P Stich. Mindreading: An integrated account of pretence,\nself-awareness, and understanding other minds. 2003.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744, 2022.\nXin Ouyang and Ting Zhou. Imperfect-information game ai agent based on reinforcement\nlearning using tree search and a deep neural network. Electronics, 12(11), 2023. ISSN\n2079-9292. doi: 10.3390\/electronics12112453. URL https:\/\/www.mdpi.com\/2079-9292\/\n12\/11\/2453.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language\nmodel connected with massive apis, 2023.\nDavid Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral\nand Brain Sciences, 1(4):515–526, 1978. doi: 10.1017\/S0140525X00076512.\nLouise R¨oska-Hardy. Theory theory (simulation theory, theory of mind). 2008.\nTim Roughgarden. Twenty lectures on algorithmic game theory. Cambridge University Press,\n2016.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can\nteach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\nDale Schuurmans. Memory augmented large language models are computationally univer-\nsal. arXiv preprint arXiv:2301.04589, 2023.\nBilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu Wang, Ruoxi Jia, and Ming Jin.\nAlgorithm of thoughts: Enhancing exploration of ideas in large language models. arXiv\npreprint arXiv:2308.10379, 2023.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\nSamuel Sokota, Ryan D’Orazio, Chun Kai Ling, David J. Wu, J. Zico Kolter, and Noam\nBrown. Abstracting imperfect information away from two-player zero-sum games, 2023.\nFinnegan Southey, Michael P Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse\nBillings, and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. arXiv preprint\narXiv:1207.1411, 2012.\nOskari Tammelin. Solving large imperfect information games using cfr+. arXiv preprint\narXiv:1407.5042, 2014.\nMaxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox. Multi-view 3d models from\nsingle images with a convolutional network. In Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14, pp.\n322–337. Springer, 2016.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al.\nGemma: Open models based on gemini research and technology.\narXiv preprint\narXiv:2403.08295, 2024.\n14\nPublished as a conference paper at COLM 2024\nMeta Fundamental AI Research Diplomacy Team, Anton Bakhtin, Noam Brown, Emily\nDinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray,\nHengyuan Hu, et al. Human-level play in the game of diplomacy by combining language\nmodels with strategic reasoning. Science, 378(6624):1067–1074, 2022.\nMeta Llama Team. Introducing meta llama 3: The most capable openly available llm to date.\nhttps:\/\/ai.meta.com\/blog\/meta-llama-3\/, 2024.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi\nFan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023a.\nHongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Hongyuan\nLu, Fei Mi, and Kam-Fai Wong. Tpe: Towards better compositional reasoning over\nconceptual tools with multi-persona collaboration, 2023b.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\nJiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A\nsurvey on large language model-based autonomous agents, 2023c.\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao,\nChaofei Wang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against\ndeception through recursive contemplation. arXiv preprint arXiv:2310.01320, 2023d.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task\nagents. arXiv preprint arXiv:2302.01560, 2023e.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners.\narXiv preprint arXiv:2109.01652, 2021.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of\nlarge language models. arXiv preprint arXiv:2206.07682, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V\nLe, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language\nmodels. Advances in Neural Information Processing Systems, 35:24824–24837, 2022b.\nYue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos\nAzaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by\nstudying papers and reasoning. arXiv preprint arXiv:2305.15486, 2023.\nMichael Wunder, Michael Kaisers, John Robert Yaros, and Michael Littman. Using iter-\nated reasoning to predict opponent strategies. In The 10th International Conference on\nAutonomous Agents and Multiagent Systems - Volume 2, AAMAS ’11, pp. 593–600, Richland,\nSC, 2011. International Foundation for Autonomous Agents and Multiagent Systems.\nISBN 0982657161.\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang\nLiu. Exploring large language models for communication games: An empirical study on\nwerewolf, 2023a.\nZelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement\nlearning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023b.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and\nYuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint\narXiv:2210.03629, 2022.\n15\nPublished as a conference paper at COLM 2024\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and\nKarthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language\nmodels. arXiv preprint arXiv:2305.10601, 2023.\nDaochen Zha, Kwei-Herng Lai, Yuanpu Cao, Songyi Huang, Ruzhe Wei, Junyu Guo, and\nXia Hu. Rlcard: A toolkit for reinforcement learning in card games. arXiv preprint\narXiv:1910.04376, 2019.\nDaochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, Xiangru Lian, Xia Hu, and Ji Liu.\nDouzero: Mastering doudizhu with self-play deep reinforcement learning. In international\nconference on machine learning, pp. 12333–12344. PMLR, 2021.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang,\nEnbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean: A survey on\nhallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.\nMartin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret\nminimization in games with incomplete information. Advances in neural information\nprocessing systems, 20, 2007.\n16\nPublished as a conference paper at COLM 2024\nA\nAppendix\nA.1\nRelated Works\nImperfect Information Game Imperfect information games, like poker, have garnered\nsignificant research attention due to the lack of complete information about player states\nand actions (Brown & Sandholm, 2018; 2019; Moravˇc´ık et al., 2017; Southey et al., 2012; Frank\n& Basin, 2001). Unlike perfect information games, these settings enable strategies involving\ndeception and stochastic approaches (Montes et al., 2022; Kreps & Wilson, 1982). Previous\nresearch has explored various aspects, including game theory (e.g., CFR, CFR+) (Lu et al.,\n2023b; Zinkevich et al., 2007; Tammelin, 2014; Farina et al., 2019b;a), reinforcement learning\n(Ouyang & Zhou, 2023; Roughgarden, 2016), deep reinforcement learning (e.g., NFSP, DQN,\nDMC) (Brown et al., 2020a; Heinrich & Silver, 2016; Mnih et al., 2015; Zha et al., 2021), and\nabstraction techniques (Sokota et al., 2023). However, these methods often require extensive\ncomputation and data collection. Recent studies have applied large language models (LLMs)\nto imperfect information games (Gupta, 2023), showing strong performance in predicting\nplayer behaviors and simplifying natural language interactions (Akata et al., 2023; Xu et al.,\n2023a). GTBench (Duan et al., 2024) evaluates LLM performance in these games, while\nour work introduces a tailored prompt system for GPT-4, which outperforms traditional\nalgorithms like DMC and NFSP without specialized training. The zero-shot capabilities of\nLLMs eliminate the need for pre-training and extensive data, offering a distinct advantage\nover traditional methods.\nReasoning and Planning of LLMs LLMs have recently demonstrated exceptional abilities\nin reasoning and planning across various tasks (Brown et al., 2020b; Chowdhery et al., 2022;\nTouvron et al., 2023), utilizing evidence, arguments, and logic to make informed judgments\n(Huang & Chang, 2022). The Chain-of-Thought (CoT) approach, which involves generating\nintermediate reasoning steps, has significantly improved performance in reasoning tasks\n(Wei et al., 2022b), while the zero-shot capabilities of LLMs have been enhanced by simple\nprompt phrases (Kojima et al., 2022). The Tree-of-Thought (ToT) framework further extends\nCoT by enabling exploration over text units as intermediary steps toward problem-solving\n(Yao et al., 2023). Inspired by developments like Voyager, BabyAGI, and ReAct (Wang\net al., 2023a; Nakajima, 2023; Yao et al., 2022), we propose that the reasoning and planning\nabilities of LLMs can support agents in imperfect information games using only game\nrules and observations. With well-crafted prompts, LLM-based agents can autonomously\ngenerate text to aid in reflection and planning in these contexts (Schuurmans, 2023; Kim\net al., 2023; Wang et al., 2023d; Xu et al., 2023b). Our paper specifically focuses on integrating\ntheory-of-mind (ToM) capabilities into the planning process.\nTheory of Mind (ToM) In imperfect information games, classical methodologies often draw\nfrom game theory, but they acknowledge that human decision-making typically follows a\n”cognitive hierarchy” of strategies rather than strictly adhering to Nash equilibrium (Wunder\net al., 2011). The concept of Theory of Mind (ToM) is essential for understanding human\ncognitive abilities, as it involves predicting behaviors by attributing mental states like beliefs,\ndesires, and intentions (Premack & Woodruff, 1978; Frith & Frith, 2005; Davies, 1994; Nichols\n& Stich, 2003; Hurley, 2008). In these games, ToM enhances decision-making by anticipating\nopponents’ actions (De Weerd et al., 2013). Level-k thinking within cognitive hierarchy\ntheory posits that players predict others’ actions based on varying depths of strategic\nthinking, closely related to ToM (Crawford, 2018). Simulating ToM involves adopting an\nopponent’s perspective to infer potential actions, with higher-order ToM enabling recursive\nbeliefs about others’ thoughts and self-reflection on how one’s actions influence future\ninteractions (Frith & Frith, 2005). Recent studies have begun evaluating ToM capabilities\nin LLMs (Frith & Frith, 2005; Kosinski, 2023), but there remains a gap in integrating ToM\nwithin LLMs for imperfect information games.\nA.2\nBackground\nPrompting in LLMs\nRecent research has shown that LLMs, notably GPT-4, can function as\nuniversal approximators (Schuurmans, 2023; Kim et al., 2023). Given well-crafted prompts,\nthese models can autonomously perform a broad range of tasks using autoregressive text\ngeneration (Schuurmans, 2023; Kim et al., 2023). To formalize this, let Fθ denote a pre-trained\nLLM characterized by the parameter θ. Consider L = (L[0], L[1], . . . , L[M]) as a sequence\n17\nPublished as a conference paper at COLM 2024\nof language tokens, where each L[i] is a distinct token. The distribution Fθ(L) can then be\nexpressed as: Fθ(L) = ∏M\ni=1 Fθ(L[i]|L[0, . . . , i −1]). By framing the input to the language\nmodel as specific task instructions or as few-shot input-output examples (Kojima et al., 2022;\nWei et al., 2022b), it becomes possible to guide Fθ to generate meaningful output for various\ntasks. For convenience, let’s denote the prompt as P = (P[0], P[1], . . . , P[M]). The output\nfrom the LLM, represented by Y, can then be defined by the following probability distri-\nbution: pY = ∏M\ni=1 Fθ(Y[i]|P, Y[1, . . . , i −1]), where M is the total number of output tokens.\nFor convenience, the conditional function Fθ(Y[i]|P, Y[1, . . . , i −1]) can be abbreviated as\nFP\nθ , indicating that it is the function specific to the given prompt P.\nA.3\nGame Rules of Leduc Hold’em\nIn Leduc Hold’em, the deck consists only of two Jacks, two Queens, and two Kings. Each\nplayer is dealt one of these as a hole card, and a single public card is revealed. The winner is\ndetermined by matching the rank of the public card with their hole card; if no such match\nexists, the player with the higher-ranked hole card wins. Note that the big blind in our\nsetting is 2, and the payoff of a single game ranges from 1 to 14. Following (Southey et al.,\n2012), we also add the opponent’s observation into the single game history h after the end\nof each game, which also conforms with the real-world experience, but we also perform the\nablation study about it in Section 4.3 and A.10.\nA.4\nGap between GPT-3.5 and GPT-4\nUpon closer examination, we’ve identified several areas where GPT-3.5 falls short: (a) In-\nstruction Comprehension: GPT-3.5 struggles with understanding instructions as effectively\nas GPT-4 does. (b) Long Prompt Handling: The performance of GPT-3.5 significantly deteri-\norates in scenarios involving lengthy prompts. (c) Reasoning and ToM: Even when GPT-3.5\nsuccessfully comprehends the instructions, its reasoning and ToM capabilities are markedly\ninferior to those of GPT-4, resulting in less reasonable outcomes.\nA.5\nLimitations\nRobustness of Results\nDue to budgetary constraints, our experiments are limited to\nrunning 100 games for each comparison with baseline methods. (1) Although this sample\nsize may not be extensive, the superior performance observed over four different baseline\nalgorithms with varying behavioral patterns can still serve as a preliminary demonstration\nof the cognitive capabilities and potential of large language models like GPT-4 in imperfect\ninformation games. (2) Given the same game sequences, Suspicion-Agent can outperform\nbaselines in both positions. This consistency highlights the adaptability and robustness of\nSuspicion-Agent even when faced with varied strategies that give the same card strength.\nConsidering the limited budgets and the experimental results we get, it is safe to claim that\nSuspicion-Agent based on GPT-4 can potentially outperform previous methods designed\nfor imperfect information games.\nHallucination Problem of Large Language Model\nHallucination problem (Zhang et al.,\n2023; McKenna et al., 2023; Bang et al., 2023)—generating outputs that are nonsensical or\nunfaithful to the provided source content (Ji et al., 2023)—poses a significant challenge\nin LLMs. In our experiments, we found that when given only simple instructions, LLMs\ncan produce outputs that are either meaningful and rigorous or less rigorous and even\ninvalid. This variability compromises the reliability of LLMs, particularly when they\ninteract with models trained for specialized tasks. In addition, the outputs of LLMs are very\nsensitive to the prompts. To mitigate this issue, we developed multiple output templates\nto improve the quality of the outputs of LLMs, the effectiveness of which is empirically\ndemonstrated in our main results (these templates will be made publicly available in our\ncode repository). However, further work is needed to better align LLM-generated outputs\nwith given instruction prompts. Enhancing this alignment is a critical area of research for\nimproving the reliability and real-world applicability of LLMs.\n18\nPublished as a conference paper at COLM 2024\nLong Reasoning Problem\nThe limitations of LLMs like GPT-4 manifest in two ways when\napplied to complex tasks in imperfect information games:\n1) Long Prompts Problem: To adapt LLMs for different imperfect information games, it is\nnecessary to input both game rules and game conversion rules. When these are combined\nwith the specialized prompts designed for our Suspicion-Agent, the resulting language\nmodel prompts become excessively long. We have observed a rapid decline in the quality of\nthe model’s output as the length of these prompts increases. Limited by the budgets, we\nimplement the planning and evaluator model into a single function, which results in a quite\nlong sequence generation, leading to the performance decline to some extent.\n2) Complex Reasoning\/Calculation Problem: When tasked with conducting intricate calcu-\nlations—such as computing the average win, lose, or draw rate when an opponent holds\ndifferent cards—GPT-4 struggles to consistently generate accurate mathematical equations\nand results.\nExpensive Inference Cost and Slow Inference Time\nAs demonstrated in Table 1, only\nGPT-4 is capable of performing well in the game of Leduc Hold’em. Due to the extensive\nprompts and inference tokens, the cost per game reaches nearly one dollar. Additionally, the\nlarge model size of GPT-4 leads to a longer inference time for Suspicion-Agent, requiring\nseveral minutes to complete a single game of Leduc Hold’em. These two limitations\nunderscore the importance of developing a specialized local language model for this task,\nwhich also serves as our motivation for releasing all associated data.\nPlanning Depth\nIn our paper, we only focus on single-step planning, but note that our\nplanning method is orthogonal to recently proposed approaches that leverage large lan-\nguage models for planning with depth, such as Tree-of-Thoughts (Yao et al., 2023), Graph-\nof-Thoughts (Besta et al., 2023), and Algorithm-of-Thoughts (Sel et al., 2023). While these\napproaches offer promising directions for future research, they come with high computa-\ntional costs, and thus we do not incorporate them into our current methods.\nMore Language Model Evaluation\nIn this paper, the evaluation is confined to the per-\nformance of GPT-3.5 and GPT-4 on imperfect information games, which represent only\na fraction of the large language models in the contemporary research landscape. For fu-\nture work, we aim to expand the scope of our evaluation to include other state-of-the-art\nlarge language models, such as PaLM2 (Anil et al., 2023), Claude2 (Models, 2023), and\nLLaMA2 (Touvron et al., 2023), among others. This broader evaluation will not only offer a\nmore comprehensive understanding of the capabilities and limitations of these models in\nimperfect information games but also facilitate a nuanced comparative analysis. Such an\napproach is expected to yield richer insights into the adaptability and generalizability of\nlarge language models in complex, real-world scenarios, thereby contributing to the field’s\ncollective understanding of their potential applications and limitations.\nA.6\nFuture Work\nTool Use As outlined in the section A.5, Suspicion-Agent suffers from hallucination problems\nand struggles with long context reasoning. This can lead to calculation inaccuracies and\nsometimes produce responses that deviate from factual information or are out of context.\nSuch issues considerably degrade the performance in final decision-making. A natural\nsolution is to break down the problem into multiple sub-problems and employ specialized\nsmaller models or tools (Wang et al., 2023c; Schick et al., 2023; Wang et al., 2023b; Patil et al.,\n2023; Lu et al., 2023a; Patil et al., 2023), for better task completion.\nMulti-Modality In the present paper, the analytical scope is limited to text-based imperfect\ninformation games. However, it is important to recognize that real-world interactions\noften encompass more than just textual information. For instance, human communication\nfrequently involves a variety of modalities such as facial expressions and vocal tones, which\ncan serve as additional cues for interpreting beliefs or intentions. Given the increasing\nadvancements in multi-modal large language models—e.g., InstrucBLIP (Dai et al., 2023),\nLLaVa (Liu et al., 2023a)—we aim to extend our research to incorporate these multi-modal\n19\nPublished as a conference paper at COLM 2024\naspects. By doing so, we aspire to develop AI agents capable of navigating imperfect\ninformation games that more closely mimic real-world complexities. Integrating multi-\nmodal observations into our model will not only enrich the agents’ understanding of the\ngame environment but also broaden the applicability of our methods. This will potentially\nlead to a more nuanced and comprehensive understanding of the strategic behavior of\nLLMs in scenarios that more accurately reflect real-world conditions.\nMulti-Player Setting In the paper, our focus is restricted to two-player imperfect information\ngames. However, it is worth acknowledging that real-world scenarios often involve multi-\nplayer settings, which introduce additional complexities and nuances that are not captured\nin a two-player framework. Recent developments have given rise to novel multi-player\ngame environments, such as AgentVerse (Chen et al., 2023) and Mind Agent (Gong et al.,\n2023). These environments present more realistic settings for evaluating the applicability and\nefficacy of large language models in game theory scenarios. Therefore, a natural extension\nof our research will be to adapt our methods to these multi-player environments.\nA.7\nOutput Samples\nA.7.1\nFirst-Order ToM aware Planning\nOpponent’s Pattern:\n1st Round: - When GoodGuy holds a King, he tends to raise (70%) or call (30%). - When\nGoodGuy holds a Queen, he tends to call (60%) or raise (40%). - When GoodGuy holds a\nJack, he tends to fold (60%) or call (40%).\n2nd Round: - When GoodGuy holds a King and the public card is a Queen or Jack, he tends\nto raise (80%) or check (20%).- When GoodGuy holds a Queen and the public card is a King\nor Jack, he tends to call (70%) or check (30%). - When GoodGuy holds a Jack and the public\ncard is a King or Queen, he tends to fold (60%) or call (40%).\nBelief on GoodGuy’s cards:\nIn the 1st round, GoodGuy raised (probability 70%), which according to his behavior pattern,\nsuggests that he might be holding a King (70%) or a Queen (30%). In the current round,\nGoodGuy raised again (probability 80%), which according to his behavior pattern and the\nmatch with the current public card (Queen), suggests that he might be holding a King (80%)\nor a Queen (20%).\n20\nPublished as a conference paper at COLM 2024\nReasonable Plans:\nPlan 1: Call - If I call, I will match GoodGuy’s bet of 10 chips, bringing the total pot to 20\nchips. This is a conservative strategy that allows me to stay in the game without risking too\nmany chips. However, it also means that I am not putting any pressure on GoodGuy, which\ncould allow him to win the game easily if he has a higher-ranking card. Plan 2: Raise - If I\nraise, I will increase the bet by 8 chips, bringing the total pot to 24 chips. This is an aggressive\nstrategy that puts pressure on GoodGuy and could potentially force him to fold if he has a\nlow-ranking card. However, it also risks losing more chips if GoodGuy has a higher-ranking\ncard and decides to call or raise. Plan 3: Fold - If I fold, I will forfeit the game and lose any\nchips I have already put into the pot. This is a defensive strategy that minimizes my losses if\nI believe that GoodGuy has a higher-ranking card. However, it also means that I give up any\nchance of winning the game.\nPotential GoodGuy’s actions and Estimate Winning\/Lose\/Draw Rate:\n- If GoodGuy holds a King (80% probability), he will likely raise (80% probability) or call\n(20% probability) . If this happens, I will likely lose (100% probability), as a King is higher\nthan my Jack. Therefore, the overall lose rate for this scenario is 80%.- If GoodGuy holds a\nQueen (20% probability), he will likely call (70% probability). If this happens, I will likely\nlose (100% probability), as a pair of Queens is stronger than my single Jack. Therefore, the\noverall lose rate for this scenario is 20% * 70% = 14%. - Therefore, my overall lose rate is\n80% + 14% = 94%. My overall win rate is 0%, as there is no scenario in which I can win.\nMy overall draw rate is also 6%.\nEstimate Expected Chips Gain for Each Plan: Plan 1: Call - Expected Chips Gain = win\nrate * winning payoff - lose rate * lose payoff = 0% * 10 - 94% * 10 = -9.4 chips. Plan 2:\nRaise - Expected Chips Gain = win rate * winning payoff - lose rate * lose payoff = 0% * 12 -\n94% * 12 = -15.6 chips. Plan 3: Fold - Expected Chips Gain = win rate * winning payoff -\nlose rate * lose payoff = 0% * 0 - 100% * 8 = -8 chips.\nPlan Selection: - Plan 1: Call: -9.4 chips - Plan 2: Raise: -15.6 chips- Plan 3: Fold: -8\nchips. Based on the estimated expected chips gains, the best plan is to fold, as it results in the\nsmallest loss of chips. This is a reasonable decision, as it minimizes my losses in a situation\nwhere I am likely to lose. It also aligns with my strategy improvement of being more cautious\nwhen I have a low-ranking card.\n21\nPublished as a conference paper at COLM 2024\nA.7.2\nSecond-Order ToM aware Planning\nOpponent’s Pattern:\n1. When GoodGuy holds a Jack: - If the opponent raises in the first round, GoodGuy tends to\nfold (probability 80%) or call (probability 20%). - If the opponent checks or calls in the first\nround, GoodGuy tends to call (probability 70%) or raise (probability 30%). - In the second\nround, if the public card is a King or Queen, GoodGuy tends to fold (probability 90%) or call\n(probability 10%). - If the public card is a Jack, GoodGuy tends to raise (probability 80%) or\ncall (probability 20%).\n2. When GoodGuy holds a Queen: - If the opponent raises in the first round, GoodGuy tends\nto call (probability 60%) or fold (probability 40%). - If the opponent checks or calls in the\nfirst round, GoodGuy tends to raise (probability 60%) or call (probability 40%). - In the\nsecond round, if the public card is a King, GoodGuy tends to fold (probability 70%) or call\n(probability 30%). - If the public card is a Queen or Jack, GoodGuy tends to raise (probability\n70%) or call (probability 30%).\n3. When GoodGuy holds a King: - If the opponent raises in the first round, GoodGuy tends\nto call (probability 70%) or raise (probability 30%). - If the opponent checks or calls in the\nfirst round, GoodGuy tends to raise (probability 80%) or call (probability 20%). - In the\nsecond round, regardless of the public card, GoodGuy tends to raise (probability 70%) or call\n(probability 30%).\nThe opponent’s guess on Suspicion-Agent’s game pattern:\n1. When I hold a Jack: - GoodGuy might believe that I tend to raise in the first round\n(probability 60%) or call (probability 40%). - In the second round, if the public card is a King\nor Queen, GoodGuy might believe that I tend to fold (probability 70%) or call (probability\n30%). - If the public card is a Jack, GoodGuy might believe that I tend to raise (probability\n80%) or call (probability 20%).\n2. When I hold a Queen: - GoodGuy might believe that I tend to raise in the first round\n(probability 70%) or call (probability 30%). - In the second round, if the public card is a\nKing, GoodGuy might believe that I tend to fold (probability 60%) or call (probability 40%).\n- If the public card is a Queen or Jack, GoodGuy might believe that I tend to raise (probability\n70%) or call (probability 30%).\n3. When I hold a King: - GoodGuy might believe that I tend to raise in the first round\n(probability 80%) or call (probability 20%). - In the second round, regardless of the public\ncard, GoodGuy might believe that I tend to raise (probability 70%) or call (probability 30%).\nBelief on GoodGuy’s Cards:\nIn the first round, GoodGuy chose to call. According to GoodGuy’s behavior pattern, when\nhe holds a Jack, he tends to call in the first round with a probability of 70%. When he holds a\nQueen, he tends to raise in the first round with a probability of 60%, and when he holds a\nKing, he tends to raise in the first round with a probability of 80%. Since GoodGuy chose to\ncall, it is more likely that he holds a Jack (probability 70%), less likely a Queen (probability\n20%), and least likely a King (probability 10%).\nGoodGuy’s Belief on My Cards:\nIn the 1st round, I haven’t taken any action yet. However, based on GoodGuy’s guess on my\ngame pattern, if I choose to raise, he might believe that I have a King (80% probability) or\na Queen (20% probability). If I choose to call, he might believe that I have a Queen (60%\nprobability) or a Jack (40% probability). If I choose to fold, he might believe that I have a Jack\n(100% probability).\n22\nPublished as a conference paper at COLM 2024\nMake Reasonable Plans:\nPlan 1: Raise- If I raise, I can potentially confuse GoodGuy and make him believe that I have\na higher card. This could lead him to fold, especially if he has a Jack. Plan 2: Fold - Folding\nwould be a safe option, as I currently hold the lowest rank card. However, this would mean\ngiving up the current game and losing the chips I have already put in the pot.Plan 3: Check -\nChecking would allow me to see the public card without putting more chips in the pot. This\ncould be a good strategy if I believe that GoodGuy has a higher card.\nEstimate Winning\/Lose\/Draw Rate for Each Plan:\nPlan 1: Raise- If GoodGuy holds a Jack (70% probability), he will likely fold (80%\nprobability). This would result in a win for me.- If GoodGuy holds a Queen (20%\nprobability), he will likely call (60% probability) or fold (40% probability). This would result\nin a draw or a loss for me.- If GoodGuy holds a King (10% probability), he will likely call\n(70% probability) or raise (30% probability). This would result in a loss for me.Plan 2:\nFold- If I fold, I will lose the current game regardless of GoodGuy’s card. Plan 3: Check\n- If GoodGuy holds a Jack (70% probability), he will likely call (70% probability) or raise\n(30% probability). This would result in a draw or a loss for me. - If GoodGuy holds a Queen\n(20% probability), he will likely raise (60% probability) or call (40% probability). This would\nresult in a loss for me. - If GoodGuy holds a King (10% probability), he will likely raise (80%\nprobability) or call (20% probability). This would result in a loss for me. Potential believes\nabout the number of winning and lose payoffs for each plan: Plan 1: Raise - After the action,\nchips in the pot: 6. If win, the winning payoff would be 3. If lose, the lose payoff would be 3.\nPlan 2: Fold - After the action, chips in the pot: 4. If win, the winning payoff would be 0\n(as I would have already folded). If lose, the lose payoff would be 2. Plan 3: Check - After\nthe action, chips in the pot: 4. If win, the winning payoff would be 2. If lose, the lose payoff\nwould be 2. Estimate Expected Chips Gain for Each Plan: Plan 1: Raise - Expected chips\ngain = winning rate * winning payoff - lose rate * lose payoff = 0.56 * 3 - 0.44 * 3 = 0.36 Plan\n2: Fold - Expected chips gain = 0 (as I would have already folded) Plan 3: Check - Expected\nchips gain = winning rate * winning payoff - lose rate * lose payoff = 0.14 * 2 - 0.86 * 2 =\n-1.44\nPlan Selection:\nBased on the estimated expected chips gain, Plan 1 (Raise) would be the best strategy to\nimplement in the current game. This plan has the highest expected chips gain and aligns\nwith the strategy improvement of confusing GoodGuy by making him believe that I have a\nhigher card.\nA.8\nAblation Study on Levels of ToM\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparison of Action Percentage of Agent with Different Planning\nSecond-Order\nFirst-Order\nVanilla\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparison of Action Percentage of Agent with Different Planning\nSecond-Order\nFirst-Order\nVanilla\nFigure 5: Left Figure. The illustration of the action percentage of Suspicion-Agent with\ndifferent levels of ToM when playing against CFR+. Right Figure. he illustration of the\naction percentage of Suspicion-Agent with different level of ToM when playing against\nDMC.\n23\nPublished as a conference paper at COLM 2024\nA.9\nAlgorithm\nAlgorithm 1 The inference procedure of our Suspicion-Agent\nInitialize the game history D, large language model Fθ, Game Rule Rule, Observation\nConversion Rule Ruleobs\nfor i = 1 to Max Game Num do\nInitialize the current game buffer hi\nGet the Opponent’s Behaviour Pattern and Opponent’s Guess on My Pattern, and\nReflexion Oi\nbp, Oi\nf ∼Fbp\nθ (Rule, D).\nfor t = 1 to TaskHorizon do\nGet the Private Observation St\npri(0), St\npub and action of the opponent at\n1 from the\nenvironment.\nGet the Text Observation Description Obst\nr ∼Fobs\nθ\n(Rule, Ruleobs, St\npri(0), St\npub).\nGet the Card Prediction Ot\ncard pred ∼Fcard pred\nθ\n(Rule, hT, Obst\nr, Oi\nbp).\nMake Plans Ot\nplan ∼Fplan\nθ\n(Rule, hT, Obst\nr, Oi\nbp, Oi\nf ).\nEvaluate Plans and Make Action at\n0 ∼Feval\nθ\n(Rule, Ot\nplan, Ot\ncard pred, Oi\nbp, Oi\nf ).\nExecute Action and Get the Game Result ri\nAdd the Game State and Action into the current game buffer hi\n←\nhi ∪\n{St\npri(0), St\npub, at\n0, at\n1, ri}\nend for\nAdd the current game buffer hi into game history D ←D ∪hi\nend for\nA.10\nAblation Study on the Effect of Hindsight Observation\nIn our primary results, following (Southey et al., 2012), we assume that Suspicion-Agent has\naccess to hindsight observations at the conclusion of each individual game,i.e., Hindsight\nObservation. To assess the impact of this feature, we conduct an ablation study in which\nhindsight observations are not incorporated into the current game. In the absence of these\nhindsight observations, we augment the Reflexion module with additional prompts to\nenable it to infer the opponent’s cards based on game outcomes and Suspicion-Agent’s own\nobservations.\nAs demonstrated in Table 5 and 4, Suspicion-Agent retains its performance advantage over\nthe baseline methods even without the benefit of hindsight observations. Specifically, we\nobserve that Suspicion-Agent adopts a more conservative strategy under the increased\nuncertainty that comes without hindsight observations. This leads to reduced bluffing,\nresulting in fewer gains when playing against CFR+. However, it also minimizes the risk of\nover-bluffing when facing DMC, thus yielding higher chip earnings.\nTable 5: The comparison results indicate the impact of including opponent observations\nin the game history when Suspicion-Agent competes against CFR in Leduc Hold’em envi-\nronments. The results are the win\/lose chips after 100 games with different seeds, and the\nnumber of win\/lose chips ranges from 1 to 14.\nOpponent Model\nCFR+\nDMC\nOurs (w\/o hindsight observation)\n-42\n+51\nOurs (w\/ hindsight observation)\n-22\n+24\n24\nPublished as a conference paper at COLM 2024\nTable 6: More Suspicion-Agent experimental results using open-sourced large language\nmodels.\nDMC\nDQN\nNSFP\nCFR+\nGPT-4\n24\n45\n142\n-22\nMistral-v2 7B\n23\n33\n-10\n-66\nGemma 7B (Team et al., 2024)\n26\n8\n10\n-72\nLlama3-8B Team (2024)\n54\n-67\n-61\n-103\nPhi3-medium (Abdin et al., 2024)\n82\n57\n36\n-14\nA.11\nQualitative Samples\nA.11.1\nLeduc Hold’em\nWe present qualitative samples showcasing Suspicion-Agent’s behaviour under different\nstrategies: Vanilla Planning, Planning with First-Order ToM, and Planning with Second-\nOrder ToM in Leduc Hold’em. These samples can be viewed in Figure 6, 7, 8, and 9,\nrespectively. In each scenario, Suspicion-Agent holds a Jack, while the opponent has either\na Jack or Queen. The opponent’s initial choice to call, rather than raise, suggests they have a\nweak hand. Under the Vanilla Planning strategy, Suspicion-Agent chooses to call in order\nto see the public hand. When this reveals a weak hand, the opponent promptly raises,\nleaving Suspicion-Agent in a precarious position, given that a Jack is the weakest hand.\nWith the First-Order ToM strategy, Suspicion-Agent opts to fold, aiming to minimize losses.\nThis decision is driven by the observation that the opponent typically calls when holding a\nQueen or Jack and has a knack for successfully bluffing Suspicion-Agent. However, these\nstrategies fall short in capitalizing on the inferred weakness of the opponent’s hand. This\nshortcoming stems from the fact that they don’t consider how Suspicion-Agent’s moves\nmight influence the opponent’s reactions. In contrast, as demonstrated in Figure 9, simple\nprompts enable Suspicion-Agent to understand how to influence the opponent’s actions.\nIntentionally choosing to raise places pressure on the opponent, urging them to fold and\nminimize their losses. As a result, Suspicion-Agent manages to win numerous games even\nwhen holding hands of similar strength, so that Suspicion-Agent can earn more chips than\nbaselines. Moreover, as depicted in Figure 10, if the opponent either calls or counters the\nraise made by Suspicion-Agent, it suggests the opponent has a strong hand. Recognizing\nthis, Suspicion-Agent swiftly adapts its strategy, opting to fold to prevent further losses.\nThis demonstrates the impressive strategic flexibility of Suspicion-Agent.\nAction: Call (Holds Jack or Queen)\nObs:In this round, you are holding a \nJack of Hearts. So far, you have putx\n2 chips into the pot, while your \nopponent has put 2 chips.\nAction: Check\nAction: Raise\nVanilla Planning\nPlanning with First-Order ToM\nPlanning with Second-Order ToM\nAction: Call (Holds Jack or Queen)\nObs:In this round, you are holding a \nJack of Hearts. So far, you have putx\n2 chips into the pot, while your \nopponent has put 2 chips.\nAction: Fold\nAction: Call (Holds Jack or Queen)\nObs:In this round, you are holding a \nJack of Hearts. So far, you have putx\n2 chips into the pot, while your \nopponent has put 2 chips.\nAction: Raise\nAction: Fold\nFigure 6: The qualitative sample of Suspicion-Agent on Leduc Hold’em. The whole outputs\nof Suspicion-Agent are given in Appendix 7, 8 and 9, respectively.\n25\nPublished as a conference paper at COLM 2024\nAction: Call\nObs:In this round, you are holding a Jack of Hearts. So far, you have put 2 chips into the pot, while your opponent \nhas put 2 chips.\nVanilla Planning\nMake Plans: Plan 1: Raise In this plan, I will raise the bet by 2 chips. This will increase the pot to 6 chips \nand put pressure on GoodGuy. If he has a low card, he might fold, giving me the win. If he has a high card, he \nmight call or raise, leading to a larger pot. Plan 2: Fold In this plan, I will fold my hand. This will result \nin a loss of the current game, but I will save my chips for future games where I might have a better hand. Plan \n3: Check In this plan, I will check, which means I will not add any more chips to the pot. This will allow me to \nsee the public card without risking any more chips.\nEstimate Winning\/Loss\/Draw Rate for Each Plan: Plan 1: Raise If I raise, I have a 1\/3 chance of winning if the \npublic card matches my Jack, a 1\/3 chance of losing if the public card is a King or Queen, and a 1\/3 chance of a \ndraw if the public card is another Jack. Overall, my winning rate is 33%, my losing rate is 33%, and my draw \nrate is 33%. Plan 2: Fold If I fold, I will lose this game, so my losing rate is 100%. Plan 3: Check If I check, \nthe outcome will depend on the public card. The probabilities are the same as in Plan 1: a 33% chance of winning, \na 33% chance of losing, and a 33% chance of a draw.\nExpected Chips Gain for Each Plan: Plan 1: Raise The expected chips gain is 33% * 3 chips - 33% * 2 chips = 0.33 \nchips. Plan 2: Fold The expected chips gain is -2 chips, because I will lose 2 chips. Plan 3: Check The expected \nchips gain is 33% * 2 chips = 0.66 chips.\nPlan Selection: Comparing the expected chips gain for each plan, Plan 3: Check has the highest expected chips \ngain of 0.66 chips. Therefore, I will choose Plan 3: Check.\nAction: Check\nFigure 7:\nThe qualitative sample of the vanilla planning of Suspicion-Agent on Leduc\nHold’em.\nAction: Call (Holds Jack too)\nObs:In this round, you are holding a Jack of Hearts. So far, you have put 2 chips into the pot, while your opponent \nhas put 2 chips.\nVanilla Planning\nBelief on GoodGuy's cards:  In the 1st round, GoodGuy chose to 'call' (probability 100%). According to my \nunderstanding of GoodGuy's behavior pattern, when he holds a King, he tends to raise (probability 70%) or call \n(probability 30%). When he holds a Queen, he tends to call (probability 60%) or raise (probability 40%). When he \nholds a Jack, he tends to fold (probability 60%) or call (probability 40%). Given that he chose to 'call', it is \nmore likely that he is holding a Queen (probability 60%) than a King (probability 30%) or a Jack (probability 40%). \nTherefore, I estimate that GoodGuy has a Queen (60%), a King (30%), or a Jack (10%).\nMake Reasonable Plans: Plan 1: Raise - If I raise, I will put 4 more chips into the pot, making the total pot \n8 chips. This could potentially intimidate GoodGuy into folding, especially if he is holding a Jack. However, \nif he is holding a King or Queen, he might call or even raise, which could result in me losing more chips. \n\\n\\nPlan 2: Fold\\n- If I fold, I will lose the 2 chips I‘ve already put into the pot. However, I will not risk \nlosing any more chips. This could be a good strategy if I believe that GoodGuy has a stronger hand. Plan 3: \nCheck\\n- If I check, I will not put any more chips into the pot. This could be a good strategy if I want to \nsee the public card before making a decision. However, this could also give GoodGuy the opportunity to raise, \nwhich could force me to fold or call.\nPotential GoodGuy‘s actions and Estimate Winning\/Lose\/Draw Rate: If GoodGuy holds a Queen (60% probability): -\nHe will likely call (60% probability). If the public card is a Queen, I will likely lose (as I hold a Jack). \nIf the public card is a King or Jack, I have a chance to win if I raise or check and GoodGuy folds. \\n- He \nmight raise (40% probability). If he raises, it’s likely that the public card is a Queen, and I will likely \nlose. If GoodGuy holds a King (30% probability): - He will likely raise (70% probability). If he raises, it‘s \nlikely that the public card is a King, and I will likely lose. - He might call (30% probability). If he calls, \nit’s possible that the public card is a King or Queen, and I will also likely lose. If GoodGuy holds a Jack \n(10% probability):\\n- He will likely fold (60% probability). If he folds, I will win regardless of the public \ncard. \\n- He might call (40% probability). If he calls, it‘s possible that the public card is a King or Queen, \nand I have a chance to win if I raise or check and GoodGuy folds. Overall, my winning rate is 20%, my losing \nrate is 74%, and my draw rate is 6%\nExpected Chips Gain for Each Plan: Plan 1: Raise - Expected Chips Gain = Winning Rate * Winning Payoff -\nLosing Rate * Losing Payoff = 20% * 4 - 74% * 4 = -2.16 chips Plan 2: Fold - Expected Chips Gain = Winning \nRate * Winning Payoff - Losing Rate * Losing Payoff = 1 - 100% * 1 = -1 chips\\n\\nPlan 3: Check\\n- Expected \nChips Gain = Winning Rate * Winning Payoff - Losing Rate * Losing Payoff = 20% * 2 - 74% * 2 = -0.108 chips\nPlan Selection: Based on the estimated expected chips gains, the ranking of the plans is as follows:1. Plan 2: \nFold 2. Plan 3: Check 3. Plan 1: Raise Therefore, I should choose Plan 2: Fold\nAction: Fold\nFigure 8: The qualitative sample of planning with First-Order ToM Suspicion-Agent on\nLeduc Hold’em.\n26\nPublished as a conference paper at COLM 2024\nAction: Call (Holds Jack or Queen)\nObs:In this round, you are holding a Jack of Hearts. So far, you have put 2 chips into the pot, while your opponent \nhas put 2 chips.\nVanilla Planning\nBelief on GoodGuy's cards: GoodGuy chose to 'call' in the first round. According to his behavior pattern, when he \nholds a King, he would like to raise in the first round. Therefore, it is unlikely that he holds a King. When he \nholds a Queen or a Jack, he would like to call in the first round. Therefore, he is likely to hold a Queen or a \nJack. So, the probabilities are: King (0%), Queen (50%), Jack (50%).\nPotential GoodGuy's current believes about your cards:  I have not taken any action in the first round yet. \nAccording to GoodGuy's guess on my game pattern, he believes that I would like to raise when I hold a King, Queen, \nor Jack. Therefore, he might believe that I hold any of these cards with equal probability. So, the probabilities \nare: King (33.33%), Queen (33.33%), Jack (33.33%).\nMake Plans: Plan 1: Raise\n- If GoodGuy holds a Queen (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\n- If GoodGuy holds a Jack (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\nPlan 2: Fold\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). I will lose the pot, which \nwill be 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate \nis 0%.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). I will lose the pot, which will \nbe 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate is \n0%.\nPlan 3: Check\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\nEstimate Expected Chips Gain for Each Plan:\n- Plan 1: Expected Chips Gain = 50% * 3 chips - 0% * 0 chips = 1.5 chips\n- Plan 2: Expected Chips Gain = 0% * 0 chips - 50% * 2 chips = -1 chips\n- Plan 3: The Expected Chips Gain cannot be determined at this stage.\nPlan Selection: Plan 1: raise\nAction: Raise\nAction: Fold\nFigure 9:\nThe qualitative sample about Bluffing of planning with Second-Order ToM\nSuspicion-Agent on Leduc Hold’em.\nAction: Call (Holds King or Queen)\nBelief on board_game_expert‘s cards: In the first round, board_game_expert saw my raise and decided to call. \nAccording to his behavior pattern, when he holds a King, he would call if he sees a raise. When he holds a Queen, \nhe would either fold or call if he sees a raise. When he holds a Jack, he would fold if he sees a raise. \nTherefore, it is likely that board_game_expert holds a King (50% probability) or a Queen (50% probability). In \nthe second round, the public card is a Queen. If board_game_expert holds a Queen, he would raise. However, he \nhasn’t made his move yet. Therefore, the probability of him holding a Queen is uncertain.\nPotential board_game_expert's current believes about your cards:In the first round, I raised the bet. According \nto board_game_expert's guess on my game pattern, when I hold a King, he believes I would raise. When I hold a \nQueen, he believes I would either call or raise. When I hold a Jack, he believes I would fold. Therefore, from \nthe perspective of board_game_expert, I tend to have a King (50% probability) or a Queen (50% probability).\"}}\nMake Plans: Plan 1: Raise\\n- If board_game_expert holds a King (50% probability), he will call (100% \nprobability). I will lose as King is higher than Jack. \\n- If board_game_expert holds a Queen (50% probability), \nhe will call (100% probability). I will lose as Queen is higher than Jack. \\n\\nPlan 2: Fold\\n- If \nboard_game_expert holds a King (50% probability), I will lose the chips I have already put in the pot. \\n- If \nboard_game_expert holds a Queen (50% probability), I will lose the chips I have already put in the pot. \\n\\nPlan\n3: Check\\n- If board_game_expert holds a King (50% probability), he will call (100% probability). I will lose as \nKing is higher than Jack. \\n- If board_game_expert holds a Queen (50% probability), he will call (100% \nprobability). I will lose as Queen is higher than Jack. \nEstimate Expected Chips Gain for Each Plan:\n- Plan 1: Expected Chips Gain = 0% * 8 - 100% * 8 = -8 chips\n- Plan 2: Expected Chips Gain = 0% * 0 - 100% * 4 = -4 chips \n- Plan 3: Expected Chips Gain = 0% * 8 - 100% * 8 = -8 chips\\\nPlan Selection: - Plan 2 has the highest expected chips gain (-4 chips), so I should choose Plan 2: Fold.\"\nAction: Fold (Minimize the loss)\nRaise (Bluff)\nObs: You are currently holding a Jack of Hearts (HJ) and the public card is a Queen of Hearts (HQ). \nFigure 10: The qualitative sample about Strategic Fold of planning with Second-Order\nToM Suspicion-Agent on Leduc Hold’em.\n27\nPublished as a conference paper at COLM 2024\nAction: Foreign Aid (Means that he does not have Duke)\n\"readable_text_obs\": You have two hidden Assassin cards and 2 coins. None of your cards are dead\nBelief on GoodGuy’s cards: In the first round, GoodGuy rt attempted to take foreign aid. According to his \nbehavior pattern, if he holds a Duke, he would likely block foreign aid (90%) or pass (10%). If he holds an \nAssassin or a Captain, he would pass (100%). Since he didn't block the foreign aid, it's unlikely that he has a \nDuke. Therefore, the probabilities of his cards could be: Duke (10%), Assassin (45%), Captain (45%)\nPotential GoodGuy’s current believes about your cards: In the first round, I didn't take any action yet. \nAccording to GoodGuy’s guess on my game pattern, if I hold a Duke, I would likely tax (70%) or take income (30%). \nIf I hold an Assassin, I would likely take income (80%) or tax (20%). If I hold a Captain, I would likely steal \n(60%) or take income (40%). \nMake Plans: Plan 1: Block Foreign Aid\\n- GoodGuy holds Duke (10%): He would likely pass (100%). I would win \n(100%) as I successfully block his foreign aid.\\n- If GoodGuy holds Assassin (45%): He would likely challenge \n(50%) or pass (50%). If he challenges, I would lose (100%) as I don't have a Duke to reveal. If he passes, I \nwould win (100%) as I successfully block his foreign aid.\\n- If GoodGuy holds Captain (45%): He would likely \nchallenge (50%) or pass (50%). If he challenges, I would lose (100%) as I don't have a Duke to reveal. If he \npasses, I would win (100%) as I successfully block his foreign aid.\\\nPlan 1 overall Winning\/Lose\/Draw rates: \\n- Winning rate: 10%*100% + 45%*50% + 45%*50% = 55%\\n- Lose rate: \n45%*50% + 45%*50% = 45%\\n- Draw rate: 0%\nPlan 2: Pass Foreign Aid\\n- GoodGuy holds Duke (10%): He would likely pass (100%). I would neither win nor lose \nas I didn't take any action.\\n- If GoodGuy holds Assassin (45%): He would likely pass (100%). I would neither \nwin nor lose as I didn't take any action.\\n- If GoodGuy holds Captain (45%): He would likely pass (100%). I \nwould neither win nor lose as I didn't take any action.\\n\\nPlan 2 overall Winning\/Lose\/Draw rates: \\n- Winning \nrate: 0%\\n- Lose rate: 0%\\n- Draw rate: 100%\nEstimate Expected Chips Gain for Each Plan:\\n- Plan 1: 55% * 1 - 45% * 1 = 0.1 chips\\n- Plan 2: 0\nPlan Selection:\\n- Plan 1 is the better strategy as it has a positive expected chips gain.\"\nAction: block_foreign_aid (Bluff as a Duke)\nAction: Pass_Block\n\"readable_text_obs\": You have two hidden Assassin cards and 2 coins. None of your cards are dead\nBelief on board_game_expert's cards: In the first round, board_game_expert attempted to take foreign aid, which \nis a neutral action that doesn't reveal much about his cards. However, when I blocked his foreign aid, he chose \nnot to challenge me. This suggests that he might not have a Duke card. Therefore, I believe that the probability \nof board_game_expert having a Duke card is low (10%). Since he didn't challenge my block, he might have an \nAssassin or Captain card. Given his passive play, I would estimate that he has an Assassin card (50%) and a \nCaptain card (40%)\nPotential board_game_expert's current believes about your cards: In the first round, I blocked \nboard_game_expert's foreign aid. This action might lead him to believe that I have a Duke card, as a Duke can \nblock foreign aid. However, I could also be bluffing. Given his knowledge of my game pattern, he might think that \nI have a Duke card (60%) or an Assassin card (40%).\"}}\nMake Plans: Plan 1: Take 'income' action - If board_game_expert holds Duke, he might challenge (60%) or pass \n(40%). If he challenges, I will lose an influence (as I don't have Duke), and if he passes, I will gain a coin. \n\\n- If board_game_expert holds Assassin, he will pass (100%), and I will gain a coin. \\n- If board_game_expert\nholds Captain, he will pass (100%), and I will gain a coin. \nPlan 2: Take 'tax' action (bluffing as Duke)\\n- If board_game_expert holds Duke (low probability), he might \nchallenge (60%) or pass (40%). If he challenges, I will lose an influence (as I don't have Duke), and if he \npasses, I will gain three coins. \\n- If board_game_expert holds Assassin, he might challenge (50%) or pass \n(50%). If he challenges, I will lose an influence (as I don't have Duke), and if he passes, I will gain three \ncoins. \\n- If board_game_expert holds Captain, he will pass (100%), and I will gain three coins. \nPlan 3: Take 'steal' action (bluffing as Captain)\\n- If board_game_expert holds Duke, he will pass (100%), and \nI will steal two coins from him. \\n- If board_game_expert holds Assassin, he will pass (100%), and I will \nsteal two coins from him. \\n- If board_game_expert holds Captain, he might block (80%) or pass (20%). If he \nblocks, I will lose an influence (as I don't have Captain), and if he passes, I will steal two coins from him. \n\\n\\nPlan 4: Take 'exchange' action (bluffing as Ambassador)\\n- If board_game_expert holds Duke, he will pass \n(100%), and I will exchange my cards. \\n- If board_game_expert holds Assassin, he will pass (100%), and I will \nexchange my cards. \\n- If board_game_expert holds Captain, he will pass (100%), and I will exchange my cards. \nBased on the above analysis, Plan 2 (bluffing as Duke and taking 'tax' action) seems to have the highest \npotential for gaining coins. However, it also carries a risk of losing an influence if board_game_expert\ndecides to challenge. Plan 1 (taking 'income' action) is the safest option, but the gain is minimal. Plan 3 \n(bluffing as Captain and taking 'steal' action) could potentially gain two coins, but it also carries a risk \nof losing an influence if board_game_expert holds Captain and decides to block. Plan 4 (bluffing as Ambassador \nand taking 'exchange' action) doesn't gain any coins but could potentially change the game dynamics by getting \nnew cards. \nConsidering the potential gains and risks, I would rank the plans as follows: Plan 2 > Plan 3 > Plan 1 > Plan \n4. Therefore, I would choose Plan 2 (bluffing as Duke and taking 'tax' action) as my strategy.\nAction: Tax (Bluff as a Duke)\nAction: Pass_Tax (Be bluffed) \nFigure 11: The qualitative sample of Suspicion-Agent on Coup, where the GoodGuy is the\nopponent model and board game expert is Suspicion-Agent.\n28\nPublished as a conference paper at COLM 2024\nObservation:{\"raw_obs\": {\"hand\": [\"DQ\", \"H5\"], \"public_cards\": [\"D9\", \"CK\", \"C2\", \"H7\"], \n\"all_chips\": [14, 10], \"my_chips\": 10, \"legal_actions\": [\"call\", \"raise\", \"fold\"], \n\"raise_nums\": [2, 2, 1, 0]}}}\nAction: Raise\nObservation: {\"hand\": [\"D3\", \"H9\"], \"public_cards\": [\"D9\", \"CK\", \"C2\", \"H7\"], \"all_chips\": [14, 18], \n\"my_chips\": 14, \"legal_actions\": [\"call\", \"raise\", \"fold\"], \"raise_nums\": [2, 2, 2, 0], \"game_num\": 6, \n\"rest_chips\": 48.0, \"opponent_rest_chips\": 52.0}, \nReadable Observation: In this round of Texas Hold'em Poker Limit, you are holding a 3 of Diamonds and \na 9 of Hearts. The community cards on the table are a 9 of Diamonds, a King of Clubs, a 2 of Clubs, \nand a 7 of Hearts. Currently, you have 14 chips in the pot, while your opponent has 18 chips. You \nhave the options to call, raise, or fold. So far, you have raised twice in each of the first three \nrounds of betting, and have not raised in the current round. This is the 6th game out of a total of \n40 games. You have 48 chips left, while your opponent has 52 chips left. Your best current hand is a \npair of 9s. The potential future combinations, depending on the last community card, could be two \npairs if a 3, 7, or King comes up, or a three-of-a-kind if another 9 comes up.\"}}\nGoodGuy's game pattern: GoodGuy seems to be a confident player who is not afraid to raise the bet \nwhen he has a decent hand. He also seems to be good at bluffing, as seen in the first game where he \nmanaged to win despite having a high card of 10.  1. When GoodGuy holds a high card (10 or above) or \na pair, he is likely to raise the bet (70% probability) or call (30% probability) in the first round, \nregardless of the opponent's action.  2. If the flop reveals a potential straight or flush, GoodGuy\nis likely to raise the bet (60% probability) or call (40% probability) in the second round, \nregardless of the opponent's action.  3. If the turn or river does not improve his hand, GoodGuy is \nlikely to check (50% probability) or fold (50% probability) if the opponent raises the bet. \nGoodGuy's guess on my game pattern: GoodGuy might perceive board_game_expert as a cautious player \nwho is not afraid to fold when the situation calls for it. \n1st Round: \\n- When I hold a high card (10 or above) or a pair, GoodGuy might think that I will \nraise the bet (probability 70%) or call (probability 30%).\\n- When I hold a low card (9 or below), \nGoodGuy might think that I will check (probability 60%) or fold (probability 40%).\\n\\n2nd Round: \\n-\nIf I have a potential straight or flush after the flop, GoodGuy might think that I will raise the \nbet (probability 80%) or call (probability 20%).\\n- If I do not have a potential straight or flush \nafter the flop, GoodGuy might think that I will check (probability 50%) or fold (probability 50%)\nBelief on GoodGuy's cards: In the first round, GoodGuy raised after I raised, which according to his \nbehavior pattern, suggests that he likely has a high card (10 or above) or a pair (70% probability). \nIn the second round, the flop revealed a potential straight or flush, and GoodGuy raised again, \nsuggesting that he might have a potential straight or flush (30% probability). \nPotential GoodGuy's current believes about my cards: In the first round, I raised, which might lead \nGoodGuy to believe that I have a high card (10 or above) or a pair (60% probability). In the second \nround, the flop did not improve my hand, but I raised again, which might confuse GoodGuy and lead him \nto believe that I am bluffing or that I have a strong hand (50% probability). In the current round, I \nhave not raised yet, which might lead GoodGuy to believe that I am cautious and do not have a strong \nhand (70% probability). From the perspective of GoodGuy, I might have a pair (30% probability), a \npotential straight (20% probability), a potential flush (20% probability), or a high card (30% \nprobability).\"}}\nMake Reasonable Plans: Plan 1: Call- If I call, I will match GoodGuy's bet. This is a safe move \nthat does not risk too many chips. I can say, \\\"I'll match your bet, GoodGuy. Let's see the next \ncard.\\\"Plan 2: Raise\\n- If I raise, I will increase the bet and put pressure on GoodGuy. This \ncould potentially lead to a bigger win, but it also risks more chips. I can say, \\\"I'm feeling \nconfident, GoodGuy. I'll raise the bet.\"Plan 3: Fold- If I fold, I will forfeit the current game \nand lose the chips I have already bet. This is a cautious move that minimizes losses. I can say, \n\\\"I think I'll sit this one out, GoodGuy.\\\"\nPotential GoodGuy's actions and Estimate Winning\/Lose\/Draw Rate for Each Plan: Plan 1: Call- If \nGoodGuy has a potential straight or flush (30% probability, he will likely raise (70% \nprobability). I will likely lose (100% probability).-If GoodGuy has a high card (10 or above) or \na pair (70% probability), he will likely raise (50% probability) or call (50% probability). I \nwill likely win (60% probability), draw (20% probability), or lose (20% probability).\\ Overall, \nthe winning rate for plan 1 is 42.0%, the losing rate is 46.0%, and the draw rate is 14.0%.\nPlan 2: Raise - If GoodGuy has a potential straight or flush (30% probability, he will likely \nraise (70% probability). I will likely lose (100% probability)- ). -If GoodGuy has a high card \n(10 or above) or a pair (70% probability), he will likely call (50% probability) or fold (50% \nprobability). I will likely win (80% probability), draw (10% probability), or lose (10% \nprobability).\\ Overall, the winning rate for plan 1 is 56.0%, the losing rate is 37.0%, and the \ndraw rate is 7.0%. \nPlan 3: Fold\\n- If I fold, I will automatically lose the game. The winning rate for plan 3 is 0%, \nthe losing rate is 100%, and the draw rate is 0%.\nPotential believes about the number of winning and lose payoffs for each plan: Plan 1: Call\\n-\nAfter the action, Chips in the pot: 18. If win, the winning payoff would be 9 chips. If lose, the \nlose payoff would be 9 chips.\\n\\nPlan 2: Raise\\n- After the action, Chips in the pot: 22. If win, \nthe winning payoff would be 11 chips. If lose, the lose payoff would be 11 chips.\\n\\nPlan 3: \nFold\\n- After the action, Chips in the pot: 16. If win, the winning payoff would be 0 chips. If \nlose, the lose payoff would be 8 chips.\nEstimate Expected Chips Gain for Each Plan: Plan 1: Call - Expected Chips Gain = 42.0% * 9 -\n46.0% * 9 = -3.6 chips\\n\\nPlan 2: Raise\\n- Expected Chips Gain = 56% * 11 - 37% * 11 =  21 chips \nPlan 3: Fold\\n- Expected Chips Gain = 0% * 0 - 100% * 9 = -9 chips Plan Selection:\\n\\nThe rank of \nestimated expected chips gains for every plan is as follows:\\n1. Plan 2: Raise\\n2. Plan 1: \nCall\\n3. Plan 3: Fold\\n\\nTherefore, I should execute Plan 2: Raise, as it has the highest \nestimated expected chips gain.\"}}\nAction: Raise\nAction: Fold\nFigure 12: The qualitative sample of Suspicion-Agent on Texas Hold’em Limited, where the\nGoodGuy is the opponent model and board game expert is Suspicion-Agent.\n29\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4.pdf"}
{"title":"Enhancing Agent Learning through World Dynamics Modeling","authors":"Zhiyuan Sun, Haochen Shi, Marc-Alexandre Côté, Glen Berseth, Xingdi Yuan, Bang Liu","summary":"Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.","url":"http:\/\/arxiv.org\/abs\/2407.17695v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.17695v2","published":1721871161000,"comment":null,"pdf_text":"Enhancing Agent Learning through World Dynamics Modeling\nZhiyuan Sun1* †, Haochen Shi1*\nMarc-Alexandre Côté2, Glen Berseth1,3, Xingdi Yuan2‡, Bang Liu1,3‡\n1 Université de Montréal & Mila, Montréal, Canada\n2 Microsoft Research, Montréal, Canada\n3 Canada CIFAR AI Chair\nAbstract\nLarge language models (LLMs) have been in-\ncreasingly applied to tasks in language under-\nstanding and interactive decision-making, with\ntheir impressive performance largely attributed\nto the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of\nthis knowledge can vary across domains. Many\nexisting approaches assume that LLMs pos-\nsess a comprehensive understanding of their\nenvironment, often overlooking potential gaps\nin their grasp of actual world dynamics. To\naddress this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discov-\ners world dynamics from a small number of\ndemonstrations, verifies the accuracy of these\ndynamics, and evolves new, advanced dynam-\nics tailored to the current situation. Through\nextensive evaluations, we assess the impact of\neach component on performance and compare\nthe dynamics generated by DiVE to human-\nannotated dynamics. Our results show that\nLLMs guided by DiVE make more informed\ndecisions, achieving rewards comparable to hu-\nman players in the Crafter environment and sur-\npassing methods that require prior task-specific\ntraining in the MiniHack environment1.\n1\nIntroduction\nBy absorbing internet-scale knowledge autoregres-\nsively, large language models (LLMs) develop a\nbroad understanding of the world (Achiam et al.,\n2023; Team et al., 2023; Brown et al., 2020; Tou-\nvron et al., 2023). This understanding enables them\nto perform well across a variety of tasks (Brown\net al., 2020; Huang et al., 2022; Yao et al., 2022,\n2024). However, achieving this level of compre-\nhension requires a training dataset that is diverse,\n* Equal contribution.\n† zhiyuan.sun@umontreal.ca\n‡ Equal advising.\n1 The\ncode\nis\navailable\nat\nhttps:\/\/github.com\/\nZhiyuuanS\/DiVE\n: LLMs’ Knowledge\n: Domain Knowledge\n: Overlapping Knowledge\nFigure 1: The knowledge gap between LLMs and down-\nstream domains. Although LLMs have a broad under-\nstanding of the world, they may struggle to grasp the\ncomplex dynamics of specific downstream domains.\nprecise, and in-depth to cover essential domain\ninformation. Otherwise, a knowledge gap may\nemerge between LLMs and the target domain, as\nillustrated in Figure 1.\nSince LLMs tend to follow the most common\npatterns in the dataset (Gunasekar et al., 2023) and\nare prone to hallucination, it is crucial that the nec-\nessary information not only appears frequently on\nthe internet but is also reliable (Dziri et al., 2022).\nFor tasks and environments that are newly emerg-\ning (Hafner, 2021; Samvelyan et al., 2021), these\nrequirements are often difficult to meet due to the\nnoisy nature of internet data, leading to potential\nknowledge gaps.\nWhile LLMs may have a general understanding\nof a domain, optimal decision-making requires in-\ndepth, state-specific knowledge. Providing LLMs\nwith such tailored information enhances their grasp\nof both the environment and the current state. For\nexample, mastering Go involves not just knowing\nthe rules but also applying strategies for specific\nboard states. This specialized knowledge varies\nacross domains and states, making offline collec-\ntion impractical.\nTo address these challenges,\nwe propose\nDiscover, Verify, and Evolve (DiVE). Building on\narXiv:2407.17695v2  [cs.AI]  15 Oct 2024\nthe concept of the World Model (Ha and Schmid-\nhuber, 2018), DiVE not only discovers and veri-\nfies primitive world dynamics from demonstrations\nbut also evolves state-specific knowledge for the\ndownstream domain. By providing LLMs with this\ncomprehensive set of dynamics, DiVE bridges the\nknowledge gap between LLMs and the downstream\ndomain, thereby enhancing their decision-making\nabilities.\nDiVE consists of three distinct components: The\nDiscoverer: this component iteratively uncovers\nthe environment’s dynamics from demonstrations\nusing a curriculum learning approach. The Veri-\nfier: this component eliminates unreliable dynam-\nics caused by LLMs’ tendency to hallucinate. The\nEvolver: this component reasons through in-depth,\nstate-specific strategies tailored to the current situa-\ntion based on the learned dynamics.\nIn the Crafter environment (Hafner, 2021) and\nthe MiniHack environment (Samvelyan et al.,\n2021), DiVE learns comprehensive and reliable\ndynamics from demonstrations, guiding the agent’s\ndecision-making process by evolving in-depth\nstrategies. This enables the agent to outperform all\nbaselines, achieving rewards comparable to human\nplayers in the Crafter environment and surpassing\nthe performance of methods requiring task-specific\ntraining in the MiniHack environment. To gain\ndeeper insights into DiVE’s behavior, we provide\nboth quantitative and qualitative analyses.\nIn summary, our primary contribution is a frame-\nwork that learns world dynamics from demonstra-\ntions, guiding LLMs in the decision-making pro-\ncess by online evolving contextual strategies. This\napproach bridges potential knowledge gaps, result-\ning in a more optimal decision-making process for\nLLMs.\n2\nThe Knowledge Gap\nIn this work, we consider a setting where a pre-\ntrained LLM-based agent is employed to solve\ntasks in downstream domains. Conceptually, we\ndefine KLLM as the set of knowledge embedded\nin the LLMs through their training process, and\nKtarget as the universal set of knowledge relevant to\nthe downstream domains.\nTo ensure the effectiveness of the LLMs, we aim\nfor Krelevant, the subset of KLLM that is relevant to\nKtarget, to cover the broadest possible scope. Ad-\nditionally, we seek for Krelevant to contain more\nreliable knowledge K+ than inaccurate knowledge\nK−, where Krelevant = K+ ∪K−. Thus, we de-\nfine three desirable properties to guide our system\ndesign:\n• Recall R =\n|K+|\n|Ktarget| measures the extent to which\nthe knowledge required to solve tasks in the tar-\nget domain is covered by the LLMs. A low recall\ntypically indicates a significant knowledge gap\nbetween the LLMs’ training data and the target\ndomain.\n• Precision P =\n|K+|\n|Krelevant| measures the accuracy of\nthe LLMs’ knowledge when applied to the target\ndomain. An example of inaccurate dynamics is\nshown in Figure 2 (left), where the LLMs hallu-\ncinate that defeating a skeleton will drop items,\nwhich is not true in Crafter. This may be due to\nthe LLMs memorizing Minecraft-specific data.\n• Depth D measures the abstraction levels of\nknowledge representations. Knowledge can be\nrepresented at varying levels of abstraction, from\nbasic game rules to higher-level strategies.\nWe recognize that precisely quantifying the\nknowledge overlap between LLMs and a down-\nstream domain is challenging. In Section 3, we pro-\nvide a mathematical analysis showing how DiVE\nimproves across all three dimensions.\nIn Sec-\ntion 5.4, we measure the knowledge overlap to\ndemonstrate that DiVE effectively learns useful\ngame dynamics and, to some extent, bridges the\nknowledge gap.\n3\nDiVE: Discover, Verify, and Evolve\nIn an ideal scenario, one could bridge the knowl-\nedge gap by fine-tuning the LLM to adapt to the\ntarget domain. However, this approach is often less\npractical due to its reliance on large amounts of\nannotated data and significant computational over-\nhead (Hu et al., 2021; Zheng et al., 2024; Carta\net al., 2023; Ouyang et al., 2022). Our frame-\nwork, DiVE, is designed to address the knowledge\ngap while considering all three desirable proper-\nties—recall, precision, and depth—without requir-\ning extensive data collection from the target do-\nmain. It is a prompt-based method that learns world\ndynamics W directly from the environment.\nAs shown in Figure 2, DiVE is initially boot-\nstrapped using a small set of human demonstra-\ntion trajectories H, each consisting of observations\not, actions at, and rewards rt at each timestep\nt.\nWe then transform each observation ot into\nIt is daytime\\nState description: \\n-\nEast: immediate (grass); nearby \n(unexplored_area); \\n- North: \nimmediate (grass); nearby \n(unexplored_area); \\n- North East: \nimmediate (grass); nearby (cow, \nunexplored_area); \\n- North West: \nimmediate (grass); nearby \n(unexplored_area, tree); \\n- South: \nimmediate (grass); nearby \n(unexplored_area); cow: North East 5 \nblocks away (nearby) (objects in \nbetween: {'grass'}) \\n- Facing grass on \nthe south.\\nYour status:\\n- health: \n9\/9\\n- food: 9\/9\\n- drink: 9\/9\\n- energy: \n9\/9\\nYou have nothing in your \ninventory.\nIt is daytime\\nState description: \\n-\nEast: immediate (grass); nearby \n(unexplored_area); \\n- North: \nimmediate (grass); nearby \n(unexplored_area); \\n- North East: \nimmediate (grass); nearby (cow, \nunexplored_area); \\n- North West: \nimmediate (grass); nearby \n(unexplored_area, tree); \\n- South: \nimmediate (grass); nearby \n(unexplored_area); cow: North East 5 \nblocks away (nearby) (objects in \nbetween: {'grass'}) \\n- Facing grass on \nthe south.\\nYour status:\\n- health: \n9\/9\\n- food: 9\/9\\n- drink: 9\/9\\n- energy: \n9\/9\\nYou have nothing in your \ninventory.\nIt is nighttime\\nState description: \\n-\nEast: immediate (sand); nearby (path, \ngrass); distant (tree, unexplored_area);\n\\n- North: immediate (zombie); nearby \n(path, sand, grass); distant (tree, \nunexplored_area); \\n…… - zombie: \nNorth 1 blocks away (immediate) \n(objects in between: None) \\n- table: \nSouth East 9 blocks away (nearby) \n(objects in between: {'path', 'sand', \n'water', 'grass'}) \\n- Facing zombie on \nthe north.\\nYour status:\\n- health: \n2\/9\\n- food: 8\/9\\n- drink: 8\/9\\n- energy: \n3\/9\\nYour inventory:\\n- wood: 1\\n-\nstone: 9\\n- coal: 4\\n- wood_pickaxe: \n1\\n- stone_pickaxe: 1\\n- wood_sword: \n1\\n- stone_sword: 1\\n\nIt is nighttime\\nState description: \\n-\nEast: immediate (sand); nearby (path, \ngrass); distant (tree, unexplored_area);\n\\n- North: immediate (zombie); nearby \n(path, sand, grass); distant (tree, \nunexplored_area); \\n …… - zombie: \nNorth 1 blocks away (immediate) \n(objects in between: None) \\n- table: \nSouth East 9 blocks away (nearby) \n(objects in between: {'path', 'sand', \n'water', 'grass'}) \\n- Facing zombie on \nthe north.\\nYour status:\\n- health: \n2\/9\\n- food: 8\/9\\n- drink: 8\/9\\n- energy: \n3\/9\\nYour inventory:\\n- wood: 1\\n-\nstone: 9\\n- coal: 4\\n- wood_pickaxe: \n1\\n- stone_pickaxe: 1\\n- wood_sword: \n1\\n- stone_sword: 1\\n\nVerbalize\nDiscover\nVerify\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\nskeleton turn into path after defeat_skeleton, \nincrease bone by 1 and arrow by 1.\n……\nzombie turn into grass after defeat_skeleton, \nincrease rotten_fresh by 1 and bone by 1.\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\nskeleton turn into path after defeat_skeleton, \nincrease bone by 1 and arrow by 1.\n……\nzombie turn into grass after defeat_skeleton, \nincrease rotten_fresh by 1 and bone by 1.\n(verbalized\ndemonstration)\n(discovered game dynamics)\n(verified game dynamics)\n(demonstration)\n𝑜𝑡\n𝑜𝑡+1\n𝑎𝑡\nCrafter Env\nLLM Agent\n+\nEvolve\nUse collected stones to build a \nprotective barrier around the safe \nlocation.\n……\n(situational strategies)\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\n……\n(verified game dynamics)\nFigure 2: Overall pipeline of DiVE. Left: Learning basic game dynamics from offline demonstrations (Section 3.1).\nWe want to highlight the incorrect game dynamics being identified by the Verifier (labeled by ×), they are evidence\nof the LLMs hallucinate false facts perhaps because of memorizing Minecraft data. Right: Learning situational\nstrategies from online interactions (Section 3.2). For simplicity, we omit the verbalization process in the right figure.\nthe language space as ˜ot using a Verbalizer, re-\nsulting in trajectories represented by ˜ot, at, and\nrt at each timestep t. Subsequently, the Discov-\nerer extracts a set of world dynamic candidates,\n˜\nW = { ˜W+, ˜W−}, from human demonstrations H,\nwhere ˜W+ and ˜W−represent the correct and inac-\ncurate world dynamic sets, respectively.\nEmpirically, we find that the inclusion of ˜W−in\n˜\nW is often unavoidable, either due to LLM’s diffi-\nculties in extracting meaningful knowledge from\ntrajectory data or its tendency to hallucinate. To\naddress this, we employ the Verifier to filter out\npotentially invalid and conflicting world dynamic\ncandidates from ˜\nW, leaving only the valid dynam-\nics W. Lastly, we use the Evolver, designed to\nderive advanced dynamics I tailored to the ver-\nbalized observation ˜ot based on the filtered world\ndynamics W.\nThe final decision-making process on primitive\nactions at ∈A is hierarchically decomposed as\nplanning tasks on sub-goals SG, sub-tasks ST,\nand actions A. The planning procedure is further\nguided by both W and I. In cases where W ̸=\n∅, Recall, Precision and Depth are guaranteed to\nincrease as formulated below:\nRecall :\n|K+|\n|Ktarget|\nDiscoverer\n======⇒\nH\n|K+| + |W|\n|Ktarget|\nPrecision :\n|K+|\n|Krelevant|\nVerifier\n====⇒\n˜\nW,H\n|K+| + |W|\n|Krelevant| + |W|\nDepth : ∅Discoverer\n======⇒\nVerifier\nW Evolver\n====⇒\nW,H\nI ∪W\nThe DiVE framework can be divided into two\nstages: an offline dynamics learning stage and an\nonline strategy learning stage.\n3.1\nOffline Dynamics Learning\nThe offline dynamics learning procedure aims to\nbridge the knowledge gap between LLMs’ under-\nstanding and the basic dynamics of downstream\ndomains by learning the world dynamics W as a\nprior for the decision-making process. Instead of\nrelying on human-authored game manuals or hand-\nbooks to extract world dynamics, as in (Wu et al.,\n2024b,a), which are not only difficult to obtain in\nmany real-world scenarios but also often lack criti-\ncal details (as demonstrated in Table 6), we propose\nlearning world dynamics W directly from experi-\nences H, which are more accessible and provide\nricher information.\nHierarchical Curriculum Learning\nGiven the\nvarying complexities of learning the dynamics of\ndifferent elements in downstream domains, we\nadopt a curriculum learning approach (Bengio et al.,\n2009). Our method follows a sequential learning\nstrategy that progresses from simpler to more com-\nplex dynamics, thereby enabling more effective\nlearning. Specifically, we propose a method for\nlearning the dynamics of each element within the\ntask decomposition hierarchy, denoted as TD =\n{A ∪O, ST, SG}, where O represents the set of\nobjects in downstream domains.\nOur approach begins with elements of lower\nabstraction, such as actions a ∈A and objects\no ∈O, and gradually progresses to higher-level\nelements, such as sub-tasks sti ∈ST. The sub-\ntasks ST are represented as nodes in the achieve-\nment graph G = (V, E) within the downstream\ndomains, i.e., ST = V . Finally, we transition to\nthe subgoals sgi ∈SG. The subgoal sequence\nSG = [sg1, sg2, . . .] is an ordered list used to un-\nlock achievements in the achievement graph G,\nwhere SG = TopologicalSort(G), and each sgi\ncorresponds to a vertex in V .\nWe leverage the Discoverer to extract this order\nfrom human demonstrations H. Achieving a sub-\ngoal sgi may involve completing several sub-tasks\nmultiple times. This approach ensures a logical pro-\ngression through tasks, thereby enabling a deeper\nunderstanding and integration of downstream do-\nmain dynamics.\nDiscoverer\nThe Discoverer is designed to iden-\ntify dynamic candidates ˜\nW related to elements\nwithin the task decomposition hierarchy TD. A\nsingle dynamics discovery step for an element\nE ∈TD involves the following three main steps:\n1. Construction of the Semantic Experience\nBank: For each element E, we construct a se-\nmantic experience bank BE using demonstra-\ntions H. This bank stores experiences that are\ntransformed from H into a suitable granular-\nity for analyzing dynamics related to E. The\ntransformation process involves chunking and\nsummarizing the verbalized demonstrations to\ncapture essential semantic details.\n2. Sampling of Relevant Experiences: For each\nattribute of an instance e ∈E, a subset of expe-\nriences BE\ne that are relevant to the instance e is\nsampled from BE.\n3. Identification of Dynamic Candidates: A dy-\nnamic candidate ˜w is identified from the subset\nBE\ne by recognizing patterns that are consistent\nacross all experiences within BE\ne .\nThe action-level semantic experience bank, de-\nnoted as BA, stores transition tuples derived from\nverbal demonstrations and is represented as: BA =\n{{˜ot, at, ˜ot+1}i}|BA|\ni=1 . Similarly, the object-level\nsemantic experience bank, denoted as BO, gath-\ners individual observations contain an specific\nobject and is represented as: BO = {˜oi}|BO|\ni=1 .\nThe sub-task-level semantic experience bank, de-\nnoted as BST , aggregates trajectory segments rep-\nresenting the completion of sub-tasks and is for-\nmatted as: BST = {{˜ot, . . . , atst, ˜otst+1}i}|BST |\ni=1\n,\nwhere tst denotes the timestep at which a sub-\ntask st ∈ST is completed. For sub-goals, the\nsub-goal-level experience bank, denoted as BSG,\nrecords sequences of sub-tasks that culminate in\nthe completion of sub-goals, expressed as: BSG =\n{{stt, . . . , sttsg}i}|BSG|\ni=1\n, where tsg is the timestep\nat which the sub-goal sg is achieved.\nFor action-level dynamics learning, the relevant\nexperiences, denoted as BA\na , are compiled by ran-\ndomly sampling transition tuples from BA where\nthe action a has been successfully executed. A sim-\nilar approach is employed for dynamics learning\nof other elements within the task decomposition\nhierarchy TD.\nFor action-level dynamics discovery, we iden-\ntify the prerequisites and outcomes of each action\n(e.g., The action MakeWoodPickaxe requires\n1 wood). For object-level dynamics, we focus on\nco-occurrence relationships between objects and\ntheir temporal patterns. The attribute set for a sub-\ntask generally encompasses the necessary steps for\ncompletion, as well as its prerequisites, outcomes,\nand termination conditions. In contrast, the primary\nattribute of interest for a sub-goal is its correct po-\nsition within the sub-goal sequence SG.\nVerifier\nDynamic discovery processes are vul-\nnerable to noise from various sources, including\nconfounders, hallucinations by the LLMs, and dif-\nficulties in the LLMs’ ability to extract meaning-\nful insights from trajectory data. To address these\nchallenges, we introduce a dynamic verifier de-\nsigned to filter out noisy dynamic candidates from\n˜\nW. For each dynamic candidate w ∈˜\nW, the ver-\nifier begins by sampling a subset of relevant se-\nmantic experiences, denoted as BE\ne , from the cor-\nresponding semantic experience bank BE. Here,\nw represents a dynamic candidate associated with\na specific attribute of the instance e ∈E, where\nE ∈{A ∪O, ST, SG} corresponds to an element\nof the task decomposition hierarchy TD. The veri-\nfication of w is conducted as follows: w is deemed\ninaccurate and filtered out if it does not consistently\nhold across experiences within BE\ne or if it conflicts\nwith any established dynamics. The dynamics that\npass this verification process are classified as veri-\nfied dynamics and are denoted as W.\n3.2\nOnline Strategy Learning\nTo effectively incorporate the learned world dynam-\nics W into the downstream domains, we deploy an\nLLM-based agent defined by π : S × W →P(A).\nHere, S represents the state space, A denotes the\naction space, and P symbolizes the probability dis-\ntribution over the action space. Instead of directly\nmapping the world dynamics W and the current\nstate observation ot to the action at, we tackle the\nchallenge of long-horizon planning by integrating\nan online strategy learning method. This approach\ndecomposes the planning process into three distinct\nstages: sub-goal planning, sub-task planning, and\naction planning.\nSub-goal Planning\nGiven that the sub-goal se-\nquence SG = [sg1, sg2, . . .] is derived from hu-\nman demonstrations H and treated as a fixed se-\nquence, we utilize a straightforward heuristic for\nsub-goal planning. When a sub-goal is completed,\nthe current sub-goal is updated to the first uncom-\npleted sub-goal in SG.\nSub-task Planning\nFor a given current sub-goal\nsgi, we have developed an LLM-based sub-task\nplanner. This planner evaluates and ranks all sub-\ntasks st ∈ST based on the learned world dynam-\nics W, the verbalized current observation ˜ot, and\nthe most recently executed sub-task stt−1. The\nhighest-ranked sub-task is then designated as the\ncurrent sub-task stt. To ensure accurate execution,\nthe completion of a sub-task st is contingent upon\nsatisfying its specific termination condition. This\ncondition is verified by querying an LLM using the\ncurrent verbalized observation, the observation at\nthe time the sub-task began, and the termination\nconditions of the current sub-task.\nLearning Strategies\nIn addition to learning the\nfundamental rules of the downstream domains, we\nalso focus on developing advanced game-playing\nstrategies based on these dynamics. Unlike world\ndynamics learning, the strategy space is often too\nexpansive for exhaustive exploration. To address\nthis challenge, we propose evolving the dynam-\nics into strategies, denoted as I, using an online\nlearning approach.\nThis method reduces the search space by condi-\ntioning not only on the dynamics W, but also on the\nverbalized current observation ˜ot and the sub-task\nsti. This targeted approach enables the generation\nof strategies that are more contextually grounded\nand responsive to current game scenarios compared\nto those developed through offline methods. To sup-\nport this process, we have designed an LLM-based\nEvolver that generates strategy candidates ˜I by ap-\nplying deductive reasoning to the learned dynamics\nW. Specifically, the Evolver derives strategy can-\ndidates using rules of inference, such as modus\nponens. These strategy candidates, denoted as ˜I,\nare then evaluated for validity and ranked based on\ntheir utility by an LLM-based critic. Finally, the\nvalid and useful candidates are incorporated into\nthe situational strategy set I.\nAction Planning\nThe final action selection pro-\ncess is executed in two main steps:\n1. Invalid Action Masking: This step involves fil-\ntering out actions that are infeasible under the\ncurrent situation, based on the verified dynamics\nW and current verbalized observation ˜ot.\n2. Action Selection: From the set of valid actions,\na specific primitive action a is chosen based on\nmultiple factors: the current sub-task sti, the\nverbalized current observation ˜ot, the world dy-\nnamics W, a windowed history of previously\nplanned actions and observations, and the de-\nrived strategies I.\n4\nExperiment Setup\nTo demonstrate the effectiveness of DiVE in bridg-\ning the knowledge gap, we evaluate its performance\nwithin the Crafter and MiniHack environments. For\na fair comparison, all LLM-based agents, includ-\ning ours, utilize the GPT-4o model and the same\nenvironment seed in the Crafter setting. Further\ndetails on the setups can be found in Appendix A.1\nand B.1.\n4.1\nCrafter\nCrafter (Hafner, 2021) is an open-world survival\ngame set on 64 × 64 grid-based maps, featuring a\ndiverse array of materials such as tree, stone, and\ncoal, as well as entities including cow, zombie,\nand skeleton semi-randomly spawn on the maps.\nThe games include an achievement graph with 22\nunique achievements across 7 levels. The agent\nperceives its surroundings through a local 7 × 9\nobservation window and maintains awareness of its\nstatus within the game environment.\nThe text description generated by the verbalizer\nincludes: the nearest object of each type within\nthe accumulated observations, the objects situated\nbetween these nearest objects, the objects in each\ndirection, as well as the agent’s current inventory\nand status. An example of the verbalization process\nis provided in Appendix B.5.\nThe agent is evaluated using two primary met-\nrics: reward and score. Agents receive a +1 reward\nfor each new achievement unlocked (e.g., make\nwood pickaxe, place furnace) and a ±0.1 re-\nward for every health point gained or lost. The\nscore is calculated by aggregating the success rates\nsi for each achievement and is formulated as:\nS .= exp\n \n1\nN\nN\nX\ni=1\nln (1 + si)\n!\n−1.\nWe compare DiVE against the following baselines:\n• LLM-based approaches:\nSPRING (Wu\net al., 2024b), ELLM (Du et al., 2023), and\nChain-of-Thought (CoT)(Wei et al., 2022).\n• Reinforcement Learning (RL) approaches:\nDreamerV3(Hafner et al., 2023), PPO (Schul-\nman et al., 2017), and AD (Moon et al., 2024).\n• Human Players: Expert performance on the\nCrafter environment.\n4.2\nMiniHack\nMiniHack (Samvelyan et al., 2021) is a grid-based\nenvironment built on the video game NetHack (Küt-\ntler et al., 2020). Unlike the Crafter environment,\nit supports the creation of tasks that target specific\nagent capabilities. In our work, we focus on the\nSkill Acquisition Tasks subset, which evaluates\nthe agent’s ability to leverage the rich diversity of\nNetHack’s objects, monsters, and dungeon features,\nas well as their interactions. These tasks introduce\ncomplex world dynamics, where actions are fac-\ntorized autoregressively and require executing a\nsequence of follow-up actions for the initial action\nto produce the desired effect.\nWe use the Lava Crossing, Wand of Death, and\nQuest tasks as testbeds to evaluate DiVE. We pro-\nvide detailed descriptions of these task in Ap-\npendix A.2. The agent is rewarded for unlocking\nachievements within each task. To facilitate in-\nteraction with LLMs, we use the NLE language\nwrapper (Goodger et al., 2023) to verbalize both\nobservations and actions from the environment.\nWe compare DiVE against SSO (Nottingham\net al., 2024) and Reflexion (Shinn et al., 2024), both\nof which require prior training on the tasks. Follow-\ning the official guidelines, we run each method for\n30 iterations, evaluating performance after every\n10 iterations by attempting the task 10 times using\na fixed set of skills or reflections. The final perfor-\nmance is determined based on the results from the\nlast 10 evaluation attempts.\nMethod\nScore\nReward\nHuman Experts\n50.5 ± 6.8%\n14.3 ± 2.3\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nSPRING∗\n8.2 ± 2.3%\n6.9 ± 1.8\nCoT\n1.3 ± 0.3%\n2.5 ± 0.5\nAD\n21.79 ± 1.4%\n12.6 ± 0.3\nELLM\nN\/A\n6.0 ± 0.4\nDreamerV3\n14.5 ± 1.6%\n11.7 ± 1.9\nPPO\n4.6 ± 0.3%\n4.2 ± 1.2\nRandom\n1.6 ± 0.0%\n2.1 ± 1.3\nTable 1: Performance comparison of DiVE against base-\nline models in the Crafter environment. Methods with ∗\nindicate that they were obtained using the official code\nimplementation, executed with the same five random\nseeds and model configurations as used for DiVE.\n5\nExperimental Results\nWe evaluate the performance of DiVE in the Crafter\nand MiniHack environments. In Section 5.1, we\npresent the overall results to demonstrate DiVE’s\neffectiveness in bridging the knowledge gap in\nthese tasks. Section 5.2 provides a detailed analy-\nsis of the contributions of individual components\nthrough controlled experiments.\nThen, in Sec-\ntion 5.3, we evaluate the effectiveness of the dy-\nnamics learned by DiVE in the Crafter environ-\nment. Finally, in Section 5.4, we further analyze\nthe learned dynamics in both the Crafter environ-\nment and a modified MiniHack setting with altered\ndynamics.\n5.1\nDiVE’s Performance\nTable 1 and 2 demonstrate that DiVE surpasses\nall other baselines in the Crafter and MiniHack\nenvironments. In the Crafter environment, DiVE\nexceeds the previous state-of-the-art (SOTA) LLM-\nbased method, SPRING, by a substantial margin,\nachieving a 337.8% relative improvement in score\nand a 110.1% enhancement in reward. Addition-\nally, DiVE also surpasses the prior SOTA RL-based\napproach, DreamerV3, with a 21.4% absolute im-\nprovement in score and a 2.8 absolute increase in\nreward. Notably, DiVE achieves rewards compara-\nble to human players using 10 demonstrations.\nIn the MiniHack environment, with only a single\ndemonstration, DiVE matches the performance of\nSSO and Reflexion (both of which require 30 iter-\nations of training) on the Lava Crossing task, and\noutperforms both baselines on the Wand of Death\nand Quest tasks. Specifically, DiVE achieves a 68%\nimprovement over Reflexion on the Wand of Death\ntask and a 30% and 36% improvement over SSO\nand Reflexion, respectively, on the Quest task.\nMethod\nLava Crossing\nWand of Death\nQuest\nReflexion\n0.8±0.0\n0.32±0.15\n0.87±0.43\nSSO∗\n0.8±0.0\n0.52±0.31\n0.91±0.29\nDiVE\n0.8±0.0\n0.54±0.34\n1.18±0.38\nTable 2: Comparison of DiVE performance against\nbaseline models in the MiniHack environment. Results\nmarked with ∗were obtained using the official code\nimplementation and evaluated over 10 runs.\nMethods\nScore\nReward\nComponent analysis\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nw\/o E\n21.1 ± 9.7%\n11.3 ± 4.3\nw\/o V\n9.8 ± 1.0%\n10.1 ± 0.7\nw\/o V&E\n11.5 ± 4.9%\n8.3 ± 3.8\nw\/o D&V&E\n0.9 ± 0.1%\n2.5 ± 1.3\nCoT\n1.3 ± 0.3%\n2.5 ± 0.5\nCoT + D&V\n3.6 ± 0.9%\n3.9 ± 2.3\nDynamics from distinct sources\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nw\/o D&V+S†\n15.7 ± 5.3%\n8.9 ± 5.1\nw\/o D&V&E+S†\n12.1 ± 4.6%\n8.7 ± 3.0\nw\/o D&V&E+H†\n34.2 ± 2.8%\n14.5 ± 0.9\nTable 3: Impact of different components on perfor-\nmance: Crafter. D, V, and E represent Discover, Verifier,\nand Evolver, respectively; S† refers to dynamics derived\nfrom the game manual, H† refers to human-annotated\ndynamics.\n5.2\nContribution of Individual Components\nWe conduct a series of ablation studies to clar-\nify the contribution of each individual element to\nDiVE’s overall performance. We report the results\non Crafter in the first section of Table 3, and the\nresults on MiniHack in Table 4.\nCrafter\nThe significant performance gap be-\ntween DiVE and its variant without the Evolver\ncomponent empirically demonstrates the Evolver’s\neffectiveness in developing gameplay strategies\nbased on world dynamics W, thereby enhancing\nthe agent’s overall proficiency in this environment.\nSimilarly, the performance decline observed in the\nvariant without the Verifier underscores the impor-\ntance of formulating strategies I based on accurate\nworld dynamics W. Moreover, the further perfor-\nmance drop in the version lacking both the Verifier\nand Evolver components highlights their comple-\nmentary roles—the Verifier ensures precision in\ncapturing dynamics, while the Evolver focuses on\nstrategy development.\nThe performance of DiVE without the Discov-\nerer, Verifier, and Evolver components reverts to\nthe CoT baseline, indicating that simply decom-\nposing the task according to the hierarchy H with-\nout integrating domain knowledge Ktarget provides\nComponents\nLava Crossing\nWand of Death\nQuest\nDiVE\n0.80±0.00\n0.54±0.34\n1.18±0.38\nw\/o E\n0.72±0.24\n0.52±0.30\n1.16±0.48\nw\/o V\n0.69±0.25\n0.52±0.38\n1.18±0.38\nw\/o V&E\n0.72±0.24\n0.40±0.40\n1.11±0.48\nw\/o D&V&E\n0.72±0.17\n0.20±0.00\n0.41±0.29\nTable 4: Impact of different components on perfor-\nmance: MiniHack. D, V, and E represent Discover,\nVerifier, and Evolver, respectively.\nno performance benefit. The substantial gap be-\ntween CoT + D&V and DiVE w\/o E further demon-\nstrates that an LLM-based agent struggles with\nlong-horizon planning tasks in the absence of task\ndecomposition, underscoring the importance of the\ndecomposition hierarchy H.\nMiniHack\nSince MiniHack is built on the popu-\nlar video game NetHack (Goodger et al., 2023),\nLLMs already possess a certain level of under-\nstanding of the environment. For example, DiVE\nwithout the Discover, Verifier, and Evolver com-\nponents (CoT baseline) can successfully solve the\nLava Crossing task. However, as the complexity\nof the tasks increases, using LLMs alone is insuf-\nficient for solving more challenging scenarios like\nthe Wand of Death and Quest tasks, highlighting\na potential knowledge gap for these more intricate\nproblems.\nThe primary performance gains of DiVE in the\nMiniHack environment can be attributed to the\nDiscover component, which accurately identifies\nthe underlying environment dynamics required for\neach task. Due to the embedded knowledge of\nLLMs about the MiniHack environment, they can\nidentify these dynamics effectively, as shown by\nthe minimal performance drop when DiVE is used\nwithout the Verifier. However, in the modified Mini-\nHack setting described in Section 5.4, we highlight\nthe critical role of the Verifier in maintaining relia-\nbility. Additionally, since the tasks have a relatively\nshort horizon, the dynamics uncovered by the Dis-\ncover component are sufficient to complete them\nsuccessfully.\n5.3\nEvaluation of Learned Dynamics\nWe investigate the performance of DiVE leveraging\nworld dynamics derived from different sources in\nCrafter. As shown in the second section of Table 3,\nDiVE significantly outperforms variants that uti-\nlize the dynamics S† from the game manual (Wu\net al., 2024b). This performance improvement in-\ndicates that the learned dynamics W are more ad-\nFigure 3: Recall of learned dynamics over discovery\nsteps, presented with mean and standard deviation, in\nthe Crafter environment.\nvantageous than S†, likely because S† lacks certain\nbeneficial details that are captured in W. The per-\nformance gap between methods using S† with and\nwithout the Evolver further highlights the impor-\ntance of strategy evolution, whose effectiveness is\nclosely tied to the quality of the underlying world\ndynamics.\nIn addition to dynamics learned from human\ndemonstrations and game manual, we have ex-\nplored a third source: human-annotated dynamics.\nThe results show that DiVE performs comparably\nto the variants using human-annotated dynamics,\ndemonstrating the robustness and effectiveness of\nDiVE’s approach to dynamic learning.\nAs previously mentioned, it is difficult to quan-\ntify the desired properties because we cannot pre-\ncisely measure the domain-relevant information\nKrelevant in LLMs or the exact amount of domain\nknowledge Ktarget required. However, by using\nhuman-annotated dynamics H† as a reference\nbenchmark for Ktarget, we can estimate the preci-\nsion and recall of the learned dynamics W, en-\nabling us to effectively assess the progress of LLM-\nbased dynamic learning. Specifically, we define re-\ncall as R = |W∩H†|\n|H†|\nand precision as P = |W∩H†|\n|W|\n.\nAs illustrated in Figure 3, both the discovered\ndynamics ˜\nW and the verified dynamics W exhibit\nan increase in recall as the number of discovery\nsteps progresses, indicating that the richness of the\nlearned dynamics improves over time. Moreover,\nthe narrowing gap in recall between ˜\nW and W sug-\ngests that the Verifier effectively filters out ’noisy’\ndynamic candidates while preserving those that\ngeneralize across different trajectory segments.\nTo evaluate whether the Verifier preserves cor-\nrect world dynamic candidates ˜\nW+ while filtering\nout unreliable ones ˜\nW−, we analyze the precision\nCorrectness\nOutcome\nCorrect (✓)\nnone\nConfounder (✗)\n1 health\nIn-domain Hallucination (✗)\n1 wood\nOut-domain Hallucination (✗)\n1 bone\nTable 5: The dynamics underlying the outcome of de-\nfeating zombie in the Crafter environment.\nof both the discovered dynamics ˜\nW and the verified\ndynamics W. As shown in Figure 4, the precision\nof the verified dynamics consistently and signif-\nicantly exceeds that of the discovered dynamics,\ndemonstrating the Verifier’s effectiveness in identi-\nfying and eliminating inaccurate candidates. This\nconfirms the Verifier’s role in enhancing the relia-\nbility of the dynamics used for decision-making.\n5.4\nAnalysis of Learned Dynamics\nThe correctness of the learned and verified dynam-\nics is classified as either correct or erroneous, with\nerrors stemming from confounders, in-domain hal-\nlucinations, or out-of-domain hallucinations. As\nshown in Table 5, an example of a confounder-\nrelated mistake occurs in Crafter when a simulta-\nneous increase in health points is incorrectly at-\ntributed to the act of defeating a zombie. In this\nscenario, the Discoverer misclassifies the health\nincrease as a direct result of defeating the zombie.\nIn the case of in-domain hallucinations, the Dis-\ncoverer incorrectly associates an increase in wood\nwith defeating the zombie, even though it is not\npossible for wood to increase during this event,\ndespite its presence in the observation. Lastly, out-\nof-domain hallucinations involve the discovery of\ndynamics that reference nonexistent objects in the\nobservation or elements not present in the Crafter\nenvironment.\nCompared to the dynamics from the game man-\nual, as shown in Table 6, we found that DiVE’s\ndynamics are not only more precise but also more\ndetailed. For instance, while SPRING only iden-\ntified that placing a stone requires stones, DiVE\ndetermined that it specifically requires exactly one\nstone and the precise facing condition needed for\nsuccessful placement. Moreover, using this infor-\nmation, the Evolver can infer advanced dynamics\nfor placing a stone, such as its potential to serve as a\nbarrier between the agent and dangerous creatures.\nIn a customized MiniHack Lava Crossing setting,\nwe modified the task such that it requires the agent\nSources\nDynamics\nManual\nPlace stone requires stones\nDiscoverer Place stone requires 1 stone and\nfaces paths, grass, sand, water,\nand lava\nEvolver\nPlace stone to block zombies\nand skeletons, preventing them\nfrom reaching the player\nTable 6: Comparing SPRING and DiVE on place stone’s\ndynamics in the Crafter environment.\nto use the Wand of Death to freeze the lava for\ncrossing, even though the wand’s original purpose\nis to zap monsters. Based on the demonstration,\nthe Discoverer mistakenly identifies the precon-\ndition for using the wand as The player must\nbe adjacent to a wall or obstacle that\ncan be destroyed or altered by the wand\nof death, with the outcome being The wand of\ndeath was used, altering the environment\nby removing walls and revealing a dark\narea to the east, southeast, and south.\nThis unreliable dynamic is successfully identified\nand filtered out by the Verifier, highlighting its cru-\ncial role in maintaining the reliability of learned\ndynamics.\n6\nRelated Work\nLanguage Models\nLanguage models (LLMs) are\ntrained autoregressively in a left-to-right sequence,\npredicting each token based on its preceding con-\ntext from an internet-scale corpus. Through this\ntraining, LLMs develop a comprehensive under-\nstanding of both language and the world it repre-\nsents (Achiam et al., 2023; Touvron et al., 2023;\nDubey et al., 2024; The, 2024), enabling them\nto perform competently across a wide range of\ntasks (Yao et al., 2024; Shinn et al., 2024).\nEmbodied Agent\nBuilding an embodied agent\nusing LLMs is challenging because LLMs lack em-\nbodied experience (Valmeekam et al., 2022; Wang\net al., 2024) in downstream environments (Weir\net al., 2022; Shridhar et al., 2020; Côté et al.,\n2019). However, LLMs can still provide founda-\ntional world knowledge that serves as a prior for the\nagent (Shi et al., 2024; Colas et al., 2023; Zhong\net al., 2024; Fu et al., 2024a). A notable distinction\nof our work is that we do not assume LLMs have\nthe necessary knowledge to solve tasks in specific\ndomains.\nFigure 4: Precision of learned dynamics before and after\nverification in the Crafter environment.\nDiscover Dynamics\nLLMs can discover knowl-\nedge by inducing and deducing rules for reasoning\ntasks (Zhu et al., 2023) and by extracting under-\nlying domain knowledge from prior trajectories\nand interactive experiences (Colas et al., 2023;\nMajumder et al., 2023; Fu et al., 2024b; Zhao\net al., 2024). However, the knowledge obtained\nthrough these methods is often unstructured, not\nsuited for addressing long-horizon planning prob-\nlems, and lacks verification for reliability, as it\noverlooks LLMs’ tendency to hallucinate (Zhang\net al., 2023b).\nEvolve\nDynamics\nLLMs\ncan\nrefine\ntheir\ndecision-making process by reflecting on past\ntrajectories (Shinn et al., 2024).\nLeveraging\nthis ability, studies such as (Wang et al., 2023;\nStengel-Eskin et al., 2024; Zhang et al., 2023a;\nNottingham et al., 2024) focus on evolving\nnew and advanced skills from pre-defined ones.\nHowever, these approaches often assume that\nLLMs already possess comprehensive domain\nknowledge and can derive new dynamics based\nsolely on their understanding.\n7\nConclusion\nWe introduce DiVE, a framework that bridges the\nknowledge gap between LLMs and downstream\ndomains. The Discoverer extracts world dynamics,\nwhile the Verifier filters unreliable candidates. In\nan online setting, the Evolver reasons strategies\nthrough interaction. Our experiments demonstrate\nthat DiVE effectively bridges the knowledge gap\nbetween LLMs and downstream domains.\nLimitations\nEvaluating DiVE in embodied environments may\nnot fully capture the diversity and complexity of\nreal-world dynamics that an agent might encounter\nin practice. If the environment’s dynamics change\nacross episodes, the offline-learned dynamics from\nDiVE may struggle to adapt, introducing potential\nbiases in the agent’s understanding and hindering\nthe development of advanced strategies. Moreover,\nacquiring these dynamics from human demonstra-\ntions can be difficult or impractical in certain sce-\nnarios.\nEthical Concerns\nWe do not anticipate any immediate ethical or so-\ncietal impact from our work. This study aims to\nbridge the knowledge gap between LLMs and the\ntarget domain. However, despite our efforts, DiVE\nmay still exhibit hallucinations due to the inherent\ntendency of LLMs to hallucinate.\nAcknowledgements\nThis work is supported by the Canada CIFAR AI\nChair Program and the Canada NSERC Discovery\nGrant (RGPIN-2021-03115).\nReferences\n2024. The claude 3 model family: Opus, sonnet, haiku.\nhttps:\/\/api.semanticscholar.org\/CorpusID:268232499.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th annual international confer-\nence on machine learning, pages 41–48.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nThomas Carta, Clément Romac, Thomas Wolf, Sylvain\nLamprier, Olivier Sigaud, and Pierre-Yves Oudeyer.\n2023. Grounding large language models in interac-\ntive environments with online reinforcement learning.\nIn Proceedings of the 40th International Conference\non Machine Learning, volume 202 of Proceedings\nof Machine Learning Research, pages 3676–3713.\nPMLR.\nCédric\nColas,\nLaetitia\nTeodorescu,\nPierre-Yves\nOudeyer, Xingdi Yuan, and Marc-Alexandre Côté.\n2023. Augmenting autotelic agents with large lan-\nguage models. In Proceedings of The 2nd Conference\non Lifelong Learning Agents, volume 232 of Proceed-\nings of Machine Learning Research, pages 205–226.\nPMLR.\nMarc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben\nKybartas, Tavian Barnes, Emery Fine, James Moore,\nMatthew Hausknecht, Layla El Asri, Mahmoud\nAdada, et al. 2019. Textworld: A learning environ-\nment for text-based games. In Computer Games:\n7th Workshop, CGW 2018, Held in Conjunction with\nthe 27th International Conference on Artificial In-\ntelligence, IJCAI 2018, Stockholm, Sweden, July\n13, 2018, Revised Selected Papers 7, pages 41–75.\nSpringer.\nYuqing Du, Olivia Watkins, Zihan Wang, Cédric Co-\nlas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta,\nand Jacob Andreas. 2023. Guiding pretraining in\nreinforcement learning with large language models.\nIn Proceedings of the 40th International Conference\non Machine Learning, volume 202 of Proceedings\nof Machine Learning Research, pages 8657–8677.\nPMLR.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783.\nNouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and\nSiva Reddy. 2022. On the origin of hallucinations\nin conversational models: Is it the datasets or the\nmodels? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5271–5285, Seattle, United States.\nAssociation for Computational Linguistics.\nHaotian Fu, Pratyusha Sharma, Elias Stengel-Eskin,\nGeorge Konidaris, Nicolas Le Roux, Marc-Alexandre\nCôté, and Xingdi Yuan. 2024a. Language-guided\nskill learning with temporal variational inference.\narXiv preprint arXiv:2402.16354.\nYao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull\nSohn, Lajanugen Logeswaran, Kyunghoon Bae,\nand Honglak Lee. 2024b. Autoguide: Automated\ngeneration and selection of state-aware guidelines\nfor large language model agents.\narXiv preprint\narXiv:2403.08978.\nNikolaj Goodger, Peter Vamplew, Cameron Foale, and\nRichard Dazeley. 2023. A nethack learning envi-\nronment language wrapper for autonomous agents.\nJournal of Open Research Software, 11.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nCésar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, et al. 2023. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644.\nDavid Ha and Jürgen Schmidhuber. 2018. Recurrent\nworld models facilitate policy evolution.\nIn Ad-\nvances in Neural Information Processing Systems 31,\npages 2451–2463. Curran Associates, Inc. https:\n\/\/worldmodels.github.io.\nDanijar Hafner. 2021. Benchmarking the spectrum of\nagent capabilities. In International Conference on\nLearning Representations.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and\nTimothy Lillicrap. 2023.\nMastering diverse do-\nmains through world models.\narXiv preprint\narXiv:2301.04104.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents. In International Conference on Ma-\nchine Learning, pages 9118–9147. PMLR.\nHeinrich Küttler, Nantas Nardelli, Alexander H. Miller,\nRoberta Raileanu, Marco Selvatici, Edward Grefen-\nstette, and Tim Rocktäschel. 2020. The NetHack\nLearning Environment. In Proceedings of the Con-\nference on Neural Information Processing Systems\n(NeurIPS).\nBodhisattwa Prasad Majumder, Bhavana Dalvi Mishra,\nPeter Jansen,\nOyvind Tafjord,\nNiket Tandon,\nLi Zhang, Chris Callison-Burch, and Peter Clark.\n2023. Clin: A continually learning language agent\nfor rapid task adaptation and generalization. arXiv\npreprint arXiv:2310.10134.\nSeungyong Moon, Junyoung Yeom, Bumsoo Park,\nand Hyun Oh Song. 2024. Discovering hierarchi-\ncal achievements in reinforcement learning via con-\ntrastive learning. Advances in Neural Information\nProcessing Systems, 36.\nKolby Nottingham, Bodhisattwa Prasad Majumder, Bha-\nvana Dalvi Mishra, Sameer Singh, Peter Clark, and\nRoy Fox. 2024. Skill set optimization: Reinforc-\ning language model behavior via transferable skills.\narXiv.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in neural in-\nformation processing systems, 35:27730–27744.\nMikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack\nParker-Holder, Minqi Jiang, Eric Hambro, Fabio\nPetroni, Heinrich Kuttler, Edward Grefenstette, and\nTim Rocktäschel. 2021.\nMinihack the planet: A\nsandbox for open-ended reinforcement learning re-\nsearch. In Thirty-fifth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks\nTrack (Round 1).\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nHaochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-\nAlexandre Côté, and Bang Liu. 2024.\nOpex: A\ncomponent-wise analysis of llm-centric agents in\nembodied instruction following.\narXiv preprint\narXiv:2403.03017.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2024. Re-\nflexion: Language agents with verbal reinforcement\nlearning. Advances in Neural Information Process-\ning Systems, 36.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté,\nYonatan Bisk,\nAdam Trischler,\nand Matthew\nHausknecht. 2020. Alfworld: Aligning text and em-\nbodied environments for interactive learning. arXiv\npreprint arXiv:2010.03768.\nElias Stengel-Eskin, Archiki Prasad, and Mohit Bansal.\n2024.\nRegal:\nRefactoring programs to dis-\ncover generalizable abstractions.\narXiv preprint\narXiv:2401.16467.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai,\nAnja Hauth, et al. 2023.\nGemini: a family of\nhighly capable multimodal models. arXiv preprint\narXiv:2312.11805.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can’t plan (a benchmark for llms on plan-\nning and reasoning about change). In NeurIPS 2022\nFoundation Models for Decision Making Workshop.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. 2023. Voyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nRuoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan,\nMarc-Alexandre Côté, Peter Clark, and Peter Jansen.\n2024.\nCan language models serve as text-based\nworld simulators? arXiv preprint arXiv:2406.06485.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837.\nNathaniel Weir, Xingdi Yuan, Marc-Alexandre Côté,\nMatthew Hausknecht, Romain Laroche, Ida Momen-\nnejad, Harm Van Seijen, and Benjamin Van Durme.\n2022.\nOne-shot learning from a demonstration\nwith hierarchical latent language.\narXiv preprint\narXiv:2203.04806.\nYue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria,\nYuanzhi Li, and Tom M Mitchell. 2024a. Read and\nreap the rewards: Learning to play atari with the help\nof instruction manuals. Advances in Neural Informa-\ntion Processing Systems, 36.\nYue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan\nBisk, Russ R Salakhutdinov, Amos Azaria, Tom M\nMitchell, and Yuanzhi Li. 2024b. Spring: Studying\npapers and reasoning to play games. Advances in\nNeural Information Processing Systems, 36.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\n2024. Tree of thoughts: Deliberate problem solving\nwith large language models. Advances in Neural\nInformation Processing Systems, 36.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nJesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xi-\nang Ren, Minsuk Chang, Shao-Hua Sun, and Joseph J\nLim. 2023a. Bootstrap your own skills: Learning to\nsolve new tasks with large language model guidance.\nIn 7th Annual Conference on Robot Learning.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\nBi, Freda Shi, and Shuming Shi. 2023b. Siren’s song\nin the ai ocean: A survey on hallucination in large\nlanguage models. arXiv preprint arXiv:2309.01219.\nAndrew Zhao, Daniel Huang, Quentin Xu, Matthieu\nLin, Yong-Jin Liu, and Gao Huang. 2024. Expel:\nLlm agents are experiential learners. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 38, pages 19632–19642.\nJiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong\nSu, Yonggui Liang, and Shikai Wu. 2024.\nFine-\ntuning large language models for domain-specific ma-\nchine translation. arXiv preprint arXiv:2402.15061.\nVictor Zhong, Dipendra Misra, Xingdi Yuan, and\nMarc-Alexandre Côté. 2024. Policy improvement\nusing language feedback models.\narXiv preprint\narXiv:2402.07876.\nZhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny\nZhou, Jian Tang, Dale Schuurmans, and Hanjun Dai.\n2023. Large language models can learn rules. arXiv\npreprint arXiv:2310.07064.\nAppendix\nA MiniHack\n15\nA.1\nConfiguration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.2\nDes-Files for Each MiniHack Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.3\nPrompts for MiniHack Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nB\nCrafter\n19\nB.1\nConfiguration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB.2\nAn Example of Learned Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB.3\nAn Example of Trajectory\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB.4\nChanges in Dynamics Precision Over Time\n. . . . . . . . . . . . . . . . . . . . . . . .\n21\nB.5\nAn Example of Verbalized Observation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nB.6\nTech Tree in the Crafter Environment\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nB.7\nPrompts for Crafter Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nA\nMiniHack\nThis section offers further details on the MiniHack environment, along with a comprehensive description\nof the prompts utilized by DiVE within this environment.\nA.1\nConfiguration\nFor this environment, we employ the gpt-4o-2024-08-06 model.\nA.2\nDes-Files for Each MiniHack Task\nIn this subsection, we present the Des-Files utilized for the Lava Crossing, Wand of Death, and Quest\ntasks.\nA.2.1\nLava Crossing\nMAZE: \"mylevel\", ' '\nFLAGS:hardfloor\nINIT_MAP: solidfill ,' '\nGEOMETRY:center ,center\nMAP\n-------------\n|.....L.....|\n|.....L.....|\n|.....L.....|\n|.....L.....|\n|.....L.....|\n-------------\nENDMAP\nREGION :(0,0,12,6),lit ,\" ordinary\"\n$left_bank = selection:fillrect (1,1,5,5)\n$right_bank = selection:fillrect (7,1,11,5)\nOBJECT:('=',\" levitation \"),rndcoord($left_bank),blessed\nSTAIR:rndcoord($right_bank),down\nBRANCH :(1,1,5,5) ,(0,0,0,0)\nA.2.2\nWand of Death\nMAZE: \"mylevel\", ' '\nFLAGS:hardfloor\nINIT_MAP: solidfill ,' '\nGEOMETRY:center ,center\nMAP\n|---------------------------|\n|...........................|\n|.....|---------------------|\n|.....|\n|.....|\n|-----|\nENDMAP\nREGION :(1,1,5,5),lit ,\" ordinary\"\nREGION :(6,1,26,1),lit ,\" ordinary\"\nREGION :(26 ,1,27,1),lit ,\" ordinary\"\n$safe_room = selection:fillrect (1,1,5,5)\nOBJECT:('\/',\"death\") ,(1,1),blessed\nMONSTER:('H', \"minotaur \"), (26,1)\nSTAIR :(27 ,1),down\nBRANCH :(1,1,5,5) ,(1,1,2,2)\nA.2.3\nQuest\nMAZE: \"mylevel\", ' '\nFLAGS:hardfloor\nINIT_MAP: solidfill ,' '\nGEOMETRY:center ,center\nMAP\n-------------|\n|.....L......|--\n|-----|\n|.....L........| - - - - - -|.....|\n|.....L.....................|\n|.....L........| - - - - - -|.....|\n|.....L......|--\n|-----|\n-------------|\nENDMAP\nREGION :(0,0,28,6),lit ,\" ordinary\"\n$left_bank = selection:fillrect (1,1,5,5)\n$right_bank = selection:fillrect (7,1,11,5)\n$goal_room = selection:fillrect (25,2,27,4)\nOBJECT:('\/',\"cold\") ,(2,3),blessed\nBRANCH :(1,1,3,3) ,(0,0,0,0)\nMONSTER:random , (25,3)\nSTAIR :(26 ,3),down\nA.3\nPrompts for MiniHack Tasks\nThis section includes the prompts used to solve the MiniHack tasks.\nA.3.1\nOffline Dynamics Learning\nThe prompts for offline learning the dynamics from demonstrations.\nDiscover Action Dynamics\nThe player is playing a MiniHack game and would like you to analyze the game 's\ndynamics. Specifically , they need help identifying the general preconditions and\noutcomes of actions by comparing consecutive observations.\n- In the previous state , the player observed: {previous_state['state_description ']}\nand took the action: {action }.\n- In the current state , the player observed: {state['state_description ']}.\nDefinitions:\n- ** General precondition **: The condition that must be met for the action to succeed\n, regardless of the specific game state.\n- ** General outcome **: The effect of the action , regardless of the specific game\nstate.\nHere is the action -to-text mapping: {state['action_prompt ']}.\nDiscover Object Dynamics\nThe player is playing a MiniHack game and would like you to analyze the game 's\ndynamics. Specifically , they need help identifying the general preconditions and\noutcomes of actions by comparing consecutive observations.\n- In the previous state , the player observed: {previous_state['state_description ']}\nand took the action: {action }.\n- In the current state , the player observed: {state['state_description ']}.\nDefinitions:\n- ** General precondition **: The condition that must be met for the action to succeed\n, regardless of the specific game state.\n- ** General outcome **: The effect of the action , regardless of the specific game\nstate.\nHere is the action -to-text mapping: {state['action_prompt ']}.\nDiscover Subtask Dynamics\nThe player is playing a MiniHack game and would like your help in discovering the\ndetailed steps to complete a subtask , and the preconditions and outcomes of the\nsubtask. The player has provided the following information:\n- In previous states , the player observed: {previous_state_description} and took the\nfollowing actions: {previous_action }.\nDefinition:\n- ** Steps for completing the subtask **: The general and detailed sequence of actions\n, represented in text , that the player must follow to achieve the subtask. These\nsteps should be general and not tied to the specific state of the environment.\n- ** Preconditions **: The general conditions that must be met for the subtask to be\ncompleted.\n- ** Outcomes **: The general effects of completing the subtask.\nHere is the mapping of actions to their text representations: {state['action_prompt\n']}.\nA.3.2\nOnline Dynamics Learning\nThe prompts for online evolving the dynamics from existing dynamics.\nDiscover Subtask Dynamics\nYou are playing the MiniHack game.\nYour current subtask is: {subtask }.\nYour current observation is: {observation }.\nThe primitive dynamics of the game are: {action_dynamics }.\nThe object dynamics of the game are: {object_dynamics }.\nYou are asked to evolve useful strategies that aid in completing the subtask based\non the provided information.\nInstructions for evolving strategies:\n- Do not introduce any new objects that are not part of the primitive dynamics.\n- The evolved dynamics should not contradict the primitive dynamics.\n- If a difficulty cannot be resolved by existing dynamics , evolve new and advanced\ndynamics by combining only existing dynamics using deductive reasoning.\nNow , consider:\n- All the potential difficulties that may arise , based on the primitive dynamics and\ncurrent observation.\n- All the potential dangers that may be encountered , based on the primitive dynamics\nand current observation.\n- The strategies should be general that do not include specific actions , directions\nand dis\nList all the potential difficulties , dangers and the evolved strategies to resolve\nthem.\nA.3.3\nGrounding\nThe prompts for grounding the agent.\nTask Termination\nYou are playing the MiniHack game.\nYour observation is: {description }.\nYour current subtask is: {current_subtask }.\nYour previous history is: {previous_history }.\nThe action and character mapping is: {action_prompt }.\nYou are asked to decide whether the current subtask should been terminated:\n- If the termination conditions are met or the player is in danger or the player is\nin deadlock , the subtask should be terminated.\nOutput yes if the subtask has been completed , and no if it has not been completed.\nTask Termination\nYou are playing the MiniHack game.\nYour observation is: {description }.\nYour current subtask is: {current_subtask }.\nYour previous history is: {previous_history }.\nThe action and character mapping is: {action_prompt }.\nYou are asked to decide whether the current subtask should been terminated:\n- If the termination conditions are met or the player is in danger or the player is\nin deadlock , the subtask should be terminated.\nOutput yes if the subtask has been completed , and no if it has not been completed.\nTask Selection\nYou are playing the MiniHack game.\nYour goal is: {task_description }.\nYour observation is: {description }.\nYour previously completed subtask is: {previous_subtask }.\nAll the subtasks are: {subtask_dynamics }.\nAll the environment dynamics are: {action_dynamics }.\nAll the object dynamics are: {object_dynamics }.\nYou are asked to select the next best subtask for completing the goal , based on all\nthe provided information.\nAction Selection\nYou are playing the MiniHack game.\nYour current subtask is: {subtask }.\nYour current observation is: {description }.\nYou also observe the feedback from the environment for the previous action: {\nobservation}, which you need to consider for the next action.\nThe action and character mapping is: {action_prompt }.\nHere are the strategies for the subtask: {evolved_strategy }.\nHere are the primitive dynamics of the game: {action_dynamics} and {object_dynamics\n}.\nYour past action and thought history: {history}\nNow , you are asked to:\n- List the potential difficulties and dangers that may arise based on the ** current\nobservation ** and primitive dynamics for completing the subtask.\n- Output the thoughts on the future actions how to resolve the difficulties and\ndangers for completing the subtask.\n- Select the best action for the next step based on the thoughts.\n- Do not copy the previous thoughts.\nIn the game , the distance is only measured as very far > far > near\nB\nCrafter\nThis section provides a detailed overview of the Crafter environment and a comprehensive description of\nthe prompts used by DiVE.\nB.1\nConfiguration\nFor this setup, we utilize the gpt-4o-2024-05-13 model in the Crafter environment with seeds 1, 2, 3, 4,\nand 5.\nB.2\nAn Example of Learned Dynamics\ngrass\ngrass can be found near ['tree ', 'water ', 'path '], but it is not associated with\n['diamond ', 'coal ', 'iron ']\nYou can walk directly through grass.\ngrass can only be used for: ['collect_sapling ', 'eat_plant ']\n,\ncoal\ncoal can be found near ['stone ', 'iron ', 'diamond '], but it is not associated\nwith ['grass ', 'cow ', 'skeleton ']\nYou cannot walk directly through coal.\ncoal turn into path after collect_coal\ncoal can only be used for: ['make_iron_pickaxe ', 'make_iron_sword ', '\ncollect_coal ']\ncoal can be collected by ['wood_pickaxe ']\n,\nB.3\nAn Example of Trajectory\nsubgoal\nCollect 4 units of wood and maintain a\nhealthy level of health , food , drink ,\nand energy.\nsubtask\ncollect_wood \": {\n\"General Plan\": {\n\"step_1 \": \"Locate a tree , ensuring 'tree ' is within immediate distance.\",\n\"step_2 \": \"Move closer to the tree if it 's not within immediate distance.\nThis may involve navigating around obstacles such as grass , stone , or\npaths.\",\n\"step_3 \": \"Face the tree to meet the precondition of the action.\",\n\"step_4 \": \"Execute the 'collect_wood ' action to gather wood from the tree.\"\n},\n\"termination_condition \": \"The inventory 's wood amount increases by 1, or if\nhealth , food , drink , or energy levels drop critically low , or a threat (such\nas skeletons or zombies) is detected nearby .\"\n}\nstate description\nIt is daytimeState description:\n- East: immediate (grass); nearby (unexplored_area);\n- North: immediate (grass); nearby (unexplored_area);\n- North East: immediate (grass); nearby (unexplored_area);\n- North West: immediate (grass); nearby (unexplored_area);\n- South: immediate (grass); nearby (unexplored_area);\n- South East: immediate (grass); nearby (cow , unexplored_area);\n- South West: immediate (grass); nearby (unexplored_area);\n- West: immediate (grass); nearby (unexplored_area);\nClosest:\n- grass: North 1 blocks away (immediate) (objects in between: None)\n- cow: South East 5 blocks away (nearby) (objects in between: {'grass '})\n- Facing grass on the south.\nYour status:\n- health: 9\/9\n- food: 9\/9\n- drink: 9\/9\n- energy: 9\/9\nYou have nothing in your inventory.\nevolve dynamics\nExpand the exploration area to discover new surroundings that could potentially\ninclude a tree.\naction\nmove_north\nB.4\nChanges in Dynamics Precision Over Time\nFigure 5: Precision of verified dynamics over verified steps\nAs shown in Figure 5, the precision of the verified dynamics W increases significantly during the initial\nverification step. However, as the process continues, further improvements in precision are minimal,\nindicating that the first step successfully filters out most of the unreliable dynamics.\nB.5\nAn Example of Verbalized Observation\nFigure 6: The observation within the Crafter environment\nVerbalized Observation\nIt is daytime\nState description:\n- East: immediate (grass); nearby (unexplored_area);\n- North: immediate (grass); nearby (unexplored_area);\n- North East: immediate (grass); nearby (unexplored_area);\n- North West: immediate (grass); nearby (tree , unexplored_area);\n- South: immediate (grass); nearby (unexplored_area);\n- South East: immediate (grass); nearby (unexplored_area , tree);\n- South West: immediate (grass); nearby (tree , unexplored_area);\n- West: immediate (grass); nearby (tree , unexplored_area);\nClosest:\n- grass: North 1 blocks away (immediate) (objects in between: None)\n- tree: West 4 blocks away (nearby) (objects in between: {'grass '})\n- Facing grass on the south.\nYour status:\n- health: 9\/9\n- food: 9\/9\n- drink: 9\/9\n- energy: 9\/9\nYou have nothing in your inventory.\nB.6\nTech Tree in the Crafter Environment\nCollect Iron\nCollect Drink\nPlace Table\nMake Wood Pickaxe\nDefeat Skeleton\nDefeat Zombie\nCollect Wood\nCollect Stone\nCollect Coal\nMake Stone Pickaxe\nMake Iron Pickaxe\nCollect Diamond\nPlace Furnace\nMake Wood Sword\nEat Cow\nPlace Stone\nMake Iron Sword\nMake Stone Sword\nPlace Plant\nCollect Sapling\nEat Plant\nWake Up\nFigure 7: The tech tree within the Crafter environment\nB.7\nPrompts for Crafter Environment\nThis section includes the prompts used in the Crafter environment.\nB.7.1\nOnline Dynamics Learning\nGiven the following details:\n- Primitive dynamics: {env_dynamics}\n- Current subtask: {transition['subtask ']}\n- Current observation: {transition['state_description ']}\nYou are asked to identify the difficulties in completing the current subtask and\nprovide 3 primitive or evolved dynamics to solve each of these difficulties.\nInstructions for identifying difficulties:\n- List the objects required to complete the subtask , specify their locations , and\nexplain where to find them if they are not in the current observation.\n- Outline all possible obstacles that may be encountered along the way.\nInstructions for evolving advanced dynamics:\n- Do not introduce any new objects that are not part of the primitive dynamics.\n- The evolved dynamics should not contradict the primitive dynamics.\n- If a difficulty cannot be resolved by existing dynamics , evolve new and advanced\ndynamics by combining only existing dynamics using deductive reasoning.\nInstructions for providing deductive reasoning steps:\n- For each evolved dynamics , provide the used primitive dynamics.\n- For the deductive reasoning steps , provide the steps to combine the primitive\ndynamics to evolve the advanced dynamics and the rule of inference used (Modus\nPonens , Modus Tollens , ......)\nLast , for each situation and dynamics should be general and do not contain details\nabout specific locations about the objects.\nOutput in the following format :{ output_format} and leaves 'None ' for\ndeductive_reasoning_steps if the dynamics are primitive.\nGiven the following details:\n- Primitive dynamics: {env_dynamics}\n- Evolved dynamics: {transition['reformatted_dynamics ']}\n- Current subtask: {transition['subtask ']}\nYou are asked to examine the validity of the evolved dynamics and provide feedback.\nInstructions for examining the validity of the evolved dynamics:\n- The evolved dynamics should only be a combination of existing dynamics using\ndeductive reasoning and should not introduce new dynamics. Output 'True ' if the\nevolved dynamics introduce new dynamics; otherwise , output 'False '.\n- The evolved dynamics should not introduce any new objects that are not mentioned\nin the primitive dynamics. Output 'True ' if the evolved dynamics introduce new\nobjects; otherwise , output 'False '.\n- Each deductive reasoning step should not contradict any of the primitive dynamics.\nOutput 'True ' if the evolved dynamics contradict the primitive dynamics;\notherwise , output 'False '.\nInstructions for examing the usefulness of the evolved dynamics:\n- The difficulties should be directly related to the current subtask.\n- The evolved dynamics should be useful in solving the difficulties identified in\nthe subtask.\n- Evluating the usefulness of the evolved dynamics on a scale of 1 to 5, where 5 is\nthe most useful and 1 is the least useful.\nLast , output the validity of the evolved dynamics in the following format: {\noutput_format }.\nB.7.2\nGrounding\nThe prompts for grounding the agent.\nSubtask Termination\nGiven the following details:\n- Subtask description: {self._transition['subtask ']},\n- Current observation: {self._transition['state_description ']},\n- Initial observation: {self._transition['initial_state_description ']},\n- Previous executed actions: {self._transition['previous_actions '][ -3:]}\nyou are asked to decide whether the subtask should be terminated or not.\nFor deciding whether to terminate the subtask , consider:\n- The previous action , provided it was executed successfully.\n- The difference between the initial and current observations , including the\ninventory changes.\nThe subtask should be terminated , and the output should be 'True ' only if any of its\ntermination conditions are met.\nOtherwise , if none of the termination conditions are met , the subtask should\ncontinue running , and the output should be 'False '.\nJustify whether the termination conditions are met or not first , and then provide\nthe termination decision.\nOutput in this format: {output_format }.\nAction Selection\nGiven the following details:\n- Current observation: {self._transition['state_description ']}\n- Current subtask 's description: {self._transition['subtask ']}\n- Previous actions: {self._transition['previous_actions '][ -3:]}\n- Primitive dynamics: {self._transition['primitive_dynamics ']}\nYou are asked to:\n- identify the objects related to the current subtask and provide their locations\nand dynamics.\n- select the top 3 actions that contributes to the subtask by either moving closer\nto the object or interacting with the object; and provide all the objects and\ndynamics directly related with each action.\n- based on each action 's related objects , provide the rationale and detailed\nconsequences of executing each action on the objects.\n- select the best action to execute next and provide the justification for your\nchoice.\nLastly , select the action only from the available actions: {self._transition['\navailable_actions ']}; {feedback }.\nNote: Avoid unnecessary crafting and placement if the items are within reachable\ndistance.\nPlease format your response in the following format: {action_format}\nB.7.3\nOffline Dynamics Learning\nLearning Basic Attribute\nIn the Crafter environment , the world dynamics are unique.\nYou are tasked with discovering the unique dynamics of actions within this\nenvironment.\nGiven the state transitions described as follows: {partial_description}, identify\nthe materials used in the inventory for executing this action.\nTo discover the materials used in the inventory , consider the following aspects:\n- List all the materials used not gained for each state transition and\ntheir corresponding quantities;\n- List all the common materials used in all state transitions and their\ncorresponding\nquantities.\nIf no materials are used in common , simply output as \"None\".\nmaterials are: wood , stone , coal , iron , diamond , sapling.\nOutput as many pre -conditions as you can think of in the following format: {\noutput_format}\nLearning Basic Attribute\nIn the Crafter environment , the world dynamics are unique.\nYou are tasked with discovering the unique dynamics of actions within this\nenvironment.\nGiven the state transitions described as follows: {partial_description}, identify\nthe objects within immediate distance required for executing this action.\nTo discover the objects within immediate distance , consider the following aspects:\n- List all the objects within immediate distance for each state transition before\nexecuting the action , which you can only choose from the object list.\n- Identify and list the objects that are present in all state transitions within the\nimmediate distance before executing the action , which you can only choose from\nthe object list.\n- The common object **must** be present in all state transitions ' immediate objects.\nIf no objects within immediate distance are in common , simply output as \"None\".\nObject list: [coal , cow , diamond , furnace , iron , lava , skeleton , stone , table , tree ,\nwater , zombie , plant]\nOutput as many pre -conditions as you can think of in the following format: {\noutput_format}\nLearning Basic Attribute\nIn the Crafter environment , the world dynamics are unique.\nYou are tasked with discovering the unique dynamics of actions within this\nenvironment.\nGiven the state transitions described as follows: {partial_description}, identify\nthe facing object required for executing this action.\nTo discover the objects within immediate distance , consider the following aspects:\n- List the facing object for each state transition before this action executed , not\nthe direction.\n- List the union of all the facing object across all the state transitions before\nthis action executed.\n- The union of all the facing object must be present in at least one state\ntransitions ' facing object.\nObject list: [coal , cow , diamond , furnace , iron , lava , skeleton , stone , table , tree ,\nwater , zombie , plant]\nOutput as many pre -conditions as you can think of in the following format: {\noutput_format}\nLearning Basic Attribute\nIn the Crafter environment , the world dynamics are unique.\nYou are tasked with discovering the unique dynamics of actions within this\nenvironment.\nGiven the state transitions described as follows: {partial_description}, identify\nthe the inventory tool required for executing this action.\nTo discover the objects within immediate distance , consider the following aspects:\n- List all the tools in the inventory for each state transition.\n- List the tools that are common across all the state transitions.\n- The common tools must be present in all state transitions ' tools; if no tools are\nrequired , simply output as \"None\".\n- List the most advanced tool required for executing the action within the common\ntools.\nTool list: [None , wood pickaxe , stone pickaxe , iron pickaxe , wood sword , stone sword\n, iron sword].\nOutput as many pre -conditions as you can think of in the following format: {\noutput_format}\nLearning Basic Attribute\nIn the Crafter environment , the world dynamics are unique.\nYou are tasked with discovering the unique dynamics of actions within this\nenvironment.\nGiven the state transitions described as follows: {partial_description}, identify\nthe inventory and status increase after executing this action.\nTo discover the increase about the inventory and status , consider the following\naspects:\n- List all the increase of the inventory and status for each state transition after\nexecuting the action.\n- List the increase of the inventory and status that are common across all the state\ntransitions after executing the action.\n- The common increase must be present in all state transition 's increase; if no\ninventory and status can be found , simply output as \"None\";\nOutput as many pre -conditions as you can think of in the following format: {\noutput_format}\nLearning Basic Attribute\nIn the Crafter environment , the world dynamics are unique.\nYou are tasked with discovering the unique dynamics of actions within this\nenvironment.\nGiven the state transitions described as follows: {partial_description}, identify\nthe facing object changes for executing this action.\nTo discover the change about the facing object , consider the following aspects:\n- List all the change about the facing object after executing the action for each\nstate transition.\n- List the common change about the facing object across all the state transitions.\n- The common change must be present in all state transitions ' facing object change;\nif no change can be found , simply output as \"None\".\ninventory: [wood , stone , coal , iron , diamond , sapling , wood_pickaxe , stone_pickaxe ,\niron_pickaxe , wood_sword , stone_sword , iron_sword]\nstatus: [health , food , drink , energy], the maximum value of status is 9 and you can\nuse 'increased_to_9 ' to represent the status increase to the maximum value.\nOutput as many pre -conditions as you can think of in the following format: {\noutput_format}\nVerifying Basic Attribute\nIn the crafter environment , you have discovered unique dynamics and need to\nverify the pre -conditions for the action '{action}'.\nGiven the state transitions before and after taking the action '{action}',\ndescribed as follows: {sampled_descriptions}\nYou are asked to verify the inventory materials used pre -conditions for the\naction '{action}' based on the discovered dynamics.\nPrecondition: You need to use {precondition} to execute the action.\nFor the verification , you need to consider the followings:\n- List all the inventory materials that are consumed not gained for each state\ntransition and their corresponding quantity ,\n- List the common inventory materials used across all the state transitions.\n- Determine if the inventory materials used mentioned in the pre -condition are\nwithin the common inventory materials used.\n- If it is within the common inventory materials used , then it is valid;\notherwise , it is invalid.\nFinally , if there are more advanced tools exist , then it is invalid; otherwise ,\nit is valid.\nmaterials are: wood , stone , coal , iron , diamond , sapling.\nOutput in this format: {output_format}\nVerifying Basic Attribute\nIn the crafter environment , you have discovered unique dynamics and need to verify\nthe pre -conditions for the action '{action}'.\nGiven the state transitions before and after taking the action '{action}', described\nas follows: {sampled_descriptions}\nYou are asked to verify the immediate objects pre -conditions for the action '{action\n}' based on the discovered dynamics.\nPrecondition: You only need {precondition} within immediate distance.\nFor the verification , you need to consider the followings:\n- List all the objects within immediate distance before executing the action for\neach state transition only from the object list.\n- Identify and list the objects that are present in all state transitions within the\nimmediate distance before executing the action as common immediate objects ,\nonly from the object list.\n- The common object **must** be present in all state transitions ' immediate objects.\n- Determine if there are objects mentioned in the pre -condition that are not in the\ncommon immediate objects.\n- If there are objects mentioned in the pre -condition that are not in the common\nimmediate objects , then it is invalid; otherwise , it is valid.\nObject list: [coal , cow , diamond , furnace , iron , lava , skeleton , stone , table , tree ,\nwater , zombie , plant]\nOutput in this format: {output_format}\nVerifying Basic Attribute\nIn the crafter environment , you have discovered unique dynamics and need to verify\nthe pre -conditions for the action '{action}'.\nGiven the state transitions before and after taking the action '{action}', described\nas follows: {sampled_descriptions}\nYou are asked to verify the facing object pre -conditions for the action '{action}'\nbased on the discovered dynamics.\nPrecondition: You need to face {precondition} before execute the action.\nFor the verification , you need to consider the followings:\n- List the facing object for each state transition before this action executed.\n- List the union of all the facing object across all the state transitions before\nthis action executed.\n- Determine if the facing object mentioned in the pre -condition is within the union\nof all the facing object.\n- If the facing object mentioned in the pre -condition is within the union of all the\nfacing objects , then it is valid; otherwise , it is invalid.\nFinally , if there are more advanced tools exist , then it is invalid; otherwise , it\nis valid.\nObject list: [coal , cow , diamond , furnace , iron , lava , skeleton , stone , table , tree ,\nwater , zombie , plant]\nOutput in this format: {output_format}\nVerifying Basic Attribute\nIn the crafter environment , you have discovered unique dynamics and need to verify\nthe pre -conditions for the action '{action}'.\nGiven the state transitions before and after taking the action '{action}', described\nas follows: {sampled_descriptions}\nYou are asked to verify the inventory pre -conditions for the action '{action}' based\non the discovered dynamics.\nPrecondition: You only need {preconditions} to execute the action.\nFor the verification , you need to consider the followings:\nList all the tools in the inventory for each state transition.\n- List the tools that are common across all the state transitions.\n- The common tools must be present in all state transitions ' tools; if no tools are\nrequired , simply output as \"None\".\n- If the tools mentioned in the pre -condition are within the common tools , then it\nis valid; otherwise , it is invalid.\nTool list: [None , wood pickaxe , stone pickaxe , iron pickaxe , wood sword , stone sword\n, iron sword].\nOutput in this format: {output_format}\nVerifying Basic Attribute\nIn the crafter environment , you have discovered unique dynamics and need to verify\nthe outcome for the action '{action}'.\nGiven the state transitions before and after taking the action '{action}', described\nas follows: {sampled_descriptions}\nYou are asked to verify the inventory and status increase after executing this\naction. '{action}' based on the discovered dynamics.\npredicted_increases: You only increase {precondition} in the inventory and status\nafter executing the action.\nFor the verification , you need to consider the followings:\n- List all the increases of the inventory and status for each state transition after\nexecuting the action as the increases.\n- List the increases of the inventory and status that are common across all the\nstate transitions after executing the action as the common increases.\n- The common increases must be present in all state transition 's increases; if no\ninventory and status increases can be found , simply output as \"None\".\n- Determine if there are increases mentioned in the predicted_increases that are not\nin the common increases.\n- If there are increases mentioned in the predicted_increases that are not in the\ncommon increases , then it is invalid; otherwise , it is valid.\nOutput in this format: {output_format}\ninventory: [wood , stone , coal , iron , diamond , sapling , wood_pickaxe , stone_pickaxe ,\niron_pickaxe , wood_sword , stone_sword , iron_sword]\nstatus: [health , food , drink , energy], the maximum value of status is 9 and you can\nuse 'increased_to_9 ' to represent the status increase to the maximum value from\nthe non -maximum value.\nVerifying Basic Attribute\nIn the crafter environment , you have discovered unique dynamics and need to verify\nthe outcome for the action '{action}'.\nGiven the state transitions before and after taking the action '{action}', described\nas follows: {sampled_descriptions}\nYou are asked to verify the facing object changes after executing this action. '{\naction}' based on the discovered dynamics.\npredicted_changes: The facing object changes is {precondition} after executing the\naction.\nFor the verification , you need to consider the followings:\n- List all the change about the facing object after executing the action for each\nstate transition.\n- List the common change about the facing object across all the state transitions.\n- The common change must be present in all state transitions ' facing object change;\nif no change can be found , simply output as \"None\".\n- Determine if the predicted changes are within the common changes.\n- If the predicted changes are within the common changes , then it is valid;\notherwise , it is invalid.\nVerifying Basic Attribute\nGiven randomly selected frames featuring '{obj}', please complete the following:\n1. Identify which objects are most closely related to '{obj}'.\n- Rate their relevance using these terms: 'Very related ', 'Not related '.\n- Here are the selected frames concerning '{obj}': {description}\n2. Identify the common time relationship for this object.\n3. Format your response as follows: {object_relationship_format}\nOnly consider the relationship with the following objects: ['grass ', 'coal ', 'cow ',\n'diamond ', 'iron ', 'lava ', 'skeleton ', 'stone ', 'tree ', 'water ', 'zombie ', '\nplant ', 'path ', 'sand ', 'plant -ripe ']\nFor the relationship definitions:\n- Very related: They can always be found together.\n- Not related: They cannot be found together.\nVerifying Basic Attribute\nGiven randomly selected frames featuring '{obj}' described as: {description}, please\ncomplete the following tasks:\n1. Identify the objects and time most closely related to '{obj}' in the provided\ndescription.\n- Rate their relevance using the following terms: 'Very related , 'Not related '.\n2. Based on the discovered dynamics {dynamics}, verify the relationships between the\nobjects and time.\n3. Output all the valid relationships within the discovered dynamics that match all\nthe relevance levels of the objects in the provided frames only about the object\n'{obj}'.\nConsider relationships with the following objects only: ['grass ', 'coal ', 'cow ', '\ndiamond ', 'furnace ', 'iron ', 'lava ', 'skeleton ', 'stone ', 'table ', 'tree ', '\nwater ', 'zombie ', 'plant ', 'path ', 'sand ', 'plant -ripe '].\nRelationship definitions:\n- Very related: They are always found together.\n- Not related: They are never found together.\nOutput in the following format: {object_relationship_format}\nDiscover Subtask Attribute\nIn the Crafter environment , the world dynamics are complex and there are many\nsubtasks to be completed.\nHere are the dynamics about the core action {action }:\nThe precondition:\n- it need to face: {self.facing_object_preconditions[action ]}\n- it need to have {self.immediate_object_preconditions[action ]} within immediate\ndistance\n- it need to have {self.inventory_materials_precondition[action ]} in the inventory.\n- it need to have {self.inventory_tool_precondition[action ]} in the inventory.\nThe outcome:\n- its facing object changes to {self.facing_object_change[action ]}\n- its inventory and status outcome increases on {self.inventory_outcome[action ]}\nNote: 'None ' indicates that there are no specific preconditions or outcomes for the\ncorresponding elements of the dynamics .\"\nGiven the subtask \"{ subtask}\", I want you to write the step -plan for completing this\nsubtask.\nFirst , read and understand all the provided world dynamics.\nThen , locate the world dynamics that are relevant to the subtask.\nNext , write the subtask 's requirements , the step -plan for completing the subtask\nbased on the world dynamics and the outcomes in this format {subtask_plan_format\n}\n-steps: the general steps required to complete the subtask.\n-termination_condition: when should this subtask be terminated; you should consider\nthe status , potential danger and the outcome of the action\nFinally , only output the plan for the subtask.\nHere are a few examples: {examples}\nHere are the observations on how to completing the subtask: {partial_description}\nVerify Subtask Attribute\nIn the Crafter environment , the world dynamics are complex and there are many\nsubtasks to be completed.\nHere are the dynamics about the core action {action }:\nThe precondition:\n- it need to face: {self.facing_object_preconditions[action ]}\n- it need to have {self.immediate_object_preconditions[action ]} within immediate\ndistance\n- it need to have {self.inventory_materials_precondition[action ]} in the inventory.\n- it need to have {self.inventory_tool_precondition[action ]} in the inventory.\nThe outcome:\n- its facing object changes to {self.facing_object_change[action ]}\n- its inventory and status outcome increases on {self.inventory_outcome[action ]}\nNote: 'None ' indicates that there are no specific preconditions or outcomes for the\ncorresponding elements of the dynamics .\"\nGiven the subtask \"{ subtask}\", I want you to verify the plan for completing this\nsubtask {discovered_plan }.\nFirst , read and understand all the provided world dynamics.\nThen , locate the world dynamics that are relevant to the subtask.\nNext , examine the provided plan 's requirements , the step -plan and the termination\nconditions.\nLastly , if it can be applied across different examples , then it is a valid plan.\nHere are the observations on how to completing the subtask: {partial_description}\nOutput within this format{subtask_verification_format}\nDiscover Subgoal Attribute\nGiven the following considerations:\n- The game 's final goal: collect diamonds and survive.\n- All the subtasks: {['collect_coal ', 'collect_diamond ', 'collect_drink ', '\ncollect_iron ', 'collect_sapling ', 'collect_stone ', 'collect_wood ', '\ndefeat_skeleton ', 'defeat_zombie ', 'eat_cow ', 'eat_plant ', 'make_iron_pickaxe ',\n'make_iron_sword ', 'make_stone_pickaxe ', 'make_stone_sword ', 'make_wood_pickaxe\n', 'make_wood_sword ', 'move ', 'place_furnace ', 'place_plant ', 'place_stone ', '\nplace_table ', 'sleep ']}\n- Environment dynamics: {env_dynamics}\n- Human player completed subtasks in chronological order: {human_demo}\nNote: action_x_y means this action is performed consecutively for y times\nYour task is to generate a plan by reordering the subtasks to help the player\nachieve the final game goal with different subgoals.\nFor plan generation , consider:\n- List all the subtasks that contributes to the final goal from the human player 's\ntrajectory.\n- Re -order the subtasks that fits the game 's dynamics.\n- For each subtask , state it as a subgoal with a description and the exact\nrepetition of the subtask if needed.\nLastly , output the plan in this format: {output_format };\nFor example: {output_example}\nVerify Subgoal Attribute\nGiven the following considerations:\n- The game 's final goal: collect diamonds and survive.\n- All the subtasks: {['collect_coal ', 'collect_diamond ', 'collect_drink ', '\ncollect_iron ', 'collect_sapling ', 'collect_stone ', 'collect_wood ', '\ndefeat_skeleton ', 'defeat_zombie ', 'eat_cow ', 'eat_plant ', 'make_iron_pickaxe ',\n'make_iron_sword ', 'make_stone_pickaxe ', 'make_stone_sword ', 'make_wood_pickaxe\n', 'make_wood_sword ', 'move ', 'place_furnace ', 'place_plant ', 'place_stone ', '\nplace_table ', 'sleep ']}\n- Environment dynamics: {env_dynamics}\n- Human player completed subtasks in chronological order: {human_demo}\nNote: action_x_y means this action is performed consecutively for y times\n- Discovered plan: {discovered_plan}\nYour task is to verify the provided plan.\nFor plan verification , consider:\n- List all the subtasks that contributes to the final goal from the human player 's\ntrajectory.\n- Re -order the subtasks that fits the game 's dynamics.\n- If the provided plan can be applicable to the different trajectories , then it is\nvalid.\nLastly , output this format: {output_format };\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Enhancing Agent Learning through World Dynamics Modeling.pdf"}
