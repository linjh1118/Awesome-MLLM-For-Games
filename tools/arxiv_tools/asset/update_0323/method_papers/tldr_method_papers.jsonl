{"title":"JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse","authors":"Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, Yitao Liang","summary":"Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps:\/\/craftjarvis.github.io\/JarvisVLA.","url":"http:\/\/arxiv.org\/abs\/2503.16365v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.16365v1","published":1742491318000,"comment":"22 pages, 5 figures","pdf_text":"March 2025\nJARVIS-VLA: Post-Training Large-Scale Vision\nLanguage Models to Play Visual Games with\nKeyboards and Mouse\nMuyao Li1†, Zihao Wang1†, Kaichen He1, Xiaojian Ma2 and Yitao Liang1B\n1Peking University, 2BIGAI, All authors are affiliated with Team CraftJarvis\nRecently, action-based decision-making in open-world environments has gained significant attention.\nVisual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in\ndecision-making tasks. However, previous work has primarily focused on action post-training, often\nneglecting enhancements to the foundational model itself. In response, we introduce a novel approach,\nAct from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual\nand linguistic guidance in a self-supervised manner. This enhancement improves the models’ capabilities\nin world knowledge, visual recognition, and spatial grounding in open-world environments. Following\nthe above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human\ninstructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing.\nOur experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40%\nimprovement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate\nthat our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-\nof-the-art performance. We have open-sourced the code, models, and datasets to foster further research.\nThe project page can be found in https:\/\/craftjarvis.github.io\/JarvisVLA.\n1. Introduction\nPretraining foundation models on large-scale,\nnoisy internet datasets has become a mainstream\napproach in NLP and vision [1, 18, 38, 43]. The\nsuccess of models like GPT and LLAMA [35, 40]\nhas shown that large, capable language models\ncan infer and execute tasks described by lan-\nguage prompts.\nHowever, this paradigm has\nyet to achieve similar success in the decision-\nmaking domain [14, 50]. In particular, while Ope-\nnAI’s Video Pre-Training (VPT) model [4] has at-\ntempted to apply a similar approach in Minecraft,\nit still relies heavily on imitation learning (IL)\nafter collecting large-scale YouTube videos of hu-\nman play. VPT’s approach of pretraining with\nimitation learning, followed by downstream su-\npervised fine-tuning and reinforcement learning,\nmade significant strides—culminating in the suc-\ncessful crafting of diamond tools, a key challenge\nin Minecraft1.\n1Diamond tools are considered a grand challenge, with\nexperienced human players taking up to 20 minutes (24,000\nactions) to craft them.\nDespite this success, the reliance on next-action\nprediction in imitation learning limits the develop-\nment of robust, multi-task decision-making abil-\nities [6, 34, 39, 49]. Moreover, this pretraining\nparadigm struggles to generalize to unseen en-\nvironments or tasks due to the intricacies of the\ninteractions between observations and behavior,\nwhereas language tokens are more standardized.\nTo overcome these challenges, a new approach\nhas emerged that leverages pretrained Vision Lan-\nguage Models (VLMs) for decision-making. These\nmodels, known as Visual Language Action mod-\nels (VLAs), integrate language understanding\nwith action generation and can be further en-\nhanced through post-training on visual-language\ntasks [7, 25, 54]. A more detailed discussion can\nbe found in Figure 1 (left) and subsection 2.2.\nHowever, much like traditional imitation learn-\ning, current VLA approaches predominantly fo-\ncus on action post-training. In these models, the\nlearning objective is to generate correct actions\nbased on large-scale cross-task imitation data.\nWe propose that, in addition to action genera-\ntion, understanding the environment and incor-\nCorresponding author(s): Yitao Liang <yitaol@pku.edu.cn>\n† indicates co-first author.\nMuyao Li <2200017405@stu.pku.edu.cn>, Zihao Wang <zhwang@stu.pku.edu.cn>, Xiaojian Ma <xiaojian.ma@ucla.edu>\narXiv:2503.16365v1  [cs.CV]  20 Mar 2025\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 1 | We present JARVIS-VLA, a novel Vision-Language-Action (VLA) model trained with ActVLP paradigm,\npost-trained on vision language tasks (non-decision-making tasks) before training on trajectory datasets to\nhave better decision-making capabilities.\nporating task-related knowledge could be equally\nimportant for achieving more flexible and gener-\nalizable decision-making. To this end, we intro-\nduce a novel training paradigm—Visual Language\nPost-Training (ActVLP)—which integrates visual-\nlanguage tasks into the post-training phase of\nVLA models. Following the above paradigms, we\nobtain the first VLA models in Minecraft that can\nfollow human instructions on over 1k different\natomic tasks, including crafting, smelting, cook-\ning, mining, and killing.\nOur contributions are as follows: (1) We pio-\nneer the use of VLA in the open-world environ-\nment of Minecraft by introducing JARVIS-VLA, a\npowerful model achieving state-of-the-art perfor-\nmance in action-based decision-making. (2) We\nintroduce the concept of Visual Language Post-\nTraining and identify key visual-language guid-\nance strategies that enhance decision-making.\n(3) We investigate the scaling laws of VLA mod-\nels, demonstrating that expanding the scale of\nnon-trajectory vision-language tasks during post-\ntraining leads to significant improvements in\ndownstream task performance.\n(4) We open-\nsource the code, models, and datasets to support\nfurther research in this area.\n2. Learning to Act from Vision Lan-\nguage Post-Training\nIn this section, we present a detailed introduction\nto ActVLP, a new paradigm for training VLA mod-\nels. One of the most significant improvements\nis that we investigate a post-training stage prior\nto imitation learning. Specifically, we instantiate\nthis paradigm in our proposed model, JARVIS-\nVLA. We begin by discussing the architecture for\nJARVIS-VLA in subsection 2.1, followed by an\nexplanation of the training pipeline in subsec-\ntion 2.2 and the datasets used in subsection 2.3.\n2.1. Model Structure\nAs illustrated in Figure 1, JARVIS-VLA employs\nan architecture similar to Llava [27] but with\nslight modifications. The structural framework,\nconsists of several key components:: 1) Visual En-\ncoder: A Vision Transformer [18] that processes\nraw image pixels and converts them into a se-\nquence of fixed-size image patches. 2) Image\nProjection Module: A lightweight two-layer MLP\nthat projects image patch embeddings into the\nsame representational space as word embeddings.\n2\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nLarge Language Models\nViT\nWhat tool should be\nused to kill sheep?\nStage I: World Knowledge Post-Training\nLarge Language Models\nViT\nHow many sheep\ncan be seen?\nStage II: Visual Knowledge and Spatial\nGrounding Post-Training\nLarge Language Models\nViT\n<ins> Kill the bigger\nred sheep. <\/ins>\nStage III: Downstream Instruction\nFollowing Post-Training\nNext-Token Prediction\nSupervised Finetuning\nNext-Token Prediction\nSupervised Finetuning\nNext-Token Prediction\nImitation Learning\nLarge Language Models\nViT\n<ins> Kill the bigger\nred sheep. <\/ins>\nLarge-scale Multi-Task Pre-Training on\nDecision-Making Datasets\nNext-Token Prediction\nImitation Learning\nTraditional VLA Training\nOur Multi-Stage Post-Training VLA Pipeline\nFigure 2 | Previous VLA methods usually directly use imitation learning to finetune original vision-language\nmodels on large-scale multi-domain decision-making datasets to predict the actions [7, 25]. Our ActVLP\ntraining pipeline includes three stages: 1) post-training language models on text-only world knowledge with\nnext-token prediction supervised fine-tuning, 2) post-training both vision encoder and language models on\nmultimodal vision-language alignment and spatial grounding datasets with next-token prediction supervised\nfine-tuning, and 3) post-training only language models on multi-modal instruction following datasets with\nimitation learning.\n3) Language Model Transformers [3, 40]: A pow-\nerful autoregressive language model that serves\nas the core of the system, facilitating multimodal\nreasoning and decision-making.\nUnlike OpenVLA [25], our framework is de-\nsigned for partially observable environments. To\naccommodate this, we adopt a non-Markovian\narchitecture by incorporating a history of obser-\nvation images within the prompt. This approach\nensures that the model retains temporal context,\nwhich is crucial for tasks requiring multi-step\nreasoning and long-horizon decision-making. In\nour experiments, we employ Llava-Next [27] and\nQwen2-VL [43] as base vision language models,\nas both models provide robust support for multi-\nimage reasoning, enabling enhanced perception\nand contextual understanding.\nAnother key distinguishing feature of JARVIS-\nVLA compared to prior VLA models is the integra-\ntion of an action decoder. This module is respon-\nsible for generating both discrete and continuous\nactions. For discrete actions, we consolidate re-\nlated action dimensions into unified categories to\nreduce redundancy and improve efficiency. For\ncontinuous actions, we discretize the action space\ninto bins, which are then mapped to discrete to-\nkens. These tokens are subsequently appended to\nthe vocabulary of the original foundation model,\nallowing the model to generate both textual and\naction-based outputs in a unified manner.\nInstead of retraining the base VLM’s tokenizer,\nwe adopt a strategy inspired by RT-2 [7], repur-\nposing the least frequently used tokens from the\nlanguage tokenizer’s vocabulary to represent ac-\ntion semantics. Specifically, we replace the 51\nleast-used tokens, allocating 22 tokens for mouse\ncontrol (e.g., cursor movements) and 29 tokens\nfor special keyboard inputs (e.g., function keys\nand command shortcuts), which can be found\nin Appendix A. We introduce no other modifica-\ntions to the original VLM architecture to maintain\nmodel generalizability and ensure broad compat-\nibility across different foundation models. This\ndesign choice allows JARVIS-VLA to be easily inte-\ngrated with various pre-trained multimodal mod-\nels while preserving their inherent capabilities.\n2.2. Training Pipeline\nTraditional VLA methods typically employ pre-\ntrained VLMs and train them via imitation learn-\ning on large-scale trajectory data, which includes\ntextual instructions, visual observations, and\naction token sequences, as illustrated in Fig-\nure 2(left). These methods assume that VLMs,\npretrained on diverse internet-scale data, possess\nstrong generalization and fitting capabilities. Con-\nsequently, they are fine-tuned directly on down-\nstream decision-making tasks, leveraging multi-\n3\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 3 | Illustration of various post-training datasets. Models can post-train on various vision-language\ndatasets using a unified tokenizer and support diverse vision-language applications, such as question answering,\nimage captioning, image\/video question answering, visual grounding (including points and bounding box),\nand decision-making. More examples can be found in Appendix D.\nscenario data to enhance action understanding\nand generalization.\nHowever, learning world knowledge from\naction-labeled trajectory data is inherently chal-\nlenging [4]. Moreover, the lack of large-scale\naction-labeled datasets makes it challenging to\npretrain expansive models using only trajectory\ndata [34].\nTo address these challenges, ActVLP enhances\nthe VLM through a structured post-training pro-\ncess, utilizing data that follows the same format\nas pretraining but is more relevant to decision-\nmaking tasks. As shown in Figure 2(right), our\ntraining pipeline consists of three stages.\nStage I: Post-Training Language Models. We\nfirst refine the language transformer of the VLM\nusing large-scale textual datasets related to world\nknowledge in downstream environments, e.g.,\nMinecraft. During this stage, vision-related com-\nponents, including the ViT and vision adapter\nmodules, are frozen.\nThis step enhances the\nmodel’s understanding of decision-making con-\ntexts before incorporating multimodal alignment.\nStage II: Post-Training Vision Encoder and Lan-\nguage Models. Following language post-training,\nwe fully unfreeze the VLM and fine-tune it using\n4\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\ncaptioning, visual question-answering (VQA), and\nspatial grounding datasets, which are multimodal\nand have images in datasets. This stage ensures\nimproved vision-language alignment, enhancing\nthe model’s capacity to integrate world knowl-\nedge with visual perception. Both Stage 1 and\nStage 2 employ next-token prediction through\nsupervised fine-tuning, with the optimization ob-\njective being:\nLSFT = −\n∑︁\n𝑖=1\nlog P𝜃(𝑥𝑖| 𝑥𝑣, 𝑥ins, 𝑥1:𝑖−1)\n(1)\nwhere 𝑥𝑣denotes visual tokens, 𝑥ins represents\nthe instruction, and 𝑥corresponds to the answer.\nThis loss function maintains consistency with the\nstandard causal mask training approach.\nStage III: Imitation Learning on Trajectories.\nIn the final stage, we fine-tune the VLMs on tra-\njectory data, requiring the model to mimic ex-\npert actions given textual instructions 𝑥ins and\nthe current observation image 𝑜𝑡∈ℝ𝐻×𝑊×3. The\nimitation learning objective is defined as:\nLIL = −\n∑︁\n𝑡=1\nlog 𝜋𝜃(𝑎𝑡:𝑡+𝜏| 𝑜𝑡, 𝑥ins)\n(2)\nwhere 𝜋represents the learned policy, and 𝑎𝑡:𝑡+𝜏\ndenotes the future action chunk predicted from\nthe current step. During this phase, vision-related\nmodules remain frozen, while the language tok-\nenizer is modified to incorporate action tokens,\nand the language transformer undergoes full-\nparameter fine-tuning. Additionally, we adopt\naction chunking to further enhance training effi-\nciency and optimize trajectory learning [15].\nThis structured pipeline ensures that the VLM\nis progressively refined before being adapted to\ntrajectory-based imitation learning, resulting in\nimproved world knowledge acquisition, vision-\nlanguage alignment and grounding, and action\ngeneralization in decision-making tasks.\n2.3. Datasets\nTo support the ActVLP training pipeline, we\nconstructed a large-scale multimodal dataset.\nThis dataset includes both non-trajectory task\ndatasets for post-training and trajectory datasets\nfor downstream imitation learning.\nThe non-\ntrajectory datasets are divided into three cate-\ngories: knowledge-based question answering, vi-\nsual language alignment, and spatial grounding.\nThese categories are designed to enhance the\nmodel’s decision-making capabilities before tra-\njectory fine-tuning. For trajectory datasets, we\ncollected over 7.4 million frames of Minecraft\ngameplay data, including expert actions from di-\nverse sources such as human-playing [4], youtube\nvideos, and existing agents [45].\nThe dataset for world knowledge comprehension\ncomprises approximately 277K entries that signif-\nicantly bolster textual understanding, employed\nduring training stage I. The visual-language align-\nment dataset incorporates 35K keyframes en-\nhanced with advanced Vision-Language Models\nto produce both captions and question-answer\npairs, facilitating multimodal supervised fine-\ntuning in the subsequent training stage. The\nspatial grounding dataset focuses on detailed\nobject localization, generating over 404K data\npoints that are instrumental in refining spatial\nunderstanding for ActVLP models.\nBoth the\nvisual-language alignment datasets and the spa-\ntial grounding datasets primarily utilize Minecraft\nobservations, which strengthen the VLM’s under-\nstanding of the world and are used to support\ntraining stage II.\nImitation Learning Trajectory Dataset. VLA\ntraining is constructed on a dataset of human\ngameplay trajectories, particularly from the Ope-\nnAI contractor dataset in Minecraft [4], which\nincludes diverse tasks. We also incorporated an\nadditional 3M rollout frames from VPT [4] and\nJARVIS-1 [45] agents. For structured GUI-based\ntasks like crafting and smelting, we synthesized\n6.4M expert data entries to improve imitation\nlearning. Representative examples of our datasets\nare shown in Figure 3, with further details in Ap-\npendix D.\n3. Experiments\nOur experiments (starting from subsection 4.2)\naim to address the following questions:\nQ1: How do JARVIS-VLA compare to sota open-\n5\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nworld agents and imitation learning methods?\nQ2: Is visual language post-training the true\ncause of the performance improvement?\nQ3: Whether VLAs exhibit scaling laws and how\nActVLP influences them?\nQ4: Is ActVLP sensitive to different VLM back-\nbones? Due to space constraints, we quickly re-\nspond with an affirmative no, detailed experiment\ndiscussion deferred to Appendix F.\n3.1. Experimental Setup\nEvaluation Environment.\nWe use Minecraft\n1.16.5 as our experimental platform [23]. As an\nopen-world game with a substantial knowledge\nbase on platforms such as Reddit and wiki [21],\nMinecraft poses significant challenges to agents\nwhile simultaneously offering rich resources for\nresearch. To ensure fair comparisons, we align\nthe action and visual observation spaces with\nthose of human players [4]. Additionally, we hide\ninformation unavailable to human players as well,\nsuch as agent location and inventory stats.\nBenchmark and Evaluation Metrics. We con-\nduct evaluations using two broad benchmarks: (i)\nthe agent’s capacity to interact with the Minecraft\nenvironment to complete tasks; and (ii) vision-\nlanguage tasks (e.g., question answering, spa-\ntial grounding) designed to assess the VLM’s\nunderstanding of Minecraft-specific knowledge.\nFor the instruction-following tasks, we adopt\nthe MCU Benchmark [31], focusing on four\ncategories—Mine Blocks, Kill Entities,\nCraft Items, and Smelt Items—that repre-\nsent a wide range of typical game-play behaviors\nin Minecraft. Notably, Craft and Smelt require\n2D GUI manipulation through the mouse (cover-\ning thousands of item categories), whereas Mine\nand Kill involve recognizing, navigating, and in-\nteracting with targets in a 3D environment. Each\ncategory contains at least 5 distinct tasks. For\ninstance, the Mine Blocks category includes\nmining iron ore\nwith a stone pickaxe, oak logs\nwith bare hands, grass\n, dirt\n, and obsid-\nian\nwith a diamond pickaxe. Our evaluation\nset includes both simpler tasks (e.g., mining oak\nlogs) and more complex ones (e.g., mining obsid-\nian for over 10 seconds) that have proven chal-\nlenging for prior state-of-the-art agents [12, 30].\nWe perform each task at least 30 times and re-\nport the success rate per task, as well as the av-\nerage success rate within each category. To en-\nsure fairness, maximum execution steps for se-\nlected tasks match those reported by Lin et al.\n[31]. For vision-language assessments, the task\nformulations are illustrated in Figure 3. We pro-\nvide human-written ground-truth answers and\nemploy an LLM-as-judge to evaluate the perfor-\nmance of various VLMs (GPT-4o, Llava, Qwen-VL,\nand our post-trained VLMs). Detailed informa-\ntion on these vision-language benchmarks and\nresults can be found in Appendix E.\nTraining and VLA Configurations.. Our training\npipeline follows the process described in subsec-\ntion 2.2: we first obtain a visual-language post-\ntraining intermediate model, then further train\nit on trajectory tasks to produce the JARVIS-VLA.\nWe conduct experiments using two popular frame-\nworks: Qwen2-VL [43] and Llava [27]. We de-\nvelop a discretized action tokenizer specific to\nMinecraft, comprising 51 tokens that represent\ncamera movements and button actions. We utilize\nthe trl SFT Trainer [41] for finetuning and de-\nploy the VLA with vLLM [26]. Training is carried\nout on 32 A800-80G GPUs, while inference runs\non a single NVIDIA RTX 3090. Further training\ndetails are provided in Appendix B.\nBaselines.\nWe compare our model with: 1)\nVPT [4], including both the behavior cloning\n(VPT-BC) and reinforcement learning (VPT-RL)\nvariants; 2) STEVE-1[30], a text-conditioned pol-\nicy that combines VPT and MineCLIP[21] for in-\nstruction following; 3) GROOT [12], which uses\nvideo prompts as task instructions; and 4) Mine-\nDreamer [58], which leverages a vision-language\nmodel and a diffusion model to guide the STEVE-\n1 policy. Each method follows the default configu-\nration provided in the MCU benchmark for a fair\ncomparison.\n3.2. VLA Performance Evaluation\nWe present the performance results of our pro-\nposed model across four categories from the MCU\nbenchmark [31], as shown in Table 1. For each\n6\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nModel\nModel Size\nMine Blocks\nKill Entities\nCraft Items\nSmelt Items\nAvg.\nAvg.\nAvg.\nAvg.\nVPT-BC [4]\n248M\n0.15\n0.38\n0.33\n0.55\n0.35\n0.44\n0.30\n0.50\n0.45\n0.41\n0.10\n0.00\n0.05\nVPT-RL [4]\n248M\n0.05\n0.35\n0.25\n0.35\n0.25\n0.28\n0.50\n0.30\n0.62\n0.55\n0.05\n0.35\n0.20\nSTEVE-1 [30]\n248M\n0.20\n0.35\n0.54\n0.30\n0.75\n0.38\n0.45\n0.20\n0.70\n0.57\n0.25\n0.40\n0.33\nGROOT [12]\n248M\n0.56\n0.40\n0.67\n0.50\n0.50\n0.52\n0.45\n0.35\n0.25\n0.40\n0.35\n0.25\n0.30\nMineDreamer [58]\n7B\n0.25\n0.40\n0.55\n0.30\n0.70\n0.39\n0.50\n0.25\n0.30\n0.42\n0.30\n0.30\n0.30\nQwen2-VL (orig)\n7B\n0.77\n0.60\n0.79\n0.93\n0.80\n0.84\n0.83\n0.53\n0.40\n0.60\n0.03\n0.10\n0.07\nQwen2-VL (IL)\n7B\n0.70\n0.73\n0.75\n0.97\n0.83\n0.86\n0.73\n0.67\n0.50\n0.65\n0.17\n0.37\n0.29\nJARVIS-VLA-Qwen2\n7B\n0.80\n0.95\n0.88\n0.97\n0.93\n0.95\n0.87\n0.83\n0.63\n0.77\n0.77\n0.70\n0.70\nTable 1 | Evaluation results of different policies on Minecraft tasks. Each group includes multiple tasks (at least\n5), and the Avg. column reports the average success rate within each group. Qwen2-VL, Qwen2-VL (IL) and\nJARVIS-VLA-Qwen2-VL represent the training on the original qwen checkpoint, post-training on only large-scale\nimitation learning trajectories, and post-trained on VLP intermediate model. Qwen2-VL (ActVLP) achieves the\nhighest success rates across all task groups. We test JARVIS-VLA-Qwen2-VL on 1k different instructions and\nthe rollout videos can be found in the project page.\nMCU task, we collect over 100 random trajecto-\nries, which are used to fine-tune base VLMs to\ncreate our final VLA models.\nWe evaluate three variants of the VLMs as base\nmodels: 1) Qwen2-VL (raw): the original VLM\ncheckpoint fine-tuned for downstream tasks. 2)\nQwen2-VL (IL): post-trained on large-scale cross-\ntask trajectories (distinct from downstream tasks)\nusing imitation learning. 3) JARVIS-VLA-Qwen2:\npost-trained on our proposed off-trajectory vision-\nlanguage tasks. Performance is measured by the\naverage success rate across tasks within each cat-\negory. Our results show that JARVIS-VLA-Qwen2-\nVL, post-trained using our approach, consistently\noutperforms prior methods across almost all tasks.\nRemarkably, even without task-specific post-\ntraining, raw Qwen2-VL model, fine-tuned on\ndownstream tasks, outperforms several previ-\nous baselines, including STEVE-1 [30] and\nGROOT [12], which were trained using large-\nscale imitation learning. This highlights the ef-\nfectiveness of using a robust pre-trained VLM as\nthe base model for the policy, leading to strong\nperformance even without additional fine-tuning.\nNotably, we observe a significant performance\nboost with ActVLP post-training. For tasks such as\nCraft Items and Smelt Items, where pre-\nvious methods struggled, JARVIS-VLA-Qwen2-VL\nachieves success rates more than double those of\nthe baseline models. This underscores the effec-\ntiveness of our off-trajectory vision-language task\nstrategy. Furthermore, JARVIS-VLA-Qwen2-VL\noutperforms Qwen2-VL (IL) by over 15%, despite\nusing only 21% of the training trajectory data. In\ncrafting category tasks, the JARVIS-VLA model\nsurpasses traditional baselines by more than dou-\nble, outperforming models like VPT-BC [4] and\nSTEVE-1 [30] on tasks such as \"Craft crafting\ntable\" (\n). This significant improvement is pri-\nmarily due to the use of ViT in VLM and high-\nresolution processing, which are crucial for tasks\nlike crafting and smelting that demand precise\ncontrol in the GUI interface. This suggests that in-\ntegrating off-trajectory vision-language tasks into\nthe training pipeline enhances decision-making\ncapabilities, enabling more accurate action pre-\ndictions in VLA models. Further analysis and ad-\nditional experiments will be presented in the next\nsection.\n3.3. Ablation on Non-Trajectory Datasets\nIn this section, we focus on the post-training of\nQwen2-VL using various non-trajectory vision-\nlanguage tasks to investigate the specific contri-\nbutions to its enhanced performance.\nTo understand the impact of different task en-\nhancements, we conduct an ablation study by\ndividing the non-trajectory datasets and training\nQwen2-VL separately on three types of tasks: spa-\ntial grounding, visual language alignment, and\nknowledge-based question-answering, which are\nall related to Minecraft games. This results in\nthree variants of the VLM, each augmented with\none of these capabilities—spatial grounding, vi-\n7\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 4 | Ablation results on different post-training datasets. We select knowledge datasets, visual question-\nanswering datasets, and spatial grounding datasets to conduct ablation experiments. Our goal is to evaluate\nwhich capabilities and post-training datasets most significantly influence downstream decision-making tasks.\nFigure 5 | The relation between downstream task success rate, training loss, and training steps. The\ncurve shows that scaling downstream finetuning trajectories can scale up the success rate when the loss is\nlower than 0.22.\nsual recognition, and world knowledge. All mod-\nels are finetuned using the same gameplay dataset\nand imitation learning techniques. We also de-\nvelop a benchmark, detailed in Appendix E, to\nevaluate these capabilities. For this evaluation,\nwe select three long-sequence atomic tasks: \"Craft\nthe diamond sword\" (\n), \"Mine the obsidian\"\n(\n), and \"Cook the beef\" (\n), as downstream\ninstruction-following tasks.\nThe results of our ablation studies, presented in\nFigure 4, demonstrate that post-training with non-\ntrajectory vision-language tasks significantly en-\nhances the core capabilities of the VLM across the\nrespective benchmarks. Notably, after fine-tuning,\nmodels enhanced with spatial grounding exhibit\nthe most substantial improvement in downstream\ndecision-making tasks. These findings underscore\nthe effectiveness of non-trajectory post-training\nin boosting the performance of Vision-Language-\nAction models in decision-making tasks, even\nwhen the focus is on a single task. We find that\nnon-trajectory vision-language tasks, which are\nessential for agent pipelines [44, 45], are more\neffective for fine-tuning end-to-end VLA models.\nThis demonstrates the connection between devel-\noping LLM-based agent pipelines with separate\nmodules and fine-tuning end-to-end VLA models.\n3.4. Scaling Experiments\nRecent work on large language models (LLMs)\ntrained on vast amounts of text via next-token\nprediction has shown strong scaling laws [19,\n32, 47, 48]. We investigate whether VLAs, ob-\ntained through post-training on VLMs, exhibit\nsimilar scaling behavior. Specifically, we explore\ntwo questions: Q1) Can scaling up downstream\nimitation learning trajectories further improve\nthe VLA’s task success rate? Q2) Does increas-\ning the amount of non-trajectory vision-language\ntasks used during post-training enhance task com-\npletion success?\nThe results for Q1 are shown in Figure 5. Using\nthe same base model, we observe that increasing\nthe number of downstream trajectories improves\nthe VLA model’s task success rate. However, since\nthe success rate is a discrete metric, we find that\ntasks only show a non-zero success rate when the\nevaluation loss is below 0.30. This indicates that\nthe dataset size for downstream fine-tuning must\n8\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 6 | The relationship between post-training\nloss and downstream task success rates.\nOur\nfindings indicate that increasing the size of post-\ntraining non-trajectory datasets can significantly en-\nhance downstream task success rates, even with a\nfixed number of fine-tuning trajectories.\nbe sufficiently large enough. Furthermore, we ob-\nserve that different tasks require varying amounts\nof downstream data to reduce the evaluation loss\nbelow 0.30, which correlates with the length and\ndifficulty of the tasks.\nThe results for Q2 are illuminated in Figure 6.\nWe also explore the relationship between the eval-\nuation loss during post-training on non-trajectory\nvision-language tasks and task success rate in\ndownstream tasks. We use base models from dif-\nferent stages of post-training (with different eval\nloss on post-training datasets), fine-tuning them\nwith the same downstream trajectory dataset.\nThe baseline represents post-training using im-\nitation learning on cross-task trajectories. We\nfind that, for nearly all tasks, the success rate\nin downstream tasks correlates linearly with\nevaluation loss in post-training, with the lowest\nloss yielding the best results. Notably, models\npost-trained with knowledge-based tasks exhibit\nthe best downstream performance for a given\nevaluation loss. Models enhanced with spatial\ngrounding show the lowest evaluation loss and\nthe highest task success rates. These findings\ndemonstrate that scaling up off-trajectory vision-\nlanguage datasets directly enhances downstream\ntask performance, which has been overlooked in\nprevious VLA works [4, 5, 25].\n4. Related Works\n4.1. Visual-Language-Action Models\nImitation learning (IL) involves learning by mim-\nicking expert interactions with the environment,\nwith the primary challenge being the collection\nof high-quality expert demonstration datasets.\nNumerous studies have sought to enhance tradi-\ntional IL approaches [6, 15, 39]. A promising di-\nrection is the use of Visual-Language-Action (VLA)\nmodels [7, 25, 51, 55, 57], which adopt end-\nto-end imitation learning by fine-tuning VLMs.\nOpenVLA [25] has demonstrated the importance\nof selecting a capable VLM backbone, a conclusion\nfurther reinforced by RoboVLM [28]. Similarly,\nBrohan et al. [7] highlighted that co-training with\nweb-scale vision-language data significantly im-\nproves the generalization of VLA models. While\nprevious works primarily focused on optimiz-\ning the selection of VLMs, several recent studies\nhave begun to pay attention to the comprehen-\nsion capabilities of VLA models [13, 52, 59, 60].\nHowever, few have explicitly focused on en-\nhancing the VLM backbone itself through visual-\nlanguage post-training. Our work addresses this\ngap by proposing targeted visual-language post-\ntraining methods to enrich the capabilities of\nVLMs, thereby improving their performance on\ndownstream VLA tasks.\n4.2. VLM-based Agents in Minecraft\nExisting Minecraft agents based on VLMs typi-\ncally adopt hierarchical architectures [9, 17, 21,\n44, 53, 58]. These methods leverage a VLM’s\nworld knowledge for planning via zero-shot or\nfew-shot in-context learning, without modify-\ning the VLM parameters during agent optimiza-\ntion [29, 42, 44, 45]. STEVE-EYE [56] fine-tuned\nLlama language models [40] using internet text\ndata, achieving improved planning over zero-\nshot prompting. MineDreamer [58] employs the\ninstruction-following capability of VLMs to pre-\ndict future visual observations and generate ac-\ntions based on STEVE-1 [30]. OmniJARVIS [46]\nuses a behavior tokenizer [11, 12] to model hu-\nman trajectories in Minecraft with pretrained\nVLMs. While these approaches optimize VLMs,\nthey still rely on additional policies for action\ngrounding. In contrast, we propose a VLA-based\nagent model that generates actions directly from\ntextual instructions and visual inputs, eliminating\nthe need for extra grounding policies.\n9\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\n5. Conclusions\nWe present ActVLP, a novel training frame-\nwork for visual-language-action models that lever-\nages vision-language post-training to enhance\ndecision-making capabilities in dynamic environ-\nments. Our experiments demonstrate that post-\ntraining on non-trajectory tasks significantly en-\nhances foundation models’ ability to understand\ncomplex environments, resulting in substantial\nimprovements in downstream imitation learning\non trajectory data. The effectiveness of this model\nis validated across multiple VLM architectures,\nproviding strong evidence of its broad applica-\nbility and potential for visual-language-action\nmodel training, as exemplified by our state-of-\nthe-art model, JARVIS-VLA.\nLimitations\nLooking ahead, there are several avenues for im-\nprovement in future work. First, it is crucial to\nenhance the inference throughput of JARVIS-VLA,\nwhich is currently constrained by the large pa-\nrameter size of the VLA based on VLM [8]. We\nbelieve that future integration with MoE [22, 24]\ncould further improve the model’s inference effi-\nciency, with the goal of achieving gameplay per-\nformance levels exceeding 40Hz. Additionally,\nthere remains potential for further performance\ngains. While JARVIS-VLA outperforms previous\nMinecraft policies, it still falls short of the perfor-\nmance demonstrated by top human players, who\nachieve success rates above 90%.\nAcknowledgement\nThis work is funded in part by the Na-\ntional Science and Technology Major Project\n2022ZD0114902. We thank a grant from CCF-\nBaidu Open Fund.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\n```\n#### 2. 论文摘要\n```\nRecently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps:\/\/craftjarvis.github.io\/JarvisVLA.\n```\n\n#### 3. 论文全文\n```\nMarch 2025\nJARVIS-VLA: Post-Training Large-Scale Vision\nLanguage Models to Play Visual Games with\nKeyboards and Mouse\nMuyao Li1†, Zihao Wang1†, Kaichen He1, Xiaojian Ma2 and Yitao Liang1B\n1Peking University, 2BIGAI, All authors are affiliated with Team CraftJarvis\nRecently, action-based decision-making in open-world environments has gained significant attention.\nVisual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in\ndecision-making tasks. However, previous work has primarily focused on action post-training, often\nneglecting enhancements to the foundational model itself. In response, we introduce a novel approach,\nAct from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual\nand linguistic guidance in a self-supervised manner. This enhancement improves the models’ capabilities\nin world knowledge, visual recognition, and spatial grounding in open-world environments. Following\nthe above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human\ninstructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing.\nOur experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40%\nimprovement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate\nthat our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-\nof-the-art performance. We have open-sourced the code, models, and datasets to foster further research.\nThe project page can be found in https:\/\/craftjarvis.github.io\/JarvisVLA.\n1. Introduction\nPretraining foundation models on large-scale,\nnoisy internet datasets has become a mainstream\napproach in NLP and vision [1, 18, 38, 43]. The\nsuccess of models like GPT and LLAMA [35, 40]\nhas shown that large, capable language models\ncan infer and execute tasks described by lan-\nguage prompts.\nHowever, this paradigm has\nyet to achieve similar success in the decision-\nmaking domain [14, 50]. In particular, while Ope-\nnAI’s Video Pre-Training (VPT) model [4] has at-\ntempted to apply a similar approach in Minecraft,\nit still relies heavily on imitation learning (IL)\nafter collecting large-scale YouTube videos of hu-\nman play. VPT’s approach of pretraining with\nimitation learning, followed by downstream su-\npervised fine-tuning and reinforcement learning,\nmade significant strides—culminating in the suc-\ncessful crafting of diamond tools, a key challenge\nin Minecraft1.\n1Diamond tools are considered a grand challenge, with\nexperienced human players taking up to 20 minutes (24,000\nactions) to craft them.\nDespite this success, the reliance on next-action\nprediction in imitation learning limits the develop-\nment of robust, multi-task decision-making abil-\nities [6, 34, 39, 49]. Moreover, this pretraining\nparadigm struggles to generalize to unseen en-\nvironments or tasks due to the intricacies of the\ninteractions between observations and behavior,\nwhereas language tokens are more standardized.\nTo overcome these challenges, a new approach\nhas emerged that leverages pretrained Vision Lan-\nguage Models (VLMs) for decision-making. These\nmodels, known as Visual Language Action mod-\nels (VLAs), integrate language understanding\nwith action generation and can be further en-\nhanced through post-training on visual-language\ntasks [7, 25, 54]. A more detailed discussion can\nbe found in Figure 1 (left) and subsection 2.2.\nHowever, much like traditional imitation learn-\ning, current VLA approaches predominantly fo-\ncus on action post-training. In these models, the\nlearning objective is to generate correct actions\nbased on large-scale cross-task imitation data.\nWe propose that, in addition to action genera-\ntion, understanding the environment and incor-\nCorresponding author(s): Yitao Liang <yitaol@pku.edu.cn>\n† indicates co-first author.\nMuyao Li <2200017405@stu.pku.edu.cn>, Zihao Wang <zhwang@stu.pku.edu.cn>, Xiaojian Ma <xiaojian.ma@ucla.edu>\narXiv:2503.16365v1  [cs.CV]  20 Mar 2025\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 1 | We present JARVIS-VLA, a novel Vision-Language-Action (VLA) model trained with ActVLP paradigm,\npost-trained on vision language tasks (non-decision-making tasks) before training on trajectory datasets to\nhave better decision-making capabilities.\nporating task-related knowledge could be equally\nimportant for achieving more flexible and gener-\nalizable decision-making. To this end, we intro-\nduce a novel training paradigm—Visual Language\nPost-Training (ActVLP)—which integrates visual-\nlanguage tasks into the post-training phase of\nVLA models. Following the above paradigms, we\nobtain the first VLA models in Minecraft that can\nfollow human instructions on over 1k different\natomic tasks, including crafting, smelting, cook-\ning, mining, and killing.\nOur contributions are as follows: (1) We pio-\nneer the use of VLA in the open-world environ-\nment of Minecraft by introducing JARVIS-VLA, a\npowerful model achieving state-of-the-art perfor-\nmance in action-based decision-making. (2) We\nintroduce the concept of Visual Language Post-\nTraining and identify key visual-language guid-\nance strategies that enhance decision-making.\n(3) We investigate the scaling laws of VLA mod-\nels, demonstrating that expanding the scale of\nnon-trajectory vision-language tasks during post-\ntraining leads to significant improvements in\ndownstream task performance.\n(4) We open-\nsource the code, models, and datasets to support\nfurther research in this area.\n2. Learning to Act from Vision Lan-\nguage Post-Training\nIn this section, we present a detailed introduction\nto ActVLP, a new paradigm for training VLA mod-\nels. One of the most significant improvements\nis that we investigate a post-training stage prior\nto imitation learning. Specifically, we instantiate\nthis paradigm in our proposed model, JARVIS-\nVLA. We begin by discussing the architecture for\nJARVIS-VLA in subsection 2.1, followed by an\nexplanation of the training pipeline in subsec-\ntion 2.2 and the datasets used in subsection 2.3.\n2.1. Model Structure\nAs illustrated in Figure 1, JARVIS-VLA employs\nan architecture similar to Llava [27] but with\nslight modifications. The structural framework,\nconsists of several key components:: 1) Visual En-\ncoder: A Vision Transformer [18] that processes\nraw image pixels and converts them into a se-\nquence of fixed-size image patches. 2) Image\nProjection Module: A lightweight two-layer MLP\nthat projects image patch embeddings into the\nsame representational space as word embeddings.\n2\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nLarge Language Models\nViT\nWhat tool should be\nused to kill sheep?\nStage I: World Knowledge Post-Training\nLarge Language Models\nViT\nHow many sheep\ncan be seen?\nStage II: Visual Knowledge and Spatial\nGrounding Post-Training\nLarge Language Models\nViT\n<ins> Kill the bigger\nred sheep. <\/ins>\nStage III: Downstream Instruction\nFollowing Post-Training\nNext-Token Prediction\nSupervised Finetuning\nNext-Token Prediction\nSupervised Finetuning\nNext-Token Prediction\nImitation Learning\nLarge Language Models\nViT\n<ins> Kill the bigger\nred sheep. <\/ins>\nLarge-scale Multi-Task Pre-Training on\nDecision-Making Datasets\nNext-Token Prediction\nImitation Learning\nTraditional VLA Training\nOur Multi-Stage Post-Training VLA Pipeline\nFigure 2 | Previous VLA methods usually directly use imitation learning to finetune original vision-language\nmodels on large-scale multi-domain decision-making datasets to predict the actions [7, 25]. Our ActVLP\ntraining pipeline includes three stages: 1) post-training language models on text-only world knowledge with\nnext-token prediction supervised fine-tuning, 2) post-training both vision encoder and language models on\nmultimodal vision-language alignment and spatial grounding datasets with next-token prediction supervised\nfine-tuning, and 3) post-training only language models on multi-modal instruction following datasets with\nimitation learning.\n3) Language Model Transformers [3, 40]: A pow-\nerful autoregressive language model that serves\nas the core of the system, facilitating multimodal\nreasoning and decision-making.\nUnlike OpenVLA [25], our framework is de-\nsigned for partially observable environments. To\naccommodate this, we adopt a non-Markovian\narchitecture by incorporating a history of obser-\nvation images within the prompt. This approach\nensures that the model retains temporal context,\nwhich is crucial for tasks requiring multi-step\nreasoning and long-horizon decision-making. In\nour experiments, we employ Llava-Next [27] and\nQwen2-VL [43] as base vision language models,\nas both models provide robust support for multi-\nimage reasoning, enabling enhanced perception\nand contextual understanding.\nAnother key distinguishing feature of JARVIS-\nVLA compared to prior VLA models is the integra-\ntion of an action decoder. This module is respon-\nsible for generating both discrete and continuous\nactions. For discrete actions, we consolidate re-\nlated action dimensions into unified categories to\nreduce redundancy and improve efficiency. For\ncontinuous actions, we discretize the action space\ninto bins, which are then mapped to discrete to-\nkens. These tokens are subsequently appended to\nthe vocabulary of the original foundation model,\nallowing the model to generate both textual and\naction-based outputs in a unified manner.\nInstead of retraining the base VLM’s tokenizer,\nwe adopt a strategy inspired by RT-2 [7], repur-\nposing the least frequently used tokens from the\nlanguage tokenizer’s vocabulary to represent ac-\ntion semantics. Specifically, we replace the 51\nleast-used tokens, allocating 22 tokens for mouse\ncontrol (e.g., cursor movements) and 29 tokens\nfor special keyboard inputs (e.g., function keys\nand command shortcuts), which can be found\nin Appendix A. We introduce no other modifica-\ntions to the original VLM architecture to maintain\nmodel generalizability and ensure broad compat-\nibility across different foundation models. This\ndesign choice allows JARVIS-VLA to be easily inte-\ngrated with various pre-trained multimodal mod-\nels while preserving their inherent capabilities.\n2.2. Training Pipeline\nTraditional VLA methods typically employ pre-\ntrained VLMs and train them via imitation learn-\ning on large-scale trajectory data, which includes\ntextual instructions, visual observations, and\naction token sequences, as illustrated in Fig-\nure 2(left). These methods assume that VLMs,\npretrained on diverse internet-scale data, possess\nstrong generalization and fitting capabilities. Con-\nsequently, they are fine-tuned directly on down-\nstream decision-making tasks, leveraging multi-\n3\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 3 | Illustration of various post-training datasets. Models can post-train on various vision-language\ndatasets using a unified tokenizer and support diverse vision-language applications, such as question answering,\nimage captioning, image\/video question answering, visual grounding (including points and bounding box),\nand decision-making. More examples can be found in Appendix D.\nscenario data to enhance action understanding\nand generalization.\nHowever, learning world knowledge from\naction-labeled trajectory data is inherently chal-\nlenging [4]. Moreover, the lack of large-scale\naction-labeled datasets makes it challenging to\npretrain expansive models using only trajectory\ndata [34].\nTo address these challenges, ActVLP enhances\nthe VLM through a structured post-training pro-\ncess, utilizing data that follows the same format\nas pretraining but is more relevant to decision-\nmaking tasks. As shown in Figure 2(right), our\ntraining pipeline consists of three stages.\nStage I: Post-Training Language Models. We\nfirst refine the language transformer of the VLM\nusing large-scale textual datasets related to world\nknowledge in downstream environments, e.g.,\nMinecraft. During this stage, vision-related com-\nponents, including the ViT and vision adapter\nmodules, are frozen.\nThis step enhances the\nmodel’s understanding of decision-making con-\ntexts before incorporating multimodal alignment.\nStage II: Post-Training Vision Encoder and Lan-\nguage Models. Following language post-training,\nwe fully unfreeze the VLM and fine-tune it using\n4\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\ncaptioning, visual question-answering (VQA), and\nspatial grounding datasets, which are multimodal\nand have images in datasets. This stage ensures\nimproved vision-language alignment, enhancing\nthe model’s capacity to integrate world knowl-\nedge with visual perception. Both Stage 1 and\nStage 2 employ next-token prediction through\nsupervised fine-tuning, with the optimization ob-\njective being:\nLSFT = −\n∑︁\n𝑖=1\nlog P𝜃(𝑥𝑖| 𝑥𝑣, 𝑥ins, 𝑥1:𝑖−1)\n(1)\nwhere 𝑥𝑣denotes visual tokens, 𝑥ins represents\nthe instruction, and 𝑥corresponds to the answer.\nThis loss function maintains consistency with the\nstandard causal mask training approach.\nStage III: Imitation Learning on Trajectories.\nIn the final stage, we fine-tune the VLMs on tra-\njectory data, requiring the model to mimic ex-\npert actions given textual instructions 𝑥ins and\nthe current observation image 𝑜𝑡∈ℝ𝐻×𝑊×3. The\nimitation learning objective is defined as:\nLIL = −\n∑︁\n𝑡=1\nlog 𝜋𝜃(𝑎𝑡:𝑡+𝜏| 𝑜𝑡, 𝑥ins)\n(2)\nwhere 𝜋represents the learned policy, and 𝑎𝑡:𝑡+𝜏\ndenotes the future action chunk predicted from\nthe current step. During this phase, vision-related\nmodules remain frozen, while the language tok-\nenizer is modified to incorporate action tokens,\nand the language transformer undergoes full-\nparameter fine-tuning. Additionally, we adopt\naction chunking to further enhance training effi-\nciency and optimize trajectory learning [15].\nThis structured pipeline ensures that the VLM\nis progressively refined before being adapted to\ntrajectory-based imitation learning, resulting in\nimproved world knowledge acquisition, vision-\nlanguage alignment and grounding, and action\ngeneralization in decision-making tasks.\n2.3. Datasets\nTo support the ActVLP training pipeline, we\nconstructed a large-scale multimodal dataset.\nThis dataset includes both non-trajectory task\ndatasets for post-training and trajectory datasets\nfor downstream imitation learning.\nThe non-\ntrajectory datasets are divided into three cate-\ngories: knowledge-based question answering, vi-\nsual language alignment, and spatial grounding.\nThese categories are designed to enhance the\nmodel’s decision-making capabilities before tra-\njectory fine-tuning. For trajectory datasets, we\ncollected over 7.4 million frames of Minecraft\ngameplay data, including expert actions from di-\nverse sources such as human-playing [4], youtube\nvideos, and existing agents [45].\nThe dataset for world knowledge comprehension\ncomprises approximately 277K entries that signif-\nicantly bolster textual understanding, employed\nduring training stage I. The visual-language align-\nment dataset incorporates 35K keyframes en-\nhanced with advanced Vision-Language Models\nto produce both captions and question-answer\npairs, facilitating multimodal supervised fine-\ntuning in the subsequent training stage. The\nspatial grounding dataset focuses on detailed\nobject localization, generating over 404K data\npoints that are instrumental in refining spatial\nunderstanding for ActVLP models.\nBoth the\nvisual-language alignment datasets and the spa-\ntial grounding datasets primarily utilize Minecraft\nobservations, which strengthen the VLM’s under-\nstanding of the world and are used to support\ntraining stage II.\nImitation Learning Trajectory Dataset. VLA\ntraining is constructed on a dataset of human\ngameplay trajectories, particularly from the Ope-\nnAI contractor dataset in Minecraft [4], which\nincludes diverse tasks. We also incorporated an\nadditional 3M rollout frames from VPT [4] and\nJARVIS-1 [45] agents. For structured GUI-based\ntasks like crafting and smelting, we synthesized\n6.4M expert data entries to improve imitation\nlearning. Representative examples of our datasets\nare shown in Figure 3, with further details in Ap-\npendix D.\n3. Experiments\nOur experiments (starting from subsection 4.2)\naim to address the following questions:\nQ1: How do JARVIS-VLA compare to sota open-\n5\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nworld agents and imitation learning methods?\nQ2: Is visual language post-training the true\ncause of the performance improvement?\nQ3: Whether VLAs exhibit scaling laws and how\nActVLP influences them?\nQ4: Is ActVLP sensitive to different VLM back-\nbones? Due to space constraints, we quickly re-\nspond with an affirmative no, detailed experiment\ndiscussion deferred to Appendix F.\n3.1. Experimental Setup\nEvaluation Environment.\nWe use Minecraft\n1.16.5 as our experimental platform [23]. As an\nopen-world game with a substantial knowledge\nbase on platforms such as Reddit and wiki [21],\nMinecraft poses significant challenges to agents\nwhile simultaneously offering rich resources for\nresearch. To ensure fair comparisons, we align\nthe action and visual observation spaces with\nthose of human players [4]. Additionally, we hide\ninformation unavailable to human players as well,\nsuch as agent location and inventory stats.\nBenchmark and Evaluation Metrics. We con-\nduct evaluations using two broad benchmarks: (i)\nthe agent’s capacity to interact with the Minecraft\nenvironment to complete tasks; and (ii) vision-\nlanguage tasks (e.g., question answering, spa-\ntial grounding) designed to assess the VLM’s\nunderstanding of Minecraft-specific knowledge.\nFor the instruction-following tasks, we adopt\nthe MCU Benchmark [31], focusing on four\ncategories—Mine Blocks, Kill Entities,\nCraft Items, and Smelt Items—that repre-\nsent a wide range of typical game-play behaviors\nin Minecraft. Notably, Craft and Smelt require\n2D GUI manipulation through the mouse (cover-\ning thousands of item categories), whereas Mine\nand Kill involve recognizing, navigating, and in-\nteracting with targets in a 3D environment. Each\ncategory contains at least 5 distinct tasks. For\ninstance, the Mine Blocks category includes\nmining iron ore\nwith a stone pickaxe, oak logs\nwith bare hands, grass\n, dirt\n, and obsid-\nian\nwith a diamond pickaxe. Our evaluation\nset includes both simpler tasks (e.g., mining oak\nlogs) and more complex ones (e.g., mining obsid-\nian for over 10 seconds) that have proven chal-\nlenging for prior state-of-the-art agents [12, 30].\nWe perform each task at least 30 times and re-\nport the success rate per task, as well as the av-\nerage success rate within each category. To en-\nsure fairness, maximum execution steps for se-\nlected tasks match those reported by Lin et al.\n[31]. For vision-language assessments, the task\nformulations are illustrated in Figure 3. We pro-\nvide human-written ground-truth answers and\nemploy an LLM-as-judge to evaluate the perfor-\nmance of various VLMs (GPT-4o, Llava, Qwen-VL,\nand our post-trained VLMs). Detailed informa-\ntion on these vision-language benchmarks and\nresults can be found in Appendix E.\nTraining and VLA Configurations.. Our training\npipeline follows the process described in subsec-\ntion 2.2: we first obtain a visual-language post-\ntraining intermediate model, then further train\nit on trajectory tasks to produce the JARVIS-VLA.\nWe conduct experiments using two popular frame-\nworks: Qwen2-VL [43] and Llava [27]. We de-\nvelop a discretized action tokenizer specific to\nMinecraft, comprising 51 tokens that represent\ncamera movements and button actions. We utilize\nthe trl SFT Trainer [41] for finetuning and de-\nploy the VLA with vLLM [26]. Training is carried\nout on 32 A800-80G GPUs, while inference runs\non a single NVIDIA RTX 3090. Further training\ndetails are provided in Appendix B.\nBaselines.\nWe compare our model with: 1)\nVPT [4], including both the behavior cloning\n(VPT-BC) and reinforcement learning (VPT-RL)\nvariants; 2) STEVE-1[30], a text-conditioned pol-\nicy that combines VPT and MineCLIP[21] for in-\nstruction following; 3) GROOT [12], which uses\nvideo prompts as task instructions; and 4) Mine-\nDreamer [58], which leverages a vision-language\nmodel and a diffusion model to guide the STEVE-\n1 policy. Each method follows the default configu-\nration provided in the MCU benchmark for a fair\ncomparison.\n3.2. VLA Performance Evaluation\nWe present the performance results of our pro-\nposed model across four categories from the MCU\nbenchmark [31], as shown in Table 1. For each\n6\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nModel\nModel Size\nMine Blocks\nKill Entities\nCraft Items\nSmelt Items\nAvg.\nAvg.\nAvg.\nAvg.\nVPT-BC [4]\n248M\n0.15\n0.38\n0.33\n0.55\n0.35\n0.44\n0.30\n0.50\n0.45\n0.41\n0.10\n0.00\n0.05\nVPT-RL [4]\n248M\n0.05\n0.35\n0.25\n0.35\n0.25\n0.28\n0.50\n0.30\n0.62\n0.55\n0.05\n0.35\n0.20\nSTEVE-1 [30]\n248M\n0.20\n0.35\n0.54\n0.30\n0.75\n0.38\n0.45\n0.20\n0.70\n0.57\n0.25\n0.40\n0.33\nGROOT [12]\n248M\n0.56\n0.40\n0.67\n0.50\n0.50\n0.52\n0.45\n0.35\n0.25\n0.40\n0.35\n0.25\n0.30\nMineDreamer [58]\n7B\n0.25\n0.40\n0.55\n0.30\n0.70\n0.39\n0.50\n0.25\n0.30\n0.42\n0.30\n0.30\n0.30\nQwen2-VL (orig)\n7B\n0.77\n0.60\n0.79\n0.93\n0.80\n0.84\n0.83\n0.53\n0.40\n0.60\n0.03\n0.10\n0.07\nQwen2-VL (IL)\n7B\n0.70\n0.73\n0.75\n0.97\n0.83\n0.86\n0.73\n0.67\n0.50\n0.65\n0.17\n0.37\n0.29\nJARVIS-VLA-Qwen2\n7B\n0.80\n0.95\n0.88\n0.97\n0.93\n0.95\n0.87\n0.83\n0.63\n0.77\n0.77\n0.70\n0.70\nTable 1 | Evaluation results of different policies on Minecraft tasks. Each group includes multiple tasks (at least\n5), and the Avg. column reports the average success rate within each group. Qwen2-VL, Qwen2-VL (IL) and\nJARVIS-VLA-Qwen2-VL represent the training on the original qwen checkpoint, post-training on only large-scale\nimitation learning trajectories, and post-trained on VLP intermediate model. Qwen2-VL (ActVLP) achieves the\nhighest success rates across all task groups. We test JARVIS-VLA-Qwen2-VL on 1k different instructions and\nthe rollout videos can be found in the project page.\nMCU task, we collect over 100 random trajecto-\nries, which are used to fine-tune base VLMs to\ncreate our final VLA models.\nWe evaluate three variants of the VLMs as base\nmodels: 1) Qwen2-VL (raw): the original VLM\ncheckpoint fine-tuned for downstream tasks. 2)\nQwen2-VL (IL): post-trained on large-scale cross-\ntask trajectories (distinct from downstream tasks)\nusing imitation learning. 3) JARVIS-VLA-Qwen2:\npost-trained on our proposed off-trajectory vision-\nlanguage tasks. Performance is measured by the\naverage success rate across tasks within each cat-\negory. Our results show that JARVIS-VLA-Qwen2-\nVL, post-trained using our approach, consistently\noutperforms prior methods across almost all tasks.\nRemarkably, even without task-specific post-\ntraining, raw Qwen2-VL model, fine-tuned on\ndownstream tasks, outperforms several previ-\nous baselines, including STEVE-1 [30] and\nGROOT [12], which were trained using large-\nscale imitation learning. This highlights the ef-\nfectiveness of using a robust pre-trained VLM as\nthe base model for the policy, leading to strong\nperformance even without additional fine-tuning.\nNotably, we observe a significant performance\nboost with ActVLP post-training. For tasks such as\nCraft Items and Smelt Items, where pre-\nvious methods struggled, JARVIS-VLA-Qwen2-VL\nachieves success rates more than double those of\nthe baseline models. This underscores the effec-\ntiveness of our off-trajectory vision-language task\nstrategy. Furthermore, JARVIS-VLA-Qwen2-VL\noutperforms Qwen2-VL (IL) by over 15%, despite\nusing only 21% of the training trajectory data. In\ncrafting category tasks, the JARVIS-VLA model\nsurpasses traditional baselines by more than dou-\nble, outperforming models like VPT-BC [4] and\nSTEVE-1 [30] on tasks such as \"Craft crafting\ntable\" (\n). This significant improvement is pri-\nmarily due to the use of ViT in VLM and high-\nresolution processing, which are crucial for tasks\nlike crafting and smelting that demand precise\ncontrol in the GUI interface. This suggests that in-\ntegrating off-trajectory vision-language tasks into\nthe training pipeline enhances decision-making\ncapabilities, enabling more accurate action pre-\ndictions in VLA models. Further analysis and ad-\nditional experiments will be presented in the next\nsection.\n3.3. Ablation on Non-Trajectory Datasets\nIn this section, we focus on the post-training of\nQwen2-VL using various non-trajectory vision-\nlanguage tasks to investigate the specific contri-\nbutions to its enhanced performance.\nTo understand the impact of different task en-\nhancements, we conduct an ablation study by\ndividing the non-trajectory datasets and training\nQwen2-VL separately on three types of tasks: spa-\ntial grounding, visual language alignment, and\nknowledge-based question-answering, which are\nall related to Minecraft games. This results in\nthree variants of the VLM, each augmented with\none of these capabilities—spatial grounding, vi-\n7\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 4 | Ablation results on different post-training datasets. We select knowledge datasets, visual question-\nanswering datasets, and spatial grounding datasets to conduct ablation experiments. Our goal is to evaluate\nwhich capabilities and post-training datasets most significantly influence downstream decision-making tasks.\nFigure 5 | The relation between downstream task success rate, training loss, and training steps. The\ncurve shows that scaling downstream finetuning trajectories can scale up the success rate when the loss is\nlower than 0.22.\nsual recognition, and world knowledge. All mod-\nels are finetuned using the same gameplay dataset\nand imitation learning techniques. We also de-\nvelop a benchmark, detailed in Appendix E, to\nevaluate these capabilities. For this evaluation,\nwe select three long-sequence atomic tasks: \"Craft\nthe diamond sword\" (\n), \"Mine the obsidian\"\n(\n), and \"Cook the beef\" (\n), as downstream\ninstruction-following tasks.\nThe results of our ablation studies, presented in\nFigure 4, demonstrate that post-training with non-\ntrajectory vision-language tasks significantly en-\nhances the core capabilities of the VLM across the\nrespective benchmarks. Notably, after fine-tuning,\nmodels enhanced with spatial grounding exhibit\nthe most substantial improvement in downstream\ndecision-making tasks. These findings underscore\nthe effectiveness of non-trajectory post-training\nin boosting the performance of Vision-Language-\nAction models in decision-making tasks, even\nwhen the focus is on a single task. We find that\nnon-trajectory vision-language tasks, which are\nessential for agent pipelines [44, 45], are more\neffective for fine-tuning end-to-end VLA models.\nThis demonstrates the connection between devel-\noping LLM-based agent pipelines with separate\nmodules and fine-tuning end-to-end VLA models.\n3.4. Scaling Experiments\nRecent work on large language models (LLMs)\ntrained on vast amounts of text via next-token\nprediction has shown strong scaling laws [19,\n32, 47, 48]. We investigate whether VLAs, ob-\ntained through post-training on VLMs, exhibit\nsimilar scaling behavior. Specifically, we explore\ntwo questions: Q1) Can scaling up downstream\nimitation learning trajectories further improve\nthe VLA’s task success rate? Q2) Does increas-\ning the amount of non-trajectory vision-language\ntasks used during post-training enhance task com-\npletion success?\nThe results for Q1 are shown in Figure 5. Using\nthe same base model, we observe that increasing\nthe number of downstream trajectories improves\nthe VLA model’s task success rate. However, since\nthe success rate is a discrete metric, we find that\ntasks only show a non-zero success rate when the\nevaluation loss is below 0.30. This indicates that\nthe dataset size for downstream fine-tuning must\n8\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\nFigure 6 | The relationship between post-training\nloss and downstream task success rates.\nOur\nfindings indicate that increasing the size of post-\ntraining non-trajectory datasets can significantly en-\nhance downstream task success rates, even with a\nfixed number of fine-tuning trajectories.\nbe sufficiently large enough. Furthermore, we ob-\nserve that different tasks require varying amounts\nof downstream data to reduce the evaluation loss\nbelow 0.30, which correlates with the length and\ndifficulty of the tasks.\nThe results for Q2 are illuminated in Figure 6.\nWe also explore the relationship between the eval-\nuation loss during post-training on non-trajectory\nvision-language tasks and task success rate in\ndownstream tasks. We use base models from dif-\nferent stages of post-training (with different eval\nloss on post-training datasets), fine-tuning them\nwith the same downstream trajectory dataset.\nThe baseline represents post-training using im-\nitation learning on cross-task trajectories. We\nfind that, for nearly all tasks, the success rate\nin downstream tasks correlates linearly with\nevaluation loss in post-training, with the lowest\nloss yielding the best results. Notably, models\npost-trained with knowledge-based tasks exhibit\nthe best downstream performance for a given\nevaluation loss. Models enhanced with spatial\ngrounding show the lowest evaluation loss and\nthe highest task success rates. These findings\ndemonstrate that scaling up off-trajectory vision-\nlanguage datasets directly enhances downstream\ntask performance, which has been overlooked in\nprevious VLA works [4, 5, 25].\n4. Related Works\n4.1. Visual-Language-Action Models\nImitation learning (IL) involves learning by mim-\nicking expert interactions with the environment,\nwith the primary challenge being the collection\nof high-quality expert demonstration datasets.\nNumerous studies have sought to enhance tradi-\ntional IL approaches [6, 15, 39]. A promising di-\nrection is the use of Visual-Language-Action (VLA)\nmodels [7, 25, 51, 55, 57], which adopt end-\nto-end imitation learning by fine-tuning VLMs.\nOpenVLA [25] has demonstrated the importance\nof selecting a capable VLM backbone, a conclusion\nfurther reinforced by RoboVLM [28]. Similarly,\nBrohan et al. [7] highlighted that co-training with\nweb-scale vision-language data significantly im-\nproves the generalization of VLA models. While\nprevious works primarily focused on optimiz-\ning the selection of VLMs, several recent studies\nhave begun to pay attention to the comprehen-\nsion capabilities of VLA models [13, 52, 59, 60].\nHowever, few have explicitly focused on en-\nhancing the VLM backbone itself through visual-\nlanguage post-training. Our work addresses this\ngap by proposing targeted visual-language post-\ntraining methods to enrich the capabilities of\nVLMs, thereby improving their performance on\ndownstream VLA tasks.\n4.2. VLM-based Agents in Minecraft\nExisting Minecraft agents based on VLMs typi-\ncally adopt hierarchical architectures [9, 17, 21,\n44, 53, 58]. These methods leverage a VLM’s\nworld knowledge for planning via zero-shot or\nfew-shot in-context learning, without modify-\ning the VLM parameters during agent optimiza-\ntion [29, 42, 44, 45]. STEVE-EYE [56] fine-tuned\nLlama language models [40] using internet text\ndata, achieving improved planning over zero-\nshot prompting. MineDreamer [58] employs the\ninstruction-following capability of VLMs to pre-\ndict future visual observations and generate ac-\ntions based on STEVE-1 [30]. OmniJARVIS [46]\nuses a behavior tokenizer [11, 12] to model hu-\nman trajectories in Minecraft with pretrained\nVLMs. While these approaches optimize VLMs,\nthey still rely on additional policies for action\ngrounding. In contrast, we propose a VLA-based\nagent model that generates actions directly from\ntextual instructions and visual inputs, eliminating\nthe need for extra grounding policies.\n9\nJARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse\n5. Conclusions\nWe present ActVLP, a novel training frame-\nwork for visual-language-action models that lever-\nages vision-language post-training to enhance\ndecision-making capabilities in dynamic environ-\nments. Our experiments demonstrate that post-\ntraining on non-trajectory tasks significantly en-\nhances foundation models’ ability to understand\ncomplex environments, resulting in substantial\nimprovements in downstream imitation learning\non trajectory data. The effectiveness of this model\nis validated across multiple VLM architectures,\nproviding strong evidence of its broad applica-\nbility and potential for visual-language-action\nmodel training, as exemplified by our state-of-\nthe-art model, JARVIS-VLA.\nLimitations\nLooking ahead, there are several avenues for im-\nprovement in future work. First, it is crucial to\nenhance the inference throughput of JARVIS-VLA,\nwhich is currently constrained by the large pa-\nrameter size of the VLA based on VLM [8]. We\nbelieve that future integration with MoE [22, 24]\ncould further improve the model’s inference effi-\nciency, with the goal of achieving gameplay per-\nformance levels exceeding 40Hz. Additionally,\nthere remains potential for further performance\ngains. While JARVIS-VLA outperforms previous\nMinecraft policies, it still falls short of the perfor-\nmance demonstrated by top human players, who\nachieve success rates above 90%.\nAcknowledgement\nThis work is funded in part by the Na-\ntional Science and Technology Major Project\n2022ZD0114902. We thank a grant from CCF-\nBaidu Open Fund.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | JARVIS-VLA：通过视觉语言后训练提升大型视觉语言模型在开放世界环境中的决策能力\n\n## 📌 背景痛点\/本文动机\n近年来，基于动作的决策在开放世界环境中受到了广泛关注。视觉语言动作（VLA）模型在大型网络数据集上进行了预训练，并在决策任务中显示出潜力。然而，先前的工作主要集中在动作后训练上，往往忽略了基础模型本身的改进。为了解决这个问题，本文提出了一种新的方法，即“从视觉语言后训练中学习动作”，通过视觉和语言指导以自监督的方式对视觉语言模型（VLM）进行微调。这种增强提高了模型在开放世界环境中的世界知识、视觉识别和空间定位能力。\n\n## 🚀 核心方法\n💡 创新点1：视觉语言后训练（ActVLP）\n本文提出了一种新的训练范式——视觉语言后训练（ActVLP），将视觉语言任务整合到VLA模型的后训练阶段。ActVLP包括三个阶段：\n1. 对语言模型进行后训练，使用与下游环境相关的世界知识文本数据集进行微调。\n2. 对视觉编码器和语言模型进行后训练，使用多模态视觉语言对齐和空间定位数据集进行微调。\n3. 在轨迹数据集上进行模仿学习，要求模型模仿专家动作。\n\n💡 创新点2：JARVIS-VLA模型结构\nJARVIS-VLA采用了一种类似于Llava的架构，但进行了轻微的修改。它包括视觉编码器、图像投影模块、语言模型Transformer和动作解码器等关键组件。此外，JARVIS-VLA还采用了非马尔可夫架构，通过在提示中包含观察图像的历史来确保模型保留时间上下文。\n\n## 📈 实验结果\n实验结果表明，JARVIS-VLA在Minecraft游戏中表现出色，能够遵循人类指令完成超过1k个不同的原子任务，包括制作、熔炼、烹饪、采矿和杀戮等。与最佳代理基线相比，在非轨迹任务上进行后训练导致在一系列原子任务上显著提高了40%。此外，JARVIS-VLA在Minecraft中超越了传统的基于模仿学习的策略，实现了最先进的性能。\n\n## 💬 可借鉴之处\n本文提出的ActVLP训练范式和JARVIS-VLA模型结构为视觉语言动作模型在开放世界环境中的决策能力提升提供了新的思路和方法。此外，本文还探讨了VLA模型的缩放规律，为未来研究提供了重要的参考。","llm_summary_res_status":200}
{"title":"Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy","authors":"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie","summary":"Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.","url":"http:\/\/arxiv.org\/abs\/2502.19902v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2502.19902v2","published":1740647884000,"comment":"Accept to CVPR 2025, Project page:\n  https:\/\/cybertronagent.github.io\/Optimus-2.github.io\/","pdf_text":"Optimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nZaijing Li1 2, Yuquan Xie1, Rui Shao1*, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1*\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/\nAbstract\nBuilding an agent that can mimic human behavior pat-\nterns to accomplish various open-world tasks is a long-\nterm goal.\nTo enable agents to effectively learn behav-\nioral patterns across diverse tasks, a key challenge lies\nin modeling the intricate relationships among observa-\ntions, actions, and language.\nTo this end, we propose\nOptimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-\nlevel planning, alongside a Goal-Observation-Action Con-\nditioned Policy (GOAP) for low-level control. GOAP con-\ntains (1) an Action-guided Behavior Encoder that models\ncausal relationships between observations and actions at\neach timestep, then dynamically interacts with the histori-\ncal observation-action sequence, consolidating it into fixed-\nlength behavior tokens, and (2) an MLLM that aligns be-\nhavior tokens with open-ended language instructions to pre-\ndict actions auto-regressively. Moreover, we introduce a\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset, which contains 25,000 videos across 8 atomic\ntasks, providing about 30M goal-observation-action pairs.\nThe automated construction method, along with the MGOA\ndataset, can contribute to the community’s efforts to train\nMinecraft agents. Extensive experimental results demon-\nstrate that Optimus-2 exhibits superior performance across\natomic tasks, long-horizon tasks, and open-ended instruc-\ntion tasks in Minecraft.\nPlease see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n1. Introduction\nEnabling agents to learn human behavioral patterns for\ncompleting complex tasks in open-world environments, is\na long-standing goal in the field of artificial intelligence\n[5, 23, 34, 47]. To effectively handle diverse tasks in an\n*Corresponding authors\nTransformer\nXL\nGoal \nEncoder\nkeyboard: W\nmouse: [0.0, 1.0]\n⊕\nLLM\nMLLM as Planner\nVision \nEncoder\nCausal \nPerceiver \nHistorical Sequence\nHistory\nAggregator \nAction-guided Behavior Encoder\nExsiting Goal-conditioned Policy\nOurs\n  1. chop a tree\n  2. craft four planks\n  3. craft two sticks\n  4. craft a wood sword\nSub-goals\nGoal-conditioned\nPolicy\nAction\nor\nsub-goal: chop a tree\nsub-goal: chop a tree\nI need a \nwooden \nsword\nVision \nEncoder\n...\nFigure 1.\nLeft: General agent framework.\nRight: Compari-\nson between existing goal-conditioned policies and ours. Existing\nTransformer-XL-based policies [3, 25] exhibit limited natural lan-\nguage understanding capabilities and rely solely on combining im-\nplicit goal embeddings with visual embeddings as inputs. In con-\ntrast, our GOAP achieves superior action prediction by 1) employ-\ning an Action-guided behavior encoder to strengthen causal mod-\neling between observations and actions, as well as to improve his-\ntorical sequence modeling capabilities, and 2) leveraging MLLM\nto enhance open-ended language comprehension.\nopen-world environment like Minecraft [20, 32], a promi-\nnent agent framework [24, 32, 41, 42] integrates a task\nplanner with a goal-conditioned policy. As illustrated in\nFigure 1 (left), this framework first utilizes the task plan-\nner’s language comprehension and visual perception abili-\nties to decompose complex task instructions into sequential\nsub-goals. These sub-goals are then processed by a goal-\nconditioned policy to generate actions.\nAlthough existing agents [24, 32, 42] have made promis-\ning progress by using Multimodal Large Language Models\n(MLLM) [4, 37, 45] as planners, the current performance\nbottleneck for agents lies in the improvement of the goal-\narXiv:2502.19902v2  [cs.AI]  11 Mar 2025\nconditioned policy [24]. As the sub-goal serves as a natu-\nral language description of an observation-action sequence,\nthe goal-conditioned policy needs to learn the crucial re-\nlationships among sub-goals, observations, and actions to\npredict actions. However, existing goal-conditioned poli-\ncies exhibit the following limitations: (1) Existing policies\nneglect the modeling of the relationship between observa-\ntions and actions. As shown in Figure 1, they only model\nthe relationship between the sub-goal and the current ob-\nservation by adding the sub-goal embedding to the obser-\nvation features [3, 25, 43]. However, the current observa-\ntion is generated by the previous action interacting with the\nenvironment. This implies a causal relationship between\naction and observation, which is neglected by current poli-\ncies; (2) Existing policies struggle to model the relation-\nship between open-ended sub-goals and observation-action\nsequences. As depicted in Figure 1, existing policies pri-\nmarily rely on either video encoders [3, 43] or conditional\nvariational autoencoders (CVAE) [25] as goal encoder to\nproduce implicit goal embeddings. Such embeddings have\nlimited representation ability [43]. Simply adding it to ob-\nservation features is sub-optimal and unable to handle the\ncomplex relationship between sub-goals and observation-\naction sequences.\nIn this paper, we propose Optimus-2, a novel agent that\nincorporates an MLLM for planning, alongside a Goal-\nObservation-Action Conditioned Policy (GOAP). To ad-\ndress the aforementioned challenges, we propose GOAP,\nwhich can better model the relationship among the obser-\nvations, actions, and sub-goals in two aspects.\nAn Action-guided Behavior Encoder for observation-\naction sequence modeling. To capture the relationship be-\ntween observations and actions, the Action-guided Behav-\nior Encoder first employs a Causal Perceiver to integrate ac-\ntion embeddings into observation features. It utilizes task-\nrelevant action information as guidance to adjust the obser-\nvation features, thereby providing fine-grained observation-\naction information for action prediction. Additionally, to\nmodel a long-term observation-action sequence without ex-\nceeding input length limitations, a History Aggregator is\nintroduced to dynamically integrate current observation-\naction information with the historical sequence into fixed-\nlength behavior tokens. Behavior tokens can capture the\nlong-term dependencies of the observation-action sequence\nwith a fixed and appropriate length. It enables the agent to\npredict actions that align with the logic of the observation-\naction sequence, rather than making isolated action predic-\ntions based solely on the current observation.\nAn MLLM to model the relationship between sub-\ngoal and observation-action sequence. To explicitly en-\ncode the semantics of sub-goals, we introduce an MLLM as\nthe backbone of GOAP. It aligns the sub-goal with behav-\nior tokens to predict subsequent actions auto-regressively.\nLeveraging the MLLM’s language comprehension and mul-\ntimodal perception capabilities, it can better integrate fea-\ntures from open-ended sub-goals and observation-action\nsequences, thereby enhancing the policy’s action predic-\ntion ability. To the best of our knowledge, GOAP is the\nfirst effort to employ MLLM as the core architecture of\na Minecraft policy, which demonstrates strong instruction\ncomprehension capabilities for open-ended sub-goals.\nMoreover, current Minecraft datasets either lack align-\nment among essential elements [10] or are not publicly\naccessible [1], resulting in a significant scarcity of high-\nquality observation-goal-action pairs necessary for policy\ntraining. To this end, we introduce an automated approach\nfor constructing the Minecraft Goal-Observation-Action\n(MGOA) dataset. The MGOA dataset comprises 25,000\nvideos across 8 atomic tasks, providing approximately 30\nmillion aligned observation-goal-action pairs.\nIt will be\nmade openly available to support advancements within\nthe research community.\nWe conducted comprehensive\nevaluations in the open-world environment of Minecraft,\nand the experimental results demonstrate that Optimus-\n2 achieves superior performance.\nCompared to previous\nSOTA, Optimus-2 achieves an average improvements of\n27%, 10%, and 18% on atomic tasks, long-horizon tasks,\nand open-ended sub-goal tasks, respectively.\nIn summary, our contributions are as follows:\n• We propose a novel agent Optimus-2, which consists of\nan MLLM for planning, and a policy for low-level con-\ntrol. The experimental results demonstrate that Optimus-\n2 exhibits superior performance on atomic tasks, long-\nhorizon tasks, and open-ended sub-goal tasks.\n• To better model the relationship among the observations,\nactions, and sub-goals, we propose Goal-Observation-\nAction Conditioned Policy, GOAP. It contains an Action-\nguided Behavior Encoder for observation-action se-\nquence modeling, and an MLLM to model the relation-\nship between sub-goal and observation-action sequence.\n• To address the scarcity of large-scale, high-quality\ndatasets, we introduce the MGOA dataset. It comprises\napproximately 30 million aligned observation-goal-action\npairs and is generated through an automated process with-\nout any manual annotations. The proposed dataset con-\nstruction method and the released MGOA dataset can\ncontribute to the community’s efforts to train agents.\n2. Related Work\nMinecraft Agents. Previous works [2, 8, 13, 31] have con-\nstructed policies in Minecraft using reinforcement learn-\ning or imitation learning. VPT [1] was training on large-\nscale video data recorded by human players, using behavior\ncloning to mimic human behavior patterns. GROOT [3] em-\nploys a video encoder as a goal encoder to learn semantic in-\nformation from videos. However, these policies rely solely\nLarge Language Model\nAction Head\n�t+1\n ViT\nLlama Tokenizer\nSub-goal: \nChop a tree.\nImage Token\nFFN\nCross-Attention\nV\nK\nQ\n...\nCross-Attention\nHistory-Attention\nV\nK\nQ\nV\nK\nQ\nText Token\nAction Token\nBehavior Token\nCausal Perceiver \nHistory\nAggregator \nAction-guided Behavior Encoder\nHistorical Memory Bank \n...\nAction-guided\nBehavior Encoder\n�t\n�t\n�1\n�t−3\n�t−2\n�t−1\n�t+1\n��\n...\n...\n�t\n...\n�t\n�1\n�2\n�3\n��−1\nLoRA\nMLLM-based\nPlanner\nTask: I need a \nwooden sword.\nFigure 2. Overview of Optimus-2. Given a task and the current observation, Optimus-2 first uses an MLLM-based Planner to generate a\nseries of sub-goals. Optimus-2 then sequentially executes these sub-goals through GOAP. GOAP obtains behavior tokens for the current\ntimestep via the Action-guided Behavior Encoder, and these behavior tokens, along with image and text tokens, are fed into the LLM to\npredict subsequent actions.\non visual observations as input and cannot follow human\ninstructions to accomplish specific tasks. MineCLIP [10]\nintroduces a video-text contrastive learning module as a re-\nward model for policy, and STEVE-1 [25] builds on VPT\n[1] by incorporating MineCLIP as goal encoder, enabling\npolicy to follow natural language instructions. Despite these\nadvancements, these policies are constrained by language\nunderstanding and reasoning capabilities. To address this,\ncurrent agents [20, 24, 32, 40, 42, 43] leverage MLLM’s\ninstruction following capabilities to decompose complex\ntasks into executable sub-goal sequences, which are then\nfed into a goal-conditioned policy [3, 25] or formed as exe-\ncutable code [26, 28, 51, 52]. Despite significant progress,\nthe performance of current policies remains constrained by\ntheir limited ability to understand sub-goals. In this paper,\nwe aim to develop an MLLM-based goal-conditioned pol-\nicy to enhance the policy’s comprehension of open-ended\nsub-goals, thereby improving overall performance.\nLong-term Video Modeling. Previous work [1, 3, 10, 25]\nhave segmented videos into multiple clips for training to\nalleviate the challenges posed by long-sequence video in-\nputs. However, this approach prevents the agent from learn-\ning comprehensive behavior representations from the entire\nvideo. To handle long-term video sequences [22, 48, 49],\nexisting studies employ temporal pooling [30], querying\ntransformers [14, 46], or token merging [16, 38, 50] to\nintegrate long-sequence visual tokens. Inspired by previ-\nous works [6, 18, 19, 44], we propose a Q-former [7, 21]\nstructure with a memory bank [14], enabling effective long-\nterm sequence modeling through interactions with histori-\ncal queries. Unlike existing methods that model only the\nobservation sequence, we focus on multimodal learning\n[33, 35, 36]. Moreover, different from previous work [14]\nthat primarily compress video features into fixed-length to-\nkens, our Action-guided Behavior Encoder dynamically in-\nteracts with the historical sequence at each timestep, pro-\nducing behavior tokens corresponding to the observation-\naction sequence from the start to the current timestep.\n3. Preliminaries and Problem Formulation\nIn Minecraft, agents [1, 3, 25] exhibit behavior patterns\nsimilar to humans: at each time step t, the agent receives\na visual observation ot and generates control actions at+1\nusing the mouse and keyboard.\nThese actions interact\nwith the environment, resulting in a new visual observa-\ntion ot+1.\nThrough continuous interactions, a trajectory\nJ = {(o1, a1), (o2, a2), (o3, a3), . . . , (oT , aT )} is formed,\nwhere T represents the length of the trajectory. Previous\nwork primarily trained Minecraft agents using reinforce-\nment learning [10] or behavior cloning [3, 25]. For exam-\nple, in behavior cloning, the goal of the policy pθ(at+1|o1:t)\nis to minimize the negative log-likelihood of the actions at\neach time step t given the trajectory J. Considering that\nsuch trajectories are typically generated under explicit or\nimplicit goals, many recent approaches condition the be-\nhavior on a (implicit or explicit) goal g and learn goal-\nconditioned policy pθ(at+1|o1:t, g) [3, 25]. Generally, for\nboth agents and humans, the explicit goal g is a natural lan-\nguage instruction.\nFormally, given a trajectory J with length T, standard\nbehavior cloning trains the policy pθ(·) with parameters θ\nby minimizing the negative log-likelihood of actions:\nmin\nθ\nT\nX\nt=1\n−log pθ(at+1|o1:t, g)\n(1)\n4. Optimus-2\nIn this section, we first give an overview of our proposed\nagent framework, Optimus-2. As shown in Figure 1 (left),\nit includes a planner for generating a series of executable\nsub-goals and a policy that sequentially executes these sub-\ngoals to complete the task.\nNext, we introduce how to implement Optimus-2’s plan-\nner (Sec. 4.1). Subsequently, we elaborate on how to imple-\nment the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we\nintroduce an automated dataset generation method to obtain\na high-quality Minecraft Goal-Observation-Action dataset\n(MGOA) for training GOAP.\n4.1. MLLM-based Task Planner\nIn Minecraft, a complex task consists of multiple interme-\ndiate steps, i.e., sub-goals. For example, the task “I need\na wooden pickaxe” includes five sub-goals: ‘chop a tree to\nget logs\n’, ‘craft four planks\n’, ‘craft a crafting table\n’, ‘craft two sticks\n’, and ‘craft a wooden pickaxe\n’.\nTherefore, a planner is essential for the agent, as it needs to\ndecompose the given complex task into a sequence of exe-\ncutable sub-goals for the policy to execute sequentially. In\nthis paper, we follow Li et al. [24], employing an MLLM\nas the planner, which takes current observation and task in-\nstruction as input to generate sub-goals.\n4.2. Goal-Observation-Action Conditioned Policy\nAccording to Sec 3., a key insight into the relationship\namong observation o, action a, and sub-goal g is: that the\nobservation o and action a at the same time step have a\ncausal relationship; and the sub-goal g is a natural language\ndescription of the observation-action sequence over a cer-\ntain time. To better model the relationships among the three\nelements mentioned above, we propose first integrating the\nrepresentations of observation and action at each time step,\nthen modeling the observation-action sequences along the\ntemporal dimension, and finally aligning the observation-\naction sequences with the sub-goal for action prediction.\nMotivated\nby\nthis,\nwe\npropose\na\nnovel\nGoal-\nObservation-Action\nconditioned\nPolicy,\nGOAP.\nAs\nshown in Figure 2, our GOAP consists of an Action-guided\nBehavior Encoder that dynamically models observation-\naction sequences into fixed-length behavior tokens and an\nTable 1. Comparison of the MGOA dataset with existing datasets.\nO, G, and A represent observation, goal, and action. VPT† in-\ndicates the amount of data that is openly accessible. MineCLIP‡\ndenotes narrated Minecraft videos available on YouTube.\nFormat\nDataset\nO\nG\nA\n# Frames\nImage-Text Pairs\nMP5 [32]\n!\n!\n500K\nOmniJARVIS [43]\n!\n!\n!\n600K\nGameplay Video\nVPT† [1]\n!\n!\n6M\nMineCLIP‡ [10]\n!\n!\n20B\nSTEVE-1 [25]\n!\n!\n!\n32K\nMGOA (Ours)\n!\n!\n!\n30M\nMLLM that aligns such behavior tokens with sub-goal for\naction prediction.\n4.2.1. Action-guided Behavior Encoder\nPrevious policies often overlook the causal relationship be-\ntween observation and action at each timestep. Moreover,\nit remains a challenge to model the long-term observation-\naction sequence without exceeding input length constraints.\nTo this end, we propose an Action-guided Behavior En-\ncoder that integrates the representations of observation and\naction at each time step and then dynamically models the\nhistorical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation ot into a\nvisual encoder VE to obtain the visual features:\nvt ←VE(ot)\n(2)\nwhere vt ∈RP ×d, P is the number of patches for each im-\nage, and d is the dimension of the extracted image feature.\nIn practice, we employ ViT [9] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model\nthe relationship between observations and actions. It takes\nthe visual feature vt as query tokens and the action embed-\nding at as key and value. The module then constructs the\ninformation interaction between action at and vt through a\ncross-attention mechanism:\nQ = vtW Q\nv , K = atW K\na , V = atW V\na\n(3)\nˆvt = CrossAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(4)\nwhere W Q\nv , W K\na , and W V\na represent the weight matrices\nfor the query (Q), key (K), and value (V), respectively.\nCrossAttn(·) denotes the cross-attention layer, and d is\nthe dimension of the image features. In this way, it explic-\nitly assigns action information at at time step t to the visual\nfeatures ˆvt, enhancing the causal relationship between ob-\nservations and actions.\nSubsequently, we introduce a History Aggregator mod-\nule to capture the information of the observation-action se-\nquence along the temporal dimension, serving as the behav-\nior representation. At each timestep t, behavior tokens Bt\nserve as queries, while the sequence of historical behavior\ntokens Ht = [B1, B2, . . . , Bt−1] acts as keys and values.\nThe current behavior tokens interact with the historical se-\nquence through a history-attention layer HisAttn(·):\nˆBt = HisAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(5)\nwhere Q, K, and V are calculated similarly to Eq 3.\nFinally, another cross-attention layer is introduced, using\nthe behavior tokens ˆBt as queries, and the visual features ˆvt\nas keys and values. In this way, the behavior tokens incor-\nporate the current observation-action information. Follow-\ning the approach of He et al. [14], we construct a memory\nbank for historical behavior tokens Ht, utilizing the simi-\nlarity between adjacent features to aggregate and compress\nthe behavior tokens. This method not only preserves early\nhistorical information but also keeps the historical behav-\nior token sequence Ht at a fixed length to reduce compu-\ntational costs. Leveraging the Action-guided Behavior En-\ncoder, we obtain behavior tokens ˆBt, which correspond to\nthe observation-action sequence from the start to the current\ntime step t.\n4.2.2. MLLM Backbone\nTo model the relationship between the sub-goal and\nobservation-action sequence, we introduce an MLLM that\ntakes the sub-goal g, current observation features vt, and\nbehavior tokens Bt as input to predict subsequent actions\nauto-regressively. To enable the MLLM backbone MLLM\nto predict low-level actions, we employ VPT [1] as action\nhead AH to map output embeddings ¯at+1 of language model\ninto the action space.\n¯at+1 ←MLLM([g, vt, Bt])\n(6)\nat+1 ←AH(¯at+1)\n(7)\nFormally, given a dataset D = {(o1:T , a1:T )}M with M\ncomplete trajectories, we train GOAP to learn the behavior\ndistribution from D via behavioral cloning. Moreover, we\nintroduce a KL-divergence loss to measure the output dis-\ntribution similarity between GOAP and VPT [1]. This helps\nour model effectively learn the knowledge from the teacher\nmodel VPT. The training loss can be formulated as follows:\nLθ = λBC\nT\nX\nt=1\n−log pθ(at+1|o1:t, a1:t, g)\n+λKL\nT\nX\nt=1\nDKL(qϕ(at+1|o1:t) ∥pθ(at+1|o1:t, g))\n(8)\nwhere λBC and λKL are trade off coefficients, pθ is the\nGOAP, qϕ is the teacher model.\nTable 2. Main Result of GOAP on Atomic Tasks. We report the\naverage rewards of each task.\nPolicy\nLogs\nSeeds\nDirt\nStone\nVPT (text) [1]\n2.6\n0.8\n9.2\n0.0\nSTEVE-1 [25]\n11.0\n5.1\n10.0\n3.2\nGROOT [3]\n14.3\n7.3\n19.7\n19.0\nFSQ GROOT [43]\n10.8\n8.2\n20.3\n5.8\nGOAP [MLP ]\n7.2\n4.3\n14.4\n15.5\nGOAP [V P T ]\n15.0\n8.5\n26.7\n25.7\n4.3. MGOA Dataset\nIn Minecraft, there remains a significant lack of high-\nquality goal-observation-action pairs to support behavior\ncloning training.\nPrevious work has primarily relied on\ngameplay videos as training data. These datasets either lack\nnatural language instructions (explicit goals) [1, 3], or use\nactions predicted by IDM models [1] for each observation\nas pseudo-labels [1, 25], which leads to a risk of misalign-\nment between observations and actions. Inspired by Li et\nal. [24], we propose an automated data generation pipeline\nthat enables the creation of aligned goal-observation-action\npairs without the need for manual annotations or human\ncontractors. First, we utilize existing agents [25], provid-\ning them with clear natural language instructions to attempt\ntask completion in Minecraft. We then record the actions\nand corresponding observations during goal execution, gen-\nerating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply\nthe following filtering criteria: 1) only recording videos\nin which the task is successfully completed, and 2) dis-\ncarding videos where task execution takes an excessive\namount of time.\nFor more details, please refer to Sup.\nC. Through this automated approach, we obtained 25k\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset. A comparison of the MGOA dataset with the exist-\ning Minecraft datasets is shown in Table 1. Our automated\ndata generation pipeline offers several key advantages: 1)\nit enables the generation of aligned goal-observation-action\npairs without the need for manual annotation or pseudo-\nlabeling; 2) its construction process is parallelizable, allow-\ning for rapid dataset generation; and 3) it leverages local\nagents for data generation, resulting in low-cost production.\n5. Experiments\n5.1. Experiments Setting\nEnvironment. Following [1, 25], we conduct experiments\nin the complex, open-world environment of Minecraft on\nthe MineRL [12] platform. The agent interacts with the\nMineRL environment at 20 frames per second, generating\nlow-level control signals for the mouse and keyboard. For\neach task execution, the agent is initialized in a randomized\nTable 3. Main Result of Optimus-2 on Long-horizon Tasks. We report the average success rate (SR) on each task group, the results of each\ntask can be found in the Sup. F.1. Pure GPT-4V† denotes the use of GPT-4V in a zero-shot manner to generate executable sub-goals for\nthe policy. Human‡ denotes the human-level baseline, with results sourced from previous work [24].\nMethod\nPolicy\nWood\nStone\nIron\nGold\nDiamond\nRedStone\nArmor\nPure GPT-4V †\nVPT (text)\n0.22\n0.08\n0.00\n0.00\n0.00\n0.00\n0.00\nSTEVE-1\n0.41\n0.20\n0.00\n0.00\n0.00\n0.00\n0.00\nGOAP\n0.50\n0.31\n0.12\n0.02\n0.01\n0.03\n0.03\nDEPS [41]\nSTEVE-1\n0.77\n0.48\n0.16\n0.00\n0.01\n0.00\n0.10\nJarvis-1 [42]\nSTEVE-1\n0.93\n0.89\n0.36\n0.07\n0.08\n0.16\n0.15\nOptimus-1 [24]\nSTEVE-1\n0.98\n0.92\n0.46\n0.08\n0.11\n0.25\n0.19\nOptimus-2\nGOAP\n0.99\n0.93\n0.53\n0.09\n0.13\n0.28\n0.21\nHuman‡ [24]\n-\n1.00\n1.00\n0.86\n0.17\n0.16\n0.33\n0.28\nTable 4.\nMain Result of GOAP on Open-Ended Instruction\nTasks. We report the average success rate (SR) on Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nGROOT [3] and FSQ GROOT [43] were not included as base-\nlines, as they are unable to process language input.\nPlanner\nPolicy\nGLM-4V\nVPT (text)\n0.05\n0\n0\n0\n0\nSTEVE-1\n0.60\n0\n0\n0\n0\nGOAP\n0.71\n0.39\n0.11\n0.14\n0.13\nGPT-4V\nVPT (text)\n0.11\n0\n0\n0\n0\nSTEVE-1\n0.66\n0.10\n0\n0\n0\nGOAP\n0.75\n0.47\n0.13\n0.16\n0.17\nenvironment, allowing us to evaluate the agent’s generaliza-\ntion across diverse environments. Please refer to Sup. B for\nmore details about the Minecraft environment.\nImplementation details. For the planner, we follow Li et\nal. [24], using a hybrid multimodal memory empowered\nGPT-4V 1 as the agent’s planner. As for the policy, we ini-\ntialize GOAP with the weights of DeepSeek-VL-1.3B [29]\nas initialization. We train it on the MGOA dataset and the\npublicly available OpenAI Contractor Dataset [1] through\nbehavior cloning. All experiments were conducted on 8x\nNVIDIA L40 GPUs. Training details and hyperparameter\nsetting can be found in Sup. D.\nEvaluation Tasks & Metrics. Evaluation tasks are catego-\nrized into three types: Atomic Tasks, Long-Horizon Tasks,\nand Open-Ended Instruction Tasks. For each task, the en-\nvironment is randomly reinitialized on each attempt, with a\nminimum of 30 executions per task to ensure robustness.\n• Atomic Tasks represent short-term skills in Minecraft. We\nselect “chop a tree to get logs\n”, “collect seeds\n”,\n“collect dirt\n”, and “mine stone\nwith a pickaxe” as\nevaluation tasks. These tasks evaluate the policy’s basic\ncapabilities in Minecraft. We report the average rewards\n1https:\/\/openai.com\/index\/gpt-4v-system-card\n(number of items obtained) per task execution as an eval-\nuation metric.\n• Long-horizon Tasks consist of an interdependent atomic\ntasks sequence, where the failure of any single atomic\ntask results in the failure of the entire sequence. These\nlong-horizon tasks are designed to evaluate the agent’s ca-\npability to execute a series of diverse tasks continuously\nwithin a complex environment. We follow the setup of Li\net al. [24], conducting experiments on long-horizon tasks\ncomprising 67 tasks grouped into 7 categories. We report\nthe average Success Rate (SR) as an evaluation metric.\n• Open-Ended Instruction Tasks are not limited to prede-\nfined text formats; rather, they involve flexible language\ndirectives that prompt the agent to accomplish long-\nhorizon tasks. These tasks evaluate the agent’s capacity\nto interpret and execute instructions expressed in open-\nended natural language. We selected the Torch\nfrom\nthe Stone Group, Rail\nfrom the Iron Group, Golden\nShovel\nfrom the Gold Group, Diamond Pickaxe\nfrom the Diamond Group, and Compass\nfrom the Red-\nstone Group as evaluation tasks. Given a crafting rela-\ntionship graph, we instructed GPT-4V and GLM-4V [11]\nto generate five open-ended instructions for each task.\nThis allows us to evaluate the policies’ understanding and\nexecution capabilities regarding open-ended instructions.\nTask instructions are provided in the Sup. E.1.\nBaseline. For Atomic Tasks and Open-ended Instruction\nTasks, we compare GOAP with existing goal-conditioned\npolicies, including VPT [1], STEVE-1 [25], GROOT [3]\nand FSQ GROOT [43]. For Long-horizon Tasks, we employ\nGPT-4V, DEPS [41], Jarvis-1 [42], and Optimus-1 [24] as\nbaselines. We also introduce a human-level baseline [24] to\nevaluate the performance gap between existing agents and\nhuman capabilities.\n5.2. Experimental Results\nThe experimental results for Optimus-2 compared to the\nbaselines across Atomic Tasks, Long-horizon Tasks, and\nAgent\nInstruction: I need some iron ores, what should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 3. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need some iron ores,\nwhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nTable 5. Ablation study of Action-guided Behavior Encoder on\nAtomic Tasks. We report average rewards on each task. CP., HA.,\nand MB. represent the Causal Perceiver, History Aggregator, and\nMemory Bank, respectively.\nAblation Setting\nAtomic Task\nCP.\nHA.\nMB.\nLogs\nSeeds\nDirt\nStone\nAverage\n!\n!\n!\n15.0\n8.5\n26.7\n25.7\n19.0\n6.1\n5.4\n12.7\n15.7\n10.0 (↓47.4%)\n!\n10.2\n4.7\n12.8\n21.1\n12.2 (↓35.8%)\n!\n!\n7.4\n6.2\n13.1\n15.5\n10.6 (↓44.2%)\n!\n!\n12.0\n6.8\n22.7\n16.8\n14.6 (↓23.2%)\nOpen-ended Instruction Tasks are presented in Table 2, Ta-\nble 3, and Table 4, respectively.\nGOAP excels in Atomic Tasks. Table 2 shows that pro-\nposed GOAP achieves improvements of 5%, 4%, 31%, and\n35% over the current SOTA on the Logs\n, Seeds\n, Dirt\n, and Stone\n, respectively. These results demonstrate\nthat GOAP has successfully mastered a range of short-term\nskills across diverse environments, and can acquire items\nmore effectively than existing policies.\nOptimus-2 surpasses SOTA in Long-horizon Tasks.\nTable 3 shows that Optimus-2 achieved the highest success\nrates across all seven task groups, particularly excelling in\nthe challenging Diamond Group and Redstone Group with\nsuccess rates of 13% and 28%, respectively. This indicates\nthat Optimus-2 has effectively learned complex behavior\npatterns across atomic tasks, enabling it to sequentially ex-\necute multiple sub-goals and successfully complete long-\nhorizon tasks within complex environments.\nGOAP\noutperforms\nin\nOpen-ended\nInstruction\nTasks. As shown in Table 4, GOAP achieved significantly\nhigher success rates than existing agents across all tasks.\nNotably, on the challenging tasks of Golden Shovel\n, Di-\namond Pickaxe\n, and Compass\n, existing policies fail to\ncomplete these tasks, whereas GOAP achieves success rates\nof 13%, 16%, and 17%, respectively. This advantage stems\nSuccess Rate\n0.0\n5.0\n10.0\n15.0\nGolden \nShovel\nDiamond \nPickaxe\nCompass\nLLM \n20.0\n17.1\n13.1\n16.2\nTransformer-XL\n0.5\n0.5\n0.0\nFigure 4. Ablation of LLM backbone on Open-ended Instruction\nTasks, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nfrom GOAP’s superior comprehension of open-ended nat-\nural language instructions, whereas existing agents exhibit\nweaker instruction-following capabilities. Moreover, Fig-\nure 3 illustrates an example of different policies executing\nan open-ended goal. Due to the limited representation ca-\npability of their goal encoders, VPT [1] and STEVE-1 [25]\nfail to understand the goal, “I need some iron ores, what\nshould I do?” In contrast, GOAP leverages the MLLM’s\nunderstanding of open-ended instructions to effectively ac-\ncomplish the goal (obtaining iron ore\n).\n5.3. Ablation Study\nThere are many unexplored questions around best practices\nfor developing MLLM-based policy in Minecraft. In this\nsection, we conduct an extensive ablation study and sum-\nmarize our key findings.\nThe Action-guided Behavior Encoder plays a crucial\nrole in task execution. As shown in Table 5, the removal\nof the Causal Perceiver leads to an average performance de-\ncline of 42% across all tasks, highlighting the importance\nof capturing the causal relationship between observations\nand actions. Moreover, eliminating the History Aggregator\nAverage Rewards\n0.0\n5.0\n10.0\n15.0\n25.0\n30.0\nLogs\nSeeds\nDirt\nStone\nOCD_MGOA_Mix \n20.0\n26.7\n25.7\n6.0\n15.0\n8.5\n2.3\n14.3\n2.6\nOCD\n2.3\n1.4\n17.1\n22.1\nMGOA\nFigure 5. Ablation study on Training data. OCD refers to the\nOpenAI Contractor Dataset [1]. We report the average rewards on\neach Atomic Task.\nand Memory Bank also results in an average performance\ndecline of 36% across all tasks. This emphasizes the cru-\ncial role of the History Aggregator in modeling observation-\naction sequences and the Memory Bank in dynamically\nstoring long-sequence information.\nLLM significantly enhances policy’s ability to under-\nstand open-ended instructions. As shown in Figure 4, re-\nplacing the LLM backbone with a Transformer-XL leads to\na noticeable decline in performance. We attribute this to\nthe LLM’s pretraining on large-scale textual corpora, which\nendows it with a robust comprehension of open-ended lan-\nguage, a capability that Transformer-XL lacks.\nA pretrained action head improves performance in\nMinecraft. As shown in Table 2, replacing VPT with a\n2-layer MLP projector as the action head leads to a no-\nticeable decline in Optimus-2’s performance. While MLP-\nbased action heads have shown promising results in other\ndomains [17, 27], this substitution is less effective in the\nMinecraft environment. We attribute this to VPT’s exten-\nsive pretraining on large-scale gameplay data, which equips\nit with substantial domain-specific knowledge critical for\neffective task execution in Minecraft.\nThe MGOA datsaset is beneficial for training GOAP.\nWe conducted comparative experiments to evaluate the im-\npact of different training datasets on performance.\nAs\nshown in Figure 5, training only with the current most com-\nmonly used dataset, OpenAI Contractor Dataset (OCD), re-\nsults in suboptimal performance for GOAP on all Atomic\nTasks.\nFor example, compared to training with a mixed\ndataset, its performance on Stone\ndropped by 89%. We\nattribute this to the fact that OCD offers a wide variety of\ntasks but lacks high data quality.\nIn contrast, using our\nMGOA dataset, performance on the four atomic tasks im-\nproved by an average of 70% compared to using only the\nOCD data. We attribute this to the fact that MGOA contains\nhigh-quality aligned goal-observation-action pairs, which\nis beneficial for policy training. Further, we mix the two\ndatasets to train the policy in order to balance task diversity\nLog\nDirt\nSeed\nStone\n(a) ViT\n(c) Action-guided\n          Behavior Encoder\n(b) MineCLIP\nFigure 6. t-SNE visualization of representations extracted by (a)\nViT (b) MineCLIP and (c) Action-guided Behavior Encoder across\nAtomic Tasks. The visualization results show that the represen-\ntations in (a) and (b) cannot distinguish between different tasks,\nwhereas our Action-guided Behavior Encoder clearly differenti-\nates the behavior representations for the four tasks.\nand data quality, leading to improved performance.\n5.4. Visualization of Behavior Representation\nAs shown in Figure 6, we apply t-SNE [39] to visualize\nobservation features extracted by ViT [9], MineCLIP [10],\nand the Action-guided Behavior Encoder for four tasks.\nFrom (a) and (b) in Figure 6, it is evident that the behavior\nrepresentations extracted by ViT and MineCLIP are highly\nmixed, making it challenging to delineate the boundaries\nbetween different tasks. This lack of clear distinction be-\ntween task-specific behavior representations can hinder the\nmodel’s ability to understand the unique behavior patterns\nassociated with each task, potentially leading to task fail-\nure. In contrast, the visualization in (c) of Figure 6 reveals\nclear, distinct clusters for each task, demonstrating that the\nAction-guided Behavior Encoder effectively captures subtle\ndifferences in observation-action sequences, thereby learn-\ning robust behavior representations across tasks.\n6. Conclusion\nIn this paper, we propose a novel agent, Optimus-2, which\ncan excel in various tasks in the open-world environment of\nMinecraft. Optimus-2 integrates an MLLM for high-level\nplanning and a Goal-Observation-Action conditioned Pol-\nicy (GOAP) for low-level control. As a core contribution\nof this paper, GOAP includes an Action-guided Behavior\nEncoder to model the observation-action sequence and an\nMLLM to align the goal with the observation-action se-\nquence for predicting subsequent actions.\nExtensive ex-\nperimental results demonstrate that GOAP has mastered\nvarious atomic tasks and can comprehend open-ended lan-\nguage instructions. This enables Optimus-2 to achieve su-\nperior performance on long-horizon tasks, surpassing ex-\nisting SOTA. Moreover, we introduce a Minecraft Goal-\nObservation-Action dataset to provide the community with\nlarge-scale, high-quality data for training Minecraft agents.\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nSupplementary Material\nThe supplementary document is organized as follows:\n• Sec. A: Limitation and Future Work.\n• Sec. B: Minecraft Environment.\n• Sec. C: MGOA Dataset.\n• Sec. D: Training Details.\n• Sec. E: Evaluation Benchmark.\n• Sec. F: Experimental Results.\n• Sec. G: Case Study.\nA. Limitation and Future Work\nIn this paper, we aim to explore how agents can mimic\nhuman behavior patterns in Minecraft to accomplish vari-\nous tasks. Experimental results demonstrate that Optimus-\n2 performs exceptionally well in both atomic tasks and\nlong-horizon tasks. However, due to the lack of sufficient\nhigh-quality data for open-ended tasks (such as “building a\nhouse” and “defeating the Ender Dragon”), there remains\nsignificant room for improvement. Once such datasets are\navailable, the ability of Optimus-2 to complete open-ended\ntasks will be enhanced. Moreover, despite showing promis-\ning performance in Minecraft, we have not yet extended our\nexploration to other simulation platforms, which represents\na potential direction for future research.\nB. Minecraft\nMinecraft is an extremely popular sandbox video game de-\nveloped by Mojang Studios 2. It allows players to explore\na blockly, procedurally generated 3D world with infinite\nterrain, discover and extract raw materials, craft tools and\nitems, and build structures or earthworks. In this enviro-\nment, AI agents need to face situations that are highly sim-\nilar to the real world, making judgments and decisions to\ndeal with various environments and problems. As shown\nin Figure 7, both agents and humans are required to receive\nnatural language instructions and current observations as in-\nput, and then output low-level actions, such as mouse and\nkeyboard control commands. Therefore, Minecraft serves\n2https:\/\/www.minecraft.net\/en-us\/article\/meet-mojang-studios\n...\nObservation\n...\nAgent Action\nHuman Action\nspace\nW\nW\nKeyboard:\n {forward}\nMouse:{(2.7, 1.5)}\nKeyboard: \n{forward,jump}\nMouse: {(1.2, 0)}\nMouse: {attack}\nTask: chop a tree to get logs\nLanguage\nFigure 7.\nIllustration of behavior patterns of both human and\nagents in Minecraft.\nas an ideal open-world environment for training agent that\ncan learn human behavior patterns.\nB.1. Basic Rules\nBiomes. The Minecraft world is divided into different areas\ncalled “biomes”. Different biomes contain different blocks\nand plants and change how the land is shaped. There are 79\nbiomes in Minecraft 1.16.5, including ocean, plains, forest,\ndesert, etc. Diverse environments have high requirements\nfor the generalization of agents.\nItem. In Minecraft 1.16.5, there are 975 items can be ob-\ntained, such as wooden pickaxe\n, iron sword\n. Item can\nbe obtained by crafting or destroying blocks or attacking\nentities. For example, agent can attack cows\nto obtain\nleather\nand beef\n. Agent also can use 1 stick\nand 2\ndiamonds\nto craft diamond sword\n.\nTechnology Tree.\nIn Minecraft, the technology hierar-\nchy comprises six levels: wood\n, stone\n, iron\n, gold\n, diamond\n, and redstone\n.\nEach tool level corre-\nsponds to specific mining capabilities. Wooden tools can\nmine stone-level blocks but are incapable of mining iron-\nlevel or higher-level blocks.\nStone tools can mine iron-\nlevel blocks but cannot mine diamond-level or higher-level\nblocks.\nIron tools are capable of mining diamond-level\nblocks. Finally, diamond tools can mine blocks of any level,\nincluding redstone-level.\nGameplay progress. Progression in Minecraft primarily\ninvolves discovering and utilizing various materials and re-\nsources, each unlocking new capabilities and opportunities.\nFor instance, crafting a wooden pickaxe\nenables players\nTable 6. Action space of agent in Minecraft.\nIndex\nAgent Action\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove back.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current movement.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPlace blocks, entity, open items or other interact actions defined by game.\n10\nhotbar [1-9]\nkeys 1-9\nSelects the appropriate hotbar item.\n11\nOpen\/Close Inventory\nkey E\nOpens the Inventory. Close any open GUI.\n12\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nto mine stone\n, which can then be used to create a stone\npickaxe\nand a furnace\n. These tools allow for the mining\nand smelting of iron ore\n. Subsequently, crafting an iron\npickaxe\nenables the extraction of diamonds\n, while a di-\namond pickaxe\ncan mine virtually any block in the game.\nSimilarly, cultivating crops facilitates breeding various an-\nimals, each providing unique resources beyond sustenance.\nDrops from enemies also serve specific purposes, with some\noffering greater utility than others. By integrating resources\nfrom mining, farming, and breeding, players can enchant\ntheir equipment, further enhancing their capabilities. Addi-\ntionally, collecting and crafting materials support construc-\ntion, enabling players to create diverse structures. Beyond\npractical functions, such as building secure bases or farms,\nconstructing personalized structures forms a significant as-\npect of the Minecraft experience. Figure 11 illustrates an\nexample of progression: crafting an iron sword\n.\nB.2. Observation and Action Spaces\nObservation. In this paper, observation space of agent is\ncompletely consistent with human players. The agent only\nreceives an RGB image with dimensions of 640 × 360 dur-\ning the gameplay process, including the hotbar, health in-\ndicators, food saturation, and animations of the player’s\nhands. It is worth helping the agent see more clearly in\nextremely dark environments, we have added a night vision\neffect for the agent, which increases the brightness of the\nenvironment during the night.\nAction Spaces. In MineRL [12] environment, agent’s ac-\ntion space is almost similar to human players. It consists of\ntwo parts: the mouse and the keyboard. The keypresses are\nLog\n48.1%\nSeed\n6.0%\nDirt\n6.0%\nStone\n8.2%\nIron\n8.1%\nGold\n8.0%\nDiamond\n7.9%\nRedstone\n7.7%\nFigure 8. Statistical information on MGOA dataset. It contains 8\nAtomic Tasks: ‘Log\n’, ‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’,\n‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’.\nresponsible for controlling the movement of agents, such as\njumping, forward, back, etc. The mouse movements are re-\nsponsible for controlling the perspective of agents and the\ncursor movements when the GUI is opened. The left and\nright buttons of the mouse are responsible for attacking and\nInstruction\nPool\nFiltering\ninteract\nfeedback\nGPT-4\nScript\nEnvironment\ncollect logs\nkeyboard: W\nmouse: [0.0, 1.0]\nMGOA Dataset\nInstruction\nItems\nFigure 9. The pipeline for generating the MGOA dataset. First, we extracted item names from the Minecraft Wiki and employed GPT-\n4 to generate corresponding instructions. These instructions were then provided as input to STEVE-1, enabling it to interact with the\nenvironment to accomplish the tasks. During task execution, each observation was paired with its corresponding action, resulting in the\ncreation of goal-observation-action pairs.\nusing or placing items. In Minecraft, precise mouse move-\nments are important when completing complex tasks that\nneed open inventory or crafting table. In order to achieve\nboth the same action space with MineDojo [10], we abstract\nthe craft and the smelt action into action space. The detailed\naction space is described in Table 6.\nC. MGOA Dataset\nIn Minecraft, there is still a lack of sufficient high-\nquality goal-observation-action pairs to support the train-\ning of Optimus-2.\nTo address this, we propose an au-\ntomated dataset construction process aimed at creating\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndatasets. Through this method. MGOA contains 25,000\nvideos, providing about 30M goal-observation-action pairs.\nIt contains 8 Atomic Tasks across 5 tech levels: ‘Log\n’,\n‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’, ‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’. Note that the Atomic Tasks in MGOA re-\nquire minimal steps and can typically be completed within\n2 ∼3 minutes. For instance, the task ‘Iron\n’ involves min-\ning iron with a stone pickaxe, without the need to gather raw\nmaterials to craft the stone pickaxe. The statistics for the\nMGOA dataset is shown in Figure 8. We provide several ex-\namples of the dataset in the MGOA samples folder within\nthe supplementary materials. We will release this dataset to\ncontribute to the development of open-world agents within\nthe community.\nC.1. Dataset Construction\nPipeline. Inspired by Li et al. [24], we employed a prior\npolicy (STEVE-1 [25] in our work) to perform specific tasks\nin Minecraft, and recorded the corresponding videos and\nactions to generate goal-observation-action pairs. As illus-\ntrated in Figure 9, we employed a custom script to extract\nitem names from the Minecraft Wiki3. Using these item\nnames, we queried GPT-44 with a predefined prompt tem-\nplate to generate task instructions, thereby constructing an\nInstruction Pool. The task instructions from the Instruc-\ntion Pool serve as input to STEVE-1 [25], enabling it to\ninteract with the environment to complete the tasks. During\ntask execution, each frame and corresponding action were\nrecorded and stored. To expedite data generation, we instan-\ntiated multiple policies and used parallelization to quickly\nproduce large amounts of data.\nData Filtering. We judged task success based on environ-\nmental feedback. For example, feedback like “obtained new\nitem, diamond axe” indicated that the task “craft a diamond\naxe” was successfully completed. During the dataset gen-\neration process, we observed a significant amount of low-\nquality video data due to limitations in the policy’s abil-\nity to follow instructions. Examples of low-quality data in-\ncluded task failures or task completion timeouts. To address\nthis issue, we established two filtering criteria to ensure data\nquality: (1) only retaining data from successfully completed\ntasks, and (2) removing data for tasks that lasted longer than\n2 minutes. These criteria allowed us to automatically filter\nout low-quality data, significantly reducing the cost of con-\nstructing the dataset. As a result, we obtained a high-quality\nMGOA dataset consisting of 25,000 samples.\n3https:\/\/minecraft.wiki\/\n4https:\/\/openai.com\/index\/gpt-4-research\/\nC.2. Comparison with Existing Datasets\nPrevious gameplay videos were primarily obtained through\ntwo methods below.\nVideo Platform: For example, MineDojo [10] collected\ngame videos uploaded by human players on platforms such\nas YouTube and Twitter, combining the video content with\ncorresponding titles or subtitles to form video-text pairs.\nHowever, this dataset lacked recorded actions. To address\nthis, VPT [1] used an Inverse Dynamics Model (IDM) to\ngenerate action sequences from the videos. However, the\nactions predicted by the IDM model are only approxima-\ntions, which introduces a potential risk of misalignment be-\ntween the frames and the corresponding actions.\nHuman Contractors: VPT [1] hired human players to\nfreely explore Minecraft and used the frames and actions\nto construct a video-action dataset. However, this dataset\nlacked explicit natural language instructions.\nTo create\ngoal-observation-action pairs, STEVE-1 [25] used GPT-3.5\nto generate specific task descriptions based on the game-\nplay, thereby integrating natural language instructions into\nthe dataset. However, they provide only approximately 32k\naligned goal-observation-action pairs, which remains a rel-\natively scarce amount of data.\nIn addition, some work [32, 43] have utilized GPT-4V\nto generate image captions, task planning, and reflections,\nthereby creating image-text pairs that form instruction-\nfollowing datasets.\nDistinct from the aforementioned datasets, the MGOA\ndataset directly captures agents performing specific tasks,\noffering clear natural language instructions with a one-to-\none correspondence between observations and actions. Fur-\nthermore, through rigorous data filtering, redundant action\nsequences that do not contribute to task completion are ex-\ncluded from MGOA. In addition, compared to the small-\nscale goal-observation-action datasets currently available,\nMGOA offers 25,000 videos, encompassing approximately\n30 million goal-observation-action pairs. This dataset is not\nonly significantly larger but also highly scalable in an auto-\nmated manner.\nD. Training Details\nD.1. Training Pipeline\nOne of the key factors in implementing our proposed\nmethod lies in the efficient alignment of language with the\nobservation-action sequence, and subsequently translating\nlanguage space into the action space. To tackle this prob-\nlem, we adopt a two-phase training approach. First, we\nalign language with the observation-action sequence via be-\nhavior pre-training. Then, we transform the language space\ninto the action space through action fine-tuning.\nBehavior Pre-training: During the pre-training phase, we\nintegrated the Vision-guided Behavior Encoder into the\nTable 7. Hyperparameter setting for pre-training and finetuning.\nHyperparameter\nPre-training\nFine-tuning\nOptimizer\nAdamW\nAdamW\nLearning Rate\n0.0001\n0.00004\nWarmup Steps\n0\n0\nEpochs\n5\n10\nBatch Size\n32\n2048\nNum. Frames\n5M\n80M\nLoRA r\n64\n64\nLoRA alpha\n128\n128\nmodel.\nWe used OpenAI Contractor Dataset [1] and a\nsubset of MGOA as training data, which comprised ap-\nproximately 5,000 videos. To balance efficiency and ef-\nfectiveness, we freeze the visual encoder, then tune the\nVision-guided Behavior Encoder along with a large lan-\nguage model (LoRA [15]). During pre-training, we set the\nlearning rate to 0.0001 and trained for 5 epochs. The hyper-\nparameter settings are shown in Table 7.\nAction Fine-tuning:\nDuring the fine-tuning phase, we\nadapted the general MLLM DeepSeek-VL-1.3B [29] to the\nMinecraft environment, transitioning the model’s output\nspace from language to low-level actions. We fine-tuned\nit using OpenAI Contractor Dataset [1] and MGOA, which\ncomprises approximately 20,000 videos. In this phase, we\nfreeze the Vision-guided Behavior Encoder, visual encoder,\nand large language model (LoRA), and only fine-tuned the\naction head. During fine-tuning, we set the learning rate to\n0.00004 and train for 10 epochs. The hyperparameter set-\ntings are shown in Table 7.\nD.2. Implementation Details\nFor the planner, we follow Li et al. [24], employing Mul-\ntimodal Hybrid Memory empowered GPT-4V for planning\nand reflection. For the policy, we train the GOAP through\nthe above pipeline. All experiments were conducted on 8x\nNVIDIA L40 GPUs. For the MGOA dataset, data collec-\ntion and filtering were conducted in parallel, taking approx-\nimately 7 days.\nTraining required around 2 days, while\ninference and evaluation on atomic tasks, long-horizon\ntasks, and open-ended instruction tasks took approximately\n4 days.\nE. Benchmark\nE.1. Evaluation Tasks\nThe evaluation tasks are divided into three categories:\nAtomic Tasks, Long-horizon Tasks, and Open-ended In-\nstruction Tasks. For each task, the agent’s environment is\n(a) chop a tree to get logs\n(b) mine dirt\n(c) collect seeds\n(d) dig down to mine stone\nFigure 10. Examples of Atomic Task. The agent must follow the instructions to collect resources. These four tasks represent the basic\ncapabilities of the agent. The more resources are collected, the stronger the basic capabilities of the agent will be.\nrandomly initialized each time, and every task is executed\nat least 30 times. For Atomic Tasks, we follow the setting of\nprior work [25, 43], which requires the agent to execute the\ntask within 2 minutes. We then report the average reward\nfor the task, defined as the number of items obtained. For\nOpen-ended Instruction Tasks and Long-horizon Tasks, we\nreport the average success rate (SR) for each task.\nAtomic Tasks. As shown in Figure 10, Atomic Tasks are\nshort-term skills in Minecraft, such as “chop a tree to get\nlogs\n”, “mine dirt\n”, “collect seeds\n”, and “dig down to\nmine stone\n”, etc.\nLong-horizon Tasks.\nAs shown in Figure 11, Long-\nHorizon Tasks are a sequence of Atomic Tasks. For exam-\nple, “craft an iron sword from scratch” requires complet-\ning the atomic tasks of “chop 7 logs”, “craft 21 planks”,\n“craft 5 sticks”, “craft 1 crafting table”, and so on. These\nAtomic Tasks are interdependent, meaning that the failure of\nany single atomic task will result in the failure of the entire\nLong-horizon Task.\nOpen-ended Instruction Tasks. Open-Ended Instruction\nTasks are not limited to predefined text formats; rather, they\ninvolve flexible language directives that prompt the agent\nto accomplish long-horizon tasks. These tasks evaluate the\nagent’s capacity to interpret and execute instructions ex-\npressed in open-ended natural language. We selected Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and\nCompass\nas evaluation tasks. Instruction for each task\nare shown in Table 8, Table 9, Table 10, Table 11 and Table\n12.\nE.2. Baselines\nIn this section, we provide a brief overview of existing\nMinecraft agents and compare them with our proposed\nOptimus-2. Current agents can be broadly categorized into\ntwo types: policy-based agents and planner-policy agents.\nPolicy-based Agents. Policy-based agents [1–3, 10, 25] re-\nfer to those trained through reinforcement learning or imi-\ntation learning, capable of completing atomic tasks within\nMinecraft. However, due to limitations in instruction un-\nderstanding and reasoning abilities, they struggle to accom-\nplish long-horizon tasks.\nPlanner-Policy Agents. Planner-policy agents [20, 24, 32,\n41–43] refer to non-end-to-end architectures that utilize a\nMLLM (Multi-Layered Language Model) as a planner to\ndecompose complex instructions into a sequence of sub-\ngoals executable by a policy. While significant progress has\n(a) Chop 7 logs\n(b) Craft 21 planks\n(c) Craft 5 sticks\n(d) Craft 1 crafting table\n(e) Craft 1 wooden pickaxe\n(f) Mine 11 cobblestone\n(g) Craft 1 furnace\n(h) Craft 1 stone pickaxe\n(i) Dig down more deeper to find iron ore\n(j) Mine 2 iron ores\n(k) Smelt 2 iron ingots\n(l) Craft 1 iron sword\nFigure 11. An example of long-horizon task “crafting an iron sword”. The agent must sequentially complete each atomic task in order to\nsuccessfully craft the iron sword. Failure in any of the atomic tasks will result in the failure of the entire long-horizon task.\nbeen made, the current performance bottleneck stems from\nthe policy’s ability to effectively understand and execute the\nsub-goals generated by the planner.\nComparison with Existing Agents. As a core contribu-\ntion of this work, we propose a novel Goal-Observation-\nAction Conditioned Policy, GOAP. It integrates two key\ncomponents: an Action-Guided Behavior Encoder for mod-\neling observation-action sequences, and an MLLM for\naligning sub-goals with these sequences. Leveraging the\nMLLM’s advanced understanding of open-ended instruc-\ntions, GOAP demonstrates superior instruction-following\ncapabilities compared to existing policies. On top of GOAP,\nthe proposed agent, Optimus-2, exhibits superior perfor-\nmance in long-horizon tasks, outperforming the current\nstate-of-the-art across all seven task groups.\nF. Experimental Results\nIn this section, we report the experimental results of\nOptimus-2 on each Long-horizon task.\nF.1. Results on Long-horizon Task\nIn this section, we report the results of Optimus-2 on each\nLong-horizon Task, with details including task name, num-\nbers of sub-goals, success rate (SR), and eval times. As\nshown in Tables 13 and 14, Optimus-2 demonstrates supe-\nrior performance across all 67 Long-horizon Tasks. Since\nOptimus-2 is randomly initialized in arbitrary environments\nfor each task execution, the experimental results also high-\nlight its generalization capability across diverse environ-\nments.\nG. Case Study\nIn this section, we provide additional cases to illustrate the\ndifferences in the ability of VPT (text) [1], STEVE-1 [25],\nand Optimus-2 to perform Open-ended Instruction Tasks.\nWe provide different open-ended instructions requiring the\nagent to perform tasks across various biomes. As shown in\nFigure 12, Figure 13, and Figure 14, Optimus-2 effectively\ncompletes all tasks, while VPT (text) and STEVE-1 fail due\nto limitations in language understanding and multimodal\nperception capabilities. Moreover, we provide several demo\nvideos of Optimus-2 performing long-horizon tasks in the\nOptimus2 videos folder within the supplementary ma-\nterials.\nTable 8. Open-ended instruction examples of “Craft a torch”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, I need to locate and chop down a couple of trees to collect 4 logs.\n2. Next, I should craft these logs into 13 planks. 3. Then, I need to use some of the\nplanks to craft 3 sticks. 4. I will use some of the planks to craft a crafting table. 5.\nNext, I should craft a wooden pickaxe using some of the planks and sticks. 6. With\nthe wooden pickaxe, I need to find and mine at least 1 coal from a nearby coal deposit\nor cave. 7. Finally, I can use one stick and one coal to craft a torch.\nPlan 2 1. First, I need to find a tree to gather 4 logs. 2. Next, I have to craft the logs\ninto planks. 3. From the planks, I will craft 3 sticks. 4. I need to craft a crafting table\nusing some of the planks. 5. I should then craft a wooden pickaxe to mine coal. 6. I\nwill locate and mine 1 piece of coal using the wooden pickaxe. 7. Finally, I will use\nthe coal and sticks to craft a torch.\nPlan 3 1. I have to find a tree to get 4 logs. 2. Next, I have to craft planks from the\nlogs. 3. Get sticks from some of the planks. 4. I need a crafting table, which requires\nplanks to craft. 5. I should craft a wooden pickaxe using the crafting table. 6. I need\nto find and mine some coal using the wooden pickaxe. 7. Finally, with the stick and\ncoal, I can craft a torch.\nGLM4V\nPlan 1 1. Locate and chop down a tree to gather 4 logs. 2. Convert the logs into 13\nplanks using the crafting menu. 3. Use the planks to craft a crafting table. 4. Craft 3\nsticks from the remaining planks. 5. With the crafting table and sticks, craft a wooden\npickaxe if one is not already available. 6. Use the wooden pickaxe to dig for coal\nor kill some Creepers or zombies for coal drops. 7. Once 1 coal is obtained, use the\ncrafting table to combine it with a stick to craft 1 torch.\nPlan 2 1. Locate and chop down a tree to gather the 4 logs required. 2. Convert the\nlogs into 13 planks using the crafting menu, as this will be the material needed for\ncrafting the crafting table and the wooden pickaxe. 3. Use some of the planks to craft\na crafting table, which is necessary for advanced crafting recipes. 4. Craft a wooden\npickaxe using some of the planks and sticks. The wooden pickaxe will be used to\nmine coal. 5. Gather 3 sticks, which can be obtained from the logs or by crafting them\nfrom planks. 6. Use the wooden pickaxe to mine coal. Only 1 coal is needed to craft\na torch. 7. Once all materials are gathered, open the crafting table and combine 1 coal\nand 1 stick to craft a torch.\nPlan 3 1. Locate and gather 4 logs from the nearest trees. 2. Convert the logs into\n13 planks using the crafting menu. 3. Use some planks to craft a crafting table for\nmore complex recipes. 4. Craft a wooden pickaxe using some planks and sticks to\ngather coal and other resources. 5. Convert some planks into 3 sticks required for\ncrafting torches and the wooden pickaxe. 6. Use the wooden pickaxe to mine coal\nore, ensuring to gather at least 1 coal. 7. With all materials gathered, open the crafting\ntable and craft 1 torch using 1 coal and 1 stick.\nTable 9. Open-ended instruction examples of “Craft a rail”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and punch down a tree to acquire 7 logs. 2. Convert these logs\ninto planks to obtain a total of 25 planks. 3. Use some of these planks to craft 5 sticks.\n4. Create a crafting table using 4 planks. 5. Craft a wooden pickaxe using 3 planks\nand 2 sticks. 6. Use the wooden pickaxe to mine 11 cobblestones. 7. Craft a furnace\nusing 8 cobblestones. 8. Craft a stone pickaxe using 3 cobblestones and 2 sticks. 9.\nUse the stone pickaxe to mine 6 iron ores. 10. Smelt the iron ores in the furnace to\ncreate 6 iron ingots. 11. Finally, craft a rail using the iron ingots and sticks.\nPlan 2 1. I have to find a tree to get 7 logs. 2. Next, I have to craft the logs into 25\nplanks. 3. Craft 5 sticks from the planks. 4. I need to create a crafting table. 5. Use\nthe crafting table to craft a wooden pickaxe. 6. Use the wooden pickaxe to mine 11\npieces of cobblestone. 7. I’ll then craft a furnace using the cobblestone. 8. Craft a\nstone pickaxe with the cobblestone and sticks. 9. Use the stone pickaxe to mine 6\npieces of iron ore. 10. Smelt the iron ore in the furnace to obtain 6 iron ingots. 11.\nFinally, use the crafting table to craft a rail using the iron ingots and sticks.\nPlan 3 1. I have to find a tree to get 7 logs. 2. Next, I need to craft planks from\nthe logs. 3. Get sticks from the planks. 4. Craft a crafting table using some of the\nplanks. 5. Craft a wooden pickaxe with sticks and planks to mine stone. 6. Mine 11\ncobblestone with the wooden pickaxe. 7. Craft a furnace using the cobblestone. 8.\nCraft a stone pickaxe using cobblestone and sticks. 9. Use the stone pickaxe to mine\n6 iron ore. 10. Smelt the iron ore in the furnace to get 6 iron ingots. 11. Finally, use\nthe iron ingots and sticks to craft a rail.\nGLM4V\nPlan 1 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down\na crafting table to access more crafting recipes. 5. Use the crafting table to craft\na wooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones from the\nground or from stone blocks. 7. Craft a furnace using the cobblestones. 8. Upgrade\nthe wooden pickaxe to a stone pickaxe by crafting it at the crafting table with the\ncobblestones. 9. Use the stone pickaxe to mine 6 iron ore from the appropriate rock\nlayers. 10. Smelt the 6 iron ore in the furnace to produce 6 iron ingots. 11. Finally,\nuse the crafting table to craft 1 rail using the 6 iron ingots and 1 stick.\nPlan 2 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu\nto convert the logs into 25 planks. 3. Craft 5 sticks using the planks in the crafting\nmenu. 4. Place a crafting table to enable the crafting of more complex items. 5.\nCraft a wooden pickaxe using some of the planks and sticks. 6. Use the wooden\npickaxe to mine 11 cobblestones from stone outcrops. 7. Construct a furnace using\nthe cobblestones. 8. Upgrade to a stone pickaxe by crafting it with the cobblestones\nand sticks. 9. Use the stone pickaxe to mine 6 iron ore from iron veins. 10. Smelt the\n6 iron ore in the furnace to produce 6 iron ingots. 11. Finally, use the crafting table to\ncraft 1 rail using the iron ingots and sticks.\nPlan 3 1. Find and chop down a tree to gather 7 logs. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down a\ncrafting table to access more crafting options. 5. Craft a wooden pickaxe using some\nof the planks and sticks. 6. Use the wooden pickaxe to mine 11 cobblestones from\nstone. 7. Construct a furnace using some of the cobblestones. 8. Upgrade the wooden\npickaxe to a stone pickaxe by crafting it with the cobblestones. 9. Search for and\nmine 6 iron ore blocks with the stone pickaxe. 10. Smelt the iron ore in the furnace to\nproduce 6 iron ingots. 11. Finally, use the crafting table to craft 1 rail using the iron\ningots and sticks.\nTable 10. Open-ended instruction examples of “Craft a golden shovel”\nPlanner Instruction\nGPT4V\nPlan 1 1. Start by finding and chopping down trees to collect 9 logs. 2. Convert the logs into 35 planks\nusing the crafting interface. 3. Use some of the planks to craft 8 sticks. 4. Create a crafting table using\nsome of the planks. 5. Craft a wooden pickaxe. 6. Mine 19 cobblestone. 7. Craft a furnace using 8 of the\ncobblestones. 8. Craft a stone pickaxe. 9. Mine 3 iron ores using the stone pickaxe. 10. Smelt the iron ore\nin the furnace. 11. Craft an iron pickaxe using sticks and iron ingots. 12. Locate and mine a gold block\nusing the iron pickaxe. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Craft a golden\nshovel using the gold ingot and sticks.\nPlan 2 1. Start by finding and chopping down trees to collect 9 logs. 2. Use the logs to craft planks. 3.\nCraft sticks. 4. Use some of the planks to craft a crafting table. 5. Use the crafting table to craft a wooden\npickaxe. 6. Use the wooden pickaxe to mine 19 pieces of cobblestone. 7. Use some of the cobblestone to\ncraft a furnace. 8. Use the remaining cobblestone to craft a stone pickaxe. 9. Mine iron ore with the stone\npickaxe. 10. Smelt the iron ore in the furnace to obtain 3 iron ingots. 11. Use 2 of the iron ingots and sticks\nto craft an iron pickaxe. 12. Use the iron pickaxe to mine gold ore or find a gold block; you need at least 1\ngold ingot. 13. If mining gold ore, smelt it in the furnace to obtain gold ingots. 14. Use the gold ingot and\nstick to craft a golden shovel.\nPlan 3 1. Find and cut down trees to collect 9 logs. 2. Use the logs to craft 35 planks. 3. Craft 8 sticks\nusing the planks. 4. Use some of the planks to craft a crafting table. 5. Craft a wooden pickaxe using planks\nand sticks. 6. Mine cobblestone until you have 19 pieces. 7. Use some cobblestone and the crafting table to\ncraft a furnace. 8. Craft a stone pickaxe using cobblestone and remaining sticks. 9. Use the stone pickaxe\nto mine 3 pieces of iron ore. 10. Smelt the iron ore in the furnace to get 3 iron ingots. 11. Craft an iron\npickaxe. 12. Use the iron pickaxe to mine gold ore to obtain gold blocks. 13. Use 1 gold ingot to craft a\ngolden shovel. 14. use a gold ingot and sticks to craft a golden shovel using the crafting table.\nGLM4V\nPlan 1 1. Find and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to expand the crafting options. 5. Craft\na wooden pickaxe with the planks and sticks. 6. Cine cobblestone until you have a total of 19. 7. Craft a\nfurnace using 8 cobblestone. 8. Use the furnace to smelt the cobblestone into 19 stone, then use the crafting\ntable to craft a stone pickaxe with the stone and sticks. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks at the crafting table. 12. Use the iron pickaxe to mine a gold block. 13. Smelt the gold block in\nthe furnace to get 1 gold ingot. 14. Finally, use the crafting table to craft the golden shovel using 1 gold\ningot and 2 sticks.\nPlan 2 1. Find and gather 9 logs from nearby trees. 2. Convert the logs into 35 planks using a crafting\ntable. 3. Craft 8 sticks from the planks. 4. Build a crafting table. 5. Use the crafting table to craft a wooden\npickaxe with the planks and sticks. 6. Use the wooden pickaxe to mine 19 cobblestones from stone. 7.\nConstruct a furnace to smelt materials. 8. Craft a stone pickaxe using the cobblestones and sticks. 9. Use\nthe stone pickaxe to mine 3 iron ore from iron deposits. 10. Smelt the 3 iron ore in the furnace to produce\n3 iron ingots. 11. Craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine a\ngold block. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Finally, craft the golden shovel\nusing the gold ingot and stick in the crafting table.\nPlan 3 1. Find and gather 9 logs from nearby trees. 2. Use the crafting menu to convert the logs into 35\nplanks. 3. Craft 8 sticks from the planks. 4. Craft a crafting table using some of the planks. 5. Use the\ncrafting table to craft a wooden pickaxe with some of the sticks and planks. 6. Use the wooden pickaxe to\nmine 19 cobblestones from stone. 7. Craft a furnace using some of the cobblestones. 8. Use the furnace to\nsmelt the cobblestones into 19 stone, then craft a stone pickaxe with the sticks and stone. 9. Mine 3 iron\nore from the appropriate rock faces. 10. Smelt the iron ore in the furnace to produce 3 iron ingots. 11. Use\nthe crafting table to craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine\na gold block. 13. Smelt the gold block in the furnace to produce 1 gold ingot. 14. Finally, use the crafting\ntable to craft the golden shovel with the gold ingot and 2 sticks.\nTable 11. Open-ended instruction examples of “Craft a diamond pickaxe”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some\nof the planks to craft 8 sticks. 4. Create a crafting table using 4 planks. 5. With the crafting table, craft\na wooden pickaxe using planks and sticks. 6. Use the wooden pickaxe to collect at least 11 cobblestones\nfrom stone. 7. With the cobblestones, craft a furnace. 8. Use more cobblestones to craft a stone pickaxe. 9.\nUse the stone pickaxe to mine 3 iron ores. 10. Smelt the iron ores into 3 iron ingots using the furnace. 11.\nCraft an iron pickaxe with iron ingots and sticks. 12. Use the iron pickaxe to mine 3 diamonds. 13. Finally,\ncraft the diamond pickaxe using the diamonds and sticks.\nPlan 2 1. Begin by locating a tree to gather 9 logs. 2. Convert the logs into planks. 3. Use some of the\nplanks to craft sticks. 4. Craft a crafting table to facilitate further crafting. 5. Create a wooden pickaxe to\nstart mining stone. 6. Mine 11 cobblestones. 7. Use the cobblestones to craft a furnace. 8. Craft a stone\npickaxe to advance to mining iron ore. 9. Locate and mine 3 iron ores using the stone pickaxe. 10. Smelt\nthe iron ores in the furnace to produce 3 iron ingots. 11. Use two of the iron ingots and sticks to craft an\niron pickaxe. 12. Locate a diamond ore vein and mine 3 diamonds using the iron pickaxe. 13. Finally, craft\nthe diamond pickaxe with the 3 diamonds and sticks.\nPlan 3 1. I need to find a tree to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some planks to\ncraft 8 sticks. 4. Craft a crafting table using some of the planks. 5. With the crafting table, make a wooden\npickaxe to mine stone. 6. Mine 11 cobblestones using the wooden pickaxe. 7. Craft a furnace using the\ncollected cobblestones. 8. Make a stone pickaxe to efficiently mine iron ore. 9. Find and mine 3 iron ores\nusing the stone pickaxe. 10. Smelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron\npickaxe using some planks, sticks, and the iron ingots. 12. Locate a source of diamonds and use the iron\npickaxe to mine 3 diamonds. 13. Finally, use the crafting table along with the sticks and diamonds to craft\na diamond pickaxe.\nGLM4V\nPlan 1 1. Start by finding trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to make a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ores. 10.\nSmelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine 3 diamonds, which may require exploring caves or mining deep underground with the\niron pickaxe. 13. Finally, use the crafting table to combine the diamonds and the sticks to craft a diamond\npickaxe.\nPlan 2 1. Locate and chop down trees to gather a total of 9 logs. 2. Use the logs to craft 35 planks, as this\nwill be the base material for several other items. 3. Craft 8 sticks from the planks, which will be needed to\ncreate various tools. 4. Craft a crafting table using some of the planks, as this is essential for crafting more\ncomplex items. 5. Use the crafting table to craft a wooden pickaxe with the planks and sticks, which will be\nused to mine cobblestone. 6. Mine 11 cobblestone using the wooden pickaxe. 7. Craft a stone pickaxe with\nthe cobblestone and sticks, as this is more durable and efficient for mining. 8. Construct a furnace using the\ncobblestone, which will be required to smelt ores into ingots. 9. Mine 3 iron ore with the stone pickaxe. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks, as this is necessary to mine diamonds. 12. Find and mine diamond ore with the iron pickaxe\nto gather 3 diamonds. 13. Finally, use the crafting table to craft a diamond pickaxe with the diamonds and\nsticks, completing the task.\nPlan 3 1. Begin by locating trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine diamond ore with the iron pickaxe to obtain 3 diamonds. 13. Finally, use the crafting\ntable to craft a diamond pickaxe with the diamonds.\nTable 12. Open-ended instruction examples of “Craft a compass”\nPlanner Instruction\nGPT4V\nPlan 1 1. Locate a tree and gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks from the\nplanks. 4. Use some planks to craft a crafting table. 5. With the crafting table, craft a wooden pickaxe. 6.\nMine cobblestone until you have 12 pieces. 7. Upgrade your tools by crafting a stone pickaxe. 8. Construct\na furnace using some of the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron\nore in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots. 12. Find and\nmine redstone using the iron pickaxe to obtain 1 redstone dust. 13. Finally, use the remaining iron ingots\nand redstone dust at the crafting table to craft a compass.\nPlan 2 1. Start by finding and chopping down trees to collect 10 logs. 2. Use the logs to craft 38 planks. 3.\nCraft sticks using the planks to get 8 sticks. 4. Craft a crafting table from the planks. 5. Use the crafting\ntable to create a wooden pickaxe. 6. Locate and mine 12 pieces of cobblestone using the wooden pickaxe.\n7. Craft a stone pickaxe from cobblestone and sticks. 8. Use the cobblestone to craft a furnace. 9. Mine\n7 iron ores using the stone pickaxe. 10. Smelt the iron ore in the furnace to get 7 iron ingots. 11. Craft\nan iron pickaxe using iron ingots and sticks. 12. Search for and mine redstone using the iron pickaxe. 13.\nFinally, use the iron ingots and redstone to craft a compass.\nPlan 3 1. First, I need to find a tree and collect 10 logs. 2. Convert the logs into planks, ensuring I get at\nleast 38 planks. 3. Use some of the planks to craft sticks, collecting at least 8 sticks. 4. Use some of the\nplanks to create a crafting table. 5. Craft a wooden pickaxe. 6. Locate cobblestone and mine at least 12\npieces using the wooden pickaxe. 7. Use some cobblestone to craft a furnace. 8. Craft a stone pickaxe for\nimproved mining. 9. Search for iron ore and mine at least 7 using the stone pickaxe. 10. Smelt the iron ore\nin the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using some of the iron ingots and sticks.\n12. Locate a redstone ore and mine it using the iron pickaxe to obtain redstone. 13. Finally, use 4 of the\niron ingots and the redstone to craft a compass at the crafting table.\nGLM4V\nPlan 1 1. Locate and gather 10 logs from nearby trees. 2. Convert the logs into 38 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to access more crafting options. 5. Use\nthe crafting table to make a wooden pickaxe. 6. With the wooden pickaxe, mine 12 cobblestones from\nstone. 7. Craft a stone pickaxe using the cobblestones and the crafting table. 8. Construct a furnace to smelt\nmaterials. 9. Use the stone pickaxe to mine 7 iron ores from the appropriate rock formations. 10. Smelt\nthe 7 iron ores in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe with the iron ingots at the\ncrafting table. 12. Mine for redstone, which can sometimes be found in the ground. 13. Use the crafting\ntable to craft a compass with the iron ingot and redstone.\nPlan 2 1. Locate and chop down trees to gather the required 10 logs. 2. Convert the logs into 38 planks\nusing the crafting menu. 3. Craft 8 sticks from the planks. 4. Use some planks to craft a crafting table.\n5. With the crafting table and the sticks and planks crafted, make a wooden pickaxe. 6. Use the wooden\npickaxe to mine 12 cobblestone. 7. Craft a stone pickaxe using the cobblestone. 8. Construct a furnace\nwith the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron ore in the furnace to\nproduce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots and the crafting table. 12. Search for\nredstone; gather at least 1 redstone. 13. Finally, use the crafting table to combine the redstone with the iron\ningots to craft a compass.\nPlan 3 1. Find and chop down trees to gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks\nfrom the planks. 4. Craft a crafting table using some of the planks. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine cobblestone until you have at least 12. 7. Upgrade the\nwooden pickaxe to a stone pickaxe using the cobblestone. 8. Construct a furnace out of the cobblestone. 9.\nUse the stone pickaxe to mine 7 iron ore. 10. Smelt the 7 iron ore in the furnace to produce 7 iron ingots.\n11. Craft an iron pickaxe using the iron ingots and sticks at the crafting table. 12. Search for redstone; you\nwill need at least 1 piece. 13. Finally, use the crafting table to combine the redstone with the iron ingots to\ncraft a compass.\nTable 13. The results of Optimus-2 on the Wood Group, Stone Group, and Iron Group. SR denotes success rate.\nGroup\nTask\nSub-Goal Num.\nSR\nEval Times\nWood\nCraft a wooden shovel\n6\n100.00\n40\nCraft a wooden pickaxe\n5\n100.00\n30\nCraft a wooden axe\n5\n97.37\n38\nCraft a wooden hoe\n5\n100.00\n30\nCraft a stick\n4\n100\n30\nCraft a crafting table\n3\n93.02\n43\nCraft a wooden sword\n5\n100.00\n30\nCraft a chest\n4\n100.00\n30\nCraft a bowl\n4\n100.00\n30\nCraft a ladder\n4\n100.00\n30\nStone\nCraft a stone shovel\n8\n89.47\n57\nCraft a stone pickaxe\n10\n98.00\n50\nCraft a stone axe\n10\n94.44\n54\nCraft a stone hoe\n8\n95.74\n47\nCraft a charcoal\n9\n85.71\n42\nCraft a smoker\n9\n90.00\n40\nCraft a stone sword\n8\n95.45\n44\nCraft a furnace\n9\n94.44\n36\nCraft a torch\n8\n89.36\n47\nIron\nCraft an iron shovel\n13\n52.08\n48\nCraft an iron pickaxe\n13\n56.00\n50\nCraft an iron axe\n13\n48.15\n54\nCraft an iron hoe\n13\n56.60\n53\nCraft a bucket\n13\n45.10\n51\nCraft a hopper\n14\n54.90\n51\nCraft a rail\n13\n51.02\n49\nCraft an iron sword\n12\n56.52\n46\nCraft a shears\n12\n48.28\n58\nCraft a smithing table\n12\n53.33\n45\nCraft a tripwire hook\n13\n55.56\n45\nCraft a chain\n13\n52.17\n46\nCraft an iron bars\n12\n51.06\n47\nCraft an iron nugget\n12\n54.55\n44\nCraft a blast furnace\n14\n52.27\n44\nCraft a stonecutter\n13\n52.27\n44\nTable 14. The results of Optimus-2 on the Gold group, Diamond Group, Redstone Group, and Armor Group. SR denotes success rate.\nGroup\nTask\nSub Goal Num.\nSR\nEval Times\nGold\nCraft a golden shovel\n16\n8.93\n56\nCraft a golden pickaxe\n16\n11.29\n62\nCraft a golden axe\n16\n8.93\n56\nCraft a golden hoe\n16\n8.96\n67\nCraft a golden sword\n16\n8.20\n61\nSmelt and craft an golden ingot\n15\n9.68\n62\nDiamond\nCraft a diamond shovel\n15\n15.91\n44\nCraft a diamond pickaxe\n15\n11.76\n34\nCraft a diamond axe\n16\n11.00\n36\nCraft a diamond hoe\n15\n15.91\n44\nCraft a diamond sword\n15\n11.11\n36\nDig down and mine a diamond\n15\n11.42\n35\nCraft a jukebox\n15\n13.15\n38\nRedstone\nCraft a piston\n16\n28.33\n60\nCraft a redstone torch\n16\n27.69\n65\nCraft an activator rail\n18\n25.81\n62\nCraft a compass\n23\n28.36\n67\nCraft a dropper\n16\n30.30\n66\nCraft a note block\n16\n25.40\n63\nArmor\nCraft shield\n14\n45.16\n62\nCraft iron chestplate\n14\n43.86\n57\nCraft iron boots\n14\n40.35\n57\nCraft iron leggings\n14\n8.57\n35\nCraft iron helmet\n14\n47.46\n56\nCraft diamond helmet\n17\n9.09\n33\nCraft diamond chestplate\n17\n7.89\n38\nCraft diamond leggings\n17\n5.41\n37\nCraft diamond boots\n17\n12.50\n40\nCraft golden helmet\n17\n13.89\n36\nCraft golden leggings\n17\n12.20\n41\nCraft golden boots\n17\n10.26\n39\nCraft golden chestplate\n17\n10.00\n40\nAgent\nInstruction: I want to get some logs to craft wooden sword, what should I do first?\nSuccess\n❌\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n✔\nFigure 12. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to get some logs\nto craft wooden sword, what should I do first?”. Existing policies are limited by their instruction comprehension abilities and thus fail to\ncomplete the task, whereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I need coal for heating. What should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 13. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need coal for heating.\nWhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I want to collect some seeds, Can you help me?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 14. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to collect some\nseeds, Can you help me?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task,\nwhereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nOptimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy\n```\n#### 2. 论文摘要\n```\nBuilding an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n```\n\n#### 3. 论文全文\n```\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nZaijing Li1 2, Yuquan Xie1, Rui Shao1*, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1*\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/\nAbstract\nBuilding an agent that can mimic human behavior pat-\nterns to accomplish various open-world tasks is a long-\nterm goal.\nTo enable agents to effectively learn behav-\nioral patterns across diverse tasks, a key challenge lies\nin modeling the intricate relationships among observa-\ntions, actions, and language.\nTo this end, we propose\nOptimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-\nlevel planning, alongside a Goal-Observation-Action Con-\nditioned Policy (GOAP) for low-level control. GOAP con-\ntains (1) an Action-guided Behavior Encoder that models\ncausal relationships between observations and actions at\neach timestep, then dynamically interacts with the histori-\ncal observation-action sequence, consolidating it into fixed-\nlength behavior tokens, and (2) an MLLM that aligns be-\nhavior tokens with open-ended language instructions to pre-\ndict actions auto-regressively. Moreover, we introduce a\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset, which contains 25,000 videos across 8 atomic\ntasks, providing about 30M goal-observation-action pairs.\nThe automated construction method, along with the MGOA\ndataset, can contribute to the community’s efforts to train\nMinecraft agents. Extensive experimental results demon-\nstrate that Optimus-2 exhibits superior performance across\natomic tasks, long-horizon tasks, and open-ended instruc-\ntion tasks in Minecraft.\nPlease see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n1. Introduction\nEnabling agents to learn human behavioral patterns for\ncompleting complex tasks in open-world environments, is\na long-standing goal in the field of artificial intelligence\n[5, 23, 34, 47]. To effectively handle diverse tasks in an\n*Corresponding authors\nTransformer\nXL\nGoal \nEncoder\nkeyboard: W\nmouse: [0.0, 1.0]\n⊕\nLLM\nMLLM as Planner\nVision \nEncoder\nCausal \nPerceiver \nHistorical Sequence\nHistory\nAggregator \nAction-guided Behavior Encoder\nExsiting Goal-conditioned Policy\nOurs\n  1. chop a tree\n  2. craft four planks\n  3. craft two sticks\n  4. craft a wood sword\nSub-goals\nGoal-conditioned\nPolicy\nAction\nor\nsub-goal: chop a tree\nsub-goal: chop a tree\nI need a \nwooden \nsword\nVision \nEncoder\n...\nFigure 1.\nLeft: General agent framework.\nRight: Compari-\nson between existing goal-conditioned policies and ours. Existing\nTransformer-XL-based policies [3, 25] exhibit limited natural lan-\nguage understanding capabilities and rely solely on combining im-\nplicit goal embeddings with visual embeddings as inputs. In con-\ntrast, our GOAP achieves superior action prediction by 1) employ-\ning an Action-guided behavior encoder to strengthen causal mod-\neling between observations and actions, as well as to improve his-\ntorical sequence modeling capabilities, and 2) leveraging MLLM\nto enhance open-ended language comprehension.\nopen-world environment like Minecraft [20, 32], a promi-\nnent agent framework [24, 32, 41, 42] integrates a task\nplanner with a goal-conditioned policy. As illustrated in\nFigure 1 (left), this framework first utilizes the task plan-\nner’s language comprehension and visual perception abili-\nties to decompose complex task instructions into sequential\nsub-goals. These sub-goals are then processed by a goal-\nconditioned policy to generate actions.\nAlthough existing agents [24, 32, 42] have made promis-\ning progress by using Multimodal Large Language Models\n(MLLM) [4, 37, 45] as planners, the current performance\nbottleneck for agents lies in the improvement of the goal-\narXiv:2502.19902v2  [cs.AI]  11 Mar 2025\nconditioned policy [24]. As the sub-goal serves as a natu-\nral language description of an observation-action sequence,\nthe goal-conditioned policy needs to learn the crucial re-\nlationships among sub-goals, observations, and actions to\npredict actions. However, existing goal-conditioned poli-\ncies exhibit the following limitations: (1) Existing policies\nneglect the modeling of the relationship between observa-\ntions and actions. As shown in Figure 1, they only model\nthe relationship between the sub-goal and the current ob-\nservation by adding the sub-goal embedding to the obser-\nvation features [3, 25, 43]. However, the current observa-\ntion is generated by the previous action interacting with the\nenvironment. This implies a causal relationship between\naction and observation, which is neglected by current poli-\ncies; (2) Existing policies struggle to model the relation-\nship between open-ended sub-goals and observation-action\nsequences. As depicted in Figure 1, existing policies pri-\nmarily rely on either video encoders [3, 43] or conditional\nvariational autoencoders (CVAE) [25] as goal encoder to\nproduce implicit goal embeddings. Such embeddings have\nlimited representation ability [43]. Simply adding it to ob-\nservation features is sub-optimal and unable to handle the\ncomplex relationship between sub-goals and observation-\naction sequences.\nIn this paper, we propose Optimus-2, a novel agent that\nincorporates an MLLM for planning, alongside a Goal-\nObservation-Action Conditioned Policy (GOAP). To ad-\ndress the aforementioned challenges, we propose GOAP,\nwhich can better model the relationship among the obser-\nvations, actions, and sub-goals in two aspects.\nAn Action-guided Behavior Encoder for observation-\naction sequence modeling. To capture the relationship be-\ntween observations and actions, the Action-guided Behav-\nior Encoder first employs a Causal Perceiver to integrate ac-\ntion embeddings into observation features. It utilizes task-\nrelevant action information as guidance to adjust the obser-\nvation features, thereby providing fine-grained observation-\naction information for action prediction. Additionally, to\nmodel a long-term observation-action sequence without ex-\nceeding input length limitations, a History Aggregator is\nintroduced to dynamically integrate current observation-\naction information with the historical sequence into fixed-\nlength behavior tokens. Behavior tokens can capture the\nlong-term dependencies of the observation-action sequence\nwith a fixed and appropriate length. It enables the agent to\npredict actions that align with the logic of the observation-\naction sequence, rather than making isolated action predic-\ntions based solely on the current observation.\nAn MLLM to model the relationship between sub-\ngoal and observation-action sequence. To explicitly en-\ncode the semantics of sub-goals, we introduce an MLLM as\nthe backbone of GOAP. It aligns the sub-goal with behav-\nior tokens to predict subsequent actions auto-regressively.\nLeveraging the MLLM’s language comprehension and mul-\ntimodal perception capabilities, it can better integrate fea-\ntures from open-ended sub-goals and observation-action\nsequences, thereby enhancing the policy’s action predic-\ntion ability. To the best of our knowledge, GOAP is the\nfirst effort to employ MLLM as the core architecture of\na Minecraft policy, which demonstrates strong instruction\ncomprehension capabilities for open-ended sub-goals.\nMoreover, current Minecraft datasets either lack align-\nment among essential elements [10] or are not publicly\naccessible [1], resulting in a significant scarcity of high-\nquality observation-goal-action pairs necessary for policy\ntraining. To this end, we introduce an automated approach\nfor constructing the Minecraft Goal-Observation-Action\n(MGOA) dataset. The MGOA dataset comprises 25,000\nvideos across 8 atomic tasks, providing approximately 30\nmillion aligned observation-goal-action pairs.\nIt will be\nmade openly available to support advancements within\nthe research community.\nWe conducted comprehensive\nevaluations in the open-world environment of Minecraft,\nand the experimental results demonstrate that Optimus-\n2 achieves superior performance.\nCompared to previous\nSOTA, Optimus-2 achieves an average improvements of\n27%, 10%, and 18% on atomic tasks, long-horizon tasks,\nand open-ended sub-goal tasks, respectively.\nIn summary, our contributions are as follows:\n• We propose a novel agent Optimus-2, which consists of\nan MLLM for planning, and a policy for low-level con-\ntrol. The experimental results demonstrate that Optimus-\n2 exhibits superior performance on atomic tasks, long-\nhorizon tasks, and open-ended sub-goal tasks.\n• To better model the relationship among the observations,\nactions, and sub-goals, we propose Goal-Observation-\nAction Conditioned Policy, GOAP. It contains an Action-\nguided Behavior Encoder for observation-action se-\nquence modeling, and an MLLM to model the relation-\nship between sub-goal and observation-action sequence.\n• To address the scarcity of large-scale, high-quality\ndatasets, we introduce the MGOA dataset. It comprises\napproximately 30 million aligned observation-goal-action\npairs and is generated through an automated process with-\nout any manual annotations. The proposed dataset con-\nstruction method and the released MGOA dataset can\ncontribute to the community’s efforts to train agents.\n2. Related Work\nMinecraft Agents. Previous works [2, 8, 13, 31] have con-\nstructed policies in Minecraft using reinforcement learn-\ning or imitation learning. VPT [1] was training on large-\nscale video data recorded by human players, using behavior\ncloning to mimic human behavior patterns. GROOT [3] em-\nploys a video encoder as a goal encoder to learn semantic in-\nformation from videos. However, these policies rely solely\nLarge Language Model\nAction Head\n�t+1\n ViT\nLlama Tokenizer\nSub-goal: \nChop a tree.\nImage Token\nFFN\nCross-Attention\nV\nK\nQ\n...\nCross-Attention\nHistory-Attention\nV\nK\nQ\nV\nK\nQ\nText Token\nAction Token\nBehavior Token\nCausal Perceiver \nHistory\nAggregator \nAction-guided Behavior Encoder\nHistorical Memory Bank \n...\nAction-guided\nBehavior Encoder\n�t\n�t\n�1\n�t−3\n�t−2\n�t−1\n�t+1\n��\n...\n...\n�t\n...\n�t\n�1\n�2\n�3\n��−1\nLoRA\nMLLM-based\nPlanner\nTask: I need a \nwooden sword.\nFigure 2. Overview of Optimus-2. Given a task and the current observation, Optimus-2 first uses an MLLM-based Planner to generate a\nseries of sub-goals. Optimus-2 then sequentially executes these sub-goals through GOAP. GOAP obtains behavior tokens for the current\ntimestep via the Action-guided Behavior Encoder, and these behavior tokens, along with image and text tokens, are fed into the LLM to\npredict subsequent actions.\non visual observations as input and cannot follow human\ninstructions to accomplish specific tasks. MineCLIP [10]\nintroduces a video-text contrastive learning module as a re-\nward model for policy, and STEVE-1 [25] builds on VPT\n[1] by incorporating MineCLIP as goal encoder, enabling\npolicy to follow natural language instructions. Despite these\nadvancements, these policies are constrained by language\nunderstanding and reasoning capabilities. To address this,\ncurrent agents [20, 24, 32, 40, 42, 43] leverage MLLM’s\ninstruction following capabilities to decompose complex\ntasks into executable sub-goal sequences, which are then\nfed into a goal-conditioned policy [3, 25] or formed as exe-\ncutable code [26, 28, 51, 52]. Despite significant progress,\nthe performance of current policies remains constrained by\ntheir limited ability to understand sub-goals. In this paper,\nwe aim to develop an MLLM-based goal-conditioned pol-\nicy to enhance the policy’s comprehension of open-ended\nsub-goals, thereby improving overall performance.\nLong-term Video Modeling. Previous work [1, 3, 10, 25]\nhave segmented videos into multiple clips for training to\nalleviate the challenges posed by long-sequence video in-\nputs. However, this approach prevents the agent from learn-\ning comprehensive behavior representations from the entire\nvideo. To handle long-term video sequences [22, 48, 49],\nexisting studies employ temporal pooling [30], querying\ntransformers [14, 46], or token merging [16, 38, 50] to\nintegrate long-sequence visual tokens. Inspired by previ-\nous works [6, 18, 19, 44], we propose a Q-former [7, 21]\nstructure with a memory bank [14], enabling effective long-\nterm sequence modeling through interactions with histori-\ncal queries. Unlike existing methods that model only the\nobservation sequence, we focus on multimodal learning\n[33, 35, 36]. Moreover, different from previous work [14]\nthat primarily compress video features into fixed-length to-\nkens, our Action-guided Behavior Encoder dynamically in-\nteracts with the historical sequence at each timestep, pro-\nducing behavior tokens corresponding to the observation-\naction sequence from the start to the current timestep.\n3. Preliminaries and Problem Formulation\nIn Minecraft, agents [1, 3, 25] exhibit behavior patterns\nsimilar to humans: at each time step t, the agent receives\na visual observation ot and generates control actions at+1\nusing the mouse and keyboard.\nThese actions interact\nwith the environment, resulting in a new visual observa-\ntion ot+1.\nThrough continuous interactions, a trajectory\nJ = {(o1, a1), (o2, a2), (o3, a3), . . . , (oT , aT )} is formed,\nwhere T represents the length of the trajectory. Previous\nwork primarily trained Minecraft agents using reinforce-\nment learning [10] or behavior cloning [3, 25]. For exam-\nple, in behavior cloning, the goal of the policy pθ(at+1|o1:t)\nis to minimize the negative log-likelihood of the actions at\neach time step t given the trajectory J. Considering that\nsuch trajectories are typically generated under explicit or\nimplicit goals, many recent approaches condition the be-\nhavior on a (implicit or explicit) goal g and learn goal-\nconditioned policy pθ(at+1|o1:t, g) [3, 25]. Generally, for\nboth agents and humans, the explicit goal g is a natural lan-\nguage instruction.\nFormally, given a trajectory J with length T, standard\nbehavior cloning trains the policy pθ(·) with parameters θ\nby minimizing the negative log-likelihood of actions:\nmin\nθ\nT\nX\nt=1\n−log pθ(at+1|o1:t, g)\n(1)\n4. Optimus-2\nIn this section, we first give an overview of our proposed\nagent framework, Optimus-2. As shown in Figure 1 (left),\nit includes a planner for generating a series of executable\nsub-goals and a policy that sequentially executes these sub-\ngoals to complete the task.\nNext, we introduce how to implement Optimus-2’s plan-\nner (Sec. 4.1). Subsequently, we elaborate on how to imple-\nment the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we\nintroduce an automated dataset generation method to obtain\na high-quality Minecraft Goal-Observation-Action dataset\n(MGOA) for training GOAP.\n4.1. MLLM-based Task Planner\nIn Minecraft, a complex task consists of multiple interme-\ndiate steps, i.e., sub-goals. For example, the task “I need\na wooden pickaxe” includes five sub-goals: ‘chop a tree to\nget logs\n’, ‘craft four planks\n’, ‘craft a crafting table\n’, ‘craft two sticks\n’, and ‘craft a wooden pickaxe\n’.\nTherefore, a planner is essential for the agent, as it needs to\ndecompose the given complex task into a sequence of exe-\ncutable sub-goals for the policy to execute sequentially. In\nthis paper, we follow Li et al. [24], employing an MLLM\nas the planner, which takes current observation and task in-\nstruction as input to generate sub-goals.\n4.2. Goal-Observation-Action Conditioned Policy\nAccording to Sec 3., a key insight into the relationship\namong observation o, action a, and sub-goal g is: that the\nobservation o and action a at the same time step have a\ncausal relationship; and the sub-goal g is a natural language\ndescription of the observation-action sequence over a cer-\ntain time. To better model the relationships among the three\nelements mentioned above, we propose first integrating the\nrepresentations of observation and action at each time step,\nthen modeling the observation-action sequences along the\ntemporal dimension, and finally aligning the observation-\naction sequences with the sub-goal for action prediction.\nMotivated\nby\nthis,\nwe\npropose\na\nnovel\nGoal-\nObservation-Action\nconditioned\nPolicy,\nGOAP.\nAs\nshown in Figure 2, our GOAP consists of an Action-guided\nBehavior Encoder that dynamically models observation-\naction sequences into fixed-length behavior tokens and an\nTable 1. Comparison of the MGOA dataset with existing datasets.\nO, G, and A represent observation, goal, and action. VPT† in-\ndicates the amount of data that is openly accessible. MineCLIP‡\ndenotes narrated Minecraft videos available on YouTube.\nFormat\nDataset\nO\nG\nA\n# Frames\nImage-Text Pairs\nMP5 [32]\n!\n!\n500K\nOmniJARVIS [43]\n!\n!\n!\n600K\nGameplay Video\nVPT† [1]\n!\n!\n6M\nMineCLIP‡ [10]\n!\n!\n20B\nSTEVE-1 [25]\n!\n!\n!\n32K\nMGOA (Ours)\n!\n!\n!\n30M\nMLLM that aligns such behavior tokens with sub-goal for\naction prediction.\n4.2.1. Action-guided Behavior Encoder\nPrevious policies often overlook the causal relationship be-\ntween observation and action at each timestep. Moreover,\nit remains a challenge to model the long-term observation-\naction sequence without exceeding input length constraints.\nTo this end, we propose an Action-guided Behavior En-\ncoder that integrates the representations of observation and\naction at each time step and then dynamically models the\nhistorical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation ot into a\nvisual encoder VE to obtain the visual features:\nvt ←VE(ot)\n(2)\nwhere vt ∈RP ×d, P is the number of patches for each im-\nage, and d is the dimension of the extracted image feature.\nIn practice, we employ ViT [9] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model\nthe relationship between observations and actions. It takes\nthe visual feature vt as query tokens and the action embed-\nding at as key and value. The module then constructs the\ninformation interaction between action at and vt through a\ncross-attention mechanism:\nQ = vtW Q\nv , K = atW K\na , V = atW V\na\n(3)\nˆvt = CrossAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(4)\nwhere W Q\nv , W K\na , and W V\na represent the weight matrices\nfor the query (Q), key (K), and value (V), respectively.\nCrossAttn(·) denotes the cross-attention layer, and d is\nthe dimension of the image features. In this way, it explic-\nitly assigns action information at at time step t to the visual\nfeatures ˆvt, enhancing the causal relationship between ob-\nservations and actions.\nSubsequently, we introduce a History Aggregator mod-\nule to capture the information of the observation-action se-\nquence along the temporal dimension, serving as the behav-\nior representation. At each timestep t, behavior tokens Bt\nserve as queries, while the sequence of historical behavior\ntokens Ht = [B1, B2, . . . , Bt−1] acts as keys and values.\nThe current behavior tokens interact with the historical se-\nquence through a history-attention layer HisAttn(·):\nˆBt = HisAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(5)\nwhere Q, K, and V are calculated similarly to Eq 3.\nFinally, another cross-attention layer is introduced, using\nthe behavior tokens ˆBt as queries, and the visual features ˆvt\nas keys and values. In this way, the behavior tokens incor-\nporate the current observation-action information. Follow-\ning the approach of He et al. [14], we construct a memory\nbank for historical behavior tokens Ht, utilizing the simi-\nlarity between adjacent features to aggregate and compress\nthe behavior tokens. This method not only preserves early\nhistorical information but also keeps the historical behav-\nior token sequence Ht at a fixed length to reduce compu-\ntational costs. Leveraging the Action-guided Behavior En-\ncoder, we obtain behavior tokens ˆBt, which correspond to\nthe observation-action sequence from the start to the current\ntime step t.\n4.2.2. MLLM Backbone\nTo model the relationship between the sub-goal and\nobservation-action sequence, we introduce an MLLM that\ntakes the sub-goal g, current observation features vt, and\nbehavior tokens Bt as input to predict subsequent actions\nauto-regressively. To enable the MLLM backbone MLLM\nto predict low-level actions, we employ VPT [1] as action\nhead AH to map output embeddings ¯at+1 of language model\ninto the action space.\n¯at+1 ←MLLM([g, vt, Bt])\n(6)\nat+1 ←AH(¯at+1)\n(7)\nFormally, given a dataset D = {(o1:T , a1:T )}M with M\ncomplete trajectories, we train GOAP to learn the behavior\ndistribution from D via behavioral cloning. Moreover, we\nintroduce a KL-divergence loss to measure the output dis-\ntribution similarity between GOAP and VPT [1]. This helps\nour model effectively learn the knowledge from the teacher\nmodel VPT. The training loss can be formulated as follows:\nLθ = λBC\nT\nX\nt=1\n−log pθ(at+1|o1:t, a1:t, g)\n+λKL\nT\nX\nt=1\nDKL(qϕ(at+1|o1:t) ∥pθ(at+1|o1:t, g))\n(8)\nwhere λBC and λKL are trade off coefficients, pθ is the\nGOAP, qϕ is the teacher model.\nTable 2. Main Result of GOAP on Atomic Tasks. We report the\naverage rewards of each task.\nPolicy\nLogs\nSeeds\nDirt\nStone\nVPT (text) [1]\n2.6\n0.8\n9.2\n0.0\nSTEVE-1 [25]\n11.0\n5.1\n10.0\n3.2\nGROOT [3]\n14.3\n7.3\n19.7\n19.0\nFSQ GROOT [43]\n10.8\n8.2\n20.3\n5.8\nGOAP [MLP ]\n7.2\n4.3\n14.4\n15.5\nGOAP [V P T ]\n15.0\n8.5\n26.7\n25.7\n4.3. MGOA Dataset\nIn Minecraft, there remains a significant lack of high-\nquality goal-observation-action pairs to support behavior\ncloning training.\nPrevious work has primarily relied on\ngameplay videos as training data. These datasets either lack\nnatural language instructions (explicit goals) [1, 3], or use\nactions predicted by IDM models [1] for each observation\nas pseudo-labels [1, 25], which leads to a risk of misalign-\nment between observations and actions. Inspired by Li et\nal. [24], we propose an automated data generation pipeline\nthat enables the creation of aligned goal-observation-action\npairs without the need for manual annotations or human\ncontractors. First, we utilize existing agents [25], provid-\ning them with clear natural language instructions to attempt\ntask completion in Minecraft. We then record the actions\nand corresponding observations during goal execution, gen-\nerating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply\nthe following filtering criteria: 1) only recording videos\nin which the task is successfully completed, and 2) dis-\ncarding videos where task execution takes an excessive\namount of time.\nFor more details, please refer to Sup.\nC. Through this automated approach, we obtained 25k\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset. A comparison of the MGOA dataset with the exist-\ning Minecraft datasets is shown in Table 1. Our automated\ndata generation pipeline offers several key advantages: 1)\nit enables the generation of aligned goal-observation-action\npairs without the need for manual annotation or pseudo-\nlabeling; 2) its construction process is parallelizable, allow-\ning for rapid dataset generation; and 3) it leverages local\nagents for data generation, resulting in low-cost production.\n5. Experiments\n5.1. Experiments Setting\nEnvironment. Following [1, 25], we conduct experiments\nin the complex, open-world environment of Minecraft on\nthe MineRL [12] platform. The agent interacts with the\nMineRL environment at 20 frames per second, generating\nlow-level control signals for the mouse and keyboard. For\neach task execution, the agent is initialized in a randomized\nTable 3. Main Result of Optimus-2 on Long-horizon Tasks. We report the average success rate (SR) on each task group, the results of each\ntask can be found in the Sup. F.1. Pure GPT-4V† denotes the use of GPT-4V in a zero-shot manner to generate executable sub-goals for\nthe policy. Human‡ denotes the human-level baseline, with results sourced from previous work [24].\nMethod\nPolicy\nWood\nStone\nIron\nGold\nDiamond\nRedStone\nArmor\nPure GPT-4V †\nVPT (text)\n0.22\n0.08\n0.00\n0.00\n0.00\n0.00\n0.00\nSTEVE-1\n0.41\n0.20\n0.00\n0.00\n0.00\n0.00\n0.00\nGOAP\n0.50\n0.31\n0.12\n0.02\n0.01\n0.03\n0.03\nDEPS [41]\nSTEVE-1\n0.77\n0.48\n0.16\n0.00\n0.01\n0.00\n0.10\nJarvis-1 [42]\nSTEVE-1\n0.93\n0.89\n0.36\n0.07\n0.08\n0.16\n0.15\nOptimus-1 [24]\nSTEVE-1\n0.98\n0.92\n0.46\n0.08\n0.11\n0.25\n0.19\nOptimus-2\nGOAP\n0.99\n0.93\n0.53\n0.09\n0.13\n0.28\n0.21\nHuman‡ [24]\n-\n1.00\n1.00\n0.86\n0.17\n0.16\n0.33\n0.28\nTable 4.\nMain Result of GOAP on Open-Ended Instruction\nTasks. We report the average success rate (SR) on Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nGROOT [3] and FSQ GROOT [43] were not included as base-\nlines, as they are unable to process language input.\nPlanner\nPolicy\nGLM-4V\nVPT (text)\n0.05\n0\n0\n0\n0\nSTEVE-1\n0.60\n0\n0\n0\n0\nGOAP\n0.71\n0.39\n0.11\n0.14\n0.13\nGPT-4V\nVPT (text)\n0.11\n0\n0\n0\n0\nSTEVE-1\n0.66\n0.10\n0\n0\n0\nGOAP\n0.75\n0.47\n0.13\n0.16\n0.17\nenvironment, allowing us to evaluate the agent’s generaliza-\ntion across diverse environments. Please refer to Sup. B for\nmore details about the Minecraft environment.\nImplementation details. For the planner, we follow Li et\nal. [24], using a hybrid multimodal memory empowered\nGPT-4V 1 as the agent’s planner. As for the policy, we ini-\ntialize GOAP with the weights of DeepSeek-VL-1.3B [29]\nas initialization. We train it on the MGOA dataset and the\npublicly available OpenAI Contractor Dataset [1] through\nbehavior cloning. All experiments were conducted on 8x\nNVIDIA L40 GPUs. Training details and hyperparameter\nsetting can be found in Sup. D.\nEvaluation Tasks & Metrics. Evaluation tasks are catego-\nrized into three types: Atomic Tasks, Long-Horizon Tasks,\nand Open-Ended Instruction Tasks. For each task, the en-\nvironment is randomly reinitialized on each attempt, with a\nminimum of 30 executions per task to ensure robustness.\n• Atomic Tasks represent short-term skills in Minecraft. We\nselect “chop a tree to get logs\n”, “collect seeds\n”,\n“collect dirt\n”, and “mine stone\nwith a pickaxe” as\nevaluation tasks. These tasks evaluate the policy’s basic\ncapabilities in Minecraft. We report the average rewards\n1https:\/\/openai.com\/index\/gpt-4v-system-card\n(number of items obtained) per task execution as an eval-\nuation metric.\n• Long-horizon Tasks consist of an interdependent atomic\ntasks sequence, where the failure of any single atomic\ntask results in the failure of the entire sequence. These\nlong-horizon tasks are designed to evaluate the agent’s ca-\npability to execute a series of diverse tasks continuously\nwithin a complex environment. We follow the setup of Li\net al. [24], conducting experiments on long-horizon tasks\ncomprising 67 tasks grouped into 7 categories. We report\nthe average Success Rate (SR) as an evaluation metric.\n• Open-Ended Instruction Tasks are not limited to prede-\nfined text formats; rather, they involve flexible language\ndirectives that prompt the agent to accomplish long-\nhorizon tasks. These tasks evaluate the agent’s capacity\nto interpret and execute instructions expressed in open-\nended natural language. We selected the Torch\nfrom\nthe Stone Group, Rail\nfrom the Iron Group, Golden\nShovel\nfrom the Gold Group, Diamond Pickaxe\nfrom the Diamond Group, and Compass\nfrom the Red-\nstone Group as evaluation tasks. Given a crafting rela-\ntionship graph, we instructed GPT-4V and GLM-4V [11]\nto generate five open-ended instructions for each task.\nThis allows us to evaluate the policies’ understanding and\nexecution capabilities regarding open-ended instructions.\nTask instructions are provided in the Sup. E.1.\nBaseline. For Atomic Tasks and Open-ended Instruction\nTasks, we compare GOAP with existing goal-conditioned\npolicies, including VPT [1], STEVE-1 [25], GROOT [3]\nand FSQ GROOT [43]. For Long-horizon Tasks, we employ\nGPT-4V, DEPS [41], Jarvis-1 [42], and Optimus-1 [24] as\nbaselines. We also introduce a human-level baseline [24] to\nevaluate the performance gap between existing agents and\nhuman capabilities.\n5.2. Experimental Results\nThe experimental results for Optimus-2 compared to the\nbaselines across Atomic Tasks, Long-horizon Tasks, and\nAgent\nInstruction: I need some iron ores, what should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 3. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need some iron ores,\nwhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nTable 5. Ablation study of Action-guided Behavior Encoder on\nAtomic Tasks. We report average rewards on each task. CP., HA.,\nand MB. represent the Causal Perceiver, History Aggregator, and\nMemory Bank, respectively.\nAblation Setting\nAtomic Task\nCP.\nHA.\nMB.\nLogs\nSeeds\nDirt\nStone\nAverage\n!\n!\n!\n15.0\n8.5\n26.7\n25.7\n19.0\n6.1\n5.4\n12.7\n15.7\n10.0 (↓47.4%)\n!\n10.2\n4.7\n12.8\n21.1\n12.2 (↓35.8%)\n!\n!\n7.4\n6.2\n13.1\n15.5\n10.6 (↓44.2%)\n!\n!\n12.0\n6.8\n22.7\n16.8\n14.6 (↓23.2%)\nOpen-ended Instruction Tasks are presented in Table 2, Ta-\nble 3, and Table 4, respectively.\nGOAP excels in Atomic Tasks. Table 2 shows that pro-\nposed GOAP achieves improvements of 5%, 4%, 31%, and\n35% over the current SOTA on the Logs\n, Seeds\n, Dirt\n, and Stone\n, respectively. These results demonstrate\nthat GOAP has successfully mastered a range of short-term\nskills across diverse environments, and can acquire items\nmore effectively than existing policies.\nOptimus-2 surpasses SOTA in Long-horizon Tasks.\nTable 3 shows that Optimus-2 achieved the highest success\nrates across all seven task groups, particularly excelling in\nthe challenging Diamond Group and Redstone Group with\nsuccess rates of 13% and 28%, respectively. This indicates\nthat Optimus-2 has effectively learned complex behavior\npatterns across atomic tasks, enabling it to sequentially ex-\necute multiple sub-goals and successfully complete long-\nhorizon tasks within complex environments.\nGOAP\noutperforms\nin\nOpen-ended\nInstruction\nTasks. As shown in Table 4, GOAP achieved significantly\nhigher success rates than existing agents across all tasks.\nNotably, on the challenging tasks of Golden Shovel\n, Di-\namond Pickaxe\n, and Compass\n, existing policies fail to\ncomplete these tasks, whereas GOAP achieves success rates\nof 13%, 16%, and 17%, respectively. This advantage stems\nSuccess Rate\n0.0\n5.0\n10.0\n15.0\nGolden \nShovel\nDiamond \nPickaxe\nCompass\nLLM \n20.0\n17.1\n13.1\n16.2\nTransformer-XL\n0.5\n0.5\n0.0\nFigure 4. Ablation of LLM backbone on Open-ended Instruction\nTasks, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nfrom GOAP’s superior comprehension of open-ended nat-\nural language instructions, whereas existing agents exhibit\nweaker instruction-following capabilities. Moreover, Fig-\nure 3 illustrates an example of different policies executing\nan open-ended goal. Due to the limited representation ca-\npability of their goal encoders, VPT [1] and STEVE-1 [25]\nfail to understand the goal, “I need some iron ores, what\nshould I do?” In contrast, GOAP leverages the MLLM’s\nunderstanding of open-ended instructions to effectively ac-\ncomplish the goal (obtaining iron ore\n).\n5.3. Ablation Study\nThere are many unexplored questions around best practices\nfor developing MLLM-based policy in Minecraft. In this\nsection, we conduct an extensive ablation study and sum-\nmarize our key findings.\nThe Action-guided Behavior Encoder plays a crucial\nrole in task execution. As shown in Table 5, the removal\nof the Causal Perceiver leads to an average performance de-\ncline of 42% across all tasks, highlighting the importance\nof capturing the causal relationship between observations\nand actions. Moreover, eliminating the History Aggregator\nAverage Rewards\n0.0\n5.0\n10.0\n15.0\n25.0\n30.0\nLogs\nSeeds\nDirt\nStone\nOCD_MGOA_Mix \n20.0\n26.7\n25.7\n6.0\n15.0\n8.5\n2.3\n14.3\n2.6\nOCD\n2.3\n1.4\n17.1\n22.1\nMGOA\nFigure 5. Ablation study on Training data. OCD refers to the\nOpenAI Contractor Dataset [1]. We report the average rewards on\neach Atomic Task.\nand Memory Bank also results in an average performance\ndecline of 36% across all tasks. This emphasizes the cru-\ncial role of the History Aggregator in modeling observation-\naction sequences and the Memory Bank in dynamically\nstoring long-sequence information.\nLLM significantly enhances policy’s ability to under-\nstand open-ended instructions. As shown in Figure 4, re-\nplacing the LLM backbone with a Transformer-XL leads to\na noticeable decline in performance. We attribute this to\nthe LLM’s pretraining on large-scale textual corpora, which\nendows it with a robust comprehension of open-ended lan-\nguage, a capability that Transformer-XL lacks.\nA pretrained action head improves performance in\nMinecraft. As shown in Table 2, replacing VPT with a\n2-layer MLP projector as the action head leads to a no-\nticeable decline in Optimus-2’s performance. While MLP-\nbased action heads have shown promising results in other\ndomains [17, 27], this substitution is less effective in the\nMinecraft environment. We attribute this to VPT’s exten-\nsive pretraining on large-scale gameplay data, which equips\nit with substantial domain-specific knowledge critical for\neffective task execution in Minecraft.\nThe MGOA datsaset is beneficial for training GOAP.\nWe conducted comparative experiments to evaluate the im-\npact of different training datasets on performance.\nAs\nshown in Figure 5, training only with the current most com-\nmonly used dataset, OpenAI Contractor Dataset (OCD), re-\nsults in suboptimal performance for GOAP on all Atomic\nTasks.\nFor example, compared to training with a mixed\ndataset, its performance on Stone\ndropped by 89%. We\nattribute this to the fact that OCD offers a wide variety of\ntasks but lacks high data quality.\nIn contrast, using our\nMGOA dataset, performance on the four atomic tasks im-\nproved by an average of 70% compared to using only the\nOCD data. We attribute this to the fact that MGOA contains\nhigh-quality aligned goal-observation-action pairs, which\nis beneficial for policy training. Further, we mix the two\ndatasets to train the policy in order to balance task diversity\nLog\nDirt\nSeed\nStone\n(a) ViT\n(c) Action-guided\n          Behavior Encoder\n(b) MineCLIP\nFigure 6. t-SNE visualization of representations extracted by (a)\nViT (b) MineCLIP and (c) Action-guided Behavior Encoder across\nAtomic Tasks. The visualization results show that the represen-\ntations in (a) and (b) cannot distinguish between different tasks,\nwhereas our Action-guided Behavior Encoder clearly differenti-\nates the behavior representations for the four tasks.\nand data quality, leading to improved performance.\n5.4. Visualization of Behavior Representation\nAs shown in Figure 6, we apply t-SNE [39] to visualize\nobservation features extracted by ViT [9], MineCLIP [10],\nand the Action-guided Behavior Encoder for four tasks.\nFrom (a) and (b) in Figure 6, it is evident that the behavior\nrepresentations extracted by ViT and MineCLIP are highly\nmixed, making it challenging to delineate the boundaries\nbetween different tasks. This lack of clear distinction be-\ntween task-specific behavior representations can hinder the\nmodel’s ability to understand the unique behavior patterns\nassociated with each task, potentially leading to task fail-\nure. In contrast, the visualization in (c) of Figure 6 reveals\nclear, distinct clusters for each task, demonstrating that the\nAction-guided Behavior Encoder effectively captures subtle\ndifferences in observation-action sequences, thereby learn-\ning robust behavior representations across tasks.\n6. Conclusion\nIn this paper, we propose a novel agent, Optimus-2, which\ncan excel in various tasks in the open-world environment of\nMinecraft. Optimus-2 integrates an MLLM for high-level\nplanning and a Goal-Observation-Action conditioned Pol-\nicy (GOAP) for low-level control. As a core contribution\nof this paper, GOAP includes an Action-guided Behavior\nEncoder to model the observation-action sequence and an\nMLLM to align the goal with the observation-action se-\nquence for predicting subsequent actions.\nExtensive ex-\nperimental results demonstrate that GOAP has mastered\nvarious atomic tasks and can comprehend open-ended lan-\nguage instructions. This enables Optimus-2 to achieve su-\nperior performance on long-horizon tasks, surpassing ex-\nisting SOTA. Moreover, we introduce a Minecraft Goal-\nObservation-Action dataset to provide the community with\nlarge-scale, high-quality data for training Minecraft agents.\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nSupplementary Material\nThe supplementary document is organized as follows:\n• Sec. A: Limitation and Future Work.\n• Sec. B: Minecraft Environment.\n• Sec. C: MGOA Dataset.\n• Sec. D: Training Details.\n• Sec. E: Evaluation Benchmark.\n• Sec. F: Experimental Results.\n• Sec. G: Case Study.\nA. Limitation and Future Work\nIn this paper, we aim to explore how agents can mimic\nhuman behavior patterns in Minecraft to accomplish vari-\nous tasks. Experimental results demonstrate that Optimus-\n2 performs exceptionally well in both atomic tasks and\nlong-horizon tasks. However, due to the lack of sufficient\nhigh-quality data for open-ended tasks (such as “building a\nhouse” and “defeating the Ender Dragon”), there remains\nsignificant room for improvement. Once such datasets are\navailable, the ability of Optimus-2 to complete open-ended\ntasks will be enhanced. Moreover, despite showing promis-\ning performance in Minecraft, we have not yet extended our\nexploration to other simulation platforms, which represents\na potential direction for future research.\nB. Minecraft\nMinecraft is an extremely popular sandbox video game de-\nveloped by Mojang Studios 2. It allows players to explore\na blockly, procedurally generated 3D world with infinite\nterrain, discover and extract raw materials, craft tools and\nitems, and build structures or earthworks. In this enviro-\nment, AI agents need to face situations that are highly sim-\nilar to the real world, making judgments and decisions to\ndeal with various environments and problems. As shown\nin Figure 7, both agents and humans are required to receive\nnatural language instructions and current observations as in-\nput, and then output low-level actions, such as mouse and\nkeyboard control commands. Therefore, Minecraft serves\n2https:\/\/www.minecraft.net\/en-us\/article\/meet-mojang-studios\n...\nObservation\n...\nAgent Action\nHuman Action\nspace\nW\nW\nKeyboard:\n {forward}\nMouse:{(2.7, 1.5)}\nKeyboard: \n{forward,jump}\nMouse: {(1.2, 0)}\nMouse: {attack}\nTask: chop a tree to get logs\nLanguage\nFigure 7.\nIllustration of behavior patterns of both human and\nagents in Minecraft.\nas an ideal open-world environment for training agent that\ncan learn human behavior patterns.\nB.1. Basic Rules\nBiomes. The Minecraft world is divided into different areas\ncalled “biomes”. Different biomes contain different blocks\nand plants and change how the land is shaped. There are 79\nbiomes in Minecraft 1.16.5, including ocean, plains, forest,\ndesert, etc. Diverse environments have high requirements\nfor the generalization of agents.\nItem. In Minecraft 1.16.5, there are 975 items can be ob-\ntained, such as wooden pickaxe\n, iron sword\n. Item can\nbe obtained by crafting or destroying blocks or attacking\nentities. For example, agent can attack cows\nto obtain\nleather\nand beef\n. Agent also can use 1 stick\nand 2\ndiamonds\nto craft diamond sword\n.\nTechnology Tree.\nIn Minecraft, the technology hierar-\nchy comprises six levels: wood\n, stone\n, iron\n, gold\n, diamond\n, and redstone\n.\nEach tool level corre-\nsponds to specific mining capabilities. Wooden tools can\nmine stone-level blocks but are incapable of mining iron-\nlevel or higher-level blocks.\nStone tools can mine iron-\nlevel blocks but cannot mine diamond-level or higher-level\nblocks.\nIron tools are capable of mining diamond-level\nblocks. Finally, diamond tools can mine blocks of any level,\nincluding redstone-level.\nGameplay progress. Progression in Minecraft primarily\ninvolves discovering and utilizing various materials and re-\nsources, each unlocking new capabilities and opportunities.\nFor instance, crafting a wooden pickaxe\nenables players\nTable 6. Action space of agent in Minecraft.\nIndex\nAgent Action\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove back.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current movement.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPlace blocks, entity, open items or other interact actions defined by game.\n10\nhotbar [1-9]\nkeys 1-9\nSelects the appropriate hotbar item.\n11\nOpen\/Close Inventory\nkey E\nOpens the Inventory. Close any open GUI.\n12\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nto mine stone\n, which can then be used to create a stone\npickaxe\nand a furnace\n. These tools allow for the mining\nand smelting of iron ore\n. Subsequently, crafting an iron\npickaxe\nenables the extraction of diamonds\n, while a di-\namond pickaxe\ncan mine virtually any block in the game.\nSimilarly, cultivating crops facilitates breeding various an-\nimals, each providing unique resources beyond sustenance.\nDrops from enemies also serve specific purposes, with some\noffering greater utility than others. By integrating resources\nfrom mining, farming, and breeding, players can enchant\ntheir equipment, further enhancing their capabilities. Addi-\ntionally, collecting and crafting materials support construc-\ntion, enabling players to create diverse structures. Beyond\npractical functions, such as building secure bases or farms,\nconstructing personalized structures forms a significant as-\npect of the Minecraft experience. Figure 11 illustrates an\nexample of progression: crafting an iron sword\n.\nB.2. Observation and Action Spaces\nObservation. In this paper, observation space of agent is\ncompletely consistent with human players. The agent only\nreceives an RGB image with dimensions of 640 × 360 dur-\ning the gameplay process, including the hotbar, health in-\ndicators, food saturation, and animations of the player’s\nhands. It is worth helping the agent see more clearly in\nextremely dark environments, we have added a night vision\neffect for the agent, which increases the brightness of the\nenvironment during the night.\nAction Spaces. In MineRL [12] environment, agent’s ac-\ntion space is almost similar to human players. It consists of\ntwo parts: the mouse and the keyboard. The keypresses are\nLog\n48.1%\nSeed\n6.0%\nDirt\n6.0%\nStone\n8.2%\nIron\n8.1%\nGold\n8.0%\nDiamond\n7.9%\nRedstone\n7.7%\nFigure 8. Statistical information on MGOA dataset. It contains 8\nAtomic Tasks: ‘Log\n’, ‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’,\n‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’.\nresponsible for controlling the movement of agents, such as\njumping, forward, back, etc. The mouse movements are re-\nsponsible for controlling the perspective of agents and the\ncursor movements when the GUI is opened. The left and\nright buttons of the mouse are responsible for attacking and\nInstruction\nPool\nFiltering\ninteract\nfeedback\nGPT-4\nScript\nEnvironment\ncollect logs\nkeyboard: W\nmouse: [0.0, 1.0]\nMGOA Dataset\nInstruction\nItems\nFigure 9. The pipeline for generating the MGOA dataset. First, we extracted item names from the Minecraft Wiki and employed GPT-\n4 to generate corresponding instructions. These instructions were then provided as input to STEVE-1, enabling it to interact with the\nenvironment to accomplish the tasks. During task execution, each observation was paired with its corresponding action, resulting in the\ncreation of goal-observation-action pairs.\nusing or placing items. In Minecraft, precise mouse move-\nments are important when completing complex tasks that\nneed open inventory or crafting table. In order to achieve\nboth the same action space with MineDojo [10], we abstract\nthe craft and the smelt action into action space. The detailed\naction space is described in Table 6.\nC. MGOA Dataset\nIn Minecraft, there is still a lack of sufficient high-\nquality goal-observation-action pairs to support the train-\ning of Optimus-2.\nTo address this, we propose an au-\ntomated dataset construction process aimed at creating\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndatasets. Through this method. MGOA contains 25,000\nvideos, providing about 30M goal-observation-action pairs.\nIt contains 8 Atomic Tasks across 5 tech levels: ‘Log\n’,\n‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’, ‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’. Note that the Atomic Tasks in MGOA re-\nquire minimal steps and can typically be completed within\n2 ∼3 minutes. For instance, the task ‘Iron\n’ involves min-\ning iron with a stone pickaxe, without the need to gather raw\nmaterials to craft the stone pickaxe. The statistics for the\nMGOA dataset is shown in Figure 8. We provide several ex-\namples of the dataset in the MGOA samples folder within\nthe supplementary materials. We will release this dataset to\ncontribute to the development of open-world agents within\nthe community.\nC.1. Dataset Construction\nPipeline. Inspired by Li et al. [24], we employed a prior\npolicy (STEVE-1 [25] in our work) to perform specific tasks\nin Minecraft, and recorded the corresponding videos and\nactions to generate goal-observation-action pairs. As illus-\ntrated in Figure 9, we employed a custom script to extract\nitem names from the Minecraft Wiki3. Using these item\nnames, we queried GPT-44 with a predefined prompt tem-\nplate to generate task instructions, thereby constructing an\nInstruction Pool. The task instructions from the Instruc-\ntion Pool serve as input to STEVE-1 [25], enabling it to\ninteract with the environment to complete the tasks. During\ntask execution, each frame and corresponding action were\nrecorded and stored. To expedite data generation, we instan-\ntiated multiple policies and used parallelization to quickly\nproduce large amounts of data.\nData Filtering. We judged task success based on environ-\nmental feedback. For example, feedback like “obtained new\nitem, diamond axe” indicated that the task “craft a diamond\naxe” was successfully completed. During the dataset gen-\neration process, we observed a significant amount of low-\nquality video data due to limitations in the policy’s abil-\nity to follow instructions. Examples of low-quality data in-\ncluded task failures or task completion timeouts. To address\nthis issue, we established two filtering criteria to ensure data\nquality: (1) only retaining data from successfully completed\ntasks, and (2) removing data for tasks that lasted longer than\n2 minutes. These criteria allowed us to automatically filter\nout low-quality data, significantly reducing the cost of con-\nstructing the dataset. As a result, we obtained a high-quality\nMGOA dataset consisting of 25,000 samples.\n3https:\/\/minecraft.wiki\/\n4https:\/\/openai.com\/index\/gpt-4-research\/\nC.2. Comparison with Existing Datasets\nPrevious gameplay videos were primarily obtained through\ntwo methods below.\nVideo Platform: For example, MineDojo [10] collected\ngame videos uploaded by human players on platforms such\nas YouTube and Twitter, combining the video content with\ncorresponding titles or subtitles to form video-text pairs.\nHowever, this dataset lacked recorded actions. To address\nthis, VPT [1] used an Inverse Dynamics Model (IDM) to\ngenerate action sequences from the videos. However, the\nactions predicted by the IDM model are only approxima-\ntions, which introduces a potential risk of misalignment be-\ntween the frames and the corresponding actions.\nHuman Contractors: VPT [1] hired human players to\nfreely explore Minecraft and used the frames and actions\nto construct a video-action dataset. However, this dataset\nlacked explicit natural language instructions.\nTo create\ngoal-observation-action pairs, STEVE-1 [25] used GPT-3.5\nto generate specific task descriptions based on the game-\nplay, thereby integrating natural language instructions into\nthe dataset. However, they provide only approximately 32k\naligned goal-observation-action pairs, which remains a rel-\natively scarce amount of data.\nIn addition, some work [32, 43] have utilized GPT-4V\nto generate image captions, task planning, and reflections,\nthereby creating image-text pairs that form instruction-\nfollowing datasets.\nDistinct from the aforementioned datasets, the MGOA\ndataset directly captures agents performing specific tasks,\noffering clear natural language instructions with a one-to-\none correspondence between observations and actions. Fur-\nthermore, through rigorous data filtering, redundant action\nsequences that do not contribute to task completion are ex-\ncluded from MGOA. In addition, compared to the small-\nscale goal-observation-action datasets currently available,\nMGOA offers 25,000 videos, encompassing approximately\n30 million goal-observation-action pairs. This dataset is not\nonly significantly larger but also highly scalable in an auto-\nmated manner.\nD. Training Details\nD.1. Training Pipeline\nOne of the key factors in implementing our proposed\nmethod lies in the efficient alignment of language with the\nobservation-action sequence, and subsequently translating\nlanguage space into the action space. To tackle this prob-\nlem, we adopt a two-phase training approach. First, we\nalign language with the observation-action sequence via be-\nhavior pre-training. Then, we transform the language space\ninto the action space through action fine-tuning.\nBehavior Pre-training: During the pre-training phase, we\nintegrated the Vision-guided Behavior Encoder into the\nTable 7. Hyperparameter setting for pre-training and finetuning.\nHyperparameter\nPre-training\nFine-tuning\nOptimizer\nAdamW\nAdamW\nLearning Rate\n0.0001\n0.00004\nWarmup Steps\n0\n0\nEpochs\n5\n10\nBatch Size\n32\n2048\nNum. Frames\n5M\n80M\nLoRA r\n64\n64\nLoRA alpha\n128\n128\nmodel.\nWe used OpenAI Contractor Dataset [1] and a\nsubset of MGOA as training data, which comprised ap-\nproximately 5,000 videos. To balance efficiency and ef-\nfectiveness, we freeze the visual encoder, then tune the\nVision-guided Behavior Encoder along with a large lan-\nguage model (LoRA [15]). During pre-training, we set the\nlearning rate to 0.0001 and trained for 5 epochs. The hyper-\nparameter settings are shown in Table 7.\nAction Fine-tuning:\nDuring the fine-tuning phase, we\nadapted the general MLLM DeepSeek-VL-1.3B [29] to the\nMinecraft environment, transitioning the model’s output\nspace from language to low-level actions. We fine-tuned\nit using OpenAI Contractor Dataset [1] and MGOA, which\ncomprises approximately 20,000 videos. In this phase, we\nfreeze the Vision-guided Behavior Encoder, visual encoder,\nand large language model (LoRA), and only fine-tuned the\naction head. During fine-tuning, we set the learning rate to\n0.00004 and train for 10 epochs. The hyperparameter set-\ntings are shown in Table 7.\nD.2. Implementation Details\nFor the planner, we follow Li et al. [24], employing Mul-\ntimodal Hybrid Memory empowered GPT-4V for planning\nand reflection. For the policy, we train the GOAP through\nthe above pipeline. All experiments were conducted on 8x\nNVIDIA L40 GPUs. For the MGOA dataset, data collec-\ntion and filtering were conducted in parallel, taking approx-\nimately 7 days.\nTraining required around 2 days, while\ninference and evaluation on atomic tasks, long-horizon\ntasks, and open-ended instruction tasks took approximately\n4 days.\nE. Benchmark\nE.1. Evaluation Tasks\nThe evaluation tasks are divided into three categories:\nAtomic Tasks, Long-horizon Tasks, and Open-ended In-\nstruction Tasks. For each task, the agent’s environment is\n(a) chop a tree to get logs\n(b) mine dirt\n(c) collect seeds\n(d) dig down to mine stone\nFigure 10. Examples of Atomic Task. The agent must follow the instructions to collect resources. These four tasks represent the basic\ncapabilities of the agent. The more resources are collected, the stronger the basic capabilities of the agent will be.\nrandomly initialized each time, and every task is executed\nat least 30 times. For Atomic Tasks, we follow the setting of\nprior work [25, 43], which requires the agent to execute the\ntask within 2 minutes. We then report the average reward\nfor the task, defined as the number of items obtained. For\nOpen-ended Instruction Tasks and Long-horizon Tasks, we\nreport the average success rate (SR) for each task.\nAtomic Tasks. As shown in Figure 10, Atomic Tasks are\nshort-term skills in Minecraft, such as “chop a tree to get\nlogs\n”, “mine dirt\n”, “collect seeds\n”, and “dig down to\nmine stone\n”, etc.\nLong-horizon Tasks.\nAs shown in Figure 11, Long-\nHorizon Tasks are a sequence of Atomic Tasks. For exam-\nple, “craft an iron sword from scratch” requires complet-\ning the atomic tasks of “chop 7 logs”, “craft 21 planks”,\n“craft 5 sticks”, “craft 1 crafting table”, and so on. These\nAtomic Tasks are interdependent, meaning that the failure of\nany single atomic task will result in the failure of the entire\nLong-horizon Task.\nOpen-ended Instruction Tasks. Open-Ended Instruction\nTasks are not limited to predefined text formats; rather, they\ninvolve flexible language directives that prompt the agent\nto accomplish long-horizon tasks. These tasks evaluate the\nagent’s capacity to interpret and execute instructions ex-\npressed in open-ended natural language. We selected Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and\nCompass\nas evaluation tasks. Instruction for each task\nare shown in Table 8, Table 9, Table 10, Table 11 and Table\n12.\nE.2. Baselines\nIn this section, we provide a brief overview of existing\nMinecraft agents and compare them with our proposed\nOptimus-2. Current agents can be broadly categorized into\ntwo types: policy-based agents and planner-policy agents.\nPolicy-based Agents. Policy-based agents [1–3, 10, 25] re-\nfer to those trained through reinforcement learning or imi-\ntation learning, capable of completing atomic tasks within\nMinecraft. However, due to limitations in instruction un-\nderstanding and reasoning abilities, they struggle to accom-\nplish long-horizon tasks.\nPlanner-Policy Agents. Planner-policy agents [20, 24, 32,\n41–43] refer to non-end-to-end architectures that utilize a\nMLLM (Multi-Layered Language Model) as a planner to\ndecompose complex instructions into a sequence of sub-\ngoals executable by a policy. While significant progress has\n(a) Chop 7 logs\n(b) Craft 21 planks\n(c) Craft 5 sticks\n(d) Craft 1 crafting table\n(e) Craft 1 wooden pickaxe\n(f) Mine 11 cobblestone\n(g) Craft 1 furnace\n(h) Craft 1 stone pickaxe\n(i) Dig down more deeper to find iron ore\n(j) Mine 2 iron ores\n(k) Smelt 2 iron ingots\n(l) Craft 1 iron sword\nFigure 11. An example of long-horizon task “crafting an iron sword”. The agent must sequentially complete each atomic task in order to\nsuccessfully craft the iron sword. Failure in any of the atomic tasks will result in the failure of the entire long-horizon task.\nbeen made, the current performance bottleneck stems from\nthe policy’s ability to effectively understand and execute the\nsub-goals generated by the planner.\nComparison with Existing Agents. As a core contribu-\ntion of this work, we propose a novel Goal-Observation-\nAction Conditioned Policy, GOAP. It integrates two key\ncomponents: an Action-Guided Behavior Encoder for mod-\neling observation-action sequences, and an MLLM for\naligning sub-goals with these sequences. Leveraging the\nMLLM’s advanced understanding of open-ended instruc-\ntions, GOAP demonstrates superior instruction-following\ncapabilities compared to existing policies. On top of GOAP,\nthe proposed agent, Optimus-2, exhibits superior perfor-\nmance in long-horizon tasks, outperforming the current\nstate-of-the-art across all seven task groups.\nF. Experimental Results\nIn this section, we report the experimental results of\nOptimus-2 on each Long-horizon task.\nF.1. Results on Long-horizon Task\nIn this section, we report the results of Optimus-2 on each\nLong-horizon Task, with details including task name, num-\nbers of sub-goals, success rate (SR), and eval times. As\nshown in Tables 13 and 14, Optimus-2 demonstrates supe-\nrior performance across all 67 Long-horizon Tasks. Since\nOptimus-2 is randomly initialized in arbitrary environments\nfor each task execution, the experimental results also high-\nlight its generalization capability across diverse environ-\nments.\nG. Case Study\nIn this section, we provide additional cases to illustrate the\ndifferences in the ability of VPT (text) [1], STEVE-1 [25],\nand Optimus-2 to perform Open-ended Instruction Tasks.\nWe provide different open-ended instructions requiring the\nagent to perform tasks across various biomes. As shown in\nFigure 12, Figure 13, and Figure 14, Optimus-2 effectively\ncompletes all tasks, while VPT (text) and STEVE-1 fail due\nto limitations in language understanding and multimodal\nperception capabilities. Moreover, we provide several demo\nvideos of Optimus-2 performing long-horizon tasks in the\nOptimus2 videos folder within the supplementary ma-\nterials.\nTable 8. Open-ended instruction examples of “Craft a torch”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, I need to locate and chop down a couple of trees to collect 4 logs.\n2. Next, I should craft these logs into 13 planks. 3. Then, I need to use some of the\nplanks to craft 3 sticks. 4. I will use some of the planks to craft a crafting table. 5.\nNext, I should craft a wooden pickaxe using some of the planks and sticks. 6. With\nthe wooden pickaxe, I need to find and mine at least 1 coal from a nearby coal deposit\nor cave. 7. Finally, I can use one stick and one coal to craft a torch.\nPlan 2 1. First, I need to find a tree to gather 4 logs. 2. Next, I have to craft the logs\ninto planks. 3. From the planks, I will craft 3 sticks. 4. I need to craft a crafting table\nusing some of the planks. 5. I should then craft a wooden pickaxe to mine coal. 6. I\nwill locate and mine 1 piece of coal using the wooden pickaxe. 7. Finally, I will use\nthe coal and sticks to craft a torch.\nPlan 3 1. I have to find a tree to get 4 logs. 2. Next, I have to craft planks from the\nlogs. 3. Get sticks from some of the planks. 4. I need a crafting table, which requires\nplanks to craft. 5. I should craft a wooden pickaxe using the crafting table. 6. I need\nto find and mine some coal using the wooden pickaxe. 7. Finally, with the stick and\ncoal, I can craft a torch.\nGLM4V\nPlan 1 1. Locate and chop down a tree to gather 4 logs. 2. Convert the logs into 13\nplanks using the crafting menu. 3. Use the planks to craft a crafting table. 4. Craft 3\nsticks from the remaining planks. 5. With the crafting table and sticks, craft a wooden\npickaxe if one is not already available. 6. Use the wooden pickaxe to dig for coal\nor kill some Creepers or zombies for coal drops. 7. Once 1 coal is obtained, use the\ncrafting table to combine it with a stick to craft 1 torch.\nPlan 2 1. Locate and chop down a tree to gather the 4 logs required. 2. Convert the\nlogs into 13 planks using the crafting menu, as this will be the material needed for\ncrafting the crafting table and the wooden pickaxe. 3. Use some of the planks to craft\na crafting table, which is necessary for advanced crafting recipes. 4. Craft a wooden\npickaxe using some of the planks and sticks. The wooden pickaxe will be used to\nmine coal. 5. Gather 3 sticks, which can be obtained from the logs or by crafting them\nfrom planks. 6. Use the wooden pickaxe to mine coal. Only 1 coal is needed to craft\na torch. 7. Once all materials are gathered, open the crafting table and combine 1 coal\nand 1 stick to craft a torch.\nPlan 3 1. Locate and gather 4 logs from the nearest trees. 2. Convert the logs into\n13 planks using the crafting menu. 3. Use some planks to craft a crafting table for\nmore complex recipes. 4. Craft a wooden pickaxe using some planks and sticks to\ngather coal and other resources. 5. Convert some planks into 3 sticks required for\ncrafting torches and the wooden pickaxe. 6. Use the wooden pickaxe to mine coal\nore, ensuring to gather at least 1 coal. 7. With all materials gathered, open the crafting\ntable and craft 1 torch using 1 coal and 1 stick.\nTable 9. Open-ended instruction examples of “Craft a rail”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and punch down a tree to acquire 7 logs. 2. Convert these logs\ninto planks to obtain a total of 25 planks. 3. Use some of these planks to craft 5 sticks.\n4. Create a crafting table using 4 planks. 5. Craft a wooden pickaxe using 3 planks\nand 2 sticks. 6. Use the wooden pickaxe to mine 11 cobblestones. 7. Craft a furnace\nusing 8 cobblestones. 8. Craft a stone pickaxe using 3 cobblestones and 2 sticks. 9.\nUse the stone pickaxe to mine 6 iron ores. 10. Smelt the iron ores in the furnace to\ncreate 6 iron ingots. 11. Finally, craft a rail using the iron ingots and sticks.\nPlan 2 1. I have to find a tree to get 7 logs. 2. Next, I have to craft the logs into 25\nplanks. 3. Craft 5 sticks from the planks. 4. I need to create a crafting table. 5. Use\nthe crafting table to craft a wooden pickaxe. 6. Use the wooden pickaxe to mine 11\npieces of cobblestone. 7. I’ll then craft a furnace using the cobblestone. 8. Craft a\nstone pickaxe with the cobblestone and sticks. 9. Use the stone pickaxe to mine 6\npieces of iron ore. 10. Smelt the iron ore in the furnace to obtain 6 iron ingots. 11.\nFinally, use the crafting table to craft a rail using the iron ingots and sticks.\nPlan 3 1. I have to find a tree to get 7 logs. 2. Next, I need to craft planks from\nthe logs. 3. Get sticks from the planks. 4. Craft a crafting table using some of the\nplanks. 5. Craft a wooden pickaxe with sticks and planks to mine stone. 6. Mine 11\ncobblestone with the wooden pickaxe. 7. Craft a furnace using the cobblestone. 8.\nCraft a stone pickaxe using cobblestone and sticks. 9. Use the stone pickaxe to mine\n6 iron ore. 10. Smelt the iron ore in the furnace to get 6 iron ingots. 11. Finally, use\nthe iron ingots and sticks to craft a rail.\nGLM4V\nPlan 1 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down\na crafting table to access more crafting recipes. 5. Use the crafting table to craft\na wooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones from the\nground or from stone blocks. 7. Craft a furnace using the cobblestones. 8. Upgrade\nthe wooden pickaxe to a stone pickaxe by crafting it at the crafting table with the\ncobblestones. 9. Use the stone pickaxe to mine 6 iron ore from the appropriate rock\nlayers. 10. Smelt the 6 iron ore in the furnace to produce 6 iron ingots. 11. Finally,\nuse the crafting table to craft 1 rail using the 6 iron ingots and 1 stick.\nPlan 2 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu\nto convert the logs into 25 planks. 3. Craft 5 sticks using the planks in the crafting\nmenu. 4. Place a crafting table to enable the crafting of more complex items. 5.\nCraft a wooden pickaxe using some of the planks and sticks. 6. Use the wooden\npickaxe to mine 11 cobblestones from stone outcrops. 7. Construct a furnace using\nthe cobblestones. 8. Upgrade to a stone pickaxe by crafting it with the cobblestones\nand sticks. 9. Use the stone pickaxe to mine 6 iron ore from iron veins. 10. Smelt the\n6 iron ore in the furnace to produce 6 iron ingots. 11. Finally, use the crafting table to\ncraft 1 rail using the iron ingots and sticks.\nPlan 3 1. Find and chop down a tree to gather 7 logs. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down a\ncrafting table to access more crafting options. 5. Craft a wooden pickaxe using some\nof the planks and sticks. 6. Use the wooden pickaxe to mine 11 cobblestones from\nstone. 7. Construct a furnace using some of the cobblestones. 8. Upgrade the wooden\npickaxe to a stone pickaxe by crafting it with the cobblestones. 9. Search for and\nmine 6 iron ore blocks with the stone pickaxe. 10. Smelt the iron ore in the furnace to\nproduce 6 iron ingots. 11. Finally, use the crafting table to craft 1 rail using the iron\ningots and sticks.\nTable 10. Open-ended instruction examples of “Craft a golden shovel”\nPlanner Instruction\nGPT4V\nPlan 1 1. Start by finding and chopping down trees to collect 9 logs. 2. Convert the logs into 35 planks\nusing the crafting interface. 3. Use some of the planks to craft 8 sticks. 4. Create a crafting table using\nsome of the planks. 5. Craft a wooden pickaxe. 6. Mine 19 cobblestone. 7. Craft a furnace using 8 of the\ncobblestones. 8. Craft a stone pickaxe. 9. Mine 3 iron ores using the stone pickaxe. 10. Smelt the iron ore\nin the furnace. 11. Craft an iron pickaxe using sticks and iron ingots. 12. Locate and mine a gold block\nusing the iron pickaxe. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Craft a golden\nshovel using the gold ingot and sticks.\nPlan 2 1. Start by finding and chopping down trees to collect 9 logs. 2. Use the logs to craft planks. 3.\nCraft sticks. 4. Use some of the planks to craft a crafting table. 5. Use the crafting table to craft a wooden\npickaxe. 6. Use the wooden pickaxe to mine 19 pieces of cobblestone. 7. Use some of the cobblestone to\ncraft a furnace. 8. Use the remaining cobblestone to craft a stone pickaxe. 9. Mine iron ore with the stone\npickaxe. 10. Smelt the iron ore in the furnace to obtain 3 iron ingots. 11. Use 2 of the iron ingots and sticks\nto craft an iron pickaxe. 12. Use the iron pickaxe to mine gold ore or find a gold block; you need at least 1\ngold ingot. 13. If mining gold ore, smelt it in the furnace to obtain gold ingots. 14. Use the gold ingot and\nstick to craft a golden shovel.\nPlan 3 1. Find and cut down trees to collect 9 logs. 2. Use the logs to craft 35 planks. 3. Craft 8 sticks\nusing the planks. 4. Use some of the planks to craft a crafting table. 5. Craft a wooden pickaxe using planks\nand sticks. 6. Mine cobblestone until you have 19 pieces. 7. Use some cobblestone and the crafting table to\ncraft a furnace. 8. Craft a stone pickaxe using cobblestone and remaining sticks. 9. Use the stone pickaxe\nto mine 3 pieces of iron ore. 10. Smelt the iron ore in the furnace to get 3 iron ingots. 11. Craft an iron\npickaxe. 12. Use the iron pickaxe to mine gold ore to obtain gold blocks. 13. Use 1 gold ingot to craft a\ngolden shovel. 14. use a gold ingot and sticks to craft a golden shovel using the crafting table.\nGLM4V\nPlan 1 1. Find and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to expand the crafting options. 5. Craft\na wooden pickaxe with the planks and sticks. 6. Cine cobblestone until you have a total of 19. 7. Craft a\nfurnace using 8 cobblestone. 8. Use the furnace to smelt the cobblestone into 19 stone, then use the crafting\ntable to craft a stone pickaxe with the stone and sticks. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks at the crafting table. 12. Use the iron pickaxe to mine a gold block. 13. Smelt the gold block in\nthe furnace to get 1 gold ingot. 14. Finally, use the crafting table to craft the golden shovel using 1 gold\ningot and 2 sticks.\nPlan 2 1. Find and gather 9 logs from nearby trees. 2. Convert the logs into 35 planks using a crafting\ntable. 3. Craft 8 sticks from the planks. 4. Build a crafting table. 5. Use the crafting table to craft a wooden\npickaxe with the planks and sticks. 6. Use the wooden pickaxe to mine 19 cobblestones from stone. 7.\nConstruct a furnace to smelt materials. 8. Craft a stone pickaxe using the cobblestones and sticks. 9. Use\nthe stone pickaxe to mine 3 iron ore from iron deposits. 10. Smelt the 3 iron ore in the furnace to produce\n3 iron ingots. 11. Craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine a\ngold block. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Finally, craft the golden shovel\nusing the gold ingot and stick in the crafting table.\nPlan 3 1. Find and gather 9 logs from nearby trees. 2. Use the crafting menu to convert the logs into 35\nplanks. 3. Craft 8 sticks from the planks. 4. Craft a crafting table using some of the planks. 5. Use the\ncrafting table to craft a wooden pickaxe with some of the sticks and planks. 6. Use the wooden pickaxe to\nmine 19 cobblestones from stone. 7. Craft a furnace using some of the cobblestones. 8. Use the furnace to\nsmelt the cobblestones into 19 stone, then craft a stone pickaxe with the sticks and stone. 9. Mine 3 iron\nore from the appropriate rock faces. 10. Smelt the iron ore in the furnace to produce 3 iron ingots. 11. Use\nthe crafting table to craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine\na gold block. 13. Smelt the gold block in the furnace to produce 1 gold ingot. 14. Finally, use the crafting\ntable to craft the golden shovel with the gold ingot and 2 sticks.\nTable 11. Open-ended instruction examples of “Craft a diamond pickaxe”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some\nof the planks to craft 8 sticks. 4. Create a crafting table using 4 planks. 5. With the crafting table, craft\na wooden pickaxe using planks and sticks. 6. Use the wooden pickaxe to collect at least 11 cobblestones\nfrom stone. 7. With the cobblestones, craft a furnace. 8. Use more cobblestones to craft a stone pickaxe. 9.\nUse the stone pickaxe to mine 3 iron ores. 10. Smelt the iron ores into 3 iron ingots using the furnace. 11.\nCraft an iron pickaxe with iron ingots and sticks. 12. Use the iron pickaxe to mine 3 diamonds. 13. Finally,\ncraft the diamond pickaxe using the diamonds and sticks.\nPlan 2 1. Begin by locating a tree to gather 9 logs. 2. Convert the logs into planks. 3. Use some of the\nplanks to craft sticks. 4. Craft a crafting table to facilitate further crafting. 5. Create a wooden pickaxe to\nstart mining stone. 6. Mine 11 cobblestones. 7. Use the cobblestones to craft a furnace. 8. Craft a stone\npickaxe to advance to mining iron ore. 9. Locate and mine 3 iron ores using the stone pickaxe. 10. Smelt\nthe iron ores in the furnace to produce 3 iron ingots. 11. Use two of the iron ingots and sticks to craft an\niron pickaxe. 12. Locate a diamond ore vein and mine 3 diamonds using the iron pickaxe. 13. Finally, craft\nthe diamond pickaxe with the 3 diamonds and sticks.\nPlan 3 1. I need to find a tree to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some planks to\ncraft 8 sticks. 4. Craft a crafting table using some of the planks. 5. With the crafting table, make a wooden\npickaxe to mine stone. 6. Mine 11 cobblestones using the wooden pickaxe. 7. Craft a furnace using the\ncollected cobblestones. 8. Make a stone pickaxe to efficiently mine iron ore. 9. Find and mine 3 iron ores\nusing the stone pickaxe. 10. Smelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron\npickaxe using some planks, sticks, and the iron ingots. 12. Locate a source of diamonds and use the iron\npickaxe to mine 3 diamonds. 13. Finally, use the crafting table along with the sticks and diamonds to craft\na diamond pickaxe.\nGLM4V\nPlan 1 1. Start by finding trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to make a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ores. 10.\nSmelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine 3 diamonds, which may require exploring caves or mining deep underground with the\niron pickaxe. 13. Finally, use the crafting table to combine the diamonds and the sticks to craft a diamond\npickaxe.\nPlan 2 1. Locate and chop down trees to gather a total of 9 logs. 2. Use the logs to craft 35 planks, as this\nwill be the base material for several other items. 3. Craft 8 sticks from the planks, which will be needed to\ncreate various tools. 4. Craft a crafting table using some of the planks, as this is essential for crafting more\ncomplex items. 5. Use the crafting table to craft a wooden pickaxe with the planks and sticks, which will be\nused to mine cobblestone. 6. Mine 11 cobblestone using the wooden pickaxe. 7. Craft a stone pickaxe with\nthe cobblestone and sticks, as this is more durable and efficient for mining. 8. Construct a furnace using the\ncobblestone, which will be required to smelt ores into ingots. 9. Mine 3 iron ore with the stone pickaxe. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks, as this is necessary to mine diamonds. 12. Find and mine diamond ore with the iron pickaxe\nto gather 3 diamonds. 13. Finally, use the crafting table to craft a diamond pickaxe with the diamonds and\nsticks, completing the task.\nPlan 3 1. Begin by locating trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine diamond ore with the iron pickaxe to obtain 3 diamonds. 13. Finally, use the crafting\ntable to craft a diamond pickaxe with the diamonds.\nTable 12. Open-ended instruction examples of “Craft a compass”\nPlanner Instruction\nGPT4V\nPlan 1 1. Locate a tree and gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks from the\nplanks. 4. Use some planks to craft a crafting table. 5. With the crafting table, craft a wooden pickaxe. 6.\nMine cobblestone until you have 12 pieces. 7. Upgrade your tools by crafting a stone pickaxe. 8. Construct\na furnace using some of the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron\nore in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots. 12. Find and\nmine redstone using the iron pickaxe to obtain 1 redstone dust. 13. Finally, use the remaining iron ingots\nand redstone dust at the crafting table to craft a compass.\nPlan 2 1. Start by finding and chopping down trees to collect 10 logs. 2. Use the logs to craft 38 planks. 3.\nCraft sticks using the planks to get 8 sticks. 4. Craft a crafting table from the planks. 5. Use the crafting\ntable to create a wooden pickaxe. 6. Locate and mine 12 pieces of cobblestone using the wooden pickaxe.\n7. Craft a stone pickaxe from cobblestone and sticks. 8. Use the cobblestone to craft a furnace. 9. Mine\n7 iron ores using the stone pickaxe. 10. Smelt the iron ore in the furnace to get 7 iron ingots. 11. Craft\nan iron pickaxe using iron ingots and sticks. 12. Search for and mine redstone using the iron pickaxe. 13.\nFinally, use the iron ingots and redstone to craft a compass.\nPlan 3 1. First, I need to find a tree and collect 10 logs. 2. Convert the logs into planks, ensuring I get at\nleast 38 planks. 3. Use some of the planks to craft sticks, collecting at least 8 sticks. 4. Use some of the\nplanks to create a crafting table. 5. Craft a wooden pickaxe. 6. Locate cobblestone and mine at least 12\npieces using the wooden pickaxe. 7. Use some cobblestone to craft a furnace. 8. Craft a stone pickaxe for\nimproved mining. 9. Search for iron ore and mine at least 7 using the stone pickaxe. 10. Smelt the iron ore\nin the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using some of the iron ingots and sticks.\n12. Locate a redstone ore and mine it using the iron pickaxe to obtain redstone. 13. Finally, use 4 of the\niron ingots and the redstone to craft a compass at the crafting table.\nGLM4V\nPlan 1 1. Locate and gather 10 logs from nearby trees. 2. Convert the logs into 38 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to access more crafting options. 5. Use\nthe crafting table to make a wooden pickaxe. 6. With the wooden pickaxe, mine 12 cobblestones from\nstone. 7. Craft a stone pickaxe using the cobblestones and the crafting table. 8. Construct a furnace to smelt\nmaterials. 9. Use the stone pickaxe to mine 7 iron ores from the appropriate rock formations. 10. Smelt\nthe 7 iron ores in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe with the iron ingots at the\ncrafting table. 12. Mine for redstone, which can sometimes be found in the ground. 13. Use the crafting\ntable to craft a compass with the iron ingot and redstone.\nPlan 2 1. Locate and chop down trees to gather the required 10 logs. 2. Convert the logs into 38 planks\nusing the crafting menu. 3. Craft 8 sticks from the planks. 4. Use some planks to craft a crafting table.\n5. With the crafting table and the sticks and planks crafted, make a wooden pickaxe. 6. Use the wooden\npickaxe to mine 12 cobblestone. 7. Craft a stone pickaxe using the cobblestone. 8. Construct a furnace\nwith the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron ore in the furnace to\nproduce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots and the crafting table. 12. Search for\nredstone; gather at least 1 redstone. 13. Finally, use the crafting table to combine the redstone with the iron\ningots to craft a compass.\nPlan 3 1. Find and chop down trees to gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks\nfrom the planks. 4. Craft a crafting table using some of the planks. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine cobblestone until you have at least 12. 7. Upgrade the\nwooden pickaxe to a stone pickaxe using the cobblestone. 8. Construct a furnace out of the cobblestone. 9.\nUse the stone pickaxe to mine 7 iron ore. 10. Smelt the 7 iron ore in the furnace to produce 7 iron ingots.\n11. Craft an iron pickaxe using the iron ingots and sticks at the crafting table. 12. Search for redstone; you\nwill need at least 1 piece. 13. Finally, use the crafting table to combine the redstone with the iron ingots to\ncraft a compass.\nTable 13. The results of Optimus-2 on the Wood Group, Stone Group, and Iron Group. SR denotes success rate.\nGroup\nTask\nSub-Goal Num.\nSR\nEval Times\nWood\nCraft a wooden shovel\n6\n100.00\n40\nCraft a wooden pickaxe\n5\n100.00\n30\nCraft a wooden axe\n5\n97.37\n38\nCraft a wooden hoe\n5\n100.00\n30\nCraft a stick\n4\n100\n30\nCraft a crafting table\n3\n93.02\n43\nCraft a wooden sword\n5\n100.00\n30\nCraft a chest\n4\n100.00\n30\nCraft a bowl\n4\n100.00\n30\nCraft a ladder\n4\n100.00\n30\nStone\nCraft a stone shovel\n8\n89.47\n57\nCraft a stone pickaxe\n10\n98.00\n50\nCraft a stone axe\n10\n94.44\n54\nCraft a stone hoe\n8\n95.74\n47\nCraft a charcoal\n9\n85.71\n42\nCraft a smoker\n9\n90.00\n40\nCraft a stone sword\n8\n95.45\n44\nCraft a furnace\n9\n94.44\n36\nCraft a torch\n8\n89.36\n47\nIron\nCraft an iron shovel\n13\n52.08\n48\nCraft an iron pickaxe\n13\n56.00\n50\nCraft an iron axe\n13\n48.15\n54\nCraft an iron hoe\n13\n56.60\n53\nCraft a bucket\n13\n45.10\n51\nCraft a hopper\n14\n54.90\n51\nCraft a rail\n13\n51.02\n49\nCraft an iron sword\n12\n56.52\n46\nCraft a shears\n12\n48.28\n58\nCraft a smithing table\n12\n53.33\n45\nCraft a tripwire hook\n13\n55.56\n45\nCraft a chain\n13\n52.17\n46\nCraft an iron bars\n12\n51.06\n47\nCraft an iron nugget\n12\n54.55\n44\nCraft a blast furnace\n14\n52.27\n44\nCraft a stonecutter\n13\n52.27\n44\nTable 14. The results of Optimus-2 on the Gold group, Diamond Group, Redstone Group, and Armor Group. SR denotes success rate.\nGroup\nTask\nSub Goal Num.\nSR\nEval Times\nGold\nCraft a golden shovel\n16\n8.93\n56\nCraft a golden pickaxe\n16\n11.29\n62\nCraft a golden axe\n16\n8.93\n56\nCraft a golden hoe\n16\n8.96\n67\nCraft a golden sword\n16\n8.20\n61\nSmelt and craft an golden ingot\n15\n9.68\n62\nDiamond\nCraft a diamond shovel\n15\n15.91\n44\nCraft a diamond pickaxe\n15\n11.76\n34\nCraft a diamond axe\n16\n11.00\n36\nCraft a diamond hoe\n15\n15.91\n44\nCraft a diamond sword\n15\n11.11\n36\nDig down and mine a diamond\n15\n11.42\n35\nCraft a jukebox\n15\n13.15\n38\nRedstone\nCraft a piston\n16\n28.33\n60\nCraft a redstone torch\n16\n27.69\n65\nCraft an activator rail\n18\n25.81\n62\nCraft a compass\n23\n28.36\n67\nCraft a dropper\n16\n30.30\n66\nCraft a note block\n16\n25.40\n63\nArmor\nCraft shield\n14\n45.16\n62\nCraft iron chestplate\n14\n43.86\n57\nCraft iron boots\n14\n40.35\n57\nCraft iron leggings\n14\n8.57\n35\nCraft iron helmet\n14\n47.46\n56\nCraft diamond helmet\n17\n9.09\n33\nCraft diamond chestplate\n17\n7.89\n38\nCraft diamond leggings\n17\n5.41\n37\nCraft diamond boots\n17\n12.50\n40\nCraft golden helmet\n17\n13.89\n36\nCraft golden leggings\n17\n12.20\n41\nCraft golden boots\n17\n10.26\n39\nCraft golden chestplate\n17\n10.00\n40\nAgent\nInstruction: I want to get some logs to craft wooden sword, what should I do first?\nSuccess\n❌\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n✔\nFigure 12. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to get some logs\nto craft wooden sword, what should I do first?”. Existing policies are limited by their instruction comprehension abilities and thus fail to\ncomplete the task, whereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I need coal for heating. What should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 13. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need coal for heating.\nWhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I want to collect some seeds, Can you help me?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 14. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to collect some\nseeds, Can you help me?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task,\nwhereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Optimus-2：基于多模态大语言模型的Minecraft智能体\n\n## 📌 背景痛点\/本文动机\n在开放世界环境中，构建能够模仿人类行为模式并完成各种任务的智能体一直是人工智能领域的长期目标。然而，要使智能体有效地学习跨任务的行为模式，关键挑战在于建模观察、动作和语言之间的复杂关系。现有的智能体在处理开放世界环境中的多样化任务时，通常采用任务规划器和目标条件策略的框架。尽管现有的智能体在利用多模态大语言模型（MLLM）作为规划器方面取得了进展，但目标条件策略的性能瓶颈仍然存在。现有的策略通常忽略了观察和动作之间的关系，并且难以建模开放式的子目标和观察-动作序列之间的关系。\n\n## 🚀 核心方法\n为了解决上述挑战，本文提出了Optimus-2，一个新颖的Minecraft智能体，它结合了MLLM进行高级规划，并采用目标-观察-动作条件策略（GOAP）进行低级控制。GOAP包含两个关键组件：\n\n💡 创新点1：动作引导的行为编码器\n动作引导的行为编码器用于建模观察-动作序列。它首先使用因果感知器将动作嵌入到观察特征中，利用任务相关的动作信息作为指导来调整观察特征，从而为动作预测提供细粒度的观察-动作信息。此外，为了在不超出输入长度限制的情况下对长期观察-动作序列进行建模，引入了历史聚合器，将当前观察-动作信息与历史序列动态地整合成固定长度的行为标记。行为标记可以以固定且适当的长度捕获观察-动作序列的长期依赖关系，使智能体能够预测与观察-动作序列逻辑一致的动作，而不是仅基于当前观察进行孤立的动作预测。\n\n💡 创新点2：多模态大语言模型\n为了明确编码子目标的语义，引入了MLLM作为GOAP的骨干网络。它将子目标与行为标记对齐，以自回归方式预测后续动作。利用MLLM的语言理解和多模态感知能力，它可以更好地整合开放式子目标和观察-动作序列的特征，从而增强策略的动作预测能力。\n\n## 📈 实验结果\n在Minecraft的开放世界环境中进行了广泛的评估，实验结果表明Optimus-2在原子任务、长期任务和开放式指令任务中表现出优异的性能。与之前的SOTA相比，Optimus-2在原子任务、长期任务和开放式子目标任务上分别实现了平均27%、10%和18%的提升。\n\n## 💬 可借鉴之处\n本文提出的Optimus-2智能体及其GOAP策略为开放世界环境中的智能体设计提供了新的思路。动作引导的行为编码器和MLLM的引入有效地解决了观察、动作和语言之间的复杂关系建模问题，使得智能体能够更好地理解和执行开放式指令。此外，本文提出的MGOA数据集为训练Minecraft智能体提供了高质量的数据资源，有助于推动相关研究的发展。","llm_summary_res_status":200}
{"title":"MineStudio: A Streamlined Package for Minecraft AI Agent Development","authors":"Shaofei Cai, Zhancun Mu, Kaichen He, Bowei Zhang, Xinyue Zheng, Anji Liu, Yitao Liang","summary":"Minecraft has emerged as a valuable testbed for embodied intelligence and\nsequential decision-making research, yet the development and validation of\nnovel agents remains hindered by significant engineering challenges. This paper\npresents MineStudio, an open-source software package designed to streamline\nembodied policy development in Minecraft. MineStudio represents the first\ncomprehensive integration of seven critical engineering components: simulator,\ndata, model, offline pretraining, online finetuning, inference, and benchmark,\nthereby allowing users to concentrate their efforts on algorithm innovation. We\nprovide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at\nhttps:\/\/github.com\/CraftJarvis\/MineStudio.","url":"http:\/\/arxiv.org\/abs\/2412.18293v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.18293v2","published":1735030903000,"comment":null,"pdf_text":"MineStudio: A Streamlined Package for\nMinecraft AI Agent Development\nShaofei Cai†1, Zhancun Mu†1, Kaichen He1, Bowei Zhang1, Xinyue Zheng1, Anji Liu2 and Yitao Liang B1\n1Peking University, 2University of California, Los Angeles, All authors are affiliated with Team CraftJarvis\nMinecraft has emerged as a valuable testbed for embodied intelligence and sequential decision-making\nresearch, yet the development and validation of novel agents remains hindered by significant engineering\nchallenges. This paper presents MineStudio, an open-source software package designed to stream-\nline embodied policy development in Minecraft. MineStudio represents the first comprehensive\nintegration of seven critical engineering components: simulator, data, model, offline pretraining, online\nfinetuning, inference, and benchmark, thereby allowing users to concentrate their efforts on the algo-\nrithm innovation. We provide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at https:\/\/github.com\/CraftJarvis\/MineStudio.\n1. Introduction\nAs an open-world 3D sandbox game, Minecraft has garnered widespread acclaim for its high degree of\nfreedom and rich playability. In recent years, with the rapid advancement of general-purpose artificial\nintelligence, Minecraft has gradually emerged as a key testbed for research on embodied intelligence\nand decision-making control (Baker et al., 2022; Fan et al., 2022; Guss et al., 2019; Lifshitz et al.,\n2023). Although the community has introduced a variety of environments (Fan et al., 2022; Guss\net al., 2019; Johnson et al., 2016), datasets (Baker et al., 2022; Fan et al., 2022; Guss et al., 2019),\ntraining algorithms and policy models (Baker et al., 2022; Cai et al., 2023, 2024; Lifshitz et al.,\n2023), and benchmarking methods (Fan et al., 2022; Lin et al., 2023) for this platform, a substantial\nengineering gap remains in the development and validation of novel agents.\nMinecraft’s complexity has led to varied task customizations, hindering progress tracking across\nthe community. For example, Dreamer V3 (Hafner et al., 2023) simplified the environment to destroy\nblocks with a single attack, while Voyager (Wang et al., 2023) relied on the rule-based Mineflayer\ncontroller (PrismarineJS, 2013) to interact with the environment. Moreover, experimenting in\nMinecraft is harder than in other AI domains. For example, despite OpenAI’s 160-million-frame\ndataset, the lack of flexible data structures hampers efficient storage and retrieval. Reinforcement\nlearning shows promise in tackling complex tasks (Baker et al., 2022), but mainstream frameworks\noften lack memory-policy support (Liang et al., 2018; Raffin et al., 2021) and crash safeguards (Raffin\net al., 2021). These require researchers to make great engineering efforts before validating an idea.\nTo address these challenges, we present MineStudio, an open-source software package for\nefficiently developing embodied agents in Minecraft. We also integrate many existing efforts, including\ndata (Baker et al., 2022), environment (Guss et al., 2019; Johnson et al., 2016), and model (Baker\net al., 2022; Cai et al., 2023, 2024; Lifshitz et al., 2023), into a unified framework and present\nthem to users in a concise manner. To minimize redundant engineering work, we have introduced\nseveral key improvements and optimizations: (1) support flexible custom environment configurations;\n(2) a flexible and training-efficient trajectory data structure; (3) a unified policy model template\nwith multiple built-in Minecraft agent algorithms; (4) a simplified offline pre-training process; (5)\na high-performance distributed online fine-tuning pipeline tailored to the Minecraft platform; (6)\na parallel inference pipeline; and (7) the integration of an MCU (Lin et al., 2023) benchmark to\nCorresponding author(s): Yitao Liang\n† indicates co-first author\n{caishaofei, muzhancun, hekaichen, zhangbowei}@stu.pku.edu.cn, zhengxinyue@bigai.ai, liuanji@cs.ucla.edu, yitaol@pku.edu.cn\narXiv:2412.18293v2  [cs.AI]  25 Dec 2024\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nFigure 1 | MineStudio enables users to address classic requirements such as offline pretraining, online\nfine-tuning, system evaluation, and dataset generation with minimal coding effort. Users only need to configure\nthe model component with a small amount of PyTorch code. Each module in the workflow is fully customizable,\nallowing users to configure settings or extend functionality through inheritance and overrides as needed.\nenable fair comparisons. With MineStudio, users can not only conduct interactive evaluations\nof existing mainstream Minecraft agents but also quickly implement a model file using just a few\nlines of PyTorch (Paszke et al., 2019) code, enjoying a fully integrated workflow from pre-training\nthrough finetuning to evaluation. The source code is publicly available at GitHub, accompanied by\ncomprehensive documentation and extensive tutorials to facilitate rapid onboarding.\n2. MineStudio\nThe MineStudio is an efficient toolkit for developing Minecraft AI agents, encompassing the full\nworkflow from data preparation and training to evaluation. It offers a flexible interface design that\nnot only includes various out-of-the-box features but also allows users to tailor the toolkit to their\nspecific requirements. In doing so, it maximizes flexibility while minimizing engineering overhead.\nFigure 1 provides an overview of the pipeline, and subsequent paragraphs detail each module.\nSimulator. This component implements a hook-based Minecraft wrapper that offers a high degree\nof customization. By inheriting from the MinecraftCallback class, users can tailor the environ-\nment in various ways, including (but not limited to) monitoring rendering framerate, issuing cheat\ncommands, simulating rapid resets, logging episodes, generating reward functions, and overriding\nobservations. To further reduce user workload, we provide a set of commonly used callbacks out of the\nbox. In particular, PlayCallback loads a graphical user interface and enables switching between\nmanual and agent-controlled modes, allowing users to interactively inspect model performance. In\naddition, we have integrated a bunch of tricks to improve rendering speed, thereby increasing the\nefficiency of model evaluation, data collection, and reinforcement learning.\nData. This component introduces a flexible and efficient data structure for handling offline trajectory\ndata. Different modalities within the trajectory are segmented into clips and stored independently\nin LMDB files while preserving their temporal relationships. This design seeks to balance storage\nsize and video decoding efficiency, enabling users to retrieve trajectory segments based on semantic\nlabels rapidly. To facilitate the training of models requiring long-term memory, we have implemented\na distributed batch sampler that supports continuous reading of lengthy trajectory sequences. Ad-\nditionally, we provide data format conversion scripts, making it convenient for users to incorporate\ntheir own collected trajectory data, as well as a suite of visualization tools to assist with troubleshoot-\ning. Our framework comes with the largest Contractor Dataset (Baker et al., 2022) available in the\nMinecraft domain, complemented by our pre-processed frame-level semantic segmentation data.\n2\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nThese resources can be utilized directly for training ROCKET-1 models (Cai et al., 2024).\nModel. This component provides a unified template for policy models along with action and value\nheads specifically designed for the Minecraft domain, allowing users to focus solely on their model\narchitecture design. We have pre-integrated models such as VPTs (Baker et al., 2022), STEVE-1\n(Lifshitz et al., 2023), GROOT-1 (Cai et al., 2023), and ROCKET-1 (Cai et al., 2024). The standardized\ninterface functions ensure that policies can be seamlessly integrated with other MineStudio\nmodules, enabling efficient training and inference as well as fair comparisons.\nOffline Training. This component builds upon the PyTorch Lightning (Falcon and team, 2019)\nframework to deliver an enhanced Trainer module. It incorporates dedicated mechanisms for handling\npolicy memory (e.g., TransformerXL (Dai et al., 2019)) and integrates seamlessly with the distributed\nbatch sampler in the MineStudio data module, thereby enabling training on ultra-long trajectories.\nIn addition, we provide a proven set of hyperparameter configurations—covering aspects such as\nwarmup steps, optimizers, and learning rates—while also allowing flexible customization through\nobjective callbacks. This approach maximizes user flexibility while minimizing their overhead.\nOnline Training. This component implements the KL-constrained Proximal Policy Optimization\nalgorithm introduced in Baker et al. (2022). The code is designed for integration with our model and\nsimulator. It has been optimized to deal with long episodes and handle the instability of the Minecraft\nenvironment. Additionally, we provide a set of hyperparameters that have been tested to achieve high\ntraining efficiency and strong performance across various tasks. We hope this resource serves as an\neasy starting point and lowers the barriers to reinforcement learning research in Minecraft.\nInference. This component provides a Ray-based (Moritz et al., 2018) inference framework for\nMineStudio, to support distributed inference. The framework is designed to consist of three\nparts: generator, filter and recorder, forming an asynchronous inference pipeline for easily\nevaluating the performance of different agents. The generator part, equipped with an agent creator\nand an environment creator, generates trajectories in batches. Each produced trajectory is immediately\npassed to a filter for post-processing and then summarized and stored by the recorder. By\ncustomizing the filter and recorder, users can effortlessly conduct comprehensive evaluations\nof policy checkpoints. Furthermore, this pipeline allows for efficient data synthesis, which, when\ncombined with the data module’s conversion scripts, enables a closed-loop data workflow.\nBenchmark. This component evaluates agent performance in MineStudio environments. It\nsupports a variety of tasks such as building, mining, and crafting, and offers both simple and\nchallenging game modes to test agents under different levels of difficulty. The framework includes an\nautomatic evaluation pipeline that leverages Vision-Language Models to analyze task videos, and\nprovides batch task execution capabilities to run multiple tasks simultaneously and record completion\nvideos. Additionally, it offers a quick benchmarking tool that simplifies the process of task execution\nand evaluation, enabling researchers to compare different agent models efficiently.\n3. Comparison to Existing Interest of Minecraft\nTo highlight the contributions of MineStudio, we compare its features with those of other promi-\nnent Minecraft development frameworks, such as MineRL (Guss et al., 2019), MineDojo (Fan et al.,\n2022), and Mineflayer (PrismarineJS, 2013), as shown in table 1. Although these frameworks offer\nvaluable testbeds for Minecraft, they lack a seamless pipeline that integrates building, training, and\nevaluating agents, particularly when it comes to environment customization. While Baker et al. (2022)\nextends MineRL by offering data loading and training code, its dataloader is primarily designed for\ndemonstration purposes and is inefficient, while the RL training process remains closed-source and\n3\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nTable 1 | Comparison of features across Minecraft development frameworks.\nFeature\nMineRL\nMineDojo\nMineflayer\nMineStudio\nObservation\/Action Space\noriginal\nmodified\nmodified\noriginal\nDataset Features\nannotated state-action pairs\nof human demonstrations\nmultimodal data\nscraped from Internet\nN\/A\nefficient data structure\nfor both storing and loading\nCustomizing Environments\nlimited\neasy\nN\/A\neasy\nCreating Agents\ndifficult\ndifficult\neasy\neasy\nTraining Agents\nlimited\nlimited\nN\/A\npipelined, supporting both\nonline and offline training\nBenchmarking Agents\n11 tasks\n3000+ tasks\nN\/A\nunlimited with MCU (Lin et al., 2023)\nEvaluating Agents\nN\/A\nreward only\nN\/A\ncustomizable metrics with\ndistributed framework\nBaseline Implementations\nN\/A\nN\/A\nN\/A\nsupports a family of SOTA baselines\ndifficult to replicate. Moreover, the relatively slow speed of the MineRL and MineDojo simulators\nhampers RL training, adding considerable engineering challenges to Minecraft development. LLM-\nbased agents (Wang et al., 2023; Zhu et al., 2023) oversimplify the Minecraft problem by relying\non Mineflayer APIs, whereas our goal is to build embodied agents that mirror the observation and\ncontrol interface of human players. The motivation behind MineStudio is to unify Minecraft\nagent development through an efficient and reliable training template. By integrating MCU (Lin et al.,\n2023), our distributed inference framework, and state-of-the-art (SOTA) baseline implementations,\nwe provide a standardized evaluation paradigm for Minecraft agents.\n4. Conclusions\nIn this work, we introduced MineStudio, a comprehensive and streamlined framework designed\nto advance the development of AI agents in Minecraft. By addressing the significant engineering\nchallenges inherent in creating embodied policies for open-world environments, MineStudio\nbridges the gap between conceptual algorithmic innovations and practical implementation. Its\nmodular design integrates crucial components, such as a flexible simulator wrapper, efficient data\nstructures, pre-integrated models, and high-performance training pipelines, enabling researchers to\nfocus on algorithmic advancements rather than engineering overhead.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MineStudio: A Streamlined Package for Minecraft AI Agent Development.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\n```\n#### 2. 论文摘要\n```\nMinecraft has emerged as a valuable testbed for embodied intelligence and\nsequential decision-making research, yet the development and validation of\nnovel agents remains hindered by significant engineering challenges. This paper\npresents MineStudio, an open-source software package designed to streamline\nembodied policy development in Minecraft. MineStudio represents the first\ncomprehensive integration of seven critical engineering components: simulator,\ndata, model, offline pretraining, online finetuning, inference, and benchmark,\nthereby allowing users to concentrate their efforts on algorithm innovation. We\nprovide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at\nhttps:\/\/github.com\/CraftJarvis\/MineStudio.\n```\n\n#### 3. 论文全文\n```\nMineStudio: A Streamlined Package for\nMinecraft AI Agent Development\nShaofei Cai†1, Zhancun Mu†1, Kaichen He1, Bowei Zhang1, Xinyue Zheng1, Anji Liu2 and Yitao Liang B1\n1Peking University, 2University of California, Los Angeles, All authors are affiliated with Team CraftJarvis\nMinecraft has emerged as a valuable testbed for embodied intelligence and sequential decision-making\nresearch, yet the development and validation of novel agents remains hindered by significant engineering\nchallenges. This paper presents MineStudio, an open-source software package designed to stream-\nline embodied policy development in Minecraft. MineStudio represents the first comprehensive\nintegration of seven critical engineering components: simulator, data, model, offline pretraining, online\nfinetuning, inference, and benchmark, thereby allowing users to concentrate their efforts on the algo-\nrithm innovation. We provide a user-friendly API design accompanied by comprehensive documentation\nand tutorials. The complete codebase is publicly available at https:\/\/github.com\/CraftJarvis\/MineStudio.\n1. Introduction\nAs an open-world 3D sandbox game, Minecraft has garnered widespread acclaim for its high degree of\nfreedom and rich playability. In recent years, with the rapid advancement of general-purpose artificial\nintelligence, Minecraft has gradually emerged as a key testbed for research on embodied intelligence\nand decision-making control (Baker et al., 2022; Fan et al., 2022; Guss et al., 2019; Lifshitz et al.,\n2023). Although the community has introduced a variety of environments (Fan et al., 2022; Guss\net al., 2019; Johnson et al., 2016), datasets (Baker et al., 2022; Fan et al., 2022; Guss et al., 2019),\ntraining algorithms and policy models (Baker et al., 2022; Cai et al., 2023, 2024; Lifshitz et al.,\n2023), and benchmarking methods (Fan et al., 2022; Lin et al., 2023) for this platform, a substantial\nengineering gap remains in the development and validation of novel agents.\nMinecraft’s complexity has led to varied task customizations, hindering progress tracking across\nthe community. For example, Dreamer V3 (Hafner et al., 2023) simplified the environment to destroy\nblocks with a single attack, while Voyager (Wang et al., 2023) relied on the rule-based Mineflayer\ncontroller (PrismarineJS, 2013) to interact with the environment. Moreover, experimenting in\nMinecraft is harder than in other AI domains. For example, despite OpenAI’s 160-million-frame\ndataset, the lack of flexible data structures hampers efficient storage and retrieval. Reinforcement\nlearning shows promise in tackling complex tasks (Baker et al., 2022), but mainstream frameworks\noften lack memory-policy support (Liang et al., 2018; Raffin et al., 2021) and crash safeguards (Raffin\net al., 2021). These require researchers to make great engineering efforts before validating an idea.\nTo address these challenges, we present MineStudio, an open-source software package for\nefficiently developing embodied agents in Minecraft. We also integrate many existing efforts, including\ndata (Baker et al., 2022), environment (Guss et al., 2019; Johnson et al., 2016), and model (Baker\net al., 2022; Cai et al., 2023, 2024; Lifshitz et al., 2023), into a unified framework and present\nthem to users in a concise manner. To minimize redundant engineering work, we have introduced\nseveral key improvements and optimizations: (1) support flexible custom environment configurations;\n(2) a flexible and training-efficient trajectory data structure; (3) a unified policy model template\nwith multiple built-in Minecraft agent algorithms; (4) a simplified offline pre-training process; (5)\na high-performance distributed online fine-tuning pipeline tailored to the Minecraft platform; (6)\na parallel inference pipeline; and (7) the integration of an MCU (Lin et al., 2023) benchmark to\nCorresponding author(s): Yitao Liang\n† indicates co-first author\n{caishaofei, muzhancun, hekaichen, zhangbowei}@stu.pku.edu.cn, zhengxinyue@bigai.ai, liuanji@cs.ucla.edu, yitaol@pku.edu.cn\narXiv:2412.18293v2  [cs.AI]  25 Dec 2024\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nFigure 1 | MineStudio enables users to address classic requirements such as offline pretraining, online\nfine-tuning, system evaluation, and dataset generation with minimal coding effort. Users only need to configure\nthe model component with a small amount of PyTorch code. Each module in the workflow is fully customizable,\nallowing users to configure settings or extend functionality through inheritance and overrides as needed.\nenable fair comparisons. With MineStudio, users can not only conduct interactive evaluations\nof existing mainstream Minecraft agents but also quickly implement a model file using just a few\nlines of PyTorch (Paszke et al., 2019) code, enjoying a fully integrated workflow from pre-training\nthrough finetuning to evaluation. The source code is publicly available at GitHub, accompanied by\ncomprehensive documentation and extensive tutorials to facilitate rapid onboarding.\n2. MineStudio\nThe MineStudio is an efficient toolkit for developing Minecraft AI agents, encompassing the full\nworkflow from data preparation and training to evaluation. It offers a flexible interface design that\nnot only includes various out-of-the-box features but also allows users to tailor the toolkit to their\nspecific requirements. In doing so, it maximizes flexibility while minimizing engineering overhead.\nFigure 1 provides an overview of the pipeline, and subsequent paragraphs detail each module.\nSimulator. This component implements a hook-based Minecraft wrapper that offers a high degree\nof customization. By inheriting from the MinecraftCallback class, users can tailor the environ-\nment in various ways, including (but not limited to) monitoring rendering framerate, issuing cheat\ncommands, simulating rapid resets, logging episodes, generating reward functions, and overriding\nobservations. To further reduce user workload, we provide a set of commonly used callbacks out of the\nbox. In particular, PlayCallback loads a graphical user interface and enables switching between\nmanual and agent-controlled modes, allowing users to interactively inspect model performance. In\naddition, we have integrated a bunch of tricks to improve rendering speed, thereby increasing the\nefficiency of model evaluation, data collection, and reinforcement learning.\nData. This component introduces a flexible and efficient data structure for handling offline trajectory\ndata. Different modalities within the trajectory are segmented into clips and stored independently\nin LMDB files while preserving their temporal relationships. This design seeks to balance storage\nsize and video decoding efficiency, enabling users to retrieve trajectory segments based on semantic\nlabels rapidly. To facilitate the training of models requiring long-term memory, we have implemented\na distributed batch sampler that supports continuous reading of lengthy trajectory sequences. Ad-\nditionally, we provide data format conversion scripts, making it convenient for users to incorporate\ntheir own collected trajectory data, as well as a suite of visualization tools to assist with troubleshoot-\ning. Our framework comes with the largest Contractor Dataset (Baker et al., 2022) available in the\nMinecraft domain, complemented by our pre-processed frame-level semantic segmentation data.\n2\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nThese resources can be utilized directly for training ROCKET-1 models (Cai et al., 2024).\nModel. This component provides a unified template for policy models along with action and value\nheads specifically designed for the Minecraft domain, allowing users to focus solely on their model\narchitecture design. We have pre-integrated models such as VPTs (Baker et al., 2022), STEVE-1\n(Lifshitz et al., 2023), GROOT-1 (Cai et al., 2023), and ROCKET-1 (Cai et al., 2024). The standardized\ninterface functions ensure that policies can be seamlessly integrated with other MineStudio\nmodules, enabling efficient training and inference as well as fair comparisons.\nOffline Training. This component builds upon the PyTorch Lightning (Falcon and team, 2019)\nframework to deliver an enhanced Trainer module. It incorporates dedicated mechanisms for handling\npolicy memory (e.g., TransformerXL (Dai et al., 2019)) and integrates seamlessly with the distributed\nbatch sampler in the MineStudio data module, thereby enabling training on ultra-long trajectories.\nIn addition, we provide a proven set of hyperparameter configurations—covering aspects such as\nwarmup steps, optimizers, and learning rates—while also allowing flexible customization through\nobjective callbacks. This approach maximizes user flexibility while minimizing their overhead.\nOnline Training. This component implements the KL-constrained Proximal Policy Optimization\nalgorithm introduced in Baker et al. (2022). The code is designed for integration with our model and\nsimulator. It has been optimized to deal with long episodes and handle the instability of the Minecraft\nenvironment. Additionally, we provide a set of hyperparameters that have been tested to achieve high\ntraining efficiency and strong performance across various tasks. We hope this resource serves as an\neasy starting point and lowers the barriers to reinforcement learning research in Minecraft.\nInference. This component provides a Ray-based (Moritz et al., 2018) inference framework for\nMineStudio, to support distributed inference. The framework is designed to consist of three\nparts: generator, filter and recorder, forming an asynchronous inference pipeline for easily\nevaluating the performance of different agents. The generator part, equipped with an agent creator\nand an environment creator, generates trajectories in batches. Each produced trajectory is immediately\npassed to a filter for post-processing and then summarized and stored by the recorder. By\ncustomizing the filter and recorder, users can effortlessly conduct comprehensive evaluations\nof policy checkpoints. Furthermore, this pipeline allows for efficient data synthesis, which, when\ncombined with the data module’s conversion scripts, enables a closed-loop data workflow.\nBenchmark. This component evaluates agent performance in MineStudio environments. It\nsupports a variety of tasks such as building, mining, and crafting, and offers both simple and\nchallenging game modes to test agents under different levels of difficulty. The framework includes an\nautomatic evaluation pipeline that leverages Vision-Language Models to analyze task videos, and\nprovides batch task execution capabilities to run multiple tasks simultaneously and record completion\nvideos. Additionally, it offers a quick benchmarking tool that simplifies the process of task execution\nand evaluation, enabling researchers to compare different agent models efficiently.\n3. Comparison to Existing Interest of Minecraft\nTo highlight the contributions of MineStudio, we compare its features with those of other promi-\nnent Minecraft development frameworks, such as MineRL (Guss et al., 2019), MineDojo (Fan et al.,\n2022), and Mineflayer (PrismarineJS, 2013), as shown in table 1. Although these frameworks offer\nvaluable testbeds for Minecraft, they lack a seamless pipeline that integrates building, training, and\nevaluating agents, particularly when it comes to environment customization. While Baker et al. (2022)\nextends MineRL by offering data loading and training code, its dataloader is primarily designed for\ndemonstration purposes and is inefficient, while the RL training process remains closed-source and\n3\nMineStudio: A Streamlined Package for Minecraft AI Agent Development\nTable 1 | Comparison of features across Minecraft development frameworks.\nFeature\nMineRL\nMineDojo\nMineflayer\nMineStudio\nObservation\/Action Space\noriginal\nmodified\nmodified\noriginal\nDataset Features\nannotated state-action pairs\nof human demonstrations\nmultimodal data\nscraped from Internet\nN\/A\nefficient data structure\nfor both storing and loading\nCustomizing Environments\nlimited\neasy\nN\/A\neasy\nCreating Agents\ndifficult\ndifficult\neasy\neasy\nTraining Agents\nlimited\nlimited\nN\/A\npipelined, supporting both\nonline and offline training\nBenchmarking Agents\n11 tasks\n3000+ tasks\nN\/A\nunlimited with MCU (Lin et al., 2023)\nEvaluating Agents\nN\/A\nreward only\nN\/A\ncustomizable metrics with\ndistributed framework\nBaseline Implementations\nN\/A\nN\/A\nN\/A\nsupports a family of SOTA baselines\ndifficult to replicate. Moreover, the relatively slow speed of the MineRL and MineDojo simulators\nhampers RL training, adding considerable engineering challenges to Minecraft development. LLM-\nbased agents (Wang et al., 2023; Zhu et al., 2023) oversimplify the Minecraft problem by relying\non Mineflayer APIs, whereas our goal is to build embodied agents that mirror the observation and\ncontrol interface of human players. The motivation behind MineStudio is to unify Minecraft\nagent development through an efficient and reliable training template. By integrating MCU (Lin et al.,\n2023), our distributed inference framework, and state-of-the-art (SOTA) baseline implementations,\nwe provide a standardized evaluation paradigm for Minecraft agents.\n4. Conclusions\nIn this work, we introduced MineStudio, a comprehensive and streamlined framework designed\nto advance the development of AI agents in Minecraft. By addressing the significant engineering\nchallenges inherent in creating embodied policies for open-world environments, MineStudio\nbridges the gap between conceptual algorithmic innovations and practical implementation. Its\nmodular design integrates crucial components, such as a flexible simulator wrapper, efficient data\nstructures, pre-integrated models, and high-performance training pipelines, enabling researchers to\nfocus on algorithmic advancements rather than engineering overhead.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | “MineStudio：打造Minecraft AI智能体开发的利器”\n\n## 📌 背景痛点\/本文动机\nMinecraft因其开放世界和丰富的游戏性，已成为研究具身智能和顺序决策的重要平台。然而，新型智能体的开发和验证面临着巨大的工程挑战。本文提出了MineStudio，一个开源软件包，旨在简化Minecraft中具身策略的开发流程。MineStudio集成了七个关键的工程组件：模拟器、数据、模型、离线预训练、在线微调、推理和基准测试，使用户能够集中精力进行算法创新。\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\nMineStudio提供了一个钩子式的Minecraft包装器，允许用户高度定制环境，包括监控渲染帧率、发出作弊命令、模拟快速重置等。此外，它还提供了一套常用的回调函数，减少用户的工作量。\n\n💡 创新点2\n该软件包引入了一种灵活高效的数据结构来处理离线轨迹数据，支持对超长轨迹的分布式批量采样，有助于训练需要长期记忆的模型。\n\n💡 创新点3\n提供了一个统一的策略模型模板，以及专门为Minecraft领域设计的动作和价值头，允许用户专注于模型架构设计。\n\n💡 创新点4\n集成了高效的训练和推理管道，支持分布式训练和推理，以及一个自动化的评估管道，可以分析任务视频并提供批量任务执行能力。\n\n## 📈 实验结果\n本文通过与现有Minecraft开发框架的比较，展示了MineStudio在环境定制、智能体创建、训练、评估等方面的优势。它不仅提供了无缝的管道来集成构建、训练和评估智能体，还提供了分布式推理框架和最先进的基线实现，为Minecraft智能体提供了一个标准化的评估范式。\n\n## 💬 可借鉴之处\nMineStudio通过模块化设计，降低了开发Minecraft智能体时的工程难度，使得研究人员可以更多地关注算法创新而不是工程细节。它的开源特性和丰富的文档和教程，为快速上手提供了便利。此外，其高效的训练和推理管道，以及集成的基准测试，为Minecraft智能体的研究和开发提供了有力的支持。","llm_summary_res_status":200}
{"title":"GROOT: Learning to Follow Instructions by Watching Gameplay Videos","authors":"Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"We study the problem of building a controller that can follow open-ended\ninstructions in open-world environments. We propose to follow reference videos\nas instructions, which offer expressive goal specifications while eliminating\nthe need for expensive text-gameplay annotations. A new learning framework is\nderived to allow learning such instruction-following controllers from gameplay\nvideos while producing a video instruction encoder that induces a structured\ngoal space. We implement our agent GROOT in a simple yet effective\nencoder-decoder architecture based on causal transformers. We evaluate GROOT\nagainst open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the\nhuman-machine gap as well as exhibiting a 70% winning rate over the best\ngeneralist agent baseline. Qualitative analysis of the induced goal space\nfurther demonstrates some interesting emergent properties, including the goal\ncomposition and complex gameplay behavior synthesis. The project page is\navailable at https:\/\/craftjarvis-groot.github.io.","url":"http:\/\/arxiv.org\/abs\/2310.08235v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.08235v2","published":1697110261000,"comment":null,"pdf_text":"GROOT: Learning to Follow Instructions by\nWatching Gameplay Videos\nShaofei Cai1, Bowei Zhang1, Zihao Wang1, Xiaojian Ma3, Anji Liu2 and Yitao Liang1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis,\nWe study the problem of building a controller that can follow open-ended instructions in open-world\nenvironments. We propose to follow reference videos as instructions, which offer expressive goal\nspecifications while eliminating the need for expensive text-gameplay annotations. A new learning\nframework is derived to allow learning such instruction-following controllers from gameplay videos\nwhile producing a video instruction encoder that induces a structured goal space. We implement our\nagent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers.\nWe evaluate GROOT against open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap\nas well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis. The project page is available at https:\n\/\/craftjarvis-groot.github.io.\nFigure 1 | Through the cultivation of extensive gameplay videos, GROOT has grown a rich set of skill fruits\n(number denotes success rate; skills shown above do not mean to be exhaustive; kudos to our artist Haowei).\n1. Introduction\nDeveloping human-level embodied agents that can solve endless tasks in open-world environments,\nsuch as Minecraft (Fan et al., 2022; Johnson et al., 2016), has always been a long-term goal pursued\nin AI. Recent works have explored using Large Language Models (LLMs) to generate high-level plans,\nwhich guide the agent to accomplish challenging long-horizon tasks (Wang et al., 2023a,b; Zhu\net al., 2023). However, a major gap between these LLM-based agents and generalist agents that can\ncomplete endless amounts of tasks is the capability of their low-level controllers, which map the plans\nto motor commands. Recently developed controllers are only capable of completing a predefined and\nnarrow set of programmatic tasks (Baker et al., 2022; Cai et al., 2023; Lin et al., 2021), which hinders\nLLM-based planning agents from unleashing their full potential. We attribute the limitation of these\nlow-level controllers to how the goal is specified. Specifically, existing controllers use task indicator\nCorresponding author(s): Yitao Liang\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>,Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2310.08235v2  [cs.AI]  29 Nov 2023\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(Yu et al., 2019), future outcome (Chen et al., 2021; Lifshitz et al., 2023), and language (Brohan\net al., 2022) to represent the goal. While it is easy to learn a controller with some of these goal\nspecifications, they may not be expressive enough for diverse tasks. Taking future outcome goals as an\nexample, an image of a desired house clearly lacks procedural information on how the house was built.\nOne exception is language, but learning a controller that can receive language goals is prohibitively\nexpensive as it requires a huge number of trajectory-text pairs with text that precisely depicts the full\ndetails of the gameplay, therefore preventing them from scaling up to more open-ended tasks.\nHaving observed the limitations of goal specification in the prior works, this paper seeks to find\na balance between the capacity of goal specification and the cost of controller learning. Concretely,\nwe propose to specify the goal as a reference gameplay video clip. While such video instruction is\nindeed expressive, there are two challenges: 1) How can the controller understand the actual goal\nbeing specified as the video itself can be ambiguous, i.e. a goal space or video instruction encoder\nhas to be learned; 2) How to ultimately map such goal to actual motor commands? To this end, we\nintroduce a learning framework that simultaneously produces a goal space and a video instruction\nfollowing controller from gameplay videos. The fundamental idea is casting the problem as future\nstate prediction based on past observations:\n• The predicting model needs to identify which goal is being pursued from the past observations,\nwhich requires a good goal space (induced by a video instruction encoder);\n• Since the transition dynamics model is fixed, a policy that maps both the state and the recognized\ngoal to action is also needed by the predicting model when rolling the future state predictions.\nEffectively, this results in the goal space and control policy we need. We introduce a variational learning\nobjective for this problem, which leads to a combination of a cloning loss and a KL regularization loss.\nBased on this framework, we implement GROOT, an agent with an encoder-decoder architecture to\nsolve open-ended Minecraft tasks by following video instructions. The video encoder is a non-causal\ntransformer that extracts the semantic information expressed in the video and maps it to the latent\ngoal space. The controller policy is a decoder module implemented by a causal transformer, which\ndecodes the goal information in the latent space and translates it into a sequence of actions in the\ngiven environment states in an autoregressive manner.\nTo comprehensively evaluate an agent’s mastery of skills, we designed a benchmark called\nMinecraft SkillForge. The benchmark covers six common Minecraft task groups: collect, build,\nsurvive, explore, tool, and craft, testing the agent’s abilities in resource collection, structure\nbuilding, environmental understanding, and tool usage, in a total of 30 tasks. We calculate Elo\nratings among GROOT, several counterparts, and human players based on human evaluations. Our\nexperiments showed that GROOT is closing the human-machine gap and outperforms the best\nbaseline by 150 points (or 70% winning rate) in an Elo tournament system. Our qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis.\nTo sum up, our main contributions are as follows:\n• Start by maximizing the log-likelihood of future states given past ones, we have discovered the\nlearning objectives that lead to a good goal space and ultimately the instruction-following controller\nfrom gameplay videos. It provides theoretical guidance for the agent architecture design and model\noptimization.\n• Based on our proposed learning framework, we implemented a simple yet efficient encoder-\ndecoder agent based on causal transformers. The encoder is responsible for understanding the goal\ninformation in the video instruction while the decoder as the policy emits motor commands.\n• On our newly introduced benchmark, Minecraft SkillForge, GROOT is closing the human-machine\n2\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ngap and surpassing the state-of-the-art baselines by a large margin in the overall Elo rating com-\nparison. GROOT also exhibits several interesting emergent properties, including goal composition\nand complex gameplay behavior synthesis.\n2. Preliminaries and Problem Formulation\nReinforcement Learning (RL) concerns the problem in which an agent interacts with an environment\nat discrete time steps, aiming to maximize its expected cumulative reward (Espeholt et al., 2018; Mnih\net al., 2015; Schulman et al., 2017). Specifically, the environment is defined as a Markov Decision\nProcess (MDP) ⟨S, A, R, P, 𝑑0⟩, where S is the state space, A is the action space, R : S × A →ℝis\nthe reward function, P : S × A →S is the transition dynamics, and 𝑑0 is the initial state distribution.\nOur goal is to learn a policy 𝜋(𝑎|𝑠) that maximizes the expected cumulative reward 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑡], where\n𝛾∈(0, 1] is a discount factor.\nIn goal-conditioned RL (GCRL) tasks, we are additionally provided with a goal 𝑔∈G (Andrychowicz\net al., 2017; Cai et al., 2023; Ding et al., 2019; Jing et al., 2020, 2021; Liu et al., 2022; Yang et al.,\n2019). And the task becomes learning a goal-conditioned policy 𝜋(𝑎|𝑠, 𝑔) that maximizes the expected\nreturn 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑔\n𝑡], where 𝑟𝑔\n𝑡is the goal-specific reward achieved at time step 𝑡. Apart from being\na new type of RL task, GCRL has been widely studied as a pre-training stage toward conquering\nmore challenging environments\/tasks (Aytar et al., 2018b; Baker et al., 2022; Zhang et al., 2022).\nSpecifically, suppose we are provided with a good goal-condition policy, the goal can be viewed as a\nmeta-action that drives the agent to accomplish various sub-tasks, which significantly simplifies tasks\nthat require an extended horizon to accomplish. Further, when equipped with goal planners, we can\nachieve zero- or few-shot learning on compositional tasks that are beyond the reach of RL algorithms\n(Gong et al., 2023; Huang et al., 2022; Wang et al., 2023a,b; Zhu et al., 2023).\nAt the heart of leveraging such benefits, a key requirement is to have a properly-defined goal\nspace that (i) has a wide coverage of common tasks\/behaviors, and (ii) succinctly describes the task\nwithout including unnecessary information about the state. Many prior works establish goal spaces\nusing guidance from other modalities such as language (Cai et al., 2023; Hong et al., 2020; Stone\net al., 2023) or code (Huang et al., 2023; Wang et al., 2023a). While effective, the requirement on\nlarge-scale trajectory data paired with this auxiliary information could be hard to fulfill in practice.\nInstead, this paper studies the problem of simultaneously learning a rich and coherent goal space\nand the corresponding goal-conditioned policy, given a pre-trained inverse dynamic model and raw\ngameplay videos, i.e. sequences of states {𝑠(𝑖)\n0:𝑇}𝑖collected using unknown policies.\n3. Goal Space Discovery via Future State Prediction\nThis section explains our learning framework: discovering a “good” goal space as well as a video\ninstruction following the controller through the task of predicting future states given previous ones.\nWe start with an illustrative example in Minecraft (Johnson et al., 2016). Imagine that an agent is\nstanding inside a grassland holding an axe that can be used to chop the tree in front of them. Suppose\nin the gameplay video, players either go straight to chop the tree or bypass it to explore the territory.\nIn order to predict future frames, it is sufficient to know (i) which goal (chop tree or bypass tree) is\nbeing pursued by the agent, and (ii) what will happen if the agent chooses a particular option (i.e.\ntransition dynamics). Apart from the latter information that is irrelevant to the past observations, we\nonly need to capture the goal information, i.e. whether the agent decides to chop the tree or bypass\nthe tree. Therefore, the task of establishing a comprehensive yet succinct goal space can be interpreted\nas predicting future states while conditioning on the transition dynamics of the environment.\n3\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFormally, our learning objective is to maximize the log-likelihood of future states given past\nones: log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡). Define 𝑔as a latent variable conditioned on past states (think of it as the\npotential goals the agent is pursuing given past states), the evidence lower-bound of the objective\ngiven variational posterior 𝑞𝜙(𝑔|𝑠0:𝑇) is the following (see Appendix A for the derivation of this and\nthe following equations):\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑔\n𝑝𝜃(𝑠𝑡+1:𝑇, 𝑔|𝑠0:𝑡)\n≥𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇) [log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔)] −𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n,\nwhere 𝐷KL(·∥·) denotes the KL-divergence. Next, we break down the first term (i.e. 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔))\ninto components contributed by the (unknown) goal-conditioned policy 𝜋𝜃(𝑎|𝑠, 𝑔) and the transition\ndynamics 𝑝𝜃(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡) :\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔) =\n𝑇\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) · 𝑝𝜃(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\n≥\n𝑇\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝𝜃(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) + 𝐶\n\u0003\n,\nwhere the constant 𝐶contains terms that depend solely on the environment dynamics and are\nirrelevant to what we want to learn (i.e. the goal space and the goal-conditioned policy). Bring it\nback to the original objective, we have\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇),𝑎𝜏∼𝑝𝜃(·|𝑠0:𝜏+1) [log 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔)]\n|                                                         {z                                                         }\nbehaviour cloning\n−\n𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n|                               {z                               }\ngoal space constraint (KL regularization)\n,\nwhere 𝑞𝜙(·|𝑠0:𝑇) is implemented as a video encoder that maps the whole state sequence into the latent\ngoal space. 𝑝𝜃(·|𝑠0:𝜏+1) is the inverse dynamic model (IDM) that predicts actions required to achieve\na desired change in the states, which is usually a pre-trained model, details are in Appendix C. Thus,\nthe objective can be explained as jointly learning a video encoder and a goal-controller policy through\nbehavior cloning under succinct goal space constraints.\n4. GROOT Architecture Design and Training Strategy\nThis section illustrates how to create an agent (we call it GROOT) that can understand the semantic\nmeaning of a reference video and interact with the environment based on the aforementioned learning\nframework. According to the discussion in Section 3, the learnable parts of GROOT include the\nvideo encoder and the goal-conditioned policy. Recently, Transformer (Vaswani et al., 2017) has\ndemonstrated effectiveness in solving sequential decision-making problems (Brohan et al., 2022; Chen\net al., 2021; Parisotto et al., 2019). Motivated by this, we implement GROOT with transformer-based\nencoder-decoder architecture, as shown in Figure 2. The video encoder is a non-causal transformer\nthat extracts semantic information and generates goal embeddings. The policy is a causal transformer\ndecoder that receives the goal embeddings as the instruction and autoregressively translates the state\nsequence into a sequence of actions. Next, we describe how each module is constructed together with\nthe training strategy.\n4\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nVideo Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑠!\n𝑠\"\n𝑎#\n𝑎$\n𝑎\"\nVideo Encoder\nDecoder as Policy\n𝑠̂#\n𝑠̂%\n𝑠̂\"\n𝑠#\n𝑠%\n𝑠!\n𝑎#\n𝑎%\n𝑎!\n𝑠%\n𝑠!\nBehavior Cloning\nReference Video\nStates\nStates\nRollout Observations\n(a) Training\n(b) Inference\n𝑐̂#\n𝑐̂&\nLearnable Tokens\nLearned Tokens\n1.0\n0.5\n0.0\n𝜇\n𝒒𝒈𝟏:𝑵𝒔𝟏:𝑻)\nsample\n𝑠#\n𝑐#\n𝑐&\n𝑐#\n𝑐&\n𝑐̂#\n𝑐̂&\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n1.0\n0.5\n0.0\n𝜇\n𝒑𝒈𝟏:𝑵𝒔𝟏:𝒕)\n𝑫𝑲𝑳\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n𝑠#\n⋯\n𝝁(⋅)\nFigure 2 | Our GROOT agent architecture. Left: In the training stage, a video encoder (non-causal\ntransformer) learns to extract the semantic meaning and transfer the video (state sequence) into the goal\nembedding space. A goal-conditioned policy (causal transformer) is learned to predict actions following the\ngiven instructions. We learn the agent using behavior cloning under a KL constraint. Right: During the\ninference, any reference video is passed into the video encoder to generate the goal embeddings that drive the\npolicy to interact with the environment.\n4.1. Video Encoder\nThe video encoder includes a Convolutional Neural Network (CNN) to extract spatial information\nfrom image states 𝑠1:𝑇and a non-causal transformer to capture temporal information from videos.\nSpecifically, we use a CNN backbone to extract visual embeddings {𝑥1:𝑇} for all frames. Additionally,\nmotivated by Devlin et al. (2019); Dosovitskiy et al. (2020), we construct a set of learnable embeddings\n(or summary tokens), represented as {𝑐1:𝑁}, to capture the semantic information present in the video.\nThe visual embeddings and summary tokens are passed to a non-causal transformer, resulting in the\noutput corresponding to the summary tokens as {ˆ𝑐1:𝑁}\n𝑥1:𝑇←Backbone(𝑠1:𝑇),\nˆ𝑐1:𝑁←Transformer([𝑥1:𝑇, 𝑐1:𝑁]).\n(1)\nSimilar to VAE (Kingma & Welling, 2013), we assume that the latent goal space follows a Gaussian\ndistribution, hence we use two fully connected layers, 𝜇(·) and 𝜎(·), to generate the mean and\nstandard deviation of the distribution, respectively. During training, we use the reparameterization\ntrick to sample a set of embeddings {𝑔1:𝑁} from the distribution, where 𝑔𝑡∼N (𝜇(ˆ𝑐𝑡), 𝜎(ˆ𝑐𝑡)). During\ninference, we use the mean of the distribution as the goal embeddings, i.e. 𝑔𝑡←𝜇(ˆ𝑐𝑡).\n4.2. Decoder as Policy\nTo introduce our policy module, we start with VPT (Baker et al., 2022), a Minecraft foundation model\ntrained with standard behavioral cloning. It is built on Transformer-XL (Dai et al., 2019) that can\nleverage long-horizon historical states and predict the next action seeing the current observation.\nHowever, the vanilla VPT architecture does not support instruction input. To condition the policy\non goal embeddings, we draw the inspiration from Flamingo (Alayrac et al., 2022), that is, to insert\ngated cross-attention dense layers into every Transformer-XL block. The keys and values in these layers\nare obtained from goal embeddings, while the queries are derived from the environment states\nˆ𝑥(𝑙)\n1:𝑡←GatedXATTN(kv = 𝑔1:𝑁, q = 𝑥(𝑙−1)\n1:𝑡\n; 𝜃𝑙),\n𝑥(𝑙)\n1:𝑡←TransformerXL(qkv = ˆ𝑥(𝑙)\n1:𝑡; 𝜃𝑙),\nˆ𝑎𝑡←FeedForward(𝑥(𝑀)\n𝑡\n),\n(2)\n5\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nwhere the policy reuses the visual embeddings extracted by the video encoder, i.e. 𝑥(0)\n1:𝑡= 𝑥1:𝑡, the\npolicy consists of 𝑀transformer blocks, 𝜃𝑙is the parameter of 𝑙-th block, ˆ𝑎𝑡is the predicted action.\nSince our goal space contains information about how to complete a task that is richer than previous\nlanguage-conditioned policy (Cai et al., 2023; Lifshitz et al., 2023), the cross-attention mechanism is\nnecessary. It allows the GROOT to query the task progress from instruction information based on\npast states, and then perform corresponding behaviors to complete the remaining progress.\n4.3. Training and Inference\nThe training dataset can be a mixture of Minecraft gameplay videos and offline trajectories. For\nthose videos without actions, an inverse dynamic model (Baker et al., 2022) can be used to generate\napproximate actions. Limited by the computation resources, we truncated all the trajectories into\nsegments with a fixed length of 𝑇without using any prior. We denote the final dataset as D =\n{(𝑥1:𝑇, 𝑎1:𝑇)}𝑀, where 𝑀is the number of trajectories. We train GROOT in a fully self-supervised\nmanner while the training process can be viewed as self-imitating, that is, training GROOT jointly\nusing behavioral cloning and KL divergence loss\nL(𝜃, 𝜙) = 𝔼(𝑠,𝑎)∼D\n\"\n𝜆𝐵𝐶\n∑︁\n𝑡\n−log 𝜋𝜃(𝑎𝑡|𝑠1:𝑡, 𝑔) + 𝜆𝐾𝐿\n∑︁\n𝜏\n𝐷𝐾𝐿\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝜏)\u0001\n#\n,\n(3)\nwhere 𝜆𝐵𝐶, 𝜆𝐾𝐿are tradeoff coefficients, 𝑞𝜙is a posterior visual encoder, 𝑝𝜃is a prior video encoder\nwith the same architecture, 𝑔∼𝑞𝜙(·|𝑠0:𝑇). More details are in the Appendix D.\n5. Result\n5.1. Performance on Mastering Minecraft Skills\nMinecraft SkillForge Benchmark. In order to comprehensively evaluate the mastery of tasks by\nagents in Minecraft, we created a diverse benchmark called Minecraft SkillForge. It covers 30 tasks\nfrom 6 major categories of representative skills in Minecraft, including collect, explore, craft,\ntool, survive, and build. For example, the task “dig three down and fill one up” in the build\ncategory asks the agent to first dig three blocks of dirt, then use the dirt to fill the space above; The\ntask of “building a snow golem” ( ) requires the agent to sequentially stack 2 snow blocks (\n) and\n1 carved pumpkin (\n). We put the details of this benchmark in the Appendix H. Apart from some\nrelatively simple or common tasks such as “collect wood” and “hunt animals”, other tasks require the\nagent to have the ability to perform multiple steps in succession.\nWe compare GROOT with the following baselines: (a) VPT (Baker et al., 2022), a foundation\nmodel pre-trained on large-scale YouTube data, with three variants: VPT (fd), VPT(bc), and VPT(rl),\nindicating vanilla foundation model, behavior cloning finetuned model, and RL finetuned model; (b)\nSTEVE-1 (Lifshitz et al., 2023), an instruction-following agent finetuned from VPT, with two variants:\nSTEVE-1 (visual) and STEVE-1 (text) that receives visual and test instructions. More details are in\nAppendix F.1. It is worth noting that GROOT was trained from scratch.\nHuman Evaluation with Elo Rating. We evaluated the relative strength of agents by running an\ninternal tournament and reporting their Elo ratings, as in Mnih et al. (2015). Before the tournament,\neach agent is required to generate 10 videos of length 600 on each task. Note that, all the reference\nvideos used by GROOT are generated from another biome to ensure generalization. Additionally,\nwe also invited 3 experienced players to do these tasks following the same settings. After the video\ncollection, we asked 10 players to judge the quality of each pair of sampled videos from different\n6\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Elo Rating Comparison\n(b) Winning Rate of GROOT vs. Baselines\n(c) Success Rate Comparison\nFigure 3 | Results on Minecraft SkillForge benchmark. Left: Tournament evaluation of GROOT assessed\nby human players. GROOT performs better than state-of-the-art Minecraft agent STEVE-1. A 150-score gap\ncorresponds to a 70% probability of winning. Middle: Winning rate of GROOT v.s. other agents on specific\ntask categories. Colors from red to blue denote a decrease in the winning rate. Apart from the human player,\nGROOT surpasses all other baselines. Right: Success rate on 9 representative tasks. GROOT champions\nprocess-oriented tasks, such as dig three and fill one (\n) and build snow golems ( ).\nagents. Considering the diversity of tasks, we designed specific evaluation criteria for every task to\nmeasure the quality of rollout trajectories. For example, in the task of “build snow golem”, we rank\nthe completion degree of the task in ascending order: no blocks placed, one type of block placed,\ntwo types of blocks placed, and snow golem built successfully. After 1500 comparisons, the Elo\nrating converged as in Figure 3 (left). Although there is a large performance gap compared with\nhuman players, GROOT has significantly surpassed the current state-of-the-art STEVE-1 series and\ncondition-free VPT series on the overall tasks. Additional details are in Appendix G.\nIn Figure 3 (middle), we compare GROOT with other baselines in winning rate on six task groups.\nWe found that except for the performance on craft tasks, where STEVE-1 (visual) outperforms our\nmodel, GROOT achieves state-of-the-art results. In particular, GROOT greatly outperforms other\nbaselines by a large margin on build and tool. For build, the goal space needs to contain more\ndetailed procedural information, which is the disadvantage of methods that use future outcomes as\nthe goal. Moreover, such tasks are distributed sparsely in the dataset, or even absent in the dataset,\nwhich requires the agent to have strong generalization ability. As for craft group, GROOT is not\nsuperior enough, especially on the “crafting table” task. We attribute this to the wide task distribution\nin the dataset. Thus the future outcomes can prompt STEVE-1 to achieve a high success rate.\nProgrammatic Evaluation. To quantitatively compare the performance of the agents, we selected\n9 representative tasks out of 30 and reported the success rate of GROOT, STEVE-1 (visual), and\nVPT (bc) on these tasks in Figure 3 (right). We found that, based on the success rate on tasks such\nas dye and shear sheep(\n), enchat sword (\n), smelt food (\n), use bow (\n), sleep\n(\n), and lead animals (\n), GROOT has already reached a level comparable to that of human\nplayers (100%). However, the success rates for build snow golems ( ) and build obsidian\n(\n) tasks are only 60% and 50%. By observing the generated videos, we found that GROOT cannot\nprecisely identify the items in Hotbar (such as buckets, lava buckets, snow blocks, and pumpkin\nheads), resulting in a low probability of switching to the correct item. STEVE-1 also has the same\nproblem. This may be due to the current training paradigm lacking strong supervisory signals at the\nimage level. Future work may introduce auxiliary tasks such as vision-question answering (VQA) to\nhelp alleviate this phenomenon. Details are in Appendix F.3.\n7\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Random Initialized\n(b) GROOT w\/o KL\n(c) GROOT w\/ KL\n(d) Synthesized Videos\nFigure 4 | t-SNE visualization of the goal space. Each color corresponds to a specific video category. (Left):\nSpace of randomly initialized video encoder. All the videos are entangled together. Middle: Space of GROOT\ntrained with self-supervised learning w\/ and w\/o KL regularization, respectively. The videos are clustered\nbased on their semantics. Visualization shows the subtle differences between the two. Right: Synthesized\nvideos using concatenation manner. The concatenated videos lay on the position between the source videos.\n5.2. Properties of Learned Goal Space\nThis section studies the properties of learned goal space. We used the t-SNE algorithm (van der Maaten\n& Hinton, 2008) to visualize the clustering effect of reference videos encoded in goal space, as in Figure\n4. We select 7 kinds of videos, including craft items, combat enemies, harvest crops,\nhunt animals, chop trees, trade with villagers, and mine ores. These videos are\nsampled from the contractor data (Baker et al., 2022) according to the meta information (details\nare in Appendix F.2). Each category contains 1k video segments. As a control group, in Figure 4\n(left), we showed the initial goal space of the video encoder (with a pre-trained EfficientNet-B0 (Tan\n& Le, 2019) as the backbone) before training. We found that the points are entangled together. After\nbeing trained on offline trajectories, as in Figure 4 (middle), it well understands reference videos and\nclusters them according to their semantics. This proves that it is efficient to learn behavior-relevant\ntask representations using our self-supervised training strategy. The clustering effect is slightly better\nwith KL regularization, though the difference is not very significant. Inevitably, there are still some\nvideos from different categories entangled together. We attribute this to the possibility of overlap\nin the performed behaviors of these videos. For example, chop trees and harvest crops both\nrely on a sequential of “attack” actions.\nCondition on Concatenated Videos. We also study the possibility of conditioning the policy\non concatenated videos. First, we collect 3 kinds of source videos, including chop trees, hunt\nanimals, and trade with villagers. We randomly sampled two videos from sources of chop\ntrees and hunt animals, downsampled and concatenated them into a synthetic video, denoted as\n[chop trees, hunt animals]. By the same token, we can obtain [hunt animals, trade\nwith villagers]. We visualize these videos together with the source videos in Figure 4 (right). We\nfound that the source videos lie far away from each other while the concatenated videos are distributed\nbetween their source videos. Based on this intriguing phenomenon, we infer that concatenated videos\nmay prompt GROOT to solve both tasks simultaneously. To verify this, we evaluate GROOT on\nthree kinds of reference videos, i.e. chop trees, hunt animals, and [chop trees, hunt\nanimals]. We launched GROOT in the forest and in the animal plains, respectively. The collected\nwood and killed mobs are reported in Figure 5. We found that although the concatenated video may\nnot be as effective as raw video in driving an agent to complete a single task (60% of the performance\nof raw video), it does possess the ability to drive the agent to perform multiple tasks. This is an\nimportant ability. As discussed in Wang et al. (2023b), sometimes the high-level planner will propose\nmultiple candidate goals, it will be efficient if the low-level controller can automatically determine\n8\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nchop\nhunt\nconcat\nWood Collected\n11.0\n1.5\n6.5\nchop\nhunt\nconcat\nMobs Killed\n0.5\n2.2\n1.5\nFigure 5 | Comparison on using raw and concate-\nnated reference videos as conditions. Left: Col-\nlected wood in the forest biome. Right: Killed mobs\nin the plains biome. “concat” denotes the reference\nvideo is [chop trees, hunt animals]. Statis-\ntics are measured over 10 episodes.\nw\/ KL\nw\/o KL\nSeagrass Collected\n3.7\n1.8\nw\/ KL\nw\/o KL\nWood Collected\n11.0\n7.3\nw\/ KL\nw\/o KL\nArrows Fired\n10.7\n9.3\nFigure 6 | Ablation study on KL loss.\nAfter being\njointly trained with KL loss, GROOT can collect 2×\nmore seagrass (\n) underwater and 1.5× wood (\n)\nin the forest while the difference is not as impressive\non the use bow (\n) task. Statistics are measured\nover 10 episodes.\ndiamonds distribute in this level\nGROOT\nSTEVE-1\ncondition changed\ncondition changed\ndiamonds distribute in this level\n~𝟏𝟔%\nstuck in the bedrock\nFigure 7 | Results on solving challenging obtain diamond task. The vertical dashed lines represent the\ntime when a certain item is first obtained. Left: GROOT first dug down to the depth of 12 and then mined\nhorizontally to obtain diamonds with an average success rate of 16%. Right: STEVE-1 quickly dug down to the\nspecific depth but struggled to maintain its height.\nwhich to accomplish based on the current observation.\nAblation on KL Divergence Loss. To investigate the role of KL loss in training, we evaluated\nGROOT (w\/ KL) and its variant (w\/o KL) on three tasks: collect seagrass (\n), collect\nwood (\n), and use bow (\n). As shown in Figure 6, we found that introducing the constraint of\nKL loss improved agent performance by 2× and 1.5× in the first two tasks, whereas there was no\nsignificant effect in the use bow task. This may be because the first two tasks require the agent to\ngeneralize the corresponding skills to different terrains (e.g. locating trees in the environment for\ncollecting wood and sinking to specific locations for collecting seagrass). Therefore, it puts higher\ndemands on the agent’s ability to generalize in the goal space, and this is exactly the role played by\nthe KL loss. The use bow task is relatively simple in comparison because it only requires charging\nand shooting the arrow, without considering environmental factors.\n5.3. Combining Skills for Long-horizon Tasks\nIn this section, we explore whether GROOT can combine skills to solve long-horizon tasks, which is\nkey to its integration with a high-level planner. Taking the task of mining diamonds as an example,\nprior knowledge is that diamond ores are generally distributed between the 7th and 14th floors\nunderground, and the probability of appearing in other depths is almost zero. Therefore, the agent\nneeds to first dig down to the specified depth (12) and then maintain horizontal mining. To achieve\nthis, we designed two reference videos, each 128 frames long. One describes the policy of starting\n9\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfrom the surface and digging down, and the other demonstrates the behaviors of horizontal mining.\nWe show an example in Figure 7 (left). In the beginning, GROOT quickly digs down to the specified\ndepth and then switches to horizontal mining mode. It maintains the same height for a long time and\nfound diamonds at 11k steps. In addition, we compared STEVE-1 (visual) under the same setting in\nFigure 7 (right). After switching to the horizontal mining prompt, STEVE-1 maintains its height for a\nshort time before it stuck in the bedrock layer (unbreakable in survival mode), greatly reducing the\nprobability of finding diamonds. This indicates that our goal space is expressive enough to instruct\nthe way of mining, and the policy can follow the instructions persistently and reliably. In contrast,\nSTEVE-1, which relies on future outcomes as a condition, was unable to maintain its depth, despite\nattempts at various visual prompts. We conducted 25 experiments each on GROOT and STEVE-1,\nwith success rates of 16% and 0% for finding diamonds. Additional details are in the Appendix F.4.\n6. Related Works\nPre-train Policy on Offline Data. Pre-training neural networks on web-scale data has been demon-\nstrated as an effective training paradigm in Nature Language Processing (Brown et al., 2020) and\nComputer Vision (Kirillov et al., 2023). Inspired by this, researchers tried to transfer the success\nto the field of decision-making from pre-training visual representations and directly distilling the\npolicy from offline data. As the former, Aytar et al. (2018a); Bruce et al. (2023) leveraged temporal\ninformation present in videos as the supervision signal to learn visual representations. The represen-\ntations are then used to generate intrinsic rewards for boosting downstream policy learning, which\nstill requires expensive online interactions with the environment. Chen et al. (2021); Schmidhuber\n(2019) leveraged scalable offline trajectories to train optimal policy by conditioning it on cumulated\nrewards. Laskin et al. (2022) proposed to learn an in-context policy improvement operator that\ncan distill an RL algorithm in high data efficiency. Reed et al. (2022) learned a multi-task agent\nGato by doing behavior cloning on a large-scale expert dataset. By serializing task data into a flat of\nsequence, they use the powerful transformer architecture to model the behavior distribution. However,\nthese methods either require elaborated reward functions or explicit task definitions. This makes\nit hard to be applied to open worlds, where tasks are infinite while rewards are lacking. Another\ninteresting direction is to use pre-trained language models for reasoning and vision language models\nfor discrimination, to guide the policy in life-long learning in the environment (Di Palo et al., 2023).\nCondition Policy on Goal Space. Researchers have explored many goal modalities, such as\nlanguage (Khandelwal et al., 2021), image (Du et al., 2021), and future video (Xie et al., 2023), to\nbuild a controllable policy. Brohan et al. (2022) collected a large-scale dataset of trajectory-text pairs\nand trained a transformer policy to follow language instructions. Despite the language being a natural\ninstruction interface, the cost of collecting paired training data is expensive. As a solution, Majumdar\net al. (2022) sorted to use hindsight relabeling to first train a policy conditioned on the target image,\nthen aligned text to latent image space, which greatly improves training efficiency. Lifshitz et al.\n(2023) moved a big step on this paradigm by replacing the target image with a 16-frame future video\nand reformulating the modality alignment problem into training a prior of latent goal given text.\nBuild Agents in Minecraft. As a challenging open-world environment, Minecraft is attracting an\nincreasing number of researchers to develop AI agents on it, which can be divided into plan-oriented\n(Wang et al., 2023a,b) and control-oriented methods (Baker et al., 2022; Cai et al., 2023; Lifshitz\net al., 2023) based on their emphasis. Plan-oriented agents aim to reason with Minecraft knowledge\nand decompose the long-horizon task into sub-tasks followed by calling a low-level controller. Control-\noriented works follow the given instructions and directly interact with the environments using low-level\nactions (mouse and keyboard). Baker et al. (2022) pre-trained the first foundation model VPT in\nMinecraft using internet-scale videos. Although it achieves the first obtaining diamond milestone by\n10\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfine-tuning with RL, it does not support instruction input. Lifshitz et al. (2023) created the first agent\nthat can solve open-ended tasks by bridging VPT and MineCLIP (Fan et al., 2022). However, its goal\nspace is not expressive enough and prevents it from solving multi-step tasks.\n7. Limitation and Conclusion\nAlthough GROOT has demonstrated powerful capabilities in expressing open-ended tasks in the\nform of video instructions, training such a goal space remains highly challenging. We found that\nGROOT is quite sensitive to the selection of reference videos, which we attribute to the fact that\nthe goal space trained from an unsupervised perspective may not be fully aligned with the human\nintention for understanding the semantics of the reference video. Therefore, it would be a promising\nresearch direction in the future to use SFT (supervised fine-tuning, Sanh et al. (2021)) and RLHF\n(Ziegler et al., 2019) to align the pre-trained goal space with human preference.\nIn conclusion, we propose a paradigm for learning to follow instructions by watching gameplay\nvideos. We prove that video instruction is a good form of goal space that not only expresses open-ended\ntasks but can be trained through self-imitation (once the IDM is available to label pseudo actions for raw\ngameplay videos). Based on this, we built an encoder-decoder transformer architecture agent named\nGROOT in Minecraft. Without collecting any text-video data, GROOT demonstrated extraordinary\ninstruction-following ability and crowned the Minecraft SkillForge benchmark. Additionally, we also\nshowed its potential as a planner downstream controller in the challenging obtain diamond task.\nWe believe that this training paradigm can be generalized in other complex open-world environments.\n11\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GROOT: Learning to Follow Instructions by Watching Gameplay Videos.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n```\n#### 2. 论文摘要\n```\nWe study the problem of building a controller that can follow open-ended\ninstructions in open-world environments. We propose to follow reference videos\nas instructions, which offer expressive goal specifications while eliminating\nthe need for expensive text-gameplay annotations. A new learning framework is\nderived to allow learning such instruction-following controllers from gameplay\nvideos while producing a video instruction encoder that induces a structured\ngoal space. We implement our agent GROOT in a simple yet effective\nencoder-decoder architecture based on causal transformers. We evaluate GROOT\nagainst open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the\nhuman-machine gap as well as exhibiting a 70% winning rate over the best\ngeneralist agent baseline. Qualitative analysis of the induced goal space\nfurther demonstrates some interesting emergent properties, including the goal\ncomposition and complex gameplay behavior synthesis. The project page is\navailable at https:\/\/craftjarvis-groot.github.io.\n```\n\n#### 3. 论文全文\n```\nGROOT: Learning to Follow Instructions by\nWatching Gameplay Videos\nShaofei Cai1, Bowei Zhang1, Zihao Wang1, Xiaojian Ma3, Anji Liu2 and Yitao Liang1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis,\nWe study the problem of building a controller that can follow open-ended instructions in open-world\nenvironments. We propose to follow reference videos as instructions, which offer expressive goal\nspecifications while eliminating the need for expensive text-gameplay annotations. A new learning\nframework is derived to allow learning such instruction-following controllers from gameplay videos\nwhile producing a video instruction encoder that induces a structured goal space. We implement our\nagent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers.\nWe evaluate GROOT against open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap\nas well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis. The project page is available at https:\n\/\/craftjarvis-groot.github.io.\nFigure 1 | Through the cultivation of extensive gameplay videos, GROOT has grown a rich set of skill fruits\n(number denotes success rate; skills shown above do not mean to be exhaustive; kudos to our artist Haowei).\n1. Introduction\nDeveloping human-level embodied agents that can solve endless tasks in open-world environments,\nsuch as Minecraft (Fan et al., 2022; Johnson et al., 2016), has always been a long-term goal pursued\nin AI. Recent works have explored using Large Language Models (LLMs) to generate high-level plans,\nwhich guide the agent to accomplish challenging long-horizon tasks (Wang et al., 2023a,b; Zhu\net al., 2023). However, a major gap between these LLM-based agents and generalist agents that can\ncomplete endless amounts of tasks is the capability of their low-level controllers, which map the plans\nto motor commands. Recently developed controllers are only capable of completing a predefined and\nnarrow set of programmatic tasks (Baker et al., 2022; Cai et al., 2023; Lin et al., 2021), which hinders\nLLM-based planning agents from unleashing their full potential. We attribute the limitation of these\nlow-level controllers to how the goal is specified. Specifically, existing controllers use task indicator\nCorresponding author(s): Yitao Liang\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>,Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2310.08235v2  [cs.AI]  29 Nov 2023\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(Yu et al., 2019), future outcome (Chen et al., 2021; Lifshitz et al., 2023), and language (Brohan\net al., 2022) to represent the goal. While it is easy to learn a controller with some of these goal\nspecifications, they may not be expressive enough for diverse tasks. Taking future outcome goals as an\nexample, an image of a desired house clearly lacks procedural information on how the house was built.\nOne exception is language, but learning a controller that can receive language goals is prohibitively\nexpensive as it requires a huge number of trajectory-text pairs with text that precisely depicts the full\ndetails of the gameplay, therefore preventing them from scaling up to more open-ended tasks.\nHaving observed the limitations of goal specification in the prior works, this paper seeks to find\na balance between the capacity of goal specification and the cost of controller learning. Concretely,\nwe propose to specify the goal as a reference gameplay video clip. While such video instruction is\nindeed expressive, there are two challenges: 1) How can the controller understand the actual goal\nbeing specified as the video itself can be ambiguous, i.e. a goal space or video instruction encoder\nhas to be learned; 2) How to ultimately map such goal to actual motor commands? To this end, we\nintroduce a learning framework that simultaneously produces a goal space and a video instruction\nfollowing controller from gameplay videos. The fundamental idea is casting the problem as future\nstate prediction based on past observations:\n• The predicting model needs to identify which goal is being pursued from the past observations,\nwhich requires a good goal space (induced by a video instruction encoder);\n• Since the transition dynamics model is fixed, a policy that maps both the state and the recognized\ngoal to action is also needed by the predicting model when rolling the future state predictions.\nEffectively, this results in the goal space and control policy we need. We introduce a variational learning\nobjective for this problem, which leads to a combination of a cloning loss and a KL regularization loss.\nBased on this framework, we implement GROOT, an agent with an encoder-decoder architecture to\nsolve open-ended Minecraft tasks by following video instructions. The video encoder is a non-causal\ntransformer that extracts the semantic information expressed in the video and maps it to the latent\ngoal space. The controller policy is a decoder module implemented by a causal transformer, which\ndecodes the goal information in the latent space and translates it into a sequence of actions in the\ngiven environment states in an autoregressive manner.\nTo comprehensively evaluate an agent’s mastery of skills, we designed a benchmark called\nMinecraft SkillForge. The benchmark covers six common Minecraft task groups: collect, build,\nsurvive, explore, tool, and craft, testing the agent’s abilities in resource collection, structure\nbuilding, environmental understanding, and tool usage, in a total of 30 tasks. We calculate Elo\nratings among GROOT, several counterparts, and human players based on human evaluations. Our\nexperiments showed that GROOT is closing the human-machine gap and outperforms the best\nbaseline by 150 points (or 70% winning rate) in an Elo tournament system. Our qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis.\nTo sum up, our main contributions are as follows:\n• Start by maximizing the log-likelihood of future states given past ones, we have discovered the\nlearning objectives that lead to a good goal space and ultimately the instruction-following controller\nfrom gameplay videos. It provides theoretical guidance for the agent architecture design and model\noptimization.\n• Based on our proposed learning framework, we implemented a simple yet efficient encoder-\ndecoder agent based on causal transformers. The encoder is responsible for understanding the goal\ninformation in the video instruction while the decoder as the policy emits motor commands.\n• On our newly introduced benchmark, Minecraft SkillForge, GROOT is closing the human-machine\n2\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ngap and surpassing the state-of-the-art baselines by a large margin in the overall Elo rating com-\nparison. GROOT also exhibits several interesting emergent properties, including goal composition\nand complex gameplay behavior synthesis.\n2. Preliminaries and Problem Formulation\nReinforcement Learning (RL) concerns the problem in which an agent interacts with an environment\nat discrete time steps, aiming to maximize its expected cumulative reward (Espeholt et al., 2018; Mnih\net al., 2015; Schulman et al., 2017). Specifically, the environment is defined as a Markov Decision\nProcess (MDP) ⟨S, A, R, P, 𝑑0⟩, where S is the state space, A is the action space, R : S × A →ℝis\nthe reward function, P : S × A →S is the transition dynamics, and 𝑑0 is the initial state distribution.\nOur goal is to learn a policy 𝜋(𝑎|𝑠) that maximizes the expected cumulative reward 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑡], where\n𝛾∈(0, 1] is a discount factor.\nIn goal-conditioned RL (GCRL) tasks, we are additionally provided with a goal 𝑔∈G (Andrychowicz\net al., 2017; Cai et al., 2023; Ding et al., 2019; Jing et al., 2020, 2021; Liu et al., 2022; Yang et al.,\n2019). And the task becomes learning a goal-conditioned policy 𝜋(𝑎|𝑠, 𝑔) that maximizes the expected\nreturn 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑔\n𝑡], where 𝑟𝑔\n𝑡is the goal-specific reward achieved at time step 𝑡. Apart from being\na new type of RL task, GCRL has been widely studied as a pre-training stage toward conquering\nmore challenging environments\/tasks (Aytar et al., 2018b; Baker et al., 2022; Zhang et al., 2022).\nSpecifically, suppose we are provided with a good goal-condition policy, the goal can be viewed as a\nmeta-action that drives the agent to accomplish various sub-tasks, which significantly simplifies tasks\nthat require an extended horizon to accomplish. Further, when equipped with goal planners, we can\nachieve zero- or few-shot learning on compositional tasks that are beyond the reach of RL algorithms\n(Gong et al., 2023; Huang et al., 2022; Wang et al., 2023a,b; Zhu et al., 2023).\nAt the heart of leveraging such benefits, a key requirement is to have a properly-defined goal\nspace that (i) has a wide coverage of common tasks\/behaviors, and (ii) succinctly describes the task\nwithout including unnecessary information about the state. Many prior works establish goal spaces\nusing guidance from other modalities such as language (Cai et al., 2023; Hong et al., 2020; Stone\net al., 2023) or code (Huang et al., 2023; Wang et al., 2023a). While effective, the requirement on\nlarge-scale trajectory data paired with this auxiliary information could be hard to fulfill in practice.\nInstead, this paper studies the problem of simultaneously learning a rich and coherent goal space\nand the corresponding goal-conditioned policy, given a pre-trained inverse dynamic model and raw\ngameplay videos, i.e. sequences of states {𝑠(𝑖)\n0:𝑇}𝑖collected using unknown policies.\n3. Goal Space Discovery via Future State Prediction\nThis section explains our learning framework: discovering a “good” goal space as well as a video\ninstruction following the controller through the task of predicting future states given previous ones.\nWe start with an illustrative example in Minecraft (Johnson et al., 2016). Imagine that an agent is\nstanding inside a grassland holding an axe that can be used to chop the tree in front of them. Suppose\nin the gameplay video, players either go straight to chop the tree or bypass it to explore the territory.\nIn order to predict future frames, it is sufficient to know (i) which goal (chop tree or bypass tree) is\nbeing pursued by the agent, and (ii) what will happen if the agent chooses a particular option (i.e.\ntransition dynamics). Apart from the latter information that is irrelevant to the past observations, we\nonly need to capture the goal information, i.e. whether the agent decides to chop the tree or bypass\nthe tree. Therefore, the task of establishing a comprehensive yet succinct goal space can be interpreted\nas predicting future states while conditioning on the transition dynamics of the environment.\n3\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFormally, our learning objective is to maximize the log-likelihood of future states given past\nones: log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡). Define 𝑔as a latent variable conditioned on past states (think of it as the\npotential goals the agent is pursuing given past states), the evidence lower-bound of the objective\ngiven variational posterior 𝑞𝜙(𝑔|𝑠0:𝑇) is the following (see Appendix A for the derivation of this and\nthe following equations):\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑔\n𝑝𝜃(𝑠𝑡+1:𝑇, 𝑔|𝑠0:𝑡)\n≥𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇) [log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔)] −𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n,\nwhere 𝐷KL(·∥·) denotes the KL-divergence. Next, we break down the first term (i.e. 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔))\ninto components contributed by the (unknown) goal-conditioned policy 𝜋𝜃(𝑎|𝑠, 𝑔) and the transition\ndynamics 𝑝𝜃(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡) :\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔) =\n𝑇\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) · 𝑝𝜃(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\n≥\n𝑇\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝𝜃(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) + 𝐶\n\u0003\n,\nwhere the constant 𝐶contains terms that depend solely on the environment dynamics and are\nirrelevant to what we want to learn (i.e. the goal space and the goal-conditioned policy). Bring it\nback to the original objective, we have\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇),𝑎𝜏∼𝑝𝜃(·|𝑠0:𝜏+1) [log 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔)]\n|                                                         {z                                                         }\nbehaviour cloning\n−\n𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n|                               {z                               }\ngoal space constraint (KL regularization)\n,\nwhere 𝑞𝜙(·|𝑠0:𝑇) is implemented as a video encoder that maps the whole state sequence into the latent\ngoal space. 𝑝𝜃(·|𝑠0:𝜏+1) is the inverse dynamic model (IDM) that predicts actions required to achieve\na desired change in the states, which is usually a pre-trained model, details are in Appendix C. Thus,\nthe objective can be explained as jointly learning a video encoder and a goal-controller policy through\nbehavior cloning under succinct goal space constraints.\n4. GROOT Architecture Design and Training Strategy\nThis section illustrates how to create an agent (we call it GROOT) that can understand the semantic\nmeaning of a reference video and interact with the environment based on the aforementioned learning\nframework. According to the discussion in Section 3, the learnable parts of GROOT include the\nvideo encoder and the goal-conditioned policy. Recently, Transformer (Vaswani et al., 2017) has\ndemonstrated effectiveness in solving sequential decision-making problems (Brohan et al., 2022; Chen\net al., 2021; Parisotto et al., 2019). Motivated by this, we implement GROOT with transformer-based\nencoder-decoder architecture, as shown in Figure 2. The video encoder is a non-causal transformer\nthat extracts semantic information and generates goal embeddings. The policy is a causal transformer\ndecoder that receives the goal embeddings as the instruction and autoregressively translates the state\nsequence into a sequence of actions. Next, we describe how each module is constructed together with\nthe training strategy.\n4\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nVideo Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑠!\n𝑠\"\n𝑎#\n𝑎$\n𝑎\"\nVideo Encoder\nDecoder as Policy\n𝑠̂#\n𝑠̂%\n𝑠̂\"\n𝑠#\n𝑠%\n𝑠!\n𝑎#\n𝑎%\n𝑎!\n𝑠%\n𝑠!\nBehavior Cloning\nReference Video\nStates\nStates\nRollout Observations\n(a) Training\n(b) Inference\n𝑐̂#\n𝑐̂&\nLearnable Tokens\nLearned Tokens\n1.0\n0.5\n0.0\n𝜇\n𝒒𝒈𝟏:𝑵𝒔𝟏:𝑻)\nsample\n𝑠#\n𝑐#\n𝑐&\n𝑐#\n𝑐&\n𝑐̂#\n𝑐̂&\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n1.0\n0.5\n0.0\n𝜇\n𝒑𝒈𝟏:𝑵𝒔𝟏:𝒕)\n𝑫𝑲𝑳\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n𝑠#\n⋯\n𝝁(⋅)\nFigure 2 | Our GROOT agent architecture. Left: In the training stage, a video encoder (non-causal\ntransformer) learns to extract the semantic meaning and transfer the video (state sequence) into the goal\nembedding space. A goal-conditioned policy (causal transformer) is learned to predict actions following the\ngiven instructions. We learn the agent using behavior cloning under a KL constraint. Right: During the\ninference, any reference video is passed into the video encoder to generate the goal embeddings that drive the\npolicy to interact with the environment.\n4.1. Video Encoder\nThe video encoder includes a Convolutional Neural Network (CNN) to extract spatial information\nfrom image states 𝑠1:𝑇and a non-causal transformer to capture temporal information from videos.\nSpecifically, we use a CNN backbone to extract visual embeddings {𝑥1:𝑇} for all frames. Additionally,\nmotivated by Devlin et al. (2019); Dosovitskiy et al. (2020), we construct a set of learnable embeddings\n(or summary tokens), represented as {𝑐1:𝑁}, to capture the semantic information present in the video.\nThe visual embeddings and summary tokens are passed to a non-causal transformer, resulting in the\noutput corresponding to the summary tokens as {ˆ𝑐1:𝑁}\n𝑥1:𝑇←Backbone(𝑠1:𝑇),\nˆ𝑐1:𝑁←Transformer([𝑥1:𝑇, 𝑐1:𝑁]).\n(1)\nSimilar to VAE (Kingma & Welling, 2013), we assume that the latent goal space follows a Gaussian\ndistribution, hence we use two fully connected layers, 𝜇(·) and 𝜎(·), to generate the mean and\nstandard deviation of the distribution, respectively. During training, we use the reparameterization\ntrick to sample a set of embeddings {𝑔1:𝑁} from the distribution, where 𝑔𝑡∼N (𝜇(ˆ𝑐𝑡), 𝜎(ˆ𝑐𝑡)). During\ninference, we use the mean of the distribution as the goal embeddings, i.e. 𝑔𝑡←𝜇(ˆ𝑐𝑡).\n4.2. Decoder as Policy\nTo introduce our policy module, we start with VPT (Baker et al., 2022), a Minecraft foundation model\ntrained with standard behavioral cloning. It is built on Transformer-XL (Dai et al., 2019) that can\nleverage long-horizon historical states and predict the next action seeing the current observation.\nHowever, the vanilla VPT architecture does not support instruction input. To condition the policy\non goal embeddings, we draw the inspiration from Flamingo (Alayrac et al., 2022), that is, to insert\ngated cross-attention dense layers into every Transformer-XL block. The keys and values in these layers\nare obtained from goal embeddings, while the queries are derived from the environment states\nˆ𝑥(𝑙)\n1:𝑡←GatedXATTN(kv = 𝑔1:𝑁, q = 𝑥(𝑙−1)\n1:𝑡\n; 𝜃𝑙),\n𝑥(𝑙)\n1:𝑡←TransformerXL(qkv = ˆ𝑥(𝑙)\n1:𝑡; 𝜃𝑙),\nˆ𝑎𝑡←FeedForward(𝑥(𝑀)\n𝑡\n),\n(2)\n5\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nwhere the policy reuses the visual embeddings extracted by the video encoder, i.e. 𝑥(0)\n1:𝑡= 𝑥1:𝑡, the\npolicy consists of 𝑀transformer blocks, 𝜃𝑙is the parameter of 𝑙-th block, ˆ𝑎𝑡is the predicted action.\nSince our goal space contains information about how to complete a task that is richer than previous\nlanguage-conditioned policy (Cai et al., 2023; Lifshitz et al., 2023), the cross-attention mechanism is\nnecessary. It allows the GROOT to query the task progress from instruction information based on\npast states, and then perform corresponding behaviors to complete the remaining progress.\n4.3. Training and Inference\nThe training dataset can be a mixture of Minecraft gameplay videos and offline trajectories. For\nthose videos without actions, an inverse dynamic model (Baker et al., 2022) can be used to generate\napproximate actions. Limited by the computation resources, we truncated all the trajectories into\nsegments with a fixed length of 𝑇without using any prior. We denote the final dataset as D =\n{(𝑥1:𝑇, 𝑎1:𝑇)}𝑀, where 𝑀is the number of trajectories. We train GROOT in a fully self-supervised\nmanner while the training process can be viewed as self-imitating, that is, training GROOT jointly\nusing behavioral cloning and KL divergence loss\nL(𝜃, 𝜙) = 𝔼(𝑠,𝑎)∼D\n\"\n𝜆𝐵𝐶\n∑︁\n𝑡\n−log 𝜋𝜃(𝑎𝑡|𝑠1:𝑡, 𝑔) + 𝜆𝐾𝐿\n∑︁\n𝜏\n𝐷𝐾𝐿\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝜏)\u0001\n#\n,\n(3)\nwhere 𝜆𝐵𝐶, 𝜆𝐾𝐿are tradeoff coefficients, 𝑞𝜙is a posterior visual encoder, 𝑝𝜃is a prior video encoder\nwith the same architecture, 𝑔∼𝑞𝜙(·|𝑠0:𝑇). More details are in the Appendix D.\n5. Result\n5.1. Performance on Mastering Minecraft Skills\nMinecraft SkillForge Benchmark. In order to comprehensively evaluate the mastery of tasks by\nagents in Minecraft, we created a diverse benchmark called Minecraft SkillForge. It covers 30 tasks\nfrom 6 major categories of representative skills in Minecraft, including collect, explore, craft,\ntool, survive, and build. For example, the task “dig three down and fill one up” in the build\ncategory asks the agent to first dig three blocks of dirt, then use the dirt to fill the space above; The\ntask of “building a snow golem” ( ) requires the agent to sequentially stack 2 snow blocks (\n) and\n1 carved pumpkin (\n). We put the details of this benchmark in the Appendix H. Apart from some\nrelatively simple or common tasks such as “collect wood” and “hunt animals”, other tasks require the\nagent to have the ability to perform multiple steps in succession.\nWe compare GROOT with the following baselines: (a) VPT (Baker et al., 2022), a foundation\nmodel pre-trained on large-scale YouTube data, with three variants: VPT (fd), VPT(bc), and VPT(rl),\nindicating vanilla foundation model, behavior cloning finetuned model, and RL finetuned model; (b)\nSTEVE-1 (Lifshitz et al., 2023), an instruction-following agent finetuned from VPT, with two variants:\nSTEVE-1 (visual) and STEVE-1 (text) that receives visual and test instructions. More details are in\nAppendix F.1. It is worth noting that GROOT was trained from scratch.\nHuman Evaluation with Elo Rating. We evaluated the relative strength of agents by running an\ninternal tournament and reporting their Elo ratings, as in Mnih et al. (2015). Before the tournament,\neach agent is required to generate 10 videos of length 600 on each task. Note that, all the reference\nvideos used by GROOT are generated from another biome to ensure generalization. Additionally,\nwe also invited 3 experienced players to do these tasks following the same settings. After the video\ncollection, we asked 10 players to judge the quality of each pair of sampled videos from different\n6\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Elo Rating Comparison\n(b) Winning Rate of GROOT vs. Baselines\n(c) Success Rate Comparison\nFigure 3 | Results on Minecraft SkillForge benchmark. Left: Tournament evaluation of GROOT assessed\nby human players. GROOT performs better than state-of-the-art Minecraft agent STEVE-1. A 150-score gap\ncorresponds to a 70% probability of winning. Middle: Winning rate of GROOT v.s. other agents on specific\ntask categories. Colors from red to blue denote a decrease in the winning rate. Apart from the human player,\nGROOT surpasses all other baselines. Right: Success rate on 9 representative tasks. GROOT champions\nprocess-oriented tasks, such as dig three and fill one (\n) and build snow golems ( ).\nagents. Considering the diversity of tasks, we designed specific evaluation criteria for every task to\nmeasure the quality of rollout trajectories. For example, in the task of “build snow golem”, we rank\nthe completion degree of the task in ascending order: no blocks placed, one type of block placed,\ntwo types of blocks placed, and snow golem built successfully. After 1500 comparisons, the Elo\nrating converged as in Figure 3 (left). Although there is a large performance gap compared with\nhuman players, GROOT has significantly surpassed the current state-of-the-art STEVE-1 series and\ncondition-free VPT series on the overall tasks. Additional details are in Appendix G.\nIn Figure 3 (middle), we compare GROOT with other baselines in winning rate on six task groups.\nWe found that except for the performance on craft tasks, where STEVE-1 (visual) outperforms our\nmodel, GROOT achieves state-of-the-art results. In particular, GROOT greatly outperforms other\nbaselines by a large margin on build and tool. For build, the goal space needs to contain more\ndetailed procedural information, which is the disadvantage of methods that use future outcomes as\nthe goal. Moreover, such tasks are distributed sparsely in the dataset, or even absent in the dataset,\nwhich requires the agent to have strong generalization ability. As for craft group, GROOT is not\nsuperior enough, especially on the “crafting table” task. We attribute this to the wide task distribution\nin the dataset. Thus the future outcomes can prompt STEVE-1 to achieve a high success rate.\nProgrammatic Evaluation. To quantitatively compare the performance of the agents, we selected\n9 representative tasks out of 30 and reported the success rate of GROOT, STEVE-1 (visual), and\nVPT (bc) on these tasks in Figure 3 (right). We found that, based on the success rate on tasks such\nas dye and shear sheep(\n), enchat sword (\n), smelt food (\n), use bow (\n), sleep\n(\n), and lead animals (\n), GROOT has already reached a level comparable to that of human\nplayers (100%). However, the success rates for build snow golems ( ) and build obsidian\n(\n) tasks are only 60% and 50%. By observing the generated videos, we found that GROOT cannot\nprecisely identify the items in Hotbar (such as buckets, lava buckets, snow blocks, and pumpkin\nheads), resulting in a low probability of switching to the correct item. STEVE-1 also has the same\nproblem. This may be due to the current training paradigm lacking strong supervisory signals at the\nimage level. Future work may introduce auxiliary tasks such as vision-question answering (VQA) to\nhelp alleviate this phenomenon. Details are in Appendix F.3.\n7\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Random Initialized\n(b) GROOT w\/o KL\n(c) GROOT w\/ KL\n(d) Synthesized Videos\nFigure 4 | t-SNE visualization of the goal space. Each color corresponds to a specific video category. (Left):\nSpace of randomly initialized video encoder. All the videos are entangled together. Middle: Space of GROOT\ntrained with self-supervised learning w\/ and w\/o KL regularization, respectively. The videos are clustered\nbased on their semantics. Visualization shows the subtle differences between the two. Right: Synthesized\nvideos using concatenation manner. The concatenated videos lay on the position between the source videos.\n5.2. Properties of Learned Goal Space\nThis section studies the properties of learned goal space. We used the t-SNE algorithm (van der Maaten\n& Hinton, 2008) to visualize the clustering effect of reference videos encoded in goal space, as in Figure\n4. We select 7 kinds of videos, including craft items, combat enemies, harvest crops,\nhunt animals, chop trees, trade with villagers, and mine ores. These videos are\nsampled from the contractor data (Baker et al., 2022) according to the meta information (details\nare in Appendix F.2). Each category contains 1k video segments. As a control group, in Figure 4\n(left), we showed the initial goal space of the video encoder (with a pre-trained EfficientNet-B0 (Tan\n& Le, 2019) as the backbone) before training. We found that the points are entangled together. After\nbeing trained on offline trajectories, as in Figure 4 (middle), it well understands reference videos and\nclusters them according to their semantics. This proves that it is efficient to learn behavior-relevant\ntask representations using our self-supervised training strategy. The clustering effect is slightly better\nwith KL regularization, though the difference is not very significant. Inevitably, there are still some\nvideos from different categories entangled together. We attribute this to the possibility of overlap\nin the performed behaviors of these videos. For example, chop trees and harvest crops both\nrely on a sequential of “attack” actions.\nCondition on Concatenated Videos. We also study the possibility of conditioning the policy\non concatenated videos. First, we collect 3 kinds of source videos, including chop trees, hunt\nanimals, and trade with villagers. We randomly sampled two videos from sources of chop\ntrees and hunt animals, downsampled and concatenated them into a synthetic video, denoted as\n[chop trees, hunt animals]. By the same token, we can obtain [hunt animals, trade\nwith villagers]. We visualize these videos together with the source videos in Figure 4 (right). We\nfound that the source videos lie far away from each other while the concatenated videos are distributed\nbetween their source videos. Based on this intriguing phenomenon, we infer that concatenated videos\nmay prompt GROOT to solve both tasks simultaneously. To verify this, we evaluate GROOT on\nthree kinds of reference videos, i.e. chop trees, hunt animals, and [chop trees, hunt\nanimals]. We launched GROOT in the forest and in the animal plains, respectively. The collected\nwood and killed mobs are reported in Figure 5. We found that although the concatenated video may\nnot be as effective as raw video in driving an agent to complete a single task (60% of the performance\nof raw video), it does possess the ability to drive the agent to perform multiple tasks. This is an\nimportant ability. As discussed in Wang et al. (2023b), sometimes the high-level planner will propose\nmultiple candidate goals, it will be efficient if the low-level controller can automatically determine\n8\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nchop\nhunt\nconcat\nWood Collected\n11.0\n1.5\n6.5\nchop\nhunt\nconcat\nMobs Killed\n0.5\n2.2\n1.5\nFigure 5 | Comparison on using raw and concate-\nnated reference videos as conditions. Left: Col-\nlected wood in the forest biome. Right: Killed mobs\nin the plains biome. “concat” denotes the reference\nvideo is [chop trees, hunt animals]. Statis-\ntics are measured over 10 episodes.\nw\/ KL\nw\/o KL\nSeagrass Collected\n3.7\n1.8\nw\/ KL\nw\/o KL\nWood Collected\n11.0\n7.3\nw\/ KL\nw\/o KL\nArrows Fired\n10.7\n9.3\nFigure 6 | Ablation study on KL loss.\nAfter being\njointly trained with KL loss, GROOT can collect 2×\nmore seagrass (\n) underwater and 1.5× wood (\n)\nin the forest while the difference is not as impressive\non the use bow (\n) task. Statistics are measured\nover 10 episodes.\ndiamonds distribute in this level\nGROOT\nSTEVE-1\ncondition changed\ncondition changed\ndiamonds distribute in this level\n~𝟏𝟔%\nstuck in the bedrock\nFigure 7 | Results on solving challenging obtain diamond task. The vertical dashed lines represent the\ntime when a certain item is first obtained. Left: GROOT first dug down to the depth of 12 and then mined\nhorizontally to obtain diamonds with an average success rate of 16%. Right: STEVE-1 quickly dug down to the\nspecific depth but struggled to maintain its height.\nwhich to accomplish based on the current observation.\nAblation on KL Divergence Loss. To investigate the role of KL loss in training, we evaluated\nGROOT (w\/ KL) and its variant (w\/o KL) on three tasks: collect seagrass (\n), collect\nwood (\n), and use bow (\n). As shown in Figure 6, we found that introducing the constraint of\nKL loss improved agent performance by 2× and 1.5× in the first two tasks, whereas there was no\nsignificant effect in the use bow task. This may be because the first two tasks require the agent to\ngeneralize the corresponding skills to different terrains (e.g. locating trees in the environment for\ncollecting wood and sinking to specific locations for collecting seagrass). Therefore, it puts higher\ndemands on the agent’s ability to generalize in the goal space, and this is exactly the role played by\nthe KL loss. The use bow task is relatively simple in comparison because it only requires charging\nand shooting the arrow, without considering environmental factors.\n5.3. Combining Skills for Long-horizon Tasks\nIn this section, we explore whether GROOT can combine skills to solve long-horizon tasks, which is\nkey to its integration with a high-level planner. Taking the task of mining diamonds as an example,\nprior knowledge is that diamond ores are generally distributed between the 7th and 14th floors\nunderground, and the probability of appearing in other depths is almost zero. Therefore, the agent\nneeds to first dig down to the specified depth (12) and then maintain horizontal mining. To achieve\nthis, we designed two reference videos, each 128 frames long. One describes the policy of starting\n9\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfrom the surface and digging down, and the other demonstrates the behaviors of horizontal mining.\nWe show an example in Figure 7 (left). In the beginning, GROOT quickly digs down to the specified\ndepth and then switches to horizontal mining mode. It maintains the same height for a long time and\nfound diamonds at 11k steps. In addition, we compared STEVE-1 (visual) under the same setting in\nFigure 7 (right). After switching to the horizontal mining prompt, STEVE-1 maintains its height for a\nshort time before it stuck in the bedrock layer (unbreakable in survival mode), greatly reducing the\nprobability of finding diamonds. This indicates that our goal space is expressive enough to instruct\nthe way of mining, and the policy can follow the instructions persistently and reliably. In contrast,\nSTEVE-1, which relies on future outcomes as a condition, was unable to maintain its depth, despite\nattempts at various visual prompts. We conducted 25 experiments each on GROOT and STEVE-1,\nwith success rates of 16% and 0% for finding diamonds. Additional details are in the Appendix F.4.\n6. Related Works\nPre-train Policy on Offline Data. Pre-training neural networks on web-scale data has been demon-\nstrated as an effective training paradigm in Nature Language Processing (Brown et al., 2020) and\nComputer Vision (Kirillov et al., 2023). Inspired by this, researchers tried to transfer the success\nto the field of decision-making from pre-training visual representations and directly distilling the\npolicy from offline data. As the former, Aytar et al. (2018a); Bruce et al. (2023) leveraged temporal\ninformation present in videos as the supervision signal to learn visual representations. The represen-\ntations are then used to generate intrinsic rewards for boosting downstream policy learning, which\nstill requires expensive online interactions with the environment. Chen et al. (2021); Schmidhuber\n(2019) leveraged scalable offline trajectories to train optimal policy by conditioning it on cumulated\nrewards. Laskin et al. (2022) proposed to learn an in-context policy improvement operator that\ncan distill an RL algorithm in high data efficiency. Reed et al. (2022) learned a multi-task agent\nGato by doing behavior cloning on a large-scale expert dataset. By serializing task data into a flat of\nsequence, they use the powerful transformer architecture to model the behavior distribution. However,\nthese methods either require elaborated reward functions or explicit task definitions. This makes\nit hard to be applied to open worlds, where tasks are infinite while rewards are lacking. Another\ninteresting direction is to use pre-trained language models for reasoning and vision language models\nfor discrimination, to guide the policy in life-long learning in the environment (Di Palo et al., 2023).\nCondition Policy on Goal Space. Researchers have explored many goal modalities, such as\nlanguage (Khandelwal et al., 2021), image (Du et al., 2021), and future video (Xie et al., 2023), to\nbuild a controllable policy. Brohan et al. (2022) collected a large-scale dataset of trajectory-text pairs\nand trained a transformer policy to follow language instructions. Despite the language being a natural\ninstruction interface, the cost of collecting paired training data is expensive. As a solution, Majumdar\net al. (2022) sorted to use hindsight relabeling to first train a policy conditioned on the target image,\nthen aligned text to latent image space, which greatly improves training efficiency. Lifshitz et al.\n(2023) moved a big step on this paradigm by replacing the target image with a 16-frame future video\nand reformulating the modality alignment problem into training a prior of latent goal given text.\nBuild Agents in Minecraft. As a challenging open-world environment, Minecraft is attracting an\nincreasing number of researchers to develop AI agents on it, which can be divided into plan-oriented\n(Wang et al., 2023a,b) and control-oriented methods (Baker et al., 2022; Cai et al., 2023; Lifshitz\net al., 2023) based on their emphasis. Plan-oriented agents aim to reason with Minecraft knowledge\nand decompose the long-horizon task into sub-tasks followed by calling a low-level controller. Control-\noriented works follow the given instructions and directly interact with the environments using low-level\nactions (mouse and keyboard). Baker et al. (2022) pre-trained the first foundation model VPT in\nMinecraft using internet-scale videos. Although it achieves the first obtaining diamond milestone by\n10\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfine-tuning with RL, it does not support instruction input. Lifshitz et al. (2023) created the first agent\nthat can solve open-ended tasks by bridging VPT and MineCLIP (Fan et al., 2022). However, its goal\nspace is not expressive enough and prevents it from solving multi-step tasks.\n7. Limitation and Conclusion\nAlthough GROOT has demonstrated powerful capabilities in expressing open-ended tasks in the\nform of video instructions, training such a goal space remains highly challenging. We found that\nGROOT is quite sensitive to the selection of reference videos, which we attribute to the fact that\nthe goal space trained from an unsupervised perspective may not be fully aligned with the human\nintention for understanding the semantics of the reference video. Therefore, it would be a promising\nresearch direction in the future to use SFT (supervised fine-tuning, Sanh et al. (2021)) and RLHF\n(Ziegler et al., 2019) to align the pre-trained goal space with human preference.\nIn conclusion, we propose a paradigm for learning to follow instructions by watching gameplay\nvideos. We prove that video instruction is a good form of goal space that not only expresses open-ended\ntasks but can be trained through self-imitation (once the IDM is available to label pseudo actions for raw\ngameplay videos). Based on this, we built an encoder-decoder transformer architecture agent named\nGROOT in Minecraft. Without collecting any text-video data, GROOT demonstrated extraordinary\ninstruction-following ability and crowned the Minecraft SkillForge benchmark. Additionally, we also\nshowed its potential as a planner downstream controller in the challenging obtain diamond task.\nWe believe that this training paradigm can be generalized in other complex open-world environments.\n11\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | GROOT：通过观看游戏视频学习指令遵循\n\n## 📌 背景痛点\/本文动机\n在开放世界环境中，构建能够遵循开放指令的控制器一直是人工智能领域的长期目标。然而，现有的控制器通常只能完成预定义的、有限的程序性任务，这限制了它们在开放世界环境中的应用。本文旨在解决这个问题，提出了一种新的学习框架，通过观看游戏视频来学习指令遵循控制器。\n\n## 🚀 核心方法\n💡 创新点1：将目标指定为参考游戏视频片段，从而提供丰富的目标规范，同时消除对昂贵的文本-游戏注释的需求。\n💡 创新点2：引入了一种新的学习框架，该框架同时产生一个目标空间和一个视频指令遵循控制器，从而实现从游戏视频中学习指令遵循控制器。\n\n## 📈 实验结果\n在Minecraft SkillForge基准测试中，GROOT在整体Elo评分比较中超过了最先进的基线，并且在解决具有挑战性的获取钻石任务中表现出色。\n\n## 💬 可借鉴之处\n本文提出的学习框架和GROOT代理的架构设计为构建能够遵循开放指令的控制器提供了新的思路和方法。此外，本文还展示了目标空间和控制器策略的潜在应用，为解决开放世界环境中的复杂任务提供了新的可能性。","llm_summary_res_status":200}
{"title":"ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment","authors":"Shaofei Cai, Zhancun Mu, Anji Liu, Yitao Liang","summary":"We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.","url":"http:\/\/arxiv.org\/abs\/2503.02505v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2503.02505v1","published":1741087006000,"comment":null,"pdf_text":"ROCKET-2: Steering Visuomotor Policy via\nCross-View Goal Alignment\nShaofei Cai1, Zhancun Mu1, Anji Liu2 and Yitao Liang B1\n1Peking University, 2University of California, Los Angeles, All authors are affiliated with Team CraftJarvis\nWe aim to develop a goal specification method that is semantically clear, spatially sensitive, and intuitive\nfor human users to guide agent interactions in embodied environments. Specifically, we propose a novel\ncross-view goal alignment framework that allows users to specify target objects using segmentation\nmasks from their own camera views rather than the agent’s observations. We highlight that behavior\ncloning alone fails to align the agent’s behavior with human intent when the human and agent camera\nviews differ significantly. To address this, we introduce two auxiliary objectives: cross-view consistency\nloss and target visibility loss, which explicitly enhance the agent’s spatial reasoning ability. According to\nthis, we develop ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an improvement\nin the efficiency of inference 3× to 6×. We show ROCKET-2 can directly interpret goals from human\ncamera views for the first time, paving the way for better human-agent interaction. The project page is\navailable at https:\/\/craftjarvis.github.io\/ROCKET-2\/.\nFigure 1 | Cross-view Goal Alignment as a Human-Friendly Goal Specification Method. The target object\ncan be specified using a segmentation mask from the human’s camera view, while the agent learns to align with\nhuman intent and take actions based on its own observations. Visual landmarks serve as key cues to establish\nspatial relationships across camera views.\n1. Introduction\nLearning an agent to achieve desired goals is a\nlong-standing challenge in the field of embod-\nied intelligence, with significant implications for\nthe development of robots (Brohan et al., 2022,\n2023; Jang et al., 2022) and virtual players (Wang\net al., 2023a,b,c). A key challenge is to find goal\nrepresentations that are (i) flexible for human\nusers to specify and (ii) expressive and precise to\ncapture as many tasks as possible. Most current\napproaches address only one of these aspects. For\nexample, traditional works (Brohan et al., 2022;\nDriess et al., 2023; Lynch et al., 2023) focus on\ntraining agents to follow language instructions.\nAs pointed out in Cai et al. (2024b); Sundaresan\net al. (2024), while language is intuitive, it relies\non numerous prepositions to express spatial re-\nlationships, which can be vague and inefficient.\nFurthermore, language also suffers from the gen-\nCorresponding author(s): Yitao Liang\nShaofei Cai, Zhancun Mu <{caishaofei, muzhancun}@stu.pku.edu.cn>, Anji Liu <liuanji@cs.ucla.edu>, Yitao Liang <yitaol@pku.edu.cn>\narXiv:2503.02505v1  [cs.AI]  4 Mar 2025\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\neralization problem of novel visual concepts (Cai\net al., 2023). Realizing these limitations, some\nworks attempted to introduce visual modalities\ninto goal representations. For example, Sundare-\nsan et al. (2024) employs hand-drawn target\nlayouts in robot manipulation environments to\nrepresent human intent; Gu et al. (2023) uses\nend-effector trajectory sketches for fine-grained\ncontrol of robot arms; and ROCKET-1 (Cai et al.,\n2024b) specifies the objects to interact with by\napplying segmentation masks to the agent’s ob-\nservations. These methods have greatly improved\nthe expressiveness of spatial relationships and\ngeneralization across tasks. However, both trajec-\ntory sketches and object segmentation are closely\ntied to the agent’s current observation, causing\nissues in partially observable 3D worlds. These in-\nclude: (i) goals need to be generated in real-time\nas the agent’s camera view changes; (ii) goals\ncannot be specified when the target is occluded.\nTo strike a balance between expressiveness and\nflexibility, we propose an innovative and user-\nfriendly cross-view goal specification method. It\nallows human users to specify the target object\nusing segmentation masks from their own cam-\nera view, rather than the agent’s camera view.\nThe agent is then trained to align with human\nintent and take actions based on its own obser-\nvations via imitation learning. Decoupling the\ngoal specification from the camera view of the\nagent will significantly enhance the efficiency of\nhuman-agent interaction. However, the partial\nobservability of open worlds makes aligning goals\nacross camera views challenging. This involves\nhandling occlusion, geometric deformation, and\nthe distinction of objects of similar look. In Figure\n1, we show an agent in the left corner and a hu-\nman player standing on a farmland. The human\nintends to command the agent to hunt a sheep\nnear the house, even though the agent cannot\ninitially observe the target sheep. To achieve this,\nthe agent must establish spatial relationships us-\ning shared visual landmarks between the human’s\ncamera view and its own. We find that relying\nsolely on a behavior cloning loss is insufficient.\nTo address these challenges, we highlight an\nimportant property of behavior datasets (Cai\net al., 2024b): The target object remains con-\nsistent across camera views in an interaction\ntrajectory. Motivated by this, we propose two\nauxiliary objective functions: cross-view consis-\ntency loss and target visibility loss, to explicitly\nenhance the agent’s ability to align goals across\ncamera views. Specifically, cross-view consistency\nloss requires the agent to accurately predict the\ntarget object’s centroid point w.r.t. its camera view,\nwhile target visibility loss helps the agent deter-\nmine whether the target object is occluded. To\nfurther leverage temporal consistency, we use a\ncausal Transformer (Vaswani et al., 2017) in the\narchitecture to model the relationship between\npast predictions and the current observation. It\nencourages the agent to maintain tracking even\nwhen the target is occluded. By combining these\nauxiliary losses with behavior cloning loss, we\ndevelop ROCKET-2, a state-of-the-art agent in\nMinecraft. Our experiments show that ROCKET-\n2 can autonomously track the target object as the\ncamera changes, eliminating the need for SAM’s\n(Ravi et al., 2024) real-time semantic segmenta-\ntion, speeding up inference 3× to 6× compared\nto ROCKET-1. To our knowledge, we are the first\nto demonstrate that agents can interpret inten-\ntions from a human’s camera view and make de-\ncisions to achieve expected goals in the embodied\nworld. Extensive visualization experiments and\ncase studies offer deeper insights into its behavior.\nOur contributions are threefold: (1) We in-\ntroduce a user-friendly interface that allows hu-\nmans to specify goals using segmentation masks\nfrom their camera view, paving the way for better\nhuman-agent interaction. (2) We propose cross-\nview consistency loss and target visibility loss to ex-\nplicitly enhance the agent’s ability to align targets\nacross camera views and improve its steerabil-\nity. (3) We train ROCKET-2, an agent that au-\ntonomously tracks targets, eliminating the need\nfor real-time goal segmentation and significantly\nspeeding up inference.\n2. Related Works\nPartial Observability. We address policy learn-\ning in partially observable 3D worlds (Cai et al.,\n2024a; Savva et al., 2019), where the agent re-\nceives only egocentric images rather than the full\n2\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nenvironmental state. Since the movement of the\ncamera view is part of the action space, the policy\nmust actively explore to locate key objects, a com-\nmon challenge in visual navigation (Savva et al.,\n2019) and FPS games (Pearce and Zhu, 2022).\nWhile some methods (Huang et al., 2023; Jiang\net al., 2024) incorporate 3D point clouds to pro-\nvide global context, such data is often unavailable.\nMore commonly, memory-based architectures in-\ntegrate historical observations to build implicit\n3D perception. For example, Gadre et al. (2022);\nZhao et al. (2023) use RNN to avoid redundant ex-\nploration, and Baker et al. (2022) employs Trans-\nformerXL to retain achievements and complete\nthe long-horizon diamond challenge in Minecraft\n(Guss et al., 2019). Building on this, we require\nthe policy to align objects across camera views to\nensure consistent tracking of targets as the cam-\nera view changes. Although cross-view alignment\nhas been explored in computer vision for tasks\nlike BEV segmentation (Borse et al., 2023) and\npedestrian re-identification (Xu et al., 2019), we\nare the first to study its application in open-world\ndecision-making.\nGoal-Conditioned Imitation Learning. GCIL\nrefers to algorithms that optimize conditional poli-\ncies through imitation learning, primarily using\nbehavior cloning loss (Pomerleau, 1988). The\npolicy conditions can take various forms, such as\nlanguage (Brohan et al., 2022, 2023; Lynch et al.,\n2023), images (Lifshitz et al., 2023; Majumdar\net al., 2022; Sundaresan et al., 2024), videos (Cai\net al., 2023, 2024c), or trajectory sketches (Gu\net al., 2023; Wang et al.). Compared to tradi-\ntional imitation learning, GCIL provides a more\nexplicit target during training, reducing the com-\nplexity of modeling the entire behavior space and\nmaking the policy steerable during inference.\nHumans often use language to express their\nintentions, leading many studies (Brohan et al.,\n2022; Driess et al., 2023; Padalkar et al., 2023;\nWang et al., 2023c) to focus on learning language-\nconditioned policies. However, language goals\nare often ambiguous and struggle to capture spa-\ntial details (Cai et al., 2024b; Gu et al., 2023).\nIn tasks like navigation (Majumdar et al., 2022)\nand object manipulation (Wang et al.), image-\nbased goal modalities have been explored, where\nusers provide target images to guide the policy.\nWhile effective at conveying spatial information,\nimages often over-specify details, making policies\nsensitive to irrelevant factors like lighting, object\nappearance, or background textures. Sundaresan\net al. (2024) addresses these issues by replacing\nthe image with hand-drawn sketches. However,\ngenerating sketches that align with the current\nstate and the desired goal is non-trivial. Gu et al.\n(2023) further proposes using trajectory sketches\nfor finer control and better generalization across\ntasks, but this approach is not applicable in par-\ntially observable 3D worlds.\nROCKET-1 (Cai et al., 2024b) tackles inter-\naction problems in 3D worlds by training a vi-\nsuomotor policy to identify interaction targets\nbased on semantic segmentations in the visual\ncontext. While it resolves traditional goal images’\nambiguity and generation challenges, it relies\non SAM-2 (Ravi et al., 2024) to track goals and\nsegment them during inference, severely limit-\ning real-time performance. We propose a cross-\nview segmentation-conditioned policy that, un-\nlike ROCKET-1, enables the policy to align the\ngoal across camera views by itself and removes\nthe need for real-time segmentation.\nHindsight Trajectory Relabeling.\nThere are\ntwo main approaches to collecting labeled tra-\njectory data: (1) providing instructions for con-\ntractors to collect trajectories in real-time, en-\nsuring a causal link between actions and labels\nbut incurring high costs and limited scalability;\nand (2) gathering large amounts of trajectories\nand generating labels through post-processing,\nknown as hindsight trajectory relabeling. While\nthe first approach (Lynch et al., 2023; Padalkar\net al., 2023) produces higher-quality data, its\ncost constraints have led most research to adopt\nthe second, more scalable one. Andrychowicz\net al. (2017) was the first to reinterpret a trajec-\ntory’s behavior using its final frame, greatly im-\nproving data utilization and inspiring subsequent\nresearch on goal-conditioned policies. Lifshitz\net al. (2023) extended the approach by using the\nlast 16 frames of a trajectory as the more expres-\nsive goal. Sundaresan et al. (2024) introduced\nhand-drawn sketches as a goal modality, reducing\nsemantic ambiguity. Gu et al. (2023) converted\n3\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nthe robotic end-effector moving sketch to a 2D\nimage as the goal, providing richer procedural\ndetails. Cai et al. (2024b) introduced backward\ntrajectory relabeling, which first identifies inter-\naction objects and events, then utilizes object\ntracking models (Ravi et al., 2024) to generate\nframe-level segmentations. This data supports\ntraining segmentation-conditioned policies. Our\npaper explores using this dataset to train policies\nfor cross-view goal alignment.\n3. Methods\nIn this section, we first introduce the problem of\ncross-view segmentation-conditioned policy, dis-\ncussing it from the perspective of imitation learn-\ning. Next, we describe the process of generating\ncross-view trajectories annotated with semantic\nsegmentation. We then present two auxiliary ob-\njectives designed to enhance cross-view object\nalignment in 3D scenes: the cross-view consistency\nloss and the target visibility loss. Finally, we de-\ntail the architecture of ROCKET-2 and outline the\noverall optimization objectives.\nProblem Statement.\nOur goal is to learn a\ngoal-conditioned visuomotor policy, which al-\nlows humans to specify goal objects for inter-\naction using semantic segmentation across cam-\nera views. Formally, we aim to learn a policy\n𝜋cross(𝑎𝑡|𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}, 𝑐𝑔), where 𝑎𝑡represents the\naction at time 𝑡, 𝑐𝑔denotes the type of interac-\ntion. In the Minecraft environment, an action\ncorresponds to raw mouse and keyboard inputs.\n𝑜𝑡∈ℝ𝐻×𝑊×3 denotes the environment observa-\ntion at time 𝑡, and 𝑜𝑔∈ℝ𝐻×𝑊×3 represents an\nobservation of the local environment from a spe-\ncific camera view. Generally, 𝑜𝑔and 𝑜𝑡have some\nvisual content overlap. 𝑚𝑔∈{0, 1}𝐻×𝑊×1 is a seg-\nmentation mask for 𝑜𝑔, highlighting the target\nobject within the camera view 𝑜𝑔. During infer-\nence, users select a view 𝑜𝑔containing the desired\nobject from historical observations returned by\nthe environment and generates its correspond-\ning semantic segmentation 𝑚𝑔. To train such vi-\nsuomotor policy, we assume access to a dataset\nDcross = {𝑐𝑛, (𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡, 𝑚𝑛\n𝑡)𝐿(𝑛)\n𝑡=1 }𝑁\n𝑛=1 consisting of 𝑁\nsuccessful demonstration episodes, 𝐿(𝑛) is the\nlength of episode 𝑛. Within each episode, if\n𝑚𝑡is non-empty, all (𝑜𝑡, 𝑚𝑡) pairs indicate the\nsame object. Consequently, we can arbitrarily\npick one observation frame as the goal view con-\ndition for the entire trajectory.\nCross-View Dataset Generation. Without loss\nof generality, we use the Minecraft world as an\nexample to illustrate the data generation process.\nManually collecting datasets that meet the re-\nquirements is highly expensive. Thus, we employ\nthe backward trajectory relabeling technique pro-\nposed in Cai et al. (2024b) to automate the anno-\ntation of the OpenAI Contractor Dataset (Baker\net al., 2022), which consists of free-play trajecto-\nries from human players: Draw = {(𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡)𝐿(𝑛)\n𝑡=1 }𝑁\n𝑛=1.\nSpecifically, for any given episode 𝑛, we first de-\ntect all frames 𝑜𝑛\n𝑗where interaction events oc-\ncur, identify the interaction type 𝑐𝑛\n𝑗, and localize\nthe interacted object near frame 𝑗using bound-\ning boxes and point-based prompts. The SAM-2\n(Ravi et al., 2024) model is then employed to gen-\nerate the segmentation mask 𝑚𝑛\n𝑗for the object.\nStarting from frame 𝑗, we traverse the trajectory\nbackward and use the SAM-2 model to continu-\nously generate segmentation masks for the object\nin real-time until either a new interaction event\nis encountered or a maximum tracking length\nis reached. Let 𝑖denote the end frame. The re-\nsulting trajectory clip is then added to the train-\ning dataset: Dcross ←Dcross ∪{𝑐𝑗, (𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡, 𝑚𝑛\n𝑡) 𝑗\n𝑡=𝑖}.\nThis ensures that every extracted clip is associ-\nated with a consistent interaction intent. The\ngenerated data encompasses the fundamental in-\nteraction types in Minecraft, including use, break,\napproach, craft, and kill entity. Among these, ap-\nproach is a unique event, identified by detecting\ntrajectory clips where the displacement exceeds\na specified threshold. The object located at the\ncenter of the clip’s final frame is designated as\nthe goal of the approach event.\nCross-View Consistency Loss. Accurately in-\nterpreting the cross-view goal requires the pol-\nicy to possess cross-view visual object alignment\nability in 3D scenes. To achieve this, the model\nmust fully exploit visual cues from different cam-\nera views, such as scene layout and landmark\nbuildings, while being robust to challenges like\nocclusion, shape variations, and changes in dis-\ntance. We observe that relying solely on behav-\n4\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nT E M P O R A L \nt=10\nt=15\nt=22\nt=92\nt=94\nt=98\nDECODE\nDECODE\nDECODE\nDECODE\nDECODE\nDECODE\nKV Cached Causal Transformer\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n𝒐𝟏𝟎\n𝒐𝟏𝟓\n𝒐𝟐𝟐\n𝒐𝟗𝟐\n𝒐𝟗𝟒\n𝒐𝟗𝟖\n𝒄𝒈\"\n𝒄𝒈\"\n𝒄𝒈\"\n𝒄𝒈\"\"\n𝒄𝒈\"\"\n𝒄𝒈\"\"\n𝒂𝟏𝟎\n𝒑𝟏𝟎\n𝒗𝟏𝟎\n𝒂𝟏𝟓\n𝒑𝟏𝟓\n𝒗𝟏𝟓\n𝒂𝟐𝟐\n𝒑𝟐𝟐\n𝒗𝟐𝟐\n𝒂𝟗𝟐\n𝒑𝟗𝟐\n𝒗𝟗𝟐\n𝒂𝟗𝟒\n𝒑𝟗𝟒\n𝒗𝟗𝟒\n𝒂𝟗𝟖\n𝒑𝟗𝟖\n𝒗𝟗𝟖\nFigure 2 | ROCKET-2 Architecture. It consists of three parts: (1) a non-causal transformer for spatial fusion,\nwhich establishes the relationship between the agent’s and human’s camera views; (2) a causal transformer for\ntemporal fusion, ensuring consistency for goal tracking; (3) a decoder module, made of a feedforward neural\nnetwork (FFN), which predicts goal-related visuals cues and actions.\nior cloning loss (Pomerleau, 1988) is insufficient.\nTherefore, we propose a cross-view consistency\nloss. Since the segmentation across different cam-\nera views corresponds to the same object, we\ntrain the model to condition on the segmentation\nfrom one camera view to generate the segmen-\ntation for another camera view, thereby directly\nenhancing the model’s 3D spatial perception. To\nreduce computational complexity, we opt to pre-\ndict the centroid of the segmentation mask in-\nstead of the complete mask, formally expressed\nas: 𝜋cross(𝑝𝑡| 𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}, 𝑐𝑔), where\n𝑝𝑡=\nÍ𝐻\n𝑖=1\nÍ𝑊\n𝑗=1(𝑖, 𝑗) · 𝑚𝑡(𝑖, 𝑗)\nÍ𝐻\n𝑖=1\nÍ𝑊\n𝑗=1 𝑚𝑡(𝑖, 𝑗)\n.\n(1)\nIt is worth noting that incorporating the histor-\nical observation sequence 𝑜1:𝑡−1 as input is es-\nsential, especially when there is limited shared\nvisual content between 𝑜𝑡and 𝑜𝑔. This histori-\ncal sequence acts as a smooth bridge to facilitate\nalignment. Since the goal object represented by\nthe segmentation corresponds to the target of the\npolicy’s interaction, this auxiliary task aligns the\npolicy’s actions with its visual focus, effectively\nimproving task performance.\nTarget Visibility Loss. Due to the partial ob-\nservability in 3D environments, it is common for\ntarget objects in interaction trajectories to disap-\npear from the field of view and reappear later.\nDuring such intervals, the segmentation mask for\nthe missing object is empty. To leverage this infor-\nmation, we propose training the model to predict\nwhether the target object is currently visible, for-\nmulated as: 𝜋cross(𝑣𝑡| 𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}), where 𝑣𝑡is\na binary indicator for empty segmentation masks.\nOn the one hand, accurately predicting object\nvisibility helps the policy better match the target\nobject, avoiding a simple appearance similarity\nmeasurement between two frames. On the other\nhand, visibility information guides the policy to\nmake more reasonable decisions, such as confi-\ndently approaching the goal object when it is\nvisible or actively adjusting its camera to explore\nwhen the target is absent.\nROCKET-2 Architecture. Lets a training trajec-\ntory 𝑛be denoted as (𝑐𝑔, {𝑜𝑡, 𝑚𝑡}𝐿(𝑛)\n𝑡=1 ). A cross view\nindex 𝑔is sampled from {𝑖|𝑖∈[1, 𝐿(𝑛)], 𝑚𝑖≠𝜙}.\nWe resize all visual observations and their seg-\nmentation masks to 224 × 224. For encoding\nthe visual observation 𝑜𝑡, we utilize a DINO-\npretrained (Caron et al., 2021) 3-channel ViT-\nB\/16 (Dosovitskiy et al., 2020) (16 is the patch\nsize), which outputs a token sequence of length\n196, denoted as {ˆ𝑜𝑖\n𝑡}196\n𝑖=1 . Similarly, we encode the\nsegmentation mask 𝑚𝑡using a 1-input-channel\nViT-tiny\/16, yielding { ˆ𝑚𝑖\n𝑡}196\n𝑖=1 . The ViT-base\/16\nencoder is frozen during training for efficiency,\nwhile the ViT-tiny\/16 is trainable. To ensure spa-\ntial alignment, we fuse the cross-view condition\n(𝑜𝑔, 𝑚𝑔) by concatenating the feature channels:\nℎ𝑖\n𝑔= FFN(concat([ˆ𝑜𝑖\n𝑔∥ˆ𝑚𝑖\n𝑔])).\n(2)\n5\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nGiven the ability of self-attention mechanisms to\ncapture spatial details across views, we concate-\nnate the token sequences from two views into\na single sequence of length 392. A non-causal\nTransformer encoder module is applied (Vaswani\net al., 2017) for spatial fusion, obtaining a frame-\nlevel representation 𝑥𝑡:\n𝑥𝑡←SpatialFusion({ˆ𝑜𝑖\n𝑡}196\n𝑖=1 , {ℎ𝑖\n𝑔}196\n𝑖=1 ).\n(3)\nSubsequently,\nwe leverage a causal Trans-\nformerXL (Dai et al., 2019) architecture to cap-\nture temporal information across the sequence:\n𝑓𝑡←TransformerXL({𝑥𝑖}𝑡\n𝑖=1, 𝑐𝑔).\n(4)\nFinally, a simple linear layer maps 𝑓𝑡to predict\nthe action ˆ𝑎𝑡, centroid ˆ𝑝𝑡, and visibility ˆ𝑣𝑡. The\nloss function for episode 𝑛is defined as:\nL(𝑛) =\n𝐿(𝑛)\n∑︁\n𝑡=1\n−𝑎𝑛\n𝑡log ˆ𝑎𝑛\n𝑡−𝑝𝑛\n𝑡log ˆ𝑝𝑛\n𝑡−𝑣𝑛\n𝑡log ˆ𝑣𝑛\n𝑡. (5)\n4. Experiments\nWe aim to address the following questions: (1)\nHow does ROCKET-2 perform in terms of both\naccuracy and efficiency during inference? (2) Can\nROCKET-2 follow the intention of a human from\na cross-camera view? (3) Under what circum-\nstances does ROCKET-2 fail to work? (4) How\nimportant are landmarks in cross-view goal align-\nment? (5) Can ROCKET-2 interpret goal views\nfrom cross-episode scenarios? (6) Which modules\ncontribute effectively to training ROCKET-2?\n4.1. Experimental Setup\nImplementation Details. We present the model\narchitecture, hyperparameters, and optimizer\nconfigurations of ROCKET-2 in Table 1. Dur-\ning training, each trajectory is divided into seg-\nments of length 128 to reduce memory require-\nments. We initialize the view backbone that is\nused to encode RGB images with DINO weights\nand freeze it for training efficiency. During infer-\nence, ROCKET-2 can access up to 128 key-value\nattention caches of past observations. Most train-\ning parameters follow those from prior works such\nas Baker et al. (2022); Cai et al. (2024b).\nhunt the sheep in the \nright fence\nhunt the cow while do not \ntouch the sheep\ninteract with the left \nchest\nplace the oak door on the \ndiamond block\nuse bucket to get lava\nset fire on a trunk\nFigure 3 | The Evaluation Metric is Spatial-Sensitive.\n✓and × indicate the correct and incorrect objects for\ninteraction, respectively. None of the task configura-\ntions were seen during the training.\nTable 1 | Detailed Training Hyperparameters.\nHyperparameter\nValue\nInput Image Size\n224 × 224\nHidden Dimension\n1024\nView Backbone\nViT-base\/16 (DINO-v1)\nMask Backbone\nViT-tiny\/16 (1-channel)\nSpatial Transformer\nPyTorch Transformer\nNumber of Spatial Blocks\n4\nTemporal Transformer\nTransformerXL\nNumber of Temporal Blocks\n4\nTrajectory Chunk size\n128\nOptimizer\nAdamW\nLearning Rate\n0.00004\nEnvironment and Benchmark. We use the orig-\ninal Minecraft 1.16.5 (Cai et al., 2024a; Guss\net al., 2019; Lin et al., 2023) as our testing en-\nvironment, which accepts mouse and keyboard\ninputs and outputs a 640×360 RGB image at each\nstep. Following Cai et al. (2024b), we employ the\nMinecraft Interaction Benchmark to evaluate the\nagent’s interaction capabilities. This benchmark\nincludes six categories and a total of 12 tasks,\ncovering all basic Minecraft interaction types:\nHunt, Mine, Interact, Navigate, Tool, and Place.\nAs this benchmark emphasizes object interaction\nand spatial localization, its evaluation criteria are\nmore stringent than those in Lifshitz et al. (2023)\nand Cai et al. (2023). We show some examples in\nFigure 3. In the “hunt the sheep in the right fence”\ntask, success requires the agent to kill the sheep\nwithin the right fence, while killing it in the left\nfence results in failure. Similarly, in the “place\nthe oak door on the diamond block” task, success\nis only achieved if the oak door is adjacent to the\n6\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nFigure 4 | Performance-Efficiency Comparison on the Minecraft Interaction Benchmark. The x-axis\nrepresents inference speed (FPS), and the y-axis shows the interaction success rate. Numbers in parentheses\nindicate the Molmo invocation interval, where larger values mean higher FPS. “+ track” denotes real-time\nSAM-2 segmentation between Molmo calls, increasing inference time (applicable only to ROCKET-1). In most\ncases, ROCKET-2 achieves 3× to 6× faster while matching or surpassing ROCKET-1’s peak performance.\ndiamond block on at least one side.\nBaselines. We compare our ROCKET-2 with\nthe following instruction-following baselines: (1)\nSTEVE-1 (Lifshitz et al., 2023): An instruction-\nfollowing agent fine-tuned from VPT (Baker et al.,\n2022), capable of solving various short-horizon\ntasks. We use the text-conditioned version of\nSTEVE-1 for comparison. (2) GROOT-1 (Cai et al.,\n2023): A reference-video-conditioned policy de-\nsigned for open-ended tasks, trained on 2,000\nhours of long-form videos using latent variable\nmodels. (3) ROCKET-1 (Cai et al., 2024b): A\nsegmentation-conditioned policy capable of mas-\ntering 12 interaction tasks. While it achieves a\nhigh interaction success rate, its reliance on SAM-\n2’s real-time tracking during inference creates an\nefficiency bottleneck.\n4.2. Performance-Efficiency Analysis\nWe demonstrate that cross-view goal specification\n(implemented as ROCKET-2) significantly im-\nproves inference speed while maintaining high\ninteraction success. Following Cai et al. (2024b),\nwe evaluate STEVE-1, GROOT-1, and variants of\nROCKET-1 and our ROCKET-2 on the Minecraft\nInteraction Benchmark. Due to the limited spatial\nreasoning ability, STEVE-1 and GROOT-1 achieve\nsuccess rates below 20% on most tasks. We il-\nlustrate the inference pipeline of ROCKET-series\nTable 2 | ROCKET-Series Inference Pipeline Details.\nMolmo can pinpoint the target object based on the task\nprompt. SAM uses the point to generate object mask\n𝑚𝑡w.r.t. 𝑜𝑡and supports real-time object tracking.\nModel\nInference Pipeline\nR1(3)\nm1 ←SAM(𝑜1, Molmo(𝑜1, prompt))\n𝜋𝑅1(𝑎𝑡|𝑜1, m1, 𝑜2, 𝑜3, 𝑜4, m4, 𝑜5, 𝑜6, 𝑜7, m7, · · · )\nR1(30)+track\nm1:30 ←SAM(𝑜1:30, Molmo(𝑜1, prompt))\n𝜋𝑅1(𝑎𝑡|𝑜1, m1, 𝑜2, m2, 𝑜3, m3, 𝑜4, m4, · · · )\nR2(60)\nm1 ←SAM(𝑜1, Molmo(𝑜1, prompt))\n𝜋𝑅2(𝑎𝑡|𝑜1, m1, 𝑜2, 𝑜3, 𝑜4, · · · , 𝑜60, 𝑜61, m61, · · · )\nagents in Table 2. Automated evaluation of ROCK-\nETs relies on Molmo (Deitke et al., 2024) and\nSAM (Ravi et al., 2024) to generate a segmenta-\ntion mask for the target object in the given views.\nIn general, ROCKET-1 requires object masks for\nall agent observations, whereas ROCKET-2 only\nneeds one or a few object masks. We observe\nthat ROCKET-1 attains over 80% success with\nhigh-frequency Molmo point corrections (every 3\nframes) but suffers from slow inference. Lowering\nMolmo’s frequency greatly degrades ROCKET-1’s\nperformance, with tasks like “collecting lava” fail-\ning entirely at 30-frame intervals. While one can\nenable SAM’s tracking mode to provide dense\ngoal signals, it remains computationally expen-\nsive. In contrast, our ROCKET-2 decouples the\ngoal specification from the agent view, it does not\nneed frequent segmentation mask modification\n7\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nHuman View\nBird-Eye Map\nAgent View, t=96\nAgent View, t=109\nAgent View, t=125\nAgent View, t=143\nPredicted Point\nPredicted Point\nPredicted Point\nPredicted Point\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTrajectories\nHuman Position\nAgent Position\nGoal Mask\nHuman View\nBird-Eye Map\nAgent View, t=4\nAgent View, t=27\nAgent View, t=81\nAgent View, t=157\nHuman \nPosition\nAgent Position\nTrajectories\nPredicted Point\nPredicted Point\nPredicted Point\nPredicted Point\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTarget Visibility\nGoal Mask\nDistractor:\nFigure 5 | Case Study of Human-Agent Interaction. We demonstrate how a human interacts with ROCKET-2,\nleveraging its spatial reasoning abilities. (Top Row) The human specifies a hay bale (\n) that is not visible\nto ROCKET-2. By exploring the area around the visible landmark (house), ROCKET-2 successfully locates\nthe goal. (Bottom Row) The human specifies a target tree in the presence of a tree distractor. ROCKET-2\naccurately identifies the correct tree by reasoning about spatial relationships and landmarks. The agent’s\ntrajectories are visualized in bird’s-eye view maps.\nand can autonomously track the target object. It\nachieves comparable or superior performance to\nROCKET-1 with a 3× to 6× inference speedup.\n4.3. Intuitive Human-Agent Interaction\nIn Figure 5, we present two case studies illustrat-\ning ROCKET-2 interprets human intent under\nthe cross-view goal specification interface. The\nfirst case (top row) involves a task requiring the\nagent to approach a hay bale (\n) located behind a\nhouse (\n). From the human view, both the house\nand the hay bale are visible, whereas ROCKET-2\ninitially observes only the house. A key challenge\narises from the differing camera views: the hu-\nman and ROCKET-2 perceive the scene from op-\nposite sides of the house. To analyze the agent’s\nbehavior, we visualize both its camera views and\nits trajectories on a bird’s-eye map. We observe\nthat ROCKET-2 effectively infers the hay bale’s\npotential location and successfully navigates to-\nward it. This is reflected in the increasing target\nvisibility score and the movement of the predicted\npoint. Interestingly, the bird’s-eye view reveals\nthat ROCKET-2 approaches the target from both\nsides of the house, demonstrating diversity in\nroute selection. The second case (bottom row)\nshowcases ROCKET-2 ’s ability to distinguish\nbetween a distractor and the human-specified\ngoal object, despite their visual similarity. This\nfinding highlights that ROCKET-2 ’s spatial rea-\nsoning extends beyond object appearance and\nincorporates scene alignment for goal inference.\n4.4. Analyzing Failure Cases\nWe analyze failure cases in ROCKET-2 ’s task\nexecution and identify three main issues: (1) Pre-\ndiction Drift: When pursuing distant targets for\nextended periods, the predicted point gradually\nshifts away from the object. Since distant targets\nrely on temporal consistency from memory for\nrecognition, but the model was only trained with\nmemory lengths up to 128, it faces long-sequence\ngeneralization challenges during inference. (2)\nDistance Perception Error: When the goal and\nagent camera views differ significantly, the agent\nsometimes stops one step before reaching the tar-\nget, leading to interaction failure. We observe\nthat updating the goal view to the agent’s cur-\nrent observation alleviates this issue, likely due to\nthe greater cross-view discrepancy encountered\nduring inference compared to training. (3) Ac-\ntion Jitter: When inferring the original version of\nROCKET-2, we observe significant action jitter,\nthis could cause failures in precise interactions\nsuch as placing blocks. We find that incorporat-\ning previous actions during training and inference\n8\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nFigure 6 | Visualization Analysis of Cross-View Alignment. The vision patches (identified by white grid)\nrepresent a chosen background landmark in the agent’s current view (instead of the goal object). We generated\nan attention map with the spatial fusion transformer using these patches as queries and the goal view patches\nas keys and values. We found that ROCKET-2 perfectly aligned with the selected landmarks across views.\ngreatly improves action smoothness.\n4.5. Landmarks Attention Visualization\nProminent non-goal objects, referred to as “land-\nmarks”, play a crucial role in assisting humans or\nagents in localizing goal objects within a scene.\nFor instance, when multiple objects with simi-\nlar appearances are present, spatial relationships\nbetween the goal and landmarks can aid in dis-\ntinction. In this subsection, we aim to explore\nwhether ROCKET-2 implicitly learns landmark\nalignment by visualizing the attention weights of\nits spatial transformer.\nSpecifically, we prepare a current view obser-\nvation and a third view with goal segmentation.\nBefore being fed into the spatial transformer, both\nviews are encoded into 14 × 14 = 196 tokens:\n{ˆ𝑜𝑖\n𝑡}196\n𝑖=1 and {ℎ𝑖\n𝑔}196\n𝑖=1 (notations are consistent with\nSec. 3). We inspect the softmax-normalized at-\ntention map of the first self-attention layer in the\nspatial transformer, denoted as {𝑎𝑖,𝑗}392\n𝑖,𝑗=1, where\n𝑎𝑖,197:392 represents the attention map generated\nby using patch 𝑖from the current view as the\nquery and all patches from the third view as keys\nand values. This map is overlaid on the third\nview (goal view) to reflect its responsiveness to\npatch 𝑖in the current view. Since landmarks may\nspan multiple patches, we aggregate the response\nmaps of different patches to form the final atten-\ntion map {𝑚𝑖}196\n𝑖=1 :\n𝑚𝑖= 1\n|𝐿|\n∑︁\n𝑥∈𝐿\n𝑎𝑥,𝑖+196,\n(6)\nwhere 𝐿denotes the set of patches in the cur-\nrent view representing a specific landmark. No-\ntably, the selected landmarks do not overlap with\nthe goal segmentation. As shown in Figure 6,\nwe present four sets of data covering villages,\nplains, deserts, and forest terrains. In the first\nplot, the white grid indicates the selected land-\nmark patches, while the third plot shows the third\nview response to the chosen landmarks. Our find-\nings reveal that ROCKET-2 effectively matches\ncross-view consistency even under significant geo-\nmetric deformations and distance variations. Sur-\nprisingly, in data point (4), even subtle forest\ndepressions at a considerable distance are accu-\nrately matched.\n4.6. Cross-Episode Goal Alignment\nWe observe that ROCKET-2 exhibits cross-\nepisode generalization capabilities. As shown in\nFigure 7, the selected goal views come from dif-\nferent episodes, each generated with a unique\nworld seed. In the top-row example, the goal\nview is from a “bridge-building” episode set in\nthe savanna biome, where the player is placing\na dirt block to build the bridge. After feeding\n9\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nGoal View\nAgent Views\nt=10\nt=698\nt=702\nt=25\nt=258\nt=330\nFigure 7 | Cross-Episode Generalization. The goal\nview does not exist within the agent’s world but origi-\nnates from a different episode. We observe that the\nagent attempts to infer the semantic information un-\nderlying the goal specification.\nTable 3 | Ablation Study on Auxiliary Objectives.\nThe final loss function for each row is the cumulative\nsum of all loss functions from the preceding rows.\nTraining Variants\nAvg.\nbehavior cloning\n0.52\n0.78\n0.65\n0.65\n+target visibility\n0.63\n0.83\n0.68\n0.71\n+cross-view consistency\n0.85\n0.97\n1.00\n0.94\nforward the goal view, we place ROCKET-2 in\na forest biome and observe its behavior. We find\nthat it first exhibits pillar-jumping behavior, and\nafter placing many blocks, it begins to build the\nbridge horizontally. Although it ultimately failed\nto build the perfect bridge, the emergent behav-\nior still indicates that ROCKET-2 attempts to\nunderstand the underlying semantic information\nwhen there is no landmark match across views.\nIn the bottom row, the goal view is taken from a\nMinecraft creative mode, observing a house from\nthe sky— a view never seen during training. We\nfind that ROCKET-2 explores its environment\nand successfully identifies a visually similar house.\nThis demonstrates ROCKET-2 ’s robustness to a\nvariety of goal views.\n4.7. Ablation Studies on Auxiliary Objectives\nTo evaluate the impact of auxiliary losses on\nmodel performance, we define three variants: (1)\nonly behavior cloning loss, (2) + target visibility\nloss, and (3) the full version with + cross-view con-\nsistency loss. We conduct experiments on three\ntasks: Navigate to House in a Village(\n), Mine\nEmerald(\n), and Interact with the Left Chest(\n).\nWe find that the BC-only variant achieves an aver-\nage success rate of only 65%, demonstrating that\nthe action signal is insufficient for learning spatial\nalignment. Adding target visibility loss improves\nperformance by 6%, while further incorporating\ncross-view consistency loss boosts the success rate\nto 94%. This proves that leveraging temporal\nconsistency and introducing vision-based auxil-\niary losses can greatly enhance cross-view goal\nalignment and inference-time decision-making\ncapabilities.\n5. Conclusions and Limitations\nTo improve human-agent interaction in embod-\nied worlds, we propose a cross-view goal speci-\nfication approach. Since behavior cloning alone\nfails to align the agent with human views, we\nintroduce cross-view consistency and target vis-\nibility losses to enhance alignment. ROCKET-\n2 achieves state-of-the-art performance on the\nMinecraft Interaction Benchmark with a 3× to 6×\nefficiency boost. Visualizations and case stud-\nies validate our method. We also observe that\nROCKET-2 struggles with visual reasoning when\nthe discrepancy between the agent’s and human’s\nviews is large. As memory length increases, the\npredicted points exhibit noticeable drift. We at-\ntribute this to the relabeled dataset, whose mem-\nory window is short and view variation is limited.\nEnhancing data quality could help address this\nissue.\n6. Acknowledgements\nThis work is funded by the National Science and\nTechnology Major Project #2022ZD0114902.\n10\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\n```\n#### 2. 论文摘要\n```\nWe aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.\n```\n\n#### 3. 论文全文\n```\nROCKET-2: Steering Visuomotor Policy via\nCross-View Goal Alignment\nShaofei Cai1, Zhancun Mu1, Anji Liu2 and Yitao Liang B1\n1Peking University, 2University of California, Los Angeles, All authors are affiliated with Team CraftJarvis\nWe aim to develop a goal specification method that is semantically clear, spatially sensitive, and intuitive\nfor human users to guide agent interactions in embodied environments. Specifically, we propose a novel\ncross-view goal alignment framework that allows users to specify target objects using segmentation\nmasks from their own camera views rather than the agent’s observations. We highlight that behavior\ncloning alone fails to align the agent’s behavior with human intent when the human and agent camera\nviews differ significantly. To address this, we introduce two auxiliary objectives: cross-view consistency\nloss and target visibility loss, which explicitly enhance the agent’s spatial reasoning ability. According to\nthis, we develop ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an improvement\nin the efficiency of inference 3× to 6×. We show ROCKET-2 can directly interpret goals from human\ncamera views for the first time, paving the way for better human-agent interaction. The project page is\navailable at https:\/\/craftjarvis.github.io\/ROCKET-2\/.\nFigure 1 | Cross-view Goal Alignment as a Human-Friendly Goal Specification Method. The target object\ncan be specified using a segmentation mask from the human’s camera view, while the agent learns to align with\nhuman intent and take actions based on its own observations. Visual landmarks serve as key cues to establish\nspatial relationships across camera views.\n1. Introduction\nLearning an agent to achieve desired goals is a\nlong-standing challenge in the field of embod-\nied intelligence, with significant implications for\nthe development of robots (Brohan et al., 2022,\n2023; Jang et al., 2022) and virtual players (Wang\net al., 2023a,b,c). A key challenge is to find goal\nrepresentations that are (i) flexible for human\nusers to specify and (ii) expressive and precise to\ncapture as many tasks as possible. Most current\napproaches address only one of these aspects. For\nexample, traditional works (Brohan et al., 2022;\nDriess et al., 2023; Lynch et al., 2023) focus on\ntraining agents to follow language instructions.\nAs pointed out in Cai et al. (2024b); Sundaresan\net al. (2024), while language is intuitive, it relies\non numerous prepositions to express spatial re-\nlationships, which can be vague and inefficient.\nFurthermore, language also suffers from the gen-\nCorresponding author(s): Yitao Liang\nShaofei Cai, Zhancun Mu <{caishaofei, muzhancun}@stu.pku.edu.cn>, Anji Liu <liuanji@cs.ucla.edu>, Yitao Liang <yitaol@pku.edu.cn>\narXiv:2503.02505v1  [cs.AI]  4 Mar 2025\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\neralization problem of novel visual concepts (Cai\net al., 2023). Realizing these limitations, some\nworks attempted to introduce visual modalities\ninto goal representations. For example, Sundare-\nsan et al. (2024) employs hand-drawn target\nlayouts in robot manipulation environments to\nrepresent human intent; Gu et al. (2023) uses\nend-effector trajectory sketches for fine-grained\ncontrol of robot arms; and ROCKET-1 (Cai et al.,\n2024b) specifies the objects to interact with by\napplying segmentation masks to the agent’s ob-\nservations. These methods have greatly improved\nthe expressiveness of spatial relationships and\ngeneralization across tasks. However, both trajec-\ntory sketches and object segmentation are closely\ntied to the agent’s current observation, causing\nissues in partially observable 3D worlds. These in-\nclude: (i) goals need to be generated in real-time\nas the agent’s camera view changes; (ii) goals\ncannot be specified when the target is occluded.\nTo strike a balance between expressiveness and\nflexibility, we propose an innovative and user-\nfriendly cross-view goal specification method. It\nallows human users to specify the target object\nusing segmentation masks from their own cam-\nera view, rather than the agent’s camera view.\nThe agent is then trained to align with human\nintent and take actions based on its own obser-\nvations via imitation learning. Decoupling the\ngoal specification from the camera view of the\nagent will significantly enhance the efficiency of\nhuman-agent interaction. However, the partial\nobservability of open worlds makes aligning goals\nacross camera views challenging. This involves\nhandling occlusion, geometric deformation, and\nthe distinction of objects of similar look. In Figure\n1, we show an agent in the left corner and a hu-\nman player standing on a farmland. The human\nintends to command the agent to hunt a sheep\nnear the house, even though the agent cannot\ninitially observe the target sheep. To achieve this,\nthe agent must establish spatial relationships us-\ning shared visual landmarks between the human’s\ncamera view and its own. We find that relying\nsolely on a behavior cloning loss is insufficient.\nTo address these challenges, we highlight an\nimportant property of behavior datasets (Cai\net al., 2024b): The target object remains con-\nsistent across camera views in an interaction\ntrajectory. Motivated by this, we propose two\nauxiliary objective functions: cross-view consis-\ntency loss and target visibility loss, to explicitly\nenhance the agent’s ability to align goals across\ncamera views. Specifically, cross-view consistency\nloss requires the agent to accurately predict the\ntarget object’s centroid point w.r.t. its camera view,\nwhile target visibility loss helps the agent deter-\nmine whether the target object is occluded. To\nfurther leverage temporal consistency, we use a\ncausal Transformer (Vaswani et al., 2017) in the\narchitecture to model the relationship between\npast predictions and the current observation. It\nencourages the agent to maintain tracking even\nwhen the target is occluded. By combining these\nauxiliary losses with behavior cloning loss, we\ndevelop ROCKET-2, a state-of-the-art agent in\nMinecraft. Our experiments show that ROCKET-\n2 can autonomously track the target object as the\ncamera changes, eliminating the need for SAM’s\n(Ravi et al., 2024) real-time semantic segmenta-\ntion, speeding up inference 3× to 6× compared\nto ROCKET-1. To our knowledge, we are the first\nto demonstrate that agents can interpret inten-\ntions from a human’s camera view and make de-\ncisions to achieve expected goals in the embodied\nworld. Extensive visualization experiments and\ncase studies offer deeper insights into its behavior.\nOur contributions are threefold: (1) We in-\ntroduce a user-friendly interface that allows hu-\nmans to specify goals using segmentation masks\nfrom their camera view, paving the way for better\nhuman-agent interaction. (2) We propose cross-\nview consistency loss and target visibility loss to ex-\nplicitly enhance the agent’s ability to align targets\nacross camera views and improve its steerabil-\nity. (3) We train ROCKET-2, an agent that au-\ntonomously tracks targets, eliminating the need\nfor real-time goal segmentation and significantly\nspeeding up inference.\n2. Related Works\nPartial Observability. We address policy learn-\ning in partially observable 3D worlds (Cai et al.,\n2024a; Savva et al., 2019), where the agent re-\nceives only egocentric images rather than the full\n2\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nenvironmental state. Since the movement of the\ncamera view is part of the action space, the policy\nmust actively explore to locate key objects, a com-\nmon challenge in visual navigation (Savva et al.,\n2019) and FPS games (Pearce and Zhu, 2022).\nWhile some methods (Huang et al., 2023; Jiang\net al., 2024) incorporate 3D point clouds to pro-\nvide global context, such data is often unavailable.\nMore commonly, memory-based architectures in-\ntegrate historical observations to build implicit\n3D perception. For example, Gadre et al. (2022);\nZhao et al. (2023) use RNN to avoid redundant ex-\nploration, and Baker et al. (2022) employs Trans-\nformerXL to retain achievements and complete\nthe long-horizon diamond challenge in Minecraft\n(Guss et al., 2019). Building on this, we require\nthe policy to align objects across camera views to\nensure consistent tracking of targets as the cam-\nera view changes. Although cross-view alignment\nhas been explored in computer vision for tasks\nlike BEV segmentation (Borse et al., 2023) and\npedestrian re-identification (Xu et al., 2019), we\nare the first to study its application in open-world\ndecision-making.\nGoal-Conditioned Imitation Learning. GCIL\nrefers to algorithms that optimize conditional poli-\ncies through imitation learning, primarily using\nbehavior cloning loss (Pomerleau, 1988). The\npolicy conditions can take various forms, such as\nlanguage (Brohan et al., 2022, 2023; Lynch et al.,\n2023), images (Lifshitz et al., 2023; Majumdar\net al., 2022; Sundaresan et al., 2024), videos (Cai\net al., 2023, 2024c), or trajectory sketches (Gu\net al., 2023; Wang et al.). Compared to tradi-\ntional imitation learning, GCIL provides a more\nexplicit target during training, reducing the com-\nplexity of modeling the entire behavior space and\nmaking the policy steerable during inference.\nHumans often use language to express their\nintentions, leading many studies (Brohan et al.,\n2022; Driess et al., 2023; Padalkar et al., 2023;\nWang et al., 2023c) to focus on learning language-\nconditioned policies. However, language goals\nare often ambiguous and struggle to capture spa-\ntial details (Cai et al., 2024b; Gu et al., 2023).\nIn tasks like navigation (Majumdar et al., 2022)\nand object manipulation (Wang et al.), image-\nbased goal modalities have been explored, where\nusers provide target images to guide the policy.\nWhile effective at conveying spatial information,\nimages often over-specify details, making policies\nsensitive to irrelevant factors like lighting, object\nappearance, or background textures. Sundaresan\net al. (2024) addresses these issues by replacing\nthe image with hand-drawn sketches. However,\ngenerating sketches that align with the current\nstate and the desired goal is non-trivial. Gu et al.\n(2023) further proposes using trajectory sketches\nfor finer control and better generalization across\ntasks, but this approach is not applicable in par-\ntially observable 3D worlds.\nROCKET-1 (Cai et al., 2024b) tackles inter-\naction problems in 3D worlds by training a vi-\nsuomotor policy to identify interaction targets\nbased on semantic segmentations in the visual\ncontext. While it resolves traditional goal images’\nambiguity and generation challenges, it relies\non SAM-2 (Ravi et al., 2024) to track goals and\nsegment them during inference, severely limit-\ning real-time performance. We propose a cross-\nview segmentation-conditioned policy that, un-\nlike ROCKET-1, enables the policy to align the\ngoal across camera views by itself and removes\nthe need for real-time segmentation.\nHindsight Trajectory Relabeling.\nThere are\ntwo main approaches to collecting labeled tra-\njectory data: (1) providing instructions for con-\ntractors to collect trajectories in real-time, en-\nsuring a causal link between actions and labels\nbut incurring high costs and limited scalability;\nand (2) gathering large amounts of trajectories\nand generating labels through post-processing,\nknown as hindsight trajectory relabeling. While\nthe first approach (Lynch et al., 2023; Padalkar\net al., 2023) produces higher-quality data, its\ncost constraints have led most research to adopt\nthe second, more scalable one. Andrychowicz\net al. (2017) was the first to reinterpret a trajec-\ntory’s behavior using its final frame, greatly im-\nproving data utilization and inspiring subsequent\nresearch on goal-conditioned policies. Lifshitz\net al. (2023) extended the approach by using the\nlast 16 frames of a trajectory as the more expres-\nsive goal. Sundaresan et al. (2024) introduced\nhand-drawn sketches as a goal modality, reducing\nsemantic ambiguity. Gu et al. (2023) converted\n3\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nthe robotic end-effector moving sketch to a 2D\nimage as the goal, providing richer procedural\ndetails. Cai et al. (2024b) introduced backward\ntrajectory relabeling, which first identifies inter-\naction objects and events, then utilizes object\ntracking models (Ravi et al., 2024) to generate\nframe-level segmentations. This data supports\ntraining segmentation-conditioned policies. Our\npaper explores using this dataset to train policies\nfor cross-view goal alignment.\n3. Methods\nIn this section, we first introduce the problem of\ncross-view segmentation-conditioned policy, dis-\ncussing it from the perspective of imitation learn-\ning. Next, we describe the process of generating\ncross-view trajectories annotated with semantic\nsegmentation. We then present two auxiliary ob-\njectives designed to enhance cross-view object\nalignment in 3D scenes: the cross-view consistency\nloss and the target visibility loss. Finally, we de-\ntail the architecture of ROCKET-2 and outline the\noverall optimization objectives.\nProblem Statement.\nOur goal is to learn a\ngoal-conditioned visuomotor policy, which al-\nlows humans to specify goal objects for inter-\naction using semantic segmentation across cam-\nera views. Formally, we aim to learn a policy\n𝜋cross(𝑎𝑡|𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}, 𝑐𝑔), where 𝑎𝑡represents the\naction at time 𝑡, 𝑐𝑔denotes the type of interac-\ntion. In the Minecraft environment, an action\ncorresponds to raw mouse and keyboard inputs.\n𝑜𝑡∈ℝ𝐻×𝑊×3 denotes the environment observa-\ntion at time 𝑡, and 𝑜𝑔∈ℝ𝐻×𝑊×3 represents an\nobservation of the local environment from a spe-\ncific camera view. Generally, 𝑜𝑔and 𝑜𝑡have some\nvisual content overlap. 𝑚𝑔∈{0, 1}𝐻×𝑊×1 is a seg-\nmentation mask for 𝑜𝑔, highlighting the target\nobject within the camera view 𝑜𝑔. During infer-\nence, users select a view 𝑜𝑔containing the desired\nobject from historical observations returned by\nthe environment and generates its correspond-\ning semantic segmentation 𝑚𝑔. To train such vi-\nsuomotor policy, we assume access to a dataset\nDcross = {𝑐𝑛, (𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡, 𝑚𝑛\n𝑡)𝐿(𝑛)\n𝑡=1 }𝑁\n𝑛=1 consisting of 𝑁\nsuccessful demonstration episodes, 𝐿(𝑛) is the\nlength of episode 𝑛. Within each episode, if\n𝑚𝑡is non-empty, all (𝑜𝑡, 𝑚𝑡) pairs indicate the\nsame object. Consequently, we can arbitrarily\npick one observation frame as the goal view con-\ndition for the entire trajectory.\nCross-View Dataset Generation. Without loss\nof generality, we use the Minecraft world as an\nexample to illustrate the data generation process.\nManually collecting datasets that meet the re-\nquirements is highly expensive. Thus, we employ\nthe backward trajectory relabeling technique pro-\nposed in Cai et al. (2024b) to automate the anno-\ntation of the OpenAI Contractor Dataset (Baker\net al., 2022), which consists of free-play trajecto-\nries from human players: Draw = {(𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡)𝐿(𝑛)\n𝑡=1 }𝑁\n𝑛=1.\nSpecifically, for any given episode 𝑛, we first de-\ntect all frames 𝑜𝑛\n𝑗where interaction events oc-\ncur, identify the interaction type 𝑐𝑛\n𝑗, and localize\nthe interacted object near frame 𝑗using bound-\ning boxes and point-based prompts. The SAM-2\n(Ravi et al., 2024) model is then employed to gen-\nerate the segmentation mask 𝑚𝑛\n𝑗for the object.\nStarting from frame 𝑗, we traverse the trajectory\nbackward and use the SAM-2 model to continu-\nously generate segmentation masks for the object\nin real-time until either a new interaction event\nis encountered or a maximum tracking length\nis reached. Let 𝑖denote the end frame. The re-\nsulting trajectory clip is then added to the train-\ning dataset: Dcross ←Dcross ∪{𝑐𝑗, (𝑜𝑛\n𝑡, 𝑎𝑛\n𝑡, 𝑚𝑛\n𝑡) 𝑗\n𝑡=𝑖}.\nThis ensures that every extracted clip is associ-\nated with a consistent interaction intent. The\ngenerated data encompasses the fundamental in-\nteraction types in Minecraft, including use, break,\napproach, craft, and kill entity. Among these, ap-\nproach is a unique event, identified by detecting\ntrajectory clips where the displacement exceeds\na specified threshold. The object located at the\ncenter of the clip’s final frame is designated as\nthe goal of the approach event.\nCross-View Consistency Loss. Accurately in-\nterpreting the cross-view goal requires the pol-\nicy to possess cross-view visual object alignment\nability in 3D scenes. To achieve this, the model\nmust fully exploit visual cues from different cam-\nera views, such as scene layout and landmark\nbuildings, while being robust to challenges like\nocclusion, shape variations, and changes in dis-\ntance. We observe that relying solely on behav-\n4\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nSPATIAL\na\nT E M P O R A L \nt=10\nt=15\nt=22\nt=92\nt=94\nt=98\nDECODE\nDECODE\nDECODE\nDECODE\nDECODE\nDECODE\nKV Cached Causal Transformer\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\" , 𝒎𝒈\" ]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n[𝒐𝒈\"\", 𝒎𝒈\"\"]\n𝒐𝟏𝟎\n𝒐𝟏𝟓\n𝒐𝟐𝟐\n𝒐𝟗𝟐\n𝒐𝟗𝟒\n𝒐𝟗𝟖\n𝒄𝒈\"\n𝒄𝒈\"\n𝒄𝒈\"\n𝒄𝒈\"\"\n𝒄𝒈\"\"\n𝒄𝒈\"\"\n𝒂𝟏𝟎\n𝒑𝟏𝟎\n𝒗𝟏𝟎\n𝒂𝟏𝟓\n𝒑𝟏𝟓\n𝒗𝟏𝟓\n𝒂𝟐𝟐\n𝒑𝟐𝟐\n𝒗𝟐𝟐\n𝒂𝟗𝟐\n𝒑𝟗𝟐\n𝒗𝟗𝟐\n𝒂𝟗𝟒\n𝒑𝟗𝟒\n𝒗𝟗𝟒\n𝒂𝟗𝟖\n𝒑𝟗𝟖\n𝒗𝟗𝟖\nFigure 2 | ROCKET-2 Architecture. It consists of three parts: (1) a non-causal transformer for spatial fusion,\nwhich establishes the relationship between the agent’s and human’s camera views; (2) a causal transformer for\ntemporal fusion, ensuring consistency for goal tracking; (3) a decoder module, made of a feedforward neural\nnetwork (FFN), which predicts goal-related visuals cues and actions.\nior cloning loss (Pomerleau, 1988) is insufficient.\nTherefore, we propose a cross-view consistency\nloss. Since the segmentation across different cam-\nera views corresponds to the same object, we\ntrain the model to condition on the segmentation\nfrom one camera view to generate the segmen-\ntation for another camera view, thereby directly\nenhancing the model’s 3D spatial perception. To\nreduce computational complexity, we opt to pre-\ndict the centroid of the segmentation mask in-\nstead of the complete mask, formally expressed\nas: 𝜋cross(𝑝𝑡| 𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}, 𝑐𝑔), where\n𝑝𝑡=\nÍ𝐻\n𝑖=1\nÍ𝑊\n𝑗=1(𝑖, 𝑗) · 𝑚𝑡(𝑖, 𝑗)\nÍ𝐻\n𝑖=1\nÍ𝑊\n𝑗=1 𝑚𝑡(𝑖, 𝑗)\n.\n(1)\nIt is worth noting that incorporating the histor-\nical observation sequence 𝑜1:𝑡−1 as input is es-\nsential, especially when there is limited shared\nvisual content between 𝑜𝑡and 𝑜𝑔. This histori-\ncal sequence acts as a smooth bridge to facilitate\nalignment. Since the goal object represented by\nthe segmentation corresponds to the target of the\npolicy’s interaction, this auxiliary task aligns the\npolicy’s actions with its visual focus, effectively\nimproving task performance.\nTarget Visibility Loss. Due to the partial ob-\nservability in 3D environments, it is common for\ntarget objects in interaction trajectories to disap-\npear from the field of view and reappear later.\nDuring such intervals, the segmentation mask for\nthe missing object is empty. To leverage this infor-\nmation, we propose training the model to predict\nwhether the target object is currently visible, for-\nmulated as: 𝜋cross(𝑣𝑡| 𝑜1:𝑡, {𝑜𝑔, 𝑚𝑔}), where 𝑣𝑡is\na binary indicator for empty segmentation masks.\nOn the one hand, accurately predicting object\nvisibility helps the policy better match the target\nobject, avoiding a simple appearance similarity\nmeasurement between two frames. On the other\nhand, visibility information guides the policy to\nmake more reasonable decisions, such as confi-\ndently approaching the goal object when it is\nvisible or actively adjusting its camera to explore\nwhen the target is absent.\nROCKET-2 Architecture. Lets a training trajec-\ntory 𝑛be denoted as (𝑐𝑔, {𝑜𝑡, 𝑚𝑡}𝐿(𝑛)\n𝑡=1 ). A cross view\nindex 𝑔is sampled from {𝑖|𝑖∈[1, 𝐿(𝑛)], 𝑚𝑖≠𝜙}.\nWe resize all visual observations and their seg-\nmentation masks to 224 × 224. For encoding\nthe visual observation 𝑜𝑡, we utilize a DINO-\npretrained (Caron et al., 2021) 3-channel ViT-\nB\/16 (Dosovitskiy et al., 2020) (16 is the patch\nsize), which outputs a token sequence of length\n196, denoted as {ˆ𝑜𝑖\n𝑡}196\n𝑖=1 . Similarly, we encode the\nsegmentation mask 𝑚𝑡using a 1-input-channel\nViT-tiny\/16, yielding { ˆ𝑚𝑖\n𝑡}196\n𝑖=1 . The ViT-base\/16\nencoder is frozen during training for efficiency,\nwhile the ViT-tiny\/16 is trainable. To ensure spa-\ntial alignment, we fuse the cross-view condition\n(𝑜𝑔, 𝑚𝑔) by concatenating the feature channels:\nℎ𝑖\n𝑔= FFN(concat([ˆ𝑜𝑖\n𝑔∥ˆ𝑚𝑖\n𝑔])).\n(2)\n5\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nGiven the ability of self-attention mechanisms to\ncapture spatial details across views, we concate-\nnate the token sequences from two views into\na single sequence of length 392. A non-causal\nTransformer encoder module is applied (Vaswani\net al., 2017) for spatial fusion, obtaining a frame-\nlevel representation 𝑥𝑡:\n𝑥𝑡←SpatialFusion({ˆ𝑜𝑖\n𝑡}196\n𝑖=1 , {ℎ𝑖\n𝑔}196\n𝑖=1 ).\n(3)\nSubsequently,\nwe leverage a causal Trans-\nformerXL (Dai et al., 2019) architecture to cap-\nture temporal information across the sequence:\n𝑓𝑡←TransformerXL({𝑥𝑖}𝑡\n𝑖=1, 𝑐𝑔).\n(4)\nFinally, a simple linear layer maps 𝑓𝑡to predict\nthe action ˆ𝑎𝑡, centroid ˆ𝑝𝑡, and visibility ˆ𝑣𝑡. The\nloss function for episode 𝑛is defined as:\nL(𝑛) =\n𝐿(𝑛)\n∑︁\n𝑡=1\n−𝑎𝑛\n𝑡log ˆ𝑎𝑛\n𝑡−𝑝𝑛\n𝑡log ˆ𝑝𝑛\n𝑡−𝑣𝑛\n𝑡log ˆ𝑣𝑛\n𝑡. (5)\n4. Experiments\nWe aim to address the following questions: (1)\nHow does ROCKET-2 perform in terms of both\naccuracy and efficiency during inference? (2) Can\nROCKET-2 follow the intention of a human from\na cross-camera view? (3) Under what circum-\nstances does ROCKET-2 fail to work? (4) How\nimportant are landmarks in cross-view goal align-\nment? (5) Can ROCKET-2 interpret goal views\nfrom cross-episode scenarios? (6) Which modules\ncontribute effectively to training ROCKET-2?\n4.1. Experimental Setup\nImplementation Details. We present the model\narchitecture, hyperparameters, and optimizer\nconfigurations of ROCKET-2 in Table 1. Dur-\ning training, each trajectory is divided into seg-\nments of length 128 to reduce memory require-\nments. We initialize the view backbone that is\nused to encode RGB images with DINO weights\nand freeze it for training efficiency. During infer-\nence, ROCKET-2 can access up to 128 key-value\nattention caches of past observations. Most train-\ning parameters follow those from prior works such\nas Baker et al. (2022); Cai et al. (2024b).\nhunt the sheep in the \nright fence\nhunt the cow while do not \ntouch the sheep\ninteract with the left \nchest\nplace the oak door on the \ndiamond block\nuse bucket to get lava\nset fire on a trunk\nFigure 3 | The Evaluation Metric is Spatial-Sensitive.\n✓and × indicate the correct and incorrect objects for\ninteraction, respectively. None of the task configura-\ntions were seen during the training.\nTable 1 | Detailed Training Hyperparameters.\nHyperparameter\nValue\nInput Image Size\n224 × 224\nHidden Dimension\n1024\nView Backbone\nViT-base\/16 (DINO-v1)\nMask Backbone\nViT-tiny\/16 (1-channel)\nSpatial Transformer\nPyTorch Transformer\nNumber of Spatial Blocks\n4\nTemporal Transformer\nTransformerXL\nNumber of Temporal Blocks\n4\nTrajectory Chunk size\n128\nOptimizer\nAdamW\nLearning Rate\n0.00004\nEnvironment and Benchmark. We use the orig-\ninal Minecraft 1.16.5 (Cai et al., 2024a; Guss\net al., 2019; Lin et al., 2023) as our testing en-\nvironment, which accepts mouse and keyboard\ninputs and outputs a 640×360 RGB image at each\nstep. Following Cai et al. (2024b), we employ the\nMinecraft Interaction Benchmark to evaluate the\nagent’s interaction capabilities. This benchmark\nincludes six categories and a total of 12 tasks,\ncovering all basic Minecraft interaction types:\nHunt, Mine, Interact, Navigate, Tool, and Place.\nAs this benchmark emphasizes object interaction\nand spatial localization, its evaluation criteria are\nmore stringent than those in Lifshitz et al. (2023)\nand Cai et al. (2023). We show some examples in\nFigure 3. In the “hunt the sheep in the right fence”\ntask, success requires the agent to kill the sheep\nwithin the right fence, while killing it in the left\nfence results in failure. Similarly, in the “place\nthe oak door on the diamond block” task, success\nis only achieved if the oak door is adjacent to the\n6\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nFigure 4 | Performance-Efficiency Comparison on the Minecraft Interaction Benchmark. The x-axis\nrepresents inference speed (FPS), and the y-axis shows the interaction success rate. Numbers in parentheses\nindicate the Molmo invocation interval, where larger values mean higher FPS. “+ track” denotes real-time\nSAM-2 segmentation between Molmo calls, increasing inference time (applicable only to ROCKET-1). In most\ncases, ROCKET-2 achieves 3× to 6× faster while matching or surpassing ROCKET-1’s peak performance.\ndiamond block on at least one side.\nBaselines. We compare our ROCKET-2 with\nthe following instruction-following baselines: (1)\nSTEVE-1 (Lifshitz et al., 2023): An instruction-\nfollowing agent fine-tuned from VPT (Baker et al.,\n2022), capable of solving various short-horizon\ntasks. We use the text-conditioned version of\nSTEVE-1 for comparison. (2) GROOT-1 (Cai et al.,\n2023): A reference-video-conditioned policy de-\nsigned for open-ended tasks, trained on 2,000\nhours of long-form videos using latent variable\nmodels. (3) ROCKET-1 (Cai et al., 2024b): A\nsegmentation-conditioned policy capable of mas-\ntering 12 interaction tasks. While it achieves a\nhigh interaction success rate, its reliance on SAM-\n2’s real-time tracking during inference creates an\nefficiency bottleneck.\n4.2. Performance-Efficiency Analysis\nWe demonstrate that cross-view goal specification\n(implemented as ROCKET-2) significantly im-\nproves inference speed while maintaining high\ninteraction success. Following Cai et al. (2024b),\nwe evaluate STEVE-1, GROOT-1, and variants of\nROCKET-1 and our ROCKET-2 on the Minecraft\nInteraction Benchmark. Due to the limited spatial\nreasoning ability, STEVE-1 and GROOT-1 achieve\nsuccess rates below 20% on most tasks. We il-\nlustrate the inference pipeline of ROCKET-series\nTable 2 | ROCKET-Series Inference Pipeline Details.\nMolmo can pinpoint the target object based on the task\nprompt. SAM uses the point to generate object mask\n𝑚𝑡w.r.t. 𝑜𝑡and supports real-time object tracking.\nModel\nInference Pipeline\nR1(3)\nm1 ←SAM(𝑜1, Molmo(𝑜1, prompt))\n𝜋𝑅1(𝑎𝑡|𝑜1, m1, 𝑜2, 𝑜3, 𝑜4, m4, 𝑜5, 𝑜6, 𝑜7, m7, · · · )\nR1(30)+track\nm1:30 ←SAM(𝑜1:30, Molmo(𝑜1, prompt))\n𝜋𝑅1(𝑎𝑡|𝑜1, m1, 𝑜2, m2, 𝑜3, m3, 𝑜4, m4, · · · )\nR2(60)\nm1 ←SAM(𝑜1, Molmo(𝑜1, prompt))\n𝜋𝑅2(𝑎𝑡|𝑜1, m1, 𝑜2, 𝑜3, 𝑜4, · · · , 𝑜60, 𝑜61, m61, · · · )\nagents in Table 2. Automated evaluation of ROCK-\nETs relies on Molmo (Deitke et al., 2024) and\nSAM (Ravi et al., 2024) to generate a segmenta-\ntion mask for the target object in the given views.\nIn general, ROCKET-1 requires object masks for\nall agent observations, whereas ROCKET-2 only\nneeds one or a few object masks. We observe\nthat ROCKET-1 attains over 80% success with\nhigh-frequency Molmo point corrections (every 3\nframes) but suffers from slow inference. Lowering\nMolmo’s frequency greatly degrades ROCKET-1’s\nperformance, with tasks like “collecting lava” fail-\ning entirely at 30-frame intervals. While one can\nenable SAM’s tracking mode to provide dense\ngoal signals, it remains computationally expen-\nsive. In contrast, our ROCKET-2 decouples the\ngoal specification from the agent view, it does not\nneed frequent segmentation mask modification\n7\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nHuman View\nBird-Eye Map\nAgent View, t=96\nAgent View, t=109\nAgent View, t=125\nAgent View, t=143\nPredicted Point\nPredicted Point\nPredicted Point\nPredicted Point\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTrajectories\nHuman Position\nAgent Position\nGoal Mask\nHuman View\nBird-Eye Map\nAgent View, t=4\nAgent View, t=27\nAgent View, t=81\nAgent View, t=157\nHuman \nPosition\nAgent Position\nTrajectories\nPredicted Point\nPredicted Point\nPredicted Point\nPredicted Point\nTarget Visibility\nTarget Visibility\nTarget Visibility\nTarget Visibility\nGoal Mask\nDistractor:\nFigure 5 | Case Study of Human-Agent Interaction. We demonstrate how a human interacts with ROCKET-2,\nleveraging its spatial reasoning abilities. (Top Row) The human specifies a hay bale (\n) that is not visible\nto ROCKET-2. By exploring the area around the visible landmark (house), ROCKET-2 successfully locates\nthe goal. (Bottom Row) The human specifies a target tree in the presence of a tree distractor. ROCKET-2\naccurately identifies the correct tree by reasoning about spatial relationships and landmarks. The agent’s\ntrajectories are visualized in bird’s-eye view maps.\nand can autonomously track the target object. It\nachieves comparable or superior performance to\nROCKET-1 with a 3× to 6× inference speedup.\n4.3. Intuitive Human-Agent Interaction\nIn Figure 5, we present two case studies illustrat-\ning ROCKET-2 interprets human intent under\nthe cross-view goal specification interface. The\nfirst case (top row) involves a task requiring the\nagent to approach a hay bale (\n) located behind a\nhouse (\n). From the human view, both the house\nand the hay bale are visible, whereas ROCKET-2\ninitially observes only the house. A key challenge\narises from the differing camera views: the hu-\nman and ROCKET-2 perceive the scene from op-\nposite sides of the house. To analyze the agent’s\nbehavior, we visualize both its camera views and\nits trajectories on a bird’s-eye map. We observe\nthat ROCKET-2 effectively infers the hay bale’s\npotential location and successfully navigates to-\nward it. This is reflected in the increasing target\nvisibility score and the movement of the predicted\npoint. Interestingly, the bird’s-eye view reveals\nthat ROCKET-2 approaches the target from both\nsides of the house, demonstrating diversity in\nroute selection. The second case (bottom row)\nshowcases ROCKET-2 ’s ability to distinguish\nbetween a distractor and the human-specified\ngoal object, despite their visual similarity. This\nfinding highlights that ROCKET-2 ’s spatial rea-\nsoning extends beyond object appearance and\nincorporates scene alignment for goal inference.\n4.4. Analyzing Failure Cases\nWe analyze failure cases in ROCKET-2 ’s task\nexecution and identify three main issues: (1) Pre-\ndiction Drift: When pursuing distant targets for\nextended periods, the predicted point gradually\nshifts away from the object. Since distant targets\nrely on temporal consistency from memory for\nrecognition, but the model was only trained with\nmemory lengths up to 128, it faces long-sequence\ngeneralization challenges during inference. (2)\nDistance Perception Error: When the goal and\nagent camera views differ significantly, the agent\nsometimes stops one step before reaching the tar-\nget, leading to interaction failure. We observe\nthat updating the goal view to the agent’s cur-\nrent observation alleviates this issue, likely due to\nthe greater cross-view discrepancy encountered\nduring inference compared to training. (3) Ac-\ntion Jitter: When inferring the original version of\nROCKET-2, we observe significant action jitter,\nthis could cause failures in precise interactions\nsuch as placing blocks. We find that incorporat-\ning previous actions during training and inference\n8\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nFigure 6 | Visualization Analysis of Cross-View Alignment. The vision patches (identified by white grid)\nrepresent a chosen background landmark in the agent’s current view (instead of the goal object). We generated\nan attention map with the spatial fusion transformer using these patches as queries and the goal view patches\nas keys and values. We found that ROCKET-2 perfectly aligned with the selected landmarks across views.\ngreatly improves action smoothness.\n4.5. Landmarks Attention Visualization\nProminent non-goal objects, referred to as “land-\nmarks”, play a crucial role in assisting humans or\nagents in localizing goal objects within a scene.\nFor instance, when multiple objects with simi-\nlar appearances are present, spatial relationships\nbetween the goal and landmarks can aid in dis-\ntinction. In this subsection, we aim to explore\nwhether ROCKET-2 implicitly learns landmark\nalignment by visualizing the attention weights of\nits spatial transformer.\nSpecifically, we prepare a current view obser-\nvation and a third view with goal segmentation.\nBefore being fed into the spatial transformer, both\nviews are encoded into 14 × 14 = 196 tokens:\n{ˆ𝑜𝑖\n𝑡}196\n𝑖=1 and {ℎ𝑖\n𝑔}196\n𝑖=1 (notations are consistent with\nSec. 3). We inspect the softmax-normalized at-\ntention map of the first self-attention layer in the\nspatial transformer, denoted as {𝑎𝑖,𝑗}392\n𝑖,𝑗=1, where\n𝑎𝑖,197:392 represents the attention map generated\nby using patch 𝑖from the current view as the\nquery and all patches from the third view as keys\nand values. This map is overlaid on the third\nview (goal view) to reflect its responsiveness to\npatch 𝑖in the current view. Since landmarks may\nspan multiple patches, we aggregate the response\nmaps of different patches to form the final atten-\ntion map {𝑚𝑖}196\n𝑖=1 :\n𝑚𝑖= 1\n|𝐿|\n∑︁\n𝑥∈𝐿\n𝑎𝑥,𝑖+196,\n(6)\nwhere 𝐿denotes the set of patches in the cur-\nrent view representing a specific landmark. No-\ntably, the selected landmarks do not overlap with\nthe goal segmentation. As shown in Figure 6,\nwe present four sets of data covering villages,\nplains, deserts, and forest terrains. In the first\nplot, the white grid indicates the selected land-\nmark patches, while the third plot shows the third\nview response to the chosen landmarks. Our find-\nings reveal that ROCKET-2 effectively matches\ncross-view consistency even under significant geo-\nmetric deformations and distance variations. Sur-\nprisingly, in data point (4), even subtle forest\ndepressions at a considerable distance are accu-\nrately matched.\n4.6. Cross-Episode Goal Alignment\nWe observe that ROCKET-2 exhibits cross-\nepisode generalization capabilities. As shown in\nFigure 7, the selected goal views come from dif-\nferent episodes, each generated with a unique\nworld seed. In the top-row example, the goal\nview is from a “bridge-building” episode set in\nthe savanna biome, where the player is placing\na dirt block to build the bridge. After feeding\n9\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\nGoal View\nAgent Views\nt=10\nt=698\nt=702\nt=25\nt=258\nt=330\nFigure 7 | Cross-Episode Generalization. The goal\nview does not exist within the agent’s world but origi-\nnates from a different episode. We observe that the\nagent attempts to infer the semantic information un-\nderlying the goal specification.\nTable 3 | Ablation Study on Auxiliary Objectives.\nThe final loss function for each row is the cumulative\nsum of all loss functions from the preceding rows.\nTraining Variants\nAvg.\nbehavior cloning\n0.52\n0.78\n0.65\n0.65\n+target visibility\n0.63\n0.83\n0.68\n0.71\n+cross-view consistency\n0.85\n0.97\n1.00\n0.94\nforward the goal view, we place ROCKET-2 in\na forest biome and observe its behavior. We find\nthat it first exhibits pillar-jumping behavior, and\nafter placing many blocks, it begins to build the\nbridge horizontally. Although it ultimately failed\nto build the perfect bridge, the emergent behav-\nior still indicates that ROCKET-2 attempts to\nunderstand the underlying semantic information\nwhen there is no landmark match across views.\nIn the bottom row, the goal view is taken from a\nMinecraft creative mode, observing a house from\nthe sky— a view never seen during training. We\nfind that ROCKET-2 explores its environment\nand successfully identifies a visually similar house.\nThis demonstrates ROCKET-2 ’s robustness to a\nvariety of goal views.\n4.7. Ablation Studies on Auxiliary Objectives\nTo evaluate the impact of auxiliary losses on\nmodel performance, we define three variants: (1)\nonly behavior cloning loss, (2) + target visibility\nloss, and (3) the full version with + cross-view con-\nsistency loss. We conduct experiments on three\ntasks: Navigate to House in a Village(\n), Mine\nEmerald(\n), and Interact with the Left Chest(\n).\nWe find that the BC-only variant achieves an aver-\nage success rate of only 65%, demonstrating that\nthe action signal is insufficient for learning spatial\nalignment. Adding target visibility loss improves\nperformance by 6%, while further incorporating\ncross-view consistency loss boosts the success rate\nto 94%. This proves that leveraging temporal\nconsistency and introducing vision-based auxil-\niary losses can greatly enhance cross-view goal\nalignment and inference-time decision-making\ncapabilities.\n5. Conclusions and Limitations\nTo improve human-agent interaction in embod-\nied worlds, we propose a cross-view goal speci-\nfication approach. Since behavior cloning alone\nfails to align the agent with human views, we\nintroduce cross-view consistency and target vis-\nibility losses to enhance alignment. ROCKET-\n2 achieves state-of-the-art performance on the\nMinecraft Interaction Benchmark with a 3× to 6×\nefficiency boost. Visualizations and case stud-\nies validate our method. We also observe that\nROCKET-2 struggles with visual reasoning when\nthe discrepancy between the agent’s and human’s\nviews is large. As memory length increases, the\npredicted points exhibit noticeable drift. We at-\ntribute this to the relabeled dataset, whose mem-\nory window is short and view variation is limited.\nEnhancing data quality could help address this\nissue.\n6. Acknowledgements\nThis work is funded by the National Science and\nTechnology Major Project #2022ZD0114902.\n10\nROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | ROCKET-2：通过跨视图目标对齐引导视觉运动策略\n\n## 📌 背景痛点\/本文动机\n在机器人学习和虚拟玩家领域，让智能体实现人类期望的目标是一个长期挑战。关键在于找到既能让人类用户轻松指定，又能精确捕捉多种任务的表示方法。现有的方法往往只关注其中一个方面，例如语言指令直观但表达空间关系模糊，视觉模态则容易受到光照、物体外观等因素的影响。\n\n## 🚀 核心方法\n💡 创新点1：用户友好的跨视图目标指定方法\nROCKET-2 允许用户使用自己的相机视图中的分割掩码来指定目标对象，而不是使用智能体的观察结果。这种方法将目标指定与智能体的相机视图解耦，从而显著提高了人机交互的效率。\n\n💡 创新点2：跨视图一致性损失和目标可见性损失\n为了解决开放世界中部分可观察性的挑战，ROCKET-2 引入了两个辅助目标函数：跨视图一致性损失和目标可见性损失。这些损失函数可以显式地增强智能体在跨视图目标对齐方面的能力，并提高其可操控性。\n\n## 📈 实验结果\n在 Minecraft 环境中，ROCKET-2 实现了最先进的性能，并在推理效率方面比 ROCKET-1 提高了 3 倍到 6 倍。实验结果表明，ROCKET-2 可以直接从人类的相机视图中解释目标，为更好的人机交互铺平了道路。\n\n## 💬 可借鉴之处\nROCKET-2 的跨视图目标对齐方法为更直观的人机交互提供了新的思路。其引入的辅助目标函数和架构设计也为解决开放世界中部分可观察性的挑战提供了新的解决方案。","llm_summary_res_status":200}
{"title":"Open-World Reinforcement Learning over Long Short-Term Imagination","authors":"Jiajian Li, Qi Wang, Yunbo Wang, Xin Jin, Yang Li, Wenjun Zeng, Xiaokang Yang","summary":"Training visual reinforcement learning agents in a high-dimensional open\nworld presents significant challenges. While various model-based methods have\nimproved sample efficiency by learning interactive world models, these agents\ntend to be \"short-sighted\", as they are typically trained on short snippets of\nimagined experiences. We argue that the primary challenge in open-world\ndecision-making is improving the exploration efficiency across a vast state\nspace, especially for tasks that demand consideration of long-horizon payoffs.\nIn this paper, we present LS-Imagine, which extends the imagination horizon\nwithin a limited number of state transition steps, enabling the agent to\nexplore behaviors that potentially lead to promising long-term feedback. The\nfoundation of our approach is to build a $\\textit{long short-term world\nmodel}$. To achieve this, we simulate goal-conditioned jumpy state transitions\nand compute corresponding affordance maps by zooming in on specific areas\nwithin single images. This facilitates the integration of direct long-term\nvalues into behavior learning. Our method demonstrates significant improvements\nover state-of-the-art techniques in MineDojo.","url":"http:\/\/arxiv.org\/abs\/2410.03618v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.03618v2","published":1728062250000,"comment":"Accepted by ICLR 2025 Oral. Project page:\n  https:\/\/qiwang067.github.io\/ls-imagine","pdf_text":"Published as a conference paper at ICLR 2025\nOPEN-WORLD REINFORCEMENT LEARNING OVER\nLONG SHORT-TERM IMAGINATION\nJiajian Li1∗\nQi Wang1,2∗\nYunbo Wang1†\nXin Jin2\nYang Li3\nWenjun Zeng2\nXiaokang Yang1\n1 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\n2 Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China\n3 School of Computer Science and Technology, East China Normal University, Shanghai, China\nhttps:\/\/qiwang067.github.io\/ls-imagine\nABSTRACT\nTraining visual reinforcement learning agents in a high-dimensional open world\npresents significant challenges. While various model-based methods have im-\nproved sample efficiency by learning interactive world models, these agents tend\nto be “short-sighted”, as they are typically trained on short snippets of imagined\nexperiences. We argue that the primary challenge in open-world decision-making\nis improving the exploration efficiency across a vast state space, especially for\ntasks that demand consideration of long-horizon payoffs. In this paper, we present\nLS-Imagine, which extends the imagination horizon within a limited number of\nstate transition steps, enabling the agent to explore behaviors that potentially lead\nto promising long-term feedback. The foundation of our approach is to build a long\nshort-term world model. To achieve this, we simulate goal-conditioned jumpy state\ntransitions and compute corresponding affordance maps by zooming in on specific\nareas within single images. This facilitates the integration of direct long-term\nvalues into behavior learning. Our method demonstrates significant improvements\nover state-of-the-art techniques in MineDojo.\n1\nINTRODUCTION\nOpen-world decision-making in the context of reinforcement learning (RL) involves the following\ncharacteristics: (i) The agent operates within an interactive environment that features a vast state\nspace; (ii) The learned policy presents a high degree of flexibility, allowing interaction with various\nobjects in the environment; (iii) The agent lacks full visibility of the internal states and physical\ndynamics of the external world, meaning that its perception of the environment (e.g., raw images)\ncarries substantial uncertainty. For example, Minecraft serves as a typical open-world game.\nBuilding upon recent progress in visual control, open-world decision-making aims to train agents to ap-\nproach human-level intelligence based solely on high-dimensional visual observations. However, this\npresents significant challenges. For example, in Minecraft tasks, existing methods like Voyager (Wang\net al., 2024a) employ specific Minecraft APIs as the high-level controller, which is incompatible with\nstandard visual control settings. While approaches such as PPO-with-MineCLIP (Fan et al., 2022) and\nDECKARD (Nottingham et al., 2023) perform low-level visual control, these model-free RL methods\nstruggle to grasp the underlying mechanics of the environment. This may result in high trial-and-error\ncosts, leading to inefficiencies in both exploration and sample usage. Although DreamerV3 (Hafner\net al., 2023) employs a model-based RL (MBRL) approach to improve sample efficiency, it is often\n“short-sighted” since the policy is optimized using short-term experiences—typically 15 time steps—\ngenerated by the world model. The absence of long-term guidance significantly hampers an effective\nexploration of the vast solution space of the open world.\nTo improve the behavior learning efficiency of MBRL, in this paper, we introduce a novel method\nnamed Long Short-Term Imagination (LS-Imagine). Our key approach involves enabling the world\nmodel to efficiently simulate the long-term effects of specific behaviors without the need for repeatedly\n∗Equal contribution.\n†Corresponding author: Yunbo Wang <yunbow@sjtu.edu.cn>.\n1\narXiv:2410.03618v2  [cs.LG]  4 Mar 2025\nPublished as a conference paper at ICLR 2025\nCut a tree\n🏆\nenvironment\nShort-Term Imagination \nLong-Term Imagination \nFigure 1: The general framework of LS-Imagine, an MBRL agent that operates solely on raw pixels.\nThe fundamental idea is to extend the imagination horizon within a limited number of state transition\nsteps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback.\nrolling out one-step predictions. As illustrated in Figure 1, once trained, the world model provides\nboth instant and jumpy state transitions1 along with corresponding (intrinsic) rewards, facilitating\npolicy optimization in a joint space of short- and long-term imaginations. This encourages the agent\nto explore behaviors that lead to promising long-term outcomes.\nThe foundation of LS-Imagine is to train a long short-term world model, which requires integrating\ntask-specific guidance into the representation learning phase based on off-policy experience replay.\nHowever, this creates a classic “chicken-and-egg” dilemma: without true data showing the agent has\nreached the goal, how can we effectively train the model to simulate jumpy transitions from current\nstates to pivotal future states that suggest a high likelihood of achieving that goal? To address this\nissue, we first continuously zoom in on individual images to simulate the consecutive video frames as\nthe agent approaches the goal. We then generate affordance maps2 by evaluating the relevance of the\npseudo video to task-specific goals presented in textual instructions, using the established MineCLIP\nreward model (Fan et al., 2022). Subsequently, we train specific branches of the world model to\ncapture both instant and jumpy state transitions, using pairs of image observations from adjacent time\nsteps as well as those across longer intervals. Finally, we optimize the agent’s policy based on a finite\nsequence of imagined latent states generated by the world model, integrating a more direct estimate\nof long-term values into decision-making.\nLet’s use the example in Figure 1 to further elaborate the novel aspects of the behavior learning\nprocess: After receiving the instruction “cut a tree”, the agent simulates near-future states based on\nthe current real observation. It initially performs several single-step rollouts until it identifies a point\nin time for a long-distance state jump that allows it to approach the tree. The agent then executes this\njump and optimizes its policy network to maximize the long-sight value function.\nWe evaluate our approach in the challenging open-world tasks from MineDojo (Fan et al., 2022).\nLS-Imagine demonstrates superior performance compared to existing visual RL methods.\nThe contributions of this work are summarized as follows:\n• We present a novel model-based RL method that captures both instant and jumpy state transitions\nand leverages them in behavior learning to improve exploration efficiency in the open world.\n• Our approach presents four concrete contributions: (i) a long short-term world model architecture,\n(ii) a method for generating affordance maps through image zoom-in, (iii) a novel form of intrinsic\nrewards based on the affordance map, and (iv) an improved behavior learning method that integrates\nlong-term values and operates on a mixed long short-term imagination pathway.\n2\nPROBLEM FORMULATION AND NOTATIONS\nWe solve visual reinforcement learning as a partially observable Markov decision process (POMDP),\nusing MineDojo as the test bench. Specifically, our method manipulates low-dimensional control\nsignals at while receiving only sequential high-dimensional visual observations o<t and episodic\nsparse rewards renv, without access to the internal APIs of the open-world games. In comparison, as\n1As shown in Figure 1, a jumpy transition allows the agent to bypass intermediate states and directly simulate\na task-relevant future state st+H in one step. This process occurs exclusively during world model imagination.\n2Affordance maps highlight regions within an observation that are potentially relevant to the task (Qi et al.,\n2020; Wang et al., 2022).\n2\nPublished as a conference paper at ICLR 2025\nTable 1: Experimental setups of the Minecraft AI agents. IL is short for imitation learning.\nModel\nController\nObservation\nVideo Demos\nDECKARD (2023)\nRL\nPixels & Inventory\n✓\nAuto MC-Reward (2024a)\nIL + RL\nPixels & GPS\n✗\nVoyager (2024a)\nGPT-4\nMinecraft simulation & Error trace\n✗\nDEPS (2023)\nIL\nPixels & Yaw\/pitch angle & GPS & Voxel\n✗\nSTEVE-1 (2023)\nGenerative model\nPixels\n✗\nVPT (2022)\nIL + RL\nPixels\n✓\nDreamerV3 (2023)\nRL\nPixels\n✗\nLS-Imagine\nRL\nPixels\n✗\nshown in Table 1, existing Minecraft agents present notable distinctions in learning paradigms (i.e.,\ncontroller), observation data, and the use of expert demonstrations.\nThe world model presented in this paper consists of two main components: a short-term transition\nbranch and a long-term imagination branch. As a result, it employs a complex notation system. We\nnow introduce the key notations that will be frequently used throughout the paper:\n• Mt represents the affordance map.\n• ct denotes the episode continuation flag.\n• jt is the jumping flag that triggers jumpy state imaginations.\n• ∆t represents the number of environmental steps between the jumpy transitions.\n• Gt is the cumulative reward over ∆t.\nWe use (o′\nt, a′\nt, M′\nt, r′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt) to represent the simulated environment data that are used to\ntrain the long-term imagination branch of the world model. The policy is learned on trajectories of\nmixed long- and short-term imaginations {(ˆst, ˆat, ˆrt, ˆct, ˆjt, ˆ∆t, ˆGt)}, where ˆst represents the latent\nstate, and the variables predicted by the model are indicated using the superscript (ˆ).\n3\nMETHOD\n3.1\nOVERVIEW OF LS-IMAGINE\nIn this section, we present the details of LS-Imagine, which involves the following algorithm steps,\nincluding world model learning, behavior learning, and environment interaction:\n1. Affordance map computation (Sec. 3.2.1): We employ a sliding bounding box to scan individual\nimages and execute continuous zoom-ins inside the bounding box, simulating consecutive video\nframes that correspond to long-distance state transitions. We then create affordance maps by\nassessing the relevance of the fake video clips to task-specific goals expressed in text using the\nestablished MineCLIP reward model (Fan et al., 2022).\n2. Rapid affordance map generation (Sec. 3.2.2): Given that affordance maps will be frequently\nused in subsequent Step 5 to evaluate the necessities for jumpy state transitions, we train a U-Net\nmodule to approximate the affordance maps annotated in Step 1 for the sake of efficiency.\n3. World model training (Sec. 3.3): We train the world model to capture short- and long-term state\ntransitions, using replay data with high responses from the affordance map. Each trajectory from\nthe buffer includes pairs of samples from both adjacent time steps and long-distance intervals.\n4. Behavior learning (Sec. 3.4): We perform an actor-critic algorithm to optimize the agent’s policy\nbased on a finite sequence of long short-term imaginations generated by the world model.\n5. Data update: We apply the agent to interact with the environment and gather new data. Next,\nwe leverage the generated affordance map to efficiently filter sample pairs suitable for long-term\nmodeling, incorporating both short- and long-term sample pairs to update the replay buffer.\n6. Iterate Steps 3–5.\nBelow, we discuss each training step in detail. The full algorithm can be found in Appendix C.4.\n3.2\nAFFORDANCE MAP AND INTRINSIC REWARD\nWe generate affordance maps using visual observations and textual task definitions to improve the\nsample efficiency of model-based reinforcement learning in open-world tasks. The core idea is to di-\nrect the agent’s attention to task-relevant areas of the visual observation, leading to higher exploration\n3\nPublished as a conference paper at ICLR 2025\n...\nMineCLIP\nCut a tree\n...\n16 frames of video \nto represent the\nsimulated exploration\nCorr = 0.60\nCorr = 0.90\nCorr = 0.15\n Fusion \n(a) Affordance map calculation\nMultimodal U-Net\nZoom\nin\nMultimodal U-Net\nMultimodal U-Net\nenv\nagent\n(b) Rapid affordance map generation\nFigure 2: The two steps for on-the-fly affordance map estimation: (a) Simulate exploration via image\nzoom-in and calculate the task-correlation scores of the virtual explorations using MineCLIP. (b)\nLearn to generate affordance maps more efficiently using a multimodal U-Net.\nefficiency. Let Mot,I(w, h) be the affordance map that represents the potential exploration value at\npixel position (w, h) on the image observation ot, given textual instruction I (e.g., “cut a tree”). The\naffordance map highlights the relevance between regions of the observation and the task description,\nserving as a spatial prior that effectively directs the agent’s exploration toward areas of interest.\n3.2.1\nAFFORDANCE MAP COMPUTATION VIA VIRTUAL EXPLORATION\nTo create the affordance map, as shown in Figure 2(a), we simulate and evaluate the agent’s exploration\nwithout relying on real successful trajectories. Concretely, we first adopt a random agent to interact\nwith task-specific environments for data collection. Starting with the agent’s observation ot at time\nstep t, we use a sliding bounding box with dimensions scaled to 15% of the observation’s width and\nheight to traverse the entire observation from left to right and top to bottom. The sliding bounding\nbox moves horizontally and vertically in 9 steps, respectively, covering every potential region in\nboth dimensions. For each position on the sliding bounding box of the observation ot, we crop 16\nimages from ot. These cropped images narrow the field of view to focus on the region and are resized\nback to the original image dimensions. These resized images are denoted as xk\nt (where 0 ≤k < 16).\nThe ordered set Xt = [xk\nt | k = 0, 1, . . . , 15] represents a sequence of 16 frames simulating the\nvisual transition as the agent moves towards the position specified by the current sliding bounding\nbox. Subsequently, we employ the MineCLIP model3 to calculate the correlation between the Xt of\nimages, simulating the virtual exploration process, and the task description I. In this way, we quantify\nthe affordance value of the sliding bounding box, indicating the potential exploration interest of the\narea. After calculating the correlation score for each sliding bounding box, we fuse these values to\nobtain a smooth affordance map Mot,I. For pixels that are covered by multiple sliding bounding\nboxes due to overlapping regions, the integrated affordance value is obtained by averaging the values\nfrom all the overlapping windows.\n3.2.2\nMULTIMODAL U-NET FOR RAPID AFFORDANCE MAP GENERATION\nThe annotation of affordance maps, as previously described, involves extensive window traversal and\ncomputations for each window position using a pre-trained video-text alignment model. This method\nis computationally demanding and time-consuming, making real-time applications challenging. To\naddress this issue, we first use a random agent to interact with the environment for data collection.\nNext, we annotate the affordance maps for the collected images using the aforementioned method\nbased on virtual exploration. We gather a dataset of tuples (ot, I, Mot,I) and use it to train a\nmultimodal U-Net based on Swin-Unet (Cao et al., 2022). To handle multimodal inputs, we extract\ntext features from the language instructions and image features from the downsampling process of\nSwin-Unet, and fuse them with multi-head attention. We present architecture details in Figure 9 in the\nappendix. In this way, with the pretrained multimodal U-Net, we can efficiently generate affordance\nmaps at each time step using visual observations and language instructions.\n3MineCLIP (Fan et al., 2022) pretrains a video-language representation using Minecraft videos, enabling it\nto compute the correlation between a text string and a 16-frame video segment.\n4\nPublished as a conference paper at ICLR 2025\n3.2.3\nAFFORDANCE-DRIVEN INTRINSIC REWARD\nTo leverage the task-relevant prior knowledge presented by the affordance map for efficient exploration\nin the open world, we introduce the following intrinsic reward function:\nrintr\nt\n=\n1\nWH\nW\nX\nw=1\nH\nX\nh=1\nMot,I(w, h) · G(w, h),\n(1)\nwhere W and H denote the width and height of the visual observation. G represents a Gaussian\nmatrix with dimensions matching those of the affordance map. It corresponds to a 2D Gaussian\ndistribution, with its peak located at the center of the affordance map. The values in the matrix are\ndetermined by standard deviations (σx, σy), while the mean is uniformly set to 1 across the entire\nmatrix. We present visualizations of G and conduct hyperparameter analyses on (σx, σy) in Appendix\nD.6. The intuition behind this design is to encourage the agent to move toward the target.\nOverall, the agent receives a composite reward consisting of the episodic sparse reward from the\nenvironment, the reward from MineCLIP (Fan et al., 2022), and the intrinsic reward from the\naffordance map: rt = renv\nt\n+ rMineCLIP\nt\n+ αrintr\nt , where α is a hyperparameter. In contrast to the\nMineCLIP reward, which relies on the agent’s past performance, our affordance-driven intrinsic\nreward emphasizes long-term values derived from future virtual exploration. It encourages the agent\nto adjust the policy to pursue task-related targets when they appear in its view, ensuring these targets\nare centrally positioned in future visual observations to maximize this reward function.\n3.3\nLONG SHORT-TERM WORLD MODEL\n3.3.1\nLEARNING JUMPING FLAGS\nIn LS-Imagine, the world model is customized for long-term and short-term state transitions. It\ndecides which type of transition to adopt based on the current state and predicts the next state\nwith the selected transition branch. To facilitate the switch between long-term and short-term state\ntransitions, we introduce a jumping flag jt, which indicates whether a jumpy transition or long-term\nstate transition, should be adopted at time step t. When a distant task-related target appears in the\nagent’s observation, which can be reflected by a higher kurtosis in the affordance map, a jumpy\ntransition allows the agent to imagine the future state of approaching the target. To this end, we\ndefine relative kurtosis Kr which measures whether there are significantly higher target areas than\nthe surrounding areas in the affordance map, and absolute kurtosis Ka represents the confidence level\nof target presence in that area. Formally,\nKr =\n1\nWH\nW\nX\nw=1\nH\nX\nh=1\n\"\u0012Mo,I(w, h) −mean(Mo,I)\nstd(Mo,I)\n\u00134#\n,\nKa = max(Mo,I) −mean(Mo,I).\n(2)\nTo normalize the relative kurtosis, we apply the sigmoid function to it, and then multiply it by the\nabsolute kurtosis to calculate the jumping probability:\nPjump = sigmoid(Kr) × Ka.\n(3)\nThe jumping probability measures the confidence in the presence of task-relevant targets far from the\nagent in the visual observation. To determine whether to employ long-term state transition, we use a\ndynamic threshold, which is the mean of the collected jumping probabilities at each time step, plus\none standard deviation. For a detailed explanation, please refer to C.1. If Pjump exceeds this threshold,\nthe jump flag jt is True and the agent switches to jumpy state transitions in the imagination phase.\n3.3.2\nLEARNING JUMPY STATE TRANSITIONS\nIn LS-Imagine, the state transition model includes both short-term and long-term branches. As\nshown in Figure 3 (a), the short-term transition model integrates the previous deterministic recurrent\nstate ht−1, stochastic state zt−1, and action at−1 to adopt the single-step transition. In contrast, the\nlong-term branch simulates jumpy state transitions toward the target. It is important to clarify that the\nindex t does not denote the time step during real environmental interactions but instead represents the\npositional order of states within the imagination sequence. The overall world model of LS-Imagine is\n5\nPublished as a conference paper at ICLR 2025\nenc\ndec\nenc\ndec\nShort-Term Branch\nCritic\nActor\nCritic\nCritic\nActor\nCritic\nShort-Term Imagination\nLong-Term Imagination\nTransition model\nReward predictor\nJump predictor\nInterval predictor\nLong-Term Branch\nFigure 3: The overall architecture of the world model and the behavior learning process.\nprimarily based on DreamerV3 (Hafner et al., 2023), with novel components specifically designed to\ncapture jumpy state transitions:\nShort-term transition model:\nht = fϕ(ht−1, zt−1, at−1)\nLong-term transition model:\nh′\nt = fϕ(ht−1, zt−1)\nEncoder:\nzt ∼qϕ(zt | ht, ot, Mt)\nDynamics predictor:\nˆzt ∼pϕ(ˆzt | ht)\nReward predictor:\nˆrt, ˆct ∼pϕ(ˆrt, ˆct | ht, zt)\nDecoder:\nˆot, ˆ\nMt ∼pϕ(ˆot, ˆ\nMt | ht, zt)\nJump predictor:\nˆjt ∼pϕ(ˆjt | ht, zt)\nInterval predictor:\nˆ∆′\nt, ˆG′\nt ∼pϕ( ˆ∆′\nt, ˆG′\nt | ht−1, zt−1, h′\nt, z′\nt)\n.\n(4)\nAt time step t, we feed the recurrent state ht, the observation ot, and the affordance map Mt into the\nencoder to obtain posterior state zt. We also use the affordance map as an input of the encoder, which\nserves as the goal-conditioned prior guidance to the agent. Notably, the prediction of prior state ˆzt\ndoes not involve the current observation or affordance map, relying solely on historical information.\nWe use (ht, zt) to reconstruct the visual observation ˆot and the affordance map ˆ\nMt, and predict the\nreward ˆrt, episode continuation flag ˆct, and jumping flag ˆjt. For long-term state transitions, we use an\ninterval predictor to estimate the expected number of interaction steps ˆ∆′\nt required to transition from\nthe pre-jump state (ht−1, zt−1) to the post-jump state (h′\nt, z′\nt), along with the expected cumulative\nreward ˆG′\nt that the agent may receive during this time interval. We detail the approach to annotate ∆′\nt\nand G′\nt using the real interaction data in Appendix C.1.\nWe collect short-term tuples Dt = (ot, at, Mt, rt, ct, jt, ∆t, Gt) from each interaction with the\nenvironment using the current policy. When observing jt = 1, we additionally construct long-term\ntuples D′\nt+1 = (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1) based on Dt. More details for this\nprocess can be found in Appendix C.1. During representation learning, we sample short-term tuples\n{Dt}T\nt=1 and the long-term tuples following jumpy transitions {D′\nt+1}t∈T from the replay buffer\nB, where T denotes the set of time steps at which long-term state transitions are required. The loss\nfunctions for each component of the short-term and long-term world model branch are as follows:\nShort-term branch:\n\n\n\n\n\n\n\n\n\nLdyn .= max (1, KL [sg (qϕ (zt | ht, ot, Mt)) ∥pϕ (zt | ht)])\nLenc .= max (1, KL [qϕ (zt | ht, ot, Mt) ∥sg (pϕ (zt | ht))])\nLdec .= −ln pϕ (ot, Mt | ht, zt)\nLpred .= −ln pϕ (rt, ct | ht, zt) −ln pϕ (jt | ht, zt)\n.\n(5)\n6\nPublished as a conference paper at ICLR 2025\nLong-term branch:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL′\ndyn\n.= max (1, KL [sg (qϕ (z′\nt | h′\nt, o′\nt, M′\nt)) ∥pϕ (z′\nt | h′\nt)])\nL′\nenc\n.= max (1, KL [qϕ (z′\nt | h′\nt, o′\nt, M′\nt) ∥sg (pϕ (z′\nt | h′\nt))])\nL′\ndec\n.= −ln pϕ (o′\nt, M′\nt | h′\nt, z′\nt)\nL′\npred\n.= −ln pϕ(r′\nt, c′\nt | h′\nt, z′\nt) −ln pϕ(j′\nt | h′\nt, z′\nt)\nL′\nint\n.= −ln pϕ (∆′\nt, G′\nt | ht−1, zt−1, h′\nt, z′\nt)\n.\n(6)\nWe can optimize the world model Wϕ by minimizing over replay buffer B:\nL .= E\nhP\n{Dt}T\nt=1 (βdynLdyn + βencLenc + βpred (Ldec + Lpred)) +\nβlong\nP\n{D′\nt+1}t∈T\n\u0000βdynL′\ndyn + βencL′\nenc + βpred\n\u0000L′\ndec + L′\npred + L′\nint\n\u0001\u0001 i\n.\n(7)\n3.4\nBEHAVIOR LEARNING OVER MIXED LONG SHORT-TERM IMAGINATIONS\nAs shown in Figure 3 (b), LS-Imagine employs an actor-critic algorithm to learn behavior from the\nlatent state sequences predicted by the world model. The goal of the actor is to optimize the policy\nto maximize the discounted cumulative reward Rt, while the role of the critic is to estimate the\ndiscounted cumulative rewards using the current policy for each state ˆst .= {ht, ˆzt}:\nActor:\nˆat ∼πθ (ˆat | ˆst) ,\nCritic:\nvψ\n\u0010\nˆRt | ˆst\n\u0011\n.\n(8)\nStarting from the initial state encoded from the sampled observation and the affordance map, we\ndynamically select either the long-term transition model or the short-term transition model to predict\nsubsequent states based on the jumping flag ˆjt. For the long short-term imagination sequence\n{(ˆst, ˆat)}L\nt=1 with an imagination horizon of L, we predict reward sequence ˆr1:L and the continuation\nflag sequence ˆc1:L through the reward predictor. Similar to Eq. (4), the index t does not represent the\ntime step in the environment, but rather the positional order of the states in the imagination sequence.\nSpecifically, starting from state ˆst, any subsequent state obtained via either a short-term transition or\na long-term transition is indexed sequentially as t + 1.\nFor jumpy states predicted by long-term imagination, the interval predictor estimates (i) the number of\nsteps ˆ∆t from ˆst−1 to ˆst and (ii) the potential discounted cumulative reward ˆGt over the time interval\nof ˆ∆t. Otherwise, for states obtained via short-term imagination, which correspond to single-step\ntransitions in the environment, we set ˆ∆t = 1 and ˆGt = ˆrt. Consequently, within one imagination\nepisode, we obtain a sequence of step intervals ˆ∆2:L and a sequence of predicted rewards ˆG2:L\nbetween consecutive imagination states.\nWe employ a modified bootstrapped λ-returns that considers both long-term and short-term imagina-\ntions to calculate the discounted cumulative rewards for each state:\nRλ\nt\n.=\n\u001a\nˆct{ ˆGt+1 + γ ˆ∆t+1 \u0002\n(1 −λ)vψ(ˆst+1) + λRλ\nt+1\n\u0003\n}\nif t < L\nvψ(ˆsL)\nif t = L .\n(9)\nThe critic uses the maximum likelihood loss to predict the distribution of the return estimates Rλ\nt :\nL(ψ) .= −\nL\nX\nt=1\nln pψ\n\u0000Rλ\nt | ˆst\n\u0001\n.\n(10)\nFollowing DreamerV3 (Hafner et al., 2023), we train the actor to maximize the return estimates\nRλ\nt . Notably, since long-term imagination does not involve actions, we do not optimize the actor at\ntime steps when jumpy state transitions are adopted. Therefore, unlike DreamerV3, we apply an\nadditional factor of (1 −ˆjt) to ignore updates at long-term imagination steps:\nL(θ) .= −\nL\nX\nt=1\nsg\n\u0014\u0010\n1 −ˆjt\n\u0011 Rλ\nt −vψ(ˆst)\nmax(1, S)\n\u0015\nlog πθ(ˆat | ˆst) + η H [πθ(ˆat | ˆst)] .\n(11)\n7\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n(a) Harvest log in plains\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n(b) Harvest water with bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess Rate\n(c) Harvest sand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\nSuccess Rate\n(d) Shear sheep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.1\n0.2\n0.3\n0.4\nSuccess Rate\n(e) Mine iron ore\nlog\nwater\nsand\nwool iron_ore\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMineCLIP Score\n(f) Comparison in MineCLIP reward\nVPT\nSTEVE-1\nPTGM\nDirector\nDreamerV3\nLS-Imagine\nFigure 4: Comparison of LS-Imagine against strong Minecraft agents, including DreamerV3 (Hafner\net al., 2023), VPT (Baker et al., 2022), STEVE-1 (Lifshitz et al., 2023), PTGM (Yuan et al., 2024),\nand Director (Hafner et al., 2022). We present the numerical results in Table 3 in the appendix.\n4\nEXPERIMENTS\nWe explore LS-Imagine on the challenging MineDojo (Fan et al., 2022) benchmark on top of the\npopular Minecraft game, which is a comprehensive simulation platform with various open-ended\ntasks. We use 5 tasks, i.e., harvest log in plains, harvest water with bucket, harvest sand, shear sheep,\nand mine iron ore. These tasks demand numerous steps to complete and present significant challenges\nfor agent learning. We adopt a binary reward that indicates whether the task was completed, along\nwith the MineCLIP reward (Fan et al., 2022). Further details of the environmental setups are provided\nin Appendix A. Besides, we introduce the compared models in Appendix B.\nImplementation details.\nWe conduct our experiments on the MineDojo environment, where both\nvisual observation and corresponding affordance maps are resized to 64 × 64 pixels. To generate\naccurate affordance maps, we collect 2,000 images from the environment using a random agent\nunder the current task instruction and generate a discrete set of (ot, I, Mot,I), which are then used to\nfinetune the multimodal U-Net for 200 epochs. For tasks in the MineDojo benchmark, we train the\nagent for 1 × 106 environment steps. Each training of LS-Imagine takes approximately 23 GB of\nVRAM and requires around 1.7 days to complete on a single RTX 3090 GPU.\n4.1\nMAIN COMPARISON\nWe evaluate all the Minecraft agents in terms of success rate shown in Figure 4 and per-episode\nsteps shown in Figure 5. We find that LS-Imagine significantly outperforms the compared models,\nparticularly in scenarios where sparse targets are distributed in the task. In Figure 4 (f), we showcase\nthe MineCLIP values achieved by LS-Imagine and DreamerV3. Specifically, a sliding window of\nlength 16 is used to compute the local MineCLIP values for each segment. The mean value is then\ncalculated from all sliding windows. We can see that agents trained using our method achieve higher\nMineCLIP values within a single episode compared to DreamerV3. This suggests that LS-Imagine\nfacilitates quicker detection of task-relevant visual targets in open-world environments.\nAdditionally, we present qualitative results in Figure 6(a). In the top row, we decode the latent\nstates before and after the jumpy state transitions back to the pixel space. To better understand how\naffordance maps facilitate the jumpy state transitions and whether they can provide effective goal-\nconditioned guidance, the bottom rows visualize the affordance maps reconstructed from the latent\nstates. These visualizations demonstrate that the proposed world model can adaptively determine\nwhen to utilize long-term imagination based on the current visual observation. Furthermore, the\n8\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n400\n600\n800\n1000\nSteps per episode\n(a) Harvest log in plains\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n400\n600\n800\n1000\n(b) Harvest water with bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n500\n600\n700\n800\n900\n1000\n(c) Harvest sand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n500\n600\n700\n800\n900\n1000\n(d) Shear sheep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n1500\n1600\n1700\n1800\n1900\n2000\n(e) Mine iron ore\nVPT\nSTEVE-1\nPTGM\nDirector\nDreamerV3\nLS-Imagine\nFigure 5: The number of steps per episode for task completion.\nLong-term imagination\nShort-term imagination\n(a) Visualization of long short-term imaginations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLS-Imagine (series)\nLS-Imagine (parallel)\n(b) Series vs. parallel connections\nFigure 6: (a) Visualization of long short-term imaginations and (b) a further discussion on possible\narchitecture designs of Series and Parallel connections of these two imagination pathways.\ngenerated affordance maps align effectively with areas that are highly relevant to the final goal,\nthereby enabling the agent to perform more efficient policy exploration.\n4.2\nMODEL ANALYSES\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nLS-Imagine\nw\/o long-term imagination\nw\/o intrinsic reward\nFigure 7: Ablation study results.\nAblation studies.\nWe conduct the ablation studies to\nvalidate the effect of the affordance-driven intrinsic re-\nward and long short-term imagination. Figure 7 presents\ncorresponding results in the challenging MineDojo tasks.\nAs shown by the blue curve, removing the long-term\nimagination of LS-Imagine leads to a performance de-\ncline, which indicates the necessity of introducing long-\nterm imagination and switching between it and short-\nterm imagination adaptively. For the model represented\nby the green curve, we do not employ affordance-driven\nintrinsic reward. It shows that the affordance-driven in-\ntrinsic reward also plays an important role during the\nearly training stage of agents. Additionally, unlike the\nMineCLIP reward being calculated based on a series of\nstates, the affordance-driven intrinsic reward relies solely on a single independent state. This approach\nenables a more accurate estimation of the reward for the post-jumpy-transition state.\nAlternative pathways of mixed imaginations.\nIt is worth highlighting that the long short-term\nimagination is implemented sequentially. In Figure 10(a) in the appendix, we provide a visualization\nillustrating how the agent sequentially performs short-term and long-term imaginations within a\nsingle sequence. Alternatively, as illustrated in Figure 10(b), we could structure long- and short-term\nimagination pathways in parallel. Specifically, we begin by applying short-term imagination within a\nsingle sequence. For each predicted state, we examine the jumping flag: If ˆjt = 1, we initiate a new\nimagination sequence starting from the post-jump state, which is predicted by the long-term transition\nmodel and the dynamics predictor. In other words, whenever a long-term state jump occurs, the world\nmodel generates a new sequence from the post-jump state, while the intermediate state transitions\nwithin the sequence are governed exclusively by short-term dynamics. Importantly, we optimize the\nactor independently for each sequence, ensuring that there is no gradient or value transfer between\nsequences. To evaluate the advantages of using sequential long short-term imagination, we conduct\nan experimental comparison between LS-Imagine (series) and LS-Imagine (parallel). Figure 6(b)\nshows that the LS-Imagine (series) outperforms LS-Imagine (parallel) by large margins. This implies\n9\nPublished as a conference paper at ICLR 2025\nthat the parallel imagination sequences are independent of one another, meaning that the sequence\nstarting with a post-jumping state does not guide the prior-jumping transitions.\nIn the appendix, we further include (i) experiments on the long-horizon “Tech Tree” task, (ii) analyses\nof the long-term imagination frequency and corresponding state jumping intervals ˆ∆t predicted by\nthe model, and (iii) visualization of affordance maps with occluded target objects.\n5\nRELATED WORK\nVisual MBRL.\nRecently, learning control policies from images, i.e., visual RL has been used widely,\nwhereas previous RL algorithms learn policies from low-dimensional states. Existing approaches\ncan be grouped by the use of model-free RL methods (Laskin et al., 2020; Schwarzer et al., 2021;\nStooke et al., 2021; Xiao et al., 2022; Parisi et al., 2022; Yarats et al., 2022; Zheng et al., 2023)\nor model-based RL methods (Hafner et al., 2019; 2020; 2021; Seo et al., 2022; Pan et al., 2022;\nZhang et al., 2023a; Mazzaglia et al., 2023; Micheli et al., 2023; Zhang et al., 2023b; Ying et al.,\n2023; Seo et al., 2023; Alonso et al., 2024; Hansen et al., 2024; Wang et al., 2024b). The following\nmethods specifically enhance the modeling of long-term dynamics in visual MBRL. Lee et al. (2024b)\nproposed the prediction of temporally smoothed rewards to address long-horizon sparse-reward\ntasks. R2I (Samsami et al., 2024) improves long-term memory and long-horizon credit assignment\nin MBRL. Unlike existing methods, our work presents a long short-term world model architecture\nspecifically designed for visual control in open-world environments.\nAffordance maps for robot learning.\nOur work is also related to the affordance map for robot\nlearning (Mo et al., 2021; Jiang et al., 2021; Yarats et al., 2021; Mo et al., 2022; Geng et al., 2022;\nXu et al., 2022a; Wang et al., 2022; Wu et al., 2022; Ha & Song, 2022; Xu et al., 2022b; Cheng\net al., 2024; Lee et al., 2024a; Li et al., 2024b). Where2Explore (Ning et al., 2023) introduces a\ncross-category few-shot affordance learning framework that leverages the similarities in geometries\nacross different categories. DualAfford (Zhao et al., 2023) learns collaborative actionable affordance\nfor dual-gripper manipulation tasks over various 3D shapes. VoxPoser (Huang et al., 2023) unleashes\nthe power of large language models and vision-language models for extracting affordances and\nconstraints of real-world manipulation tasks, which are grounded in 3D perceptual space. VRB (Bahl\net al., 2023) trains a visual affordance model with videos of human interactions and deploys the\nmodel in real-world robotic tasks directly. Qi et al. (2020) adopted a spatial affordance map that is\ntrained by interacting with the environment for navigation. However, our approach distinguishes\nitself by employing visual observation to generate affordance maps as guidance to mitigate the low\nexploration efficiency in open-world environments.\nHierarchical methods.\nLike our approach, Director (Hafner et al., 2022) learns hierarchical\nbehaviors in the latent space, which adopts high-level policy (manager) to produce latent goals to\nguide low-level policy (worker). Dr. Strategy (Hamed et al., 2024) proposes strategic dreaming\nwith latent landmarks to learn a highway policy that enables the agent to move to a landmark in the\ndream. Gumbsch et al. (2024) presented a hierarchy of world models, which perform high-level and\nlow-level prediction adaptively, and the high-level predictions depend on the low-level predictions.\nOur method distinguishes itself by generating affordance maps through image zoom-in to encourage\nthe agent to explicitly execute long-term imagination in the world model.\n6\nCONCLUSIONS AND LIMITATIONS\nIn this paper, we presented a novel approach to overcoming the challenges of training visual rein-\nforcement learning agents in high-dimensional open worlds. By extending the imagination horizon\nand leveraging a long short-term world model, our method facilitates efficient off-policy exploration\nacross expansive state spaces. The incorporation of goal-conditioned jumpy state transitions and\naffordance maps allows agents to better grasp long-term value, enhancing their decision-making\nabilities. Our results demonstrate substantial improvements over existing state-of-the-art techniques\nin MineDojo, highlighting the potential of our approach for open-world reinforcement learning and\ninspiring future research in this domain.\nA limitation of LS-Imagine is the computational overhead it introduces. Additionally, its effectiveness\nhas only been validated in 3D navigation environments with embodied agents. We aim to enhance\nthe generalization of our approach across a wider range of tasks.\n10\nPublished as a conference paper at ICLR 2025\nETHICS STATEMENT\nIn this work, we are committed to upholding ethical research practices. This work does not involve\nhuman subjects, personal data, or sensitive information. All environments and datasets used are\nsynthetic and publicly available. We recognize the potential for reinforcement learning models to\nbe misused, particularly in decision-making scenarios where unintended outcomes could arise. To\nmitigate these risks, we emphasize responsible deployment and encourage careful consideration of\nthe broader impact of such systems, restricting the use of our work strictly to research purposes.\nREPRODUCIBILITY STATEMENT\nWe prioritize the reproducibility of our work. All results can be reproduced on publicly available\nRL environments by following the experimental details presented in Sec. 4 and Appendix D.6. We\nprovide the source code at https:\/\/github.com\/qiwang067\/LS-Imagine.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science Foundation of China (Grants 62250062,\n62302246), the Smart Grid National Science and Technology Major Project (Grant 2024ZD0801200),\nthe Shanghai Municipal Science and Technology Major Project (Grant 2021SHZDZX0102), the\nFundamental Research Funds for the Central Universities, and the CCF-Tencent Rhino-Bird Open\nResearch Fund. Additional support was provided by the Natural Science Foundation of Zhejiang\nProvince, China (Grant LQ23F010008), the High Performance Computing Center at Eastern Institute\nof Technology, Ningbo, and Ningbo Institute of Digital Twin.\nREFERENCES\nEloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and\nFrançois Fleuret. Diffusion for world modeling: Visual details matter in atari. In NeurIPS, 2024.\nShikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from\nhuman videos as a versatile representation for robotics. In CVPR, pp. 13778–13790, 2023.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In NeurIPS, 2022.\nHu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang.\nSwin-unet: Unet-like pure transformer for medical image segmentation. In ECCVW, 2022.\nGuangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin Sun, and Jiang Bian. Empow-\nering large language models on robotic manipulation with affordance prompting. arXiv preprint\narXiv:2404.11027, 2024.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In NeurIPS, volume 35, pp. 18343–18362, 2022.\nYiran Geng, Boshi An, Haoran Geng, Yuanpei Chen, Yaodong Yang, and Hao Dong. End-to-end\naffordance learning for robotic manipulation. arXiv preprint arXiv:2209.12941, 2022.\nChristian Gumbsch, Noor Sajid, Georg Martius, and Martin V Butz. Learning hierarchical world\nmodels with adaptive temporal abstractions from discrete latent dynamics. In ICLR, 2024.\nHuy Ha and Shuran Song. Flingbot: The unreasonable effectiveness of dynamic manipulation for\ncloth unfolding. In CoRL, pp. 24–33, 2022.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\nDavidson. Learning latent dynamics for planning from pixels. In ICML, pp. 2555–2565, 2019.\n11\nPublished as a conference paper at ICLR 2025\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\nbehaviors by latent imagination. In ICLR, 2020.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete\nworld models. In ICLR, 2021.\nDanijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter Abbeel. Deep hierarchical planning from\npixels. In NeurIPS, 2022.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nHany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, and Sungjin Ahn. Dr. strategy: Model-based\ngeneralist agents with strategic dreaming. In ICML, 2024.\nNicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous\ncontrol. In ICLR, 2024.\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models. In CoRL, 2023.\nZhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, and Yuke Zhu. Synergies between affordance\nand geometry: 6-dof grasp detection via implicit representations. Robotics: science and systems,\n2021.\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations\nfor reinforcement learning. In ICML, pp. 5639–5650, 2020.\nOlivia Y Lee, Annie Xie, Kuan Fang, Karl Pertsch, and Chelsea Finn. Affordance-guided reinforce-\nment learning via visual prompting. arXiv preprint arXiv:2407.10341, 2024a.\nVint Lee, Pieter Abbeel, and Youngwoon Lee. Dreamsmooth: Improving model-based reinforcement\nlearning via reward smoothing. In ICLR, 2024b.\nHao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li,\nLewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language\nmodels for minecraft. In CVPR, pp. 16426–16435, 2024a.\nXiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming\nLiu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric\nrobotic manipulation. In CVPR, pp. 18061–18070, 2024b.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. In NeurIPS, 2023.\nPietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, and Sai Rajeswar. Choreographer:\nLearning and adapting skills in imagination. In ICLR, 2023.\nVincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models.\nIn ICLR, 2023.\nKaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani.\nWhere2act: From pixels to actions for articulated 3d objects. In ICCV, pp. 6813–6823, 2021.\nKaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and Leonidas Guibas. O2o-afford: Annotation-free\nlarge-scale object-object affordance learning. In CoRL, pp. 1666–1677, 2022.\nChuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, and Hao Dong. Where2explore: Few-shot\naffordance learning for unseen novel categories of articulated objects. NeurIPS, 2023.\nKolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer\nSingh, and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision making\nusing language guided world modelling. In ICML, pp. 26311–26325, 2023.\n12\nPublished as a conference paper at ICLR 2025\nMinting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging\nnoncontrollable visual dynamics in world models. In NeurIPS, volume 35, pp. 23178–23191,\n2022.\nSimone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising\neffectiveness of pre-trained vision models for control. In ICML, pp. 17359–17371, 2022.\nWilliam Qi, Ravi Teja Mullapudi, Saurabh Gupta, and Deva Ramanan. Learning to move with\naffordance maps. In ICLR, 2020.\nMohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar. Mastering\nmemory tasks with world models. In ICLR, 2024.\nMax Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R De-\nvon Hjelm, Philip Bachman, and Aaron C Courville. Pretraining representations for data-efficient\nreinforcement learning. In NeurIPS, volume 34, pp. 12686–12699, 2021.\nYounggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel. Reinforcement learning with\naction-free pre-training from videos. In ICML, pp. 19561–19579, 2022.\nYounggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view\nmasked world models for visual robotic manipulation. In ICML, pp. 30613–30632, 2023.\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning\nfrom reinforcement learning. In ICML, pp. 9870–9879, 2021.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nTMLR, 2024a.\nQi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, and Xiaokang Yang. Making offline rl\nonline: Collaborative world models for offline visual reinforcement learning. In NeurIPS, 2024b.\nYian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas J Guibas, and Hao Dong.\nAdaafford: Learning to adapt manipulation affordance for 3d articulated objects via few-shot\ninteractions. In ECCV, pp. 90–107, 2022.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with large language models enables open-world\nmulti-task agents. In NeurIPS, 2023.\nRuihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin\nChen, Leonidas Guibas, and Hao Dong. Vat-mart: Learning visual action trajectory proposals for\nmanipulating 3d articulated objects. ICLR, 2022.\nTete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for\nmotor control. arXiv preprint arXiv:2203.06173, 2022.\nChao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin Zhu, and Siyuan Huang. Partafford:\nPart-level affordance discovery from 3d objects. arXiv preprint arXiv:2202.13519, 2022a.\nZhenjia Xu, Zhanpeng He, and Shuran Song. Universal manipulation policy network for articulated\nobjects. IEEE robotics and automation letters, 7(2):2447–2454, 2022b.\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. In ICLR, 2021.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control:\nImproved data-augmented reinforcement learning. In ICLR, 2022.\nChengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Songming Liu, Jialian Li, Dong Yan,\nand Jun Zhu. Reward informed dreamer for task generalization in reinforcement learning. arXiv\npreprint arXiv:2303.05092, 2023.\n13\nPublished as a conference paper at ICLR 2025\nHaoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu. Pre-training goal-based models for\nsample-efficient reinforcement learning. In ICLR, 2024.\nWeipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang. Storm: Efficient stochastic\ntransformer based world models for reinforcement learning. In NeurIPS, 2023a.\nWendong Zhang, Geng Chen, Xiangming Zhu, Siyu Gao, Yunbo Wang, and Xiaokang Yang.\nPredictive experience replay for continual visual control and forecasting.\narXiv preprint\narXiv:2303.06572, 2023b.\nYan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang, Qingnan Fan, Kaichun Mo, and Hao Dong.\nDualafford: Learning collaborative visual affordance for dual-gripper manipulation. ICLR, 2023.\nRuijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daumé III,\nand Furong Huang. Taco: Temporal latent action-driven contrastive loss for visual reinforcement\nlearning. In NeurIPS, volume 36, 2023.\n14\nPublished as a conference paper at ICLR 2025\nAPPENDIX\nA\nENVIRONMENT DETAILS\nAs illustrated in Table 2, language description is employed for calculating the MineCLIP reward (Fan\net al., 2022). Initial tools are the items provided in the inventory at the beginning of each episode.\nInitial mobs and distance specifies the types of mobs present at the start of each episode and their\ninitial distance from the agent. Max steps refers to the maximum allowed steps per episode.\nTable 2: Details of the MineDojo tasks.\nTask\nLanguage description\nInitial tools\nInitial mobs and distance Max steps\nHarvest log in plains\n“Cut a tree.”\n–\n–\n1000\nHarvest water with bucket\n“Obtain water.”\nbucket\n–\n1000\nHarvest sand\n“Obtain sand.”\n–\n–\n1000\nShear sheep\n“Obtain wool.”\nshear\nsheep, 15\n1000\nMine iron ore\n“Mine iron ore.”\nstone pickaxe\n–\n2000\nB\nCOMPARED METHODS\nWe compare LS-Imagine with strong Minecraft agents, including:\n• DreamerV3 (Hafner et al., 2023): An MBRL approach that learns directly from the step-by-step\nimaginations of future latent states generated by the world model.\n• VPT (Baker et al., 2022): A foundation model designed for Minecraft trained through behavior\ncloning, on a dataset consisting of 70,000 hours of game playing collected from the Internet.\n• STEVE-1 (Lifshitz et al., 2023): An instruction-following Minecraft agent that translates lan-\nguage instructions into specific goals. To evaluate its effectiveness, we assess Steve-1’s zero-shot\nperformance on our tasks by supplying it with task instructions.\n• Director (Hafner et al., 2022): An agent learns hierarchical behaviors by leveraging a world model\nto plan within its latent space.\n• PTGM (Yuan et al., 2024): An RL method that pretrains goal-based policy and adopts temporal\nabstractions and behavior regularization.\nC\nMODEL DETAILS\nC.1\nENVIRONMENTAL INTERACTION AND DATA COLLECTION\nTo train LS-Imagine’s world model, we collect both short-term and long-term transition data through\ninteractions with the environment. As shown in Figure 8, at each time step t, the agent interacts with\nthe environment following the current policy. At each time step, the data buffer collects a tuple Dt,\nwhich includes (ot, at, Mt, rt, ct, jt, ∆t, Gt):\n• ot represents the observed image.\n• at represents the agent’s action taken given ot.\n• Mt is the affordance map generated by a multimodal U-Net given ot and task instructions I.\n• rt is defined in Sec. 3.2.3, which is the immediate reward computed as a weighted sum of the sparse\nenvironmental reward renv\nt\nafter executing at−1, the MineCLIP reward rMineCLIP\nt\nfrom a pretrained\nscoring model (Fan et al., 2022), and the intrinsic reward rintr\nt\ndefined in Eq. (1) and based on Mt.\n• ct is the continuation flag received from the environment, which indicates whether further interac-\ntion is required after this step.\n• jt is the jumping flag, which is used to train the world model to trigger long-term imagination\nduring model-based behavior learning. We first estimate the jumping probability Pjump using Eq. (3)\nbased on Mt. To stabilize training, we establish a dynamic threshold Pthresh, which accounts for\nthe varying guidance strength provided by the affordance map across different tasks, resulting in\n15\nPublished as a conference paper at ICLR 2025\nMultimodal U-Net\n   ... ...   \n   ... ...   \nMultimodal U-Net\nenv\n Zoom in \nagent\nMultimodal U-Net\n...\nFigure 8: Environmental interaction and data collection.\ntask-specific distributions of Pjump. Specifically, from the beginning of training, we store the Pjump\nvalues for every interaction step in a dedicated buffer. The threshold Pthresh is then dynamically\ncalculated as the mean of all Pjump values currently in the buffer plus their standard deviation. This\ndynamic adjustment ensures that the threshold adapts to the characteristics of the task and remains\nrobust throughout training. If Pjump > Pthresh, we set jt = 1; otherwise, jt = 0.\n• ∆t represents the expected number of step intervals in the jumpy state transitions during long-term\nimaginations. Specifically, we set ∆t = 1 by default, corresponding to a short-term transition.\n• Gt represents the expected cumulative reward between the pre- and post-jump states when long-\nterm imagination occurs. Specifically, for a short-term transition, we set Gt = rt by default.\nIf jt = 0, Dt is defined as the starting point of a short-term transition within the pair (Dt, Dt+1).\nDuring world model training, (Dt, Dt+1) is replayed to train the related modules associated with\nshort-term dynamics. Once we obtain jt = 1 during interactions, we define the current step as the\nstarting point of a simulated long-term transition (Dt, D′\nt+1). Notably, we use D′\nt+1 to differentiate\nfrom its short-term counterparts.\nWe define D′\nt+1 = (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1), where r′\nt+1 and c′\nt+1 are com-\nputed in the same manner as in short-term tuples but with o′\nt+1 and M′\nt+1 as inputs. Similarly, a′\nt+1\nand j′\nt+1 are also computed in the same way as in short-term tuples. We record them in the data\nbuffer for better training of the reward predictor and the jump predictor.\nThe next question is how to annotate ∆′\nt+1, G′\nt+1, and o′\nt+1 to train the long-term branch.\n• o′\nt+1 is a simulated image rather than a real-captured image. It is obtained by cropping the original\nobservation ot based on the high-value regions in the affordance map Mt.\n• ∆′\nt+1 is an estimation of the number of real interaction steps between the pre-jump state and\nthe post-jump state. Since the post-jump state is not real data obtained from the environment,\nwe first identify a real state that closely resembles the post-jump state. We then calculate the\nnumber of steps required to transition from the pre-jump state to this identified real post-jump state.\nSpecifically, we use the intrinsic reward as a measurement. Starting from the pre-jump state, during\nsubsequent interactions with the environment, if the agent reaches a real state where the intrinsic\nreward satisfies rintr\nt+∆′\nt+1 ≥rintr ′\nt+1 , we take this state as the real post-jump state and take ∆′\nt+1 as\nthe long-term jumping interval.\n• G′\nt+1 is the cumulative reward within ∆′\nt+1 interaction steps, i.e., G′\nt+1 = P∆′\nt+1\ni=1 γi−1rt+i.\n16\nPublished as a conference paper at ICLR 2025\nPatch Partition\n Linear Embedding\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nSwin Transformer\nBlock × 2\nSwin Transformer\nBlock × 2\nPatch Expanding\nSwin Transformer\nBlock × 2\nPatch Expanding\nSwin Transformer\nBlock × 2\nPatch Expanding\nPatch Expanding\nLinear Projection\nCut a tree\nCompute affordance\nmap with MineCLIP\nText \nEncoder\nTIA\nTIA\nTIA\nTIA\nFigure 9: The architecture of multimodal U-Net.\nC.2\nFRAMEWORK OF MULTIMODAL U-NET\nAs described in Sec. 3.2.2, we train a multimodal U-Net to rapidly generate affordance maps based on\nobservation images and task instructions. Our enhanced multimodal U-Net architecture, as illustrated\nin Figure 9, is based on Swin-Unet (Cao et al., 2022), a U-shaped encoder-decoder architecture built\non Swin Transformer blocks. The enhanced multimodal U-Net consists of an encoder, a decoder, a\nbridge layer, and a text processing module. In the Swin-Unet-inspired structure, the basic unit is the\nSwin Transformer block. For the encoder, the input image is divided into non-overlapping patches\nof size 4 × 4 to convert the input into a sequence of patch embeddings. Through this method, each\npatch has a feature dimension of 4 × 4 × 3 = 48. The patch embeddings are then projected through a\nlinear embedding layer (denoted as C), and the transformed patch tokens are passed through several\nSwin Transformer blocks and patch merging layers to produce hierarchical feature representations.\nThe patch merging layers are responsible for downsampling and increasing the dimensionality, while\nthe Swin Transformer blocks handle feature representation learning.\nFor the task instruction, the text description is processed through the text encoder of MineCLIP (Cao\net al., 2022) to obtain text embeddings, which are integrated with the image features extracted at\neach layer of the encoder via the Text-Image Attention (TIA) module. The TIA module employs a\nmulti-head attention mechanism to fuse image features (as keys and values) with text features (as\nqueries) in a multi-scale attention-based fusion. The resulting fused text-image features are passed\nthrough the bridge layer and are subsequently combined with the corresponding features during the\nupsampling process in the decoder.\nThe decoder comprises Swin Transformer blocks and patch-expanding layers. The extracted context\nfeatures are combined through the bridge layer with the multi-scale text-image features from the\nencoder to compensate for the spatial information lost during downsampling and to integrate the text\ninformation. Unlike the patch merging layers, the patch expanding layers are specifically designed for\nupsampling. They reshape the adjacent feature maps by performing a 2× upsampling of the resolution,\nexpanding the feature maps into larger ones. Finally, a final patch expanding layer performs a 4×\nupsampling to restore the resolution of the feature map to the input resolution W × H ), followed by\na linear projection layer applied on the upsampled features to produce pixel-level affordance maps.\n17\nPublished as a conference paper at ICLR 2025\n...\n(a) LS-Imagine (series)\n...\n...\n...\n...\n(b) LS-Imagine (parallel)\nFigure 10: Comparison with series and parallel variants of mixed imaginations.\nC.3\nVARIANTS OF LONG SHORT-TERM IMAGINATIONS\nWe compare two alternative pathways of the long short-term imaginations in Figure 10.\nC.4\nFULL ALGORITHM\nWe present the training pipeline of LS-Imagine in Algorithm 1.\nAlgorithm 1 The training pipeline of LS-Imagine.\n1: Initialize parameters ϕ, θ, ψ.\n2: Compute affordance map with MineCLIP.\n▷Affordance map generation\n3: Train multimodal U-Net with annotated data. ▷To enable real-time interaction with the affordance maps\n4: Train the random agent and collect a replay buffer B.\n5: while not converged do\n6:\nSample long short-term transitions from B.\n▷Representation learning\n7:\nUpdate the world model ϕ using Eq. (7).\n8:\nGenerate (s1, ˆa1, ˆj1) using πθ and Wϕ.\n9:\nfor time step t = 2 · · · L do\n▷Behavior learning\n10:\nif jump flag ˆjt−1 then\n11:\nGenerate (s′\nt, a′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt) using πθ and long-term imagination of Wϕ.\n12:\nUpdate (ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt) ←(s′\nt, a′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt).\n13:\nelse\n14:\nGenerate (˜st, ˜at, ˜rt, ˜ct, ˜jt) using πθ and short-term imagination of Wϕ.\n15:\nUpdate (ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt) ←(˜st, ˜at, ˜ct, ˜jt, 1, ˜rt).\n16:\nend if\n17:\nend for\n18:\nCalculate value estimate using Eq. (9).\n19:\nOptimize actor πθ using Eq. (11) over {(ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt)}L\nt=1.\n20:\nOptimize critic vψ using Eq. (10) over {(ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt)}L\nt=1.\n21:\nfor time step t = 1 · · · T do\n▷Environment interaction\n22:\nSample ˆat ∼πθ (ˆat | ˆst)\n23:\nrenv\nt , ot+1, ct ←env.step(ˆat)\n24:\nGenerate affordance map Mt with multimodal U-Net for each ot.\n25:\nCalculate intrinsic reward rintr\nt\nand jump flag jt based on the affordance map.\n26:\nCollect short-term data (ot, at, Mt, rt, ct, jt, ∆t, Gt).\n27:\nif jumpy flag jt then\n28:\nConstruct long-term data (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1).\n29:\nend if\n30:\nend for\n31:\nAppend long short-term transitions to B.\n32: end while\n18\nPublished as a conference paper at ICLR 2025\nTable 3: The success rate and the number of steps per episode for task completion.\nModel\nHarvest log in plains Harvest water with bucket\nHarvest sand\nShear sheep\nMine iron ore\nsucc. (%)\nsucc. step\nsucc. (%)\nsucc. step\nsucc. (%) succ. step succ. (%) succ. step succ. (%) succ. step\nVPT\n6.97\n963.32\n0.61\n987.65\n12.99\n880.54\n1.94\n987.49\n0.00\n—\nSTEVE-1\n57.00\n752.47\n6.00\n989.07\n37.00\n770.40\n3.00\n992.36\n0.00\n—\nPTGM\n41.86\n811.19\n2.78\n977.78\n17.71\n833.64\n21.54\n887.03\n15.14\n1586.03\nDirector\n8.67\n968.09\n20.90\n931.74\n36.36\n825.35\n1.27\n995.99\n7.82\n1906.31\nDreamerV3\n53.33\n711.22\n55.72\n628.79\n59.88\n548.76\n25.13\n841.14\n16.79\n1789.06\nLS-Imagine\n80.63\n503.35\n77.31\n502.61\n62.68\n601.18\n54.28\n633.78\n20.28\n1748.55\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(a) Jumping frequency\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0\n100\n200\n300\n400\n500\n(b) Interval ˆ∆t\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.00\n0.05\n0.10\n0.15\n0.20\nHarvest log in plains\nHarvest water with bucket\n(c) Dynamic threshold Pthresh\nFigure 11: Analyses of long-term imaginations throughout training.\nC.5\nCLARIFICATION ON STOCHASTIC LONG-TERM IMAGINATION\nOne might argue that long-term imagination could skip essential intermediate steps that gradually\nlead to the objective, potentially resulting in a lack of learning for these crucial actions. To address\nthis issue, we adopt a probabilistic mechanism. Specifically, even when ˆjt = True, indicating that a\nlong-term transition is to be executed, we implement a probability of 0.7 for executing the jump and\n0.3 for not jumping. This allocation ensures a 30% chance that the transition will execute the short-\nterm imagination with gradient feedback attached to the actions. This stochastic decision-making\nis based on a uniform distribution, providing a balanced approach between leveraging long-term\nimagination and capturing essential short-term behaviors.\nC.6\nADDITIONAL LIMITATION\nIt is worth mentioning that LS-Imagine simulates the agent’s state when approaching a target object\nin 3D navigation environments with embodied agents by zooming in on the observed image, and\nsets intrinsic rewards based on whether the agent is close to and has positioned the target object\nat the center of the observation. Therefore, LS-Imagine is not suitable for environments with\nfixed viewpoints, 2D environments, or those where the reward mechanism is more complex than\napproaching objects (e.g., driving).\nD\nADDITIONAL RESULTS\nD.1\nNUMERICAL COMPARISONS\nTable 3 compares existing approaches on the challenging MineDojo environment.\nD.2\nANALYSES ON LONG-TERM IMAGINATIONS\nWe use the task harvest log in plains as an example to facilitate the understanding of the long short-\nterm imagination process. In Figure 11(a), we first track the frequency of long-term imaginations and\nthe corresponding predicted state intervals ˆ∆t throughout the training process. The curve shows the\nproportion of imagination sequences involving jumpy state transitions relative to the total number of\nimagination sequences. Initially, the jumping frequency is low because the world model has not yet\nlearned to identify when a jump is necessary based on the state. As the model’s predictions improve\nin the early stages of training, the frequency increases, likely due to the agent’s underdeveloped\npolicies, which result in more observations far from the goal and necessitate long-term exploration.\nOver time, as the agent learns policies that bring it closer to the target, the frequency of observations\nfar from the goal decreases, reducing the need for jumps.\n19\nPublished as a conference paper at ICLR 2025\n(a) Harvest log in plains\nShort-term imagination\nLong-term imagination\n(b) Harvest water with bucket\n(c) Harvest sand\n(d) Shear sheep\n(e) Mine iron ore\nFigure 12: Visualization of the complete long short-term imagination sequences.\nAdditionally, we find that among all sequences with jumpy state transitions, the average number of\njumpy transitions per sequence, within a horizon of 15 steps, 1.02. This indicates that, in most cases\nof these tasks, a single jumpy transition is sufficient to bring the agent close to the target.\nIn Figure 11(b), we track the variations of the jumping state intervals, ˆ∆t, throughout training. At the\nbeginning, ˆ∆t is high, indicating that the policy requires many steps to reach the target. As the policy\nimproves, fewer steps are needed to approach the target, leading to a gradual decrease in ˆ∆t. Notably,\nas ˆ∆t evolves with the updated policy, it also ensures minimal misalignment in Eq. (9) between the\nfuture cumulative rewards computed with jumpy imaginations and the behavior policy.\nFurthermore, in Figure 11(c), we track the variation curves of the dynamic threshold Pthresh during\ntraining in different tasks, and observe that:\n20\nPublished as a conference paper at ICLR 2025\n(a) Explore for a village\n(b) Mine ore\nFigure 13: Affordance maps when the target is invisible or occluded.\nlog\nplanks\ncrafting\ntable\nstick\nwooden\npickaxe\ncobblestone\nstone\npickaxe\n20\n30\n40\n50\n60\n70\n80\n90\n100\nSuccess rate (%)\n83\n83\n83\n83\n80\n44\n32\n91\n91\n88\n88\n84\n80\n69\nDreamerV3\nLS-Imagine\n(a) Success rate\nlog\nplanks\ncrafting\ntable\nstick\nwooden\npickaxe\ncobblestone\nstone\npickaxe\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nSteps per episode\n437\n454\n465\n481\n552\n854\n923\n320\n329\n363\n381\n447\n613\n684\nDreamerV3\nLS-Imagine\n(b) The number of steps required for task completion\nFigure 14: Comparison of LS-Imagine and DreamerV3 on a long-horizon “Tech Tree” task.\n• For task such as harvest log in plains, the variance of Pthresh is high during the early stages of training.\nSince Pthresh serves as a temporal smoothing of Pjump, this reflects the significant fluctuations of\nPjump at the beginning of training, highlighting the importance of adopting a dynamic threshold.\n• Across various tasks, Pthresh consistently converges in the later stages of training, demonstrating its\neffectiveness in improving the stability of exploratory imaginations.\n• The converged values of Pthresh differ across tasks, indicating that involving an automated computa-\ntion of Pthresh enables us to avoid tedious hyperparameter tuning.\nD.3\nVISUALIZATION OF LONG SHORT-TERM IMAGINATIONS\nAs illustrated in Figure 12, we visualize the complete long short-term imagination sequences for\nthe agent across various tasks. This visualization further demonstrates how the affordance map\naccurately identifies regions of high exploration potential in the image, and how the long short-term\nimagination approach provides reasonable and applicable guidance for the agent’s task execution.\nThese qualitative results reinforce the effectiveness of our method in guiding the agent toward its\ngoal with greater precision and efficiency.\nD.4\nDEPENDENCE ON THE VISIBILITY OF OBJECTS\nThe long-term transitions of our approach rely on the affordance map to identify high-value ex-\nploration areas. However, it is crucial to note that our affordance map generation method is not\nmerely an object recognition algorithm that highlights areas only when the target is present. Thanks\nto MineCLIP’s pretraining on extensive expert demonstration videos, our approach can generate\naffordance maps that provide guidance even when the target is completely occluded.\n21\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nLong-term branch loss scale\nlong = 0.1\nlong = 0.5\nlong = 1.0\nlong = 5.0\nlong = 10.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nIntrinsic reward weight\n= 0.1\n= 0.5\n= 1.0\n= 5.0\n= 10.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nGaussian distribution std\n(\nx,\ny) = (64, 40)\n(\nx,\ny) = (128, 80)\n(\nx,\ny) = (256, 160)\nFigure 15: Results of hyperparameter sensitivity analyses.\nFor instance, as illustrated in Figure 13, throughout the task of locating a village, the affordance map\nconsistently provides effective guidance to the agent, suggesting exploration of the forest to the right\nor the open area on the left hillside, even when the village is not visible in the current observation.\nSimilarly, in mining tasks where ores are typically underground, the affordance map directs the agent\nto dig into the mountain area on the right. As we can see, even when the target is occluded, the\naffordance map enables the agent to continue exploring effectively.\nFurther research direction.\nDue to the complexity of open-world environments, the affordance\nmap may fail to provide effective guidance in scenarios that the MineCLIP model has not encountered\nbefore. To address this issue, we plan to progressively finetune the MineCLIP model with the\ncollected new data and introduce a new prompt to the agent: “Explore the widest possible area to find\n{target}” when the affordance map fails to identify high-value areas. This prompt, combined with\nintrinsic rewards generated by MineCLIP, encourages the agent to conduct extensive exploration.\nD.5\nRESULTS ON LONG-HORIZON TASKS\nTo demonstrate the potential application of LS-Imagine in more complex tasks, we conduct experi-\nments on a “Tech Tree” task in MineDojo, specifically crafting a stone pickaxe from scratch. This\ntask involves seven subgoals: log, planks, crafting table, stick, wooden pickaxe, cobblestone, and\nstone pickaxe. Since LS-Imagine is primarily designed to focus on environmental interactions and\ntask execution under fixed objectives, rather than task decomposition and planning, we adopt the\nDECKARD method (Nottingham et al., 2023) for task planning. This method provides top-level\nguidance, with LS-Imagine executing the corresponding subtasks. Each subtask was trained for\n1 million steps and then tested within 1,000 steps per episode. The results are shown in Figure\n14, which demonstrate that our LS-Imagine consistently outperforms DreamerV3, achieving higher\nsuccess rates and requiring fewer steps to complete each subgoal.\nD.6\nHYPERPARAMETER ANALYSES\nWe conduct sensitivity analyses on three hyperparameters:\n• The long-term branch loss scale βlong: As shown in Figure 15 (Left), we observe that when βlong\nfor the long-term branch is too small or too large, it impedes the learning of long-term imagination,\nleading to a decline in performance.\n• The intrinsic reward weight α: From Figure 15 (Middle), we observe that if the hyperparameter\nα for intrinsic reward is excessively small, it may result in insufficient guidance and inaccurate\nreward estimation for the post-jumpy-transition state.\n• The intrinsic reward Gaussian parameters (σx, σy): As shown in Figure 16, (σx, σy) control the\nstandard deviations of the Gaussian distribution along the horizontal and vertical axes, respectively.\nIntuitively, setting these hyperparameters too low may cause the model to overlook targets located\nat the edges of the observed images. Conversely, excessively high (σx, σy) may reduce the reward\ndiscrepancy for targets at different positions within the observation, thereby diminishing the agent’s\nincentive to focus on the target precisely. From Figure 15 (Right), we observe that the final\nperformance is robust to the tested parameters, with all configurations outperforming the baseline\nmodels presented in previous experiments.\nThe final hyperparameters of LS-Imagine are shown in Table 4.\n22\nPublished as a conference paper at ICLR 2025\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (64, 40)\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (128, 80)\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (256, 160)\n0.5\n1.0\n1.5\n2.0\n2.5\nFigure 16: Visualization of Gaussian matrices with different standard deviations.\nTable 4: Hyperparameters of LS-Imagine.\nName\nNotation\nValue\nAffordance map generation\nSliding window size\n—\n0.15 × 0.15\nSliding steps\n—\n9 × 9\nU-Net train epochs\n—\n500\nU-Net initial learning rate\n—\n5 × 10−4\nU-Net learning rate decay epochs\n—\n50\nU-Net learning rate decay rate\n—\n0.10\nText feature dimensions\n—\n512\nGaussian distribution standard deviations\n(σx, σy)\n(128, 80)\nGeneral\nReplay capacity\n—\n1 × 106\nBatch size\nB\n16\nBatch length\nT\n32\nTrain ratio\n—\n16\nWorld Model\nIntrinsic reward weight\nα\n1\nDeterministic latent dimensions\n—\n4,096\nStochastic latent dimensions\n—\n32\nDiscrete latent classes\n—\n32\nRSSM number of units\n—\n1,024\nWorld model learning rate\n—\n1 × 10−4\nLong-term branch loss scale\nβlong\n1\nReconstruction loss scale\nβpred\n1\nDynamics loss scale\nβdyn\n1\nRepresentation loss scale\nβrep\n0.1\nBehavior Learning\nImagination horizon\nL\n15\nDiscount\nγ\n0.997\nλ-target\nλ\n0.95\nActor learning rate\n—\n3 · 10−5\nCritic learning rate\n—\n3 · 10−5\n23\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Open-World Reinforcement Learning over Long Short-Term Imagination.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nOpen-World Reinforcement Learning over Long Short-Term Imagination\n```\n#### 2. 论文摘要\n```\nTraining visual reinforcement learning agents in a high-dimensional open\nworld presents significant challenges. While various model-based methods have\nimproved sample efficiency by learning interactive world models, these agents\ntend to be \"short-sighted\", as they are typically trained on short snippets of\nimagined experiences. We argue that the primary challenge in open-world\ndecision-making is improving the exploration efficiency across a vast state\nspace, especially for tasks that demand consideration of long-horizon payoffs.\nIn this paper, we present LS-Imagine, which extends the imagination horizon\nwithin a limited number of state transition steps, enabling the agent to\nexplore behaviors that potentially lead to promising long-term feedback. The\nfoundation of our approach is to build a $\\textit{long short-term world\nmodel}$. To achieve this, we simulate goal-conditioned jumpy state transitions\nand compute corresponding affordance maps by zooming in on specific areas\nwithin single images. This facilitates the integration of direct long-term\nvalues into behavior learning. Our method demonstrates significant improvements\nover state-of-the-art techniques in MineDojo.\n```\n\n#### 3. 论文全文\n```\nPublished as a conference paper at ICLR 2025\nOPEN-WORLD REINFORCEMENT LEARNING OVER\nLONG SHORT-TERM IMAGINATION\nJiajian Li1∗\nQi Wang1,2∗\nYunbo Wang1†\nXin Jin2\nYang Li3\nWenjun Zeng2\nXiaokang Yang1\n1 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\n2 Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China\n3 School of Computer Science and Technology, East China Normal University, Shanghai, China\nhttps:\/\/qiwang067.github.io\/ls-imagine\nABSTRACT\nTraining visual reinforcement learning agents in a high-dimensional open world\npresents significant challenges. While various model-based methods have im-\nproved sample efficiency by learning interactive world models, these agents tend\nto be “short-sighted”, as they are typically trained on short snippets of imagined\nexperiences. We argue that the primary challenge in open-world decision-making\nis improving the exploration efficiency across a vast state space, especially for\ntasks that demand consideration of long-horizon payoffs. In this paper, we present\nLS-Imagine, which extends the imagination horizon within a limited number of\nstate transition steps, enabling the agent to explore behaviors that potentially lead\nto promising long-term feedback. The foundation of our approach is to build a long\nshort-term world model. To achieve this, we simulate goal-conditioned jumpy state\ntransitions and compute corresponding affordance maps by zooming in on specific\nareas within single images. This facilitates the integration of direct long-term\nvalues into behavior learning. Our method demonstrates significant improvements\nover state-of-the-art techniques in MineDojo.\n1\nINTRODUCTION\nOpen-world decision-making in the context of reinforcement learning (RL) involves the following\ncharacteristics: (i) The agent operates within an interactive environment that features a vast state\nspace; (ii) The learned policy presents a high degree of flexibility, allowing interaction with various\nobjects in the environment; (iii) The agent lacks full visibility of the internal states and physical\ndynamics of the external world, meaning that its perception of the environment (e.g., raw images)\ncarries substantial uncertainty. For example, Minecraft serves as a typical open-world game.\nBuilding upon recent progress in visual control, open-world decision-making aims to train agents to ap-\nproach human-level intelligence based solely on high-dimensional visual observations. However, this\npresents significant challenges. For example, in Minecraft tasks, existing methods like Voyager (Wang\net al., 2024a) employ specific Minecraft APIs as the high-level controller, which is incompatible with\nstandard visual control settings. While approaches such as PPO-with-MineCLIP (Fan et al., 2022) and\nDECKARD (Nottingham et al., 2023) perform low-level visual control, these model-free RL methods\nstruggle to grasp the underlying mechanics of the environment. This may result in high trial-and-error\ncosts, leading to inefficiencies in both exploration and sample usage. Although DreamerV3 (Hafner\net al., 2023) employs a model-based RL (MBRL) approach to improve sample efficiency, it is often\n“short-sighted” since the policy is optimized using short-term experiences—typically 15 time steps—\ngenerated by the world model. The absence of long-term guidance significantly hampers an effective\nexploration of the vast solution space of the open world.\nTo improve the behavior learning efficiency of MBRL, in this paper, we introduce a novel method\nnamed Long Short-Term Imagination (LS-Imagine). Our key approach involves enabling the world\nmodel to efficiently simulate the long-term effects of specific behaviors without the need for repeatedly\n∗Equal contribution.\n†Corresponding author: Yunbo Wang <yunbow@sjtu.edu.cn>.\n1\narXiv:2410.03618v2  [cs.LG]  4 Mar 2025\nPublished as a conference paper at ICLR 2025\nCut a tree\n🏆\nenvironment\nShort-Term Imagination \nLong-Term Imagination \nFigure 1: The general framework of LS-Imagine, an MBRL agent that operates solely on raw pixels.\nThe fundamental idea is to extend the imagination horizon within a limited number of state transition\nsteps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback.\nrolling out one-step predictions. As illustrated in Figure 1, once trained, the world model provides\nboth instant and jumpy state transitions1 along with corresponding (intrinsic) rewards, facilitating\npolicy optimization in a joint space of short- and long-term imaginations. This encourages the agent\nto explore behaviors that lead to promising long-term outcomes.\nThe foundation of LS-Imagine is to train a long short-term world model, which requires integrating\ntask-specific guidance into the representation learning phase based on off-policy experience replay.\nHowever, this creates a classic “chicken-and-egg” dilemma: without true data showing the agent has\nreached the goal, how can we effectively train the model to simulate jumpy transitions from current\nstates to pivotal future states that suggest a high likelihood of achieving that goal? To address this\nissue, we first continuously zoom in on individual images to simulate the consecutive video frames as\nthe agent approaches the goal. We then generate affordance maps2 by evaluating the relevance of the\npseudo video to task-specific goals presented in textual instructions, using the established MineCLIP\nreward model (Fan et al., 2022). Subsequently, we train specific branches of the world model to\ncapture both instant and jumpy state transitions, using pairs of image observations from adjacent time\nsteps as well as those across longer intervals. Finally, we optimize the agent’s policy based on a finite\nsequence of imagined latent states generated by the world model, integrating a more direct estimate\nof long-term values into decision-making.\nLet’s use the example in Figure 1 to further elaborate the novel aspects of the behavior learning\nprocess: After receiving the instruction “cut a tree”, the agent simulates near-future states based on\nthe current real observation. It initially performs several single-step rollouts until it identifies a point\nin time for a long-distance state jump that allows it to approach the tree. The agent then executes this\njump and optimizes its policy network to maximize the long-sight value function.\nWe evaluate our approach in the challenging open-world tasks from MineDojo (Fan et al., 2022).\nLS-Imagine demonstrates superior performance compared to existing visual RL methods.\nThe contributions of this work are summarized as follows:\n• We present a novel model-based RL method that captures both instant and jumpy state transitions\nand leverages them in behavior learning to improve exploration efficiency in the open world.\n• Our approach presents four concrete contributions: (i) a long short-term world model architecture,\n(ii) a method for generating affordance maps through image zoom-in, (iii) a novel form of intrinsic\nrewards based on the affordance map, and (iv) an improved behavior learning method that integrates\nlong-term values and operates on a mixed long short-term imagination pathway.\n2\nPROBLEM FORMULATION AND NOTATIONS\nWe solve visual reinforcement learning as a partially observable Markov decision process (POMDP),\nusing MineDojo as the test bench. Specifically, our method manipulates low-dimensional control\nsignals at while receiving only sequential high-dimensional visual observations o<t and episodic\nsparse rewards renv, without access to the internal APIs of the open-world games. In comparison, as\n1As shown in Figure 1, a jumpy transition allows the agent to bypass intermediate states and directly simulate\na task-relevant future state st+H in one step. This process occurs exclusively during world model imagination.\n2Affordance maps highlight regions within an observation that are potentially relevant to the task (Qi et al.,\n2020; Wang et al., 2022).\n2\nPublished as a conference paper at ICLR 2025\nTable 1: Experimental setups of the Minecraft AI agents. IL is short for imitation learning.\nModel\nController\nObservation\nVideo Demos\nDECKARD (2023)\nRL\nPixels & Inventory\n✓\nAuto MC-Reward (2024a)\nIL + RL\nPixels & GPS\n✗\nVoyager (2024a)\nGPT-4\nMinecraft simulation & Error trace\n✗\nDEPS (2023)\nIL\nPixels & Yaw\/pitch angle & GPS & Voxel\n✗\nSTEVE-1 (2023)\nGenerative model\nPixels\n✗\nVPT (2022)\nIL + RL\nPixels\n✓\nDreamerV3 (2023)\nRL\nPixels\n✗\nLS-Imagine\nRL\nPixels\n✗\nshown in Table 1, existing Minecraft agents present notable distinctions in learning paradigms (i.e.,\ncontroller), observation data, and the use of expert demonstrations.\nThe world model presented in this paper consists of two main components: a short-term transition\nbranch and a long-term imagination branch. As a result, it employs a complex notation system. We\nnow introduce the key notations that will be frequently used throughout the paper:\n• Mt represents the affordance map.\n• ct denotes the episode continuation flag.\n• jt is the jumping flag that triggers jumpy state imaginations.\n• ∆t represents the number of environmental steps between the jumpy transitions.\n• Gt is the cumulative reward over ∆t.\nWe use (o′\nt, a′\nt, M′\nt, r′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt) to represent the simulated environment data that are used to\ntrain the long-term imagination branch of the world model. The policy is learned on trajectories of\nmixed long- and short-term imaginations {(ˆst, ˆat, ˆrt, ˆct, ˆjt, ˆ∆t, ˆGt)}, where ˆst represents the latent\nstate, and the variables predicted by the model are indicated using the superscript (ˆ).\n3\nMETHOD\n3.1\nOVERVIEW OF LS-IMAGINE\nIn this section, we present the details of LS-Imagine, which involves the following algorithm steps,\nincluding world model learning, behavior learning, and environment interaction:\n1. Affordance map computation (Sec. 3.2.1): We employ a sliding bounding box to scan individual\nimages and execute continuous zoom-ins inside the bounding box, simulating consecutive video\nframes that correspond to long-distance state transitions. We then create affordance maps by\nassessing the relevance of the fake video clips to task-specific goals expressed in text using the\nestablished MineCLIP reward model (Fan et al., 2022).\n2. Rapid affordance map generation (Sec. 3.2.2): Given that affordance maps will be frequently\nused in subsequent Step 5 to evaluate the necessities for jumpy state transitions, we train a U-Net\nmodule to approximate the affordance maps annotated in Step 1 for the sake of efficiency.\n3. World model training (Sec. 3.3): We train the world model to capture short- and long-term state\ntransitions, using replay data with high responses from the affordance map. Each trajectory from\nthe buffer includes pairs of samples from both adjacent time steps and long-distance intervals.\n4. Behavior learning (Sec. 3.4): We perform an actor-critic algorithm to optimize the agent’s policy\nbased on a finite sequence of long short-term imaginations generated by the world model.\n5. Data update: We apply the agent to interact with the environment and gather new data. Next,\nwe leverage the generated affordance map to efficiently filter sample pairs suitable for long-term\nmodeling, incorporating both short- and long-term sample pairs to update the replay buffer.\n6. Iterate Steps 3–5.\nBelow, we discuss each training step in detail. The full algorithm can be found in Appendix C.4.\n3.2\nAFFORDANCE MAP AND INTRINSIC REWARD\nWe generate affordance maps using visual observations and textual task definitions to improve the\nsample efficiency of model-based reinforcement learning in open-world tasks. The core idea is to di-\nrect the agent’s attention to task-relevant areas of the visual observation, leading to higher exploration\n3\nPublished as a conference paper at ICLR 2025\n...\nMineCLIP\nCut a tree\n...\n16 frames of video \nto represent the\nsimulated exploration\nCorr = 0.60\nCorr = 0.90\nCorr = 0.15\n Fusion \n(a) Affordance map calculation\nMultimodal U-Net\nZoom\nin\nMultimodal U-Net\nMultimodal U-Net\nenv\nagent\n(b) Rapid affordance map generation\nFigure 2: The two steps for on-the-fly affordance map estimation: (a) Simulate exploration via image\nzoom-in and calculate the task-correlation scores of the virtual explorations using MineCLIP. (b)\nLearn to generate affordance maps more efficiently using a multimodal U-Net.\nefficiency. Let Mot,I(w, h) be the affordance map that represents the potential exploration value at\npixel position (w, h) on the image observation ot, given textual instruction I (e.g., “cut a tree”). The\naffordance map highlights the relevance between regions of the observation and the task description,\nserving as a spatial prior that effectively directs the agent’s exploration toward areas of interest.\n3.2.1\nAFFORDANCE MAP COMPUTATION VIA VIRTUAL EXPLORATION\nTo create the affordance map, as shown in Figure 2(a), we simulate and evaluate the agent’s exploration\nwithout relying on real successful trajectories. Concretely, we first adopt a random agent to interact\nwith task-specific environments for data collection. Starting with the agent’s observation ot at time\nstep t, we use a sliding bounding box with dimensions scaled to 15% of the observation’s width and\nheight to traverse the entire observation from left to right and top to bottom. The sliding bounding\nbox moves horizontally and vertically in 9 steps, respectively, covering every potential region in\nboth dimensions. For each position on the sliding bounding box of the observation ot, we crop 16\nimages from ot. These cropped images narrow the field of view to focus on the region and are resized\nback to the original image dimensions. These resized images are denoted as xk\nt (where 0 ≤k < 16).\nThe ordered set Xt = [xk\nt | k = 0, 1, . . . , 15] represents a sequence of 16 frames simulating the\nvisual transition as the agent moves towards the position specified by the current sliding bounding\nbox. Subsequently, we employ the MineCLIP model3 to calculate the correlation between the Xt of\nimages, simulating the virtual exploration process, and the task description I. In this way, we quantify\nthe affordance value of the sliding bounding box, indicating the potential exploration interest of the\narea. After calculating the correlation score for each sliding bounding box, we fuse these values to\nobtain a smooth affordance map Mot,I. For pixels that are covered by multiple sliding bounding\nboxes due to overlapping regions, the integrated affordance value is obtained by averaging the values\nfrom all the overlapping windows.\n3.2.2\nMULTIMODAL U-NET FOR RAPID AFFORDANCE MAP GENERATION\nThe annotation of affordance maps, as previously described, involves extensive window traversal and\ncomputations for each window position using a pre-trained video-text alignment model. This method\nis computationally demanding and time-consuming, making real-time applications challenging. To\naddress this issue, we first use a random agent to interact with the environment for data collection.\nNext, we annotate the affordance maps for the collected images using the aforementioned method\nbased on virtual exploration. We gather a dataset of tuples (ot, I, Mot,I) and use it to train a\nmultimodal U-Net based on Swin-Unet (Cao et al., 2022). To handle multimodal inputs, we extract\ntext features from the language instructions and image features from the downsampling process of\nSwin-Unet, and fuse them with multi-head attention. We present architecture details in Figure 9 in the\nappendix. In this way, with the pretrained multimodal U-Net, we can efficiently generate affordance\nmaps at each time step using visual observations and language instructions.\n3MineCLIP (Fan et al., 2022) pretrains a video-language representation using Minecraft videos, enabling it\nto compute the correlation between a text string and a 16-frame video segment.\n4\nPublished as a conference paper at ICLR 2025\n3.2.3\nAFFORDANCE-DRIVEN INTRINSIC REWARD\nTo leverage the task-relevant prior knowledge presented by the affordance map for efficient exploration\nin the open world, we introduce the following intrinsic reward function:\nrintr\nt\n=\n1\nWH\nW\nX\nw=1\nH\nX\nh=1\nMot,I(w, h) · G(w, h),\n(1)\nwhere W and H denote the width and height of the visual observation. G represents a Gaussian\nmatrix with dimensions matching those of the affordance map. It corresponds to a 2D Gaussian\ndistribution, with its peak located at the center of the affordance map. The values in the matrix are\ndetermined by standard deviations (σx, σy), while the mean is uniformly set to 1 across the entire\nmatrix. We present visualizations of G and conduct hyperparameter analyses on (σx, σy) in Appendix\nD.6. The intuition behind this design is to encourage the agent to move toward the target.\nOverall, the agent receives a composite reward consisting of the episodic sparse reward from the\nenvironment, the reward from MineCLIP (Fan et al., 2022), and the intrinsic reward from the\naffordance map: rt = renv\nt\n+ rMineCLIP\nt\n+ αrintr\nt , where α is a hyperparameter. In contrast to the\nMineCLIP reward, which relies on the agent’s past performance, our affordance-driven intrinsic\nreward emphasizes long-term values derived from future virtual exploration. It encourages the agent\nto adjust the policy to pursue task-related targets when they appear in its view, ensuring these targets\nare centrally positioned in future visual observations to maximize this reward function.\n3.3\nLONG SHORT-TERM WORLD MODEL\n3.3.1\nLEARNING JUMPING FLAGS\nIn LS-Imagine, the world model is customized for long-term and short-term state transitions. It\ndecides which type of transition to adopt based on the current state and predicts the next state\nwith the selected transition branch. To facilitate the switch between long-term and short-term state\ntransitions, we introduce a jumping flag jt, which indicates whether a jumpy transition or long-term\nstate transition, should be adopted at time step t. When a distant task-related target appears in the\nagent’s observation, which can be reflected by a higher kurtosis in the affordance map, a jumpy\ntransition allows the agent to imagine the future state of approaching the target. To this end, we\ndefine relative kurtosis Kr which measures whether there are significantly higher target areas than\nthe surrounding areas in the affordance map, and absolute kurtosis Ka represents the confidence level\nof target presence in that area. Formally,\nKr =\n1\nWH\nW\nX\nw=1\nH\nX\nh=1\n\"\u0012Mo,I(w, h) −mean(Mo,I)\nstd(Mo,I)\n\u00134#\n,\nKa = max(Mo,I) −mean(Mo,I).\n(2)\nTo normalize the relative kurtosis, we apply the sigmoid function to it, and then multiply it by the\nabsolute kurtosis to calculate the jumping probability:\nPjump = sigmoid(Kr) × Ka.\n(3)\nThe jumping probability measures the confidence in the presence of task-relevant targets far from the\nagent in the visual observation. To determine whether to employ long-term state transition, we use a\ndynamic threshold, which is the mean of the collected jumping probabilities at each time step, plus\none standard deviation. For a detailed explanation, please refer to C.1. If Pjump exceeds this threshold,\nthe jump flag jt is True and the agent switches to jumpy state transitions in the imagination phase.\n3.3.2\nLEARNING JUMPY STATE TRANSITIONS\nIn LS-Imagine, the state transition model includes both short-term and long-term branches. As\nshown in Figure 3 (a), the short-term transition model integrates the previous deterministic recurrent\nstate ht−1, stochastic state zt−1, and action at−1 to adopt the single-step transition. In contrast, the\nlong-term branch simulates jumpy state transitions toward the target. It is important to clarify that the\nindex t does not denote the time step during real environmental interactions but instead represents the\npositional order of states within the imagination sequence. The overall world model of LS-Imagine is\n5\nPublished as a conference paper at ICLR 2025\nenc\ndec\nenc\ndec\nShort-Term Branch\nCritic\nActor\nCritic\nCritic\nActor\nCritic\nShort-Term Imagination\nLong-Term Imagination\nTransition model\nReward predictor\nJump predictor\nInterval predictor\nLong-Term Branch\nFigure 3: The overall architecture of the world model and the behavior learning process.\nprimarily based on DreamerV3 (Hafner et al., 2023), with novel components specifically designed to\ncapture jumpy state transitions:\nShort-term transition model:\nht = fϕ(ht−1, zt−1, at−1)\nLong-term transition model:\nh′\nt = fϕ(ht−1, zt−1)\nEncoder:\nzt ∼qϕ(zt | ht, ot, Mt)\nDynamics predictor:\nˆzt ∼pϕ(ˆzt | ht)\nReward predictor:\nˆrt, ˆct ∼pϕ(ˆrt, ˆct | ht, zt)\nDecoder:\nˆot, ˆ\nMt ∼pϕ(ˆot, ˆ\nMt | ht, zt)\nJump predictor:\nˆjt ∼pϕ(ˆjt | ht, zt)\nInterval predictor:\nˆ∆′\nt, ˆG′\nt ∼pϕ( ˆ∆′\nt, ˆG′\nt | ht−1, zt−1, h′\nt, z′\nt)\n.\n(4)\nAt time step t, we feed the recurrent state ht, the observation ot, and the affordance map Mt into the\nencoder to obtain posterior state zt. We also use the affordance map as an input of the encoder, which\nserves as the goal-conditioned prior guidance to the agent. Notably, the prediction of prior state ˆzt\ndoes not involve the current observation or affordance map, relying solely on historical information.\nWe use (ht, zt) to reconstruct the visual observation ˆot and the affordance map ˆ\nMt, and predict the\nreward ˆrt, episode continuation flag ˆct, and jumping flag ˆjt. For long-term state transitions, we use an\ninterval predictor to estimate the expected number of interaction steps ˆ∆′\nt required to transition from\nthe pre-jump state (ht−1, zt−1) to the post-jump state (h′\nt, z′\nt), along with the expected cumulative\nreward ˆG′\nt that the agent may receive during this time interval. We detail the approach to annotate ∆′\nt\nand G′\nt using the real interaction data in Appendix C.1.\nWe collect short-term tuples Dt = (ot, at, Mt, rt, ct, jt, ∆t, Gt) from each interaction with the\nenvironment using the current policy. When observing jt = 1, we additionally construct long-term\ntuples D′\nt+1 = (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1) based on Dt. More details for this\nprocess can be found in Appendix C.1. During representation learning, we sample short-term tuples\n{Dt}T\nt=1 and the long-term tuples following jumpy transitions {D′\nt+1}t∈T from the replay buffer\nB, where T denotes the set of time steps at which long-term state transitions are required. The loss\nfunctions for each component of the short-term and long-term world model branch are as follows:\nShort-term branch:\n\n\n\n\n\n\n\n\n\nLdyn .= max (1, KL [sg (qϕ (zt | ht, ot, Mt)) ∥pϕ (zt | ht)])\nLenc .= max (1, KL [qϕ (zt | ht, ot, Mt) ∥sg (pϕ (zt | ht))])\nLdec .= −ln pϕ (ot, Mt | ht, zt)\nLpred .= −ln pϕ (rt, ct | ht, zt) −ln pϕ (jt | ht, zt)\n.\n(5)\n6\nPublished as a conference paper at ICLR 2025\nLong-term branch:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL′\ndyn\n.= max (1, KL [sg (qϕ (z′\nt | h′\nt, o′\nt, M′\nt)) ∥pϕ (z′\nt | h′\nt)])\nL′\nenc\n.= max (1, KL [qϕ (z′\nt | h′\nt, o′\nt, M′\nt) ∥sg (pϕ (z′\nt | h′\nt))])\nL′\ndec\n.= −ln pϕ (o′\nt, M′\nt | h′\nt, z′\nt)\nL′\npred\n.= −ln pϕ(r′\nt, c′\nt | h′\nt, z′\nt) −ln pϕ(j′\nt | h′\nt, z′\nt)\nL′\nint\n.= −ln pϕ (∆′\nt, G′\nt | ht−1, zt−1, h′\nt, z′\nt)\n.\n(6)\nWe can optimize the world model Wϕ by minimizing over replay buffer B:\nL .= E\nhP\n{Dt}T\nt=1 (βdynLdyn + βencLenc + βpred (Ldec + Lpred)) +\nβlong\nP\n{D′\nt+1}t∈T\n\u0000βdynL′\ndyn + βencL′\nenc + βpred\n\u0000L′\ndec + L′\npred + L′\nint\n\u0001\u0001 i\n.\n(7)\n3.4\nBEHAVIOR LEARNING OVER MIXED LONG SHORT-TERM IMAGINATIONS\nAs shown in Figure 3 (b), LS-Imagine employs an actor-critic algorithm to learn behavior from the\nlatent state sequences predicted by the world model. The goal of the actor is to optimize the policy\nto maximize the discounted cumulative reward Rt, while the role of the critic is to estimate the\ndiscounted cumulative rewards using the current policy for each state ˆst .= {ht, ˆzt}:\nActor:\nˆat ∼πθ (ˆat | ˆst) ,\nCritic:\nvψ\n\u0010\nˆRt | ˆst\n\u0011\n.\n(8)\nStarting from the initial state encoded from the sampled observation and the affordance map, we\ndynamically select either the long-term transition model or the short-term transition model to predict\nsubsequent states based on the jumping flag ˆjt. For the long short-term imagination sequence\n{(ˆst, ˆat)}L\nt=1 with an imagination horizon of L, we predict reward sequence ˆr1:L and the continuation\nflag sequence ˆc1:L through the reward predictor. Similar to Eq. (4), the index t does not represent the\ntime step in the environment, but rather the positional order of the states in the imagination sequence.\nSpecifically, starting from state ˆst, any subsequent state obtained via either a short-term transition or\na long-term transition is indexed sequentially as t + 1.\nFor jumpy states predicted by long-term imagination, the interval predictor estimates (i) the number of\nsteps ˆ∆t from ˆst−1 to ˆst and (ii) the potential discounted cumulative reward ˆGt over the time interval\nof ˆ∆t. Otherwise, for states obtained via short-term imagination, which correspond to single-step\ntransitions in the environment, we set ˆ∆t = 1 and ˆGt = ˆrt. Consequently, within one imagination\nepisode, we obtain a sequence of step intervals ˆ∆2:L and a sequence of predicted rewards ˆG2:L\nbetween consecutive imagination states.\nWe employ a modified bootstrapped λ-returns that considers both long-term and short-term imagina-\ntions to calculate the discounted cumulative rewards for each state:\nRλ\nt\n.=\n\u001a\nˆct{ ˆGt+1 + γ ˆ∆t+1 \u0002\n(1 −λ)vψ(ˆst+1) + λRλ\nt+1\n\u0003\n}\nif t < L\nvψ(ˆsL)\nif t = L .\n(9)\nThe critic uses the maximum likelihood loss to predict the distribution of the return estimates Rλ\nt :\nL(ψ) .= −\nL\nX\nt=1\nln pψ\n\u0000Rλ\nt | ˆst\n\u0001\n.\n(10)\nFollowing DreamerV3 (Hafner et al., 2023), we train the actor to maximize the return estimates\nRλ\nt . Notably, since long-term imagination does not involve actions, we do not optimize the actor at\ntime steps when jumpy state transitions are adopted. Therefore, unlike DreamerV3, we apply an\nadditional factor of (1 −ˆjt) to ignore updates at long-term imagination steps:\nL(θ) .= −\nL\nX\nt=1\nsg\n\u0014\u0010\n1 −ˆjt\n\u0011 Rλ\nt −vψ(ˆst)\nmax(1, S)\n\u0015\nlog πθ(ˆat | ˆst) + η H [πθ(ˆat | ˆst)] .\n(11)\n7\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n(a) Harvest log in plains\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n(b) Harvest water with bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess Rate\n(c) Harvest sand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\nSuccess Rate\n(d) Shear sheep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.1\n0.2\n0.3\n0.4\nSuccess Rate\n(e) Mine iron ore\nlog\nwater\nsand\nwool iron_ore\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMineCLIP Score\n(f) Comparison in MineCLIP reward\nVPT\nSTEVE-1\nPTGM\nDirector\nDreamerV3\nLS-Imagine\nFigure 4: Comparison of LS-Imagine against strong Minecraft agents, including DreamerV3 (Hafner\net al., 2023), VPT (Baker et al., 2022), STEVE-1 (Lifshitz et al., 2023), PTGM (Yuan et al., 2024),\nand Director (Hafner et al., 2022). We present the numerical results in Table 3 in the appendix.\n4\nEXPERIMENTS\nWe explore LS-Imagine on the challenging MineDojo (Fan et al., 2022) benchmark on top of the\npopular Minecraft game, which is a comprehensive simulation platform with various open-ended\ntasks. We use 5 tasks, i.e., harvest log in plains, harvest water with bucket, harvest sand, shear sheep,\nand mine iron ore. These tasks demand numerous steps to complete and present significant challenges\nfor agent learning. We adopt a binary reward that indicates whether the task was completed, along\nwith the MineCLIP reward (Fan et al., 2022). Further details of the environmental setups are provided\nin Appendix A. Besides, we introduce the compared models in Appendix B.\nImplementation details.\nWe conduct our experiments on the MineDojo environment, where both\nvisual observation and corresponding affordance maps are resized to 64 × 64 pixels. To generate\naccurate affordance maps, we collect 2,000 images from the environment using a random agent\nunder the current task instruction and generate a discrete set of (ot, I, Mot,I), which are then used to\nfinetune the multimodal U-Net for 200 epochs. For tasks in the MineDojo benchmark, we train the\nagent for 1 × 106 environment steps. Each training of LS-Imagine takes approximately 23 GB of\nVRAM and requires around 1.7 days to complete on a single RTX 3090 GPU.\n4.1\nMAIN COMPARISON\nWe evaluate all the Minecraft agents in terms of success rate shown in Figure 4 and per-episode\nsteps shown in Figure 5. We find that LS-Imagine significantly outperforms the compared models,\nparticularly in scenarios where sparse targets are distributed in the task. In Figure 4 (f), we showcase\nthe MineCLIP values achieved by LS-Imagine and DreamerV3. Specifically, a sliding window of\nlength 16 is used to compute the local MineCLIP values for each segment. The mean value is then\ncalculated from all sliding windows. We can see that agents trained using our method achieve higher\nMineCLIP values within a single episode compared to DreamerV3. This suggests that LS-Imagine\nfacilitates quicker detection of task-relevant visual targets in open-world environments.\nAdditionally, we present qualitative results in Figure 6(a). In the top row, we decode the latent\nstates before and after the jumpy state transitions back to the pixel space. To better understand how\naffordance maps facilitate the jumpy state transitions and whether they can provide effective goal-\nconditioned guidance, the bottom rows visualize the affordance maps reconstructed from the latent\nstates. These visualizations demonstrate that the proposed world model can adaptively determine\nwhen to utilize long-term imagination based on the current visual observation. Furthermore, the\n8\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n400\n600\n800\n1000\nSteps per episode\n(a) Harvest log in plains\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n400\n600\n800\n1000\n(b) Harvest water with bucket\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n500\n600\n700\n800\n900\n1000\n(c) Harvest sand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n500\n600\n700\n800\n900\n1000\n(d) Shear sheep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps 1e6\n1500\n1600\n1700\n1800\n1900\n2000\n(e) Mine iron ore\nVPT\nSTEVE-1\nPTGM\nDirector\nDreamerV3\nLS-Imagine\nFigure 5: The number of steps per episode for task completion.\nLong-term imagination\nShort-term imagination\n(a) Visualization of long short-term imaginations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLS-Imagine (series)\nLS-Imagine (parallel)\n(b) Series vs. parallel connections\nFigure 6: (a) Visualization of long short-term imaginations and (b) a further discussion on possible\narchitecture designs of Series and Parallel connections of these two imagination pathways.\ngenerated affordance maps align effectively with areas that are highly relevant to the final goal,\nthereby enabling the agent to perform more efficient policy exploration.\n4.2\nMODEL ANALYSES\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nLS-Imagine\nw\/o long-term imagination\nw\/o intrinsic reward\nFigure 7: Ablation study results.\nAblation studies.\nWe conduct the ablation studies to\nvalidate the effect of the affordance-driven intrinsic re-\nward and long short-term imagination. Figure 7 presents\ncorresponding results in the challenging MineDojo tasks.\nAs shown by the blue curve, removing the long-term\nimagination of LS-Imagine leads to a performance de-\ncline, which indicates the necessity of introducing long-\nterm imagination and switching between it and short-\nterm imagination adaptively. For the model represented\nby the green curve, we do not employ affordance-driven\nintrinsic reward. It shows that the affordance-driven in-\ntrinsic reward also plays an important role during the\nearly training stage of agents. Additionally, unlike the\nMineCLIP reward being calculated based on a series of\nstates, the affordance-driven intrinsic reward relies solely on a single independent state. This approach\nenables a more accurate estimation of the reward for the post-jumpy-transition state.\nAlternative pathways of mixed imaginations.\nIt is worth highlighting that the long short-term\nimagination is implemented sequentially. In Figure 10(a) in the appendix, we provide a visualization\nillustrating how the agent sequentially performs short-term and long-term imaginations within a\nsingle sequence. Alternatively, as illustrated in Figure 10(b), we could structure long- and short-term\nimagination pathways in parallel. Specifically, we begin by applying short-term imagination within a\nsingle sequence. For each predicted state, we examine the jumping flag: If ˆjt = 1, we initiate a new\nimagination sequence starting from the post-jump state, which is predicted by the long-term transition\nmodel and the dynamics predictor. In other words, whenever a long-term state jump occurs, the world\nmodel generates a new sequence from the post-jump state, while the intermediate state transitions\nwithin the sequence are governed exclusively by short-term dynamics. Importantly, we optimize the\nactor independently for each sequence, ensuring that there is no gradient or value transfer between\nsequences. To evaluate the advantages of using sequential long short-term imagination, we conduct\nan experimental comparison between LS-Imagine (series) and LS-Imagine (parallel). Figure 6(b)\nshows that the LS-Imagine (series) outperforms LS-Imagine (parallel) by large margins. This implies\n9\nPublished as a conference paper at ICLR 2025\nthat the parallel imagination sequences are independent of one another, meaning that the sequence\nstarting with a post-jumping state does not guide the prior-jumping transitions.\nIn the appendix, we further include (i) experiments on the long-horizon “Tech Tree” task, (ii) analyses\nof the long-term imagination frequency and corresponding state jumping intervals ˆ∆t predicted by\nthe model, and (iii) visualization of affordance maps with occluded target objects.\n5\nRELATED WORK\nVisual MBRL.\nRecently, learning control policies from images, i.e., visual RL has been used widely,\nwhereas previous RL algorithms learn policies from low-dimensional states. Existing approaches\ncan be grouped by the use of model-free RL methods (Laskin et al., 2020; Schwarzer et al., 2021;\nStooke et al., 2021; Xiao et al., 2022; Parisi et al., 2022; Yarats et al., 2022; Zheng et al., 2023)\nor model-based RL methods (Hafner et al., 2019; 2020; 2021; Seo et al., 2022; Pan et al., 2022;\nZhang et al., 2023a; Mazzaglia et al., 2023; Micheli et al., 2023; Zhang et al., 2023b; Ying et al.,\n2023; Seo et al., 2023; Alonso et al., 2024; Hansen et al., 2024; Wang et al., 2024b). The following\nmethods specifically enhance the modeling of long-term dynamics in visual MBRL. Lee et al. (2024b)\nproposed the prediction of temporally smoothed rewards to address long-horizon sparse-reward\ntasks. R2I (Samsami et al., 2024) improves long-term memory and long-horizon credit assignment\nin MBRL. Unlike existing methods, our work presents a long short-term world model architecture\nspecifically designed for visual control in open-world environments.\nAffordance maps for robot learning.\nOur work is also related to the affordance map for robot\nlearning (Mo et al., 2021; Jiang et al., 2021; Yarats et al., 2021; Mo et al., 2022; Geng et al., 2022;\nXu et al., 2022a; Wang et al., 2022; Wu et al., 2022; Ha & Song, 2022; Xu et al., 2022b; Cheng\net al., 2024; Lee et al., 2024a; Li et al., 2024b). Where2Explore (Ning et al., 2023) introduces a\ncross-category few-shot affordance learning framework that leverages the similarities in geometries\nacross different categories. DualAfford (Zhao et al., 2023) learns collaborative actionable affordance\nfor dual-gripper manipulation tasks over various 3D shapes. VoxPoser (Huang et al., 2023) unleashes\nthe power of large language models and vision-language models for extracting affordances and\nconstraints of real-world manipulation tasks, which are grounded in 3D perceptual space. VRB (Bahl\net al., 2023) trains a visual affordance model with videos of human interactions and deploys the\nmodel in real-world robotic tasks directly. Qi et al. (2020) adopted a spatial affordance map that is\ntrained by interacting with the environment for navigation. However, our approach distinguishes\nitself by employing visual observation to generate affordance maps as guidance to mitigate the low\nexploration efficiency in open-world environments.\nHierarchical methods.\nLike our approach, Director (Hafner et al., 2022) learns hierarchical\nbehaviors in the latent space, which adopts high-level policy (manager) to produce latent goals to\nguide low-level policy (worker). Dr. Strategy (Hamed et al., 2024) proposes strategic dreaming\nwith latent landmarks to learn a highway policy that enables the agent to move to a landmark in the\ndream. Gumbsch et al. (2024) presented a hierarchy of world models, which perform high-level and\nlow-level prediction adaptively, and the high-level predictions depend on the low-level predictions.\nOur method distinguishes itself by generating affordance maps through image zoom-in to encourage\nthe agent to explicitly execute long-term imagination in the world model.\n6\nCONCLUSIONS AND LIMITATIONS\nIn this paper, we presented a novel approach to overcoming the challenges of training visual rein-\nforcement learning agents in high-dimensional open worlds. By extending the imagination horizon\nand leveraging a long short-term world model, our method facilitates efficient off-policy exploration\nacross expansive state spaces. The incorporation of goal-conditioned jumpy state transitions and\naffordance maps allows agents to better grasp long-term value, enhancing their decision-making\nabilities. Our results demonstrate substantial improvements over existing state-of-the-art techniques\nin MineDojo, highlighting the potential of our approach for open-world reinforcement learning and\ninspiring future research in this domain.\nA limitation of LS-Imagine is the computational overhead it introduces. Additionally, its effectiveness\nhas only been validated in 3D navigation environments with embodied agents. We aim to enhance\nthe generalization of our approach across a wider range of tasks.\n10\nPublished as a conference paper at ICLR 2025\nETHICS STATEMENT\nIn this work, we are committed to upholding ethical research practices. This work does not involve\nhuman subjects, personal data, or sensitive information. All environments and datasets used are\nsynthetic and publicly available. We recognize the potential for reinforcement learning models to\nbe misused, particularly in decision-making scenarios where unintended outcomes could arise. To\nmitigate these risks, we emphasize responsible deployment and encourage careful consideration of\nthe broader impact of such systems, restricting the use of our work strictly to research purposes.\nREPRODUCIBILITY STATEMENT\nWe prioritize the reproducibility of our work. All results can be reproduced on publicly available\nRL environments by following the experimental details presented in Sec. 4 and Appendix D.6. We\nprovide the source code at https:\/\/github.com\/qiwang067\/LS-Imagine.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science Foundation of China (Grants 62250062,\n62302246), the Smart Grid National Science and Technology Major Project (Grant 2024ZD0801200),\nthe Shanghai Municipal Science and Technology Major Project (Grant 2021SHZDZX0102), the\nFundamental Research Funds for the Central Universities, and the CCF-Tencent Rhino-Bird Open\nResearch Fund. Additional support was provided by the Natural Science Foundation of Zhejiang\nProvince, China (Grant LQ23F010008), the High Performance Computing Center at Eastern Institute\nof Technology, Ningbo, and Ningbo Institute of Digital Twin.\nREFERENCES\nEloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and\nFrançois Fleuret. Diffusion for world modeling: Visual details matter in atari. In NeurIPS, 2024.\nShikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from\nhuman videos as a versatile representation for robotics. In CVPR, pp. 13778–13790, 2023.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In NeurIPS, 2022.\nHu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang.\nSwin-unet: Unet-like pure transformer for medical image segmentation. In ECCVW, 2022.\nGuangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin Sun, and Jiang Bian. Empow-\nering large language models on robotic manipulation with affordance prompting. arXiv preprint\narXiv:2404.11027, 2024.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In NeurIPS, volume 35, pp. 18343–18362, 2022.\nYiran Geng, Boshi An, Haoran Geng, Yuanpei Chen, Yaodong Yang, and Hao Dong. End-to-end\naffordance learning for robotic manipulation. arXiv preprint arXiv:2209.12941, 2022.\nChristian Gumbsch, Noor Sajid, Georg Martius, and Martin V Butz. Learning hierarchical world\nmodels with adaptive temporal abstractions from discrete latent dynamics. In ICLR, 2024.\nHuy Ha and Shuran Song. Flingbot: The unreasonable effectiveness of dynamic manipulation for\ncloth unfolding. In CoRL, pp. 24–33, 2022.\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James\nDavidson. Learning latent dynamics for planning from pixels. In ICML, pp. 2555–2565, 2019.\n11\nPublished as a conference paper at ICLR 2025\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\nbehaviors by latent imagination. In ICLR, 2020.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete\nworld models. In ICLR, 2021.\nDanijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter Abbeel. Deep hierarchical planning from\npixels. In NeurIPS, 2022.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nHany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, and Sungjin Ahn. Dr. strategy: Model-based\ngeneralist agents with strategic dreaming. In ICML, 2024.\nNicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous\ncontrol. In ICLR, 2024.\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models. In CoRL, 2023.\nZhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, and Yuke Zhu. Synergies between affordance\nand geometry: 6-dof grasp detection via implicit representations. Robotics: science and systems,\n2021.\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations\nfor reinforcement learning. In ICML, pp. 5639–5650, 2020.\nOlivia Y Lee, Annie Xie, Kuan Fang, Karl Pertsch, and Chelsea Finn. Affordance-guided reinforce-\nment learning via visual prompting. arXiv preprint arXiv:2407.10341, 2024a.\nVint Lee, Pieter Abbeel, and Youngwoon Lee. Dreamsmooth: Improving model-based reinforcement\nlearning via reward smoothing. In ICLR, 2024b.\nHao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li,\nLewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language\nmodels for minecraft. In CVPR, pp. 16426–16435, 2024a.\nXiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming\nLiu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric\nrobotic manipulation. In CVPR, pp. 18061–18070, 2024b.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. In NeurIPS, 2023.\nPietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, and Sai Rajeswar. Choreographer:\nLearning and adapting skills in imagination. In ICLR, 2023.\nVincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models.\nIn ICLR, 2023.\nKaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani.\nWhere2act: From pixels to actions for articulated 3d objects. In ICCV, pp. 6813–6823, 2021.\nKaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and Leonidas Guibas. O2o-afford: Annotation-free\nlarge-scale object-object affordance learning. In CoRL, pp. 1666–1677, 2022.\nChuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo, and Hao Dong. Where2explore: Few-shot\naffordance learning for unseen novel categories of articulated objects. NeurIPS, 2023.\nKolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer\nSingh, and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision making\nusing language guided world modelling. In ICML, pp. 26311–26325, 2023.\n12\nPublished as a conference paper at ICLR 2025\nMinting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging\nnoncontrollable visual dynamics in world models. In NeurIPS, volume 35, pp. 23178–23191,\n2022.\nSimone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising\neffectiveness of pre-trained vision models for control. In ICML, pp. 17359–17371, 2022.\nWilliam Qi, Ravi Teja Mullapudi, Saurabh Gupta, and Deva Ramanan. Learning to move with\naffordance maps. In ICLR, 2020.\nMohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar. Mastering\nmemory tasks with world models. In ICLR, 2024.\nMax Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R De-\nvon Hjelm, Philip Bachman, and Aaron C Courville. Pretraining representations for data-efficient\nreinforcement learning. In NeurIPS, volume 34, pp. 12686–12699, 2021.\nYounggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel. Reinforcement learning with\naction-free pre-training from videos. In ICML, pp. 19561–19579, 2022.\nYounggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view\nmasked world models for visual robotic manipulation. In ICML, pp. 30613–30632, 2023.\nAdam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning\nfrom reinforcement learning. In ICML, pp. 9870–9879, 2021.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nTMLR, 2024a.\nQi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, and Xiaokang Yang. Making offline rl\nonline: Collaborative world models for offline visual reinforcement learning. In NeurIPS, 2024b.\nYian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas J Guibas, and Hao Dong.\nAdaafford: Learning to adapt manipulation affordance for 3d articulated objects via few-shot\ninteractions. In ECCV, pp. 90–107, 2022.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with large language models enables open-world\nmulti-task agents. In NeurIPS, 2023.\nRuihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin\nChen, Leonidas Guibas, and Hao Dong. Vat-mart: Learning visual action trajectory proposals for\nmanipulating 3d articulated objects. ICLR, 2022.\nTete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for\nmotor control. arXiv preprint arXiv:2203.06173, 2022.\nChao Xu, Yixin Chen, He Wang, Song-Chun Zhu, Yixin Zhu, and Siyuan Huang. Partafford:\nPart-level affordance discovery from 3d objects. arXiv preprint arXiv:2202.13519, 2022a.\nZhenjia Xu, Zhanpeng He, and Shuran Song. Universal manipulation policy network for articulated\nobjects. IEEE robotics and automation letters, 7(2):2447–2454, 2022b.\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing\ndeep reinforcement learning from pixels. In ICLR, 2021.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control:\nImproved data-augmented reinforcement learning. In ICLR, 2022.\nChengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Songming Liu, Jialian Li, Dong Yan,\nand Jun Zhu. Reward informed dreamer for task generalization in reinforcement learning. arXiv\npreprint arXiv:2303.05092, 2023.\n13\nPublished as a conference paper at ICLR 2025\nHaoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu. Pre-training goal-based models for\nsample-efficient reinforcement learning. In ICLR, 2024.\nWeipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang. Storm: Efficient stochastic\ntransformer based world models for reinforcement learning. In NeurIPS, 2023a.\nWendong Zhang, Geng Chen, Xiangming Zhu, Siyu Gao, Yunbo Wang, and Xiaokang Yang.\nPredictive experience replay for continual visual control and forecasting.\narXiv preprint\narXiv:2303.06572, 2023b.\nYan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang, Qingnan Fan, Kaichun Mo, and Hao Dong.\nDualafford: Learning collaborative visual affordance for dual-gripper manipulation. ICLR, 2023.\nRuijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daumé III,\nand Furong Huang. Taco: Temporal latent action-driven contrastive loss for visual reinforcement\nlearning. In NeurIPS, volume 36, 2023.\n14\nPublished as a conference paper at ICLR 2025\nAPPENDIX\nA\nENVIRONMENT DETAILS\nAs illustrated in Table 2, language description is employed for calculating the MineCLIP reward (Fan\net al., 2022). Initial tools are the items provided in the inventory at the beginning of each episode.\nInitial mobs and distance specifies the types of mobs present at the start of each episode and their\ninitial distance from the agent. Max steps refers to the maximum allowed steps per episode.\nTable 2: Details of the MineDojo tasks.\nTask\nLanguage description\nInitial tools\nInitial mobs and distance Max steps\nHarvest log in plains\n“Cut a tree.”\n–\n–\n1000\nHarvest water with bucket\n“Obtain water.”\nbucket\n–\n1000\nHarvest sand\n“Obtain sand.”\n–\n–\n1000\nShear sheep\n“Obtain wool.”\nshear\nsheep, 15\n1000\nMine iron ore\n“Mine iron ore.”\nstone pickaxe\n–\n2000\nB\nCOMPARED METHODS\nWe compare LS-Imagine with strong Minecraft agents, including:\n• DreamerV3 (Hafner et al., 2023): An MBRL approach that learns directly from the step-by-step\nimaginations of future latent states generated by the world model.\n• VPT (Baker et al., 2022): A foundation model designed for Minecraft trained through behavior\ncloning, on a dataset consisting of 70,000 hours of game playing collected from the Internet.\n• STEVE-1 (Lifshitz et al., 2023): An instruction-following Minecraft agent that translates lan-\nguage instructions into specific goals. To evaluate its effectiveness, we assess Steve-1’s zero-shot\nperformance on our tasks by supplying it with task instructions.\n• Director (Hafner et al., 2022): An agent learns hierarchical behaviors by leveraging a world model\nto plan within its latent space.\n• PTGM (Yuan et al., 2024): An RL method that pretrains goal-based policy and adopts temporal\nabstractions and behavior regularization.\nC\nMODEL DETAILS\nC.1\nENVIRONMENTAL INTERACTION AND DATA COLLECTION\nTo train LS-Imagine’s world model, we collect both short-term and long-term transition data through\ninteractions with the environment. As shown in Figure 8, at each time step t, the agent interacts with\nthe environment following the current policy. At each time step, the data buffer collects a tuple Dt,\nwhich includes (ot, at, Mt, rt, ct, jt, ∆t, Gt):\n• ot represents the observed image.\n• at represents the agent’s action taken given ot.\n• Mt is the affordance map generated by a multimodal U-Net given ot and task instructions I.\n• rt is defined in Sec. 3.2.3, which is the immediate reward computed as a weighted sum of the sparse\nenvironmental reward renv\nt\nafter executing at−1, the MineCLIP reward rMineCLIP\nt\nfrom a pretrained\nscoring model (Fan et al., 2022), and the intrinsic reward rintr\nt\ndefined in Eq. (1) and based on Mt.\n• ct is the continuation flag received from the environment, which indicates whether further interac-\ntion is required after this step.\n• jt is the jumping flag, which is used to train the world model to trigger long-term imagination\nduring model-based behavior learning. We first estimate the jumping probability Pjump using Eq. (3)\nbased on Mt. To stabilize training, we establish a dynamic threshold Pthresh, which accounts for\nthe varying guidance strength provided by the affordance map across different tasks, resulting in\n15\nPublished as a conference paper at ICLR 2025\nMultimodal U-Net\n   ... ...   \n   ... ...   \nMultimodal U-Net\nenv\n Zoom in \nagent\nMultimodal U-Net\n...\nFigure 8: Environmental interaction and data collection.\ntask-specific distributions of Pjump. Specifically, from the beginning of training, we store the Pjump\nvalues for every interaction step in a dedicated buffer. The threshold Pthresh is then dynamically\ncalculated as the mean of all Pjump values currently in the buffer plus their standard deviation. This\ndynamic adjustment ensures that the threshold adapts to the characteristics of the task and remains\nrobust throughout training. If Pjump > Pthresh, we set jt = 1; otherwise, jt = 0.\n• ∆t represents the expected number of step intervals in the jumpy state transitions during long-term\nimaginations. Specifically, we set ∆t = 1 by default, corresponding to a short-term transition.\n• Gt represents the expected cumulative reward between the pre- and post-jump states when long-\nterm imagination occurs. Specifically, for a short-term transition, we set Gt = rt by default.\nIf jt = 0, Dt is defined as the starting point of a short-term transition within the pair (Dt, Dt+1).\nDuring world model training, (Dt, Dt+1) is replayed to train the related modules associated with\nshort-term dynamics. Once we obtain jt = 1 during interactions, we define the current step as the\nstarting point of a simulated long-term transition (Dt, D′\nt+1). Notably, we use D′\nt+1 to differentiate\nfrom its short-term counterparts.\nWe define D′\nt+1 = (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1), where r′\nt+1 and c′\nt+1 are com-\nputed in the same manner as in short-term tuples but with o′\nt+1 and M′\nt+1 as inputs. Similarly, a′\nt+1\nand j′\nt+1 are also computed in the same way as in short-term tuples. We record them in the data\nbuffer for better training of the reward predictor and the jump predictor.\nThe next question is how to annotate ∆′\nt+1, G′\nt+1, and o′\nt+1 to train the long-term branch.\n• o′\nt+1 is a simulated image rather than a real-captured image. It is obtained by cropping the original\nobservation ot based on the high-value regions in the affordance map Mt.\n• ∆′\nt+1 is an estimation of the number of real interaction steps between the pre-jump state and\nthe post-jump state. Since the post-jump state is not real data obtained from the environment,\nwe first identify a real state that closely resembles the post-jump state. We then calculate the\nnumber of steps required to transition from the pre-jump state to this identified real post-jump state.\nSpecifically, we use the intrinsic reward as a measurement. Starting from the pre-jump state, during\nsubsequent interactions with the environment, if the agent reaches a real state where the intrinsic\nreward satisfies rintr\nt+∆′\nt+1 ≥rintr ′\nt+1 , we take this state as the real post-jump state and take ∆′\nt+1 as\nthe long-term jumping interval.\n• G′\nt+1 is the cumulative reward within ∆′\nt+1 interaction steps, i.e., G′\nt+1 = P∆′\nt+1\ni=1 γi−1rt+i.\n16\nPublished as a conference paper at ICLR 2025\nPatch Partition\n Linear Embedding\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nPatch Merging\nSwin Transformer\nBlock × 2\nSwin Transformer\nBlock × 2\nSwin Transformer\nBlock × 2\nPatch Expanding\nSwin Transformer\nBlock × 2\nPatch Expanding\nSwin Transformer\nBlock × 2\nPatch Expanding\nPatch Expanding\nLinear Projection\nCut a tree\nCompute affordance\nmap with MineCLIP\nText \nEncoder\nTIA\nTIA\nTIA\nTIA\nFigure 9: The architecture of multimodal U-Net.\nC.2\nFRAMEWORK OF MULTIMODAL U-NET\nAs described in Sec. 3.2.2, we train a multimodal U-Net to rapidly generate affordance maps based on\nobservation images and task instructions. Our enhanced multimodal U-Net architecture, as illustrated\nin Figure 9, is based on Swin-Unet (Cao et al., 2022), a U-shaped encoder-decoder architecture built\non Swin Transformer blocks. The enhanced multimodal U-Net consists of an encoder, a decoder, a\nbridge layer, and a text processing module. In the Swin-Unet-inspired structure, the basic unit is the\nSwin Transformer block. For the encoder, the input image is divided into non-overlapping patches\nof size 4 × 4 to convert the input into a sequence of patch embeddings. Through this method, each\npatch has a feature dimension of 4 × 4 × 3 = 48. The patch embeddings are then projected through a\nlinear embedding layer (denoted as C), and the transformed patch tokens are passed through several\nSwin Transformer blocks and patch merging layers to produce hierarchical feature representations.\nThe patch merging layers are responsible for downsampling and increasing the dimensionality, while\nthe Swin Transformer blocks handle feature representation learning.\nFor the task instruction, the text description is processed through the text encoder of MineCLIP (Cao\net al., 2022) to obtain text embeddings, which are integrated with the image features extracted at\neach layer of the encoder via the Text-Image Attention (TIA) module. The TIA module employs a\nmulti-head attention mechanism to fuse image features (as keys and values) with text features (as\nqueries) in a multi-scale attention-based fusion. The resulting fused text-image features are passed\nthrough the bridge layer and are subsequently combined with the corresponding features during the\nupsampling process in the decoder.\nThe decoder comprises Swin Transformer blocks and patch-expanding layers. The extracted context\nfeatures are combined through the bridge layer with the multi-scale text-image features from the\nencoder to compensate for the spatial information lost during downsampling and to integrate the text\ninformation. Unlike the patch merging layers, the patch expanding layers are specifically designed for\nupsampling. They reshape the adjacent feature maps by performing a 2× upsampling of the resolution,\nexpanding the feature maps into larger ones. Finally, a final patch expanding layer performs a 4×\nupsampling to restore the resolution of the feature map to the input resolution W × H ), followed by\na linear projection layer applied on the upsampled features to produce pixel-level affordance maps.\n17\nPublished as a conference paper at ICLR 2025\n...\n(a) LS-Imagine (series)\n...\n...\n...\n...\n(b) LS-Imagine (parallel)\nFigure 10: Comparison with series and parallel variants of mixed imaginations.\nC.3\nVARIANTS OF LONG SHORT-TERM IMAGINATIONS\nWe compare two alternative pathways of the long short-term imaginations in Figure 10.\nC.4\nFULL ALGORITHM\nWe present the training pipeline of LS-Imagine in Algorithm 1.\nAlgorithm 1 The training pipeline of LS-Imagine.\n1: Initialize parameters ϕ, θ, ψ.\n2: Compute affordance map with MineCLIP.\n▷Affordance map generation\n3: Train multimodal U-Net with annotated data. ▷To enable real-time interaction with the affordance maps\n4: Train the random agent and collect a replay buffer B.\n5: while not converged do\n6:\nSample long short-term transitions from B.\n▷Representation learning\n7:\nUpdate the world model ϕ using Eq. (7).\n8:\nGenerate (s1, ˆa1, ˆj1) using πθ and Wϕ.\n9:\nfor time step t = 2 · · · L do\n▷Behavior learning\n10:\nif jump flag ˆjt−1 then\n11:\nGenerate (s′\nt, a′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt) using πθ and long-term imagination of Wϕ.\n12:\nUpdate (ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt) ←(s′\nt, a′\nt, c′\nt, j′\nt, ∆′\nt, G′\nt).\n13:\nelse\n14:\nGenerate (˜st, ˜at, ˜rt, ˜ct, ˜jt) using πθ and short-term imagination of Wϕ.\n15:\nUpdate (ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt) ←(˜st, ˜at, ˜ct, ˜jt, 1, ˜rt).\n16:\nend if\n17:\nend for\n18:\nCalculate value estimate using Eq. (9).\n19:\nOptimize actor πθ using Eq. (11) over {(ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt)}L\nt=1.\n20:\nOptimize critic vψ using Eq. (10) over {(ˆst, ˆat, ˆct, ˆjt, ˆ∆t, ˆGt)}L\nt=1.\n21:\nfor time step t = 1 · · · T do\n▷Environment interaction\n22:\nSample ˆat ∼πθ (ˆat | ˆst)\n23:\nrenv\nt , ot+1, ct ←env.step(ˆat)\n24:\nGenerate affordance map Mt with multimodal U-Net for each ot.\n25:\nCalculate intrinsic reward rintr\nt\nand jump flag jt based on the affordance map.\n26:\nCollect short-term data (ot, at, Mt, rt, ct, jt, ∆t, Gt).\n27:\nif jumpy flag jt then\n28:\nConstruct long-term data (o′\nt+1, a′\nt+1, M′\nt+1, r′\nt+1, c′\nt+1, j′\nt+1, ∆′\nt+1, G′\nt+1).\n29:\nend if\n30:\nend for\n31:\nAppend long short-term transitions to B.\n32: end while\n18\nPublished as a conference paper at ICLR 2025\nTable 3: The success rate and the number of steps per episode for task completion.\nModel\nHarvest log in plains Harvest water with bucket\nHarvest sand\nShear sheep\nMine iron ore\nsucc. (%)\nsucc. step\nsucc. (%)\nsucc. step\nsucc. (%) succ. step succ. (%) succ. step succ. (%) succ. step\nVPT\n6.97\n963.32\n0.61\n987.65\n12.99\n880.54\n1.94\n987.49\n0.00\n—\nSTEVE-1\n57.00\n752.47\n6.00\n989.07\n37.00\n770.40\n3.00\n992.36\n0.00\n—\nPTGM\n41.86\n811.19\n2.78\n977.78\n17.71\n833.64\n21.54\n887.03\n15.14\n1586.03\nDirector\n8.67\n968.09\n20.90\n931.74\n36.36\n825.35\n1.27\n995.99\n7.82\n1906.31\nDreamerV3\n53.33\n711.22\n55.72\n628.79\n59.88\n548.76\n25.13\n841.14\n16.79\n1789.06\nLS-Imagine\n80.63\n503.35\n77.31\n502.61\n62.68\n601.18\n54.28\n633.78\n20.28\n1748.55\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(a) Jumping frequency\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0\n100\n200\n300\n400\n500\n(b) Interval ˆ∆t\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.00\n0.05\n0.10\n0.15\n0.20\nHarvest log in plains\nHarvest water with bucket\n(c) Dynamic threshold Pthresh\nFigure 11: Analyses of long-term imaginations throughout training.\nC.5\nCLARIFICATION ON STOCHASTIC LONG-TERM IMAGINATION\nOne might argue that long-term imagination could skip essential intermediate steps that gradually\nlead to the objective, potentially resulting in a lack of learning for these crucial actions. To address\nthis issue, we adopt a probabilistic mechanism. Specifically, even when ˆjt = True, indicating that a\nlong-term transition is to be executed, we implement a probability of 0.7 for executing the jump and\n0.3 for not jumping. This allocation ensures a 30% chance that the transition will execute the short-\nterm imagination with gradient feedback attached to the actions. This stochastic decision-making\nis based on a uniform distribution, providing a balanced approach between leveraging long-term\nimagination and capturing essential short-term behaviors.\nC.6\nADDITIONAL LIMITATION\nIt is worth mentioning that LS-Imagine simulates the agent’s state when approaching a target object\nin 3D navigation environments with embodied agents by zooming in on the observed image, and\nsets intrinsic rewards based on whether the agent is close to and has positioned the target object\nat the center of the observation. Therefore, LS-Imagine is not suitable for environments with\nfixed viewpoints, 2D environments, or those where the reward mechanism is more complex than\napproaching objects (e.g., driving).\nD\nADDITIONAL RESULTS\nD.1\nNUMERICAL COMPARISONS\nTable 3 compares existing approaches on the challenging MineDojo environment.\nD.2\nANALYSES ON LONG-TERM IMAGINATIONS\nWe use the task harvest log in plains as an example to facilitate the understanding of the long short-\nterm imagination process. In Figure 11(a), we first track the frequency of long-term imaginations and\nthe corresponding predicted state intervals ˆ∆t throughout the training process. The curve shows the\nproportion of imagination sequences involving jumpy state transitions relative to the total number of\nimagination sequences. Initially, the jumping frequency is low because the world model has not yet\nlearned to identify when a jump is necessary based on the state. As the model’s predictions improve\nin the early stages of training, the frequency increases, likely due to the agent’s underdeveloped\npolicies, which result in more observations far from the goal and necessitate long-term exploration.\nOver time, as the agent learns policies that bring it closer to the target, the frequency of observations\nfar from the goal decreases, reducing the need for jumps.\n19\nPublished as a conference paper at ICLR 2025\n(a) Harvest log in plains\nShort-term imagination\nLong-term imagination\n(b) Harvest water with bucket\n(c) Harvest sand\n(d) Shear sheep\n(e) Mine iron ore\nFigure 12: Visualization of the complete long short-term imagination sequences.\nAdditionally, we find that among all sequences with jumpy state transitions, the average number of\njumpy transitions per sequence, within a horizon of 15 steps, 1.02. This indicates that, in most cases\nof these tasks, a single jumpy transition is sufficient to bring the agent close to the target.\nIn Figure 11(b), we track the variations of the jumping state intervals, ˆ∆t, throughout training. At the\nbeginning, ˆ∆t is high, indicating that the policy requires many steps to reach the target. As the policy\nimproves, fewer steps are needed to approach the target, leading to a gradual decrease in ˆ∆t. Notably,\nas ˆ∆t evolves with the updated policy, it also ensures minimal misalignment in Eq. (9) between the\nfuture cumulative rewards computed with jumpy imaginations and the behavior policy.\nFurthermore, in Figure 11(c), we track the variation curves of the dynamic threshold Pthresh during\ntraining in different tasks, and observe that:\n20\nPublished as a conference paper at ICLR 2025\n(a) Explore for a village\n(b) Mine ore\nFigure 13: Affordance maps when the target is invisible or occluded.\nlog\nplanks\ncrafting\ntable\nstick\nwooden\npickaxe\ncobblestone\nstone\npickaxe\n20\n30\n40\n50\n60\n70\n80\n90\n100\nSuccess rate (%)\n83\n83\n83\n83\n80\n44\n32\n91\n91\n88\n88\n84\n80\n69\nDreamerV3\nLS-Imagine\n(a) Success rate\nlog\nplanks\ncrafting\ntable\nstick\nwooden\npickaxe\ncobblestone\nstone\npickaxe\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nSteps per episode\n437\n454\n465\n481\n552\n854\n923\n320\n329\n363\n381\n447\n613\n684\nDreamerV3\nLS-Imagine\n(b) The number of steps required for task completion\nFigure 14: Comparison of LS-Imagine and DreamerV3 on a long-horizon “Tech Tree” task.\n• For task such as harvest log in plains, the variance of Pthresh is high during the early stages of training.\nSince Pthresh serves as a temporal smoothing of Pjump, this reflects the significant fluctuations of\nPjump at the beginning of training, highlighting the importance of adopting a dynamic threshold.\n• Across various tasks, Pthresh consistently converges in the later stages of training, demonstrating its\neffectiveness in improving the stability of exploratory imaginations.\n• The converged values of Pthresh differ across tasks, indicating that involving an automated computa-\ntion of Pthresh enables us to avoid tedious hyperparameter tuning.\nD.3\nVISUALIZATION OF LONG SHORT-TERM IMAGINATIONS\nAs illustrated in Figure 12, we visualize the complete long short-term imagination sequences for\nthe agent across various tasks. This visualization further demonstrates how the affordance map\naccurately identifies regions of high exploration potential in the image, and how the long short-term\nimagination approach provides reasonable and applicable guidance for the agent’s task execution.\nThese qualitative results reinforce the effectiveness of our method in guiding the agent toward its\ngoal with greater precision and efficiency.\nD.4\nDEPENDENCE ON THE VISIBILITY OF OBJECTS\nThe long-term transitions of our approach rely on the affordance map to identify high-value ex-\nploration areas. However, it is crucial to note that our affordance map generation method is not\nmerely an object recognition algorithm that highlights areas only when the target is present. Thanks\nto MineCLIP’s pretraining on extensive expert demonstration videos, our approach can generate\naffordance maps that provide guidance even when the target is completely occluded.\n21\nPublished as a conference paper at ICLR 2025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nLong-term branch loss scale\nlong = 0.1\nlong = 0.5\nlong = 1.0\nlong = 5.0\nlong = 10.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nIntrinsic reward weight\n= 0.1\n= 0.5\n= 1.0\n= 5.0\n= 10.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnvironment steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate\nGaussian distribution std\n(\nx,\ny) = (64, 40)\n(\nx,\ny) = (128, 80)\n(\nx,\ny) = (256, 160)\nFigure 15: Results of hyperparameter sensitivity analyses.\nFor instance, as illustrated in Figure 13, throughout the task of locating a village, the affordance map\nconsistently provides effective guidance to the agent, suggesting exploration of the forest to the right\nor the open area on the left hillside, even when the village is not visible in the current observation.\nSimilarly, in mining tasks where ores are typically underground, the affordance map directs the agent\nto dig into the mountain area on the right. As we can see, even when the target is occluded, the\naffordance map enables the agent to continue exploring effectively.\nFurther research direction.\nDue to the complexity of open-world environments, the affordance\nmap may fail to provide effective guidance in scenarios that the MineCLIP model has not encountered\nbefore. To address this issue, we plan to progressively finetune the MineCLIP model with the\ncollected new data and introduce a new prompt to the agent: “Explore the widest possible area to find\n{target}” when the affordance map fails to identify high-value areas. This prompt, combined with\nintrinsic rewards generated by MineCLIP, encourages the agent to conduct extensive exploration.\nD.5\nRESULTS ON LONG-HORIZON TASKS\nTo demonstrate the potential application of LS-Imagine in more complex tasks, we conduct experi-\nments on a “Tech Tree” task in MineDojo, specifically crafting a stone pickaxe from scratch. This\ntask involves seven subgoals: log, planks, crafting table, stick, wooden pickaxe, cobblestone, and\nstone pickaxe. Since LS-Imagine is primarily designed to focus on environmental interactions and\ntask execution under fixed objectives, rather than task decomposition and planning, we adopt the\nDECKARD method (Nottingham et al., 2023) for task planning. This method provides top-level\nguidance, with LS-Imagine executing the corresponding subtasks. Each subtask was trained for\n1 million steps and then tested within 1,000 steps per episode. The results are shown in Figure\n14, which demonstrate that our LS-Imagine consistently outperforms DreamerV3, achieving higher\nsuccess rates and requiring fewer steps to complete each subgoal.\nD.6\nHYPERPARAMETER ANALYSES\nWe conduct sensitivity analyses on three hyperparameters:\n• The long-term branch loss scale βlong: As shown in Figure 15 (Left), we observe that when βlong\nfor the long-term branch is too small or too large, it impedes the learning of long-term imagination,\nleading to a decline in performance.\n• The intrinsic reward weight α: From Figure 15 (Middle), we observe that if the hyperparameter\nα for intrinsic reward is excessively small, it may result in insufficient guidance and inaccurate\nreward estimation for the post-jumpy-transition state.\n• The intrinsic reward Gaussian parameters (σx, σy): As shown in Figure 16, (σx, σy) control the\nstandard deviations of the Gaussian distribution along the horizontal and vertical axes, respectively.\nIntuitively, setting these hyperparameters too low may cause the model to overlook targets located\nat the edges of the observed images. Conversely, excessively high (σx, σy) may reduce the reward\ndiscrepancy for targets at different positions within the observation, thereby diminishing the agent’s\nincentive to focus on the target precisely. From Figure 15 (Right), we observe that the final\nperformance is robust to the tested parameters, with all configurations outperforming the baseline\nmodels presented in previous experiments.\nThe final hyperparameters of LS-Imagine are shown in Table 4.\n22\nPublished as a conference paper at ICLR 2025\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (64, 40)\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (128, 80)\n0\n64\n128\n192\n256\n0\n40\n80\n120\n160\n(\nx,\ny) = (256, 160)\n0.5\n1.0\n1.5\n2.0\n2.5\nFigure 16: Visualization of Gaussian matrices with different standard deviations.\nTable 4: Hyperparameters of LS-Imagine.\nName\nNotation\nValue\nAffordance map generation\nSliding window size\n—\n0.15 × 0.15\nSliding steps\n—\n9 × 9\nU-Net train epochs\n—\n500\nU-Net initial learning rate\n—\n5 × 10−4\nU-Net learning rate decay epochs\n—\n50\nU-Net learning rate decay rate\n—\n0.10\nText feature dimensions\n—\n512\nGaussian distribution standard deviations\n(σx, σy)\n(128, 80)\nGeneral\nReplay capacity\n—\n1 × 106\nBatch size\nB\n16\nBatch length\nT\n32\nTrain ratio\n—\n16\nWorld Model\nIntrinsic reward weight\nα\n1\nDeterministic latent dimensions\n—\n4,096\nStochastic latent dimensions\n—\n32\nDiscrete latent classes\n—\n32\nRSSM number of units\n—\n1,024\nWorld model learning rate\n—\n1 × 10−4\nLong-term branch loss scale\nβlong\n1\nReconstruction loss scale\nβpred\n1\nDynamics loss scale\nβdyn\n1\nRepresentation loss scale\nβrep\n0.1\nBehavior Learning\nImagination horizon\nL\n15\nDiscount\nγ\n0.997\nλ-target\nλ\n0.95\nActor learning rate\n—\n3 · 10−5\nCritic learning rate\n—\n3 · 10−5\n23\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | Open-World Reinforcement Learning over Long Short-Term Imagination\n\n## 📌 背景痛点\/本文动机\n在开放世界中训练视觉强化学习智能体面临着重大挑战。虽然各种基于模型的强化学习方法通过学习交互式世界模型提高了样本效率，但这些智能体往往“目光短浅”，因为它们通常是在由世界模型生成的短期想象经验片段上进行训练的。本文认为，开放世界决策中的主要挑战是在广阔的状态空间中提高探索效率，特别是对于需要考虑长期回报的任务。\n\n## 🚀 核心方法\n💡 创新点1：长短期世界模型\n本文提出了LS-Imagine，它扩展了想象范围，在有限的状态转换步骤内，使智能体能够探索可能带来有希望长期反馈的行为。LS-Imagine的基础是构建一个长短期世界模型，通过模拟目标条件下的跳跃状态转换，并通过对单个图像中的特定区域进行缩放来计算相应的可利用性图，从而将直接长期价值整合到行为学习中。\n\n💡 创新点2：可利用性图和内在奖励\n为了提高开放世界中基于模型的强化学习的样本效率，本文使用视觉观察和文本任务定义来生成可利用性图。可利用性图突出了观察区域与任务描述的相关性，作为空间先验，有效地引导智能体的探索朝着感兴趣的区域进行。此外，本文引入了基于可利用性图的内在奖励函数，以鼓励智能体向目标移动。\n\n💡 创新点3：混合长短期想象路径\nLS-Imagine采用演员-评论家算法，根据世界模型生成的有限序列的长期和短期想象来优化智能体的策略。智能体从编码的初始状态开始，根据跳跃标志动态选择长期或短期状态转换模型来预测后续状态。对于长期想象预测的跳跃状态，间隔预测器估计从ˆst−1到ˆst的步骤数ˆ∆t和ˆ∆t时间间隔内的潜在折扣累积奖励ˆGt。否则，对于通过短期想象获得的状态，对应于环境中的单步转换，我们设置ˆ∆t = 1和ˆGt = ˆrt。因此，在一个想象集中，我们获得一个步骤间隔序列ˆ∆2:L和一个预测奖励序列ˆG2:L，这些奖励在连续的想象状态之间。\n\n## 📈 实验结果\n本文在MineDojo基准测试中对LS-Imagine进行了评估，MineDojo是一个基于Minecraft游戏的综合模拟平台，具有各种开放性任务。结果表明，LS-Imagine在完成挑战性任务方面显著优于现有视觉强化学习方法，特别是在稀疏目标分布的任务中。\n\n## 💬 可借鉴之处\nLS-Imagine提出了一种新颖的基于模型的强化学习方法，该方法捕获即时和跳跃状态转换，并利用它们在行为学习中提高开放世界的探索效率。该方法具有以下可借鉴之处：\n\n*   长短期世界模型架构，能够模拟长期和短期状态转换，并利用它们进行行为学习。\n*   通过图像缩放生成可利用性图的方法，有效地引导智能体的探索。\n*   基于可利用性图的内在奖励函数，鼓励智能体向目标移动。\n*   混合长短期想象路径，使智能体能够更好地理解长期价值，提高决策能力。\n\nLS-Imagine为开放世界强化学习提供了一种有前景的方法，并为未来研究提供了灵感。","llm_summary_res_status":200}
{"title":"VirtualHome: Simulating Household Activities via Programs","authors":"Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, Antonio Torralba","summary":"In this paper, we are interested in modeling complex activities that occur in\na typical household. We propose to use programs, i.e., sequences of atomic\nactions and interactions, as a high level representation of complex tasks.\nPrograms are interesting because they provide a non-ambiguous representation of\na task, and allow agents to execute them. However, nowadays, there is no\ndatabase providing this type of information. Towards this goal, we first\ncrowd-source programs for a variety of activities that happen in people's\nhomes, via a game-like interface used for teaching kids how to code. Using the\ncollected dataset, we show how we can learn to extract programs directly from\nnatural language descriptions or from videos. We then implement the most common\natomic (inter)actions in the Unity3D game engine, and use our programs to\n\"drive\" an artificial agent to execute tasks in a simulated household\nenvironment. Our VirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and testing of video\nunderstanding models. We further showcase examples of our agent performing\ntasks in our VirtualHome based on language descriptions.","url":"http:\/\/arxiv.org\/abs\/1806.07011v1","pdf_url":"http:\/\/arxiv.org\/pdf\/1806.07011v1","published":1529374604000,"comment":"CVPR 2018 (Oral)","pdf_text":"VirtualHome: Simulating Household Activities via Programs\nXavier Puig1∗, Kevin Ra2∗, Marko Boben3∗, Jiaman Li4, Tingwu Wang4,\nSanja Fidler4, Antonio Torralba1\n1MIT\n2McGill University\n3University of Ljubljana\n4University of Toronto\n{xavierpuig,torralba}@csail.mit.edu\nkevin.ra@mail.mcgill.ca\nmarko.boben@fri.uni-lj.si\n{tingwuwang,ljm,fidler}@cs.toronto.edu\nAbstract\nIn this paper, we are interested in modeling complex ac-\ntivities that occur in a typical household. We propose to use\nprograms, i.e., sequences of atomic actions and interactions,\nas a high level representation of complex tasks. Programs\nare interesting because they provide a non-ambiguous rep-\nresentation of a task, and allow agents to execute them.\nHowever, nowadays, there is no database providing this type\nof information. Towards this goal, we ﬁrst crowd-source\nprograms for a variety of activities that happen in people’s\nhomes, via a game-like interface used for teaching kids how\nto code. Using the collected dataset, we show how we can\nlearn to extract programs directly from natural language\ndescriptions or from videos. We then implement the most\ncommon atomic (inter)actions in the Unity3D game engine,\nand use our programs to “drive” an artiﬁcial agent to ex-\necute tasks in a simulated household environment. Our\nVirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and\ntesting of video understanding models. We further showcase\nexamples of our agent performing tasks in our VirtualHome\nbased on language descriptions.\n1. Introduction\nAutonomous agents need to know the sequences of ac-\ntions that need to be performed in order to achieve certain\ngoals. For example, we might want a robot to clean our room,\nmake the bed, or cook dinner. One can deﬁne activities with\nprocedural recipes or programs that describe how one can\naccomplish the task. A program contains a sequence of sim-\nple symbolic instructions, each referencing an atomic action\n(e.g. “sit”) or interaction (e.g. “pick-up object”) and a num-\nber of objects that the action refers to (e.g., “pick-up juice”).\nAssuming that an agent knows how to execute the atomic\nactions, programs provide an effective means of “driving” a\nrobot to perform different, more complex tasks. Programs\n∗Denotes equal contribution\nRobots\nAction:     Work on computer\nDescription:  Turn on your computer and \nsit in front of it. Type on the keyboard, \ngrab the mouse to scroll.\nAction:     Make coffee\nDescription:  Go to the kitchen and swith\non the coffee machine. Wait until it’s \ndone and pour the coffee into a cup.\nAction:     Read a book\nDescription:  Sit down in recliner. Pick up \na novel off of coffee table. Open novel to \nlast read page. Read.\nVirtualHome\nrobot playground\nprogram\nvideo\nKnowledge Base of Household Tasks\nFigure 1: We ﬁrst crowdsource a large knowledge base of household\ntasks, (top). Each task has a high level name, and a natural language in-\nstruction. We then collect “programs” for these tasks, (middle left), where\nthe annotators “translate” the instruction into simple code. We implement\nthe most frequent (inter)actions in a 3D simulator, called VirtualHouse,\nallowing us to drive an agent to execute tasks deﬁned by programs. We pro-\npose methods to generate programs automatically from text (top) and video\n(bottom), thus driving an agent via language and a video demonstration.\ncan also be used as an internal representation of an activity\nshown in a video or described by a human (or another agent).\nOur goal in this paper is to automatically generate programs\nfrom natural language descriptions, as well as from video\ndemonstrations, potentially allowing naive users to teach\ntheir robot a wide variety of novel tasks.\nTowards this goal, one important missing piece is the\nlack of a database describing activities composed of multi-\nple steps. We ﬁrst crowdsource common-sense information\nabout typical activities that happen in people’s homes, form-\ning the natural language know-how of how these activities\nare performed. We then adapt the Scratch [1] interface used\nfor teaching kids how to code in order to collect programs\nthat formalize the activity as described in the knowledge\nbase. Note that these programs include all the steps required\nfor the robot to accomplish a task, even those that are not\nmentioned in the language descriptions. We then implement\nthe most common atomic (inter)actions in the Unity3D game\nengine, such as pick-up, switch on\/off, sit, stand-up. By ex-\nploiting the physics, navigation and kinematic models in the\narXiv:1806.07011v1  [cs.CV]  19 Jun 2018\ngame engine we enable an artiﬁcial agent to execute these\nprograms in a simulated household environment.\nWe ﬁrst introduce our data collection effort and the pro-\ngram based representation of activities. In Sec. 5 we show\nhow we can learn to automatically translate natural language\ninstructions of activities into programs. In Sec. 4 we intro-\nduce the VirtualHome simulator that allows us to create a\nlarge activity video dataset with rich ground-truth by using\nprograms to drive an agent in a synthetic world. Finally,\nwe use the synthetic videos to train a system to translate\nvideos of activities into the program being executed by the\nagent. Our VirtualHome opens an important “playground”\nfor both vision and robotics, allowing agents to exploit lan-\nguage and visual demonstration to execute novel activities\nin a simulated environment. Our data is available online:\nhttp:\/\/virtual-home.org\/.\n2. Related Work\nActions as programs. A few works have deﬁned activi-\nties as programs. In [30], the authors detect objects and ac-\ntions in cooking videos and generate an “action plan” using\na probabilistic grammar. By generating the plan, the robots\nwere able to execute complex actions by simply watching\nvideos. These authors further collected a tree bank of action\nplans from annotated cooking videos [29], creating a knowl-\nedge base of actions as programs for cooking. [18] tried to\ntranslate cooking recipes into action plans using an MRF.\n[23, 3] also argued for actions as a sequence of atomic steps.\nThey aligned YouTube how-to videos with their narrations\nin order to parse videos into such programs. Most of these\nworks were limited to either a small set of activities, or to\na narrow domain (cooking). We go beyond this by creating\na knowledge base about an exhaustive set of activities and\ntasks that people do in their homes.\n[25] crowd-sourced scripts of people’s actions at home\nin the form of natural language. These were mostly com-\nprised of one or two sentences describing a short sequence\nof actions. While this is valuable information, language is\nvery versatile and thus hard to convert into a usable program\non a robot. We show how to do this in our work.\nCode generation. There is increased interest in generat-\ning and interpreting source code [13]. Work most relevant\nto ours produces code given natural language inputs. [4]\nretrieves code snippets from Stackoverﬂow based on lan-\nguage queries. Given a sentence describing conditions, [19]\nproduces If-This-Then-That code. [14] generates a program\nspecifying the logic of a card game given a short descrip-\ntion of the rules. In [9], the authors inferred programs to\nanswer visual questions about images. Our work differs in\nthe domain, and works with text or video as input.\nRobotics. A subﬁeld of robotics aims at teaching robots\nto follow instructions provided in natural language by a hu-\nman tutor. However, most of the existing literature deals\nwith a constrained problem, for example, they learn to trans-\nlate navigational instructions into a sequence of robotic ac-\ntions [27, 15, 12, 16]. These instructions are typically sim-\npler as they directly mention what to do next, and the action\nspace is small. This is not the case in our work which also\nconsiders interactions with objects, and everyday activities\nwhich are typically far more complex.\nSimulation. Simulations using game engines have been\ndeveloped to facilitate training visual models for autonomous\ndriving [8, 21, 7], quadcopter ﬂying [24], or other robotic\ntasks [5]. Recent works have focused on simulating indoor\nenvironments, allowing for target-driven indoor navigation\nor interactive question answering [11, 28, 6, 22]. A few of\nthese works [11, 6] include actionable objects, allowing to\ninteract and change the environment. Our work focuses on\nsimulating a wide range of human actions, both in terms\nof objects interactions and human poses, which allows to\nsimulate common activities. We are not aware of simulators\nat the scale of objects and actions in a home, like ours. Lastly,\nwe give credit to the popular game Sims which we draw our\ninspiration from. Sims is a strategic video game mimicking\ndaily household activities. Unfortunately, the source of the\ngame is not public and thus cannot be used for our purpose.\n3. KB of Household Activities for Robots\nOur goal is to build a large repository of common activi-\nties and tasks that we perform in our households in our daily\nlives. These tasks can include simple actions like “turning on\nTV\" or complex ones such as “make coffee with milk\". What\nmakes our effort unique is that we are interested in collect-\ning this information for robots. Unlike humans, robots need\nmore direct instructions. For example, in order to “watch\ntv”, one might describe it (to a human) as “Switch on the\ntelevision, and watch it from the sofa”. Here, the actions\n“grab remote control” and “sit\/lie on sofa” have been omit-\nted, since they are part of the commonsense knowledge that\nhumans have. In our work, we aim to collect all the steps\nrequired for a robot to successfully execute a task, including\nthe commonsense steps. In particular, we want to collect\nprograms that fully describe activities.\nDescribing actions as programs has the advantage that it\nprovides a clear and non-ambiguous description of all the\nsteps needed to complete a task. Such programs can then\nbe used to instruct a robot or a virtual character. Programs\ncan also be used as a representation of a complex task that\ninvolves a number of simpler actions, providing a way to\nunderstand and compare activities and goals.\n3.1. Data Collection\nIn this section, we describe our dataset collection using\ncrowdsourcing. Describing actions as programs can be a\nchallenging task as most annotators have no programing\nexperience. We split the data collection effort in two parts.\nRGB\npose\nclass seg.\ninst. seg\ndepth\nﬂow\nFigure 2: VirtualHome Activity Dataset is a video dataset of composite activities created with our simulator. We start by generating\nprograms using a simple probabilistic grammar. We animate each program in VirtualHome by randomizing the selection of homes, agents,\ncameras, as well as the placement of a subset of the objects, the initial location of the agent, the speed of the actions, and choice of objects\nfor interactions. Each program is shown to an annotator who is asked to describe it in natural language (top row). Videos have ground-truth:\n(second row) time-stamp for each atomic action, (bottom) 2D and 3D pose, class and object instance segmentation, depth and optical ﬂow.\nIn the ﬁrst part, we ask AMT workers to provide verbal\ndescriptions of daily household activities. In particular, each\nworker is asked to come up with a common activity\/task,\ngive it a high level name, eg “make coffee”, and describe it\nin detail. In order to cover a wide spectrum of activities we\npre-speciﬁed in which scene the activity should start. Scenes\nwere selected randomly from a list of 8 scenes (living room,\nkitchen, dining room, bedroom, kids bedroom, bathroom,\nentrance hall, and home ofﬁce). An example of a described\nactivity is shown in Fig. 3. Note that these descriptions may\nlikely omit the commonsense steps, as they were written by\n“naive” workers that were describing these activities as they\nwould to a (human) friend.\nIn the second stage, we showed the collected descrip-\ntions to the AMT workers and asked them to translate these\ndescriptions into programs using a graphical programing lan-\nguage. We told them to produce a program that will “drive” a\nrobot to successfully accomplish the described activity. Our\ninterface builds on top of MIT’s Scratch project [1] designed\nto teach young children to write symbolic code. We found\nthat workers were capable of quickly learning to produce\nuseful programs by providing them with a carefully designed\ntutorial. Fig. 3 shows a snapshot of the programing interface.\nFinally, we asked more qualiﬁed workers hired via Upwork\ncrowdsourcing platform to double check the collected data.\nWorkers had to compose a program by composing a se-\nquence of steps. Each instruction is a Scratch block from a\npredeﬁned list of 77 possible blocks compiled by analyzing\nthe frequency of verbs in the collected descriptions. Each\nstep in the program is deﬁned by a block. A block deﬁnes a\nsyntactic frame with an action and a list of arguments (e.g.,\nthe block walk requires one argument to specify the desti-\nnation, Fig. 3.c). To simplify the search for blocks they are\norganized according to 9 broad action categories (Fig. 3.b).\nWe required that the program contains all the steps, even\nthose not explicitly mentioned in the description, but that\ncould be inferred from common-sense. Fig. 3.d shows an\nexample of a program. We also allowed annotators to use\na “special” block for missing actions, where the step can be\nwritten as free-form text. Programs using this special block\nwill not be used in the rest of the paper, but allowed us in\nidentifying new blocks that needed to be added.\nMore precisely, step t in the program can be written as:\nstept = [actiont] ⟨objectt,1⟩(idt,1) ... ⟨objectt,n⟩(idt,n)\nHere, id is an unique identiﬁer (counter) of an object and\nhelps in disambiguating different instances of objects that\nbelong to the same class. An example of a program for\n“watch tv” would be:\nstep1 = [Walk] ⟨TELEVISION⟩(1)\nstep2 = [SwitchOn] ⟨TELEVISION⟩(1)\nstep3 = [Walk] ⟨SOFA⟩(1)\nstep4 = [Sit] ⟨SOFA⟩(1)\nstep5 = [Watch] ⟨TELEVISION⟩(1)\nHere, the programs deﬁnes that the television in steps 1, 2\nand 5 refer to the same object instance.\n3.2. Dataset Analysis\nIn the ﬁrst part we collected 1814 descriptions. From\nthose, we were able to collect programs for 1703 descrip-\ntions. Some of the programs contained several “special\nblocks” for missing actions, which we remove, resulting\nin 1257 programs. We ﬁnally selected a set of tasks and\nasked workers to write programs for them, obtaining 1564\nadditional programs. The resulting 2821 programs form our\nActivityPrograms dataset. On average, the collected descrip-\ntions have 3.2 sentences and 21.9 words, and the resulting\nprograms have 11.6 steps on average. The dataset statistics\nare summarized in Table 1.a.\nAction name: \nThrow away newspaper\nDescription: \nTake the newspaper \non the living room table \nand toss it.\na)\nb)\nc)\nd)\nFigure 3: a) Description provided by a worker. b) User interface showing\nthe list of block categories and 4 example blocks, c) Example of composition\nof a block by adding the arguments. Each block is like a Lego piece where\nthe user can drop arguments inside and attach one block to another. d) Final\nprogram corresponding to the description from (a).\nAction counts\nWalk\nGrab\nFind\nPutback\nPutobjback\nLookat\nOpen\nSwitchon\nClose\nSit\nSwitchoff\nPointat\nPour\nWipe\nRinse\nWash\nPull\nGreet\nScrub\nRead\nPush\nTouch\nType\nTurnto\nCrawl\nFold\nPlugin\nWatch\nLie\nCover\nLift\nWrite\nPuton\nPutoff\nEat\nPlugout\nStandup\nSleep\nEnter\nSqueeze\nDrink\nShake\nLeave\nDrop\nUncover\nSpread\nStretch\nRun\nDance\nCut\n0\n500\n1000\n1500\n2000\n2500\nObject counts\nPlate\nChair\nCabinet\nFaucet\nBed\nBook\nCloset\nKitchen\nVacuum cleaner\nLiving room\nTelevision\nBedroom\nComputer\nRag\nTable\nHome office\nFridge\nFood\nSink\nBathroom\nToy\nTowel\nCouch\nChild\nNone\nRemote control\nBowl\nShoes\nDining room\nPillow\nWater\nCleaning solution\nSoap\nClothesshirt\nLaptop\nClothespants\nClothesdress\nPaper\nBlanket\nBroom\nSpoon\nKeyboard\nSponge\nFork\nDoor\nEntrance hall\nCup\nDresser\nWoman\nArms\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\na)\nb)\n    'Wa\nc\nFigure 4: a) Counts of actions in our ActivityPrograms\ndataset, b) object counts (zoom to read)\nThe dataset covers 75 atomic actions and 308 objects,\nmaking 2709 unique steps. Fig. 4.a shows a histogram of\nthe 50 most common actions appearing in the dataset, and,\nFig. 4.b, the 50 most common objects.\nOur dataset contains activities with several examples, and\nwe analyze their diversity by comparing their programs. Ta-\nble 1.b analyzes 4 selected activities. We compute their\nsimilarities as the average length of the longest common\nsubsequences computed between all pairs of programs.\nWe can also measure distances between activities by\nmeasuring the distance between programs. The similarity\nbetween two programs is measured as the length of their\nlongest common subsequence of instructions divided by the\nlength of the longest program. Table 1.c. shows the similar-\nity matrix (sorted to better show the block diagonal structure)\nbetween different activities in our dataset.\nCompleteness of programs. We analyze whether the\ncollected programs contain all the necessary steps to execute\nthe given task. We sample 100 collected programs and ask\n5 AMT workers to rate whether the program is complete,\nmissing minor steps (sitting in a chair before walking to-\nwards it) or important steps (ﬁlling a glass before drinking).\nResults show that 64% of the programs are complete, 28%\nare missing minor steps and 8% are missing crucial steps.\n4. VirtualHome: Simulator of Household Tasks\nThe main motivation behind using programs to represent\nactivities is to “drive” robots to perform tasks by having them\nexecuting these programs. As a proxy, we here use programs\nto drive characters in a simulated 3D environment. Simula-\ntions are useful as they deﬁne a playground for “robots”, an\nenvironment where artiﬁcial agents can be taught to perform\ntasks. Here, we focus on building the simulator, and leave\nlearning inside the simulator to future work. In particular,\nwe will assume the agent has access to all 3D and semantic\ninformation about the environment, as well as to manually\ndeﬁned animations. Our focus will be to show that programs\nrepresent a good way of instructing such agents. Further-\nmore, our simulator will allow us to generate a large-scale\nvideo dataset of complex activities that is rich and diverse.\nWe can create such a dataset by simply recording the agent\nexecuting programs in the simulator. The simulator then pro-\nvides us with dense ground-truth information, eg semantic\nsegmentation, depth, pose, etc. Fig. 2 showcases this dataset.\nWe implemented our VirtualHome simulator using the\nUnity3D game engine which allows us to exploit its kine-\nmatic, physics and navigation models, as well as user-\ncontributed 3D models available through Unity’s Assets\nstore. We obtained six furnished homes and 4 rigged hu-\nmanoid models from the web. On average, each home con-\ntains 357 object instances (86 per room). We collected ob-\njects from additional 30 object classes that appear in our\ncollected programs yet are not available in the package, via\nthe 3D warehouse 1. To ensure visual diversity, we collected\nat least 3 different models per class. The apartments and\nagents are shown in Fig. 5 and Fig. 6.\n4.1. Animating Programs in VirtualHome\nEvery step in the program requires us to animate the cor-\nresponding (inter)action in our virtual environment. We thus\nneed to both, determine which object in the home (which we\nrefer to as the game object) the step requires as well as prop-\nerly animating the action. To get the former we need to solve\nan optimization problem by taking into account all steps in\nthe program and ﬁnding a feasible path. For example, if the\nprogram requires the agent to switch on a computer and type\non a keyboard, ideally the agent would type on the keyboard\nnext to the chosen computer and not navigate to another key-\nboard attached to a different computer in possibly a different\nroom. We now describe our simulator in more detail.\nAnimating atomic actions. There is a huge variety and\nnumber of atomic actions that appear in the collected pro-\ngrams, as can be seen in Fig. 4. We implemented the 12 most\nfrequent ones: walk\/run, grab, switch-on\/off, open\/close,\nplace, look-at, sit\/standup, touch. Note that there is a large\nvariability in how an action is performed depending on to\nwhich object it is applied to (e.g., opening a fridge is dif-\nferent than opening a drawer). We use Unity’s NavMesh\nframework for navigation (path planner to avoid obstacles).\nFor each action we compute the agent’s target pose and\nanimate the action using RootMotion FinalIK inverse kine-\nmatics package. We further animate certain objects the agent\ninteracts with, e.g., we shake a coffee maker, animate toast\nin a toaster, show a (random) photo on a computer or TV\n1https:\/\/3dwarehouse.sketchup.com\nDataset\n# prog. avg # steps avg # sent. avg # words\nActivityProg.\n2821\n11.6\n3.2\n21.9\nSyntheticProg. 5193\n9.6\n3.4\n20.5\n(a)\nAction\n# Prog. LCS Norm. LCS\nMake coffee\n69\n4.56\n0.26\nFold laundry\n11\n1.29\n0.08\nWatch TV\n128\n3.65\n0.40\nClean\n42\n0.76\n0.04\n(b)\n(c)\nBrowse internet\nWork\nUser computer\nCheck email\nPay bills\nStudy\nEat dinner\nSet up table\nWash dishes with dishwasher\nWash dishes by hand\nWash dishes\nBrush teeth\nWash face\nTake shower\nTake bath\nWash hands\nMake bed\nTake nap\nGo to sleep\nSleep\nTurn off light\nTuck kids in bed\nWatch movie\nChange TV channel\nWatch TV\nTable 1: (a) We analyze programs and natural language descriptions for both, real activities in ActivityPrograms (Sec. 3), and synthesized\nprograms (with real descr.). (b) ActivityPrograms: Analyzing diversity in the same activity, by computing similarities across all pairs of the\ncollected programs. “LCS\" denotes longest common subsequence. For “norm.LCS\", we normalize by max length of the two programs. (c)\nshows the similarity matrix (sorted to better show the block diagonal structure) between different activities in our dataset.\nFigure 5: 3D households in our VirtualHome. Notice the diversity in room and object layout\nand appearance. Each home has on average 357 objects. First 4 scenes are used for training,\nthe ﬁfth is also used in val, and all scenes are used when testing our video-to-script model.\nmale 1\nfemale 1\nmale 2\nfemale 2\nFigure 6: Agents in VirtualHome. We use\nmale 1 and female 1 in train., and all agents\nwhen testing our video-to-program model.\nscreen, light up a burner on a stove, and light up the lamps in\nthe room, when these objects are switched on by the agent.\nPreparing the Scene. While every 3D home already con-\ntains many objects, the programs may still mention objects\nthat are not present in the scene. To deal with this, we ﬁrst\n“set” the scene by placing all missing objects that a program\nrefers to in the home, before we try to execute the program.\nTo be able to prepare a scene in a plausible way, we collect a\nknowledge base of possible object locations. The annotator\nis shown the class name and selects a list of other objects\n(including ﬂoor, wall) that are likely to support it.\nExecuting a Program. To animate a program we need\nﬁrst to create a mapping between the objects in the program\nand the corresponding instances inside the virtual simulator.\nFurthermore, for each step in the program, we also need to\ncompute the interaction position of the agent with respect to\nan object, and any additional information needed to animate\nthe action (e.g., which hand to use, speed). We build a tree\nof all possibilities of assigning game objects to objects in the\nprogram, along with all interaction positions and attributes.\nTo traverse the tree of possible states we use backtracking\nand stop as soon as a state executing the last step is found.\nSince the number of possible object mappings for each step is\nsmall, and we can prune the number of interaction positions\nto a few, our optimization runs in a few seconds, on average.\nAnimation. We place 6-9 static cameras in each room,\n26 per home on average. During recording, we switch be-\ntween cameras based on agent’s visibility. In particular, we\nrandomly select a camera which sees the agent, and keep it\nuntil the agent is visible and within allowed distance. For\nagent-object interaction we also try to select a camera and\nadjust its ﬁeld of view to enhance the visibility of the inter-\naction. We further randomize the position, angle and ﬁeld\nof view of each camera. Randomization is important when\ncreating a dataset to ensure diversity of the ﬁnal video data.\nVirtualHome Activity dataset. Since the programs in\nActivityPrograms represent real activities that happen in\nhouseholds, they contain signiﬁcant variability in actions\nand objects that appear in steps. While our ultimate aim is\nto be able to animate all these actions in our simulator, our\ncurrent efforts only support the top 12 most frequent actions.\nWe thus create another dataset that contains programs con-\ntaining only these actions in their steps. The creation of this\ndataset is explained below.\nWe synthesized 5,193 programs using a simple probabilis-\ntic grammar, and had each one described in natural language\nby a human annotator. Although these programs were not\ngiven by annotators, they produced reasonable activities, cre-\nating a much larger dataset of paired descriptions-programs\nat a fraction of the cost. We then animated each program\nin our simulator, and automatically generated ground-truth\nwhich allows us to train and evaluate our video models.\nAs can be seen from Table 1, descriptions in VirtualHome\nActivity dataset are of comparable length. However, the\nvocabulary here was biased towards that used in programs.\nWe animate the programs as described above, by random-\nizing the selection of home, an agent, cameras, placement\nof a subset of objects, initial location of the agent, speed\nof the actions, and choice of objects for interactions. We\nbuild on top of [2] to automatically generate groundtruth: 1)\ntime-stamp of each step to video, 2) agent’s 2D\/3D pose, 3)\nclass and instance segmentation, 4) depth, 5) optical ﬂow, 6)\ncamera parameters. Example of data is shown in Fig. 2.\n5. From Videos and Descriptions to Programs\nWe introduce a novel task using our dataset. In particular,\nwe aim to generate a program for the activity from either a\nnatural language description or from a video demonstration.\nWe treat the task of transcribing an input (description or\nvideo) into a program as a translation problem. We adapt the\nseq2seq model [26] for our task, and train it with Reinforce-\nment Learning that exploits the reward from the simulator.\nOur model consists of an RNN encoder that encodes\nthe input sequence into a hidden vector representation, and\nanother RNN acting as a decoder, generating one step of the\nprogram at a time. We use LSTM with 100-dim hidden states\nas our encoder. At each step t, our RNN decoder decodes a\nstep which takes the form of eq. (1). Let xt denote an input\nvector to our RNN decoder at step t, and let ht be the hidden\nstate. Here, ht is computed as in the standard LSTM using\ntanh as the non-linearity. Let ai be a one-hot encoding of an\naction i, and oi a one-hot encoding of an object. We compute\nthe probability pt\ni of an instruction i at step t as:\n˜ai = Waai, ˜oi,n = Wooi,n, vi = mean(˜ai, ˜oi,1, ..., ˜oi,n)\npt\ni = softmaxi( vi\n∥vi∥\nT\n· Wv(ht∥xatt\nt )) (1)\nwhere Wa and Wo and Wv are learnable matrices, and vi\ndenotes an embedding of an instruction. Note that here, n is\neither 1 or 2 (our instructions have at most two objects).\nThe input vector xt concatenates multiple features. In par-\nticular, we use the embedding v of the step with the highest\nprobability from the previous time instance of the decoder.\nFollowing [26], we further use the attention mechanism over\nthe encoder’s states to get another feature xatt\nt . In particular:\nαt\nj = softmaxj(vT \u0000Watt (ht∥hj\nenc)\n\u0001\u0001\n(2)\nxatt\nt\n=\nX\nj\nαt\njhj\nenc\n(3)\nwhere Watt, v are learnable parameters. Our full model is\nvisualized in Fig. 7.\nLearning and inference. Our goal is to generate pro-\ngrams that are both close to the ground-truth programs in\nterms of their LCS (longest common subsequence) and are\nalso executable by our renderer. To that end, we train our\nmodel in two phases. Firstly, we pre-train the model using\ncross-entropy loss at each time step of the RNN decoder.\nWe follow the typical training strategy where we make a\nprediction at each time instance but feed in the ground-truth\nstep to the next time instance. We use the word2vec [17]\nembeddings for matrices Wa and Wo.\nIn the second stage, we treat program generation as an\nReinforcement Learning problem, where the agent is learn-\ning a policy that generates steps to compose a program.\nWe follow [20], and use policy gradient optimization to\ntrain the model, using the greedy policy as the baseline es-\ntimator. We exploit two different kinds of reward r(ws, g)\nfor RL training, where ws denotes the sampled program,\nand g the ground-truth program. To ensure that the gener-\nated program is semantically correct (follows the descrip-\ntion\/video), we use the normalized LCS metric (length of the\nlongest common subsequence) between the two programs\nas our ﬁrst reward rLCS(ws, g). The second reward comes\nfrom our simulator, and measures whether the generated\nprogram is executable or not. This reward, rsim(ws), is a\nsimple binary value. We carefully balance the total reward\nas, r(ws, g) = rLCS(ws, g) + 0.1 · rsim(ws).\nSo far we did not specify the input to the RNN encoder.\nOur model accepts either a language description or a video.\nTextual Description.\nTo encode a textual description our\nRNN encoder gets as input the word2vec [17] embedding of\nthe word in the description at each time instance.\nVideo.\nTo generate programs from videos, we partition\neach video into 2-second clips and train a model to predict\nthe step at the middle frame. We use DilatedNet to obtain the\nsemantic segmentation of each frame and use the Temporal\nRelation Network [31] with 4-frame relations to predict the\nembedding of an instruction (action+object+object). We use\nthis embedding to obtain the likelihood of each instruction.\nThe prediction at each clip is used as input to the RNN\nencoder for program generation.\n6. Experiments\nIn our experiments we exploit both of our datasets: Activ-\nityPrograms containing descriptions and programs for real\nactivities, and VirtualHome Activity dataset that contains\nsynthesized programs, yet natural descriptions to describe\nthem. VirtualHome Activity dataset further contains videos\nanimating the programs.\n6.1. Step (Instruction) Classiﬁcation from Video\nWe ﬁrst evaluate our model for the task of video-based\naction and action-object-object (step\/instruction in the pro-\ngram) classiﬁcation. Here, we partition each video in 2-sec\nclips, and use the clip-based TRN to perform classiﬁcation.\nWe compute performance as the mean per-class accuracy\nacross all 2-sec clips in test. To better understand the gen-\neralization properties of the video-based models, we further\ndivide the test set into videos recorded in homes seen at\ntrain time, and videos in homes not seen at train time. We\nreport the results in Table 2 (left). To set the lower bound,\nwe also report a simple random retrieval baseline, in which\na step is randomly retrieved from the training set. We can\nsee that our model performs signiﬁcantly better. However,\na large number of actions and objects of interest, makes the\nprediction task challenging for the model.\nFigure 7: Our encoder-decoder LSTM for generating programs from natural language descriptions or videos.\nAction\nObjects\nSteps\nMean\nRand. Retrieval\n8.30%\n1.50%\n0.51%\n3.43%\nSeen homes\n70.32 %\n42.14 %\n23.81 %\n45.42%\nUnseen homes\n31.34%\n14.55%\n11.48%\n19.12%\nAll\n46.85%\n25.76%\n18.41%\n30.34%\nAction\nObjects\nSteps\nMean\nSimulator\nRand. Retrieval\n.473\n.079\n.071\n.207\n100.0%\nMLE\n.735\n.359\n.341\n.478\n19.4%\nPG(LCS)\n.761\n.383\n.364\n.502\n19.0%\nPG(LCS+Sim)\n.751\n.377\n.358\n.495\n22.4%\nPG(LCS+Sim) Seen homes\n.851\n.556\n.528\n.645\n24.6%\nPG(LCS+Sim) Unseen homes\n.680\n.250\n.236\n.389\n20.9%\nTable 2: Left: Accuracy of video-based action classiﬁcation and action-object-object (step or instruction in the program) prediction in\n2-sec clips from our VirtualHome Activity dataset. Right: Video-based program generation.\nMethod\nAction\nObjects\nSteps\nMean\nSimulator (%)\nRand. Sampling\n.226\n.039\n.020\n.095\n0.6%\nRand. Retrieval\n.473\n.079\n.071\n.207\n100.0%\nSkipthoughts\n.642\n.272\n.252\n.389\n100.0%\nMLE\n.777\n.723\n.686\n.729\n38.6%\nPG(LCS)\n.803\n.766\n.732\n.767\n35.5%\nPG(LCS+Sim)\n.806\n.775\n.740\n.774\n39.8%\nMethod\nAction\nObjects\nSteps\nMean\nRand. Sampling\n.106\n.018\n.004\n.043\nRand. Retrieval\n.320\n.037\n.032\n.130\nSkipthoughts\n.469\n.297\n.266\n.344\nMLE\n.497\n.392\n.340\n.410\nPG(LCS)\n.522\n.433\n.387\n.447\nTable 3: Programs from descr.: Accuracy on (left) VirtualHome Act., and (right) ActivityPrograms. We compute the length of longest\ncommon subsequence between a predicted script and GT and divide by max length of the two programs, mimicking IoU for programs. Since\nreal programs are mainly not executable in our simulator due to the lack of implemented actions, we cannot report the executability metric.\n6.2. Program Generation\nWe now evaluate the task of program generation.\nMetrics. We evaluate program induction using a measure\nsimilar to IOU. We compute the longest common subse-\nquence between a GT and a predicted program, where we\nallow gaps between the matching steps, but require their\norder to be correct. We obtain accuracy as the length of the\nsubseq. divided by the max of the two programs’ lengths.\nWe also compute accuracies for actions and objects alone.\nSince LCS does not measure whether the program is valid,\nwe report another metrics that computes the percentage of\ngenerated programs that are executable in our simulator.\nLanguage-based prediction. Since we have descrip-\ntions for all activities, we ﬁrst evaluate how well our model\ntranslates natural language descriptions into programs. We\nreport results on ActivityPrograms (real activities), as well\nas on VirtualHome Activity datasets (where we ﬁrst only\nconsider descriptions, not videos). We compare our models\nto four baselines: 1) random sampling, where we randomly\npick both an action for each step and its arguments, 2) ran-\ndom retrieval, where we randomly pick a program from\nthe training set, 3) skipthoughts, where we embed the de-\nscription using [10, 32], retrieve the closest description from\ntraining set and take its program, 4) our model trained with\nMLE (no RL). Table 3 provides the results. We can see that\nour model outperforms all baselines on both datasets. Our\nRL model that exploits LCS reward outperforms the MLE\nmodel on both metrics (LCS and executability). Our model\nthat uses both rewards slightly decreases the LCS score, but\nsigniﬁcantly improves the executability metrics.\nVideo-based prediction. We also report results on the\nmost challenging task of video-based program generation.\nThe results are shown in Table 2 (right). One can observe\nthat RL training with LCS reward improves the overall ac-\ncuracy over the MLE model (the generated programs are\nmore meaningful given the description\/video), however its\nexecutability score decreases. This is expected: MLE model\ntypically generates shorter programs, which are thus more\nlikely to be executable (an empty program is always exe-\ncutable). A careful balance of both metrics is necessary. RL\nwith both the LCS and the simulator reward improves both\nLCS and the executability metrics over the LCS-only model.\nExecuting programs in VirtualHome. In Fig. 8 we\nshow a few examples of our agent executing programs gen-\nerated from natural descriptions. To understand the quality\nof our simulator as well as the plausibility of our program\nevaluation metrics, we perform a human study. We randomly\nselected 10 examples per level of performance: (a) [0.95−1],\n(b) [0.8 −0.95], (c) [0.65 −0.8], and (d) [0.5 −0.65]. For\neach example we had 5 AMT workers judge the quality of\nthe performed activity in our simulator, given its language\ndescription. Results are shown in Fig. 9. One can notice\n[Walk]\n[Grab] ⟨CUP⟩\n[Open] ⟨FRIDGE⟩\n[Grab] ⟨MILK⟩\nDescription: Get an empty glass. Take milk from refrigerator and open it. Pour milk into glass.\n[SwitchOn] ⟨TV⟩\n[Sit] ⟨SOFA⟩\n[SwitchOff] ⟨TV⟩\n[Put] ⟨COFF.-POT⟩⟨TABLE⟩\nDescription: Go watch TV on the couch. Turn the TV off and grab the coffee pot. Put the coffee pot on the table and go turn the light on.\n[Walk]\n[Grab] ⟨MAGAZINE⟩\n[Sit] ⟨TOILET⟩\n[Walk]\n[Put] ⟨MAGAZINE⟩⟨DESK⟩\nDescription: Look at the clock then get the magazine and use the toilet. When done put the magazine on the table.\n[Walk]\n[Grab] ⟨FACE SOAP⟩\n[Put] ⟨F.SOAP⟩⟨COUNTER⟩\n[SwitchOn] ⟨TOASTER⟩\n[Put] ⟨POT⟩⟨STOVE⟩\nDescription: Take the face soap to the kitchen counter and place it there. Turn toaster on and then switch it off. Place the pot on the stove.\nFigure 8: Our agent executing generated programs from descriptions, in our VirtualHome. Top description is from ActivityPrograms, while\nthe rest are from VirtualHome Activity dataset. Notice that the top agent uses his left to open the fridge and to grab an object since he already\nholds an item in his right. There are also some limitations, for example, in row 3 the agent sits on the toilet fully clothed. Furthermore,\nsometimes the carried item slightly penetrates into the character’s body due to imprecisions of the colliders.\n[0.95-1.00]\n[0.80-0.95]\n[0.65-0.80]\n[0.50-0.65]\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFailed\nMarginal\nAdequate\nGood\nExcellent\nPerfect\nFigure 9: Human evaluation of our agent executing described ac-\ntivities via program generation from text: x axis shows the program\nprediction accuracy, y axis is the human score.\nagreement between our metrics and human scores. Gener-\nally, at perfect performance the simulations got high human\nscores, however, there are examples where this was not the\ncase. This may be due to imperfect animation, an indication\nthat further improvements to our simulator are possible.\nImplications. The high performance of text-based ac-\ntivity animation opens exciting possibilities for the future.\nIt would allow us to replace the more rigid program syn-\nthesis that we used to create our dataset, by having annota-\ntors create these animations directly via natural language or\ncrowd-sourcing scripts from existing text corpora.\n7. Conclusion\nWe collected a large knowledge base of how-to for house-\nhold activities speciﬁcally aimed for robots. Our dataset\ncontains natural language descriptions of activities as well as\nprograms, a formal symbolic representation of activities in\nthe form of a sequence of steps. What makes these programs\nunique is that they contain all the steps necessary to per-\nform an activity. We further introduced VirtualHome, a 3D\nsimulator of household activities, which we used to create\na large video activity dataset with rich ground-truth. We\nproposed a simple model that infers a program from either a\nvideo or a textual description, allowing robots to be “driven”\nby naive users via natural language or video demonstration.\nWe showed examples of agents performing these programs\nin our simulator. There are many exciting avenues going\nforward, for example, training agents to perform tasks from\nvisual observation alone using RL techniques.\nAcknowledgements: We acknowledge partial support from \"La Caixa\" Fel-\nlowship, NSERC COHESA NETGP485577-15, Samsung, DARPA Explainable\nAI (XAI) program and IARPA D17PC00341. The U.S. Government is autho-\nrized to reproduce and distribute reprints for Governmental purposes notwith-\nstanding any copyright annotation thereon. Disclaimer: The views and conclu-\nsions contained herein are those of the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies or endorsements, either expressed\nor implied, of IARPA, DOI\/IBC, or the U.S. Government. We also gratefully\nacknowledge NVIDIA for donating several GPUs used in this research.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/VirtualHome: Simulating Household Activities via Programs.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nVirtualHome: Simulating Household Activities via Programs\n```\n#### 2. 论文摘要\n```\nIn this paper, we are interested in modeling complex activities that occur in\na typical household. We propose to use programs, i.e., sequences of atomic\nactions and interactions, as a high level representation of complex tasks.\nPrograms are interesting because they provide a non-ambiguous representation of\na task, and allow agents to execute them. However, nowadays, there is no\ndatabase providing this type of information. Towards this goal, we first\ncrowd-source programs for a variety of activities that happen in people's\nhomes, via a game-like interface used for teaching kids how to code. Using the\ncollected dataset, we show how we can learn to extract programs directly from\nnatural language descriptions or from videos. We then implement the most common\natomic (inter)actions in the Unity3D game engine, and use our programs to\n\"drive\" an artificial agent to execute tasks in a simulated household\nenvironment. Our VirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and testing of video\nunderstanding models. We further showcase examples of our agent performing\ntasks in our VirtualHome based on language descriptions.\n```\n\n#### 3. 论文全文\n```\nVirtualHome: Simulating Household Activities via Programs\nXavier Puig1∗, Kevin Ra2∗, Marko Boben3∗, Jiaman Li4, Tingwu Wang4,\nSanja Fidler4, Antonio Torralba1\n1MIT\n2McGill University\n3University of Ljubljana\n4University of Toronto\n{xavierpuig,torralba}@csail.mit.edu\nkevin.ra@mail.mcgill.ca\nmarko.boben@fri.uni-lj.si\n{tingwuwang,ljm,fidler}@cs.toronto.edu\nAbstract\nIn this paper, we are interested in modeling complex ac-\ntivities that occur in a typical household. We propose to use\nprograms, i.e., sequences of atomic actions and interactions,\nas a high level representation of complex tasks. Programs\nare interesting because they provide a non-ambiguous rep-\nresentation of a task, and allow agents to execute them.\nHowever, nowadays, there is no database providing this type\nof information. Towards this goal, we ﬁrst crowd-source\nprograms for a variety of activities that happen in people’s\nhomes, via a game-like interface used for teaching kids how\nto code. Using the collected dataset, we show how we can\nlearn to extract programs directly from natural language\ndescriptions or from videos. We then implement the most\ncommon atomic (inter)actions in the Unity3D game engine,\nand use our programs to “drive” an artiﬁcial agent to ex-\necute tasks in a simulated household environment. Our\nVirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and\ntesting of video understanding models. We further showcase\nexamples of our agent performing tasks in our VirtualHome\nbased on language descriptions.\n1. Introduction\nAutonomous agents need to know the sequences of ac-\ntions that need to be performed in order to achieve certain\ngoals. For example, we might want a robot to clean our room,\nmake the bed, or cook dinner. One can deﬁne activities with\nprocedural recipes or programs that describe how one can\naccomplish the task. A program contains a sequence of sim-\nple symbolic instructions, each referencing an atomic action\n(e.g. “sit”) or interaction (e.g. “pick-up object”) and a num-\nber of objects that the action refers to (e.g., “pick-up juice”).\nAssuming that an agent knows how to execute the atomic\nactions, programs provide an effective means of “driving” a\nrobot to perform different, more complex tasks. Programs\n∗Denotes equal contribution\nRobots\nAction:     Work on computer\nDescription:  Turn on your computer and \nsit in front of it. Type on the keyboard, \ngrab the mouse to scroll.\nAction:     Make coffee\nDescription:  Go to the kitchen and swith\non the coffee machine. Wait until it’s \ndone and pour the coffee into a cup.\nAction:     Read a book\nDescription:  Sit down in recliner. Pick up \na novel off of coffee table. Open novel to \nlast read page. Read.\nVirtualHome\nrobot playground\nprogram\nvideo\nKnowledge Base of Household Tasks\nFigure 1: We ﬁrst crowdsource a large knowledge base of household\ntasks, (top). Each task has a high level name, and a natural language in-\nstruction. We then collect “programs” for these tasks, (middle left), where\nthe annotators “translate” the instruction into simple code. We implement\nthe most frequent (inter)actions in a 3D simulator, called VirtualHouse,\nallowing us to drive an agent to execute tasks deﬁned by programs. We pro-\npose methods to generate programs automatically from text (top) and video\n(bottom), thus driving an agent via language and a video demonstration.\ncan also be used as an internal representation of an activity\nshown in a video or described by a human (or another agent).\nOur goal in this paper is to automatically generate programs\nfrom natural language descriptions, as well as from video\ndemonstrations, potentially allowing naive users to teach\ntheir robot a wide variety of novel tasks.\nTowards this goal, one important missing piece is the\nlack of a database describing activities composed of multi-\nple steps. We ﬁrst crowdsource common-sense information\nabout typical activities that happen in people’s homes, form-\ning the natural language know-how of how these activities\nare performed. We then adapt the Scratch [1] interface used\nfor teaching kids how to code in order to collect programs\nthat formalize the activity as described in the knowledge\nbase. Note that these programs include all the steps required\nfor the robot to accomplish a task, even those that are not\nmentioned in the language descriptions. We then implement\nthe most common atomic (inter)actions in the Unity3D game\nengine, such as pick-up, switch on\/off, sit, stand-up. By ex-\nploiting the physics, navigation and kinematic models in the\narXiv:1806.07011v1  [cs.CV]  19 Jun 2018\ngame engine we enable an artiﬁcial agent to execute these\nprograms in a simulated household environment.\nWe ﬁrst introduce our data collection effort and the pro-\ngram based representation of activities. In Sec. 5 we show\nhow we can learn to automatically translate natural language\ninstructions of activities into programs. In Sec. 4 we intro-\nduce the VirtualHome simulator that allows us to create a\nlarge activity video dataset with rich ground-truth by using\nprograms to drive an agent in a synthetic world. Finally,\nwe use the synthetic videos to train a system to translate\nvideos of activities into the program being executed by the\nagent. Our VirtualHome opens an important “playground”\nfor both vision and robotics, allowing agents to exploit lan-\nguage and visual demonstration to execute novel activities\nin a simulated environment. Our data is available online:\nhttp:\/\/virtual-home.org\/.\n2. Related Work\nActions as programs. A few works have deﬁned activi-\nties as programs. In [30], the authors detect objects and ac-\ntions in cooking videos and generate an “action plan” using\na probabilistic grammar. By generating the plan, the robots\nwere able to execute complex actions by simply watching\nvideos. These authors further collected a tree bank of action\nplans from annotated cooking videos [29], creating a knowl-\nedge base of actions as programs for cooking. [18] tried to\ntranslate cooking recipes into action plans using an MRF.\n[23, 3] also argued for actions as a sequence of atomic steps.\nThey aligned YouTube how-to videos with their narrations\nin order to parse videos into such programs. Most of these\nworks were limited to either a small set of activities, or to\na narrow domain (cooking). We go beyond this by creating\na knowledge base about an exhaustive set of activities and\ntasks that people do in their homes.\n[25] crowd-sourced scripts of people’s actions at home\nin the form of natural language. These were mostly com-\nprised of one or two sentences describing a short sequence\nof actions. While this is valuable information, language is\nvery versatile and thus hard to convert into a usable program\non a robot. We show how to do this in our work.\nCode generation. There is increased interest in generat-\ning and interpreting source code [13]. Work most relevant\nto ours produces code given natural language inputs. [4]\nretrieves code snippets from Stackoverﬂow based on lan-\nguage queries. Given a sentence describing conditions, [19]\nproduces If-This-Then-That code. [14] generates a program\nspecifying the logic of a card game given a short descrip-\ntion of the rules. In [9], the authors inferred programs to\nanswer visual questions about images. Our work differs in\nthe domain, and works with text or video as input.\nRobotics. A subﬁeld of robotics aims at teaching robots\nto follow instructions provided in natural language by a hu-\nman tutor. However, most of the existing literature deals\nwith a constrained problem, for example, they learn to trans-\nlate navigational instructions into a sequence of robotic ac-\ntions [27, 15, 12, 16]. These instructions are typically sim-\npler as they directly mention what to do next, and the action\nspace is small. This is not the case in our work which also\nconsiders interactions with objects, and everyday activities\nwhich are typically far more complex.\nSimulation. Simulations using game engines have been\ndeveloped to facilitate training visual models for autonomous\ndriving [8, 21, 7], quadcopter ﬂying [24], or other robotic\ntasks [5]. Recent works have focused on simulating indoor\nenvironments, allowing for target-driven indoor navigation\nor interactive question answering [11, 28, 6, 22]. A few of\nthese works [11, 6] include actionable objects, allowing to\ninteract and change the environment. Our work focuses on\nsimulating a wide range of human actions, both in terms\nof objects interactions and human poses, which allows to\nsimulate common activities. We are not aware of simulators\nat the scale of objects and actions in a home, like ours. Lastly,\nwe give credit to the popular game Sims which we draw our\ninspiration from. Sims is a strategic video game mimicking\ndaily household activities. Unfortunately, the source of the\ngame is not public and thus cannot be used for our purpose.\n3. KB of Household Activities for Robots\nOur goal is to build a large repository of common activi-\nties and tasks that we perform in our households in our daily\nlives. These tasks can include simple actions like “turning on\nTV\" or complex ones such as “make coffee with milk\". What\nmakes our effort unique is that we are interested in collect-\ning this information for robots. Unlike humans, robots need\nmore direct instructions. For example, in order to “watch\ntv”, one might describe it (to a human) as “Switch on the\ntelevision, and watch it from the sofa”. Here, the actions\n“grab remote control” and “sit\/lie on sofa” have been omit-\nted, since they are part of the commonsense knowledge that\nhumans have. In our work, we aim to collect all the steps\nrequired for a robot to successfully execute a task, including\nthe commonsense steps. In particular, we want to collect\nprograms that fully describe activities.\nDescribing actions as programs has the advantage that it\nprovides a clear and non-ambiguous description of all the\nsteps needed to complete a task. Such programs can then\nbe used to instruct a robot or a virtual character. Programs\ncan also be used as a representation of a complex task that\ninvolves a number of simpler actions, providing a way to\nunderstand and compare activities and goals.\n3.1. Data Collection\nIn this section, we describe our dataset collection using\ncrowdsourcing. Describing actions as programs can be a\nchallenging task as most annotators have no programing\nexperience. We split the data collection effort in two parts.\nRGB\npose\nclass seg.\ninst. seg\ndepth\nﬂow\nFigure 2: VirtualHome Activity Dataset is a video dataset of composite activities created with our simulator. We start by generating\nprograms using a simple probabilistic grammar. We animate each program in VirtualHome by randomizing the selection of homes, agents,\ncameras, as well as the placement of a subset of the objects, the initial location of the agent, the speed of the actions, and choice of objects\nfor interactions. Each program is shown to an annotator who is asked to describe it in natural language (top row). Videos have ground-truth:\n(second row) time-stamp for each atomic action, (bottom) 2D and 3D pose, class and object instance segmentation, depth and optical ﬂow.\nIn the ﬁrst part, we ask AMT workers to provide verbal\ndescriptions of daily household activities. In particular, each\nworker is asked to come up with a common activity\/task,\ngive it a high level name, eg “make coffee”, and describe it\nin detail. In order to cover a wide spectrum of activities we\npre-speciﬁed in which scene the activity should start. Scenes\nwere selected randomly from a list of 8 scenes (living room,\nkitchen, dining room, bedroom, kids bedroom, bathroom,\nentrance hall, and home ofﬁce). An example of a described\nactivity is shown in Fig. 3. Note that these descriptions may\nlikely omit the commonsense steps, as they were written by\n“naive” workers that were describing these activities as they\nwould to a (human) friend.\nIn the second stage, we showed the collected descrip-\ntions to the AMT workers and asked them to translate these\ndescriptions into programs using a graphical programing lan-\nguage. We told them to produce a program that will “drive” a\nrobot to successfully accomplish the described activity. Our\ninterface builds on top of MIT’s Scratch project [1] designed\nto teach young children to write symbolic code. We found\nthat workers were capable of quickly learning to produce\nuseful programs by providing them with a carefully designed\ntutorial. Fig. 3 shows a snapshot of the programing interface.\nFinally, we asked more qualiﬁed workers hired via Upwork\ncrowdsourcing platform to double check the collected data.\nWorkers had to compose a program by composing a se-\nquence of steps. Each instruction is a Scratch block from a\npredeﬁned list of 77 possible blocks compiled by analyzing\nthe frequency of verbs in the collected descriptions. Each\nstep in the program is deﬁned by a block. A block deﬁnes a\nsyntactic frame with an action and a list of arguments (e.g.,\nthe block walk requires one argument to specify the desti-\nnation, Fig. 3.c). To simplify the search for blocks they are\norganized according to 9 broad action categories (Fig. 3.b).\nWe required that the program contains all the steps, even\nthose not explicitly mentioned in the description, but that\ncould be inferred from common-sense. Fig. 3.d shows an\nexample of a program. We also allowed annotators to use\na “special” block for missing actions, where the step can be\nwritten as free-form text. Programs using this special block\nwill not be used in the rest of the paper, but allowed us in\nidentifying new blocks that needed to be added.\nMore precisely, step t in the program can be written as:\nstept = [actiont] ⟨objectt,1⟩(idt,1) ... ⟨objectt,n⟩(idt,n)\nHere, id is an unique identiﬁer (counter) of an object and\nhelps in disambiguating different instances of objects that\nbelong to the same class. An example of a program for\n“watch tv” would be:\nstep1 = [Walk] ⟨TELEVISION⟩(1)\nstep2 = [SwitchOn] ⟨TELEVISION⟩(1)\nstep3 = [Walk] ⟨SOFA⟩(1)\nstep4 = [Sit] ⟨SOFA⟩(1)\nstep5 = [Watch] ⟨TELEVISION⟩(1)\nHere, the programs deﬁnes that the television in steps 1, 2\nand 5 refer to the same object instance.\n3.2. Dataset Analysis\nIn the ﬁrst part we collected 1814 descriptions. From\nthose, we were able to collect programs for 1703 descrip-\ntions. Some of the programs contained several “special\nblocks” for missing actions, which we remove, resulting\nin 1257 programs. We ﬁnally selected a set of tasks and\nasked workers to write programs for them, obtaining 1564\nadditional programs. The resulting 2821 programs form our\nActivityPrograms dataset. On average, the collected descrip-\ntions have 3.2 sentences and 21.9 words, and the resulting\nprograms have 11.6 steps on average. The dataset statistics\nare summarized in Table 1.a.\nAction name: \nThrow away newspaper\nDescription: \nTake the newspaper \non the living room table \nand toss it.\na)\nb)\nc)\nd)\nFigure 3: a) Description provided by a worker. b) User interface showing\nthe list of block categories and 4 example blocks, c) Example of composition\nof a block by adding the arguments. Each block is like a Lego piece where\nthe user can drop arguments inside and attach one block to another. d) Final\nprogram corresponding to the description from (a).\nAction counts\nWalk\nGrab\nFind\nPutback\nPutobjback\nLookat\nOpen\nSwitchon\nClose\nSit\nSwitchoff\nPointat\nPour\nWipe\nRinse\nWash\nPull\nGreet\nScrub\nRead\nPush\nTouch\nType\nTurnto\nCrawl\nFold\nPlugin\nWatch\nLie\nCover\nLift\nWrite\nPuton\nPutoff\nEat\nPlugout\nStandup\nSleep\nEnter\nSqueeze\nDrink\nShake\nLeave\nDrop\nUncover\nSpread\nStretch\nRun\nDance\nCut\n0\n500\n1000\n1500\n2000\n2500\nObject counts\nPlate\nChair\nCabinet\nFaucet\nBed\nBook\nCloset\nKitchen\nVacuum cleaner\nLiving room\nTelevision\nBedroom\nComputer\nRag\nTable\nHome office\nFridge\nFood\nSink\nBathroom\nToy\nTowel\nCouch\nChild\nNone\nRemote control\nBowl\nShoes\nDining room\nPillow\nWater\nCleaning solution\nSoap\nClothesshirt\nLaptop\nClothespants\nClothesdress\nPaper\nBlanket\nBroom\nSpoon\nKeyboard\nSponge\nFork\nDoor\nEntrance hall\nCup\nDresser\nWoman\nArms\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\na)\nb)\n    'Wa\nc\nFigure 4: a) Counts of actions in our ActivityPrograms\ndataset, b) object counts (zoom to read)\nThe dataset covers 75 atomic actions and 308 objects,\nmaking 2709 unique steps. Fig. 4.a shows a histogram of\nthe 50 most common actions appearing in the dataset, and,\nFig. 4.b, the 50 most common objects.\nOur dataset contains activities with several examples, and\nwe analyze their diversity by comparing their programs. Ta-\nble 1.b analyzes 4 selected activities. We compute their\nsimilarities as the average length of the longest common\nsubsequences computed between all pairs of programs.\nWe can also measure distances between activities by\nmeasuring the distance between programs. The similarity\nbetween two programs is measured as the length of their\nlongest common subsequence of instructions divided by the\nlength of the longest program. Table 1.c. shows the similar-\nity matrix (sorted to better show the block diagonal structure)\nbetween different activities in our dataset.\nCompleteness of programs. We analyze whether the\ncollected programs contain all the necessary steps to execute\nthe given task. We sample 100 collected programs and ask\n5 AMT workers to rate whether the program is complete,\nmissing minor steps (sitting in a chair before walking to-\nwards it) or important steps (ﬁlling a glass before drinking).\nResults show that 64% of the programs are complete, 28%\nare missing minor steps and 8% are missing crucial steps.\n4. VirtualHome: Simulator of Household Tasks\nThe main motivation behind using programs to represent\nactivities is to “drive” robots to perform tasks by having them\nexecuting these programs. As a proxy, we here use programs\nto drive characters in a simulated 3D environment. Simula-\ntions are useful as they deﬁne a playground for “robots”, an\nenvironment where artiﬁcial agents can be taught to perform\ntasks. Here, we focus on building the simulator, and leave\nlearning inside the simulator to future work. In particular,\nwe will assume the agent has access to all 3D and semantic\ninformation about the environment, as well as to manually\ndeﬁned animations. Our focus will be to show that programs\nrepresent a good way of instructing such agents. Further-\nmore, our simulator will allow us to generate a large-scale\nvideo dataset of complex activities that is rich and diverse.\nWe can create such a dataset by simply recording the agent\nexecuting programs in the simulator. The simulator then pro-\nvides us with dense ground-truth information, eg semantic\nsegmentation, depth, pose, etc. Fig. 2 showcases this dataset.\nWe implemented our VirtualHome simulator using the\nUnity3D game engine which allows us to exploit its kine-\nmatic, physics and navigation models, as well as user-\ncontributed 3D models available through Unity’s Assets\nstore. We obtained six furnished homes and 4 rigged hu-\nmanoid models from the web. On average, each home con-\ntains 357 object instances (86 per room). We collected ob-\njects from additional 30 object classes that appear in our\ncollected programs yet are not available in the package, via\nthe 3D warehouse 1. To ensure visual diversity, we collected\nat least 3 different models per class. The apartments and\nagents are shown in Fig. 5 and Fig. 6.\n4.1. Animating Programs in VirtualHome\nEvery step in the program requires us to animate the cor-\nresponding (inter)action in our virtual environment. We thus\nneed to both, determine which object in the home (which we\nrefer to as the game object) the step requires as well as prop-\nerly animating the action. To get the former we need to solve\nan optimization problem by taking into account all steps in\nthe program and ﬁnding a feasible path. For example, if the\nprogram requires the agent to switch on a computer and type\non a keyboard, ideally the agent would type on the keyboard\nnext to the chosen computer and not navigate to another key-\nboard attached to a different computer in possibly a different\nroom. We now describe our simulator in more detail.\nAnimating atomic actions. There is a huge variety and\nnumber of atomic actions that appear in the collected pro-\ngrams, as can be seen in Fig. 4. We implemented the 12 most\nfrequent ones: walk\/run, grab, switch-on\/off, open\/close,\nplace, look-at, sit\/standup, touch. Note that there is a large\nvariability in how an action is performed depending on to\nwhich object it is applied to (e.g., opening a fridge is dif-\nferent than opening a drawer). We use Unity’s NavMesh\nframework for navigation (path planner to avoid obstacles).\nFor each action we compute the agent’s target pose and\nanimate the action using RootMotion FinalIK inverse kine-\nmatics package. We further animate certain objects the agent\ninteracts with, e.g., we shake a coffee maker, animate toast\nin a toaster, show a (random) photo on a computer or TV\n1https:\/\/3dwarehouse.sketchup.com\nDataset\n# prog. avg # steps avg # sent. avg # words\nActivityProg.\n2821\n11.6\n3.2\n21.9\nSyntheticProg. 5193\n9.6\n3.4\n20.5\n(a)\nAction\n# Prog. LCS Norm. LCS\nMake coffee\n69\n4.56\n0.26\nFold laundry\n11\n1.29\n0.08\nWatch TV\n128\n3.65\n0.40\nClean\n42\n0.76\n0.04\n(b)\n(c)\nBrowse internet\nWork\nUser computer\nCheck email\nPay bills\nStudy\nEat dinner\nSet up table\nWash dishes with dishwasher\nWash dishes by hand\nWash dishes\nBrush teeth\nWash face\nTake shower\nTake bath\nWash hands\nMake bed\nTake nap\nGo to sleep\nSleep\nTurn off light\nTuck kids in bed\nWatch movie\nChange TV channel\nWatch TV\nTable 1: (a) We analyze programs and natural language descriptions for both, real activities in ActivityPrograms (Sec. 3), and synthesized\nprograms (with real descr.). (b) ActivityPrograms: Analyzing diversity in the same activity, by computing similarities across all pairs of the\ncollected programs. “LCS\" denotes longest common subsequence. For “norm.LCS\", we normalize by max length of the two programs. (c)\nshows the similarity matrix (sorted to better show the block diagonal structure) between different activities in our dataset.\nFigure 5: 3D households in our VirtualHome. Notice the diversity in room and object layout\nand appearance. Each home has on average 357 objects. First 4 scenes are used for training,\nthe ﬁfth is also used in val, and all scenes are used when testing our video-to-script model.\nmale 1\nfemale 1\nmale 2\nfemale 2\nFigure 6: Agents in VirtualHome. We use\nmale 1 and female 1 in train., and all agents\nwhen testing our video-to-program model.\nscreen, light up a burner on a stove, and light up the lamps in\nthe room, when these objects are switched on by the agent.\nPreparing the Scene. While every 3D home already con-\ntains many objects, the programs may still mention objects\nthat are not present in the scene. To deal with this, we ﬁrst\n“set” the scene by placing all missing objects that a program\nrefers to in the home, before we try to execute the program.\nTo be able to prepare a scene in a plausible way, we collect a\nknowledge base of possible object locations. The annotator\nis shown the class name and selects a list of other objects\n(including ﬂoor, wall) that are likely to support it.\nExecuting a Program. To animate a program we need\nﬁrst to create a mapping between the objects in the program\nand the corresponding instances inside the virtual simulator.\nFurthermore, for each step in the program, we also need to\ncompute the interaction position of the agent with respect to\nan object, and any additional information needed to animate\nthe action (e.g., which hand to use, speed). We build a tree\nof all possibilities of assigning game objects to objects in the\nprogram, along with all interaction positions and attributes.\nTo traverse the tree of possible states we use backtracking\nand stop as soon as a state executing the last step is found.\nSince the number of possible object mappings for each step is\nsmall, and we can prune the number of interaction positions\nto a few, our optimization runs in a few seconds, on average.\nAnimation. We place 6-9 static cameras in each room,\n26 per home on average. During recording, we switch be-\ntween cameras based on agent’s visibility. In particular, we\nrandomly select a camera which sees the agent, and keep it\nuntil the agent is visible and within allowed distance. For\nagent-object interaction we also try to select a camera and\nadjust its ﬁeld of view to enhance the visibility of the inter-\naction. We further randomize the position, angle and ﬁeld\nof view of each camera. Randomization is important when\ncreating a dataset to ensure diversity of the ﬁnal video data.\nVirtualHome Activity dataset. Since the programs in\nActivityPrograms represent real activities that happen in\nhouseholds, they contain signiﬁcant variability in actions\nand objects that appear in steps. While our ultimate aim is\nto be able to animate all these actions in our simulator, our\ncurrent efforts only support the top 12 most frequent actions.\nWe thus create another dataset that contains programs con-\ntaining only these actions in their steps. The creation of this\ndataset is explained below.\nWe synthesized 5,193 programs using a simple probabilis-\ntic grammar, and had each one described in natural language\nby a human annotator. Although these programs were not\ngiven by annotators, they produced reasonable activities, cre-\nating a much larger dataset of paired descriptions-programs\nat a fraction of the cost. We then animated each program\nin our simulator, and automatically generated ground-truth\nwhich allows us to train and evaluate our video models.\nAs can be seen from Table 1, descriptions in VirtualHome\nActivity dataset are of comparable length. However, the\nvocabulary here was biased towards that used in programs.\nWe animate the programs as described above, by random-\nizing the selection of home, an agent, cameras, placement\nof a subset of objects, initial location of the agent, speed\nof the actions, and choice of objects for interactions. We\nbuild on top of [2] to automatically generate groundtruth: 1)\ntime-stamp of each step to video, 2) agent’s 2D\/3D pose, 3)\nclass and instance segmentation, 4) depth, 5) optical ﬂow, 6)\ncamera parameters. Example of data is shown in Fig. 2.\n5. From Videos and Descriptions to Programs\nWe introduce a novel task using our dataset. In particular,\nwe aim to generate a program for the activity from either a\nnatural language description or from a video demonstration.\nWe treat the task of transcribing an input (description or\nvideo) into a program as a translation problem. We adapt the\nseq2seq model [26] for our task, and train it with Reinforce-\nment Learning that exploits the reward from the simulator.\nOur model consists of an RNN encoder that encodes\nthe input sequence into a hidden vector representation, and\nanother RNN acting as a decoder, generating one step of the\nprogram at a time. We use LSTM with 100-dim hidden states\nas our encoder. At each step t, our RNN decoder decodes a\nstep which takes the form of eq. (1). Let xt denote an input\nvector to our RNN decoder at step t, and let ht be the hidden\nstate. Here, ht is computed as in the standard LSTM using\ntanh as the non-linearity. Let ai be a one-hot encoding of an\naction i, and oi a one-hot encoding of an object. We compute\nthe probability pt\ni of an instruction i at step t as:\n˜ai = Waai, ˜oi,n = Wooi,n, vi = mean(˜ai, ˜oi,1, ..., ˜oi,n)\npt\ni = softmaxi( vi\n∥vi∥\nT\n· Wv(ht∥xatt\nt )) (1)\nwhere Wa and Wo and Wv are learnable matrices, and vi\ndenotes an embedding of an instruction. Note that here, n is\neither 1 or 2 (our instructions have at most two objects).\nThe input vector xt concatenates multiple features. In par-\nticular, we use the embedding v of the step with the highest\nprobability from the previous time instance of the decoder.\nFollowing [26], we further use the attention mechanism over\nthe encoder’s states to get another feature xatt\nt . In particular:\nαt\nj = softmaxj(vT \u0000Watt (ht∥hj\nenc)\n\u0001\u0001\n(2)\nxatt\nt\n=\nX\nj\nαt\njhj\nenc\n(3)\nwhere Watt, v are learnable parameters. Our full model is\nvisualized in Fig. 7.\nLearning and inference. Our goal is to generate pro-\ngrams that are both close to the ground-truth programs in\nterms of their LCS (longest common subsequence) and are\nalso executable by our renderer. To that end, we train our\nmodel in two phases. Firstly, we pre-train the model using\ncross-entropy loss at each time step of the RNN decoder.\nWe follow the typical training strategy where we make a\nprediction at each time instance but feed in the ground-truth\nstep to the next time instance. We use the word2vec [17]\nembeddings for matrices Wa and Wo.\nIn the second stage, we treat program generation as an\nReinforcement Learning problem, where the agent is learn-\ning a policy that generates steps to compose a program.\nWe follow [20], and use policy gradient optimization to\ntrain the model, using the greedy policy as the baseline es-\ntimator. We exploit two different kinds of reward r(ws, g)\nfor RL training, where ws denotes the sampled program,\nand g the ground-truth program. To ensure that the gener-\nated program is semantically correct (follows the descrip-\ntion\/video), we use the normalized LCS metric (length of the\nlongest common subsequence) between the two programs\nas our ﬁrst reward rLCS(ws, g). The second reward comes\nfrom our simulator, and measures whether the generated\nprogram is executable or not. This reward, rsim(ws), is a\nsimple binary value. We carefully balance the total reward\nas, r(ws, g) = rLCS(ws, g) + 0.1 · rsim(ws).\nSo far we did not specify the input to the RNN encoder.\nOur model accepts either a language description or a video.\nTextual Description.\nTo encode a textual description our\nRNN encoder gets as input the word2vec [17] embedding of\nthe word in the description at each time instance.\nVideo.\nTo generate programs from videos, we partition\neach video into 2-second clips and train a model to predict\nthe step at the middle frame. We use DilatedNet to obtain the\nsemantic segmentation of each frame and use the Temporal\nRelation Network [31] with 4-frame relations to predict the\nembedding of an instruction (action+object+object). We use\nthis embedding to obtain the likelihood of each instruction.\nThe prediction at each clip is used as input to the RNN\nencoder for program generation.\n6. Experiments\nIn our experiments we exploit both of our datasets: Activ-\nityPrograms containing descriptions and programs for real\nactivities, and VirtualHome Activity dataset that contains\nsynthesized programs, yet natural descriptions to describe\nthem. VirtualHome Activity dataset further contains videos\nanimating the programs.\n6.1. Step (Instruction) Classiﬁcation from Video\nWe ﬁrst evaluate our model for the task of video-based\naction and action-object-object (step\/instruction in the pro-\ngram) classiﬁcation. Here, we partition each video in 2-sec\nclips, and use the clip-based TRN to perform classiﬁcation.\nWe compute performance as the mean per-class accuracy\nacross all 2-sec clips in test. To better understand the gen-\neralization properties of the video-based models, we further\ndivide the test set into videos recorded in homes seen at\ntrain time, and videos in homes not seen at train time. We\nreport the results in Table 2 (left). To set the lower bound,\nwe also report a simple random retrieval baseline, in which\na step is randomly retrieved from the training set. We can\nsee that our model performs signiﬁcantly better. However,\na large number of actions and objects of interest, makes the\nprediction task challenging for the model.\nFigure 7: Our encoder-decoder LSTM for generating programs from natural language descriptions or videos.\nAction\nObjects\nSteps\nMean\nRand. Retrieval\n8.30%\n1.50%\n0.51%\n3.43%\nSeen homes\n70.32 %\n42.14 %\n23.81 %\n45.42%\nUnseen homes\n31.34%\n14.55%\n11.48%\n19.12%\nAll\n46.85%\n25.76%\n18.41%\n30.34%\nAction\nObjects\nSteps\nMean\nSimulator\nRand. Retrieval\n.473\n.079\n.071\n.207\n100.0%\nMLE\n.735\n.359\n.341\n.478\n19.4%\nPG(LCS)\n.761\n.383\n.364\n.502\n19.0%\nPG(LCS+Sim)\n.751\n.377\n.358\n.495\n22.4%\nPG(LCS+Sim) Seen homes\n.851\n.556\n.528\n.645\n24.6%\nPG(LCS+Sim) Unseen homes\n.680\n.250\n.236\n.389\n20.9%\nTable 2: Left: Accuracy of video-based action classiﬁcation and action-object-object (step or instruction in the program) prediction in\n2-sec clips from our VirtualHome Activity dataset. Right: Video-based program generation.\nMethod\nAction\nObjects\nSteps\nMean\nSimulator (%)\nRand. Sampling\n.226\n.039\n.020\n.095\n0.6%\nRand. Retrieval\n.473\n.079\n.071\n.207\n100.0%\nSkipthoughts\n.642\n.272\n.252\n.389\n100.0%\nMLE\n.777\n.723\n.686\n.729\n38.6%\nPG(LCS)\n.803\n.766\n.732\n.767\n35.5%\nPG(LCS+Sim)\n.806\n.775\n.740\n.774\n39.8%\nMethod\nAction\nObjects\nSteps\nMean\nRand. Sampling\n.106\n.018\n.004\n.043\nRand. Retrieval\n.320\n.037\n.032\n.130\nSkipthoughts\n.469\n.297\n.266\n.344\nMLE\n.497\n.392\n.340\n.410\nPG(LCS)\n.522\n.433\n.387\n.447\nTable 3: Programs from descr.: Accuracy on (left) VirtualHome Act., and (right) ActivityPrograms. We compute the length of longest\ncommon subsequence between a predicted script and GT and divide by max length of the two programs, mimicking IoU for programs. Since\nreal programs are mainly not executable in our simulator due to the lack of implemented actions, we cannot report the executability metric.\n6.2. Program Generation\nWe now evaluate the task of program generation.\nMetrics. We evaluate program induction using a measure\nsimilar to IOU. We compute the longest common subse-\nquence between a GT and a predicted program, where we\nallow gaps between the matching steps, but require their\norder to be correct. We obtain accuracy as the length of the\nsubseq. divided by the max of the two programs’ lengths.\nWe also compute accuracies for actions and objects alone.\nSince LCS does not measure whether the program is valid,\nwe report another metrics that computes the percentage of\ngenerated programs that are executable in our simulator.\nLanguage-based prediction. Since we have descrip-\ntions for all activities, we ﬁrst evaluate how well our model\ntranslates natural language descriptions into programs. We\nreport results on ActivityPrograms (real activities), as well\nas on VirtualHome Activity datasets (where we ﬁrst only\nconsider descriptions, not videos). We compare our models\nto four baselines: 1) random sampling, where we randomly\npick both an action for each step and its arguments, 2) ran-\ndom retrieval, where we randomly pick a program from\nthe training set, 3) skipthoughts, where we embed the de-\nscription using [10, 32], retrieve the closest description from\ntraining set and take its program, 4) our model trained with\nMLE (no RL). Table 3 provides the results. We can see that\nour model outperforms all baselines on both datasets. Our\nRL model that exploits LCS reward outperforms the MLE\nmodel on both metrics (LCS and executability). Our model\nthat uses both rewards slightly decreases the LCS score, but\nsigniﬁcantly improves the executability metrics.\nVideo-based prediction. We also report results on the\nmost challenging task of video-based program generation.\nThe results are shown in Table 2 (right). One can observe\nthat RL training with LCS reward improves the overall ac-\ncuracy over the MLE model (the generated programs are\nmore meaningful given the description\/video), however its\nexecutability score decreases. This is expected: MLE model\ntypically generates shorter programs, which are thus more\nlikely to be executable (an empty program is always exe-\ncutable). A careful balance of both metrics is necessary. RL\nwith both the LCS and the simulator reward improves both\nLCS and the executability metrics over the LCS-only model.\nExecuting programs in VirtualHome. In Fig. 8 we\nshow a few examples of our agent executing programs gen-\nerated from natural descriptions. To understand the quality\nof our simulator as well as the plausibility of our program\nevaluation metrics, we perform a human study. We randomly\nselected 10 examples per level of performance: (a) [0.95−1],\n(b) [0.8 −0.95], (c) [0.65 −0.8], and (d) [0.5 −0.65]. For\neach example we had 5 AMT workers judge the quality of\nthe performed activity in our simulator, given its language\ndescription. Results are shown in Fig. 9. One can notice\n[Walk]\n[Grab] ⟨CUP⟩\n[Open] ⟨FRIDGE⟩\n[Grab] ⟨MILK⟩\nDescription: Get an empty glass. Take milk from refrigerator and open it. Pour milk into glass.\n[SwitchOn] ⟨TV⟩\n[Sit] ⟨SOFA⟩\n[SwitchOff] ⟨TV⟩\n[Put] ⟨COFF.-POT⟩⟨TABLE⟩\nDescription: Go watch TV on the couch. Turn the TV off and grab the coffee pot. Put the coffee pot on the table and go turn the light on.\n[Walk]\n[Grab] ⟨MAGAZINE⟩\n[Sit] ⟨TOILET⟩\n[Walk]\n[Put] ⟨MAGAZINE⟩⟨DESK⟩\nDescription: Look at the clock then get the magazine and use the toilet. When done put the magazine on the table.\n[Walk]\n[Grab] ⟨FACE SOAP⟩\n[Put] ⟨F.SOAP⟩⟨COUNTER⟩\n[SwitchOn] ⟨TOASTER⟩\n[Put] ⟨POT⟩⟨STOVE⟩\nDescription: Take the face soap to the kitchen counter and place it there. Turn toaster on and then switch it off. Place the pot on the stove.\nFigure 8: Our agent executing generated programs from descriptions, in our VirtualHome. Top description is from ActivityPrograms, while\nthe rest are from VirtualHome Activity dataset. Notice that the top agent uses his left to open the fridge and to grab an object since he already\nholds an item in his right. There are also some limitations, for example, in row 3 the agent sits on the toilet fully clothed. Furthermore,\nsometimes the carried item slightly penetrates into the character’s body due to imprecisions of the colliders.\n[0.95-1.00]\n[0.80-0.95]\n[0.65-0.80]\n[0.50-0.65]\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFailed\nMarginal\nAdequate\nGood\nExcellent\nPerfect\nFigure 9: Human evaluation of our agent executing described ac-\ntivities via program generation from text: x axis shows the program\nprediction accuracy, y axis is the human score.\nagreement between our metrics and human scores. Gener-\nally, at perfect performance the simulations got high human\nscores, however, there are examples where this was not the\ncase. This may be due to imperfect animation, an indication\nthat further improvements to our simulator are possible.\nImplications. The high performance of text-based ac-\ntivity animation opens exciting possibilities for the future.\nIt would allow us to replace the more rigid program syn-\nthesis that we used to create our dataset, by having annota-\ntors create these animations directly via natural language or\ncrowd-sourcing scripts from existing text corpora.\n7. Conclusion\nWe collected a large knowledge base of how-to for house-\nhold activities speciﬁcally aimed for robots. Our dataset\ncontains natural language descriptions of activities as well as\nprograms, a formal symbolic representation of activities in\nthe form of a sequence of steps. What makes these programs\nunique is that they contain all the steps necessary to per-\nform an activity. We further introduced VirtualHome, a 3D\nsimulator of household activities, which we used to create\na large video activity dataset with rich ground-truth. We\nproposed a simple model that infers a program from either a\nvideo or a textual description, allowing robots to be “driven”\nby naive users via natural language or video demonstration.\nWe showed examples of agents performing these programs\nin our simulator. There are many exciting avenues going\nforward, for example, training agents to perform tasks from\nvisual observation alone using RL techniques.\nAcknowledgements: We acknowledge partial support from \"La Caixa\" Fel-\nlowship, NSERC COHESA NETGP485577-15, Samsung, DARPA Explainable\nAI (XAI) program and IARPA D17PC00341. The U.S. Government is autho-\nrized to reproduce and distribute reprints for Governmental purposes notwith-\nstanding any copyright annotation thereon. Disclaimer: The views and conclu-\nsions contained herein are those of the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies or endorsements, either expressed\nor implied, of IARPA, DOI\/IBC, or the U.S. Government. We also gratefully\nacknowledge NVIDIA for donating several GPUs used in this research.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | VirtualHome：通过程序模拟家庭活动\n\n## 📌 背景痛点\/本文动机\n随着人工智能和机器人技术的发展，让机器人执行复杂的家庭活动成为可能。然而，如何有效地描述和执行这些活动仍然是一个挑战。本文提出了一个名为 VirtualHome 的模拟器，旨在通过程序来模拟家庭活动，从而为机器人执行复杂任务提供一种新的方法。\n\n## 🚀 核心方法\n💡 创新点1：构建家庭活动知识库\n本文首先通过众包的方式收集了大量的家庭活动描述，并将其转化为程序形式。这些程序包含了执行任务所需的全部步骤，包括一些常识性步骤，从而为机器人提供了清晰的执行指南。\n\n💡 创新点2：开发 VirtualHome 模拟器\n本文开发了一个名为 VirtualHome 的 3D 模拟器，可以模拟家庭环境中的各种活动。通过将程序输入到 VirtualHome 中，可以生成丰富的活动视频数据集，并用于训练和测试视频理解模型。\n\n💡 创新点3：从文本和视频中生成程序\n本文提出了一个基于 seq2seq 模型的方法，可以从自然语言描述或视频演示中自动生成程序。这使得机器人可以通过自然语言或视频演示来学习执行新的任务。\n\n## 📈 实验结果\n本文在 VirtualHome 模拟器上进行了实验，结果表明，从文本和视频中生成的程序可以有效地驱动机器人执行各种家庭活动。此外，本文还进行了一项人类研究，结果表明，生成的程序与人类对活动的理解具有较高的相关性。\n\n## 💬 可借鉴之处\n本文提出的 VirtualHome 模拟器和程序生成方法为机器人执行复杂任务提供了一种新的思路。此外，本文收集的家庭活动知识库和视频数据集也为相关研究提供了宝贵的资源。","llm_summary_res_status":200}
{"title":"EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents","authors":"Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal","summary":"Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,\na novel framework to address this question. We first prompt an LLM to generate\ntraining environments by giving it the task description and simulator\nobjectives that the agents should learn and then asking it to generate a set of\nenvironment configurations (e.g., different terrains, items initially given to\nagents, etc.). Next, we train a small RL agent in a mixture of the original and\nLLM-generated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent is\nweak at, by providing feedback to the LLM in the form of the agent's\nperformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist environments. We find that a small RL agent\ntrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and\nlearns long-horizon tasks significantly faster. We also show that using an LLM\nto adapt environments dynamically outperforms curriculum learning approaches\nand how the environments are adapted to help improve RL agents' weaker skills\nover time. Additionally, EnvGen is substantially more efficient as it only uses\na small number of LLM calls (e.g., 4 in total), whereas LLM agents require\nthousands of calls. Lastly, we present detailed ablation studies for EnvGen\ndesign choices.","url":"http:\/\/arxiv.org\/abs\/2403.12014v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2403.12014v2","published":1710784276000,"comment":"COLM 2024; First two authors contributed equally; Project website:\n  https:\/\/envgen-llm.github.io\/","pdf_text":"Published as a conference paper at COLM 2024\nEnvGen: Generating and Adapting Environments via LLMs\nfor Training Embodied Agents\nAbhay Zala∗\nJaemin Cho∗\nHan Lin\nJaehong Yoon\nMohit Bansal\nUNC Chapel Hill\n{aszala, jmincho, hanlincs, jhyoon, mbansal}@cs.unc.edu\nhttps:\/\/envgen-llm.github.io\nAbstract\nRecent state-of-the-art approaches for embodied learning via interaction\ndirectly employ large language models (LLMs) as agents to determine the\nnext steps in an environment. Due to their world knowledge and reason-\ning capabilities, LLM agents achieve stronger performance than previous\nsmaller agents based on reinforcement learning (RL); however, frequently\ncalling LLMs is slow and expensive. This begs an interesting question: In-\nstead of directly employing LLMs as embodied agents, can we use LLMs’ reasoning\ncapabilities to adaptively create training environments to help smaller embodied\nRL agents learn useful skills that they are weak at? In this work, we propose\nEnvGen, a novel framework to address this question. First, we prompt an\nLLM to generate training environments that allow agents to quickly learn\ndifferent tasks in parallel. Concretely, the LLM is given the task description\nand environment simulator objectives that the agents should learn and is\nthen asked to generate a set of environment configurations (e.g., different\nterrains, items initially given to agents, chances of finding certain objects,\netc.). Next, we train a small RL agent in a mixture of the original and LLM-\ngenerated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent\nis weak at, by providing feedback to the LLM in the form of the agent’s per-\nformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist game environments. We find that a small\nRL agent trained with EnvGen can outperform SOTA methods, including\na GPT-4 agent, and learns long-horizon tasks significantly faster. We also\nshow that using an LLM to adapt environments dynamically outperforms\ncurriculum learning approaches and how the LLM adapts training environ-\nments to help improve RL agents’ weaker skills over time. Additionally,\nEnvGen is substantially more efficient as it only uses a small number of\nLLM calls (e.g., 4 in total), whereas LLM agents require one or more LLM\ncalls per step (resulting in thousands of LLM calls per episode). We also\npresent detailed analyses of EnvGen’s design choices.\n1\nIntroduction\nThere has been growing interest in embodied AI, where agents learn through interactions\nwith environments instead of static datasets (Ahn et al., 2022; Duan et al., 2022; Wang et al.,\n2023a; Yao et al., 2023; Driess et al., 2023). Open-world games such as Minecraft (Mojang\nStudios, 2009) and Crafter (Hafner, 2022) have been widely used as research environments\nfor embodied agents, where the agents visually perceive their surroundings, traverse large\nterrains, and learn to unlock various achievements (e.g., collecting resources, building tools,\ndefeating monsters, etc.). Some achievements can be easily unlocked within a few steps,\nwhereas others are more challenging as they only become accessible after the agent com-\npletes a series of prerequisite achievements, requiring hundreds of steps (i.e., long-horizon\ntasks). As illustrated in Fig. 1 (a), traditional embodied agents are based on reinforcement\n∗equal contribution\n1\narXiv:2403.12014v2  [cs.CL]  12 Jul 2024\nPublished as a conference paper at COLM 2024\n(a) RL agents: Explore skills with rewards\n(b) LLM agents: Explore skills with LLM knowledge\n(c) EnvGen (Ours): Bootstrap skill exploration with LLM-generated environments\nLLM\nAgent\nInteract\nInteract\nEnvironment\nSmall RL\nAgent\nEnvironment\n Custom Environments\n…\nStep 1: Generate \ntraining environments\nLLM\nOriginal\nEnvironment\nSmall RL\nAgent\nStep 2: Train small agent in\ngenerated environments\nStep 3: Train and measure \nagent performance in the original environment\nStep 4: Share agent \nperformance with LLM\nFeedback to \nupdate \nenvironments\nLoop NCycle times\nFigure 1: Comparison of different methods for creating embodied agents. Previous works\ncommonly use (a) small RL agents or (b) LLM agents to explore skills. In (c) EnvGen, we\ntrain a small RL agent with diverse LLM-generated environments that train different skills\nin parallel and can be adapted via feedback to help the agents progressively improve skills\nthat they are weaker at. Our method benefits from using the world knowledge from LLMs\nwhile maintaining efficient training through a lightweight RL agent.\nlearning (RL) (Hafner et al., 2020; 2021; 2023; Schulman et al., 2017; Burda et al., 2018; Hessel\net al., 2018; Sekar et al., 2020; Moon et al., 2023). However, these RL agents usually struggle\nwhen learning such long-horizon tasks since the reward is sparsely given only after the\ncorrect execution of successive actions, and it is very expensive to automatically find many\naction sequences which lead to the reward (Aytar et al., 2018; Li et al., 2022a; Yuan et al.,\n2023), even after long pretraining with curiosity-driven intrinsic reward (Walker et al., 2023).\nAs large language models (LLMs) have shown remarkable progress in various tasks that\nrequire complex reasoning (Brown et al., 2020; OpenAI, 2023a; Touvron et al., 2023a;b;\nChowdhery et al., 2023; Anil et al., 2023), recent works study implementing embodied\nagents based on LLMs. As illustrated in Fig. 1 (b), these methods leverage LLMs’ world\nknowledge with chain-of-thought reasoning (Nye et al., 2021; Kojima et al., 2022; Wei et al.,\n2022) by creating action plans, giving feedback, and obtaining rewards throughout the\nepisode (Yuan et al., 2023; Wang et al., 2023c; Wu et al., 2023; Wang et al., 2023a;d; Zhao\net al., 2023; Du et al., 2023). While these LLM-based agents that verbalize their knowledge in\nreasoning steps have seen success in achieving better performance over previous approaches,\niteratively calling LLMs throughout the episode is prohibitively slow and expensive (e.g.,\nSPRING (Wu et al., 2023) calls GPT-4 (OpenAI, 2023a) 9 times to take any action step, which\nresults in $270 USD to complete an episode). Du et al. (2023) use LLMs to create rewards to\ntrain smaller agents, but the training is still costly, as it requires many interactions between\nthe LLMs and student agents. This begs the question: Instead of directly employing LLMs as\nembodied agents, can we use LLMs’ reasoning capability to adaptively create training environments\nto help smaller embodied RL agents learn useful skills that they are weak at?\nTo address this question, we propose EnvGen, a novel framework where an LLM adaptively\ngenerates training environments to teach smaller embodied RL agents. We aim to generate\nenvironments that can create various conditions (e.g., have different terrains or some sub-\ngoals are already achieved) so that agents can learn different skills in parallel and obtain\nmore frequent rewards for challenging long-horizon tasks than in the original environment.\nAs shown in Fig. 1 (c), EnvGen iterates over multiple training cycles, each consisting of the\nfollowing four steps:\n• Step 1: We generate configurations for custom training environments (i.e., specifically\ncreated to train an RL agent on certain skills) by providing an LLM with a prompt\nincluding task description, controllable simulator settings, and simulator constraints (see\nFig. 2 and Sec. 2 for details). Then we use the generated configurations to create different\ncustom environments (e.g., different terrains, items initially given to agents, and chance\nof finding certain objects) that can teach multiple skills in parallel.\n• Step 2: We first train the RL agent in multiple LLM-generated environments (i.e., LLM\nenvironments), so that it can learn different useful skills in parallel.\n• Step 3: We then train the RL agent in the original environment to mitigate overfitting to\nthe LLM environments. Afterwards, we measure the current RL agent’s performance in\ndifferent tasks in the original environment to check which skills\/tasks the agent is still\nweak at.\n2\nPublished as a conference paper at COLM 2024\n• Step 4: We provide the RL agent’s successes\/failures in different tasks (from step 3) as\nfeedback to the LLM, so that the LLM can adapt the custom training environments to\nfocus on progressively improving the skills that the agent is weak at.\nNote that EnvGen only requires a few LLM calls (e.g., 4) for environment genera-\ntion\/updating during the entire RL agent training process, whereas other works based on\nLLM agents query an LLM once or multiple times every step (resulting in thousands of\nLLM calls for a single episode).\nWe study the usefulness of EnvGen in different game environments: Crafter (Hafner, 2022)\nand Heist (Cobbe et al., 2020). In the Crafter environment, a simple PPO-based (Schulman\net al., 2017) lightweight (< 5M parameters) RL agent trained with our LLM-generated\nenvironments outperforms strong baselines including a GPT-4 based agent that queries\nan LLM multiple times at every step, and RL agents that use extensive pretraining (e.g.,\n150M steps vs. less than 1M steps for us). When compared to just training longer in\nthe original Crafter environment and curriculum learning approaches such as easy-to-\nhard and adversarial environments, an RL agent trained with EnvGen achieves significant\nimprovements on the overall score and long-horizon tasks. In Heist, we also show that our\nLLM-generated environments can improve overall agent performance and training stability.\nWe also show a qualitative study on how the LLM adapts training environments to help\nimprove RL agents’ weaker skills over time. Finally, we provide comprehensive analysis\nand ablation studies of the design choices of EnvGen, including dynamically updating\nLLM environments (i.e., using adaptive environments) vs. curriculum learning methods,\nfrequency of environment updates, EnvGen vs. longer training in the original environment,\ndifferent LLMs for generating environments, the number of LLM-generated environments,\nand the mixture ratio between the original and LLM environment during training.\n2\nEnvGen: Generating and Adapting Environments via LLMs for\nTraining Embodied Agents\nWe propose EnvGen, a novel framework where an LLM adaptively generates training\nenvironments to train smaller embodied RL agents, enabling them to accomplish various\ntasks within an environment, particularly long-horizon tasks. During the training process,\nthe LLM is given feedback (in the form of the agent’s performance) and can adaptively\nupdate the training environments to progressively focus on improving the tasks that the\nagent is weak at. In the following, we first explain why it is challenging to explore long-\nhorizon tasks in open-world games (Sec. 2.1). Then we explain our method details, including\nhow we generate environments and how agents are trained in EnvGen (Sec. 2.2).\n2.1\nPreliminary: Exploration is Hard for Long-Horizon Tasks\nIn the RL framework, agents explore various states along a trajectory and amplify policies\nbased on the rewards received from those trajectories. However, exploration for long-\nhorizon tasks is slow and computationally expensive, as rewards for such tasks are sparsely\ngiven only after a sequence of successful actions that often involve achieving multiple\nsubgoals. For example, the goal in Crafter (Hafner, 2022) is to unlock 22 achievements,\nwhere some achievements can be unlocked quickly through several simple actions and\nothers require long chains of prerequisites (e.g., collect iron requires make stone pickaxe, which\nmust be preceded by collect stone, ... etc.); see Sec. 3.1 for details. As shown in Hafner (2022),\nexisting agents in Crafter spend most exploration steps learning low-level achievements but\nfail to unlock high-order achievements with many prerequisites.\n2.2\nEnvGen Method Details\nWe introduce EnvGen, where we train an embodied RL agent in multiple LLM-generated\nenvironments (we refer to these as ‘LLM environments’ in the paper) that progressively\nadapt to improve agent performance in multiple skills. The generated environments can\nprovide various conditions (e.g., different terrains, or some subgoals are already achieved)\n3\nPublished as a conference paper at COLM 2024\nCustom Env 1\nCustom Env 2\nCustom Env N\n…\nLLM\nEnv 1: Basic Resource \nCollection and Crafting\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"grassland\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 3,\n        \"stone\": 0,\n        \"wood_pickaxe\": 1\n    }\n}\n```\nPurpose: \nThe agent can \nlearn to collect \nwood and craft \nitems like a wood \nsword or a table ...\n# Game and task description\nYou are an environment designer \nagent for a game called \"Crafter\". Your \njob is to design a few environments \nwhich can be used to teach an agent \nhow to play...\n# Game Objectives\nHere is a list of things an agent would \nneed to learn how to do:\n●collect_coal, collect_diamond ...\n# Controllable Simulator Settings\nHere is a list of parameters you can \ncontrol when making an environment:\n●target_biome: grassland | mountain \n| beaches | natural \n●...\nHere is a list of items the agent can \nstart with:\n●wood_pickaxe: 0-1 \n●…\n(Continued)\n# Simulator Constraints\nHere is a list of constraints:\n●natural biome will set the \nenvironment to have all the \nbiomes\n●coal, iron, and diamond can only \nbe found in a mountain biome ...\n# Output Template\nOutput in the following format:\nEnvironment 1:\n```json\n{ \n   \"environment_settings\": {...},\n   \"inventory_settings\": {...} \n}\n```\nPurpose:  ...\nEnvironment 2: \n...\nEnv 2: Advanced Resource \nCollection and Combat Training\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"mountain\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 5,\n        \"stone\": 3,\n        \"wood_pickaxe\": 1,\n        \"stone_pickaxe\": 1,\n        \"wood_sword\": 1\n    }\n}\n```\nPurpose: ...\nEnv 3: Survival and Crafting \nMastery\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"natural\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"sapling\": 2,\n        \"wood\": 0,\n        \"stone\": 0,\n        \"coal\": 0,\n        \"iron\": 0,\n        \"diamond\": 0\n    }\n}\n```\nPurpose: ...\nSmall RL\nAgent\nOriginal\nEnvironment\nThose environments resulted in the agent improving up to these scores: \n●collect_coal: 38% +\/- 6%\n●defeat_skeleton: 10% +\/- 4%\n●make_stone_pickaxe: 31% +\/- 3%\n●…\nCould you generate new environments based on these scores?\nStep 1: Generate\ntraining environments\nStep 2: Train small agent \nin generated environments\nStep 3: \nTrain and measure agent \nperformance in the original environment\nStep 4: \nShare agent \nperformance with LLM\nFeedback to update environments \nLoop NCycle times\nFigure 2: In EnvGen, we generate and adapt multiple training environments with an LLM\nto let the agent learn different skills effectively. EnvGen iterates over NCycle cycles, each\nconsisting of four steps (see Sec. 2.2).\nso that agents can learn different skills in parallel and obtain more frequent rewards for long-\nhorizon tasks. As shown in Fig. 2, EnvGen iterates NCycle training cycles, each consisting of\nthe following four steps:\nStep 1: Generate training environments with an LLM. As illustrated in step 1 of Fig. 2,\nwe use an LLM (e.g., GPT-4 (OpenAI, 2023a)) to first generate NLLM-Env custom training\nenvironment configurations1 that can cover various objectives and skills that are required in\nthe original environment. The following describes the LLM input prompt components used\nto create environment configurations.\n1. Task description: We provide a brief description of the environment and what the LLM\nshould do (e.g., “generate a set of training environments...”).\n2. Game\/simulator details: We provide a list of objectives that need to be achieved in the\nenvironment (e.g., “collect coal, collect iron, etc.” for Crafter); a list of which simulator\nsettings can be controlled (e.g., terrain, agent inventory); and a list of constraints\/rules\nthat the simulator has (e.g., “skeletons only spawn in mountains; ...” for Crafter).\n3. Output environment configuration template: We provide a blank output configuration\ntemplate (i.e., a JSON object where the environment settings are empty) to the LLM, and\nrequest it to fill in the values, creating NLLM-Env environment configurations. Along\nwith filling the templates, we also ask the LLM to verbally explain the purpose for each\nenvironment (e.g., what the environment would teach the agent); this would help users\neasily understand the environment generation process.\n4. Adaptation feedback based on the RL agent’s performance: We provide the LLM with\nthe performance of the RL agent from the original environment (measured in step 3 and\nsummarized in step 4), as feedback for adapting LLM environments to focus on skills\nthat the RL agent is weak at. The feedback is given at the end of each cycle, so it is only\nprovided to LLM from the second cycle onwards.\nThe obtained environment configurations are then rendered in the game’s simulator. Fig. 2\npresents the summary of input prompt and output environments from the GPT-4 model.\nWe provide more prompt details in Appendix F.\n1We find that N=4 works well; see Table 6 for details.\n4\nPublished as a conference paper at COLM 2024\n(a) Agent View\n(b) Achievement Hierarchy\nCollect Sapling\nDefeat Zombie\nDefeat\nSkeleton\nEat Cow\nEat Plant\nPlace Plant\nCollect Wood\nPlace Table\nMake Wood \nSword\nMake Wood \nPickaxe\nMake Stone \nSword\nPlace Stone\nMake Stone \nPickaxe\nPlace Furnace\nCollect Iron\nMake Iron\nPickaxe\nMake Iron\nSword\nCollect Coal\nCollect Stone\nCollect\nDiamond\nCollect Drink\nWake Up\nFigure 3: (a): Crafter gameplay screenshot. An agent explores a 2D world and completes 22\nachievements. (b): Crafter achievement hierarchy. Some achievements can be completed\nright away; others require previous achievements to be unlocked first (i.e., in a hierarchical\norder following the arrows).\nStep 2: Train a small RL agent in the LLM-generated environments. As shown in step 2\nof Fig. 2, we train the small RL agent in the LLM-generated environments. Concretely, we\ntrain the agent in the NLLM-Env LLM environments for TLLM-Env total steps in parallel.\nStep 3: Train and measure the RL agent’s performance in the original environment. It\nis important to note that the goal of EnvGen is to improve the RL agent’s performance\nin the original environment, instead of the performance only in the LLM environments.\nTo help the RL agent effectively adapt to the original environment and provide the LLM\nwith the current agent’s performance as feedback, we train the agent and measure its\nperformance in the original environment, as shown in step 3 of Fig. 2. First, to mitigate\nthe overfitting to LLM environments, we train the agent in the original environment for\nTOrig-Env steps.2 Next, to find the skills that the RL agent needs to improve at, we test the\nagent in the original environment, without any parameter updates. Concretely, we measure\nindividual success rates for each environment task (e.g., Crafter achievements). The agent\nperformance is summarized (in step 4) and is provided to LLM as feedback (in step 1) to\nadapt training environments in the next cycle. Moreover, importantly, to obtain a more\ncalibrated estimation of agent performance, we calculate the average and variance of the\ntask-specific scores by testing agents with multiple random seeds (i.e., 12).\nStep 4: Send feedback to LLM to adapt environments (to focus on weak skills). We\nprovide the LLM with the agent’s performance from the original environment (measured in\nstep 3), as feedback for updating LLM environments. Concretely, we list the agent’s average\ntask-specific success rate in percentages along with one standard deviation (e.g., “. . . collect\ncoal: 38% ± 6%, defeat skeleton: 10% ± 4% . . . ”), as shown in step 4 of Fig. 2. In step 1 of\nthe next cycle, the LLM can adaptively generate new environments (by using the agent’s\nperformance as feedback) to better help the RL agent learn the skills it is weak at (e.g., defeat\nskeleton). EnvGen iterates this four-step training cycle NCycle times.\n3\nExperimental Setup\nIn the following subsections, we present the benchmarks in which we evaluate EnvGen\nframework on (Sec. 3.1) and the agent architectures that we use for experiments (Sec. 3.2).\n3.1\nEvaluated Benchmarks and Training Details\nCrafter. Crafter (Hafner, 2022) is an open-world 2D survival game focused on evaluating\na broad range of agent capabilities (see Fig. 3). Crafter features 22 achievements that an\nagent can unlock during an episode of play. Some achievements can be unlocked in a\nfew steps (e.g., collect wood, collect sapling, etc.), but other achievements, such as make iron\npickaxe or collect diamond, require many training\/exploration steps and several prerequisite\n2We find that TLLM-Env = TOrig-Env works well; see Table 7 for details.\n5\nPublished as a conference paper at COLM 2024\nachievements to be unlocked (see Fig. 3 b). For example, to make an iron pickaxe, an agent\nmust first collect enough wood to make a table and a wooden pickaxe, then go collect stone\nand return to the table (or collect more wood to make a new one) and then construct a stone\npickaxe. Then the agent still needs to make a furnace, collect coal, and collect iron before\nthe option to make the iron pickaxe is possible.\nFor EnvGen setup, we use NCycle = 4 training cycles during agent training (see Table 3 for\nablation of having a different number of cycles). Each cycle uses 0.12M LLM-generated\nenvironment steps (i.e., CrafterEnvGen steps, see step 2 in Fig. 2) and 0.12M Crafter steps\n(step 3 in Fig. 2) and then we train for 1M steps in Crafter. In total, we train for 1.96M\nsteps ((0.12M + 0.12M) × 4 + 1M). Note that in order to maintain a fair score comparison to\nbaselines, we do not count any achievement during our training cycle for score calculation\nsince the training scores derived from LLM environments and the original environment\nare not directly comparable. Instead, we only take into account the achievements from the\nlast 1M training steps in Crafter for the score calculation. We also experiment with giving\nthe baseline model additional original environment steps to match the number of EnvGen\nsteps (i.e., an additional 0.96M steps) to ensure that EnvGen is not better simply because\nof more steps. The score for Crafter is computed as the geometric mean of individual\nsuccess rates of each achievement for each episode it is completed within 1M training steps:\nS = exp( 1\n22 ∑22\ni=1 ln(1 + si)) −1, where si is the average success rate of the ith achievement\nacross all episodes that occurred during training. We report the average performance with\n30 runs (= 3 different initial LLM-generated CrafterEnvGen environments × 10 different\nrandom seeds).\nHeist. Heist is part of the OpenAI Procgen (Cobbe et al., 2020) benchmark. In this environ-\nment, agents must successfully ‘steal’ the gem after navigating a maze and opening all locks.\nSee more details in Appendix C.2.\n3.2\nAgent Architectures\nOur base RL agent. For both Crafter and Heist, we test the EnvGen framework with a\nsimple (CNN + linear layer) and lightweight (<5M) agent used in Moon et al. (2023), which\nis slightly modified from the agent architecture used in IMPALA (Espeholt et al., 2018).\nFollowing Moon et al. (2023), we train the agent with a PPO (Schulman et al., 2017) objective.\nAt every step, the agent takes an RGB image (surroundings for Crafter, entire maze for\nHeist) as input and outputs the value estimates and policy (action probability). See Fig. 3 (a)\nfor an agent visual input example. We provide additional model details in Appendix E.\nBaseline methods. For Crafter, we compare our method to two groups of recent baselines –\n(1) methods that use frequent (i.e., more than thousands of) LLM calls during training or\ninference: SPRING (Wu et al., 2023) (based on GPT-4) and ELLM (Du et al., 2023) (based on\nCodex (Chen et al., 2021)) and (2) methods that do not use an LLM: DreamerV3 (Hafner et al.,\n2023), MuZero + SPR (Walker et al., 2023), LSTM-SPCNN (Stani´c et al., 2023), PPO (Schulman\net al., 2017), and Achievement Distillation (AD) (Moon et al., 2023). For Heist, we compare\nagainst the PPO agent. For the PPO and AD agents, we follow the implementation of Moon\net al. (2023). See Appendix E for the PPO\/AD agent details.\n4\nResults and Analysis\nWe demonstrate the usefulness of the EnvGen method with comprehensive experiments\nand analysis. We first compare RL agents trained with EnvGen to different baseline methods\non Crafter, an open-world game with 22 hierarchical achievements (Sec. 4.1). Next, we\npresent a detailed analysis of the improvements that training with EnvGen environments\ncan give RL agents on long-horizon tasks (Sec. 4.2). Then, we analyze how the LLM-based\nenvironment adaptation can help an RL agent progressively improve the skills that the agent\nis weak at (Sec. 4.3). Lastly, we present various additional analysis including experiments\non Heist (a maze navigation game) and ablation studies on EnvGen design choices (Sec. 4.4\nand also in the Appendix C).\n6\nPublished as a conference paper at COLM 2024\nModels\nDescription\n# LLM calls\n# Agent Params\nScore (%)\nReward\nHuman∗\n50.5 ± 6.8\n14.3 ± 2.3\nRandom∗\n1.6 ± 0.0\n2.1 ± 1.3\nELLM* (Du et al., 2023)\n5M step PT in Crafter w\/ Codex reward\n5M\n62M\n-\n6.0 ± 0.4\nLSTM-SPCNN∗(Stani´c et al., 2023)\n135M\n11.7 ± 0.8\n9.3 ± 0.2\nDreamerV3∗(Hafner et al., 2023)\n201M\n14.8 ± 1.4\n10.9 ± 0.5\nMuZero + SPR∗(Walker et al., 2023)\n150M step PT in Crafter w\/ RND reward\n54M\n16.4 ± 1.5\n12.7 ± 0.4\nSPRING* (Wu et al., 2023)\n9 queries to call GPT-4 per step\n2.7K†\nUnknown\n27.3 ± 1.2\n12.3 ± 0.7\nPPO (Moon et al., 2023)\n4M\n15.5 ± 0.6\n10.5 ± 0.6\nPPO (Moon et al., 2023)\n0.96M step PT in Crafter\n4M\n26.4 ± 2.1\n12.1 ± 1.0\nAD* (Moon et al., 2023)\n9M\n21.8 ± 1.4\n12.6 ± 0.3\nAD (Moon et al., 2023)\n0.96M step PT in Crafter\n9M\n31.8 ± 0.7\n13.3 ± 1.2\nPPO + EnvGen (Ours)\n0.96M step PT w\/ CrafterEnvGen\n4\n4M\n32.2 ± 0.6\n12.6 ± 0.6\nAD + EnvGen (Ours)\n0.96M step PT w\/ CrafterEnvGen\n4\n9M\n35.3 ± 0.7\n13.7 ± 0.8\nTable 1: Comparison of different agents in the Crafter (Hafner, 2022) environment. Following\nprevious works, we report the geometric mean of success rates across its 22 achievements\nand rewards for 1M Crafter steps. We experiment with EnvGen on two models, PPO\nand Achievement Distillation. *: scores from the Crafter Scoreboard (Hafner, 2022) and\nMoon et al. (2023). †: average number of LLM calls to run a single episode, according to\nSPRING (Wu et al., 2023). PT: Pretraining; AD: Achievement Distillation.\n4.1\nComparison with State-of-the-art Methods on Crafter Environment\nSmall RL agent trained with EnvGen outperforms state-of-the-art baselines. On the\nCrafter environment (described in Sec. 3.1), we compare a small PPO agent trained in\nCrafterEnvGen (i.e., Crafter environments generated with EnvGen) to state-of-the-art baseline\nmethods. As shown in Table 1, we find that a small (4M parameters) PPO agent with\nEnvGen achieves an average score of 32.2% and significantly outperforms the baselines\n(and also in terms of the average reward). Note that some baseline agents have many more\nparameters or pretraining steps such as SPRING (GPT-4 agent; 27.3%), and MuZero + SPR\n(150M pretraining steps; 16.4%). Our method also only uses orders of magnitude fewer LLM\ncalls (only 4) than works like SPRING (2.7K on average) and ELLM (5M), making it much\ncheaper\/more efficient. EnvGen can also work with other RL agents such as Achievement\nDistillation (AD) (Moon et al., 2023) to achieve an even higher score (35.3%).\n4.2\nDetailed Achievement Analysis on Crafter Environment\nNext, we analyze where EnvGen improves the overall score by checking individual achieve-\nment success rates in detail. For this, we compare the same PPO agent architecture trained\nwith different setups: (1) an agent trained on Crafter for 1.96M steps and (2) an agent trained\non CrafterEnvGen for 0.96M steps (0.24M steps × 4 training cycles, see Sec. 2.2) and then\ntrained on Crafter for 1M steps. We measure the success rate (Fig. 4) of each achievement\nand unlocking speed (Fig. 5) of iron tools in the last 1M training steps.\nEnvGen helps RL agents to tackle challenging long-horizon achievements. Fig. 4 shows\nthat training in CrafterEnvGen improves scores of several achievements. Notably, training in\nCrafterEnvGen significantly improves the scores of long-horizon achievements (with many\nprerequisites; see Fig. 3) such as stone and iron tools. Fig. 5 shows that after unlocking\nthe stone pickaxe, the RL agent trained in CrafterEnvGen is significantly faster in unlocking\niron tools. In Appendix C.1, we also compare two AD agents, and show that CrafterEnvGen\nimproves the success rate of the most challenging achievement – ‘collect diamond’.\n4.3\nAdaptation of Training Environments Helps the Agent Improve Weaker Skills\nFig. 6 shows how the LLM adaptively generates new training environments based on the\nintermediate performance of our PPO-based RL agent. In the intermediate performance\nplots, we compare the baseline agent trained only in Crafter and our RL agent trained in\nCrafterEnvGen. In the cycle 2, given the feedback that the current RL agent is not good at\ncollecting coal, the LLM generates an environment to help the agent focus on it, improving\n7\nPublished as a conference paper at COLM 2024\nCollect Coal\nCollect Diamond\nCollect Drink\nCollect Iron\nCollect Sapling\nCollect Stone\nCollect Wood\nDefeat Skeleton\nDefeat Zombie\nEat Cow\nEat Plant\nMake Iron Pickaxe\nMake Iron Sword\nMake Stone Pickaxe\nMake Stone Sword\nMake Wood Pickaxe\nMake Wood Sword\nPlace Furnace\nPlace Plant\nPlace Stone\nPlace Table\nWake Up\n0.001\n0.01\n0.1\n1\n10\n100\n1.96M Crafter Steps\n0.96M steps in Crafter EnvGen + 1M Crafter Steps (Ours)\nSuccess Rate (%)\nFigure 4: Success rates for all the Crafter achievements of two PPO agents (Moon et al.,\n2023) – (1) Baseline: trained in Crafter for 1.96M steps, and (2) Ours: trained in 0.96M steps\nin CrafterEnvGen and 1M in Crafter.\nMake Iron \nSword\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nSword\nFigure 5: Unlock times (the first moment when the agent completed an achievement) for\nthree long-horizon achievements (‘make stone pickaxe’, ‘make iron pickaxe’, and ‘make iron\nsword’) of two PPO agents (Moon et al., 2023) – (1) Baseline: trained in Crafter for 1.96M\nsteps, and (2) Ours: trained for 0.96M steps in CrafterEnvGen and for 1M steps in Crafter. The\nplot shows the last 1M training steps out of 1.96M steps.\nthe agent’s performance for the skill. Likewise, in the cycle 3, given the feedback that the\nagent is weak at making stone pickaxes, the LLM generates an environment to help the\nagent more easily craft the stone pickaxe, helping the agent improve it’s score for the skill.\nPowered by the adaptive LLM environment generation of EnvGen, our agent learns to\nunlock these two achievements significantly faster than the baseline agent.\n4.4\nAdditional Analysis and Ablation Studies\nIn the following, we show comprehensive design analysis and ablation studies of EnvGen\nmethod: dynamically updating LLM environments (i.e., using adaptive environments)\nvs. curriculum learning methods, and different frequencies of environment updates. In\nAppendix C, we show comprehensive analysis and ablation studies of EnvGen method:\nEnvGen vs. longer training in the original environment, different LLMs for generating\nenvironments, the number of LLM environments, and the ratio of training steps in the LLM\nvs. original environments. We also include experiments on the Heist environment (see\nSec. 3.1) in Appendix C.2.\nDifferent environment curricula: fixed, easy-to-hard, adversarial vs. adaptive. Table 2\nshows that using LLM environments that are adaptively updated based on intermediate\nagent performance to improve weaker skills (last row) results in overall higher scoring\nagents than just using the initial LLM environments for the whole training (32.2% vs. 29.9%).\n8\nPublished as a conference paper at COLM 2024\nCollect\nCoal\nMake Stone \nPickaxe\nCycle 2\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 1: Introduction to \nMining and Crafting\n```json\n{\n \"target_biome\": \"mountain\",        \n \"wood_pickaxe\": 1,\n ...\n}\n```\nPurpose: This environment is \ndesigned to teach the agent \nhow to mine coal ...\n...\nFeedback from \nprevious cycle\nCollect coal is 2%\nMake stone pickaxe is 1% \n…\nLLM\nCycle 3\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 2: Introduce \ncombat and advanced tool \ncrafting\n```json\n{\n \"wood\": 3,\n \"stone\": 2,\n ...\n}\n```\nPurpose: With stone in the \ninventory, the agent is nudged \nto craft a stone pickaxe, a \ntask it hasn't learned yet ...\n...\nFeedback from \nprevious cycle\nCollect coal is 22%\nMake stone pickaxe is 1%\n…\nPerformance \nfrom Cycle 1\nPerformance \nfrom Cycle 3\nPerformance \nfrom Cycle 2\nTrained in \nCrafter\nTrained in \nCrafterEnvGen (Ours)\nLLM\nCollect\nCoal\nMake Stone \nPickaxe\nCollect\nCoal\nMake Stone \nPickaxe\nFigure 6: Adaptation of training environments based on agent performance over EnvGen\ncycles. At the end of each cycle, the RL agent’s performance is given to the LLM as\nfeedback (e.g., ‘Collect coal is 2%’). The LLM uses the feedback to adaptively generate new\nenvironments that can help the agent progressively tackle skills it was previously weak at.\nTraining Curriculum\nScore (%)\nReward\nFixed (no curriculum)\n29.9 ± 0.9\n12.6 ± 0.8\nEasy-to-Hard\n26.8 ± 1.5\n12.7 ± 0.7\nAdversarial\n26.8 ± 0.8\n12.2 ± 0.7\nAdaptive+Dynamic Environments (EnvGen)\n32.2 ± 0.6\n12.6 ± 0.6\nTable 2: Comparison of RL agents trained in Crafter (Hafner, 2022) using no curriculum, an\neasy-to-hard curriculum, an adversarial curriculum, and our adaptive+dynamic environ-\nments. Agents are trained for 0.96M steps using the curriculum and then 1M in the default\nCrafter environment.\nThese results indicate the effectiveness of the agent feedback and environment updating\n(step 4 described in Sec. 2).\nTable 2 also compares an agent trained via EnvGen to the same agent trained with curriculum\nlearning approaches such as an easy-to-hard curriculum, similar to Ammanabrolu et al.\n(2022) (i.e., pre-defined training environment order based on environment difficulty) and\nadversarial curriculum, similar to Parker-Holder et al. (2022) (i.e., updating to training\nenvironments that agent does worse in) in the Crafter environment. Detailed setups of both\nbaseline approaches are in the appendix. The agent trained with EnvGen is able to achieve\nmuch higher performance (32.2% vs. 26.8% for both curricula) indicating the effectiveness\nEnvGen’s approach of adaptively generating training environments to improve agent weak\nskills. The result indicates that creating more difficult environments does not necessarily\nhelp the agent learn new skills over time.\nEnvironment Update Frequency\n# Training cycles NCycle\nScore (%)\nReward\nEvery 0.012M steps\n40 cycles\n30.8 ± 0.7\n12.8 ± 0.6\nEvery 0.06M steps\n8 cycles\n32.1 ± 0.5\n12.7 ± 0.8\nEvery 0.12M steps (default)\n4 cycles\n32.2 ± 0.6\n12.6 ± 0.6\nTable 3: Different frequencies to give feedback to the LLM and update the environments\n(see Sec. 2 for details). Agents are trained with 0.96M steps in CrafterEnvGen and 1M steps in\nCrafter environment.\n9\nPublished as a conference paper at COLM 2024\nFrequency of LLM feedback \/ environment updates.\nTable 3 shows that updating the\nLLM environments at every 0.12M steps results in the best agent performance. While\nincreasing the cycles of environment feedback beyond 4 does not improve further, we\nfind that updating environments with feedback always helps improve the RL agent’s\nperformance compared to training only with the original Crafter environment in Table 1\n(26.4%) or the fixed LLM environment in Table 2 (29.9%).\n5\nRelated Works\nLLMs as open-world game agents. Recent works study using LLMs to create action plans\n(i.e., a list of subgoals or skills to target) for embodied agents in open-world games like\nMinecraft and Crafter (Hafner, 2022). Most of these methods require calling LLMs frequently\n(e.g., at every step) for planning the next steps (Yuan et al., 2023; Wang et al., 2023c; Wu\net al., 2023; Wang et al., 2023a;d; Zhao et al., 2023). Other methods, such as Li et al. (2024);\nKwon et al. (2023); Ma et al. (2023); Du et al. (2023), have used LLMs to create\/adjust\nrewards to train agents. Although these works show initial promising results leveraging the\nworld knowledge of LLMs to tackle long-horizon tasks, iteratively calling LLMs throughout\nepisodes is prohibitively slow and expensive (e.g., running a single episode in the Crafter\nenvironment with SPRING (Wu et al., 2023) costs around $270 USD as they have 2.7K LLM\ncalls on average). EnvGen only calls LLMs a few times (e.g., 4 in total) to create training\nenvironments that focus on helping the RL agent progressively improve its weaker skills.\nDeep learning-based game\/simulator content generation. Procedural content generation\n(PCG) for games is about the automatic generation of levels, landscapes, items, rules, quests,\nor other types of game contents (Shaker et al., 2016). While traditional PCG methods are\nbased on search\/solver\/rule\/grammar-based methods, recent works apply deep learn-\ning methods such as GAN (Goodfellow et al., 2014) for PCG (Liu et al., 2021; Kumaran\net al., 2020; Schubert et al., 2022). Several works have explored using LLMs to generate\ngame content such as difficulty levels (Sudhakaran et al., 2023; Todd et al., 2023) and\nscenes\/environments (Kumaran et al., 2023; Wang et al., 2023b; Afshar & Li, 2024). While\nthese works aim to help developers create new game content, we aim to improve RL agent\nperformance in the original environment. A line of work proposes unsupervised envi-\nronment design (UED) that manipulates the difficulty level of environments to be more\nchallenging to RL agents (Dennis et al., 2020; Jiang et al., 2021; Parker-Holder et al., 2022).\nWhile these works use a learned environment manipulator or evolutionary algorithms to\nmaximize the ‘regret’ (the difference between the expected return of the current and optimal\npolicies) in simple games such as MiniGrid (Chevalier-Boisvert et al., 2023), we use the\nworld knowledge of LLMs to generate and adapt training environments that can improve\nweaker skills based on comprehensive skill-specific feedback from RL agents in open-world\ngames with many challenging long-horizon tasks. To help agents generalize to unseen tasks\nin a text-based dialogue game, Ammanabrolu et al. (2022) augment new tasks with LMs\nand use a manually designed, fixed curriculum. Unlike this work, we adaptively generate\ntraining environments using LLMs’ world knowledge and automatically learning a dynamic\ncurriculum based on the RL agent’s feedback, so as to improve the agent’s weaker skills\nin open-world games with visual inputs. Beyond game content generation, several works\nvisually augment vision-and-language navigation (VLN) simulators (e.g., rendering the\nenvironments with different styles) using image generation models (Li et al., 2022b; Wang\net al., 2023e; Li & Bansal, 2023). Such works could complement our LLM environments (e.g.,\naugmenting our environments with diverse colors and textures).\n6\nConclusion\nWe propose EnvGen, a novel framework to improve embodied RL agent performance by\nutilizing the world knowledge of LLMs to adaptively generate training environments. In\nEnvGen, we give an LLM a prompt describing a game\/simulator and ask the LLM to\ngenerate the configurations to create new environments that can teach different skills. Next,\nwe train an agent in the LLM-generated environments, give feedback to the LLM by testing\nthe agent in the original environments, and then ask the LLM to update the environments to\n10\nPublished as a conference paper at COLM 2024\nteach agents skills they are weaker at. In two challenging games, Crafter and Heist, we show\nthat EnvGen increases agent performance significantly, and training with LLM-generated\nenvironments is more effective than training longer in the original environments. We also\nshow that using an LLM to adapt environments dynamically outperforms curriculum\nlearning approaches and how the LLM adapts training environments to help improve RL\nagents’ weaker skills over time. Moreover, a lightweight model (< 5M parameters) trained\nwith LLM-generated environments even outperforms an LLM agent with significantly fewer\nLLM calls. We hope our work can guide future works in leveraging LLMs for embodied\nagents.\nAcknowledgments\nWe thank Elias Stengel-Eskin and the reviewers for the thoughtful discussion and feedback.\nThis work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage\nInstitute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031,\nARO Award W911NF2110220, ONR Grant N00014-23-1-2356, and a Bloomberg Data Science\nPh.D. Fellowship. The views contained in this article are those of the authors and not of the\nfunding agency.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nEnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents\n```\n#### 2. 论文摘要\n```\nRecent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller RL agents learn useful skills that they are weak at? We propose EnvGen,\na novel framework to address this question. We first prompt an LLM to generate\ntraining environments by giving it the task description and simulator\nobjectives that the agents should learn and then asking it to generate a set of\nenvironment configurations (e.g., different terrains, items initially given to\nagents, etc.). Next, we train a small RL agent in a mixture of the original and\nLLM-generated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent is\nweak at, by providing feedback to the LLM in the form of the agent's\nperformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist environments. We find that a small RL agent\ntrained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and\nlearns long-horizon tasks significantly faster. We also show that using an LLM\nto adapt environments dynamically outperforms curriculum learning approaches\nand how the environments are adapted to help improve RL agents' weaker skills\nover time. Additionally, EnvGen is substantially more efficient as it only uses\na small number of LLM calls (e.g., 4 in total), whereas LLM agents require\nthousands of calls. Lastly, we present detailed ablation studies for EnvGen\ndesign choices.\n```\n\n#### 3. 论文全文\n```\nPublished as a conference paper at COLM 2024\nEnvGen: Generating and Adapting Environments via LLMs\nfor Training Embodied Agents\nAbhay Zala∗\nJaemin Cho∗\nHan Lin\nJaehong Yoon\nMohit Bansal\nUNC Chapel Hill\n{aszala, jmincho, hanlincs, jhyoon, mbansal}@cs.unc.edu\nhttps:\/\/envgen-llm.github.io\nAbstract\nRecent state-of-the-art approaches for embodied learning via interaction\ndirectly employ large language models (LLMs) as agents to determine the\nnext steps in an environment. Due to their world knowledge and reason-\ning capabilities, LLM agents achieve stronger performance than previous\nsmaller agents based on reinforcement learning (RL); however, frequently\ncalling LLMs is slow and expensive. This begs an interesting question: In-\nstead of directly employing LLMs as embodied agents, can we use LLMs’ reasoning\ncapabilities to adaptively create training environments to help smaller embodied\nRL agents learn useful skills that they are weak at? In this work, we propose\nEnvGen, a novel framework to address this question. First, we prompt an\nLLM to generate training environments that allow agents to quickly learn\ndifferent tasks in parallel. Concretely, the LLM is given the task description\nand environment simulator objectives that the agents should learn and is\nthen asked to generate a set of environment configurations (e.g., different\nterrains, items initially given to agents, chances of finding certain objects,\netc.). Next, we train a small RL agent in a mixture of the original and LLM-\ngenerated environments. Then, we enable the LLM to continuously adapt the\ngenerated environments to progressively improve the skills that the agent\nis weak at, by providing feedback to the LLM in the form of the agent’s per-\nformance. We demonstrate the usefulness of EnvGen with comprehensive\nexperiments in Crafter and Heist game environments. We find that a small\nRL agent trained with EnvGen can outperform SOTA methods, including\na GPT-4 agent, and learns long-horizon tasks significantly faster. We also\nshow that using an LLM to adapt environments dynamically outperforms\ncurriculum learning approaches and how the LLM adapts training environ-\nments to help improve RL agents’ weaker skills over time. Additionally,\nEnvGen is substantially more efficient as it only uses a small number of\nLLM calls (e.g., 4 in total), whereas LLM agents require one or more LLM\ncalls per step (resulting in thousands of LLM calls per episode). We also\npresent detailed analyses of EnvGen’s design choices.\n1\nIntroduction\nThere has been growing interest in embodied AI, where agents learn through interactions\nwith environments instead of static datasets (Ahn et al., 2022; Duan et al., 2022; Wang et al.,\n2023a; Yao et al., 2023; Driess et al., 2023). Open-world games such as Minecraft (Mojang\nStudios, 2009) and Crafter (Hafner, 2022) have been widely used as research environments\nfor embodied agents, where the agents visually perceive their surroundings, traverse large\nterrains, and learn to unlock various achievements (e.g., collecting resources, building tools,\ndefeating monsters, etc.). Some achievements can be easily unlocked within a few steps,\nwhereas others are more challenging as they only become accessible after the agent com-\npletes a series of prerequisite achievements, requiring hundreds of steps (i.e., long-horizon\ntasks). As illustrated in Fig. 1 (a), traditional embodied agents are based on reinforcement\n∗equal contribution\n1\narXiv:2403.12014v2  [cs.CL]  12 Jul 2024\nPublished as a conference paper at COLM 2024\n(a) RL agents: Explore skills with rewards\n(b) LLM agents: Explore skills with LLM knowledge\n(c) EnvGen (Ours): Bootstrap skill exploration with LLM-generated environments\nLLM\nAgent\nInteract\nInteract\nEnvironment\nSmall RL\nAgent\nEnvironment\n Custom Environments\n…\nStep 1: Generate \ntraining environments\nLLM\nOriginal\nEnvironment\nSmall RL\nAgent\nStep 2: Train small agent in\ngenerated environments\nStep 3: Train and measure \nagent performance in the original environment\nStep 4: Share agent \nperformance with LLM\nFeedback to \nupdate \nenvironments\nLoop NCycle times\nFigure 1: Comparison of different methods for creating embodied agents. Previous works\ncommonly use (a) small RL agents or (b) LLM agents to explore skills. In (c) EnvGen, we\ntrain a small RL agent with diverse LLM-generated environments that train different skills\nin parallel and can be adapted via feedback to help the agents progressively improve skills\nthat they are weaker at. Our method benefits from using the world knowledge from LLMs\nwhile maintaining efficient training through a lightweight RL agent.\nlearning (RL) (Hafner et al., 2020; 2021; 2023; Schulman et al., 2017; Burda et al., 2018; Hessel\net al., 2018; Sekar et al., 2020; Moon et al., 2023). However, these RL agents usually struggle\nwhen learning such long-horizon tasks since the reward is sparsely given only after the\ncorrect execution of successive actions, and it is very expensive to automatically find many\naction sequences which lead to the reward (Aytar et al., 2018; Li et al., 2022a; Yuan et al.,\n2023), even after long pretraining with curiosity-driven intrinsic reward (Walker et al., 2023).\nAs large language models (LLMs) have shown remarkable progress in various tasks that\nrequire complex reasoning (Brown et al., 2020; OpenAI, 2023a; Touvron et al., 2023a;b;\nChowdhery et al., 2023; Anil et al., 2023), recent works study implementing embodied\nagents based on LLMs. As illustrated in Fig. 1 (b), these methods leverage LLMs’ world\nknowledge with chain-of-thought reasoning (Nye et al., 2021; Kojima et al., 2022; Wei et al.,\n2022) by creating action plans, giving feedback, and obtaining rewards throughout the\nepisode (Yuan et al., 2023; Wang et al., 2023c; Wu et al., 2023; Wang et al., 2023a;d; Zhao\net al., 2023; Du et al., 2023). While these LLM-based agents that verbalize their knowledge in\nreasoning steps have seen success in achieving better performance over previous approaches,\niteratively calling LLMs throughout the episode is prohibitively slow and expensive (e.g.,\nSPRING (Wu et al., 2023) calls GPT-4 (OpenAI, 2023a) 9 times to take any action step, which\nresults in $270 USD to complete an episode). Du et al. (2023) use LLMs to create rewards to\ntrain smaller agents, but the training is still costly, as it requires many interactions between\nthe LLMs and student agents. This begs the question: Instead of directly employing LLMs as\nembodied agents, can we use LLMs’ reasoning capability to adaptively create training environments\nto help smaller embodied RL agents learn useful skills that they are weak at?\nTo address this question, we propose EnvGen, a novel framework where an LLM adaptively\ngenerates training environments to teach smaller embodied RL agents. We aim to generate\nenvironments that can create various conditions (e.g., have different terrains or some sub-\ngoals are already achieved) so that agents can learn different skills in parallel and obtain\nmore frequent rewards for challenging long-horizon tasks than in the original environment.\nAs shown in Fig. 1 (c), EnvGen iterates over multiple training cycles, each consisting of the\nfollowing four steps:\n• Step 1: We generate configurations for custom training environments (i.e., specifically\ncreated to train an RL agent on certain skills) by providing an LLM with a prompt\nincluding task description, controllable simulator settings, and simulator constraints (see\nFig. 2 and Sec. 2 for details). Then we use the generated configurations to create different\ncustom environments (e.g., different terrains, items initially given to agents, and chance\nof finding certain objects) that can teach multiple skills in parallel.\n• Step 2: We first train the RL agent in multiple LLM-generated environments (i.e., LLM\nenvironments), so that it can learn different useful skills in parallel.\n• Step 3: We then train the RL agent in the original environment to mitigate overfitting to\nthe LLM environments. Afterwards, we measure the current RL agent’s performance in\ndifferent tasks in the original environment to check which skills\/tasks the agent is still\nweak at.\n2\nPublished as a conference paper at COLM 2024\n• Step 4: We provide the RL agent’s successes\/failures in different tasks (from step 3) as\nfeedback to the LLM, so that the LLM can adapt the custom training environments to\nfocus on progressively improving the skills that the agent is weak at.\nNote that EnvGen only requires a few LLM calls (e.g., 4) for environment genera-\ntion\/updating during the entire RL agent training process, whereas other works based on\nLLM agents query an LLM once or multiple times every step (resulting in thousands of\nLLM calls for a single episode).\nWe study the usefulness of EnvGen in different game environments: Crafter (Hafner, 2022)\nand Heist (Cobbe et al., 2020). In the Crafter environment, a simple PPO-based (Schulman\net al., 2017) lightweight (< 5M parameters) RL agent trained with our LLM-generated\nenvironments outperforms strong baselines including a GPT-4 based agent that queries\nan LLM multiple times at every step, and RL agents that use extensive pretraining (e.g.,\n150M steps vs. less than 1M steps for us). When compared to just training longer in\nthe original Crafter environment and curriculum learning approaches such as easy-to-\nhard and adversarial environments, an RL agent trained with EnvGen achieves significant\nimprovements on the overall score and long-horizon tasks. In Heist, we also show that our\nLLM-generated environments can improve overall agent performance and training stability.\nWe also show a qualitative study on how the LLM adapts training environments to help\nimprove RL agents’ weaker skills over time. Finally, we provide comprehensive analysis\nand ablation studies of the design choices of EnvGen, including dynamically updating\nLLM environments (i.e., using adaptive environments) vs. curriculum learning methods,\nfrequency of environment updates, EnvGen vs. longer training in the original environment,\ndifferent LLMs for generating environments, the number of LLM-generated environments,\nand the mixture ratio between the original and LLM environment during training.\n2\nEnvGen: Generating and Adapting Environments via LLMs for\nTraining Embodied Agents\nWe propose EnvGen, a novel framework where an LLM adaptively generates training\nenvironments to train smaller embodied RL agents, enabling them to accomplish various\ntasks within an environment, particularly long-horizon tasks. During the training process,\nthe LLM is given feedback (in the form of the agent’s performance) and can adaptively\nupdate the training environments to progressively focus on improving the tasks that the\nagent is weak at. In the following, we first explain why it is challenging to explore long-\nhorizon tasks in open-world games (Sec. 2.1). Then we explain our method details, including\nhow we generate environments and how agents are trained in EnvGen (Sec. 2.2).\n2.1\nPreliminary: Exploration is Hard for Long-Horizon Tasks\nIn the RL framework, agents explore various states along a trajectory and amplify policies\nbased on the rewards received from those trajectories. However, exploration for long-\nhorizon tasks is slow and computationally expensive, as rewards for such tasks are sparsely\ngiven only after a sequence of successful actions that often involve achieving multiple\nsubgoals. For example, the goal in Crafter (Hafner, 2022) is to unlock 22 achievements,\nwhere some achievements can be unlocked quickly through several simple actions and\nothers require long chains of prerequisites (e.g., collect iron requires make stone pickaxe, which\nmust be preceded by collect stone, ... etc.); see Sec. 3.1 for details. As shown in Hafner (2022),\nexisting agents in Crafter spend most exploration steps learning low-level achievements but\nfail to unlock high-order achievements with many prerequisites.\n2.2\nEnvGen Method Details\nWe introduce EnvGen, where we train an embodied RL agent in multiple LLM-generated\nenvironments (we refer to these as ‘LLM environments’ in the paper) that progressively\nadapt to improve agent performance in multiple skills. The generated environments can\nprovide various conditions (e.g., different terrains, or some subgoals are already achieved)\n3\nPublished as a conference paper at COLM 2024\nCustom Env 1\nCustom Env 2\nCustom Env N\n…\nLLM\nEnv 1: Basic Resource \nCollection and Crafting\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"grassland\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 3,\n        \"stone\": 0,\n        \"wood_pickaxe\": 1\n    }\n}\n```\nPurpose: \nThe agent can \nlearn to collect \nwood and craft \nitems like a wood \nsword or a table ...\n# Game and task description\nYou are an environment designer \nagent for a game called \"Crafter\". Your \njob is to design a few environments \nwhich can be used to teach an agent \nhow to play...\n# Game Objectives\nHere is a list of things an agent would \nneed to learn how to do:\n●collect_coal, collect_diamond ...\n# Controllable Simulator Settings\nHere is a list of parameters you can \ncontrol when making an environment:\n●target_biome: grassland | mountain \n| beaches | natural \n●...\nHere is a list of items the agent can \nstart with:\n●wood_pickaxe: 0-1 \n●…\n(Continued)\n# Simulator Constraints\nHere is a list of constraints:\n●natural biome will set the \nenvironment to have all the \nbiomes\n●coal, iron, and diamond can only \nbe found in a mountain biome ...\n# Output Template\nOutput in the following format:\nEnvironment 1:\n```json\n{ \n   \"environment_settings\": {...},\n   \"inventory_settings\": {...} \n}\n```\nPurpose:  ...\nEnvironment 2: \n...\nEnv 2: Advanced Resource \nCollection and Combat Training\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"mountain\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"wood\": 5,\n        \"stone\": 3,\n        \"wood_pickaxe\": 1,\n        \"stone_pickaxe\": 1,\n        \"wood_sword\": 1\n    }\n}\n```\nPurpose: ...\nEnv 3: Survival and Crafting \nMastery\n```json\n{\n    \"environment_settings\": {\n        \"target_biome\": \"natural\",\n        \"tree_rarity\": \"common\",\n        \"coal_rarity\": \"common\",\n        \"iron_rarity\": \"common\",\n        \"diamond_rarity\": \"rare\"\n    },\n    \"inventory_settings\": {\n        \"sapling\": 2,\n        \"wood\": 0,\n        \"stone\": 0,\n        \"coal\": 0,\n        \"iron\": 0,\n        \"diamond\": 0\n    }\n}\n```\nPurpose: ...\nSmall RL\nAgent\nOriginal\nEnvironment\nThose environments resulted in the agent improving up to these scores: \n●collect_coal: 38% +\/- 6%\n●defeat_skeleton: 10% +\/- 4%\n●make_stone_pickaxe: 31% +\/- 3%\n●…\nCould you generate new environments based on these scores?\nStep 1: Generate\ntraining environments\nStep 2: Train small agent \nin generated environments\nStep 3: \nTrain and measure agent \nperformance in the original environment\nStep 4: \nShare agent \nperformance with LLM\nFeedback to update environments \nLoop NCycle times\nFigure 2: In EnvGen, we generate and adapt multiple training environments with an LLM\nto let the agent learn different skills effectively. EnvGen iterates over NCycle cycles, each\nconsisting of four steps (see Sec. 2.2).\nso that agents can learn different skills in parallel and obtain more frequent rewards for long-\nhorizon tasks. As shown in Fig. 2, EnvGen iterates NCycle training cycles, each consisting of\nthe following four steps:\nStep 1: Generate training environments with an LLM. As illustrated in step 1 of Fig. 2,\nwe use an LLM (e.g., GPT-4 (OpenAI, 2023a)) to first generate NLLM-Env custom training\nenvironment configurations1 that can cover various objectives and skills that are required in\nthe original environment. The following describes the LLM input prompt components used\nto create environment configurations.\n1. Task description: We provide a brief description of the environment and what the LLM\nshould do (e.g., “generate a set of training environments...”).\n2. Game\/simulator details: We provide a list of objectives that need to be achieved in the\nenvironment (e.g., “collect coal, collect iron, etc.” for Crafter); a list of which simulator\nsettings can be controlled (e.g., terrain, agent inventory); and a list of constraints\/rules\nthat the simulator has (e.g., “skeletons only spawn in mountains; ...” for Crafter).\n3. Output environment configuration template: We provide a blank output configuration\ntemplate (i.e., a JSON object where the environment settings are empty) to the LLM, and\nrequest it to fill in the values, creating NLLM-Env environment configurations. Along\nwith filling the templates, we also ask the LLM to verbally explain the purpose for each\nenvironment (e.g., what the environment would teach the agent); this would help users\neasily understand the environment generation process.\n4. Adaptation feedback based on the RL agent’s performance: We provide the LLM with\nthe performance of the RL agent from the original environment (measured in step 3 and\nsummarized in step 4), as feedback for adapting LLM environments to focus on skills\nthat the RL agent is weak at. The feedback is given at the end of each cycle, so it is only\nprovided to LLM from the second cycle onwards.\nThe obtained environment configurations are then rendered in the game’s simulator. Fig. 2\npresents the summary of input prompt and output environments from the GPT-4 model.\nWe provide more prompt details in Appendix F.\n1We find that N=4 works well; see Table 6 for details.\n4\nPublished as a conference paper at COLM 2024\n(a) Agent View\n(b) Achievement Hierarchy\nCollect Sapling\nDefeat Zombie\nDefeat\nSkeleton\nEat Cow\nEat Plant\nPlace Plant\nCollect Wood\nPlace Table\nMake Wood \nSword\nMake Wood \nPickaxe\nMake Stone \nSword\nPlace Stone\nMake Stone \nPickaxe\nPlace Furnace\nCollect Iron\nMake Iron\nPickaxe\nMake Iron\nSword\nCollect Coal\nCollect Stone\nCollect\nDiamond\nCollect Drink\nWake Up\nFigure 3: (a): Crafter gameplay screenshot. An agent explores a 2D world and completes 22\nachievements. (b): Crafter achievement hierarchy. Some achievements can be completed\nright away; others require previous achievements to be unlocked first (i.e., in a hierarchical\norder following the arrows).\nStep 2: Train a small RL agent in the LLM-generated environments. As shown in step 2\nof Fig. 2, we train the small RL agent in the LLM-generated environments. Concretely, we\ntrain the agent in the NLLM-Env LLM environments for TLLM-Env total steps in parallel.\nStep 3: Train and measure the RL agent’s performance in the original environment. It\nis important to note that the goal of EnvGen is to improve the RL agent’s performance\nin the original environment, instead of the performance only in the LLM environments.\nTo help the RL agent effectively adapt to the original environment and provide the LLM\nwith the current agent’s performance as feedback, we train the agent and measure its\nperformance in the original environment, as shown in step 3 of Fig. 2. First, to mitigate\nthe overfitting to LLM environments, we train the agent in the original environment for\nTOrig-Env steps.2 Next, to find the skills that the RL agent needs to improve at, we test the\nagent in the original environment, without any parameter updates. Concretely, we measure\nindividual success rates for each environment task (e.g., Crafter achievements). The agent\nperformance is summarized (in step 4) and is provided to LLM as feedback (in step 1) to\nadapt training environments in the next cycle. Moreover, importantly, to obtain a more\ncalibrated estimation of agent performance, we calculate the average and variance of the\ntask-specific scores by testing agents with multiple random seeds (i.e., 12).\nStep 4: Send feedback to LLM to adapt environments (to focus on weak skills). We\nprovide the LLM with the agent’s performance from the original environment (measured in\nstep 3), as feedback for updating LLM environments. Concretely, we list the agent’s average\ntask-specific success rate in percentages along with one standard deviation (e.g., “. . . collect\ncoal: 38% ± 6%, defeat skeleton: 10% ± 4% . . . ”), as shown in step 4 of Fig. 2. In step 1 of\nthe next cycle, the LLM can adaptively generate new environments (by using the agent’s\nperformance as feedback) to better help the RL agent learn the skills it is weak at (e.g., defeat\nskeleton). EnvGen iterates this four-step training cycle NCycle times.\n3\nExperimental Setup\nIn the following subsections, we present the benchmarks in which we evaluate EnvGen\nframework on (Sec. 3.1) and the agent architectures that we use for experiments (Sec. 3.2).\n3.1\nEvaluated Benchmarks and Training Details\nCrafter. Crafter (Hafner, 2022) is an open-world 2D survival game focused on evaluating\na broad range of agent capabilities (see Fig. 3). Crafter features 22 achievements that an\nagent can unlock during an episode of play. Some achievements can be unlocked in a\nfew steps (e.g., collect wood, collect sapling, etc.), but other achievements, such as make iron\npickaxe or collect diamond, require many training\/exploration steps and several prerequisite\n2We find that TLLM-Env = TOrig-Env works well; see Table 7 for details.\n5\nPublished as a conference paper at COLM 2024\nachievements to be unlocked (see Fig. 3 b). For example, to make an iron pickaxe, an agent\nmust first collect enough wood to make a table and a wooden pickaxe, then go collect stone\nand return to the table (or collect more wood to make a new one) and then construct a stone\npickaxe. Then the agent still needs to make a furnace, collect coal, and collect iron before\nthe option to make the iron pickaxe is possible.\nFor EnvGen setup, we use NCycle = 4 training cycles during agent training (see Table 3 for\nablation of having a different number of cycles). Each cycle uses 0.12M LLM-generated\nenvironment steps (i.e., CrafterEnvGen steps, see step 2 in Fig. 2) and 0.12M Crafter steps\n(step 3 in Fig. 2) and then we train for 1M steps in Crafter. In total, we train for 1.96M\nsteps ((0.12M + 0.12M) × 4 + 1M). Note that in order to maintain a fair score comparison to\nbaselines, we do not count any achievement during our training cycle for score calculation\nsince the training scores derived from LLM environments and the original environment\nare not directly comparable. Instead, we only take into account the achievements from the\nlast 1M training steps in Crafter for the score calculation. We also experiment with giving\nthe baseline model additional original environment steps to match the number of EnvGen\nsteps (i.e., an additional 0.96M steps) to ensure that EnvGen is not better simply because\nof more steps. The score for Crafter is computed as the geometric mean of individual\nsuccess rates of each achievement for each episode it is completed within 1M training steps:\nS = exp( 1\n22 ∑22\ni=1 ln(1 + si)) −1, where si is the average success rate of the ith achievement\nacross all episodes that occurred during training. We report the average performance with\n30 runs (= 3 different initial LLM-generated CrafterEnvGen environments × 10 different\nrandom seeds).\nHeist. Heist is part of the OpenAI Procgen (Cobbe et al., 2020) benchmark. In this environ-\nment, agents must successfully ‘steal’ the gem after navigating a maze and opening all locks.\nSee more details in Appendix C.2.\n3.2\nAgent Architectures\nOur base RL agent. For both Crafter and Heist, we test the EnvGen framework with a\nsimple (CNN + linear layer) and lightweight (<5M) agent used in Moon et al. (2023), which\nis slightly modified from the agent architecture used in IMPALA (Espeholt et al., 2018).\nFollowing Moon et al. (2023), we train the agent with a PPO (Schulman et al., 2017) objective.\nAt every step, the agent takes an RGB image (surroundings for Crafter, entire maze for\nHeist) as input and outputs the value estimates and policy (action probability). See Fig. 3 (a)\nfor an agent visual input example. We provide additional model details in Appendix E.\nBaseline methods. For Crafter, we compare our method to two groups of recent baselines –\n(1) methods that use frequent (i.e., more than thousands of) LLM calls during training or\ninference: SPRING (Wu et al., 2023) (based on GPT-4) and ELLM (Du et al., 2023) (based on\nCodex (Chen et al., 2021)) and (2) methods that do not use an LLM: DreamerV3 (Hafner et al.,\n2023), MuZero + SPR (Walker et al., 2023), LSTM-SPCNN (Stani´c et al., 2023), PPO (Schulman\net al., 2017), and Achievement Distillation (AD) (Moon et al., 2023). For Heist, we compare\nagainst the PPO agent. For the PPO and AD agents, we follow the implementation of Moon\net al. (2023). See Appendix E for the PPO\/AD agent details.\n4\nResults and Analysis\nWe demonstrate the usefulness of the EnvGen method with comprehensive experiments\nand analysis. We first compare RL agents trained with EnvGen to different baseline methods\non Crafter, an open-world game with 22 hierarchical achievements (Sec. 4.1). Next, we\npresent a detailed analysis of the improvements that training with EnvGen environments\ncan give RL agents on long-horizon tasks (Sec. 4.2). Then, we analyze how the LLM-based\nenvironment adaptation can help an RL agent progressively improve the skills that the agent\nis weak at (Sec. 4.3). Lastly, we present various additional analysis including experiments\non Heist (a maze navigation game) and ablation studies on EnvGen design choices (Sec. 4.4\nand also in the Appendix C).\n6\nPublished as a conference paper at COLM 2024\nModels\nDescription\n# LLM calls\n# Agent Params\nScore (%)\nReward\nHuman∗\n50.5 ± 6.8\n14.3 ± 2.3\nRandom∗\n1.6 ± 0.0\n2.1 ± 1.3\nELLM* (Du et al., 2023)\n5M step PT in Crafter w\/ Codex reward\n5M\n62M\n-\n6.0 ± 0.4\nLSTM-SPCNN∗(Stani´c et al., 2023)\n135M\n11.7 ± 0.8\n9.3 ± 0.2\nDreamerV3∗(Hafner et al., 2023)\n201M\n14.8 ± 1.4\n10.9 ± 0.5\nMuZero + SPR∗(Walker et al., 2023)\n150M step PT in Crafter w\/ RND reward\n54M\n16.4 ± 1.5\n12.7 ± 0.4\nSPRING* (Wu et al., 2023)\n9 queries to call GPT-4 per step\n2.7K†\nUnknown\n27.3 ± 1.2\n12.3 ± 0.7\nPPO (Moon et al., 2023)\n4M\n15.5 ± 0.6\n10.5 ± 0.6\nPPO (Moon et al., 2023)\n0.96M step PT in Crafter\n4M\n26.4 ± 2.1\n12.1 ± 1.0\nAD* (Moon et al., 2023)\n9M\n21.8 ± 1.4\n12.6 ± 0.3\nAD (Moon et al., 2023)\n0.96M step PT in Crafter\n9M\n31.8 ± 0.7\n13.3 ± 1.2\nPPO + EnvGen (Ours)\n0.96M step PT w\/ CrafterEnvGen\n4\n4M\n32.2 ± 0.6\n12.6 ± 0.6\nAD + EnvGen (Ours)\n0.96M step PT w\/ CrafterEnvGen\n4\n9M\n35.3 ± 0.7\n13.7 ± 0.8\nTable 1: Comparison of different agents in the Crafter (Hafner, 2022) environment. Following\nprevious works, we report the geometric mean of success rates across its 22 achievements\nand rewards for 1M Crafter steps. We experiment with EnvGen on two models, PPO\nand Achievement Distillation. *: scores from the Crafter Scoreboard (Hafner, 2022) and\nMoon et al. (2023). †: average number of LLM calls to run a single episode, according to\nSPRING (Wu et al., 2023). PT: Pretraining; AD: Achievement Distillation.\n4.1\nComparison with State-of-the-art Methods on Crafter Environment\nSmall RL agent trained with EnvGen outperforms state-of-the-art baselines. On the\nCrafter environment (described in Sec. 3.1), we compare a small PPO agent trained in\nCrafterEnvGen (i.e., Crafter environments generated with EnvGen) to state-of-the-art baseline\nmethods. As shown in Table 1, we find that a small (4M parameters) PPO agent with\nEnvGen achieves an average score of 32.2% and significantly outperforms the baselines\n(and also in terms of the average reward). Note that some baseline agents have many more\nparameters or pretraining steps such as SPRING (GPT-4 agent; 27.3%), and MuZero + SPR\n(150M pretraining steps; 16.4%). Our method also only uses orders of magnitude fewer LLM\ncalls (only 4) than works like SPRING (2.7K on average) and ELLM (5M), making it much\ncheaper\/more efficient. EnvGen can also work with other RL agents such as Achievement\nDistillation (AD) (Moon et al., 2023) to achieve an even higher score (35.3%).\n4.2\nDetailed Achievement Analysis on Crafter Environment\nNext, we analyze where EnvGen improves the overall score by checking individual achieve-\nment success rates in detail. For this, we compare the same PPO agent architecture trained\nwith different setups: (1) an agent trained on Crafter for 1.96M steps and (2) an agent trained\non CrafterEnvGen for 0.96M steps (0.24M steps × 4 training cycles, see Sec. 2.2) and then\ntrained on Crafter for 1M steps. We measure the success rate (Fig. 4) of each achievement\nand unlocking speed (Fig. 5) of iron tools in the last 1M training steps.\nEnvGen helps RL agents to tackle challenging long-horizon achievements. Fig. 4 shows\nthat training in CrafterEnvGen improves scores of several achievements. Notably, training in\nCrafterEnvGen significantly improves the scores of long-horizon achievements (with many\nprerequisites; see Fig. 3) such as stone and iron tools. Fig. 5 shows that after unlocking\nthe stone pickaxe, the RL agent trained in CrafterEnvGen is significantly faster in unlocking\niron tools. In Appendix C.1, we also compare two AD agents, and show that CrafterEnvGen\nimproves the success rate of the most challenging achievement – ‘collect diamond’.\n4.3\nAdaptation of Training Environments Helps the Agent Improve Weaker Skills\nFig. 6 shows how the LLM adaptively generates new training environments based on the\nintermediate performance of our PPO-based RL agent. In the intermediate performance\nplots, we compare the baseline agent trained only in Crafter and our RL agent trained in\nCrafterEnvGen. In the cycle 2, given the feedback that the current RL agent is not good at\ncollecting coal, the LLM generates an environment to help the agent focus on it, improving\n7\nPublished as a conference paper at COLM 2024\nCollect Coal\nCollect Diamond\nCollect Drink\nCollect Iron\nCollect Sapling\nCollect Stone\nCollect Wood\nDefeat Skeleton\nDefeat Zombie\nEat Cow\nEat Plant\nMake Iron Pickaxe\nMake Iron Sword\nMake Stone Pickaxe\nMake Stone Sword\nMake Wood Pickaxe\nMake Wood Sword\nPlace Furnace\nPlace Plant\nPlace Stone\nPlace Table\nWake Up\n0.001\n0.01\n0.1\n1\n10\n100\n1.96M Crafter Steps\n0.96M steps in Crafter EnvGen + 1M Crafter Steps (Ours)\nSuccess Rate (%)\nFigure 4: Success rates for all the Crafter achievements of two PPO agents (Moon et al.,\n2023) – (1) Baseline: trained in Crafter for 1.96M steps, and (2) Ours: trained in 0.96M steps\nin CrafterEnvGen and 1M in Crafter.\nMake Iron \nSword\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nPickaxe\nMake Stone \nPickaxe\nMake Iron \nSword\nFigure 5: Unlock times (the first moment when the agent completed an achievement) for\nthree long-horizon achievements (‘make stone pickaxe’, ‘make iron pickaxe’, and ‘make iron\nsword’) of two PPO agents (Moon et al., 2023) – (1) Baseline: trained in Crafter for 1.96M\nsteps, and (2) Ours: trained for 0.96M steps in CrafterEnvGen and for 1M steps in Crafter. The\nplot shows the last 1M training steps out of 1.96M steps.\nthe agent’s performance for the skill. Likewise, in the cycle 3, given the feedback that the\nagent is weak at making stone pickaxes, the LLM generates an environment to help the\nagent more easily craft the stone pickaxe, helping the agent improve it’s score for the skill.\nPowered by the adaptive LLM environment generation of EnvGen, our agent learns to\nunlock these two achievements significantly faster than the baseline agent.\n4.4\nAdditional Analysis and Ablation Studies\nIn the following, we show comprehensive design analysis and ablation studies of EnvGen\nmethod: dynamically updating LLM environments (i.e., using adaptive environments)\nvs. curriculum learning methods, and different frequencies of environment updates. In\nAppendix C, we show comprehensive analysis and ablation studies of EnvGen method:\nEnvGen vs. longer training in the original environment, different LLMs for generating\nenvironments, the number of LLM environments, and the ratio of training steps in the LLM\nvs. original environments. We also include experiments on the Heist environment (see\nSec. 3.1) in Appendix C.2.\nDifferent environment curricula: fixed, easy-to-hard, adversarial vs. adaptive. Table 2\nshows that using LLM environments that are adaptively updated based on intermediate\nagent performance to improve weaker skills (last row) results in overall higher scoring\nagents than just using the initial LLM environments for the whole training (32.2% vs. 29.9%).\n8\nPublished as a conference paper at COLM 2024\nCollect\nCoal\nMake Stone \nPickaxe\nCycle 2\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 1: Introduction to \nMining and Crafting\n```json\n{\n \"target_biome\": \"mountain\",        \n \"wood_pickaxe\": 1,\n ...\n}\n```\nPurpose: This environment is \ndesigned to teach the agent \nhow to mine coal ...\n...\nFeedback from \nprevious cycle\nCollect coal is 2%\nMake stone pickaxe is 1% \n…\nLLM\nCycle 3\nHere are new environments that \ncan help the agent improve the \nlow scores:\n...\nEnvironment 2: Introduce \ncombat and advanced tool \ncrafting\n```json\n{\n \"wood\": 3,\n \"stone\": 2,\n ...\n}\n```\nPurpose: With stone in the \ninventory, the agent is nudged \nto craft a stone pickaxe, a \ntask it hasn't learned yet ...\n...\nFeedback from \nprevious cycle\nCollect coal is 22%\nMake stone pickaxe is 1%\n…\nPerformance \nfrom Cycle 1\nPerformance \nfrom Cycle 3\nPerformance \nfrom Cycle 2\nTrained in \nCrafter\nTrained in \nCrafterEnvGen (Ours)\nLLM\nCollect\nCoal\nMake Stone \nPickaxe\nCollect\nCoal\nMake Stone \nPickaxe\nFigure 6: Adaptation of training environments based on agent performance over EnvGen\ncycles. At the end of each cycle, the RL agent’s performance is given to the LLM as\nfeedback (e.g., ‘Collect coal is 2%’). The LLM uses the feedback to adaptively generate new\nenvironments that can help the agent progressively tackle skills it was previously weak at.\nTraining Curriculum\nScore (%)\nReward\nFixed (no curriculum)\n29.9 ± 0.9\n12.6 ± 0.8\nEasy-to-Hard\n26.8 ± 1.5\n12.7 ± 0.7\nAdversarial\n26.8 ± 0.8\n12.2 ± 0.7\nAdaptive+Dynamic Environments (EnvGen)\n32.2 ± 0.6\n12.6 ± 0.6\nTable 2: Comparison of RL agents trained in Crafter (Hafner, 2022) using no curriculum, an\neasy-to-hard curriculum, an adversarial curriculum, and our adaptive+dynamic environ-\nments. Agents are trained for 0.96M steps using the curriculum and then 1M in the default\nCrafter environment.\nThese results indicate the effectiveness of the agent feedback and environment updating\n(step 4 described in Sec. 2).\nTable 2 also compares an agent trained via EnvGen to the same agent trained with curriculum\nlearning approaches such as an easy-to-hard curriculum, similar to Ammanabrolu et al.\n(2022) (i.e., pre-defined training environment order based on environment difficulty) and\nadversarial curriculum, similar to Parker-Holder et al. (2022) (i.e., updating to training\nenvironments that agent does worse in) in the Crafter environment. Detailed setups of both\nbaseline approaches are in the appendix. The agent trained with EnvGen is able to achieve\nmuch higher performance (32.2% vs. 26.8% for both curricula) indicating the effectiveness\nEnvGen’s approach of adaptively generating training environments to improve agent weak\nskills. The result indicates that creating more difficult environments does not necessarily\nhelp the agent learn new skills over time.\nEnvironment Update Frequency\n# Training cycles NCycle\nScore (%)\nReward\nEvery 0.012M steps\n40 cycles\n30.8 ± 0.7\n12.8 ± 0.6\nEvery 0.06M steps\n8 cycles\n32.1 ± 0.5\n12.7 ± 0.8\nEvery 0.12M steps (default)\n4 cycles\n32.2 ± 0.6\n12.6 ± 0.6\nTable 3: Different frequencies to give feedback to the LLM and update the environments\n(see Sec. 2 for details). Agents are trained with 0.96M steps in CrafterEnvGen and 1M steps in\nCrafter environment.\n9\nPublished as a conference paper at COLM 2024\nFrequency of LLM feedback \/ environment updates.\nTable 3 shows that updating the\nLLM environments at every 0.12M steps results in the best agent performance. While\nincreasing the cycles of environment feedback beyond 4 does not improve further, we\nfind that updating environments with feedback always helps improve the RL agent’s\nperformance compared to training only with the original Crafter environment in Table 1\n(26.4%) or the fixed LLM environment in Table 2 (29.9%).\n5\nRelated Works\nLLMs as open-world game agents. Recent works study using LLMs to create action plans\n(i.e., a list of subgoals or skills to target) for embodied agents in open-world games like\nMinecraft and Crafter (Hafner, 2022). Most of these methods require calling LLMs frequently\n(e.g., at every step) for planning the next steps (Yuan et al., 2023; Wang et al., 2023c; Wu\net al., 2023; Wang et al., 2023a;d; Zhao et al., 2023). Other methods, such as Li et al. (2024);\nKwon et al. (2023); Ma et al. (2023); Du et al. (2023), have used LLMs to create\/adjust\nrewards to train agents. Although these works show initial promising results leveraging the\nworld knowledge of LLMs to tackle long-horizon tasks, iteratively calling LLMs throughout\nepisodes is prohibitively slow and expensive (e.g., running a single episode in the Crafter\nenvironment with SPRING (Wu et al., 2023) costs around $270 USD as they have 2.7K LLM\ncalls on average). EnvGen only calls LLMs a few times (e.g., 4 in total) to create training\nenvironments that focus on helping the RL agent progressively improve its weaker skills.\nDeep learning-based game\/simulator content generation. Procedural content generation\n(PCG) for games is about the automatic generation of levels, landscapes, items, rules, quests,\nor other types of game contents (Shaker et al., 2016). While traditional PCG methods are\nbased on search\/solver\/rule\/grammar-based methods, recent works apply deep learn-\ning methods such as GAN (Goodfellow et al., 2014) for PCG (Liu et al., 2021; Kumaran\net al., 2020; Schubert et al., 2022). Several works have explored using LLMs to generate\ngame content such as difficulty levels (Sudhakaran et al., 2023; Todd et al., 2023) and\nscenes\/environments (Kumaran et al., 2023; Wang et al., 2023b; Afshar & Li, 2024). While\nthese works aim to help developers create new game content, we aim to improve RL agent\nperformance in the original environment. A line of work proposes unsupervised envi-\nronment design (UED) that manipulates the difficulty level of environments to be more\nchallenging to RL agents (Dennis et al., 2020; Jiang et al., 2021; Parker-Holder et al., 2022).\nWhile these works use a learned environment manipulator or evolutionary algorithms to\nmaximize the ‘regret’ (the difference between the expected return of the current and optimal\npolicies) in simple games such as MiniGrid (Chevalier-Boisvert et al., 2023), we use the\nworld knowledge of LLMs to generate and adapt training environments that can improve\nweaker skills based on comprehensive skill-specific feedback from RL agents in open-world\ngames with many challenging long-horizon tasks. To help agents generalize to unseen tasks\nin a text-based dialogue game, Ammanabrolu et al. (2022) augment new tasks with LMs\nand use a manually designed, fixed curriculum. Unlike this work, we adaptively generate\ntraining environments using LLMs’ world knowledge and automatically learning a dynamic\ncurriculum based on the RL agent’s feedback, so as to improve the agent’s weaker skills\nin open-world games with visual inputs. Beyond game content generation, several works\nvisually augment vision-and-language navigation (VLN) simulators (e.g., rendering the\nenvironments with different styles) using image generation models (Li et al., 2022b; Wang\net al., 2023e; Li & Bansal, 2023). Such works could complement our LLM environments (e.g.,\naugmenting our environments with diverse colors and textures).\n6\nConclusion\nWe propose EnvGen, a novel framework to improve embodied RL agent performance by\nutilizing the world knowledge of LLMs to adaptively generate training environments. In\nEnvGen, we give an LLM a prompt describing a game\/simulator and ask the LLM to\ngenerate the configurations to create new environments that can teach different skills. Next,\nwe train an agent in the LLM-generated environments, give feedback to the LLM by testing\nthe agent in the original environments, and then ask the LLM to update the environments to\n10\nPublished as a conference paper at COLM 2024\nteach agents skills they are weaker at. In two challenging games, Crafter and Heist, we show\nthat EnvGen increases agent performance significantly, and training with LLM-generated\nenvironments is more effective than training longer in the original environments. We also\nshow that using an LLM to adapt environments dynamically outperforms curriculum\nlearning approaches and how the LLM adapts training environments to help improve RL\nagents’ weaker skills over time. Moreover, a lightweight model (< 5M parameters) trained\nwith LLM-generated environments even outperforms an LLM agent with significantly fewer\nLLM calls. We hope our work can guide future works in leveraging LLMs for embodied\nagents.\nAcknowledgments\nWe thank Elias Stengel-Eskin and the reviewers for the thoughtful discussion and feedback.\nThis work was supported by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage\nInstitute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031,\nARO Award W911NF2110220, ONR Grant N00014-23-1-2356, and a Bloomberg Data Science\nPh.D. Fellowship. The views contained in this article are those of the authors and not of the\nfunding agency.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | EnvGen：利用LLM生成和适应环境，训练具身智能体\n\n## 📌 背景痛点\/本文动机\n随着具身智能体在开放世界游戏中的兴起，如何让智能体快速学习并掌握各种技能成为了一个挑战。传统的强化学习（RL）方法在处理长时任务时效率低下，而直接使用大型语言模型（LLM）作为智能体虽然性能强大，但调用成本高昂。本文提出了一种新的框架EnvGen，旨在利用LLM的推理能力来生成和适应训练环境，帮助小型RL智能体学习它们不擅长的技能。\n\n## 🚀 核心方法\n💡 创新点1：LLM生成环境\nEnvGen首先通过向LLM提供任务描述和模拟器目标，让LLM生成一系列环境配置，例如不同的地形、初始物品等。这些环境可以并行训练智能体，使其快速学习不同的技能。\n\n💡 创新点2：LLM适应环境\nEnvGen通过将智能体在原始环境中的表现反馈给LLM，让LLM不断调整生成的环境，使其更加专注于智能体不擅长的技能。这种动态适应过程可以帮助智能体逐步提高其技能水平。\n\n## 📈 实验结果\n在Crafter和Heist游戏环境中进行的实验表明，使用EnvGen训练的小型RL智能体在性能上超过了包括GPT-4在内的SOTA方法，并且学习长时任务的速度显著提高。此外，EnvGen的效率也远高于直接使用LLM作为智能体的方法，因为它只需要很少的LLM调用次数。\n\n## 💬 可借鉴之处\nEnvGen提供了一种利用LLM推理能力来提高RL智能体性能的有效方法。它可以应用于各种开放世界游戏和模拟器，帮助智能体快速学习并掌握各种技能。此外，EnvGen的动态适应机制也为RL智能体的训练提供了一种新的思路。","llm_summary_res_status":200}
{"title":"Multi-Stage Episodic Control for Strategic Exploration in Text Games","authors":"Jens Tuyls, Shunyu Yao, Sham Kakade, Karthik Narasimhan","summary":"Text adventure games present unique challenges to reinforcement learning\nmethods due to their combinatorially large action spaces and sparse rewards.\nThe interplay of these two factors is particularly demanding because large\naction spaces require extensive exploration, while sparse rewards provide\nlimited feedback. This work proposes to tackle the explore-vs-exploit dilemma\nusing a multi-stage approach that explicitly disentangles these two strategies\nwithin each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins\neach episode using an exploitation policy that imitates a set of promising\ntrajectories from the past, and then switches over to an exploration policy\naimed at discovering novel actions that lead to unseen state spaces. This\npolicy decomposition allows us to combine global decisions about which parts of\nthe game space to return to with curiosity-based local exploration in that\nspace, motivated by how a human may approach these games. Our method\nsignificantly outperforms prior approaches by 27% and 11% average normalized\nscore over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in\nboth deterministic and stochastic settings, respectively. On the game of Zork1,\nin particular, XTX obtains a score of 103, more than a 2x improvement over\nprior methods, and pushes past several known bottlenecks in the game that have\nplagued previous state-of-the-art methods.","url":"http:\/\/arxiv.org\/abs\/2201.01251v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2201.01251v3","published":1641316792000,"comment":"ICLR 2022 (Spotlight) - https:\/\/sites.google.com\/princeton.edu\/xtx","pdf_text":"Published as a conference paper at ICLR 2022\nMULTI-STAGE EPISODIC CONTROL FOR STRATEGIC\nEXPLORATION IN TEXT GAMES\nJens Tuyls1, Shunyu Yao1, Sham Kakade2 & Karthik Narasimhan1\n1Department of Computer Science, Princeton University\n2John A. Paulson School of Engineering and Applied Sciences, Harvard University\n{jtuyls, shunyuy, karthikn}@princeton.edu, sham@seas.harvard.edu\nABSTRACT\nText adventure games present unique challenges to reinforcement learning meth-\nods due to their combinatorially large action spaces and sparse rewards. The inter-\nplay of these two factors is particularly demanding because large action spaces re-\nquire extensive exploration, while sparse rewards provide limited feedback. This\nwork proposes to tackle the explore-vs-exploit dilemma using a multi-stage ap-\nproach that explicitly disentangles these two strategies within each episode. Our\nalgorithm, called eXploit-Then-eXplore (XTX), begins each episode using an ex-\nploitation policy that imitates a set of promising trajectories from the past, and then\nswitches over to an exploration policy aimed at discovering novel actions that lead\nto unseen state spaces. This policy decomposition allows us to combine global de-\ncisions about which parts of the game space to return to with curiosity-based local\nexploration in that space, motivated by how a human may approach these games.\nOur method signiﬁcantly outperforms prior approaches by 27% and 11% average\nnormalized score over 12 games from the Jericho benchmark (Hausknecht et al.,\n2020) in both deterministic and stochastic settings, respectively. On the game of\nZork1, in particular, XTX obtains a score of 103, more than a 2x improvement\nover prior methods, and pushes past several known bottlenecks in the game that\nhave plagued previous state-of-the-art methods.1\n1\nINTRODUCTION\nText adventure games provide a unique test-bed for algorithms that integrate reinforcement learning\n(RL) with natural language understanding. Aside from the linguistic ingredient, a key challenge\nin these games is the combination of very large action spaces with sparse rewards, which calls for\na delicate balance between exploration and exploitation. For instance, the game of ZORK1 can\ncontain up to ﬁfty valid action commands per state2 to choose from. Importantly, unlike other RL\nenvironments (Bellemare et al., 2013; Todorov et al., 2012), the set of valid action choices does not\nremain constant across states, with unseen actions frequently appearing in later states of the game.\nFor example, Figure 1 shows several states from ZORK1 where a player has to issue unique action\ncommands like ‘kill troll with sword’, ‘echo’ or ‘odysseus’ to progress further in the game. This\nrequires a game-playing agent to perform extensive exploration to determine the appropriateness\nof actions, which is hard to bootstrap from previous experience. On the other hand, since rewards\nare sparse, the agent only gets a few high-scoring trajectories to learn from, requiring vigorous\nexploitation in order to get back to the furthest point of the game and make progress thereon. Prior\napproaches to solving these games (He et al., 2016a; Hausknecht et al., 2020; Ammanabrolu &\nHausknecht, 2020; Guo et al., 2020) usually employ a single policy and action selection strategy,\nmaking it difﬁcult to strike the right balance between exploration and exploitation.\nIn this paper, we propose eXploit-Then-eXplore (XTX), an algorithm for multi-stage control to ex-\nplicitly decompose the exploitation and exploration phases within each episode. In the ﬁrst phase,\nthe agent selects actions according to an exploitation policy which is trained using self-imitation\n1Our code is available at https:\/\/github.com\/princeton-nlp\/XTX\n2Valid actions are a feature in the Jericho simulator (Hausknecht et al., 2020) to improve computational\ntractability for RL. Without this handicap, the number of possible action commands is almost 200 billion.\n1\narXiv:2201.01251v3  [cs.CL]  16 Mar 2022\nPublished as a conference paper at ICLR 2022\n…\n“East\"\n“West”\n…\n“Move rug”\n“Open window”\n…\n…\nScore: 10\nScore: 35\nCellar\nLiving Room\nWest of House\nScore: 0\nTroll Room\nScore: 35\nLoud Room\nScore: 40\nCyclops Room\nScore: 40\n“You are \nstanding in an \nopen field …”\n“It is pitch \nblack …”\n“… you have a  \nlantern and a \nsword …”\n“… a troll \nblocks all \npassages …”\n“Kill troll \nwith sword”\n“Echo”\n“Odysseus”\n“… The room is \ndeafeningly \nloud …”\n“A cyclops, \nwho looks \nprepared …”\n… …\n“Go up”\n“Down”\nFigure 1: Sample game paths and state observations from ZORK1. Starting from the leftmost\nstate (‘West of House’), the agent encounters several novel and unique valid actions (e.g Odysseus,\nEcho) (in brown) across different states in the game. In order to make progress, our algorithm\n(XTX) strategically re-visits different frontiers in the state space (red and blue circles) and performs\nstrategic local exploration to overcome bottleneck states (e.g. ‘Troll Room’) and dead-ends (e.g.\n‘Cellar’). Solid borders indicate visited states, dotted ones indicate potential future states.\nlearning on a mixture of promising trajectories from its past experience sampled using a combina-\ntion of factors such as episodic scores and path length. This policy allows the agent to return to a\nstate at the frontier of the state space it has explored so far. Importantly, we ensure that this policy\nis trained on a mixture of trajectories with different scores, in order to prevent the agent from falling\ninto a local minimum in the state space (e.g. red space in Figure 1). In the second phase, an explo-\nration policy takes over and the agent chooses actions using a value function that is trained using a\ncombination of a temporal difference (TD) loss and an auxiliary inverse dynamics loss (Pathak et al.,\n2017). This allows the agent to perform strategic exploration around the frontier by reusing values\nof previously seen actions while exploring novel ones in order to ﬁnd rewards and make progress in\nthe game. To allow for more ﬁne-grained control, we use a mixture of policies for both exploration\nand exploitation, and only change a single interpolation parameter to switch between phases.\nThe two-stage approach to gameplay in XTX allows an agent to combine global decisions about\nwhich parts of the game space to advance, followed by local exploration of sub-strategies in that\nspace. This is similar to how humans tackle these games: if a player were to lose to a troll in the\ndungeon, they would immediately head back to the dungeon after the game restarts and explore\nstrategies thereon to try and defeat the troll. XTX’s multi-stage episodic control differs from prior\napproaches that add exploration biases to a single policy through curiosity bonuses (Pathak et al.,\n2017; Tang et al., 2017) or use different reward functions to train a separate exploration policy (Colas\net al., 2018; Sch¨afer et al., 2021; Whitney et al., 2021). Moreover, in contrast to methods like Go-\nExplore (Ecoffet et al., 2021; Madotto et al., 2020), XTX does not have global phases of random\nexploration followed by learning —instead, both our policies are continuously updated with new\nexperience, allowing XTX to adapt and scale as the agent goes deeper into the game. XTX also does\nnot make any assumptions about the environment being deterministic, and does not require access\nto underlying game simulator or additional memory archives to keep track of game trees.\nWe evaluate XTX on a set of games from the Jericho benchmark (Hausknecht et al., 2020), consider-\ning both deterministic and stochastic variants of the games. XTX outperforms competitive baselines\non all 12 games, and achieves an average improvement of 5.8% in terms of normalized scores across\nall games. For instance, on Zork1, our method obtains a score of 103 in the deterministic setting and\n67 in the stochastic setting — substantial improvements over baseline scores of 44 and 41, respec-\ntively. We also perform ablation studies to demonstrate the importance of the multi-stage approach,\nas well as several key design choices in our exploitation and exploration policies.\n2\nRELATED WORK\nReinforcement learning for text-based games Prior work on building autonomous agents for text\nadventure games has explored several variants of reinforcement learning (RL) agents equipped with\na language understanding module (see Osborne et al. (2021) for a detailed survey). Innovations on\n2\nPublished as a conference paper at ICLR 2022\nthe language representation side include using deep neural networks for handling text sequences\ntrained using RL (Narasimhan et al., 2015; He et al., 2016a), knowledge graphs to track states\nacross trajectories (Ammanabrolu & Hausknecht, 2020; Adhikari et al., 2020; Xu et al., 2020), and\nincorporating question answering or reading comprehension modules (Ammanabrolu et al., 2020;\nGuo et al., 2020). While these approaches focus mainly on the issues of partial observability and\nlanguage semantics, they all suffer from challenges due to the large action space and sparse rewards\nfound in games from benchmarks like Jericho (Hausknecht et al., 2020). Some approaches aim to\nnavigate the large action space by ﬁltering inadmissible actions (Zahavy et al., 2018; Jain et al.,\n2020), leveraging pre-trained language models for action selection (Yao et al., 2020; Jang et al.,\n2020) or word embeddings for affordance detection (Fulda et al., 2017). Recent work has also\nexplored tackling sparse rewards by employing hierarchical policies (Xu et al., 2021).\nNavigating the exploration-exploitation trade-off in RL The trade-off between exploration and\nexploitation is a well-known issue in RL (Sutton & Barto, 2018; Franc¸ois-Lavet et al., 2018; Kearns\n& Singh, 2002; Brafman & Tennenholtz, 2002). In this respect, we can broadly categorize prior\ntechniques into two types. The ﬁrst type includes methods with mixed objectives that balance ex-\nploration with exploitation. Oh et al. (2018) introduced the idea of self-imitation learning on high-\nscoring episodes to exploit good trajectories, as an auxiliary objective to standard actor-critic meth-\nods. Prior work has also explored the addition of curiosity bonuses to encourage exploration (Pathak\net al., 2017; Tang et al., 2017; Li et al., 2020; Bellemare et al., 2016; Machado et al., 2020; Taiga\net al., 2021).\nWhile we leverage self-imitation learning for exploitation and inverse dynamics\nbonuses for exploration, we use a multi-stage mixed policy. Other works learn a mixture of policies\nfor decoupling exploration and exploitation, either by using a conditional architecture with shared\nweights (Badia et al., 2020), pre-deﬁning an exploration mechanism for restricted policy optimiza-\ntion (Shani et al., 2019), or learning separate task and exploration policies to maximize different\nreward functions (Colas et al., 2018; Sch¨afer et al., 2021; Whitney et al., 2021). While we also train\nmultiple policies, our multi-stage algorithm performs distinct exploitation and exploration phases\nwithin each episode, not requiring pre-deﬁned exploration policies or phases. Further, we consider\nenvironments with signiﬁcantly larger action spaces that evolve dynamically as the game progresses.\nThe second class of algorithms explicitly separate exploitation and exploration in each episode.\nMethods like E3 (Kearns & Singh, 2002; Henaff, 2019) maintain a set of dynamics models to en-\ncourage exploration. Policy-based Go-Explore (Ecoffet et al., 2021) uses self-imitation learning\nto ‘exploit’ high-reward trajectories, but requires choosing intermediate sub-goals for the agent to\ncondition its policy on. PC-PG (Agarwal et al., 2020) uses a policy cover to globally choose state\nspaces to return to, followed by random exploration. Compared to these approaches, we perform\nmore strategic local exploration due to the use of a Q-function with inverse dynamics bonus and do\nnot require any assumptions about determinism or linearity of the MDP. We provide a more technical\ndiscussion on the novelty of our approach at the end of Section 3.\nDirected exploration in text-based games As previously mentioned, the large dynamic action\nspace in text games warrant speciﬁc strategies for directed exploration. Ammanabrolu et al. (2020)\nused a knowledge-graph based intrinsic motivation reward to encourage exploration. Jang et al.\n(2020) incorporated language semantics into action selection for planning using MCTS. Both meth-\nods utilize the determinism of the game or require access to a simulator to restart the game from\nspeciﬁc states. Madotto et al. (2020) modiﬁed the Go-Explore algorithm to test generalization in the\nCoinCollector (Yuan et al., 2018) and Cooking world domains (Cˆot´e et al., 2018). Their method has\ntwo phases — the agent ﬁrst randomly explores and collects trajectories and then a policy is learned\nthrough imitation of the best trajectories in the experience replay buffer. In contrast, our algorithm\nprovides for better exploration of new, unseen actions in later stages of the game through the use\nof an inverse dynamics module and performs multiple rounds of imitation learning for continuous\nscaling to deeper trajectories in the game. Recently, Yao et al. (2021) used inverse dynamics to\nimprove exploration and Yao et al. (2020) used a language model to generate action candidates that\nguide exploration. However, both approaches did not employ a two-stage rollout like our work, and\nthe latter considers a different setup without any valid action handicap.\n3\nPublished as a conference paper at ICLR 2022\n3\nMETHOD\nBackground Text-adventure games can be formalized as a Partially Observable Markov Decision\nProcess (POMDP) ⟨S, T, A, O, R, γ⟩. The underlying state space S contains all conﬁgurations of\nthe game state within the simulator, which is unobserved by the agent. The agent receives obser-\nvations from O from which it has to infer the underlying state s ∈S. The action set A consists\nof short phrases from the game vocabulary, T(s′|s, a) is the transition function which determines\nthe probability of moving to the next state s′ given the agent has taken action a in state s, R(s, a)\ndetermines the instantaneous reward, and γ is the reward discount factor.\nExisting RL approaches that tackle these games usually learn a value function using game rewards.\nOne example is the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016b) which\ntrains a deep neural network with parameters φ to approximate Qφ(o, a). This model encodes each\nobservation o and action candidate a using two recurrent networks fo and fa and aggregates the rep-\nresentations to derive the Q-value through an MLP q: Qφ(o, a) = q(fo(o), fa(a)). The parameters\nφ of the model are trained by minimizing the temporal difference (TD) loss on tuples (o, a, r, o′) of\nobservation, action, reward and the next observation sampled from an experience replay buffer:\nLTD(φ) = (r + γ max\na′∈A Qφ(o′, a′) −Qφ(o, a))2\n(1)\nThe agent samples actions using a softmax exploration policy π(a|o; φ) ∝exp(Qφ(o, a)).\nChallenges There are two unique aspects of text adventure games that make them challenging. First,\nthe action space is combinatorially large – usually games can accept action commands of 1-4 words\nwith vocabularies of up to 2257 words. This means an agent potentially has to choose between\nO(22574) = 2.6 ∗1013 actions at each state of the game. To make this problem more tractable,\nbenchmarks like Jericho (Hausknecht et al., 2020) provide a valid action detector that ﬁlters out the\nset of inadmissible commands (i.e. commands that are either unrecognized by the game engine or\ndo not change the underlying state of the game). However, this still results in the issue of dynamic\naction spaces for the agent that change with the state. For example, the action ‘echo’ is unique to\nthe Loud Room (see Figure 1). Second, these games have very sparse rewards (see Appendix A.1)\nand several bottleneck states (Ammanabrolu et al., 2020), making learning difﬁcult for RL agents\nthat use the same policy for exploration and exploitation (He et al., 2016a; Hausknecht et al., 2020;\nAmmanabrolu & Riedl, 2019). This results in issues of derailment (Ecoffet et al., 2021), with agents\nunable to return to promising parts of the state space, resulting in a substantial gap between average\nepisode performance of the agent and the maximum score it sees in the game (Yao et al., 2020).\n3.1\nOUR ALGORITHM: EXPLOIT-THEN-EXPLORE (XTX)\nWe tackle the two challenges outlined above using a multi-stage control policy that allows an agent\nto globally pick promising states of the game to visit while allowing for strategic local exploration\nthereafter. To this end, we develop eXploit-Then-eXplore, where an agent performs the following\ntwo distinct phases of action selection in each episode.\nPHASE 1: Exploitation In the exploitation phase, the agent makes a global decision about revisiting\npromising states of the game it has seen in its past episodes. We sample k trajectories from the\nexperience replay D using a combination of factors such as game score and trajectory length. These\ntrajectories are then used to learn a policy cover πexploit using self-imitation learning (Oh et al.,\n2018) (see Section 3.2 for details). The agent then samples actions from πexploit until it has reached\neither (1) the maximum score seen during training or (2) a number of steps in the episode equal to\nthe longest of the k sampled trajectories. The second condition ensures the agent can always return\nto the longest of the k sampled trajectories3. Once either of the above conditions is satisﬁed, the\nagent transitions to the exploration phase, adjusting its policy.\nPHASE 2: Exploration In the exploration phase, the agent uses a different policy πexplore trained\nusing both a temporal difference loss and an auxiliary inverse dynamics bonus (Pathak et al., 2017;\nYao et al., 2021) (see Section 3.3 for details). The intuition here is that the exploitation policy\nπexploit in phase 1 has brought the agent to the game frontier, which is under-explored and may\n3It is possible that the agent needs less steps to return to a particular part of the game space and hence wastes\nsome steps, but we empirically didn’t ﬁnd this to be a problem.\n4\nPublished as a conference paper at ICLR 2022\ncontain a combination of common (e.g. “open door”) and novel (e.g. “kill troll with sword”) ac-\ntions. Therefore, using a combination of Q-values and inverse dynamics bonus enables the agent to\nperform strategic, local exploration to expand the frontier and discover new rewarding states. The\nagent continues sampling from πexplore until a terminal state or episode limit is reached.\n3.1.1\nMIXTURE OF POLICIES FOR FINE-GRAINED CONTROL\nWhile one could employ two completely disjoint policies for the exploitation and exploration phases,\nwe choose a more general approach of having a policy mixture, with a single parameter λ that can\nbe varied to provide more ﬁne-grained control during the two phases. Speciﬁcally, we deﬁne:\nπλ(a|c, o; θ, φ, ξ) = λπinv−dy(a|o; θ, φ) + (1 −λ)πil(a|c; ξ).\n(2)\nHere, πinv−dy refers to an exploration-inducing policy trained using TD loss with an inverse-\ndynamics bonus (Section 3.3) and is parameterized by θ and φ. πil refers to an exploitation-inducing\npolicy trained through self-imitation learning (Oh et al., 2018) (Section 3.2) and is parameterized\nby ξ. Note that the action distribution over actions a induced by πinv−dy is conditioned only on the\ncurrent observation o, while the one induced by πil is conditioned on context c which is an augmen-\ntation of o with past information. We can observe that λ provides a trade-off between exploration\n(high λ) and exploitation (low λ). In our experiments, we choose a small, dynamic value, λ =\n1\n2∗T\nfor exploitation (where T is episode limit) and λ = 1 for exploration. As we demonstrate later\n(Section 4.2), the non-zero λ in exploitation is critical for the agent to avoid getting stuck in regions\nof local minima (e.g. Zork1). We now describe the individual components of the mixture.\n3.2\nIMITATION LEARNING FOR BUILDING A GLOBAL POLICY COVER (πil)\nWe parameterize the imitation policy πil using a Transformer model (Vaswani et al., 2017) based\non the GPT-2 architecture (Radford et al., 2019) that takes in a context c = [at−2; at−1; ot], i.e.\nthe concatenation of two most recent past actions along with the current observation separated by\n[SEP] tokens, and outputs a sequence of hidden representations h0, . . . , hm where m is the number\nof tokens in the sequence. hm is then projected to vocabulary size by multiplication with the output\nembedding matrix, after which softmax is applied to get probabilities for the next action token. In-\nspired by (Yao et al., 2020), the GPT-2 model is trained to predict the next action at given the context\nc using a language modeling objective (Equation 5). The training data consists of k trajectories sam-\npled from an experience replay memory D which stores transition tuples (ct, at, rt, ot+1, terminal).\nSampling trajectories Let us deﬁne a trajectory τ as a sequence of observations, actions, and re-\nwards, i.e. τ = o1, a1, r1, o2, a2, r2 . . . , ol+1, where lτ denotes the trajectory length (i.e. number\nof actions) and thus l ≤T where T is the episode limit. We sample a trajectory from D using a\ntwo-step process. First, we sample a score u from a categorical distribution:\nP(u) ∝exp (β1(u −µU)\/σU) ,\nu ∈U\n(3)\nwhere U is the set of all unique scores encountered in the game so far, µU is the mean of the set\nU, and σU is its standard deviation. β1 is the temperature and determines how biased the sampling\nprocess is towards high scores. The second step collects all trajectories with the sampled score u\nand samples a trajectory τ based on the trajectory length lτ:\nP(τ | u) ∝exp (−β2(lτ −µLu)\/σLu) ,\nlτ ∈Lu\n(4)\nwhere Lu is the multiset of trajectory lengths lτ with score u, µLu is the mean of the elements\nin Lu, and σLu is its standard deviation. β2 deﬁnes the temperature and determines the strength\nof the bias towards shorter length trajectories. We perform this sampling procedure k times (with\nreplacement) to obtain a trajectory buffer B on which we perform imitation learning. This allows\nthe agent to globally explore the game space by emulating promising experiences from its past, with\na bias towards trajectories with high game score and shorter lengths. The motivation for sampling\nshorter length trajectories among the ones that reach the same score is because those tend to be the\nones that waste less time performing meaningless actions (e.g. “pick up sword”, “drop sword”, etc.).\nLearning from trajectories Given the trajectory buffer B containing single-step (c, a) pairs of\ncontext c and actions a from the trajectories sampled in the previous step, we train πil by minimizing\nthe cross-entropy loss over action commands (Yao et al., 2020):\nL(ξ) = −E(c,a)∼B log πil(a|c; ξ),\n(5)\n5\nPublished as a conference paper at ICLR 2022\nwhere c deﬁnes the context of past observations and actions as before, and ξ deﬁnes the parameters\nof the GPT-2 model. We perform several passes of optimization through the trajectory buffer B until\nconvergence4 and periodically perform this optimization every n epochs in gameplay to update πil.\nFurthermore, πil is renormalized over the valid action set Av during gameplay. Note that while πil is\ntrained similarly to the GPT-2 model in (Yao et al., 2020), their model generates action candidates.\n3.3\nEFFICIENT LOCAL EXPLORATION WITH INVERSE DYNAMICS (πinv−dy)\nIn the second phase of our algorithm, we would like to use a policy that tackles (1) the large action\nspace and (2) the dynamic nature of the action set at every step in the game, which makes it crucial\nto keep trying under-explored actions and is difﬁcult for the Q network alone to generalize over. To\nthis end, we use the inverse dynamics model (INV-DY) from (Yao et al., 2021). INV-DY is a Q-\nbased policy πinv−dy similar to DRRN (He et al., 2016a), optimized with the standard TD loss (see\nBackground of Section 3). In addition, it adds an auxiliary loss Linv capturing an inverse dynamics\nprediction error (Pathak et al., 2017), which is added as an intrinsic reward to the game reward\n(r = rgame + α1 ∗Linv) and hence incorporated into the TD loss. Formally, Linv is deﬁned as:\nLinv(θ, φ) = −log pd(a|ginv(concat(fo(o), fo(o′))),\n(6)\nwhere θ denotes the parameters for the recurrent decoder d and the MLP ginv (neither of which are\nused in πinv−dy during gameplay to score the actions), and fo is the encoder deﬁned in Section 3.\nThis loss is optimized together with the TD loss as well as with an action decoding loss Ldec to\nobtain the following overall objective that is used to train πinv−dy:\nL(φ, θ) = LTD + α2Linv(φ, θ) + α3Ldec(φ, θ),\n(7)\nwhere Ldec(φ, θ) = −log pd(a|fa(a)). Here, fa is a recurrent network (see Section 3). Please refer\nto Yao et al. (2021) for details. We train the model by sampling batches of transitions from a prior-\nitized experience replay buffer D (Schaul et al., 2015) and performing stochastic gradient descent.\nInverse dynamics ameliorates the challenges (1) and (2) mentioned above by rewarding underex-\nplored actions (i.e. a high loss in Equation 6) and by generalizing over novel action commands.\nSpeciﬁcally, the INV-DY network might generalize through learning of past bonuses to what new\nactions would look like and hence identify novel actions before having tried them once.\n3.4\nEPISODIC ROLLOUTS WITH XTX (ALGORITHM 1)\nWe now describe how XTX operates in a single episode. The agent starts in phase 1, where actions at\nare sampled from πexploit. Following this exploitation policy brings the agent to the game frontier,\nwhich we estimate to happen when either (1) the current episode score ≥M, the maximum score in\nthe trajectory buffer B or when (2) the current time step t > lmax, the length of the longest trajectory\nin B. The agent then enters phase 2 and switches its strategy to sample actions only from πinv−dy\nby setting λ = 1. At every time step t during all phases, a transition tuple (ct, at, rt, ot+1, terminal)\nis added to the replay buffer D. The policy πexploit is updated every n episodes using the process in\nSection 3.2, while πexplore is updated within episodes at every step using the TD loss in equation 7,\nsampling high rewarding trajectories with priority fraction ρ, similar to (Guo et al., 2020)5.\n3.5\nNOVELTY IN COMPARISON TO PRIOR ALGORITHMS\nWe now more explicitly discuss comparisons to a few other approaches. The most closely related ap-\nproaches are multi-stage algorithms, including Go-Explore (Ecoffet et al., 2021) and PC-PG (Agar-\nwal et al., 2020) (and somewhat the E3 algorithm (Kearns & Singh, 2002; Henaff, 2019)). Both of\nthese algorithms can be viewed as approaches which explicitly use a “roll-in” policy, with the goal\nof visiting a novel region of the state-action space. Go-Explore is limited to deterministic MDPs\n(where it is easy to re-visit any state in the past), while PC-PG (applicable to more general MDPs\nwith provable guarantees under certain linearity assumptions) more explicitly builds a set of policies\n(‘policy cover’) capable of visiting different regions of the state space. However, in both of these\n4We empirically ﬁnd 40 passes to be sufﬁcient for convergence.\n5We slightly differ from their approach as we only prioritize transitions from trajectories that achieve the\nmaximum score seen so far.\n6\nPublished as a conference paper at ICLR 2022\nAlgorithm 1 The eXploit-Then-eXplore (XTX) algorithm\n1: Initialize prioritized replay memory D to capacity N with priority fraction ρ.\n2: Initialize πexploit (with parameters ξ) and πexplore (with parameters θ and φ).\n3: Set max score M, max length lmax in B to 0.\n4: Exploration steps R = 50; episode limit T = 50\n5: for episode ←1, . . . , E do\n6:\nfor t ←1, . . . , T do\n7:\nReceive observation ot and valid action set Av ⊂A for current state.\n8:\nif current episode score < M and t < T −R then\n▷PHASE 1\n9:\nλ ←\n1\n2∗T\n10:\nelse\n▷PHASE 2\n11:\nλ ←1\n12:\nend if\n13:\nSample an action at from policy πλ(at|ot, at−1, at−2; φ, θ, ξ).\n▷Equation 2\n14:\nStep with at and receive (rt, ot+1, terminal) from game engine.\n15:\nStore transition tuple (ct, at, rt, ot+1, terminal) in D.\n16:\nUpdate πinv−dy using TD loss with inverse dynamics.\n▷Equation 7\n17:\nend for\n18:\nif n episodes have passed then\n19:\nSample k trajectories from D to form the new trajectory buffer B.\n▷Section 3.2\n20:\nUpdate πil with cross-entropy loss.\n▷Equation 5\n21:\nUpdate M, lmax and set T ←lmax + R.\n22:\nend if\n23: end for\napproaches, once the agent reaches a novel part of the state-space, the agent acts randomly. A key\ndistinction in our approach is that once the agent reaches a novel part of the state space, it uses\nan exploration with novelty bonuses, which may more effectively select promising actions over a\nrandom behavioral policy in large action spaces.\nThe other broad class of approaches that handle exploration use novelty bonuses, with either a policy\ngradient approach or in conjunction with Q-learning (see Section 2). The difﬁculty with the former\nclass of algorithms is the “catastrophic forgetting” effect (see Agarwal et al. (2020) for discussion).\nThe difﬁculty with Q-learning approaches (with a novelty bonus) is that bootstrapping approaches,\nwith function approximation, can be unstable in long planning horizon problems (sometimes re-\nferred to as the “deadly triad” (Jiang et al., 2021)). While XTX also uses Q-learning (with a novelty\nbonus), we only use this policy (πinv−dy) in the second phase of the algorithm in contrast to (Yao\net al., 2021) where πinv−dy is used throughout the entire episode; our hope is that this instability\ncan be alleviated since we are effectively using Q-learning to solve a shorter horizon exploration\nproblem (as opposed to globally using Q-learning, with a novelty bonus).\n4\nEXPERIMENTS\nEnvironments We evaluate on 12 human-created games across several genres from the Jericho\nbenchmark (Hausknecht et al., 2020). They provide a variety of challenges such as darkness, non-\nstandard actions, inventory management, and dialog (Hausknecht et al., 2020). At every step t, the\nobservation from the Jericho game engine contains a description of the state, which is augmented\nwith location and inventory information (by issuing “look” and “inventory” commands) to form\not (Hausknecht et al., 2019). In addition, we use of the valid action handicap provided by Jericho,\nwhich ﬁlters actions to remove those that do not change the underlying game state. We found this\naction handicap to be imperfect for some games (marked with * in Table 1), and manually added\nsome actions required for agent progress from game walkthroughs to the game engine’s grammar.\nEvaluation We evaluate agents under two settings: (a) a deterministic setting where the transition\ndynamics T(s′|s, a) is a one-hot vector over all the next states s′ and (b) a stochastic setting6 where\nthe T(s′|s, a) deﬁnes a distribution over next states s′, and the observations o can be perturbed with\n6Only six games have stochastic variants, and DRAGON was left out due to memory issues in the baselines.\n7\nPublished as a conference paper at ICLR 2022\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n40.3\n55.0\n44.1\n105.0\n41.7\n53.0\n34.1\n52.3\n103.4\n152.7\n+17%\n350\nINHUMANE*\n34.8\n56.7\n27.7\n63.3\n29.8\n53.3\n59.2\n76.7\n64.0\n76.7\n+5%\n90\nLUDICORP*\n17.1\n48.7\n19.6\n49.3\n10.9\n40.7\n67.3\n86.0\n78.8\n91.0\n+8%\n150\nZORK3*\n0.3\n4.3\n0.5\n5.0\n3.0\n5.0\n3.8\n4.7\n4.2\n5.0\n+6%\n7\nPENTARI*\n45.6\n58.3\n34.5\n53.3\n33.4\n46.7\n43.4\n60.0\n49.6\n60.0\n+6%\n70\nDETECTIVE\n289.9\n320.0\n289.5\n323.3\n269.3\n346.7\n296.0\n336.7\n312.2\n340.0\n+4%\n360\nBALANCES*\n14.1\n25.0\n12.5\n25.0\n10.0\n18.3\n21.9\n25.0\n24.0\n26.7\n+4%\n51\nLIBRARY*\n24.8\n30.0\n24.7\n30.0\n24.2\n30.0\n26.1\n30.0\n28.5\n30.0\n+8%\n30\nDEEPHOME*\n58.8\n68.0\n58.9\n72.7\n1.0\n1.0\n52.6\n70.0\n77.7\n92.3\n+6%\n300\nENCHANTER*\n42.0\n66.7\n44.2\n63.3\n26.8\n38.3\n24.3\n28.3\n52.0\n66.7\n+2%\n400\nDRAGON7\n-3.7\n8.0\n-2.3\n8.7\n3.2\n8.0\n40.7\n126.0\n96.7\n127.0\n0%\n25\nOMNIQUEST\n8.2\n10.0\n9.9\n13.3\n9.3\n10.0\n8.6\n10.0\n11.6\n13.3\n+3%\n50\nAvg. Norm Score\n29.5%\n48.8%\n28.4%\n51.8%\n29.7%\n44.5%\n49.2%\n58.6%\n56.3%\n64.0%\n5.8%\n100%\nTable 1:\nResults on deterministic games for the best XTX model, where the inverse dynamics\nscaling coefﬁcient α1 was tuned per game. We outperform the baselines on all 12 games, achieving\nan average normalized game score of 56%. * indicates actions were added to the game grammar.\n∆indicates the absolute performance difference between XTX and the best baseline on Avg scores.\nScores are averaged across 3 seeds. Baselines were rerun with the latest Jericho version.\nirrelevant sentences such as “you hear in the distance the chirping of a song bird”. We report both\nthe episode score average (Avg) over the last 100 episodes at the end of training, as well as the\nmaximum score (Max) seen in any episode during training.\nBaselines We consider four baselines. 1) DRRN (He et al., 2016a): This model uses a Q-based\nsoftmax policy, i.e. π ∝exp(Q(o, a; φ)), parameterized using GRU encoders and decoders, and\ntrained using the TD loss (Equation 1). 2) INV-DY (Yao et al., 2021): Refer to Section 3.3. 3)\nRC-DQN (Guo et al., 2020): This is a state-of-the-art model that uses an object-centric reading\ncomprehension (RC) module to encode observations and output actions. The training loss is that of\nDRRN above, and during gameplay, the agent uses an ϵ-greedy strategy. 4) XTX-Uniform (∼Go-\nExplore): Here, we replace πinv−dy with a policy that samples actions uniformly during Phase 2,\nkeeping all other factors of our algorithm the same. This is closest to a version of the Go-Explore\nalgorithm (Ecoffet et al., 2021) that returns to promising states and performs random exploration.\nHowever, while conceptually similar to Go-Explore, XTX-Uniform does not make use of any addi-\ntional memory archives, and avoids training a goal-based policy. See Appendix A.2 for implemen-\ntation details and hyperparameters.\n4.1\nRESULTS\nDeterministic games We report results in Table 1 (refer to Appendix A.5 and A.6 for more details).\nOverall, XTX outperforms DRRN, INV-DY, RC-DQN, and XTX-Uniform by 27%, 28%, 27%, and\n7% respectively, in terms of average normalized game score (i.e. average episode score divided\nby max score, averaged over all the games). Our algorithm performs particularly well on Zork1,\nachieving a 17% absolute improvement in average episode score and a 14% improvement in average\nmaximum score compared to the best baseline. In particular, XTX manages to advance past several\ndocumented bottlenecks like the dark Cellar (see Figure 1) which have proved to be very challenging\nfor existing methods (Ammanabrolu et al., 2020). While performance with XTX-Uniform is some-\ntimes close, exploration with inverse dynamics instead of random exploration pushes past several\nbottlenecks present in Zork1 and leads to signiﬁcant gains on Deephome, Enchanter, Omniquest,\nand Ludicorp, showing the potential usefulness of strategic exploration at the game frontier.\nStochastic games To show the robustness of XTX to stochasticity, we evaluate our agent in the\nstochastic setting (Table 2). XTX outperforms the baselines on 4 out of 5 games, and pushes past the\nmaximum scores of XTX-Uniform on the same fraction. Especially impressive is the performance\non Zork1, which is still higher than the state-of-the-art score in the deterministic setting.\n4.2\nABLATION STUDIES\nIn order to evaluate the importance of the various components in XTX, we perform several ablations\non a subset of the games as described below and shown in Figure 2 (more in Appendix A.3 and A.4).\n7Interestingly, XTX manages to achieve a very high score on Dragon by exploiting an integer underﬂow bug.\n8\nPublished as a conference paper at ICLR 2022\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n41.3\n55.7\n36.9\n85.7\n40.3\n53.0\n31.2\n48.0\n67.7\n143.0\n+8%\n350\nZORK3*\n0.2\n4.0\n0.4\n4.7\n2.7\n4.7\n2.3\n4.3\n2.6\n5.0\n-1%\n7\nPENTARI*\n38.2\n60.0\n37.5\n55.0\n33.3\n41.7\n38.8\n60.0\n47.3\n60.0\n+12%\n70\nDEEPHOME*\n43.0\n65.7\n58.4\n73.0\n1.0\n1.0\n50.7\n69.3\n70.9\n96.0\n+4%\n300\nENCHANTER*\n42.0\n56.7\n34.5\n53.3\n27.1\n43.3\n30.2\n45.0\n44.8\n58.3\n+1%\n400\nAvg. Norm Score\n18.9%\n39.0%\n19.7%\n41.5%\n20.9%\n30.5%\n24.3%\n39.1%\n31.8%\n48.9%\n4.8%\n100%\nTable 2: Results on stochastic games. We outperform baselines on 4 out of 5 games, with an average\nnormalized game score of 32%. Scores are averaged across 3 seeds. Baselines were rerun with the\nlatest Jericho version.\nZork1\nInhumane\nZork3\nLudicorp\nDeephome\nOmniquest\nGame\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Score\nAblation\nPure IL (  = 0)\nPure Inv Dy (  = 1)\nMix (  = 0.5)\nXTX (no-mix)\nXTX (full model)\nFigure 2: Average episode scores for 4 ablation models across 6 games. Overall, we ﬁnd both the\nstrategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm.\nPure imitation learning (λ = 0) This ablation sets λ = 0 in equation 2, meaning the agent will\nalways use the imitation learning policy πil. As expected, this model performs quite badly since it\nis based on pure exploitation and is hence unlikely to reach deep states in the game.\nPure inverse dynamics (λ = 1) This ablation sets λ = 1 in equation 2, hence always using the\ninverse dynamics exploration policy πinv−dy, resulting in the model proposed in (Yao et al., 2021).\nWhile this model can sometimes achieve high maximum scores, it is unable to learn from these and\nhence its average episode score remains quite low, consistent with ﬁndings in (Yao et al., 2021).\nMixing exploration and exploitation (λ = 0.5) By setting λ = 0.5, this ablation constantly alter-\nnates between exploitation and exploration, never committing to one or the other. This causes the\nagent to suffer from issues of both the λ = 0 and λ = 1 models, resulting in weak results.\nPure separation of exploitation and exploration (XTX no-mix) In this ablation, we examine the\nimportance of having a mixture policy in Phase 1 of the algorithm instead of setting λ = 0 in Phase 1\nand to 1 in Phase 2. This explicitly separated model, denoted as XTX (no-mix), performs a bit better\nin the games of Inhumane and Zork3, but sometimes fails to push past certain stages in Ludicorp\nand completely gets stuck in the game of Zork1. This shows it is crucial to have a mixture policy in\nPhase 1 in order to get past bottleneck states in difﬁcult games.\n5\nCONCLUSION\nWe have proposed XTX, an algorithm for multi-stage episodic control in text adventure games. XTX\nexplicitly disentangles exploitation and exploration into different policies, which are used by the\nagent for action selection in different phases of the same episode. Decomposing the policy allows\nthe agent to combine global decisions on which state spaces in the environment to (re-)explore, fol-\nlowed by strategic local exploration that can handle novel, unseen actions – aspects that help tackle\nthe challenges of sparse rewards and dynamic action spaces in these games. Our method signiﬁ-\ncantly outperforms prior methods on the Jericho benchmark (Hausknecht et al., 2020) under both\ndeterministic and stochastic settings, and even surpasses several challenging bottlenecks in games\nlike Zork1 (Ammanabrolu et al., 2020). Future work can integrate our algorithm with approaches\nthat better leverage linguistic signals to achieve further progress in these games.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGEMENTS\nWe thank the members of the Princeton NLP group and the anonymous reviewers for their valuable\nfeedback. JT was supported by a graduate fellowship at Princeton University. We are grateful to the\nGoogle Cloud Research program for computational support in running our experiments. We would\nalso like to thank Matthew Hausknecht for all the help regarding the Jericho environment.\nETHICAL CONSIDERATIONS\nThis work focuses on building better agents for text-adventure games and hence does not have im-\nmediate direct ethical concerns. However, the techniques introduced in this paper may be generally\nuseful for other autonomous agents that combine sequential decision making with language under-\nstanding (e.g. dialog systems). As such agents become more capable and inﬂuential in our lives, it\nis important to make sure their objectives align with those of humans, and that they are free of bias.\nREPRODUCIBILITY\nOur code is publicly available here https:\/\/github.com\/princeton-nlp\/XTX. We pro-\nvide all implementation details such as hyperparameters, model architectures and training regimes\nin Appendix A.2. We used Weights & Biases for experiment tracking and visualizations to develop\ninsights for this paper.\nREFERENCES\nAshutosh Adhikari, Xingdi (Eric) Yuan, Marc-Alexandre Cˆot´e, Mikulas Zelinka, Marc-Antoine\nRondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton\n. Learning dynamic knowledge graphs to generalize on text-based games. In NeurIPS 2020, 2020.\nAlekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-\nration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.\nDeep reinforcement learning at the edge of the statistical precipice. Advances in Neural Informa-\ntion Processing Systems, 34, 2021.\nPrithviraj Ammanabrolu and Matthew J. Hausknecht. Graph constrained reinforcement learning for\nnatural language action spaces. In 8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:\n\/\/openreview.net\/forum?id=B1x6w0EtwH.\nPrithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep\nreinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pp. 3557–3565, Minneapolis, Minnesota, 2019. Association for Computational\nLinguistics. doi: 10.18653\/v1\/N19-1358. URL https:\/\/www.aclweb.org\/anthology\/\nN19-1358.\nPrithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. How to avoid being eaten\nby a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795,\n2020.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven\nKapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, et al. Never\ngive up: Learning directed exploration strategies. In International Conference on Learning Rep-\nresentations, 2020.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.\nUnifying count-based exploration and intrinsic motivation. Advances in neural information pro-\ncessing systems, 29:1471–1479, 2016.\n10\nPublished as a conference paper at ICLR 2022\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-\nronment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,\n2013.\nRonen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-\noptimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213–231, 2002.\nC´edric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.\nGEP-PG: Decoupling exploration and\nexploitation in deep reinforcement learning algorithms.\nIn Jennifer Dy and Andreas Krause\n(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of\nProceedings of Machine Learning Research, pp. 1039–1048. PMLR, 10–15 Jul 2018.\nURL\nhttps:\/\/proceedings.mlr.press\/v80\/colas18a.html.\nMarc-Alexandre Cˆot´e, ´Akos K´ad´ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler.\nTextworld: A learning environment for text-based games. CoRR, 2018.\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then\nexplore. Nature, 590(7847):580–586, 2021.\nVincent Franc¸ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An\nintroduction to deep reinforcement learning. arXiv preprint arXiv:1811.12560, 2018.\nNancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock?\naffordance extraction via word embeddings. In Carles Sierra (ed.), Proceedings of the Twenty-\nSixth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, Melbourne, Australia,\nAugust 19-25, 2017, pp. 1039–1045. ijcai.org, 2017. doi: 10.24963\/ijcai.2017\/144. URL https:\n\/\/doi.org\/10.24963\/ijcai.2017\/144.\nXiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. Interac-\ntive ﬁction game playing as multi-paragraph reading comprehension with reinforcement learn-\ning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pp. 7755–7765, Online, 2020. Association for Computational Linguistics.\ndoi: 10.18653\/v1\/2020.emnlp-main.624. URL https:\/\/www.aclweb.org\/anthology\/\n2020.emnlp-main.624.\nMatthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. Nail:\nA general interactive ﬁction agent. arXiv preprint arXiv:1902.04259, 2019.\nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Cˆot´e, and Xingdi Yuan. Inter-\nactive ﬁction games: A colossal adventure. Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 34(05):7903–7910, Apr. 2020.\ndoi: 10.1609\/aaai.v34i05.6297.\nURL https:\n\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/6297.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep\nreinforcement learning with a natural language action space. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1621–\n1630, Berlin, Germany, August 2016a. Association for Computational Linguistics. doi: 10.18653\/\nv1\/P16-1153. URL https:\/\/aclanthology.org\/P16-1153.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep\nreinforcement learning with a natural language action space. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1621–\n1630, Berlin, Germany, 2016b. Association for Computational Linguistics. doi: 10.18653\/v1\/\nP16-1153. URL https:\/\/www.aclweb.org\/anthology\/P16-1153.\nMikael Henaff.\nExplicit explore-exploit algorithms in continuous state spaces.\narXiv preprint\narXiv:1911.00617, 2019.\nVishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G. Bellemare. Algorithmic\nimprovements for deep reinforcement learning applied to interactive ﬁction. In The Thirty-Fourth\n11\nPublished as a conference paper at ICLR 2022\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applica-\ntions of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020,\npp. 4328–4336. AAAI Press, 2020. URL https:\/\/aaai.org\/ojs\/index.php\/AAAI\/\narticle\/view\/5857.\nYoungsoo Jang, Seokin Seo, Jongmin Lee, and Kee-Eung Kim. Monte-carlo planning and learning\nwith language action value estimates. In International Conference on Learning Representations,\n2020.\nRay Jiang, Tom Zahavy, Zhongwen Xu, Adam White, Matteo Hessel, Charles Blundell, and Hado\nVan Hasselt. Emphatic algorithms for deep reinforcement learning. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume\n139. PMLR, 2021.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-\nchine learning, 49(2):209–232, 2002.\nJing Li, Xinxin Shi, Jiehao Li, Xin Zhang, and Junzheng Wang. Random curiosity-driven explo-\nration in deep reinforcement learning. Neurocomputing, 418:139–147, 2020.\nMarlos C. Machado, Marc G. Bellemare, and Michael Bowling. Count-based exploration with the\nsuccessor representation. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(04):\n5125–5133, Apr. 2020. doi: 10.1609\/aaai.v34i04.5955. URL https:\/\/ojs.aaai.org\/\nindex.php\/AAAI\/article\/view\/5955.\nAndrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng,\nAlexandros Papangelis, Dian Yu, Chandra Khatri, and G¨okhan T¨ur. Exploration based language\nlearning for text-based games. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artiﬁcial Intelligence, IJCAI 2020, pp. 1488–1494. ijcai.org, 2020.\ndoi: 10.24963\/ijcai.2020\/207. URL https:\/\/doi.org\/10.24963\/ijcai.2020\/207.\nKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based\ngames using deep reinforcement learning. In Proceedings of the 2015 Conference on Empir-\nical Methods in Natural Language Processing, pp. 1–11, Lisbon, Portugal, 2015. Association\nfor Computational Linguistics. doi: 10.18653\/v1\/D15-1001. URL https:\/\/www.aclweb.\norg\/anthology\/D15-1001.\nJunhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International\nConference on Machine Learning, pp. 3878–3887. PMLR, 2018.\nPhilip Osborne, Heido N˜omm, and Andre Freitas. A survey of text games for reinforcement learning\ninformed by natural language. arXiv preprint arXiv:2109.09478, 2021.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pp. 2778–2787.\nPMLR, 2017.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\nLukas Sch¨afer, Filippos Christianos, Josiah Hanna, and Stefano V. Albrecht. Decoupling exploration\nand exploitation in reinforcement learning. CoRR, abs\/2107.08966, 2021. URL https:\/\/\narxiv.org\/abs\/2107.08966.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv\npreprint arXiv:1511.05952, 2015.\nLior Shani, Yonathan Efroni, and Shie Mannor. Exploration conscious reinforcement learning re-\nvisited. In International Conference on Machine Learning, pp. 5680–5689. PMLR, 2019.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n12\nPublished as a conference paper at ICLR 2022\nAdrien Ali Taiga, William Fedus, Marlos C Machado, Aaron Courville, and Marc G Belle-\nmare. On bonus-based exploration methods in the arcade learning environment. arXiv preprint\narXiv:2109.11052, 2021.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep\nreinforcement learning. In 31st Conference on Neural Information Processing Systems (NIPS),\nvolume 30, pp. 1–18, 2017.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE\/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.\nIEEE, 2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nWilliam F. Whitney, Michael Bloesch, Jost Tobias Springenberg, Abbas Abdolmaleki, and Martin A.\nRiedmiller. Decoupled exploration and exploitation policies for sample-efﬁcient reinforcement\nlearning. CoRR, abs\/2101.09458, 2021. URL https:\/\/arxiv.org\/abs\/2101.09458.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. Deep rein-\nforcement learning with stacked hierarchical attention for text-based games. Advances in Neural\nInformation Processing Systems, 33, 2020.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, and Chengqi Zhang. Generalization in text-based\ngames via hierarchical reinforcement learning. arXiv preprint arXiv:2109.09968, 2021.\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\nLanguage models for action generation in text-based games. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online,\nNovember 2020. Association for Computational Linguistics. doi: 10.18653\/v1\/2020.emnlp-main.\n704. URL https:\/\/aclanthology.org\/2020.emnlp-main.704.\nShunyu Yao, Karthik Narasimhan, and Matthew Hausknecht.\nReading and acting while blind-\nfolded: The need for semantics in text game agents. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pp. 3097–3102, Online, June 2021. Association for Computational Linguis-\ntics. doi: 10.18653\/v1\/2021.naacl-main.247. URL https:\/\/aclanthology.org\/2021.\nnaacl-main.247.\nXingdi Yuan, Marc-Alexandre Cˆot´e, Alessandro Sordoni, Romain Laroche, Remi Tachet des\nCombes, Matthew J. Hausknecht, and Adam Trischler. Counting to explore and generalize in\ntext-based games. CoRR, 2018.\nTom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, and Shie Mannor. Learn what\nnot to learn: Action elimination with deep reinforcement learning. In Samy Bengio, Hanna M.\nWallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.),\nAdvances in Neural Information Processing Systems 31: Annual Conference on Neural Infor-\nmation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp.\n3566–3577, 2018. URL https:\/\/proceedings.neurips.cc\/paper\/2018\/hash\/\n645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html.\n13\nPublished as a conference paper at ICLR 2022\nA\nAPPENDIX\nA.1\nGAME STATISTICS\nGame\nZORK1\nINHUMANE\nLUDICORP\nLIBRARY\nZORK3\nPENTARI\nAvg.\/Max\n9 \/ 51\n14 \/ 28\n4 \/ 45\n5 \/ 6\n39 \/ 41\n5 \/ 16\nGame\nDETECTIVE\nBALANCES\nDEEPHOME\nENCHANTER\nDRAGON\nOMNIQUEST\nAvg.\/Max\n2 \/ 5\n12 \/ 54\n6 \/ 53\n15 \/ 40\n9 \/ 24\n13 \/ 26\nTable 3: Average and maximum number of steps between rewards for games in Jericho (based on\nhuman walkthroughs). Several games have long sequences of actions without reward.\nTable 3 contains the average and maximum number of steps between rewards in these games, show-\ncasing their challenging nature.\nA.2\nIMPLEMENTATION DETAILS\nWe use a learning rate of 10−3 and 10−4 for πil and πinv−dy, respectively. Both policies are trained\non batches of size 64, with hidden dimensions of size 128. The scaling coefﬁcient α1 for the inverse\ndynamics intrinsic reward is set to 1 for all games except for Deephome (α1 = 0.1), Enchanter\n(α1 = 0.5), Omniquest (α1 = 2), Ludicorp (α1 = 0.5), Detective (α1 = 2), and Pentari (α1 = 2).\nThe Transformer πil has 3 layers and 4 attention heads. β1 in equation 3 is set to 1, β2 in equation 4\nis set to 10k to encourage picking the shortest length trajectory, and k is set to 10. In equation 6,\nα1 = α2 = 1. The priority fraction ρ is set to 0.5. Every time πil is trained, we also scale the\nepisode length T to have at least R remaining steps of exploration by setting T = lmax + R, where\nlmax is the length of the longest trajectory in the trajectory buffer B. In practice, R = 50, and hence\nthe agent will be guaranteed at least 50 steps of exploration each episode. XTX and DRRN are run\nfor 800k interaction steps, while RC-DQN which is run for 100k interaction steps following Guo\net al. (2020).\nA.3\nFULL SET OF ABLATIONS\nZork1\nInhumane\nZork3\nLudicorp\nDeephome\nOmniquest\nGame\n0.0\n0.2\n0.4\n0.6\n0.8\nNormalized Score\nAblation\nPure IL (  = 0)\nPure Inv Dy (  = 1)\nMix (  = 0.5)\nXTX (no-mix)\nXTX (full model)\nEnchanter\nPentari\nBalances\nDetective\nLibrary\nDragon\nGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Score\nFigure 3: Average episode scores for 4 ablation models across 12 games. Overall, we ﬁnd both the\nstrategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm.\nScores for dragon were clipped to be between 0 and 1.\n14\nPublished as a conference paper at ICLR 2022\nA.4\nABLATION TRAINING PLOTS\nFigure 4: Average episode score throughout training for all ablations on Zork1. Shaded areas\nindicate one standard deviation.\nFigure 5: Average episode score throughout training for all ablations on Inhumane. Shaded areas\nindicate one standard deviation.\n15\nPublished as a conference paper at ICLR 2022\nFigure 6: Average episode score throughout training for all ablations on Zork3. Shaded areas\nindicate one standard deviation.\nFigure 7: Average episode score throughout training for all ablations on Ludicorp. Shaded areas\nindicate one standard deviation.\n16\nPublished as a conference paper at ICLR 2022\nFigure 8: Average episode score throughout training for all ablations on Balances. Shaded areas\nindicate one standard deviation.\nFigure 9: Average episode score throughout training for all ablations on Deephome. Shaded areas\nindicate one standard deviation.\n17\nPublished as a conference paper at ICLR 2022\nFigure 10: Average episode score throughout training for all ablations on Detective. Shaded areas\nindicate one standard deviation.\nFigure 11: Average episode score throughout training for all ablations on Enchanter. Shaded areas\nindicate one standard deviation.\n18\nPublished as a conference paper at ICLR 2022\nFigure 12: Average episode score throughout training for all ablations on Omniquest. Shaded areas\nindicate one standard deviation.\nFigure 13: Average episode score throughout training for all ablations on Pentari. Shaded areas\nindicate one standard deviation.\n19\nPublished as a conference paper at ICLR 2022\nFigure 14: Average episode score throughout training for all ablations on Dragon. Shaded areas\nindicate one standard deviation.\nFigure 15: Average episode score throughout training for all ablations on Library. Shaded areas\nindicate one standard deviation.\n20\nPublished as a conference paper at ICLR 2022\nA.5\nFULL DETERMINISTIC AND STOCHASTIC RESULTS\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n40.3 (2.2)\n55.0 (6.4)\n44.1 (12.6)\n105.0 (19.9)\n41.7 (0.6)\n53.0 (0.0)\n34.1 (1.5)\n52.3 (3.8)\n103.4 (10.9)\n152.7 (1.7)\n+17%\n350\nINHUMANE\n31.0 (1.0)\n56.7 (4.7)\n28.1 (3.6)\n60.0 (0.0)\n31.8 (1.4)\n63.3 (4.7)\n68.9 (8.9)\n83.3 (9.4)\n60.9 (5.9)\n70.0 (14.1)\n-9%\n90\nINHUMANE*\n34.8 (3.9)\n56.7 (4.7)\n27.7 (5.3)\n63.3 (4.7)\n29.8 (2.3)\n53.3 (4.7)\n59.2 (1.2)\n76.7 (9.4)\n64.0 (7.7)\n76.7 (9.4)\n+5%\n90\nLUDICORP\n15.6 (0.1)\n23.0 (0.0)\n15.6 (0.2)\n23.0 (0.0)\n12.4 (1.1)\n21.0 (2.2)\n19.9 (0.4)\n23.0 (0.0)\n20.9 (0.1)\n23.0 (0.0)\n+1%\n150\nLUDICORP*\n17.1 (1.7)\n48.7 (2.1)\n19.6 (5.5)\n49.3 (16.2)\n10.9 (1.7)\n40.7 (2.5)\n67.3 (4.2)\n86.0 (2.8)\n78.8 (5.1)\n91.0 (3.6)\n+8%\n150\nZORK3\n0.3 (0.0)\n4.7 (0.5)\n0.4 (0.0)\n5.0 (0.0)\n3.2 (0.5)\n5.0 (0.0)\n3.7 (0.2)\n4.7 (0.5)\n4.2 (0.0)\n5.0 (0.0)\n+7%\n7\nZORK3*\n0.3 (0.0)\n4.3 (0.5)\n0.5 (0.1)\n5.0 (0.0)\n3.0 (0.3)\n5.0 (0.0)\n3.8 (0.4)\n4.7 (0.5)\n4.2 (0.1)\n5.0 (0.0)\n+6%\n7\nPENTARI\n43.4 (4.5)\n58.3 (2.4)\n29.8 (14.1)\n46.7 (6.2)\n37.4 (7.0)\n46.7 (11.8)\n43.4 (1.7)\n60.0 (0.0)\n45.5 (4.3)\n60.0 (0.0)\n+3%\n70\nPENTARI*\n45.6 (1.9)\n58.3 (2.4)\n34.5 (7.5)\n53.3 (6.2)\n33.4 (6.9)\n46.7 (6.2)\n43.4 (0.4)\n60.0 (0.0)\n49.6 (1.3)\n60.0 (0.0)\n+6%\n70\nDETECTIVE\n289.9 (0.1)\n320.0 (8.2)\n289.5 (0.4)\n323.3 (4.7)\n269.3 (14.8)\n346.7 (4.7)\n296.0 (9.0)\n336.7 (12.5)\n312.2 (10.3)\n340.0 (8.2)\n+4%\n360\nBALANCES\n10.0 (0.0)\n10.0 (0.0)\n9.9 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n9.6 (0.1)\n10.0 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n0%\n51\nBALANCES*\n14.1 (0.4)\n25.0 (0.0)\n12.5 (1.6)\n25.0 (0.0)\n10.0 (0.1)\n18.3 (2.4)\n21.9 (0.4)\n25.0 (0.0)\n24.0 (0.3)\n26.7 (2.4)\n+4%\n51\nLIBRARY\n17.3 (0.7)\n21.0 (0.0)\n17.0 (0.2)\n21.0 (0.0)\n16.2 (1.4)\n21.0 (0.0)\n18.8 (0.4)\n21.0 (0.0)\n19.7 (0.5)\n21.0 (0.0)\n+3%\n30\nLIBRARY*\n24.8 (0.6)\n30.0 (0.0)\n24.7 (0.4)\n30.0 (0.0)\n24.2 (1.4)\n30.0 (0.0)\n26.1 (0.4)\n30.0 (0.0)\n28.5 (0.3)\n30.0 (0.0)\n+8%\n30\nDEEPHOME\n57.9 (0.4)\n68.7 (0.5)\n44.8 (19.9)\n76.0 (5.0)\n1.0 (0.0)\n1.0 (0.0)\n46.3 (9.0)\n60.7 (13.2)\n75.7 (5.0)\n93.7 (5.6)\n+6%\n300\nDEEPHOME*\n58.8 (0.1)\n68.0 (0.8)\n58.9 (0.2)\n72.7 (3.8)\n1.0 (0.0)\n1.0 (0.0)\n52.6 (0.4)\n70.0 (0.8)\n77.7 (2.1)\n92.3 (3.3)\n+6%\n300\nENCHANTER\n46.1 (11.1)\n70.0 (21.2)\n46.0 (3.6)\n73.3 (8.5)\n25.8 (8.5)\n36.7 (14.3)\n43.4 (18.9)\n53.3 (23.6)\n34.7 (21.2)\n36.7 (23.6)\n-3%\n400\nENCHANTER*\n42.0 (1.2)\n66.7 (2.4)\n44.2 (18.3)\n63.3 (30.6)\n26.8 (1.9)\n38.3 (4.7)\n24.3 (10.8)\n28.3 (11.8)\n52.0 (23.1)\n66.7 (33.0)\n+2%\n400\nDRAGON\n-3.7 (0.4)\n8.0 (0.0)\n-2.3 (0.5)\n8.7 (1.7)\n3.2 (1.6)\n8.0 (0.0)\n40.7 (0.0)\n126.0 (0.0)\n96.7 (1.1)\n127.0 (0.0)\n0%\n25\nOMNIQUEST\n8.2 (0.1)\n10.0 (0.0)\n9.9 (0.0)\n13.3 (2.4)\n9.3 (0.7)\n10.0 (0.0)\n8.6 (0.1)\n10.0 (0.0)\n11.6 (1.3)\n13.3 (2.4)\n+3%\n50\nAvg. Norm Score\n29.5% (29.8)\n48.8% (28.8)\n28.4% (27.2)\n51.8% (27.3)\n29.7% (25.6)\n44.5% (32.1)\n49.2% (30.4)\n58.6% (33.6)\n56.3% (28.1)\n64.0% (28.6)\n5.8% (4.1)\n100%\nTable 4: Full Deterministic Results. Standard deviations are in parentheses. Scores are averaged\nacross 3 seeds. Note that the average normalized scores only take into account the games listed in\nTable 1. Baselines were rerun with the latest Jericho version.\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n41.3 (3.2)\n55.7 (3.3)\n36.9 (2.4)\n85.7 (14.8)\n40.3 (1.6)\n53.0 (0.0)\n31.2 (1.1)\n48.0 (5.0)\n67.7 (8.0)\n143.0 (10.7)\n+8%\n350\nZORK3\n0.2 (0.0)\n4.3 (0.5)\n0.7 (0.2)\n5.0 (0.0)\n2.7 (0.0)\n5.0 (0.0)\n1.8 (0.1)\n4.0 (0.0)\n2.7 (0.4)\n5.0 (0.0)\n0%\n7\nZORK3*\n0.2 (0.0)\n4.0 (0.0)\n0.4 (0.3)\n4.7 (0.5)\n2.7 (0.1)\n4.7 (0.5)\n2.3 (0.5)\n4.3 (0.5)\n2.6 (0.6)\n5.0 (0.0)\n-1%\n7\nPENTARI\n42.3 (0.8)\n60.0 (0.0)\n28.9 (8.5)\n45.0 (0.0)\n31.2 (3.9)\n38.3 (11.8)\n38.4 (1.3)\n60.0 (0.0)\n48.2 (0.4)\n60.0 (0.0)\n+8%\n70\nPENTARI*\n38.2 (3.6)\n60.0 (0.0)\n37.5 (8.0)\n55.0 (7.1)\n33.3 (6.0)\n41.7 (10.3)\n38.8 (0.4)\n60.0 (0.0)\n47.3 (0.4)\n60.0 (0.0)\n+12%\n70\nDEEPHOME\n58.2 (0.6)\n72.0 (5.7)\n58.2 (0.5)\n72.7 (2.5)\n1.0 (0.0)\n1.0 (0.0)\n48.0 (10.1)\n62.0 (14.2)\n73.9 (4.3)\n99.3 (13.9)\n+5%\n300\nDEEPHOME*\n43.0 (20.0)\n65.7 (3.3)\n58.4 (0.5)\n73.0 (1.4)\n1.0 (0.0)\n1.0 (0.0)\n50.7 (2.3)\n69.3 (0.9)\n70.9 (2.7)\n96.0 (7.8)\n+4%\n300\nENCHANTER\n41.0 (0.6)\n71.7 (9.4)\n38.9 (14.5)\n63.3 (30.6)\n25.0 (4.0)\n30.0 (7.1)\n32.1 (10.9)\n53.3 (23.6)\n46.2 (18.9)\n51.7 (22.5)\n+1%\n400\nENCHANTER*\n42.0 (18.5)\n56.7 (27.2)\n34.5 (10.3)\n53.3 (23.6)\n27.1 (2.7)\n43.3 (8.5)\n30.2 (9.1)\n45.0 (20.4)\n44.8 (19.4)\n58.3 (27.8)\n+1%\n400\nAvg. Norm Score\n18.9% (18.2)\n39.0% (28.1)\n19.7% (17.5)\n41.5% (26.0)\n20.9% (18.6)\n30.5% (27.1)\n24.3% (17.9)\n39.1% (29.6)\n31.8% (19.8)\n48.9% (26.0)\n4.8% (4.7)\n100%\nTable 5: Full Stochastic Results. Standard deviations are in parentheses. Scores are averaged\nacross 3 seeds. Note that the average normalized scores only take into account the games listed in\nTable 2. Baselines were rerun with the latest Jericho version.\nA.6\nAGGREGATE METRICS & PERFORMANCE PROFILES\n0.24 0.32 0.40 0.48 0.56\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX\nMedian\n0.24 0.32 0.40 0.48 0.56\nIQM\n0.30 0.36 0.42 0.48 0.54\nMean\n0.48 0.54 0.60 0.66 0.72\nOptimality Gap\nNormalized Score\nFigure 16: Aggregate metrics with 95% CIs for the deterministic games listed in Table 1, follow-\ning Agarwal et al. (2021). The CIs use percentile bootstrap with stratiﬁed sampling.\n0.12 0.15 0.18 0.21 0.24\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX\nMedian\n0.12 0.16 0.20 0.24 0.28\nIQM\n0.20\n0.24\n0.28\n0.32\nMean\n0.68\n0.72\n0.76\n0.80\n0.84\nOptimality Gap\nNormalized Score\nFigure 17: Aggregate metrics with 95% CIs for the stochastic games listed in Table 2, follow-\ning Agarwal et al. (2021). The CIs use percentile bootstrap with stratiﬁed sampling.\n21\nPublished as a conference paper at ICLR 2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Score ( )\n0.00\n0.25\n0.50\n0.75\n1.00\nFraction of runs with score >\nDRRN\nINV-DY\nXTX-Uniform\nXTX\nRC-DQN\nFigure 18: Performance proﬁles based on score distributions for the deterministic games listed in\nTable 1, following Agarwal et al. (2021). Shaded regions show pointwise 95% conﬁdence bands\nbased on percentile bootstrap with stratiﬁed sampling.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Score ( )\n0.00\n0.25\n0.50\n0.75\n1.00\nFraction of runs with score >\nDRRN\nINV-DY\nXTX-Uniform\nXTX\nRC-DQN\nFigure 19: Performance proﬁles based on score distributions for the stochastic games listed in\nTable 2, following Agarwal et al. (2021). Shaded regions show pointwise 95% conﬁdence bands\nbased on percentile bootstrap with stratiﬁed sampling.\n22\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Multi-Stage Episodic Control for Strategic Exploration in Text Games.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nMulti-Stage Episodic Control for Strategic Exploration in Text Games\n```\n#### 2. 论文摘要\n```\nText adventure games present unique challenges to reinforcement learning\nmethods due to their combinatorially large action spaces and sparse rewards.\nThe interplay of these two factors is particularly demanding because large\naction spaces require extensive exploration, while sparse rewards provide\nlimited feedback. This work proposes to tackle the explore-vs-exploit dilemma\nusing a multi-stage approach that explicitly disentangles these two strategies\nwithin each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins\neach episode using an exploitation policy that imitates a set of promising\ntrajectories from the past, and then switches over to an exploration policy\naimed at discovering novel actions that lead to unseen state spaces. This\npolicy decomposition allows us to combine global decisions about which parts of\nthe game space to return to with curiosity-based local exploration in that\nspace, motivated by how a human may approach these games. Our method\nsignificantly outperforms prior approaches by 27% and 11% average normalized\nscore over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in\nboth deterministic and stochastic settings, respectively. On the game of Zork1,\nin particular, XTX obtains a score of 103, more than a 2x improvement over\nprior methods, and pushes past several known bottlenecks in the game that have\nplagued previous state-of-the-art methods.\n```\n\n#### 3. 论文全文\n```\nPublished as a conference paper at ICLR 2022\nMULTI-STAGE EPISODIC CONTROL FOR STRATEGIC\nEXPLORATION IN TEXT GAMES\nJens Tuyls1, Shunyu Yao1, Sham Kakade2 & Karthik Narasimhan1\n1Department of Computer Science, Princeton University\n2John A. Paulson School of Engineering and Applied Sciences, Harvard University\n{jtuyls, shunyuy, karthikn}@princeton.edu, sham@seas.harvard.edu\nABSTRACT\nText adventure games present unique challenges to reinforcement learning meth-\nods due to their combinatorially large action spaces and sparse rewards. The inter-\nplay of these two factors is particularly demanding because large action spaces re-\nquire extensive exploration, while sparse rewards provide limited feedback. This\nwork proposes to tackle the explore-vs-exploit dilemma using a multi-stage ap-\nproach that explicitly disentangles these two strategies within each episode. Our\nalgorithm, called eXploit-Then-eXplore (XTX), begins each episode using an ex-\nploitation policy that imitates a set of promising trajectories from the past, and then\nswitches over to an exploration policy aimed at discovering novel actions that lead\nto unseen state spaces. This policy decomposition allows us to combine global de-\ncisions about which parts of the game space to return to with curiosity-based local\nexploration in that space, motivated by how a human may approach these games.\nOur method signiﬁcantly outperforms prior approaches by 27% and 11% average\nnormalized score over 12 games from the Jericho benchmark (Hausknecht et al.,\n2020) in both deterministic and stochastic settings, respectively. On the game of\nZork1, in particular, XTX obtains a score of 103, more than a 2x improvement\nover prior methods, and pushes past several known bottlenecks in the game that\nhave plagued previous state-of-the-art methods.1\n1\nINTRODUCTION\nText adventure games provide a unique test-bed for algorithms that integrate reinforcement learning\n(RL) with natural language understanding. Aside from the linguistic ingredient, a key challenge\nin these games is the combination of very large action spaces with sparse rewards, which calls for\na delicate balance between exploration and exploitation. For instance, the game of ZORK1 can\ncontain up to ﬁfty valid action commands per state2 to choose from. Importantly, unlike other RL\nenvironments (Bellemare et al., 2013; Todorov et al., 2012), the set of valid action choices does not\nremain constant across states, with unseen actions frequently appearing in later states of the game.\nFor example, Figure 1 shows several states from ZORK1 where a player has to issue unique action\ncommands like ‘kill troll with sword’, ‘echo’ or ‘odysseus’ to progress further in the game. This\nrequires a game-playing agent to perform extensive exploration to determine the appropriateness\nof actions, which is hard to bootstrap from previous experience. On the other hand, since rewards\nare sparse, the agent only gets a few high-scoring trajectories to learn from, requiring vigorous\nexploitation in order to get back to the furthest point of the game and make progress thereon. Prior\napproaches to solving these games (He et al., 2016a; Hausknecht et al., 2020; Ammanabrolu &\nHausknecht, 2020; Guo et al., 2020) usually employ a single policy and action selection strategy,\nmaking it difﬁcult to strike the right balance between exploration and exploitation.\nIn this paper, we propose eXploit-Then-eXplore (XTX), an algorithm for multi-stage control to ex-\nplicitly decompose the exploitation and exploration phases within each episode. In the ﬁrst phase,\nthe agent selects actions according to an exploitation policy which is trained using self-imitation\n1Our code is available at https:\/\/github.com\/princeton-nlp\/XTX\n2Valid actions are a feature in the Jericho simulator (Hausknecht et al., 2020) to improve computational\ntractability for RL. Without this handicap, the number of possible action commands is almost 200 billion.\n1\narXiv:2201.01251v3  [cs.CL]  16 Mar 2022\nPublished as a conference paper at ICLR 2022\n…\n“East\"\n“West”\n…\n“Move rug”\n“Open window”\n…\n…\nScore: 10\nScore: 35\nCellar\nLiving Room\nWest of House\nScore: 0\nTroll Room\nScore: 35\nLoud Room\nScore: 40\nCyclops Room\nScore: 40\n“You are \nstanding in an \nopen field …”\n“It is pitch \nblack …”\n“… you have a  \nlantern and a \nsword …”\n“… a troll \nblocks all \npassages …”\n“Kill troll \nwith sword”\n“Echo”\n“Odysseus”\n“… The room is \ndeafeningly \nloud …”\n“A cyclops, \nwho looks \nprepared …”\n… …\n“Go up”\n“Down”\nFigure 1: Sample game paths and state observations from ZORK1. Starting from the leftmost\nstate (‘West of House’), the agent encounters several novel and unique valid actions (e.g Odysseus,\nEcho) (in brown) across different states in the game. In order to make progress, our algorithm\n(XTX) strategically re-visits different frontiers in the state space (red and blue circles) and performs\nstrategic local exploration to overcome bottleneck states (e.g. ‘Troll Room’) and dead-ends (e.g.\n‘Cellar’). Solid borders indicate visited states, dotted ones indicate potential future states.\nlearning on a mixture of promising trajectories from its past experience sampled using a combina-\ntion of factors such as episodic scores and path length. This policy allows the agent to return to a\nstate at the frontier of the state space it has explored so far. Importantly, we ensure that this policy\nis trained on a mixture of trajectories with different scores, in order to prevent the agent from falling\ninto a local minimum in the state space (e.g. red space in Figure 1). In the second phase, an explo-\nration policy takes over and the agent chooses actions using a value function that is trained using a\ncombination of a temporal difference (TD) loss and an auxiliary inverse dynamics loss (Pathak et al.,\n2017). This allows the agent to perform strategic exploration around the frontier by reusing values\nof previously seen actions while exploring novel ones in order to ﬁnd rewards and make progress in\nthe game. To allow for more ﬁne-grained control, we use a mixture of policies for both exploration\nand exploitation, and only change a single interpolation parameter to switch between phases.\nThe two-stage approach to gameplay in XTX allows an agent to combine global decisions about\nwhich parts of the game space to advance, followed by local exploration of sub-strategies in that\nspace. This is similar to how humans tackle these games: if a player were to lose to a troll in the\ndungeon, they would immediately head back to the dungeon after the game restarts and explore\nstrategies thereon to try and defeat the troll. XTX’s multi-stage episodic control differs from prior\napproaches that add exploration biases to a single policy through curiosity bonuses (Pathak et al.,\n2017; Tang et al., 2017) or use different reward functions to train a separate exploration policy (Colas\net al., 2018; Sch¨afer et al., 2021; Whitney et al., 2021). Moreover, in contrast to methods like Go-\nExplore (Ecoffet et al., 2021; Madotto et al., 2020), XTX does not have global phases of random\nexploration followed by learning —instead, both our policies are continuously updated with new\nexperience, allowing XTX to adapt and scale as the agent goes deeper into the game. XTX also does\nnot make any assumptions about the environment being deterministic, and does not require access\nto underlying game simulator or additional memory archives to keep track of game trees.\nWe evaluate XTX on a set of games from the Jericho benchmark (Hausknecht et al., 2020), consider-\ning both deterministic and stochastic variants of the games. XTX outperforms competitive baselines\non all 12 games, and achieves an average improvement of 5.8% in terms of normalized scores across\nall games. For instance, on Zork1, our method obtains a score of 103 in the deterministic setting and\n67 in the stochastic setting — substantial improvements over baseline scores of 44 and 41, respec-\ntively. We also perform ablation studies to demonstrate the importance of the multi-stage approach,\nas well as several key design choices in our exploitation and exploration policies.\n2\nRELATED WORK\nReinforcement learning for text-based games Prior work on building autonomous agents for text\nadventure games has explored several variants of reinforcement learning (RL) agents equipped with\na language understanding module (see Osborne et al. (2021) for a detailed survey). Innovations on\n2\nPublished as a conference paper at ICLR 2022\nthe language representation side include using deep neural networks for handling text sequences\ntrained using RL (Narasimhan et al., 2015; He et al., 2016a), knowledge graphs to track states\nacross trajectories (Ammanabrolu & Hausknecht, 2020; Adhikari et al., 2020; Xu et al., 2020), and\nincorporating question answering or reading comprehension modules (Ammanabrolu et al., 2020;\nGuo et al., 2020). While these approaches focus mainly on the issues of partial observability and\nlanguage semantics, they all suffer from challenges due to the large action space and sparse rewards\nfound in games from benchmarks like Jericho (Hausknecht et al., 2020). Some approaches aim to\nnavigate the large action space by ﬁltering inadmissible actions (Zahavy et al., 2018; Jain et al.,\n2020), leveraging pre-trained language models for action selection (Yao et al., 2020; Jang et al.,\n2020) or word embeddings for affordance detection (Fulda et al., 2017). Recent work has also\nexplored tackling sparse rewards by employing hierarchical policies (Xu et al., 2021).\nNavigating the exploration-exploitation trade-off in RL The trade-off between exploration and\nexploitation is a well-known issue in RL (Sutton & Barto, 2018; Franc¸ois-Lavet et al., 2018; Kearns\n& Singh, 2002; Brafman & Tennenholtz, 2002). In this respect, we can broadly categorize prior\ntechniques into two types. The ﬁrst type includes methods with mixed objectives that balance ex-\nploration with exploitation. Oh et al. (2018) introduced the idea of self-imitation learning on high-\nscoring episodes to exploit good trajectories, as an auxiliary objective to standard actor-critic meth-\nods. Prior work has also explored the addition of curiosity bonuses to encourage exploration (Pathak\net al., 2017; Tang et al., 2017; Li et al., 2020; Bellemare et al., 2016; Machado et al., 2020; Taiga\net al., 2021).\nWhile we leverage self-imitation learning for exploitation and inverse dynamics\nbonuses for exploration, we use a multi-stage mixed policy. Other works learn a mixture of policies\nfor decoupling exploration and exploitation, either by using a conditional architecture with shared\nweights (Badia et al., 2020), pre-deﬁning an exploration mechanism for restricted policy optimiza-\ntion (Shani et al., 2019), or learning separate task and exploration policies to maximize different\nreward functions (Colas et al., 2018; Sch¨afer et al., 2021; Whitney et al., 2021). While we also train\nmultiple policies, our multi-stage algorithm performs distinct exploitation and exploration phases\nwithin each episode, not requiring pre-deﬁned exploration policies or phases. Further, we consider\nenvironments with signiﬁcantly larger action spaces that evolve dynamically as the game progresses.\nThe second class of algorithms explicitly separate exploitation and exploration in each episode.\nMethods like E3 (Kearns & Singh, 2002; Henaff, 2019) maintain a set of dynamics models to en-\ncourage exploration. Policy-based Go-Explore (Ecoffet et al., 2021) uses self-imitation learning\nto ‘exploit’ high-reward trajectories, but requires choosing intermediate sub-goals for the agent to\ncondition its policy on. PC-PG (Agarwal et al., 2020) uses a policy cover to globally choose state\nspaces to return to, followed by random exploration. Compared to these approaches, we perform\nmore strategic local exploration due to the use of a Q-function with inverse dynamics bonus and do\nnot require any assumptions about determinism or linearity of the MDP. We provide a more technical\ndiscussion on the novelty of our approach at the end of Section 3.\nDirected exploration in text-based games As previously mentioned, the large dynamic action\nspace in text games warrant speciﬁc strategies for directed exploration. Ammanabrolu et al. (2020)\nused a knowledge-graph based intrinsic motivation reward to encourage exploration. Jang et al.\n(2020) incorporated language semantics into action selection for planning using MCTS. Both meth-\nods utilize the determinism of the game or require access to a simulator to restart the game from\nspeciﬁc states. Madotto et al. (2020) modiﬁed the Go-Explore algorithm to test generalization in the\nCoinCollector (Yuan et al., 2018) and Cooking world domains (Cˆot´e et al., 2018). Their method has\ntwo phases — the agent ﬁrst randomly explores and collects trajectories and then a policy is learned\nthrough imitation of the best trajectories in the experience replay buffer. In contrast, our algorithm\nprovides for better exploration of new, unseen actions in later stages of the game through the use\nof an inverse dynamics module and performs multiple rounds of imitation learning for continuous\nscaling to deeper trajectories in the game. Recently, Yao et al. (2021) used inverse dynamics to\nimprove exploration and Yao et al. (2020) used a language model to generate action candidates that\nguide exploration. However, both approaches did not employ a two-stage rollout like our work, and\nthe latter considers a different setup without any valid action handicap.\n3\nPublished as a conference paper at ICLR 2022\n3\nMETHOD\nBackground Text-adventure games can be formalized as a Partially Observable Markov Decision\nProcess (POMDP) ⟨S, T, A, O, R, γ⟩. The underlying state space S contains all conﬁgurations of\nthe game state within the simulator, which is unobserved by the agent. The agent receives obser-\nvations from O from which it has to infer the underlying state s ∈S. The action set A consists\nof short phrases from the game vocabulary, T(s′|s, a) is the transition function which determines\nthe probability of moving to the next state s′ given the agent has taken action a in state s, R(s, a)\ndetermines the instantaneous reward, and γ is the reward discount factor.\nExisting RL approaches that tackle these games usually learn a value function using game rewards.\nOne example is the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016b) which\ntrains a deep neural network with parameters φ to approximate Qφ(o, a). This model encodes each\nobservation o and action candidate a using two recurrent networks fo and fa and aggregates the rep-\nresentations to derive the Q-value through an MLP q: Qφ(o, a) = q(fo(o), fa(a)). The parameters\nφ of the model are trained by minimizing the temporal difference (TD) loss on tuples (o, a, r, o′) of\nobservation, action, reward and the next observation sampled from an experience replay buffer:\nLTD(φ) = (r + γ max\na′∈A Qφ(o′, a′) −Qφ(o, a))2\n(1)\nThe agent samples actions using a softmax exploration policy π(a|o; φ) ∝exp(Qφ(o, a)).\nChallenges There are two unique aspects of text adventure games that make them challenging. First,\nthe action space is combinatorially large – usually games can accept action commands of 1-4 words\nwith vocabularies of up to 2257 words. This means an agent potentially has to choose between\nO(22574) = 2.6 ∗1013 actions at each state of the game. To make this problem more tractable,\nbenchmarks like Jericho (Hausknecht et al., 2020) provide a valid action detector that ﬁlters out the\nset of inadmissible commands (i.e. commands that are either unrecognized by the game engine or\ndo not change the underlying state of the game). However, this still results in the issue of dynamic\naction spaces for the agent that change with the state. For example, the action ‘echo’ is unique to\nthe Loud Room (see Figure 1). Second, these games have very sparse rewards (see Appendix A.1)\nand several bottleneck states (Ammanabrolu et al., 2020), making learning difﬁcult for RL agents\nthat use the same policy for exploration and exploitation (He et al., 2016a; Hausknecht et al., 2020;\nAmmanabrolu & Riedl, 2019). This results in issues of derailment (Ecoffet et al., 2021), with agents\nunable to return to promising parts of the state space, resulting in a substantial gap between average\nepisode performance of the agent and the maximum score it sees in the game (Yao et al., 2020).\n3.1\nOUR ALGORITHM: EXPLOIT-THEN-EXPLORE (XTX)\nWe tackle the two challenges outlined above using a multi-stage control policy that allows an agent\nto globally pick promising states of the game to visit while allowing for strategic local exploration\nthereafter. To this end, we develop eXploit-Then-eXplore, where an agent performs the following\ntwo distinct phases of action selection in each episode.\nPHASE 1: Exploitation In the exploitation phase, the agent makes a global decision about revisiting\npromising states of the game it has seen in its past episodes. We sample k trajectories from the\nexperience replay D using a combination of factors such as game score and trajectory length. These\ntrajectories are then used to learn a policy cover πexploit using self-imitation learning (Oh et al.,\n2018) (see Section 3.2 for details). The agent then samples actions from πexploit until it has reached\neither (1) the maximum score seen during training or (2) a number of steps in the episode equal to\nthe longest of the k sampled trajectories. The second condition ensures the agent can always return\nto the longest of the k sampled trajectories3. Once either of the above conditions is satisﬁed, the\nagent transitions to the exploration phase, adjusting its policy.\nPHASE 2: Exploration In the exploration phase, the agent uses a different policy πexplore trained\nusing both a temporal difference loss and an auxiliary inverse dynamics bonus (Pathak et al., 2017;\nYao et al., 2021) (see Section 3.3 for details). The intuition here is that the exploitation policy\nπexploit in phase 1 has brought the agent to the game frontier, which is under-explored and may\n3It is possible that the agent needs less steps to return to a particular part of the game space and hence wastes\nsome steps, but we empirically didn’t ﬁnd this to be a problem.\n4\nPublished as a conference paper at ICLR 2022\ncontain a combination of common (e.g. “open door”) and novel (e.g. “kill troll with sword”) ac-\ntions. Therefore, using a combination of Q-values and inverse dynamics bonus enables the agent to\nperform strategic, local exploration to expand the frontier and discover new rewarding states. The\nagent continues sampling from πexplore until a terminal state or episode limit is reached.\n3.1.1\nMIXTURE OF POLICIES FOR FINE-GRAINED CONTROL\nWhile one could employ two completely disjoint policies for the exploitation and exploration phases,\nwe choose a more general approach of having a policy mixture, with a single parameter λ that can\nbe varied to provide more ﬁne-grained control during the two phases. Speciﬁcally, we deﬁne:\nπλ(a|c, o; θ, φ, ξ) = λπinv−dy(a|o; θ, φ) + (1 −λ)πil(a|c; ξ).\n(2)\nHere, πinv−dy refers to an exploration-inducing policy trained using TD loss with an inverse-\ndynamics bonus (Section 3.3) and is parameterized by θ and φ. πil refers to an exploitation-inducing\npolicy trained through self-imitation learning (Oh et al., 2018) (Section 3.2) and is parameterized\nby ξ. Note that the action distribution over actions a induced by πinv−dy is conditioned only on the\ncurrent observation o, while the one induced by πil is conditioned on context c which is an augmen-\ntation of o with past information. We can observe that λ provides a trade-off between exploration\n(high λ) and exploitation (low λ). In our experiments, we choose a small, dynamic value, λ =\n1\n2∗T\nfor exploitation (where T is episode limit) and λ = 1 for exploration. As we demonstrate later\n(Section 4.2), the non-zero λ in exploitation is critical for the agent to avoid getting stuck in regions\nof local minima (e.g. Zork1). We now describe the individual components of the mixture.\n3.2\nIMITATION LEARNING FOR BUILDING A GLOBAL POLICY COVER (πil)\nWe parameterize the imitation policy πil using a Transformer model (Vaswani et al., 2017) based\non the GPT-2 architecture (Radford et al., 2019) that takes in a context c = [at−2; at−1; ot], i.e.\nthe concatenation of two most recent past actions along with the current observation separated by\n[SEP] tokens, and outputs a sequence of hidden representations h0, . . . , hm where m is the number\nof tokens in the sequence. hm is then projected to vocabulary size by multiplication with the output\nembedding matrix, after which softmax is applied to get probabilities for the next action token. In-\nspired by (Yao et al., 2020), the GPT-2 model is trained to predict the next action at given the context\nc using a language modeling objective (Equation 5). The training data consists of k trajectories sam-\npled from an experience replay memory D which stores transition tuples (ct, at, rt, ot+1, terminal).\nSampling trajectories Let us deﬁne a trajectory τ as a sequence of observations, actions, and re-\nwards, i.e. τ = o1, a1, r1, o2, a2, r2 . . . , ol+1, where lτ denotes the trajectory length (i.e. number\nof actions) and thus l ≤T where T is the episode limit. We sample a trajectory from D using a\ntwo-step process. First, we sample a score u from a categorical distribution:\nP(u) ∝exp (β1(u −µU)\/σU) ,\nu ∈U\n(3)\nwhere U is the set of all unique scores encountered in the game so far, µU is the mean of the set\nU, and σU is its standard deviation. β1 is the temperature and determines how biased the sampling\nprocess is towards high scores. The second step collects all trajectories with the sampled score u\nand samples a trajectory τ based on the trajectory length lτ:\nP(τ | u) ∝exp (−β2(lτ −µLu)\/σLu) ,\nlτ ∈Lu\n(4)\nwhere Lu is the multiset of trajectory lengths lτ with score u, µLu is the mean of the elements\nin Lu, and σLu is its standard deviation. β2 deﬁnes the temperature and determines the strength\nof the bias towards shorter length trajectories. We perform this sampling procedure k times (with\nreplacement) to obtain a trajectory buffer B on which we perform imitation learning. This allows\nthe agent to globally explore the game space by emulating promising experiences from its past, with\na bias towards trajectories with high game score and shorter lengths. The motivation for sampling\nshorter length trajectories among the ones that reach the same score is because those tend to be the\nones that waste less time performing meaningless actions (e.g. “pick up sword”, “drop sword”, etc.).\nLearning from trajectories Given the trajectory buffer B containing single-step (c, a) pairs of\ncontext c and actions a from the trajectories sampled in the previous step, we train πil by minimizing\nthe cross-entropy loss over action commands (Yao et al., 2020):\nL(ξ) = −E(c,a)∼B log πil(a|c; ξ),\n(5)\n5\nPublished as a conference paper at ICLR 2022\nwhere c deﬁnes the context of past observations and actions as before, and ξ deﬁnes the parameters\nof the GPT-2 model. We perform several passes of optimization through the trajectory buffer B until\nconvergence4 and periodically perform this optimization every n epochs in gameplay to update πil.\nFurthermore, πil is renormalized over the valid action set Av during gameplay. Note that while πil is\ntrained similarly to the GPT-2 model in (Yao et al., 2020), their model generates action candidates.\n3.3\nEFFICIENT LOCAL EXPLORATION WITH INVERSE DYNAMICS (πinv−dy)\nIn the second phase of our algorithm, we would like to use a policy that tackles (1) the large action\nspace and (2) the dynamic nature of the action set at every step in the game, which makes it crucial\nto keep trying under-explored actions and is difﬁcult for the Q network alone to generalize over. To\nthis end, we use the inverse dynamics model (INV-DY) from (Yao et al., 2021). INV-DY is a Q-\nbased policy πinv−dy similar to DRRN (He et al., 2016a), optimized with the standard TD loss (see\nBackground of Section 3). In addition, it adds an auxiliary loss Linv capturing an inverse dynamics\nprediction error (Pathak et al., 2017), which is added as an intrinsic reward to the game reward\n(r = rgame + α1 ∗Linv) and hence incorporated into the TD loss. Formally, Linv is deﬁned as:\nLinv(θ, φ) = −log pd(a|ginv(concat(fo(o), fo(o′))),\n(6)\nwhere θ denotes the parameters for the recurrent decoder d and the MLP ginv (neither of which are\nused in πinv−dy during gameplay to score the actions), and fo is the encoder deﬁned in Section 3.\nThis loss is optimized together with the TD loss as well as with an action decoding loss Ldec to\nobtain the following overall objective that is used to train πinv−dy:\nL(φ, θ) = LTD + α2Linv(φ, θ) + α3Ldec(φ, θ),\n(7)\nwhere Ldec(φ, θ) = −log pd(a|fa(a)). Here, fa is a recurrent network (see Section 3). Please refer\nto Yao et al. (2021) for details. We train the model by sampling batches of transitions from a prior-\nitized experience replay buffer D (Schaul et al., 2015) and performing stochastic gradient descent.\nInverse dynamics ameliorates the challenges (1) and (2) mentioned above by rewarding underex-\nplored actions (i.e. a high loss in Equation 6) and by generalizing over novel action commands.\nSpeciﬁcally, the INV-DY network might generalize through learning of past bonuses to what new\nactions would look like and hence identify novel actions before having tried them once.\n3.4\nEPISODIC ROLLOUTS WITH XTX (ALGORITHM 1)\nWe now describe how XTX operates in a single episode. The agent starts in phase 1, where actions at\nare sampled from πexploit. Following this exploitation policy brings the agent to the game frontier,\nwhich we estimate to happen when either (1) the current episode score ≥M, the maximum score in\nthe trajectory buffer B or when (2) the current time step t > lmax, the length of the longest trajectory\nin B. The agent then enters phase 2 and switches its strategy to sample actions only from πinv−dy\nby setting λ = 1. At every time step t during all phases, a transition tuple (ct, at, rt, ot+1, terminal)\nis added to the replay buffer D. The policy πexploit is updated every n episodes using the process in\nSection 3.2, while πexplore is updated within episodes at every step using the TD loss in equation 7,\nsampling high rewarding trajectories with priority fraction ρ, similar to (Guo et al., 2020)5.\n3.5\nNOVELTY IN COMPARISON TO PRIOR ALGORITHMS\nWe now more explicitly discuss comparisons to a few other approaches. The most closely related ap-\nproaches are multi-stage algorithms, including Go-Explore (Ecoffet et al., 2021) and PC-PG (Agar-\nwal et al., 2020) (and somewhat the E3 algorithm (Kearns & Singh, 2002; Henaff, 2019)). Both of\nthese algorithms can be viewed as approaches which explicitly use a “roll-in” policy, with the goal\nof visiting a novel region of the state-action space. Go-Explore is limited to deterministic MDPs\n(where it is easy to re-visit any state in the past), while PC-PG (applicable to more general MDPs\nwith provable guarantees under certain linearity assumptions) more explicitly builds a set of policies\n(‘policy cover’) capable of visiting different regions of the state space. However, in both of these\n4We empirically ﬁnd 40 passes to be sufﬁcient for convergence.\n5We slightly differ from their approach as we only prioritize transitions from trajectories that achieve the\nmaximum score seen so far.\n6\nPublished as a conference paper at ICLR 2022\nAlgorithm 1 The eXploit-Then-eXplore (XTX) algorithm\n1: Initialize prioritized replay memory D to capacity N with priority fraction ρ.\n2: Initialize πexploit (with parameters ξ) and πexplore (with parameters θ and φ).\n3: Set max score M, max length lmax in B to 0.\n4: Exploration steps R = 50; episode limit T = 50\n5: for episode ←1, . . . , E do\n6:\nfor t ←1, . . . , T do\n7:\nReceive observation ot and valid action set Av ⊂A for current state.\n8:\nif current episode score < M and t < T −R then\n▷PHASE 1\n9:\nλ ←\n1\n2∗T\n10:\nelse\n▷PHASE 2\n11:\nλ ←1\n12:\nend if\n13:\nSample an action at from policy πλ(at|ot, at−1, at−2; φ, θ, ξ).\n▷Equation 2\n14:\nStep with at and receive (rt, ot+1, terminal) from game engine.\n15:\nStore transition tuple (ct, at, rt, ot+1, terminal) in D.\n16:\nUpdate πinv−dy using TD loss with inverse dynamics.\n▷Equation 7\n17:\nend for\n18:\nif n episodes have passed then\n19:\nSample k trajectories from D to form the new trajectory buffer B.\n▷Section 3.2\n20:\nUpdate πil with cross-entropy loss.\n▷Equation 5\n21:\nUpdate M, lmax and set T ←lmax + R.\n22:\nend if\n23: end for\napproaches, once the agent reaches a novel part of the state-space, the agent acts randomly. A key\ndistinction in our approach is that once the agent reaches a novel part of the state space, it uses\nan exploration with novelty bonuses, which may more effectively select promising actions over a\nrandom behavioral policy in large action spaces.\nThe other broad class of approaches that handle exploration use novelty bonuses, with either a policy\ngradient approach or in conjunction with Q-learning (see Section 2). The difﬁculty with the former\nclass of algorithms is the “catastrophic forgetting” effect (see Agarwal et al. (2020) for discussion).\nThe difﬁculty with Q-learning approaches (with a novelty bonus) is that bootstrapping approaches,\nwith function approximation, can be unstable in long planning horizon problems (sometimes re-\nferred to as the “deadly triad” (Jiang et al., 2021)). While XTX also uses Q-learning (with a novelty\nbonus), we only use this policy (πinv−dy) in the second phase of the algorithm in contrast to (Yao\net al., 2021) where πinv−dy is used throughout the entire episode; our hope is that this instability\ncan be alleviated since we are effectively using Q-learning to solve a shorter horizon exploration\nproblem (as opposed to globally using Q-learning, with a novelty bonus).\n4\nEXPERIMENTS\nEnvironments We evaluate on 12 human-created games across several genres from the Jericho\nbenchmark (Hausknecht et al., 2020). They provide a variety of challenges such as darkness, non-\nstandard actions, inventory management, and dialog (Hausknecht et al., 2020). At every step t, the\nobservation from the Jericho game engine contains a description of the state, which is augmented\nwith location and inventory information (by issuing “look” and “inventory” commands) to form\not (Hausknecht et al., 2019). In addition, we use of the valid action handicap provided by Jericho,\nwhich ﬁlters actions to remove those that do not change the underlying game state. We found this\naction handicap to be imperfect for some games (marked with * in Table 1), and manually added\nsome actions required for agent progress from game walkthroughs to the game engine’s grammar.\nEvaluation We evaluate agents under two settings: (a) a deterministic setting where the transition\ndynamics T(s′|s, a) is a one-hot vector over all the next states s′ and (b) a stochastic setting6 where\nthe T(s′|s, a) deﬁnes a distribution over next states s′, and the observations o can be perturbed with\n6Only six games have stochastic variants, and DRAGON was left out due to memory issues in the baselines.\n7\nPublished as a conference paper at ICLR 2022\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n40.3\n55.0\n44.1\n105.0\n41.7\n53.0\n34.1\n52.3\n103.4\n152.7\n+17%\n350\nINHUMANE*\n34.8\n56.7\n27.7\n63.3\n29.8\n53.3\n59.2\n76.7\n64.0\n76.7\n+5%\n90\nLUDICORP*\n17.1\n48.7\n19.6\n49.3\n10.9\n40.7\n67.3\n86.0\n78.8\n91.0\n+8%\n150\nZORK3*\n0.3\n4.3\n0.5\n5.0\n3.0\n5.0\n3.8\n4.7\n4.2\n5.0\n+6%\n7\nPENTARI*\n45.6\n58.3\n34.5\n53.3\n33.4\n46.7\n43.4\n60.0\n49.6\n60.0\n+6%\n70\nDETECTIVE\n289.9\n320.0\n289.5\n323.3\n269.3\n346.7\n296.0\n336.7\n312.2\n340.0\n+4%\n360\nBALANCES*\n14.1\n25.0\n12.5\n25.0\n10.0\n18.3\n21.9\n25.0\n24.0\n26.7\n+4%\n51\nLIBRARY*\n24.8\n30.0\n24.7\n30.0\n24.2\n30.0\n26.1\n30.0\n28.5\n30.0\n+8%\n30\nDEEPHOME*\n58.8\n68.0\n58.9\n72.7\n1.0\n1.0\n52.6\n70.0\n77.7\n92.3\n+6%\n300\nENCHANTER*\n42.0\n66.7\n44.2\n63.3\n26.8\n38.3\n24.3\n28.3\n52.0\n66.7\n+2%\n400\nDRAGON7\n-3.7\n8.0\n-2.3\n8.7\n3.2\n8.0\n40.7\n126.0\n96.7\n127.0\n0%\n25\nOMNIQUEST\n8.2\n10.0\n9.9\n13.3\n9.3\n10.0\n8.6\n10.0\n11.6\n13.3\n+3%\n50\nAvg. Norm Score\n29.5%\n48.8%\n28.4%\n51.8%\n29.7%\n44.5%\n49.2%\n58.6%\n56.3%\n64.0%\n5.8%\n100%\nTable 1:\nResults on deterministic games for the best XTX model, where the inverse dynamics\nscaling coefﬁcient α1 was tuned per game. We outperform the baselines on all 12 games, achieving\nan average normalized game score of 56%. * indicates actions were added to the game grammar.\n∆indicates the absolute performance difference between XTX and the best baseline on Avg scores.\nScores are averaged across 3 seeds. Baselines were rerun with the latest Jericho version.\nirrelevant sentences such as “you hear in the distance the chirping of a song bird”. We report both\nthe episode score average (Avg) over the last 100 episodes at the end of training, as well as the\nmaximum score (Max) seen in any episode during training.\nBaselines We consider four baselines. 1) DRRN (He et al., 2016a): This model uses a Q-based\nsoftmax policy, i.e. π ∝exp(Q(o, a; φ)), parameterized using GRU encoders and decoders, and\ntrained using the TD loss (Equation 1). 2) INV-DY (Yao et al., 2021): Refer to Section 3.3. 3)\nRC-DQN (Guo et al., 2020): This is a state-of-the-art model that uses an object-centric reading\ncomprehension (RC) module to encode observations and output actions. The training loss is that of\nDRRN above, and during gameplay, the agent uses an ϵ-greedy strategy. 4) XTX-Uniform (∼Go-\nExplore): Here, we replace πinv−dy with a policy that samples actions uniformly during Phase 2,\nkeeping all other factors of our algorithm the same. This is closest to a version of the Go-Explore\nalgorithm (Ecoffet et al., 2021) that returns to promising states and performs random exploration.\nHowever, while conceptually similar to Go-Explore, XTX-Uniform does not make use of any addi-\ntional memory archives, and avoids training a goal-based policy. See Appendix A.2 for implemen-\ntation details and hyperparameters.\n4.1\nRESULTS\nDeterministic games We report results in Table 1 (refer to Appendix A.5 and A.6 for more details).\nOverall, XTX outperforms DRRN, INV-DY, RC-DQN, and XTX-Uniform by 27%, 28%, 27%, and\n7% respectively, in terms of average normalized game score (i.e. average episode score divided\nby max score, averaged over all the games). Our algorithm performs particularly well on Zork1,\nachieving a 17% absolute improvement in average episode score and a 14% improvement in average\nmaximum score compared to the best baseline. In particular, XTX manages to advance past several\ndocumented bottlenecks like the dark Cellar (see Figure 1) which have proved to be very challenging\nfor existing methods (Ammanabrolu et al., 2020). While performance with XTX-Uniform is some-\ntimes close, exploration with inverse dynamics instead of random exploration pushes past several\nbottlenecks present in Zork1 and leads to signiﬁcant gains on Deephome, Enchanter, Omniquest,\nand Ludicorp, showing the potential usefulness of strategic exploration at the game frontier.\nStochastic games To show the robustness of XTX to stochasticity, we evaluate our agent in the\nstochastic setting (Table 2). XTX outperforms the baselines on 4 out of 5 games, and pushes past the\nmaximum scores of XTX-Uniform on the same fraction. Especially impressive is the performance\non Zork1, which is still higher than the state-of-the-art score in the deterministic setting.\n4.2\nABLATION STUDIES\nIn order to evaluate the importance of the various components in XTX, we perform several ablations\non a subset of the games as described below and shown in Figure 2 (more in Appendix A.3 and A.4).\n7Interestingly, XTX manages to achieve a very high score on Dragon by exploiting an integer underﬂow bug.\n8\nPublished as a conference paper at ICLR 2022\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n41.3\n55.7\n36.9\n85.7\n40.3\n53.0\n31.2\n48.0\n67.7\n143.0\n+8%\n350\nZORK3*\n0.2\n4.0\n0.4\n4.7\n2.7\n4.7\n2.3\n4.3\n2.6\n5.0\n-1%\n7\nPENTARI*\n38.2\n60.0\n37.5\n55.0\n33.3\n41.7\n38.8\n60.0\n47.3\n60.0\n+12%\n70\nDEEPHOME*\n43.0\n65.7\n58.4\n73.0\n1.0\n1.0\n50.7\n69.3\n70.9\n96.0\n+4%\n300\nENCHANTER*\n42.0\n56.7\n34.5\n53.3\n27.1\n43.3\n30.2\n45.0\n44.8\n58.3\n+1%\n400\nAvg. Norm Score\n18.9%\n39.0%\n19.7%\n41.5%\n20.9%\n30.5%\n24.3%\n39.1%\n31.8%\n48.9%\n4.8%\n100%\nTable 2: Results on stochastic games. We outperform baselines on 4 out of 5 games, with an average\nnormalized game score of 32%. Scores are averaged across 3 seeds. Baselines were rerun with the\nlatest Jericho version.\nZork1\nInhumane\nZork3\nLudicorp\nDeephome\nOmniquest\nGame\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Score\nAblation\nPure IL (  = 0)\nPure Inv Dy (  = 1)\nMix (  = 0.5)\nXTX (no-mix)\nXTX (full model)\nFigure 2: Average episode scores for 4 ablation models across 6 games. Overall, we ﬁnd both the\nstrategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm.\nPure imitation learning (λ = 0) This ablation sets λ = 0 in equation 2, meaning the agent will\nalways use the imitation learning policy πil. As expected, this model performs quite badly since it\nis based on pure exploitation and is hence unlikely to reach deep states in the game.\nPure inverse dynamics (λ = 1) This ablation sets λ = 1 in equation 2, hence always using the\ninverse dynamics exploration policy πinv−dy, resulting in the model proposed in (Yao et al., 2021).\nWhile this model can sometimes achieve high maximum scores, it is unable to learn from these and\nhence its average episode score remains quite low, consistent with ﬁndings in (Yao et al., 2021).\nMixing exploration and exploitation (λ = 0.5) By setting λ = 0.5, this ablation constantly alter-\nnates between exploitation and exploration, never committing to one or the other. This causes the\nagent to suffer from issues of both the λ = 0 and λ = 1 models, resulting in weak results.\nPure separation of exploitation and exploration (XTX no-mix) In this ablation, we examine the\nimportance of having a mixture policy in Phase 1 of the algorithm instead of setting λ = 0 in Phase 1\nand to 1 in Phase 2. This explicitly separated model, denoted as XTX (no-mix), performs a bit better\nin the games of Inhumane and Zork3, but sometimes fails to push past certain stages in Ludicorp\nand completely gets stuck in the game of Zork1. This shows it is crucial to have a mixture policy in\nPhase 1 in order to get past bottleneck states in difﬁcult games.\n5\nCONCLUSION\nWe have proposed XTX, an algorithm for multi-stage episodic control in text adventure games. XTX\nexplicitly disentangles exploitation and exploration into different policies, which are used by the\nagent for action selection in different phases of the same episode. Decomposing the policy allows\nthe agent to combine global decisions on which state spaces in the environment to (re-)explore, fol-\nlowed by strategic local exploration that can handle novel, unseen actions – aspects that help tackle\nthe challenges of sparse rewards and dynamic action spaces in these games. Our method signiﬁ-\ncantly outperforms prior methods on the Jericho benchmark (Hausknecht et al., 2020) under both\ndeterministic and stochastic settings, and even surpasses several challenging bottlenecks in games\nlike Zork1 (Ammanabrolu et al., 2020). Future work can integrate our algorithm with approaches\nthat better leverage linguistic signals to achieve further progress in these games.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGEMENTS\nWe thank the members of the Princeton NLP group and the anonymous reviewers for their valuable\nfeedback. JT was supported by a graduate fellowship at Princeton University. We are grateful to the\nGoogle Cloud Research program for computational support in running our experiments. We would\nalso like to thank Matthew Hausknecht for all the help regarding the Jericho environment.\nETHICAL CONSIDERATIONS\nThis work focuses on building better agents for text-adventure games and hence does not have im-\nmediate direct ethical concerns. However, the techniques introduced in this paper may be generally\nuseful for other autonomous agents that combine sequential decision making with language under-\nstanding (e.g. dialog systems). As such agents become more capable and inﬂuential in our lives, it\nis important to make sure their objectives align with those of humans, and that they are free of bias.\nREPRODUCIBILITY\nOur code is publicly available here https:\/\/github.com\/princeton-nlp\/XTX. We pro-\nvide all implementation details such as hyperparameters, model architectures and training regimes\nin Appendix A.2. We used Weights & Biases for experiment tracking and visualizations to develop\ninsights for this paper.\nREFERENCES\nAshutosh Adhikari, Xingdi (Eric) Yuan, Marc-Alexandre Cˆot´e, Mikulas Zelinka, Marc-Antoine\nRondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton\n. Learning dynamic knowledge graphs to generalize on text-based games. In NeurIPS 2020, 2020.\nAlekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed explo-\nration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.\nDeep reinforcement learning at the edge of the statistical precipice. Advances in Neural Informa-\ntion Processing Systems, 34, 2021.\nPrithviraj Ammanabrolu and Matthew J. Hausknecht. Graph constrained reinforcement learning for\nnatural language action spaces. In 8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:\n\/\/openreview.net\/forum?id=B1x6w0EtwH.\nPrithviraj Ammanabrolu and Mark Riedl. Playing text-adventure games with graph-based deep\nreinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pp. 3557–3565, Minneapolis, Minnesota, 2019. Association for Computational\nLinguistics. doi: 10.18653\/v1\/N19-1358. URL https:\/\/www.aclweb.org\/anthology\/\nN19-1358.\nPrithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. How to avoid being eaten\nby a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795,\n2020.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven\nKapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, et al. Never\ngive up: Learning directed exploration strategies. In International Conference on Learning Rep-\nresentations, 2020.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.\nUnifying count-based exploration and intrinsic motivation. Advances in neural information pro-\ncessing systems, 29:1471–1479, 2016.\n10\nPublished as a conference paper at ICLR 2022\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-\nronment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,\n2013.\nRonen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-\noptimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213–231, 2002.\nC´edric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.\nGEP-PG: Decoupling exploration and\nexploitation in deep reinforcement learning algorithms.\nIn Jennifer Dy and Andreas Krause\n(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of\nProceedings of Machine Learning Research, pp. 1039–1048. PMLR, 10–15 Jul 2018.\nURL\nhttps:\/\/proceedings.mlr.press\/v80\/colas18a.html.\nMarc-Alexandre Cˆot´e, ´Akos K´ad´ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler.\nTextworld: A learning environment for text-based games. CoRR, 2018.\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then\nexplore. Nature, 590(7847):580–586, 2021.\nVincent Franc¸ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An\nintroduction to deep reinforcement learning. arXiv preprint arXiv:1811.12560, 2018.\nNancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock?\naffordance extraction via word embeddings. In Carles Sierra (ed.), Proceedings of the Twenty-\nSixth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, Melbourne, Australia,\nAugust 19-25, 2017, pp. 1039–1045. ijcai.org, 2017. doi: 10.24963\/ijcai.2017\/144. URL https:\n\/\/doi.org\/10.24963\/ijcai.2017\/144.\nXiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu Chang. Interac-\ntive ﬁction game playing as multi-paragraph reading comprehension with reinforcement learn-\ning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pp. 7755–7765, Online, 2020. Association for Computational Linguistics.\ndoi: 10.18653\/v1\/2020.emnlp-main.624. URL https:\/\/www.aclweb.org\/anthology\/\n2020.emnlp-main.624.\nMatthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. Nail:\nA general interactive ﬁction agent. arXiv preprint arXiv:1902.04259, 2019.\nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Cˆot´e, and Xingdi Yuan. Inter-\nactive ﬁction games: A colossal adventure. Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 34(05):7903–7910, Apr. 2020.\ndoi: 10.1609\/aaai.v34i05.6297.\nURL https:\n\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/6297.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep\nreinforcement learning with a natural language action space. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1621–\n1630, Berlin, Germany, August 2016a. Association for Computational Linguistics. doi: 10.18653\/\nv1\/P16-1153. URL https:\/\/aclanthology.org\/P16-1153.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep\nreinforcement learning with a natural language action space. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1621–\n1630, Berlin, Germany, 2016b. Association for Computational Linguistics. doi: 10.18653\/v1\/\nP16-1153. URL https:\/\/www.aclweb.org\/anthology\/P16-1153.\nMikael Henaff.\nExplicit explore-exploit algorithms in continuous state spaces.\narXiv preprint\narXiv:1911.00617, 2019.\nVishal Jain, William Fedus, Hugo Larochelle, Doina Precup, and Marc G. Bellemare. Algorithmic\nimprovements for deep reinforcement learning applied to interactive ﬁction. In The Thirty-Fourth\n11\nPublished as a conference paper at ICLR 2022\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applica-\ntions of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020,\npp. 4328–4336. AAAI Press, 2020. URL https:\/\/aaai.org\/ojs\/index.php\/AAAI\/\narticle\/view\/5857.\nYoungsoo Jang, Seokin Seo, Jongmin Lee, and Kee-Eung Kim. Monte-carlo planning and learning\nwith language action value estimates. In International Conference on Learning Representations,\n2020.\nRay Jiang, Tom Zahavy, Zhongwen Xu, Adam White, Matteo Hessel, Charles Blundell, and Hado\nVan Hasselt. Emphatic algorithms for deep reinforcement learning. In Marina Meila and Tong\nZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume\n139. PMLR, 2021.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-\nchine learning, 49(2):209–232, 2002.\nJing Li, Xinxin Shi, Jiehao Li, Xin Zhang, and Junzheng Wang. Random curiosity-driven explo-\nration in deep reinforcement learning. Neurocomputing, 418:139–147, 2020.\nMarlos C. Machado, Marc G. Bellemare, and Michael Bowling. Count-based exploration with the\nsuccessor representation. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 34(04):\n5125–5133, Apr. 2020. doi: 10.1609\/aaai.v34i04.5955. URL https:\/\/ojs.aaai.org\/\nindex.php\/AAAI\/article\/view\/5955.\nAndrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng,\nAlexandros Papangelis, Dian Yu, Chandra Khatri, and G¨okhan T¨ur. Exploration based language\nlearning for text-based games. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artiﬁcial Intelligence, IJCAI 2020, pp. 1488–1494. ijcai.org, 2020.\ndoi: 10.24963\/ijcai.2020\/207. URL https:\/\/doi.org\/10.24963\/ijcai.2020\/207.\nKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based\ngames using deep reinforcement learning. In Proceedings of the 2015 Conference on Empir-\nical Methods in Natural Language Processing, pp. 1–11, Lisbon, Portugal, 2015. Association\nfor Computational Linguistics. doi: 10.18653\/v1\/D15-1001. URL https:\/\/www.aclweb.\norg\/anthology\/D15-1001.\nJunhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International\nConference on Machine Learning, pp. 3878–3887. PMLR, 2018.\nPhilip Osborne, Heido N˜omm, and Andre Freitas. A survey of text games for reinforcement learning\ninformed by natural language. arXiv preprint arXiv:2109.09478, 2021.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pp. 2778–2787.\nPMLR, 2017.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\nLukas Sch¨afer, Filippos Christianos, Josiah Hanna, and Stefano V. Albrecht. Decoupling exploration\nand exploitation in reinforcement learning. CoRR, abs\/2107.08966, 2021. URL https:\/\/\narxiv.org\/abs\/2107.08966.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv\npreprint arXiv:1511.05952, 2015.\nLior Shani, Yonathan Efroni, and Shie Mannor. Exploration conscious reinforcement learning re-\nvisited. In International Conference on Machine Learning, pp. 5680–5689. PMLR, 2019.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n12\nPublished as a conference paper at ICLR 2022\nAdrien Ali Taiga, William Fedus, Marlos C Machado, Aaron Courville, and Marc G Belle-\nmare. On bonus-based exploration methods in the arcade learning environment. arXiv preprint\narXiv:2109.11052, 2021.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep\nreinforcement learning. In 31st Conference on Neural Information Processing Systems (NIPS),\nvolume 30, pp. 1–18, 2017.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE\/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033.\nIEEE, 2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nWilliam F. Whitney, Michael Bloesch, Jost Tobias Springenberg, Abbas Abdolmaleki, and Martin A.\nRiedmiller. Decoupled exploration and exploitation policies for sample-efﬁcient reinforcement\nlearning. CoRR, abs\/2101.09458, 2021. URL https:\/\/arxiv.org\/abs\/2101.09458.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. Deep rein-\nforcement learning with stacked hierarchical attention for text-based games. Advances in Neural\nInformation Processing Systems, 33, 2020.\nYunqiu Xu, Meng Fang, Ling Chen, Yali Du, and Chengqi Zhang. Generalization in text-based\ngames via hierarchical reinforcement learning. arXiv preprint arXiv:2109.09968, 2021.\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\nLanguage models for action generation in text-based games. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP), pp. 8736–8754, Online,\nNovember 2020. Association for Computational Linguistics. doi: 10.18653\/v1\/2020.emnlp-main.\n704. URL https:\/\/aclanthology.org\/2020.emnlp-main.704.\nShunyu Yao, Karthik Narasimhan, and Matthew Hausknecht.\nReading and acting while blind-\nfolded: The need for semantics in text game agents. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pp. 3097–3102, Online, June 2021. Association for Computational Linguis-\ntics. doi: 10.18653\/v1\/2021.naacl-main.247. URL https:\/\/aclanthology.org\/2021.\nnaacl-main.247.\nXingdi Yuan, Marc-Alexandre Cˆot´e, Alessandro Sordoni, Romain Laroche, Remi Tachet des\nCombes, Matthew J. Hausknecht, and Adam Trischler. Counting to explore and generalize in\ntext-based games. CoRR, 2018.\nTom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, and Shie Mannor. Learn what\nnot to learn: Action elimination with deep reinforcement learning. In Samy Bengio, Hanna M.\nWallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.),\nAdvances in Neural Information Processing Systems 31: Annual Conference on Neural Infor-\nmation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pp.\n3566–3577, 2018. URL https:\/\/proceedings.neurips.cc\/paper\/2018\/hash\/\n645098b086d2f9e1e0e939c27f9f2d6f-Abstract.html.\n13\nPublished as a conference paper at ICLR 2022\nA\nAPPENDIX\nA.1\nGAME STATISTICS\nGame\nZORK1\nINHUMANE\nLUDICORP\nLIBRARY\nZORK3\nPENTARI\nAvg.\/Max\n9 \/ 51\n14 \/ 28\n4 \/ 45\n5 \/ 6\n39 \/ 41\n5 \/ 16\nGame\nDETECTIVE\nBALANCES\nDEEPHOME\nENCHANTER\nDRAGON\nOMNIQUEST\nAvg.\/Max\n2 \/ 5\n12 \/ 54\n6 \/ 53\n15 \/ 40\n9 \/ 24\n13 \/ 26\nTable 3: Average and maximum number of steps between rewards for games in Jericho (based on\nhuman walkthroughs). Several games have long sequences of actions without reward.\nTable 3 contains the average and maximum number of steps between rewards in these games, show-\ncasing their challenging nature.\nA.2\nIMPLEMENTATION DETAILS\nWe use a learning rate of 10−3 and 10−4 for πil and πinv−dy, respectively. Both policies are trained\non batches of size 64, with hidden dimensions of size 128. The scaling coefﬁcient α1 for the inverse\ndynamics intrinsic reward is set to 1 for all games except for Deephome (α1 = 0.1), Enchanter\n(α1 = 0.5), Omniquest (α1 = 2), Ludicorp (α1 = 0.5), Detective (α1 = 2), and Pentari (α1 = 2).\nThe Transformer πil has 3 layers and 4 attention heads. β1 in equation 3 is set to 1, β2 in equation 4\nis set to 10k to encourage picking the shortest length trajectory, and k is set to 10. In equation 6,\nα1 = α2 = 1. The priority fraction ρ is set to 0.5. Every time πil is trained, we also scale the\nepisode length T to have at least R remaining steps of exploration by setting T = lmax + R, where\nlmax is the length of the longest trajectory in the trajectory buffer B. In practice, R = 50, and hence\nthe agent will be guaranteed at least 50 steps of exploration each episode. XTX and DRRN are run\nfor 800k interaction steps, while RC-DQN which is run for 100k interaction steps following Guo\net al. (2020).\nA.3\nFULL SET OF ABLATIONS\nZork1\nInhumane\nZork3\nLudicorp\nDeephome\nOmniquest\nGame\n0.0\n0.2\n0.4\n0.6\n0.8\nNormalized Score\nAblation\nPure IL (  = 0)\nPure Inv Dy (  = 1)\nMix (  = 0.5)\nXTX (no-mix)\nXTX (full model)\nEnchanter\nPentari\nBalances\nDetective\nLibrary\nDragon\nGame\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Score\nFigure 3: Average episode scores for 4 ablation models across 12 games. Overall, we ﬁnd both the\nstrategic inverse dynamics policy and the explicit exploitation policy to be key for our algorithm.\nScores for dragon were clipped to be between 0 and 1.\n14\nPublished as a conference paper at ICLR 2022\nA.4\nABLATION TRAINING PLOTS\nFigure 4: Average episode score throughout training for all ablations on Zork1. Shaded areas\nindicate one standard deviation.\nFigure 5: Average episode score throughout training for all ablations on Inhumane. Shaded areas\nindicate one standard deviation.\n15\nPublished as a conference paper at ICLR 2022\nFigure 6: Average episode score throughout training for all ablations on Zork3. Shaded areas\nindicate one standard deviation.\nFigure 7: Average episode score throughout training for all ablations on Ludicorp. Shaded areas\nindicate one standard deviation.\n16\nPublished as a conference paper at ICLR 2022\nFigure 8: Average episode score throughout training for all ablations on Balances. Shaded areas\nindicate one standard deviation.\nFigure 9: Average episode score throughout training for all ablations on Deephome. Shaded areas\nindicate one standard deviation.\n17\nPublished as a conference paper at ICLR 2022\nFigure 10: Average episode score throughout training for all ablations on Detective. Shaded areas\nindicate one standard deviation.\nFigure 11: Average episode score throughout training for all ablations on Enchanter. Shaded areas\nindicate one standard deviation.\n18\nPublished as a conference paper at ICLR 2022\nFigure 12: Average episode score throughout training for all ablations on Omniquest. Shaded areas\nindicate one standard deviation.\nFigure 13: Average episode score throughout training for all ablations on Pentari. Shaded areas\nindicate one standard deviation.\n19\nPublished as a conference paper at ICLR 2022\nFigure 14: Average episode score throughout training for all ablations on Dragon. Shaded areas\nindicate one standard deviation.\nFigure 15: Average episode score throughout training for all ablations on Library. Shaded areas\nindicate one standard deviation.\n20\nPublished as a conference paper at ICLR 2022\nA.5\nFULL DETERMINISTIC AND STOCHASTIC RESULTS\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n40.3 (2.2)\n55.0 (6.4)\n44.1 (12.6)\n105.0 (19.9)\n41.7 (0.6)\n53.0 (0.0)\n34.1 (1.5)\n52.3 (3.8)\n103.4 (10.9)\n152.7 (1.7)\n+17%\n350\nINHUMANE\n31.0 (1.0)\n56.7 (4.7)\n28.1 (3.6)\n60.0 (0.0)\n31.8 (1.4)\n63.3 (4.7)\n68.9 (8.9)\n83.3 (9.4)\n60.9 (5.9)\n70.0 (14.1)\n-9%\n90\nINHUMANE*\n34.8 (3.9)\n56.7 (4.7)\n27.7 (5.3)\n63.3 (4.7)\n29.8 (2.3)\n53.3 (4.7)\n59.2 (1.2)\n76.7 (9.4)\n64.0 (7.7)\n76.7 (9.4)\n+5%\n90\nLUDICORP\n15.6 (0.1)\n23.0 (0.0)\n15.6 (0.2)\n23.0 (0.0)\n12.4 (1.1)\n21.0 (2.2)\n19.9 (0.4)\n23.0 (0.0)\n20.9 (0.1)\n23.0 (0.0)\n+1%\n150\nLUDICORP*\n17.1 (1.7)\n48.7 (2.1)\n19.6 (5.5)\n49.3 (16.2)\n10.9 (1.7)\n40.7 (2.5)\n67.3 (4.2)\n86.0 (2.8)\n78.8 (5.1)\n91.0 (3.6)\n+8%\n150\nZORK3\n0.3 (0.0)\n4.7 (0.5)\n0.4 (0.0)\n5.0 (0.0)\n3.2 (0.5)\n5.0 (0.0)\n3.7 (0.2)\n4.7 (0.5)\n4.2 (0.0)\n5.0 (0.0)\n+7%\n7\nZORK3*\n0.3 (0.0)\n4.3 (0.5)\n0.5 (0.1)\n5.0 (0.0)\n3.0 (0.3)\n5.0 (0.0)\n3.8 (0.4)\n4.7 (0.5)\n4.2 (0.1)\n5.0 (0.0)\n+6%\n7\nPENTARI\n43.4 (4.5)\n58.3 (2.4)\n29.8 (14.1)\n46.7 (6.2)\n37.4 (7.0)\n46.7 (11.8)\n43.4 (1.7)\n60.0 (0.0)\n45.5 (4.3)\n60.0 (0.0)\n+3%\n70\nPENTARI*\n45.6 (1.9)\n58.3 (2.4)\n34.5 (7.5)\n53.3 (6.2)\n33.4 (6.9)\n46.7 (6.2)\n43.4 (0.4)\n60.0 (0.0)\n49.6 (1.3)\n60.0 (0.0)\n+6%\n70\nDETECTIVE\n289.9 (0.1)\n320.0 (8.2)\n289.5 (0.4)\n323.3 (4.7)\n269.3 (14.8)\n346.7 (4.7)\n296.0 (9.0)\n336.7 (12.5)\n312.2 (10.3)\n340.0 (8.2)\n+4%\n360\nBALANCES\n10.0 (0.0)\n10.0 (0.0)\n9.9 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n9.6 (0.1)\n10.0 (0.0)\n10.0 (0.0)\n10.0 (0.0)\n0%\n51\nBALANCES*\n14.1 (0.4)\n25.0 (0.0)\n12.5 (1.6)\n25.0 (0.0)\n10.0 (0.1)\n18.3 (2.4)\n21.9 (0.4)\n25.0 (0.0)\n24.0 (0.3)\n26.7 (2.4)\n+4%\n51\nLIBRARY\n17.3 (0.7)\n21.0 (0.0)\n17.0 (0.2)\n21.0 (0.0)\n16.2 (1.4)\n21.0 (0.0)\n18.8 (0.4)\n21.0 (0.0)\n19.7 (0.5)\n21.0 (0.0)\n+3%\n30\nLIBRARY*\n24.8 (0.6)\n30.0 (0.0)\n24.7 (0.4)\n30.0 (0.0)\n24.2 (1.4)\n30.0 (0.0)\n26.1 (0.4)\n30.0 (0.0)\n28.5 (0.3)\n30.0 (0.0)\n+8%\n30\nDEEPHOME\n57.9 (0.4)\n68.7 (0.5)\n44.8 (19.9)\n76.0 (5.0)\n1.0 (0.0)\n1.0 (0.0)\n46.3 (9.0)\n60.7 (13.2)\n75.7 (5.0)\n93.7 (5.6)\n+6%\n300\nDEEPHOME*\n58.8 (0.1)\n68.0 (0.8)\n58.9 (0.2)\n72.7 (3.8)\n1.0 (0.0)\n1.0 (0.0)\n52.6 (0.4)\n70.0 (0.8)\n77.7 (2.1)\n92.3 (3.3)\n+6%\n300\nENCHANTER\n46.1 (11.1)\n70.0 (21.2)\n46.0 (3.6)\n73.3 (8.5)\n25.8 (8.5)\n36.7 (14.3)\n43.4 (18.9)\n53.3 (23.6)\n34.7 (21.2)\n36.7 (23.6)\n-3%\n400\nENCHANTER*\n42.0 (1.2)\n66.7 (2.4)\n44.2 (18.3)\n63.3 (30.6)\n26.8 (1.9)\n38.3 (4.7)\n24.3 (10.8)\n28.3 (11.8)\n52.0 (23.1)\n66.7 (33.0)\n+2%\n400\nDRAGON\n-3.7 (0.4)\n8.0 (0.0)\n-2.3 (0.5)\n8.7 (1.7)\n3.2 (1.6)\n8.0 (0.0)\n40.7 (0.0)\n126.0 (0.0)\n96.7 (1.1)\n127.0 (0.0)\n0%\n25\nOMNIQUEST\n8.2 (0.1)\n10.0 (0.0)\n9.9 (0.0)\n13.3 (2.4)\n9.3 (0.7)\n10.0 (0.0)\n8.6 (0.1)\n10.0 (0.0)\n11.6 (1.3)\n13.3 (2.4)\n+3%\n50\nAvg. Norm Score\n29.5% (29.8)\n48.8% (28.8)\n28.4% (27.2)\n51.8% (27.3)\n29.7% (25.6)\n44.5% (32.1)\n49.2% (30.4)\n58.6% (33.6)\n56.3% (28.1)\n64.0% (28.6)\n5.8% (4.1)\n100%\nTable 4: Full Deterministic Results. Standard deviations are in parentheses. Scores are averaged\nacross 3 seeds. Note that the average normalized scores only take into account the games listed in\nTable 1. Baselines were rerun with the latest Jericho version.\nGames\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX (ours)\n∆(%)\nGame Max\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nAvg\nMax\nZORK1\n41.3 (3.2)\n55.7 (3.3)\n36.9 (2.4)\n85.7 (14.8)\n40.3 (1.6)\n53.0 (0.0)\n31.2 (1.1)\n48.0 (5.0)\n67.7 (8.0)\n143.0 (10.7)\n+8%\n350\nZORK3\n0.2 (0.0)\n4.3 (0.5)\n0.7 (0.2)\n5.0 (0.0)\n2.7 (0.0)\n5.0 (0.0)\n1.8 (0.1)\n4.0 (0.0)\n2.7 (0.4)\n5.0 (0.0)\n0%\n7\nZORK3*\n0.2 (0.0)\n4.0 (0.0)\n0.4 (0.3)\n4.7 (0.5)\n2.7 (0.1)\n4.7 (0.5)\n2.3 (0.5)\n4.3 (0.5)\n2.6 (0.6)\n5.0 (0.0)\n-1%\n7\nPENTARI\n42.3 (0.8)\n60.0 (0.0)\n28.9 (8.5)\n45.0 (0.0)\n31.2 (3.9)\n38.3 (11.8)\n38.4 (1.3)\n60.0 (0.0)\n48.2 (0.4)\n60.0 (0.0)\n+8%\n70\nPENTARI*\n38.2 (3.6)\n60.0 (0.0)\n37.5 (8.0)\n55.0 (7.1)\n33.3 (6.0)\n41.7 (10.3)\n38.8 (0.4)\n60.0 (0.0)\n47.3 (0.4)\n60.0 (0.0)\n+12%\n70\nDEEPHOME\n58.2 (0.6)\n72.0 (5.7)\n58.2 (0.5)\n72.7 (2.5)\n1.0 (0.0)\n1.0 (0.0)\n48.0 (10.1)\n62.0 (14.2)\n73.9 (4.3)\n99.3 (13.9)\n+5%\n300\nDEEPHOME*\n43.0 (20.0)\n65.7 (3.3)\n58.4 (0.5)\n73.0 (1.4)\n1.0 (0.0)\n1.0 (0.0)\n50.7 (2.3)\n69.3 (0.9)\n70.9 (2.7)\n96.0 (7.8)\n+4%\n300\nENCHANTER\n41.0 (0.6)\n71.7 (9.4)\n38.9 (14.5)\n63.3 (30.6)\n25.0 (4.0)\n30.0 (7.1)\n32.1 (10.9)\n53.3 (23.6)\n46.2 (18.9)\n51.7 (22.5)\n+1%\n400\nENCHANTER*\n42.0 (18.5)\n56.7 (27.2)\n34.5 (10.3)\n53.3 (23.6)\n27.1 (2.7)\n43.3 (8.5)\n30.2 (9.1)\n45.0 (20.4)\n44.8 (19.4)\n58.3 (27.8)\n+1%\n400\nAvg. Norm Score\n18.9% (18.2)\n39.0% (28.1)\n19.7% (17.5)\n41.5% (26.0)\n20.9% (18.6)\n30.5% (27.1)\n24.3% (17.9)\n39.1% (29.6)\n31.8% (19.8)\n48.9% (26.0)\n4.8% (4.7)\n100%\nTable 5: Full Stochastic Results. Standard deviations are in parentheses. Scores are averaged\nacross 3 seeds. Note that the average normalized scores only take into account the games listed in\nTable 2. Baselines were rerun with the latest Jericho version.\nA.6\nAGGREGATE METRICS & PERFORMANCE PROFILES\n0.24 0.32 0.40 0.48 0.56\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX\nMedian\n0.24 0.32 0.40 0.48 0.56\nIQM\n0.30 0.36 0.42 0.48 0.54\nMean\n0.48 0.54 0.60 0.66 0.72\nOptimality Gap\nNormalized Score\nFigure 16: Aggregate metrics with 95% CIs for the deterministic games listed in Table 1, follow-\ning Agarwal et al. (2021). The CIs use percentile bootstrap with stratiﬁed sampling.\n0.12 0.15 0.18 0.21 0.24\nDRRN\nINV-DY\nRC-DQN\nXTX-Uniform\nXTX\nMedian\n0.12 0.16 0.20 0.24 0.28\nIQM\n0.20\n0.24\n0.28\n0.32\nMean\n0.68\n0.72\n0.76\n0.80\n0.84\nOptimality Gap\nNormalized Score\nFigure 17: Aggregate metrics with 95% CIs for the stochastic games listed in Table 2, follow-\ning Agarwal et al. (2021). The CIs use percentile bootstrap with stratiﬁed sampling.\n21\nPublished as a conference paper at ICLR 2022\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Score ( )\n0.00\n0.25\n0.50\n0.75\n1.00\nFraction of runs with score >\nDRRN\nINV-DY\nXTX-Uniform\nXTX\nRC-DQN\nFigure 18: Performance proﬁles based on score distributions for the deterministic games listed in\nTable 1, following Agarwal et al. (2021). Shaded regions show pointwise 95% conﬁdence bands\nbased on percentile bootstrap with stratiﬁed sampling.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAvg. Normalized Score ( )\n0.00\n0.25\n0.50\n0.75\n1.00\nFraction of runs with score >\nDRRN\nINV-DY\nXTX-Uniform\nXTX\nRC-DQN\nFigure 19: Performance proﬁles based on score distributions for the stochastic games listed in\nTable 2, following Agarwal et al. (2021). Shaded regions show pointwise 95% conﬁdence bands\nbased on percentile bootstrap with stratiﬁed sampling.\n22\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 多阶段策略探索：文本游戏中的强化学习新突破\n\n## 📌 背景痛点\/本文动机\n文本冒险游戏为强化学习算法提供了独特的测试平台，但同时也带来了巨大的挑战。这些游戏具有组合爆炸式的动作空间和稀疏的奖励，这使得探索与利用之间的平衡变得尤为困难。大动作空间需要广泛的探索，而稀疏的奖励则提供了有限的反馈。现有的方法通常采用单一策略和动作选择策略，难以在探索与利用之间找到合适的平衡点。\n\n## 🚀 核心方法\n本文提出了名为 eXploit-Then-eXplore (XTX) 的多阶段控制算法，该算法在每个回合中明确地将探索和利用策略分离。XTX 算法包含两个阶段：\n\n* **利用阶段**：该阶段使用一个模仿过去成功轨迹的策略，使智能体能够返回到游戏空间中已探索的前沿状态。\n* **探索阶段**：该阶段使用一个基于好奇心驱动的策略，旨在发现新颖的动作，并探索未知的游戏状态空间。\n\n这种策略分解允许智能体结合全局决策和局部探索，从而更好地应对稀疏奖励和动态动作空间的挑战。\n\n## 📈 实验结果\n在 Jericho 基准测试的 12 个游戏中，XTX 算法在确定性和随机性设置下分别比现有方法提高了 27% 和 11% 的平均归一化分数。特别是在 Zork1 游戏中，XTX 算法取得了 103 分的成绩，比现有方法提高了 2 倍以上，并克服了游戏中一些已知的瓶颈。\n\n## 💬 可借鉴之处\n* **多阶段策略**：将探索和利用策略分离，可以更好地平衡两者之间的关系，从而提高学习效率。\n* **模仿学习**：利用过去成功的经验来指导智能体的行为，可以加快学习速度。\n* **好奇心驱动探索**：通过奖励新颖的动作，可以鼓励智能体探索未知的游戏状态空间。\n* **混合策略**：使用混合策略可以提供更细粒度的控制，从而更好地适应不同的游戏环境。\n\n## 🌟 总结\nXTX 算法为文本冒险游戏中的强化学习提供了一种新的思路，通过多阶段策略和混合策略，有效地解决了探索与利用之间的平衡问题，并在实际游戏中取得了显著的性能提升。","llm_summary_res_status":200}
{"title":"Pre-trained Language Models as Prior Knowledge for Playing Text-based Games","authors":"Ishika Singh, Gargi Singh, Ashutosh Modi","summary":"Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires an understanding of and\ninteraction using natural language in a partially observable environment.\nAgents observe the environment via textual descriptions designed to be\nchallenging enough for even human players. Past approaches have not paid enough\nattention to the language understanding capability of the proposed agents.\nTypically, these approaches train from scratch, an agent that learns both\ntextual representations and the gameplay online during training using a\ntemporal loss function. Given the sample-inefficiency of RL approaches, it is\ninefficient to learn rich enough textual representations to be able to\nunderstand and reason using the textual observation in such a complicated game\nenvironment setting. In this paper, we improve the semantic understanding of\nthe agent by proposing a simple RL with LM framework where we use\ntransformer-based language models with Deep RL models. We perform a detailed\nstudy of our framework to demonstrate how our model outperforms all existing\nagents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6\nhigher than the state-of-the-art model. Overall, our proposed approach\noutperforms 4 games out of the 14 text-based games, while performing comparable\nto the state-of-the-art models on the remaining games.","url":"http:\/\/arxiv.org\/abs\/2107.08408v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2107.08408v2","published":1626604128000,"comment":"40 Pages (8 Pages main content + 1 Page references + 31 Pages\n  Appendix). Some new results added","pdf_text":"Pre-trained Language Models as Prior Knowledge for Playing\nText-based Games\nIshika Singh, Gargi Singh, Ashutosh Modi\nDepartment of Computer Science and Engineering\nIndian Institute of Technology Kanpur (IITK), India\n{ishikas,sgargi}@iitk.ac.in, ashutoshm@cse.iitk.ac.in\nABSTRACT\nRecently, text world games have been proposed to enable artifi-\ncial agents to understand and reason about real-world scenarios.\nThese text-based games are challenging for artificial agents, as it re-\nquires an understanding of and interaction using natural language\nin a partially observable environment. Agents observe the environ-\nment via textual descriptions designed to be challenging enough\nfor even human players. Past approaches have not paid enough\nattention to the language understanding capability of the proposed\nagents. Typically, these approaches train from scratch, an agent\nthat learns both textual representations and the gameplay online\nduring training using a temporal loss function. Given the sample-\ninefficiency of RL approaches, it is inefficient to learn rich enough\ntextual representations to be able to understand and reason using\nthe textual observation in such a complicated game environment\nsetting. In this paper, we improve the semantic understanding of\nthe agent by proposing a simple RL with LM framework where we\nuse transformer-based language models with Deep RL models. We\nperform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1,\nto achieve a score of 44.7, which is 1.6 higher than the state-of-the-\nart model. Overall, our proposed approach outperforms 4 games\nout of the 14 text-based games, while performing comparable to\nthe state-of-the-art models on the remaining games.\n1\nINTRODUCTION\nArtificial autonomous agents suffer from a number of challenges\nduring training, such as reward, goal, or task under-specification(s).\nMost approaches design a specific reward function for a given en-\nvironment and task, which is not generalizable to any other setting.\nThe reward specification gets more complicated with complex en-\nvironments and tasks. Most agents are trained to do specific tasks\nor achieve specific goals, as there are not many efficient ways for\nspecifying multiple or complex goals for a given agent. In this\nscenario, being able to utilize language as an interface between a\nuser and an artificial agent simplifies a number of these challenges.\nA setting where an agent understands and communicates using\nnatural language is more efficient for rewarding the agent online\nor via a text description, as opposed to using an expert-designed\nreward function. Moreover, it is easier to specify complicated tasks\nor instructions for the agent via the natural language.\nText-based Interactive Fiction (IF) [7] games provide such an en-\nvironment where the agent learns to consume and produce natural\nlanguage-based inputs and outputs. IF games require an artificial\nagent to learn policies and operate in a real-world environment\ncreated using natural language. These games are designed such that\na player receives a textual observation consisting of information\nabout the environment such as a description of the surroundings,\nobjects available for interaction, and states of objects. Based on this\ninformation, the player takes an action such as interacting with one\nof the objects, moving around. etc. A short example of a game-play\nis shown in Figure 1. The textual setting of the games in terms of the\nobservations given to and the actions taken by the agent requires\nsolid language understanding and reasoning. Such a setting has\nbeen promoted to develop essential skills required by intelligent\nagents for various real-life use-cases, including dialogue systems\nand personal digital assistants. IF games, such as Zork1, have been\ncreated to be challenging for human players and consist of texts\nwith dramatic and artful narratives. The game engine, in the case\nof IF games, generates human-level sophistication and diversity\nin textual descriptions (as can be seen in the example in Figure\n1) in contrast to games using template-generated synthetic texts.\nConsequently, such games provide an ideal test-bed for artificial\nagents that interact with the real-world environment using natural\nlanguage. Given the setting, agents trained using IF games can be\ndeployed in real-world user-centered robotics applications.\nIF games present several challenges for artificial agents as these\ncover the real world settings. It requires an agent to understand\nthe textual state description, handle combinatorial textual action\nspace, and learn a policy in a partially observable environment to\nmaximize the game score. The key challenge is to decipher the long\ntextual observations, extract reward cues from them, and generate\na semantically rich representation such that the policy learned on\ntop of it is well informed. Most of the existing works learn textual\nrepresentations from scratch during the RL training [1, 2, 7, 23–25],\nexcept [6] which uses pre-trained GloVe embeddings [14]. Online\nreinforcement learning is known to be sample-inefficient [19]. The\nreward-based temporal difference objective used for training is\nproposed for learning the gameplay, and it does not necessarily\nreinforce the agent to learn the semantics of the game. Therefore,\nlearning textual representations solely from the game-generated\ntext during online learning does not provide rich enough represen-\ntation to be able to handle such complicated decision-making tasks.\nIn addition, these games also need some prior knowledge about\nthe functioning of the world to be able to make correct decisions\nefficiently. For example, as shown in Figure 1, if it is dark and the\nagent has a lantern, it should turn it on.\nGiven the requirements for IF games, we propose a pre-trained\ntransformer-based [20] language model (LM) as a candidate for\nequipping the RL agent with both language understanding capa-\nbilities and real-world knowledge. Transformer-based LMs such as\nBERT [5], ALBERT [10], RoBERTa [11] have produced state-of-the-\nart performance on a plethora of natural language understanding\n(NLU) tasks. The routine for using the transformer models includes\narXiv:2107.08408v2  [cs.CL]  23 Dec 2021\nwith a great eﬀort, the rug is moved to one side of the room, revealing the dusty cover of a closed trap \ndoor\nOpen case\nyou are in the living room. there is a doorway to the east, a wooden door with strange gothic lettering to \nthe west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the \nroom. above the trophy case hangs an elvish sword of great antiquity. a battery powered brass lantern \nis on the trophy case. [you are carrying : a jewel encrusted egg] [same as current observation]\npush rug\nOpened.\nTake lantern\nTaken. [you are carrying : a brass lantern a jewel encrusted egg] [living room you are in the living room. \nthere is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to \nbe nailed shut, a trophy case, and a closed trap door at your feet. above the trophy case hangs an elvish \nsword of great antiquity.]\nPut egg in case\n+5\ndone. [you are carrying : brass lantern] […a closed trap door at your feet. above the trophy case hangs \nan elvish sword of great antiquity. your collection of treasures consists of : jewel encrusted egg] Open trap\nthe door reluctantly opens to reveal a rickety staircase descending into darkness. […nailed shut, a \ntrophy case, and a rug lying beside an open trap door. above the trophy case hangs an elvish sword of \ngreat antiquity. your collection of treasures consists of : a jewel encrusted egg]\nDown\n+25\nyou have moved into a dark place. the trap door crashes shut, and you hear someone barring it. [it is \npitch black. you are likely to be eaten by a grue.]\nturn  on lantern\neast of chasm you are on the east edge of a chasm, the bottom of which cannot be seen. a narrow \npassage goes north, and the path you are on continues to the east. [same as current observation.] East\nthe brass lantern is now on. cellar you are in a dark and damp cellar with a narrow passageway leading \nnorth, and a crawlway to the south. on the west is the bottom of a steep metal ramp which is \nunclimbable. [you are carrying : a brass lantern ( providing light )] [same as current observation.] South\nthis is an art gallery. most of the paintings have been stolen by vandals with exceptional taste. the vandals \nleft through either the north or west exits. fortunately, there is still one chance for you to be a vandal, for \non the far wall is a painting of unparalleled beauty. [same as current observation.]\nTake art\n+4\nFigure 1: A sample gameplay by our model (DBERT-DRRN) for the classic text game, Zork1. The aim is to solve puzzles and\ncollect 19 treasures in the trophy case, but the agent is not aware of the goal and learns from rewards. Each white box is a\nstate of the game at a step; the green and orange boxes are the action taken, and the reward received correspondingly. The\nstate contains location if visible (underlined), current observation (black), inventory with the player (red), and the current\nlocation description (blue). In this example, location, inventory, and description are only mentioned when it changes, while\nthe complete state information is provided to the agent during gameplay. This example shows how our model is able to perform\nbetter by learning to use the egg and the lantern in the correct way, as highlighted.\npre-training on large generic English corpora, followed by fine-\ntuning on a specific downstream task-specific corpus. Moreover,\nPetroni et al. [15] have shown that pre-trained LMs can act as\nknowledge bases containing relational and factual knowledge, com-\nparable in utility with other NLP methods having access to oracle\nknowledge.\nIn this work, we propose a simple approach that performs bet-\nter than previously proposed complex approaches. We deploy a\npre-trained LM with the existing game agents, namely Deep Re-\ninforcement Relevance Network (DRRN) [8] and Template-Deep\nQ-Network (TDQN) [7], used in the recently proposed Jericho IF\ngame-suite [7]. To the best of our knowledge, we are the first to\nutilize pre-trained LMs for IF games. We use DistilBERT[17] (here-\nafter referred to as DBERT) as the LM owing to its compact size,\nthereby computing rich text encoding efficiently. We fine-tune\nDBERT on an independent set of human gameplay transcripts to\nadd a game sense to the pre-trained LM. This set of transcripts\ncontain games other than those in the evaluation game-suite to\ntest the generalizability of the proposed approach. We test both\nDBERT-DRRN and DBERT-TDQN setup on Zork1, and additionally,\nwe also evaluate DBERT-DRRN on a set of other games from Jericho\nto test the generalization capability of the model. We achieve new\nstate-of-the-art results on 4 out of 14 games while getting compara-\nble performance on others relative to the past approaches (section\n2) utilizing sophisticated attention models and knowledge graphs,\nindicating that pre-trained LMs could be employed as a key com-\nponent in text-based reinforcement learning models. We release\nthe implementation code via GitHub1. The transcripts (generated\nwhen the agent is playing the game) is included in the appendix.\n2\nRELATED WORK\nRecently, there have been attempts to model real world settings via\ntext based environments. The TextWorld environment [4] proposes\nprocedurally generated interactive fiction games, with control over\ndifficulty, and description language. Recently, Hausknecht et al. [7]\nproposed a learning environment (Jericho) that supports a set of 32\nhuman-written IF games. These games are written to be challenging\nfor human players, thereby providing a more realistic test-bed for\ntraining intelligent agents. [7] also present performance results for\nchoice-based (DRRN) and parser-based (TDQN) agents (see section\n3.2 for details), where the former performed better on average\nacross most of the games. Jericho also provides certain handicaps\nsuch as action templates, determining valid actions by detecting\nworld state change, commands to check current items with the\nagent (inventory) or in the current state (look). We propose models\nfor solving the games in the Jericho environment.\nMany of the recent approaches [1, 2, 23] use a dynamically up-\ndated Knowledge Graph (KG) to represent the current state of the\nuniverse (game). KG-A2C [1] is the first such proposal. One of\nthe SOTA methods by Xu et al. [23], in addition to KG-A2C archi-\ntecture, reasons on the KG using low and high-level attention on\nsub-components of the graph and on the complete graph, respec-\ntively, to compute a representation of the game state. Next, they\nselect the actions via recurrent decoding using GRUs, conditioned\non the computed game state representation. Similar to KG-A2C\napproach, they train the agent via the Advantage Actor Critic (A2C)\nmethod with a supervised auxiliary task of “valid action prediction”\nusing the action templates handicap. Q∗BERT [2] presents an open\ndomain QA method to update the KG with more information and\nan additional intrinsic motivation reward to enable structured ex-\nploration. Our agent outperforms all the above agents without an\nexplicit knowledge graph or any additional reward-based super-\nvision or sophisticated reasoning. We show that better language\nunderstanding capabilities and appropriate utilization of the world\nknowledge in game-plays are comparable (and sometimes better)\nthan using explicit KGs.\nMulti-Paragraph Reading Comprehension Deep Q-Network [6]\n(MPRC-DQN) breaks down the problem into two challenges: the\npartial observability of the environment and large natural language\naction space. They solve the partial observability by object-centric\nretrieval of relevant past observations, and an attention mechanism\nis deployed to focus on the most relevant context. Template actions\nfrom Jericho are filled up in question answering (QA) format to\ngenerate candidate actions. In contrast, CALM [25] generates the\nnext set of possible action by fine-tuning a GPT-2 model [16]. These\nactions, which are essentially a subset of valid actions, are then fed\nto a DRRN [8] agent to compute the Q-values. CALM asserts that\nthe action generation model provides linguistic priors to the RL\nagent. However, since they do not send any feedback to the action\ngeneration model and the model only replaces the valid action set\n1https:\/\/github.com\/Exploration-Lab\/IFG-Pretrained-LM\nby its subset, the RL agent may not necessarily benefit from the\nlinguistic priors. On the other hand, we extract representations\nfrom the pre-trained LM and feed them to the DRRN agent for\nincorporating linguistic priors. With a better observation under-\nstanding augmented with the world knowledge via the pre-trained\nLM, our model also solves the challenge of partial observability to\nsome extent.\nYao et al. [24] investigate to what extent semantic information is\nutilized by the DRRN agent. They show that even in the complete\nabsence of text, the “DRRN + valid action handicap\" setup is able to\nachieve significant scores, indicating the underlying overfitting to\nthe reward system and memorization tendency of DRRN. They\nuse an inverse-dynamics loss function to regularize the DRRN\nrepresentation space for improving the semantics.\nTransformers [20] are a new class of feed-forward neural net-\nworks architectures that use a self-attention mechanism to effec-\ntively learn short\/long-term relationships between tokens in the\ntext. Transformers have been shown to have SOTA performance on\nalmost all the NLP tasks [18, 21]. Transformers are trained using\nlanguage modeling objective and its variants (e.g., masked language\nmodeling); based on the training objective a number of variants\nof transformer architectures have been proposed e.g., BERT [5],\nDistilBERT [17], ALBERT [10], RoBERTa [11]. There have been\nseveral transformer-based approaches for both text-based games\nand RL in general [3, 9, 12, 22], but these approaches train a trans-\nformer from scratch, which is inefficient given that transformers\nare data-intensive and online RL is sample-inefficient. Xu et al. [22]\nand Parisotto et al. [12] modify the transformer structure to add\ngates and make it light-weight for online training, while Chen et al.\n[3] and Janner et al. [9] use a smaller transformer but only for of-\nfline learning. Using a pre-trained LM brings an additional set of\nlinguistic priors and world knowledge to the RL agent, and in this\npaper, we propose a way to do so. We chose DistilBERT [17] for\nour framework. DistilBERT is a lighter and faster version of BERT\ntrained by knowledge-distilling BERT base with masked language\nmodeling on Toronto Book Corpus and Wikipedia, thus providing\nus a computationally efficient solution to integrate in the RL setup.\n3\nMETHOD\n3.1\nProblem Statement\nA text-based game can be formulated as a partially observable\nMarkov Decision Process (POMDP) defined by (𝑆,𝑇,𝐴,𝑂, 𝑅). An\nagent interacts with the game environment to receive textual ob-\nservations 𝑜𝑡∈𝑂at the interaction step 𝑡. The latent states 𝑠𝑡∈𝑆\ncorrespond to 𝑜𝑡along with player, item locations, and inventory\ncontents. The agent interacts with the environment by executing\nan action 𝑎𝑡∈𝐴that changes the game state according to a mostly-\ndeterministic but latent transition function𝑇(𝑠′|𝑠,𝑎), and the agent\nreceives rewards 𝑟𝑡from an unknown reward function 𝑅(𝑠,𝑎) de-\nfined by the game designers. The objective of an RL agent is to\ntake a series of actions that maximize expected cumulative dis-\ncounted rewards E\n\u0002Í∞\n𝑖=𝑡𝛾𝑟𝑖\n\u0003\nat any time 𝑡. Value-based RL models\nestimate this reward by learning a Q-function, 𝑄(𝑜𝑡,𝑎𝑡) i.e., the\nexpected return when taking an action 𝑎𝑡, given the observation 𝑜𝑡.\nThe problem statement presents several challenges for the agent:\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n*58\n2EVHUYDWLRQ\n(QFRGHU\n*58\n\/RRN\n(QFRGHU\n*58\n,QYHQWRU\\\u0003\n(QFRGHU\n*58\n$FWLRQ\n(QFRGHU\n&RQFDW\n0\/3\n$FW\u0010VFRUHU\n4\u000bR\u000fD\f\n2EVHUYDWLRQ\n\/RRN\n,QYHQWRU\\\u0003\n$FWLRQ\n&RQFDW\n(a)\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n*58\n2EVHUYDWLRQ\n(QFRGHU\n*58\n\/RRN\n(QFRGHU\n*58\n,QYHQWRU\\\u0003\n(QFRGHU\n*58\n$FWLRQ\n(QFRGHU\n&RQFDW\n0\/3\n2EVHUYDWLRQ\n\/RRN\n,QYHQWRU\\\u0003\n3UHYLRXV\u0003$FWLRQ\nW\u0010VFRUHU\nR\u0014\u0010VFRUHU\nR\u0015\u0010VFRUHU\n4\u000bR\u000fX\f\n4\u000bR\u000fS\u0014\f\n4\u000bR\u000fS\u0015\f\n(b)\nFigure 2: RL agents - (a) DRRN: Q-value Q(o,a) is computed for observation o and action a, (b) TDQN: Q-values Q(o,u), Q(o,𝑝1),\nQ(o,𝑝2) for all templates 𝑢∈𝑇and all vocabulary 𝑝∈𝑉.\nunderstanding long sophisticated textual state descriptions, ex-\ntracting reward cues, handling combinatorial textual action space,\nreasoning-based decision-making, and learning a value function in\na partially observable environment to maximize the game score.\n3.2\nRL Agents\nWe build our model on top of existing RL agents. We select the\nfollowing two agents as these are base architectures [7] on which\nall other subsequent works build on.\nDRRN: Deep Reinforcement Relevance Network (DRRN) [7] uses\nfour GRU encoders to encode the state and each of the actions from\nthe valid action handicap, concatenate the representations, and pass\nit through an MLP layer to estimate the Q-values (𝑄(𝑜𝑡,𝑎𝑡𝑖)|𝑖=1...𝑁),\nwhere 𝑎𝑡𝑖∈A (valid action set). The next action is chosen by\nsoftmax sampling the predicted Q-values. The DRRN is trained\nusing tuples (𝑜𝑡,𝑎𝑡,𝑟𝑡,𝑜𝑡+1) sampled from a prioritized experience\nreplay buffer with the traditional temporal difference (TD) loss:\nLTD(𝜙) =\n\u0012\n𝑟𝑡+ 𝛾max\n𝑎∈A 𝑄𝜙(𝑜𝑡+1,𝑎) −𝑄𝜙(𝑜𝑡,𝑎𝑡)\n\u00132\nwhere 𝜙represents the overall DRRN parameters, 𝑟𝑡is the reward\nat 𝑡-th time step, and 𝛾is the discount factor.\nTDQN: Template Deep Q-Network (TDQN) [7] like DRRN uses\nGRU encoders to encode information that consists of the state and\nprevious action in this case. It performs template-based action gen-\neration using three Q-values based on the observation 𝑜: 𝑄(𝑜,𝑢) for\nall action templates 𝑢∈𝑇, 𝑄(𝑜, 𝑝1) and 𝑄(𝑜, 𝑝2) for all vocabulary\n𝑝∈𝑉. 𝑝1, 𝑝2 fill-up the blanks in the template to generate candidate\nactions. The authors used a supervised binary-cross entropy loss\nto train the model for valid actions in addition to the TD loss.\n3.3\nLanguage Modeling (LM)\nThe textual modality of the IF games comes with additional chal-\nlenges for an RL agent. Since the modality of the games is natural\nlanguage, the modeling of the text-based observations and states be-\ncomes important, and this problem has not been explored in-depth\nin the RL research community. We employ DBERT [17] to address\nthis in our model. We fine-tune DBERT to model the language of\ngames. We take inspiration from one of the language modeling\ntechniques, masked language modeling (MLM), used to pre-train\nDBERT. Given a pair of observation𝑜𝑡and action 𝑎𝑡composed of to-\nkens [CLS],𝑜𝑡1,𝑜𝑡2, ...,𝑜𝑡𝑛, [SEP],𝑎𝑡1,𝑎𝑡2, ...,𝑎𝑡𝑚, random tokens are\nmasked by a special token, [MASK]. Here 𝑛is the length of the\nobservation, and 𝑚is the length of the action. The masked tokens\nare predicted from the vocabulary of the model. For example, given\nsequence “[CLS] Yes, there is a diorama at one end of the [MASK],\nbut you find yourself unable to [MASK] on it with that fascinating\nstatue sitting in the middle of the room. Once you figure out how it\nmoves, you’ll be able to concentrate on the [MASK] parts of the room.\n[SEP] remove boots\", the correct predictions would be “room\", “con-\ncentrate\" and “other\" respectively. A softmax activation function is\nused for masked token prediction. Cross entropy loss function is\nused to learn the parameters, i.e.,\n𝐿= CrossEntropy\n\u0010\n𝑥∗, 𝑃(𝑥[MASK]|𝑆)\n\u0011\nwhere 𝑥∗is the ground truth, 𝑥[MASK] is the predicted token, 𝑆is\nthe masked sentence, and 𝑃(𝑥[MASK]|𝑆) is the predicted distribution\nover the vocabulary. This training enables DBERT to tune its prior\nknowledge in the language of games.\n3.4\nRL with Pre-trained LM\nWe use the fine-tuned DBERT to model the input for the requisite\nRL agent. We encode the state and the actions separately using\nDBERT and feed the representations to the respective GRUs in\nDRRN and TDQN agent. The agents are then trained as described\nin 3.2 (Figure 2a and 2b). In this way, the trained DBERT is able\nto transfer its prior knowledge about the world to different agents\nacross different games. The agents, in turn, learn more informed\npolicies on the top of DBERT optimized for specific games. We use\na single instance of the trained DBERT and do not fine-tune DBERT\nfurther for specific games to keep the language model general and\nadaptable to any RL agent or game.\n4\nEXPERIMENTS\nWe evaluate how the addition of a pre-trained LM facilitates differ-\nent agents and games. We also present ablation studies to demon-\nstrate the need for training the pre-trained LM. Lastly, we present\na study on how our model is able to achieve new state-of-the-art\nresults on Zork1.\n4.1\nLM training\nWe use the ClubFloyd dataset [25] for fine-tuning DBERT. It is a\ncollection of human game-play trajectories on 590 games. These\nare not the optimal trajectories, but it does impart a general game-\nplay sense. We pre-process this data to obtain around 217K pairs\nof observation and action, (𝑜𝑡,𝑎𝑡). The transcripts are from games\nnot included in our evaluation (section 4.2) to keep our approach\ngeneral. We use a 768-dimensional vanilla DBERT (base cased)\nmodel pre-trained on English Wikipedia and Toronto Book Corpus.\nWe train this model for 2 epochs on pre-processed ClubFloyd dataset\nusing the MLM technique.\n4.2\nRL Training\nJericho Environment: We evaluate our agent on 14 games with\ndifferent difficulty levels2 using the Jericho framework, and use the\nsame Jericho handicaps as in the respective DRRN and TDQN base-\nlines [7]. The states are observations (𝑜𝑡) concatenated with items\nin possession of the player and its current location description pro-\nvided by the game engine using commands inventory and look. We\nalso use valid actions and action templates for DBERT-DRRN and\nDBERT-TDQN respectively, following the baselines. A single game\nepisode runs for 100 environment steps at max, or gets terminated\nbefore the game is over or won.\nTraining Setup: We test our framework on two RL models: DRRN\n(Figure 2a) and TDQN (Figure 2b). The GRUs used in the agents have\nan embedding dimension of 768 features and a hidden dimension of\n128 features. Similar to [7, 25] we collect interaction data for DBERT-\nDRRN and DBERT-TDQN on 8 parallel game environment instances\nand use a prioritized experience replay buffer to store trajectories\nwith best scores. We sample transitions from this priority buffer\nwith a priority fraction of 0.5, while taking the remaining training\ndata from a general replay buffer. Most importantly, we train our\nagents for a maximum of 5×104 steps, which is less than half of the\ntraining steps used in most other baselines, and achieve SOTA and\ncomparable scores on different games. Each training step makes\nan interaction with the environment in all 8 instances, therefore\nwe only use half the amount of interaction data for training in\ncomparison to the baselines. We use a softmax exploration policy\nand a learning rate of 10−4, along with other presets the same as the\nbaseline. We report the average of scores on the last 100 finished\nepisodes as the score on a game run. We train two independent runs\n2We choose games across varying difficulty level (possible\/easy, difficult, extreme) as\nindicated in Table 1. The difficulty level assigned to each game [7] is based on optimal\nsolution length, reward density, puzzle complexity, etc. Please refer to [7] for more\ndetails on difficulty level.\nfor each model and game, and report the average of their scores\nas the final (raw) score, along with the average maximum score\nseen by the runs. The maximum seen scores are a measure of the\nexploration ability, while the raw scores are that of the learning\nability of an agent. We report the learning curves for Zork1 in\nFigure 3.\n5\nRESULTS AND ANALYSIS\nWe performed a detailed analysis of our method. We present overall\ngame-scores as compared to baseline models across a set of games.\nWe also present an ablation study of our model done on Zork1, along\nwith a semantic understanding analysis. Finally, to actually interpret\nthe agent from inside beyond numbers, we also present a qualitative\nanalysis of our model on Zork1 via a case study discussing the\nfactors leading to higher score.\n5.1\nOverall Scores\nTable 1 reports the final scores of our best performing model (trained\nDBERT with DRRN) in comparison with 7 existing baselines on\n14 games. We report raw or maximum or both the scores as given\nin original papers. Different baselines achieve SOTA scores on\ndifferent games. Our model achieves SOTA results on 4 games:\nZork1, Inhumane, Snacktime, and Jewel, while being second best\nor comparable on most of the other games. We present the learn-\ning curves for Zork1, Inhumane, and Jewel in Figures 3, 4, and 5\nrespectively. INV-DY [24] uses additional loss objectives inspired\nfrom curiosity-driven exploration [13]. While it helps them achieve\nhigher maximum scores on Zork1, but are not able to learn the high\nscore trajectories. On the other hand, our agent efficiently learns the\nmax score trajectories explored by it, thereby indicating that with\na better exploration strategy our model has the potential to achieve\nbetter scores. None of the other agents, with a max score of 55, are\nable to stably reach a score as high as our model, that maintains a\nmargin of 6.4 from the best model [6]. Our agent explores higher\nmax score on Inhumane, but more importantly, it is able to learn\nthe best-explored trajectories, thereby plateauing closer to the max\nscores for many games (Inhumane, Jewel, Omniquest, Zork1),\nindicating that the trained DBERT is an important learning compo-\nnent, and it also facilitates the exploration to some extent. We also\nreport the normalized score (raw score as a factor of max possible\nscore collected from human-written optimal walk-through) aver-\naged across all games. We get an overall norm of 15.7%, followed\nby 14.2% achieved by MPRC-DQN [6]. Our model does not suffer a\nlot on any game, while the second best (MPRC-DQN) gets 0 scores\non Inhumane and Snacktime, and notably higher scores on others\n(Ludicorp, Spirit, Zork3). It indicates both the generalization\ntendency and the necessity of the pre-trained LM deployed in our\nmodel. When compared to the average norm of 18.9% for INV-DY\nevaluated on 6 of the 14 games they reported, we get 21.1%. The\naverage norm is also a measure of human-machine gap for text-\nbased games, indicating that IF games are at best only 15.7% solved.\nHence, it is a good benchmark for developing language understand-\ning agents. We have included the learning curves for independent\nruns on Zork1 and its game transcripts in the Appendix.\nTable 1: Final (raw) and max scores seen by agents (available via prior work) comparing trained DBERT-DRRN with all the\ncurrent SOTA baselines across a set of games from Jericho game-suite. For the baselines, we use the results reported in the\noriginal papers. The missing scores not in the previous work are denoted as ’-’. Max denotes the maximum possible scores\nbased on human-written optimal walkthroughs for winning the game without the step limit of 100. Note that our model\n(DBERT-DRRN) is trained for less than half of the training steps used in other baselines. Bold scores denote the best score,\nblue* scores denote the second best. DBERT-DRRN gets an avg. norm of 15.7% on all games, and 21.1% on 6 games reported\nfor INV-DY. *the games with rising learning curve for DBERT-DRRN till the last training step. 𝑝(possible\/easy), 𝑑(difficult),\n𝑒(extreme) refers to game difficulty level as given in [7].\nGame\nTDQN\nDRRN\nMPRC-DQN\nSHA-KG\nQ*BERT\nCALM\nINV-DY\nDBERT-DRRN\nMax\n[7]\n[7]\n[6]\n[23]\n[2]\n[25]\n[24]\n(ours)\nraw\nraw \/ max\nraw\nraw\nmax\nraw\nraw \/ max\nraw \/ max\nInhumane∗𝑝\n0.7\n0\n0\n5.4\n-\n25.7*\n19.6 \/ 45\n32.8 \/ 50\n90\nJewel𝑑\n0\n1.6\n4.46*\n1.8\n-\n0.3\n-\n6.5 \/ 13\n90\nLibrary∗𝑝\n6.3\n17.0*\n17.7\n15.8\n19\n9.0\n16.2 \/ 21\n17.0* \/ 21\n30\nLudicorp𝑑\n6\n13.8\n19.7\n17.8*\n22.8\n10.1\n13.5 \/ 23\n12.5 \/ 18\n150\nOmniquest𝑝\n16.8\n5\n10.0\n-\n-\n6.9*\n5.3 \/ 10\n4.9 \/ 5\n50\nReverb𝑝\n0.3\n8.2*\n2.0\n10.6\n-\n-\n-\n6.1 \/ 12\n50\nSnacktime𝑝\n9.7\n0 \/ 0.25\n0\n-\n-\n19.4*\n-\n20.0 \/ 20\n50\nSpellbrkr∗𝑒\n18.7\n37.8\n25\n40\n-\n40\n-\n38.2* \/ 40\n600\nSpirit𝑒\n0.6\n0.8\n3.8\n3.8\n-\n1.4\n-\n2.1* \/ 8\n250\nTemple𝑝\n7.9*\n7.4\n8.0\n7.9*\n8.0\n0\n-\n8.0 \/ 8.0\n35\nTryst205∗𝑒\n0\n9.6*\n10.0\n6.9\n-\n-\n9.3 \/ 17\n350\nZork1𝑑\n9.9\n32.6 \/ 53\n38.3\n34.5\n41.6\n30.4\n43.1* \/ 87\n44.7 \/ 55\n350\nZork3𝑑\n0\n0.5\n3.63\n0.7*\n-\n0.5\n0.4 \/ 4\n0.2 \/ 4\n7\nYomomma∗𝑑\n0\n0.4\n1.0\n-\n-\n-\n-\n0.5* \/ 1.0\n35\nAvg. Norm (%)\n7.7\n10.2\n14.2\n13.3\n-\n12.8\n18.9\n15.7 (21.1)\n100\n5.2\nDBERT Training Ablation\nOur framework includes a DBERT trained on human trajectories\nfrom different games. This training induces a language acquaintance\nwith games. We tested empirically how this affected the model.\nWe obtain runs on both DRRN and TDQN based models for the\nvanilla DBERT and DBERT pretrained on games from ClubFloyd\ndataset. We present the results in Figure 3 where we see DRRN\nwith pretrained DBERT outperforms DRRN with vanilla DBERT\nwith a final score of 44.72 compared to 34.73. The TDQN based\nmodels do not exhibit much difference, but pretrained version still\nhas a higher score of 10.35, whereas vanilla DBERT version of the\nframework obtains 9.85. It indicates the importance of acquainting\nthe pre-trained LM with the game language, so that it can transfer\nits prior knowledge appropriately to the agents.\n5.3\nSemantic Understanding: Transfer\nYao et al. [24] show that “TD loss + valid action handicap\" setup\nwith mostly deterministic rewards can lead to overfitting and mem-\norization, thus, hindering the actual goal of understanding and\nlearning to operate in natural language. Therefore, we also test\nwhether an agent trained for one game can transfer its learning to\nother similar games to see if the agent is learning the semantics\nor memorizing the trajectories. We train an agent on Zork1 and\ntest it without any training on Zork3, as these are the two most\nsimilar games in the Jericho game-suite. The average episode score\nover 300 episodes was 0.06 for DBERT-DRRN and 0.007 for DRRN.\nThese scores are not as high as achieved by training fresh agents\non Zork3; however, DBERT-DRRN is able to transfer its knowledge\nbetter than DRRN.\n5.4\nAvoiding being eaten by a grue and\nthereafter\nWe compare scores over an episode for (vanilla and trained) DBERT-\nDRRN and DRRN. We sample 100 random episodes from the last\nfew hundred episodes during the training, and plot the average\nscores over an episode in Figure 6. An episode is set to terminate\nafter 100 steps or before (if the game is over or won). The goal in\nZork1 is to collect 19 treasures in the trophy case, which agents are\nnot aware of, and they learn via in-game rewards. Existing agents\ntraverse the Forest for the Egg (1st treasure), followed by Kitchen,\n0\n100\n200\n300\n400\nSteps (x100)\n10\n0\n10\n20\n30\n40\n50\nLast100EpisodeScores\ntdqn + vanilla_dbert\ntdqn + trained_dbert\ndrrn + vanilla_dbert\ndrrn + trained_dbert\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n10\n0\n10\n20\n30\n40\n50\nMax score seen\ntdqn + vanilla_dbert\ntdqn + trained_dbert\ndrrn + vanilla_dbert\ndrrn + trained_dbert\n(b)\nFigure 3: DBERT performance ablation results on Zork1\n0\n100\n200\n300\n400\nSteps (x100)\n0\n5\n10\n15\n20\n25\n30\nLast100EpisodeScores\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nMax score seen\n(b)\nFigure 4: DBERT-DRRN performance results on Inhumane\nCellar, and Gallery for the painting (next treasure), while solving\nvarious puzzle and escaping the grue. In DRRN, the agent gets to\nthe Kitchen with a reward of +10 at around step-10 and then to\nthe Cellar with a reward of +25 around step-15 as we can see in\n0\n50\n100\n150\n200\n250\n300\n350\n400\nSteps (x100)\n1\n2\n3\n4\n5\n6\n7\nLast100EpisodeScores\n(a)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nSteps (x100)\n4\n6\n8\n10\n12\nMax score seen\n(b)\nFigure 5: DBERT-DRRN performance results on Jewel\nthe plot. It doesn’t learn to take and carry the Egg for a reward\nof +5 (and another +5 for putting it in the case) before moving\nto the Kitchen even though the observations present the Egg as\nsomething precious “..in the bird’s nest is a large egg encrusted with\n0\n20\n40\n60\n80\n100\nSteps\n0\n10\n20\n30\n40\n50\nAverage scores over an episode\ndrrn\ndrrn + vanilla_dbert\ndrrn + trained_dbert\nFigure 6: Avoiding being eaten by a grue: Average scores over\nan episode for Zork1\nprecious jewels, apparently scavenged by a childless songbird. the\negg is covered with fine gold inlay,..\". Ammanabrolu et al. [2] use an\nintrinsic motivation reward in addition to the rewards provided by\nthe game engine, in order to get past when stuck in the game, i.e.,\nwhen the observations do not change. This motivation also helps\nthem provide rewards to add dependencies to their KG for the Egg.\nHowever, the LM in trained DBERT-DRRN guesses that the Egg\nis important without any extra supervision, and takes it around\nstep-10, hence, achieving higher scores later on. Moreover, trained\nDBERT-DRRN avoids being eaten by a grue in the Cellar which says\n“the trap door crashes shut, and you hear someone barring it. it is pitch\nblack. you are likely to be eaten by a grue.\" by instantly turning on\nthe lantern without any reward, while DRRN takes random actions\nat this point and dies. Then our agent finds the way to the gallery,\nand gets the painting for +4 points. Thus, merely improving the\nunderstanding of the agent enables it to take better actions without\nany additional supervision. A part of a sample episode transcript\nis shown in Figure 1, and the complete episode is provided in the\nAppendix.\nIn summary, the pre-trained LM enables the DBERT-DRRN agent\nto extract reward cues from the observations as well as use its\npriors on world sense and game sense to get past the grue. We\nobserved that the Vanilla-DBERT is only as good as the base DRRN\nagent. Training it on a set of gameplays is improving the model\nconsiderably, indicating the importance of this training, which is\nessentially channeling the world sense of Vanilla-DBERT into a\ngameplay mode. The DBERT-DRRN agent understands that a “jewel\nencrusted egg appearing extremely fragile\" should be taken, and\nshould not be thrown, or if it’s dark the lantern should be turned\non. Trained DBERT-DRRN assigns reasonably high Q-values to\nsuch actions in the valid action set, while DRRN assigns similar\nvalues when rewards are absent for the same (as can be seen in\nthe trajectories provided in the Appendix). Both DBERT-DRRN and\nall other baselines are able to reach the gallery. Our agent does so\nmore often as it escapes the grue in the Cellar using the lantern.\nMoreover, it extracts intermediate rewards cues and ends up with\na higher score at the end of an episode. The baselines with a score\naround 39 or less are the ones which could not learn these aspects\nof the game. After getting the painting, the agents explore several\noptions, but none of them, including ours, are able to find and learn\nto find the third treasure. Moving past trained DBERT-DRRN score\nwill likely require a more intelligent agent with better exploration\nand learning strategies.\n6\nCONCLUSION AND DISCUSSION\nThis paper proposes using a pre-trained LM fine-tuned on game\ndynamics for RL agents trained for text-based games. It provides\nthree-fold benefits to the RL agent: linguistic priors, world sense\npriors, and game sense priors. It facilitates the agent to achieve\nSOTA results on several text-based games, even though being a\nsimpler approach than all other baselines. The proposed approach\nindicates the importance of using pre-trained LMs for RL agents\nin text-based games. Through this work, we want to draw the\nresearch community’s attention and motivate research in this direc-\ntion to create even better priors using pre-trained LM for language\nunderstanding agents, such as distilling its knowledge to agents\nwith better architectures. Such unsupervised components for explo-\nration and learning will also be more useful in real-life scenarios\nwhere there are no explicit determinate rewards than reward-based\nrepresentation learning. We have shown that training RL agents\nfor text-based games in the absence of a prior is inefficient. The\ntwo key reasons are: learning language from scratch using the lim-\nited text data with a TD loss or related objective does not produce\nrich enough representations, and the agent has no source of world\nknowledge to be able to take appropriate actions given the state.\nThe agents thus trained are not really understanding their state\nin the game, leading to the suspicion that the agents are simply\nmemorizing the best-explored trajectories [24].\nIF games are still far from solved. We need agents with better\nexploration strategies (for instance curiosity-driven learning [13])\nto find high score trajectories, as well as learning architectures\nand objectives that can facilitate learning these trajectories while\nefficiently leveraging priors from pre-trained language models. Cur-\nrent results show the performance of the agents trained on the same\ngame environments. Another line of future work is to test trained\nagents on unseen games. This will require a strong understanding\nof the world functioning and generalized training strategies to be\nable to acquire skills from multiple games and transfer them to\nthe unseen test environments. This will also be a robust test-bed\nfor semantic understanding of the agent, as memorization will no\nlonger make any points in unseen games. Leveraging commonsense\nknowledge for text-based game agents can be another interesting\ndirection for improving the knowledge priors. Many actions taken\nduring these games are associated with object affordances. With a\ncommonsense knowledge source, the agent will be able to extract\nrational interaction possibilities with the objects.\nREFERENCES\n[1] Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph Constrained\nReinforcement Learning for Natural Language Action Spaces. In International\nConference on Learning Representations.\nhttps:\/\/openreview.net\/forum?id=\nB1x6w0EtwH\n[2] Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, and Mark Riedl.\n2021. How to Avoid Being Eaten by a Grue: Structured Exploration Strategies\nfor Textual Worlds. https:\/\/openreview.net\/forum?id=eYgI3cTPTq9\n[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael\nLaskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision\nTransformer: Reinforcement Learning via Sequence Modeling. arXiv preprint\narXiv:2106.01345 (2021).\n[4] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes,\nEmery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri,\nMahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld: A Learning\nEnvironment for Text-based Games. CoRR abs\/1806.11532 (2018).\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL]\n[6] Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu\nChang. 2020. Interactive Fiction Game Playing as Multi-Paragraph Reading Com-\nprehension with Reinforcement Learning. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Online, 7755–7765. https:\/\/doi.org\/10.18653\/v1\/2020.\nemnlp-main.624\n[7] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and\nXingdi Yuan. 2020. Interactive Fiction Games: A Colossal Adventure. Proceedings\nof the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020), 7903–7910.\nhttps:\/\/doi.org\/10.1609\/aaai.v34i05.6297\n[8] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari\nOstendorf. 2016. Deep Reinforcement Learning with a Natural Language Action\nSpace. In Proceedings of the 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers). Association for Computational\nLinguistics, Berlin, Germany, 1621–1630. https:\/\/doi.org\/10.18653\/v1\/P16-1153\n[9] Michael Janner, Qiyang Li, and Sergey Levine. 2021. Reinforcement Learning as\nOne Big Sequence Modeling Problem. arXiv:2106.02039 [cs.LG]\n[10] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. arXiv:1909.11942 [cs.CL]\n[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n[12] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Sid-\ndhant Jayakumar, Max Jaderberg, Raphaël Lopez Kaufman, Aidan Clark, Seb\nNoury, Matthew Botvinick, Nicolas Heess, and Raia Hadsell. 2020. Stabiliz-\ning Transformers for Reinforcement Learning. In Proceedings of the 37th In-\nternational Conference on Machine Learning (Proceedings of Machine Learning\nResearch, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, 7487–7498.\nhttp:\/\/proceedings.mlr.press\/v119\/parisotto20a.html\n[13] Deepak\nPathak,\nPulkit\nAgrawal,\nAlexei\nA.\nEfros,\nand\nTrevor\nDar-\nrell. 2017.\nCuriosity-driven Exploration by Self-supervised Prediction.\narXiv:1705.05363 [cs.LG]\n[14] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Doha, Qatar, 1532–1543. https:\/\/doi.org\/10.3115\/v1\/\nD14-1162\n[15] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong,\nChina, 2463–2473. https:\/\/doi.org\/10.18653\/v1\/D19-1250\n[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nblog 1, 8 (2019), 9.\n[17] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs.CL]\n[18] Sushant Singh and Ausif Mahmood. 2021. The NLP Cookbook: Modern Recipes\nfor Transformer Based Deep Learning Architectures. IEEE Access 9 (2021), 68675–\n68702.\n[19] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction. MIT press.\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL]\n[21] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. arXiv preprint arXiv:1804.07461 (2018).\n[22] Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, and Chengqi Zhang. 2020. Deep\nReinforcement Learning with Transformers for Text Adventure Games. In 2020\nIEEE Conference on Games (CoG). 65–72. https:\/\/doi.org\/10.1109\/CoG47356.2020.\n9231622\n[23] Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi\nZhang. 2020. Deep Reinforcement Learning with Stacked Hierarchical Attention\nfor Text-based Games. In Advances in Neural Information Processing Systems,\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33.\nCurran Associates, Inc., 16495–16507.\nhttps:\/\/proceedings.neurips.cc\/paper\/\n2020\/file\/bf65417dcecc7f2b0006e1f5793b7143-Paper.pdf\n[24] Shunyu Yao, Karthik Narasimhan, and Matthew Hausknecht. 2021. Reading and\nActing while Blindfolded: The Need for Semantics in Text Game Agents. In North\nAmerican Association for Computational Linguistics (NAACL).\n[25] Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020.\nKeep CALM and Explore: Language Models for Action Generation in Text-based\nGames. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP). Association for Computational Linguistics, Online,\n8736–8754. https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.704\nAPPENDIX\nA\nLEARNING CURVES\nThe learning curves for Zork1 are shown in Figure 7.\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nLast100EpisodeScores\nrun1\nrun2\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n10\n20\n30\n40\n50\nMax score seen\nrun1\nrun2\n(b)\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nEpisodeScore\nrun1\nrun2\n(c)\n0\n100\n200\n300\n400\nSteps (x100)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nLoss\nrun1\nrun2\n(d)\nFigure 7: Results for Zork1: Two independent runs of\n(DBERT-DRRN)\nB\nGAMEPLAY TRANSCRIPTS FOR TRAINED\nDBERT-DRRN\nThe transcripts presents the game-play over an episode of 100 steps.\nFor each step, the game state (observation, inventory, description),\nlist of valid actions and corresponding Q-Values estimated by the\nagent, the action taken, reward received, cumulative score, and\nwhether the episode is over (Done) is given.\nList of transcripts:\n• D.1 Zork1\n– D.1.1 DRRN (for comparison)\n• D.2 Inhumane\nB.1\nZork1\nSTATE 0\nCopyright ( c ) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\nZORK is a registered trademark of Infocom, Inc. Revision 88 \/ Se-\nrial number 840726 West of House You are standing in an open\nfield west of a white house, with a boarded front door. There is a\nsmall mailbox here. You are empty handed. West of House You are\nstanding in an open field west of a white house, with a boarded\nfront door. There is a small mailbox here. Actions: [’west’, ’north’,\n’south’, ’open mailbox’] Qvalues: [9.07, 7.74, 7.42, 5.98] Action: west\nReward: 0, Score 0, Done False\nSTATE 1\nforest this is a forest, with trees in all directions. to the east, there\nappears to be sunlight. you are empty handed. forest this is a forest,\nwith trees in all directions. to the east, there appears to be sunlight.\nActions: [’east’, ’go around trees’, ’north’, ’south’] Qvalues: [10.05,\n7.14, 6.8, 6.64] Action: south Reward: 0, Score 0, Done False\nSTATE 2\nforest this is a dimly lit forest, with large trees all around. you are\nempty handed. forest this is a dimly lit forest, with large trees all\naround. Actions: [’northwest’, ’north’, ’go around trees’, ’west’]\nQvalues: [6.59, 6.52, 6.1, 5.37] Action: northwest Reward: 0, Score 0,\nDone False\nSTATE 3\nsouth of house you are facing the south side of a white house. there\nis no door here, and all the windows are boarded. you are empty\nhanded. south of house you are facing the south side of a white\nhouse. there is no door here, and all the windows are boarded. Ac-\ntions: [’east’, ’south’, ’west’] Qvalues: [7.71, 6.15, 5.75] Action: east\nReward: 0, Score 0, Done False\nSTATE 4\nbehind house you are behind the white house. a path leads into the\nforest to the east. in one corner of the house there is a small window\nwhich is slightly ajar. you are empty handed. behind house you are\nbehind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is slightly\najar. Actions: [’open window’, ’east’, ’north’, ’south’] Qvalues: [9.39,\n7.99, 7.91, 7.4] Action: east Reward: 0, Score 0, Done False\nSTATE 5\nclearing you are in a small clearing in a well marked forest path\nthat extends to the east and west. you are empty handed. clearing\nyou are in a small clearing in a well marked forest path that extends\nto the east and west. Actions: [’west’, ’north’, ’east’, ’south’, ’go\naround forest’] Qvalues: [7.97, 6.53, 6.35, 6.21, 5.76] Action: north\nReward: 0, Score 0, Done False\nSTATE 6\nforest this is a dimly lit forest, with large trees all around. you are\nempty handed. you hear in the distance the chirping of a song bird.\nforest this is a dimly lit forest, with large trees all around. you hear\nin the distance the chirping of a song bird. Actions: [’south’, ’go\naround trees’, ’west’, ’east’] Qvalues: [6.83, 6.45, 6.22, 5.98] Action:\nwest Reward: 0, Score 0, Done False\nSTATE 7\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you are empty handed.\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you hear in the dis-\ntance the chirping of a song bird. Actions: [’up’, ’go around forest’,\n’south’, ’east’, ’north’, ’west’] Qvalues: [10.91, 8.65, 8.4, 7.61, 7.09,\n5.01] Action: up Reward: 0, Score 0, Done False\nSTATE 8\nup a tree you are about 10 feet above the ground nestled among\nsome large branches. the nearest branch above you is above your\nreach. beside you on the branch is a small bird’s nest. in the bird’s\nnest is a large egg encrusted with precious jewels, apparently scav-\nenged by a childless songbird. the egg is covered with fine gold\ninlay, and ornamented in lapis lazuli and mother of pearl. unlike\nmost eggs, this one is hinged and closed with a delicate looking\nclasp. the egg appears extremely fragile. you are empty handed. up\na tree you are about 10 feet above the ground nestled among some\nlarge branches. the nearest branch above you is above your reach.\nbeside you on the branch is a small bird’s nest. in the bird’s nest is\na large egg encrusted with precious jewels, apparently scavenged\nby a childless songbird. the egg is covered with fine gold inlay, and\nornamented in lapis lazuli and mother of pearl. unlike most eggs,\nthis one is hinged and closed with a delicate looking clasp. the egg\nappears extremely fragile. Actions: [’take egg’, ’close nest’, ’take\nnest’, ’take on egg’, ’down’] Qvalues: [11.59, 4.09, 2.25, 0.82, -4.72]\nAction: take egg Reward: 5, Score 5, Done False\nSTATE 9\ntaken. you are carrying : a jewel encrusted egg up a tree you are\nabout 10 feet above the ground nestled among some large branches.\nthe nearest branch above you is above your reach. beside you on the\nbranch is a small bird’s nest. you hear in the distance the chirping\nof a song bird. Actions: [’down’, ’close nest’, ’put egg in nest’, ’take\nnest’, ’take on egg’, ’throw egg at nest’, ’put down egg’] Qvalues:\n[8.07, 6.47, 5.62, 5.51, 5.29, 5.04, 4.88] Action: down Reward: 0, Score\n5, Done False\nSTATE 10\nforest path you are carrying : a jewel encrusted egg forest path this\nis a path winding through a dimly lit forest. the path heads north\nsouth here. one particularly large tree with some low branches\nstands at the edge of the path. you hear in the distance the chirping\nof a song bird. Actions: [’south’, ’go around forest’, ’north’, ’east’,\n’west’, ’open egg with all’, ’up’, ’take on egg’, ’throw egg at tree’,\n’put down egg’] Qvalues: [8.94, 8.82, 8.04, 7.9, 7.49, 7.45, 7.02, 6.23,\n6.19, 5.91] Action: east Reward: 0, Score 5, Done False\nSTATE 11\nforest you are carrying : a jewel encrusted egg you hear in the\ndistance the chirping of a song bird. forest this is a dimly lit for-\nest, with large trees all around. Actions: [’south’, ’go around trees’,\n’east’, ’west’, ’open egg with all’, ’throw egg at large’, ’take on egg’,\n’put down egg’] Qvalues: [8.72, 7.81, 7.81, 7.49, 7.4, 6.49, 6.14, 6.09]\nAction: go around trees Reward: 0, Score 5, Done False\nSTATE 12\nforest you are carrying : a jewel encrusted egg forest this is a dimly\nlit forest, with large trees all around. Actions: [’northwest’, ’north’,\n’go around trees’, ’west’, ’open egg with all’, ’take on egg’, ’throw\negg at large’, ’put down egg’] Qvalues: [8.56, 8.06, 7.97, 7.58, 7.44,\n6.38, 6.35, 6.14] Action: northwest Reward: 0, Score 5, Done False\nSTATE 13\nsouth of house you are carrying : a jewel encrusted egg south of\nhouse you are facing the south side of a white house. there is no\ndoor here, and all the windows are boarded. Actions: [’east’, ’south’,\n’open egg with all’, ’west’, ’take on egg’, ’put down egg’, ’throw egg\nat house’] Qvalues: [9.39, 8.03, 7.8, 7.47, 5.81, 5.77, 5.59] Action: east\nReward: 0, Score 5, Done False\nSTATE 14\nbehind house you are carrying : a jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nslightly ajar. Actions: [’open window’, ’open egg with all’, ’south’,\n’east’, ’north’, ’put down egg’, ’take on egg’, ’throw egg at path’]\nQvalues: [10.97, 9.79, 9.03, 8.74, 8.71, 7.78, 7.72, 6.85] Action: open\nwindow Reward: 0, Score 5, Done False\nSTATE 15\nwith great effort, you open the window far enough to allow entry.\nyou are carrying : a jewel encrusted egg behind house you are\nbehind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is open.\nActions: [’west’, ’close window’, ’open egg with all’, ’east’, ’south’,\n’north’, ’put down egg’, ’take on egg’, ’throw egg at path’] Qvalues:\n[11.56, 9.85, 9.56, 9.54, 9.12, 8.97, 7.34, 6.58, 6.2] Action: east Reward:\n0, Score 5, Done False\nSTATE 16\nclearing you are carrying : a jewel encrusted egg clearing you are\nin a small clearing in a well marked forest path that extends to the\neast and west. Actions: [’west’, ’open egg with all’, ’east’, ’south’,\n’north’, ’go around forest’, ’put down egg’, ’throw egg at forest’,\n’take on egg’] Qvalues: [9.27, 8.16, 8.13, 7.81, 7.76, 7.49, 6.35, 6.08,\n5.98] Action: west Reward: 0, Score 5, Done False\nSTATE 17\nbehind house you are carrying : a jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nopen. Actions: [’open egg with all’, ’close window’, ’west’, ’east’,\n’south’, ’north’, ’put down egg’, ’take on egg’, ’throw egg at path’]\nQvalues: [9.71, 9.64, 9.54, 8.65, 8.58, 8.54, 7.81, 7.27, 6.8] Action: west\nReward: 10, Score 15, Done False\nSTATE 18\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains : a\nquantity of water you are carrying : a jewel encrusted egg kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads\nto the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water Actions: [’west’, ’close window’, ’open sack’,\n’east’, ’open bottle’, ’take sack’, ’put down egg’, ’put egg on ta-\nble’, ’throw egg at bottle’, ’take on egg’, ’take bottle’, ’take all’, ’up’,\n’jump’] Qvalues: [21.67, 9.94, 9.52, 9.26, 8.74, 7.63, 7.6, 7.41, 6.13, 5.58,\n4.72, 3.89, -2.46, -11.57] Action: west Reward: 0, Score 15, Done False\nSTATE 19\nliving room you are in the living room. there is a doorway to the\neast, a wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a large oriental rug\nin the center of the room. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on the\ntrophy case. you are carrying : a jewel encrusted egg living room\nyou are in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a large oriental rug in the center\nof the room. above the trophy case hangs an elvish sword of great\nantiquity. a battery powered brass lantern is on the trophy case.\nActions: [’take lantern’, ’push rug’, ’open case’, ’turn on lantern’,\n’throw egg at case’, ’throw egg at lantern’, ’take sword’, ’put down\negg’, ’take on egg’, ’take all’, ’east’] Qvalues: [25.17, 25.08, 24.82,\n20.51, 20.26, 20.21, 17.84, 17.42, 17.37, 13.44, 13.38] Action: push rug\nReward: 0, Score 15, Done False\nSTATE 20\nwith a great effort, the rug is moved to one side of the room, re-\nvealing the dusty cover of a closed trap door. you are carrying : a\njewel encrusted egg living room you are in the living room. there is\na doorway to the east, a wooden door with strange gothic lettering\nto the west, which appears to be nailed shut, a trophy case, and a\nclosed trap door at your feet. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on the\ntrophy case. Actions: [’take lantern’, ’open case’, ’open trap’, ’turn\non lantern’, ’throw egg at case’, ’throw egg at lantern’, ’take sword’,\n’take on egg’, ’put down egg’, ’take all’, ’east’] Qvalues: [27.57, 26.49,\n26.36, 22.97, 22.6, 21.73, 19.57, 19.16, 19.04, 15.18, 13.41] Action: open\ncase Reward: 0, Score 15, Done False\nSTATE 21\nopened. you are carrying : a jewel encrusted egg living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity. a\nbattery powered brass lantern is on the trophy case. Actions: [’put\negg in case’, ’take lantern’, ’open trap’, ’close case’, ’turn on lantern’,\n’throw egg at case’, ’throw egg at lantern’, ’fill case with egg’, ’take\nsword’, ’take on egg’, ’put down egg’, ’take all’, ’east’] Qvalues:\n[27.91, 27.9, 25.05, 24.38, 23.28, 22.29, 21.63, 20.13, 19.53, 19.27, 18.89,\n15.31, 12.56] Action: take lantern Reward: 0, Score 15, Done False\nSTATE 22\ntaken. you are carrying : a brass lantern a jewel encrusted egg living\nroom you are in the living room. there is a doorway to the east,\na wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a closed trap door at\nyour feet. above the trophy case hangs an elvish sword of great\nantiquity. Actions: [’put egg in case’, ’open trap’, ’close case’, ’open\negg with lantern’, ’put all in case’, ’turn on lantern’, ’throw egg at\ncase’, ’put lantern in case’, ’put down lantern’, ’fill case with egg’,\n’take sword’, ’throw lantern at case’, ’take on egg’, ’throw lantern\nat egg’, ’put down all’, ’put down egg’, ’east’] Qvalues: [30.7, 28.07,\n27.18, 26.67, 24.57, 23.87, 22.85, 22.05, 21.95, 21.19, 20.45, 20.32, 19.95,\n19.49, 19.48, 19.32, 14.5] Action: put egg in case Reward: 5, Score\n20, Done False\nSTATE 23\ndone. you are carrying : a brass lantern living room you are in the\nliving room. there is a doorway to the east, a wooden door with\nstrange gothic lettering to the west, which appears to be nailed shut,\na trophy case, and a closed trap door at your feet. above the trophy\ncase hangs an elvish sword of great antiquity. your collection of\ntreasures consists of : a jewel encrusted egg Actions: [’open trap’,\n’close case’, ’open egg with lantern’, ’take egg from sword’, ’turn on\nlantern’, ’take egg’, ’put down lantern’, ’put lantern in case’, ’throw\nlantern at case’, ’take sword’, ’throw lantern at egg’, ’take on egg’,\n’east’] Qvalues: [30.74, 27.85, 27.84, 27.56, 25.61, 24.67, 24.36, 23.05,\n22.62, 22.33, 21.85, 21.0, 15.86] Action: open trap Reward: 0, Score\n20, Done False\nSTATE 24\nthe door reluctantly opens to reveal a rickety staircase descending\ninto darkness. you are carrying : a brass lantern living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a rug lying beside an open trap\ndoor. above the trophy case hangs an elvish sword of great antiq-\nuity. your collection of treasures consists of : a jewel encrusted egg\nActions: [’down’, ’open egg with lantern’, ’close case’, ’take egg\nfrom sword’, ’close trap’, ’turn on lantern’, ’take egg’, ’put down\nlantern’, ’put lantern in case’, ’throw lantern at case’, ’take sword’,\n’throw lantern at egg’, ’take on egg’, ’east’] Qvalues: [32.52, 27.82,\n27.35, 27.33, 26.53, 26.21, 24.31, 24.03, 23.0, 22.09, 21.81, 21.24, 20.86,\n15.54] Action: down Reward: 25, Score 45, Done False\nSTATE 25\nyou have moved into a dark place. the trap door crashes shut, and\nyou hear someone barring it. it is pitch black. you are likely to be\neaten by a grue. you are carrying : a brass lantern it is pitch black.\nyou are likely to be eaten by a grue. Actions: [’turn on lantern’, ’put\ndown lantern’, ’throw lantern at grue’, ’south’, ’east’, ’north’] Qval-\nues: [7.76, -1.33, -3.82, -4.74, -7.48, -10.65] Action: turn on lantern\nReward: 0, Score 45, Done False\nSTATE 26\nthe brass lantern is now on. cellar you are in a dark and damp cellar\nwith a narrow passageway leading north, and a crawlway to the\nsouth. on the west is the bottom of a steep metal ramp which is un-\nclimbable. you are carrying : a brass lantern ( providing light ) cellar\nyou are in a dark and damp cellar with a narrow passageway lead-\ning north, and a crawlway to the south. on the west is the bottom of\na steep metal ramp which is unclimbable. Actions: [’south’, ’put out\nlantern’, ’put down lantern’, ’north’, ’throw lantern at crawlway’]\nQvalues: [8.37, 4.86, 4.5, 4.2, 0.44] Action: south Reward: 0, Score\n45, Done False\nSTATE 27\neast of chasm you are on the east edge of a chasm, the bottom\nof which cannot be seen. a narrow passage goes north, and the\npath you are on continues to the east. you are carrying : a brass\nlantern ( providing light ) east of chasm you are on the east edge\nof a chasm, the bottom of which cannot be seen. a narrow passage\ngoes north, and the path you are on continues to the east. Actions:\n[’east’, ’north’, ’put out lantern’, ’put down lantern’, ’throw lantern\nat path’, ’jump’] Qvalues: [9.28, 4.51, 4.16, 3.46, 0.51, -7.56] Action:\neast Reward: 0, Score 45, Done False\nSTATE 28\ngallery this is an art gallery. most of the paintings have been stolen\nby vandals with exceptional taste. the vandals left through either\nthe north or west exits. fortunately, there is still one chance for\nyou to be a vandal, for on the far wall is a painting of unparalleled\nbeauty. you are carrying : a brass lantern ( providing light ) gallery\nthis is an art gallery. most of the paintings have been stolen by\nvandals with exceptional taste. the vandals left through either the\nnorth or west exits. fortunately, there is still one chance for you to\nbe a vandal, for on the far wall is a painting of unparalleled beauty.\nActions: [’take art’, ’put out lantern’, ’put down lantern’, ’throw\nlantern at art’, ’north’, ’west’] Qvalues: [10.35, 10.21, 8.89, 6.58, 6.06,\n3.92] Action: take art Reward: 4, Score 49, Done False\nSTATE 29\ntaken. you are carrying : a painting a brass lantern ( providing\nlight ) gallery this is an art gallery. most of the paintings have been\nstolen by vandals with exceptional taste. the vandals left through\neither the north or west exits. Actions: [’north’, ’put out lantern’,\n’put down lantern’, ’put down art’, ’west’, ’put down all’, ’throw art\nat lantern’] Qvalues: [7.74, 7.08, 6.64, 6.36, 6.36, 6.35, 4.02] Action:\nnorth Reward: 0, Score 49, Done False\nSTATE 30\nstudio this appears to have been an artist’s studio. the walls and\nfloors are splattered with paints of 69 different colors. strangely\nenough, nothing of value is hanging here. at the south end of the\nroom is an open door ( also covered with paint ). a dark and narrow\nchimney leads up from a fireplace ; although you might be able\nto get up it, it seems unlikely you could get back down. loosely\nattached to a wall is a small piece of paper. you are carrying : a\npainting a brass lantern ( providing light ) studio this appears to\nhave been an artist’s studio. the walls and floors are splattered with\npaints of 69 different colors. strangely enough, nothing of value is\nhanging here. at the south end of the room is an open door ( also\ncovered with paint ). a dark and narrow chimney leads up from a\nfireplace ; although you might be able to get up it, it seems unlikely\nyou could get back down. loosely attached to a wall is a small piece\nof paper. Actions: [’up’, ’put out lantern’, ’south’, ’put down lantern’,\n’take manual’, ’put down painting’, ’put down all’, ’throw lantern at\nmanual’] Qvalues: [8.6, 7.77, 6.81, 6.69, 6.59, 6.38, 6.37, 4.9] Action:\nput out lantern Reward: 0, Score 49, Done False\nSTATE 31\nthe brass lantern is now off. it is now pitch black. you are carrying\n: a painting a brass lantern it is pitch black. you are likely to be\neaten by a grue. Actions: [’up’, ’turn on lantern’, ’south’, ’put down\nlantern’, ’put down all’, ’put down painting’, ’throw lantern at grue’,\n’north’] Qvalues: [8.42, 5.97, 5.91, 4.04, 3.45, 3.42, 2.89, 1.35] Action:\nup Reward: 0, Score 49, Done False\nSTATE 32\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water you are carrying : a painting a brass lantern\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water Actions: [’west’, ’open bottle’, ’close window’,\n’open sack’, ’east’, ’put lantern on table’, ’take sack’, ’put down\nlantern’, ’throw lantern at bottle’, ’turn on lantern’, ’put down all’,\n’put down painting’, ’take bottle’, ’take all’, ’put painting on table’,\n’up’, ’jump’] Qvalues: [10.1, 9.01, 8.93, 8.91, 8.41, 8.23, 8.14, 8.08,\n7.97, 7.87, 7.63, 7.56, 7.29, 6.6, 6.56, 5.92, -2.09] Action: open bottle\nReward: 0, Score 49, Done False\nSTATE 33\nopened. you are carrying : a painting a brass lantern kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains :\na quantity of water Actions: [’west’, ’open sack’, ’close window’,\n’close bottle’, ’east’, ’put lantern on table’, ’take sack’, ’throw lantern\nat bottle’, ’put down lantern’, ’turn on lantern’, ’put down all’, ’put\ndown painting’, ’take bottle’, ’put painting on table’, ’take all’, ’up’,\n’jump’] Qvalues: [10.07, 9.15, 9.04, 8.95, 8.73, 8.54, 8.45, 8.35, 8.34,\n8.17, 7.95, 7.73, 7.63, 6.95, 6.95, 5.69, -2.51] Action: put lantern on\ntable Reward: 0, Score 49, Done False\nSTATE 34\ndone. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is\n: a brass lantern a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’take lantern’, ’open sack’, ’close bottle’,\n’east’, ’take sack’, ’close window’, ’throw painting at lantern’, ’turn\non lantern’, ’take bottle’, ’put down painting’, ’take all’, ’up’, ’jump’]\nQvalues: [9.0, 8.75, 8.31, 8.18, 8.11, 8.08, 8.02, 7.95, 7.84, 7.16, 7.12,\n6.95, 4.29, -4.68] Action: close bottle Reward: 0, Score 49, Done False\nSTATE 35\nclosed. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is\n: a brass lantern a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’take lantern’, ’open bottle’, ’open sack’,\n’close window’, ’take sack’, ’east’, ’throw painting at lantern’, ’turn\non lantern’, ’take bottle’, ’put down painting’, ’take all’, ’up’, ’jump’]\nQvalues: [9.01, 8.76, 8.58, 8.53, 8.18, 8.18, 8.17, 7.97, 7.94, 7.29, 7.17,\n7.02, 4.29, -4.77] Action: take lantern Reward: 0, Score 49, Done False\nSTATE 36\ntaken. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’west’, ’open bottle’, ’open sack’, ’close\nwindow’, ’east’, ’put lantern on table’, ’take sack’, ’throw lantern\nat bottle’, ’put down lantern’, ’turn on lantern’, ’put down all’, ’put\ndown painting’, ’take bottle’, ’take all’, ’put painting on table’, ’up’,\n’jump’] Qvalues: [9.82, 9.15, 9.08, 9.02, 8.61, 8.37, 8.31, 8.15, 8.15, 8.07,\n7.82, 7.61, 7.53, 6.82, 6.8, 6.16, -1.96] Action: open bottle Reward: 0,\nScore 49, Done False\nSTATE 37\nopened. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains :\na quantity of water Actions: [’west’, ’open sack’, ’close window’,\n’close bottle’, ’east’, ’put lantern on table’, ’take sack’, ’put down\nlantern’, ’throw lantern at bottle’, ’turn on lantern’, ’put down all’,\n’take bottle’, ’put down painting’, ’take all’, ’put painting on table’,\n’up’, ’jump’] Qvalues: [9.93, 9.02, 8.99, 8.76, 8.6, 8.48, 8.27, 8.14, 8.12,\n7.97, 7.84, 7.38, 7.38, 6.84, 6.52, 5.84, -2.44] Action: close window\nReward: 0, Score 49, Done False\nSTATE 38\nthe window closes ( more easily than it opened ). you are carrying :\na brass lantern a painting kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is slightly ajar. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is :\na glass bottle the glass bottle contains : a quantity of water Actions:\n[’open window’, ’west’, ’open sack’, ’close bottle’, ’put lantern on\ntable’, ’take sack’, ’put down lantern’, ’throw lantern at bottle’, ’turn\non lantern’, ’put down all’, ’put down painting’, ’take bottle’, ’take\nall’, ’put painting on table’, ’up’, ’jump’] Qvalues: [9.55, 9.15, 8.8,\n8.59, 8.13, 8.0, 7.89, 7.86, 7.71, 7.48, 7.33, 7.22, 6.5, 6.43, 6.4, -1.57]\nAction: put down all Reward: 0, Score 49, Done False\nSTATE 39\nbrass lantern : dropped. painting : dropped. you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis slightly ajar. a painting by a neglected genius is here. there is a\nbrass lantern ( battery powered ) here. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is :\na glass bottle the glass bottle contains : a quantity of water Actions:\n[’open window’, ’open sack’, ’close bottle’, ’turn on lantern’, ’west’,\n’take painting’, ’take all’, ’take lantern’, ’take sack’, ’take bottle’,\n’take all from table’, ’up’, ’jump’] Qvalues: [6.81, 6.37, 6.11, 6.08,\n5.92, 5.9, 5.89, 5.83, 5.7, 5.25, 5.0, 2.44, -6.37] Action: open window\nReward: 0, Score 49, Done False\nSTATE 40\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be\nseen leading upward. a dark chimney leads down and to the east\nis a small window which is open. a painting by a neglected genius\nis here. there is a brass lantern ( battery powered ) here. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’open sack’, ’west’, ’close bottle’,\n’close window’, ’turn on lantern’, ’take sack’, ’take lantern’, ’take\nall’, ’take painting’, ’take bottle’, ’take all from table’, ’up’, ’jump’]\nQvalues: [6.76, 6.36, 6.29, 6.08, 6.06, 5.9, 5.59, 5.56, 5.48, 5.17, 5.14,\n4.9, 0.7, -8.47] Action: east Reward: 0, Score 49, Done False\nSTATE 41\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’west’, ’east’, ’south’] Qvalues: [7.41, 6.37,\n6.31, 6.13, 5.86] Action: west Reward: 0, Score 49, Done False\nSTATE 42\nkitchen a painting by a neglected genius is here. there is a brass\nlantern ( battery powered ) here. on the table is an elongated brown\nsack, smelling of hot peppers. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. a painting by a neglected genius is\nhere. there is a brass lantern ( battery powered ) here. on the table\nis an elongated brown sack, smelling of hot peppers. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’east’, ’open sack’, ’close window’, ’close\nbottle’, ’take lantern’, ’turn on lantern’, ’take sack’, ’take painting’,\n’take bottle’, ’take all’, ’take all from table’, ’up’, ’jump’] Qvalues:\n[9.43, 8.18, 7.32, 7.28, 6.96, 6.84, 6.75, 6.41, 5.8, 5.8, 5.79, 5.22, 1.47,\n-7.43] Action: take painting Reward: 0, Score 49, Done False\nSTATE 43\ntaken. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. there is a brass\nlantern ( battery powered ) here. on the table is an elongated brown\nsack, smelling of hot peppers. sitting on the kitchen table is : a glass\nbottle the glass bottle contains : a quantity of water Actions: [’take\nlantern’, ’west’, ’east’, ’open sack’, ’close bottle’, ’take sack’, ’close\nwindow’, ’turn on lantern’, ’throw painting at lantern’, ’take bottle’,\n’take all’, ’put down painting’, ’take all from table’, ’put painting\non table’, ’up’, ’jump’] Qvalues: [8.45, 8.2, 8.0, 7.88, 7.75, 7.68, 7.63,\n7.61, 7.42, 6.97, 6.81, 6.75, 6.24, 6.19, 5.3, -3.67] Action: take lantern\nReward: 0, Score 49, Done False\nSTATE 44\ntaken. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. on the table\nis an elongated brown sack, smelling of hot peppers. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’open sack’, ’close window’, ’close bottle’,\n’east’, ’put lantern on table’, ’take sack’, ’turn on lantern’, ’put down\nlantern’, ’throw lantern at bottle’, ’put down all’, ’take bottle’, ’put\ndown painting’, ’take all’, ’put painting on table’, ’up’, ’jump’] Qval-\nues: [9.89, 8.94, 8.93, 8.69, 8.64, 8.29, 8.14, 8.0, 8.0, 7.92, 7.64, 7.39,\n7.27, 6.79, 6.4, 6.21, -2.21] Action: west Reward: 0, Score 49, Done\nFalse\nSTATE 45\nliving room above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a jewel encrusted\negg you are carrying : a brass lantern a painting living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity. your\ncollection of treasures consists of : a jewel encrusted egg Actions:\n[’put painting in case’, ’put all in case’, ’put down painting’, ’take\ntreasures from sword’, ’put down all’, ’open egg with lantern’, ’put\ndown lantern’, ’put lantern in case’, ’take sword’, ’close case’, ’turn\non lantern’, ’throw lantern at case’, ’throw painting at treasures’,\n’open trap’, ’throw lantern at treasures’, ’take treasures’, ’take on\ntreasures’, ’east’] Qvalues: [10.96, 10.79, 10.25, 10.18, 9.93, 9.89, 9.77,\n9.74, 9.61, 9.59, 9.55, 9.5, 9.43, 9.4, 8.77, 8.09, 7.49, 6.26] Action: put\nall in case Reward: 6, Score 55, Done False\nSTATE 46\nbrass lantern : done. painting : done. you are empty handed. living\nroom you are in the living room. there is a doorway to the east,\na wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a closed trap door at\nyour feet. above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a painting a brass\nlantern a jewel encrusted egg Actions: [’east’, ’open trap’, ’turn on\nlantern’, ’take lantern’, ’close case’, ’take on egg’, ’take egg from\nsword’, ’take sword’, ’take painting’, ’take egg’, ’take all from case’]\nQvalues: [4.89, 4.27, 4.15, 3.98, 3.98, 3.72, 3.7, 3.69, 3.19, 3.05, 1.15]\nAction: east Reward: 0, Score 55, Done False\nSTATE 47\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. sitting on the kitchen table is : a glass bottle the glass\nbottle contains : a quantity of water you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. on the table is an elongated brown sack, smelling of hot pep-\npers. sitting on the kitchen table is : a glass bottle the glass bottle\ncontains : a quantity of water Actions: [’west’, ’east’, ’open sack’,\n’close window’, ’close bottle’, ’take sack’, ’take bottle’, ’take all’, ’up’,\n’jump’] Qvalues: [7.6, 7.38, 6.33, 6.29, 6.05, 5.44, 5.09, 4.99, 1.82, -7.47]\nAction: east Reward: 0, Score 55, Done False\nSTATE 48\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’west’, ’south’] Qvalues: [7.72, 6.65,\n6.54, 6.41, 6.24] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 49\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.61, 6.8, 6.53, 6.42] Action: open window\nReward: 0, Score 55, Done False\nSTATE 50\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [9.23, 7.55, 6.84, 6.76, 6.48]\nAction: west Reward: 0, Score 55, Done False\nSTATE 51\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. sitting on the kitchen table is : a glass bottle the glass\nbottle contains : a quantity of water you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. on the table is an elongated brown sack, smelling of hot pep-\npers. sitting on the kitchen table is : a glass bottle the glass bottle\ncontains : a quantity of water Actions: [’east’, ’open sack’, ’close\nbottle’, ’close window’, ’take sack’, ’take bottle’, ’take all’, ’west’,\n’up’, ’jump’] Qvalues: [7.0, 6.25, 5.99, 5.77, 5.28, 5.25, 5.2, 5.18, 3.37,\n-6.41] Action: take sack Reward: 0, Score 55, Done False\nSTATE 52\ntaken. you are carrying : a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’east’, ’put down sack’, ’west’,\n’take bottle’, ’close bottle’, ’open sack’, ’close window’, ’up’, ’jump’]\nQvalues: [5.79, 5.65, 5.52, 5.38, 5.1, 5.03, 5.0, 4.56, 3.88, -4.0] Action:\nput sack on table Reward: 0, Score 55, Done False\nSTATE 53\ndone. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table\nis : a brown sack a glass bottle the glass bottle contains : a quantity\nof water Actions: [’east’, ’open sack’, ’west’, ’close bottle’, ’close\nwindow’, ’take sack’, ’take bottle’, ’take all’, ’up’, ’jump’] Qvalues:\n[6.97, 5.93, 5.76, 5.66, 5.45, 5.06, 5.0, 4.82, 3.54, -5.91] Action: east\nReward: 0, Score 55, Done False\nSTATE 54\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’west’, ’south’] Qvalues: [7.59, 6.53,\n6.4, 6.29, 6.18] Action: south Reward: 0, Score 55, Done False\nSTATE 55\nsouth of house you are empty handed. south of house you are fac-\ning the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’east’, ’south’, ’west’] Qvalues:\n[6.57, 5.66, 4.74] Action: east Reward: 0, Score 55, Done False\nSTATE 56\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [7.37, 6.67,\n6.36, 6.29, 5.95] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 57\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.24, 6.57, 6.44, 6.12] Action: open window\nReward: 0, Score 55, Done False\nSTATE 58\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [8.37, 7.33, 6.58, 6.38, 6.19]\nAction: close window Reward: 0, Score 55, Done False\nSTATE 59\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.14, 6.58, 6.43, 6.16] Action: east Reward:\n0, Score 55, Done False\nSTATE 60\nclearing you are empty handed. clearing you are in a small clear-\ning in a well marked forest path that extends to the east and west.\nActions: [’west’, ’south’, ’north’, ’east’, ’go around forest’] Qvalues:\n[6.88, 5.64, 5.62, 5.59, 5.35] Action: south Reward: 0, Score 55, Done\nFalse\nSTATE 61\nforest you are empty handed. forest this is a dimly lit forest, with\nlarge trees all around. Actions: [’northwest’, ’north’, ’go around\ntrees’, ’west’] Qvalues: [6.36, 6.01, 5.76, 5.31] Action: northwest\nReward: 0, Score 55, Done False\nSTATE 62\nsouth of house you are empty handed. south of house you are fac-\ning the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’east’, ’south’, ’west’] Qvalues:\n[6.54, 5.42, 5.03] Action: east Reward: 0, Score 55, Done False\nSTATE 63\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’open window’, ’north’, ’south’, ’east’] Qvalues: [8.04, 6.16,\n6.05, 5.93] Action: open window Reward: 0, Score 55, Done False\nSTATE 64\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [8.2, 7.53, 7.02, 6.89, 6.46]\nAction: west Reward: 0, Score 55, Done False\nSTATE 65\nkitchen sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. sitting on the kitchen table is : a brown sack a glass bot-\ntle the glass bottle contains : a quantity of water Actions: [’east’,\n’west’, ’open sack’, ’close bottle’, ’close window’, ’take sack’, ’take\nbottle’, ’take all’, ’up’, ’jump’] Qvalues: [7.18, 6.16, 5.93, 5.53, 5.5,\n4.9, 4.69, 4.64, 2.8, -6.25] Action: west Reward: 0, Score 55, Done False\nSTATE 66\nliving room above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a painting a brass\nlantern a jewel encrusted egg you are empty handed. living room\nyou are in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity.\nyour collection of treasures consists of : a painting a brass lantern\na jewel encrusted egg Actions: [’east’, ’open trap’, ’close case’, ’turn\non lantern’, ’take egg from sword’, ’take sword’, ’take on egg’, ’take\nlantern’, ’take egg’, ’take painting’, ’take all from case’] Qvalues:\n[5.2, 4.77, 4.54, 4.41, 4.18, 4.15, 4.09, 4.06, 3.41, 3.14, 1.54] Action:\neast Reward: 0, Score 55, Done False\nSTATE 67\nkitchen sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’east’, ’open\nsack’, ’close bottle’, ’west’, ’close window’, ’take all’, ’take sack’,\n’take bottle’, ’up’, ’jump’] Qvalues: [6.58, 5.57, 5.2, 5.13, 5.03, 4.57,\n4.56, 4.5, 3.0, -6.25] Action: open sack Reward: 0, Score 55, Done\nFalse\nSTATE 68\nopening the brown sack reveals a lunch, and a clove of garlic. you\nare empty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of food.\na passage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. sitting on the kitchen table is : a brown sack\nthe brown sack contains : a lunch a clove of garlic a glass bottle the\nglass bottle contains : a quantity of water Actions: [’east’, ’west’,\n’close sack’, ’close bottle’, ’close window’, ’take all from sack’, ’take\nsack’, ’take garlic’, ’take food’, ’take all’, ’take bottle’, ’up’, ’jump’]\nQvalues: [6.66, 5.33, 5.29, 5.0, 4.84, 4.49, 4.48, 4.46, 4.45, 4.26, 4.17,\n2.18, -6.95] Action: close bottle Reward: 0, Score 55, Done False\nSTATE 69\nclosed. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table is\n: a brown sack the brown sack contains : a lunch a clove of garlic a\nglass bottle the glass bottle contains : a quantity of water Actions:\n[’east’, ’open bottle’, ’close sack’, ’west’, ’take all from sack’, ’take\nsack’, ’close window’, ’take food’, ’take garlic’, ’take bottle’, ’take\nall’, ’up’, ’jump’] Qvalues: [6.26, 5.2, 5.13, 4.97, 4.6, 4.55, 4.52, 4.43,\n4.39, 4.33, 4.22, 2.04, -7.58] Action: take all from sack Reward: 0,\nScore 55, Done False\nSTATE 70\nlunch : taken. clove of garlic : taken. you are carrying : a clove of\ngarlic a lunch kitchen you are in the kitchen of the white house. a\ntable seems to have been used recently for the preparation of food.\na passage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. sitting on the kitchen table is : a brown sack a\nglass bottle the glass bottle contains : a quantity of water Actions:\n[’put all in sack’, ’put all on table’, ’put down all’, ’put food on table’,\n’put garlic on table’, ’put down food’, ’west’, ’east’, ’put food in sack’,\n’take sack’, ’open bottle’, ’put garlic in sack’, ’put down garlic’, ’close\nsack’, ’take bottle’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues:\n[5.74, 5.73, 5.59, 5.43, 5.38, 5.34, 5.34, 5.3, 5.27, 5.26, 5.25, 5.18, 5.16,\n4.96, 4.91, 4.61, 4.48, 4.17, -2.97] Action: put down garlic Reward: 0,\nScore 55, Done False\nSTATE 71\ndropped. you are carrying : a lunch kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to\nthe east is a small window which is open. there is a clove of garlic\nhere. sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’put food\non table’, ’put down food’, ’east’, ’west’, ’take garlic’, ’put food in\nsack’, ’take sack’, ’open bottle’, ’close sack’, ’take bottle’, ’take all’,\n’close window’, ’take all from table’, ’up’, ’jump’] Qvalues: [5.24,\n5.24, 5.21, 5.16, 5.11, 4.98, 4.98, 4.86, 4.77, 4.57, 4.45, 4.25, 4.16, 2.27,\n-6.13] Action: put food in sack Reward: 0, Score 55, Done False\nSTATE 72\ndone. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a clove of garlic here.\nsitting on the kitchen table is : a brown sack the brown sack con-\ntains : a lunch a glass bottle the glass bottle contains : a quantity of\nwater Actions: [’east’, ’close sack’, ’open bottle’, ’west’, ’take food’,\n’take garlic’, ’take sack’, ’take all’, ’close window’, ’take bottle’, ’take\nall from table’, ’up’, ’jump’] Qvalues: [6.19, 5.16, 5.15, 4.86, 4.61, 4.54,\n4.51, 4.48, 4.48, 4.37, 3.97, 2.19, -7.34] Action: take sack Reward: 0,\nScore 55, Done False\nSTATE 73\ntaken. you are carrying : a brown sack the brown sack contains :\na lunch kitchen you are in the kitchen of the white house. a table\nseems to have been used recently for the preparation of food. a\npassage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’put food on table’, ’put down\nsack’, ’put down food’, ’west’, ’take food’, ’open bottle’, ’shake sack’,\n’take garlic’, ’east’, ’close sack’, ’take bottle’, ’take all’, ’close win-\ndow’, ’up’, ’jump’] Qvalues: [5.56, 5.37, 5.37, 5.31, 5.31, 5.22, 5.13,\n5.13, 5.1, 5.01, 4.91, 4.82, 4.57, 4.44, 3.26, -4.31] Action: shake sack\nReward: 0, Score 55, Done False\nSTATE 74\nthe contents of the brown sack spills to the ground. you are carrying\n: a brown sack kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. a hot pepper sandwich is here. there\nis a clove of garlic here. sitting on the kitchen table is : a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’put sack\non table’, ’west’, ’put down sack’, ’take food’, ’east’, ’take garlic’,\n’open bottle’, ’take bottle’, ’close sack’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.61, 5.48, 5.42, 5.32, 5.28, 5.14, 5.1, 5.01, 4.85,\n4.49, 4.37, 3.37, -4.37] Action: close sack Reward: 0, Score 55, Done\nFalse\nSTATE 75\nclosed. you are carrying : a brown sack kitchen you are in the\nkitchen of the white house. a table seems to have been used re-\ncently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’west’, ’put down sack’, ’take\nfood’, ’east’, ’take garlic’, ’open bottle’, ’open sack’, ’take bottle’,\n’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.67, 5.52, 5.47, 5.4,\n5.32, 5.26, 5.2, 5.01, 4.97, 4.7, 4.48, 2.85, -4.86] Action: put down sack\nReward: 0, Score 55, Done False\nSTATE 76\ndropped. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a brown sack here. a\nhot pepper sandwich is here. there is a clove of garlic here. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’west’, ’open sack’, ’open bottle’,\n’take sack’, ’take food’, ’take garlic’, ’take all’, ’close window’, ’take\nbottle’, ’up’, ’jump’] Qvalues: [6.33, 5.31, 5.27, 5.17, 4.77, 4.69, 4.63,\n4.58, 4.57, 4.47, 1.86, -7.42] Action: east Reward: 0, Score 55, Done\nFalse\nSTATE 77\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [6.43, 5.68,\n5.43, 5.25, 5.15] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 78\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [7.54, 6.26, 5.95, 5.91] Action: north Reward:\n0, Score 55, Done False\nSTATE 79\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. you are empty handed. north\nof house you are facing the north side of a white house. there is\nno door here, and all the windows are boarded up. to the north\na narrow path winds through the trees. Actions: [’east’, ’north’,\n’west’] Qvalues: [7.13, 6.52, 5.61] Action: east Reward: 0, Score 55,\nDone False\nSTATE 80\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’open window’, ’north’, ’south’, ’east’] Qvalues: [6.88, 5.47,\n5.28, 5.27] Action: open window Reward: 0, Score 55, Done False\nSTATE 81\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [7.23, 6.18, 5.72, 5.56, 5.36]\nAction: west Reward: 0, Score 55, Done False\nSTATE 82\nkitchen there is a brown sack here. a hot pepper sandwich is here.\nthere is a clove of garlic here. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a brown sack here. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’east’, ’open sack’, ’open bottle’, ’close\nwindow’, ’take garlic’, ’take food’, ’take sack’, ’take bottle’, ’take all’,\n’up’, ’jump’] Qvalues: [8.05, 7.66, 6.21, 5.98, 5.91, 5.52, 5.44, 5.4, 4.96,\n4.93, 2.11, -6.64] Action: east Reward: 0, Score 55, Done False\nSTATE 83\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [6.52, 6.0,\n5.7, 5.69, 5.22] Action: west Reward: 0, Score 55, Done False\nSTATE 84\nkitchen there is a brown sack here. a hot pepper sandwich is here.\nthere is a clove of garlic here. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a brown sack here. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’east’, ’west’, ’open sack’, ’open bottle’, ’close\nwindow’, ’take garlic’, ’take food’, ’take sack’, ’take bottle’, ’take\nall’, ’up’, ’jump’] Qvalues: [7.35, 7.25, 6.16, 6.01, 5.81, 5.52, 5.44, 5.41,\n5.16, 5.06, 2.48, -6.1] Action: take food Reward: 0, Score 55, Done\nFalse\nSTATE 85\ntaken. you are carrying : a lunch kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a brown sack here.\nthere is a clove of garlic here. sitting on the kitchen table is : a glass\nbottle the glass bottle contains : a quantity of water Actions: [’put\ndown food’, ’put food on table’, ’east’, ’take garlic’, ’take sack’, ’west’,\n’open bottle’, ’open sack’, ’take bottle’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.65, 5.57, 5.53, 5.48, 5.47, 5.43, 5.25, 5.19, 5.13,\n4.96, 4.56, 3.21, -4.72] Action: take garlic Reward: 0, Score 55, Done\nFalse\nSTATE 86\ntaken. you are carrying : a clove of garlic a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. there is a\nbrown sack here. sitting on the kitchen table is : a glass bottle the\nglass bottle contains : a quantity of water Actions: [’put all on table’,\n’put down all’, ’west’, ’put food on table’, ’take sack’, ’put down\nfood’, ’put garlic on table’, ’east’, ’open bottle’, ’put down garlic’,\n’take bottle’, ’open sack’, ’take all’, ’close window’, ’up’, ’jump’]\nQvalues: [5.93, 5.82, 5.62, 5.59, 5.57, 5.56, 5.51, 5.47, 5.45, 5.42, 5.32,\n5.28, 4.87, 4.68, 3.83, -3.17] Action: put all on table Reward: 0, Score\n55, Done False\nSTATE 87\nclove of garlic : done. lunch : done. you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. there is a brown sack here. sitting on the kitchen table is :\na lunch a clove of garlic a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’west’, ’open sack’, ’open bottle’,\n’close window’, ’take garlic’, ’take food’, ’take sack’, ’take all’, ’take\nbottle’, ’take all from table’, ’up’, ’jump’] Qvalues: [6.17, 5.34, 5.27,\n5.11, 4.66, 4.62, 4.55, 4.55, 4.39, 4.28, 3.72, 2.16, -6.62] Action: take\nsack Reward: 0, Score 55, Done False\nSTATE 88\ntaken. you are carrying : a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table\nis : a lunch a clove of garlic a glass bottle the glass bottle contains :\na quantity of water Actions: [’put sack on table’, ’put down sack’,\n’west’, ’take food’, ’east’, ’take garlic’, ’open bottle’, ’take bottle’,\n’open sack’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.72,\n5.61, 5.46, 5.46, 5.37, 5.31, 5.26, 5.25, 5.13, 4.8, 4.53, 3.63, -3.52] Action:\ntake food Reward: 0, Score 55, Done False\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table is\n: a clove of garlic a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put all on table’, ’put down all’, ’put sack on\ntable’, ’put down sack’, ’west’, ’put down food’, ’put food on table’,\n’take garlic’, ’east’, ’open bottle’, ’open sack’, ’take bottle’, ’take all’,\n’close window’, ’up’, ’jump’] Qvalues: [5.32, 5.29, 5.25, 5.12, 5.09,\n5.06, 5.05, 4.91, 4.9, 4.79, 4.66, 4.56, 4.33, 4.07, 3.08, -4.21] Action:\ntake bottle Reward: 0, Score 55, Done False\nSTATE 90\ntaken. you are carrying : a glass bottle the glass bottle contains :\na quantity of water a lunch a brown sack kitchen you are in the\nkitchen of the white house. a table seems to have been used recently\nfor the preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. sitting on the\nkitchen table is : a clove of garlic Actions: [’put all on table’, ’put\ndown bottle’, ’put down all’, ’put bottle on table’, ’east’, ’put sack\non table’, ’west’, ’put down sack’, ’put down food’, ’put food on\ntable’, ’open bottle’, ’put water on table’, ’open sack’, ’take garlic’,\n’up’, ’close window’, ’jump’] Qvalues: [5.07, 5.04, 5.02, 5.0, 4.88, 4.87,\n4.82, 4.74, 4.7, 4.62, 4.59, 4.55, 4.35, 4.34, 3.94, 3.75, -2.22] Action: put\ndown bottle Reward: 0, Score 55, Done False\nSTATE 91\ndropped. you are carrying : a lunch a brown sack kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a glass bottle here. the glass bottle contains : a quantity of water\nsitting on the kitchen table is : a clove of garlic Actions: [’put all\non table’, ’put sack on table’, ’put down sack’, ’put down all’, ’west’,\n’put down food’, ’put food on table’, ’east’, ’take garlic’, ’open bottle’,\n’open sack’, ’take bottle’, ’take all’, ’close window’, ’up’, ’jump’]\nQvalues: [5.43, 5.42, 5.32, 5.3, 5.3, 5.25, 5.17, 5.02, 4.94, 4.83, 4.66,\n4.62, 4.28, 4.06, 3.09, -4.1] Action: put down sack Reward: 0, Score\n55, Done False\nSTATE 92\ndropped. you are carrying : a lunch kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. there is a brown\nsack here. there is a glass bottle here. the glass bottle contains : a\nquantity of water sitting on the kitchen table is : a clove of garlic\nActions: [’east’, ’put down food’, ’west’, ’put food on table’, ’take\nsack’, ’take garlic’, ’open bottle’, ’open sack’, ’take bottle’, ’take all’,\n’close window’, ’up’, ’jump’] Qvalues: [5.35, 5.25, 5.23, 5.12, 4.97,\n4.97, 4.73, 4.73, 4.72, 4.69, 4.11, 3.21, -4.36] Action: east Reward: 0,\nScore 55, Done False\nSTATE 93\nbehind house you are carrying : a lunch behind house you are be-\nhind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is open.\nActions: [’put down lunch’, ’north’, ’close window’, ’south’, ’east’,\n’west’] Qvalues: [5.7, 4.52, 4.49, 4.3, 4.28, 4.03] Action: close window\nReward: 0, Score 55, Done False\nSTATE 94\nthe window closes ( more easily than it opened ). you are carrying :\na lunch behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’put down lunch’, ’open\nwindow’, ’south’, ’east’, ’north’] Qvalues: [5.45, 4.72, 4.17, 4.1, 3.96]\nAction: south Reward: 0, Score 55, Done False\nSTATE 95\nsouth of house you are carrying : a lunch south of house you are\nfacing the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’put down lunch’, ’east’, ’south’,\n’west’] Qvalues: [4.75, 4.42, 4.13, 3.16] Action: east Reward: 0, Score\n55, Done False\nSTATE 96\nbehind house you are carrying : a lunch behind house you are be-\nhind the white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’put down lunch’, ’open window’, ’south’, ’north’, ’east’]\nQvalues: [5.48, 4.63, 4.06, 4.03, 3.82] Action: open window Reward:\n0, Score 55, Done False\nSTATE 97\nwith great effort, you open the window far enough to allow entry.\nyou are carrying : a lunch behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’put down\nlunch’, ’west’, ’south’, ’north’, ’east’, ’close window’] Qvalues: [5.21,\n4.88, 4.3, 4.26, 4.1, 3.92] Action: west Reward: 0, Score 55, Done False\nSTATE 98\nkitchen there is a brown sack here. there is a glass bottle here. the\nglass bottle contains : a quantity of water sitting on the kitchen\ntable is : a clove of garlic you are carrying : a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a brown sack here. there is a glass bottle here. the glass bottle\ncontains : a quantity of water sitting on the kitchen table is : a clove\nof garlic Actions: [’put down food’, ’put food on table’, ’west’, ’take\nsack’, ’take garlic’, ’east’, ’open bottle’, ’open sack’, ’take bottle’,\n’take all’, ’close window’, ’up’, ’jump’] Qvalues: [4.8, 4.68, 4.6, 4.58,\n4.46, 4.44, 4.44, 4.38, 4.36, 4.0, 3.7, 3.02, -4.39] Action: take garlic\nReward: 0, Score 55, Done False\nSTATE 99\ntaken. you are carrying : a clove of garlic a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a brown sack here. there is a glass bottle here. the glass bottle\ncontains : a quantity of water Actions: [’put down all’, ’put all on\ntable’, ’put down food’, ’west’, ’put food on table’, ’put garlic on\ntable’, ’take sack’, ’put down garlic’, ’open bottle’, ’east’, ’open sack’,\n’take bottle’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.33,\n5.29, 5.05, 5.04, 4.98, 4.92, 4.91, 4.81, 4.79, 4.65, 4.61, 4.54, 4.26, 3.94,\n3.29, -3.82] Action: put down food Reward: 0, Score 55, Done True\nSTATE 100\ndropped. you are carrying : a clove of garlic kitchen you are in\nthe kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. a hot pepper\nsandwich is here. there is a brown sack here. there is a glass bottle\nhere. the glass bottle contains : a quantity of water Actions: [’west’,\n’put garlic on table’, ’take sack’, ’open bottle’, ’take food’, ’put down\ngarlic’, ’open sack’, ’take bottle’, ’east’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.1, 4.79, 4.73, 4.68, 4.67, 4.65, 4.57, 4.32, 4.3,\n3.95, 3.91, 2.32, -5.11] Action: reset Reward: 0, Score 0, Done False\nB.1.1\nDRRN (for comparison). STATE 0\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\nZORK is a registered trademark of Infocom, Inc. Revision 88 \/ Se-\nrial number 840726 West of House You are standing in an open\nfield west of a white house, with a boarded front door. There is a\nsmall mailbox here. You are empty handed. West of House You are\nstanding in an open field west of a white house, with a boarded\nfront door. There is a small mailbox here. Actions: [’north’, ’south’,\n’west’, ’open mailbox’] Qvalues: [9.25, 8.75, 8.4, 8.11] Action: north\nReward: 0, Score 0, Done False\nSTATE 1\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. you are empty handed. north\nof house you are facing the north side of a white house. there is\nno door here, and all the windows are boarded up. to the north\na narrow path winds through the trees. Actions: [’north’, ’east’,\n’west’] Qvalues: [10.23, 10.22, 8.8] Action: north Reward: 0, Score 0,\nDone False\nSTATE 2\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you are empty handed.\nforest path this is a path winding through a dimly lit forest. the path\nheads north south here. one particularly large tree with some low\nbranches stands at the edge of the path. Actions: [’up’, ’go around\nforest’, ’south’, ’east’, ’north’, ’west’] Qvalues: [11.3, 9.84, 9.53, 8.64,\n7.8, 5.7] Action: up Reward: 0, Score 0, Done False\nSTATE 3\nup a tree you are about 10 feet above the ground nestled among\nsome large branches. the nearest branch above you is above your\nreach. beside you on the branch is a small bird’s nest. in the bird’s\nnest is a large egg encrusted with precious jewels, apparently scav-\nenged by a childless songbird. the egg is covered with fine gold\ninlay, and ornamented in lapis lazuli and mother of pearl. unlike\nmost eggs, this one is hinged and closed with a delicate looking\nclasp. the egg appears extremely fragile. you are empty handed. up\na tree you are about 10 feet above the ground nestled among some\nlarge branches. the nearest branch above you is above your reach.\nbeside you on the branch is a small bird’s nest. in the bird’s nest is\na large egg encrusted with precious jewels, apparently scavenged\nby a childless songbird. the egg is covered with fine gold inlay, and\nornamented in lapis lazuli and mother of pearl. unlike most eggs,\nthis one is hinged and closed with a delicate looking clasp. the egg\nappears extremely fragile. Actions: [’take egg’, ’take on egg’, ’take\nnest’, ’close nest’, ’down’] Qvalues: [12.46, 10.87, 10.22, 9.65, 5.91]\nAction: take egg Reward: 5, Score 5, Done False\nSTATE 4\ntaken. you are carrying: a jewel encrusted egg you hear in the dis-\ntance the chirping of a song bird. up a tree you are about 10 feet\nabove the ground nestled among some large branches. the nearest\nbranch above you is above your reach. beside you on the branch is\na small bird’s nest. you hear in the distance the chirping of a song\nbird. Actions: [’take on egg’, ’put egg in nest’, ’down’, ’take nest’,\n’put down egg’, ’close nest’, ’throw egg at nest’] Qvalues: [8.24, 8.11,\n7.77, 7.37, 7.34, 7.27, 6.01] Action: down Reward: 0, Score 5, Done\nFalse\nSTATE 5\nforest path you are carrying: a jewel encrusted egg forest path this\nis a path winding through a dimly lit forest. the path heads north\nsouth here. one particularly large tree with some low branches\nstands at the edge of the path. you hear in the distance the chirping\nof a song bird. Actions: [’south’, ’take on egg’, ’up’, ’open egg with\nall’, ’go around forest’, ’put down egg’, ’east’, ’north’, ’throw egg\nat tree’, ’west’] Qvalues: [9.33, 9.13, 9.02, 8.27, 8.21, 7.74, 7.41, 6.32,\n4.05, 3.81] Action: south Reward: 0, Score 5, Done False\nSTATE 6\nnorth of house you are carrying: a jewel encrusted egg north of\nhouse you are facing the north side of a white house. there is no\ndoor here, and all the windows are boarded up. to the north a nar-\nrow path winds through the trees. Actions: [’east’, ’take on egg’,\n’put down egg’, ’north’, ’open egg with all’, ’west’, ’throw egg at\npath’] Qvalues: [10.13, 8.82, 7.17, 6.75, 6.43, 4.81, 3.01] Action: east\nReward: 0, Score 5, Done False\nSTATE 7\nbehind house you are behind the white house. a path leads into\nthe forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. you are carrying: a jewel encrusted\negg behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a\nsmall window which is slightly ajar. Actions: [’take on egg’, ’open\nwindow’, ’put down egg’, ’open egg with all’, ’north’, ’east’, ’south’,\n’throw egg at path’] Qvalues: [11.34, 10.97, 10.23, 10.2, 8.65, 8.62,\n8.47, 5.23] Action: put down egg Reward: 0, Score 5, Done False\nSTATE 8\ndropped. you are empty handed. behind house you are behind the\nwhite house. a path leads into the forest to the east. in one corner\nof the house there is a small window which is slightly ajar. there\nis a jewel encrusted egg here. Actions: [’take egg’, ’take on egg’,\n’open window’, ’north’, ’east’, ’south’] Qvalues: [11.04, 10.84, 9.91,\n9.62, 9.43, 9.09] Action: north Reward: 0, Score 5, Done False\nSTATE 9\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.09, 8.65, 6.82]\nAction: east Reward: 0, Score 5, Done False\nSTATE 10\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’take on egg’, ’north’, ’open window’, ’south’,\n’east’] Qvalues: [10.77, 10.26, 10.26, 10.17, 10.01, 9.07] Action: north\nReward: 0, Score 5, Done False\nSTATE 11\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.04, 8.6, 6.73]\nAction: north Reward: 0, Score 5, Done False\nSTATE 12\nforest path you are empty handed. forest path this is a path winding\nthrough a dimly lit forest. the path heads north south here. one\nparticularly large tree with some low branches stands at the edge\nof the path. Actions: [’up’, ’south’, ’go around forest’, ’east’, ’north’,\n’west’] Qvalues: [9.02, 8.89, 8.28, 7.72, 7.19, 4.43] Action: south Re-\nward: 0, Score 5, Done False\nSTATE 13\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.03, 8.58, 6.68]\nAction: east Reward: 0, Score 5, Done False\nSTATE 14\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’north’, ’take on egg’, ’open window’, ’south’,\n’east’] Qvalues: [10.64, 10.2, 10.18, 10.07, 9.98, 9.04] Action: north\nReward: 0, Score 5, Done False\nSTATE 15\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [9.99, 8.53, 6.58]\nAction: east Reward: 0, Score 5, Done False\nSTATE 16\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’north’, ’take on egg’, ’open window’, ’south’,\n’east’] Qvalues: [10.54, 10.14, 10.09, 9.99, 9.93, 8.99] Action: open\nwindow Reward: 0, Score 5, Done False\nSTATE 17\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of\nthe house there is a small window which is open. there is a jewel\nencrusted egg here. Actions: [’take egg’, ’take on egg’, ’west’, ’close\nwindow’, ’east’, ’south’, ’north’] Qvalues: [11.46, 9.71, 9.65, 8.11,\n6.49, 6.19, 5.6] Action: take egg Reward: 0, Score 5, Done False\nSTATE 18\ntaken. you are carrying: a jewel encrusted egg behind house you\nare behind the white house. a path leads into the forest to the east.\nin one corner of the house there is a small window which is open.\nActions: [’take on egg’, ’west’, ’north’, ’open egg with all’, ’put down\negg’, ’close window’, ’south’, ’east’, ’throw egg at path’] Qvalues:\n[10.51, 10.42, 10.08, 9.93, 9.81, 9.6, 9.4, 9.12, 6.64] Action: north Re-\nward: 0, Score 5, Done False\nSTATE 19\nnorth of house you are carrying: a jewel encrusted egg north of\nhouse you are facing the north side of a white house. there is no\ndoor here, and all the windows are boarded up. to the north a nar-\nrow path winds through the trees. Actions: [’east’, ’take on egg’,\n’put down egg’, ’north’, ’open egg with all’, ’west’, ’throw egg at\npath’] Qvalues: [9.59, 8.34, 6.73, 6.21, 5.94, 4.12, 2.61] Action: take\non egg Reward: 0, Score 5, Done False\nSTATE 20\nthere is a noticeable crunch from beneath you, and inspection re-\nveals that the egg is lying open, badly damaged. there is a golden\nclockwork canary nestled in the egg. it seems to have recently\nhad a bad experience. the mountings for its jewel like eyes are\nempty, and its silver beak is crumpled. through a cracked crystal\nwindow below its left wing you can see the remains of intricate\nmachinery. it is not clear what result winding it would have, as\nthe mainspring seems sprung. you are carrying: a broken jewel\nencrusted egg the broken jewel encrusted egg contains: a broken\nclockwork canary north of house you are facing the north side\nof a white house. there is no door here, and all the windows are\nboarded up. to the north a narrow path winds through the trees.\nActions: [’take canary’, ’shake egg’, ’close egg’, ’north’, ’east’, ’put\ndown canary’, ’put down egg’, ’west’] Qvalues: [10.64, 10.34, 10.01,\n9.8, 9.8, 9.68, 9.67, 8.01] Action: north Reward: 0, Score 5, Done False\nSTATE 21\nforest path you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nforest path this is a path winding through a dimly lit forest. the path\nheads north south here. one particularly large tree with some low\nbranches stands at the edge of the path. you hear in the distance\nthe chirping of a song bird. Actions: [’south’, ’take canary’, ’up’,\n’shake egg’, ’put down canary’, ’east’, ’close egg’, ’go around forest’,\n’north’, ’put down egg’, ’west’] Qvalues: [11.15, 9.93, 9.35, 9.24, 8.89,\n8.88, 8.79, 8.74, 8.22, 8.19, 5.5] Action: south Reward: 0, Score 5,\nDone False\nSTATE 22\nnorth of house you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. Actions: [’east’, ’shake egg’,\n’take canary’, ’close egg’, ’north’, ’put down canary’, ’put down egg’,\n’west’] Qvalues: [11.93, 10.06, 9.78, 9.49, 8.68, 8.53, 8.35, 6.3] Action:\neast Reward: 0, Score 5, Done False\nSTATE 23\nbehind house you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nbehind house you are behind the white house. a path leads into\nthe forest to the east. in one corner of the house there is a small\nwindow which is open. Actions: [’close window’, ’shake egg’, ’close\negg’, ’take canary’, ’east’, ’south’, ’north’, ’put down egg’, ’put down\ncanary’, ’west’] Qvalues: [11.21, 10.7, 10.61, 10.2, 10.12, 9.76, 9.66,\n9.53, 8.86, 8.7] Action: close egg Reward: 0, Score 5, Done False\nSTATE 24\nclosed. you are carrying: a broken jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nopen. Actions: [’close window’, ’south’, ’north’, ’west’, ’east’, ’open\negg’, ’put down egg’] Qvalues: [12.79, 12.65, 11.54, 11.37, 10.81, 10.31,\n10.01] Action: south Reward: 0, Score 5, Done False\nSTATE 25\nsouth of house you are facing the south side of a white house. there\nis no door here, and all the windows are boarded. you are carrying:\na broken jewel encrusted egg south of house you are facing the\nsouth side of a white house. there is no door here, and all the win-\ndows are boarded. Actions: [’east’, ’south’, ’open egg’, ’put down\negg’, ’west’] Qvalues: [14.39, 10.78, 10.45, 9.31, 8.39] Action: east\nReward: 0, Score 5, Done False\nSTATE 26\nbehind house you are carrying: a broken jewel encrusted egg be-\nhind house you are behind the white house. a path leads into the\nforest to the east. in one corner of the house there is a small window\nwhich is open. Actions: [’close window’, ’south’, ’east’, ’open egg’,\n’north’, ’west’, ’put down egg’] Qvalues: [13.61, 12.05, 11.88, 11.29,\n11.11, 10.28, 10.27] Action: close window Reward: 0, Score 5, Done\nFalse\nSTATE 27\nthe window closes (more easily than it opened). you are carrying: a\nbroken jewel encrusted egg behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is slightly ajar. Actions: [’open\nwindow’, ’north’, ’east’, ’south’, ’open egg’, ’put down egg’] Qval-\nues: [15.18, 12.67, 12.56, 12.56, 12.07, 11.34] Action: open window\nReward: 0, Score 5, Done False\nSTATE 28\nwith great effort, you open the window far enough to allow entry.\nyou are carrying: a broken jewel encrusted egg behind house you\nare behind the white house. a path leads into the forest to the east.\nin one corner of the house there is a small window which is open.\nActions: [’west’, ’close window’, ’open egg’, ’north’, ’put down egg’,\n’east’, ’south’] Qvalues: [18.49, 15.61, 14.4, 13.72, 13.06, 12.87, 12.11]\nAction: west Reward: 10, Score 15, Done False\nSTATE 29\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains: a\nquantity of water you are carrying: a broken jewel encrusted egg\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains:\na quantity of water Actions: [’east’, ’west’, ’close window’, ’take\nbottle’, ’take all’, ’open bottle’, ’open sack’, ’take sack’, ’open egg’,\n’up’, ’put egg on table’, ’put down egg’, ’jump’] Qvalues: [12.6, 12.33,\n12.29, 11.66, 11.3, 11.3, 11.0, 10.48, 10.26, 10.12, 9.82, 9.81, 4.29] Ac-\ntion: put down egg Reward: 0, Score 15, Done False\nSTATE 30\ndropped. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a somewhat ruined\negg here. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains:\na quantity of water Actions: [’take egg’, ’east’, ’open sack’, ’west’,\n’take bottle’, ’take sack’, ’take all from table’, ’close window’, ’take\nall’, ’open bottle’, ’up’, ’open egg’, ’jump’] Qvalues: [9.16, 9.1, 9.02,\n8.95, 8.85, 8.65, 8.54, 8.49, 8.48, 8.42, 8.29, 8.02, 0.73] Action: open\nsack Reward: 0, Score 15, Done False\nSTATE 31\nopening the brown sack reveals a lunch, and a clove of garlic. you\nare empty handed. kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a somewhat ruined egg here.\na bottle is sitting on the table. the glass bottle contains: a quantity\nof water a brown sack the brown sack contains: a lunch a clove of\ngarlic Actions: [’take egg’, ’take all from table’, ’take all’, ’take all\nfrom sack’, ’west’, ’take sack’, ’close sack’, ’take food’, ’take bottle’,\n’take garlic’, ’open egg’, ’open bottle’, ’close window’, ’east’, ’up’,\n’jump’] Qvalues: [9.83, 9.13, 8.97, 8.85, 8.69, 8.59, 8.55, 8.51, 8.43,\n8.31, 8.09, 7.93, 7.75, 7.02, 6.99, -1.14] Action: close sack Reward: 0,\nScore 15, Done False\nSTATE 32\nclosed. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a somewhat ruined\negg here. a bottle is sitting on the table. the glass bottle contains: a\nquantity of water a brown sack Actions: [’take egg’, ’take all’, ’take\nall from table’, ’take sack’, ’open sack’, ’take bottle’, ’close window’,\n’open egg’, ’open bottle’, ’west’, ’east’, ’up’, ’jump’] Qvalues: [11.85,\n11.45, 11.15, 11.06, 10.74, 10.47, 10.35, 10.14, 10.06, 9.89, 9.32, 9.24,\n2.32] Action: take bottle Reward: 0, Score 15, Done False\nSTATE 33\ntaken. you are carrying: a glass bottle the glass bottle contains: a\nquantity of water kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a somewhat ruined egg here.\nsitting on the kitchen table is: a brown sack Actions: [’take egg’,\n’take all’, ’west’, ’put water on table’, ’open sack’, ’take sack’, ’put\nbottle on table’, ’open egg’, ’open bottle’, ’close window’, ’up’, ’put\ndown bottle’, ’east’, ’jump’] Qvalues: [11.16, 10.75, 10.52, 9.92, 9.89,\n9.88, 9.78, 9.66, 9.51, 9.4, 9.36, 9.12, 8.99, 1.01] Action: close window\nReward: 0, Score 15, Done False\nSTATE 34\nthe window closes (more easily than it opened). you are carrying: a\nglass bottle the glass bottle contains: a quantity of water kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nslightly ajar. there is a somewhat ruined egg here. sitting on the\nkitchen table is: a brown sack Actions: [’take egg’, ’west’, ’take\nall’, ’open egg’, ’open sack’, ’take sack’, ’put water on table’, ’put\nbottle on table’, ’open bottle’, ’open window’, ’put down bottle’, ’up’,\n’jump’] Qvalues: [10.72, 9.98, 9.86, 9.09, 9.02, 8.96, 8.92, 8.83, 8.8,\n8.65, 8.29, 7.96, -1.33] Action: put water on table Reward: 0, Score\n15, Done False\nSTATE 35\ndone. you are carrying: a glass bottle kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is slightly ajar. there is a somewhat\nruined egg here. sitting on the kitchen table is: a quantity of water\na brown sack Actions: [’take egg’, ’take all’, ’west’, ’open window’,\n’open sack’, ’take sack’, ’open egg’, ’open bottle’, ’put bottle on table’,\n’put down bottle’, ’up’, ’jump’] Qvalues: [11.52, 10.95, 10.61, 10.33,\n10.11, 10.07, 10.03, 9.92, 9.8, 9.75, 8.45, 1.04] Action: put bottle on\ntable Reward: 0, Score 15, Done False\nSTATE 36\ndone. you are empty handed. kitchen you are in the kitchen of the\nwhite house. a table seems to have been used recently for the prepa-\nration of food. a passage leads to the west and a dark staircase can\nbe seen leading upward. a dark chimney leads down and to the east\nis a small window which is slightly ajar. there is a somewhat ruined\negg here. sitting on the kitchen table is: a glass bottle a quantity of\nwater a brown sack Actions: [’take egg’, ’take all’, ’open window’,\n’take all from table’, ’west’, ’take sack’, ’take bottle’, ’open sack’,\n’open bottle’, ’open egg’, ’up’, ’jump’] Qvalues: [11.36, 10.61, 10.58,\n10.56, 10.46, 10.29, 10.28, 10.13, 9.7, 9.65, 8.73, 1.69] Action: take sack\nReward: 0, Score 15, Done False\nSTATE 37\ntaken. you are carrying: a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is slightly ajar. there is a\nsomewhat ruined egg here. sitting on the kitchen table is: a glass\nbottle a quantity of water Actions: [’west’, ’take egg’, ’take all’, ’put\nsack on table’, ’open window’, ’open sack’, ’take bottle’, ’put down\nsack’, ’open egg’, ’open bottle’, ’up’, ’jump’] Qvalues: [9.59, 9.28,\n8.93, 8.76, 8.47, 8.47, 8.34, 8.32, 7.95, 7.83, 7.77, 0.83] Action: west\nReward: 0, Score 15, Done False\nSTATE 38\nliving room you are in the living room. there is a doorway to the\neast, a wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a large oriental rug\nin the center of the room. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on\nthe trophy case. you are carrying: a brown sack living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a large oriental rug in the center\nof the room. above the trophy case hangs an elvish sword of great\nantiquity. a battery powered brass lantern is on the trophy case.\nActions: [’take all’, ’push rug’, ’open sack’, ’put down sack’, ’open\ncase’, ’take sword’, ’take lantern’, ’turn on lantern’, ’throw sack at\nlantern’, ’east’] Qvalues: [10.66, 10.2, 9.58, 9.11, 9.0, 8.97, 8.77, 8.68,\n8.31, 7.35] Action: push rug Reward: 0, Score 15, Done False\nSTATE 39\nwith a great effort, the rug is moved to one side of the room, reveal-\ning the dusty cover of a closed trap door. you are carrying: a brown\nsack living room you are in the living room. there is a doorway to\nthe east, a wooden door with strange gothic lettering to the west,\nwhich appears to be nailed shut, a trophy case, and a closed trap\ndoor at your feet. above the trophy case hangs an elvish sword of\ngreat antiquity. a battery powered brass lantern is on the trophy\ncase. Actions: [’take all’, ’open trap’, ’open sack’, ’take sword’, ’put\ndown sack’, ’take lantern’, ’open case’, ’turn on lantern’, ’throw sack\nat lantern’, ’east’] Qvalues: [11.53, 11.1, 10.37, 10.0, 9.76, 9.69, 9.66,\n9.61, 9.1, 8.05] Action: open trap Reward: 0, Score 15, Done False\nSTATE 40\nthe door reluctantly opens to reveal a rickety staircase descending\ninto darkness. you are carrying: a brown sack living room you are\nin the living room. there is a doorway to the east, a wooden door\nwith strange gothic lettering to the west, which appears to be nailed\nshut, a trophy case, and a rug lying beside an open trap door. above\nthe trophy case hangs an elvish sword of great antiquity. a battery\npowered brass lantern is on the trophy case. Actions: [’take all’,\n’take lantern’, ’open sack’, ’put down sack’, ’take sword’, ’open case’,\n’throw sack at lantern’, ’turn on lantern’, ’close trap’, ’east’, ’down’]\nQvalues: [12.63, 11.7, 11.21, 10.74, 10.64, 10.54, 10.47, 10.07, 9.7, 9.09,\n7.73] Action: take all Reward: 0, Score 15, Done False\nSTATE 41\ntrophy case: the trophy case is securely fastened to the wall. sword:\ntaken. brass lantern: taken. carpet: the rug is extremely heavy and\ncannot be carried. you are carrying: a brass lantern a sword a brown\nsack living room you are in the living room. there is a doorway\nto the east, a wooden door with strange gothic lettering to the\nwest, which appears to be nailed shut, a trophy case, and a rug\nlying beside an open trap door. Actions: [’open sack’, ’put down\nall’, ’down’, ’open case’, ’put down sack’, ’throw sack at lantern’,\n’close trap’, ’put down lantern’, ’put down sword’, ’turn on lantern’,\n’east’] Qvalues: [14.22, 14.12, 13.88, 13.79, 13.66, 13.2, 13.14, 13.11,\n13.01, 12.88, 12.33] Action: put down all Reward: 0, Score 15, Done\nFalse\nSTATE 42\nbrass lantern: dropped. sword: dropped. brown sack: dropped. you\nare empty handed. living room you are in the living room. there is\na doorway to the east, a wooden door with strange gothic lettering\nto the west, which appears to be nailed shut, a trophy case, and\na rug lying beside an open trap door. there is a brown sack here.\nthere is a sword here. there is a brass lantern (battery powered)\nhere. Actions: [’open sack’, ’take lantern’, ’take sack’, ’take all’, ’take\nsword’, ’turn on lantern’, ’open case’, ’close trap’, ’down’, ’east’]\nQvalues: [11.7, 11.61, 11.57, 11.39, 11.37, 11.37, 10.79, 10.57, 10.13,\n10.1] Action: down Reward: 25, Score 40, Done False\nSTATE 43\nyou have moved into a dark place. the trap door crashes shut, and\nyou hear someone barring it. it is pitch black. you are likely to be\neaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[-7.44, -9.98] Action: south Reward: 0, Score 40, Done False\nSTATE 44\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.4, -3.4, -8.58] Action: east Reward: 0, Score 40, Done False\nSTATE 45\ngallery this is an art gallery. most of the paintings have been stolen\nby vandals with exceptional taste. the vandals left through either\nthe north or west exits. fortunately, there is still one chance for\nyou to be a vandal, for on the far wall is a painting of unparalleled\nbeauty. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. fortunately,\nthere is still one chance for you to be a vandal, for on the far wall\nis a painting of unparalleled beauty. Actions: [’take art’, ’north’,\n’west’] Qvalues: [6.02, 1.29, 0.19] Action: take art Reward: 4, Score\n44, Done False\nSTATE 46\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.41, 2.35, 2.15] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 47\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’west’, ’north’] Qval-\nues: [2.91, 1.66, 1.11] Action: take art Reward: 0, Score 44, Done False\nSTATE 48\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.38, 2.36, 2.16] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 49\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting\nby a neglected genius is here. Actions: [’take art’, ’west’, ’north’]\nQvalues: [2.8, 1.52, 0.96] Action: take art Reward: 0, Score 44, Done\nFalse\nSTATE 50\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [2.34, 2.29, 2.1] Action:\nnorth Reward: 0, Score 44, Done False\nSTATE 51\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are carrying: a painting it is pitch black.\nyou are likely to be eaten by a grue. Actions: [’south’, ’put down\npainting’, ’north’] Qvalues: [1.71, -2.26, -3.91] Action: south Reward:\n0, Score 44, Done False\nSTATE 52\ngallery you are carrying: a painting gallery this is an art gallery.\nmost of the paintings have been stolen by vandals with exceptional\ntaste. the vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.0, 1.94, 1.62] Action: put\ndown art Reward: 0, Score 44, Done False\nSTATE 53\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’west’, ’north’] Qval-\nues: [2.44, 1.08, 0.47] Action: take art Reward: 0, Score 44, Done False\nSTATE 54\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [2.23, 1.96, 1.84] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 55\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting\nby a neglected genius is here. Actions: [’take art’, ’west’, ’north’]\nQvalues: [2.1, 0.73, 0.08] Action: west Reward: 0, Score 44, Done\nFalse\nSTATE 56\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.75, -4.31, -9.21] Action: east Reward: 0, Score 44, Done False\nSTATE 57\ngallery a painting by a neglected genius is here. you are empty\nhanded. gallery this is an art gallery. most of the paintings have\nbeen stolen by vandals with exceptional taste. the vandals left\nthrough either the north or west exits. a painting by a neglected\ngenius is here. Actions: [’take art’, ’west’, ’north’] Qvalues: [1.99,\n0.53, -0.2] Action: take art Reward: 0, Score 44, Done False\nSTATE 58\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [1.93, 1.43, 1.35] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 59\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’wait’, ’west’, ’north’]\nQvalues: [1.48, 0.25, 0.06, -0.58] Action: wait Reward: 0, Score 44,\nDone False\nSTATE 60\ntime passes... you are empty handed. gallery this is an art gallery.\nmost of the paintings have been stolen by vandals with exceptional\ntaste. the vandals left through either the north or west exits. Ac-\ntions: [’north’, ’west’] Qvalues: [1.24, 0.15] Action: west Reward: 0,\nScore 44, Done False\nSTATE 61\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.87, -4.34, -9.29] Action: east Reward: 0, Score 44, Done False\nSTATE 62\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.38, 0.23] Action: west Reward: 0, Score\n44, Done False\nSTATE 63\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.92, -4.3, -9.3] Action: east Reward: 0, Score 44, Done False\nSTATE 64\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.29, 0.16] Action: north Reward: 0, Score\n44, Done False\nSTATE 65\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.59, -4.18] Action: south Reward: 0, Score 44, Done False\nSTATE 66\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.29, 0.17] Action: west Reward: 0, Score\n44, Done False\nSTATE 67\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.21, -4.03, -9.06] Action: east Reward: 0, Score 44, Done False\nSTATE 68\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.38, 0.25] Action: west Reward: 0, Score\n44, Done False\nSTATE 69\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.27, -3.95, -8.96] Action: east Reward: 0, Score 44, Done False\nSTATE 70\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.46, 0.31] Action: north Reward: 0, Score\n44, Done False\nSTATE 71\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.78, -3.93] Action: south Reward: 0, Score 44, Done False\nSTATE 72\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.62, 0.45] Action: west Reward: 0, Score\n44, Done False\nSTATE 73\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.32, -3.86, -8.82] Action: east Reward: 0, Score 44, Done False\nSTATE 74\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’, ’wait’] Qvalues: [0.75, 0.57, 0.39] Action: north Re-\nward: 0, Score 44, Done False\nSTATE 75\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.87, -3.84] Action: south Reward: 0, Score 44, Done False\nSTATE 76\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.93, 0.75] Action: north Reward: 0, Score\n44, Done False\nSTATE 77\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.99, -3.68] Action: south Reward: 0, Score 44, Done False\nSTATE 78\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.14, 0.97] Action: west Reward: 0, Score\n44, Done False\nSTATE 79\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.51, -3.52, -8.51] Action: east Reward: 0, Score 44, Done False\nSTATE 80\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.33, 1.18] Action: west Reward: 0, Score\n44, Done False\nSTATE 81\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.66, -3.32, -8.33] Action: east Reward: 0, Score 44, Done False\nSTATE 82\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.55, 1.4] Action: north Reward: 0, Score\n44, Done False\nSTATE 83\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.41, -3.15] Action: south Reward: 0, Score 44, Done False\nSTATE 84\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.65, 1.51] Action: west Reward: 0, Score\n44, Done False\nSTATE 85\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.81, -3.09, -8.17] Action: east Reward: 0, Score 44, Done False\nSTATE 86\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.66, 1.56] Action: west Reward: 0, Score\n44, Done False\nSTATE 87\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.79, -3.06, -8.2] Action: east Reward: 0, Score 44, Done False\nSTATE 88\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.6, 1.53] Action: north Reward: 0, Score\n44, Done False\nSTATE 89\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.48, -3.07] Action: south Reward: 0, Score 44, Done False\nSTATE 90\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.53, 1.5] Action: north Reward: 0, Score\n44, Done False\nSTATE 91\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.46, -3.08] Action: south Reward: 0, Score 44, Done False\nSTATE 92\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.45, 1.42] Action: west Reward: 0, Score\n44, Done False\nSTATE 93\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.46, -3.16, -8.41] Action: east Reward: 0, Score 44, Done False\nSTATE 94\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.38, 1.33] Action: north Reward: 0, Score\n44, Done False\nSTATE 95\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.27, -3.26] Action: south Reward: 0, Score 44, Done False\nSTATE 96\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.29, 1.22] Action: west Reward: 0, Score\n44, Done False\nSTATE 97\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.18, -3.36, -8.54] Action: east Reward: 0, Score 44, Done False\nSTATE 98\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.25, 1.13] Action: north Reward: 0, Score\n44, Done False\nSTATE 99\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.99, -3.47] Action: south Reward: 0, Score 44, Done True\nSTATE 100\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.21, 1.04] Action: reset Reward13458: 0,\nScore 0, Done False\nB.2\nInhumane\nSTATE 0\nYou awaken in your tent, lying on the floor, with a hangover like\nyou wouldn’t believe. As you lever yourself up, you notice that\nthere is complete silence instead of the usual morning complaints\nof the workers and Roboff’s usual shouting. This can only mean\none thing Roboff must have finally run off on you. Panicked, you\nlook around and see your amulet lying on the floor. At least Roboff\nhad the sense to not trigger off your alarm. Knowing him, he also\nhad the sense to take the emergency radio. How the megakrip\nare you going to get out of this one? INHUMANE : An Infralogic\nMassacre. Copyright ( C ) by Infobom Inhumane and Infobom are\ntrademarks of Sadistic Software, Ins. Revision 2 \/ Serial Number\n31415926 \/ Inform v1405 Library 5 \/ 8 First time players should type\n\" about \". In Your Tent You are in your tent in the encampment.\nThe tent appears to have been completely cleaned out. It is very\nhot in here and getting hotter. The tent flap to the south is open.\nOn the ground is the ancient jeweled amulet that you \" borrowed\n\" from Mr. Meshugge. You are holding nothing. In Your Tent You\nare in your tent in the encampment. The tent appears to have been\ncompletely cleaned out. It is very hot in here and getting hotter.\nThe tent flap to the south is open. On the ground is the ancient\njeweled amulet that you \" borrowed \" from Mr. Meshugge. Actions:\n[’south’, ’take amulet’] Qvalues: [6.04, 4.66] Action: south Reward:\n0, Score 0, Done False\nSTATE 1\ncenter of camp this is the center of your camp. the entire place\nseems to be completely deserted. you can see roboff’s tent to the\nsouth and the storage tent west. east is open desert. you are holding\nnothing. center of camp this is the center of your camp. the entire\nplace seems to be completely deserted. you can see roboff’s tent to\nthe south and the storage tent west. east is open desert. Actions:\n[’east’, ’north’, ’south’, ’west’] Qvalues: [7.01, 5.93, 5.76, 5.3] Action:\nsouth Reward: 0, Score 0, Done False\nSTATE 2\nroboff’s tent you are in francisco roboff’s tent. the place is bare\nexcept for the emergency signal radio and roboff’s toilet in one cor-\nner. there is a note taped to the tent wall. you are holding nothing.\nroboff’s tent you are in francisco roboff’s tent. the place is bare\nexcept for the emergency signal radio and roboff’s toilet in one\ncorner. there is a note taped to the tent wall. Actions: [’north’, ’take\nsignal off radio’] Qvalues: [6.3, 4.72] Action: north Reward: 0, Score\n0, Done False\nSTATE 3\ncenter of campyou are holding nothing. center of camp this is the\ncenter of your camp. the entire place seems to be completely de-\nserted. you can see roboff’s tent to the south and the storage tent\nwest. east is open desert. Actions: [’east’, ’north’, ’south’, ’west’]\nQvalues: [7.15, 5.82, 5.72, 5.61] Action: north Reward: 0, Score 0,\nDone False\nSTATE 4\nin your tent on the ground is the ancient jeweled amulet that you \"\nborrowed \" from mr. meshugge. you are holding nothing. in your\ntent you are in your tent in the encampment. the tent appears to\nhave been completely cleaned out. it is very hot in here and get-\nting hotter. the tent flap to the south is open. on the ground is the\nancient jeweled amulet that you \" borrowed \" from mr. meshugge.\nActions: [’south’, ’take amulet’] Qvalues: [6.36, 5.16] Action: south\nReward: 0, Score 0, Done False\nSTATE 5\ncenter of campyou are holding nothing. center of camp this is the\ncenter of your camp. the entire place seems to be completely de-\nserted. you can see roboff’s tent to the south and the storage tent\nwest. east is open desert. Actions: [’east’, ’north’, ’west’, ’south’]\nQvalues: [7.14, 5.84, 5.66, 5.66] Action: east Reward: 0, Score 0, Done\nFalse\nSTATE 6\nyou venture out onto the sands and rapidly lose sight of camp. in\nthe desert all you can see in any direction is sand and sunlight. you\nare sweating like a pig. you are holding nothing. in the desert all\nyou can see in any direction is sand and sunlight. you are sweating\nlike a pig. Actions: [’put all down’, ’north’, ’west’] Qvalues: [7.71,\n7.29, 6.3] Action: put all down Reward: 0, Score 0, Done False\nSTATE 7\nit falls and is lost forever in the sands. it is very dark. before you\ncan do anything, something begins gnawing on you. you have died\nyou are floating disembodied in absolute blackness. time passes at a\nstrange, relative rate. after an infinite \/ infinitesmal time, you sense\na presence near you. a voice speaks, resounding in your head as\nin a hollow gourd... \" ah! another of those wimpy delvers come to\nsteal my treasure. how boring. most of it was taken by that other\nguy who had the sense to avoid the nastier traps... but i am getting\npretty tired of guarding what’s left. i don’t suppose that you have\nany chance of thinking your way through my traps, of course. but\ni’ll let you try, and i won’t even vaporize you when you try to\nleave. in fact, i’ll make you a deal. if you prove to me that you are\nan incompetent harry sullivan by getting yourself killed by every\nsingle one of my deathtraps, after getting as far towards solving it\nas possible, then i’ll give up and let you take the coffin. \" that sound\ngood? i thought so. \" that hardly counts as being killed by a trap,\nyou know. even you could have gotten farther into that situation. \"\nyou don’t seem to have really gotten fooled by any traps yet. maybe\nthere is hope for you. \" round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nthere is a shovel lying here. you are holding : a torch round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’push button’, ’north’, ’east’, ’west’, ’south’, ’take button off torch’,\n’take shovel’, ’put torch down’, ’examine obelisk’, ’up’] Qvalues:\n[8.67, 8.06, 8.02, 7.76, 7.63, 7.4, 6.97, 6.88, 6.55, 5.09] Action: north\nReward: 0, Score 0, Done False\nSTATE 8\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you don’t\nseem to have really gotten fooled by any traps yet. maybe there is\nhope for you. \" round room this room is at the peak of the pyramid.\nit is perfectly conical. in the center is a tall stone obelisk that you\ncould probably use to get back up to the fake toilet. there are four\nstairways leaving the room at the four compass points. there is a\nshovel lying here. you are holding : a torch round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. Actions: [’push\nbutton’, ’east’, ’north’, ’west’, ’south’, ’take button off torch’, ’take\nshovel’, ’put torch down’, ’examine obelisk’, ’up’] Qvalues: [8.76,\n8.1, 7.99, 7.62, 7.55, 7.38, 6.93, 6.9, 6.49, 5.19] Action: take button off\ntorch Reward: 0, Score 0, Done False\nSTATE 9\nremoved. you are holding : a torch button a torch round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’push button’, ’north’, ’west’, ’east’, ’south’, ’take shovel’, ’put all\ndown’, ’put torch down’, ’put button down’, ’examine obelisk’, ’up’]\nQvalues: [8.26, 8.02, 7.8, 7.78, 7.59, 6.96, 6.76, 6.72, 6.71, 6.49, 5.56]\nAction: push button Reward: 0, Score 0, Done False\nSTATE 10\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’north’, ’south’, ’take shovel’, ’put button down’, ’examine\nobelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [9.97,\n8.69, 8.26, 8.14, 7.65, 7.56, 7.41, 7.09, 6.9, 6.68, 5.32] Action: south\nReward: 0, Score 0, Done False\nSTATE 11\ncorridor at alcoves the corridor goes north to the stairs and south\nfrom here. there are alcoves to the east and west. in the floor be-\ntween the alcoves is a quartz window. you are holding : a torch\nbutton a torch ( providing light ) corridor at alcoves the corridor\ngoes north to the stairs and south from here. there are alcoves to the\neast and west. in the floor between the alcoves is a quartz window.\nActions: [’south’, ’west’, ’push button’, ’north’, ’put button down’,\n’put all down’, ’put torch down’, ’east’] Qvalues: [8.79, 8.53, 8.07,\n7.82, 7.62, 7.56, 7.45, 7.41] Action: south Reward: 0, Score 0, Done\nFalse\nSTATE 12\noutside door you are at the end of the hall. to the south is a wooden\ndoor with a steel knob on it. the door is closed. you are holding :\na torch button a torch ( providing light ) outside door you are at\nthe end of the hall. to the south is a wooden door with a steel knob\non it. the door is closed. Actions: [’open door’, ’push button’, ’take\nbrass off door’, ’put button down’, ’north’, ’put torch down’, ’put\nall down’] Qvalues: [9.73, 8.84, 8.64, 8.39, 8.32, 8.16, 8.05] Action:\npush button Reward: 0, Score 0, Done False\nSTATE 13\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) outside door you are\nat the end of the hall. to the south is a wooden door with a steel\nknob on it. the door is closed. Actions: [’open door’, ’push button’,\n’take brass off door’, ’put button down’, ’north’, ’put all down’, ’put\ntorch down’] Qvalues: [9.83, 8.92, 8.7, 8.67, 8.23, 8.15, 8.09] Action:\npush button Reward: 0, Score 0, Done False\nSTATE 14\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) outside door you are\nat the end of the hall. to the south is a wooden door with a steel\nknob on it. the door is closed. Actions: [’open door’, ’push button’,\n’put button down’, ’take brass off door’, ’north’, ’put all down’, ’put\ntorch down’] Qvalues: [9.93, 9.04, 8.76, 8.76, 8.28, 8.19, 8.13] Action:\nopen door Reward: 0, Score 0, Done False\nSTATE 15\nthe door slams open. it is apparently on a very strong spring. if\nyou were on the other side, you would have been slammed into\nthe wall. as it is, the steel door knob slams into the inside wall. you\nare holding : a torch button a torch ( providing light ) outside door\nyou are at the end of the hall. to the south is a wooden door with a\nsteel knob on it. the door is open. Actions: [’south’, ’push button’,\n’take brass off door’, ’put button down’, ’close door’, ’put all down’,\n’north’, ’put torch down’] Qvalues: [10.85, 9.79, 9.52, 9.4, 9.14, 8.61,\n8.56, 8.52] Action: south Reward: 0, Score 0, Done False\nSTATE 16\nroom with chest this room is empty except for a small chest at one\nend. the only exit is the door to the north. the door is open. you\nsee something glimmering behind the door. you’ll have to close the\ndoor to get it. the chest is closed and locked. you are holding : a\ntorch button a torch ( providing light ) room with chest this room\nis empty except for a small chest at one end. the only exit is the\ndoor to the north. the door is open. you see something glimmering\nbehind the door. you’ll have to close the door to get it. the chest\nis closed and locked. Actions: [’close door’, ’push button’, ’take\nbrass off door’, ’put torch down’, ’north’, ’put all down’, ’put but-\nton down’] Qvalues: [12.47, 11.55, 11.36, 10.31, 10.26, 10.19, 10.05]\nAction: close door Reward: 0, Score 0, Done False\nSTATE 17\nafter much tugging, you manage to get the door shut. behind it\nis a brass needle that you might use to unlock the chest. you are\nholding : a torch button a torch ( providing light ) room with chest\nthis room is empty except for a small chest at one end. the only exit\nis the door to the north. the door is closed. on the ground is a long\nbrass needle. the chest is closed and locked. Actions: [’open chest\nwith needle’, ’take brass’, ’push button’, ’put all down’, ’put torch\ndown’, ’put button down’] Qvalues: [13.9, 12.14, 12.14, 11.0, 10.83,\n10.5] Action: open chest with needle Reward: 0, Score 0, Done False\nSTATE 18\n( first taking the brass needle ) there is a click and the chest lid pops\nopen. inside is a priceless dorkmid! as you stare at it, you notice\nthe smell of coal gas leaking out. you are holding : a brass needle a\ntorch button a torch ( providing light ) room with chest this room is\nempty except for a small chest at one end. the only exit is the door\nto the north. the door is closed. the chest is open. the chest contains\n: a priceless dorkmid Actions: [’open door’, ’put needle down’, ’put\nneedle in chest’, ’push button’, ’put button in chest’, ’put torch in\nchest’, ’put all in chest’, ’put all down’, ’put torch down’, ’put button\ndown’, ’take dorkmid’] Qvalues: [15.18, 14.95, 14.61, 13.77, 13.53,\n13.52, 13.44, 12.72, 12.46, 12.15, 11.09] Action: open door Reward:\n10, Score 10, Done False\nSTATE 19\nthe door slams open. you barely avoid the swinging knob. instead,\nthe steel knob hits the wall with a bang. the wall seems to be made\nof flint. you have died you are back in the infinite blackness. eventu-\nally the voice returns, senses you disgustedly, and speaks. \" sparks\nin a room full of natural gas is not really a smart thing to do. \"\nyou’ve only gotten killed by one trap. not bad. \" round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. you are\nholding : a torch button a torch ( providing light ) round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’east’, ’north’, ’push button’, ’south’, ’take shovel’, ’west’, ’examine\nobelisk’, ’put button down’, ’put torch down’, ’put all down’, ’up’]\nQvalues: [11.75, 8.96, 8.88, 8.11, 7.56, 7.54, 7.53, 7.32, 6.79, 6.74, 6.41]\nAction: south Reward: 0, Score 10, Done False\nSTATE 20\ncorridor at alcovesyou are holding : a torch button a torch ( provid-\ning light ) corridor at alcoves the corridor goes north to the stairs\nand south from here. there are alcoves to the east and west. in the\nfloor between the alcoves is a quartz window. Actions: [’south’,\n’west’, ’push button’, ’put button down’, ’north’, ’put all down’,\n’east’, ’put torch down’] Qvalues: [8.68, 8.19, 7.98, 7.51, 7.43, 7.36,\n7.34, 7.22] Action: put button down Reward: 0, Score 10, Done False\nSTATE 21\ndropped. you are holding : a torch ( providing light ) corridor at\nalcoves the corridor goes north to the stairs and south from here.\nthere are alcoves to the east and west. in the floor between the\nalcoves is a quartz window. you can see a torch button here. Ac-\ntions: [’south’, ’take button’, ’west’, ’push button’, ’put torch down’,\n’north’, ’east’] Qvalues: [7.64, 7.58, 7.48, 7.33, 6.72, 6.63, 6.35] Action:\nwest Reward: 0, Score 10, Done False\nSTATE 22\nwest alcove this alcove has a stone platform set in the floor. the\nplatform shifts a little but does not move when you step on it. stick-\ning out of the platform is an iron crank. you are holding : a torch (\nproviding light ) west alcove this alcove has a stone platform set in\nthe floor. the platform shifts a little but does not move when you\nstep on it. sticking out of the platform is an iron crank. Actions:\n[’turn crank’, ’east’, ’put torch down’] Qvalues: [8.75, 7.73, 6.44]\nAction: east Reward: 0, Score 10, Done False\nSTATE 23\ncorridor at alcoves you can see a torch button here. you are holding\n: a torch ( providing light ) corridor at alcoves the corridor goes\nnorth to the stairs and south from here. there are alcoves to the east\nand west. in the floor between the alcoves is a quartz window. you\ncan see a torch button here. Actions: [’south’, ’take button’, ’west’,\n’push button’, ’put torch down’, ’north’, ’east’] Qvalues: [7.75, 7.69,\n7.48, 7.46, 6.76, 6.7, 6.62] Action: take button Reward: 0, Score 10,\nDone False\nSTATE 24\ntaken. you are holding : a torch button a torch ( providing light ) cor-\nridor at alcoves the corridor goes north to the stairs and south from\nhere. there are alcoves to the east and west. in the floor between the\nalcoves is a quartz window. Actions: [’south’, ’west’, ’push button’,\n’put button down’, ’put all down’, ’put torch down’, ’north’, ’east’]\nQvalues: [8.53, 7.97, 7.74, 7.28, 7.28, 7.24, 7.09, 6.74] Action: west\nReward: 0, Score 10, Done False\nSTATE 25\nwest alcoveyou are holding : a torch button a torch ( providing\nlight ) west alcove this alcove has a stone platform set in the floor.\nthe platform shifts a little but does not move when you step on it.\nsticking out of the platform is an iron crank. Actions: [’turn crank’,\n’push button’, ’east’, ’put button down’, ’put torch down’, ’put all\ndown’] Qvalues: [9.2, 7.92, 7.65, 7.19, 6.75, 6.7] Action: turn crank\nReward: 0, Score 10, Done False\nSTATE 26\nthe platform slowly moves down a shaft below it. in the other al-\ncove the other platform slowly moves up, counterbalancing the\none you are on. on platform you are on the crank platform at the\nbottom of the shaft under the alcove. there is an opening to the\neast. you are holding : a torch button a torch ( providing light ) on\nplatform you are on the crank platform at the bottom of the shaft\nunder the alcove. there is an opening to the east. Actions: [’push\nbutton’, ’turn crank’, ’put button down’, ’put torch down’, ’put all\ndown’, ’east’] Qvalues: [10.23, 10.22, 9.02, 8.93, 8.7, 8.63] Action:\npush button Reward: 0, Score 10, Done False\nSTATE 27\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) on platform you are\non the crank platform at the bottom of the shaft under the alcove.\nthere is an opening to the east. Actions: [’turn crank’, ’push button’,\n’put button down’, ’east’, ’put torch down’, ’put all down’] Qvalues:\n[11.21, 10.29, 9.09, 9.06, 8.8, 8.62] Action: put all down Reward: 0,\nScore 10, Done False\nSTATE 28\ntorch button : dropped. torch : dropped. you are holding nothing.\non platform you are on the crank platform at the bottom of the\nshaft under the alcove. there is an opening to the east. lying on the\nground is a brass torch of the \" self igniting \" type. the ignite button\nis on the handle. you can also see a torch button here. Actions: [’take\nall’, ’take torch’, ’take button’, ’push button’, ’turn crank’, ’east’]\nQvalues: [9.66, 8.96, 8.88, 8.8, 8.21, 7.81] Action: take all Reward: 0,\nScore 10, Done False\nSTATE 29\niron crank : a wimp like you will never get that done. torch : taken.\ntorch button : taken. you are holding : a torch button a torch (\nproviding light ) on platform you are on the crank platform at the\nbottom of the shaft under the alcove. there is an opening to the\neast. Actions: [’turn crank’, ’push button’, ’put button down’, ’put\ntorch down’, ’put all down’, ’east’] Qvalues: [11.85, 10.14, 8.88, 8.85,\n8.72, 8.24] Action: east Reward: 0, Score 10, Done False\nSTATE 30\nin tiny room this is a tiny room. the only exit is back west. you can\nsee the quartz window directly above your head. there is a huge\ndiamond lying on the ground! you are holding : a torch button a\ntorch ( providing light ) in tiny room this is a tiny room. the only\nexit is back west. you can see the quartz window directly above\nyour head. there is a huge diamond lying on the ground! Actions:\n[’west’, ’take diamond’, ’push button’, ’put torch down’, ’put button\ndown’, ’put all down’] Qvalues: [10.17, 9.35, 8.97, 8.14, 8.13, 8.12]\nAction: west Reward: 0, Score 10, Done False\nSTATE 31\non platformyou are holding : a torch button a torch ( providing\nlight ) on platform you are on the crank platform at the bottom of\nthe shaft under the alcove. there is an opening to the east. Actions:\n[’turn crank’, ’push button’, ’put button down’, ’put torch down’,\n’put all down’, ’east’] Qvalues: [11.12, 10.08, 8.86, 8.74, 8.66, 7.84]\nAction: turn crank Reward: 10, Score 20, Done False\nSTATE 32\nthe platform moves back up to the alcove level. in the other alcove,\nyou see the other platform moving back down until both of the\nplatforms are even. west alcove this alcove has a stone platform\nset in the floor. the platform shifts a little but does not move when\nyou step on it. sticking out of the platform is an iron crank. as the\nother platform moves down from the hole in the ceiling, you see a\nsmall robot standing on it. when it gets down to the floor, it looks\nat you, says \" ex trrr mnnn ate \", points an arm at you, and turns\nyou into a small pile of monatomic dust. you have died you are\nback in the infinite blackness. eventually the voice returns, senses\nyou disgustedly, and speaks. \" giving nasties a free ride down to\nwhere you are seems a little stupid. \" you’ve been killed by two\ntraps. that isn’t really so bad. \" round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. you are holding : a\ntorch button a torch ( providing light ) round room this room is at\nthe peak of the pyramid. it is perfectly conical. in the center is a\ntall stone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’push button’,\n’north’, ’south’, ’take shovel’, ’examine obelisk’, ’put button down’,\n’west’, ’put torch down’, ’put all down’, ’east’, ’up’] Qvalues: [8.24,\n7.81, 7.57, 7.32, 7.08, 6.95, 6.66, 6.58, 6.47, 6.31, 5.27] Action: west\nReward: 0, Score 20, Done False\nSTATE 33\nt intersection the east \/ west corridor branches to the south here.\nyou are holding : a torch button a torch ( providing light ) t inter-\nsection the east \/ west corridor branches to the south here. Actions:\n[’east’, ’push button’, ’west’, ’put torch down’, ’put all down’, ’put\nbutton down’, ’south’] Qvalues: [7.36, 6.89, 6.85, 6.17, 6.08, 6.05,\n5.85] Action: east Reward: 0, Score 20, Done False\nSTATE 34\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’north’,\n’south’, ’take shovel’, ’examine obelisk’, ’east’, ’put button down’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [8.32, 7.79,\n7.61, 7.21, 7.12, 6.97, 6.93, 6.61, 6.56, 6.4, 5.75] Action: put button\ndown Reward: 0, Score 20, Done False\nSTATE 35\ndropped. you are holding : a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to\nget back up to the fake toilet. there are four stairways leaving the\nroom at the four compass points. there is a shovel lying here. you\ncan also see a torch button here. Actions: [’take button’, ’north’,\n’push button’, ’take all’, ’take shovel’, ’east’, ’south’, ’west’, ’examine\nobelisk’, ’put torch down’, ’up’] Qvalues: [7.53, 7.32, 7.3, 7.18, 7.07,\n7.0, 6.82, 6.78, 6.67, 6.18, 5.21] Action: north Reward: 0, Score 20,\nDone False\nSTATE 36\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. you are holding : a\ntorch ( providing light ) landing this is a landing on the stairway.\nyou can go down to the north or up to the south. a passage leads\neast. Actions: [’north’, ’south’, ’east’, ’put torch down’] Qvalues:\n[8.16, 6.7, 6.56, 6.28] Action: east Reward: 0, Score 20, Done False\nSTATE 37\nt intersection the hallway goes east and west here with a branch\nnorth. you are holding : a torch ( providing light ) t intersection\nthe hallway goes east and west here with a branch north. Actions:\n[’west’, ’north’, ’put torch down’, ’east’] Qvalues: [7.2, 6.93, 6.24,\n6.01] Action: put torch down Reward: 0, Score 20, Done False\nSTATE 38\ndropped. you are holding nothing. t intersection the hallway goes\neast and west here with a branch north. lying on the ground is a\nbrass torch of the \" self igniting \" type. the ignite button is on the\nhandle. Actions: [’west’, ’north’, ’east’, ’take torch’] Qvalues: [6.88,\n6.7, 6.47, 6.28] Action: east Reward: 0, Score 20, Done False\nSTATE 39\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve been\nkilled by two traps. that isn’t really so bad. \" round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. you can also see a torch\nbutton here. you are holding : a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. you can\nalso see a torch button here. Actions: [’take button’, ’north’, ’push\nbutton’, ’take all’, ’take shovel’, ’south’, ’west’, ’examine obelisk’,\n’put torch down’, ’east’, ’up’] Qvalues: [7.77, 7.65, 7.54, 7.45, 7.14,\n7.03, 6.69, 6.66, 6.31, 6.06, 5.33] Action: north Reward: 0, Score 20,\nDone False\nSTATE 40\nlandingyou are holding : a torch ( providing light ) landing this is\na landing on the stairway. you can go down to the north or up to\nthe south. a passage leads east. Actions: [’north’, ’south’, ’east’, ’put\ntorch down’] Qvalues: [8.24, 6.67, 6.57, 6.31] Action: north Reward:\n0, Score 20, Done False\nSTATE 41\nbottom of shaft you are at the bottom of a ladder that goes up a\nshaft from here. to the south is the stairway up. on the floor is a\nsmall coin bag made of leather. it appears to have something in it.\nyou are holding : a torch ( providing light ) bottom of shaft you are\nat the bottom of a ladder that goes up a shaft from here. to the south\nis the stairway up. on the floor is a small coin bag made of leather.\nit appears to have something in it. Actions: [’up’, ’take coin-bag’,\n’south’, ’put torch down’] Qvalues: [9.37, 8.64, 7.42, 7.26] Action:\ntake coin-bag Reward: 0, Score 20, Done False\nSTATE 42\ntaken. you are holding : a small leather coin bag a torch ( providing\nlight ) bottom of shaft you are at the bottom of a ladder that goes up\na shaft from here. to the south is the stairway up. Actions: [’up’, ’put\ncoin-bag down’, ’south’, ’put torch down’, ’put all down’] Qvalues:\n[9.71, 8.47, 7.53, 7.14, 7.05] Action: up Reward: 0, Score 20, Done\nFalse\nSTATE 43\nyou climb up the ladder to a hole at the top of the shaft. exercise\nwheel room this room is laid out like an exercise wheel for rats ex-\ncept that it is 40 ft. across. you can turn the wheel by walking north\nor south. equally spaced around the rim are twelve coin holders.\nsome have gold coins in them. the coin holder at your feet has a\ncoin in it. the hole is at the bottom of the wheel. below it is the shaft\nthat you entered this room by. you are holding : a small leather coin\nbag a torch ( providing light ) exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet has a coin in it. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered this\nroom by. Actions: [’south’, ’put coin-bag down’, ’put torch down’,\n’put all down’, ’down’, ’north’] Qvalues: [10.62, 8.9, 8.48, 8.29, 8.23,\n8.13] Action: south Reward: 0, Score 20, Done False\nSTATE 44\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the north side. you\nare holding : a small leather coin bag a torch ( providing light )\nexercise wheel room this room is laid out like an exercise wheel\nfor rats except that it is 40 ft. across. you can turn the wheel by\nwalking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is slightly up the north side. Actions:\n[’south’, ’put coin-bag down’, ’north’, ’put torch down’, ’put all\ndown’] Qvalues: [11.58, 9.76, 9.28, 9.2, 9.07] Action: put torch down\nReward: 0, Score 20, Done False\nSTATE 45\ndropped. you are holding : a small leather coin bag exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is slightly up the north side. lying on the ground is a\nbrass torch of the \" self igniting \" type. the ignite button is on the\nhandle. Actions: [’take brass’, ’south’, ’put coin-bag down’, ’north’]\nQvalues: [10.36, 10.14, 9.24, 7.98] Action: north Reward: 0, Score 20,\nDone False\nSTATE 46\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is at the bottom of the wheel.\nbelow it is the shaft that you entered this room by. you are holding\n: a small leather coin bag exercise wheel room this room is laid out\nlike an exercise wheel for rats except that it is 40 ft. across. you can\nturn the wheel by walking north or south. equally spaced around\nthe rim are twelve coin holders. some have gold coins in them. the\ncoin holder at your feet has a coin in it. the hole is at the bottom of\nthe wheel. below it is the shaft that you entered this room by. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. Actions: [’south’, ’take torch’, ’put coin-bag\ndown’, ’down’, ’north’] Qvalues: [8.84, 8.82, 8.14, 7.12, 7.07] Action:\nput coin-bag down Reward: 0, Score 20, Done False\nSTATE 47\ndropped. you are holding nothing. exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet has a coin in it. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered\nthis room by. on the floor is a small coin bag made of leather. it\nappears to have something in it. lying on the ground is a brass\ntorch of the \" self igniting \" type. the ignite button is on the handle.\nActions: [’take all’, ’south’, ’take torch’, ’take coin-bag’, ’take in\nfloor’, ’north’] Qvalues: [9.69, 9.44, 9.1, 8.53, 8.18, 7.63] Action: take\nall Reward: 0, Score 20, Done False\nSTATE 48\nsmall leather coin bag : taken. torch : taken. coin holder : you pick\nup the coin and put it in your bag. you are holding : a torch ( pro-\nviding light ) a small leather coin bag exercise wheel room this\nroom is laid out like an exercise wheel for rats except that it is 40 ft.\nacross. you can turn the wheel by walking north or south. equally\nspaced around the rim are twelve coin holders. some have gold\ncoins in them. the coin holder at your feet is empty. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered this\nroom by. Actions: [’south’, ’put coin-bag down’, ’put torch down’,\n’put all down’, ’north’, ’down’] Qvalues: [9.59, 8.74, 8.14, 8.11, 7.7,\n7.53] Action: north Reward: 0, Score 20, Done False\nSTATE 49\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the south side. you\nare holding : a torch ( providing light ) a small leather coin bag\nexercise wheel room this room is laid out like an exercise wheel\nfor rats except that it is 40 ft. across. you can turn the wheel by\nwalking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at\nyour feet has a coin in it. the hole is slightly up the south side.\nActions: [’south’, ’put coin-bag down’, ’put all down’, ’put torch\ndown’, ’north’] Qvalues: [8.63, 8.06, 7.47, 7.36, 7.29] Action: put\ncoin-bag down Reward: 0, Score 20, Done False\nSTATE 50\ndropped. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is slightly up the south side. on the floor is a small coin\nbag made of leather. it appears to have something in it. Actions:\n[’south’, ’take coin-bag’, ’north’, ’put torch down’] Qvalues: [9.01,\n8.69, 8.03, 7.6] Action: south Reward: 0, Score 20, Done False\nSTATE 51\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet is empty. the hole is at the bottom of the wheel. below it\nis the shaft that you entered this room by. you are holding : a torch\n( providing light ) exercise wheel room this room is laid out like an\nexercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet is empty. the hole is at the bottom of the wheel.\nbelow it is the shaft that you entered this room by. on the floor is a\nsmall coin bag made of leather. it appears to have something in it.\nActions: [’south’, ’take coin-bag’, ’put torch down’, ’north’, ’take\nin floor’] Qvalues: [10.48, 10.08, 8.75, 8.58, 8.15] Action: put torch\ndown Reward: 0, Score 20, Done False\nSTATE 52\ndropped. you are holding nothing. exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet is empty. the hole is at the bottom\nof the wheel. below it is the shaft that you entered this room by.\nlying on the ground is a brass torch of the \" self igniting \" type. the\nignite button is on the handle. on the floor is a small coin bag made\nof leather. it appears to have something in it. Actions: [’take all’,\n’south’, ’take torch’, ’take coin-bag’, ’take in floor’, ’north’] Qvalues:\n[10.04, 9.65, 9.47, 8.81, 8.42, 8.17] Action: take torch Reward: 0, Score\n20, Done False\nSTATE 53\ntaken. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north\nor south. equally spaced around the rim are twelve coin holders.\nsome have gold coins in them. the coin holder at your feet is empty.\nthe hole is at the bottom of the wheel. below it is the shaft that\nyou entered this room by. on the floor is a small coin bag made of\nleather. it appears to have something in it. Actions: [’south’, ’take\ncoin-bag’, ’put torch down’, ’north’, ’take in floor’] Qvalues: [10.47,\n9.96, 8.81, 8.62, 8.6] Action: south Reward: 0, Score 20, Done False\nSTATE 54\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the north side. you\nare holding : a torch ( providing light ) exercise wheel room this\nroom is laid out like an exercise wheel for rats except that it is 40 ft.\nacross. you can turn the wheel by walking north or south. equally\nspaced around the rim are twelve coin holders. some have gold\ncoins in them. the coin holder at your feet has a coin in it. the hole\nis slightly up the north side. on the floor is a small coin bag made of\nleather. it appears to have something in it. Actions: [’south’, ’take\ncoin-bag’, ’put torch down’, ’north’] Qvalues: [11.74, 11.16, 9.81,\n9.42] Action: take coin-bag Reward: 0, Score 20, Done False\nSTATE 55\ntaken. you are holding : a small leather coin bag a torch ( providing\nlight ) exercise wheel room this room is laid out like an exercise\nwheel for rats except that it is 40 ft. across. you can turn the wheel\nby walking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is slightly up the north side. Actions:\n[’south’, ’put coin-bag down’, ’north’, ’put torch down’, ’put all\ndown’] Qvalues: [12.41, 10.59, 10.06, 9.98, 9.9] Action: south Re-\nward: 0, Score 20, Done False\nSTATE 56\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder\nat your feet has a coin in it. the hole is less than half way up the\nnorth side. you are holding : a small leather coin bag a torch (\nproviding light ) exercise wheel room this room is laid out like an\nexercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet has a coin in it. the hole is less than half way\nup the north side. Actions: [’south’, ’put coin-bag down’, ’put torch\ndown’, ’north’, ’put all down’] Qvalues: [13.58, 11.65, 10.99, 10.91,\n10.85] Action: put all down Reward: 0, Score 20, Done False\nSTATE 57\nsmall leather coin bag : dropped. torch : dropped. you are holding\nnothing. exercise wheel room this room is laid out like an exercise\nwheel for rats except that it is 40 ft. across. you can turn the wheel\nby walking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is less than half way up the north side.\nlying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. on the floor is a small coin bag\nmade of leather. it appears to have something in it. Actions: [’take\nall’, ’south’, ’take torch’, ’take coin-bag’, ’north’] Qvalues: [12.65,\n12.31, 12.0, 11.39, 9.82] Action: south Reward: 0, Score 20, Done False\nSTATE 58\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is half way up the north side. you\nare holding nothing. exercise wheel room this room is laid out like\nan exercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet has a coin in it. the hole is half way up the north\nside. lying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. on the floor is a small coin bag\nmade of leather. it appears to have something in it. Actions: [’take\nall’, ’south’, ’take torch’, ’take coin-bag’, ’north’] Qvalues: [14.04,\n13.49, 13.35, 12.75, 10.79] Action: take torch Reward: 0, Score 20,\nDone False\nSTATE 59\ntaken. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is half way up the north side. on the floor is a small coin\nbag made of leather. it appears to have something in it. Actions:\n[’south’, ’take coin-bag’, ’put torch down’, ’north’] Qvalues: [14.71,\n14.12, 12.3, 11.95] Action: south Reward: 0, Score 20, Done False\nSTATE 60\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is more than half way up the north\nside. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is more than half way up the north side. on the floor\nis a small coin bag made of leather. it appears to have something\nin it. Actions: [’south’, ’take coin-bag’, ’put torch down’, ’north’]\nQvalues: [15.89, 15.25, 13.22, 13.01] Action: south Reward: 10, Score\n30, Done False\nSTATE 61\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder\nat your feet has a coin in it. the hole is almost to the top on the\nnorth side. behind it is a hole from which water is pouring very fast.\nthe water turns the wheel back to its original position, then pours\ndown the hole. you are taken with it. you have died you are back\nin the infinite blackness. eventually the voice returns, senses you\ndisgustedly, and speaks. \" don’t you know how lab rats eventually\nend up? \" three deathtraps have zocked you so far. you’re beginning\nto seem rather dumb. \" round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nthere is a shovel lying here. you can also see a torch button here.\nyou are holding : a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. you can also see\na torch button here. Actions: [’take button’, ’north’, ’push button’,\n’take all’, ’south’, ’take shovel’, ’west’, ’examine obelisk’, ’put torch\ndown’, ’east’, ’up’] Qvalues: [8.61, 8.49, 8.34, 8.29, 7.95, 7.88, 7.43,\n7.37, 7.15, 7.11, 5.9] Action: put torch down Reward: 0, Score 30,\nDone False\nSTATE 62\ndropped. you are holding nothing. round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. lying on the ground is a brass torch of the \" self\nigniting \" type. the ignite button is on the handle. there is a shovel\nlying here. you can also see a torch button here. Actions: [’west’,\n’north’, ’take all’, ’push button’, ’south’, ’east’, ’take button’, ’take\ntorch’, ’take shovel’, ’examine obelisk’, ’up’] Qvalues: [7.85, 7.68,\n7.68, 7.68, 7.66, 7.62, 7.51, 7.44, 7.13, 6.9, 6.1] Action: south Reward:\n0, Score 30, Done False\nSTATE 63\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know. even\nyou could have gotten farther into that situation. \" three deathtraps\nhave zocked you so far. you’re beginning to seem rather dumb. \"\nround room this room is at the peak of the pyramid. it is perfectly\nconical. in the center is a tall stone obelisk that you could probably\nuse to get back up to the fake toilet. there are four stairways leaving\nthe room at the four compass points. there is a shovel lying here.\nyou can also see a torch button here. you are holding : a torch (\nproviding light ) round room this room is at the peak of the pyramid.\nit is perfectly conical. in the center is a tall stone obelisk that you\ncould probably use to get back up to the fake toilet. there are four\nstairways leaving the room at the four compass points. there is a\nshovel lying here. you can also see a torch button here. Actions:\n[’take button’, ’north’, ’push button’, ’take all’, ’south’, ’take shovel’,\n’west’, ’examine obelisk’, ’put torch down’, ’east’, ’up’] Qvalues:\n[8.62, 8.38, 8.34, 8.28, 7.94, 7.89, 7.54, 7.35, 7.17, 6.62, 5.78] Action:\ntake all Reward: 0, Score 30, Done False\nSTATE 64\ntorch button : taken. shovel : taken. stone obelisk : a wimp like you\nwill never get that done. you are holding : a shovel a torch button a\ntorch ( providing light ) round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nActions: [’push button’, ’north’, ’south’, ’put shovel down’, ’east’,\n’put button down’, ’examine obelisk’, ’put torch down’, ’west’, ’put\nall down’, ’up’] Qvalues: [8.96, 8.86, 8.77, 8.23, 8.21, 7.93, 7.84, 7.54,\n7.53, 7.5, 5.75] Action: push button Reward: 0, Score 30, Done False\nSTATE 65\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a shovel a torch button a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. Actions: [’east’, ’push button’, ’north’,\n’south’, ’put shovel down’, ’put button down’, ’examine obelisk’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [10.04, 9.23,\n9.06, 9.0, 8.39, 8.16, 8.09, 7.59, 7.55, 7.38, 6.29] Action: east Reward:\n0, Score 30, Done False\nSTATE 66\nt intersection to the hall goes east from here. to the west is the\nstairway to the round room. a narrow passage leads south. you are\nholding : a shovel a torch button a torch ( providing light ) t inter-\nsection to the hall goes east from here. to the west is the stairway\nto the round room. a narrow passage leads south. Actions: [’east’,\n’west’, ’push button’, ’south’, ’put shovel down’, ’put torch down’,\n’put button down’, ’put all down’] Qvalues: [13.19, 8.57, 8.34, 7.48,\n7.11, 6.89, 6.88, 6.69] Action: east Reward: 0, Score 30, Done False\nSTATE 67\ncorridor at doorway the hall goes east and west from here and there\nis a doorway to the north. you are holding : a shovel a torch button\na torch ( providing light ) corridor at doorway the hall goes east\nand west from here and there is a doorway to the north. Actions:\n[’north’, ’east’, ’west’, ’push button’, ’put shovel down’, ’put torch\ndown’, ’put button down’, ’put all down’] Qvalues: [15.16, 11.38,\n8.99, 8.52, 7.44, 7.26, 7.01, 6.99] Action: north Reward: 0, Score 30,\nDone False\nSTATE 68\nroom full of gadgets this is the south end of a long room. the walls\nand floor and ceiling are covered with thousands of metal gadgets\nthat you might want to examine more carefully. there is a path free\nof gadgets to the north end of the room. you are holding : a shovel\na torch button a torch ( providing light ) room full of gadgets this\nis the south end of a long room. the walls and floor and ceiling are\ncovered with thousands of metal gadgets that you might want to\nexamine more carefully. there is a path free of gadgets to the north\nend of the room. Actions: [’north’, ’push button’, ’south’, ’put shovel\ndown’, ’put torch down’, ’put button down’, ’put all down’] Qvalues:\n[16.71, 10.81, 9.31, 9.13, 8.81, 8.61, 8.58] Action: north Reward: 0,\nScore 30, Done False\nSTATE 69\nnorth gadget room the room stretches away back south. the metal\ngadgets are clustered even more densely here. sitting on the floor\nin a clear space is a cut crystal bowl. the bowl contains : a large\nperfect ruby a hundreds of quartz chipsyou are holding : a shovel\na torch button a torch ( providing light ) north gadget room the\nroom stretches away back south. the metal gadgets are clustered\neven more densely here. sitting on the floor in a clear space is a cut\ncrystal bowl. the bowl contains : a large perfect ruby a hundreds of\nquartz chips Actions: [’take chips’, ’push button’, ’put torch down’,\n’put all down’, ’put shovel down’, ’put button down’, ’south’] Qval-\nues: [13.84, 9.5, 8.06, 7.99, 7.98, 7.77, 7.26] Action: take chips Reward:\n10, Score 40, Done False\nSTATE 70\nas you do, several of the quartz chips spill onto the floor. some\nhit metal gadgets, triggering them. darts fly. some darts hit more\ngadgets. more darts fly. after a few seconds of this, you have been\npunctured several million times. you have died you are back in the\ninfinite blackness. eventually the voice returns, senses you disgust-\nedly, and speaks. \" when standing inside a mass of uranium, it is\nnot usually a good idea to drop a neutron. \" you’ve stumbled into\nfour traps. you’re not looking very good. \" round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. you are holding\n: a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. Actions: [’push\nbutton’, ’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put button\ndown’, ’put torch down’, ’west’, ’put all down’, ’east’, ’up’] Qvalues:\n[9.13, 8.58, 8.54, 8.07, 7.74, 7.73, 7.45, 7.42, 7.36, 6.8, 5.76] Action:\npush button Reward: 0, Score 40, Done False\nSTATE 71\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’north’, ’south’, ’take shovel’, ’put button down’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.62, 9.35, 8.78, 8.78, 8.0, 7.93, 7.87, 7.55, 7.35, 7.17, 6.39] Action:\npush button Reward: 0, Score 40, Done False\nSTATE 72\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’put button down’, ’take shovel’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.63, 9.25, 8.68, 8.54, 7.96, 7.96, 7.71, 7.57, 7.32, 7.15, 6.03] Action:\neast Reward: 0, Score 40, Done False\nSTATE 73\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [8.01, 7.65, 7.22, 7.01, 6.94,\n6.93, 6.46] Action: east Reward: 0, Score 40, Done False\nSTATE 74\ncorridor at doorwayyou are holding : a torch button a torch ( pro-\nviding light ) corridor at doorway the hall goes east and west from\nhere and there is a doorway to the north. Actions: [’west’, ’push\nbutton’, ’north’, ’put torch down’, ’put all down’, ’put button down’,\n’east’] Qvalues: [7.61, 7.48, 7.24, 6.89, 6.78, 6.58, 5.01] Action: west\nReward: 0, Score 40, Done False\nSTATE 75\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [7.94, 7.53, 7.02, 6.93, 6.89,\n6.83, 6.15] Action: push button Reward: 0, Score 40, Done False\nSTATE 76\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [8.16, 7.75, 7.7, 7.34, 7.01, 6.85, 6.73] Action: west Reward:\n0, Score 40, Done False\nSTATE 77\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’put button down’, ’take shovel’, ’examine obelisk’,\n’put torch down’, ’west’, ’put all down’, ’up’] Qvalues: [9.42, 8.58,\n8.33, 7.99, 7.82, 7.74, 7.61, 7.2, 7.18, 7.05, 5.89] Action: west Reward:\n0, Score 40, Done False\nSTATE 78\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection the east \/ west corridor branches to the south\nhere. Actions: [’east’, ’push button’, ’west’, ’put torch down’, ’put\nall down’, ’put button down’, ’south’] Qvalues: [7.66, 7.53, 7.46, 6.63,\n6.6, 6.53, 6.31] Action: put all down Reward: 0, Score 40, Done False\nSTATE 79\ntorch button : dropped. torch : dropped. you are holding nothing.\nt intersection the east \/ west corridor branches to the south here.\nlying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. you can also see a torch button\nhere. Actions: [’west’, ’east’, ’push button’, ’south’, ’take all’, ’take\nbutton’, ’take torch’] Qvalues: [7.47, 7.33, 7.29, 7.08, 7.01, 6.79, 6.66]\nAction: take all Reward: 0, Score 40, Done False\nSTATE 80\ntorch : taken. torch button : taken. you are holding : a torch button\na torch ( providing light ) t intersection the east \/ west corridor\nbranches to the south here. Actions: [’east’, ’west’, ’push button’,\n’put all down’, ’put torch down’, ’put button down’, ’south’] Qvalues:\n[8.2, 7.56, 7.44, 6.46, 6.42, 6.34, 6.2] Action: east Reward: 0, Score 40,\nDone False\nSTATE 81\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’put button down’, ’take shovel’, ’examine obelisk’,\n’put torch down’, ’west’, ’put all down’, ’up’] Qvalues: [9.17, 8.31,\n8.05, 8.02, 7.6, 7.51, 7.33, 6.95, 6.94, 6.82, 5.48] Action: push button\nReward: 0, Score 40, Done False\nSTATE 82\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’put button down’, ’take shovel’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.47, 9.04, 8.33, 8.11, 7.78, 7.62, 7.35, 7.27, 7.01, 6.81, 5.67] Action:\neast Reward: 0, Score 40, Done False\nSTATE 83\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [7.63, 7.29, 6.78, 6.6, 6.56,\n6.53, 6.3] Action: push button Reward: 0, Score 40, Done False\nSTATE 84\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [7.92, 7.45, 7.27, 7.0, 6.68, 6.57, 6.44] Action: push button\nReward: 0, Score 40, Done False\nSTATE 85\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [7.89, 7.4, 7.27, 6.96, 6.64, 6.55, 6.41] Action: put all down\nReward: 0, Score 40, Done False\nSTATE 86\ntorch button : dropped. torch : dropped. you are holding nothing.\nt intersection to the hall goes east from here. to the west is the\nstairway to the round room. a narrow passage leads south. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. you can also see a torch button here. Ac-\ntions: [’east’, ’west’, ’push button’, ’take all’, ’south’, ’take button’,\n’take torch’] Qvalues: [7.4, 7.13, 7.1, 7.08, 7.05, 6.86, 6.75] Action:\ntake all Reward: 0, Score 40, Done False\nSTATE 87\ntorch : taken. torch button : taken. you are holding : a torch button\na torch ( providing light ) t intersection to the hall goes east from\nhere. to the west is the stairway to the round room. a narrow pas-\nsage leads south. Actions: [’west’, ’push button’, ’south’, ’east’, ’put\nbutton down’, ’put torch down’, ’put all down’] Qvalues: [7.89, 7.18,\n6.81, 6.68, 6.31, 6.29, 6.27] Action: west Reward: 0, Score 40, Done\nFalse\nSTATE 88\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’take shovel’, ’put button down’, ’examine obelisk’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [8.74, 7.99,\n7.93, 7.42, 7.38, 7.27, 7.14, 6.84, 6.76, 6.65, 5.54] Action: push button\nReward: 0, Score 40, Done False\nSTATE 89\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’take shovel’, ’put button down’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.16, 8.88, 8.24, 8.2, 7.52, 7.49, 7.35, 7.07, 6.85, 6.63, 5.99] Action:\nnorth Reward: 0, Score 40, Done False\nSTATE 90\nlandingyou are holding : a torch button a torch ( providing light )\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. Actions: [’north’,\n’push button’, ’south’, ’put button down’, ’put torch down’, ’east’,\n’put all down’] Qvalues: [8.7, 8.17, 7.45, 7.15, 6.9, 6.81, 6.81] Action:\nput all down Reward: 0, Score 40, Done False\nSTATE 91\ntorch button : dropped. torch : dropped. you are holding nothing.\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. lying on the ground\nis a brass torch of the \" self igniting \" type. the ignite button is on\nthe handle. you can also see a torch button here. Actions: [’take\nall’, ’take torch’, ’north’, ’take button’, ’east’, ’push button’, ’south’]\nQvalues: [7.96, 7.45, 7.44, 7.34, 7.3, 7.3, 7.09] Action: north Reward:\n0, Score 40, Done False\nSTATE 92\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is at the peak of the pyramid. it is perfectly conical.\nin the center is a tall stone obelisk that you could probably use to\nget back up to the fake toilet. there are four stairways leaving the\nroom at the four compass points. there is a shovel lying here. you\nare holding : a torch ( providing light ) round room this room is at\nthe peak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four com-\npass points. there is a shovel lying here. Actions: [’south’, ’north’,\n’take shovel’, ’examine obelisk’, ’put torch down’, ’west’, ’east’, ’up’]\nQvalues: [7.93, 7.62, 7.36, 7.07, 6.82, 6.67, 6.48, 4.8] Action: put torch\ndown Reward: 0, Score 40, Done False\nSTATE 93\ndropped. you are holding nothing. round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. lying on the ground is a brass torch of the \" self igniting \"\ntype. the ignite button is on the handle. there is a shovel lying here.\nActions: [’west’, ’south’, ’east’, ’take all’, ’north’, ’take brass’, ’take\nshovel’, ’examine obelisk’, ’up’] Qvalues: [7.63, 7.56, 7.54, 7.46, 7.46,\n7.34, 6.94, 6.73, 6.22] Action: examine obelisk Reward: 0, Score 40,\nDone False\nSTATE 94\nthe obelisk is a monument to the great rahn. the inscription starts :\n\" for rahn hath given us the postulates and the theorems ; we shalt\nthank rahn. for rahn hath said’between any two points there shalt\nbe but one line’; and he found it good. for rahn hath said’if two\nplanes intersecteth, they shalt form but one straight line’; and he\nfound it good. for rahn hath said... \" it goes on like this for a long\ntime. when you manage to break the trance, your mind is numbed\nand full of rahn’s strange commandments. round room this room\nis a conic surface with exits at even multiples of 90 degrees. in the\ncenter is a regular square pyramid with a narrow vertex angle. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. there is a shovel lying here. you are holding\nnothing. round room this room is a conic surface with exits at even\nmultiples of 90 degrees. in the center is a regular square pyramid\nwith a narrow vertex angle. lying on the ground is a brass torch of\nthe \" self igniting \" type. the ignite button is on the handle. there is\na shovel lying here. Actions: [’south’, ’take all’, ’east’, ’west’, ’take\nbrass’, ’north’, ’take shovel’, ’examine obelisk’, ’up’] Qvalues: [7.34,\n7.27, 7.13, 7.1, 7.08, 7.05, 6.59, 6.37, 5.73] Action: south Reward: 0,\nScore 40, Done False\nSTATE 95\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is a conic surface with exits at even multiples of 90\ndegrees. in the center is a regular square pyramid with a narrow\nvertex angle. there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [8.0, 7.7, 7.34, 7.26, 6.71, 6.65,\n6.15, 4.76] Action: examine obelisk Reward: 0, Score 40, Done False\nSTATE 97\nthe obelisk is a monument to the great rahn. the inscription starts :\n\" for rahn hath given us the postulates and the theorems ; we shalt\nthank rahn. for rahn hath said’between any two points there shalt\nbe but one line’; and he found it good. for rahn hath said’if two\nplanes intersecteth, they shalt form but one straight line’; and he\nfound it good. for rahn hath said... \" it goes on like this for a long\ntime. when you manage to break the trance, your mind is numbed\nand full of rahn’s strange commandments. round room this room\nis a conic surface with exits at even multiples of 90 degrees. in\nthe center is a regular square pyramid with a narrow vertex angle.\nthere is a shovel lying here. you are holding : a torch ( providing\nlight ) round room this room is a conic surface with exits at even\nmultiples of 90 degrees. in the center is a regular square pyramid\nwith a narrow vertex angle. there is a shovel lying here. Actions:\n[’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch down’,\n’west’, ’east’, ’up’] Qvalues: [7.91, 7.7, 7.27, 7.16, 6.6, 6.57, 6.45, 5.07]\nAction: put torch down Reward: 0, Score 40, Done False\nSTATE 98\ndropped. you are holding nothing. round room this room is a conic\nsurface with exits at even multiples of 90 degrees. in the center is a\nregular square pyramid with a narrow vertex angle. lying on the\nground is a brass torch of the \" self igniting \" type. the ignite button\nis on the handle. there is a shovel lying here. Actions: [’take all’,\n’south’, ’west’, ’take brass’, ’east’, ’north’, ’take shovel’, ’examine\nobelisk’, ’up’] Qvalues: [7.4, 7.34, 7.26, 7.24, 7.21, 7.19, 6.74, 6.51,\n6.06] Action: south Reward: 0, Score 40, Done False\nSTATE 99\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is a conic surface with exits at even multiples of 90\ndegrees. in the center is a regular square pyramid with a narrow\nvertex angle. there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [8.02, 7.67, 7.34, 7.23, 6.75, 6.66,\n6.23, 4.74] Action: west Reward: 0, Score 40, Done False\nSTATE 100\nt intersectionyou are holding : a torch ( providing light ) t intersec-\ntion the east \/ west corridor branches to the south here. Actions:\n[’west’, ’east’, ’put torch down’, ’south’] Qvalues: [7.21, 6.7, 6.47,\n5.87] Action: east Reward: 0, Score 40, Done True\nSTATE 101\nround room there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [7.82, 7.45, 7.12, 6.99, 6.59, 6.5,\n6.18, 4.91] Action: reset Reward: 0, Score 0, Done False\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Pre-trained Language Models as Prior Knowledge for Playing Text-based Games.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nPre-trained Language Models as Prior Knowledge for Playing Text-based Games\n```\n#### 2. 论文摘要\n```\nRecently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires an understanding of and\ninteraction using natural language in a partially observable environment.\nAgents observe the environment via textual descriptions designed to be\nchallenging enough for even human players. Past approaches have not paid enough\nattention to the language understanding capability of the proposed agents.\nTypically, these approaches train from scratch, an agent that learns both\ntextual representations and the gameplay online during training using a\ntemporal loss function. Given the sample-inefficiency of RL approaches, it is\ninefficient to learn rich enough textual representations to be able to\nunderstand and reason using the textual observation in such a complicated game\nenvironment setting. In this paper, we improve the semantic understanding of\nthe agent by proposing a simple RL with LM framework where we use\ntransformer-based language models with Deep RL models. We perform a detailed\nstudy of our framework to demonstrate how our model outperforms all existing\nagents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6\nhigher than the state-of-the-art model. Overall, our proposed approach\noutperforms 4 games out of the 14 text-based games, while performing comparable\nto the state-of-the-art models on the remaining games.\n```\n\n#### 3. 论文全文\n```\nPre-trained Language Models as Prior Knowledge for Playing\nText-based Games\nIshika Singh, Gargi Singh, Ashutosh Modi\nDepartment of Computer Science and Engineering\nIndian Institute of Technology Kanpur (IITK), India\n{ishikas,sgargi}@iitk.ac.in, ashutoshm@cse.iitk.ac.in\nABSTRACT\nRecently, text world games have been proposed to enable artifi-\ncial agents to understand and reason about real-world scenarios.\nThese text-based games are challenging for artificial agents, as it re-\nquires an understanding of and interaction using natural language\nin a partially observable environment. Agents observe the environ-\nment via textual descriptions designed to be challenging enough\nfor even human players. Past approaches have not paid enough\nattention to the language understanding capability of the proposed\nagents. Typically, these approaches train from scratch, an agent\nthat learns both textual representations and the gameplay online\nduring training using a temporal loss function. Given the sample-\ninefficiency of RL approaches, it is inefficient to learn rich enough\ntextual representations to be able to understand and reason using\nthe textual observation in such a complicated game environment\nsetting. In this paper, we improve the semantic understanding of\nthe agent by proposing a simple RL with LM framework where we\nuse transformer-based language models with Deep RL models. We\nperform a detailed study of our framework to demonstrate how our\nmodel outperforms all existing agents on the popular game, Zork1,\nto achieve a score of 44.7, which is 1.6 higher than the state-of-the-\nart model. Overall, our proposed approach outperforms 4 games\nout of the 14 text-based games, while performing comparable to\nthe state-of-the-art models on the remaining games.\n1\nINTRODUCTION\nArtificial autonomous agents suffer from a number of challenges\nduring training, such as reward, goal, or task under-specification(s).\nMost approaches design a specific reward function for a given en-\nvironment and task, which is not generalizable to any other setting.\nThe reward specification gets more complicated with complex en-\nvironments and tasks. Most agents are trained to do specific tasks\nor achieve specific goals, as there are not many efficient ways for\nspecifying multiple or complex goals for a given agent. In this\nscenario, being able to utilize language as an interface between a\nuser and an artificial agent simplifies a number of these challenges.\nA setting where an agent understands and communicates using\nnatural language is more efficient for rewarding the agent online\nor via a text description, as opposed to using an expert-designed\nreward function. Moreover, it is easier to specify complicated tasks\nor instructions for the agent via the natural language.\nText-based Interactive Fiction (IF) [7] games provide such an en-\nvironment where the agent learns to consume and produce natural\nlanguage-based inputs and outputs. IF games require an artificial\nagent to learn policies and operate in a real-world environment\ncreated using natural language. These games are designed such that\na player receives a textual observation consisting of information\nabout the environment such as a description of the surroundings,\nobjects available for interaction, and states of objects. Based on this\ninformation, the player takes an action such as interacting with one\nof the objects, moving around. etc. A short example of a game-play\nis shown in Figure 1. The textual setting of the games in terms of the\nobservations given to and the actions taken by the agent requires\nsolid language understanding and reasoning. Such a setting has\nbeen promoted to develop essential skills required by intelligent\nagents for various real-life use-cases, including dialogue systems\nand personal digital assistants. IF games, such as Zork1, have been\ncreated to be challenging for human players and consist of texts\nwith dramatic and artful narratives. The game engine, in the case\nof IF games, generates human-level sophistication and diversity\nin textual descriptions (as can be seen in the example in Figure\n1) in contrast to games using template-generated synthetic texts.\nConsequently, such games provide an ideal test-bed for artificial\nagents that interact with the real-world environment using natural\nlanguage. Given the setting, agents trained using IF games can be\ndeployed in real-world user-centered robotics applications.\nIF games present several challenges for artificial agents as these\ncover the real world settings. It requires an agent to understand\nthe textual state description, handle combinatorial textual action\nspace, and learn a policy in a partially observable environment to\nmaximize the game score. The key challenge is to decipher the long\ntextual observations, extract reward cues from them, and generate\na semantically rich representation such that the policy learned on\ntop of it is well informed. Most of the existing works learn textual\nrepresentations from scratch during the RL training [1, 2, 7, 23–25],\nexcept [6] which uses pre-trained GloVe embeddings [14]. Online\nreinforcement learning is known to be sample-inefficient [19]. The\nreward-based temporal difference objective used for training is\nproposed for learning the gameplay, and it does not necessarily\nreinforce the agent to learn the semantics of the game. Therefore,\nlearning textual representations solely from the game-generated\ntext during online learning does not provide rich enough represen-\ntation to be able to handle such complicated decision-making tasks.\nIn addition, these games also need some prior knowledge about\nthe functioning of the world to be able to make correct decisions\nefficiently. For example, as shown in Figure 1, if it is dark and the\nagent has a lantern, it should turn it on.\nGiven the requirements for IF games, we propose a pre-trained\ntransformer-based [20] language model (LM) as a candidate for\nequipping the RL agent with both language understanding capa-\nbilities and real-world knowledge. Transformer-based LMs such as\nBERT [5], ALBERT [10], RoBERTa [11] have produced state-of-the-\nart performance on a plethora of natural language understanding\n(NLU) tasks. The routine for using the transformer models includes\narXiv:2107.08408v2  [cs.CL]  23 Dec 2021\nwith a great eﬀort, the rug is moved to one side of the room, revealing the dusty cover of a closed trap \ndoor\nOpen case\nyou are in the living room. there is a doorway to the east, a wooden door with strange gothic lettering to \nthe west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the \nroom. above the trophy case hangs an elvish sword of great antiquity. a battery powered brass lantern \nis on the trophy case. [you are carrying : a jewel encrusted egg] [same as current observation]\npush rug\nOpened.\nTake lantern\nTaken. [you are carrying : a brass lantern a jewel encrusted egg] [living room you are in the living room. \nthere is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to \nbe nailed shut, a trophy case, and a closed trap door at your feet. above the trophy case hangs an elvish \nsword of great antiquity.]\nPut egg in case\n+5\ndone. [you are carrying : brass lantern] […a closed trap door at your feet. above the trophy case hangs \nan elvish sword of great antiquity. your collection of treasures consists of : jewel encrusted egg] Open trap\nthe door reluctantly opens to reveal a rickety staircase descending into darkness. […nailed shut, a \ntrophy case, and a rug lying beside an open trap door. above the trophy case hangs an elvish sword of \ngreat antiquity. your collection of treasures consists of : a jewel encrusted egg]\nDown\n+25\nyou have moved into a dark place. the trap door crashes shut, and you hear someone barring it. [it is \npitch black. you are likely to be eaten by a grue.]\nturn  on lantern\neast of chasm you are on the east edge of a chasm, the bottom of which cannot be seen. a narrow \npassage goes north, and the path you are on continues to the east. [same as current observation.] East\nthe brass lantern is now on. cellar you are in a dark and damp cellar with a narrow passageway leading \nnorth, and a crawlway to the south. on the west is the bottom of a steep metal ramp which is \nunclimbable. [you are carrying : a brass lantern ( providing light )] [same as current observation.] South\nthis is an art gallery. most of the paintings have been stolen by vandals with exceptional taste. the vandals \nleft through either the north or west exits. fortunately, there is still one chance for you to be a vandal, for \non the far wall is a painting of unparalleled beauty. [same as current observation.]\nTake art\n+4\nFigure 1: A sample gameplay by our model (DBERT-DRRN) for the classic text game, Zork1. The aim is to solve puzzles and\ncollect 19 treasures in the trophy case, but the agent is not aware of the goal and learns from rewards. Each white box is a\nstate of the game at a step; the green and orange boxes are the action taken, and the reward received correspondingly. The\nstate contains location if visible (underlined), current observation (black), inventory with the player (red), and the current\nlocation description (blue). In this example, location, inventory, and description are only mentioned when it changes, while\nthe complete state information is provided to the agent during gameplay. This example shows how our model is able to perform\nbetter by learning to use the egg and the lantern in the correct way, as highlighted.\npre-training on large generic English corpora, followed by fine-\ntuning on a specific downstream task-specific corpus. Moreover,\nPetroni et al. [15] have shown that pre-trained LMs can act as\nknowledge bases containing relational and factual knowledge, com-\nparable in utility with other NLP methods having access to oracle\nknowledge.\nIn this work, we propose a simple approach that performs bet-\nter than previously proposed complex approaches. We deploy a\npre-trained LM with the existing game agents, namely Deep Re-\ninforcement Relevance Network (DRRN) [8] and Template-Deep\nQ-Network (TDQN) [7], used in the recently proposed Jericho IF\ngame-suite [7]. To the best of our knowledge, we are the first to\nutilize pre-trained LMs for IF games. We use DistilBERT[17] (here-\nafter referred to as DBERT) as the LM owing to its compact size,\nthereby computing rich text encoding efficiently. We fine-tune\nDBERT on an independent set of human gameplay transcripts to\nadd a game sense to the pre-trained LM. This set of transcripts\ncontain games other than those in the evaluation game-suite to\ntest the generalizability of the proposed approach. We test both\nDBERT-DRRN and DBERT-TDQN setup on Zork1, and additionally,\nwe also evaluate DBERT-DRRN on a set of other games from Jericho\nto test the generalization capability of the model. We achieve new\nstate-of-the-art results on 4 out of 14 games while getting compara-\nble performance on others relative to the past approaches (section\n2) utilizing sophisticated attention models and knowledge graphs,\nindicating that pre-trained LMs could be employed as a key com-\nponent in text-based reinforcement learning models. We release\nthe implementation code via GitHub1. The transcripts (generated\nwhen the agent is playing the game) is included in the appendix.\n2\nRELATED WORK\nRecently, there have been attempts to model real world settings via\ntext based environments. The TextWorld environment [4] proposes\nprocedurally generated interactive fiction games, with control over\ndifficulty, and description language. Recently, Hausknecht et al. [7]\nproposed a learning environment (Jericho) that supports a set of 32\nhuman-written IF games. These games are written to be challenging\nfor human players, thereby providing a more realistic test-bed for\ntraining intelligent agents. [7] also present performance results for\nchoice-based (DRRN) and parser-based (TDQN) agents (see section\n3.2 for details), where the former performed better on average\nacross most of the games. Jericho also provides certain handicaps\nsuch as action templates, determining valid actions by detecting\nworld state change, commands to check current items with the\nagent (inventory) or in the current state (look). We propose models\nfor solving the games in the Jericho environment.\nMany of the recent approaches [1, 2, 23] use a dynamically up-\ndated Knowledge Graph (KG) to represent the current state of the\nuniverse (game). KG-A2C [1] is the first such proposal. One of\nthe SOTA methods by Xu et al. [23], in addition to KG-A2C archi-\ntecture, reasons on the KG using low and high-level attention on\nsub-components of the graph and on the complete graph, respec-\ntively, to compute a representation of the game state. Next, they\nselect the actions via recurrent decoding using GRUs, conditioned\non the computed game state representation. Similar to KG-A2C\napproach, they train the agent via the Advantage Actor Critic (A2C)\nmethod with a supervised auxiliary task of “valid action prediction”\nusing the action templates handicap. Q∗BERT [2] presents an open\ndomain QA method to update the KG with more information and\nan additional intrinsic motivation reward to enable structured ex-\nploration. Our agent outperforms all the above agents without an\nexplicit knowledge graph or any additional reward-based super-\nvision or sophisticated reasoning. We show that better language\nunderstanding capabilities and appropriate utilization of the world\nknowledge in game-plays are comparable (and sometimes better)\nthan using explicit KGs.\nMulti-Paragraph Reading Comprehension Deep Q-Network [6]\n(MPRC-DQN) breaks down the problem into two challenges: the\npartial observability of the environment and large natural language\naction space. They solve the partial observability by object-centric\nretrieval of relevant past observations, and an attention mechanism\nis deployed to focus on the most relevant context. Template actions\nfrom Jericho are filled up in question answering (QA) format to\ngenerate candidate actions. In contrast, CALM [25] generates the\nnext set of possible action by fine-tuning a GPT-2 model [16]. These\nactions, which are essentially a subset of valid actions, are then fed\nto a DRRN [8] agent to compute the Q-values. CALM asserts that\nthe action generation model provides linguistic priors to the RL\nagent. However, since they do not send any feedback to the action\ngeneration model and the model only replaces the valid action set\n1https:\/\/github.com\/Exploration-Lab\/IFG-Pretrained-LM\nby its subset, the RL agent may not necessarily benefit from the\nlinguistic priors. On the other hand, we extract representations\nfrom the pre-trained LM and feed them to the DRRN agent for\nincorporating linguistic priors. With a better observation under-\nstanding augmented with the world knowledge via the pre-trained\nLM, our model also solves the challenge of partial observability to\nsome extent.\nYao et al. [24] investigate to what extent semantic information is\nutilized by the DRRN agent. They show that even in the complete\nabsence of text, the “DRRN + valid action handicap\" setup is able to\nachieve significant scores, indicating the underlying overfitting to\nthe reward system and memorization tendency of DRRN. They\nuse an inverse-dynamics loss function to regularize the DRRN\nrepresentation space for improving the semantics.\nTransformers [20] are a new class of feed-forward neural net-\nworks architectures that use a self-attention mechanism to effec-\ntively learn short\/long-term relationships between tokens in the\ntext. Transformers have been shown to have SOTA performance on\nalmost all the NLP tasks [18, 21]. Transformers are trained using\nlanguage modeling objective and its variants (e.g., masked language\nmodeling); based on the training objective a number of variants\nof transformer architectures have been proposed e.g., BERT [5],\nDistilBERT [17], ALBERT [10], RoBERTa [11]. There have been\nseveral transformer-based approaches for both text-based games\nand RL in general [3, 9, 12, 22], but these approaches train a trans-\nformer from scratch, which is inefficient given that transformers\nare data-intensive and online RL is sample-inefficient. Xu et al. [22]\nand Parisotto et al. [12] modify the transformer structure to add\ngates and make it light-weight for online training, while Chen et al.\n[3] and Janner et al. [9] use a smaller transformer but only for of-\nfline learning. Using a pre-trained LM brings an additional set of\nlinguistic priors and world knowledge to the RL agent, and in this\npaper, we propose a way to do so. We chose DistilBERT [17] for\nour framework. DistilBERT is a lighter and faster version of BERT\ntrained by knowledge-distilling BERT base with masked language\nmodeling on Toronto Book Corpus and Wikipedia, thus providing\nus a computationally efficient solution to integrate in the RL setup.\n3\nMETHOD\n3.1\nProblem Statement\nA text-based game can be formulated as a partially observable\nMarkov Decision Process (POMDP) defined by (𝑆,𝑇,𝐴,𝑂, 𝑅). An\nagent interacts with the game environment to receive textual ob-\nservations 𝑜𝑡∈𝑂at the interaction step 𝑡. The latent states 𝑠𝑡∈𝑆\ncorrespond to 𝑜𝑡along with player, item locations, and inventory\ncontents. The agent interacts with the environment by executing\nan action 𝑎𝑡∈𝐴that changes the game state according to a mostly-\ndeterministic but latent transition function𝑇(𝑠′|𝑠,𝑎), and the agent\nreceives rewards 𝑟𝑡from an unknown reward function 𝑅(𝑠,𝑎) de-\nfined by the game designers. The objective of an RL agent is to\ntake a series of actions that maximize expected cumulative dis-\ncounted rewards E\n\u0002Í∞\n𝑖=𝑡𝛾𝑟𝑖\n\u0003\nat any time 𝑡. Value-based RL models\nestimate this reward by learning a Q-function, 𝑄(𝑜𝑡,𝑎𝑡) i.e., the\nexpected return when taking an action 𝑎𝑡, given the observation 𝑜𝑡.\nThe problem statement presents several challenges for the agent:\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n*58\n2EVHUYDWLRQ\n(QFRGHU\n*58\n\/RRN\n(QFRGHU\n*58\n,QYHQWRU\\\u0003\n(QFRGHU\n*58\n$FWLRQ\n(QFRGHU\n&RQFDW\n0\/3\n$FW\u0010VFRUHU\n4\u000bR\u000fD\f\n2EVHUYDWLRQ\n\/RRN\n,QYHQWRU\\\u0003\n$FWLRQ\n&RQFDW\n(a)\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n'LVWLO%(57\n*58\n2EVHUYDWLRQ\n(QFRGHU\n*58\n\/RRN\n(QFRGHU\n*58\n,QYHQWRU\\\u0003\n(QFRGHU\n*58\n$FWLRQ\n(QFRGHU\n&RQFDW\n0\/3\n2EVHUYDWLRQ\n\/RRN\n,QYHQWRU\\\u0003\n3UHYLRXV\u0003$FWLRQ\nW\u0010VFRUHU\nR\u0014\u0010VFRUHU\nR\u0015\u0010VFRUHU\n4\u000bR\u000fX\f\n4\u000bR\u000fS\u0014\f\n4\u000bR\u000fS\u0015\f\n(b)\nFigure 2: RL agents - (a) DRRN: Q-value Q(o,a) is computed for observation o and action a, (b) TDQN: Q-values Q(o,u), Q(o,𝑝1),\nQ(o,𝑝2) for all templates 𝑢∈𝑇and all vocabulary 𝑝∈𝑉.\nunderstanding long sophisticated textual state descriptions, ex-\ntracting reward cues, handling combinatorial textual action space,\nreasoning-based decision-making, and learning a value function in\na partially observable environment to maximize the game score.\n3.2\nRL Agents\nWe build our model on top of existing RL agents. We select the\nfollowing two agents as these are base architectures [7] on which\nall other subsequent works build on.\nDRRN: Deep Reinforcement Relevance Network (DRRN) [7] uses\nfour GRU encoders to encode the state and each of the actions from\nthe valid action handicap, concatenate the representations, and pass\nit through an MLP layer to estimate the Q-values (𝑄(𝑜𝑡,𝑎𝑡𝑖)|𝑖=1...𝑁),\nwhere 𝑎𝑡𝑖∈A (valid action set). The next action is chosen by\nsoftmax sampling the predicted Q-values. The DRRN is trained\nusing tuples (𝑜𝑡,𝑎𝑡,𝑟𝑡,𝑜𝑡+1) sampled from a prioritized experience\nreplay buffer with the traditional temporal difference (TD) loss:\nLTD(𝜙) =\n\u0012\n𝑟𝑡+ 𝛾max\n𝑎∈A 𝑄𝜙(𝑜𝑡+1,𝑎) −𝑄𝜙(𝑜𝑡,𝑎𝑡)\n\u00132\nwhere 𝜙represents the overall DRRN parameters, 𝑟𝑡is the reward\nat 𝑡-th time step, and 𝛾is the discount factor.\nTDQN: Template Deep Q-Network (TDQN) [7] like DRRN uses\nGRU encoders to encode information that consists of the state and\nprevious action in this case. It performs template-based action gen-\neration using three Q-values based on the observation 𝑜: 𝑄(𝑜,𝑢) for\nall action templates 𝑢∈𝑇, 𝑄(𝑜, 𝑝1) and 𝑄(𝑜, 𝑝2) for all vocabulary\n𝑝∈𝑉. 𝑝1, 𝑝2 fill-up the blanks in the template to generate candidate\nactions. The authors used a supervised binary-cross entropy loss\nto train the model for valid actions in addition to the TD loss.\n3.3\nLanguage Modeling (LM)\nThe textual modality of the IF games comes with additional chal-\nlenges for an RL agent. Since the modality of the games is natural\nlanguage, the modeling of the text-based observations and states be-\ncomes important, and this problem has not been explored in-depth\nin the RL research community. We employ DBERT [17] to address\nthis in our model. We fine-tune DBERT to model the language of\ngames. We take inspiration from one of the language modeling\ntechniques, masked language modeling (MLM), used to pre-train\nDBERT. Given a pair of observation𝑜𝑡and action 𝑎𝑡composed of to-\nkens [CLS],𝑜𝑡1,𝑜𝑡2, ...,𝑜𝑡𝑛, [SEP],𝑎𝑡1,𝑎𝑡2, ...,𝑎𝑡𝑚, random tokens are\nmasked by a special token, [MASK]. Here 𝑛is the length of the\nobservation, and 𝑚is the length of the action. The masked tokens\nare predicted from the vocabulary of the model. For example, given\nsequence “[CLS] Yes, there is a diorama at one end of the [MASK],\nbut you find yourself unable to [MASK] on it with that fascinating\nstatue sitting in the middle of the room. Once you figure out how it\nmoves, you’ll be able to concentrate on the [MASK] parts of the room.\n[SEP] remove boots\", the correct predictions would be “room\", “con-\ncentrate\" and “other\" respectively. A softmax activation function is\nused for masked token prediction. Cross entropy loss function is\nused to learn the parameters, i.e.,\n𝐿= CrossEntropy\n\u0010\n𝑥∗, 𝑃(𝑥[MASK]|𝑆)\n\u0011\nwhere 𝑥∗is the ground truth, 𝑥[MASK] is the predicted token, 𝑆is\nthe masked sentence, and 𝑃(𝑥[MASK]|𝑆) is the predicted distribution\nover the vocabulary. This training enables DBERT to tune its prior\nknowledge in the language of games.\n3.4\nRL with Pre-trained LM\nWe use the fine-tuned DBERT to model the input for the requisite\nRL agent. We encode the state and the actions separately using\nDBERT and feed the representations to the respective GRUs in\nDRRN and TDQN agent. The agents are then trained as described\nin 3.2 (Figure 2a and 2b). In this way, the trained DBERT is able\nto transfer its prior knowledge about the world to different agents\nacross different games. The agents, in turn, learn more informed\npolicies on the top of DBERT optimized for specific games. We use\na single instance of the trained DBERT and do not fine-tune DBERT\nfurther for specific games to keep the language model general and\nadaptable to any RL agent or game.\n4\nEXPERIMENTS\nWe evaluate how the addition of a pre-trained LM facilitates differ-\nent agents and games. We also present ablation studies to demon-\nstrate the need for training the pre-trained LM. Lastly, we present\na study on how our model is able to achieve new state-of-the-art\nresults on Zork1.\n4.1\nLM training\nWe use the ClubFloyd dataset [25] for fine-tuning DBERT. It is a\ncollection of human game-play trajectories on 590 games. These\nare not the optimal trajectories, but it does impart a general game-\nplay sense. We pre-process this data to obtain around 217K pairs\nof observation and action, (𝑜𝑡,𝑎𝑡). The transcripts are from games\nnot included in our evaluation (section 4.2) to keep our approach\ngeneral. We use a 768-dimensional vanilla DBERT (base cased)\nmodel pre-trained on English Wikipedia and Toronto Book Corpus.\nWe train this model for 2 epochs on pre-processed ClubFloyd dataset\nusing the MLM technique.\n4.2\nRL Training\nJericho Environment: We evaluate our agent on 14 games with\ndifferent difficulty levels2 using the Jericho framework, and use the\nsame Jericho handicaps as in the respective DRRN and TDQN base-\nlines [7]. The states are observations (𝑜𝑡) concatenated with items\nin possession of the player and its current location description pro-\nvided by the game engine using commands inventory and look. We\nalso use valid actions and action templates for DBERT-DRRN and\nDBERT-TDQN respectively, following the baselines. A single game\nepisode runs for 100 environment steps at max, or gets terminated\nbefore the game is over or won.\nTraining Setup: We test our framework on two RL models: DRRN\n(Figure 2a) and TDQN (Figure 2b). The GRUs used in the agents have\nan embedding dimension of 768 features and a hidden dimension of\n128 features. Similar to [7, 25] we collect interaction data for DBERT-\nDRRN and DBERT-TDQN on 8 parallel game environment instances\nand use a prioritized experience replay buffer to store trajectories\nwith best scores. We sample transitions from this priority buffer\nwith a priority fraction of 0.5, while taking the remaining training\ndata from a general replay buffer. Most importantly, we train our\nagents for a maximum of 5×104 steps, which is less than half of the\ntraining steps used in most other baselines, and achieve SOTA and\ncomparable scores on different games. Each training step makes\nan interaction with the environment in all 8 instances, therefore\nwe only use half the amount of interaction data for training in\ncomparison to the baselines. We use a softmax exploration policy\nand a learning rate of 10−4, along with other presets the same as the\nbaseline. We report the average of scores on the last 100 finished\nepisodes as the score on a game run. We train two independent runs\n2We choose games across varying difficulty level (possible\/easy, difficult, extreme) as\nindicated in Table 1. The difficulty level assigned to each game [7] is based on optimal\nsolution length, reward density, puzzle complexity, etc. Please refer to [7] for more\ndetails on difficulty level.\nfor each model and game, and report the average of their scores\nas the final (raw) score, along with the average maximum score\nseen by the runs. The maximum seen scores are a measure of the\nexploration ability, while the raw scores are that of the learning\nability of an agent. We report the learning curves for Zork1 in\nFigure 3.\n5\nRESULTS AND ANALYSIS\nWe performed a detailed analysis of our method. We present overall\ngame-scores as compared to baseline models across a set of games.\nWe also present an ablation study of our model done on Zork1, along\nwith a semantic understanding analysis. Finally, to actually interpret\nthe agent from inside beyond numbers, we also present a qualitative\nanalysis of our model on Zork1 via a case study discussing the\nfactors leading to higher score.\n5.1\nOverall Scores\nTable 1 reports the final scores of our best performing model (trained\nDBERT with DRRN) in comparison with 7 existing baselines on\n14 games. We report raw or maximum or both the scores as given\nin original papers. Different baselines achieve SOTA scores on\ndifferent games. Our model achieves SOTA results on 4 games:\nZork1, Inhumane, Snacktime, and Jewel, while being second best\nor comparable on most of the other games. We present the learn-\ning curves for Zork1, Inhumane, and Jewel in Figures 3, 4, and 5\nrespectively. INV-DY [24] uses additional loss objectives inspired\nfrom curiosity-driven exploration [13]. While it helps them achieve\nhigher maximum scores on Zork1, but are not able to learn the high\nscore trajectories. On the other hand, our agent efficiently learns the\nmax score trajectories explored by it, thereby indicating that with\na better exploration strategy our model has the potential to achieve\nbetter scores. None of the other agents, with a max score of 55, are\nable to stably reach a score as high as our model, that maintains a\nmargin of 6.4 from the best model [6]. Our agent explores higher\nmax score on Inhumane, but more importantly, it is able to learn\nthe best-explored trajectories, thereby plateauing closer to the max\nscores for many games (Inhumane, Jewel, Omniquest, Zork1),\nindicating that the trained DBERT is an important learning compo-\nnent, and it also facilitates the exploration to some extent. We also\nreport the normalized score (raw score as a factor of max possible\nscore collected from human-written optimal walk-through) aver-\naged across all games. We get an overall norm of 15.7%, followed\nby 14.2% achieved by MPRC-DQN [6]. Our model does not suffer a\nlot on any game, while the second best (MPRC-DQN) gets 0 scores\non Inhumane and Snacktime, and notably higher scores on others\n(Ludicorp, Spirit, Zork3). It indicates both the generalization\ntendency and the necessity of the pre-trained LM deployed in our\nmodel. When compared to the average norm of 18.9% for INV-DY\nevaluated on 6 of the 14 games they reported, we get 21.1%. The\naverage norm is also a measure of human-machine gap for text-\nbased games, indicating that IF games are at best only 15.7% solved.\nHence, it is a good benchmark for developing language understand-\ning agents. We have included the learning curves for independent\nruns on Zork1 and its game transcripts in the Appendix.\nTable 1: Final (raw) and max scores seen by agents (available via prior work) comparing trained DBERT-DRRN with all the\ncurrent SOTA baselines across a set of games from Jericho game-suite. For the baselines, we use the results reported in the\noriginal papers. The missing scores not in the previous work are denoted as ’-’. Max denotes the maximum possible scores\nbased on human-written optimal walkthroughs for winning the game without the step limit of 100. Note that our model\n(DBERT-DRRN) is trained for less than half of the training steps used in other baselines. Bold scores denote the best score,\nblue* scores denote the second best. DBERT-DRRN gets an avg. norm of 15.7% on all games, and 21.1% on 6 games reported\nfor INV-DY. *the games with rising learning curve for DBERT-DRRN till the last training step. 𝑝(possible\/easy), 𝑑(difficult),\n𝑒(extreme) refers to game difficulty level as given in [7].\nGame\nTDQN\nDRRN\nMPRC-DQN\nSHA-KG\nQ*BERT\nCALM\nINV-DY\nDBERT-DRRN\nMax\n[7]\n[7]\n[6]\n[23]\n[2]\n[25]\n[24]\n(ours)\nraw\nraw \/ max\nraw\nraw\nmax\nraw\nraw \/ max\nraw \/ max\nInhumane∗𝑝\n0.7\n0\n0\n5.4\n-\n25.7*\n19.6 \/ 45\n32.8 \/ 50\n90\nJewel𝑑\n0\n1.6\n4.46*\n1.8\n-\n0.3\n-\n6.5 \/ 13\n90\nLibrary∗𝑝\n6.3\n17.0*\n17.7\n15.8\n19\n9.0\n16.2 \/ 21\n17.0* \/ 21\n30\nLudicorp𝑑\n6\n13.8\n19.7\n17.8*\n22.8\n10.1\n13.5 \/ 23\n12.5 \/ 18\n150\nOmniquest𝑝\n16.8\n5\n10.0\n-\n-\n6.9*\n5.3 \/ 10\n4.9 \/ 5\n50\nReverb𝑝\n0.3\n8.2*\n2.0\n10.6\n-\n-\n-\n6.1 \/ 12\n50\nSnacktime𝑝\n9.7\n0 \/ 0.25\n0\n-\n-\n19.4*\n-\n20.0 \/ 20\n50\nSpellbrkr∗𝑒\n18.7\n37.8\n25\n40\n-\n40\n-\n38.2* \/ 40\n600\nSpirit𝑒\n0.6\n0.8\n3.8\n3.8\n-\n1.4\n-\n2.1* \/ 8\n250\nTemple𝑝\n7.9*\n7.4\n8.0\n7.9*\n8.0\n0\n-\n8.0 \/ 8.0\n35\nTryst205∗𝑒\n0\n9.6*\n10.0\n6.9\n-\n-\n9.3 \/ 17\n350\nZork1𝑑\n9.9\n32.6 \/ 53\n38.3\n34.5\n41.6\n30.4\n43.1* \/ 87\n44.7 \/ 55\n350\nZork3𝑑\n0\n0.5\n3.63\n0.7*\n-\n0.5\n0.4 \/ 4\n0.2 \/ 4\n7\nYomomma∗𝑑\n0\n0.4\n1.0\n-\n-\n-\n-\n0.5* \/ 1.0\n35\nAvg. Norm (%)\n7.7\n10.2\n14.2\n13.3\n-\n12.8\n18.9\n15.7 (21.1)\n100\n5.2\nDBERT Training Ablation\nOur framework includes a DBERT trained on human trajectories\nfrom different games. This training induces a language acquaintance\nwith games. We tested empirically how this affected the model.\nWe obtain runs on both DRRN and TDQN based models for the\nvanilla DBERT and DBERT pretrained on games from ClubFloyd\ndataset. We present the results in Figure 3 where we see DRRN\nwith pretrained DBERT outperforms DRRN with vanilla DBERT\nwith a final score of 44.72 compared to 34.73. The TDQN based\nmodels do not exhibit much difference, but pretrained version still\nhas a higher score of 10.35, whereas vanilla DBERT version of the\nframework obtains 9.85. It indicates the importance of acquainting\nthe pre-trained LM with the game language, so that it can transfer\nits prior knowledge appropriately to the agents.\n5.3\nSemantic Understanding: Transfer\nYao et al. [24] show that “TD loss + valid action handicap\" setup\nwith mostly deterministic rewards can lead to overfitting and mem-\norization, thus, hindering the actual goal of understanding and\nlearning to operate in natural language. Therefore, we also test\nwhether an agent trained for one game can transfer its learning to\nother similar games to see if the agent is learning the semantics\nor memorizing the trajectories. We train an agent on Zork1 and\ntest it without any training on Zork3, as these are the two most\nsimilar games in the Jericho game-suite. The average episode score\nover 300 episodes was 0.06 for DBERT-DRRN and 0.007 for DRRN.\nThese scores are not as high as achieved by training fresh agents\non Zork3; however, DBERT-DRRN is able to transfer its knowledge\nbetter than DRRN.\n5.4\nAvoiding being eaten by a grue and\nthereafter\nWe compare scores over an episode for (vanilla and trained) DBERT-\nDRRN and DRRN. We sample 100 random episodes from the last\nfew hundred episodes during the training, and plot the average\nscores over an episode in Figure 6. An episode is set to terminate\nafter 100 steps or before (if the game is over or won). The goal in\nZork1 is to collect 19 treasures in the trophy case, which agents are\nnot aware of, and they learn via in-game rewards. Existing agents\ntraverse the Forest for the Egg (1st treasure), followed by Kitchen,\n0\n100\n200\n300\n400\nSteps (x100)\n10\n0\n10\n20\n30\n40\n50\nLast100EpisodeScores\ntdqn + vanilla_dbert\ntdqn + trained_dbert\ndrrn + vanilla_dbert\ndrrn + trained_dbert\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n10\n0\n10\n20\n30\n40\n50\nMax score seen\ntdqn + vanilla_dbert\ntdqn + trained_dbert\ndrrn + vanilla_dbert\ndrrn + trained_dbert\n(b)\nFigure 3: DBERT performance ablation results on Zork1\n0\n100\n200\n300\n400\nSteps (x100)\n0\n5\n10\n15\n20\n25\n30\nLast100EpisodeScores\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nMax score seen\n(b)\nFigure 4: DBERT-DRRN performance results on Inhumane\nCellar, and Gallery for the painting (next treasure), while solving\nvarious puzzle and escaping the grue. In DRRN, the agent gets to\nthe Kitchen with a reward of +10 at around step-10 and then to\nthe Cellar with a reward of +25 around step-15 as we can see in\n0\n50\n100\n150\n200\n250\n300\n350\n400\nSteps (x100)\n1\n2\n3\n4\n5\n6\n7\nLast100EpisodeScores\n(a)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nSteps (x100)\n4\n6\n8\n10\n12\nMax score seen\n(b)\nFigure 5: DBERT-DRRN performance results on Jewel\nthe plot. It doesn’t learn to take and carry the Egg for a reward\nof +5 (and another +5 for putting it in the case) before moving\nto the Kitchen even though the observations present the Egg as\nsomething precious “..in the bird’s nest is a large egg encrusted with\n0\n20\n40\n60\n80\n100\nSteps\n0\n10\n20\n30\n40\n50\nAverage scores over an episode\ndrrn\ndrrn + vanilla_dbert\ndrrn + trained_dbert\nFigure 6: Avoiding being eaten by a grue: Average scores over\nan episode for Zork1\nprecious jewels, apparently scavenged by a childless songbird. the\negg is covered with fine gold inlay,..\". Ammanabrolu et al. [2] use an\nintrinsic motivation reward in addition to the rewards provided by\nthe game engine, in order to get past when stuck in the game, i.e.,\nwhen the observations do not change. This motivation also helps\nthem provide rewards to add dependencies to their KG for the Egg.\nHowever, the LM in trained DBERT-DRRN guesses that the Egg\nis important without any extra supervision, and takes it around\nstep-10, hence, achieving higher scores later on. Moreover, trained\nDBERT-DRRN avoids being eaten by a grue in the Cellar which says\n“the trap door crashes shut, and you hear someone barring it. it is pitch\nblack. you are likely to be eaten by a grue.\" by instantly turning on\nthe lantern without any reward, while DRRN takes random actions\nat this point and dies. Then our agent finds the way to the gallery,\nand gets the painting for +4 points. Thus, merely improving the\nunderstanding of the agent enables it to take better actions without\nany additional supervision. A part of a sample episode transcript\nis shown in Figure 1, and the complete episode is provided in the\nAppendix.\nIn summary, the pre-trained LM enables the DBERT-DRRN agent\nto extract reward cues from the observations as well as use its\npriors on world sense and game sense to get past the grue. We\nobserved that the Vanilla-DBERT is only as good as the base DRRN\nagent. Training it on a set of gameplays is improving the model\nconsiderably, indicating the importance of this training, which is\nessentially channeling the world sense of Vanilla-DBERT into a\ngameplay mode. The DBERT-DRRN agent understands that a “jewel\nencrusted egg appearing extremely fragile\" should be taken, and\nshould not be thrown, or if it’s dark the lantern should be turned\non. Trained DBERT-DRRN assigns reasonably high Q-values to\nsuch actions in the valid action set, while DRRN assigns similar\nvalues when rewards are absent for the same (as can be seen in\nthe trajectories provided in the Appendix). Both DBERT-DRRN and\nall other baselines are able to reach the gallery. Our agent does so\nmore often as it escapes the grue in the Cellar using the lantern.\nMoreover, it extracts intermediate rewards cues and ends up with\na higher score at the end of an episode. The baselines with a score\naround 39 or less are the ones which could not learn these aspects\nof the game. After getting the painting, the agents explore several\noptions, but none of them, including ours, are able to find and learn\nto find the third treasure. Moving past trained DBERT-DRRN score\nwill likely require a more intelligent agent with better exploration\nand learning strategies.\n6\nCONCLUSION AND DISCUSSION\nThis paper proposes using a pre-trained LM fine-tuned on game\ndynamics for RL agents trained for text-based games. It provides\nthree-fold benefits to the RL agent: linguistic priors, world sense\npriors, and game sense priors. It facilitates the agent to achieve\nSOTA results on several text-based games, even though being a\nsimpler approach than all other baselines. The proposed approach\nindicates the importance of using pre-trained LMs for RL agents\nin text-based games. Through this work, we want to draw the\nresearch community’s attention and motivate research in this direc-\ntion to create even better priors using pre-trained LM for language\nunderstanding agents, such as distilling its knowledge to agents\nwith better architectures. Such unsupervised components for explo-\nration and learning will also be more useful in real-life scenarios\nwhere there are no explicit determinate rewards than reward-based\nrepresentation learning. We have shown that training RL agents\nfor text-based games in the absence of a prior is inefficient. The\ntwo key reasons are: learning language from scratch using the lim-\nited text data with a TD loss or related objective does not produce\nrich enough representations, and the agent has no source of world\nknowledge to be able to take appropriate actions given the state.\nThe agents thus trained are not really understanding their state\nin the game, leading to the suspicion that the agents are simply\nmemorizing the best-explored trajectories [24].\nIF games are still far from solved. We need agents with better\nexploration strategies (for instance curiosity-driven learning [13])\nto find high score trajectories, as well as learning architectures\nand objectives that can facilitate learning these trajectories while\nefficiently leveraging priors from pre-trained language models. Cur-\nrent results show the performance of the agents trained on the same\ngame environments. Another line of future work is to test trained\nagents on unseen games. This will require a strong understanding\nof the world functioning and generalized training strategies to be\nable to acquire skills from multiple games and transfer them to\nthe unseen test environments. This will also be a robust test-bed\nfor semantic understanding of the agent, as memorization will no\nlonger make any points in unseen games. Leveraging commonsense\nknowledge for text-based game agents can be another interesting\ndirection for improving the knowledge priors. Many actions taken\nduring these games are associated with object affordances. With a\ncommonsense knowledge source, the agent will be able to extract\nrational interaction possibilities with the objects.\nREFERENCES\n[1] Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph Constrained\nReinforcement Learning for Natural Language Action Spaces. In International\nConference on Learning Representations.\nhttps:\/\/openreview.net\/forum?id=\nB1x6w0EtwH\n[2] Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, and Mark Riedl.\n2021. How to Avoid Being Eaten by a Grue: Structured Exploration Strategies\nfor Textual Worlds. https:\/\/openreview.net\/forum?id=eYgI3cTPTq9\n[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael\nLaskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision\nTransformer: Reinforcement Learning via Sequence Modeling. arXiv preprint\narXiv:2106.01345 (2021).\n[4] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes,\nEmery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri,\nMahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld: A Learning\nEnvironment for Text-based Games. CoRR abs\/1806.11532 (2018).\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL]\n[6] Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, and Shiyu\nChang. 2020. Interactive Fiction Game Playing as Multi-Paragraph Reading Com-\nprehension with Reinforcement Learning. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Online, 7755–7765. https:\/\/doi.org\/10.18653\/v1\/2020.\nemnlp-main.624\n[7] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and\nXingdi Yuan. 2020. Interactive Fiction Games: A Colossal Adventure. Proceedings\nof the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020), 7903–7910.\nhttps:\/\/doi.org\/10.1609\/aaai.v34i05.6297\n[8] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari\nOstendorf. 2016. Deep Reinforcement Learning with a Natural Language Action\nSpace. In Proceedings of the 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers). Association for Computational\nLinguistics, Berlin, Germany, 1621–1630. https:\/\/doi.org\/10.18653\/v1\/P16-1153\n[9] Michael Janner, Qiyang Li, and Sergey Levine. 2021. Reinforcement Learning as\nOne Big Sequence Modeling Problem. arXiv:2106.02039 [cs.LG]\n[10] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. arXiv:1909.11942 [cs.CL]\n[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]\n[12] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Sid-\ndhant Jayakumar, Max Jaderberg, Raphaël Lopez Kaufman, Aidan Clark, Seb\nNoury, Matthew Botvinick, Nicolas Heess, and Raia Hadsell. 2020. Stabiliz-\ning Transformers for Reinforcement Learning. In Proceedings of the 37th In-\nternational Conference on Machine Learning (Proceedings of Machine Learning\nResearch, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, 7487–7498.\nhttp:\/\/proceedings.mlr.press\/v119\/parisotto20a.html\n[13] Deepak\nPathak,\nPulkit\nAgrawal,\nAlexei\nA.\nEfros,\nand\nTrevor\nDar-\nrell. 2017.\nCuriosity-driven Exploration by Self-supervised Prediction.\narXiv:1705.05363 [cs.LG]\n[14] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Doha, Qatar, 1532–1543. https:\/\/doi.org\/10.3115\/v1\/\nD14-1162\n[15] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,\nYuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong,\nChina, 2463–2473. https:\/\/doi.org\/10.18653\/v1\/D19-1250\n[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nblog 1, 8 (2019), 9.\n[17] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs.CL]\n[18] Sushant Singh and Ausif Mahmood. 2021. The NLP Cookbook: Modern Recipes\nfor Transformer Based Deep Learning Architectures. IEEE Access 9 (2021), 68675–\n68702.\n[19] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction. MIT press.\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL]\n[21] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. arXiv preprint arXiv:1804.07461 (2018).\n[22] Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, and Chengqi Zhang. 2020. Deep\nReinforcement Learning with Transformers for Text Adventure Games. In 2020\nIEEE Conference on Games (CoG). 65–72. https:\/\/doi.org\/10.1109\/CoG47356.2020.\n9231622\n[23] Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi\nZhang. 2020. Deep Reinforcement Learning with Stacked Hierarchical Attention\nfor Text-based Games. In Advances in Neural Information Processing Systems,\nH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33.\nCurran Associates, Inc., 16495–16507.\nhttps:\/\/proceedings.neurips.cc\/paper\/\n2020\/file\/bf65417dcecc7f2b0006e1f5793b7143-Paper.pdf\n[24] Shunyu Yao, Karthik Narasimhan, and Matthew Hausknecht. 2021. Reading and\nActing while Blindfolded: The Need for Semantics in Text Game Agents. In North\nAmerican Association for Computational Linguistics (NAACL).\n[25] Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020.\nKeep CALM and Explore: Language Models for Action Generation in Text-based\nGames. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP). Association for Computational Linguistics, Online,\n8736–8754. https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.704\nAPPENDIX\nA\nLEARNING CURVES\nThe learning curves for Zork1 are shown in Figure 7.\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nLast100EpisodeScores\nrun1\nrun2\n(a)\n0\n100\n200\n300\n400\nSteps (x100)\n10\n20\n30\n40\n50\nMax score seen\nrun1\nrun2\n(b)\n0\n100\n200\n300\n400\nSteps (x100)\n0\n10\n20\n30\n40\n50\nEpisodeScore\nrun1\nrun2\n(c)\n0\n100\n200\n300\n400\nSteps (x100)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nLoss\nrun1\nrun2\n(d)\nFigure 7: Results for Zork1: Two independent runs of\n(DBERT-DRRN)\nB\nGAMEPLAY TRANSCRIPTS FOR TRAINED\nDBERT-DRRN\nThe transcripts presents the game-play over an episode of 100 steps.\nFor each step, the game state (observation, inventory, description),\nlist of valid actions and corresponding Q-Values estimated by the\nagent, the action taken, reward received, cumulative score, and\nwhether the episode is over (Done) is given.\nList of transcripts:\n• D.1 Zork1\n– D.1.1 DRRN (for comparison)\n• D.2 Inhumane\nB.1\nZork1\nSTATE 0\nCopyright ( c ) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\nZORK is a registered trademark of Infocom, Inc. Revision 88 \/ Se-\nrial number 840726 West of House You are standing in an open\nfield west of a white house, with a boarded front door. There is a\nsmall mailbox here. You are empty handed. West of House You are\nstanding in an open field west of a white house, with a boarded\nfront door. There is a small mailbox here. Actions: [’west’, ’north’,\n’south’, ’open mailbox’] Qvalues: [9.07, 7.74, 7.42, 5.98] Action: west\nReward: 0, Score 0, Done False\nSTATE 1\nforest this is a forest, with trees in all directions. to the east, there\nappears to be sunlight. you are empty handed. forest this is a forest,\nwith trees in all directions. to the east, there appears to be sunlight.\nActions: [’east’, ’go around trees’, ’north’, ’south’] Qvalues: [10.05,\n7.14, 6.8, 6.64] Action: south Reward: 0, Score 0, Done False\nSTATE 2\nforest this is a dimly lit forest, with large trees all around. you are\nempty handed. forest this is a dimly lit forest, with large trees all\naround. Actions: [’northwest’, ’north’, ’go around trees’, ’west’]\nQvalues: [6.59, 6.52, 6.1, 5.37] Action: northwest Reward: 0, Score 0,\nDone False\nSTATE 3\nsouth of house you are facing the south side of a white house. there\nis no door here, and all the windows are boarded. you are empty\nhanded. south of house you are facing the south side of a white\nhouse. there is no door here, and all the windows are boarded. Ac-\ntions: [’east’, ’south’, ’west’] Qvalues: [7.71, 6.15, 5.75] Action: east\nReward: 0, Score 0, Done False\nSTATE 4\nbehind house you are behind the white house. a path leads into the\nforest to the east. in one corner of the house there is a small window\nwhich is slightly ajar. you are empty handed. behind house you are\nbehind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is slightly\najar. Actions: [’open window’, ’east’, ’north’, ’south’] Qvalues: [9.39,\n7.99, 7.91, 7.4] Action: east Reward: 0, Score 0, Done False\nSTATE 5\nclearing you are in a small clearing in a well marked forest path\nthat extends to the east and west. you are empty handed. clearing\nyou are in a small clearing in a well marked forest path that extends\nto the east and west. Actions: [’west’, ’north’, ’east’, ’south’, ’go\naround forest’] Qvalues: [7.97, 6.53, 6.35, 6.21, 5.76] Action: north\nReward: 0, Score 0, Done False\nSTATE 6\nforest this is a dimly lit forest, with large trees all around. you are\nempty handed. you hear in the distance the chirping of a song bird.\nforest this is a dimly lit forest, with large trees all around. you hear\nin the distance the chirping of a song bird. Actions: [’south’, ’go\naround trees’, ’west’, ’east’] Qvalues: [6.83, 6.45, 6.22, 5.98] Action:\nwest Reward: 0, Score 0, Done False\nSTATE 7\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you are empty handed.\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you hear in the dis-\ntance the chirping of a song bird. Actions: [’up’, ’go around forest’,\n’south’, ’east’, ’north’, ’west’] Qvalues: [10.91, 8.65, 8.4, 7.61, 7.09,\n5.01] Action: up Reward: 0, Score 0, Done False\nSTATE 8\nup a tree you are about 10 feet above the ground nestled among\nsome large branches. the nearest branch above you is above your\nreach. beside you on the branch is a small bird’s nest. in the bird’s\nnest is a large egg encrusted with precious jewels, apparently scav-\nenged by a childless songbird. the egg is covered with fine gold\ninlay, and ornamented in lapis lazuli and mother of pearl. unlike\nmost eggs, this one is hinged and closed with a delicate looking\nclasp. the egg appears extremely fragile. you are empty handed. up\na tree you are about 10 feet above the ground nestled among some\nlarge branches. the nearest branch above you is above your reach.\nbeside you on the branch is a small bird’s nest. in the bird’s nest is\na large egg encrusted with precious jewels, apparently scavenged\nby a childless songbird. the egg is covered with fine gold inlay, and\nornamented in lapis lazuli and mother of pearl. unlike most eggs,\nthis one is hinged and closed with a delicate looking clasp. the egg\nappears extremely fragile. Actions: [’take egg’, ’close nest’, ’take\nnest’, ’take on egg’, ’down’] Qvalues: [11.59, 4.09, 2.25, 0.82, -4.72]\nAction: take egg Reward: 5, Score 5, Done False\nSTATE 9\ntaken. you are carrying : a jewel encrusted egg up a tree you are\nabout 10 feet above the ground nestled among some large branches.\nthe nearest branch above you is above your reach. beside you on the\nbranch is a small bird’s nest. you hear in the distance the chirping\nof a song bird. Actions: [’down’, ’close nest’, ’put egg in nest’, ’take\nnest’, ’take on egg’, ’throw egg at nest’, ’put down egg’] Qvalues:\n[8.07, 6.47, 5.62, 5.51, 5.29, 5.04, 4.88] Action: down Reward: 0, Score\n5, Done False\nSTATE 10\nforest path you are carrying : a jewel encrusted egg forest path this\nis a path winding through a dimly lit forest. the path heads north\nsouth here. one particularly large tree with some low branches\nstands at the edge of the path. you hear in the distance the chirping\nof a song bird. Actions: [’south’, ’go around forest’, ’north’, ’east’,\n’west’, ’open egg with all’, ’up’, ’take on egg’, ’throw egg at tree’,\n’put down egg’] Qvalues: [8.94, 8.82, 8.04, 7.9, 7.49, 7.45, 7.02, 6.23,\n6.19, 5.91] Action: east Reward: 0, Score 5, Done False\nSTATE 11\nforest you are carrying : a jewel encrusted egg you hear in the\ndistance the chirping of a song bird. forest this is a dimly lit for-\nest, with large trees all around. Actions: [’south’, ’go around trees’,\n’east’, ’west’, ’open egg with all’, ’throw egg at large’, ’take on egg’,\n’put down egg’] Qvalues: [8.72, 7.81, 7.81, 7.49, 7.4, 6.49, 6.14, 6.09]\nAction: go around trees Reward: 0, Score 5, Done False\nSTATE 12\nforest you are carrying : a jewel encrusted egg forest this is a dimly\nlit forest, with large trees all around. Actions: [’northwest’, ’north’,\n’go around trees’, ’west’, ’open egg with all’, ’take on egg’, ’throw\negg at large’, ’put down egg’] Qvalues: [8.56, 8.06, 7.97, 7.58, 7.44,\n6.38, 6.35, 6.14] Action: northwest Reward: 0, Score 5, Done False\nSTATE 13\nsouth of house you are carrying : a jewel encrusted egg south of\nhouse you are facing the south side of a white house. there is no\ndoor here, and all the windows are boarded. Actions: [’east’, ’south’,\n’open egg with all’, ’west’, ’take on egg’, ’put down egg’, ’throw egg\nat house’] Qvalues: [9.39, 8.03, 7.8, 7.47, 5.81, 5.77, 5.59] Action: east\nReward: 0, Score 5, Done False\nSTATE 14\nbehind house you are carrying : a jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nslightly ajar. Actions: [’open window’, ’open egg with all’, ’south’,\n’east’, ’north’, ’put down egg’, ’take on egg’, ’throw egg at path’]\nQvalues: [10.97, 9.79, 9.03, 8.74, 8.71, 7.78, 7.72, 6.85] Action: open\nwindow Reward: 0, Score 5, Done False\nSTATE 15\nwith great effort, you open the window far enough to allow entry.\nyou are carrying : a jewel encrusted egg behind house you are\nbehind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is open.\nActions: [’west’, ’close window’, ’open egg with all’, ’east’, ’south’,\n’north’, ’put down egg’, ’take on egg’, ’throw egg at path’] Qvalues:\n[11.56, 9.85, 9.56, 9.54, 9.12, 8.97, 7.34, 6.58, 6.2] Action: east Reward:\n0, Score 5, Done False\nSTATE 16\nclearing you are carrying : a jewel encrusted egg clearing you are\nin a small clearing in a well marked forest path that extends to the\neast and west. Actions: [’west’, ’open egg with all’, ’east’, ’south’,\n’north’, ’go around forest’, ’put down egg’, ’throw egg at forest’,\n’take on egg’] Qvalues: [9.27, 8.16, 8.13, 7.81, 7.76, 7.49, 6.35, 6.08,\n5.98] Action: west Reward: 0, Score 5, Done False\nSTATE 17\nbehind house you are carrying : a jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nopen. Actions: [’open egg with all’, ’close window’, ’west’, ’east’,\n’south’, ’north’, ’put down egg’, ’take on egg’, ’throw egg at path’]\nQvalues: [9.71, 9.64, 9.54, 8.65, 8.58, 8.54, 7.81, 7.27, 6.8] Action: west\nReward: 10, Score 15, Done False\nSTATE 18\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains : a\nquantity of water you are carrying : a jewel encrusted egg kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads\nto the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water Actions: [’west’, ’close window’, ’open sack’,\n’east’, ’open bottle’, ’take sack’, ’put down egg’, ’put egg on ta-\nble’, ’throw egg at bottle’, ’take on egg’, ’take bottle’, ’take all’, ’up’,\n’jump’] Qvalues: [21.67, 9.94, 9.52, 9.26, 8.74, 7.63, 7.6, 7.41, 6.13, 5.58,\n4.72, 3.89, -2.46, -11.57] Action: west Reward: 0, Score 15, Done False\nSTATE 19\nliving room you are in the living room. there is a doorway to the\neast, a wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a large oriental rug\nin the center of the room. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on the\ntrophy case. you are carrying : a jewel encrusted egg living room\nyou are in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a large oriental rug in the center\nof the room. above the trophy case hangs an elvish sword of great\nantiquity. a battery powered brass lantern is on the trophy case.\nActions: [’take lantern’, ’push rug’, ’open case’, ’turn on lantern’,\n’throw egg at case’, ’throw egg at lantern’, ’take sword’, ’put down\negg’, ’take on egg’, ’take all’, ’east’] Qvalues: [25.17, 25.08, 24.82,\n20.51, 20.26, 20.21, 17.84, 17.42, 17.37, 13.44, 13.38] Action: push rug\nReward: 0, Score 15, Done False\nSTATE 20\nwith a great effort, the rug is moved to one side of the room, re-\nvealing the dusty cover of a closed trap door. you are carrying : a\njewel encrusted egg living room you are in the living room. there is\na doorway to the east, a wooden door with strange gothic lettering\nto the west, which appears to be nailed shut, a trophy case, and a\nclosed trap door at your feet. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on the\ntrophy case. Actions: [’take lantern’, ’open case’, ’open trap’, ’turn\non lantern’, ’throw egg at case’, ’throw egg at lantern’, ’take sword’,\n’take on egg’, ’put down egg’, ’take all’, ’east’] Qvalues: [27.57, 26.49,\n26.36, 22.97, 22.6, 21.73, 19.57, 19.16, 19.04, 15.18, 13.41] Action: open\ncase Reward: 0, Score 15, Done False\nSTATE 21\nopened. you are carrying : a jewel encrusted egg living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity. a\nbattery powered brass lantern is on the trophy case. Actions: [’put\negg in case’, ’take lantern’, ’open trap’, ’close case’, ’turn on lantern’,\n’throw egg at case’, ’throw egg at lantern’, ’fill case with egg’, ’take\nsword’, ’take on egg’, ’put down egg’, ’take all’, ’east’] Qvalues:\n[27.91, 27.9, 25.05, 24.38, 23.28, 22.29, 21.63, 20.13, 19.53, 19.27, 18.89,\n15.31, 12.56] Action: take lantern Reward: 0, Score 15, Done False\nSTATE 22\ntaken. you are carrying : a brass lantern a jewel encrusted egg living\nroom you are in the living room. there is a doorway to the east,\na wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a closed trap door at\nyour feet. above the trophy case hangs an elvish sword of great\nantiquity. Actions: [’put egg in case’, ’open trap’, ’close case’, ’open\negg with lantern’, ’put all in case’, ’turn on lantern’, ’throw egg at\ncase’, ’put lantern in case’, ’put down lantern’, ’fill case with egg’,\n’take sword’, ’throw lantern at case’, ’take on egg’, ’throw lantern\nat egg’, ’put down all’, ’put down egg’, ’east’] Qvalues: [30.7, 28.07,\n27.18, 26.67, 24.57, 23.87, 22.85, 22.05, 21.95, 21.19, 20.45, 20.32, 19.95,\n19.49, 19.48, 19.32, 14.5] Action: put egg in case Reward: 5, Score\n20, Done False\nSTATE 23\ndone. you are carrying : a brass lantern living room you are in the\nliving room. there is a doorway to the east, a wooden door with\nstrange gothic lettering to the west, which appears to be nailed shut,\na trophy case, and a closed trap door at your feet. above the trophy\ncase hangs an elvish sword of great antiquity. your collection of\ntreasures consists of : a jewel encrusted egg Actions: [’open trap’,\n’close case’, ’open egg with lantern’, ’take egg from sword’, ’turn on\nlantern’, ’take egg’, ’put down lantern’, ’put lantern in case’, ’throw\nlantern at case’, ’take sword’, ’throw lantern at egg’, ’take on egg’,\n’east’] Qvalues: [30.74, 27.85, 27.84, 27.56, 25.61, 24.67, 24.36, 23.05,\n22.62, 22.33, 21.85, 21.0, 15.86] Action: open trap Reward: 0, Score\n20, Done False\nSTATE 24\nthe door reluctantly opens to reveal a rickety staircase descending\ninto darkness. you are carrying : a brass lantern living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a rug lying beside an open trap\ndoor. above the trophy case hangs an elvish sword of great antiq-\nuity. your collection of treasures consists of : a jewel encrusted egg\nActions: [’down’, ’open egg with lantern’, ’close case’, ’take egg\nfrom sword’, ’close trap’, ’turn on lantern’, ’take egg’, ’put down\nlantern’, ’put lantern in case’, ’throw lantern at case’, ’take sword’,\n’throw lantern at egg’, ’take on egg’, ’east’] Qvalues: [32.52, 27.82,\n27.35, 27.33, 26.53, 26.21, 24.31, 24.03, 23.0, 22.09, 21.81, 21.24, 20.86,\n15.54] Action: down Reward: 25, Score 45, Done False\nSTATE 25\nyou have moved into a dark place. the trap door crashes shut, and\nyou hear someone barring it. it is pitch black. you are likely to be\neaten by a grue. you are carrying : a brass lantern it is pitch black.\nyou are likely to be eaten by a grue. Actions: [’turn on lantern’, ’put\ndown lantern’, ’throw lantern at grue’, ’south’, ’east’, ’north’] Qval-\nues: [7.76, -1.33, -3.82, -4.74, -7.48, -10.65] Action: turn on lantern\nReward: 0, Score 45, Done False\nSTATE 26\nthe brass lantern is now on. cellar you are in a dark and damp cellar\nwith a narrow passageway leading north, and a crawlway to the\nsouth. on the west is the bottom of a steep metal ramp which is un-\nclimbable. you are carrying : a brass lantern ( providing light ) cellar\nyou are in a dark and damp cellar with a narrow passageway lead-\ning north, and a crawlway to the south. on the west is the bottom of\na steep metal ramp which is unclimbable. Actions: [’south’, ’put out\nlantern’, ’put down lantern’, ’north’, ’throw lantern at crawlway’]\nQvalues: [8.37, 4.86, 4.5, 4.2, 0.44] Action: south Reward: 0, Score\n45, Done False\nSTATE 27\neast of chasm you are on the east edge of a chasm, the bottom\nof which cannot be seen. a narrow passage goes north, and the\npath you are on continues to the east. you are carrying : a brass\nlantern ( providing light ) east of chasm you are on the east edge\nof a chasm, the bottom of which cannot be seen. a narrow passage\ngoes north, and the path you are on continues to the east. Actions:\n[’east’, ’north’, ’put out lantern’, ’put down lantern’, ’throw lantern\nat path’, ’jump’] Qvalues: [9.28, 4.51, 4.16, 3.46, 0.51, -7.56] Action:\neast Reward: 0, Score 45, Done False\nSTATE 28\ngallery this is an art gallery. most of the paintings have been stolen\nby vandals with exceptional taste. the vandals left through either\nthe north or west exits. fortunately, there is still one chance for\nyou to be a vandal, for on the far wall is a painting of unparalleled\nbeauty. you are carrying : a brass lantern ( providing light ) gallery\nthis is an art gallery. most of the paintings have been stolen by\nvandals with exceptional taste. the vandals left through either the\nnorth or west exits. fortunately, there is still one chance for you to\nbe a vandal, for on the far wall is a painting of unparalleled beauty.\nActions: [’take art’, ’put out lantern’, ’put down lantern’, ’throw\nlantern at art’, ’north’, ’west’] Qvalues: [10.35, 10.21, 8.89, 6.58, 6.06,\n3.92] Action: take art Reward: 4, Score 49, Done False\nSTATE 29\ntaken. you are carrying : a painting a brass lantern ( providing\nlight ) gallery this is an art gallery. most of the paintings have been\nstolen by vandals with exceptional taste. the vandals left through\neither the north or west exits. Actions: [’north’, ’put out lantern’,\n’put down lantern’, ’put down art’, ’west’, ’put down all’, ’throw art\nat lantern’] Qvalues: [7.74, 7.08, 6.64, 6.36, 6.36, 6.35, 4.02] Action:\nnorth Reward: 0, Score 49, Done False\nSTATE 30\nstudio this appears to have been an artist’s studio. the walls and\nfloors are splattered with paints of 69 different colors. strangely\nenough, nothing of value is hanging here. at the south end of the\nroom is an open door ( also covered with paint ). a dark and narrow\nchimney leads up from a fireplace ; although you might be able\nto get up it, it seems unlikely you could get back down. loosely\nattached to a wall is a small piece of paper. you are carrying : a\npainting a brass lantern ( providing light ) studio this appears to\nhave been an artist’s studio. the walls and floors are splattered with\npaints of 69 different colors. strangely enough, nothing of value is\nhanging here. at the south end of the room is an open door ( also\ncovered with paint ). a dark and narrow chimney leads up from a\nfireplace ; although you might be able to get up it, it seems unlikely\nyou could get back down. loosely attached to a wall is a small piece\nof paper. Actions: [’up’, ’put out lantern’, ’south’, ’put down lantern’,\n’take manual’, ’put down painting’, ’put down all’, ’throw lantern at\nmanual’] Qvalues: [8.6, 7.77, 6.81, 6.69, 6.59, 6.38, 6.37, 4.9] Action:\nput out lantern Reward: 0, Score 49, Done False\nSTATE 31\nthe brass lantern is now off. it is now pitch black. you are carrying\n: a painting a brass lantern it is pitch black. you are likely to be\neaten by a grue. Actions: [’up’, ’turn on lantern’, ’south’, ’put down\nlantern’, ’put down all’, ’put down painting’, ’throw lantern at grue’,\n’north’] Qvalues: [8.42, 5.97, 5.91, 4.04, 3.45, 3.42, 2.89, 1.35] Action:\nup Reward: 0, Score 49, Done False\nSTATE 32\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water you are carrying : a painting a brass lantern\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains :\na quantity of water Actions: [’west’, ’open bottle’, ’close window’,\n’open sack’, ’east’, ’put lantern on table’, ’take sack’, ’put down\nlantern’, ’throw lantern at bottle’, ’turn on lantern’, ’put down all’,\n’put down painting’, ’take bottle’, ’take all’, ’put painting on table’,\n’up’, ’jump’] Qvalues: [10.1, 9.01, 8.93, 8.91, 8.41, 8.23, 8.14, 8.08,\n7.97, 7.87, 7.63, 7.56, 7.29, 6.6, 6.56, 5.92, -2.09] Action: open bottle\nReward: 0, Score 49, Done False\nSTATE 33\nopened. you are carrying : a painting a brass lantern kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains :\na quantity of water Actions: [’west’, ’open sack’, ’close window’,\n’close bottle’, ’east’, ’put lantern on table’, ’take sack’, ’throw lantern\nat bottle’, ’put down lantern’, ’turn on lantern’, ’put down all’, ’put\ndown painting’, ’take bottle’, ’put painting on table’, ’take all’, ’up’,\n’jump’] Qvalues: [10.07, 9.15, 9.04, 8.95, 8.73, 8.54, 8.45, 8.35, 8.34,\n8.17, 7.95, 7.73, 7.63, 6.95, 6.95, 5.69, -2.51] Action: put lantern on\ntable Reward: 0, Score 49, Done False\nSTATE 34\ndone. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is\n: a brass lantern a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’take lantern’, ’open sack’, ’close bottle’,\n’east’, ’take sack’, ’close window’, ’throw painting at lantern’, ’turn\non lantern’, ’take bottle’, ’put down painting’, ’take all’, ’up’, ’jump’]\nQvalues: [9.0, 8.75, 8.31, 8.18, 8.11, 8.08, 8.02, 7.95, 7.84, 7.16, 7.12,\n6.95, 4.29, -4.68] Action: close bottle Reward: 0, Score 49, Done False\nSTATE 35\nclosed. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is\n: a brass lantern a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’take lantern’, ’open bottle’, ’open sack’,\n’close window’, ’take sack’, ’east’, ’throw painting at lantern’, ’turn\non lantern’, ’take bottle’, ’put down painting’, ’take all’, ’up’, ’jump’]\nQvalues: [9.01, 8.76, 8.58, 8.53, 8.18, 8.18, 8.17, 7.97, 7.94, 7.29, 7.17,\n7.02, 4.29, -4.77] Action: take lantern Reward: 0, Score 49, Done False\nSTATE 36\ntaken. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’west’, ’open bottle’, ’open sack’, ’close\nwindow’, ’east’, ’put lantern on table’, ’take sack’, ’throw lantern\nat bottle’, ’put down lantern’, ’turn on lantern’, ’put down all’, ’put\ndown painting’, ’take bottle’, ’take all’, ’put painting on table’, ’up’,\n’jump’] Qvalues: [9.82, 9.15, 9.08, 9.02, 8.61, 8.37, 8.31, 8.15, 8.15, 8.07,\n7.82, 7.61, 7.53, 6.82, 6.8, 6.16, -1.96] Action: open bottle Reward: 0,\nScore 49, Done False\nSTATE 37\nopened. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains :\na quantity of water Actions: [’west’, ’open sack’, ’close window’,\n’close bottle’, ’east’, ’put lantern on table’, ’take sack’, ’put down\nlantern’, ’throw lantern at bottle’, ’turn on lantern’, ’put down all’,\n’take bottle’, ’put down painting’, ’take all’, ’put painting on table’,\n’up’, ’jump’] Qvalues: [9.93, 9.02, 8.99, 8.76, 8.6, 8.48, 8.27, 8.14, 8.12,\n7.97, 7.84, 7.38, 7.38, 6.84, 6.52, 5.84, -2.44] Action: close window\nReward: 0, Score 49, Done False\nSTATE 38\nthe window closes ( more easily than it opened ). you are carrying :\na brass lantern a painting kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is slightly ajar. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is :\na glass bottle the glass bottle contains : a quantity of water Actions:\n[’open window’, ’west’, ’open sack’, ’close bottle’, ’put lantern on\ntable’, ’take sack’, ’put down lantern’, ’throw lantern at bottle’, ’turn\non lantern’, ’put down all’, ’put down painting’, ’take bottle’, ’take\nall’, ’put painting on table’, ’up’, ’jump’] Qvalues: [9.55, 9.15, 8.8,\n8.59, 8.13, 8.0, 7.89, 7.86, 7.71, 7.48, 7.33, 7.22, 6.5, 6.43, 6.4, -1.57]\nAction: put down all Reward: 0, Score 49, Done False\nSTATE 39\nbrass lantern : dropped. painting : dropped. you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis slightly ajar. a painting by a neglected genius is here. there is a\nbrass lantern ( battery powered ) here. on the table is an elongated\nbrown sack, smelling of hot peppers. sitting on the kitchen table is :\na glass bottle the glass bottle contains : a quantity of water Actions:\n[’open window’, ’open sack’, ’close bottle’, ’turn on lantern’, ’west’,\n’take painting’, ’take all’, ’take lantern’, ’take sack’, ’take bottle’,\n’take all from table’, ’up’, ’jump’] Qvalues: [6.81, 6.37, 6.11, 6.08,\n5.92, 5.9, 5.89, 5.83, 5.7, 5.25, 5.0, 2.44, -6.37] Action: open window\nReward: 0, Score 49, Done False\nSTATE 40\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be\nseen leading upward. a dark chimney leads down and to the east\nis a small window which is open. a painting by a neglected genius\nis here. there is a brass lantern ( battery powered ) here. on the\ntable is an elongated brown sack, smelling of hot peppers. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’open sack’, ’west’, ’close bottle’,\n’close window’, ’turn on lantern’, ’take sack’, ’take lantern’, ’take\nall’, ’take painting’, ’take bottle’, ’take all from table’, ’up’, ’jump’]\nQvalues: [6.76, 6.36, 6.29, 6.08, 6.06, 5.9, 5.59, 5.56, 5.48, 5.17, 5.14,\n4.9, 0.7, -8.47] Action: east Reward: 0, Score 49, Done False\nSTATE 41\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’west’, ’east’, ’south’] Qvalues: [7.41, 6.37,\n6.31, 6.13, 5.86] Action: west Reward: 0, Score 49, Done False\nSTATE 42\nkitchen a painting by a neglected genius is here. there is a brass\nlantern ( battery powered ) here. on the table is an elongated brown\nsack, smelling of hot peppers. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. a painting by a neglected genius is\nhere. there is a brass lantern ( battery powered ) here. on the table\nis an elongated brown sack, smelling of hot peppers. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’east’, ’open sack’, ’close window’, ’close\nbottle’, ’take lantern’, ’turn on lantern’, ’take sack’, ’take painting’,\n’take bottle’, ’take all’, ’take all from table’, ’up’, ’jump’] Qvalues:\n[9.43, 8.18, 7.32, 7.28, 6.96, 6.84, 6.75, 6.41, 5.8, 5.8, 5.79, 5.22, 1.47,\n-7.43] Action: take painting Reward: 0, Score 49, Done False\nSTATE 43\ntaken. you are carrying : a painting kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. there is a brass\nlantern ( battery powered ) here. on the table is an elongated brown\nsack, smelling of hot peppers. sitting on the kitchen table is : a glass\nbottle the glass bottle contains : a quantity of water Actions: [’take\nlantern’, ’west’, ’east’, ’open sack’, ’close bottle’, ’take sack’, ’close\nwindow’, ’turn on lantern’, ’throw painting at lantern’, ’take bottle’,\n’take all’, ’put down painting’, ’take all from table’, ’put painting\non table’, ’up’, ’jump’] Qvalues: [8.45, 8.2, 8.0, 7.88, 7.75, 7.68, 7.63,\n7.61, 7.42, 6.97, 6.81, 6.75, 6.24, 6.19, 5.3, -3.67] Action: take lantern\nReward: 0, Score 49, Done False\nSTATE 44\ntaken. you are carrying : a brass lantern a painting kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. on the table\nis an elongated brown sack, smelling of hot peppers. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’open sack’, ’close window’, ’close bottle’,\n’east’, ’put lantern on table’, ’take sack’, ’turn on lantern’, ’put down\nlantern’, ’throw lantern at bottle’, ’put down all’, ’take bottle’, ’put\ndown painting’, ’take all’, ’put painting on table’, ’up’, ’jump’] Qval-\nues: [9.89, 8.94, 8.93, 8.69, 8.64, 8.29, 8.14, 8.0, 8.0, 7.92, 7.64, 7.39,\n7.27, 6.79, 6.4, 6.21, -2.21] Action: west Reward: 0, Score 49, Done\nFalse\nSTATE 45\nliving room above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a jewel encrusted\negg you are carrying : a brass lantern a painting living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity. your\ncollection of treasures consists of : a jewel encrusted egg Actions:\n[’put painting in case’, ’put all in case’, ’put down painting’, ’take\ntreasures from sword’, ’put down all’, ’open egg with lantern’, ’put\ndown lantern’, ’put lantern in case’, ’take sword’, ’close case’, ’turn\non lantern’, ’throw lantern at case’, ’throw painting at treasures’,\n’open trap’, ’throw lantern at treasures’, ’take treasures’, ’take on\ntreasures’, ’east’] Qvalues: [10.96, 10.79, 10.25, 10.18, 9.93, 9.89, 9.77,\n9.74, 9.61, 9.59, 9.55, 9.5, 9.43, 9.4, 8.77, 8.09, 7.49, 6.26] Action: put\nall in case Reward: 6, Score 55, Done False\nSTATE 46\nbrass lantern : done. painting : done. you are empty handed. living\nroom you are in the living room. there is a doorway to the east,\na wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a closed trap door at\nyour feet. above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a painting a brass\nlantern a jewel encrusted egg Actions: [’east’, ’open trap’, ’turn on\nlantern’, ’take lantern’, ’close case’, ’take on egg’, ’take egg from\nsword’, ’take sword’, ’take painting’, ’take egg’, ’take all from case’]\nQvalues: [4.89, 4.27, 4.15, 3.98, 3.98, 3.72, 3.7, 3.69, 3.19, 3.05, 1.15]\nAction: east Reward: 0, Score 55, Done False\nSTATE 47\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. sitting on the kitchen table is : a glass bottle the glass\nbottle contains : a quantity of water you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. on the table is an elongated brown sack, smelling of hot pep-\npers. sitting on the kitchen table is : a glass bottle the glass bottle\ncontains : a quantity of water Actions: [’west’, ’east’, ’open sack’,\n’close window’, ’close bottle’, ’take sack’, ’take bottle’, ’take all’, ’up’,\n’jump’] Qvalues: [7.6, 7.38, 6.33, 6.29, 6.05, 5.44, 5.09, 4.99, 1.82, -7.47]\nAction: east Reward: 0, Score 55, Done False\nSTATE 48\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’west’, ’south’] Qvalues: [7.72, 6.65,\n6.54, 6.41, 6.24] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 49\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.61, 6.8, 6.53, 6.42] Action: open window\nReward: 0, Score 55, Done False\nSTATE 50\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [9.23, 7.55, 6.84, 6.76, 6.48]\nAction: west Reward: 0, Score 55, Done False\nSTATE 51\nkitchen on the table is an elongated brown sack, smelling of hot\npeppers. sitting on the kitchen table is : a glass bottle the glass\nbottle contains : a quantity of water you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. on the table is an elongated brown sack, smelling of hot pep-\npers. sitting on the kitchen table is : a glass bottle the glass bottle\ncontains : a quantity of water Actions: [’east’, ’open sack’, ’close\nbottle’, ’close window’, ’take sack’, ’take bottle’, ’take all’, ’west’,\n’up’, ’jump’] Qvalues: [7.0, 6.25, 5.99, 5.77, 5.28, 5.25, 5.2, 5.18, 3.37,\n-6.41] Action: take sack Reward: 0, Score 55, Done False\nSTATE 52\ntaken. you are carrying : a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’east’, ’put down sack’, ’west’,\n’take bottle’, ’close bottle’, ’open sack’, ’close window’, ’up’, ’jump’]\nQvalues: [5.79, 5.65, 5.52, 5.38, 5.1, 5.03, 5.0, 4.56, 3.88, -4.0] Action:\nput sack on table Reward: 0, Score 55, Done False\nSTATE 53\ndone. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table\nis : a brown sack a glass bottle the glass bottle contains : a quantity\nof water Actions: [’east’, ’open sack’, ’west’, ’close bottle’, ’close\nwindow’, ’take sack’, ’take bottle’, ’take all’, ’up’, ’jump’] Qvalues:\n[6.97, 5.93, 5.76, 5.66, 5.45, 5.06, 5.0, 4.82, 3.54, -5.91] Action: east\nReward: 0, Score 55, Done False\nSTATE 54\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’west’, ’south’] Qvalues: [7.59, 6.53,\n6.4, 6.29, 6.18] Action: south Reward: 0, Score 55, Done False\nSTATE 55\nsouth of house you are empty handed. south of house you are fac-\ning the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’east’, ’south’, ’west’] Qvalues:\n[6.57, 5.66, 4.74] Action: east Reward: 0, Score 55, Done False\nSTATE 56\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [7.37, 6.67,\n6.36, 6.29, 5.95] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 57\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.24, 6.57, 6.44, 6.12] Action: open window\nReward: 0, Score 55, Done False\nSTATE 58\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [8.37, 7.33, 6.58, 6.38, 6.19]\nAction: close window Reward: 0, Score 55, Done False\nSTATE 59\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [8.14, 6.58, 6.43, 6.16] Action: east Reward:\n0, Score 55, Done False\nSTATE 60\nclearing you are empty handed. clearing you are in a small clear-\ning in a well marked forest path that extends to the east and west.\nActions: [’west’, ’south’, ’north’, ’east’, ’go around forest’] Qvalues:\n[6.88, 5.64, 5.62, 5.59, 5.35] Action: south Reward: 0, Score 55, Done\nFalse\nSTATE 61\nforest you are empty handed. forest this is a dimly lit forest, with\nlarge trees all around. Actions: [’northwest’, ’north’, ’go around\ntrees’, ’west’] Qvalues: [6.36, 6.01, 5.76, 5.31] Action: northwest\nReward: 0, Score 55, Done False\nSTATE 62\nsouth of house you are empty handed. south of house you are fac-\ning the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’east’, ’south’, ’west’] Qvalues:\n[6.54, 5.42, 5.03] Action: east Reward: 0, Score 55, Done False\nSTATE 63\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’open window’, ’north’, ’south’, ’east’] Qvalues: [8.04, 6.16,\n6.05, 5.93] Action: open window Reward: 0, Score 55, Done False\nSTATE 64\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [8.2, 7.53, 7.02, 6.89, 6.46]\nAction: west Reward: 0, Score 55, Done False\nSTATE 65\nkitchen sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. sitting on the kitchen table is : a brown sack a glass bot-\ntle the glass bottle contains : a quantity of water Actions: [’east’,\n’west’, ’open sack’, ’close bottle’, ’close window’, ’take sack’, ’take\nbottle’, ’take all’, ’up’, ’jump’] Qvalues: [7.18, 6.16, 5.93, 5.53, 5.5,\n4.9, 4.69, 4.64, 2.8, -6.25] Action: west Reward: 0, Score 55, Done False\nSTATE 66\nliving room above the trophy case hangs an elvish sword of great\nantiquity. your collection of treasures consists of : a painting a brass\nlantern a jewel encrusted egg you are empty handed. living room\nyou are in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a closed trap door at your feet.\nabove the trophy case hangs an elvish sword of great antiquity.\nyour collection of treasures consists of : a painting a brass lantern\na jewel encrusted egg Actions: [’east’, ’open trap’, ’close case’, ’turn\non lantern’, ’take egg from sword’, ’take sword’, ’take on egg’, ’take\nlantern’, ’take egg’, ’take painting’, ’take all from case’] Qvalues:\n[5.2, 4.77, 4.54, 4.41, 4.18, 4.15, 4.09, 4.06, 3.41, 3.14, 1.54] Action:\neast Reward: 0, Score 55, Done False\nSTATE 67\nkitchen sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water you are empty handed.\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’east’, ’open\nsack’, ’close bottle’, ’west’, ’close window’, ’take all’, ’take sack’,\n’take bottle’, ’up’, ’jump’] Qvalues: [6.58, 5.57, 5.2, 5.13, 5.03, 4.57,\n4.56, 4.5, 3.0, -6.25] Action: open sack Reward: 0, Score 55, Done\nFalse\nSTATE 68\nopening the brown sack reveals a lunch, and a clove of garlic. you\nare empty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of food.\na passage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. sitting on the kitchen table is : a brown sack\nthe brown sack contains : a lunch a clove of garlic a glass bottle the\nglass bottle contains : a quantity of water Actions: [’east’, ’west’,\n’close sack’, ’close bottle’, ’close window’, ’take all from sack’, ’take\nsack’, ’take garlic’, ’take food’, ’take all’, ’take bottle’, ’up’, ’jump’]\nQvalues: [6.66, 5.33, 5.29, 5.0, 4.84, 4.49, 4.48, 4.46, 4.45, 4.26, 4.17,\n2.18, -6.95] Action: close bottle Reward: 0, Score 55, Done False\nSTATE 69\nclosed. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table is\n: a brown sack the brown sack contains : a lunch a clove of garlic a\nglass bottle the glass bottle contains : a quantity of water Actions:\n[’east’, ’open bottle’, ’close sack’, ’west’, ’take all from sack’, ’take\nsack’, ’close window’, ’take food’, ’take garlic’, ’take bottle’, ’take\nall’, ’up’, ’jump’] Qvalues: [6.26, 5.2, 5.13, 4.97, 4.6, 4.55, 4.52, 4.43,\n4.39, 4.33, 4.22, 2.04, -7.58] Action: take all from sack Reward: 0,\nScore 55, Done False\nSTATE 70\nlunch : taken. clove of garlic : taken. you are carrying : a clove of\ngarlic a lunch kitchen you are in the kitchen of the white house. a\ntable seems to have been used recently for the preparation of food.\na passage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. sitting on the kitchen table is : a brown sack a\nglass bottle the glass bottle contains : a quantity of water Actions:\n[’put all in sack’, ’put all on table’, ’put down all’, ’put food on table’,\n’put garlic on table’, ’put down food’, ’west’, ’east’, ’put food in sack’,\n’take sack’, ’open bottle’, ’put garlic in sack’, ’put down garlic’, ’close\nsack’, ’take bottle’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues:\n[5.74, 5.73, 5.59, 5.43, 5.38, 5.34, 5.34, 5.3, 5.27, 5.26, 5.25, 5.18, 5.16,\n4.96, 4.91, 4.61, 4.48, 4.17, -2.97] Action: put down garlic Reward: 0,\nScore 55, Done False\nSTATE 71\ndropped. you are carrying : a lunch kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to\nthe east is a small window which is open. there is a clove of garlic\nhere. sitting on the kitchen table is : a brown sack a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’put food\non table’, ’put down food’, ’east’, ’west’, ’take garlic’, ’put food in\nsack’, ’take sack’, ’open bottle’, ’close sack’, ’take bottle’, ’take all’,\n’close window’, ’take all from table’, ’up’, ’jump’] Qvalues: [5.24,\n5.24, 5.21, 5.16, 5.11, 4.98, 4.98, 4.86, 4.77, 4.57, 4.45, 4.25, 4.16, 2.27,\n-6.13] Action: put food in sack Reward: 0, Score 55, Done False\nSTATE 72\ndone. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a clove of garlic here.\nsitting on the kitchen table is : a brown sack the brown sack con-\ntains : a lunch a glass bottle the glass bottle contains : a quantity of\nwater Actions: [’east’, ’close sack’, ’open bottle’, ’west’, ’take food’,\n’take garlic’, ’take sack’, ’take all’, ’close window’, ’take bottle’, ’take\nall from table’, ’up’, ’jump’] Qvalues: [6.19, 5.16, 5.15, 4.86, 4.61, 4.54,\n4.51, 4.48, 4.48, 4.37, 3.97, 2.19, -7.34] Action: take sack Reward: 0,\nScore 55, Done False\nSTATE 73\ntaken. you are carrying : a brown sack the brown sack contains :\na lunch kitchen you are in the kitchen of the white house. a table\nseems to have been used recently for the preparation of food. a\npassage leads to the west and a dark staircase can be seen leading\nupward. a dark chimney leads down and to the east is a small win-\ndow which is open. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’put food on table’, ’put down\nsack’, ’put down food’, ’west’, ’take food’, ’open bottle’, ’shake sack’,\n’take garlic’, ’east’, ’close sack’, ’take bottle’, ’take all’, ’close win-\ndow’, ’up’, ’jump’] Qvalues: [5.56, 5.37, 5.37, 5.31, 5.31, 5.22, 5.13,\n5.13, 5.1, 5.01, 4.91, 4.82, 4.57, 4.44, 3.26, -4.31] Action: shake sack\nReward: 0, Score 55, Done False\nSTATE 74\nthe contents of the brown sack spills to the ground. you are carrying\n: a brown sack kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. a hot pepper sandwich is here. there\nis a clove of garlic here. sitting on the kitchen table is : a glass bottle\nthe glass bottle contains : a quantity of water Actions: [’put sack\non table’, ’west’, ’put down sack’, ’take food’, ’east’, ’take garlic’,\n’open bottle’, ’take bottle’, ’close sack’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.61, 5.48, 5.42, 5.32, 5.28, 5.14, 5.1, 5.01, 4.85,\n4.49, 4.37, 3.37, -4.37] Action: close sack Reward: 0, Score 55, Done\nFalse\nSTATE 75\nclosed. you are carrying : a brown sack kitchen you are in the\nkitchen of the white house. a table seems to have been used re-\ncently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put sack on table’, ’west’, ’put down sack’, ’take\nfood’, ’east’, ’take garlic’, ’open bottle’, ’open sack’, ’take bottle’,\n’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.67, 5.52, 5.47, 5.4,\n5.32, 5.26, 5.2, 5.01, 4.97, 4.7, 4.48, 2.85, -4.86] Action: put down sack\nReward: 0, Score 55, Done False\nSTATE 76\ndropped. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a brown sack here. a\nhot pepper sandwich is here. there is a clove of garlic here. sitting\non the kitchen table is : a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’west’, ’open sack’, ’open bottle’,\n’take sack’, ’take food’, ’take garlic’, ’take all’, ’close window’, ’take\nbottle’, ’up’, ’jump’] Qvalues: [6.33, 5.31, 5.27, 5.17, 4.77, 4.69, 4.63,\n4.58, 4.57, 4.47, 1.86, -7.42] Action: east Reward: 0, Score 55, Done\nFalse\nSTATE 77\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [6.43, 5.68,\n5.43, 5.25, 5.15] Action: close window Reward: 0, Score 55, Done\nFalse\nSTATE 78\nthe window closes ( more easily than it opened ). you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’open window’, ’north’,\n’south’, ’east’] Qvalues: [7.54, 6.26, 5.95, 5.91] Action: north Reward:\n0, Score 55, Done False\nSTATE 79\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. you are empty handed. north\nof house you are facing the north side of a white house. there is\nno door here, and all the windows are boarded up. to the north\na narrow path winds through the trees. Actions: [’east’, ’north’,\n’west’] Qvalues: [7.13, 6.52, 5.61] Action: east Reward: 0, Score 55,\nDone False\nSTATE 80\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’open window’, ’north’, ’south’, ’east’] Qvalues: [6.88, 5.47,\n5.28, 5.27] Action: open window Reward: 0, Score 55, Done False\nSTATE 81\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’west’, ’close\nwindow’, ’north’, ’east’, ’south’] Qvalues: [7.23, 6.18, 5.72, 5.56, 5.36]\nAction: west Reward: 0, Score 55, Done False\nSTATE 82\nkitchen there is a brown sack here. a hot pepper sandwich is here.\nthere is a clove of garlic here. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a brown sack here. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’west’, ’east’, ’open sack’, ’open bottle’, ’close\nwindow’, ’take garlic’, ’take food’, ’take sack’, ’take bottle’, ’take all’,\n’up’, ’jump’] Qvalues: [8.05, 7.66, 6.21, 5.98, 5.91, 5.52, 5.44, 5.4, 4.96,\n4.93, 2.11, -6.64] Action: east Reward: 0, Score 55, Done False\nSTATE 83\nbehind house you are empty handed. behind house you are behind\nthe white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is open. Actions:\n[’close window’, ’north’, ’east’, ’south’, ’west’] Qvalues: [6.52, 6.0,\n5.7, 5.69, 5.22] Action: west Reward: 0, Score 55, Done False\nSTATE 84\nkitchen there is a brown sack here. a hot pepper sandwich is here.\nthere is a clove of garlic here. sitting on the kitchen table is : a\nglass bottle the glass bottle contains : a quantity of water you are\nempty handed. kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a brown sack here. a hot pep-\nper sandwich is here. there is a clove of garlic here. sitting on the\nkitchen table is : a glass bottle the glass bottle contains : a quantity\nof water Actions: [’east’, ’west’, ’open sack’, ’open bottle’, ’close\nwindow’, ’take garlic’, ’take food’, ’take sack’, ’take bottle’, ’take\nall’, ’up’, ’jump’] Qvalues: [7.35, 7.25, 6.16, 6.01, 5.81, 5.52, 5.44, 5.41,\n5.16, 5.06, 2.48, -6.1] Action: take food Reward: 0, Score 55, Done\nFalse\nSTATE 85\ntaken. you are carrying : a lunch kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a brown sack here.\nthere is a clove of garlic here. sitting on the kitchen table is : a glass\nbottle the glass bottle contains : a quantity of water Actions: [’put\ndown food’, ’put food on table’, ’east’, ’take garlic’, ’take sack’, ’west’,\n’open bottle’, ’open sack’, ’take bottle’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.65, 5.57, 5.53, 5.48, 5.47, 5.43, 5.25, 5.19, 5.13,\n4.96, 4.56, 3.21, -4.72] Action: take garlic Reward: 0, Score 55, Done\nFalse\nSTATE 86\ntaken. you are carrying : a clove of garlic a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. there is a\nbrown sack here. sitting on the kitchen table is : a glass bottle the\nglass bottle contains : a quantity of water Actions: [’put all on table’,\n’put down all’, ’west’, ’put food on table’, ’take sack’, ’put down\nfood’, ’put garlic on table’, ’east’, ’open bottle’, ’put down garlic’,\n’take bottle’, ’open sack’, ’take all’, ’close window’, ’up’, ’jump’]\nQvalues: [5.93, 5.82, 5.62, 5.59, 5.57, 5.56, 5.51, 5.47, 5.45, 5.42, 5.32,\n5.28, 4.87, 4.68, 3.83, -3.17] Action: put all on table Reward: 0, Score\n55, Done False\nSTATE 87\nclove of garlic : done. lunch : done. you are empty handed. kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nopen. there is a brown sack here. sitting on the kitchen table is :\na lunch a clove of garlic a glass bottle the glass bottle contains : a\nquantity of water Actions: [’east’, ’west’, ’open sack’, ’open bottle’,\n’close window’, ’take garlic’, ’take food’, ’take sack’, ’take all’, ’take\nbottle’, ’take all from table’, ’up’, ’jump’] Qvalues: [6.17, 5.34, 5.27,\n5.11, 4.66, 4.62, 4.55, 4.55, 4.39, 4.28, 3.72, 2.16, -6.62] Action: take\nsack Reward: 0, Score 55, Done False\nSTATE 88\ntaken. you are carrying : a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table\nis : a lunch a clove of garlic a glass bottle the glass bottle contains :\na quantity of water Actions: [’put sack on table’, ’put down sack’,\n’west’, ’take food’, ’east’, ’take garlic’, ’open bottle’, ’take bottle’,\n’open sack’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.72,\n5.61, 5.46, 5.46, 5.37, 5.31, 5.26, 5.25, 5.13, 4.8, 4.53, 3.63, -3.52] Action:\ntake food Reward: 0, Score 55, Done False\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. sitting on the kitchen table is\n: a clove of garlic a glass bottle the glass bottle contains : a quantity\nof water Actions: [’put all on table’, ’put down all’, ’put sack on\ntable’, ’put down sack’, ’west’, ’put down food’, ’put food on table’,\n’take garlic’, ’east’, ’open bottle’, ’open sack’, ’take bottle’, ’take all’,\n’close window’, ’up’, ’jump’] Qvalues: [5.32, 5.29, 5.25, 5.12, 5.09,\n5.06, 5.05, 4.91, 4.9, 4.79, 4.66, 4.56, 4.33, 4.07, 3.08, -4.21] Action:\ntake bottle Reward: 0, Score 55, Done False\nSTATE 90\ntaken. you are carrying : a glass bottle the glass bottle contains :\na quantity of water a lunch a brown sack kitchen you are in the\nkitchen of the white house. a table seems to have been used recently\nfor the preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. sitting on the\nkitchen table is : a clove of garlic Actions: [’put all on table’, ’put\ndown bottle’, ’put down all’, ’put bottle on table’, ’east’, ’put sack\non table’, ’west’, ’put down sack’, ’put down food’, ’put food on\ntable’, ’open bottle’, ’put water on table’, ’open sack’, ’take garlic’,\n’up’, ’close window’, ’jump’] Qvalues: [5.07, 5.04, 5.02, 5.0, 4.88, 4.87,\n4.82, 4.74, 4.7, 4.62, 4.59, 4.55, 4.35, 4.34, 3.94, 3.75, -2.22] Action: put\ndown bottle Reward: 0, Score 55, Done False\nSTATE 91\ndropped. you are carrying : a lunch a brown sack kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a glass bottle here. the glass bottle contains : a quantity of water\nsitting on the kitchen table is : a clove of garlic Actions: [’put all\non table’, ’put sack on table’, ’put down sack’, ’put down all’, ’west’,\n’put down food’, ’put food on table’, ’east’, ’take garlic’, ’open bottle’,\n’open sack’, ’take bottle’, ’take all’, ’close window’, ’up’, ’jump’]\nQvalues: [5.43, 5.42, 5.32, 5.3, 5.3, 5.25, 5.17, 5.02, 4.94, 4.83, 4.66,\n4.62, 4.28, 4.06, 3.09, -4.1] Action: put down sack Reward: 0, Score\n55, Done False\nSTATE 92\ndropped. you are carrying : a lunch kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is open. there is a brown\nsack here. there is a glass bottle here. the glass bottle contains : a\nquantity of water sitting on the kitchen table is : a clove of garlic\nActions: [’east’, ’put down food’, ’west’, ’put food on table’, ’take\nsack’, ’take garlic’, ’open bottle’, ’open sack’, ’take bottle’, ’take all’,\n’close window’, ’up’, ’jump’] Qvalues: [5.35, 5.25, 5.23, 5.12, 4.97,\n4.97, 4.73, 4.73, 4.72, 4.69, 4.11, 3.21, -4.36] Action: east Reward: 0,\nScore 55, Done False\nSTATE 93\nbehind house you are carrying : a lunch behind house you are be-\nhind the white house. a path leads into the forest to the east. in\none corner of the house there is a small window which is open.\nActions: [’put down lunch’, ’north’, ’close window’, ’south’, ’east’,\n’west’] Qvalues: [5.7, 4.52, 4.49, 4.3, 4.28, 4.03] Action: close window\nReward: 0, Score 55, Done False\nSTATE 94\nthe window closes ( more easily than it opened ). you are carrying :\na lunch behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. Actions: [’put down lunch’, ’open\nwindow’, ’south’, ’east’, ’north’] Qvalues: [5.45, 4.72, 4.17, 4.1, 3.96]\nAction: south Reward: 0, Score 55, Done False\nSTATE 95\nsouth of house you are carrying : a lunch south of house you are\nfacing the south side of a white house. there is no door here, and all\nthe windows are boarded. Actions: [’put down lunch’, ’east’, ’south’,\n’west’] Qvalues: [4.75, 4.42, 4.13, 3.16] Action: east Reward: 0, Score\n55, Done False\nSTATE 96\nbehind house you are carrying : a lunch behind house you are be-\nhind the white house. a path leads into the forest to the east. in one\ncorner of the house there is a small window which is slightly ajar.\nActions: [’put down lunch’, ’open window’, ’south’, ’north’, ’east’]\nQvalues: [5.48, 4.63, 4.06, 4.03, 3.82] Action: open window Reward:\n0, Score 55, Done False\nSTATE 97\nwith great effort, you open the window far enough to allow entry.\nyou are carrying : a lunch behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is open. Actions: [’put down\nlunch’, ’west’, ’south’, ’north’, ’east’, ’close window’] Qvalues: [5.21,\n4.88, 4.3, 4.26, 4.1, 3.92] Action: west Reward: 0, Score 55, Done False\nSTATE 98\nkitchen there is a brown sack here. there is a glass bottle here. the\nglass bottle contains : a quantity of water sitting on the kitchen\ntable is : a clove of garlic you are carrying : a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a brown sack here. there is a glass bottle here. the glass bottle\ncontains : a quantity of water sitting on the kitchen table is : a clove\nof garlic Actions: [’put down food’, ’put food on table’, ’west’, ’take\nsack’, ’take garlic’, ’east’, ’open bottle’, ’open sack’, ’take bottle’,\n’take all’, ’close window’, ’up’, ’jump’] Qvalues: [4.8, 4.68, 4.6, 4.58,\n4.46, 4.44, 4.44, 4.38, 4.36, 4.0, 3.7, 3.02, -4.39] Action: take garlic\nReward: 0, Score 55, Done False\nSTATE 99\ntaken. you are carrying : a clove of garlic a lunch kitchen you are\nin the kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west\nand a dark staircase can be seen leading upward. a dark chimney\nleads down and to the east is a small window which is open. there\nis a brown sack here. there is a glass bottle here. the glass bottle\ncontains : a quantity of water Actions: [’put down all’, ’put all on\ntable’, ’put down food’, ’west’, ’put food on table’, ’put garlic on\ntable’, ’take sack’, ’put down garlic’, ’open bottle’, ’east’, ’open sack’,\n’take bottle’, ’take all’, ’close window’, ’up’, ’jump’] Qvalues: [5.33,\n5.29, 5.05, 5.04, 4.98, 4.92, 4.91, 4.81, 4.79, 4.65, 4.61, 4.54, 4.26, 3.94,\n3.29, -3.82] Action: put down food Reward: 0, Score 55, Done True\nSTATE 100\ndropped. you are carrying : a clove of garlic kitchen you are in\nthe kitchen of the white house. a table seems to have been used\nrecently for the preparation of food. a passage leads to the west and\na dark staircase can be seen leading upward. a dark chimney leads\ndown and to the east is a small window which is open. a hot pepper\nsandwich is here. there is a brown sack here. there is a glass bottle\nhere. the glass bottle contains : a quantity of water Actions: [’west’,\n’put garlic on table’, ’take sack’, ’open bottle’, ’take food’, ’put down\ngarlic’, ’open sack’, ’take bottle’, ’east’, ’take all’, ’close window’,\n’up’, ’jump’] Qvalues: [5.1, 4.79, 4.73, 4.68, 4.67, 4.65, 4.57, 4.32, 4.3,\n3.95, 3.91, 2.32, -5.11] Action: reset Reward: 0, Score 0, Done False\nB.1.1\nDRRN (for comparison). STATE 0\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\nZORK is a registered trademark of Infocom, Inc. Revision 88 \/ Se-\nrial number 840726 West of House You are standing in an open\nfield west of a white house, with a boarded front door. There is a\nsmall mailbox here. You are empty handed. West of House You are\nstanding in an open field west of a white house, with a boarded\nfront door. There is a small mailbox here. Actions: [’north’, ’south’,\n’west’, ’open mailbox’] Qvalues: [9.25, 8.75, 8.4, 8.11] Action: north\nReward: 0, Score 0, Done False\nSTATE 1\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. you are empty handed. north\nof house you are facing the north side of a white house. there is\nno door here, and all the windows are boarded up. to the north\na narrow path winds through the trees. Actions: [’north’, ’east’,\n’west’] Qvalues: [10.23, 10.22, 8.8] Action: north Reward: 0, Score 0,\nDone False\nSTATE 2\nforest path this is a path winding through a dimly lit forest. the\npath heads north south here. one particularly large tree with some\nlow branches stands at the edge of the path. you are empty handed.\nforest path this is a path winding through a dimly lit forest. the path\nheads north south here. one particularly large tree with some low\nbranches stands at the edge of the path. Actions: [’up’, ’go around\nforest’, ’south’, ’east’, ’north’, ’west’] Qvalues: [11.3, 9.84, 9.53, 8.64,\n7.8, 5.7] Action: up Reward: 0, Score 0, Done False\nSTATE 3\nup a tree you are about 10 feet above the ground nestled among\nsome large branches. the nearest branch above you is above your\nreach. beside you on the branch is a small bird’s nest. in the bird’s\nnest is a large egg encrusted with precious jewels, apparently scav-\nenged by a childless songbird. the egg is covered with fine gold\ninlay, and ornamented in lapis lazuli and mother of pearl. unlike\nmost eggs, this one is hinged and closed with a delicate looking\nclasp. the egg appears extremely fragile. you are empty handed. up\na tree you are about 10 feet above the ground nestled among some\nlarge branches. the nearest branch above you is above your reach.\nbeside you on the branch is a small bird’s nest. in the bird’s nest is\na large egg encrusted with precious jewels, apparently scavenged\nby a childless songbird. the egg is covered with fine gold inlay, and\nornamented in lapis lazuli and mother of pearl. unlike most eggs,\nthis one is hinged and closed with a delicate looking clasp. the egg\nappears extremely fragile. Actions: [’take egg’, ’take on egg’, ’take\nnest’, ’close nest’, ’down’] Qvalues: [12.46, 10.87, 10.22, 9.65, 5.91]\nAction: take egg Reward: 5, Score 5, Done False\nSTATE 4\ntaken. you are carrying: a jewel encrusted egg you hear in the dis-\ntance the chirping of a song bird. up a tree you are about 10 feet\nabove the ground nestled among some large branches. the nearest\nbranch above you is above your reach. beside you on the branch is\na small bird’s nest. you hear in the distance the chirping of a song\nbird. Actions: [’take on egg’, ’put egg in nest’, ’down’, ’take nest’,\n’put down egg’, ’close nest’, ’throw egg at nest’] Qvalues: [8.24, 8.11,\n7.77, 7.37, 7.34, 7.27, 6.01] Action: down Reward: 0, Score 5, Done\nFalse\nSTATE 5\nforest path you are carrying: a jewel encrusted egg forest path this\nis a path winding through a dimly lit forest. the path heads north\nsouth here. one particularly large tree with some low branches\nstands at the edge of the path. you hear in the distance the chirping\nof a song bird. Actions: [’south’, ’take on egg’, ’up’, ’open egg with\nall’, ’go around forest’, ’put down egg’, ’east’, ’north’, ’throw egg\nat tree’, ’west’] Qvalues: [9.33, 9.13, 9.02, 8.27, 8.21, 7.74, 7.41, 6.32,\n4.05, 3.81] Action: south Reward: 0, Score 5, Done False\nSTATE 6\nnorth of house you are carrying: a jewel encrusted egg north of\nhouse you are facing the north side of a white house. there is no\ndoor here, and all the windows are boarded up. to the north a nar-\nrow path winds through the trees. Actions: [’east’, ’take on egg’,\n’put down egg’, ’north’, ’open egg with all’, ’west’, ’throw egg at\npath’] Qvalues: [10.13, 8.82, 7.17, 6.75, 6.43, 4.81, 3.01] Action: east\nReward: 0, Score 5, Done False\nSTATE 7\nbehind house you are behind the white house. a path leads into\nthe forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. you are carrying: a jewel encrusted\negg behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a\nsmall window which is slightly ajar. Actions: [’take on egg’, ’open\nwindow’, ’put down egg’, ’open egg with all’, ’north’, ’east’, ’south’,\n’throw egg at path’] Qvalues: [11.34, 10.97, 10.23, 10.2, 8.65, 8.62,\n8.47, 5.23] Action: put down egg Reward: 0, Score 5, Done False\nSTATE 8\ndropped. you are empty handed. behind house you are behind the\nwhite house. a path leads into the forest to the east. in one corner\nof the house there is a small window which is slightly ajar. there\nis a jewel encrusted egg here. Actions: [’take egg’, ’take on egg’,\n’open window’, ’north’, ’east’, ’south’] Qvalues: [11.04, 10.84, 9.91,\n9.62, 9.43, 9.09] Action: north Reward: 0, Score 5, Done False\nSTATE 9\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.09, 8.65, 6.82]\nAction: east Reward: 0, Score 5, Done False\nSTATE 10\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’take on egg’, ’north’, ’open window’, ’south’,\n’east’] Qvalues: [10.77, 10.26, 10.26, 10.17, 10.01, 9.07] Action: north\nReward: 0, Score 5, Done False\nSTATE 11\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.04, 8.6, 6.73]\nAction: north Reward: 0, Score 5, Done False\nSTATE 12\nforest path you are empty handed. forest path this is a path winding\nthrough a dimly lit forest. the path heads north south here. one\nparticularly large tree with some low branches stands at the edge\nof the path. Actions: [’up’, ’south’, ’go around forest’, ’east’, ’north’,\n’west’] Qvalues: [9.02, 8.89, 8.28, 7.72, 7.19, 4.43] Action: south Re-\nward: 0, Score 5, Done False\nSTATE 13\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [10.03, 8.58, 6.68]\nAction: east Reward: 0, Score 5, Done False\nSTATE 14\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’north’, ’take on egg’, ’open window’, ’south’,\n’east’] Qvalues: [10.64, 10.2, 10.18, 10.07, 9.98, 9.04] Action: north\nReward: 0, Score 5, Done False\nSTATE 15\nnorth of house you are empty handed. north of house you are facing\nthe north side of a white house. there is no door here, and all the\nwindows are boarded up. to the north a narrow path winds through\nthe trees. Actions: [’east’, ’north’, ’west’] Qvalues: [9.99, 8.53, 6.58]\nAction: east Reward: 0, Score 5, Done False\nSTATE 16\nbehind house there is a jewel encrusted egg here. you are empty\nhanded. behind house you are behind the white house. a path leads\ninto the forest to the east. in one corner of the house there is a small\nwindow which is slightly ajar. there is a jewel encrusted egg here.\nActions: [’take egg’, ’north’, ’take on egg’, ’open window’, ’south’,\n’east’] Qvalues: [10.54, 10.14, 10.09, 9.99, 9.93, 8.99] Action: open\nwindow Reward: 0, Score 5, Done False\nSTATE 17\nwith great effort, you open the window far enough to allow entry.\nyou are empty handed. behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of\nthe house there is a small window which is open. there is a jewel\nencrusted egg here. Actions: [’take egg’, ’take on egg’, ’west’, ’close\nwindow’, ’east’, ’south’, ’north’] Qvalues: [11.46, 9.71, 9.65, 8.11,\n6.49, 6.19, 5.6] Action: take egg Reward: 0, Score 5, Done False\nSTATE 18\ntaken. you are carrying: a jewel encrusted egg behind house you\nare behind the white house. a path leads into the forest to the east.\nin one corner of the house there is a small window which is open.\nActions: [’take on egg’, ’west’, ’north’, ’open egg with all’, ’put down\negg’, ’close window’, ’south’, ’east’, ’throw egg at path’] Qvalues:\n[10.51, 10.42, 10.08, 9.93, 9.81, 9.6, 9.4, 9.12, 6.64] Action: north Re-\nward: 0, Score 5, Done False\nSTATE 19\nnorth of house you are carrying: a jewel encrusted egg north of\nhouse you are facing the north side of a white house. there is no\ndoor here, and all the windows are boarded up. to the north a nar-\nrow path winds through the trees. Actions: [’east’, ’take on egg’,\n’put down egg’, ’north’, ’open egg with all’, ’west’, ’throw egg at\npath’] Qvalues: [9.59, 8.34, 6.73, 6.21, 5.94, 4.12, 2.61] Action: take\non egg Reward: 0, Score 5, Done False\nSTATE 20\nthere is a noticeable crunch from beneath you, and inspection re-\nveals that the egg is lying open, badly damaged. there is a golden\nclockwork canary nestled in the egg. it seems to have recently\nhad a bad experience. the mountings for its jewel like eyes are\nempty, and its silver beak is crumpled. through a cracked crystal\nwindow below its left wing you can see the remains of intricate\nmachinery. it is not clear what result winding it would have, as\nthe mainspring seems sprung. you are carrying: a broken jewel\nencrusted egg the broken jewel encrusted egg contains: a broken\nclockwork canary north of house you are facing the north side\nof a white house. there is no door here, and all the windows are\nboarded up. to the north a narrow path winds through the trees.\nActions: [’take canary’, ’shake egg’, ’close egg’, ’north’, ’east’, ’put\ndown canary’, ’put down egg’, ’west’] Qvalues: [10.64, 10.34, 10.01,\n9.8, 9.8, 9.68, 9.67, 8.01] Action: north Reward: 0, Score 5, Done False\nSTATE 21\nforest path you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nforest path this is a path winding through a dimly lit forest. the path\nheads north south here. one particularly large tree with some low\nbranches stands at the edge of the path. you hear in the distance\nthe chirping of a song bird. Actions: [’south’, ’take canary’, ’up’,\n’shake egg’, ’put down canary’, ’east’, ’close egg’, ’go around forest’,\n’north’, ’put down egg’, ’west’] Qvalues: [11.15, 9.93, 9.35, 9.24, 8.89,\n8.88, 8.79, 8.74, 8.22, 8.19, 5.5] Action: south Reward: 0, Score 5,\nDone False\nSTATE 22\nnorth of house you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nnorth of house you are facing the north side of a white house. there\nis no door here, and all the windows are boarded up. to the north a\nnarrow path winds through the trees. Actions: [’east’, ’shake egg’,\n’take canary’, ’close egg’, ’north’, ’put down canary’, ’put down egg’,\n’west’] Qvalues: [11.93, 10.06, 9.78, 9.49, 8.68, 8.53, 8.35, 6.3] Action:\neast Reward: 0, Score 5, Done False\nSTATE 23\nbehind house you are carrying: a broken jewel encrusted egg the\nbroken jewel encrusted egg contains: a broken clockwork canary\nbehind house you are behind the white house. a path leads into\nthe forest to the east. in one corner of the house there is a small\nwindow which is open. Actions: [’close window’, ’shake egg’, ’close\negg’, ’take canary’, ’east’, ’south’, ’north’, ’put down egg’, ’put down\ncanary’, ’west’] Qvalues: [11.21, 10.7, 10.61, 10.2, 10.12, 9.76, 9.66,\n9.53, 8.86, 8.7] Action: close egg Reward: 0, Score 5, Done False\nSTATE 24\nclosed. you are carrying: a broken jewel encrusted egg behind house\nyou are behind the white house. a path leads into the forest to the\neast. in one corner of the house there is a small window which is\nopen. Actions: [’close window’, ’south’, ’north’, ’west’, ’east’, ’open\negg’, ’put down egg’] Qvalues: [12.79, 12.65, 11.54, 11.37, 10.81, 10.31,\n10.01] Action: south Reward: 0, Score 5, Done False\nSTATE 25\nsouth of house you are facing the south side of a white house. there\nis no door here, and all the windows are boarded. you are carrying:\na broken jewel encrusted egg south of house you are facing the\nsouth side of a white house. there is no door here, and all the win-\ndows are boarded. Actions: [’east’, ’south’, ’open egg’, ’put down\negg’, ’west’] Qvalues: [14.39, 10.78, 10.45, 9.31, 8.39] Action: east\nReward: 0, Score 5, Done False\nSTATE 26\nbehind house you are carrying: a broken jewel encrusted egg be-\nhind house you are behind the white house. a path leads into the\nforest to the east. in one corner of the house there is a small window\nwhich is open. Actions: [’close window’, ’south’, ’east’, ’open egg’,\n’north’, ’west’, ’put down egg’] Qvalues: [13.61, 12.05, 11.88, 11.29,\n11.11, 10.28, 10.27] Action: close window Reward: 0, Score 5, Done\nFalse\nSTATE 27\nthe window closes (more easily than it opened). you are carrying: a\nbroken jewel encrusted egg behind house you are behind the white\nhouse. a path leads into the forest to the east. in one corner of the\nhouse there is a small window which is slightly ajar. Actions: [’open\nwindow’, ’north’, ’east’, ’south’, ’open egg’, ’put down egg’] Qval-\nues: [15.18, 12.67, 12.56, 12.56, 12.07, 11.34] Action: open window\nReward: 0, Score 5, Done False\nSTATE 28\nwith great effort, you open the window far enough to allow entry.\nyou are carrying: a broken jewel encrusted egg behind house you\nare behind the white house. a path leads into the forest to the east.\nin one corner of the house there is a small window which is open.\nActions: [’west’, ’close window’, ’open egg’, ’north’, ’put down egg’,\n’east’, ’south’] Qvalues: [18.49, 15.61, 14.4, 13.72, 13.06, 12.87, 12.11]\nAction: west Reward: 10, Score 15, Done False\nSTATE 29\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains: a\nquantity of water you are carrying: a broken jewel encrusted egg\nkitchen you are in the kitchen of the white house. a table seems\nto have been used recently for the preparation of food. a passage\nleads to the west and a dark staircase can be seen leading upward. a\ndark chimney leads down and to the east is a small window which\nis open. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains:\na quantity of water Actions: [’east’, ’west’, ’close window’, ’take\nbottle’, ’take all’, ’open bottle’, ’open sack’, ’take sack’, ’open egg’,\n’up’, ’put egg on table’, ’put down egg’, ’jump’] Qvalues: [12.6, 12.33,\n12.29, 11.66, 11.3, 11.3, 11.0, 10.48, 10.26, 10.12, 9.82, 9.81, 4.29] Ac-\ntion: put down egg Reward: 0, Score 15, Done False\nSTATE 30\ndropped. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a somewhat ruined\negg here. on the table is an elongated brown sack, smelling of hot\npeppers. a bottle is sitting on the table. the glass bottle contains:\na quantity of water Actions: [’take egg’, ’east’, ’open sack’, ’west’,\n’take bottle’, ’take sack’, ’take all from table’, ’close window’, ’take\nall’, ’open bottle’, ’up’, ’open egg’, ’jump’] Qvalues: [9.16, 9.1, 9.02,\n8.95, 8.85, 8.65, 8.54, 8.49, 8.48, 8.42, 8.29, 8.02, 0.73] Action: open\nsack Reward: 0, Score 15, Done False\nSTATE 31\nopening the brown sack reveals a lunch, and a clove of garlic. you\nare empty handed. kitchen you are in the kitchen of the white\nhouse. a table seems to have been used recently for the preparation\nof food. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a somewhat ruined egg here.\na bottle is sitting on the table. the glass bottle contains: a quantity\nof water a brown sack the brown sack contains: a lunch a clove of\ngarlic Actions: [’take egg’, ’take all from table’, ’take all’, ’take all\nfrom sack’, ’west’, ’take sack’, ’close sack’, ’take food’, ’take bottle’,\n’take garlic’, ’open egg’, ’open bottle’, ’close window’, ’east’, ’up’,\n’jump’] Qvalues: [9.83, 9.13, 8.97, 8.85, 8.69, 8.59, 8.55, 8.51, 8.43,\n8.31, 8.09, 7.93, 7.75, 7.02, 6.99, -1.14] Action: close sack Reward: 0,\nScore 15, Done False\nSTATE 32\nclosed. you are empty handed. kitchen you are in the kitchen of\nthe white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is open. there is a somewhat ruined\negg here. a bottle is sitting on the table. the glass bottle contains: a\nquantity of water a brown sack Actions: [’take egg’, ’take all’, ’take\nall from table’, ’take sack’, ’open sack’, ’take bottle’, ’close window’,\n’open egg’, ’open bottle’, ’west’, ’east’, ’up’, ’jump’] Qvalues: [11.85,\n11.45, 11.15, 11.06, 10.74, 10.47, 10.35, 10.14, 10.06, 9.89, 9.32, 9.24,\n2.32] Action: take bottle Reward: 0, Score 15, Done False\nSTATE 33\ntaken. you are carrying: a glass bottle the glass bottle contains: a\nquantity of water kitchen you are in the kitchen of the white house.\na table seems to have been used recently for the preparation of\nfood. a passage leads to the west and a dark staircase can be seen\nleading upward. a dark chimney leads down and to the east is a\nsmall window which is open. there is a somewhat ruined egg here.\nsitting on the kitchen table is: a brown sack Actions: [’take egg’,\n’take all’, ’west’, ’put water on table’, ’open sack’, ’take sack’, ’put\nbottle on table’, ’open egg’, ’open bottle’, ’close window’, ’up’, ’put\ndown bottle’, ’east’, ’jump’] Qvalues: [11.16, 10.75, 10.52, 9.92, 9.89,\n9.88, 9.78, 9.66, 9.51, 9.4, 9.36, 9.12, 8.99, 1.01] Action: close window\nReward: 0, Score 15, Done False\nSTATE 34\nthe window closes (more easily than it opened). you are carrying: a\nglass bottle the glass bottle contains: a quantity of water kitchen\nyou are in the kitchen of the white house. a table seems to have\nbeen used recently for the preparation of food. a passage leads to\nthe west and a dark staircase can be seen leading upward. a dark\nchimney leads down and to the east is a small window which is\nslightly ajar. there is a somewhat ruined egg here. sitting on the\nkitchen table is: a brown sack Actions: [’take egg’, ’west’, ’take\nall’, ’open egg’, ’open sack’, ’take sack’, ’put water on table’, ’put\nbottle on table’, ’open bottle’, ’open window’, ’put down bottle’, ’up’,\n’jump’] Qvalues: [10.72, 9.98, 9.86, 9.09, 9.02, 8.96, 8.92, 8.83, 8.8,\n8.65, 8.29, 7.96, -1.33] Action: put water on table Reward: 0, Score\n15, Done False\nSTATE 35\ndone. you are carrying: a glass bottle kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for the\npreparation of food. a passage leads to the west and a dark staircase\ncan be seen leading upward. a dark chimney leads down and to the\neast is a small window which is slightly ajar. there is a somewhat\nruined egg here. sitting on the kitchen table is: a quantity of water\na brown sack Actions: [’take egg’, ’take all’, ’west’, ’open window’,\n’open sack’, ’take sack’, ’open egg’, ’open bottle’, ’put bottle on table’,\n’put down bottle’, ’up’, ’jump’] Qvalues: [11.52, 10.95, 10.61, 10.33,\n10.11, 10.07, 10.03, 9.92, 9.8, 9.75, 8.45, 1.04] Action: put bottle on\ntable Reward: 0, Score 15, Done False\nSTATE 36\ndone. you are empty handed. kitchen you are in the kitchen of the\nwhite house. a table seems to have been used recently for the prepa-\nration of food. a passage leads to the west and a dark staircase can\nbe seen leading upward. a dark chimney leads down and to the east\nis a small window which is slightly ajar. there is a somewhat ruined\negg here. sitting on the kitchen table is: a glass bottle a quantity of\nwater a brown sack Actions: [’take egg’, ’take all’, ’open window’,\n’take all from table’, ’west’, ’take sack’, ’take bottle’, ’open sack’,\n’open bottle’, ’open egg’, ’up’, ’jump’] Qvalues: [11.36, 10.61, 10.58,\n10.56, 10.46, 10.29, 10.28, 10.13, 9.7, 9.65, 8.73, 1.69] Action: take sack\nReward: 0, Score 15, Done False\nSTATE 37\ntaken. you are carrying: a brown sack kitchen you are in the kitchen\nof the white house. a table seems to have been used recently for\nthe preparation of food. a passage leads to the west and a dark\nstaircase can be seen leading upward. a dark chimney leads down\nand to the east is a small window which is slightly ajar. there is a\nsomewhat ruined egg here. sitting on the kitchen table is: a glass\nbottle a quantity of water Actions: [’west’, ’take egg’, ’take all’, ’put\nsack on table’, ’open window’, ’open sack’, ’take bottle’, ’put down\nsack’, ’open egg’, ’open bottle’, ’up’, ’jump’] Qvalues: [9.59, 9.28,\n8.93, 8.76, 8.47, 8.47, 8.34, 8.32, 7.95, 7.83, 7.77, 0.83] Action: west\nReward: 0, Score 15, Done False\nSTATE 38\nliving room you are in the living room. there is a doorway to the\neast, a wooden door with strange gothic lettering to the west, which\nappears to be nailed shut, a trophy case, and a large oriental rug\nin the center of the room. above the trophy case hangs an elvish\nsword of great antiquity. a battery powered brass lantern is on\nthe trophy case. you are carrying: a brown sack living room you\nare in the living room. there is a doorway to the east, a wooden\ndoor with strange gothic lettering to the west, which appears to\nbe nailed shut, a trophy case, and a large oriental rug in the center\nof the room. above the trophy case hangs an elvish sword of great\nantiquity. a battery powered brass lantern is on the trophy case.\nActions: [’take all’, ’push rug’, ’open sack’, ’put down sack’, ’open\ncase’, ’take sword’, ’take lantern’, ’turn on lantern’, ’throw sack at\nlantern’, ’east’] Qvalues: [10.66, 10.2, 9.58, 9.11, 9.0, 8.97, 8.77, 8.68,\n8.31, 7.35] Action: push rug Reward: 0, Score 15, Done False\nSTATE 39\nwith a great effort, the rug is moved to one side of the room, reveal-\ning the dusty cover of a closed trap door. you are carrying: a brown\nsack living room you are in the living room. there is a doorway to\nthe east, a wooden door with strange gothic lettering to the west,\nwhich appears to be nailed shut, a trophy case, and a closed trap\ndoor at your feet. above the trophy case hangs an elvish sword of\ngreat antiquity. a battery powered brass lantern is on the trophy\ncase. Actions: [’take all’, ’open trap’, ’open sack’, ’take sword’, ’put\ndown sack’, ’take lantern’, ’open case’, ’turn on lantern’, ’throw sack\nat lantern’, ’east’] Qvalues: [11.53, 11.1, 10.37, 10.0, 9.76, 9.69, 9.66,\n9.61, 9.1, 8.05] Action: open trap Reward: 0, Score 15, Done False\nSTATE 40\nthe door reluctantly opens to reveal a rickety staircase descending\ninto darkness. you are carrying: a brown sack living room you are\nin the living room. there is a doorway to the east, a wooden door\nwith strange gothic lettering to the west, which appears to be nailed\nshut, a trophy case, and a rug lying beside an open trap door. above\nthe trophy case hangs an elvish sword of great antiquity. a battery\npowered brass lantern is on the trophy case. Actions: [’take all’,\n’take lantern’, ’open sack’, ’put down sack’, ’take sword’, ’open case’,\n’throw sack at lantern’, ’turn on lantern’, ’close trap’, ’east’, ’down’]\nQvalues: [12.63, 11.7, 11.21, 10.74, 10.64, 10.54, 10.47, 10.07, 9.7, 9.09,\n7.73] Action: take all Reward: 0, Score 15, Done False\nSTATE 41\ntrophy case: the trophy case is securely fastened to the wall. sword:\ntaken. brass lantern: taken. carpet: the rug is extremely heavy and\ncannot be carried. you are carrying: a brass lantern a sword a brown\nsack living room you are in the living room. there is a doorway\nto the east, a wooden door with strange gothic lettering to the\nwest, which appears to be nailed shut, a trophy case, and a rug\nlying beside an open trap door. Actions: [’open sack’, ’put down\nall’, ’down’, ’open case’, ’put down sack’, ’throw sack at lantern’,\n’close trap’, ’put down lantern’, ’put down sword’, ’turn on lantern’,\n’east’] Qvalues: [14.22, 14.12, 13.88, 13.79, 13.66, 13.2, 13.14, 13.11,\n13.01, 12.88, 12.33] Action: put down all Reward: 0, Score 15, Done\nFalse\nSTATE 42\nbrass lantern: dropped. sword: dropped. brown sack: dropped. you\nare empty handed. living room you are in the living room. there is\na doorway to the east, a wooden door with strange gothic lettering\nto the west, which appears to be nailed shut, a trophy case, and\na rug lying beside an open trap door. there is a brown sack here.\nthere is a sword here. there is a brass lantern (battery powered)\nhere. Actions: [’open sack’, ’take lantern’, ’take sack’, ’take all’, ’take\nsword’, ’turn on lantern’, ’open case’, ’close trap’, ’down’, ’east’]\nQvalues: [11.7, 11.61, 11.57, 11.39, 11.37, 11.37, 10.79, 10.57, 10.13,\n10.1] Action: down Reward: 25, Score 40, Done False\nSTATE 43\nyou have moved into a dark place. the trap door crashes shut, and\nyou hear someone barring it. it is pitch black. you are likely to be\neaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[-7.44, -9.98] Action: south Reward: 0, Score 40, Done False\nSTATE 44\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.4, -3.4, -8.58] Action: east Reward: 0, Score 40, Done False\nSTATE 45\ngallery this is an art gallery. most of the paintings have been stolen\nby vandals with exceptional taste. the vandals left through either\nthe north or west exits. fortunately, there is still one chance for\nyou to be a vandal, for on the far wall is a painting of unparalleled\nbeauty. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. fortunately,\nthere is still one chance for you to be a vandal, for on the far wall\nis a painting of unparalleled beauty. Actions: [’take art’, ’north’,\n’west’] Qvalues: [6.02, 1.29, 0.19] Action: take art Reward: 4, Score\n44, Done False\nSTATE 46\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.41, 2.35, 2.15] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 47\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’west’, ’north’] Qval-\nues: [2.91, 1.66, 1.11] Action: take art Reward: 0, Score 44, Done False\nSTATE 48\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.38, 2.36, 2.16] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 49\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting\nby a neglected genius is here. Actions: [’take art’, ’west’, ’north’]\nQvalues: [2.8, 1.52, 0.96] Action: take art Reward: 0, Score 44, Done\nFalse\nSTATE 50\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [2.34, 2.29, 2.1] Action:\nnorth Reward: 0, Score 44, Done False\nSTATE 51\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are carrying: a painting it is pitch black.\nyou are likely to be eaten by a grue. Actions: [’south’, ’put down\npainting’, ’north’] Qvalues: [1.71, -2.26, -3.91] Action: south Reward:\n0, Score 44, Done False\nSTATE 52\ngallery you are carrying: a painting gallery this is an art gallery.\nmost of the paintings have been stolen by vandals with exceptional\ntaste. the vandals left through either the north or west exits. Actions:\n[’north’, ’put down art’, ’west’] Qvalues: [2.0, 1.94, 1.62] Action: put\ndown art Reward: 0, Score 44, Done False\nSTATE 53\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’west’, ’north’] Qval-\nues: [2.44, 1.08, 0.47] Action: take art Reward: 0, Score 44, Done False\nSTATE 54\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [2.23, 1.96, 1.84] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 55\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting\nby a neglected genius is here. Actions: [’take art’, ’west’, ’north’]\nQvalues: [2.1, 0.73, 0.08] Action: west Reward: 0, Score 44, Done\nFalse\nSTATE 56\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.75, -4.31, -9.21] Action: east Reward: 0, Score 44, Done False\nSTATE 57\ngallery a painting by a neglected genius is here. you are empty\nhanded. gallery this is an art gallery. most of the paintings have\nbeen stolen by vandals with exceptional taste. the vandals left\nthrough either the north or west exits. a painting by a neglected\ngenius is here. Actions: [’take art’, ’west’, ’north’] Qvalues: [1.99,\n0.53, -0.2] Action: take art Reward: 0, Score 44, Done False\nSTATE 58\ntaken. you are carrying: a painting gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’put down art’, ’north’, ’west’] Qvalues: [1.93, 1.43, 1.35] Action:\nput down art Reward: 0, Score 44, Done False\nSTATE 59\ndropped. you are empty handed. gallery this is an art gallery. most\nof the paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. a painting by\na neglected genius is here. Actions: [’take art’, ’wait’, ’west’, ’north’]\nQvalues: [1.48, 0.25, 0.06, -0.58] Action: wait Reward: 0, Score 44,\nDone False\nSTATE 60\ntime passes... you are empty handed. gallery this is an art gallery.\nmost of the paintings have been stolen by vandals with exceptional\ntaste. the vandals left through either the north or west exits. Ac-\ntions: [’north’, ’west’] Qvalues: [1.24, 0.15] Action: west Reward: 0,\nScore 44, Done False\nSTATE 61\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.87, -4.34, -9.29] Action: east Reward: 0, Score 44, Done False\nSTATE 62\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.38, 0.23] Action: west Reward: 0, Score\n44, Done False\nSTATE 63\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[0.92, -4.3, -9.3] Action: east Reward: 0, Score 44, Done False\nSTATE 64\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.29, 0.16] Action: north Reward: 0, Score\n44, Done False\nSTATE 65\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.59, -4.18] Action: south Reward: 0, Score 44, Done False\nSTATE 66\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.29, 0.17] Action: west Reward: 0, Score\n44, Done False\nSTATE 67\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.21, -4.03, -9.06] Action: east Reward: 0, Score 44, Done False\nSTATE 68\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.38, 0.25] Action: west Reward: 0, Score\n44, Done False\nSTATE 69\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.27, -3.95, -8.96] Action: east Reward: 0, Score 44, Done False\nSTATE 70\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.46, 0.31] Action: north Reward: 0, Score\n44, Done False\nSTATE 71\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.78, -3.93] Action: south Reward: 0, Score 44, Done False\nSTATE 72\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.62, 0.45] Action: west Reward: 0, Score\n44, Done False\nSTATE 73\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.32, -3.86, -8.82] Action: east Reward: 0, Score 44, Done False\nSTATE 74\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’, ’wait’] Qvalues: [0.75, 0.57, 0.39] Action: north Re-\nward: 0, Score 44, Done False\nSTATE 75\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.87, -3.84] Action: south Reward: 0, Score 44, Done False\nSTATE 76\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [0.93, 0.75] Action: north Reward: 0, Score\n44, Done False\nSTATE 77\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.99, -3.68] Action: south Reward: 0, Score 44, Done False\nSTATE 78\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.14, 0.97] Action: west Reward: 0, Score\n44, Done False\nSTATE 79\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.51, -3.52, -8.51] Action: east Reward: 0, Score 44, Done False\nSTATE 80\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.33, 1.18] Action: west Reward: 0, Score\n44, Done False\nSTATE 81\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.66, -3.32, -8.33] Action: east Reward: 0, Score 44, Done False\nSTATE 82\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.55, 1.4] Action: north Reward: 0, Score\n44, Done False\nSTATE 83\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.41, -3.15] Action: south Reward: 0, Score 44, Done False\nSTATE 84\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.65, 1.51] Action: west Reward: 0, Score\n44, Done False\nSTATE 85\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.81, -3.09, -8.17] Action: east Reward: 0, Score 44, Done False\nSTATE 86\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.66, 1.56] Action: west Reward: 0, Score\n44, Done False\nSTATE 87\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.79, -3.06, -8.2] Action: east Reward: 0, Score 44, Done False\nSTATE 88\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.6, 1.53] Action: north Reward: 0, Score\n44, Done False\nSTATE 89\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.48, -3.07] Action: south Reward: 0, Score 44, Done False\nSTATE 90\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.53, 1.5] Action: north Reward: 0, Score\n44, Done False\nSTATE 91\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.46, -3.08] Action: south Reward: 0, Score 44, Done False\nSTATE 92\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.45, 1.42] Action: west Reward: 0, Score\n44, Done False\nSTATE 93\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.46, -3.16, -8.41] Action: east Reward: 0, Score 44, Done False\nSTATE 94\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.38, 1.33] Action: north Reward: 0, Score\n44, Done False\nSTATE 95\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[1.27, -3.26] Action: south Reward: 0, Score 44, Done False\nSTATE 96\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.29, 1.22] Action: west Reward: 0, Score\n44, Done False\nSTATE 97\nyou have moved into a dark place. it is pitch black. you are likely to\nbe eaten by a grue. you are empty handed. it is pitch black. you are\nlikely to be eaten by a grue. Actions: [’east’, ’north’, ’west’] Qvalues:\n[1.18, -3.36, -8.54] Action: east Reward: 0, Score 44, Done False\nSTATE 98\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.25, 1.13] Action: north Reward: 0, Score\n44, Done False\nSTATE 99\nyou have moved into a dark place. it is pitch black. you are likely\nto be eaten by a grue. you are empty handed. it is pitch black. you\nare likely to be eaten by a grue. Actions: [’south’, ’north’] Qvalues:\n[0.99, -3.47] Action: south Reward: 0, Score 44, Done True\nSTATE 100\ngallery you are empty handed. gallery this is an art gallery. most of\nthe paintings have been stolen by vandals with exceptional taste.\nthe vandals left through either the north or west exits. Actions:\n[’west’, ’north’] Qvalues: [1.21, 1.04] Action: reset Reward13458: 0,\nScore 0, Done False\nB.2\nInhumane\nSTATE 0\nYou awaken in your tent, lying on the floor, with a hangover like\nyou wouldn’t believe. As you lever yourself up, you notice that\nthere is complete silence instead of the usual morning complaints\nof the workers and Roboff’s usual shouting. This can only mean\none thing Roboff must have finally run off on you. Panicked, you\nlook around and see your amulet lying on the floor. At least Roboff\nhad the sense to not trigger off your alarm. Knowing him, he also\nhad the sense to take the emergency radio. How the megakrip\nare you going to get out of this one? INHUMANE : An Infralogic\nMassacre. Copyright ( C ) by Infobom Inhumane and Infobom are\ntrademarks of Sadistic Software, Ins. Revision 2 \/ Serial Number\n31415926 \/ Inform v1405 Library 5 \/ 8 First time players should type\n\" about \". In Your Tent You are in your tent in the encampment.\nThe tent appears to have been completely cleaned out. It is very\nhot in here and getting hotter. The tent flap to the south is open.\nOn the ground is the ancient jeweled amulet that you \" borrowed\n\" from Mr. Meshugge. You are holding nothing. In Your Tent You\nare in your tent in the encampment. The tent appears to have been\ncompletely cleaned out. It is very hot in here and getting hotter.\nThe tent flap to the south is open. On the ground is the ancient\njeweled amulet that you \" borrowed \" from Mr. Meshugge. Actions:\n[’south’, ’take amulet’] Qvalues: [6.04, 4.66] Action: south Reward:\n0, Score 0, Done False\nSTATE 1\ncenter of camp this is the center of your camp. the entire place\nseems to be completely deserted. you can see roboff’s tent to the\nsouth and the storage tent west. east is open desert. you are holding\nnothing. center of camp this is the center of your camp. the entire\nplace seems to be completely deserted. you can see roboff’s tent to\nthe south and the storage tent west. east is open desert. Actions:\n[’east’, ’north’, ’south’, ’west’] Qvalues: [7.01, 5.93, 5.76, 5.3] Action:\nsouth Reward: 0, Score 0, Done False\nSTATE 2\nroboff’s tent you are in francisco roboff’s tent. the place is bare\nexcept for the emergency signal radio and roboff’s toilet in one cor-\nner. there is a note taped to the tent wall. you are holding nothing.\nroboff’s tent you are in francisco roboff’s tent. the place is bare\nexcept for the emergency signal radio and roboff’s toilet in one\ncorner. there is a note taped to the tent wall. Actions: [’north’, ’take\nsignal off radio’] Qvalues: [6.3, 4.72] Action: north Reward: 0, Score\n0, Done False\nSTATE 3\ncenter of campyou are holding nothing. center of camp this is the\ncenter of your camp. the entire place seems to be completely de-\nserted. you can see roboff’s tent to the south and the storage tent\nwest. east is open desert. Actions: [’east’, ’north’, ’south’, ’west’]\nQvalues: [7.15, 5.82, 5.72, 5.61] Action: north Reward: 0, Score 0,\nDone False\nSTATE 4\nin your tent on the ground is the ancient jeweled amulet that you \"\nborrowed \" from mr. meshugge. you are holding nothing. in your\ntent you are in your tent in the encampment. the tent appears to\nhave been completely cleaned out. it is very hot in here and get-\nting hotter. the tent flap to the south is open. on the ground is the\nancient jeweled amulet that you \" borrowed \" from mr. meshugge.\nActions: [’south’, ’take amulet’] Qvalues: [6.36, 5.16] Action: south\nReward: 0, Score 0, Done False\nSTATE 5\ncenter of campyou are holding nothing. center of camp this is the\ncenter of your camp. the entire place seems to be completely de-\nserted. you can see roboff’s tent to the south and the storage tent\nwest. east is open desert. Actions: [’east’, ’north’, ’west’, ’south’]\nQvalues: [7.14, 5.84, 5.66, 5.66] Action: east Reward: 0, Score 0, Done\nFalse\nSTATE 6\nyou venture out onto the sands and rapidly lose sight of camp. in\nthe desert all you can see in any direction is sand and sunlight. you\nare sweating like a pig. you are holding nothing. in the desert all\nyou can see in any direction is sand and sunlight. you are sweating\nlike a pig. Actions: [’put all down’, ’north’, ’west’] Qvalues: [7.71,\n7.29, 6.3] Action: put all down Reward: 0, Score 0, Done False\nSTATE 7\nit falls and is lost forever in the sands. it is very dark. before you\ncan do anything, something begins gnawing on you. you have died\nyou are floating disembodied in absolute blackness. time passes at a\nstrange, relative rate. after an infinite \/ infinitesmal time, you sense\na presence near you. a voice speaks, resounding in your head as\nin a hollow gourd... \" ah! another of those wimpy delvers come to\nsteal my treasure. how boring. most of it was taken by that other\nguy who had the sense to avoid the nastier traps... but i am getting\npretty tired of guarding what’s left. i don’t suppose that you have\nany chance of thinking your way through my traps, of course. but\ni’ll let you try, and i won’t even vaporize you when you try to\nleave. in fact, i’ll make you a deal. if you prove to me that you are\nan incompetent harry sullivan by getting yourself killed by every\nsingle one of my deathtraps, after getting as far towards solving it\nas possible, then i’ll give up and let you take the coffin. \" that sound\ngood? i thought so. \" that hardly counts as being killed by a trap,\nyou know. even you could have gotten farther into that situation. \"\nyou don’t seem to have really gotten fooled by any traps yet. maybe\nthere is hope for you. \" round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nthere is a shovel lying here. you are holding : a torch round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’push button’, ’north’, ’east’, ’west’, ’south’, ’take button off torch’,\n’take shovel’, ’put torch down’, ’examine obelisk’, ’up’] Qvalues:\n[8.67, 8.06, 8.02, 7.76, 7.63, 7.4, 6.97, 6.88, 6.55, 5.09] Action: north\nReward: 0, Score 0, Done False\nSTATE 8\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you don’t\nseem to have really gotten fooled by any traps yet. maybe there is\nhope for you. \" round room this room is at the peak of the pyramid.\nit is perfectly conical. in the center is a tall stone obelisk that you\ncould probably use to get back up to the fake toilet. there are four\nstairways leaving the room at the four compass points. there is a\nshovel lying here. you are holding : a torch round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. Actions: [’push\nbutton’, ’east’, ’north’, ’west’, ’south’, ’take button off torch’, ’take\nshovel’, ’put torch down’, ’examine obelisk’, ’up’] Qvalues: [8.76,\n8.1, 7.99, 7.62, 7.55, 7.38, 6.93, 6.9, 6.49, 5.19] Action: take button off\ntorch Reward: 0, Score 0, Done False\nSTATE 9\nremoved. you are holding : a torch button a torch round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’push button’, ’north’, ’west’, ’east’, ’south’, ’take shovel’, ’put all\ndown’, ’put torch down’, ’put button down’, ’examine obelisk’, ’up’]\nQvalues: [8.26, 8.02, 7.8, 7.78, 7.59, 6.96, 6.76, 6.72, 6.71, 6.49, 5.56]\nAction: push button Reward: 0, Score 0, Done False\nSTATE 10\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’north’, ’south’, ’take shovel’, ’put button down’, ’examine\nobelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [9.97,\n8.69, 8.26, 8.14, 7.65, 7.56, 7.41, 7.09, 6.9, 6.68, 5.32] Action: south\nReward: 0, Score 0, Done False\nSTATE 11\ncorridor at alcoves the corridor goes north to the stairs and south\nfrom here. there are alcoves to the east and west. in the floor be-\ntween the alcoves is a quartz window. you are holding : a torch\nbutton a torch ( providing light ) corridor at alcoves the corridor\ngoes north to the stairs and south from here. there are alcoves to the\neast and west. in the floor between the alcoves is a quartz window.\nActions: [’south’, ’west’, ’push button’, ’north’, ’put button down’,\n’put all down’, ’put torch down’, ’east’] Qvalues: [8.79, 8.53, 8.07,\n7.82, 7.62, 7.56, 7.45, 7.41] Action: south Reward: 0, Score 0, Done\nFalse\nSTATE 12\noutside door you are at the end of the hall. to the south is a wooden\ndoor with a steel knob on it. the door is closed. you are holding :\na torch button a torch ( providing light ) outside door you are at\nthe end of the hall. to the south is a wooden door with a steel knob\non it. the door is closed. Actions: [’open door’, ’push button’, ’take\nbrass off door’, ’put button down’, ’north’, ’put torch down’, ’put\nall down’] Qvalues: [9.73, 8.84, 8.64, 8.39, 8.32, 8.16, 8.05] Action:\npush button Reward: 0, Score 0, Done False\nSTATE 13\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) outside door you are\nat the end of the hall. to the south is a wooden door with a steel\nknob on it. the door is closed. Actions: [’open door’, ’push button’,\n’take brass off door’, ’put button down’, ’north’, ’put all down’, ’put\ntorch down’] Qvalues: [9.83, 8.92, 8.7, 8.67, 8.23, 8.15, 8.09] Action:\npush button Reward: 0, Score 0, Done False\nSTATE 14\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) outside door you are\nat the end of the hall. to the south is a wooden door with a steel\nknob on it. the door is closed. Actions: [’open door’, ’push button’,\n’put button down’, ’take brass off door’, ’north’, ’put all down’, ’put\ntorch down’] Qvalues: [9.93, 9.04, 8.76, 8.76, 8.28, 8.19, 8.13] Action:\nopen door Reward: 0, Score 0, Done False\nSTATE 15\nthe door slams open. it is apparently on a very strong spring. if\nyou were on the other side, you would have been slammed into\nthe wall. as it is, the steel door knob slams into the inside wall. you\nare holding : a torch button a torch ( providing light ) outside door\nyou are at the end of the hall. to the south is a wooden door with a\nsteel knob on it. the door is open. Actions: [’south’, ’push button’,\n’take brass off door’, ’put button down’, ’close door’, ’put all down’,\n’north’, ’put torch down’] Qvalues: [10.85, 9.79, 9.52, 9.4, 9.14, 8.61,\n8.56, 8.52] Action: south Reward: 0, Score 0, Done False\nSTATE 16\nroom with chest this room is empty except for a small chest at one\nend. the only exit is the door to the north. the door is open. you\nsee something glimmering behind the door. you’ll have to close the\ndoor to get it. the chest is closed and locked. you are holding : a\ntorch button a torch ( providing light ) room with chest this room\nis empty except for a small chest at one end. the only exit is the\ndoor to the north. the door is open. you see something glimmering\nbehind the door. you’ll have to close the door to get it. the chest\nis closed and locked. Actions: [’close door’, ’push button’, ’take\nbrass off door’, ’put torch down’, ’north’, ’put all down’, ’put but-\nton down’] Qvalues: [12.47, 11.55, 11.36, 10.31, 10.26, 10.19, 10.05]\nAction: close door Reward: 0, Score 0, Done False\nSTATE 17\nafter much tugging, you manage to get the door shut. behind it\nis a brass needle that you might use to unlock the chest. you are\nholding : a torch button a torch ( providing light ) room with chest\nthis room is empty except for a small chest at one end. the only exit\nis the door to the north. the door is closed. on the ground is a long\nbrass needle. the chest is closed and locked. Actions: [’open chest\nwith needle’, ’take brass’, ’push button’, ’put all down’, ’put torch\ndown’, ’put button down’] Qvalues: [13.9, 12.14, 12.14, 11.0, 10.83,\n10.5] Action: open chest with needle Reward: 0, Score 0, Done False\nSTATE 18\n( first taking the brass needle ) there is a click and the chest lid pops\nopen. inside is a priceless dorkmid! as you stare at it, you notice\nthe smell of coal gas leaking out. you are holding : a brass needle a\ntorch button a torch ( providing light ) room with chest this room is\nempty except for a small chest at one end. the only exit is the door\nto the north. the door is closed. the chest is open. the chest contains\n: a priceless dorkmid Actions: [’open door’, ’put needle down’, ’put\nneedle in chest’, ’push button’, ’put button in chest’, ’put torch in\nchest’, ’put all in chest’, ’put all down’, ’put torch down’, ’put button\ndown’, ’take dorkmid’] Qvalues: [15.18, 14.95, 14.61, 13.77, 13.53,\n13.52, 13.44, 12.72, 12.46, 12.15, 11.09] Action: open door Reward:\n10, Score 10, Done False\nSTATE 19\nthe door slams open. you barely avoid the swinging knob. instead,\nthe steel knob hits the wall with a bang. the wall seems to be made\nof flint. you have died you are back in the infinite blackness. eventu-\nally the voice returns, senses you disgustedly, and speaks. \" sparks\nin a room full of natural gas is not really a smart thing to do. \"\nyou’ve only gotten killed by one trap. not bad. \" round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. you are\nholding : a torch button a torch ( providing light ) round room this\nroom is at the peak of the pyramid. it is perfectly conical. in the\ncenter is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. Actions:\n[’east’, ’north’, ’push button’, ’south’, ’take shovel’, ’west’, ’examine\nobelisk’, ’put button down’, ’put torch down’, ’put all down’, ’up’]\nQvalues: [11.75, 8.96, 8.88, 8.11, 7.56, 7.54, 7.53, 7.32, 6.79, 6.74, 6.41]\nAction: south Reward: 0, Score 10, Done False\nSTATE 20\ncorridor at alcovesyou are holding : a torch button a torch ( provid-\ning light ) corridor at alcoves the corridor goes north to the stairs\nand south from here. there are alcoves to the east and west. in the\nfloor between the alcoves is a quartz window. Actions: [’south’,\n’west’, ’push button’, ’put button down’, ’north’, ’put all down’,\n’east’, ’put torch down’] Qvalues: [8.68, 8.19, 7.98, 7.51, 7.43, 7.36,\n7.34, 7.22] Action: put button down Reward: 0, Score 10, Done False\nSTATE 21\ndropped. you are holding : a torch ( providing light ) corridor at\nalcoves the corridor goes north to the stairs and south from here.\nthere are alcoves to the east and west. in the floor between the\nalcoves is a quartz window. you can see a torch button here. Ac-\ntions: [’south’, ’take button’, ’west’, ’push button’, ’put torch down’,\n’north’, ’east’] Qvalues: [7.64, 7.58, 7.48, 7.33, 6.72, 6.63, 6.35] Action:\nwest Reward: 0, Score 10, Done False\nSTATE 22\nwest alcove this alcove has a stone platform set in the floor. the\nplatform shifts a little but does not move when you step on it. stick-\ning out of the platform is an iron crank. you are holding : a torch (\nproviding light ) west alcove this alcove has a stone platform set in\nthe floor. the platform shifts a little but does not move when you\nstep on it. sticking out of the platform is an iron crank. Actions:\n[’turn crank’, ’east’, ’put torch down’] Qvalues: [8.75, 7.73, 6.44]\nAction: east Reward: 0, Score 10, Done False\nSTATE 23\ncorridor at alcoves you can see a torch button here. you are holding\n: a torch ( providing light ) corridor at alcoves the corridor goes\nnorth to the stairs and south from here. there are alcoves to the east\nand west. in the floor between the alcoves is a quartz window. you\ncan see a torch button here. Actions: [’south’, ’take button’, ’west’,\n’push button’, ’put torch down’, ’north’, ’east’] Qvalues: [7.75, 7.69,\n7.48, 7.46, 6.76, 6.7, 6.62] Action: take button Reward: 0, Score 10,\nDone False\nSTATE 24\ntaken. you are holding : a torch button a torch ( providing light ) cor-\nridor at alcoves the corridor goes north to the stairs and south from\nhere. there are alcoves to the east and west. in the floor between the\nalcoves is a quartz window. Actions: [’south’, ’west’, ’push button’,\n’put button down’, ’put all down’, ’put torch down’, ’north’, ’east’]\nQvalues: [8.53, 7.97, 7.74, 7.28, 7.28, 7.24, 7.09, 6.74] Action: west\nReward: 0, Score 10, Done False\nSTATE 25\nwest alcoveyou are holding : a torch button a torch ( providing\nlight ) west alcove this alcove has a stone platform set in the floor.\nthe platform shifts a little but does not move when you step on it.\nsticking out of the platform is an iron crank. Actions: [’turn crank’,\n’push button’, ’east’, ’put button down’, ’put torch down’, ’put all\ndown’] Qvalues: [9.2, 7.92, 7.65, 7.19, 6.75, 6.7] Action: turn crank\nReward: 0, Score 10, Done False\nSTATE 26\nthe platform slowly moves down a shaft below it. in the other al-\ncove the other platform slowly moves up, counterbalancing the\none you are on. on platform you are on the crank platform at the\nbottom of the shaft under the alcove. there is an opening to the\neast. you are holding : a torch button a torch ( providing light ) on\nplatform you are on the crank platform at the bottom of the shaft\nunder the alcove. there is an opening to the east. Actions: [’push\nbutton’, ’turn crank’, ’put button down’, ’put torch down’, ’put all\ndown’, ’east’] Qvalues: [10.23, 10.22, 9.02, 8.93, 8.7, 8.63] Action:\npush button Reward: 0, Score 10, Done False\nSTATE 27\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) on platform you are\non the crank platform at the bottom of the shaft under the alcove.\nthere is an opening to the east. Actions: [’turn crank’, ’push button’,\n’put button down’, ’east’, ’put torch down’, ’put all down’] Qvalues:\n[11.21, 10.29, 9.09, 9.06, 8.8, 8.62] Action: put all down Reward: 0,\nScore 10, Done False\nSTATE 28\ntorch button : dropped. torch : dropped. you are holding nothing.\non platform you are on the crank platform at the bottom of the\nshaft under the alcove. there is an opening to the east. lying on the\nground is a brass torch of the \" self igniting \" type. the ignite button\nis on the handle. you can also see a torch button here. Actions: [’take\nall’, ’take torch’, ’take button’, ’push button’, ’turn crank’, ’east’]\nQvalues: [9.66, 8.96, 8.88, 8.8, 8.21, 7.81] Action: take all Reward: 0,\nScore 10, Done False\nSTATE 29\niron crank : a wimp like you will never get that done. torch : taken.\ntorch button : taken. you are holding : a torch button a torch (\nproviding light ) on platform you are on the crank platform at the\nbottom of the shaft under the alcove. there is an opening to the\neast. Actions: [’turn crank’, ’push button’, ’put button down’, ’put\ntorch down’, ’put all down’, ’east’] Qvalues: [11.85, 10.14, 8.88, 8.85,\n8.72, 8.24] Action: east Reward: 0, Score 10, Done False\nSTATE 30\nin tiny room this is a tiny room. the only exit is back west. you can\nsee the quartz window directly above your head. there is a huge\ndiamond lying on the ground! you are holding : a torch button a\ntorch ( providing light ) in tiny room this is a tiny room. the only\nexit is back west. you can see the quartz window directly above\nyour head. there is a huge diamond lying on the ground! Actions:\n[’west’, ’take diamond’, ’push button’, ’put torch down’, ’put button\ndown’, ’put all down’] Qvalues: [10.17, 9.35, 8.97, 8.14, 8.13, 8.12]\nAction: west Reward: 0, Score 10, Done False\nSTATE 31\non platformyou are holding : a torch button a torch ( providing\nlight ) on platform you are on the crank platform at the bottom of\nthe shaft under the alcove. there is an opening to the east. Actions:\n[’turn crank’, ’push button’, ’put button down’, ’put torch down’,\n’put all down’, ’east’] Qvalues: [11.12, 10.08, 8.86, 8.74, 8.66, 7.84]\nAction: turn crank Reward: 10, Score 20, Done False\nSTATE 32\nthe platform moves back up to the alcove level. in the other alcove,\nyou see the other platform moving back down until both of the\nplatforms are even. west alcove this alcove has a stone platform\nset in the floor. the platform shifts a little but does not move when\nyou step on it. sticking out of the platform is an iron crank. as the\nother platform moves down from the hole in the ceiling, you see a\nsmall robot standing on it. when it gets down to the floor, it looks\nat you, says \" ex trrr mnnn ate \", points an arm at you, and turns\nyou into a small pile of monatomic dust. you have died you are\nback in the infinite blackness. eventually the voice returns, senses\nyou disgustedly, and speaks. \" giving nasties a free ride down to\nwhere you are seems a little stupid. \" you’ve been killed by two\ntraps. that isn’t really so bad. \" round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. you are holding : a\ntorch button a torch ( providing light ) round room this room is at\nthe peak of the pyramid. it is perfectly conical. in the center is a\ntall stone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’push button’,\n’north’, ’south’, ’take shovel’, ’examine obelisk’, ’put button down’,\n’west’, ’put torch down’, ’put all down’, ’east’, ’up’] Qvalues: [8.24,\n7.81, 7.57, 7.32, 7.08, 6.95, 6.66, 6.58, 6.47, 6.31, 5.27] Action: west\nReward: 0, Score 20, Done False\nSTATE 33\nt intersection the east \/ west corridor branches to the south here.\nyou are holding : a torch button a torch ( providing light ) t inter-\nsection the east \/ west corridor branches to the south here. Actions:\n[’east’, ’push button’, ’west’, ’put torch down’, ’put all down’, ’put\nbutton down’, ’south’] Qvalues: [7.36, 6.89, 6.85, 6.17, 6.08, 6.05,\n5.85] Action: east Reward: 0, Score 20, Done False\nSTATE 34\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’north’,\n’south’, ’take shovel’, ’examine obelisk’, ’east’, ’put button down’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [8.32, 7.79,\n7.61, 7.21, 7.12, 6.97, 6.93, 6.61, 6.56, 6.4, 5.75] Action: put button\ndown Reward: 0, Score 20, Done False\nSTATE 35\ndropped. you are holding : a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to\nget back up to the fake toilet. there are four stairways leaving the\nroom at the four compass points. there is a shovel lying here. you\ncan also see a torch button here. Actions: [’take button’, ’north’,\n’push button’, ’take all’, ’take shovel’, ’east’, ’south’, ’west’, ’examine\nobelisk’, ’put torch down’, ’up’] Qvalues: [7.53, 7.32, 7.3, 7.18, 7.07,\n7.0, 6.82, 6.78, 6.67, 6.18, 5.21] Action: north Reward: 0, Score 20,\nDone False\nSTATE 36\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. you are holding : a\ntorch ( providing light ) landing this is a landing on the stairway.\nyou can go down to the north or up to the south. a passage leads\neast. Actions: [’north’, ’south’, ’east’, ’put torch down’] Qvalues:\n[8.16, 6.7, 6.56, 6.28] Action: east Reward: 0, Score 20, Done False\nSTATE 37\nt intersection the hallway goes east and west here with a branch\nnorth. you are holding : a torch ( providing light ) t intersection\nthe hallway goes east and west here with a branch north. Actions:\n[’west’, ’north’, ’put torch down’, ’east’] Qvalues: [7.2, 6.93, 6.24,\n6.01] Action: put torch down Reward: 0, Score 20, Done False\nSTATE 38\ndropped. you are holding nothing. t intersection the hallway goes\neast and west here with a branch north. lying on the ground is a\nbrass torch of the \" self igniting \" type. the ignite button is on the\nhandle. Actions: [’west’, ’north’, ’east’, ’take torch’] Qvalues: [6.88,\n6.7, 6.47, 6.28] Action: east Reward: 0, Score 20, Done False\nSTATE 39\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve been\nkilled by two traps. that isn’t really so bad. \" round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. you can also see a torch\nbutton here. you are holding : a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. there is a shovel lying here. you can\nalso see a torch button here. Actions: [’take button’, ’north’, ’push\nbutton’, ’take all’, ’take shovel’, ’south’, ’west’, ’examine obelisk’,\n’put torch down’, ’east’, ’up’] Qvalues: [7.77, 7.65, 7.54, 7.45, 7.14,\n7.03, 6.69, 6.66, 6.31, 6.06, 5.33] Action: north Reward: 0, Score 20,\nDone False\nSTATE 40\nlandingyou are holding : a torch ( providing light ) landing this is\na landing on the stairway. you can go down to the north or up to\nthe south. a passage leads east. Actions: [’north’, ’south’, ’east’, ’put\ntorch down’] Qvalues: [8.24, 6.67, 6.57, 6.31] Action: north Reward:\n0, Score 20, Done False\nSTATE 41\nbottom of shaft you are at the bottom of a ladder that goes up a\nshaft from here. to the south is the stairway up. on the floor is a\nsmall coin bag made of leather. it appears to have something in it.\nyou are holding : a torch ( providing light ) bottom of shaft you are\nat the bottom of a ladder that goes up a shaft from here. to the south\nis the stairway up. on the floor is a small coin bag made of leather.\nit appears to have something in it. Actions: [’up’, ’take coin-bag’,\n’south’, ’put torch down’] Qvalues: [9.37, 8.64, 7.42, 7.26] Action:\ntake coin-bag Reward: 0, Score 20, Done False\nSTATE 42\ntaken. you are holding : a small leather coin bag a torch ( providing\nlight ) bottom of shaft you are at the bottom of a ladder that goes up\na shaft from here. to the south is the stairway up. Actions: [’up’, ’put\ncoin-bag down’, ’south’, ’put torch down’, ’put all down’] Qvalues:\n[9.71, 8.47, 7.53, 7.14, 7.05] Action: up Reward: 0, Score 20, Done\nFalse\nSTATE 43\nyou climb up the ladder to a hole at the top of the shaft. exercise\nwheel room this room is laid out like an exercise wheel for rats ex-\ncept that it is 40 ft. across. you can turn the wheel by walking north\nor south. equally spaced around the rim are twelve coin holders.\nsome have gold coins in them. the coin holder at your feet has a\ncoin in it. the hole is at the bottom of the wheel. below it is the shaft\nthat you entered this room by. you are holding : a small leather coin\nbag a torch ( providing light ) exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet has a coin in it. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered this\nroom by. Actions: [’south’, ’put coin-bag down’, ’put torch down’,\n’put all down’, ’down’, ’north’] Qvalues: [10.62, 8.9, 8.48, 8.29, 8.23,\n8.13] Action: south Reward: 0, Score 20, Done False\nSTATE 44\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the north side. you\nare holding : a small leather coin bag a torch ( providing light )\nexercise wheel room this room is laid out like an exercise wheel\nfor rats except that it is 40 ft. across. you can turn the wheel by\nwalking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is slightly up the north side. Actions:\n[’south’, ’put coin-bag down’, ’north’, ’put torch down’, ’put all\ndown’] Qvalues: [11.58, 9.76, 9.28, 9.2, 9.07] Action: put torch down\nReward: 0, Score 20, Done False\nSTATE 45\ndropped. you are holding : a small leather coin bag exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is slightly up the north side. lying on the ground is a\nbrass torch of the \" self igniting \" type. the ignite button is on the\nhandle. Actions: [’take brass’, ’south’, ’put coin-bag down’, ’north’]\nQvalues: [10.36, 10.14, 9.24, 7.98] Action: north Reward: 0, Score 20,\nDone False\nSTATE 46\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is at the bottom of the wheel.\nbelow it is the shaft that you entered this room by. you are holding\n: a small leather coin bag exercise wheel room this room is laid out\nlike an exercise wheel for rats except that it is 40 ft. across. you can\nturn the wheel by walking north or south. equally spaced around\nthe rim are twelve coin holders. some have gold coins in them. the\ncoin holder at your feet has a coin in it. the hole is at the bottom of\nthe wheel. below it is the shaft that you entered this room by. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. Actions: [’south’, ’take torch’, ’put coin-bag\ndown’, ’down’, ’north’] Qvalues: [8.84, 8.82, 8.14, 7.12, 7.07] Action:\nput coin-bag down Reward: 0, Score 20, Done False\nSTATE 47\ndropped. you are holding nothing. exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet has a coin in it. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered\nthis room by. on the floor is a small coin bag made of leather. it\nappears to have something in it. lying on the ground is a brass\ntorch of the \" self igniting \" type. the ignite button is on the handle.\nActions: [’take all’, ’south’, ’take torch’, ’take coin-bag’, ’take in\nfloor’, ’north’] Qvalues: [9.69, 9.44, 9.1, 8.53, 8.18, 7.63] Action: take\nall Reward: 0, Score 20, Done False\nSTATE 48\nsmall leather coin bag : taken. torch : taken. coin holder : you pick\nup the coin and put it in your bag. you are holding : a torch ( pro-\nviding light ) a small leather coin bag exercise wheel room this\nroom is laid out like an exercise wheel for rats except that it is 40 ft.\nacross. you can turn the wheel by walking north or south. equally\nspaced around the rim are twelve coin holders. some have gold\ncoins in them. the coin holder at your feet is empty. the hole is at\nthe bottom of the wheel. below it is the shaft that you entered this\nroom by. Actions: [’south’, ’put coin-bag down’, ’put torch down’,\n’put all down’, ’north’, ’down’] Qvalues: [9.59, 8.74, 8.14, 8.11, 7.7,\n7.53] Action: north Reward: 0, Score 20, Done False\nSTATE 49\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the south side. you\nare holding : a torch ( providing light ) a small leather coin bag\nexercise wheel room this room is laid out like an exercise wheel\nfor rats except that it is 40 ft. across. you can turn the wheel by\nwalking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at\nyour feet has a coin in it. the hole is slightly up the south side.\nActions: [’south’, ’put coin-bag down’, ’put all down’, ’put torch\ndown’, ’north’] Qvalues: [8.63, 8.06, 7.47, 7.36, 7.29] Action: put\ncoin-bag down Reward: 0, Score 20, Done False\nSTATE 50\ndropped. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is slightly up the south side. on the floor is a small coin\nbag made of leather. it appears to have something in it. Actions:\n[’south’, ’take coin-bag’, ’north’, ’put torch down’] Qvalues: [9.01,\n8.69, 8.03, 7.6] Action: south Reward: 0, Score 20, Done False\nSTATE 51\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet is empty. the hole is at the bottom of the wheel. below it\nis the shaft that you entered this room by. you are holding : a torch\n( providing light ) exercise wheel room this room is laid out like an\nexercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet is empty. the hole is at the bottom of the wheel.\nbelow it is the shaft that you entered this room by. on the floor is a\nsmall coin bag made of leather. it appears to have something in it.\nActions: [’south’, ’take coin-bag’, ’put torch down’, ’north’, ’take\nin floor’] Qvalues: [10.48, 10.08, 8.75, 8.58, 8.15] Action: put torch\ndown Reward: 0, Score 20, Done False\nSTATE 52\ndropped. you are holding nothing. exercise wheel room this room is\nlaid out like an exercise wheel for rats except that it is 40 ft. across.\nyou can turn the wheel by walking north or south. equally spaced\naround the rim are twelve coin holders. some have gold coins in\nthem. the coin holder at your feet is empty. the hole is at the bottom\nof the wheel. below it is the shaft that you entered this room by.\nlying on the ground is a brass torch of the \" self igniting \" type. the\nignite button is on the handle. on the floor is a small coin bag made\nof leather. it appears to have something in it. Actions: [’take all’,\n’south’, ’take torch’, ’take coin-bag’, ’take in floor’, ’north’] Qvalues:\n[10.04, 9.65, 9.47, 8.81, 8.42, 8.17] Action: take torch Reward: 0, Score\n20, Done False\nSTATE 53\ntaken. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north\nor south. equally spaced around the rim are twelve coin holders.\nsome have gold coins in them. the coin holder at your feet is empty.\nthe hole is at the bottom of the wheel. below it is the shaft that\nyou entered this room by. on the floor is a small coin bag made of\nleather. it appears to have something in it. Actions: [’south’, ’take\ncoin-bag’, ’put torch down’, ’north’, ’take in floor’] Qvalues: [10.47,\n9.96, 8.81, 8.62, 8.6] Action: south Reward: 0, Score 20, Done False\nSTATE 54\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is slightly up the north side. you\nare holding : a torch ( providing light ) exercise wheel room this\nroom is laid out like an exercise wheel for rats except that it is 40 ft.\nacross. you can turn the wheel by walking north or south. equally\nspaced around the rim are twelve coin holders. some have gold\ncoins in them. the coin holder at your feet has a coin in it. the hole\nis slightly up the north side. on the floor is a small coin bag made of\nleather. it appears to have something in it. Actions: [’south’, ’take\ncoin-bag’, ’put torch down’, ’north’] Qvalues: [11.74, 11.16, 9.81,\n9.42] Action: take coin-bag Reward: 0, Score 20, Done False\nSTATE 55\ntaken. you are holding : a small leather coin bag a torch ( providing\nlight ) exercise wheel room this room is laid out like an exercise\nwheel for rats except that it is 40 ft. across. you can turn the wheel\nby walking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is slightly up the north side. Actions:\n[’south’, ’put coin-bag down’, ’north’, ’put torch down’, ’put all\ndown’] Qvalues: [12.41, 10.59, 10.06, 9.98, 9.9] Action: south Re-\nward: 0, Score 20, Done False\nSTATE 56\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder\nat your feet has a coin in it. the hole is less than half way up the\nnorth side. you are holding : a small leather coin bag a torch (\nproviding light ) exercise wheel room this room is laid out like an\nexercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet has a coin in it. the hole is less than half way\nup the north side. Actions: [’south’, ’put coin-bag down’, ’put torch\ndown’, ’north’, ’put all down’] Qvalues: [13.58, 11.65, 10.99, 10.91,\n10.85] Action: put all down Reward: 0, Score 20, Done False\nSTATE 57\nsmall leather coin bag : dropped. torch : dropped. you are holding\nnothing. exercise wheel room this room is laid out like an exercise\nwheel for rats except that it is 40 ft. across. you can turn the wheel\nby walking north or south. equally spaced around the rim are twelve\ncoin holders. some have gold coins in them. the coin holder at your\nfeet has a coin in it. the hole is less than half way up the north side.\nlying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. on the floor is a small coin bag\nmade of leather. it appears to have something in it. Actions: [’take\nall’, ’south’, ’take torch’, ’take coin-bag’, ’north’] Qvalues: [12.65,\n12.31, 12.0, 11.39, 9.82] Action: south Reward: 0, Score 20, Done False\nSTATE 58\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is half way up the north side. you\nare holding nothing. exercise wheel room this room is laid out like\nan exercise wheel for rats except that it is 40 ft. across. you can turn\nthe wheel by walking north or south. equally spaced around the\nrim are twelve coin holders. some have gold coins in them. the coin\nholder at your feet has a coin in it. the hole is half way up the north\nside. lying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. on the floor is a small coin bag\nmade of leather. it appears to have something in it. Actions: [’take\nall’, ’south’, ’take torch’, ’take coin-bag’, ’north’] Qvalues: [14.04,\n13.49, 13.35, 12.75, 10.79] Action: take torch Reward: 0, Score 20,\nDone False\nSTATE 59\ntaken. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is half way up the north side. on the floor is a small coin\nbag made of leather. it appears to have something in it. Actions:\n[’south’, ’take coin-bag’, ’put torch down’, ’north’] Qvalues: [14.71,\n14.12, 12.3, 11.95] Action: south Reward: 0, Score 20, Done False\nSTATE 60\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder at\nyour feet has a coin in it. the hole is more than half way up the north\nside. you are holding : a torch ( providing light ) exercise wheel\nroom this room is laid out like an exercise wheel for rats except\nthat it is 40 ft. across. you can turn the wheel by walking north or\nsouth. equally spaced around the rim are twelve coin holders. some\nhave gold coins in them. the coin holder at your feet has a coin in\nit. the hole is more than half way up the north side. on the floor\nis a small coin bag made of leather. it appears to have something\nin it. Actions: [’south’, ’take coin-bag’, ’put torch down’, ’north’]\nQvalues: [15.89, 15.25, 13.22, 13.01] Action: south Reward: 10, Score\n30, Done False\nSTATE 61\nthe wheel rotates under your feet. you stay at the bottom, but the\nhole in the rim that you entered by has moved. the coin holder\nat your feet has a coin in it. the hole is almost to the top on the\nnorth side. behind it is a hole from which water is pouring very fast.\nthe water turns the wheel back to its original position, then pours\ndown the hole. you are taken with it. you have died you are back\nin the infinite blackness. eventually the voice returns, senses you\ndisgustedly, and speaks. \" don’t you know how lab rats eventually\nend up? \" three deathtraps have zocked you so far. you’re beginning\nto seem rather dumb. \" round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nthere is a shovel lying here. you can also see a torch button here.\nyou are holding : a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. you can also see\na torch button here. Actions: [’take button’, ’north’, ’push button’,\n’take all’, ’south’, ’take shovel’, ’west’, ’examine obelisk’, ’put torch\ndown’, ’east’, ’up’] Qvalues: [8.61, 8.49, 8.34, 8.29, 7.95, 7.88, 7.43,\n7.37, 7.15, 7.11, 5.9] Action: put torch down Reward: 0, Score 30,\nDone False\nSTATE 62\ndropped. you are holding nothing. round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the\nfake toilet. there are four stairways leaving the room at the four\ncompass points. lying on the ground is a brass torch of the \" self\nigniting \" type. the ignite button is on the handle. there is a shovel\nlying here. you can also see a torch button here. Actions: [’west’,\n’north’, ’take all’, ’push button’, ’south’, ’east’, ’take button’, ’take\ntorch’, ’take shovel’, ’examine obelisk’, ’up’] Qvalues: [7.85, 7.68,\n7.68, 7.68, 7.66, 7.62, 7.51, 7.44, 7.13, 6.9, 6.1] Action: south Reward:\n0, Score 30, Done False\nSTATE 63\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know. even\nyou could have gotten farther into that situation. \" three deathtraps\nhave zocked you so far. you’re beginning to seem rather dumb. \"\nround room this room is at the peak of the pyramid. it is perfectly\nconical. in the center is a tall stone obelisk that you could probably\nuse to get back up to the fake toilet. there are four stairways leaving\nthe room at the four compass points. there is a shovel lying here.\nyou can also see a torch button here. you are holding : a torch (\nproviding light ) round room this room is at the peak of the pyramid.\nit is perfectly conical. in the center is a tall stone obelisk that you\ncould probably use to get back up to the fake toilet. there are four\nstairways leaving the room at the four compass points. there is a\nshovel lying here. you can also see a torch button here. Actions:\n[’take button’, ’north’, ’push button’, ’take all’, ’south’, ’take shovel’,\n’west’, ’examine obelisk’, ’put torch down’, ’east’, ’up’] Qvalues:\n[8.62, 8.38, 8.34, 8.28, 7.94, 7.89, 7.54, 7.35, 7.17, 6.62, 5.78] Action:\ntake all Reward: 0, Score 30, Done False\nSTATE 64\ntorch button : taken. shovel : taken. stone obelisk : a wimp like you\nwill never get that done. you are holding : a shovel a torch button a\ntorch ( providing light ) round room this room is at the peak of the\npyramid. it is perfectly conical. in the center is a tall stone obelisk\nthat you could probably use to get back up to the fake toilet. there\nare four stairways leaving the room at the four compass points.\nActions: [’push button’, ’north’, ’south’, ’put shovel down’, ’east’,\n’put button down’, ’examine obelisk’, ’put torch down’, ’west’, ’put\nall down’, ’up’] Qvalues: [8.96, 8.86, 8.77, 8.23, 8.21, 7.93, 7.84, 7.54,\n7.53, 7.5, 5.75] Action: push button Reward: 0, Score 30, Done False\nSTATE 65\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a shovel a torch button a torch ( providing light ) round room\nthis room is at the peak of the pyramid. it is perfectly conical. in\nthe center is a tall stone obelisk that you could probably use to get\nback up to the fake toilet. there are four stairways leaving the room\nat the four compass points. Actions: [’east’, ’push button’, ’north’,\n’south’, ’put shovel down’, ’put button down’, ’examine obelisk’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [10.04, 9.23,\n9.06, 9.0, 8.39, 8.16, 8.09, 7.59, 7.55, 7.38, 6.29] Action: east Reward:\n0, Score 30, Done False\nSTATE 66\nt intersection to the hall goes east from here. to the west is the\nstairway to the round room. a narrow passage leads south. you are\nholding : a shovel a torch button a torch ( providing light ) t inter-\nsection to the hall goes east from here. to the west is the stairway\nto the round room. a narrow passage leads south. Actions: [’east’,\n’west’, ’push button’, ’south’, ’put shovel down’, ’put torch down’,\n’put button down’, ’put all down’] Qvalues: [13.19, 8.57, 8.34, 7.48,\n7.11, 6.89, 6.88, 6.69] Action: east Reward: 0, Score 30, Done False\nSTATE 67\ncorridor at doorway the hall goes east and west from here and there\nis a doorway to the north. you are holding : a shovel a torch button\na torch ( providing light ) corridor at doorway the hall goes east\nand west from here and there is a doorway to the north. Actions:\n[’north’, ’east’, ’west’, ’push button’, ’put shovel down’, ’put torch\ndown’, ’put button down’, ’put all down’] Qvalues: [15.16, 11.38,\n8.99, 8.52, 7.44, 7.26, 7.01, 6.99] Action: north Reward: 0, Score 30,\nDone False\nSTATE 68\nroom full of gadgets this is the south end of a long room. the walls\nand floor and ceiling are covered with thousands of metal gadgets\nthat you might want to examine more carefully. there is a path free\nof gadgets to the north end of the room. you are holding : a shovel\na torch button a torch ( providing light ) room full of gadgets this\nis the south end of a long room. the walls and floor and ceiling are\ncovered with thousands of metal gadgets that you might want to\nexamine more carefully. there is a path free of gadgets to the north\nend of the room. Actions: [’north’, ’push button’, ’south’, ’put shovel\ndown’, ’put torch down’, ’put button down’, ’put all down’] Qvalues:\n[16.71, 10.81, 9.31, 9.13, 8.81, 8.61, 8.58] Action: north Reward: 0,\nScore 30, Done False\nSTATE 69\nnorth gadget room the room stretches away back south. the metal\ngadgets are clustered even more densely here. sitting on the floor\nin a clear space is a cut crystal bowl. the bowl contains : a large\nperfect ruby a hundreds of quartz chipsyou are holding : a shovel\na torch button a torch ( providing light ) north gadget room the\nroom stretches away back south. the metal gadgets are clustered\neven more densely here. sitting on the floor in a clear space is a cut\ncrystal bowl. the bowl contains : a large perfect ruby a hundreds of\nquartz chips Actions: [’take chips’, ’push button’, ’put torch down’,\n’put all down’, ’put shovel down’, ’put button down’, ’south’] Qval-\nues: [13.84, 9.5, 8.06, 7.99, 7.98, 7.77, 7.26] Action: take chips Reward:\n10, Score 40, Done False\nSTATE 70\nas you do, several of the quartz chips spill onto the floor. some\nhit metal gadgets, triggering them. darts fly. some darts hit more\ngadgets. more darts fly. after a few seconds of this, you have been\npunctured several million times. you have died you are back in the\ninfinite blackness. eventually the voice returns, senses you disgust-\nedly, and speaks. \" when standing inside a mass of uranium, it is\nnot usually a good idea to drop a neutron. \" you’ve stumbled into\nfour traps. you’re not looking very good. \" round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. you are holding\n: a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center\nis a tall stone obelisk that you could probably use to get back up\nto the fake toilet. there are four stairways leaving the room at the\nfour compass points. there is a shovel lying here. Actions: [’push\nbutton’, ’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put button\ndown’, ’put torch down’, ’west’, ’put all down’, ’east’, ’up’] Qvalues:\n[9.13, 8.58, 8.54, 8.07, 7.74, 7.73, 7.45, 7.42, 7.36, 6.8, 5.76] Action:\npush button Reward: 0, Score 40, Done False\nSTATE 71\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’north’, ’south’, ’take shovel’, ’put button down’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.62, 9.35, 8.78, 8.78, 8.0, 7.93, 7.87, 7.55, 7.35, 7.17, 6.39] Action:\npush button Reward: 0, Score 40, Done False\nSTATE 72\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’put button down’, ’take shovel’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.63, 9.25, 8.68, 8.54, 7.96, 7.96, 7.71, 7.57, 7.32, 7.15, 6.03] Action:\neast Reward: 0, Score 40, Done False\nSTATE 73\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [8.01, 7.65, 7.22, 7.01, 6.94,\n6.93, 6.46] Action: east Reward: 0, Score 40, Done False\nSTATE 74\ncorridor at doorwayyou are holding : a torch button a torch ( pro-\nviding light ) corridor at doorway the hall goes east and west from\nhere and there is a doorway to the north. Actions: [’west’, ’push\nbutton’, ’north’, ’put torch down’, ’put all down’, ’put button down’,\n’east’] Qvalues: [7.61, 7.48, 7.24, 6.89, 6.78, 6.58, 5.01] Action: west\nReward: 0, Score 40, Done False\nSTATE 75\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [7.94, 7.53, 7.02, 6.93, 6.89,\n6.83, 6.15] Action: push button Reward: 0, Score 40, Done False\nSTATE 76\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [8.16, 7.75, 7.7, 7.34, 7.01, 6.85, 6.73] Action: west Reward:\n0, Score 40, Done False\nSTATE 77\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’put button down’, ’take shovel’, ’examine obelisk’,\n’put torch down’, ’west’, ’put all down’, ’up’] Qvalues: [9.42, 8.58,\n8.33, 7.99, 7.82, 7.74, 7.61, 7.2, 7.18, 7.05, 5.89] Action: west Reward:\n0, Score 40, Done False\nSTATE 78\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection the east \/ west corridor branches to the south\nhere. Actions: [’east’, ’push button’, ’west’, ’put torch down’, ’put\nall down’, ’put button down’, ’south’] Qvalues: [7.66, 7.53, 7.46, 6.63,\n6.6, 6.53, 6.31] Action: put all down Reward: 0, Score 40, Done False\nSTATE 79\ntorch button : dropped. torch : dropped. you are holding nothing.\nt intersection the east \/ west corridor branches to the south here.\nlying on the ground is a brass torch of the \" self igniting \" type.\nthe ignite button is on the handle. you can also see a torch button\nhere. Actions: [’west’, ’east’, ’push button’, ’south’, ’take all’, ’take\nbutton’, ’take torch’] Qvalues: [7.47, 7.33, 7.29, 7.08, 7.01, 6.79, 6.66]\nAction: take all Reward: 0, Score 40, Done False\nSTATE 80\ntorch : taken. torch button : taken. you are holding : a torch button\na torch ( providing light ) t intersection the east \/ west corridor\nbranches to the south here. Actions: [’east’, ’west’, ’push button’,\n’put all down’, ’put torch down’, ’put button down’, ’south’] Qvalues:\n[8.2, 7.56, 7.44, 6.46, 6.42, 6.34, 6.2] Action: east Reward: 0, Score 40,\nDone False\nSTATE 81\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’put button down’, ’take shovel’, ’examine obelisk’,\n’put torch down’, ’west’, ’put all down’, ’up’] Qvalues: [9.17, 8.31,\n8.05, 8.02, 7.6, 7.51, 7.33, 6.95, 6.94, 6.82, 5.48] Action: push button\nReward: 0, Score 40, Done False\nSTATE 82\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’put button down’, ’take shovel’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.47, 9.04, 8.33, 8.11, 7.78, 7.62, 7.35, 7.27, 7.01, 6.81, 5.67] Action:\neast Reward: 0, Score 40, Done False\nSTATE 83\nt intersectionyou are holding : a torch button a torch ( providing\nlight ) t intersection to the hall goes east from here. to the west\nis the stairway to the round room. a narrow passage leads south.\nActions: [’west’, ’push button’, ’south’, ’put torch down’, ’put all\ndown’, ’put button down’, ’east’] Qvalues: [7.63, 7.29, 6.78, 6.6, 6.56,\n6.53, 6.3] Action: push button Reward: 0, Score 40, Done False\nSTATE 84\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [7.92, 7.45, 7.27, 7.0, 6.68, 6.57, 6.44] Action: push button\nReward: 0, Score 40, Done False\nSTATE 85\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) t intersection to the\nhall goes east from here. to the west is the stairway to the round\nroom. a narrow passage leads south. Actions: [’west’, ’push button’,\n’east’, ’south’, ’put button down’, ’put torch down’, ’put all down’]\nQvalues: [7.89, 7.4, 7.27, 6.96, 6.64, 6.55, 6.41] Action: put all down\nReward: 0, Score 40, Done False\nSTATE 86\ntorch button : dropped. torch : dropped. you are holding nothing.\nt intersection to the hall goes east from here. to the west is the\nstairway to the round room. a narrow passage leads south. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. you can also see a torch button here. Ac-\ntions: [’east’, ’west’, ’push button’, ’take all’, ’south’, ’take button’,\n’take torch’] Qvalues: [7.4, 7.13, 7.1, 7.08, 7.05, 6.86, 6.75] Action:\ntake all Reward: 0, Score 40, Done False\nSTATE 87\ntorch : taken. torch button : taken. you are holding : a torch button\na torch ( providing light ) t intersection to the hall goes east from\nhere. to the west is the stairway to the round room. a narrow pas-\nsage leads south. Actions: [’west’, ’push button’, ’south’, ’east’, ’put\nbutton down’, ’put torch down’, ’put all down’] Qvalues: [7.89, 7.18,\n6.81, 6.68, 6.31, 6.29, 6.27] Action: west Reward: 0, Score 40, Done\nFalse\nSTATE 88\nround room there is a shovel lying here. you are holding : a torch\nbutton a torch ( providing light ) round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. there is a shovel lying here. Actions: [’push button’, ’south’,\n’north’, ’east’, ’take shovel’, ’put button down’, ’examine obelisk’,\n’west’, ’put torch down’, ’put all down’, ’up’] Qvalues: [8.74, 7.99,\n7.93, 7.42, 7.38, 7.27, 7.14, 6.84, 6.76, 6.65, 5.54] Action: push button\nReward: 0, Score 40, Done False\nSTATE 89\nthe torch flickers a few times and then glows steadily. you are hold-\ning : a torch button a torch ( providing light ) round room this room\nis at the peak of the pyramid. it is perfectly conical. in the center is\na tall stone obelisk that you could probably use to get back up to\nthe fake toilet. there are four stairways leaving the room at the four\ncompass points. there is a shovel lying here. Actions: [’east’, ’push\nbutton’, ’south’, ’north’, ’take shovel’, ’put button down’, ’exam-\nine obelisk’, ’west’, ’put torch down’, ’put all down’, ’up’] Qvalues:\n[10.16, 8.88, 8.24, 8.2, 7.52, 7.49, 7.35, 7.07, 6.85, 6.63, 5.99] Action:\nnorth Reward: 0, Score 40, Done False\nSTATE 90\nlandingyou are holding : a torch button a torch ( providing light )\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. Actions: [’north’,\n’push button’, ’south’, ’put button down’, ’put torch down’, ’east’,\n’put all down’] Qvalues: [8.7, 8.17, 7.45, 7.15, 6.9, 6.81, 6.81] Action:\nput all down Reward: 0, Score 40, Done False\nSTATE 91\ntorch button : dropped. torch : dropped. you are holding nothing.\nlanding this is a landing on the stairway. you can go down to the\nnorth or up to the south. a passage leads east. lying on the ground\nis a brass torch of the \" self igniting \" type. the ignite button is on\nthe handle. you can also see a torch button here. Actions: [’take\nall’, ’take torch’, ’north’, ’take button’, ’east’, ’push button’, ’south’]\nQvalues: [7.96, 7.45, 7.44, 7.34, 7.3, 7.3, 7.09] Action: north Reward:\n0, Score 40, Done False\nSTATE 92\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is at the peak of the pyramid. it is perfectly conical.\nin the center is a tall stone obelisk that you could probably use to\nget back up to the fake toilet. there are four stairways leaving the\nroom at the four compass points. there is a shovel lying here. you\nare holding : a torch ( providing light ) round room this room is at\nthe peak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four com-\npass points. there is a shovel lying here. Actions: [’south’, ’north’,\n’take shovel’, ’examine obelisk’, ’put torch down’, ’west’, ’east’, ’up’]\nQvalues: [7.93, 7.62, 7.36, 7.07, 6.82, 6.67, 6.48, 4.8] Action: put torch\ndown Reward: 0, Score 40, Done False\nSTATE 93\ndropped. you are holding nothing. round room this room is at the\npeak of the pyramid. it is perfectly conical. in the center is a tall\nstone obelisk that you could probably use to get back up to the fake\ntoilet. there are four stairways leaving the room at the four compass\npoints. lying on the ground is a brass torch of the \" self igniting \"\ntype. the ignite button is on the handle. there is a shovel lying here.\nActions: [’west’, ’south’, ’east’, ’take all’, ’north’, ’take brass’, ’take\nshovel’, ’examine obelisk’, ’up’] Qvalues: [7.63, 7.56, 7.54, 7.46, 7.46,\n7.34, 6.94, 6.73, 6.22] Action: examine obelisk Reward: 0, Score 40,\nDone False\nSTATE 94\nthe obelisk is a monument to the great rahn. the inscription starts :\n\" for rahn hath given us the postulates and the theorems ; we shalt\nthank rahn. for rahn hath said’between any two points there shalt\nbe but one line’; and he found it good. for rahn hath said’if two\nplanes intersecteth, they shalt form but one straight line’; and he\nfound it good. for rahn hath said... \" it goes on like this for a long\ntime. when you manage to break the trance, your mind is numbed\nand full of rahn’s strange commandments. round room this room\nis a conic surface with exits at even multiples of 90 degrees. in the\ncenter is a regular square pyramid with a narrow vertex angle. lying\non the ground is a brass torch of the \" self igniting \" type. the ignite\nbutton is on the handle. there is a shovel lying here. you are holding\nnothing. round room this room is a conic surface with exits at even\nmultiples of 90 degrees. in the center is a regular square pyramid\nwith a narrow vertex angle. lying on the ground is a brass torch of\nthe \" self igniting \" type. the ignite button is on the handle. there is\na shovel lying here. Actions: [’south’, ’take all’, ’east’, ’west’, ’take\nbrass’, ’north’, ’take shovel’, ’examine obelisk’, ’up’] Qvalues: [7.34,\n7.27, 7.13, 7.1, 7.08, 7.05, 6.59, 6.37, 5.73] Action: south Reward: 0,\nScore 40, Done False\nSTATE 95\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is a conic surface with exits at even multiples of 90\ndegrees. in the center is a regular square pyramid with a narrow\nvertex angle. there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [8.0, 7.7, 7.34, 7.26, 6.71, 6.65,\n6.15, 4.76] Action: examine obelisk Reward: 0, Score 40, Done False\nSTATE 97\nthe obelisk is a monument to the great rahn. the inscription starts :\n\" for rahn hath given us the postulates and the theorems ; we shalt\nthank rahn. for rahn hath said’between any two points there shalt\nbe but one line’; and he found it good. for rahn hath said’if two\nplanes intersecteth, they shalt form but one straight line’; and he\nfound it good. for rahn hath said... \" it goes on like this for a long\ntime. when you manage to break the trance, your mind is numbed\nand full of rahn’s strange commandments. round room this room\nis a conic surface with exits at even multiples of 90 degrees. in\nthe center is a regular square pyramid with a narrow vertex angle.\nthere is a shovel lying here. you are holding : a torch ( providing\nlight ) round room this room is a conic surface with exits at even\nmultiples of 90 degrees. in the center is a regular square pyramid\nwith a narrow vertex angle. there is a shovel lying here. Actions:\n[’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch down’,\n’west’, ’east’, ’up’] Qvalues: [7.91, 7.7, 7.27, 7.16, 6.6, 6.57, 6.45, 5.07]\nAction: put torch down Reward: 0, Score 40, Done False\nSTATE 98\ndropped. you are holding nothing. round room this room is a conic\nsurface with exits at even multiples of 90 degrees. in the center is a\nregular square pyramid with a narrow vertex angle. lying on the\nground is a brass torch of the \" self igniting \" type. the ignite button\nis on the handle. there is a shovel lying here. Actions: [’take all’,\n’south’, ’west’, ’take brass’, ’east’, ’north’, ’take shovel’, ’examine\nobelisk’, ’up’] Qvalues: [7.4, 7.34, 7.26, 7.24, 7.21, 7.19, 6.74, 6.51,\n6.06] Action: south Reward: 0, Score 40, Done False\nSTATE 99\ndarkness it is very dark. before you can do anything, something\nbegins gnawing on you. you have died you are back in the infinite\nblackness. eventually the voice returns, senses you disgustedly, and\nspeaks. \" that hardly counts as being killed by a trap, you know.\neven you could have gotten farther into that situation. \" you’ve\nstumbled into four traps. you’re not looking very good. \" round\nroom this room is a conic surface with exits at even multiples of 90\ndegrees. in the center is a regular square pyramid with a narrow\nvertex angle. there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [8.02, 7.67, 7.34, 7.23, 6.75, 6.66,\n6.23, 4.74] Action: west Reward: 0, Score 40, Done False\nSTATE 100\nt intersectionyou are holding : a torch ( providing light ) t intersec-\ntion the east \/ west corridor branches to the south here. Actions:\n[’west’, ’east’, ’put torch down’, ’south’] Qvalues: [7.21, 6.7, 6.47,\n5.87] Action: east Reward: 0, Score 40, Done True\nSTATE 101\nround room there is a shovel lying here. you are holding : a torch (\nproviding light ) round room this room is a conic surface with exits\nat even multiples of 90 degrees. in the center is a regular square\npyramid with a narrow vertex angle. there is a shovel lying here.\nActions: [’south’, ’north’, ’take shovel’, ’examine obelisk’, ’put torch\ndown’, ’west’, ’east’, ’up’] Qvalues: [7.82, 7.45, 7.12, 6.99, 6.59, 6.5,\n6.18, 4.91] Action: reset Reward: 0, Score 0, Done False\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 利用预训练语言模型提升文本游戏中的智能体表现\n\n## 📌 背景痛点\/本文动机\n文本世界游戏为人工智能体提供了理解和推理现实世界场景的机会。然而，这些游戏对智能体来说极具挑战性，因为它们需要在部分可观察的环境中理解和交互自然语言。现有的方法往往忽略了智能体的语言理解能力，并且通常从头开始训练，导致样本效率低下。本文旨在通过引入预训练语言模型来提升智能体的语义理解能力，从而在文本游戏中取得更好的表现。\n\n## 🚀 核心方法\n💡 创新点1：预训练语言模型作为先验知识\n本文提出了一种简单的RL与LM框架，使用基于Transformer的语言模型（如DistilBERT）与深度强化学习模型相结合。通过在大型通用英语语料库上进行预训练，然后针对特定下游任务进行微调，预训练语言模型能够为智能体提供丰富的语言理解和先验知识。\n\n💡 创新点2：游戏感知的预训练语言模型\n为了使预训练语言模型更好地适应游戏环境，本文使用独立的人类游戏播放轨迹数据集对DistilBERT进行微调，从而使其具备游戏感知能力。这种微调过程有助于将语言模型的知识和世界感知能力转移到不同的游戏和智能体中。\n\n## 📈 实验结果\n本文在14个文本游戏中对所提出的框架进行了评估，结果显示，在Zork1游戏中，模型取得了44.7分的成绩，比现有最佳模型高出1.6分。总体而言，该框架在4个游戏中超越了现有模型，并在其他游戏中表现与现有模型相当。\n\n## 💬 可借鉴之处\n本文提出的预训练语言模型作为先验知识的方法，为文本游戏中的智能体设计提供了新的思路。通过利用预训练语言模型的知识和世界感知能力，智能体能够更好地理解和推理游戏环境，从而在游戏中取得更好的表现。此外，本文还强调了微调预训练语言模型以适应特定游戏环境的重要性，这为未来研究提供了有价值的启示。","llm_summary_res_status":200}
{"title":"Guiding Pretraining in Reinforcement Learning with Large Language Models","authors":"Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas","summary":"Reinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks. Code available at\nhttps:\/\/github.com\/yuqingd\/ellm.","url":"http:\/\/arxiv.org\/abs\/2302.06692v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2302.06692v2","published":1676322963000,"comment":"ICML 2023","pdf_text":"Guiding Pretraining in Reinforcement Learning with Large Language Models\nYuqing Du * 1 Olivia Watkins * 1 Zihan Wang 2 C´edric Colas 3 4 Trevor Darrell 1 Pieter Abbeel 1\nAbhishek Gupta 2 Jacob Andreas 3\nAbstract\nReinforcement learning algorithms typically\nstruggle in the absence of a dense, well-shaped\nreward function. Intrinsically motivated explo-\nration methods address this limitation by reward-\ning agents for visiting novel states or transitions,\nbut these methods offer limited benefits in large\nenvironments where most discovered novelty is\nirrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from\ntext corpora to shape exploration. This method,\ncalled ELLM (Exploring with LLMs) rewards an\nagent for achieving goals suggested by a language\nmodel prompted with a description of the agent’s\ncurrent state. By leveraging large-scale language\nmodel pretraining, ELLM guides agents toward\nhuman-meaningful and plausibly useful behav-\niors without requiring a human in the loop. We\nevaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing\nthat ELLM-trained agents have better coverage\nof common-sense behaviors during pretraining\nand usually match or improve performance on\na range of downstream tasks. Code available at\nhttps:\/\/github.com\/yuqingd\/ellm.\n1. Introduction\nReinforcement learning algorithms work well when learners\nreceive frequent rewards that incentivize progress toward\ntarget behaviors. But hand-defining such reward functions\nrequires significant engineering efforts in all but the simplest\ncases (Amodei et al., 2016; Lehman et al., 2020). To master\n*Equal contribution\n1Department of Electrical Engineer-\ning and Computer Science, University of California, Berke-\nley, USA 2University of Washington, Seattle 3Massachusetts In-\nstitute of Technology, Computer Science and Artificial Intelli-\ngence Laboratory 4Inria, Flowers Laboratory. Correspondence to:\nYuqing Du <yuqing du@berkeley.edu>, Olivia Watkins <olivi-\nawatkins@berkeley.edu>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n1. Cut down the tree.\n2. Craft a pickaxe.\n3. Eat cow.\n4. Sleep.\n   . . . \nk. Build a wood house.\nYou see trees, \ncows, grass, \ntable, and \nbushes. You have \nwood in your \ninventory. You \nfeel hungry, \nthirsty, and \nsleepy. \nLLM\nPrompt:\nWhat should \nyou do next?\nFigure 1: ELLM uses a pretrained large language model\n(LLM) to suggest plausibly useful goals in a task-agnostic\nway.\nBuilding on LLM capabilities such as context-\nsensitivity and common-sense, ELLM trains RL agents to\npursue goals that are likely meaningful without requiring\ndirect human intervention. Prompt is illustrative; see full\nprompt and goal format in Appendix D.\ncomplex tasks in practice, RL agents may therefore need to\nlearn some behaviors in the absence of externally-defined\nrewards. What should they learn?\nIntrinsically motivated RL methods answer this question\nby augmenting rewards with auxiliary objectives based on\nnovelty, surprise, uncertainty, or prediction errors (Belle-\nmare et al., 2016; Pathak et al., 2017; Burda et al., 2019;\nZhang et al., 2021; Liu & Abbeel, 2021; Yarats et al., 2021).\nBut not everything novel or unpredictable is useful: noisy\nTVs and the movements of leaves on a tree may provide an\ninfinite amount of novelty, but do not lead to meaningful\nbehaviors (Burda et al., 2019). More recent approaches com-\npute novelty with higher-level representations like language\n(Tam et al., 2022; Mu et al., 2022), but can continue driving\nthe agent to explore behaviors that are unlikely to corre-\nspond to any human-meaningful goal—like enumerating\nunique configurations of furniture in a household. It is not\nsufficient for extrinsic-reward-free RL agents to optimize\nfor novelty alone: learned behaviors must also be useful.\nIn this paper, we describe a method for using not just\nlanguage-based representations but pretrained language\nmodels (LLMs) as a source of information about useful\nbehavior. LLMs are probabilistic models of text trained on\nlarge text corpora; their predictions encode rich information\n1\narXiv:2302.06692v2  [cs.LG]  15 Sep 2023\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nabout human common-sense knowledge and cultural conven-\ntions. Our method, Exploring with LLMs (ELLM), queries\nLMs for possible goals given an agent’s current context and\nrewards agents for accomplishing those suggestions. As a\nresult, exploration is biased towards completion of goals\nthat are diverse, context-sensitive, and human-meaningful.\nELLM-trained agents exhibit better coverage of useful be-\nhaviors during pretraining, and outperform or match base-\nlines when fine-tuned on downstream tasks.\n2. Background and Related Work\nIntrinsically Motivated RL.\nWhen reward functions are\nsparse, agents often need to carry out a long, specific se-\nquence of actions to achieve target tasks. As action spaces\nor target behaviors grow more complex, the space of alter-\nnative action sequences agents can explore grows combi-\nnatorially. In such scenarios, undirected exploration that\nrandomly perturbs actions or policy parameters has little\nchance of succeeding (Ten et al., 2022; Ladosz et al., 2022).\nMany distinct action sequences can lead to similar out-\ncomes (Baranes & Oudeyer, 2013)—for example, most\naction sequences cause a humanoid agent to fall, while\nvery few make it walk. Building on this observation, in-\ntrinsically motivated RL algorithms (IM-RL) choose to\nexplore outcomes rather than actions (Oudeyer & Kaplan,\n2009; Ten et al., 2022; Ladosz et al., 2022). Knowledge-\nbased IMs (KB-IMs) focus on maximising the diversity of\nstates (reviews in Aubret et al., 2019; Linke et al., 2020).\nCompetence-based IMs (CB-IMs) maximise the diversity\nof skills mastered by the agent (review in Colas et al., 2022).\nBecause most action sequences lead to a very restricted part\nof the outcome space (e.g. all different ways of falling on\nthe floor likely correspond to a single outcome), these meth-\nods lead to a greater diversity of outcomes than undirected\nexploration (Lehman et al., 2008; Colas et al., 2018).\nHowever, maximizing diversity of outcomes may not always\nbe enough. Complex environments can contain sources of\ninfinite novelty. In such environments, seeking ever-more-\nnovel states might drive learning towards behaviors that\nhave little relevance to the true task reward. Humans do\nnot explore outcome spaces uniformly, but instead rely on\ntheir physical and social common-sense to explore plausibly-\nuseful behaviors first. In video games, they know that keys\nshould be used to open doors, ladders should be climbed,\nand snakes might be enemies. If this semantic information\nis removed, their exploration becomes severely impacted\n(Dubey et al., 2018). The approach we introduce in this\npaper, ELLM, may be interpreted as a CB-IM algorithm\nthat seeks to explore the space of possible and plausibly-\nuseful skills informed by human prior knowledge.\nLinguistic Goals and Pretrained Language Models.\nOne way of representing a diverse outcome space for ex-\nploration is through language. Training agents to achieve\nlanguage goals brings several advantages: (1) goals are easy\nto express for non-expert users; (2) they can be more abstract\nthan standard state-based goals (Colas et al., 2022); and (3)\nagents can generalize better thanks to the partial composi-\ntionality and recursivity of language (Hermann et al., 2017;\nHill et al., 2019; Colas et al., 2020). Such linguistic goals\ncan be used as instructions for language-conditioned imita-\ntion learning or RL. In RL, agents typically receive language\ninstructions corresponding to the relevant reward functions\n(Luketina et al., 2019) and are only rarely intrinsically mo-\ntivated (with the exception of Mu et al., 2022; Colas et al.,\n2020; Tam et al., 2022), where language is also used as a\nmore general compact state abstraction for task-agnostic\nexploration.\nRepresenting goals in language unlocks the possibility of us-\ning text representations and generative models of text (large\nlanguage models, or LLMs) trained on large corpora. In im-\nitation learning, text pretraining can help learners automat-\nically recognize sub-goals and learn modular sub-policies\nfrom unlabelled demonstrations (Lynch & Sermanet, 2020;\nSharma et al., 2021), or chain pre-trained goal-oriented poli-\ncies together to accomplish high-level tasks (Yao et al., 2020;\nHuang et al., 2022a; Ahn et al., 2022; Huang et al., 2022b).\nIn RL, LM-encoded goal descriptions greatly improve the\ngeneralization of instruction-following agents across instruc-\ntions (Chan et al., 2019) and from synthetic to natural goals\n(Hill et al., 2020). LLMs have also been used as proxy\nreward functions when prompted with desired behaviors\n(Kwon et al., 2023). Unlike these approaches, ELLM uses\npretrained LLMs to constrain exploration towards plausibly-\nuseful goals in a task-agnostic manner. It does not assume a\npretrained low-level policy, demonstrations, or task-specific\nprompts. Most similar to our work, Choi et al. (2022) also\nprompt LLMs for priors. However, they use LM priors to\nclassify safe and unsafe states to reward, which is a subset\nof common-sense exploratory behaviors ELLM should gen-\nerate. Also similar to our work, Kant et al. (2022) query\nLLMs for zero-shot commonsense priors in the Housekeep\nenvironment, but they apply these to a planning task rather\nthan as rewards for reinforcement learning.\n3. Structuring Exploration with LLM Priors\n3.1. Problem Description\nWe consider partially observed Markov decision processes\ndefined by a tuple (S, A, O, Ω, T , γ, R), in which obser-\nvations o ∈Ωderive from environment states s ∈S and\nactions a ∈A via O(o | s, a). T (s′ | s, a) describes the\ndynamics of the environment while R and γ are the envi-\nronment’s reward function and discount factor.\n2\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nText \nobs\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n“You see grass, \ntrees, bushes, cows, \nand a crafting table. \nYour inventory has \nwood.”\nValid actions: sleep, eat, attack, chop, \ndrink, place, make, mine. You are a player \nplaying a game. Suggest the best actions the \nplayer can take based on the things you see \nand the items in your inventory. Only use \nvalid actions and objects.\nYou see plant, tree, and skeleton. You are \ntargeting skeleton. What do you do?\n- Eat plant, chop tree, attack skeleton\n{current obs}\nWhat do you do? \nLLM\n(a) Policy parametrization for ELLM. We optionally condition on embeddings of\nthe goals Etext(g1:k\nt\n) and state Etext(Cobs(ot)).\nLM Embed\nLM Embed\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n“Chop tree”\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n(b) LLM reward scheme. We reward the agent for\nthe similarity between the captioned transition and\nthe goals.\nFigure 2: ELLM uses GPT-3 to suggest adequate exploratory goals and SentenceBERT embeddings to compute the similarity\nbetween suggested goals and demonstrated behaviors as a form of intrinsically-motivated reward.\nIM agents optimize for an intrinsic reward Rint alongside\nor in place of R. CB-IM methods, in particular, define Rint\nvia a family of goal-conditioned reward functions:\nRint(o, a, o′) = Eg∼G [Rint(o, a, o′ | g)] .\n(1)\nA CB-IM agent is expected to perform well with respect to\nthe original R when the intrinsic reward Rint is both easier\nto optimize and well aligned with R, such that behaviors\nmaximizing Rint also maximize R. Every CB-IM algorithm\nmust define two elements in Equation 1: (1) the distribution\nof goals to sample from, i.e. G, and (2) the goal-conditioned\nreward functions Rint(o, a, o′ | g). Given these, A CB-IM\nalgorithm trains a goal-conditioned policy π(a | o, g) to\nmaximize Rint. For some intrinsic reward functions, agents\nmay achieve high reward under the original reward function\nR immediately; for others, additional fine-tuning with R\nmay be required. In Equation (1), the space of goals G is\ndetermined by the goal-conditioned reward function Rint(· |\ng): every choice of g induces a corresponding distribution\nover optimal behaviors.\n3.2. Goal-based Exploration Desiderata\nHow should we choose G and Rint(· | g) to help agents\nmake progress toward general reward functions R? Goals\ntargeted during exploration should satisfy three properties:\n• Diverse: targeting diverse goals increases the chance\nthat the target behavior is similar to one of them.\n• Common-sense sensitive: learning should focus on\nfeasible goals (chop a tree > drink a tree) which\nare likely under the distribution of goals humans care\nabout (drink water > walk into lava).\n• Context sensitive: learning should focus on goals that\nare feasible in the current environment configuration\n(e.g. chop a tree only if a tree is in view).\nMost CB-IM algorithms hand-define the reward functions\nRint (2) and the support of the goal distribution (1) in align-\nment with the original task R, but use various intrinsic mo-\ntivations to guide goal sampling (1): e.g. novelty, learning\nprogress, intermediate difficulty (see a review in Colas et al.,\n2022). In Exploring with Large Language Models (ELLM),\nwe propose to leverage language-based goal representations\nand language-model-based goal generation to alleviate the\nneed for environment-specific hand-coded definitions of (1)\nand (2). We hypothesize that world knowledge captured in\nLLMs will enable the automatic generation of goals that are\ndiverse, human-meaningful and context sensitive.\n3.3. Goal Generation with LLMs (G)\nPretrained large language models broadly fall into three cat-\negories: autoregressive, masked, or encoder-decoder mod-\nels (Min et al., 2021). Autoregressive models (e.g. GPT;\nRadford et al., 2018), are trained to maximize the log-\nlikelihood of the next word given all previous words, and are\nthus capable of language generation. Encoder-only models\n(e.g. BERT; Devlin et al., 2018), are trained with a masked\nobjective, enabling effective encoding of sentence seman-\ntics. Pretraining LMs on large text corpora yields impressive\nzero- or few-shot on diverse language understanding and\ngeneration tasks, including tasks requiring not just linguistic\nknowledge but world knowledge (Brown et al., 2020).\nELLM uses autoregressive LMs to generate goals and\nmasked LMs to build vector representations of goals. When\nLLMs generate goals, the support of the goal distribution\nbecomes as large as the space of natural language strings.\nWhile querying LLMs unconditionally for goals can offer\n3\nGuiding Pretraining in Reinforcement Learning with Large Language Models\ndiversity and common-sense sensitivity, context-sensitivity\nrequires knowledge of agent state. Thus, at each timestep\nwe acquire goals by prompting the LLM with a list of the\nagent’s available actions and a text description of the current\nobservation via a state captioner Cobs : Ω→Σ∗, where Σ∗\nis the set of all strings (see Figure 2).\nWe investigate two concrete strategies for extracting goals\nfrom LLMs: (1) open-ended generation, in which the LLM\noutputs text descriptions of suggested goals (e.g. next you\nshould...), and (2) closed-form, in which a possible goal\nis given to the LLM as a QA task (e.g. Should the agent\ndo X? (Yes\/No)). Here the LLM goal suggestion is only\naccepted when the log-probability of Yes is greater than\nNo. The former is more suited for open-ended exploration\nand the latter is more suited for environments with large\nbut delimitable goal spaces. While the LLM does not have\nprior knowledge of all possible goals, we can provide some\nguidance towards desirable suggestions through few-shot\nprompting. See Appendix D for the full prompt.\n3.4. Rewarding LLM Goals (Rint)\nNext we consider the goal-conditioned reward (2). We com-\npute rewards for a given goal g (Rint in Eq. 1) by measuring\nthe semantic similarity between the LLM-generated goal\nand the description of the agent’s transition in the envi-\nronment as computed by a transition captioner Ctransition :\nΩ× A × Ω→Σ:\nRint(o, a, o′ | g) =\n(\n∆(Ctransition(o, a, o′), g)\nif > T\n0\notherwise.\nHere, the semantic similarity function ∆(· , ·) is defined as\nthe cosine similarity between representations from an LM\nencoder E(·) of captions and goals:\n∆(Ctransition(o, a, o′), g) =\nE(Ctransition(o, a, o′)) · E(g)\n∥E(Ctransition(o, a, o′))∥∥E(g)∥.\nIn practice, we use a pretrained SentenceBERT model\n(Reimers & Gurevych, 2019) for E(·). We choose cosine\nsimilarity to measure alignment between atomic agent ac-\ntions and freeform LLM generations, as done in prior work\n(Huang et al., 2022a). When the caption of a transition is\nsufficiently close to the goal description (∆> T), where\nT is a similarity threshold hyperparameter, the agent is re-\nwarded proportionally to their similarity. Finally, since there\ncan be multiple goals suggested, we reward the agent for\nachieving any of the k suggestions by taking the maximum\nof the goal-specific rewards:\n∆max = max\ni=1...k ∆\n\u0000Ctransition(ot, at, ot+1), gi\nt\n\u0001\n.\nAs a result, the general reward function of CB-IM methods\nfrom Equation 1 can be rewritten:\nRint(o, a, o′) = ELLM(g1 .. k|Cobs(o)) [∆max] .\n(2)\n3.5. Implementation Details\nThe full ELLM algorithm is summarized in Algorithm 1.\nSee Figure 1 for the high-level pipeline. To impose a nov-\nelty bias, we also filter out LM suggestions that the agent\nhas already achieved earlier in the same episode. This pre-\nvents the agent from exploring the same goal repeatedly. In\nAppendix L we show this step is essential to the method.\nWe consider two forms of agent training: (1) a goal-\nconditioned setting where the agent is given a sentence\nembedding of the list of suggested goals, π(a | o, E(g1:k)),\nand (2) a goal-free setting where the agent does not have ac-\ncess to the suggested goals, π(a | o). While Rint remains the\nsame in either case, training a goal-conditioned agent intro-\nduces both challenges and benefits: it can take time for the\nagent to learn the meaning of the different goals and connect\nit to the reward, but having a language-goal conditioned pol-\nicy can be more amenable to downstream tasks than an agent\njust trained on an exploration reward. We also consider two\ntypes of policy inputs– (1) just the partially observed pixel\nobservations, or (2) the pixel observations combined with\nthe embedded language-state captions E(Cobs(o)). Since\n(2) performs better (see analysis in Appendix A), we use\n(2) for all paper experiments unless otherwise specified. All\nvariants are trained with the DQN algorithm (Mnih et al.,\n2013), with implementation details in Appendix H.\nThis paper focuses on the benefits of LLM priors for RL\nexploration and mostly assumes a pre-existing captioning\nfunction. In simulation, this can be acquired for free with\nthe ground truth simulator state. For real world applications,\none can use object-detection (Zaidi et al., 2022), caption-\ning models (Stefanini et al., 2022), or action recognition\nmodels (Kong & Fu, 2022). Alternatively, one could use\nmulti-modal vision-language models with a similar LM\ncomponent (Alayrac et al., 2022). To test the robustness of\nour method under varying captioning quality, Section 4.1\nstudies a relaxation of these assumptions by looking at a\nvariant of ELLM using a learned captioner trained on human\ndescriptions.\n4. Experiments\nOur experiments test the following hypotheses:\n• (H1) Prompted pretrained LLMs can generate\nplausibly-useful exploratory goals satisfying the\ndesiderata listed in Section 3.2: diversity, common-\nsense and context sensitivity.\n• (H2) Training an ELLM agent on these exploratory\ngoals improves performance on downstream tasks com-\npared to methods that do not leverage LLM-priors.\nWe evaluate ELLM in two complex environments:\n(1) Crafter, an open-ended environment in which explo-\nration is required to discover long-term survival strategies\n4\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nAlgorithm 1 ELLM Algorithm\nInitialize untrained policy π\nt ←0\not ←env.RESET()\nwhile t < max env steps do\n# Generate k suggestions, filtering achieved ones\ng1:k\nt\n←PREV ACHIEVED(LLM(Cobs(ot)))\n# Interact with the environment\nat ∼π(at|ot, E(Cobs(ot))), E(g1:k\nt\n))\nst+1 ←env.STEP(at)\n# Compute suggestion achievement reward\nrt ←0\n∆max ←maxi=1...k∆(Ctransition(ot, at, ot+1), gi\nt)\nif ∆max > threshold then\nrt = ∆max\nend if\n# Update agent using any RL algorithm\nBuffert+1 ←Buffert ∪(ot, at, g1:k\nt\n, rt, ot+1)\nπ ←UPDATE(π, Buffert+1)\nend while\nYou see {observation}. \nYou have in your inventory {items}*. \nYou feel {health status}*.\n  \n      *omitted if empty.\nYou see bush, grass, plant, tree, and \nwater. You have in your inventory \nsapling.\n-\nPlant sapling\n-\nChop tree\n-\nChop bush\nSeen objects: {object, receptacle}. \nSeen receptacles: {receptacles}. \nYou are holding {gripped_object}. \nSeen objects: clock in kitchen sink.\nSeen receptacles: kitchen bottom \ncabinet, kitchen sink, living room \nshelf, living room carpet …\nYou are holding a cereal box.\n-\nPlace cereal box in kitchen cabinet \n-\nPick clock\nFigure 3: Sample templated captions and suggested goals.\n(Hafner, 2021), and (2) Housekeep, an embodied robotics\nenvironment that requires common-sense to restrict the ex-\nploration of possible rearrangements of household objects\n(Kant et al., 2022). Besides environment affordances, these\nenvironments also differ in viewpoint (3rd vs 1st person) and\naction space (large high-level vs low-level). In each envi-\nronment, we compare ELLM with existing IM-RL methods\n(Liu & Abbeel, 2021; Burda et al., 2019), an oracle with\nground-truth rewards, and ablations of ELLM; see Table 1.\n4.1. Crafter\nEnvironment description.\nWe first test ELLM in the\nCrafter environment, a 2D version of Minecraft (Hafner,\n2021). Like Minecraft, Crafter is a procedurally generated\nand partially observable world that enables collecting and\ncreating a set of artifacts organized along an achievement\ntree which lists all possible achievements and their respec-\ntive prerequisites (see Figure 4 in Hafner, 2021). Although\nCrafter does not come with a single main task to solve, we\ncan track agent progress along the achievement tree.\nWe modify the original game in two ways. Crafter’s original\naction space already incorporates a great deal of human\ndomain knowledge: a single do action is interpreted in dif-\nferent ways based on the agent’s context, each of which\nwould correspond to a very different low-level action in a\nreal environment (‘do’ means ‘attack’ in front of a zombie\nbut ‘eat’ in front of a plant). We remove this assistance\nby augmenting the action space with more specific verb +\nnoun pairs that are not guaranteed to be useful (e.g. ‘eat\nzombie’). This makes it possible in Crafter to attempt a\nwide range of irrelevant\/nonsensical tasks, providing an\nopportunity for an LM narrow the goal space down to rea-\nsonable goals. See Appendix C for details. Second, to\nmake RL training easier across all conditions, we increase\nthe damage the agent does against enemies and reduce the\namount of wood required to craft a table from 2 to 1; see\nAppendix Figure 10 for comparisons.\nWe use Codex (Chen et al., 2021) as our LLM with the\nopen-ended suggestion generation variant of ELLM, where\nwe directly take the generated text from the LLM as the set\nof suggested goals to reward. Each query prompt consists\nof a list of possible verbs the agent can use (but not a list of\nall possible nouns), a description of the agent’s current state,\nand the question ‘What do you do?’. We add two examples\nof similar queries to the start of the prompt in order to guide\nthe language model to format suggestions in a consistent\nway; see the full prompt in Appendix D.\nGoals suggested by the LLM.\nTo answer H1, we study\nthe goals suggested by the LLM in Table 2: are they diverse,\ncontext-sensitive and common-sensical? The majority of\nsuggested goals (64.9%) are context-sensitive, sensible, and\nachievable in the game. Most of the 5% of goals not allowed\nby Crafter’s physics (e.g. build a house) are context- and\ncommon-sensitive as well. The last third of the goals vi-\nolate either context-sensitivity (13.6%) or common-sense\n(16.4%). See Appendix K for details.\nPretraining exploration performance.\nA perfect explo-\nration method would unlock all Crafter achievements in\nevery episode, even without prior knowledge of the set\nof possible achievements. Thus, we measure exploration\nquality as the average number of unique achievements per\nepisode across pretraining (Figure 4). Although it is not\ngiven access to Crafter’s achievement tree, ELLM learns\nto unlock about 6 achievements every episode, against 9\nfor the ground-truth-reward Oracle (Figure 4). It outper-\nforms all exploration methods that only focus on generat-\ning novel behaviors (APT, RND, Novelty) — all limited to\nless than 3 achievements in average. As shown in Table 2,\nELLM does not only focus on novelty but also generates\n5\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nMethod\nDescription\nELLM (ours)\nRewards the agent for achieving any goal suggested by the LLM using the similarity-based reward functions Rint\ndefined in Eq. 2. It only rewards the agent for achieving a given goal once per episode (novelty bias).\nOracle\nThe upper bound: it suggests all context-sensitive goals at any step, only common-sensical ones (from the list of\n(Crafter only)\nvalid goals) and uses the same novelty bias as ELLM. Rewards are computed exactly with a hard-coded Rint.\nNovelty\nThis baseline removes the common-sense sensitivity assumption of the Oracle and rewards the agent for\nachieving any of the goals expressible in the environment including invalid ones (e.g. drink tree) as long as the\nagent performs the goal-reaching action in the right context (e.g. while facing a tree). Uses a hard-coded Rint and\na novelty bias like the Oracle.\nUniform\nThis variant removes the novelty bias from Novelty and samples uniformly from the set of expressible goals.\nAPT\nState-of-the-art KB-IM algorithm that maximizes state entropy computed as the distance between the current\n(Liu & Abbeel, 2021)\nstate’s embedding es and its K nearest neighbors es[1..K] within a minibatch uniformly sampled from memory.\nThere is no goal involved and Rint = log ∥es −es[1..K]∥.\nRND\nState-of-the-art KB-IM algorithm that rewards the agent for maximizing a form of novelty estimated by the\n(Burda et al., 2019)\nprediction error of a model h trained to predict the output of a random network ˜h. Rint = ∥h(s, a) −˜h(s, a)∥.\nTable 1: Descriptions of the compared algorithms. (Additional comparisons in Appendix N).\nSuggested\nRewarded\nContext-Insensitive\n13.6%\n1.1%\nCommon-Sense Insensitive\n16.4%\n32.4%\nGood\n64.9%\n66.5%\nImpossible\n5.0%\n0%\nTable 2: Fractions of suggested and rewarded goals that fail\nto satisfy context-sensitivity or common-sense sensitivity;\nthat satisfy these properties and are achievable in Crafter\n(Good); or that are not allowed by Crafter’s physics. See\nAppendix K for examples of each.\n0\n100\n200\n300\n400\n500\nEnvironment Steps (x 1e4)\n0\n2\n4\n6\n8\n10\n12\nCrafter Achievements\nOracle (goals)\nOracle (no goals)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nNovelty\nUniform\nFigure 4: Ground truth achievements unlocked per episode\nacross pretraining, mean±std across 5 seeds.\ncommon-sensical goals. This boosts exploration in Crafter,\nsupporting H1.\nAs discussed in Section 3.5, we also test variants of each\nmethod (with \/ without goal conditioning, with \/ without\ntext observations) where applicable. We do not find goal\nconditioning to bring a significant advantage in performance\nduring pretraining. The non-conditioned agent might infer\nthe goals (and thus the rewarded behaviors) from context\nalone. Similarly to Mu et al. (2022) and Tam et al. (2022),\nwe find that agents trained on visual + textual observations\n(as computed by E(Cobs(o))) outperform agents trained on\nvisual observations only for all the tested variants (opaque\nvs semi-transparent bars in Appendix Figure 8). That said,\noptimizing for novelty alone, whether in visual or semantic\nspaces, seems to be insufficient to fully solve Crafter.\nThe na¨ıve approach of finetuning a pretrained policy on\nthe downstream task performs poorly across all pretraining\nalgorithms. We hypothesize this is because relevant features\nand Q-values change significantly between pretraining and\nfinetuning, especially when the density of rewards changes.\nInstead, we find it is more effective to use the pretrained\npolicy for guided exploration. We initialize and train a new\nagent, but replace 50% of the algorithm’s randomly-sampled\nϵ-greedy exploration actions with actions sampled from the\npretrained policy. In Appendix M we include the poor\nfinetuning results discuss why we think guided exploration\ndoes better.\nFigure 5 compares the downstream performance of ELLM\nto the performance of the two strongest baselines RND and\nAPT using both transfer methods. (full comparisons with all\nbaselines shown in Appendix B). For the goal-conditioned\nversion of ELLM, we provide the agent with the sequence\nof subgoals required to achieve the task. Even though not all\n6\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nsubgoals were mastered during pretraining, we still observe\nthat the goal-conditioned pretrained agents outperform the\nunconditioned ones.\nPerformance of the different methods varies widely task-\nto-task and even seed-to-seed since each task requires a\ndifferent set of skills, and any given agent may or may\nnot have learned a particular skill during pretraining. For\ninstance, ELLM agents typically learn to place crafting\ntables and attack cows during pretraining, leading to low-\nvariance learning curves. They typically do not learn to\nmake wood swords, so we see a high-variance learning curve\nwhich depends on how quickly each agent stumbles across\nthe goal during finetuning. Despite the variance, we see that\ngoal-conditioned ELLM stands out as the best-performing\nmethod on average. Notably, ELLM (both goal-conditioned\nand goal-free) is the only method with nonzero performance\nacross all tasks.\nELLM with imperfect transition captioner.\nPerfect cap-\ntioners might not be easy to obtain in some environments.\nHowever, trained captioners might generate more linguis-\ntic diversity and make mistakes. To test the robustness of\nELLM to diverse and imperfect captions, we replace the\noracle transition captioner Ctransition with a captioner trained\non a mixture of human and synthetic data (847+900 labels)\nusing the ClipCap algorithm (Mokady et al., 2021b). Syn-\nthetic data removes some of the human labor while still\nproviding a diversity of captions for any single transition\n(3 to 8). Appendix J presents implementation details and\nanalyzes how the trained captioner might cause errors in\ngenerated rewards. Although its false negative rate is low\n(it detects goal achievements well), its false positive rate\nis rather high. This means it might generate rewards for\nachievements that were not unlocked due to a high simi-\nlarity between the generated caption and goal description\ngenerated by the LLM. In ELLM pretraining, we use the\nlearned captioner to caption transitions where an action is\nsuccessful and use that caption to compute the reward via the\nsimilarity metric (see Section 3). Figure 6 shows that ELLM\nperformance is overall robust to this imperfect captioner.\n4.2. Housekeep\nEnvironment description. Housekeep is an embodied\nrobotics environment where the agent is tasked with clean-\ning up a house by rearranging misplaced objects (Kant et al.,\n2022). The agent must successfully match the environment’s\nground truth correct mapping of objects to receptacles with-\nout direct instructions specifying how objects need to be re-\narranged. This mapping was determined via crowd-sourcing\ncommon-sense object-receptacle combinations. An exam-\nple layout of the task can be found in Figure 1 in Kant et al.\n(2022). Common-sense priors are necessary for learning to\nrearrange misplaced objects into reasonable configurations.\nTask 1\nTask 2\nTask 3\nTask 4\nMatch Acc.\n85.7%\n87.5%\n50%\n66.7%\nMismatch Acc.\n93.8%\n90.1%\n94.0%\n87.6%\nTable 3: Classification accuracy of LLM for each Housekeep\ntask (top row is true positives, bottom row is true negatives).\nWe focus on a simplified subset of Housekeep consisting\nof 4 different scenes with one room each, each with 5 dif-\nferent misplaced objects and a suite of different possible\nreceptacles; see Appendix F for details. Because the agent\ndoes not have access to the ground truth target locations,\nwe use the game reward’s rearrangement success rate as\na measure of exploration quality: common-sensical explo-\nration should perform better. A success rate of 100% means\nthe agent has picked and placed all 5 misplaced objects in\ncorrect locations. Note that we intentionally focus on a\ndomain where the downstream application benefits strongly\nfrom exploring reasonable goals during pretraining. Rather\nthan designing reward functions that correspond to all cor-\nrect rearrangements for all possible objects, we investigate\nwhether ELLM can be a general purpose method that guides\nlearning human-meaningful behaviors.\nUnlike Crafter’s combinatorial and high-level action space,\nHousekeep operates with low-level actions: moving forward,\nturning, looking up or down, and picking or placing an ob-\nject. This allows us to investigate whether ELLM enables\nhigh-level exploration despite using lower-level control. We\nassume access to an egocentric instance segmentation sensor\nto generate captions of in-view objects and receptacles, and\nuse the text-davinci-002 InstructGPT model (Ouyang\net al., 2022) as our LLM. Given a description of visible\nobjects, the receptacles the objects are currently in, and all\npreviously seen receptacles, we create a list of all possible\nobject-receptacle mappings. We use the closed-form vari-\nant of ELLM and query the LLM for whether each object\nshould be placed in each receptacle as a yes\/no question.\nBy querying for each object-receptacle combination indi-\nvidually, we are able to cache and efficiently reuse LLM\nqueries. The agent can be given two types of goals: (1) pick-\ning an object if it is not already in a suggested receptacle,\nand (2) placing a gripped object in a suggested receptacle.\nGoals suggested by LLM.\nIn Housekeep, we assess LLM\ngoals by looking at the classification accuracy of correct\nand incorrect arrangements (Table 3). We find that the LLM\naccuracy at identifying mismatches (e.g. vase in kitchen\nsink) are all above 87%, however, accuracy of identifying\nmatches varies greatly depending on the available objects\nand receptacles (ranging from 50-90%). Since there are\nonly a few correct positions, each false negative hurts ac-\ncuracy greatly. Taking a closer look, we find that some\n7\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n0\n10\n20\nEnv Steps (x 1e4)\n0.00\n0.25\n0.50\n0.75\n1.00\nSuccess Rate\nPlace Crafting Table\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nAttack Cow\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nMake Wood Sword\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nMine Stone\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nDeforestation\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nPlant Row\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nGardening\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\n0.0\n2.5\n5.0\n7.5\n10.0\nCrafter Score\nCrafter (Game Reward)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nScratch\nFigure 5: Success rates across training for each of the seven downstream tasks in the Crafter environment. Each run trains\nan agent from scratch while leveraging a pretrained policy for exploration. Plots show mean ± std for 5 seeds. Some plots\nhave multiple overlapping curves at 0.\n0\n2\n4\n6\n8\n10\n12\n14\nSteps x1e4\n0\n2\n4\n6\n8\nCrafter Achievements\nPretraining With the Captioner\nOracle (goals) - GT Captioner\nOracle (goals) - ELLM Captioner\nELLM (goals) - GT Captioner\nELLM (goals) - ELLM Captioner\nFigure 6: Pretraining with a learned captioner vs a ground\ntruth captioner. We see performance drops, especially for\nELLM, but still relatively good performance. (3 seeds,\nmean± std.)\nLLM labels are reasonable despite disagreeing with the en-\nvironment mapping: e.g. suggesting vase in living room\ntable, and not suggesting pan in living room cabinet.\nThis suggests that there are ambiguities in the ground truth\nmappings, likely due to human disagreement.\nPretraining and downstream performance.\nTo investi-\ngate H1, we compare ELLM against the strongest baselines\n(RND, APT, Novelty) described in Table 1. In Housekeep\nthe novelty baseline rewards the agent for novel instances of\npick or place actions in an episode, allowing us to differenti-\nate between success attributable solely to the captioner and\nthe pick\/place prior, and success attributable to any LLM\ncommon-sense priors. For brevity, we focus only on the\npixel + text-observation variant of all methods. Sample ef-\nficiency curves measuring the ground truth rearrangement\nsuccess during both pretraining and finetuning are shown\nin Figure 7a. In three of the four tasks, we find that the\nELLM bias leads to higher success rates during pretraining,\nsuggesting coverage better aligned with the downstream\ntask compared to the baselines. We also find much higher\npretraining success rates in the first two tasks. Since Table 3\nshows higher LLM accuracy for these two tasks, this differ-\nence shows the impact of LLM inaccuracies on pretraining.\nFor H2, we test two different ways of using the pretrained\nmodels in the downstream rearrangement task. First, we\ndirectly finetune the pretrained model on the ground truth\ncorrect rearrangement; shown after the dashed vertical line\nin Figure 7a. Here, the success rates for finetuned ELLM\nmatches or outperform the baselines, especially if pretrain-\ning has already led to high success rates. Interestingly, we\nalso find that the goal-conditioned ELLM variant consis-\ntently suffers a drop in performance when finetuning starts.\nWe hypothesize this is due to the treatment of all suggested\ngoals as a single string, so if any single goal changes be-\ntween pretraining and finetuning the agent must relearn the\ngoal embedding changes. Second, in Figure 7b we present\nresults for directly training a new agent on the downstream\ntask, using the frozen pretrained model as an exploratory\nactor during ϵ-greedy exploration. Once again, we observe\nthat ELLM consistently matches or outperforms all base-\nlines. We also see here that the KB-IM baselines are more\ncompetitive, suggesting that this training scheme is better\nsuited for pretrained exploration agents that are not well-\naligned to the downstream task.\n5. Conclusions and Discussion\nWe have presented ELLM, an intrinsic motivation method\nthat aims to bias exploration towards common-sense and\nplausibly useful behaviors via a pretrained LLM. We have\nshown that such priors are useful for pretraining agents in\nextrinsic-reward-free settings that require common-sense\nbehaviors that other exploration methods fail to capture.\nELLM goes beyond standard novelty search approaches by\nconcentrating exploration on common-sensical goals. This\nis helpful in environments offering a wide array of possible\nbehaviors among which very few can said to be plausibly\nuseful. It is less helpful in environments with little room\nfor goal-based exploration, when human common-sense\nis irrelevant or cannot be expressed in language (e.g. fine-\ngrained manipulation), or where state information is not\nnaturally encoded as a natural language string.\nLLM performance is sensitive to prompt choice. Even with\na well-chosen prompt, LLMs sometimes make errors, often\ndue to missing domain-specific knowledge. False nega-\ntives can permanently prevent the agent from learning a\nkey skill: in Crafter, for example, the LLM never suggests\ncreating wood pickaxes. There are multiple avenues to ad-\ndress this limitation: (1) combining ELLM rewards with\nother KB-IM rewards like RND, (2) prompting LLMs with\ndescriptions of past achievements (or other feedback about\n8\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nELLM (goals) - Ours\nELLM (no goals) - Ours\nAPT\nRND\nNovelty (goals)\nNovelty (no goals)\nFinetuning Begins\n0\n2\n4\nEnvironment Steps\n1e6\n0.0\n0.2\n0.4\nHousekeep Success Rate\n0\n2\n4\nEnvironment Steps\n1e6\n0\n2\n4\nEnvironment Steps\n1e6\n0\n2\n4\nEnvironment Steps\n1e6\n(a) Pretraining and finetuning: pretraining for 4M steps then fine-\ntuning for 1M steps on the ground truth correct arrangement.\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.2\n0.4\nHousekeep Success Rate\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n(b) Downstream evaluation: Using the frozen pretrained exploration\npolicies only for ϵ-greedy-style action selection for 1M steps.\nFigure 7: Housekeep: Correct arrangement success rates on 4 object-receptacle task sets. Mean ± std over 5 seeds.\nenvironment dynamics) so that LLMs can learn about the\nspace of achievable goals, (3) injecting domain knowledge\ninto LLM prompts, or (4) fine-tuning LLMs on task-specific\ndata. While ELLM does not rely on this domain knowledge,\nwhen this information exists it is easy to incorporate.\nELLM requires states and transition captions. Our learned\ncaptioner experiments Figure 6 suggest we can learn these\nfrom human-labeled samples, but in some environments\ntraining this captioner might be less efficient than collecting\ndemonstrations or hard-coding a reward function. Still, we\nare optimistic that as progress in general-purpose caption-\ning models continues, off-the-shelf captioners will become\nfeasible for more tasks. Lastly, suggestion quality improves\nconsiderably with model size. Querying massive LLMs\nregularly may be time- and cost-prohibitive in some RL\nenvironments.\nAs general-purpose generative models become available in\ndomains other than text, ELLM-like approaches might also\nbe used to suggest plausible visual goals, or goals in other\nstate representations. ELLM may thus serve as a platform\nfor future work that develops even more general and flexible\nstrategies for incorporating human background knowledge\ninto reinforcement learning.\n6. Acknowledgements\nYD and OW are funded by the Center for Human-\nCompatible Artificial Intelligence. CC received funding\nfrom the European Union’s Horizon 2020 research and in-\nnovation programme under the Marie Skłodowska-Curie\ngrant agreement No. 101065949. This material is based\nupon work supported by the National Science Foundation\nunder Grant No. 2212310 to AG and JA. OpenAI credits for\nGPT-3 access were provided through OpenAI’s Researcher\nAccess Program. We thank Sam Toyer and the members of\nthe RLL for feedback on early iterations of this project.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Guiding Pretraining in Reinforcement Learning with Large Language Models.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n```\n#### 2. 论文摘要\n```\nReinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks. Code available at\nhttps:\/\/github.com\/yuqingd\/ellm.\n```\n\n#### 3. 论文全文\n```\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nYuqing Du * 1 Olivia Watkins * 1 Zihan Wang 2 C´edric Colas 3 4 Trevor Darrell 1 Pieter Abbeel 1\nAbhishek Gupta 2 Jacob Andreas 3\nAbstract\nReinforcement learning algorithms typically\nstruggle in the absence of a dense, well-shaped\nreward function. Intrinsically motivated explo-\nration methods address this limitation by reward-\ning agents for visiting novel states or transitions,\nbut these methods offer limited benefits in large\nenvironments where most discovered novelty is\nirrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from\ntext corpora to shape exploration. This method,\ncalled ELLM (Exploring with LLMs) rewards an\nagent for achieving goals suggested by a language\nmodel prompted with a description of the agent’s\ncurrent state. By leveraging large-scale language\nmodel pretraining, ELLM guides agents toward\nhuman-meaningful and plausibly useful behav-\niors without requiring a human in the loop. We\nevaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing\nthat ELLM-trained agents have better coverage\nof common-sense behaviors during pretraining\nand usually match or improve performance on\na range of downstream tasks. Code available at\nhttps:\/\/github.com\/yuqingd\/ellm.\n1. Introduction\nReinforcement learning algorithms work well when learners\nreceive frequent rewards that incentivize progress toward\ntarget behaviors. But hand-defining such reward functions\nrequires significant engineering efforts in all but the simplest\ncases (Amodei et al., 2016; Lehman et al., 2020). To master\n*Equal contribution\n1Department of Electrical Engineer-\ning and Computer Science, University of California, Berke-\nley, USA 2University of Washington, Seattle 3Massachusetts In-\nstitute of Technology, Computer Science and Artificial Intelli-\ngence Laboratory 4Inria, Flowers Laboratory. Correspondence to:\nYuqing Du <yuqing du@berkeley.edu>, Olivia Watkins <olivi-\nawatkins@berkeley.edu>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n1. Cut down the tree.\n2. Craft a pickaxe.\n3. Eat cow.\n4. Sleep.\n   . . . \nk. Build a wood house.\nYou see trees, \ncows, grass, \ntable, and \nbushes. You have \nwood in your \ninventory. You \nfeel hungry, \nthirsty, and \nsleepy. \nLLM\nPrompt:\nWhat should \nyou do next?\nFigure 1: ELLM uses a pretrained large language model\n(LLM) to suggest plausibly useful goals in a task-agnostic\nway.\nBuilding on LLM capabilities such as context-\nsensitivity and common-sense, ELLM trains RL agents to\npursue goals that are likely meaningful without requiring\ndirect human intervention. Prompt is illustrative; see full\nprompt and goal format in Appendix D.\ncomplex tasks in practice, RL agents may therefore need to\nlearn some behaviors in the absence of externally-defined\nrewards. What should they learn?\nIntrinsically motivated RL methods answer this question\nby augmenting rewards with auxiliary objectives based on\nnovelty, surprise, uncertainty, or prediction errors (Belle-\nmare et al., 2016; Pathak et al., 2017; Burda et al., 2019;\nZhang et al., 2021; Liu & Abbeel, 2021; Yarats et al., 2021).\nBut not everything novel or unpredictable is useful: noisy\nTVs and the movements of leaves on a tree may provide an\ninfinite amount of novelty, but do not lead to meaningful\nbehaviors (Burda et al., 2019). More recent approaches com-\npute novelty with higher-level representations like language\n(Tam et al., 2022; Mu et al., 2022), but can continue driving\nthe agent to explore behaviors that are unlikely to corre-\nspond to any human-meaningful goal—like enumerating\nunique configurations of furniture in a household. It is not\nsufficient for extrinsic-reward-free RL agents to optimize\nfor novelty alone: learned behaviors must also be useful.\nIn this paper, we describe a method for using not just\nlanguage-based representations but pretrained language\nmodels (LLMs) as a source of information about useful\nbehavior. LLMs are probabilistic models of text trained on\nlarge text corpora; their predictions encode rich information\n1\narXiv:2302.06692v2  [cs.LG]  15 Sep 2023\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nabout human common-sense knowledge and cultural conven-\ntions. Our method, Exploring with LLMs (ELLM), queries\nLMs for possible goals given an agent’s current context and\nrewards agents for accomplishing those suggestions. As a\nresult, exploration is biased towards completion of goals\nthat are diverse, context-sensitive, and human-meaningful.\nELLM-trained agents exhibit better coverage of useful be-\nhaviors during pretraining, and outperform or match base-\nlines when fine-tuned on downstream tasks.\n2. Background and Related Work\nIntrinsically Motivated RL.\nWhen reward functions are\nsparse, agents often need to carry out a long, specific se-\nquence of actions to achieve target tasks. As action spaces\nor target behaviors grow more complex, the space of alter-\nnative action sequences agents can explore grows combi-\nnatorially. In such scenarios, undirected exploration that\nrandomly perturbs actions or policy parameters has little\nchance of succeeding (Ten et al., 2022; Ladosz et al., 2022).\nMany distinct action sequences can lead to similar out-\ncomes (Baranes & Oudeyer, 2013)—for example, most\naction sequences cause a humanoid agent to fall, while\nvery few make it walk. Building on this observation, in-\ntrinsically motivated RL algorithms (IM-RL) choose to\nexplore outcomes rather than actions (Oudeyer & Kaplan,\n2009; Ten et al., 2022; Ladosz et al., 2022). Knowledge-\nbased IMs (KB-IMs) focus on maximising the diversity of\nstates (reviews in Aubret et al., 2019; Linke et al., 2020).\nCompetence-based IMs (CB-IMs) maximise the diversity\nof skills mastered by the agent (review in Colas et al., 2022).\nBecause most action sequences lead to a very restricted part\nof the outcome space (e.g. all different ways of falling on\nthe floor likely correspond to a single outcome), these meth-\nods lead to a greater diversity of outcomes than undirected\nexploration (Lehman et al., 2008; Colas et al., 2018).\nHowever, maximizing diversity of outcomes may not always\nbe enough. Complex environments can contain sources of\ninfinite novelty. In such environments, seeking ever-more-\nnovel states might drive learning towards behaviors that\nhave little relevance to the true task reward. Humans do\nnot explore outcome spaces uniformly, but instead rely on\ntheir physical and social common-sense to explore plausibly-\nuseful behaviors first. In video games, they know that keys\nshould be used to open doors, ladders should be climbed,\nand snakes might be enemies. If this semantic information\nis removed, their exploration becomes severely impacted\n(Dubey et al., 2018). The approach we introduce in this\npaper, ELLM, may be interpreted as a CB-IM algorithm\nthat seeks to explore the space of possible and plausibly-\nuseful skills informed by human prior knowledge.\nLinguistic Goals and Pretrained Language Models.\nOne way of representing a diverse outcome space for ex-\nploration is through language. Training agents to achieve\nlanguage goals brings several advantages: (1) goals are easy\nto express for non-expert users; (2) they can be more abstract\nthan standard state-based goals (Colas et al., 2022); and (3)\nagents can generalize better thanks to the partial composi-\ntionality and recursivity of language (Hermann et al., 2017;\nHill et al., 2019; Colas et al., 2020). Such linguistic goals\ncan be used as instructions for language-conditioned imita-\ntion learning or RL. In RL, agents typically receive language\ninstructions corresponding to the relevant reward functions\n(Luketina et al., 2019) and are only rarely intrinsically mo-\ntivated (with the exception of Mu et al., 2022; Colas et al.,\n2020; Tam et al., 2022), where language is also used as a\nmore general compact state abstraction for task-agnostic\nexploration.\nRepresenting goals in language unlocks the possibility of us-\ning text representations and generative models of text (large\nlanguage models, or LLMs) trained on large corpora. In im-\nitation learning, text pretraining can help learners automat-\nically recognize sub-goals and learn modular sub-policies\nfrom unlabelled demonstrations (Lynch & Sermanet, 2020;\nSharma et al., 2021), or chain pre-trained goal-oriented poli-\ncies together to accomplish high-level tasks (Yao et al., 2020;\nHuang et al., 2022a; Ahn et al., 2022; Huang et al., 2022b).\nIn RL, LM-encoded goal descriptions greatly improve the\ngeneralization of instruction-following agents across instruc-\ntions (Chan et al., 2019) and from synthetic to natural goals\n(Hill et al., 2020). LLMs have also been used as proxy\nreward functions when prompted with desired behaviors\n(Kwon et al., 2023). Unlike these approaches, ELLM uses\npretrained LLMs to constrain exploration towards plausibly-\nuseful goals in a task-agnostic manner. It does not assume a\npretrained low-level policy, demonstrations, or task-specific\nprompts. Most similar to our work, Choi et al. (2022) also\nprompt LLMs for priors. However, they use LM priors to\nclassify safe and unsafe states to reward, which is a subset\nof common-sense exploratory behaviors ELLM should gen-\nerate. Also similar to our work, Kant et al. (2022) query\nLLMs for zero-shot commonsense priors in the Housekeep\nenvironment, but they apply these to a planning task rather\nthan as rewards for reinforcement learning.\n3. Structuring Exploration with LLM Priors\n3.1. Problem Description\nWe consider partially observed Markov decision processes\ndefined by a tuple (S, A, O, Ω, T , γ, R), in which obser-\nvations o ∈Ωderive from environment states s ∈S and\nactions a ∈A via O(o | s, a). T (s′ | s, a) describes the\ndynamics of the environment while R and γ are the envi-\nronment’s reward function and discount factor.\n2\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nText \nobs\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n“You see grass, \ntrees, bushes, cows, \nand a crafting table. \nYour inventory has \nwood.”\nValid actions: sleep, eat, attack, chop, \ndrink, place, make, mine. You are a player \nplaying a game. Suggest the best actions the \nplayer can take based on the things you see \nand the items in your inventory. Only use \nvalid actions and objects.\nYou see plant, tree, and skeleton. You are \ntargeting skeleton. What do you do?\n- Eat plant, chop tree, attack skeleton\n{current obs}\nWhat do you do? \nLLM\n(a) Policy parametrization for ELLM. We optionally condition on embeddings of\nthe goals Etext(g1:k\nt\n) and state Etext(Cobs(ot)).\nLM Embed\nLM Embed\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n“Chop tree”\n-\nCut down the tree.\n-\nDig in the grass.\n-\nAttack the cow.\n(b) LLM reward scheme. We reward the agent for\nthe similarity between the captioned transition and\nthe goals.\nFigure 2: ELLM uses GPT-3 to suggest adequate exploratory goals and SentenceBERT embeddings to compute the similarity\nbetween suggested goals and demonstrated behaviors as a form of intrinsically-motivated reward.\nIM agents optimize for an intrinsic reward Rint alongside\nor in place of R. CB-IM methods, in particular, define Rint\nvia a family of goal-conditioned reward functions:\nRint(o, a, o′) = Eg∼G [Rint(o, a, o′ | g)] .\n(1)\nA CB-IM agent is expected to perform well with respect to\nthe original R when the intrinsic reward Rint is both easier\nto optimize and well aligned with R, such that behaviors\nmaximizing Rint also maximize R. Every CB-IM algorithm\nmust define two elements in Equation 1: (1) the distribution\nof goals to sample from, i.e. G, and (2) the goal-conditioned\nreward functions Rint(o, a, o′ | g). Given these, A CB-IM\nalgorithm trains a goal-conditioned policy π(a | o, g) to\nmaximize Rint. For some intrinsic reward functions, agents\nmay achieve high reward under the original reward function\nR immediately; for others, additional fine-tuning with R\nmay be required. In Equation (1), the space of goals G is\ndetermined by the goal-conditioned reward function Rint(· |\ng): every choice of g induces a corresponding distribution\nover optimal behaviors.\n3.2. Goal-based Exploration Desiderata\nHow should we choose G and Rint(· | g) to help agents\nmake progress toward general reward functions R? Goals\ntargeted during exploration should satisfy three properties:\n• Diverse: targeting diverse goals increases the chance\nthat the target behavior is similar to one of them.\n• Common-sense sensitive: learning should focus on\nfeasible goals (chop a tree > drink a tree) which\nare likely under the distribution of goals humans care\nabout (drink water > walk into lava).\n• Context sensitive: learning should focus on goals that\nare feasible in the current environment configuration\n(e.g. chop a tree only if a tree is in view).\nMost CB-IM algorithms hand-define the reward functions\nRint (2) and the support of the goal distribution (1) in align-\nment with the original task R, but use various intrinsic mo-\ntivations to guide goal sampling (1): e.g. novelty, learning\nprogress, intermediate difficulty (see a review in Colas et al.,\n2022). In Exploring with Large Language Models (ELLM),\nwe propose to leverage language-based goal representations\nand language-model-based goal generation to alleviate the\nneed for environment-specific hand-coded definitions of (1)\nand (2). We hypothesize that world knowledge captured in\nLLMs will enable the automatic generation of goals that are\ndiverse, human-meaningful and context sensitive.\n3.3. Goal Generation with LLMs (G)\nPretrained large language models broadly fall into three cat-\negories: autoregressive, masked, or encoder-decoder mod-\nels (Min et al., 2021). Autoregressive models (e.g. GPT;\nRadford et al., 2018), are trained to maximize the log-\nlikelihood of the next word given all previous words, and are\nthus capable of language generation. Encoder-only models\n(e.g. BERT; Devlin et al., 2018), are trained with a masked\nobjective, enabling effective encoding of sentence seman-\ntics. Pretraining LMs on large text corpora yields impressive\nzero- or few-shot on diverse language understanding and\ngeneration tasks, including tasks requiring not just linguistic\nknowledge but world knowledge (Brown et al., 2020).\nELLM uses autoregressive LMs to generate goals and\nmasked LMs to build vector representations of goals. When\nLLMs generate goals, the support of the goal distribution\nbecomes as large as the space of natural language strings.\nWhile querying LLMs unconditionally for goals can offer\n3\nGuiding Pretraining in Reinforcement Learning with Large Language Models\ndiversity and common-sense sensitivity, context-sensitivity\nrequires knowledge of agent state. Thus, at each timestep\nwe acquire goals by prompting the LLM with a list of the\nagent’s available actions and a text description of the current\nobservation via a state captioner Cobs : Ω→Σ∗, where Σ∗\nis the set of all strings (see Figure 2).\nWe investigate two concrete strategies for extracting goals\nfrom LLMs: (1) open-ended generation, in which the LLM\noutputs text descriptions of suggested goals (e.g. next you\nshould...), and (2) closed-form, in which a possible goal\nis given to the LLM as a QA task (e.g. Should the agent\ndo X? (Yes\/No)). Here the LLM goal suggestion is only\naccepted when the log-probability of Yes is greater than\nNo. The former is more suited for open-ended exploration\nand the latter is more suited for environments with large\nbut delimitable goal spaces. While the LLM does not have\nprior knowledge of all possible goals, we can provide some\nguidance towards desirable suggestions through few-shot\nprompting. See Appendix D for the full prompt.\n3.4. Rewarding LLM Goals (Rint)\nNext we consider the goal-conditioned reward (2). We com-\npute rewards for a given goal g (Rint in Eq. 1) by measuring\nthe semantic similarity between the LLM-generated goal\nand the description of the agent’s transition in the envi-\nronment as computed by a transition captioner Ctransition :\nΩ× A × Ω→Σ:\nRint(o, a, o′ | g) =\n(\n∆(Ctransition(o, a, o′), g)\nif > T\n0\notherwise.\nHere, the semantic similarity function ∆(· , ·) is defined as\nthe cosine similarity between representations from an LM\nencoder E(·) of captions and goals:\n∆(Ctransition(o, a, o′), g) =\nE(Ctransition(o, a, o′)) · E(g)\n∥E(Ctransition(o, a, o′))∥∥E(g)∥.\nIn practice, we use a pretrained SentenceBERT model\n(Reimers & Gurevych, 2019) for E(·). We choose cosine\nsimilarity to measure alignment between atomic agent ac-\ntions and freeform LLM generations, as done in prior work\n(Huang et al., 2022a). When the caption of a transition is\nsufficiently close to the goal description (∆> T), where\nT is a similarity threshold hyperparameter, the agent is re-\nwarded proportionally to their similarity. Finally, since there\ncan be multiple goals suggested, we reward the agent for\nachieving any of the k suggestions by taking the maximum\nof the goal-specific rewards:\n∆max = max\ni=1...k ∆\n\u0000Ctransition(ot, at, ot+1), gi\nt\n\u0001\n.\nAs a result, the general reward function of CB-IM methods\nfrom Equation 1 can be rewritten:\nRint(o, a, o′) = ELLM(g1 .. k|Cobs(o)) [∆max] .\n(2)\n3.5. Implementation Details\nThe full ELLM algorithm is summarized in Algorithm 1.\nSee Figure 1 for the high-level pipeline. To impose a nov-\nelty bias, we also filter out LM suggestions that the agent\nhas already achieved earlier in the same episode. This pre-\nvents the agent from exploring the same goal repeatedly. In\nAppendix L we show this step is essential to the method.\nWe consider two forms of agent training: (1) a goal-\nconditioned setting where the agent is given a sentence\nembedding of the list of suggested goals, π(a | o, E(g1:k)),\nand (2) a goal-free setting where the agent does not have ac-\ncess to the suggested goals, π(a | o). While Rint remains the\nsame in either case, training a goal-conditioned agent intro-\nduces both challenges and benefits: it can take time for the\nagent to learn the meaning of the different goals and connect\nit to the reward, but having a language-goal conditioned pol-\nicy can be more amenable to downstream tasks than an agent\njust trained on an exploration reward. We also consider two\ntypes of policy inputs– (1) just the partially observed pixel\nobservations, or (2) the pixel observations combined with\nthe embedded language-state captions E(Cobs(o)). Since\n(2) performs better (see analysis in Appendix A), we use\n(2) for all paper experiments unless otherwise specified. All\nvariants are trained with the DQN algorithm (Mnih et al.,\n2013), with implementation details in Appendix H.\nThis paper focuses on the benefits of LLM priors for RL\nexploration and mostly assumes a pre-existing captioning\nfunction. In simulation, this can be acquired for free with\nthe ground truth simulator state. For real world applications,\none can use object-detection (Zaidi et al., 2022), caption-\ning models (Stefanini et al., 2022), or action recognition\nmodels (Kong & Fu, 2022). Alternatively, one could use\nmulti-modal vision-language models with a similar LM\ncomponent (Alayrac et al., 2022). To test the robustness of\nour method under varying captioning quality, Section 4.1\nstudies a relaxation of these assumptions by looking at a\nvariant of ELLM using a learned captioner trained on human\ndescriptions.\n4. Experiments\nOur experiments test the following hypotheses:\n• (H1) Prompted pretrained LLMs can generate\nplausibly-useful exploratory goals satisfying the\ndesiderata listed in Section 3.2: diversity, common-\nsense and context sensitivity.\n• (H2) Training an ELLM agent on these exploratory\ngoals improves performance on downstream tasks com-\npared to methods that do not leverage LLM-priors.\nWe evaluate ELLM in two complex environments:\n(1) Crafter, an open-ended environment in which explo-\nration is required to discover long-term survival strategies\n4\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nAlgorithm 1 ELLM Algorithm\nInitialize untrained policy π\nt ←0\not ←env.RESET()\nwhile t < max env steps do\n# Generate k suggestions, filtering achieved ones\ng1:k\nt\n←PREV ACHIEVED(LLM(Cobs(ot)))\n# Interact with the environment\nat ∼π(at|ot, E(Cobs(ot))), E(g1:k\nt\n))\nst+1 ←env.STEP(at)\n# Compute suggestion achievement reward\nrt ←0\n∆max ←maxi=1...k∆(Ctransition(ot, at, ot+1), gi\nt)\nif ∆max > threshold then\nrt = ∆max\nend if\n# Update agent using any RL algorithm\nBuffert+1 ←Buffert ∪(ot, at, g1:k\nt\n, rt, ot+1)\nπ ←UPDATE(π, Buffert+1)\nend while\nYou see {observation}. \nYou have in your inventory {items}*. \nYou feel {health status}*.\n  \n      *omitted if empty.\nYou see bush, grass, plant, tree, and \nwater. You have in your inventory \nsapling.\n-\nPlant sapling\n-\nChop tree\n-\nChop bush\nSeen objects: {object, receptacle}. \nSeen receptacles: {receptacles}. \nYou are holding {gripped_object}. \nSeen objects: clock in kitchen sink.\nSeen receptacles: kitchen bottom \ncabinet, kitchen sink, living room \nshelf, living room carpet …\nYou are holding a cereal box.\n-\nPlace cereal box in kitchen cabinet \n-\nPick clock\nFigure 3: Sample templated captions and suggested goals.\n(Hafner, 2021), and (2) Housekeep, an embodied robotics\nenvironment that requires common-sense to restrict the ex-\nploration of possible rearrangements of household objects\n(Kant et al., 2022). Besides environment affordances, these\nenvironments also differ in viewpoint (3rd vs 1st person) and\naction space (large high-level vs low-level). In each envi-\nronment, we compare ELLM with existing IM-RL methods\n(Liu & Abbeel, 2021; Burda et al., 2019), an oracle with\nground-truth rewards, and ablations of ELLM; see Table 1.\n4.1. Crafter\nEnvironment description.\nWe first test ELLM in the\nCrafter environment, a 2D version of Minecraft (Hafner,\n2021). Like Minecraft, Crafter is a procedurally generated\nand partially observable world that enables collecting and\ncreating a set of artifacts organized along an achievement\ntree which lists all possible achievements and their respec-\ntive prerequisites (see Figure 4 in Hafner, 2021). Although\nCrafter does not come with a single main task to solve, we\ncan track agent progress along the achievement tree.\nWe modify the original game in two ways. Crafter’s original\naction space already incorporates a great deal of human\ndomain knowledge: a single do action is interpreted in dif-\nferent ways based on the agent’s context, each of which\nwould correspond to a very different low-level action in a\nreal environment (‘do’ means ‘attack’ in front of a zombie\nbut ‘eat’ in front of a plant). We remove this assistance\nby augmenting the action space with more specific verb +\nnoun pairs that are not guaranteed to be useful (e.g. ‘eat\nzombie’). This makes it possible in Crafter to attempt a\nwide range of irrelevant\/nonsensical tasks, providing an\nopportunity for an LM narrow the goal space down to rea-\nsonable goals. See Appendix C for details. Second, to\nmake RL training easier across all conditions, we increase\nthe damage the agent does against enemies and reduce the\namount of wood required to craft a table from 2 to 1; see\nAppendix Figure 10 for comparisons.\nWe use Codex (Chen et al., 2021) as our LLM with the\nopen-ended suggestion generation variant of ELLM, where\nwe directly take the generated text from the LLM as the set\nof suggested goals to reward. Each query prompt consists\nof a list of possible verbs the agent can use (but not a list of\nall possible nouns), a description of the agent’s current state,\nand the question ‘What do you do?’. We add two examples\nof similar queries to the start of the prompt in order to guide\nthe language model to format suggestions in a consistent\nway; see the full prompt in Appendix D.\nGoals suggested by the LLM.\nTo answer H1, we study\nthe goals suggested by the LLM in Table 2: are they diverse,\ncontext-sensitive and common-sensical? The majority of\nsuggested goals (64.9%) are context-sensitive, sensible, and\nachievable in the game. Most of the 5% of goals not allowed\nby Crafter’s physics (e.g. build a house) are context- and\ncommon-sensitive as well. The last third of the goals vi-\nolate either context-sensitivity (13.6%) or common-sense\n(16.4%). See Appendix K for details.\nPretraining exploration performance.\nA perfect explo-\nration method would unlock all Crafter achievements in\nevery episode, even without prior knowledge of the set\nof possible achievements. Thus, we measure exploration\nquality as the average number of unique achievements per\nepisode across pretraining (Figure 4). Although it is not\ngiven access to Crafter’s achievement tree, ELLM learns\nto unlock about 6 achievements every episode, against 9\nfor the ground-truth-reward Oracle (Figure 4). It outper-\nforms all exploration methods that only focus on generat-\ning novel behaviors (APT, RND, Novelty) — all limited to\nless than 3 achievements in average. As shown in Table 2,\nELLM does not only focus on novelty but also generates\n5\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nMethod\nDescription\nELLM (ours)\nRewards the agent for achieving any goal suggested by the LLM using the similarity-based reward functions Rint\ndefined in Eq. 2. It only rewards the agent for achieving a given goal once per episode (novelty bias).\nOracle\nThe upper bound: it suggests all context-sensitive goals at any step, only common-sensical ones (from the list of\n(Crafter only)\nvalid goals) and uses the same novelty bias as ELLM. Rewards are computed exactly with a hard-coded Rint.\nNovelty\nThis baseline removes the common-sense sensitivity assumption of the Oracle and rewards the agent for\nachieving any of the goals expressible in the environment including invalid ones (e.g. drink tree) as long as the\nagent performs the goal-reaching action in the right context (e.g. while facing a tree). Uses a hard-coded Rint and\na novelty bias like the Oracle.\nUniform\nThis variant removes the novelty bias from Novelty and samples uniformly from the set of expressible goals.\nAPT\nState-of-the-art KB-IM algorithm that maximizes state entropy computed as the distance between the current\n(Liu & Abbeel, 2021)\nstate’s embedding es and its K nearest neighbors es[1..K] within a minibatch uniformly sampled from memory.\nThere is no goal involved and Rint = log ∥es −es[1..K]∥.\nRND\nState-of-the-art KB-IM algorithm that rewards the agent for maximizing a form of novelty estimated by the\n(Burda et al., 2019)\nprediction error of a model h trained to predict the output of a random network ˜h. Rint = ∥h(s, a) −˜h(s, a)∥.\nTable 1: Descriptions of the compared algorithms. (Additional comparisons in Appendix N).\nSuggested\nRewarded\nContext-Insensitive\n13.6%\n1.1%\nCommon-Sense Insensitive\n16.4%\n32.4%\nGood\n64.9%\n66.5%\nImpossible\n5.0%\n0%\nTable 2: Fractions of suggested and rewarded goals that fail\nto satisfy context-sensitivity or common-sense sensitivity;\nthat satisfy these properties and are achievable in Crafter\n(Good); or that are not allowed by Crafter’s physics. See\nAppendix K for examples of each.\n0\n100\n200\n300\n400\n500\nEnvironment Steps (x 1e4)\n0\n2\n4\n6\n8\n10\n12\nCrafter Achievements\nOracle (goals)\nOracle (no goals)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nNovelty\nUniform\nFigure 4: Ground truth achievements unlocked per episode\nacross pretraining, mean±std across 5 seeds.\ncommon-sensical goals. This boosts exploration in Crafter,\nsupporting H1.\nAs discussed in Section 3.5, we also test variants of each\nmethod (with \/ without goal conditioning, with \/ without\ntext observations) where applicable. We do not find goal\nconditioning to bring a significant advantage in performance\nduring pretraining. The non-conditioned agent might infer\nthe goals (and thus the rewarded behaviors) from context\nalone. Similarly to Mu et al. (2022) and Tam et al. (2022),\nwe find that agents trained on visual + textual observations\n(as computed by E(Cobs(o))) outperform agents trained on\nvisual observations only for all the tested variants (opaque\nvs semi-transparent bars in Appendix Figure 8). That said,\noptimizing for novelty alone, whether in visual or semantic\nspaces, seems to be insufficient to fully solve Crafter.\nThe na¨ıve approach of finetuning a pretrained policy on\nthe downstream task performs poorly across all pretraining\nalgorithms. We hypothesize this is because relevant features\nand Q-values change significantly between pretraining and\nfinetuning, especially when the density of rewards changes.\nInstead, we find it is more effective to use the pretrained\npolicy for guided exploration. We initialize and train a new\nagent, but replace 50% of the algorithm’s randomly-sampled\nϵ-greedy exploration actions with actions sampled from the\npretrained policy. In Appendix M we include the poor\nfinetuning results discuss why we think guided exploration\ndoes better.\nFigure 5 compares the downstream performance of ELLM\nto the performance of the two strongest baselines RND and\nAPT using both transfer methods. (full comparisons with all\nbaselines shown in Appendix B). For the goal-conditioned\nversion of ELLM, we provide the agent with the sequence\nof subgoals required to achieve the task. Even though not all\n6\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nsubgoals were mastered during pretraining, we still observe\nthat the goal-conditioned pretrained agents outperform the\nunconditioned ones.\nPerformance of the different methods varies widely task-\nto-task and even seed-to-seed since each task requires a\ndifferent set of skills, and any given agent may or may\nnot have learned a particular skill during pretraining. For\ninstance, ELLM agents typically learn to place crafting\ntables and attack cows during pretraining, leading to low-\nvariance learning curves. They typically do not learn to\nmake wood swords, so we see a high-variance learning curve\nwhich depends on how quickly each agent stumbles across\nthe goal during finetuning. Despite the variance, we see that\ngoal-conditioned ELLM stands out as the best-performing\nmethod on average. Notably, ELLM (both goal-conditioned\nand goal-free) is the only method with nonzero performance\nacross all tasks.\nELLM with imperfect transition captioner.\nPerfect cap-\ntioners might not be easy to obtain in some environments.\nHowever, trained captioners might generate more linguis-\ntic diversity and make mistakes. To test the robustness of\nELLM to diverse and imperfect captions, we replace the\noracle transition captioner Ctransition with a captioner trained\non a mixture of human and synthetic data (847+900 labels)\nusing the ClipCap algorithm (Mokady et al., 2021b). Syn-\nthetic data removes some of the human labor while still\nproviding a diversity of captions for any single transition\n(3 to 8). Appendix J presents implementation details and\nanalyzes how the trained captioner might cause errors in\ngenerated rewards. Although its false negative rate is low\n(it detects goal achievements well), its false positive rate\nis rather high. This means it might generate rewards for\nachievements that were not unlocked due to a high simi-\nlarity between the generated caption and goal description\ngenerated by the LLM. In ELLM pretraining, we use the\nlearned captioner to caption transitions where an action is\nsuccessful and use that caption to compute the reward via the\nsimilarity metric (see Section 3). Figure 6 shows that ELLM\nperformance is overall robust to this imperfect captioner.\n4.2. Housekeep\nEnvironment description. Housekeep is an embodied\nrobotics environment where the agent is tasked with clean-\ning up a house by rearranging misplaced objects (Kant et al.,\n2022). The agent must successfully match the environment’s\nground truth correct mapping of objects to receptacles with-\nout direct instructions specifying how objects need to be re-\narranged. This mapping was determined via crowd-sourcing\ncommon-sense object-receptacle combinations. An exam-\nple layout of the task can be found in Figure 1 in Kant et al.\n(2022). Common-sense priors are necessary for learning to\nrearrange misplaced objects into reasonable configurations.\nTask 1\nTask 2\nTask 3\nTask 4\nMatch Acc.\n85.7%\n87.5%\n50%\n66.7%\nMismatch Acc.\n93.8%\n90.1%\n94.0%\n87.6%\nTable 3: Classification accuracy of LLM for each Housekeep\ntask (top row is true positives, bottom row is true negatives).\nWe focus on a simplified subset of Housekeep consisting\nof 4 different scenes with one room each, each with 5 dif-\nferent misplaced objects and a suite of different possible\nreceptacles; see Appendix F for details. Because the agent\ndoes not have access to the ground truth target locations,\nwe use the game reward’s rearrangement success rate as\na measure of exploration quality: common-sensical explo-\nration should perform better. A success rate of 100% means\nthe agent has picked and placed all 5 misplaced objects in\ncorrect locations. Note that we intentionally focus on a\ndomain where the downstream application benefits strongly\nfrom exploring reasonable goals during pretraining. Rather\nthan designing reward functions that correspond to all cor-\nrect rearrangements for all possible objects, we investigate\nwhether ELLM can be a general purpose method that guides\nlearning human-meaningful behaviors.\nUnlike Crafter’s combinatorial and high-level action space,\nHousekeep operates with low-level actions: moving forward,\nturning, looking up or down, and picking or placing an ob-\nject. This allows us to investigate whether ELLM enables\nhigh-level exploration despite using lower-level control. We\nassume access to an egocentric instance segmentation sensor\nto generate captions of in-view objects and receptacles, and\nuse the text-davinci-002 InstructGPT model (Ouyang\net al., 2022) as our LLM. Given a description of visible\nobjects, the receptacles the objects are currently in, and all\npreviously seen receptacles, we create a list of all possible\nobject-receptacle mappings. We use the closed-form vari-\nant of ELLM and query the LLM for whether each object\nshould be placed in each receptacle as a yes\/no question.\nBy querying for each object-receptacle combination indi-\nvidually, we are able to cache and efficiently reuse LLM\nqueries. The agent can be given two types of goals: (1) pick-\ning an object if it is not already in a suggested receptacle,\nand (2) placing a gripped object in a suggested receptacle.\nGoals suggested by LLM.\nIn Housekeep, we assess LLM\ngoals by looking at the classification accuracy of correct\nand incorrect arrangements (Table 3). We find that the LLM\naccuracy at identifying mismatches (e.g. vase in kitchen\nsink) are all above 87%, however, accuracy of identifying\nmatches varies greatly depending on the available objects\nand receptacles (ranging from 50-90%). Since there are\nonly a few correct positions, each false negative hurts ac-\ncuracy greatly. Taking a closer look, we find that some\n7\nGuiding Pretraining in Reinforcement Learning with Large Language Models\n0\n10\n20\nEnv Steps (x 1e4)\n0.00\n0.25\n0.50\n0.75\n1.00\nSuccess Rate\nPlace Crafting Table\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nAttack Cow\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nMake Wood Sword\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nMine Stone\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nDeforestation\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\nPlant Row\n0\n50\n100\n150\n200\nEnv Steps (x 1e4)\nGardening\n0\n25\n50\n75\n100\nEnv Steps (x 1e4)\n0.0\n2.5\n5.0\n7.5\n10.0\nCrafter Score\nCrafter (Game Reward)\nELLM (goals) - Ours\nELLM (no goals) - Ours\nRND\nAPT\nScratch\nFigure 5: Success rates across training for each of the seven downstream tasks in the Crafter environment. Each run trains\nan agent from scratch while leveraging a pretrained policy for exploration. Plots show mean ± std for 5 seeds. Some plots\nhave multiple overlapping curves at 0.\n0\n2\n4\n6\n8\n10\n12\n14\nSteps x1e4\n0\n2\n4\n6\n8\nCrafter Achievements\nPretraining With the Captioner\nOracle (goals) - GT Captioner\nOracle (goals) - ELLM Captioner\nELLM (goals) - GT Captioner\nELLM (goals) - ELLM Captioner\nFigure 6: Pretraining with a learned captioner vs a ground\ntruth captioner. We see performance drops, especially for\nELLM, but still relatively good performance. (3 seeds,\nmean± std.)\nLLM labels are reasonable despite disagreeing with the en-\nvironment mapping: e.g. suggesting vase in living room\ntable, and not suggesting pan in living room cabinet.\nThis suggests that there are ambiguities in the ground truth\nmappings, likely due to human disagreement.\nPretraining and downstream performance.\nTo investi-\ngate H1, we compare ELLM against the strongest baselines\n(RND, APT, Novelty) described in Table 1. In Housekeep\nthe novelty baseline rewards the agent for novel instances of\npick or place actions in an episode, allowing us to differenti-\nate between success attributable solely to the captioner and\nthe pick\/place prior, and success attributable to any LLM\ncommon-sense priors. For brevity, we focus only on the\npixel + text-observation variant of all methods. Sample ef-\nficiency curves measuring the ground truth rearrangement\nsuccess during both pretraining and finetuning are shown\nin Figure 7a. In three of the four tasks, we find that the\nELLM bias leads to higher success rates during pretraining,\nsuggesting coverage better aligned with the downstream\ntask compared to the baselines. We also find much higher\npretraining success rates in the first two tasks. Since Table 3\nshows higher LLM accuracy for these two tasks, this differ-\nence shows the impact of LLM inaccuracies on pretraining.\nFor H2, we test two different ways of using the pretrained\nmodels in the downstream rearrangement task. First, we\ndirectly finetune the pretrained model on the ground truth\ncorrect rearrangement; shown after the dashed vertical line\nin Figure 7a. Here, the success rates for finetuned ELLM\nmatches or outperform the baselines, especially if pretrain-\ning has already led to high success rates. Interestingly, we\nalso find that the goal-conditioned ELLM variant consis-\ntently suffers a drop in performance when finetuning starts.\nWe hypothesize this is due to the treatment of all suggested\ngoals as a single string, so if any single goal changes be-\ntween pretraining and finetuning the agent must relearn the\ngoal embedding changes. Second, in Figure 7b we present\nresults for directly training a new agent on the downstream\ntask, using the frozen pretrained model as an exploratory\nactor during ϵ-greedy exploration. Once again, we observe\nthat ELLM consistently matches or outperforms all base-\nlines. We also see here that the KB-IM baselines are more\ncompetitive, suggesting that this training scheme is better\nsuited for pretrained exploration agents that are not well-\naligned to the downstream task.\n5. Conclusions and Discussion\nWe have presented ELLM, an intrinsic motivation method\nthat aims to bias exploration towards common-sense and\nplausibly useful behaviors via a pretrained LLM. We have\nshown that such priors are useful for pretraining agents in\nextrinsic-reward-free settings that require common-sense\nbehaviors that other exploration methods fail to capture.\nELLM goes beyond standard novelty search approaches by\nconcentrating exploration on common-sensical goals. This\nis helpful in environments offering a wide array of possible\nbehaviors among which very few can said to be plausibly\nuseful. It is less helpful in environments with little room\nfor goal-based exploration, when human common-sense\nis irrelevant or cannot be expressed in language (e.g. fine-\ngrained manipulation), or where state information is not\nnaturally encoded as a natural language string.\nLLM performance is sensitive to prompt choice. Even with\na well-chosen prompt, LLMs sometimes make errors, often\ndue to missing domain-specific knowledge. False nega-\ntives can permanently prevent the agent from learning a\nkey skill: in Crafter, for example, the LLM never suggests\ncreating wood pickaxes. There are multiple avenues to ad-\ndress this limitation: (1) combining ELLM rewards with\nother KB-IM rewards like RND, (2) prompting LLMs with\ndescriptions of past achievements (or other feedback about\n8\nGuiding Pretraining in Reinforcement Learning with Large Language Models\nELLM (goals) - Ours\nELLM (no goals) - Ours\nAPT\nRND\nNovelty (goals)\nNovelty (no goals)\nFinetuning Begins\n0\n2\n4\nEnvironment Steps\n1e6\n0.0\n0.2\n0.4\nHousekeep Success Rate\n0\n2\n4\nEnvironment Steps\n1e6\n0\n2\n4\nEnvironment Steps\n1e6\n0\n2\n4\nEnvironment Steps\n1e6\n(a) Pretraining and finetuning: pretraining for 4M steps then fine-\ntuning for 1M steps on the ground truth correct arrangement.\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.2\n0.4\nHousekeep Success Rate\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n0.0\n0.5\n1.0\nEnvironment Steps\n1e6\n(b) Downstream evaluation: Using the frozen pretrained exploration\npolicies only for ϵ-greedy-style action selection for 1M steps.\nFigure 7: Housekeep: Correct arrangement success rates on 4 object-receptacle task sets. Mean ± std over 5 seeds.\nenvironment dynamics) so that LLMs can learn about the\nspace of achievable goals, (3) injecting domain knowledge\ninto LLM prompts, or (4) fine-tuning LLMs on task-specific\ndata. While ELLM does not rely on this domain knowledge,\nwhen this information exists it is easy to incorporate.\nELLM requires states and transition captions. Our learned\ncaptioner experiments Figure 6 suggest we can learn these\nfrom human-labeled samples, but in some environments\ntraining this captioner might be less efficient than collecting\ndemonstrations or hard-coding a reward function. Still, we\nare optimistic that as progress in general-purpose caption-\ning models continues, off-the-shelf captioners will become\nfeasible for more tasks. Lastly, suggestion quality improves\nconsiderably with model size. Querying massive LLMs\nregularly may be time- and cost-prohibitive in some RL\nenvironments.\nAs general-purpose generative models become available in\ndomains other than text, ELLM-like approaches might also\nbe used to suggest plausible visual goals, or goals in other\nstate representations. ELLM may thus serve as a platform\nfor future work that develops even more general and flexible\nstrategies for incorporating human background knowledge\ninto reinforcement learning.\n6. Acknowledgements\nYD and OW are funded by the Center for Human-\nCompatible Artificial Intelligence. CC received funding\nfrom the European Union’s Horizon 2020 research and in-\nnovation programme under the Marie Skłodowska-Curie\ngrant agreement No. 101065949. This material is based\nupon work supported by the National Science Foundation\nunder Grant No. 2212310 to AG and JA. OpenAI credits for\nGPT-3 access were provided through OpenAI’s Researcher\nAccess Program. We thank Sam Toyer and the members of\nthe RLL for feedback on early iterations of this project.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 利用大型语言模型引导强化学习的预训练\n\n## 📌 背景痛点\/本文动机\n强化学习算法在缺乏密集、良好形状的奖励函数时通常会遇到困难。内在动机探索方法通过奖励代理访问新颖状态或转换来解决这一限制，但在大多数发现的新颖性对下游任务无关紧要的大型环境中，这些方法提供的益处有限。本文提出了一种方法，该方法使用来自文本语料库的背景知识来塑造探索。这种方法称为ELLM（使用大型语言模型进行探索），它奖励代理实现由语言模型提出的与代理当前状态描述相关的目标。通过利用大规模语言模型预训练，ELLM引导代理朝着人类有意义且可能有用的行为发展，而无需人工干预。\n\n## 🚀 核心方法\n💡 创新点1：利用大型语言模型（LLM）的背景知识来塑造探索。LLM是概率文本模型，其预测编码了丰富的关于人类常识知识和文化习俗的信息。ELLM通过查询LLM来获取可能的目标，并奖励代理实现这些建议，从而引导探索朝着完成多样化、上下文敏感和人类有意义的目标。\n\n💡 创新点2：使用LLM生成的目标作为内在奖励函数。ELLM通过测量LLM生成的目标与环境中代理转换的描述之间的语义相似性来计算奖励。当转换的描述与目标描述足够接近时，代理将获得与相似度成比例的奖励。\n\n## 📈 实验结果\n本文在Crafter游戏环境和Housekeep机器人模拟器中评估了ELLM。结果表明，ELLM训练的代理在预训练期间对常识行为的覆盖范围更好，并且在下游任务上的性能通常与基线相当或有所提高。\n\n## 💬 可借鉴之处\n本文提出的方法可以用于引导强化学习代理在缺乏外部定义的奖励的情况下学习有用的行为。通过利用LLM的背景知识，ELLM可以引导代理朝着人类有意义且可能有用的行为发展，从而提高强化学习算法的性能。此外，本文还探讨了LLM性能对提示选择、状态和转换描述的敏感性，并提出了改进LLM性能的潜在方法。","llm_summary_res_status":200}
{"title":"PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model","authors":"Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, Yanru Zhang","summary":"Poker, also known as Texas Hold'em, has always been a typical research target\nwithin imperfect information games (IIGs). IIGs have long served as a measure\nof artificial intelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret minimization\n(CFR) to tackle heads-up no-limit Poker. However, it is challenging for\nsubsequent researchers to learn CFR from previous models and apply it to other\nreal-world applications due to the expensive computational cost of CFR\niterations. Additionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work, we introduce\nPokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number\nof players and gaining high win rates, established on a lightweight large\nlanguage model (LLM). PokerGPT only requires simple textual information of\nPoker games for generating decision-making advice, thus guaranteeing the\nconvenient interaction between AI and humans. We mainly transform a set of\ntextual records acquired from real games into prompts, and use them to\nfine-tune a lightweight pre-trained LLM using reinforcement learning human\nfeedback technique. To improve fine-tuning performance, we conduct prompt\nengineering on raw data, including filtering useful information, selecting\nbehaviors of players with high win rates, and further processing them into\ntextual instruction using multiple prompt engineering techniques. Through the\nexperiments, we demonstrate that PokerGPT outperforms previous approaches in\nterms of win rate, model size, training time, and response speed, indicating\nthe great potential of LLMs in solving IIGs.","url":"http:\/\/arxiv.org\/abs\/2401.06781v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2401.06781v1","published":1704374870000,"comment":null,"pdf_text":"PokerGPT: An End-to-End Lightweight\nSolver for Multi-Player Texas Hold’em via\nLarge Language Model\nChenghao Huang‡, Yanbo Cao‡, Yinlong Wen†, Tao Zhou‡, and Yanru Zhang‡§\n‡University of Electronic Science and Technology of China\n§Shenzhen Institute of Advanced Study, UESTC, China\nzydhjh4593@gmail.com, yanbocao@std.uestc.edu.cn, zhutou@ustc.edu, yanruzhang@uestc.edu.cn,\n†Sichuan Optical Character Technology Co., Ltd\nlywen@chineseocr.com\nAbstract—Poker, also known as Texas Hold’em, has always\nbeen a typical research target within imperfect information\ngames (IIGs). IIGs have long served as a measure of artificial\nintelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret\nminimization (CFR) to tackle heads-up no-limit Poker. However,\nit is challenging for subsequent researchers to learn CFR from\nprevious models and apply it to other real-world applications\ndue to the expensive computational cost of CFR iterations.\nAdditionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work,\nwe introduce PokerGPT, an end-to-end solver for playing Texas\nHold’em with arbitrary number of players and gaining high win\nrates, established on a lightweight large language model (LLM).\nPokerGPT only requires simple textual information of Poker\ngames for generating decision-making advice, thus guaranteeing\nthe convenient interaction between AI and humans. We mainly\ntransform a set of textual records acquired from real games\ninto prompts, and use them to fine-tune a lightweight pre-\ntrained LLM using reinforcement learning human feedback\ntechnique. To improve fine-tuning performance, we conduct\nprompt engineering on raw data, including filtering useful infor-\nmation, selecting behaviors of players with high win rates, and\nfurther processing them into textual instruction using multiple\nprompt engineering techniques. Through the experiments, we\ndemonstrate that PokerGPT outperforms previous approaches\nin terms of win rate, model size, training time, and response\nspeed, indicating the great potential of LLMs in solving IIGs.\nIndex Terms—Data-driven artificial intelligence, imperfect in-\nformation game, large language model, reinforcement learning\nhuman feedback, end-to-end learning.\nI. INTRODUCTION\nSince the birth of the artificial intelligence (AI), intelligent\ngame research has been fertile ground for the development\nand innovation of AI, and has always been an important\nevaluation criterion for measuring its development level. In\n2016, AlphaGo defeated the human world champion in the\ngame of Go, which was considered as an important milestone\nfor AI solving perfect information games. However, a dis-\ntinctive feature of real-world games is the uncertainty of the\nopponent due to incomplete information. Poker is a typical\nimperfect information game (IIG) that has a long history as a\nchallenging problem for developing AI that can address hidden\ninformation [1]. Poker is a superb experimental and testing\nplatform, providing an excellent arena for the exploration\nand validation of foundational theories and methodologies in\nintelligent games.\nIn recent years, significant advancements have been made in\noptimized solutions for the game of Poker. Notably, DeepStack\nfrom the University of Alberta, Canada [2], and Libratus from\nCarnegie Mellon University (CMU), USA, have surpassed\nhuman professionals in heads-up no-limit Poker, a 2-player\nform of Poker [3]. The works of [4], [5] have been further\nproposed to improve the computational efficiency. Subsequent\nto these achievements, the Carnegie Mellon-designed Pluribus\ndemonstrated superiority by defeating a human professional in\n6-player no-limit Poker [6]. These prominent AI models for\nPoker commonly adopt iterative Counterfactual Regret Mini-\nmization (CFR) algorithms to approximate Nash equilibrium\nstrategies, involving initially compressing the state and action\nspace of the hand to reduce the game tree’s size, followed by\niterations of the CFR algorithm over the condensed game tree.\nHowever, these methods exhibit the following drawbacks:\n• Computational and storage resource consumption:\nDespite model reduction, significant computational and\nstorage resources are still required, escalating the overall\ncomputational cost. For example, in 2015, the work of [7]\nconsumed 4800 CPU cores, 10.9 TB memory, and 2\nmonths to solve heads-up limit Poker through CFR.\n• Difficulty in multi-player extension: Directly extending\nthe CFR framework to multi-player Poker environments\nproves to be challenging. An increase in the number\nof players leads to an exponential growth in the game\ntree size, diminishing generalization performance. For\nexample, CMU used CFR to solve 6-player no-limit\nPoker and consumed 12800 CPU cores, 12.5 TB memory,\nand 8 days [6]. The lack of flexibility make it unrealistic\nto use existing works in real-world games.\n• Loss of information: Game tree compression unavoid-\nably results in the loss of crucial information for decision\nmaking, such as the behavior and psychology of different\narXiv:2401.06781v1  [cs.AI]  4 Jan 2024\nplayers. This can lead to severe variations in fraudulent\nand anti-fraudulent strategies, jeopardizing the win rate\nagainst top players.\n• Dependence on human expert knowledge: For com-\npressing the game tree in CFR to reduce computational\ncost, human expert knowledge on mathematics and com-\nputational theory is heavily needed, causing large knowl-\nedge bottleneck and labor effort for improvement.\nHence, there arises a pressing need for a lightweight model\ncapable of autonomously extracting information to optimize\nIIGs further. Such a model should exhibit superior generaliza-\ntion performance, reduced dependence on expert knowledge,\nand enhanced adaptability to multi-player environments with-\nout compromising computational efficiency. Addressing this\nchallenge will be a pivotal focus in future research within the\nfield of Poker AI.\nThanks to the strong capability of large language model\n(LLM), it is promising to achieve a prominent framework for\nplaying Poker in real games. LLM is a novel deep learning\n(DL) model that can handle massive amounts of natural\nlanguage data, with various advantages in human-computer\ninteraction. Firstly, by pre-training on large-scale text data,\nLLM can extract word vectors while also learning contextual\ninformation to enhance the natural language understanding\ncapabilities. Secondly, LLM integrates generative tasks into\nthe encoder-decoder structure to achieve a unified natural\nlanguage processing capability, thereby improving the qual-\nity of text generation. Meanwhile, LLM exhibits flexibility\nand adaptability of human-computer interaction due to its\nremarkable zero-shot and few-shot abilities, that is, in-context\nlearning that only changes the input rather than the parameters.\nLast but not least, end-to-end learning streamlines the need\nfor intricate feature engineering and intermediate steps. LLMs\ncan autonomously acquire task-relevant rules and properties\nleveraging the powerful representation capabilities of deep\nneural networks. Thus, it is feasible to develop an LLM-\nbased approach for playing real games of multi-player no-limit\nPoker.\nIn this paper, we utilize a lightweight LLM and data\nfrom real Poker games to construct an AI solver for Poker\nsolutions, named PokerGPT. We firstly collect textual records\nof real games with arbitrary amount of players. Then, we\nfilter necessary information and process these records into\nprompts, a textual data form that is understandable for both\nhumans and LLMs. For dataset preparation, we integrate these\nprompts with actions in real games as labels. Based on the\ndataset, we fine-tune OPT-1.3B, a lightweight open-source\nLLM from Facebook [8] using reinforcement learning from\nhuman feedback (RLHF), a popular technique for fine-tuning\nLLMs. The interaction process is shown in Fig. 1. Our codes\nare publicly available 1.\nThe pivotal contributions of our work are listed as follows:\n• We present a novel approach based on LLMs to address\nIIGs. This approach employs an end-to-end learning\n1https:\/\/github.com\/hch211\/TH LLM\nPlayer's number: 6\nDealer's position: 6\nPlayer 1,3 : fold;\nPlayer 2: bet 1.5;\n······\nWhat should I do?\nCall 1.5\nInput:\nOther players' actions\nOutput:\nAction instruction\nInput\nPokerGPT\n1\n2\n3\n6\n5\n4\nObtain\ninformation\nFig. 1: Interaction process of PokerGPT.\nmethod, proved to be easily trainable and considerably\ncost-effective compared with previous approaches.\n• We propose PokerGPT, a pioneering DL model specifi-\ncally designed for Poker. With the power of LLMs, our\nmodel has a prominent advantage on convenient interac-\ntion with humans. More importantly, PokerGPT has the\ncapability of dealing with an arbitrary number of players\nin Poker games, showing the outstanding flexibility which\nhas never been achieved by previous works. Besides, our\nmodel contains much less parameters and takes much less\ninference time compared with conventional methods such\nas CFR, while consumes much less training time than\nnovel DL works, achieving great resource conservation.\n• Based on data of real games, we devise an efficient data\nprocessing approach specific to Poker, consisting of data\ncleansing and prompt engineering, leading to significant\nimprovements on fine-tuning of PokerGPT.\n• Through experiments, we demonstrate that our model\nexhibits significant advantages over existing models in\nterms of various metrics, such as win rate, training speed,\nresponse time, etc. Furthermore, we design 2 metrics,\nnamed action score and average investment, to study\ncharacteristics of PokerGPT’s strategies on Poker.\nThe remainder of this paper is organised as follows: Sec-\ntion II gives a literature review of classical works of Poker and\nLLM; Section III introduces the rules of Poker and the princi-\nples of LLMs; Section IV explains the holistic architecture of\nour proposed PokerGPT, including data acquisition, prompt\nengineering, and training process; Section V describes the\ncase studies, shows performance of PokerGPT with multiple\nmetrics, and analyzes the results; Section VI concludes this\nwork and presents pathways for future work.\nII. RELATED WORK\nA. Solutions for Poker\nPoker is a popular and challenging form of poker that has\nbeen used as a testbed for IIG algorithms for decades [9].\nA major breakthrough in this field was the introduction of\ncounterfactual regret minimization (CFR) [10], a simple and\nefficient iterative algorithm that converges to a Nash equilib-\nrium by minimizing the regrets of both players over time.\nCFR enabled the solution of heads-up limit Poker in 2015 [7],\nshifting the research focus to the more complex and realistic\nvariant of Poker.\nCFR once became a widely used algorithm for solving\nPoker, and a challenging benchmark for IIGs. Previous works\nhave shown that CFR variants can achieve superhuman per-\nformance in Poker against professional human players. The\nwork of [11] introduced subgame-solving techniques that\noutperform prior methods both in theory and practice. By\nfusing these techniques, in [3], the authors presented Libratus,\nan AI based on Monte-Carlo CFR (MCCFR) that defeated 4\ntop human specialists in heads-up no-limit Poker in a 120000-\nhand competition. Though Libratus was the leading benchmark\nin IIG solving, it required millions of core hours and terabytes\n(TBs) of memory to calculate. To improve computational\nefficiency, the work of [4] substituted values at leaf nodes\nof unknown states to limit the depth of the decision tree, and\nalso outperformed prior methods. The work of [5] introduced\nnovel CFR variants to discount regrets from earlier iterations\nin some cases differently for positive and negative regrets, and\nreweight iterations to obtain the output strategies. Since games\ncan be significantly complex when there are multiple players\nin games, the work of [6] extended Libratus to Pluribus, an\nAI that defeated top human professionals in 6-player Poker,\nthe most popular form of Poker. However, Pluribus approaches\nare also computationally expensive and memory-intensive, and\nconsumed significant time for inference.\nTo overcome the limitations of CFR-based algorithms, DL\nmethods have been applied to exploit the power of big data and\nneural networks (NNs). For example, DeepStack [2] used an\nNN to approximate the CFR values and performed recursive\nreasoning, while deep CFR [12] improved the performance and\nefficiency by using NNs to directly learn the CFR strategy in\nthe full game without abstraction. To reduce the dependence\non prior knowledge and training time, self-play deep reinforce-\nment learning (DRL) was integrated with CFR to enhance the\nexploration [13]. Moreover, the works of [14] proposed an\nend-to-end framework based on DRL and introduced several\ntechniques to accelerate it and defeat DeepStack.\nIn conclusion, the above methods rely heavily on com-\nputational resources, prior knowledge, and domain-specific\nheuristics, which constrain the applicability. Besides, it is\ndifficult for humans to use these works, also harming the\npracticality. Therefore, developing more efficient algorithms\nfor Poker and other complex IIGs remains an open problem.\nB. LLM\nRecent advances in pre-trained language models, such as\nChatGPT and GPT-4 [15], have inspired a range of studies that\nleverage information from various domains and modalities.\nThese studies aim to enhance the performance and applicabil-\nity of language models for diverse tasks, and can be broadly\ndivided into system design and ene-to-end training. System\ndesign employs ChatGPT as a dispatcher that orchestrates dif-\nferent expert models for specific tasks, such as Cola [16] and\nX-GPT [17]. On the other hand, end-to-end training integrates\nmodels from different modalities into a unified framework\nthat can be trained end-to-end, and then fine-tuned on data\nfrom particular domains, such as weather forecasting [18] and\naugmented reality [19].\nThe use of LLMs for games has attracted increasing at-\ntention from researchers, who have obtained some intriguing\nfindings. For instance, the results of [20] showed that LLMs\ncould produce responses that were remarkably similar to\nhuman feedback, despite some limitations on specific tasks in\nthe Beyond the Imitation Game benchmark. This suggested\nthat LLMs had the potential to handle human-related sce-\nnarios. The authors of [21] leveraged GPT-2 and GPT-3 to\nprocedurally generate RPG video game quest descriptions.\nSimilarly, the authors of [22] used GPT-3 to provide dialogue\nsuggestions for non-player characters as well as to highlight\nrelevant game module information. They both found that GPT-\n3 could generate significantly better sentences than GPT-2,\ndemonstrating the superior performance of GPT-3 and the\npromising prospects of future GPTs. Moreover, the authors\nof [23] compared GPT-3, GPT-3.5, and GPT-4 models in\nplaying finitely repeated games with each other and with other\nhuman-like strategies. They observed that LLMs performed\nsub-optimally in games that required coordination, but ex-\ncelled in games that involved individual competition and self-\ninterest, such as the iterated Prisoner’s Dilemma family. The\nabove research implies that LLMs can be feasible as a Poker\nsolver.\nAlthough there are still many challenges regarding the use of\nLLMs for games, LLMs have shown great potential for game-\nrelated research and applications. In this work, we propose a\nframework for Poker via LLMs, emphasizing the potential for\nLLMs to solve IIGs\nIII. PREREQUISITES\nA. Poker Rules\nPoker is a game that consists of multiple rounds, each\nstarting with 2 private cards (“PREFLOP” cards) dealt to each\nplayer, followed by 5 public cards (community cards) dealt\nin 3 stages. The stages are a set of 3 cards (the “FLOP”),\nan additional single card (the “TURN”), and a final card\n(the “RIVER”). Each player aims to form the best 5-card,\nalso named as a hand, using 2 private cards and 5 public\ncards. Players can “bet”, “check”, “call”, “raise”, or “fold”\nat each round. The player who has the best hand at the\n“SHOWDOWN”, or who remains in the game after all others\n“fold”, wins the money in the pot. Fig. 2 illustrates an simple\nexample of a 2-player Poker game, and the bet size is limited\nonly by the total amount wagered in each round. Besides,\nFig. 3 shows the card strengths.\nB. LLM\nLLM is a kind of DNN that generates natural language texts\nfrom various inputs, such as prompts, queries, or contexts.\nLLMs learn from massive text corpora that cover diverse\nPREFLOP\n(Private Cards)\nPlayer 1\nPlayer 2\nBetting\nFLOP\n(First 3 public cards)\nBetting\nTURN\n(The 4th\npublic card)\nBetting\nRIVER\n(The 5th\npublic card)\nBetting\nSHOWDOWN\n(Each player gets best rank made by 5 cards\namong 2 private cards and 5 public cards)\nFlush\nTwo Pairs\nPlayer 1 wins\nAll gained by the winner\nPot\nPlayers bet in turn\nuntil everyone has\nthe same bet\nFig. 2: An example of a 2-player Poker game.\nFig. 3: Poker card strengths.\ndomains, such as books, news, articles, web pages, and social\nmedia posts. LLMs can perform various natural language\nprocessing tasks, such as text generation, text summarization,\nquestion answering, and machine translation. The main pro-\ncess of training a LLM is data preparation, pre-training, and\nfine-tuning.\n1) Pre-training: LLMs are pre-trained with self-supervised\nlearning, which leverages the data itself as supervision. Self-\nsupervised learning can capture different levels of language\nrepresentation, such as words, sentences, or documents. Com-\nmon self-supervised learning objectives for LLMs include\nmasked language modeling, next sentence prediction, per-\nmutation language modeling, causal language modeling, etc.\nSince our work focuses on fine-tuning and RLHF, we will not\nintroduce pre-training of LLM in details.\n2) Fine-tuning: LLMs are fine-tuned with supervised or\nsemi-supervised learning, which leverages labeled or partially\nlabeled data as supervision. Supervised or semi-supervised\nlearning can adapt LLMs to different natural language process-\ning tasks. After fine-tuning, LLMs can be used for inference.\nCommon inference tasks include text summarization, question\nanswering, machine translation, text generation, etc.\n3) RLHF: A popular ML technique for LLM is RLHF, that\ntrains a “reward model” based on human feedback as a reward\nfunction to optimize an agent’s policy using reinforcement\nlearning (RL) through an optimization algorithm [24], [25].\nThis method can improve the robustness and exploration of\nRL agents, especially when the reward function is sparse or\nnoisy. It is used in tasks where it is difficult to define a clear,\nalgorithmic solution but where humans can easily judge the\nquality of the model’s output.\nIV. POKERGPT\nIn this section, we introduce our framework, PokerGPT, in\ndetails. We firstly collect a group of game data on a online\nPoker platform in real world 2. Then we transform the data\ninto prompts which can be understood by both humans and\nLLMs. To improve performance, we select prompts with high\nquality for further training. By utilizing supervised fine-tuning,\nwe make a pre-trained LLM capable of making decisions when\nreceiving staged information provided by players in a Poker\ngame. For generalizability and interactivity, we adopt RLHF\nto enhance the supervised model. Besides, since the LLM\nrequires as much information as possible while the interaction\nmay frequently happen in a short period of time, we set all\nthe information as fixed instruction for simplicity. By doing\nthis, humans can interact with our model through just simple\ndialogues. Finally, we conduct supervised fine-tuning, reward\nmodeling, and RLHF to get PokerGPT. The framework of\nPokerGPT is demonstrated in Fig. 4.\nA. Data Acquisition\n1) Data Description: For increasing the practicality of our\nmodel, we acquire logs of Poker game on PokerStars. These\nlogs contain information as follows:\n• Basic information: This encompasses the establishment\nof both the big and small blinds, the seating arrangement\ndetails for each player, along with their respective chip\ncounts and the prevailing position of the current dealer.\nThis segment furnishes a thorough comprehension of the\nentire gaming scenario.\n• Dynamic information: For each game, we partition it\ninto distinct stages. Initially, there is the “PREFLOP”\nstage, during which each player receives 2 private cards,\ninitiating the first round of betting. Subsequently, the\n“FLOP”, “TURN”, and “RIVER” stages unfold, wherein\nthe community cards are progressively revealed, accom-\npanied by the betting actions of the remaining players.\nThe conclusive stage “SHOWDOWN” unveils the pri-\nvate cards, ultimately determining the definitive winner,\nwithout any action should be done by players.\n• Summary: An overview of the game pool size, the\ncommunity cards, and each player’s earnings is recorded.\nIt includes details on the winnings and losses associ-\nated with specific card combinations, instances of non-\nparticipation, and other relevant information.\nFig.5 shows an example of the game log. The chart reveals\nthat the blinds for this game are set as $0.02\/$0.05, with\n2PokerStars: https:\/\/www.pokerstars.com\nI. Data Acquisition\nCollect game data\nFilter useful information\nIf \"SHOWDOWN\"\nRounds played\nDiscard\nDiscard\nII. Prompt Engineering\nConstruct prompts\nIII. Training Process\nOnline platform: PokerStars\nGame log\n1. Small\/Big blinds.\n2. Seat arrangement.\n3. Dealer sear.\n4. Balance.\n......\n1. Stage.\n2. Public cards.\n3. Private cards.\n4. Actions.\n......\n1. Pot value.\n2. Winner info.\n3. Loser info.\n......\nBasic info\nDynamic info\nSummary\nNo\nYes\nUnder\nthreshold\nUseful\ninformation\nAbove\nthreshold\nCore info\nAdditional info\n1. Observable cards.\n2. Card characteristics.\n3. Own rank.\n4. Action sequence.\n1. Blind amount.\n2. Win rate.\n3. Player number.\n4. Pot value.\n......\nCalculate players' win rates\nPrompt selection\nSort and select\n(mbb\/h)\n150\n100\n60\n20\n-40\n...\nConstant\ninformation\nDynamic\ninformation\nEngineered\nPrompts\nOrganize\n1. Zero-shot.\n2. Role play.\n3. Discretization.\nPrompt\nengineering\ntechniques\nLow-quality\ndataset\nRoundly\nupdates\nPrompt: X\nAction: y\nHigh-quality\ndataset\nLow-level\nplayers\nHigh-level\nplayers\nTop N\nBottom M\nPre-trained LLM\nFacebook OPT-1.3b\nReward\nmodel\nSupervised\nfine-tuning\nContrastive\nfine-tuning\nPPO\nSupervised\nmodel\nRLHF\nPokerGPT\n1\n2\n7\n3\n4\n5\n6\nFig. 4: The framework of PokerGPT, consisting of data acquisition, prompt engineering, and training.\nthe dealer positioned at seat number 1. The chip count for\neach player is specified. For instance, the player in seat 1,\nphalves77, holds 5.12 in chips.\nDetails on players’ decisions and actions are as follows:\nDuring the “PREFLOP” stage, phalves77, seated in the\ndealer’s position, opted to “raise” to 0.15, while gefahren-\nsucher spent another 0.10 to “call” from the big blind. The\ncommunity cards unfolded as follows: the “FLOP” displayed\n[‘5s’, ‘Th’, ‘5c’], followed by the “TURN” card [‘2s’],\nand finally the “RIVER” card [‘Kh’]. After each round of\npublic cards, the recorded information includes the amount\nof each player’s bet, and their action choices. For instance,\nafter the “FLOP”, gefahrensucher give the “bet” of 0.16,\nwith phalves77 deciding to “call”. After the “TURN” and\n“RIVER”, gefahrensucher placed the “bet” of 0.31 and 1.21,\nrespectively, with phalves77 choosing “call” each time. During\nthe “SHOWDOWN”, gefahrensucher revealed a pair of ‘5’s\n(Fives), while phalves77 presented 2 pairs: ‘10’s (Tens) and\nFives. Consequently, phalves77 claimed the pot’s bottom as\nthe winner, gaining victory with the superior 2 pairs.\n2) Information Filtering:\nThe essential information of\nPoker games revolves around the public cards and the be-\nhaviors of the players in each game. The public cards stand\nas the pivotal information, forming an integral component of\na player’s ultimate hand. In Poker games, the unveiling of\npublic cards directly influences the rank of each player’s hand.\nThe actions of opponents during each round (“bet”, “raise”,\n“check”, “call”, “fold”) serve as vital indicators to infer their\ngefahrensucher: shows [3h 6h] (a pair of\nFives)                                                       \nphalves77: shows [Tc 4c] (two pair, Tens and\nFives)                                                     \nphalves77 collected $3.53 from pot\nSHOWDOWN\nTotal pot $3.68 | Rake $0.15\nBoard [5s Th 5c 2s Kh]\nSeat 1: phalves77 (button) showed [Tc 4c]\nand won ($3.53) with two pair, Tens and Fives\nSeat 2: Omar Hrn (small blind) folded before\nFlop\nSeat 3: gefahrensucher (big blind) showed[3h\n6h] and lost with a pair of Fives\nSeat 5: victorismyname folded before\nFlop(didn't bet)\nSeat 6: 0bjective folded before Flop(didn't bet)\ns : spade ; h : heart\nc : club ; d : diamond\nPokerStars Hand#xxxxxx ($0.02\/0.05 )\n6-max Seat    #1 is the button\nSeat 1: phalves77 ($5.12 in chips)\nSeat 2: Omar Hrn ($5.02 in chips)\nSeat 3: gefahrensucher ($6.64 in chips)\nSeat 5: victorismyname ($4.99 in chips)\nSeat 6: 0bjective ($2.14 in chips)\nOmar Hrn: posts small blind $0.02\ngefahrensucher: posts big blind $0.05\nGOBSTPPER: sits out\nBasic Information\nvictorismyname: folds  \n0bjective: folds\nphalves77: raises $0.10 to\n $0.15\nOmar Hrn: folds\ngefahrensucher: calls $0.10\nPREFLOP\nFlop : [5s Th 5c]\ngefahrensucher : bets\n   $0.16\nphalves77: calls $0.16\nFLOP\nTurn : [5s Th 5c] [2s]\ngefahrensucher : bets\n  $0.31\nphalves77: calls $0.31\nTURN\nRiver:\n[5s Th 5c 2s] [Kh]\ngefahrensucher : bets\n  $1.21\nphalves77: calls $1.21\nRIVER\nSUMMARY\nFig. 5: An example of Poker game log.\nstrategies and intentions, directly providing insights into the\nstrength of their cards. Moreover, the magnitude of money\nplaced exhibits a player’s confidence and card strength in a\nlarge extent. Also, the frequency of “raise” can enhance the\nunderstanding of opponent’s playing style and card strength,\nenabling strategic decisions on when to bluff and when to\ndiscern a bluff.\nCore information of utmost significance is crucial to be\nincluded in the prompt, listed as follows:\n• Observable cards: It is necessary to know the 2 private\ncards and public cards exhibited.\n• The characteristics of cards: We describe the charac-\nteristics of private cards in several ways. “Suit” means\n2 private cards with the same color. “High” means\npossessing one card higher than ‘9’. “Close” means the\ndifference of 2 cards is less than 5, which can form a\n“straight”.\n• The rank of the hand: Since it may be difficult for\nthe LLM to learn to recognize the rank of the hand,\nwe simply provide the rank in the prompt. The rank\ncan guide LLMs to understand the strength of different\ncombinations of private cards and public cards.\n• Action sequence: Analyzing players’ behavioral patterns\nplays a pivotal role in understanding the psychology and\nstrategy of opponents. Thus, it is crucial to record each\nplayer’s sequential actions.\nAdditional information also assists the learning and un-\nderstanding of our model, including:\n• Amounts of blinds: Small and big blinds is important for\nevaluating both the revenue and the risk of Poker games,\nthereby contributing to the understanding of the decisions\non “raise” or “fold”.\n• Positions of players: The positional context of a player\nrelative to the dealer, coupled with the player’s actions,\nserves as an indicator of the hand’s strength, which\ninfluences the players’ advantages and the game results.\n• Number of players: The variation in the number of play-\ners within a game intensely changes the game structure.\nIt is essential to make our model aware of and respond\nto the variation.\n• Balance: We record each player’s balance through the\ninitial amount and the amount each has invested in the\npot.\n• Discard: The “fold” chosen by players helps our model\nto determine the remaining players.\n• Pot value: The total amount of money put into the pot\nhelps LLM understand the concept of odds.\n• Stage: The same action in different stages generally has\ndifferent purposes.\nThe incorporation of the above information ensures an\neffective training process for our model, improving language\ngeneration and comprehension capabilities.\nB. Prompt Engineering\nOne of the advantages of LLMs over traditional DL models\nis that it has good interactivity, which can be achieved by\nprocessing verbose raw data into prompts and further used for\nfine-tuning. A prompt is a corpus that is understandable by\nboth humans and LLMs, which is easy to be produced, such\nas colloquial phrases, sentences, and paragraphs. Therefore,\nwe can quickly get suggestions on playing Poker games by\ninputting simple words with part of important information of\na Poker game into LLMs. We will introduce how we make\nraw data into prompts, and filter out high-quality prompts to\nensure that our model is able to give high-revenue or low-loss\ninsights in real-time Poker games.\n1) Prompt Selection: Based on the game log presented in\nFig. 5, we can observe that the information contained in each\ngame is diversified, and it is difficult for a pre-trained LLM to\ndistinguish which information to focus on. Thus, we construct\nprompts with rich information and then select high-quality\nones for training.\nFirstly, we conduct searching for all logs to filter out\n“SHOWDOWN” stages that with private cards publicated.\nCases without a specific winning or losing hand were sub-\nsequently excluded, because the lack of hand strength infor-\nmation makes it difficult for LLMs to learn optimal strategies\nin an observable situation.\nThen, to get high-quality dataset, we adopt win rate for\nselection of high-level players. Win rate is a standard metric in\nthe Poker AI community. We firstly counted the total number\nof games played by each player in the obtained logs. Next, we\ncalculate overall revenue and converting it to a multiple of big\nblinds. After that, we count each player’s win rate in milli-big-\nblinds per hand (mbb\/h) as a sorting criterion for descending\nrankings. To mitigate bias resulting from a limited number of\ngames, we exclude players whose number of games is under a\nthreshold. Then, we select a group of top players as our high-\nquality dataset, in which the winning games have significant\ninstructive impacts on our model. Note that, it is fine for our\nmodel to learn player actions in lost games. Intuitively, the way\nof high-level player losing games also has reference value. To\nsome extent, it can help the model learn how to lose less.\nOn the other hand, we also collect data of the worst players\nas negative examples. Through reward modeling which will\nbe mentioned in Section. IV-C, the actions and ideas of the\nlow-level players can be learned and avoided by our model.\n2) Prompt Construction: We choose zero-shot learning for\nprompt construction, which simply feeds the text into the\nmodel and ask for results. For practicality, we discard few-\nshot prompting. The purpose of presenting few-shot exam-\nples in the prompt is to describe the task instruction to\nthe model in the form of demonstrations. Though adding\nquestion-answering (QA) samples to prompts may greatly\nimprove the response quality of LLMs [26], in real Poker\ngames, the player’s decision-making time is generally about 15\nseconds, causing that players are unlikely to be able to input\na large amount of QA samples or game information. Thus,\nfor enhancement, we just add fixed instructions in prompts to\nexplain our intent to the model. During interaction, prompts\nare systematically constructed mainly using the information in\nred in in Fig. 5. Besides, few-shot learning can be expensive\nin terms of token usage and violate the length limitation,\nwhile rich-information instruction improves the model to be\nmore aligned with human intention and greatly reduces the\ncost of communication. Thus, directly giving instructions is a\npromising approach. On the other hand, due to the fine-tuning\nwe conduct, explained in Section IV-C, few-shot learning is\nnot necessary.\nIn terms of reducing tokens inputted, we roughly divide\nuseful information into constant information which is fixed at\nthe beginning of one game, and dynamic information updated\nin each decision round.\nFurthermore,\nmultiple\nprompt\nengineering\ntechniques\nadopted are outlined as follows, which are proved to be\neffective in LLM training [27]:\n• Role play: We give a role to our model and add cor-\nresponding text into our prompt, such as “You are a\nprofessional gambler”.\n• Value discretization: To reduce the learning difficulty,\nwe discretize the money needed to be determined by our\nmodel through rounding up the values of money paid\nin each stage to an integer multiple of the big blind,\ntransforming the regression task into a classification task.\nAn textual instance of our prompt is shown in Fig. 6. It\nis worth noting that in real games, sometimes opponents may\ndirectly show 1 or 2 privates cards consider from some psy-\nchological warfare perspectives. In that case, we can replace\nthe “**” with the displayed cards. An utilization example of\nthe engineered prompt can be found in Section V, shown as\nFig. 12.\nThe information received by our model encompasses all\ncore information and additional information mentioned in\nSection IV-A2. This array of data enables our model to\ncomprehensively understand the prevailing situation.\nC. Training Process\nWe utilize the prepared dataset to train our model using\nDeepSpeed-Chat [28]. DeepSpeed-Chat is a system framework\nthat enables an end-to-end training experience for ChatGPT-\nlike models. The main steps consist of: (1) Supervised fine-\ntune a pre-trained LLM on small datasets to provide specific\nprior knowledge to this model; (2) Train a reward model\nPrompt Instance\nYou are an experienced gambler. Now you need to assist\nme to make decisions in Texas Hold’em games. You have\nbeen provided with a series of observable information:\nPlayer amount: [6], Currency: USD, Blind value: [0.02\/0.05],\nOrder: [‘2’, ‘3’, ‘5’, ‘6’, ‘7’, ‘9’], Seat 2 is small blind.\nMy cards: [‘Th’, ‘Ah’], the characteristics of my cards: [“suit”,\n“high”, “close”], My seat: [Seat 2]\nStage: “PREFLOP”, Public cards: [‘**’ ‘**’ ‘**’ ‘**’ ‘**’]\nMy rank: [“High”], Money: [3.92], Action: []\nSeat 3: [‘**’, ‘**’], Money: [2.33], Action: [], Discard: [False]\nSeat 5: [‘**’, ‘**’], Money: [5.54], Action: [], Discard: [False]\nSeat 6: [‘**’, ‘**’], Money: [3.75], Action: [], Discard: [False]\nSeat 7: [‘**’, ‘**’], Money: [4.22], Action: [], Discard: [False]\nSeat 9: [‘**’, ‘**’], Money: [1.47], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.17]\nThe actions can be: [“fold”, “raise”, “call”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.92}.\nFig. 6: An instance of our prompt.\nusing data separated by quality. (3) Apply RLHF to make\nthe fine-tuned LLM in step (1) able to understand various\nhuman text assisted by the reward model in step (2). The pre-\ntrained model we choose is the OPT-1.3B from Facebook,\nwhich performs similarly to GPT-3 [8], making the goal of\ntraining domain-specific expert-level AI assistant possible. The\nreason for choosing this model is its small size and good\ncomprehension in human dialogues.\nSupervised fine-tuning is a necessary approach because re-\ntraining a LLM which has been already pre-trained consumes\na substantial amount of computational resources. On the other\nhand, involving domain-specific expert model training neces-\nsitates not large-quantity but high-quality data that contains\ncomprehensive details of the designated task. Particularly in\nthe supervised fine-tuning phase, a modest number ranging\nfrom tens to hundreds of high-quality data proves sufficient\nfor achieving satisfactory results in model refinement [15].\nThus, it is promising to fine-tune OPT-1.3B on our collected\ndataset.\nThe second step is to train a reward model, which is crucial\nfor the RLHF process. We need to train a model to predict\nthe reward signal, which is used to guide the fine-tuning of\nthe final dialogue model. For convenience, we directly use the\nwin rates calculated before as the labels of our reward model.\nThen, we input prompt-action pairs into the reward model to\ngenerate a score indicating which action is better. The model\nis also trained using supervised learning.\nFinally, RLHF fine-tuning is applied to train the supervised\nfine-tuned model based on the reward model in the second\nstep. Specifically, fine-tuning approach used here is proximal\npolicy optimization (PPO) [29], a popular and strong RL\nalgorithm, whose reward is generated by the reward model.\nFor updating, RLHF involves generating new answers with the\ncurrent model, ranking these answers using the reward model,\nand then updating the model’s parameters to favor answers\nwith higher reward. This iterative process improves our model\nfrom the perspective of robustness to various human-like\ninput, and generalizability to extensive game scale and varying\nsituations. In the end, we obtain the PokerGPT for producing\nhuman-like and engaging answers to instruct Poker games.\nV. EXPERIMENT\nA. Fundamental Setup\nWe firstly prepare our dataset based on data of over 1 million\ngames. After processing mentioned in Section IV, we get high\nquality data of about 120 thousand games. Then, we divide\n90% of them into training set and the remaining 10% into test\nset. We select Microsoft’s DeepSpeed-chat framework [28]\nfor its comprehensive functionalities. This framework also\nefficiently manages the distribution of model parameters across\nindividual GPUs during training. We fine-tune an open-source\nLLM 1.3 billion parameters on HuggingFace, named Face-\nbook’s OPT-1.3B model, known for its comparable perfor-\nmance to GPT-3 [8]. We run our experiments on a single\nNVIDIA GEFORCE 3090. The hyperparameter we use can be\nrefered to DeepSpeed-Chat [28], while we change mini batch\nsize to 4 due to the limitation of our computational device.\nB. Experimental Setup\n1) Performance Comparison: For performance comparison,\nwe make PokerGPT play against Slumbot [30] for 10000\nhands, and compare its results with 2 latest models [13], [14].\n2) Ablation Studies: We aim to study if useful information\nfiltering and prompt engineering can improve the performance\nof PokerGPT. Thus, we input raw game data and filtered\ndata into LLMs for fine-tuning, and compare the win rate.\nFurthermore, as mentioned in Section IV, we use win rate\nfor evaluating the level of players included in our dataset. To\nexplore if levels of players have impact on the fine-tuning\nresults, we sample data from 50000 games based on different\nwin rate thresholds to construct our training set for fine-tuning.\nAll datasets for fine-tuning are demonstrated in Table I. As for\ntest set, we use Slumbot for win rate and randomly sample\n1000 games of winners among the complete dataset for other\nmetrics introduced as follows.\nTABLE I: Datasets under different setup for fine-tuning.\nDataset\nTechnique\nWin Rate Threshold\nAmount\nI\nRaw\n-\n1 million\nII\nInfo filtering\n-\n1 million\nIII\nWin rate sorting\n> 1500 mbb\/h\n50000\nIV\nWin rate sorting\n600 ∼1200 mbb\/h\n50000\nV\nWin rate sorting\n0 ∼500 mbb\/h\n50000\nVI\nWin rate sorting\n< 0 mbb\/h\n50000\nTo evaluate the performance of supervised fine-tuning on\ndatasets of different quality, we regard determining actions,\nincluding “bet”, “fold”, “raise”, and “check”, as a multi-label\nclassification task. If our model gives a action different from\nthe real action in test set, we get a “False”. We take macro F1\nas the metric for this task [31]. The macro F1 score is a type\nof F1 score that gives equal weight to each class in multi-label\nclassification, regardless of the class distribution. Assume we\nhave N classes, the macro F1 can be denoted as:\nmacro-F1 = 1\nN\nN\nX\ni=1\nF1i,\n(1)\nF1i = 2 × precision × recall\nprecision + recall\n,\n(2)\nwhere F1i is the F1 score of the ith class. Since we have 5\nactions for Poker games, N equals to 5 here. Note that, for\n50000 games, each action which is regarded as classification\nlabel may have different amount of labels from 50000.\nOn the other hand, we study on value differences of value-\nrelated actions from all correct predictions, such as that our\nmodel says “bet $1.5”, while the real action in test set is\n“bet $2.0”. Since there exist multiple currencies and big\nblind values in our data, we unify currency to US dollar to\nrepresent all money using big blind. As mentioned before,\nwe discretize values of “bet” and “raise” into multiple of big\nblind. Specifically, here we adopt {0, 1, 3, 6, 10, 20, 50, 100,\nall-in} as the multiples of big blind for decision, while the\nvalues exceeding user’s owned money will be excluded. As we\nregard value prediction as a multi-label classification task, we\ncompare the frequency of each multiple chosen to evaluate the\naccuracy of predicted values, as well as using mean squared\nerror (MSE) for a general evaluation.\nBesides, perplexity in Natural Language Processing (NLP)\nis a measure of how well a probability model predicts a\nsample. Assume we have a sequence W with N words, a\nperplexity is formulated as:\nperplexity(W) = P(w1, w2, ..., wN)−1\nN ,\n(3)\nwhere W = (w1, w2, ..., wN) is the sequence of words, and P\nis the probability. It quantifies the uncertainty of a model by\ncalculating the inverse probability of the test set, normalized\nby the number of words. In the context of language models,\na lower perplexity generally indicates the model is better at\npredicting the next word in a sentence.\n3) Multi-Player Evaluation:\nWe evaluate PokerGPT in\nmulti-player games using an open-source Poker simulation 3.\nFor setup consistency, all bots in multi-player experiments\nare based on a mandatory method that calculates equity.\nTo study the strategy variation of PokerGPT during player\namount changes, we design 2 metrics, action score and average\ninvestment, to represent the occurring frequency of each action\nand the money used in each game, relatively:\n• Action score: Since some actions can occur multiple\ntimes in one game, such as “check”, “raise”, “bet”, and\n“call”, while the others can only occur once, such as\n“all-in” and “fold”, the length of one game should be\nconsidered. Thus, as a game can have up to 4 stages,\nwe calculate action score for each action by dividing the\nnumber of times this action occurs in a game by the stage\n3https:\/\/github.com\/dickreuter\/neuron Poker\nTABLE II: Performance comparison against the latest 2 models.\nModel\nWin Rate (mbb\/h)\nTraining Time (h)\nResponse Speed (s)\nPlayer Amount\nParameter Amount\nMain Tech\nYear\nReBel [13]\n45±5\n-\n-\n2\n-\nDRL+CFR\n2020\nAlphaHoldem [14]\n111±16\n580\n0.017\n2\n8.6 million\nDRL\n2022\nPokerGPT\n158±49\n9.5\n5.4\n2 or more\n1.3 billion\nLLM\n2023\nnumber of this game. For example, if a player do “check”\nfor 3 times and “fold” in “TURN” stage, the action score\nfor “check” is 1, and the action score for “fold” is 0.33.\nFinally, we divide action scores by the amount of games,\nto make action scores ranged from 0 to 1.\n• Average investment: In Poker games, more money usage\nmeans both higher revenue and higher loss. By observing\nthe amount of average investment, we can know the style\nof playing more clearly. To get the average investment,\nwe calculate the mean of total money used by our model\nin each game.\nBy analyzing action scores and average investment, we can get\nmore detailed insights of our model’s strategies for different\nplayer amount.\n4) Analysis on Interaction: We show a case of PokerGPT\ninteraction with multiple players in processed dataset. The aim\nis to evaluate the general capability of our model through\nanalyzing the answers of different questions.\nC. Comparison with Previous Works\nFirstly, we compare our performance against Slumbot with\n2 novel works, recording multiple metrics in Table II. We can\nsee that our model makes progress in win rate. Compared to\nRebel (45+5) and Alphaholdem (111+16) against Slumbot, our\nmodel comes out on top with a win rate of 158+49, measured\nin mbb\/h, showing the superiority of our model. However,\nsince the standard deviation of our model is higher than the\nother 2 models, fluctuation of our performance is more violent,\nsuggesting the larger variation of the LLM’s answers led\nby the diversified nature of generative models, which should\nbe further optimized. Furthermore, our model completes the\ntraining in just 9.5 GPU hours (3.5 for supervised fine-tuning,\n1.5 for reward model training, and 4.5 for RLHF), compared to\n580 GPU hours for Alphaholdem, showing the improvement\non training efficiency. Thanks to the adaptability of LLMs,\nempowering LLMs with specific prior knowledge through fine-\ntuning on a relatively small scale of data is effective. Last but\nnot least, our model is capable of providing action instructions\nfor arbitrary number of players, while the other 2 models\nare limited to playing one-on-one situations. Even without\ntransferring that is needed for the other 2 models, LLM-based\nmodel can be directly generalized to various situations, saving\na lot of computational resources. On the other hand, though\nPokerGPT owns a much larger amount of parameters and a\nslower response speed than Alphaholdem, both metrics are\nacceptable in real games, and much better than previous CFR-\nbased works. Additionally, we illustrate the staged revenue\ndistributions of high-level players in real games and PokerGPT\nagainst Slumbot in Fig. 7. Compared with human players,\nPokerGPT tends to keep a more stable win rate by losing less\nin later stages, while humans are more likely to take a risk in\ngambling in later stages, since the later the stage is, both the\nrevenue and the risk can be higher.\nPREFLOP\nFLOP\nTURN\nRIVER\nStage\n0\n20\n40\n60\n80\nRevenue (bb)\nWin_Real\nLose_Real\nWin_LLM\nLose_LLM\nFig. 7: The distribution of revenue among 1000 games with\nunit of big blind. The real results are counted from the\ncollected dataset, while the results of PokerGPT are from the\ngames against Slumbot.\nD. Ablation Results\nTABLE III: Ablation on different dataset setup.\nDataset\nWin Rate (mbb\/h)\nMacro F1\nMSE\nPerplexity\nI\n-376±215\n31.34\n115.41\n4.25\nII\n229±137\n62.77\n36.77\n2.71\nIII\n1221±54\n77.63\n0.21\n1.03\nIV\n1034±52\n72.22\n0.95\n1.04\nV\n608±59\n68.15\n8.28\n1.05\nVI\n-574±76\n59.69\n177.26\n1.09\nIn ablation experiments, we firstly evaluate the effectiveness\nof each technique used in our model. As shown in Table III,\nby only fine-tuning raw data, Dataset I shows the worst\nmacro F1 score and perplexity, because it is difficult for the\nLLM to learn effectively from a bunch of unformatted data,\nwhose information is in chaos. Thus, the LLM can not even\nmake actions valid for Poker games. Dataset II shows much\nbetter performance than Dataset I, indicating the importance\nof information filtering applied in our model. After selecting\nplayers with different win rate for fine-tuning, we can see\nthere is a trend that the holistic performance of our model\ncheck call fold bet raise\ncheck\ncall\nfold\nbet\nraise\nTrue Labels in Real Data\n1335 34\n189\n15\n3\n25 1776 320\n3\n6\n46\n56\n437\n18\n1\n12\n2\n34\n269\n12\n22\n23\n25\n1\n112\nDataset III\ncheck call fold bet raise\ncheck\ncall\nfold\nbet\nraise\n1307 47\n201\n19\n2\n32 1731 356\n5\n6\n58\n88\n376\n31\n5\n19\n3\n64\n229\n14\n23\n26\n30\n0\n104\nDataset IV\n0\n500\n1000\n1500\ncheck call fold bet raise\nLLM4MPNLTH Output\ncheck\ncall\nfold\nbet\nraise\nTrue Labels in Real Data\n1287 44\n215\n24\n6\n29 1667 391\n26\n17\n59\n97\n344\n41\n17\n20\n4\n71\n218\n16\n26\n22\n32\n1\n102\nDataset V\ncheck call fold bet raise\nLLM4MPNLTH Output\ncheck\ncall\nfold\nbet\nraise\n1212 38\n223\n76\n27\n31 1441 598\n16\n44\n66\n129 305\n21\n37\n51\n4\n82\n143\n49\n9\n15\n24\n1\n134\nDataset VI\n0\n500\n1000\n1500\nFig. 8: Confusion matrix of test results outputted by Pok-\nerGPT fine-tuned using Dataset III to VI.\nis positively related to win rate, where Dataset III has the\nbest performance while Dataset VI is the worst. According to\nmacro F1 score and MSE, fine-tuning on worse dataset leads\nto not only more incorrect action choices, but also irrational\ninvestment. Interestingly, Dataset VI has a similar perplexity\nto Dataset III to V while performing much worse, indicating it\nis firm about its bad action choice. This result emphasizes the\nimportance of data quality. In conclusion, all the techniques\nadopted in this work show effectiveness in different extent, in\nwhich prompt selection based on players’ win rate presents\nthe strongest effect.\nTo check the training bias, we show the confusion matrix\nof classification results of Dataset III to VI in Fig. 8. The test\nset contains 1000 games with 1576 “check”s, 2130 “call”s,\n558 “fold”s, 329 “bet”s, and 183 “raise”s. We can see that the\nmodel of Dataset III has better prediction on “check”, “call”,\n“fold”, and “bet” than the model of Dataset VI does, indicating\nthat our model can learn strategies more similar to winners’\noperations from data of high-level players. On the other hand,\nthe reason for the higher accuracy of “raise” of Dataset VI\ncan be that the players losing a lot have a stronger “raise”\ninclination which influence the action style of our model.\nBesides, an interesting appearance is that both the 2 models\ntend to give more “fold” than other actions. This could be\nattributed to that the LLM has the basic concept of game\ngoal the value judgment of Poker, that is, if it is confusing\nto choose an action, the model will choose “fold” in order to\nreduce investment. Additionally, compared to Dataset III, the\noutputs of Dataset VI shows higher ratio of choosing “raise”,\nleading to worse macro F1 score, reaffirming the importance\nof data quality for LLM fine-tuning.\nMoreover, we record the investment distribution of the truth\nand predicted investment from models fine-tuned on Dataset\nTruth\nIII\nIV\nV\nVI\nDataset\n0\n50\n100\nInvestment value (bb)\nMean\nFig. 9: The distribution of investment values among 1000\ngames, whose unit is big blind. The 2 dashed lines are the\nmean with standard deviation.\nIII to VI, shown in Fig. 9. In this figure, the mean of investment\nincreases and the distribution diversifies with the data quality\ndegrades. Generally, winners intend to invest less than 10 bb\nfor an easy win, while players who win less or lose more\nincline to invest more money. The results imply that the\nstrategies adopted by high-level players are more conservative,\nwhich is similar to winners in the truth. On the other hand,\nthe similarity is, the investment inclination under all cases is\nto focus on relatively small values, and invest higher values\nwith lower frequency, indicating that our model has learned\nthe common sense of investment risk in games. In terms of the\ndistribution of Dataset VI, the mean of the model’s investment\neven reaches about 10, undertaking much higher risk.\nPREFLOP\nFLOP\nTURN\nRIVER\nStage\n0\n200\n400\n600\n800\n1000\nAmount of Actions\ncheck_III\ncheck_VI\ncall_III\ncall_VI\nfold_III\nfold_VI\nbet_III\nbet_VI\nraise_III\nraise_VI\nFig. 10: The predicted actions of 1000 games from LLMs\nfine-tuned on Dataset III and Dataset VI. The dashed bars\nrepresent the real actions.\nAdditionally, we record staged actions of our model fine-\ntuned on Dataset III and Dataset VI. As shown in 10, the\nresults of PokerGPT on Dataset III always be more likely to\n“fold” in all stages, indicating the model’s style is conserva-\ntive. In contrast, PokerGPT on Dataset VI prone to “raise” and\n“bet”, and rarely “fold”, showing its aggressiveness. These 2\nphenomenons can explain that players with higher win rates\nare usually more cautious that those who lose more. Both\nmodels tend to “call” in later stages, implying that “call” is a\nmore general choice in some extent.\nAs a conclusion, the above ablation experiments validate\nthe effectiveness of our approaches.\nE. Multi-Player Evaluation\nTo evaluate the performance of PokerGPT when dealing\nwith multiple players, we make it play 1000 games for each\nplayer amount. The Fig. 11(a) presents the varying tendency of\n2 key metrics of our model when players’ number increases.\nThe trend of win rate changes more significantly during player\namount 2 to 5, then shows a slower declination during player\namount from 6 to 9, and finally fluctuations with a small\namplitude appears during player amount 10 to 15. When\nplaying 2-player games, our model gets a higher win rate\nagainst a rule-based bot which considers equity, indicating the\ncapability of handling the easiest Poker games. Since situations\nbecome more complex and the win rate declines when the\nplayer amount rises, our model gains less but still positive\nrevenue when competing with more opponents, showing the\nadvantages of our model in multi-player games. Meanwhile,\nthe gradual reduction of the shaded part implies that the\nfluctuation of the win rate decreases.\n5\n10\n15\nPlayer Amount\n250\n500\n750\nWin Rate (mbb\/h)\n5.5\n6.0\n6.5\nResponse Speed (s)\n(a)\n5\n10\n15\nPlayer Amount\n0.0\n0.2\n0.4\n0.6\n0.8\nAction Score\nCall\nCheck\nFold\nBet\nRaise\nAll-in\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAverage Invest (bb)\n(b)\nFig. 11: (a) The trends of win rate and response speed when\nplayer amount changes. The dashed lines are the standard de-\nviations. (b) Strategy variation during player amount changes.\nMoreover, the response time rises about linearly, while\nthe win rate declines drastically at first and gradually goes\nmoderate. This phenomenon validates the good scalability of\nour work. Different from CFR, LLMs intend not to fully\nexplore all possible scenarios. Instead, LLMs directly look\nfor connection between inputs and outputs. As the number\nof players increases, the amount of input information shows\nroughly linear growth, and the number of possible situations\ninvolved in each round of the game consequently goes up. As a\nresult, the linear time increase shows the efficiency of end-to-\nend learning on LLMs. In conclusion, all these phenomenons\ndemonstrate the advantages of our model in Poker games, and\nextensive potential in IIGs.\nFurthermore, to study the variation of PokerGPT’s strategy\nwhen facing multiple players, we calculate the action score for\neach action and the average investment, shown in Fig. 11(b).\nAccording to action scores, we can observe each action and\nget insights separately:\n• “Call”: We can see that the score of the “call” strategy\nincreases with the player amount, suggesting that the\nPokerGPT prefers to be more conservative when facing\nmore opponents.\n• “Check”: The score of “check” increases till 6-player\ngames, and then deceases into a range. Since “check”\nis not only a compromising behavior but also a bait for\naggressive opponents, players can choose “check” in any\nsituation, especially in games with small player amount.\nHowever, when the scale of games becomes larger than 10\nplayers, bait tactics become less effective. Thus, “check”\ncan usually be regarded as a compromise, and chosen in\na relatively stable frequency.\n• “Fold”: As “fold” score increases a lot when player\namount becomes larger than 6, 2 explanations can be\ndrew. When there are more players, it is complex to\nforecast the rank strength among dozens of hands, leading\nto a high rate of “fold”. Also, “fold” score can be higher\nwith the game length shortening, since it is reasonable\nfor bots to “fold” if possibility of stronger rank existing\nincreases, leading to a quick end of one game.\n• “Bet” and “Raise”: The scores of “bet” and “raise”\ndecrease with the number of players. This implies that the\nmodel intends to avoid risk when facing more players.\n• “All-in”: Actually, “all-in” is a special case of “bet” or\n“raise”. The score of the “all-in” is very low and even\ntends to 0 with more opponents. This reflects that the\nmodel rarely chooses this action when player amount\nincreases, as the risk of “all-in” is higher.\nIn summary, actions suggested by our model tend to be more\ncautious and conservative as the number of players increases,\nwhich are generally consistent with real games.\nF. Analysis of Interaction\nIn Fig. 12, we show the interaction with PokerGPT during\na Poker game. In Q1, we ask PokerGPT to give a direct\naction recommendation and get “call” as feedback. Then, in\nQ2, when we request it to be more aggressive, it suggests\nto make a “raise” to 0.8. It shows that the human language\ncomprehension of LLMs help PokerGPT adjust the strategy\naccording to the user’s request. In “FLOP” stage with 3 public\ncards exhibited, PokerGPT successively gives different an-\nswers to Q3 and Q4, from “check” to “all-in”, suggesting that\nPokerGPT has the understanding of card strengths. Besides, in\nQ6, we ask about Seat 9’s rank, with the answer that Seat 9\npossibly has a pair or a flush since this player likes to “raise”\nand “bet”, suggesting that PokerGPT has the awareness of\nan opponent’s strength based on his or her actions. However,\nfrom the answer of Q7 and Q9, PokerGPT shows its shortage\non guessing cards, probably due to the large quantities of\ncard combinations. Similarly, in Q9, PokerGPT has difficulty\nof giving a winning probability, since we do not use any\nprobability as a part of the prompt, while LLMs are not\nproficient in math, neither.\nVI. CONCLUSION AND FUTURE WORK\nInspired by the prosperity and prominence of LLMs, our\nstudy explores the applicability of LLMs for playing Poker.\nBy conducting prompt engineering on data of real games and\nfine-tuning a lightweight pre-trained LLM based on RLHF, we\nPokerGPT\nSystem (constant information): You are an experienced\ngambler. Now you need to assist me to make decisions\nin Texas Hold’em games. You have been provided with\na series of observable information:\nPlayer amount: [6], Currency: USD, Blind value: [0.02\/0.05],\nOrder: [‘2’, ‘3’, ‘5’, ‘6’, ‘7’, ‘9’], Seat 2 is small blind.\nMy cards: [‘Th’, ‘Ah’], the characteristics of my cards: [“suit”,\n“high”, “close”], My seat: [Seat 2]\nDynamic information:\nStage: “PREFLOP”, Public cards: [‘**’ ‘**’ ‘**’ ‘**’ ‘**’]\nMy rank: [“High”], Money: [3.92], Action: []\nSeat 3: [‘**’, ‘**’], Money: [2.33], Action: [], Discard: [False]\nSeat 5: [‘**’, ‘**’], Money: [5.54], Action: [“fold”], Discard:\n[True]\nSeat 6: [‘**’, ‘**’], Money: [3.75], Action: [“fold”], Discard:\n[True]\nSeat 7: [‘**’, ‘**’], Money: [4.22], Action: [“fold”], Discard:\n[True]\nSeat 9: [‘**’, ‘**’], Money: [1.47], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.17]\nThe actions can be: [“fold”, “raise”, “call”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.92}.\nUser:\nQ1:What should I do?\nPokerGPT: You should call.\nQ2: Please be aggressive.\nPokerGPT: You should raise to 0.5.\nInformation updates:\nStage: “FLOP”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘**’ ‘**’]\nMy rank: [“Flush”], Money: [3.82], Action: [“call”]\nSeat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”], Discard:\n[False]\n......\nSeat 9: [‘**’, ‘**’], Money: [1.37], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.3]\nThe actions can be: [“fold”, “check”, “bet”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.82}.\nUser:\nQ3:What should I do?\nPokerGPT: You should check.\nQ4: Should I all-in?\nPokerGPT: As you have a flush, all-in is fine.\nInformation updates:\nStage: “FLOP”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘**’ ‘**’]\nMy rank: [“Flush”], Money: [3.82], Action: [“call”, “check”]\nSeat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”, “check”],\nDiscard: [False]\nSeat 9: [‘**’, ‘**’], Money: [1.15], Action: [“raises 0.05 to\n0.1”, “bets 0.22”], Discard: [False] The pot value is [0.52]\n......\nThe actions can be: [“fold”, “call”, “raise”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.82}.\nUser:\nQ5:What should I do?\nPokerGPT: You should raise to 0.5.\nQ6: Please guess Seat 9’s rank.\nPokerGPT: Seat 9 likes to raise and bet. Therefore, he might\nbe three of a kind or flush.\nQ7: Please guess Seat 9’s cards.\nPokerGPT: Seat 9 likes to raise and bet. Therefore, he might\nbe there of a kind or flush. But there is no evidence what cards\nSeat 9 has.\nInformation updates:\nStage: “TURN”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘Ks’ ‘**’]\nMy rank: [“Flush”], Money: [3.6], Action: [“call”, “check”,\n“call”] Seat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”,\n“check”, “fold”], Discard: [True]\n......\nThe actions can be: [“fold”, “check”, “bet”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.6}.\nUser:\nQ8:What should I do?\nPokerGPT: You should bet 1.\nQ9: Please tell me my winning probability.\nPokerGPT: Sorry, I can’t calculate the probability. However,\nas you have flush, you are very likely to win.\nInformation updates:\n... My Action: [“call”, “check”, “call”, “bet 0.9”]\n......\nSeat 9: [‘**’, ‘**’], Money: [1.37], Action: [“raises 0.05 to\n0.1”, “bets 0.22”, “fold”], Discard: [True]\nThe pot value is [1.42]\nPlayer 2 wins $1.42 with “Flush”.\nFig. 12: The question-answering instance of PokerGPT.\nhave successfully developed an end-to-end solver for Poker\nwith arbitrary amount of players, which is also convenient for\ninteraction. The experimental results are notably satisfactory,\nshowing a great advantage in training cost and inference speed\namong previous works, while obtaining a competitive win rate.\nPokerGPT addresses a series of challenges encountered by\nprevious Poker AI, substantiating the considerable potential\nof LLMs in tackling IIGs.\nIn subsequent research directions, we aim to further improve\nthe capability of PokerGPT. To empower our model with\nexplainability on decisions, various data should be prepared,\nincluding explanations on actions in each round written by\nhumans, probabilities of each action which can be outputted\nby CFR models, multiple candidate decisions, etc.\nREFERENCES\n[1] D. A. Waterman, “Generalization learning techniques for automating\nthe learning of heuristics,” Artificial Intelligence, vol. 1, no. 1-2, pp.\n121–170, 1970.\n[2] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis`y, D. Morrill, N. Bard,\nT. Davis, K. Waugh, M. Johanson, and M. Bowling, “Deepstack: Expert-\nlevel artificial intelligence in heads-up no-limit poker,” Science, vol. 356,\nno. 6337, pp. 508–513, 2017.\n[3] N. Brown and T. Sandholm, “Superhuman ai for heads-up no-limit\npoker: Libratus beats top professionals,” Science, vol. 359, no. 6374,\npp. 418–424, 2018.\n[4] N. Brown, T. Sandholm, and B. Amos, “Depth-limited solving for\nimperfect-information games,” Advances in neural information process-\ning systems, vol. 31, 2018.\n[5] N. Brown and T. Sandholm, “Solving imperfect-information games via\ndiscounted regret minimization,” in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 33, no. 01, 2019, pp. 1829–1836.\n[6] ——, “Superhuman ai for multiplayer poker,” Science, vol. 365, no.\n6456, pp. 885–890, 2019.\n[7] M. Bowling, N. Burch, M. Johanson, and O. Tammelin, “Heads-up limit\nhold’em poker is solved,” Science, vol. 347, no. 6218, pp. 145–149,\n2015.\n[8] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\nM. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained transformer\nlanguage models,” arXiv preprint arXiv:2205.01068, 2022.\n[9] J. Rubin and I. Watson, “Computer poker: A review,” Artificial intelli-\ngence, vol. 175, no. 5-6, pp. 958–987, 2011.\n[10] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione, “Regret\nminimization in games with incomplete information,” Advances in\nneural information processing systems, vol. 20, 2007.\n[11] N. Brown and T. Sandholm, “Safe and nested subgame solving for\nimperfect-information games,” Advances in neural information process-\ning systems, vol. 30, 2017.\n[12] N. Brown, A. Lerer, S. Gross, and T. Sandholm, “Deep counterfactual\nregret minimization,” in International conference on machine learning.\nPMLR, 2019, pp. 793–802.\n[13] N. Brown, A. Bakhtin, A. Lerer, and Q. Gong, “Combining deep\nreinforcement learning and search for imperfect-information games,” Ad-\nvances in Neural Information Processing Systems, vol. 33, pp. 17 057–\n17 069, 2020.\n[14] E. Zhao, R. Yan, J. Li, K. Li, and J. Xing, “Alphaholdem: High-\nperformance artificial intelligence for heads-up no-limit poker via end-\nto-end reinforcement learning,” in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 36, no. 4, 2022, pp. 4689–4697.\n[15] OpenAI, “Gpt-4 technical report,” 2023.\n[16] L. Chen, B. Li, S. Shen, J. Yang, C. Li, K. Keutzer, T. Darrell, and Z. Liu,\n“Language models are visual reasoning coordinators,” in ICLR 2023\nWorkshop on Mathematical and Empirical Understanding of Foundation\nModels, 2023.\n[17] X. Zou, Z.-Y. Dou, J. Yang, Z. Gan, L. Li, C. Li, X. Dai, H. Behl,\nJ. Wang, L. Yuan et al., “Generalized decoding for pixel, image, and\nlanguage,” in Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, 2023, pp. 15 116–15 127.\n[18] K. Bi, L. Xie, H. Zhang, X. Chen, X. Gu, and Q. Tian, “Accurate\nmedium-range global weather forecasting with 3d neural networks,”\nNature, pp. 1–6, 2023.\n[19] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, “Otter: A\nmulti-modal model with in-context instruction tuning,” arXiv preprint\narXiv:2305.03726, 2023.\n[20] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso et al., “Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of\nlanguage models,” arXiv preprint arXiv:2206.04615, 2022.\n[21] S. V¨artinen, P. H¨am¨al¨ainen, and C. Guckelsberger, “Generating role-\nplaying game quests with gpt language models,” IEEE Transactions on\nGames, 2022.\n[22] J. Kelly, M. Mateas, and N. Wardrip-Fruin, “Towards computational\nsupport with language models for ttrpg game masters,” in Proceedings of\nthe 18th International Conference on the Foundations of Digital Games,\n2023, pp. 1–4.\n[23] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz,\n“Playing repeated games with large language models,” arXiv preprint\narXiv:2305.16867, 2023.\n[24] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\nD. Drain, S. Fort, D. Ganguli, T. Henighan et al., “Training a helpful and\nharmless assistant with reinforcement learning from human feedback,”\narXiv preprint arXiv:2204.05862, 2022.\n[25] J. Abramson, A. Ahuja, F. Carnevale, P. Georgiev, A. Goldin, A. Hung,\nJ. Landon, J. Lhotka, T. Lillicrap, A. Muldal et al., “Improving mul-\ntimodal interactive agents with reinforcement learning from human\nfeedback,” arXiv preprint arXiv:2211.11602, 2022.\n[26] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, “Calibrate\nbefore use: Improving few-shot performance of language models,” in\nInternational Conference on Machine Learning.\nPMLR, 2021, pp.\n12 697–12 706.\n[27] L.\nWeng,\n“Prompt\nengineering,”\nhttps:\/\/lilianweng.github.io\/posts\/\n2023-03-15-prompt-engineering\/, march 15, 2023.\n[28] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari, X. Wu, A. A.\nAwan, J. Rasley, M. Zhang, C. Li, C. Holmes et al., “Deepspeed-\nchat: Easy, fast and affordable rlhf training of chatgpt-like models at\nall scales,” arXiv preprint arXiv:2308.01320, 2023.\n[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[30] E. G. Jackson, “Slumbot nl: Solving large games with counterfactual\nregret minimization using sampling and distributed processing,” in Work-\nshops at the Twenty-Seventh AAAI Conference on Artificial Intelligence,\n2013.\n[31] Y. Yang, “An evaluation of statistical approaches to text categorization,”\nInformation retrieval, vol. 1, no. 1-2, pp. 69–90, 1999.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nPokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model\n```\n#### 2. 论文摘要\n```\nPoker, also known as Texas Hold'em, has always been a typical research target\nwithin imperfect information games (IIGs). IIGs have long served as a measure\nof artificial intelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret minimization\n(CFR) to tackle heads-up no-limit Poker. However, it is challenging for\nsubsequent researchers to learn CFR from previous models and apply it to other\nreal-world applications due to the expensive computational cost of CFR\niterations. Additionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work, we introduce\nPokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number\nof players and gaining high win rates, established on a lightweight large\nlanguage model (LLM). PokerGPT only requires simple textual information of\nPoker games for generating decision-making advice, thus guaranteeing the\nconvenient interaction between AI and humans. We mainly transform a set of\ntextual records acquired from real games into prompts, and use them to\nfine-tune a lightweight pre-trained LLM using reinforcement learning human\nfeedback technique. To improve fine-tuning performance, we conduct prompt\nengineering on raw data, including filtering useful information, selecting\nbehaviors of players with high win rates, and further processing them into\ntextual instruction using multiple prompt engineering techniques. Through the\nexperiments, we demonstrate that PokerGPT outperforms previous approaches in\nterms of win rate, model size, training time, and response speed, indicating\nthe great potential of LLMs in solving IIGs.\n```\n\n#### 3. 论文全文\n```\nPokerGPT: An End-to-End Lightweight\nSolver for Multi-Player Texas Hold’em via\nLarge Language Model\nChenghao Huang‡, Yanbo Cao‡, Yinlong Wen†, Tao Zhou‡, and Yanru Zhang‡§\n‡University of Electronic Science and Technology of China\n§Shenzhen Institute of Advanced Study, UESTC, China\nzydhjh4593@gmail.com, yanbocao@std.uestc.edu.cn, zhutou@ustc.edu, yanruzhang@uestc.edu.cn,\n†Sichuan Optical Character Technology Co., Ltd\nlywen@chineseocr.com\nAbstract—Poker, also known as Texas Hold’em, has always\nbeen a typical research target within imperfect information\ngames (IIGs). IIGs have long served as a measure of artificial\nintelligence (AI) development. Representative prior works, such\nas DeepStack and Libratus heavily rely on counterfactual regret\nminimization (CFR) to tackle heads-up no-limit Poker. However,\nit is challenging for subsequent researchers to learn CFR from\nprevious models and apply it to other real-world applications\ndue to the expensive computational cost of CFR iterations.\nAdditionally, CFR is difficult to apply to multi-player games due\nto the exponential growth of the game tree size. In this work,\nwe introduce PokerGPT, an end-to-end solver for playing Texas\nHold’em with arbitrary number of players and gaining high win\nrates, established on a lightweight large language model (LLM).\nPokerGPT only requires simple textual information of Poker\ngames for generating decision-making advice, thus guaranteeing\nthe convenient interaction between AI and humans. We mainly\ntransform a set of textual records acquired from real games\ninto prompts, and use them to fine-tune a lightweight pre-\ntrained LLM using reinforcement learning human feedback\ntechnique. To improve fine-tuning performance, we conduct\nprompt engineering on raw data, including filtering useful infor-\nmation, selecting behaviors of players with high win rates, and\nfurther processing them into textual instruction using multiple\nprompt engineering techniques. Through the experiments, we\ndemonstrate that PokerGPT outperforms previous approaches\nin terms of win rate, model size, training time, and response\nspeed, indicating the great potential of LLMs in solving IIGs.\nIndex Terms—Data-driven artificial intelligence, imperfect in-\nformation game, large language model, reinforcement learning\nhuman feedback, end-to-end learning.\nI. INTRODUCTION\nSince the birth of the artificial intelligence (AI), intelligent\ngame research has been fertile ground for the development\nand innovation of AI, and has always been an important\nevaluation criterion for measuring its development level. In\n2016, AlphaGo defeated the human world champion in the\ngame of Go, which was considered as an important milestone\nfor AI solving perfect information games. However, a dis-\ntinctive feature of real-world games is the uncertainty of the\nopponent due to incomplete information. Poker is a typical\nimperfect information game (IIG) that has a long history as a\nchallenging problem for developing AI that can address hidden\ninformation [1]. Poker is a superb experimental and testing\nplatform, providing an excellent arena for the exploration\nand validation of foundational theories and methodologies in\nintelligent games.\nIn recent years, significant advancements have been made in\noptimized solutions for the game of Poker. Notably, DeepStack\nfrom the University of Alberta, Canada [2], and Libratus from\nCarnegie Mellon University (CMU), USA, have surpassed\nhuman professionals in heads-up no-limit Poker, a 2-player\nform of Poker [3]. The works of [4], [5] have been further\nproposed to improve the computational efficiency. Subsequent\nto these achievements, the Carnegie Mellon-designed Pluribus\ndemonstrated superiority by defeating a human professional in\n6-player no-limit Poker [6]. These prominent AI models for\nPoker commonly adopt iterative Counterfactual Regret Mini-\nmization (CFR) algorithms to approximate Nash equilibrium\nstrategies, involving initially compressing the state and action\nspace of the hand to reduce the game tree’s size, followed by\niterations of the CFR algorithm over the condensed game tree.\nHowever, these methods exhibit the following drawbacks:\n• Computational and storage resource consumption:\nDespite model reduction, significant computational and\nstorage resources are still required, escalating the overall\ncomputational cost. For example, in 2015, the work of [7]\nconsumed 4800 CPU cores, 10.9 TB memory, and 2\nmonths to solve heads-up limit Poker through CFR.\n• Difficulty in multi-player extension: Directly extending\nthe CFR framework to multi-player Poker environments\nproves to be challenging. An increase in the number\nof players leads to an exponential growth in the game\ntree size, diminishing generalization performance. For\nexample, CMU used CFR to solve 6-player no-limit\nPoker and consumed 12800 CPU cores, 12.5 TB memory,\nand 8 days [6]. The lack of flexibility make it unrealistic\nto use existing works in real-world games.\n• Loss of information: Game tree compression unavoid-\nably results in the loss of crucial information for decision\nmaking, such as the behavior and psychology of different\narXiv:2401.06781v1  [cs.AI]  4 Jan 2024\nplayers. This can lead to severe variations in fraudulent\nand anti-fraudulent strategies, jeopardizing the win rate\nagainst top players.\n• Dependence on human expert knowledge: For com-\npressing the game tree in CFR to reduce computational\ncost, human expert knowledge on mathematics and com-\nputational theory is heavily needed, causing large knowl-\nedge bottleneck and labor effort for improvement.\nHence, there arises a pressing need for a lightweight model\ncapable of autonomously extracting information to optimize\nIIGs further. Such a model should exhibit superior generaliza-\ntion performance, reduced dependence on expert knowledge,\nand enhanced adaptability to multi-player environments with-\nout compromising computational efficiency. Addressing this\nchallenge will be a pivotal focus in future research within the\nfield of Poker AI.\nThanks to the strong capability of large language model\n(LLM), it is promising to achieve a prominent framework for\nplaying Poker in real games. LLM is a novel deep learning\n(DL) model that can handle massive amounts of natural\nlanguage data, with various advantages in human-computer\ninteraction. Firstly, by pre-training on large-scale text data,\nLLM can extract word vectors while also learning contextual\ninformation to enhance the natural language understanding\ncapabilities. Secondly, LLM integrates generative tasks into\nthe encoder-decoder structure to achieve a unified natural\nlanguage processing capability, thereby improving the qual-\nity of text generation. Meanwhile, LLM exhibits flexibility\nand adaptability of human-computer interaction due to its\nremarkable zero-shot and few-shot abilities, that is, in-context\nlearning that only changes the input rather than the parameters.\nLast but not least, end-to-end learning streamlines the need\nfor intricate feature engineering and intermediate steps. LLMs\ncan autonomously acquire task-relevant rules and properties\nleveraging the powerful representation capabilities of deep\nneural networks. Thus, it is feasible to develop an LLM-\nbased approach for playing real games of multi-player no-limit\nPoker.\nIn this paper, we utilize a lightweight LLM and data\nfrom real Poker games to construct an AI solver for Poker\nsolutions, named PokerGPT. We firstly collect textual records\nof real games with arbitrary amount of players. Then, we\nfilter necessary information and process these records into\nprompts, a textual data form that is understandable for both\nhumans and LLMs. For dataset preparation, we integrate these\nprompts with actions in real games as labels. Based on the\ndataset, we fine-tune OPT-1.3B, a lightweight open-source\nLLM from Facebook [8] using reinforcement learning from\nhuman feedback (RLHF), a popular technique for fine-tuning\nLLMs. The interaction process is shown in Fig. 1. Our codes\nare publicly available 1.\nThe pivotal contributions of our work are listed as follows:\n• We present a novel approach based on LLMs to address\nIIGs. This approach employs an end-to-end learning\n1https:\/\/github.com\/hch211\/TH LLM\nPlayer's number: 6\nDealer's position: 6\nPlayer 1,3 : fold;\nPlayer 2: bet 1.5;\n······\nWhat should I do?\nCall 1.5\nInput:\nOther players' actions\nOutput:\nAction instruction\nInput\nPokerGPT\n1\n2\n3\n6\n5\n4\nObtain\ninformation\nFig. 1: Interaction process of PokerGPT.\nmethod, proved to be easily trainable and considerably\ncost-effective compared with previous approaches.\n• We propose PokerGPT, a pioneering DL model specifi-\ncally designed for Poker. With the power of LLMs, our\nmodel has a prominent advantage on convenient interac-\ntion with humans. More importantly, PokerGPT has the\ncapability of dealing with an arbitrary number of players\nin Poker games, showing the outstanding flexibility which\nhas never been achieved by previous works. Besides, our\nmodel contains much less parameters and takes much less\ninference time compared with conventional methods such\nas CFR, while consumes much less training time than\nnovel DL works, achieving great resource conservation.\n• Based on data of real games, we devise an efficient data\nprocessing approach specific to Poker, consisting of data\ncleansing and prompt engineering, leading to significant\nimprovements on fine-tuning of PokerGPT.\n• Through experiments, we demonstrate that our model\nexhibits significant advantages over existing models in\nterms of various metrics, such as win rate, training speed,\nresponse time, etc. Furthermore, we design 2 metrics,\nnamed action score and average investment, to study\ncharacteristics of PokerGPT’s strategies on Poker.\nThe remainder of this paper is organised as follows: Sec-\ntion II gives a literature review of classical works of Poker and\nLLM; Section III introduces the rules of Poker and the princi-\nples of LLMs; Section IV explains the holistic architecture of\nour proposed PokerGPT, including data acquisition, prompt\nengineering, and training process; Section V describes the\ncase studies, shows performance of PokerGPT with multiple\nmetrics, and analyzes the results; Section VI concludes this\nwork and presents pathways for future work.\nII. RELATED WORK\nA. Solutions for Poker\nPoker is a popular and challenging form of poker that has\nbeen used as a testbed for IIG algorithms for decades [9].\nA major breakthrough in this field was the introduction of\ncounterfactual regret minimization (CFR) [10], a simple and\nefficient iterative algorithm that converges to a Nash equilib-\nrium by minimizing the regrets of both players over time.\nCFR enabled the solution of heads-up limit Poker in 2015 [7],\nshifting the research focus to the more complex and realistic\nvariant of Poker.\nCFR once became a widely used algorithm for solving\nPoker, and a challenging benchmark for IIGs. Previous works\nhave shown that CFR variants can achieve superhuman per-\nformance in Poker against professional human players. The\nwork of [11] introduced subgame-solving techniques that\noutperform prior methods both in theory and practice. By\nfusing these techniques, in [3], the authors presented Libratus,\nan AI based on Monte-Carlo CFR (MCCFR) that defeated 4\ntop human specialists in heads-up no-limit Poker in a 120000-\nhand competition. Though Libratus was the leading benchmark\nin IIG solving, it required millions of core hours and terabytes\n(TBs) of memory to calculate. To improve computational\nefficiency, the work of [4] substituted values at leaf nodes\nof unknown states to limit the depth of the decision tree, and\nalso outperformed prior methods. The work of [5] introduced\nnovel CFR variants to discount regrets from earlier iterations\nin some cases differently for positive and negative regrets, and\nreweight iterations to obtain the output strategies. Since games\ncan be significantly complex when there are multiple players\nin games, the work of [6] extended Libratus to Pluribus, an\nAI that defeated top human professionals in 6-player Poker,\nthe most popular form of Poker. However, Pluribus approaches\nare also computationally expensive and memory-intensive, and\nconsumed significant time for inference.\nTo overcome the limitations of CFR-based algorithms, DL\nmethods have been applied to exploit the power of big data and\nneural networks (NNs). For example, DeepStack [2] used an\nNN to approximate the CFR values and performed recursive\nreasoning, while deep CFR [12] improved the performance and\nefficiency by using NNs to directly learn the CFR strategy in\nthe full game without abstraction. To reduce the dependence\non prior knowledge and training time, self-play deep reinforce-\nment learning (DRL) was integrated with CFR to enhance the\nexploration [13]. Moreover, the works of [14] proposed an\nend-to-end framework based on DRL and introduced several\ntechniques to accelerate it and defeat DeepStack.\nIn conclusion, the above methods rely heavily on com-\nputational resources, prior knowledge, and domain-specific\nheuristics, which constrain the applicability. Besides, it is\ndifficult for humans to use these works, also harming the\npracticality. Therefore, developing more efficient algorithms\nfor Poker and other complex IIGs remains an open problem.\nB. LLM\nRecent advances in pre-trained language models, such as\nChatGPT and GPT-4 [15], have inspired a range of studies that\nleverage information from various domains and modalities.\nThese studies aim to enhance the performance and applicabil-\nity of language models for diverse tasks, and can be broadly\ndivided into system design and ene-to-end training. System\ndesign employs ChatGPT as a dispatcher that orchestrates dif-\nferent expert models for specific tasks, such as Cola [16] and\nX-GPT [17]. On the other hand, end-to-end training integrates\nmodels from different modalities into a unified framework\nthat can be trained end-to-end, and then fine-tuned on data\nfrom particular domains, such as weather forecasting [18] and\naugmented reality [19].\nThe use of LLMs for games has attracted increasing at-\ntention from researchers, who have obtained some intriguing\nfindings. For instance, the results of [20] showed that LLMs\ncould produce responses that were remarkably similar to\nhuman feedback, despite some limitations on specific tasks in\nthe Beyond the Imitation Game benchmark. This suggested\nthat LLMs had the potential to handle human-related sce-\nnarios. The authors of [21] leveraged GPT-2 and GPT-3 to\nprocedurally generate RPG video game quest descriptions.\nSimilarly, the authors of [22] used GPT-3 to provide dialogue\nsuggestions for non-player characters as well as to highlight\nrelevant game module information. They both found that GPT-\n3 could generate significantly better sentences than GPT-2,\ndemonstrating the superior performance of GPT-3 and the\npromising prospects of future GPTs. Moreover, the authors\nof [23] compared GPT-3, GPT-3.5, and GPT-4 models in\nplaying finitely repeated games with each other and with other\nhuman-like strategies. They observed that LLMs performed\nsub-optimally in games that required coordination, but ex-\ncelled in games that involved individual competition and self-\ninterest, such as the iterated Prisoner’s Dilemma family. The\nabove research implies that LLMs can be feasible as a Poker\nsolver.\nAlthough there are still many challenges regarding the use of\nLLMs for games, LLMs have shown great potential for game-\nrelated research and applications. In this work, we propose a\nframework for Poker via LLMs, emphasizing the potential for\nLLMs to solve IIGs\nIII. PREREQUISITES\nA. Poker Rules\nPoker is a game that consists of multiple rounds, each\nstarting with 2 private cards (“PREFLOP” cards) dealt to each\nplayer, followed by 5 public cards (community cards) dealt\nin 3 stages. The stages are a set of 3 cards (the “FLOP”),\nan additional single card (the “TURN”), and a final card\n(the “RIVER”). Each player aims to form the best 5-card,\nalso named as a hand, using 2 private cards and 5 public\ncards. Players can “bet”, “check”, “call”, “raise”, or “fold”\nat each round. The player who has the best hand at the\n“SHOWDOWN”, or who remains in the game after all others\n“fold”, wins the money in the pot. Fig. 2 illustrates an simple\nexample of a 2-player Poker game, and the bet size is limited\nonly by the total amount wagered in each round. Besides,\nFig. 3 shows the card strengths.\nB. LLM\nLLM is a kind of DNN that generates natural language texts\nfrom various inputs, such as prompts, queries, or contexts.\nLLMs learn from massive text corpora that cover diverse\nPREFLOP\n(Private Cards)\nPlayer 1\nPlayer 2\nBetting\nFLOP\n(First 3 public cards)\nBetting\nTURN\n(The 4th\npublic card)\nBetting\nRIVER\n(The 5th\npublic card)\nBetting\nSHOWDOWN\n(Each player gets best rank made by 5 cards\namong 2 private cards and 5 public cards)\nFlush\nTwo Pairs\nPlayer 1 wins\nAll gained by the winner\nPot\nPlayers bet in turn\nuntil everyone has\nthe same bet\nFig. 2: An example of a 2-player Poker game.\nFig. 3: Poker card strengths.\ndomains, such as books, news, articles, web pages, and social\nmedia posts. LLMs can perform various natural language\nprocessing tasks, such as text generation, text summarization,\nquestion answering, and machine translation. The main pro-\ncess of training a LLM is data preparation, pre-training, and\nfine-tuning.\n1) Pre-training: LLMs are pre-trained with self-supervised\nlearning, which leverages the data itself as supervision. Self-\nsupervised learning can capture different levels of language\nrepresentation, such as words, sentences, or documents. Com-\nmon self-supervised learning objectives for LLMs include\nmasked language modeling, next sentence prediction, per-\nmutation language modeling, causal language modeling, etc.\nSince our work focuses on fine-tuning and RLHF, we will not\nintroduce pre-training of LLM in details.\n2) Fine-tuning: LLMs are fine-tuned with supervised or\nsemi-supervised learning, which leverages labeled or partially\nlabeled data as supervision. Supervised or semi-supervised\nlearning can adapt LLMs to different natural language process-\ning tasks. After fine-tuning, LLMs can be used for inference.\nCommon inference tasks include text summarization, question\nanswering, machine translation, text generation, etc.\n3) RLHF: A popular ML technique for LLM is RLHF, that\ntrains a “reward model” based on human feedback as a reward\nfunction to optimize an agent’s policy using reinforcement\nlearning (RL) through an optimization algorithm [24], [25].\nThis method can improve the robustness and exploration of\nRL agents, especially when the reward function is sparse or\nnoisy. It is used in tasks where it is difficult to define a clear,\nalgorithmic solution but where humans can easily judge the\nquality of the model’s output.\nIV. POKERGPT\nIn this section, we introduce our framework, PokerGPT, in\ndetails. We firstly collect a group of game data on a online\nPoker platform in real world 2. Then we transform the data\ninto prompts which can be understood by both humans and\nLLMs. To improve performance, we select prompts with high\nquality for further training. By utilizing supervised fine-tuning,\nwe make a pre-trained LLM capable of making decisions when\nreceiving staged information provided by players in a Poker\ngame. For generalizability and interactivity, we adopt RLHF\nto enhance the supervised model. Besides, since the LLM\nrequires as much information as possible while the interaction\nmay frequently happen in a short period of time, we set all\nthe information as fixed instruction for simplicity. By doing\nthis, humans can interact with our model through just simple\ndialogues. Finally, we conduct supervised fine-tuning, reward\nmodeling, and RLHF to get PokerGPT. The framework of\nPokerGPT is demonstrated in Fig. 4.\nA. Data Acquisition\n1) Data Description: For increasing the practicality of our\nmodel, we acquire logs of Poker game on PokerStars. These\nlogs contain information as follows:\n• Basic information: This encompasses the establishment\nof both the big and small blinds, the seating arrangement\ndetails for each player, along with their respective chip\ncounts and the prevailing position of the current dealer.\nThis segment furnishes a thorough comprehension of the\nentire gaming scenario.\n• Dynamic information: For each game, we partition it\ninto distinct stages. Initially, there is the “PREFLOP”\nstage, during which each player receives 2 private cards,\ninitiating the first round of betting. Subsequently, the\n“FLOP”, “TURN”, and “RIVER” stages unfold, wherein\nthe community cards are progressively revealed, accom-\npanied by the betting actions of the remaining players.\nThe conclusive stage “SHOWDOWN” unveils the pri-\nvate cards, ultimately determining the definitive winner,\nwithout any action should be done by players.\n• Summary: An overview of the game pool size, the\ncommunity cards, and each player’s earnings is recorded.\nIt includes details on the winnings and losses associ-\nated with specific card combinations, instances of non-\nparticipation, and other relevant information.\nFig.5 shows an example of the game log. The chart reveals\nthat the blinds for this game are set as $0.02\/$0.05, with\n2PokerStars: https:\/\/www.pokerstars.com\nI. Data Acquisition\nCollect game data\nFilter useful information\nIf \"SHOWDOWN\"\nRounds played\nDiscard\nDiscard\nII. Prompt Engineering\nConstruct prompts\nIII. Training Process\nOnline platform: PokerStars\nGame log\n1. Small\/Big blinds.\n2. Seat arrangement.\n3. Dealer sear.\n4. Balance.\n......\n1. Stage.\n2. Public cards.\n3. Private cards.\n4. Actions.\n......\n1. Pot value.\n2. Winner info.\n3. Loser info.\n......\nBasic info\nDynamic info\nSummary\nNo\nYes\nUnder\nthreshold\nUseful\ninformation\nAbove\nthreshold\nCore info\nAdditional info\n1. Observable cards.\n2. Card characteristics.\n3. Own rank.\n4. Action sequence.\n1. Blind amount.\n2. Win rate.\n3. Player number.\n4. Pot value.\n......\nCalculate players' win rates\nPrompt selection\nSort and select\n(mbb\/h)\n150\n100\n60\n20\n-40\n...\nConstant\ninformation\nDynamic\ninformation\nEngineered\nPrompts\nOrganize\n1. Zero-shot.\n2. Role play.\n3. Discretization.\nPrompt\nengineering\ntechniques\nLow-quality\ndataset\nRoundly\nupdates\nPrompt: X\nAction: y\nHigh-quality\ndataset\nLow-level\nplayers\nHigh-level\nplayers\nTop N\nBottom M\nPre-trained LLM\nFacebook OPT-1.3b\nReward\nmodel\nSupervised\nfine-tuning\nContrastive\nfine-tuning\nPPO\nSupervised\nmodel\nRLHF\nPokerGPT\n1\n2\n7\n3\n4\n5\n6\nFig. 4: The framework of PokerGPT, consisting of data acquisition, prompt engineering, and training.\nthe dealer positioned at seat number 1. The chip count for\neach player is specified. For instance, the player in seat 1,\nphalves77, holds 5.12 in chips.\nDetails on players’ decisions and actions are as follows:\nDuring the “PREFLOP” stage, phalves77, seated in the\ndealer’s position, opted to “raise” to 0.15, while gefahren-\nsucher spent another 0.10 to “call” from the big blind. The\ncommunity cards unfolded as follows: the “FLOP” displayed\n[‘5s’, ‘Th’, ‘5c’], followed by the “TURN” card [‘2s’],\nand finally the “RIVER” card [‘Kh’]. After each round of\npublic cards, the recorded information includes the amount\nof each player’s bet, and their action choices. For instance,\nafter the “FLOP”, gefahrensucher give the “bet” of 0.16,\nwith phalves77 deciding to “call”. After the “TURN” and\n“RIVER”, gefahrensucher placed the “bet” of 0.31 and 1.21,\nrespectively, with phalves77 choosing “call” each time. During\nthe “SHOWDOWN”, gefahrensucher revealed a pair of ‘5’s\n(Fives), while phalves77 presented 2 pairs: ‘10’s (Tens) and\nFives. Consequently, phalves77 claimed the pot’s bottom as\nthe winner, gaining victory with the superior 2 pairs.\n2) Information Filtering:\nThe essential information of\nPoker games revolves around the public cards and the be-\nhaviors of the players in each game. The public cards stand\nas the pivotal information, forming an integral component of\na player’s ultimate hand. In Poker games, the unveiling of\npublic cards directly influences the rank of each player’s hand.\nThe actions of opponents during each round (“bet”, “raise”,\n“check”, “call”, “fold”) serve as vital indicators to infer their\ngefahrensucher: shows [3h 6h] (a pair of\nFives)                                                       \nphalves77: shows [Tc 4c] (two pair, Tens and\nFives)                                                     \nphalves77 collected $3.53 from pot\nSHOWDOWN\nTotal pot $3.68 | Rake $0.15\nBoard [5s Th 5c 2s Kh]\nSeat 1: phalves77 (button) showed [Tc 4c]\nand won ($3.53) with two pair, Tens and Fives\nSeat 2: Omar Hrn (small blind) folded before\nFlop\nSeat 3: gefahrensucher (big blind) showed[3h\n6h] and lost with a pair of Fives\nSeat 5: victorismyname folded before\nFlop(didn't bet)\nSeat 6: 0bjective folded before Flop(didn't bet)\ns : spade ; h : heart\nc : club ; d : diamond\nPokerStars Hand#xxxxxx ($0.02\/0.05 )\n6-max Seat    #1 is the button\nSeat 1: phalves77 ($5.12 in chips)\nSeat 2: Omar Hrn ($5.02 in chips)\nSeat 3: gefahrensucher ($6.64 in chips)\nSeat 5: victorismyname ($4.99 in chips)\nSeat 6: 0bjective ($2.14 in chips)\nOmar Hrn: posts small blind $0.02\ngefahrensucher: posts big blind $0.05\nGOBSTPPER: sits out\nBasic Information\nvictorismyname: folds  \n0bjective: folds\nphalves77: raises $0.10 to\n $0.15\nOmar Hrn: folds\ngefahrensucher: calls $0.10\nPREFLOP\nFlop : [5s Th 5c]\ngefahrensucher : bets\n   $0.16\nphalves77: calls $0.16\nFLOP\nTurn : [5s Th 5c] [2s]\ngefahrensucher : bets\n  $0.31\nphalves77: calls $0.31\nTURN\nRiver:\n[5s Th 5c 2s] [Kh]\ngefahrensucher : bets\n  $1.21\nphalves77: calls $1.21\nRIVER\nSUMMARY\nFig. 5: An example of Poker game log.\nstrategies and intentions, directly providing insights into the\nstrength of their cards. Moreover, the magnitude of money\nplaced exhibits a player’s confidence and card strength in a\nlarge extent. Also, the frequency of “raise” can enhance the\nunderstanding of opponent’s playing style and card strength,\nenabling strategic decisions on when to bluff and when to\ndiscern a bluff.\nCore information of utmost significance is crucial to be\nincluded in the prompt, listed as follows:\n• Observable cards: It is necessary to know the 2 private\ncards and public cards exhibited.\n• The characteristics of cards: We describe the charac-\nteristics of private cards in several ways. “Suit” means\n2 private cards with the same color. “High” means\npossessing one card higher than ‘9’. “Close” means the\ndifference of 2 cards is less than 5, which can form a\n“straight”.\n• The rank of the hand: Since it may be difficult for\nthe LLM to learn to recognize the rank of the hand,\nwe simply provide the rank in the prompt. The rank\ncan guide LLMs to understand the strength of different\ncombinations of private cards and public cards.\n• Action sequence: Analyzing players’ behavioral patterns\nplays a pivotal role in understanding the psychology and\nstrategy of opponents. Thus, it is crucial to record each\nplayer’s sequential actions.\nAdditional information also assists the learning and un-\nderstanding of our model, including:\n• Amounts of blinds: Small and big blinds is important for\nevaluating both the revenue and the risk of Poker games,\nthereby contributing to the understanding of the decisions\non “raise” or “fold”.\n• Positions of players: The positional context of a player\nrelative to the dealer, coupled with the player’s actions,\nserves as an indicator of the hand’s strength, which\ninfluences the players’ advantages and the game results.\n• Number of players: The variation in the number of play-\ners within a game intensely changes the game structure.\nIt is essential to make our model aware of and respond\nto the variation.\n• Balance: We record each player’s balance through the\ninitial amount and the amount each has invested in the\npot.\n• Discard: The “fold” chosen by players helps our model\nto determine the remaining players.\n• Pot value: The total amount of money put into the pot\nhelps LLM understand the concept of odds.\n• Stage: The same action in different stages generally has\ndifferent purposes.\nThe incorporation of the above information ensures an\neffective training process for our model, improving language\ngeneration and comprehension capabilities.\nB. Prompt Engineering\nOne of the advantages of LLMs over traditional DL models\nis that it has good interactivity, which can be achieved by\nprocessing verbose raw data into prompts and further used for\nfine-tuning. A prompt is a corpus that is understandable by\nboth humans and LLMs, which is easy to be produced, such\nas colloquial phrases, sentences, and paragraphs. Therefore,\nwe can quickly get suggestions on playing Poker games by\ninputting simple words with part of important information of\na Poker game into LLMs. We will introduce how we make\nraw data into prompts, and filter out high-quality prompts to\nensure that our model is able to give high-revenue or low-loss\ninsights in real-time Poker games.\n1) Prompt Selection: Based on the game log presented in\nFig. 5, we can observe that the information contained in each\ngame is diversified, and it is difficult for a pre-trained LLM to\ndistinguish which information to focus on. Thus, we construct\nprompts with rich information and then select high-quality\nones for training.\nFirstly, we conduct searching for all logs to filter out\n“SHOWDOWN” stages that with private cards publicated.\nCases without a specific winning or losing hand were sub-\nsequently excluded, because the lack of hand strength infor-\nmation makes it difficult for LLMs to learn optimal strategies\nin an observable situation.\nThen, to get high-quality dataset, we adopt win rate for\nselection of high-level players. Win rate is a standard metric in\nthe Poker AI community. We firstly counted the total number\nof games played by each player in the obtained logs. Next, we\ncalculate overall revenue and converting it to a multiple of big\nblinds. After that, we count each player’s win rate in milli-big-\nblinds per hand (mbb\/h) as a sorting criterion for descending\nrankings. To mitigate bias resulting from a limited number of\ngames, we exclude players whose number of games is under a\nthreshold. Then, we select a group of top players as our high-\nquality dataset, in which the winning games have significant\ninstructive impacts on our model. Note that, it is fine for our\nmodel to learn player actions in lost games. Intuitively, the way\nof high-level player losing games also has reference value. To\nsome extent, it can help the model learn how to lose less.\nOn the other hand, we also collect data of the worst players\nas negative examples. Through reward modeling which will\nbe mentioned in Section. IV-C, the actions and ideas of the\nlow-level players can be learned and avoided by our model.\n2) Prompt Construction: We choose zero-shot learning for\nprompt construction, which simply feeds the text into the\nmodel and ask for results. For practicality, we discard few-\nshot prompting. The purpose of presenting few-shot exam-\nples in the prompt is to describe the task instruction to\nthe model in the form of demonstrations. Though adding\nquestion-answering (QA) samples to prompts may greatly\nimprove the response quality of LLMs [26], in real Poker\ngames, the player’s decision-making time is generally about 15\nseconds, causing that players are unlikely to be able to input\na large amount of QA samples or game information. Thus,\nfor enhancement, we just add fixed instructions in prompts to\nexplain our intent to the model. During interaction, prompts\nare systematically constructed mainly using the information in\nred in in Fig. 5. Besides, few-shot learning can be expensive\nin terms of token usage and violate the length limitation,\nwhile rich-information instruction improves the model to be\nmore aligned with human intention and greatly reduces the\ncost of communication. Thus, directly giving instructions is a\npromising approach. On the other hand, due to the fine-tuning\nwe conduct, explained in Section IV-C, few-shot learning is\nnot necessary.\nIn terms of reducing tokens inputted, we roughly divide\nuseful information into constant information which is fixed at\nthe beginning of one game, and dynamic information updated\nin each decision round.\nFurthermore,\nmultiple\nprompt\nengineering\ntechniques\nadopted are outlined as follows, which are proved to be\neffective in LLM training [27]:\n• Role play: We give a role to our model and add cor-\nresponding text into our prompt, such as “You are a\nprofessional gambler”.\n• Value discretization: To reduce the learning difficulty,\nwe discretize the money needed to be determined by our\nmodel through rounding up the values of money paid\nin each stage to an integer multiple of the big blind,\ntransforming the regression task into a classification task.\nAn textual instance of our prompt is shown in Fig. 6. It\nis worth noting that in real games, sometimes opponents may\ndirectly show 1 or 2 privates cards consider from some psy-\nchological warfare perspectives. In that case, we can replace\nthe “**” with the displayed cards. An utilization example of\nthe engineered prompt can be found in Section V, shown as\nFig. 12.\nThe information received by our model encompasses all\ncore information and additional information mentioned in\nSection IV-A2. This array of data enables our model to\ncomprehensively understand the prevailing situation.\nC. Training Process\nWe utilize the prepared dataset to train our model using\nDeepSpeed-Chat [28]. DeepSpeed-Chat is a system framework\nthat enables an end-to-end training experience for ChatGPT-\nlike models. The main steps consist of: (1) Supervised fine-\ntune a pre-trained LLM on small datasets to provide specific\nprior knowledge to this model; (2) Train a reward model\nPrompt Instance\nYou are an experienced gambler. Now you need to assist\nme to make decisions in Texas Hold’em games. You have\nbeen provided with a series of observable information:\nPlayer amount: [6], Currency: USD, Blind value: [0.02\/0.05],\nOrder: [‘2’, ‘3’, ‘5’, ‘6’, ‘7’, ‘9’], Seat 2 is small blind.\nMy cards: [‘Th’, ‘Ah’], the characteristics of my cards: [“suit”,\n“high”, “close”], My seat: [Seat 2]\nStage: “PREFLOP”, Public cards: [‘**’ ‘**’ ‘**’ ‘**’ ‘**’]\nMy rank: [“High”], Money: [3.92], Action: []\nSeat 3: [‘**’, ‘**’], Money: [2.33], Action: [], Discard: [False]\nSeat 5: [‘**’, ‘**’], Money: [5.54], Action: [], Discard: [False]\nSeat 6: [‘**’, ‘**’], Money: [3.75], Action: [], Discard: [False]\nSeat 7: [‘**’, ‘**’], Money: [4.22], Action: [], Discard: [False]\nSeat 9: [‘**’, ‘**’], Money: [1.47], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.17]\nThe actions can be: [“fold”, “raise”, “call”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.92}.\nFig. 6: An instance of our prompt.\nusing data separated by quality. (3) Apply RLHF to make\nthe fine-tuned LLM in step (1) able to understand various\nhuman text assisted by the reward model in step (2). The pre-\ntrained model we choose is the OPT-1.3B from Facebook,\nwhich performs similarly to GPT-3 [8], making the goal of\ntraining domain-specific expert-level AI assistant possible. The\nreason for choosing this model is its small size and good\ncomprehension in human dialogues.\nSupervised fine-tuning is a necessary approach because re-\ntraining a LLM which has been already pre-trained consumes\na substantial amount of computational resources. On the other\nhand, involving domain-specific expert model training neces-\nsitates not large-quantity but high-quality data that contains\ncomprehensive details of the designated task. Particularly in\nthe supervised fine-tuning phase, a modest number ranging\nfrom tens to hundreds of high-quality data proves sufficient\nfor achieving satisfactory results in model refinement [15].\nThus, it is promising to fine-tune OPT-1.3B on our collected\ndataset.\nThe second step is to train a reward model, which is crucial\nfor the RLHF process. We need to train a model to predict\nthe reward signal, which is used to guide the fine-tuning of\nthe final dialogue model. For convenience, we directly use the\nwin rates calculated before as the labels of our reward model.\nThen, we input prompt-action pairs into the reward model to\ngenerate a score indicating which action is better. The model\nis also trained using supervised learning.\nFinally, RLHF fine-tuning is applied to train the supervised\nfine-tuned model based on the reward model in the second\nstep. Specifically, fine-tuning approach used here is proximal\npolicy optimization (PPO) [29], a popular and strong RL\nalgorithm, whose reward is generated by the reward model.\nFor updating, RLHF involves generating new answers with the\ncurrent model, ranking these answers using the reward model,\nand then updating the model’s parameters to favor answers\nwith higher reward. This iterative process improves our model\nfrom the perspective of robustness to various human-like\ninput, and generalizability to extensive game scale and varying\nsituations. In the end, we obtain the PokerGPT for producing\nhuman-like and engaging answers to instruct Poker games.\nV. EXPERIMENT\nA. Fundamental Setup\nWe firstly prepare our dataset based on data of over 1 million\ngames. After processing mentioned in Section IV, we get high\nquality data of about 120 thousand games. Then, we divide\n90% of them into training set and the remaining 10% into test\nset. We select Microsoft’s DeepSpeed-chat framework [28]\nfor its comprehensive functionalities. This framework also\nefficiently manages the distribution of model parameters across\nindividual GPUs during training. We fine-tune an open-source\nLLM 1.3 billion parameters on HuggingFace, named Face-\nbook’s OPT-1.3B model, known for its comparable perfor-\nmance to GPT-3 [8]. We run our experiments on a single\nNVIDIA GEFORCE 3090. The hyperparameter we use can be\nrefered to DeepSpeed-Chat [28], while we change mini batch\nsize to 4 due to the limitation of our computational device.\nB. Experimental Setup\n1) Performance Comparison: For performance comparison,\nwe make PokerGPT play against Slumbot [30] for 10000\nhands, and compare its results with 2 latest models [13], [14].\n2) Ablation Studies: We aim to study if useful information\nfiltering and prompt engineering can improve the performance\nof PokerGPT. Thus, we input raw game data and filtered\ndata into LLMs for fine-tuning, and compare the win rate.\nFurthermore, as mentioned in Section IV, we use win rate\nfor evaluating the level of players included in our dataset. To\nexplore if levels of players have impact on the fine-tuning\nresults, we sample data from 50000 games based on different\nwin rate thresholds to construct our training set for fine-tuning.\nAll datasets for fine-tuning are demonstrated in Table I. As for\ntest set, we use Slumbot for win rate and randomly sample\n1000 games of winners among the complete dataset for other\nmetrics introduced as follows.\nTABLE I: Datasets under different setup for fine-tuning.\nDataset\nTechnique\nWin Rate Threshold\nAmount\nI\nRaw\n-\n1 million\nII\nInfo filtering\n-\n1 million\nIII\nWin rate sorting\n> 1500 mbb\/h\n50000\nIV\nWin rate sorting\n600 ∼1200 mbb\/h\n50000\nV\nWin rate sorting\n0 ∼500 mbb\/h\n50000\nVI\nWin rate sorting\n< 0 mbb\/h\n50000\nTo evaluate the performance of supervised fine-tuning on\ndatasets of different quality, we regard determining actions,\nincluding “bet”, “fold”, “raise”, and “check”, as a multi-label\nclassification task. If our model gives a action different from\nthe real action in test set, we get a “False”. We take macro F1\nas the metric for this task [31]. The macro F1 score is a type\nof F1 score that gives equal weight to each class in multi-label\nclassification, regardless of the class distribution. Assume we\nhave N classes, the macro F1 can be denoted as:\nmacro-F1 = 1\nN\nN\nX\ni=1\nF1i,\n(1)\nF1i = 2 × precision × recall\nprecision + recall\n,\n(2)\nwhere F1i is the F1 score of the ith class. Since we have 5\nactions for Poker games, N equals to 5 here. Note that, for\n50000 games, each action which is regarded as classification\nlabel may have different amount of labels from 50000.\nOn the other hand, we study on value differences of value-\nrelated actions from all correct predictions, such as that our\nmodel says “bet $1.5”, while the real action in test set is\n“bet $2.0”. Since there exist multiple currencies and big\nblind values in our data, we unify currency to US dollar to\nrepresent all money using big blind. As mentioned before,\nwe discretize values of “bet” and “raise” into multiple of big\nblind. Specifically, here we adopt {0, 1, 3, 6, 10, 20, 50, 100,\nall-in} as the multiples of big blind for decision, while the\nvalues exceeding user’s owned money will be excluded. As we\nregard value prediction as a multi-label classification task, we\ncompare the frequency of each multiple chosen to evaluate the\naccuracy of predicted values, as well as using mean squared\nerror (MSE) for a general evaluation.\nBesides, perplexity in Natural Language Processing (NLP)\nis a measure of how well a probability model predicts a\nsample. Assume we have a sequence W with N words, a\nperplexity is formulated as:\nperplexity(W) = P(w1, w2, ..., wN)−1\nN ,\n(3)\nwhere W = (w1, w2, ..., wN) is the sequence of words, and P\nis the probability. It quantifies the uncertainty of a model by\ncalculating the inverse probability of the test set, normalized\nby the number of words. In the context of language models,\na lower perplexity generally indicates the model is better at\npredicting the next word in a sentence.\n3) Multi-Player Evaluation:\nWe evaluate PokerGPT in\nmulti-player games using an open-source Poker simulation 3.\nFor setup consistency, all bots in multi-player experiments\nare based on a mandatory method that calculates equity.\nTo study the strategy variation of PokerGPT during player\namount changes, we design 2 metrics, action score and average\ninvestment, to represent the occurring frequency of each action\nand the money used in each game, relatively:\n• Action score: Since some actions can occur multiple\ntimes in one game, such as “check”, “raise”, “bet”, and\n“call”, while the others can only occur once, such as\n“all-in” and “fold”, the length of one game should be\nconsidered. Thus, as a game can have up to 4 stages,\nwe calculate action score for each action by dividing the\nnumber of times this action occurs in a game by the stage\n3https:\/\/github.com\/dickreuter\/neuron Poker\nTABLE II: Performance comparison against the latest 2 models.\nModel\nWin Rate (mbb\/h)\nTraining Time (h)\nResponse Speed (s)\nPlayer Amount\nParameter Amount\nMain Tech\nYear\nReBel [13]\n45±5\n-\n-\n2\n-\nDRL+CFR\n2020\nAlphaHoldem [14]\n111±16\n580\n0.017\n2\n8.6 million\nDRL\n2022\nPokerGPT\n158±49\n9.5\n5.4\n2 or more\n1.3 billion\nLLM\n2023\nnumber of this game. For example, if a player do “check”\nfor 3 times and “fold” in “TURN” stage, the action score\nfor “check” is 1, and the action score for “fold” is 0.33.\nFinally, we divide action scores by the amount of games,\nto make action scores ranged from 0 to 1.\n• Average investment: In Poker games, more money usage\nmeans both higher revenue and higher loss. By observing\nthe amount of average investment, we can know the style\nof playing more clearly. To get the average investment,\nwe calculate the mean of total money used by our model\nin each game.\nBy analyzing action scores and average investment, we can get\nmore detailed insights of our model’s strategies for different\nplayer amount.\n4) Analysis on Interaction: We show a case of PokerGPT\ninteraction with multiple players in processed dataset. The aim\nis to evaluate the general capability of our model through\nanalyzing the answers of different questions.\nC. Comparison with Previous Works\nFirstly, we compare our performance against Slumbot with\n2 novel works, recording multiple metrics in Table II. We can\nsee that our model makes progress in win rate. Compared to\nRebel (45+5) and Alphaholdem (111+16) against Slumbot, our\nmodel comes out on top with a win rate of 158+49, measured\nin mbb\/h, showing the superiority of our model. However,\nsince the standard deviation of our model is higher than the\nother 2 models, fluctuation of our performance is more violent,\nsuggesting the larger variation of the LLM’s answers led\nby the diversified nature of generative models, which should\nbe further optimized. Furthermore, our model completes the\ntraining in just 9.5 GPU hours (3.5 for supervised fine-tuning,\n1.5 for reward model training, and 4.5 for RLHF), compared to\n580 GPU hours for Alphaholdem, showing the improvement\non training efficiency. Thanks to the adaptability of LLMs,\nempowering LLMs with specific prior knowledge through fine-\ntuning on a relatively small scale of data is effective. Last but\nnot least, our model is capable of providing action instructions\nfor arbitrary number of players, while the other 2 models\nare limited to playing one-on-one situations. Even without\ntransferring that is needed for the other 2 models, LLM-based\nmodel can be directly generalized to various situations, saving\na lot of computational resources. On the other hand, though\nPokerGPT owns a much larger amount of parameters and a\nslower response speed than Alphaholdem, both metrics are\nacceptable in real games, and much better than previous CFR-\nbased works. Additionally, we illustrate the staged revenue\ndistributions of high-level players in real games and PokerGPT\nagainst Slumbot in Fig. 7. Compared with human players,\nPokerGPT tends to keep a more stable win rate by losing less\nin later stages, while humans are more likely to take a risk in\ngambling in later stages, since the later the stage is, both the\nrevenue and the risk can be higher.\nPREFLOP\nFLOP\nTURN\nRIVER\nStage\n0\n20\n40\n60\n80\nRevenue (bb)\nWin_Real\nLose_Real\nWin_LLM\nLose_LLM\nFig. 7: The distribution of revenue among 1000 games with\nunit of big blind. The real results are counted from the\ncollected dataset, while the results of PokerGPT are from the\ngames against Slumbot.\nD. Ablation Results\nTABLE III: Ablation on different dataset setup.\nDataset\nWin Rate (mbb\/h)\nMacro F1\nMSE\nPerplexity\nI\n-376±215\n31.34\n115.41\n4.25\nII\n229±137\n62.77\n36.77\n2.71\nIII\n1221±54\n77.63\n0.21\n1.03\nIV\n1034±52\n72.22\n0.95\n1.04\nV\n608±59\n68.15\n8.28\n1.05\nVI\n-574±76\n59.69\n177.26\n1.09\nIn ablation experiments, we firstly evaluate the effectiveness\nof each technique used in our model. As shown in Table III,\nby only fine-tuning raw data, Dataset I shows the worst\nmacro F1 score and perplexity, because it is difficult for the\nLLM to learn effectively from a bunch of unformatted data,\nwhose information is in chaos. Thus, the LLM can not even\nmake actions valid for Poker games. Dataset II shows much\nbetter performance than Dataset I, indicating the importance\nof information filtering applied in our model. After selecting\nplayers with different win rate for fine-tuning, we can see\nthere is a trend that the holistic performance of our model\ncheck call fold bet raise\ncheck\ncall\nfold\nbet\nraise\nTrue Labels in Real Data\n1335 34\n189\n15\n3\n25 1776 320\n3\n6\n46\n56\n437\n18\n1\n12\n2\n34\n269\n12\n22\n23\n25\n1\n112\nDataset III\ncheck call fold bet raise\ncheck\ncall\nfold\nbet\nraise\n1307 47\n201\n19\n2\n32 1731 356\n5\n6\n58\n88\n376\n31\n5\n19\n3\n64\n229\n14\n23\n26\n30\n0\n104\nDataset IV\n0\n500\n1000\n1500\ncheck call fold bet raise\nLLM4MPNLTH Output\ncheck\ncall\nfold\nbet\nraise\nTrue Labels in Real Data\n1287 44\n215\n24\n6\n29 1667 391\n26\n17\n59\n97\n344\n41\n17\n20\n4\n71\n218\n16\n26\n22\n32\n1\n102\nDataset V\ncheck call fold bet raise\nLLM4MPNLTH Output\ncheck\ncall\nfold\nbet\nraise\n1212 38\n223\n76\n27\n31 1441 598\n16\n44\n66\n129 305\n21\n37\n51\n4\n82\n143\n49\n9\n15\n24\n1\n134\nDataset VI\n0\n500\n1000\n1500\nFig. 8: Confusion matrix of test results outputted by Pok-\nerGPT fine-tuned using Dataset III to VI.\nis positively related to win rate, where Dataset III has the\nbest performance while Dataset VI is the worst. According to\nmacro F1 score and MSE, fine-tuning on worse dataset leads\nto not only more incorrect action choices, but also irrational\ninvestment. Interestingly, Dataset VI has a similar perplexity\nto Dataset III to V while performing much worse, indicating it\nis firm about its bad action choice. This result emphasizes the\nimportance of data quality. In conclusion, all the techniques\nadopted in this work show effectiveness in different extent, in\nwhich prompt selection based on players’ win rate presents\nthe strongest effect.\nTo check the training bias, we show the confusion matrix\nof classification results of Dataset III to VI in Fig. 8. The test\nset contains 1000 games with 1576 “check”s, 2130 “call”s,\n558 “fold”s, 329 “bet”s, and 183 “raise”s. We can see that the\nmodel of Dataset III has better prediction on “check”, “call”,\n“fold”, and “bet” than the model of Dataset VI does, indicating\nthat our model can learn strategies more similar to winners’\noperations from data of high-level players. On the other hand,\nthe reason for the higher accuracy of “raise” of Dataset VI\ncan be that the players losing a lot have a stronger “raise”\ninclination which influence the action style of our model.\nBesides, an interesting appearance is that both the 2 models\ntend to give more “fold” than other actions. This could be\nattributed to that the LLM has the basic concept of game\ngoal the value judgment of Poker, that is, if it is confusing\nto choose an action, the model will choose “fold” in order to\nreduce investment. Additionally, compared to Dataset III, the\noutputs of Dataset VI shows higher ratio of choosing “raise”,\nleading to worse macro F1 score, reaffirming the importance\nof data quality for LLM fine-tuning.\nMoreover, we record the investment distribution of the truth\nand predicted investment from models fine-tuned on Dataset\nTruth\nIII\nIV\nV\nVI\nDataset\n0\n50\n100\nInvestment value (bb)\nMean\nFig. 9: The distribution of investment values among 1000\ngames, whose unit is big blind. The 2 dashed lines are the\nmean with standard deviation.\nIII to VI, shown in Fig. 9. In this figure, the mean of investment\nincreases and the distribution diversifies with the data quality\ndegrades. Generally, winners intend to invest less than 10 bb\nfor an easy win, while players who win less or lose more\nincline to invest more money. The results imply that the\nstrategies adopted by high-level players are more conservative,\nwhich is similar to winners in the truth. On the other hand,\nthe similarity is, the investment inclination under all cases is\nto focus on relatively small values, and invest higher values\nwith lower frequency, indicating that our model has learned\nthe common sense of investment risk in games. In terms of the\ndistribution of Dataset VI, the mean of the model’s investment\neven reaches about 10, undertaking much higher risk.\nPREFLOP\nFLOP\nTURN\nRIVER\nStage\n0\n200\n400\n600\n800\n1000\nAmount of Actions\ncheck_III\ncheck_VI\ncall_III\ncall_VI\nfold_III\nfold_VI\nbet_III\nbet_VI\nraise_III\nraise_VI\nFig. 10: The predicted actions of 1000 games from LLMs\nfine-tuned on Dataset III and Dataset VI. The dashed bars\nrepresent the real actions.\nAdditionally, we record staged actions of our model fine-\ntuned on Dataset III and Dataset VI. As shown in 10, the\nresults of PokerGPT on Dataset III always be more likely to\n“fold” in all stages, indicating the model’s style is conserva-\ntive. In contrast, PokerGPT on Dataset VI prone to “raise” and\n“bet”, and rarely “fold”, showing its aggressiveness. These 2\nphenomenons can explain that players with higher win rates\nare usually more cautious that those who lose more. Both\nmodels tend to “call” in later stages, implying that “call” is a\nmore general choice in some extent.\nAs a conclusion, the above ablation experiments validate\nthe effectiveness of our approaches.\nE. Multi-Player Evaluation\nTo evaluate the performance of PokerGPT when dealing\nwith multiple players, we make it play 1000 games for each\nplayer amount. The Fig. 11(a) presents the varying tendency of\n2 key metrics of our model when players’ number increases.\nThe trend of win rate changes more significantly during player\namount 2 to 5, then shows a slower declination during player\namount from 6 to 9, and finally fluctuations with a small\namplitude appears during player amount 10 to 15. When\nplaying 2-player games, our model gets a higher win rate\nagainst a rule-based bot which considers equity, indicating the\ncapability of handling the easiest Poker games. Since situations\nbecome more complex and the win rate declines when the\nplayer amount rises, our model gains less but still positive\nrevenue when competing with more opponents, showing the\nadvantages of our model in multi-player games. Meanwhile,\nthe gradual reduction of the shaded part implies that the\nfluctuation of the win rate decreases.\n5\n10\n15\nPlayer Amount\n250\n500\n750\nWin Rate (mbb\/h)\n5.5\n6.0\n6.5\nResponse Speed (s)\n(a)\n5\n10\n15\nPlayer Amount\n0.0\n0.2\n0.4\n0.6\n0.8\nAction Score\nCall\nCheck\nFold\nBet\nRaise\nAll-in\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nAverage Invest (bb)\n(b)\nFig. 11: (a) The trends of win rate and response speed when\nplayer amount changes. The dashed lines are the standard de-\nviations. (b) Strategy variation during player amount changes.\nMoreover, the response time rises about linearly, while\nthe win rate declines drastically at first and gradually goes\nmoderate. This phenomenon validates the good scalability of\nour work. Different from CFR, LLMs intend not to fully\nexplore all possible scenarios. Instead, LLMs directly look\nfor connection between inputs and outputs. As the number\nof players increases, the amount of input information shows\nroughly linear growth, and the number of possible situations\ninvolved in each round of the game consequently goes up. As a\nresult, the linear time increase shows the efficiency of end-to-\nend learning on LLMs. In conclusion, all these phenomenons\ndemonstrate the advantages of our model in Poker games, and\nextensive potential in IIGs.\nFurthermore, to study the variation of PokerGPT’s strategy\nwhen facing multiple players, we calculate the action score for\neach action and the average investment, shown in Fig. 11(b).\nAccording to action scores, we can observe each action and\nget insights separately:\n• “Call”: We can see that the score of the “call” strategy\nincreases with the player amount, suggesting that the\nPokerGPT prefers to be more conservative when facing\nmore opponents.\n• “Check”: The score of “check” increases till 6-player\ngames, and then deceases into a range. Since “check”\nis not only a compromising behavior but also a bait for\naggressive opponents, players can choose “check” in any\nsituation, especially in games with small player amount.\nHowever, when the scale of games becomes larger than 10\nplayers, bait tactics become less effective. Thus, “check”\ncan usually be regarded as a compromise, and chosen in\na relatively stable frequency.\n• “Fold”: As “fold” score increases a lot when player\namount becomes larger than 6, 2 explanations can be\ndrew. When there are more players, it is complex to\nforecast the rank strength among dozens of hands, leading\nto a high rate of “fold”. Also, “fold” score can be higher\nwith the game length shortening, since it is reasonable\nfor bots to “fold” if possibility of stronger rank existing\nincreases, leading to a quick end of one game.\n• “Bet” and “Raise”: The scores of “bet” and “raise”\ndecrease with the number of players. This implies that the\nmodel intends to avoid risk when facing more players.\n• “All-in”: Actually, “all-in” is a special case of “bet” or\n“raise”. The score of the “all-in” is very low and even\ntends to 0 with more opponents. This reflects that the\nmodel rarely chooses this action when player amount\nincreases, as the risk of “all-in” is higher.\nIn summary, actions suggested by our model tend to be more\ncautious and conservative as the number of players increases,\nwhich are generally consistent with real games.\nF. Analysis of Interaction\nIn Fig. 12, we show the interaction with PokerGPT during\na Poker game. In Q1, we ask PokerGPT to give a direct\naction recommendation and get “call” as feedback. Then, in\nQ2, when we request it to be more aggressive, it suggests\nto make a “raise” to 0.8. It shows that the human language\ncomprehension of LLMs help PokerGPT adjust the strategy\naccording to the user’s request. In “FLOP” stage with 3 public\ncards exhibited, PokerGPT successively gives different an-\nswers to Q3 and Q4, from “check” to “all-in”, suggesting that\nPokerGPT has the understanding of card strengths. Besides, in\nQ6, we ask about Seat 9’s rank, with the answer that Seat 9\npossibly has a pair or a flush since this player likes to “raise”\nand “bet”, suggesting that PokerGPT has the awareness of\nan opponent’s strength based on his or her actions. However,\nfrom the answer of Q7 and Q9, PokerGPT shows its shortage\non guessing cards, probably due to the large quantities of\ncard combinations. Similarly, in Q9, PokerGPT has difficulty\nof giving a winning probability, since we do not use any\nprobability as a part of the prompt, while LLMs are not\nproficient in math, neither.\nVI. CONCLUSION AND FUTURE WORK\nInspired by the prosperity and prominence of LLMs, our\nstudy explores the applicability of LLMs for playing Poker.\nBy conducting prompt engineering on data of real games and\nfine-tuning a lightweight pre-trained LLM based on RLHF, we\nPokerGPT\nSystem (constant information): You are an experienced\ngambler. Now you need to assist me to make decisions\nin Texas Hold’em games. You have been provided with\na series of observable information:\nPlayer amount: [6], Currency: USD, Blind value: [0.02\/0.05],\nOrder: [‘2’, ‘3’, ‘5’, ‘6’, ‘7’, ‘9’], Seat 2 is small blind.\nMy cards: [‘Th’, ‘Ah’], the characteristics of my cards: [“suit”,\n“high”, “close”], My seat: [Seat 2]\nDynamic information:\nStage: “PREFLOP”, Public cards: [‘**’ ‘**’ ‘**’ ‘**’ ‘**’]\nMy rank: [“High”], Money: [3.92], Action: []\nSeat 3: [‘**’, ‘**’], Money: [2.33], Action: [], Discard: [False]\nSeat 5: [‘**’, ‘**’], Money: [5.54], Action: [“fold”], Discard:\n[True]\nSeat 6: [‘**’, ‘**’], Money: [3.75], Action: [“fold”], Discard:\n[True]\nSeat 7: [‘**’, ‘**’], Money: [4.22], Action: [“fold”], Discard:\n[True]\nSeat 9: [‘**’, ‘**’], Money: [1.47], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.17]\nThe actions can be: [“fold”, “raise”, “call”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.92}.\nUser:\nQ1:What should I do?\nPokerGPT: You should call.\nQ2: Please be aggressive.\nPokerGPT: You should raise to 0.5.\nInformation updates:\nStage: “FLOP”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘**’ ‘**’]\nMy rank: [“Flush”], Money: [3.82], Action: [“call”]\nSeat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”], Discard:\n[False]\n......\nSeat 9: [‘**’, ‘**’], Money: [1.37], Action: [“raises 0.05 to\n0.1”], Discard: [False]\nThe pot value is [0.3]\nThe actions can be: [“fold”, “check”, “bet”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.82}.\nUser:\nQ3:What should I do?\nPokerGPT: You should check.\nQ4: Should I all-in?\nPokerGPT: As you have a flush, all-in is fine.\nInformation updates:\nStage: “FLOP”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘**’ ‘**’]\nMy rank: [“Flush”], Money: [3.82], Action: [“call”, “check”]\nSeat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”, “check”],\nDiscard: [False]\nSeat 9: [‘**’, ‘**’], Money: [1.15], Action: [“raises 0.05 to\n0.1”, “bets 0.22”], Discard: [False] The pot value is [0.52]\n......\nThe actions can be: [“fold”, “call”, “raise”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.82}.\nUser:\nQ5:What should I do?\nPokerGPT: You should raise to 0.5.\nQ6: Please guess Seat 9’s rank.\nPokerGPT: Seat 9 likes to raise and bet. Therefore, he might\nbe three of a kind or flush.\nQ7: Please guess Seat 9’s cards.\nPokerGPT: Seat 9 likes to raise and bet. Therefore, he might\nbe there of a kind or flush. But there is no evidence what cards\nSeat 9 has.\nInformation updates:\nStage: “TURN”, Public cards: [‘7h’ ‘4h’ ‘2h’ ‘Ks’ ‘**’]\nMy rank: [“Flush”], Money: [3.6], Action: [“call”, “check”,\n“call”] Seat 3: [‘**’, ‘**’], Money: [2.23], Action: [“call”,\n“check”, “fold”], Discard: [True]\n......\nThe actions can be: [“fold”, “check”, “bet”]. What should I\ndo? If I choose to “bet” or “raise”, then how much? Choose a\nnumber from {(0, 0.05, 0.15, 0.3, 0.5, 1, 2.5, 3.6}.\nUser:\nQ8:What should I do?\nPokerGPT: You should bet 1.\nQ9: Please tell me my winning probability.\nPokerGPT: Sorry, I can’t calculate the probability. However,\nas you have flush, you are very likely to win.\nInformation updates:\n... My Action: [“call”, “check”, “call”, “bet 0.9”]\n......\nSeat 9: [‘**’, ‘**’], Money: [1.37], Action: [“raises 0.05 to\n0.1”, “bets 0.22”, “fold”], Discard: [True]\nThe pot value is [1.42]\nPlayer 2 wins $1.42 with “Flush”.\nFig. 12: The question-answering instance of PokerGPT.\nhave successfully developed an end-to-end solver for Poker\nwith arbitrary amount of players, which is also convenient for\ninteraction. The experimental results are notably satisfactory,\nshowing a great advantage in training cost and inference speed\namong previous works, while obtaining a competitive win rate.\nPokerGPT addresses a series of challenges encountered by\nprevious Poker AI, substantiating the considerable potential\nof LLMs in tackling IIGs.\nIn subsequent research directions, we aim to further improve\nthe capability of PokerGPT. To empower our model with\nexplainability on decisions, various data should be prepared,\nincluding explanations on actions in each round written by\nhumans, probabilities of each action which can be outputted\nby CFR models, multiple candidate decisions, etc.\nREFERENCES\n[1] D. A. Waterman, “Generalization learning techniques for automating\nthe learning of heuristics,” Artificial Intelligence, vol. 1, no. 1-2, pp.\n121–170, 1970.\n[2] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis`y, D. Morrill, N. Bard,\nT. Davis, K. Waugh, M. Johanson, and M. Bowling, “Deepstack: Expert-\nlevel artificial intelligence in heads-up no-limit poker,” Science, vol. 356,\nno. 6337, pp. 508–513, 2017.\n[3] N. Brown and T. Sandholm, “Superhuman ai for heads-up no-limit\npoker: Libratus beats top professionals,” Science, vol. 359, no. 6374,\npp. 418–424, 2018.\n[4] N. Brown, T. Sandholm, and B. Amos, “Depth-limited solving for\nimperfect-information games,” Advances in neural information process-\ning systems, vol. 31, 2018.\n[5] N. Brown and T. Sandholm, “Solving imperfect-information games via\ndiscounted regret minimization,” in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 33, no. 01, 2019, pp. 1829–1836.\n[6] ——, “Superhuman ai for multiplayer poker,” Science, vol. 365, no.\n6456, pp. 885–890, 2019.\n[7] M. Bowling, N. Burch, M. Johanson, and O. Tammelin, “Heads-up limit\nhold’em poker is solved,” Science, vol. 347, no. 6218, pp. 145–149,\n2015.\n[8] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\nM. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained transformer\nlanguage models,” arXiv preprint arXiv:2205.01068, 2022.\n[9] J. Rubin and I. Watson, “Computer poker: A review,” Artificial intelli-\ngence, vol. 175, no. 5-6, pp. 958–987, 2011.\n[10] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione, “Regret\nminimization in games with incomplete information,” Advances in\nneural information processing systems, vol. 20, 2007.\n[11] N. Brown and T. Sandholm, “Safe and nested subgame solving for\nimperfect-information games,” Advances in neural information process-\ning systems, vol. 30, 2017.\n[12] N. Brown, A. Lerer, S. Gross, and T. Sandholm, “Deep counterfactual\nregret minimization,” in International conference on machine learning.\nPMLR, 2019, pp. 793–802.\n[13] N. Brown, A. Bakhtin, A. Lerer, and Q. Gong, “Combining deep\nreinforcement learning and search for imperfect-information games,” Ad-\nvances in Neural Information Processing Systems, vol. 33, pp. 17 057–\n17 069, 2020.\n[14] E. Zhao, R. Yan, J. Li, K. Li, and J. Xing, “Alphaholdem: High-\nperformance artificial intelligence for heads-up no-limit poker via end-\nto-end reinforcement learning,” in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 36, no. 4, 2022, pp. 4689–4697.\n[15] OpenAI, “Gpt-4 technical report,” 2023.\n[16] L. Chen, B. Li, S. Shen, J. Yang, C. Li, K. Keutzer, T. Darrell, and Z. Liu,\n“Language models are visual reasoning coordinators,” in ICLR 2023\nWorkshop on Mathematical and Empirical Understanding of Foundation\nModels, 2023.\n[17] X. Zou, Z.-Y. Dou, J. Yang, Z. Gan, L. Li, C. Li, X. Dai, H. Behl,\nJ. Wang, L. Yuan et al., “Generalized decoding for pixel, image, and\nlanguage,” in Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, 2023, pp. 15 116–15 127.\n[18] K. Bi, L. Xie, H. Zhang, X. Chen, X. Gu, and Q. Tian, “Accurate\nmedium-range global weather forecasting with 3d neural networks,”\nNature, pp. 1–6, 2023.\n[19] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, “Otter: A\nmulti-modal model with in-context instruction tuning,” arXiv preprint\narXiv:2305.03726, 2023.\n[20] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso et al., “Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of\nlanguage models,” arXiv preprint arXiv:2206.04615, 2022.\n[21] S. V¨artinen, P. H¨am¨al¨ainen, and C. Guckelsberger, “Generating role-\nplaying game quests with gpt language models,” IEEE Transactions on\nGames, 2022.\n[22] J. Kelly, M. Mateas, and N. Wardrip-Fruin, “Towards computational\nsupport with language models for ttrpg game masters,” in Proceedings of\nthe 18th International Conference on the Foundations of Digital Games,\n2023, pp. 1–4.\n[23] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz,\n“Playing repeated games with large language models,” arXiv preprint\narXiv:2305.16867, 2023.\n[24] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\nD. Drain, S. Fort, D. Ganguli, T. Henighan et al., “Training a helpful and\nharmless assistant with reinforcement learning from human feedback,”\narXiv preprint arXiv:2204.05862, 2022.\n[25] J. Abramson, A. Ahuja, F. Carnevale, P. Georgiev, A. Goldin, A. Hung,\nJ. Landon, J. Lhotka, T. Lillicrap, A. Muldal et al., “Improving mul-\ntimodal interactive agents with reinforcement learning from human\nfeedback,” arXiv preprint arXiv:2211.11602, 2022.\n[26] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, “Calibrate\nbefore use: Improving few-shot performance of language models,” in\nInternational Conference on Machine Learning.\nPMLR, 2021, pp.\n12 697–12 706.\n[27] L.\nWeng,\n“Prompt\nengineering,”\nhttps:\/\/lilianweng.github.io\/posts\/\n2023-03-15-prompt-engineering\/, march 15, 2023.\n[28] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari, X. Wu, A. A.\nAwan, J. Rasley, M. Zhang, C. Li, C. Holmes et al., “Deepspeed-\nchat: Easy, fast and affordable rlhf training of chatgpt-like models at\nall scales,” arXiv preprint arXiv:2308.01320, 2023.\n[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,\n2017.\n[30] E. G. Jackson, “Slumbot nl: Solving large games with counterfactual\nregret minimization using sampling and distributed processing,” in Work-\nshops at the Twenty-Seventh AAAI Conference on Artificial Intelligence,\n2013.\n[31] Y. Yang, “An evaluation of statistical approaches to text categorization,”\nInformation retrieval, vol. 1, no. 1-2, pp. 69–90, 1999.\n\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | PokerGPT：基于大型语言模型的轻量级多玩家德州扑克解决方案\n\n## 📌 背景痛点\/本文动机\n德州扑克作为一种典型的非完美信息游戏（IIG），一直是人工智能研究的重要目标。然而，现有的解决方案，如DeepStack和Libratus，主要依赖于反事实后悔最小化（CFR）算法，该算法在计算成本和扩展性方面存在局限性。CFR算法的计算成本高昂，难以应用于多玩家游戏，且难以从现有模型中学习并应用于其他现实世界应用。\n\n## 🚀 核心方法\n本文提出了PokerGPT，一种基于轻量级大型语言模型（LLM）的端到端德州扑克解决方案。PokerGPT通过以下创新点克服了现有方法的局限性：\n\n💡 创新点1：端到端学习框架\nPokerGPT采用端到端学习框架，避免了复杂的特征工程和中间步骤。它仅需要简单的文本信息即可生成决策建议，实现了人机交互的便捷性。\n\n💡 创新点2：轻量级LLM\nPokerGPT基于轻量级LLM，具有更少的参数和更快的推理速度，同时训练时间也更短，实现了资源的有效利用。\n\n💡 创新点3：高效的数据处理\nPokerGPT采用数据清洗和提示工程技术，将真实游戏数据转换为可理解的文本提示，并使用强化学习人类反馈技术进行微调，提高了模型性能。\n\n## 📈 实验结果\n实验结果表明，PokerGPT在胜率、模型大小、训练时间和响应速度等方面均优于现有方法。此外，PokerGPT能够处理任意数量的玩家，并展现出出色的灵活性和适应性。\n\n## 💬 可借鉴之处\nPokerGPT的成功表明，LLM在解决IIG方面具有巨大潜力。其端到端学习框架、轻量级模型和高效的数据处理技术为其他IIG研究提供了可借鉴的经验。此外，PokerGPT的交互式特性使其在现实世界应用中具有广阔前景。","llm_summary_res_status":200}
{"title":"Enhance Reasoning for Large Language Models in the Game Werewolf","authors":"Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, Haobo Fu","summary":"This paper presents an innovative framework that integrates Large Language\nModels (LLMs) with an external Thinker module to enhance the reasoning\ncapabilities of LLM-based agents. Unlike augmenting LLMs with prompt\nengineering, Thinker directly harnesses knowledge from databases and employs\nvarious optimization techniques. The framework forms a reasoning hierarchy\nwhere LLMs handle intuitive System-1 tasks such as natural language processing,\nwhile the Thinker focuses on cognitive System-2 tasks that require complex\nlogical analysis and domain-specific knowledge. Our framework is presented\nusing a 9-player Werewolf game that demands dual-system reasoning. We introduce\na communication protocol between LLMs and the Thinker, and train the Thinker\nusing data from 18800 human sessions and reinforcement learning. Experiments\ndemonstrate the framework's effectiveness in deductive reasoning, speech\ngeneration, and online game evaluation. Additionally, we fine-tune a 6B LLM to\nsurpass GPT4 when integrated with the Thinker. This paper also contributes the\nlargest dataset for social deduction games to date.","url":"http:\/\/arxiv.org\/abs\/2402.02330v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.02330v2","published":1707018430000,"comment":null,"pdf_text":"Enhance Reasoning for Large Language Models in the Game Werewolf\nShuang Wu 1 * Liwen Zhu 1 * Tao Yang 1 Shiwei Xu 1 Qiang Fu 1 Wei Yang 1 Haobo Fu 1\nAbstract\nThis paper presents an innovative framework that\nintegrates Large Language Models (LLMs) with\nan external Thinker module to enhance the reason-\ning capabilities of LLM-based agents. Unlike aug-\nmenting LLMs with prompt engineering, Thinker\ndirectly harnesses knowledge from databases and\nemploys various optimization techniques. The\nframework forms a reasoning hierarchy where\nLLMs handle intuitive System-1 tasks such as\nnatural language processing, while the Thinker\nfocuses on cognitive System-2 tasks that require\ncomplex logical analysis and domain-specific\nknowledge. Our framework is presented using a 9-\nplayer Werewolf game that demands dual-system\nreasoning. We introduce a communication proto-\ncol between LLMs and the Thinker, and train the\nThinker using data from 18,800 human sessions\nand reinforcement learning. Experiments demon-\nstrate the framework’s effectiveness in deductive\nreasoning, speech generation, and online game\nevaluation. Additionally, we fine-tune a 6B LLM\nto surpass GPT4 when integrated with the Thinker.\nThis paper also contributes the largest dataset\nhttps:\/\/github.com\/boluoweifenda\/werewolf for\nsocial deduction games to date.\n1. Introduction\nThe field of artificial intelligence has witnessed ground-\nbreaking advancements in recent years, with the develop-\nment of Large Language Models (LLMs) (Ouyang et al.,\n2022; OpenAI, 2023; Anil et al., 2023). Apart from their\nimpressive proficiency in natural language processing (NLP)\ntasks (Thoppilan et al., 2022; Zhang et al., 2023b), LLMs\nalso exhibit vast potential as a general problem solver in\nareas such as planning and decision-making (Huang et al.,\n2022), knowledge transfer and generalization (Anil et al.,\n2022) and multi-modal perception (Yin et al., 2023) due to\nthe rich world knowledge embedded in their training cor-\n*Equal contribution 1Tencent AI Lab, Shenzhen, China. Corre-\nspondence to: Shuang Wu <shawnswu@tencent.com>.\nPreliminary work. Under review.\npora. As a result, integrating LLMs as central controllers\nwith task agents to enable end-to-end solutions has become\none of the most promising research directions, leading to\nsignificant breakthroughs in domains such as tools and assis-\ntants (Schick et al., 2023; Ge et al., 2023), engineering (Ahn\net al., 2022), social simulations (Park et al., 2023), and\ngaming (Wang et al., 2023).\nLLM-based agents harness LLMs for their general-purpose\nreasoning abilities (Huang & Chang, 2022), which are pri-\nmarily enabled by prompt engineering methods such as in-\nformation profiling (Zhang et al., 2023a; Qian et al., 2023),\nstep-by-step task decomposition (Wei et al., 2022b; Zhou\net al., 2022), recursive prompting by feedback from the\nenvironment (Yao et al., 2022), human (Wu et al., 2022)\nand self-refinement (Madaan et al., 2023; Shinn et al.,\n2023). These methods thus eliminate the requirement for\ndomain-specific fine-tuning of LLMs. To augment their\ntask-specific competencies, researchers also adopt external\nmodules such as memory for storing and retrieving histor-\nical information (Lin et al., 2023; Zhong et al., 2023; Hu\net al., 2023), external tools (Schick et al., 2023), APIs (Qin\net al., 2023), knowledge bases (Lewis et al., 2020) and ex-\npert models (Yang et al., 2023; Ge et al., 2023).\nDespite these advancements, challenges persist in domain-\nspecific tasks, where LLM-based agents often serve primar-\nily as demonstrations rather than as practical solutions (Qian\net al., 2023; Liu et al., 2023b). First, while LLMs have\nemerged some basic reasoning capabilities, they require\nsufficient model scales (Kaplan et al., 2020) and substan-\ntial computational overheads, along with various aforemen-\ntioned techniques (Wei et al., 2022a). However, LLMs\nstruggle to achieve satisfactory performance when it comes\nto higher-level reasoning (Stechly et al., 2023; Dziri et al.,\n2023) and planning (Valmeekam et al., 2023; Bubeck et al.,\n2023) tasks. Second, most LLM-based agents avoid fine-\ntuning LLMs on task-specific data to preserve the model’s\ngenerality and prevent over-fitting. This strategy compli-\ncates the utilization of existing task-specific data and exper-\ntise, as well as the alignment of task scenarios with input-\noutput formats, data distribution, and human preferences.\nTo address the limitations of LLMs in complex reason-\ning, we distinctly separate reasoning tasks into two sys-\ntems based on the dual-process theory (Wason & Evans,\n1\narXiv:2402.02330v2  [cs.AI]  29 Mar 2024\nEnhance Reasoning for Large Language Model in the Game Werewolf\n1974) and propose an external Thinker module to enhance\nthe reasoning capabilities of LLM-based agents. In our\nframework, LLMs are responsible for System-1 reasoning\ntasks involving intuitive thinking, such as basic NLP in-\nteractions, common-sense, and symbolic reasoning, while\nthe Thinker handles System-2 reasoning that requires com-\nplex logical analysis, deep understanding of domain-specific\nknowledge, and strategic planning in specialized tasks. We\nestablish a communication protocol between LLMs and the\nThinker through language-based features and instructions.\nUnlike augmenting LLMs with cumbersome prompt engi-\nneering, the Thinker is directly optimized with knowledge\nfrom databases and trained using supervised and reinforce-\nment learning techniques, thus enhancing the LLM-agent’s\nperformance and domain alignment without compromising\nLLM’s generality.\nWe select the 9-player Werewolf game as a proving ground\nfor the proposed framework. Werewolf is a popular so-\ncial deduction game, current AI systems fall short when\ncompared to even moderate human players in this domain.\nSystem-1 reasoning tasks in Werewolf encompass natural\nlanguage understanding and generation of players’ state-\nments, as well as the adept use of game-specific jargon.\nMeanwhile, the hidden roles necessitate complex strate-\ngic thinking such as identity concealment, and sophisticated\ncommunication involving deception and disguise, which fall\nunder System-2 reasoning. This duality creates a significant\ngap between the players’ actual statements and their true\nintentions, making Werewolf an ideal testbed for assessing\nthe advanced reasoning capabilities of LLM agents.\nWe have collected 18,800 real human game sessions and\nanalysed the primary patterns behind human speeches. In-\nformed by these patterns, we design language-based fea-\ntures for speech understanding and instructions for speech\ngeneration. The Thinker module is optimized by imita-\ntion learning, reinforcement learning (RL) from fictitious\nself-play (Heinrich et al., 2015), and population-based train-\ning (Jaderberg et al., 2017), to output reasonable game ac-\ntions and LLM speech instructions. We compare our ap-\nproach with GPT3.5\/4 methods using Least-to-Most (LtM)\nprompting (Zhou et al., 2022) from three aspects: deductive\nreasoning and decision-making, human evaluation of speech\ngeneration, and online evaluation of a complete game. Ex-\nperiments demonstrate that the integration of an external\nThinker module substantially enhances the reasoning and\ngeneration capability of LLMs. Further, we fine-tune a\nsmaller LLM model (6B) (Du et al., 2021) to better align\nreal human speech styles and preferences, outperforming\nGPT4 in the majority of evaluative scenarios. Our primary\ncontributions include:\n• We propose an external Thinker module to enhance the\nreasoning capabilities of LLM agents and demonstrate it\nby a Werewolf AI that surpasses GPT4 in real gameplay.\n• We release a dataset1 of 18,800 Werewolf game sessions,\nwhich represents the largest known dataset for social de-\nduction games to date.\n2. Related Work\nEnhance Reasoning in LLMs.\nSeveral approaches by-\npass the need for intricate prompt engineering mentioned in\nthe Introduction. For instance, LLM+P (Liu et al., 2023a)\nemploys an external planner to address long-horizon robot\nplanning challenges. A different approach (Zhang et al.,\n2023a) heuristically designs a low-level planner to manage\nprimitive control actions. RAG (Lewis et al., 2020) com-\nbines pre-trained parametric-memory generation models\nwith a non-parametric memory to improve performance on\nknowledge-intensive tasks. Regarding the fine-tuning of\nLLMs, Galactica (Taylor et al., 2022) is trained on a scien-\ntific dataset that emphasizes detailed reasoning processes.\nWebGPT (Nakano et al., 2021) utilizes human feedback to\nfine-tune GPT-3, enabling it to answer long-form questions\nwithin a textual web-browsing context. Toolformer (Schick\net al., 2023) fine-tunes LLMs to use external tools in a\nself-supervised manner with human demonstrations. Ope-\nnAGI (Ge et al., 2023) implements RL from feedback in\nopen-ended tasks to refine the LLM’s planning strategy. Ci-\ncero (Bakhtin et al., 2022) fine-tunes LLMs to generate\ndialogue controlled by a strategic reasoning module in the\ngame Diplomacy. However, the control signals in Cicero\n(planned actions) are insufficient to convey the complex\nlanguage dynamics (both understanding and generation) in\nthe Werewolf game.\nAI for Social Deduction Games. DeepRole (Serrino et al.,\n2019) combines counterfactual regret minimization (CFR)\nwith deep value networks in the non-speech five-player\nAvalon game. Hidden Agenda (Kopparapu et al., 2022)\npresents a two-team, non-speech social deduction game in a\n2D environment. A system comprising three LLM-powered\ninterfaces is created (Zhu et al., 2023) to aid gameplay in\nDungeon Master. Regarding AI in werewolf games, boot-\nstrap aggregating and weighted ensemble learning have been\nemployed to improve voting strategies (Khan & Aranha,\n2022). (Brandizzi et al., 2021) proposes an RL framework\nto analyze the influence of diverse communication behav-\niors among agents. One Night Ultimate Werewolf (Eger &\nMartens, 2019) explores human responses to various delib-\neration strategies. In the five-player werewolf game, (Wang\n& Kaneko, 2018) builds a deep-Q network to decide whom\nto trust or kill. Deep Wolf (Shibata et al., 2023) fine-tunes a\nRoBERTa-like pretrained model with 48 game logs to con-\n1https:\/\/github.com\/boluoweifenda\/\nwerewolf\n2\nEnhance Reasoning for Large Language Model in the Game Werewolf\n(! Witch)  day 2, Player 4: \nPlayer 2, look at how your teammate\nPlayer 9's speech sounds like a werewolf.\nRight from the start, his speech seems to\nbe an attempt to save you. You claim\nyou're not afraid of being checked for\nshooting, so this round, I'll vote you out.\nTonight, I plan to poison Player 9. I\nstrongly recommend voting for Player 2,\nlet’s whether you are a Hunter. I think\nPlayers 2, 5, and 9 are three werewolves.\nSummary\nI think werewolves are Players 2, 5, and\n9. I suggest voting for Player 2 to check\nthe Hunter. I will poison Player 9 tonight.\nReasoning\nSpeech history\nSpeech\nListener\n0 6 0 3 6 0 0 0 6\n0 7 0 0 0 0 0 0 3\nGPT + LtM\nThinker\nRL & BC\nSpeech feature \nPresenter\n(\" Villager)  day 2, Player 6:\nHello, I am Player 6 and I am a\nVillager. I think Player 9 is a good\nguy.\nI\ndon't\nthink\nhe\nhas\nany\nwerewolf teammate. Both Player 3\nand 9 could be the hunter, but Player\n9 has a higher tolerance. Player 5\nhas revealed himself as a werewolf,\nPlayer 2 is likely to be one as well. I\nsuggest we vote Player 2 out first\nand\nthen\nsee\nwhat\nhappens\ntomorrow.\nLLM\nFilters\nIdentities：\n- Werewolves: 2,5\n- Hunter: 3,9\n- Villager: 6\nActions:\n- Vote out: 2\nSpeech\ninstruction\n0 6 4 0 6 5 0 0 4\n0 7 0 0 0 0 0 0 0\nGame state\nGame state\nGame state\nInstruction\nmodel\nPolicy\nmodel\nIdentity \nmodel\nIdentities：\n- Witch: 4\n- Werewolves: 2,5,9\nVote\nSkills\nLanguage \nfeature\nLanguage \nfeature\nLanguage \nfeature\nActions: \n- Vote out: 2 \n- Poison: 9\nSpeech history\nSpeech\nGPT  |  Finetune\nIdentities：\n- 2,5: Werewolves\n- 4: Witch or Werewolf\n- 6: Villager\n- 9: Hunter or Werewolf\nFigure 1: Overall processing framework and modules in the Werewolf implementation.\nstruct a value network given the current game state, human\nspeeches, and candidate actions. The seven-player version\nis explored with RL and LLMs in (Xu et al., 2023b;a). Our\napproach differs from previous studies in two fundamental\nways: First, we employ the Thinker to execute complex\nSystem-2 reasoning, in contrast to the reasoning approach\nof LLM in (Xu et al., 2023b), which generates candidate re-\nsults for the RL model to select and mitigate biases. Second,\nby collecting and leveraging authentic game sessions and\nspeech data, we aim for closer alignment with real-world\nscenarios and human interaction patterns.\n3. Methods\nWe introduce an innovative framework that synergizes\nLLMs with an external module for reasoning and decision-\nmaking, referred to as the Thinker, devised to augment\nLLM-based agents with sophisticated reasoning abilities.\nTo bridge the communication between Thinker and LLMs,\nwe introduce a protocol through structured features and\nprompt instructions. The framework is thus decomposed\ninto three processing components:\n• The Listener serves as the primary interface for natural\nlanguage understanding. It processes language inputs,\nengages in intuitive System-1 reasoning, and transforms\nthe information into structured language features that the\nThinker can interpret.\n• The Thinker functions as the cognitive core of the frame-\nwork. Utilizing language features provided by the Lis-\ntener, it specializes in System-2 reasoning tasks that re-\nquire deep logical analysis and domain-specific knowl-\nedge. The Thinker produces policies such as planning\nand actions, and generates strategic instructions for the\nPresenter.\n• The Presenter functions as the system’s articulator. It\ngenerates coherent and contextualized language output\nthat aligns with the current environment state, guided by\nthe strategic instructions from the Thinker. The Presenter\nensures that the generated language is logical, rational,\nconsistent, and free from hallucinations.\nTo demonstrate the effectiveness of our framework, we ap-\nply it to the complex social deduction game Werewolf. The\nremainder of this section will detail the implementation\nwithin the game environment, which necessitates deductive\nreasoning, speech understanding and generation, as illus-\ntrated in Figure 1.\n3.1. Data preparation\nWe collected data from the 9-player standard mode Were-\nwolf game hosted on the Fanlang platform2. The specific\nrules of the game are detailed in Appendix C. We recorded\nreal-time video in spectator mode for approximately 18,800\ngame sessions, which equates to around 7,000 hours of\ngameplay and 6,000 hours of speech. Furthermore, we\nenriched our dataset with a Werewolf domain-specific cor-\npus comprising nearly 1.4 million characters sourced from\nweb-crawled game strategies and OCR-processed Werewolf\nliterature. Each recorded session includes both the game\nstate data and the audio of players’ speeches. We captured\nexhaustive game state details, such as historical skills and\nvoting results, by utilizing an automated testing framework3.\nWe deployed the Paraformer (Gao et al., 2022) model for\nAutomatic Speech Recognition (ASR) of human speech\naudio. To improve recognition accuracy, especially for fre-\nquently used Werewolf-specific terms, we crafted a hot word\nlist from the domain corpus and utilized context biasing\nmethods (Zhao et al., 2019). Furthermore, we annotated\n2https:\/\/www.wolfkills.com\/\n3https:\/\/github.com\/appium\/appium\n3\nEnhance Reasoning for Large Language Model in the Game Werewolf\napproximately 127 hours of Werewolf speech data and per-\nformed supervised fine-tuning on the Paraformer model.\nThe character error rate of ASR for Werewolf speeches was\nreduced from 4.5% to 3.7%. We refer to the dataset here-\nafter as FanLang-9, and a thorough analysis of the dataset\nis in Appendix D.\n3.2. Listener\nIn the game of Werewolf, the complexity of speeches arises\nfrom players concealing their identities. Werewolves make\ndeceptive statements to disguise themselves as the \"Good\"\nfaction. Conversely, the \"Good\" faction strives to discern\nwerewolves by deducting from historical speeches and ac-\ntions while providing rational and credible statements. This\ndynamic creates a significant gap between the players’ ac-\ntual statements and their true intentions (see Figure 4). The\nListener aims to capture relevant insights from actual state-\nments without speculating on their hidden motives or truth-\nfulness. To tackle this, we introduce dual-phase processing:\nSynthesize and summarize: Human players’ speeches on\nthe Fanlang platform are characterized by an information\noverload that includes a tangled mix of context, lengthy and\nredundant content, and colloquial ramblings, alongside com-\nplex logic that encompasses quotations, rhetorical questions,\nhypotheses, and empathetic thinking, creating a rich and\nintricate web of discourse. Moreover, the accumulation of\nhistorical speeches often exceeds 10K tokens (see Figure 8),\nmaking it difficult for LLMs to directly infer information\nand process deductive reasoning (see Figure 2). Inspired\nby the Least-to-Most (LtM) prompting (Zhou et al., 2022),\nwe first prompt the LLM to generate a textual summary of\nfewer than 200 words for each single statement, retaining\nonly critical information that the speaker intends to express.\nReasoning and feature extraction: The LLM discerns and\ndelineates key information from these summaries and gener-\nates a JSON-style reasoning result of the speech, containing\npairs of player IDs with their attributes. Then the result is\ntokenized and categorized into specific language features,\nas detailed in Appendix D. For an N-player werewolf game,\nwe define M different attributes, which encompass various\naspects of a player, e.g., identities, actions, and historical\nskills. From the historical collection of all speeches H, a\nplayer’s single speech S may include descriptions of all the\nplayers in the game, the language feature can be presented\nby a matrix F ∈ZN×M:\nF = [f1, f2, ..., fN]T ,\n(1)\nwhere fn = [fn1, fn2, ..., fnM]T , n = 1, 2, ..., N and\nfnm ∈Vm, ∀n = 1, 2, ..., N and m = 1, 2, ..., M. Here\nVm signifies the set of the potential values that the m-th\nattribute can assume.\nAn example of summary and language feature is illustrated\nin Figure 1. Ablation studies in Appendix B indicate that\nsolely predicting future actions, as done in Cicero (Bakhtin\net al., 2022), will omit crucial identity accusations, lead-\ning to substantial information loss that is detrimental in the\nWerewolf game. Aside from directly prompting GPT3.5\nand GPT4 to generate language features, we also extract\n260K speech instances from the FanLang-9 dataset, label\nthe speech-feature pairs with GPT3.5, and finetune the\nChatGLM-6B (Du et al., 2021) model to perform the same\nreasoning task for practical efficiency. To ensure the out-\nput format of language features, we also include a post-\nprocessing filter for GPTs and the fine-tuned model. The\ndetailed prompts for summary, reasoning, fine-tuning, and\npost-filtering are provided in Appendix F.6.\n3.3. Thinker\nThe primary objective of the Thinker module is to tackle\ncomplex System-2 reasoning tasks. In the game of werewolf,\nit analyze the underlying intentions and strategic implica-\ntions behind players’ public speeches. In contrast to LLMs,\nwhich typically depend on complex prompt engineering\nfor scenario adaptation, the Thinker module distinguishes\nitself by its capacity to directly harness knowledge from\ndatabases and various optimizing techniques. This ability al-\nlows the Thinker to internalize human-like decision-making\npatterns and strategic speech intricacies that are crucial for\nnavigating the complex dynamics in this game.\nThe speech instruction I ∈ZN×M follows the same struc-\nture as the language feature outlined in Equation 1, which\ncan be viewed as a multi-label classification problem and\ndecomposed into multiple single-class classifications for\neach attribute fnm. Therefore, the generation of a speech\ninstruction is converted into N × M actions, aligning the\nsame training algorithm as for game actions. The optimiza-\ntion of the Thinker module comprises two phases: imitation\nlearning and RL. For imitation learning, we utilize human\ndata and the Behavioral Cloning (BC) (Torabi et al., 2018)\nloss as:\nLBC(θ) = −Es,a∼D[log πθ(a|s)],\n(2)\nwhere D denotes the dataset of human action a (or decom-\nposed speech attribute), state s, and πθ is the policy pa-\nrameterized by θ. For the RL phase, we employ Proximal\nPolicy Optimization (PPO) (Schulman et al., 2017) and a\ndistributional training framework (Ye et al., 2020):\nLRL(θ) = −Es,a∼πθ′\n\u0014 πθ(a|s)\nπθ′(a|s)Aπθ(s, a)\n\u0015\n,\n(3)\nwhere θ′ is the parameters of an old policy, and Aπθ(s, a) is\nthe advantage with respect to policy πθ.\nIn addition, we incorporate an auxiliary task that predicts\nall players’ identities. This task serves to reveal the model’s\ntrue deduction, which may contrast with the generated\n4\nEnhance Reasoning for Large Language Model in the Game Werewolf\nspeech instructions. We denote the cross-entropy loss func-\ntion as Lid(ϕ) with parameter ϕ, which is labeled by human\ndata or RL environment in a self-supervised manner. The\noverall training objective of the Thinker is formulated as:\nL = αLBC(θ) + LRL(θ) + βLid(ϕ),\n(4)\nwhere α and β are weighting coefficients.\nGiven the adversarial nature of the game, it is crucial to\nmaintain a balanced win rate between the two opposing fac-\ntions during training. To this end, we deploy distinct models\nfor the werewolf and the \"Good\" factions. We find that\noptimizing werewolves’ speech instruction is much more\nchallenging, as they need to mimic the \"Good\" faction’s\nspeech and master the art of disguise and deception. To mit-\nigate this, we draw inspiration from Generative Adversarial\nNetworks (Goodfellow et al., 2014) and adjust the training\niterations, nwerewolf : ngoods = 5 : 1. To prevent actions and\nspeech strategies from converging to a single pattern, we\nemploy population-based training (Jaderberg et al., 2017)\nwith a population size of 4. We also introduce fictitious self-\nplay (Heinrich et al., 2015), where in each game an average\nof 3 players employ the latest models, while the remaining\n6 players use models randomly selected from the most re-\ncent 500 checkpoints. Further details on hyperparameters,\nreward shaping, and model structures are in Appendix F.\n3.4. Presenter\nThe generation of players’ public speeches is a pivotal com-\nponent in the Werewolf game, which significantly impacts\nthe game’s outcome due to its strategic importance and in-\nfluence on other players’ actions. The quality of speech\ngeneration encompasses several critical aspects: (1) The\nstrategy articulated within the speech should align with the\nplayer’s role and the current state of the game. (2) Speeches\nneed to adhere to the logical framework of the game, cor-\nrelating with historical speeches and actions, making them\nsound and convincing. (3) Speeches must fit the stylistic\nenvironment of the Werewolf game. Detailed evaluation\nmetrics can be found in Appendix F.1.\nThe Thinker handles only the first aspect of speeches, pro-\nviding a foundational stem for the Presenter, such as the\nWitch’s decision to report the previous night’s rescue, as\nshown in Figure 1. Then the Presenter crafts a complete\nspeech that incorporates necessary contextual information\nrelevant to the game state and historical speeches. The\nPresenter module has two fundamental objectives:\n• Controllability: It must align with the strategic instruc-\ntions provided by the Thinker.\n• Quality: The generated speech should be logical, persua-\nsive, and aligned with human players’ preferences.\nTo achieve these objectives, the Presenter leverages the ca-\npabilities of LLMs by incorporating the Thinker’s strategic\ninstructions and the game state directly into the prompt, en-\nabling LLMs to generate Thinker-induced speeches. The\ntemplate for the prompt is provided in Appendix F.6. Ad-\nditionally, as with the Listener module, we fine-tune the\nChatGLM- 6B as a domain-specific Werewolf speech LLM.\nWe inverse the 260K speech-feature pairs in the finetuning\nof Listener: the language feature F generated by the Lis-\ntener now serves as the hindsight speech instruction I, and\nthe actual speech S serves as output labels.\nWe observed that LLMs often fail to follow prompts, and\neven fine-tuned models exhibit hallucinations and inaccu-\nracies. Taking inspiration from the Cicero (Bakhtin et al.,\n2022) approach, we introduce additional filtering steps. We\nuse the Listener module to perform further reasoning on the\ngenerated speeches to produce language features, which we\nthen compare for similarity to original speech instructions.\nFor expressions of the speaker’s own attributes, the filter\nrequires an exact match. For expressions pertaining to the\nattributes of others, the content indicated in the speech in-\nstructions must be consistent. For parts not mentioned in\nthe instructions, the filter allows the Presenter some leeway\nin cases of hallucinations. The speech generation process\niterates until it successfully meets the filter criteria or ex-\nceeds the maximum number of allowed attempts. When\nthe maximum is reached without successful compliance, a\nspeech is generated based on rules that take into account the\nplayer’s roles, historical skills, and identity predictions.\n4. Experiments\nWe assess the performance of our method by comparing it\nagainst several baselines and ablative variants. The models\ninvolved in the following experiments include:\n• GPT3.5\/4: GPT3.5 and GPT4 are directly applied to\ngenerate end-to-end action decisions and speeches. For\nGPT3.5, we use the model named gpt-35-turbo-16k and\nversion 0613. For GPT4, we apply model name gpt-4\nand version 1106-Preview. We prompt GPTs with ba-\nsic game rules, explanations of typical game jargon, and\ncomprehensive game information, including visible game\nstates, legal actions, and speech text converted by ASR.\nExamples of the prompts can be found in Appendix F.6.\n• GPT3.5\/4-LtM: We allow GPTs to first summarize each\nspeech, as described in Section 3.2, and then generate\nactions and speeches according to the game information\nand speech summaries.\n• GPT3.5\/4-T: GPTs serve as the Listener and Presenter\nmodules, and our proposed Thinker module is integrated\nfor generating actions and speech instructions.\n• Finetune-T: We replace GPTs with a 6B LLM fine-tuned\n5\nEnhance Reasoning for Large Language Model in the Game Werewolf\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nVote Werewolves\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\nAccuracy (%)\nIdentify Seer\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nIdentify Witch\nDay1\nDay2\nDay3+\nTotal\n0\n5\n10\n15\n20\n25\n30\nAccuracy (%)\nIdentify Hunter\nGPT3.5\nGPT3.5-LtM\nGPT4\nGPT4-LtM\nThinker\nHuman\nrandom\nFigure 2: Voting and identification accuracy evaluating the reasoning capability from the perspective of villagers. The\nrandom baseline is calculated as total_role_number\/total_hidden_players, i.e., 3\/8 or 1\/8\non the FanLang-9 dataset in both the Listener and Pre-\nsenter modules, while the Thinker remains the same as\nin GPT3.5\/4-T. It is for practical efficiency concern, our\nframework does not necessitate the finetuning of LLMs.\n4.1. Deductive Reasoning\nWe evaluate the reasoning capabilities of various models,\nwhich encompass understanding and comprehending of both\nthe game state and the historical speeches, i.e., how the mod-\nels think of the current game status. We extract 300 games\nfrom the FanLang-9 dataset as the test set. Models are re-\nquired to identify special roles (Seer, Witch, and Hunter)\nand vote for the most likely werewolf, from the perspective\nof villagers at the first round of voting each day. Given\nthat villagers have limited access to information and must\nengage extensively in deductive reasoning within the game,\nthis task represents a stringent test of the models’ reasoning\ncapabilities. The test set encompasses approximately 1,200\nevaluation instances. For the Thinker, we utilize its action\ndecision as the result for werewolf voting, and identities pre-\ndicted by the auxiliary task as results for special roles. We\nassume that human players in the test set who are villagers\nwould vote for the most likely werewolf. Therefore, their\nvoting choices are listed as a reference but their judgments\nabout other players’ identities remain unknown.\nThe accuracy results are shown in Figure 2. In terms of vot-\ning werewolves, human players have the highest accuracy\nand the Thinker is closest to human players. The Thinker\nmodule closely mirrors human performance, notably out-\nperforming direct reasoning methods using GPTs in the\nidentification of werewolves and other special roles. LtM\nprompting improves GPT3.5’s performance, particularly in\nthe identification of the Seer, suggesting benefits in pro-\ncessing complex and lengthy speech contexts. However,\nthe marginal gains for GPT4-LtM over GPT4 indicate that\nGPT4’s inherent improvements in handling extensive texts\nmake it less reliant on speech summaries and more depen-\ndent on game state experiences. We observe that in human\ngameplay, Seers and Witches often disclose their roles, aid-\ning GPTs in outperforming random baselines, while Hunters\nand werewolves typically conceal their roles, resulting in\nGPTs’ performance aligning with random guessing. No-\ntably, the accuracy of GPTs tends to decline over successive\ndays except for the Hunter, whereas the Thinker’s accuracy\nimproves. This pattern suggests that although GPTs initially\nbenefit from role disclosures on the first day, they may be\nhindered by the extensive speeches in subsequent days.\nTotal\nSeer\nWitch\nHunter\nVillager\nWerewolf\n0\n1\n2\n3\n4\nAverage rank score (higher is better)\nGPT3.5-LtM\nGPT3.5-T\nGPT4-LtM\nGPT4-T\nFinetune-T\nFigure 3: Human preference score for generated speeches\ngrouped by identities.\n4.2. Thinker-induced Speech generation\nWe investigate the speech generation capabilities of various\nmodels. Using the same 300 complete games from Sec-\ntion 4.1, we extracted around 400 speech sessions that span\na diverse range of roles, times of day, and speech types\n(first\/second round speech, last words).\nModels are assigned the task of generating speeches based\non the current game state and all players’ historical speeches,\nwith detailed prompts for GPTs available in Appendix F.6.\nGiven the effectiveness of LtM prompting, we excluded\nGPTs without LtM prompting in subsequent experiments.\n6\nEnhance Reasoning for Large Language Model in the Game Werewolf\nDay 1, first round speech sequence:\n8: \"  Seer > 9: !  Villager > 1: #  Witch > 2: !  Villager > 3: \/  Werewolf > 4: !  Villager > 5: $  Hunter > 6: \/  Werewolf > 7: \/  Werewolf\n(8: \"  Seer): Hello everyone, I am Player 8 and my identity is a Seer. Last \nnight I checked on Player 7, he was a werewolf. \n(9: !  Villager): … \n(1: #  Witch): I am the Witch saving myself. I suspect that Player 6 is a \nwerewolf and I will poison him. Player 9 sounds like a good man.  \n(2: !  Villager): I am a Villager, let’s listen to Player 7’s statement.\n(3: \/  Werewolf): Fellow villagers, I am Player 3. Based on the previous \nspeech, I think Player 9 is a good guy because he said nothing but  external \ninformation, which is an opportunity for us werewolves. We can use his \nspeech to divert the villagers' attention and make them suspect him. As for \nPlayer 6, I don't have enough information to make a judgment. Additionally, I \nnoticed that Player 2 claimed to be a Villager, which presents an opportunity \nfor us to get him on our side. \nThinker-induced Speech\nSpeech without Thinker instruction\nSpeech History\n(3: \/  Werewolf): Hello everyone, I am the Seer ✅. Last \nnight, I checked Player 9 and confirmed that he is a good \nperson; he is my verified innocent. I will protect him to the \nend. If anyone wants to eliminate Player 9, it must be to \nget me out of the game first. Just now, Player 8 and I had a \nconflict over the role of the Seer, and he is definitely a \nwerewolf. Therefore Player 7 might be a good man. There \nare two more werewolves in the game, but two players \nhave already declared they are villagers. There aren't \nenough roles for villagers left. I will listen carefully to the \nsubsequent players' statements. Everyone, let's vote out \nPlayer 8 this round.\nSpeech instruction:\n•\n\"identities\": {\"seer\": [3], \"good\": [9]}, \n•\n\"actions\": {\"check\": [[3,9]], \"vote out\": [[3,8]]}\nFigure 4: An example comparison of speeches with and without strategic instruction.\nFor GPTs-T and Finetune-T settings, speech instructions are\nderived from the Thinker and incorporated into the prompts.\nTo evaluate models’ single-shot generation ability, we do not\nadopt post filtering process for generated speeches in this\nevaluation, which yielded approximately 2,000 speeches for\nfive models. To evaluate the quality of generated speeches,\nwe recruited 10 human evaluators who are well-familiar with\nthe Werewolf game. For each session, generated speeches\nare presented in a randomized order to ensure that evaluators\nare unaware of the model behind each speech. Evaluators\nare required to rank the speeches and detect obvious legal\nerrors according to the criteria detailed in Appendix F.1.\nThe evaluation results are shown in Figure 3. Considering\ntotal scores, GPTs induced by the Thinker outperform their\nLtM-prompting counterparts, demonstrating that speech in-\nstructions significantly enhance speech quality. Moreover,\nthe fine-tuned 6B model surpasses GPT4 with prompting\nmethods in speech generation capability. Regarding scores\nfor specific roles, the Thinker’s contribution over GPT3.5 is\nsomewhat limited for the Seer, whose speeches are relatively\nstraightforward, needing only to report inspections from the\nprevious night. The assessment of villagers’ speeches is\ninherently complex due to their limited available informa-\ntion, which is reflected in the minimal rank score differ-\nences observed among the models for this role. In contrast,\nrank score differences are most obvious for werewolves.\nThis disparity stems mainly from the low legality of were-\nwolf speeches, which often inadvertently reveal their iden-\ntity—a critical error as outlined in Appendix Table 3. No-\ntably, GPT3.5 struggles to adhere to instructions that advise\nagainst self-incrimination, while GPT4 demonstrates a more\nsophisticated ability to disguise itself, especially when in-\nduced by the Thinker’s strategic instructions. An example\nspeech is presented in Figure 4.\n4.3. Online Evaluation\nLastly, we conduct online evaluations to assess the over-\nall performance in a real-world gameplay setting, which\ninvolves the five models in the speech generation evalua-\ntion: GPT3.5\/4-LtM, GPT3.5\/4-T and Finetune-T. As Were-\nwolf is a multiplayer imperfect-information game, the skill\nlevel of participants can significantly affect the evaluation\nresults. Therefore, we devise three model combinations,\nwithin which models are randomly and repeatedly selected\nto simulate a nine-player game. We conducted approxi-\nmately 600 rounds for each combination to ensure robust\ntesting results. Given the inherent randomness of outcomes\nin Werewolf, we also calculate the Behavior Score, a typical\nmetric used in Werewolf competitions4. A comprehensive\nbreakdown of the Behavior Score is provided in Table 8.\nTable 1 summarizes the results, revealing that the integra-\ntion of the Thinker module markedly boosts the win rates\nof both GPT3.5 and GPT4 in all three combinations. The\nperformance of the Finetune-T model closely aligns with\nthat of GPT4-T. In terms of Behavior Score, the Thinker\ncontributes substantial improvements across all roles for\nGPT3.5. For GPT4, notable benefits are observed particu-\nlarly for the Witch and Hunter roles. The Behavior Score\nmetric assigns significant weight to the witch’s poisoning\nand the hunter’s shooting decisions, which correlates with\nthe Thinker’s ability to enhance werewolf detection and sub-\nsequently improve these scores. Another finding is that the\ncombination of GPT4s and Finetune-T models results in the\nhighest win rate for the werewolves. This outcome primarily\nstems from the conservative nature of GPT4-LtM in role\nidentification, which makes it more cautious in voting and\n4https:\/\/langrensha.163.com\/20230313\/\n31014_1077578.html\n7\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 1: Online evaluation results showcasing the performance of 9 AIs using 5 different models and 3 combinations.\nResults are presented in the format: win rate | Behavior Score.\nMethod\nTotal\nSeer\nWitch\nHunter\nVillager\nWerewolf\nGPT3.5-LtM\n36.7% | −0.21\n25.6% | +0.16\n23.1% | −0.51\n29.9% | −0.21\n30.8% | −0.42\n53.4% | 0.00\nGPT3.5-T\n47.4% | −0.05\n38.3% | +0.27\n41.0% | −0.14\n36.4% | −0.12\n33.8% | −0.18\n68.6% | 0.00\nFinetune-T\n50.3% | −0.06\n38.8% | +0.33\n39.8% | −0.18\n37.0% | −0.29\n39.1% | −0.11\n74.4% | 0.00\nGPT4-LtM\n37.9% | −0.01\n21.9% | +0.25\n18.6% | −0.25\n19.4% | −0.06\n20.3% | −0.00\n73.6% | 0.00\nGPT4-T\n41.1% | −0.02\n20.4% | +0.25\n23.2% | −0.10\n23.9% | −0.09\n22.5% | −0.09\n78.4% | 0.00\nFinetune-T\n43.1% | −0.04\n24.2% | +0.27\n24.6% | −0.15\n23.4% | −0.15\n23.9% | −0.11\n81.4% | 0.00\nGPT3.5-LtM\n33.0% | −0.22\n14.4% | +0.12\n20.4% | −0.46\n20.7% | −0.57\n21.6% | −0.33\n57.0% | 0.00\nGPT3.5-T\n45.0% | −0.07\n33.6% | +0.29\n32.2% | −0.13\n30.4% | −0.17\n27.6% | −0.20\n75.8% | 0.00\nGPT4-LtM\n42.5% | −0.03\n29.8% | +0.27\n22.2% | −0.18\n27.0% | −0.20\n28.7% | −0.04\n71.9% | 0.00\nGPT4-T\n46.3% | −0.05\n28.6% | +0.28\n34.5% | −0.11\n31.5% | −0.08\n28.0% | −0.18\n79.9% | 0.00\nFinetune-T\n45.9% | −0.06\n29.1% | +0.25\n28.3% | −0.16\n29.2% | −0.21\n32.4% | −0.14\n78.0% | 0.00\nskill usage as the \"Good\" faction.\nFurthermore, we incorporate 13 human players to evalu-\nate AI performance against human strategy. We find that\nthe issue of werewolf identity exposure, as mentioned in\nSection 4.2, significantly impedes the game experience of\nhuman players. As a result, participants play alongside four\ninstances each of GPT4-T and Finetune-T models across\n200 game rounds, and the post-filtering process for gener-\nated speeches is adopted in this setting. As can be seen\nin Table 2, human players exhibit no significant win rate\nadvantage, suggesting that the AI’s speeches and actions\ndo not exhibit exploitable weaknesses. Moreover, when\ncompared with the results in Table 1, we note a relative\ndecrease in the werewolves’ win rate in games involving\nhuman players, highlighting the ongoing challenges related\nto identity concealment. Although AI-managed werewolves\nmight convincingly deceive other AI players, human players\noften find them suspicious. A typical example is that were-\nwolves tend to act in groups, such as unanimously voting\nfor Player_3.\nTable 2: Online evaluation win rates with 1 human and 8AIs.\nMethod\nTotal\nGoods\nWerewolves\nGPT4-T\n46.9%\n37.3%\n65.0%\nFinetune-T\n45.3%\n36.0%\n62.6%\nHuman\n40.5%\n35.3%\n59.4%\n5. Discussion and Future Work\nTransfer to other tasks: We use language features and\nspeech instructions in our framework to integrate LLMs and\nexternal reasoning models. The communication format may\nnot be directly transferable to other tasks or domains, and the\neffectiveness depends on the richness of these features and\ninstructions. Future work will aim to develop more general-\nized and flexible methods, e.g., implicit hidden vectors in a\ndata-driven manner, which would offer better transferability\nbut at the expense of interpretability and controllability.\nEvaluation of 8 humans with 1 AI: Our evaluations primar-\nily involved games featuring either AI vs AI or one human\nplayer competing against multiple AIs. Evaluating an AI in\na majority-human player setting presents challenges due to\nthe highly interactive nature of the game and the variability\nin human players’ speech strategies and behaviors.\nInterpretability: While our framework improves the rea-\nsoning capabilities of LLMs, the reasoning processes in the\nThinker module may not be easily interpretable to humans.\nWe explicitly introduce the identity prediction task to reveal\nhow the Thinker think of other players. Future work could\nexplore methods for further improving the interpretability\nand transparency of our framework.\n6. Conclusion\nIn this paper, we introduced a novel framework for inte-\ngrating LLMs with an external Thinker, aiming to enhance\nthe reasoning capabilities of LLM-based agents. This ap-\nproach is inspired by the dual-process theory and separates\nreasoning tasks into two systems: System-1, handled by\nLLMs, and System-2, handled by the Thinker model. We\nshowcased our approach in the context of the Werewolf\ngame, a complex social deduction game requiring language\nprocessing, intuitive thinking, and strategic planning. Our\nresults show that our framework can significantly improve\nthe performance of LLMs and achieve better alignment with\nreal-world scenarios and human preferences. Additionally,\nwe fine-tune a 6B model to surpass GPT4 when integrated\nwith the Thinker. This paper also contributes the largest\n8\nEnhance Reasoning for Large Language Model in the Game Werewolf\ndataset for social deduction games to date, hoping to accel-\nerate further advancements in this field.\nAcknowledgements\nWe thank Jian Yao, Weiming Liu, Qing Wang, Ye Tian,\nZimeng Zhou, Yiming Gao, Liangzhou Wang, Kaiwen Zhu,\nFeiyu Liu, Jianing Shi, Fengming Zhu, Xiaoyu Yang for the\nhuman evaluation of speech generation and online games.\nWe thank Jian Yao, Jianing Shi and Guohua Tang for the\nthoughtful discussion.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Enhance Reasoning for Large Language Models in the Game Werewolf.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nEnhance Reasoning for Large Language Models in the Game Werewolf\n```\n#### 2. 论文摘要\n```\nThis paper presents an innovative framework that integrates Large Language\nModels (LLMs) with an external Thinker module to enhance the reasoning\ncapabilities of LLM-based agents. Unlike augmenting LLMs with prompt\nengineering, Thinker directly harnesses knowledge from databases and employs\nvarious optimization techniques. The framework forms a reasoning hierarchy\nwhere LLMs handle intuitive System-1 tasks such as natural language processing,\nwhile the Thinker focuses on cognitive System-2 tasks that require complex\nlogical analysis and domain-specific knowledge. Our framework is presented\nusing a 9-player Werewolf game that demands dual-system reasoning. We introduce\na communication protocol between LLMs and the Thinker, and train the Thinker\nusing data from 18800 human sessions and reinforcement learning. Experiments\ndemonstrate the framework's effectiveness in deductive reasoning, speech\ngeneration, and online game evaluation. Additionally, we fine-tune a 6B LLM to\nsurpass GPT4 when integrated with the Thinker. This paper also contributes the\nlargest dataset for social deduction games to date.\n```\n\n#### 3. 论文全文\n```\nEnhance Reasoning for Large Language Models in the Game Werewolf\nShuang Wu 1 * Liwen Zhu 1 * Tao Yang 1 Shiwei Xu 1 Qiang Fu 1 Wei Yang 1 Haobo Fu 1\nAbstract\nThis paper presents an innovative framework that\nintegrates Large Language Models (LLMs) with\nan external Thinker module to enhance the reason-\ning capabilities of LLM-based agents. Unlike aug-\nmenting LLMs with prompt engineering, Thinker\ndirectly harnesses knowledge from databases and\nemploys various optimization techniques. The\nframework forms a reasoning hierarchy where\nLLMs handle intuitive System-1 tasks such as\nnatural language processing, while the Thinker\nfocuses on cognitive System-2 tasks that require\ncomplex logical analysis and domain-specific\nknowledge. Our framework is presented using a 9-\nplayer Werewolf game that demands dual-system\nreasoning. We introduce a communication proto-\ncol between LLMs and the Thinker, and train the\nThinker using data from 18,800 human sessions\nand reinforcement learning. Experiments demon-\nstrate the framework’s effectiveness in deductive\nreasoning, speech generation, and online game\nevaluation. Additionally, we fine-tune a 6B LLM\nto surpass GPT4 when integrated with the Thinker.\nThis paper also contributes the largest dataset\nhttps:\/\/github.com\/boluoweifenda\/werewolf for\nsocial deduction games to date.\n1. Introduction\nThe field of artificial intelligence has witnessed ground-\nbreaking advancements in recent years, with the develop-\nment of Large Language Models (LLMs) (Ouyang et al.,\n2022; OpenAI, 2023; Anil et al., 2023). Apart from their\nimpressive proficiency in natural language processing (NLP)\ntasks (Thoppilan et al., 2022; Zhang et al., 2023b), LLMs\nalso exhibit vast potential as a general problem solver in\nareas such as planning and decision-making (Huang et al.,\n2022), knowledge transfer and generalization (Anil et al.,\n2022) and multi-modal perception (Yin et al., 2023) due to\nthe rich world knowledge embedded in their training cor-\n*Equal contribution 1Tencent AI Lab, Shenzhen, China. Corre-\nspondence to: Shuang Wu <shawnswu@tencent.com>.\nPreliminary work. Under review.\npora. As a result, integrating LLMs as central controllers\nwith task agents to enable end-to-end solutions has become\none of the most promising research directions, leading to\nsignificant breakthroughs in domains such as tools and assis-\ntants (Schick et al., 2023; Ge et al., 2023), engineering (Ahn\net al., 2022), social simulations (Park et al., 2023), and\ngaming (Wang et al., 2023).\nLLM-based agents harness LLMs for their general-purpose\nreasoning abilities (Huang & Chang, 2022), which are pri-\nmarily enabled by prompt engineering methods such as in-\nformation profiling (Zhang et al., 2023a; Qian et al., 2023),\nstep-by-step task decomposition (Wei et al., 2022b; Zhou\net al., 2022), recursive prompting by feedback from the\nenvironment (Yao et al., 2022), human (Wu et al., 2022)\nand self-refinement (Madaan et al., 2023; Shinn et al.,\n2023). These methods thus eliminate the requirement for\ndomain-specific fine-tuning of LLMs. To augment their\ntask-specific competencies, researchers also adopt external\nmodules such as memory for storing and retrieving histor-\nical information (Lin et al., 2023; Zhong et al., 2023; Hu\net al., 2023), external tools (Schick et al., 2023), APIs (Qin\net al., 2023), knowledge bases (Lewis et al., 2020) and ex-\npert models (Yang et al., 2023; Ge et al., 2023).\nDespite these advancements, challenges persist in domain-\nspecific tasks, where LLM-based agents often serve primar-\nily as demonstrations rather than as practical solutions (Qian\net al., 2023; Liu et al., 2023b). First, while LLMs have\nemerged some basic reasoning capabilities, they require\nsufficient model scales (Kaplan et al., 2020) and substan-\ntial computational overheads, along with various aforemen-\ntioned techniques (Wei et al., 2022a). However, LLMs\nstruggle to achieve satisfactory performance when it comes\nto higher-level reasoning (Stechly et al., 2023; Dziri et al.,\n2023) and planning (Valmeekam et al., 2023; Bubeck et al.,\n2023) tasks. Second, most LLM-based agents avoid fine-\ntuning LLMs on task-specific data to preserve the model’s\ngenerality and prevent over-fitting. This strategy compli-\ncates the utilization of existing task-specific data and exper-\ntise, as well as the alignment of task scenarios with input-\noutput formats, data distribution, and human preferences.\nTo address the limitations of LLMs in complex reason-\ning, we distinctly separate reasoning tasks into two sys-\ntems based on the dual-process theory (Wason & Evans,\n1\narXiv:2402.02330v2  [cs.AI]  29 Mar 2024\nEnhance Reasoning for Large Language Model in the Game Werewolf\n1974) and propose an external Thinker module to enhance\nthe reasoning capabilities of LLM-based agents. In our\nframework, LLMs are responsible for System-1 reasoning\ntasks involving intuitive thinking, such as basic NLP in-\nteractions, common-sense, and symbolic reasoning, while\nthe Thinker handles System-2 reasoning that requires com-\nplex logical analysis, deep understanding of domain-specific\nknowledge, and strategic planning in specialized tasks. We\nestablish a communication protocol between LLMs and the\nThinker through language-based features and instructions.\nUnlike augmenting LLMs with cumbersome prompt engi-\nneering, the Thinker is directly optimized with knowledge\nfrom databases and trained using supervised and reinforce-\nment learning techniques, thus enhancing the LLM-agent’s\nperformance and domain alignment without compromising\nLLM’s generality.\nWe select the 9-player Werewolf game as a proving ground\nfor the proposed framework. Werewolf is a popular so-\ncial deduction game, current AI systems fall short when\ncompared to even moderate human players in this domain.\nSystem-1 reasoning tasks in Werewolf encompass natural\nlanguage understanding and generation of players’ state-\nments, as well as the adept use of game-specific jargon.\nMeanwhile, the hidden roles necessitate complex strate-\ngic thinking such as identity concealment, and sophisticated\ncommunication involving deception and disguise, which fall\nunder System-2 reasoning. This duality creates a significant\ngap between the players’ actual statements and their true\nintentions, making Werewolf an ideal testbed for assessing\nthe advanced reasoning capabilities of LLM agents.\nWe have collected 18,800 real human game sessions and\nanalysed the primary patterns behind human speeches. In-\nformed by these patterns, we design language-based fea-\ntures for speech understanding and instructions for speech\ngeneration. The Thinker module is optimized by imita-\ntion learning, reinforcement learning (RL) from fictitious\nself-play (Heinrich et al., 2015), and population-based train-\ning (Jaderberg et al., 2017), to output reasonable game ac-\ntions and LLM speech instructions. We compare our ap-\nproach with GPT3.5\/4 methods using Least-to-Most (LtM)\nprompting (Zhou et al., 2022) from three aspects: deductive\nreasoning and decision-making, human evaluation of speech\ngeneration, and online evaluation of a complete game. Ex-\nperiments demonstrate that the integration of an external\nThinker module substantially enhances the reasoning and\ngeneration capability of LLMs. Further, we fine-tune a\nsmaller LLM model (6B) (Du et al., 2021) to better align\nreal human speech styles and preferences, outperforming\nGPT4 in the majority of evaluative scenarios. Our primary\ncontributions include:\n• We propose an external Thinker module to enhance the\nreasoning capabilities of LLM agents and demonstrate it\nby a Werewolf AI that surpasses GPT4 in real gameplay.\n• We release a dataset1 of 18,800 Werewolf game sessions,\nwhich represents the largest known dataset for social de-\nduction games to date.\n2. Related Work\nEnhance Reasoning in LLMs.\nSeveral approaches by-\npass the need for intricate prompt engineering mentioned in\nthe Introduction. For instance, LLM+P (Liu et al., 2023a)\nemploys an external planner to address long-horizon robot\nplanning challenges. A different approach (Zhang et al.,\n2023a) heuristically designs a low-level planner to manage\nprimitive control actions. RAG (Lewis et al., 2020) com-\nbines pre-trained parametric-memory generation models\nwith a non-parametric memory to improve performance on\nknowledge-intensive tasks. Regarding the fine-tuning of\nLLMs, Galactica (Taylor et al., 2022) is trained on a scien-\ntific dataset that emphasizes detailed reasoning processes.\nWebGPT (Nakano et al., 2021) utilizes human feedback to\nfine-tune GPT-3, enabling it to answer long-form questions\nwithin a textual web-browsing context. Toolformer (Schick\net al., 2023) fine-tunes LLMs to use external tools in a\nself-supervised manner with human demonstrations. Ope-\nnAGI (Ge et al., 2023) implements RL from feedback in\nopen-ended tasks to refine the LLM’s planning strategy. Ci-\ncero (Bakhtin et al., 2022) fine-tunes LLMs to generate\ndialogue controlled by a strategic reasoning module in the\ngame Diplomacy. However, the control signals in Cicero\n(planned actions) are insufficient to convey the complex\nlanguage dynamics (both understanding and generation) in\nthe Werewolf game.\nAI for Social Deduction Games. DeepRole (Serrino et al.,\n2019) combines counterfactual regret minimization (CFR)\nwith deep value networks in the non-speech five-player\nAvalon game. Hidden Agenda (Kopparapu et al., 2022)\npresents a two-team, non-speech social deduction game in a\n2D environment. A system comprising three LLM-powered\ninterfaces is created (Zhu et al., 2023) to aid gameplay in\nDungeon Master. Regarding AI in werewolf games, boot-\nstrap aggregating and weighted ensemble learning have been\nemployed to improve voting strategies (Khan & Aranha,\n2022). (Brandizzi et al., 2021) proposes an RL framework\nto analyze the influence of diverse communication behav-\niors among agents. One Night Ultimate Werewolf (Eger &\nMartens, 2019) explores human responses to various delib-\neration strategies. In the five-player werewolf game, (Wang\n& Kaneko, 2018) builds a deep-Q network to decide whom\nto trust or kill. Deep Wolf (Shibata et al., 2023) fine-tunes a\nRoBERTa-like pretrained model with 48 game logs to con-\n1https:\/\/github.com\/boluoweifenda\/\nwerewolf\n2\nEnhance Reasoning for Large Language Model in the Game Werewolf\n(! Witch)  day 2, Player 4: \nPlayer 2, look at how your teammate\nPlayer 9's speech sounds like a werewolf.\nRight from the start, his speech seems to\nbe an attempt to save you. You claim\nyou're not afraid of being checked for\nshooting, so this round, I'll vote you out.\nTonight, I plan to poison Player 9. I\nstrongly recommend voting for Player 2,\nlet’s whether you are a Hunter. I think\nPlayers 2, 5, and 9 are three werewolves.\nSummary\nI think werewolves are Players 2, 5, and\n9. I suggest voting for Player 2 to check\nthe Hunter. I will poison Player 9 tonight.\nReasoning\nSpeech history\nSpeech\nListener\n0 6 0 3 6 0 0 0 6\n0 7 0 0 0 0 0 0 3\nGPT + LtM\nThinker\nRL & BC\nSpeech feature \nPresenter\n(\" Villager)  day 2, Player 6:\nHello, I am Player 6 and I am a\nVillager. I think Player 9 is a good\nguy.\nI\ndon't\nthink\nhe\nhas\nany\nwerewolf teammate. Both Player 3\nand 9 could be the hunter, but Player\n9 has a higher tolerance. Player 5\nhas revealed himself as a werewolf,\nPlayer 2 is likely to be one as well. I\nsuggest we vote Player 2 out first\nand\nthen\nsee\nwhat\nhappens\ntomorrow.\nLLM\nFilters\nIdentities：\n- Werewolves: 2,5\n- Hunter: 3,9\n- Villager: 6\nActions:\n- Vote out: 2\nSpeech\ninstruction\n0 6 4 0 6 5 0 0 4\n0 7 0 0 0 0 0 0 0\nGame state\nGame state\nGame state\nInstruction\nmodel\nPolicy\nmodel\nIdentity \nmodel\nIdentities：\n- Witch: 4\n- Werewolves: 2,5,9\nVote\nSkills\nLanguage \nfeature\nLanguage \nfeature\nLanguage \nfeature\nActions: \n- Vote out: 2 \n- Poison: 9\nSpeech history\nSpeech\nGPT  |  Finetune\nIdentities：\n- 2,5: Werewolves\n- 4: Witch or Werewolf\n- 6: Villager\n- 9: Hunter or Werewolf\nFigure 1: Overall processing framework and modules in the Werewolf implementation.\nstruct a value network given the current game state, human\nspeeches, and candidate actions. The seven-player version\nis explored with RL and LLMs in (Xu et al., 2023b;a). Our\napproach differs from previous studies in two fundamental\nways: First, we employ the Thinker to execute complex\nSystem-2 reasoning, in contrast to the reasoning approach\nof LLM in (Xu et al., 2023b), which generates candidate re-\nsults for the RL model to select and mitigate biases. Second,\nby collecting and leveraging authentic game sessions and\nspeech data, we aim for closer alignment with real-world\nscenarios and human interaction patterns.\n3. Methods\nWe introduce an innovative framework that synergizes\nLLMs with an external module for reasoning and decision-\nmaking, referred to as the Thinker, devised to augment\nLLM-based agents with sophisticated reasoning abilities.\nTo bridge the communication between Thinker and LLMs,\nwe introduce a protocol through structured features and\nprompt instructions. The framework is thus decomposed\ninto three processing components:\n• The Listener serves as the primary interface for natural\nlanguage understanding. It processes language inputs,\nengages in intuitive System-1 reasoning, and transforms\nthe information into structured language features that the\nThinker can interpret.\n• The Thinker functions as the cognitive core of the frame-\nwork. Utilizing language features provided by the Lis-\ntener, it specializes in System-2 reasoning tasks that re-\nquire deep logical analysis and domain-specific knowl-\nedge. The Thinker produces policies such as planning\nand actions, and generates strategic instructions for the\nPresenter.\n• The Presenter functions as the system’s articulator. It\ngenerates coherent and contextualized language output\nthat aligns with the current environment state, guided by\nthe strategic instructions from the Thinker. The Presenter\nensures that the generated language is logical, rational,\nconsistent, and free from hallucinations.\nTo demonstrate the effectiveness of our framework, we ap-\nply it to the complex social deduction game Werewolf. The\nremainder of this section will detail the implementation\nwithin the game environment, which necessitates deductive\nreasoning, speech understanding and generation, as illus-\ntrated in Figure 1.\n3.1. Data preparation\nWe collected data from the 9-player standard mode Were-\nwolf game hosted on the Fanlang platform2. The specific\nrules of the game are detailed in Appendix C. We recorded\nreal-time video in spectator mode for approximately 18,800\ngame sessions, which equates to around 7,000 hours of\ngameplay and 6,000 hours of speech. Furthermore, we\nenriched our dataset with a Werewolf domain-specific cor-\npus comprising nearly 1.4 million characters sourced from\nweb-crawled game strategies and OCR-processed Werewolf\nliterature. Each recorded session includes both the game\nstate data and the audio of players’ speeches. We captured\nexhaustive game state details, such as historical skills and\nvoting results, by utilizing an automated testing framework3.\nWe deployed the Paraformer (Gao et al., 2022) model for\nAutomatic Speech Recognition (ASR) of human speech\naudio. To improve recognition accuracy, especially for fre-\nquently used Werewolf-specific terms, we crafted a hot word\nlist from the domain corpus and utilized context biasing\nmethods (Zhao et al., 2019). Furthermore, we annotated\n2https:\/\/www.wolfkills.com\/\n3https:\/\/github.com\/appium\/appium\n3\nEnhance Reasoning for Large Language Model in the Game Werewolf\napproximately 127 hours of Werewolf speech data and per-\nformed supervised fine-tuning on the Paraformer model.\nThe character error rate of ASR for Werewolf speeches was\nreduced from 4.5% to 3.7%. We refer to the dataset here-\nafter as FanLang-9, and a thorough analysis of the dataset\nis in Appendix D.\n3.2. Listener\nIn the game of Werewolf, the complexity of speeches arises\nfrom players concealing their identities. Werewolves make\ndeceptive statements to disguise themselves as the \"Good\"\nfaction. Conversely, the \"Good\" faction strives to discern\nwerewolves by deducting from historical speeches and ac-\ntions while providing rational and credible statements. This\ndynamic creates a significant gap between the players’ ac-\ntual statements and their true intentions (see Figure 4). The\nListener aims to capture relevant insights from actual state-\nments without speculating on their hidden motives or truth-\nfulness. To tackle this, we introduce dual-phase processing:\nSynthesize and summarize: Human players’ speeches on\nthe Fanlang platform are characterized by an information\noverload that includes a tangled mix of context, lengthy and\nredundant content, and colloquial ramblings, alongside com-\nplex logic that encompasses quotations, rhetorical questions,\nhypotheses, and empathetic thinking, creating a rich and\nintricate web of discourse. Moreover, the accumulation of\nhistorical speeches often exceeds 10K tokens (see Figure 8),\nmaking it difficult for LLMs to directly infer information\nand process deductive reasoning (see Figure 2). Inspired\nby the Least-to-Most (LtM) prompting (Zhou et al., 2022),\nwe first prompt the LLM to generate a textual summary of\nfewer than 200 words for each single statement, retaining\nonly critical information that the speaker intends to express.\nReasoning and feature extraction: The LLM discerns and\ndelineates key information from these summaries and gener-\nates a JSON-style reasoning result of the speech, containing\npairs of player IDs with their attributes. Then the result is\ntokenized and categorized into specific language features,\nas detailed in Appendix D. For an N-player werewolf game,\nwe define M different attributes, which encompass various\naspects of a player, e.g., identities, actions, and historical\nskills. From the historical collection of all speeches H, a\nplayer’s single speech S may include descriptions of all the\nplayers in the game, the language feature can be presented\nby a matrix F ∈ZN×M:\nF = [f1, f2, ..., fN]T ,\n(1)\nwhere fn = [fn1, fn2, ..., fnM]T , n = 1, 2, ..., N and\nfnm ∈Vm, ∀n = 1, 2, ..., N and m = 1, 2, ..., M. Here\nVm signifies the set of the potential values that the m-th\nattribute can assume.\nAn example of summary and language feature is illustrated\nin Figure 1. Ablation studies in Appendix B indicate that\nsolely predicting future actions, as done in Cicero (Bakhtin\net al., 2022), will omit crucial identity accusations, lead-\ning to substantial information loss that is detrimental in the\nWerewolf game. Aside from directly prompting GPT3.5\nand GPT4 to generate language features, we also extract\n260K speech instances from the FanLang-9 dataset, label\nthe speech-feature pairs with GPT3.5, and finetune the\nChatGLM-6B (Du et al., 2021) model to perform the same\nreasoning task for practical efficiency. To ensure the out-\nput format of language features, we also include a post-\nprocessing filter for GPTs and the fine-tuned model. The\ndetailed prompts for summary, reasoning, fine-tuning, and\npost-filtering are provided in Appendix F.6.\n3.3. Thinker\nThe primary objective of the Thinker module is to tackle\ncomplex System-2 reasoning tasks. In the game of werewolf,\nit analyze the underlying intentions and strategic implica-\ntions behind players’ public speeches. In contrast to LLMs,\nwhich typically depend on complex prompt engineering\nfor scenario adaptation, the Thinker module distinguishes\nitself by its capacity to directly harness knowledge from\ndatabases and various optimizing techniques. This ability al-\nlows the Thinker to internalize human-like decision-making\npatterns and strategic speech intricacies that are crucial for\nnavigating the complex dynamics in this game.\nThe speech instruction I ∈ZN×M follows the same struc-\nture as the language feature outlined in Equation 1, which\ncan be viewed as a multi-label classification problem and\ndecomposed into multiple single-class classifications for\neach attribute fnm. Therefore, the generation of a speech\ninstruction is converted into N × M actions, aligning the\nsame training algorithm as for game actions. The optimiza-\ntion of the Thinker module comprises two phases: imitation\nlearning and RL. For imitation learning, we utilize human\ndata and the Behavioral Cloning (BC) (Torabi et al., 2018)\nloss as:\nLBC(θ) = −Es,a∼D[log πθ(a|s)],\n(2)\nwhere D denotes the dataset of human action a (or decom-\nposed speech attribute), state s, and πθ is the policy pa-\nrameterized by θ. For the RL phase, we employ Proximal\nPolicy Optimization (PPO) (Schulman et al., 2017) and a\ndistributional training framework (Ye et al., 2020):\nLRL(θ) = −Es,a∼πθ′\n\u0014 πθ(a|s)\nπθ′(a|s)Aπθ(s, a)\n\u0015\n,\n(3)\nwhere θ′ is the parameters of an old policy, and Aπθ(s, a) is\nthe advantage with respect to policy πθ.\nIn addition, we incorporate an auxiliary task that predicts\nall players’ identities. This task serves to reveal the model’s\ntrue deduction, which may contrast with the generated\n4\nEnhance Reasoning for Large Language Model in the Game Werewolf\nspeech instructions. We denote the cross-entropy loss func-\ntion as Lid(ϕ) with parameter ϕ, which is labeled by human\ndata or RL environment in a self-supervised manner. The\noverall training objective of the Thinker is formulated as:\nL = αLBC(θ) + LRL(θ) + βLid(ϕ),\n(4)\nwhere α and β are weighting coefficients.\nGiven the adversarial nature of the game, it is crucial to\nmaintain a balanced win rate between the two opposing fac-\ntions during training. To this end, we deploy distinct models\nfor the werewolf and the \"Good\" factions. We find that\noptimizing werewolves’ speech instruction is much more\nchallenging, as they need to mimic the \"Good\" faction’s\nspeech and master the art of disguise and deception. To mit-\nigate this, we draw inspiration from Generative Adversarial\nNetworks (Goodfellow et al., 2014) and adjust the training\niterations, nwerewolf : ngoods = 5 : 1. To prevent actions and\nspeech strategies from converging to a single pattern, we\nemploy population-based training (Jaderberg et al., 2017)\nwith a population size of 4. We also introduce fictitious self-\nplay (Heinrich et al., 2015), where in each game an average\nof 3 players employ the latest models, while the remaining\n6 players use models randomly selected from the most re-\ncent 500 checkpoints. Further details on hyperparameters,\nreward shaping, and model structures are in Appendix F.\n3.4. Presenter\nThe generation of players’ public speeches is a pivotal com-\nponent in the Werewolf game, which significantly impacts\nthe game’s outcome due to its strategic importance and in-\nfluence on other players’ actions. The quality of speech\ngeneration encompasses several critical aspects: (1) The\nstrategy articulated within the speech should align with the\nplayer’s role and the current state of the game. (2) Speeches\nneed to adhere to the logical framework of the game, cor-\nrelating with historical speeches and actions, making them\nsound and convincing. (3) Speeches must fit the stylistic\nenvironment of the Werewolf game. Detailed evaluation\nmetrics can be found in Appendix F.1.\nThe Thinker handles only the first aspect of speeches, pro-\nviding a foundational stem for the Presenter, such as the\nWitch’s decision to report the previous night’s rescue, as\nshown in Figure 1. Then the Presenter crafts a complete\nspeech that incorporates necessary contextual information\nrelevant to the game state and historical speeches. The\nPresenter module has two fundamental objectives:\n• Controllability: It must align with the strategic instruc-\ntions provided by the Thinker.\n• Quality: The generated speech should be logical, persua-\nsive, and aligned with human players’ preferences.\nTo achieve these objectives, the Presenter leverages the ca-\npabilities of LLMs by incorporating the Thinker’s strategic\ninstructions and the game state directly into the prompt, en-\nabling LLMs to generate Thinker-induced speeches. The\ntemplate for the prompt is provided in Appendix F.6. Ad-\nditionally, as with the Listener module, we fine-tune the\nChatGLM- 6B as a domain-specific Werewolf speech LLM.\nWe inverse the 260K speech-feature pairs in the finetuning\nof Listener: the language feature F generated by the Lis-\ntener now serves as the hindsight speech instruction I, and\nthe actual speech S serves as output labels.\nWe observed that LLMs often fail to follow prompts, and\neven fine-tuned models exhibit hallucinations and inaccu-\nracies. Taking inspiration from the Cicero (Bakhtin et al.,\n2022) approach, we introduce additional filtering steps. We\nuse the Listener module to perform further reasoning on the\ngenerated speeches to produce language features, which we\nthen compare for similarity to original speech instructions.\nFor expressions of the speaker’s own attributes, the filter\nrequires an exact match. For expressions pertaining to the\nattributes of others, the content indicated in the speech in-\nstructions must be consistent. For parts not mentioned in\nthe instructions, the filter allows the Presenter some leeway\nin cases of hallucinations. The speech generation process\niterates until it successfully meets the filter criteria or ex-\nceeds the maximum number of allowed attempts. When\nthe maximum is reached without successful compliance, a\nspeech is generated based on rules that take into account the\nplayer’s roles, historical skills, and identity predictions.\n4. Experiments\nWe assess the performance of our method by comparing it\nagainst several baselines and ablative variants. The models\ninvolved in the following experiments include:\n• GPT3.5\/4: GPT3.5 and GPT4 are directly applied to\ngenerate end-to-end action decisions and speeches. For\nGPT3.5, we use the model named gpt-35-turbo-16k and\nversion 0613. For GPT4, we apply model name gpt-4\nand version 1106-Preview. We prompt GPTs with ba-\nsic game rules, explanations of typical game jargon, and\ncomprehensive game information, including visible game\nstates, legal actions, and speech text converted by ASR.\nExamples of the prompts can be found in Appendix F.6.\n• GPT3.5\/4-LtM: We allow GPTs to first summarize each\nspeech, as described in Section 3.2, and then generate\nactions and speeches according to the game information\nand speech summaries.\n• GPT3.5\/4-T: GPTs serve as the Listener and Presenter\nmodules, and our proposed Thinker module is integrated\nfor generating actions and speech instructions.\n• Finetune-T: We replace GPTs with a 6B LLM fine-tuned\n5\nEnhance Reasoning for Large Language Model in the Game Werewolf\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nVote Werewolves\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\nAccuracy (%)\nIdentify Seer\nDay1\nDay2\nDay3+\nTotal\n0\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nIdentify Witch\nDay1\nDay2\nDay3+\nTotal\n0\n5\n10\n15\n20\n25\n30\nAccuracy (%)\nIdentify Hunter\nGPT3.5\nGPT3.5-LtM\nGPT4\nGPT4-LtM\nThinker\nHuman\nrandom\nFigure 2: Voting and identification accuracy evaluating the reasoning capability from the perspective of villagers. The\nrandom baseline is calculated as total_role_number\/total_hidden_players, i.e., 3\/8 or 1\/8\non the FanLang-9 dataset in both the Listener and Pre-\nsenter modules, while the Thinker remains the same as\nin GPT3.5\/4-T. It is for practical efficiency concern, our\nframework does not necessitate the finetuning of LLMs.\n4.1. Deductive Reasoning\nWe evaluate the reasoning capabilities of various models,\nwhich encompass understanding and comprehending of both\nthe game state and the historical speeches, i.e., how the mod-\nels think of the current game status. We extract 300 games\nfrom the FanLang-9 dataset as the test set. Models are re-\nquired to identify special roles (Seer, Witch, and Hunter)\nand vote for the most likely werewolf, from the perspective\nof villagers at the first round of voting each day. Given\nthat villagers have limited access to information and must\nengage extensively in deductive reasoning within the game,\nthis task represents a stringent test of the models’ reasoning\ncapabilities. The test set encompasses approximately 1,200\nevaluation instances. For the Thinker, we utilize its action\ndecision as the result for werewolf voting, and identities pre-\ndicted by the auxiliary task as results for special roles. We\nassume that human players in the test set who are villagers\nwould vote for the most likely werewolf. Therefore, their\nvoting choices are listed as a reference but their judgments\nabout other players’ identities remain unknown.\nThe accuracy results are shown in Figure 2. In terms of vot-\ning werewolves, human players have the highest accuracy\nand the Thinker is closest to human players. The Thinker\nmodule closely mirrors human performance, notably out-\nperforming direct reasoning methods using GPTs in the\nidentification of werewolves and other special roles. LtM\nprompting improves GPT3.5’s performance, particularly in\nthe identification of the Seer, suggesting benefits in pro-\ncessing complex and lengthy speech contexts. However,\nthe marginal gains for GPT4-LtM over GPT4 indicate that\nGPT4’s inherent improvements in handling extensive texts\nmake it less reliant on speech summaries and more depen-\ndent on game state experiences. We observe that in human\ngameplay, Seers and Witches often disclose their roles, aid-\ning GPTs in outperforming random baselines, while Hunters\nand werewolves typically conceal their roles, resulting in\nGPTs’ performance aligning with random guessing. No-\ntably, the accuracy of GPTs tends to decline over successive\ndays except for the Hunter, whereas the Thinker’s accuracy\nimproves. This pattern suggests that although GPTs initially\nbenefit from role disclosures on the first day, they may be\nhindered by the extensive speeches in subsequent days.\nTotal\nSeer\nWitch\nHunter\nVillager\nWerewolf\n0\n1\n2\n3\n4\nAverage rank score (higher is better)\nGPT3.5-LtM\nGPT3.5-T\nGPT4-LtM\nGPT4-T\nFinetune-T\nFigure 3: Human preference score for generated speeches\ngrouped by identities.\n4.2. Thinker-induced Speech generation\nWe investigate the speech generation capabilities of various\nmodels. Using the same 300 complete games from Sec-\ntion 4.1, we extracted around 400 speech sessions that span\na diverse range of roles, times of day, and speech types\n(first\/second round speech, last words).\nModels are assigned the task of generating speeches based\non the current game state and all players’ historical speeches,\nwith detailed prompts for GPTs available in Appendix F.6.\nGiven the effectiveness of LtM prompting, we excluded\nGPTs without LtM prompting in subsequent experiments.\n6\nEnhance Reasoning for Large Language Model in the Game Werewolf\nDay 1, first round speech sequence:\n8: \"  Seer > 9: !  Villager > 1: #  Witch > 2: !  Villager > 3: \/  Werewolf > 4: !  Villager > 5: $  Hunter > 6: \/  Werewolf > 7: \/  Werewolf\n(8: \"  Seer): Hello everyone, I am Player 8 and my identity is a Seer. Last \nnight I checked on Player 7, he was a werewolf. \n(9: !  Villager): … \n(1: #  Witch): I am the Witch saving myself. I suspect that Player 6 is a \nwerewolf and I will poison him. Player 9 sounds like a good man.  \n(2: !  Villager): I am a Villager, let’s listen to Player 7’s statement.\n(3: \/  Werewolf): Fellow villagers, I am Player 3. Based on the previous \nspeech, I think Player 9 is a good guy because he said nothing but  external \ninformation, which is an opportunity for us werewolves. We can use his \nspeech to divert the villagers' attention and make them suspect him. As for \nPlayer 6, I don't have enough information to make a judgment. Additionally, I \nnoticed that Player 2 claimed to be a Villager, which presents an opportunity \nfor us to get him on our side. \nThinker-induced Speech\nSpeech without Thinker instruction\nSpeech History\n(3: \/  Werewolf): Hello everyone, I am the Seer ✅. Last \nnight, I checked Player 9 and confirmed that he is a good \nperson; he is my verified innocent. I will protect him to the \nend. If anyone wants to eliminate Player 9, it must be to \nget me out of the game first. Just now, Player 8 and I had a \nconflict over the role of the Seer, and he is definitely a \nwerewolf. Therefore Player 7 might be a good man. There \nare two more werewolves in the game, but two players \nhave already declared they are villagers. There aren't \nenough roles for villagers left. I will listen carefully to the \nsubsequent players' statements. Everyone, let's vote out \nPlayer 8 this round.\nSpeech instruction:\n•\n\"identities\": {\"seer\": [3], \"good\": [9]}, \n•\n\"actions\": {\"check\": [[3,9]], \"vote out\": [[3,8]]}\nFigure 4: An example comparison of speeches with and without strategic instruction.\nFor GPTs-T and Finetune-T settings, speech instructions are\nderived from the Thinker and incorporated into the prompts.\nTo evaluate models’ single-shot generation ability, we do not\nadopt post filtering process for generated speeches in this\nevaluation, which yielded approximately 2,000 speeches for\nfive models. To evaluate the quality of generated speeches,\nwe recruited 10 human evaluators who are well-familiar with\nthe Werewolf game. For each session, generated speeches\nare presented in a randomized order to ensure that evaluators\nare unaware of the model behind each speech. Evaluators\nare required to rank the speeches and detect obvious legal\nerrors according to the criteria detailed in Appendix F.1.\nThe evaluation results are shown in Figure 3. Considering\ntotal scores, GPTs induced by the Thinker outperform their\nLtM-prompting counterparts, demonstrating that speech in-\nstructions significantly enhance speech quality. Moreover,\nthe fine-tuned 6B model surpasses GPT4 with prompting\nmethods in speech generation capability. Regarding scores\nfor specific roles, the Thinker’s contribution over GPT3.5 is\nsomewhat limited for the Seer, whose speeches are relatively\nstraightforward, needing only to report inspections from the\nprevious night. The assessment of villagers’ speeches is\ninherently complex due to their limited available informa-\ntion, which is reflected in the minimal rank score differ-\nences observed among the models for this role. In contrast,\nrank score differences are most obvious for werewolves.\nThis disparity stems mainly from the low legality of were-\nwolf speeches, which often inadvertently reveal their iden-\ntity—a critical error as outlined in Appendix Table 3. No-\ntably, GPT3.5 struggles to adhere to instructions that advise\nagainst self-incrimination, while GPT4 demonstrates a more\nsophisticated ability to disguise itself, especially when in-\nduced by the Thinker’s strategic instructions. An example\nspeech is presented in Figure 4.\n4.3. Online Evaluation\nLastly, we conduct online evaluations to assess the over-\nall performance in a real-world gameplay setting, which\ninvolves the five models in the speech generation evalua-\ntion: GPT3.5\/4-LtM, GPT3.5\/4-T and Finetune-T. As Were-\nwolf is a multiplayer imperfect-information game, the skill\nlevel of participants can significantly affect the evaluation\nresults. Therefore, we devise three model combinations,\nwithin which models are randomly and repeatedly selected\nto simulate a nine-player game. We conducted approxi-\nmately 600 rounds for each combination to ensure robust\ntesting results. Given the inherent randomness of outcomes\nin Werewolf, we also calculate the Behavior Score, a typical\nmetric used in Werewolf competitions4. A comprehensive\nbreakdown of the Behavior Score is provided in Table 8.\nTable 1 summarizes the results, revealing that the integra-\ntion of the Thinker module markedly boosts the win rates\nof both GPT3.5 and GPT4 in all three combinations. The\nperformance of the Finetune-T model closely aligns with\nthat of GPT4-T. In terms of Behavior Score, the Thinker\ncontributes substantial improvements across all roles for\nGPT3.5. For GPT4, notable benefits are observed particu-\nlarly for the Witch and Hunter roles. The Behavior Score\nmetric assigns significant weight to the witch’s poisoning\nand the hunter’s shooting decisions, which correlates with\nthe Thinker’s ability to enhance werewolf detection and sub-\nsequently improve these scores. Another finding is that the\ncombination of GPT4s and Finetune-T models results in the\nhighest win rate for the werewolves. This outcome primarily\nstems from the conservative nature of GPT4-LtM in role\nidentification, which makes it more cautious in voting and\n4https:\/\/langrensha.163.com\/20230313\/\n31014_1077578.html\n7\nEnhance Reasoning for Large Language Model in the Game Werewolf\nTable 1: Online evaluation results showcasing the performance of 9 AIs using 5 different models and 3 combinations.\nResults are presented in the format: win rate | Behavior Score.\nMethod\nTotal\nSeer\nWitch\nHunter\nVillager\nWerewolf\nGPT3.5-LtM\n36.7% | −0.21\n25.6% | +0.16\n23.1% | −0.51\n29.9% | −0.21\n30.8% | −0.42\n53.4% | 0.00\nGPT3.5-T\n47.4% | −0.05\n38.3% | +0.27\n41.0% | −0.14\n36.4% | −0.12\n33.8% | −0.18\n68.6% | 0.00\nFinetune-T\n50.3% | −0.06\n38.8% | +0.33\n39.8% | −0.18\n37.0% | −0.29\n39.1% | −0.11\n74.4% | 0.00\nGPT4-LtM\n37.9% | −0.01\n21.9% | +0.25\n18.6% | −0.25\n19.4% | −0.06\n20.3% | −0.00\n73.6% | 0.00\nGPT4-T\n41.1% | −0.02\n20.4% | +0.25\n23.2% | −0.10\n23.9% | −0.09\n22.5% | −0.09\n78.4% | 0.00\nFinetune-T\n43.1% | −0.04\n24.2% | +0.27\n24.6% | −0.15\n23.4% | −0.15\n23.9% | −0.11\n81.4% | 0.00\nGPT3.5-LtM\n33.0% | −0.22\n14.4% | +0.12\n20.4% | −0.46\n20.7% | −0.57\n21.6% | −0.33\n57.0% | 0.00\nGPT3.5-T\n45.0% | −0.07\n33.6% | +0.29\n32.2% | −0.13\n30.4% | −0.17\n27.6% | −0.20\n75.8% | 0.00\nGPT4-LtM\n42.5% | −0.03\n29.8% | +0.27\n22.2% | −0.18\n27.0% | −0.20\n28.7% | −0.04\n71.9% | 0.00\nGPT4-T\n46.3% | −0.05\n28.6% | +0.28\n34.5% | −0.11\n31.5% | −0.08\n28.0% | −0.18\n79.9% | 0.00\nFinetune-T\n45.9% | −0.06\n29.1% | +0.25\n28.3% | −0.16\n29.2% | −0.21\n32.4% | −0.14\n78.0% | 0.00\nskill usage as the \"Good\" faction.\nFurthermore, we incorporate 13 human players to evalu-\nate AI performance against human strategy. We find that\nthe issue of werewolf identity exposure, as mentioned in\nSection 4.2, significantly impedes the game experience of\nhuman players. As a result, participants play alongside four\ninstances each of GPT4-T and Finetune-T models across\n200 game rounds, and the post-filtering process for gener-\nated speeches is adopted in this setting. As can be seen\nin Table 2, human players exhibit no significant win rate\nadvantage, suggesting that the AI’s speeches and actions\ndo not exhibit exploitable weaknesses. Moreover, when\ncompared with the results in Table 1, we note a relative\ndecrease in the werewolves’ win rate in games involving\nhuman players, highlighting the ongoing challenges related\nto identity concealment. Although AI-managed werewolves\nmight convincingly deceive other AI players, human players\noften find them suspicious. A typical example is that were-\nwolves tend to act in groups, such as unanimously voting\nfor Player_3.\nTable 2: Online evaluation win rates with 1 human and 8AIs.\nMethod\nTotal\nGoods\nWerewolves\nGPT4-T\n46.9%\n37.3%\n65.0%\nFinetune-T\n45.3%\n36.0%\n62.6%\nHuman\n40.5%\n35.3%\n59.4%\n5. Discussion and Future Work\nTransfer to other tasks: We use language features and\nspeech instructions in our framework to integrate LLMs and\nexternal reasoning models. The communication format may\nnot be directly transferable to other tasks or domains, and the\neffectiveness depends on the richness of these features and\ninstructions. Future work will aim to develop more general-\nized and flexible methods, e.g., implicit hidden vectors in a\ndata-driven manner, which would offer better transferability\nbut at the expense of interpretability and controllability.\nEvaluation of 8 humans with 1 AI: Our evaluations primar-\nily involved games featuring either AI vs AI or one human\nplayer competing against multiple AIs. Evaluating an AI in\na majority-human player setting presents challenges due to\nthe highly interactive nature of the game and the variability\nin human players’ speech strategies and behaviors.\nInterpretability: While our framework improves the rea-\nsoning capabilities of LLMs, the reasoning processes in the\nThinker module may not be easily interpretable to humans.\nWe explicitly introduce the identity prediction task to reveal\nhow the Thinker think of other players. Future work could\nexplore methods for further improving the interpretability\nand transparency of our framework.\n6. Conclusion\nIn this paper, we introduced a novel framework for inte-\ngrating LLMs with an external Thinker, aiming to enhance\nthe reasoning capabilities of LLM-based agents. This ap-\nproach is inspired by the dual-process theory and separates\nreasoning tasks into two systems: System-1, handled by\nLLMs, and System-2, handled by the Thinker model. We\nshowcased our approach in the context of the Werewolf\ngame, a complex social deduction game requiring language\nprocessing, intuitive thinking, and strategic planning. Our\nresults show that our framework can significantly improve\nthe performance of LLMs and achieve better alignment with\nreal-world scenarios and human preferences. Additionally,\nwe fine-tune a 6B model to surpass GPT4 when integrated\nwith the Thinker. This paper also contributes the largest\n8\nEnhance Reasoning for Large Language Model in the Game Werewolf\ndataset for social deduction games to date, hoping to accel-\nerate further advancements in this field.\nAcknowledgements\nWe thank Jian Yao, Weiming Liu, Qing Wang, Ye Tian,\nZimeng Zhou, Yiming Gao, Liangzhou Wang, Kaiwen Zhu,\nFeiyu Liu, Jianing Shi, Fengming Zhu, Xiaoyu Yang for the\nhuman evaluation of speech generation and online games.\nWe thank Jian Yao, Jianing Shi and Guohua Tang for the\nthoughtful discussion.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 大型语言模型推理能力提升：以狼人杀游戏为案例\n\n## 📌 背景痛点\/本文动机\n随着大型语言模型（LLMs）在自然语言处理（NLP）任务上的突破，其在推理、规划和决策等领域的潜力也逐渐显现。然而，LLMs在处理复杂推理任务时仍面临挑战，尤其是在需要领域特定知识和深度逻辑分析的任务中。本文旨在通过引入外部推理模块，即“思考者”（Thinker），来增强LLM的推理能力，使其在特定任务中表现更佳。\n\n## 🚀 核心方法\n💡 创新点1：双系统推理框架\n本文提出了一个创新的框架，将LLMs与外部Thinker模块相结合，形成了一个推理层次结构。LLMs负责处理直观的System-1任务，如自然语言处理和常识推理，而Thinker则专注于需要复杂逻辑分析和领域特定知识的System-2任务。\n\n💡 创新点2：Thinker模块的设计与训练\nThinker模块直接从数据库中获取知识，并采用各种优化技术进行训练。它通过模仿学习、强化学习和基于群体的训练等方法，学习生成合理的游戏动作和LLM的语音指令。\n\n💡 创新点3：数据集贡献\n本文收集了18,800场真实人类游戏会话数据，构建了迄今为止最大的社交推理游戏数据集，为研究提供了宝贵资源。\n\n## 📈 实验结果\n实验结果表明，引入Thinker模块显著提高了LLMs的推理和生成能力。在狼人杀游戏中，Thinker模块在推理、语音生成和在线游戏评估方面均表现出色。此外，通过将Thinker与一个较小的LLM模型（6B）进行微调，其性能甚至超过了GPT4。\n\n## 💬 可借鉴之处\n本文提出的框架和方法为LLMs在复杂推理任务中的应用提供了新的思路。通过将LLMs与外部推理模块相结合，可以有效提升LLMs在特定领域的推理能力，使其在更多实际应用中发挥更大的作用。此外，本文构建的大规模数据集也为社交推理游戏的研究提供了重要的数据基础。","llm_summary_res_status":200}
{"title":"ADaPT: As-Needed Decomposition and Planning with Language Models","authors":"Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot","summary":"Large Language Models (LLMs) are increasingly being used for interactive\ndecision-making tasks requiring planning and adapting to the environment.\nRecent works employ LLMs-as-agents in broadly two ways: iteratively determining\nthe next action (iterative executors) or generating plans and executing\nsub-tasks using LLMs (plan-and-execute). However, these methods struggle with\ntask complexity, as the inability to execute any sub-task may lead to task\nfailure. To address these shortcomings, we introduce As-Needed Decomposition\nand Planning for complex Tasks (ADaPT), an approach that explicitly plans and\ndecomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute\nthem. ADaPT recursively decomposes sub-tasks to adapt to both task complexity\nand LLM capability. Our results demonstrate that ADaPT substantially\noutperforms established strong baselines, achieving success rates up to 28.3%\nhigher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel\ncompositional dataset that we introduce. Through extensive analysis, we\nillustrate the importance of multilevel decomposition and establish that ADaPT\ndynamically adjusts to the capabilities of the executor LLM as well as to task\ncomplexity.","url":"http:\/\/arxiv.org\/abs\/2311.05772v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2311.05772v2","published":1699466355000,"comment":"NAACL 2024 (findings) camera-ready. Project Page:\n  https:\/\/allenai.github.io\/adaptllm","pdf_text":"ADAPT: As-Needed Decomposition and Planning with Language Models\nArchiki Prasad♣\nAlexander Koller♡\nMareike Hartmann♡\nPeter Clark♠\nAshish Sabharwal♠\nMohit Bansal♣\nTushar Khot♠\n♣UNC Chapel Hill\n♠Allen Institute for AI\n♡Saarland University\nAbstract\nLarge Language Models (LLMs) are increas-\ningly being used for interactive decision-\nmaking tasks requiring planning and adapt-\ning to the environment.\nRecent works em-\nploy LLMs-as-agents in broadly two ways:\niteratively determining the next action (itera-\ntive executors) or generating plans and execut-\ning sub-tasks using LLMs (plan-and-execute).\nHowever, these methods struggle with task\ncomplexity, as the inability to execute any\nsub-task may lead to task failure.\nTo ad-\ndress these shortcomings, we introduce As-\nNeeded Decomposition and Planning for com-\nplex Tasks (ADAPT), an approach that explic-\nitly plans and decomposes complex sub-tasks\nas-needed, i.e., when the LLM is unable to ex-\necute them. ADAPT recursively decomposes\nsub-tasks to adapt to both task complexity and\nLLM capability. Our results demonstrate that\nADAPT substantially outperforms established\nstrong baselines, achieving success rates up to\n28.3% higher in ALFWorld, 27% in WebShop,\nand 33% in TextCraft – a novel compositional\ndataset that we introduce. Through extensive\nanalysis, we illustrate the importance of multi-\nlevel decomposition and establish that ADAPT\ndynamically adjusts to the capabilities of the\nexecutor LLM as well as to task complexity.1\n1\nIntroduction\nRecent advances in Large Language Models\n(LLMs) have expanded their application beyond\nconventional NLP tasks to more complex tasks\ninvolving mathematical, symbolic, and common-\nsense reasoning (Wei et al., 2022; Huang and\nChang, 2023). Recent models have even been ap-\nplied to decision-making tasks, such as performing\nhousehold chores, navigating a webpage, etc., that\nrequire interactions with external environments or\ntools (Yao et al., 2023b; Qin et al., 2023).\n1Project: https:\/\/allenai.github.io\/adaptllm\nPrior works on using LLMs for decision-making,\nsuch as ReAct (Yao et al., 2023b), iteratively gen-\nerate the next action to be executed in the environ-\nment given the history of actions and observations\n(see Fig. 1; top-left). However, as the tasks become\nmore complex, LLMs struggle due to their limited\ncomposition ability (Dziri et al., 2023) and inability\nto deal with the distractors (Shi et al., 2023) in a\nlong action-observation trajectory.\nTo mitigate this, modular approaches (Khot et al.,\n2023; Yang et al., 2023; Sun et al., 2023) incorpo-\nrate a separate planner module that utilizes an LLM\nto create a high-level plan.2 The planner then dele-\ngates simpler sub-tasks to an executor LLM module\nthereby reducing the compositional complexity and\nlength of action trajectory required by the execu-\ntor. We refer to this category broadly as plan-and-\nexecute approaches (see Fig. 1; top-right). While\nthe plans enable these methods to guide the execu-\ntion and track progress (Wang et al., 2023b), their\nnon-adaptive nature poses a limitation when con-\nfronting unachievable sub-tasks. These approaches\ninherently lack the flexibility to adapt to task com-\nplexity and manage execution failures, as shown\nin Fig. 1(top-right), where just one sub-task that is\ntoo complex results in overall task failure.\nTo address such failures, we propose As-Needed\nDecomposition and Planning for complex Tasks\n(ADAPT), a recursive algorithm that further de-\ncomposes sub-tasks when necessary, to dynami-\ncally accommodate to task complexity. We uti-\nlize separate planner and executor LLM modules\nwithin our framework but only decompose a task\nusing the planner, if the executor LLM detects a\nfailure. As shown in Fig. 1, the overall task of\nputting a clean mug on a desk in an unfamiliar\n2By “planning”, we refer to the colloquial concept of\ndesigning a list of sub-tasks to accomplish a complex task\nrather than its usage in classical AI-planning literature. E.g.,\na “plan” for preparing a lasagna could be to cook the pasta,\nprepare the sauce, layer the ingredients, and then bake it.\narXiv:2311.05772v2  [cs.AI]  8 Apr 2024\n> Go to countertop 1.\nYou reached loc 1...\n> Go to cabinet 12.\n...\n> Think: Mug not found.\nTask failed!\nYou reached loc 20 ...\nIterative Executor (ReAct)\nPlan:\nStep 1: Find and\ntake the mug AND\nStep 2: Clean the\nmug AND\nStep 3: Put the\nclean mug on desk\nPlan-and-Execute\n   ADaPT (Recursive Decomposition, As-needed)\nTask: Put a clean mug on desk.\nNot Executed\nExecute: Task\nPlan: \nStep 1: Find and take the mug AND\nStep 2: Clean the mug AND\nStep 3: Put the clean mug on desk\nExecute: Step 1\nPlan: \nStep 1a: Find and take the mug from countertops OR\nStep 1b: Find and take the mug from cabinets OR\nExecute: Step 1a\nExecute: Step 1b\nExecute: Step 3\nExecute: Step 2\n...\nOn execution failure,\ndecompose further\nSuccessful sub-task allows\nexecution to resume\nExecute:\nStep 1\nExecute:\nStep 2\nExecute:\nStep 3\nOR\nADaPT(Task)\nADaPT(Step1)\nFigure 1: Top-Left: Iterative executors such as Re-\nAct (Yao et al., 2023b) interact directly with the envi-\nronment, performing planning implicitly. Top-Right:\nPlan-and-Execute, e.g., Yang et al. (2023), creates a\nfixed plan for the task, without accounting for complex-\nity in executing step 1. Bottom: ADAPT dynamically\ndecomposes based on success of the executor.\nhousehold is too complex for the model, leading to\nfailure of the iterative executor. While a plan-and-\nexecute-style approach initially breaks down the\ntask into three sub-tasks, it falls short in accounting\nfor the complexity in finding a mug. Moreover, it is\nchallenging to anticipate the difficulty of such a sub-\ntask in advance, as the executor could find a mug\nin the first attempt or in an obscure location. There-\nfore, ADAPT employs its recursive structure to\ndynamically adapt to execution failures (assessed\nby LLMs), by further decomposing the complex\nsub-task of finding a mug via the planner.\nEmpirically, we demonstrate the effectiveness\nof ADAPT on three datasets involving interactive\nenvironments: ALFWorld (Shridhar et al., 2021),\nWebShop (Yao et al., 2022), and a new compo-\nsitional text game for crafting Minecraft recipes\ncalled TextCraft (Sec. 4.1). Using GPT-3.5 as the\nunderlying LLM, ADAPT outperforms strong base-\nlines (discussed in Sec. 4.2) such as ReAct (Yao\net al., 2023b), and Plan-and-Solve (Wang et al.,\n2023b) by up to 28.3%, 27%, and 33% absolute\npoints on ALFWorld, WebShop, and TextCraft re-\nspectively (Sec. 5). Compared to Reflexion (Shinn\net al., 2023), an adaptive approach that addresses\nfailures in the full task trajectory, ADAPT yields\nhigher success rates by 14.1%, 9%, and 20% on\nALFWorld, WebShop, and TextCraft respectively.\nThrough extensive analysis of ADAPT, we es-\ntablish the importance of recursive decomposi-\ntion (Sec. 6.1) and showcase dynamic adaptation\nto the capabilities of the executor LLM includ-\ning open-source models such LLaMA-2 (Touvron\net al., 2023) and Lemur (Xu et al., 2023) in Sec. 6.2.\nLastly, we demonstrate that ADAPT incorporates\ntask complexity (Sec. 6.3), where the extent of re-\ncursive decomposition aligns with the inherent task\ncomplexity. To summarize, our contributions are:\n1. We present ADAPT, a recursive algorithm that\ndynamically decomposes complex sub-tasks on\nan as-needed basis, i.e., intervening only if the\ntask is too complex for the executor.\n2. On three diverse datasets, ALFWorld, WebShop,\nand TextCraft, ADAPT improves success rate\nof GPT-3.5 over previous approaches by up to\n28.3%, 27%, and 33% points respectively.\n3. Analysis of ADAPT underscores the signifi-\ncance of recursive decomposition and the ability\nto adapt dynamically to varying LLM execution\ncapabilities and task complexities.\n2\nRelated Work\nLLMs for Decision-Making.\nLLMs have been\nsuccessfully used as agents to perform a wide vari-\nety of decision-making tasks such as robotic nav-\nigation (Ahn et al., 2022; Huang et al., 2023b;\nSingh et al., 2023), complex multi-modal games\nlike Minecraft (Fan et al., 2022; Wang et al., 2023a),\ntext-based environments (Shridhar et al., 2021; Liu\net al., 2023). While most of these works focus\non learning from trajectories, ReAct (Yao et al.,\n2023b) uses few-shot prompting to build an agent\nthat reasons about the current state (thoughts) and\ngenerates the next action in the environment, given\nprior actions and observations. Their iterative ap-\nproach (shown in Fig. 1; top-left) can handle fail-\nures, but they have to keep track of the entire plan\nimplicitly while deciding every local action (c.f.\nADAPT in Fig. 9 of Appendix A). By incorporat-\ning planning and execution into separate modules\nand enabling dynamic adaptation we are able to\nachieve higher success rates (refer to Sec. 5).\n2\nSeveral follow-up works improve upon the Re-\nAct framework by incorporating feedback in future\ntrials (Madaan et al., 2023; Shinn et al., 2023), or\nusing LLMs to develop heuristics for search (Yao\net al., 2023a; Zhou et al., 2023). In contrast to\nADAPT, they do not employ task decomposition,\nleading to unnecessary computation as they explore\nmultiple trajectories or trials for the whole task,\neven though the LLM struggles with just one sub-\ntask. Such works are complementary to ADAPT\nas they can be incorporated within the planner or\nexecutor modules to strengthen LLM performance\n(just like they are incorporated in ReAct).\nDecomposition and Modularity.\nOur work fol-\nlows extensive literature in NLP on decomposing\ntasks into neural modules (Andreas et al., 2016;\nGupta et al., 2019; Jiang and Bansal, 2019) or\nseq2seq models (Min et al., 2019; Talmor and Be-\nrant, 2018; Khot et al., 2021; Perez et al., 2020;\nSaha et al., 2023b). With the advent of few-shot\nprompted black-box LLMs, this paradigm of pro-\ngrammatic decomposition into LLMs has become\nmore popular (Yao et al., 2023b; Khot et al., 2023;\nWang et al., 2023b, inter alia), referred to as LLM\nPrograms (Schlag et al., 2023; Dohan et al., 2022).\nAdditionally, past works in program synthesis (Mu-\nrali et al., 2018; Nye et al., 2019; Zheng et al., 2023)\nalso employ task decomposition via generating a\n“program sketch” prior to program generation.\nADAPT not only decomposes tasks via the plan-\nner module and delegates them to the executor\nmodule but also automatically adapts to execu-\ntor failures by further decomposing complex tasks\nas-needed. This dynamic capability distinguishes\nADAPT from prior works with a non-adaptive\nstructure. ADAPT extends the recursive and hi-\nerarchical decomposition in Khot et al. (2023), en-\nabling inter-module communications, and robust\nstrategies for execution failures, excelling in real-\nworld textual environments like online shopping.\nHierarchical Problem Solving.\nIn AI problem-\nsolving, there is a longstanding tradition of hi-\nerarchical task decomposition employed in plan-\nning (Ghallab et al., 2004; Georgievski and Aiello,\n2014; Höller et al., 2020), reinforcement learning\n(Sutton et al., 1999; Barto and Mahadevan, 2003;\nNachum et al., 2018; Zhang et al., 2021), and navi-\ngation (She et al., 2014; Sharma et al., 2022; Blukis\net al., 2022; Min et al., 2022; Song et al., 2023).\nThese approaches, such as Hierarchical Task Net-\nworks (Erol et al., 1994), leverage domain knowl-\nedge, e.g., hand-specified library of plans, to break\ncomplex problems into simpler tasks. Our work em-\nbraces this tradition but distinguishes itself by ex-\nploring how LLMs can autonomously decompose\ntasks by leveraging their extensive world knowl-\nedge, without predefined plan libraries. Lastly,\nADAPT performs dynamic hierarchical planning\nby employing its recursive structure.\n3\nMethodology\nWe introduce As-Needed Decomposition and\nPlanning for complex Tasks (ADAPT), a mod-\nular approach for decision-making that integrates\nan LLM as an executor and a planner (Secs. 3.1\nand 3.2) within an LLM program called the con-\ntroller (Sec. 3.3). In Fig. 1, when ADAPT is given\na complex task, it first attempts to accomplish the\nentire task by running the executor iteratively, and\nresorting to the LLM planner for further decom-\nposition into sub-tasks if the executor fails. Sub-\nsequently, ADAPT is recursively called for each\nsub-task to ensure their successful completion, ulti-\nmately leading to overall task success.\n3.1\nLLM as an Executor\nOverview.\nIn a given environment, the executor\nis provided with a concise natural language task\nspecification, as shown in Fig. 2 (left). Following\nYao et al. (2023b), the executor iteratively interacts\nwith the environment via actions generated by the\nLLM. This interaction continues until the task is\neither completed or a preset maximum iteration\nlimit is reached. Consistent with Ahn et al. (2022),\nwe provide the LLM with in-context demonstra-\ntions of low-level “atomic” skills specific to the\nenvironment (listed in Table 5 of Appendix A),\nsuch as knowing how to correctly heat objects in\nALFWorld. This approach offers two advantages:\n(i) it allows us to employ the same executor with\nenvironment-specific knowledge for all baselines\n(Sec. 4.2); and (ii) it enables the planner (discussed\nin Sec. 3.2) to work at a higher level of abstraction,\nleveraging the LLM’s general world knowledge.\nExecution Capabilities of an LLM.\nAt a min-\nimum, the LLM executor should reliably execute\natomic skills. While we provide demonstrations for\nsuccessful execution of atomic skills, LLMs can\nadapt to failures by combining multiple skills to\nperform complex tasks, as discussed in Sec. 6.2.\nFor instance, in Fig. 2 (left), we show the LLM suc-\ncessfully cleaning a mug it’s carrying (an atomic\n3\n[LLM] Think:  Input assumption: I am carrying a\nmug. Now I need to verify this.\n[LLM]> inventory\n[LLM]> go to sinkbasin 1\n[LLM]> clean mug 1 with sinkbasin 1\n[LLM] Think: I cleaned the mug.Task completed!\nADaPT(Task, k)\nExecutor(Task)\nSuccess?\nPlanner(Task)\nStep 2\nStep 1\nStep 3\nADaPT(Step1,k+1)\nADaPT(Step2,k+1)\nADaPT(Step3,k+1)\nTask: Put a clean mug on desk\nTrue\nFalse\nLogic\nTrue\nFalse\nClean the mug\nExecutor(           )\nOK.\nYou are carrying: a mug 1\nYou reached loc 13, you see ...\nYou clean mug 1\n# Think: To do this task, I first need to find a\nmug, then clean, it and put it on the desk. I\nneed to perform these tasks sequentially.\nStep 1: Find and take a mug AND\n# Think: Now I found a mug, I will clean it.\nStep 2: Clean the mug with sinkbasin AND\n# Think: Now I cleaned the mug, I will put\nthe clean mug on the desk.\nStep 3: Put clean mug on desk\n LLM\nPlanner(           )\nPut a clean\nmug on desk\nController\nAND\nFigure 2: Block diagram of the ADAPT pipeline with an example from ALFWorld. Left: Use of LLM as an\nexecutor to interact iteratively with the environment along with an example execution trajectory. Middle: Overall\nrecursive algorithm (depth k ≤dmax) that embeds the executor and planner, refer to Algorithm 1 for details. Right:\nOutline of using LLM as a planner to generate sub-tasks (steps) and logical operators combining them.\nskill). An advanced executor could combine “find-\ning a mug” with the “cleaning” skill to accomplish\n“find a clean mug” without an explicit planner.\nSelf-generated Success Heuristic.\nIn order to\ndecompose based on the abilities of the executor,\nwe need to determine whether the executor is capa-\nble of finishing the given (sub-)task independently\nor if further decomposition is required. To this\nend, we employ the executor LLM to determine\nthe completion of the (sub-)task without relying\non the environment for obtaining gold rewards for\n(sub-)tasks. We include a simple instruction in the\nexecutor prompt to output “task completed” if it de-\ntermines it has succeeded, otherwise output “task\nfailed” in case it cannot proceed. Refer to example\nin Fig. 2 (left). Our success heuristic aligns with bi-\nnary classification models employed in Shinn et al.\n(2023), providing a way to simulate intermediate\nrewards, which complements end-of-task environ-\nment rewards (Rengarajan et al., 2022). We study\nthis LLM-generated heuristic in Appendix F and\nshow that it closely matches the gold reward.\n3.2\nLLM as a Planner\nOverview.\nThe objective of the planner is to\nbreak down complex tasks into smaller sub-tasks.\nTo achieve this, we instruct the LLM to generate\na concise yet comprehensive plan consisting of a\nfew steps, typically 3-5, as shown in Fig. 2 (right).\nWe opt for shorter, more abstract plans because ex-\npecting a detailed, fine-grained plan upfront can be\nimpractical, especially in unexplored environments.\nE.g., devising a 10-step plan to put a clean mug\non a desk without prior knowledge of the mug’s\nlocation can lead to cascading errors due to incor-\nrect assumptions. Therefore, we task the LLM to\ngenerate short plans, with the flexibility to decom-\npose further in subsequent iterations, based on the\nexecutor’s capabilities.\nComposition Logic for Sub-tasks.\nAlong with\nthe sub-tasks, we prompt the planner to generate\nlogical operators to combine various sub-tasks in\nthe plan to accomplish the task. We allow for two\nlogical operators: “AND” and “OR”. Sub-tasks\nare linked using AND when they must be executed\nsequentially for the task to succeed. However, in\ncases requiring exploration, such as finding an item\nin an unknown room, we employ the OR operator\nto simulate conditional checks. Here, the task suc-\nceeds if any of the sub-tasks are successful. For\ninstance, in Fig. 1, the plan to “find a mug” would\nbe to “find a mug on the countertop” OR “find a\nmug in the cabinet”. We execute the latter only\nif the agent has not found the mug yet. While ex-\namples in Figs. 1 and 2 show homogeneous logic,\nADAPT can handle complex logical expressions\nas described in Appendix B.\n3.3\nController – LLM Program\nOverall Pipeline.\nThus far, we describe two\nLLM-based modules that can perform the roles\nof low-level execution and high-level planning. We\nincorporate these modules into ADAPT via the\ncontroller which is a pre-determined and recursive\nalgorithm – making the overall pipeline of ADAPT\nan LLM program (Schlag et al., 2023; Dohan et al.,\n2022), shown in Algorithm 1. The overall flow of\nthe controller program is as follows: (i) given an\ninput task, the controller calls the executor to check\nif it can succeed in performing the task directly; (ii)\n4\nif the executor does not succeed, the controller dele-\ngates decomposing the complex task to the planner\nand recursively calls ADAPT for each sub-task un-\ntil we hit a termination criterion, i.e., if a maximum\ndepth dmax (≥1) is reached.\nFig. 2 (mid) shows the control flow of ADAPT.\nA complex task such as “put a clean mug on the\ndesk” is first assigned to the executor. If the execu-\ntor does not succeed, then ADAPT calls the planner\nto decompose the task into sub-tasks along with a\nlogical operator (AND or OR) indicating how to\ncompose them. Each sub-task (referred to as ‘step’\nin Fig. 2) is then assigned recursively to ADAPT\nand is combined using the logical operator. In the\nend, the success of sub-tasks after recursive decom-\nposition ensures overall task success (unrolled calls\nto planner and executor are shown in Fig. 1).\n4\nExperimental Setup\nWe describe the datasets used in our experiments\nand baselines used for comparison with ADAPT.\n4.1\nDatasets\nWe employ LLMs-as-agents to perform tasks in the\nfollowing three environments and use task success\nrate as our evaluation metric in Secs. 5 and 6.\nALFWorld.\nALFWorld (Shridhar et al., 2021)\nis a text-based game version of the embodied AL-\nFRED benchmark (Shridhar et al., 2020) imple-\nmented in the TextWorld environment (Côté et al.,\n2019). It encompasses 6 distinct task types, where\nan agent is required to accomplish high-level tasks\nthrough navigation and interaction via text-based\nactions in a simulated household that gives textual\nfeedback to an agent (e.g., put a clean mug on desk\ndiscussed earlier in Fig. 2). Following Shridhar\net al. (2021), we present results on 134 unseen eval-\nuation games (test set) with a separate dev set of\n10 games per task from the seen evaluation games\nsplit. Along with atomic skills, we add example\ngold trajectories, following Yao et al. (2023b), for\ntwo tasks: heat and look in the executor prompt.3\nWebShop.\nWebShop (Yao et al., 2022) is an on-\nline shopping website environment featuring 1.18\nmillion real-world products containing 500 user\nqueries in the test set.\nIt serves as a complex\n3Unlike Yao et al. (2023b), we use a standardized executor\nprompt for all ALFWorld tasks, avoiding the agent to know the\ntask-type apriori. Table 6 in Appendix C further demonstrates\nthat ADAPT still improves over task-specific executors.\nCrafting Commands:\ncraft 4 oak planks using 1 oak log\ncraft 1 honeycomb block using 4 honeycomb\ncraft 1 beehive using 6 planks, 3 honeycomb\nGoal: craft beehive\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] craft 1 beehive using 6 oak planks, 3 honeycomb \n[Env] Crafted 1 minecraft:beehive\n...\n1x\n1x\n4x\n4x\n3x\n1x\nFigure 3: Example gold trajectory in TextCraft for a\ntask with recipe depth of 2.\ndecision-making environment with practical appli-\ncations wherein an agent must navigate a website\nthrough a variety of commands to purchase an item\nmatching a user specification (e.g., grey sectional\nsofa priced less than $300 with fast delivery). Fol-\nlowing Shinn et al. (2023), we report performance\non 100 user instructions and use a different subset\nof 40 queries as the dev set.\nTextCraft.\nWe create a new text-only environ-\nment for crafting Minecraft4 items similar to Word-\nCraft (Coenen et al., 2021). Unlike existing agent-\nbased environments, tasks in TextCraft exhibit a\nnatural compositional structure, resembling cook-\ning recipes with steps of varying complexity, where\nsome sub-tasks are more intricate, such as layering\na lasagna, while others are simpler, like baking it.\nTasks in TextCraft are inherently decomposable.\nIn Fig. 3, crafting a beehive necessitates crafting\nits ingredients, like planks and honeycomb, which\nmay require further decomposition. The agent thus\nneeds to identify and adapt to varying task com-\nplexity, e.g., crafting a plank is easier than crafting\na beehive. Moreover, some recipes allow using any\nitem from a particular category. For instance, craft-\ning a beehive uses planks (a category), requiring\nthe agent to use linguistic knowledge for proper\nitem selection (e.g., select oak planks, a specific\nitem in the category planks). We evaluate our ap-\nproach on a test set of 200 tasks where the target\nitems have recipe trees of depth 2, 3, and 4 (exam-\nple tree of depth 2 is shown in Fig. 3). We use the\n4https:\/\/www.minecraft.net\n5\nMethod (dmax = 3)\nPick Clean Heat Cool Look Pick2\nAll\nReAct\n33.3\n67.7\n43.5\n33.3\n55.6\n11.8\n43.3\nPlan-and-Execute\n29.2\n61.3\n47.8\n38.1\n61.1\n11.8\n43.3\nTry Again with ReAct 50.0\n51.6\n60.8\n47.6\n61.1\n5.9\n47.8\nReflexion\n70.8\n61.3\n61.0\n66.7\n61.1\n5.9\n57.5\nADAPT (Ours)\n87.5\n80.6\n60.8\n76.2\n61.1\n52.9\n71.6\nTable 1: ADAPT yields the highest the overall success rates (%)\ncompared to baselines from prior work (discussed in Sec. 4.2) on\nALFWorld (test split). Best (highest) success rates are highlighted\nin bold and second-highest rates are underlined.\nMethod\nWebShop TextCraft\nReAct\n32.0\n19.0\nPlan-and-Execute\n17.0\n27.0\nTry Again with ReAct\n30.0\n15.0\nReflexion\n35.0†\n32.0\nLATS (Zhou et al., 2023)\n38.0†\n−\nADAPT (Ours)\n44.0\n52.0\nTable 2: ADAPT yields the highest success\nrate on WebShop and TextCraft (test split) with\ndmax = 3 and 4 respectively. †Performance\nreported by Zhou et al. (2023)\nitems with recipe tree depth of 3 (123 tasks), depth\nof 4 (11 tasks) and depth of 2 (77 out of 297) in\nour test set, and the rest of depth 2 tasks constitute\nthe dev set. Additional details about creating the\nenvironment are present in Appendix E.\n4.2\nBaseline Approaches\nWe compare ADAPT with four classes of baseline\napproaches described below.\nIterative Executor-Only (ReAct).\nIn this setting,\nwe employ the executor to interact iteratively with\nthe environment, adopting the think-act-observe\nprompting style from ReAct (Yao et al., 2023b). All\nmethods discussed below, including ADAPT, share\nthe same executor, ensuring a standardized impact\nof the executor’s strength and design choices when\ncomparing relative performance in Sec. 5. When\ndmax =1, ADAPT solely relies on this executor.\nPlan-and-Execute.\nAs shown in Fig. 1, in this\nsetting, we generate a plan first and then assign\neach sub-task to the executor. This approach only\nplans once and as a result has a non-adaptive struc-\nture (consistent with Wang et al. (2023b); Yang\net al. (2023); Sun et al. (2023)). To ensure each\nplan step is executable without further decompo-\nsition, we design new prompts with more detailed\nplans. Note that ADAPT with dmax = 2 differs\nfrom plan-and-execute as it is adaptive, i.e., de-\ncomposes only when executor fails and generates\nrelatively shorter plans (refer to Appendix B).\nTry Again with ReAct.\nBy design, ADAPT\nmakes multiple calls to the executor module, al-\nbeit with different (sub-)tasks. Like Yang et al.\n(2023), we design a simple controller that requests\nthe executor to retry the task in a total of dmax\nseparate trials and then uses the trial with the best\nperformance for each task instance.\nReflexion.\nShinn et al. (2023) execute the en-\ntire task first, and if unsuccessful, reflect and store\nfeedback in memory for subsequent dmax−1 trials.\nWhile adaptive, this approach repeats the entire\ntrial even if a single sub-task fails, redundantly\nre-executing previously successful sub-tasks.\nADAPT and Shared Implementation Details.\nFollowing (Yao et al., 2023b; Shinn et al., 2023;\nZhou et al., 2023), by default, we use the GPT-\n3.5 (Ouyang et al., 2022) LLM for both planning\nand execution in ADAPT and other baselines. We\nuse the completion-based models for ALFWorld\nand TextCraft and the chat-based model for Web-\nShop.5 Further, we use ADAPT (and other base-\nlines) with dmax =3 for ALFWorld, and WebShop\nand increase to dmax =4 for TextCraft to accommo-\ndate recipes with a depth of 4 (Sec. 4.1). For addi-\ntional details, refer to Appendix A. We increase the\nmaximum number of iterations for the ReAct base-\nline by a factor of dmax and ensure all baselines\nuse a comparable number of LLM calls (Sec. 6.5).\n5\nMain Results\nUsing GPT-3.5 as the underlying LLM, in this sec-\ntion, we show that ADAPT yields the highest suc-\ncess rate compared to baselines from prior work on\nALFWorld, WebShop, and TextCraft datasets.\nALFWorld.\nIn Table 1, we observe that ADAPT\nachieves the highest overall success rate, while\nusing ReAct alone results in the lowest overall\nperformance. By leveraging adaptive decomposi-\ntion, ADAPT improves over ReAct’s performance\nby 28.3% points (absolute) as well as over Plan-\nand-Execute and Try Again by 28.3% and 23.8%\npoints, respectively. Lastly, we find that ADAPT\n5We use the completion model as chat variants of GPT-3.5\nconsistently underperform their completion counterparts (Liu\net al., 2023; Yang et al., 2023). We discuss the effectiveness\nof ADAPT different LLMs in Sec. 6.2.\n6\n1\n2\n3\nValue of dmax in ADaPT\n20\n40\n60\nSuccess Rate\nALFWorld\nWebShop\nTextCraft\nFigure 4: Success rate of ADAPT increases with the\nmaximum depth dmax for all datasets (dev splits).\nyields 14.1% points higher overall success rate\nthan Reflexion, despite the latter having access\nto dedicated memory and natural language feed-\nback. Specifically, we find baselines yield poor\nresults on ‘pick2’ tasks (<12% success rate) as\nthey require the agent to compose two ‘pick’-style\ntasks involving a longer action history. However,\nADAPT yields significant improvements (by over\na factor of 4×) for this type of tasks.\nWebShop.\nTable 2 shows a similar trend with\nADAPT surpassing all baselines and achieving\nthe highest success rate. ADAPT outperforms Re-\nAct, Plan-and-Execute, and Try-Again baselines\nby up to 27% points. We corroborate the findings\nof Shinn et al. (2023) and observe that natural lan-\nguage feedback offers limited gains in performance,\nas compared to ADAPT (which surpasses Reflex-\nion by 9% points). Additionally, we compare with\na recent search-based baseline LATS (Zhou et al.,\n2023) and find that ADAPT outperforms the suc-\ncess rate of LATS by 6% points.\nTextCraft.\nOur results on TextCraft are summa-\nrized in Table 2. First, we observe that ADAPT\nachieves an improvement of 33% compared to the\nReAct executor. In contrast to Plan-and-Execute,\ni.e., starting with a fixed plan, having the dynamic\nability to adapt to complex sub-tasks (in this case,\ncrafting complex ingredients) in ADAPT improves\nperformance by 25% points. Lastly, ADAPT out-\nperforms Reflexion by 20% points, highlighting\nthe importance of adaptive and as-needed planning.\nWe hypothesize that ADAPT consistently outper-\nforms Reflexion across datasets as the latter relies\non generating feedback based on errors in the entire\ntrajectory. In contrast, due its design, ADAPT of-\nten handle failures of small sub-tasks and redirects\nmore resources in the form of calling the planner\nTask-Specific\nHybrid\nAtomic\nExecutor Setting\n0\n20\n40\n60\nSuccess Rate\nExecutor-Only\nADaPT (dmax=3)\nFigure 5: ADAPT improves success rates across varying\nsettings capturing different executor capabilities (i.e.,\nexecutor-only performance) on ALFWorld (dev).\nand decomposition to the challenging sub-tasks.\n6\nAnalysis and Discussion\nWe analyze ADAPT in detail by addressing the\nfollowing research questions on dev data splits.\n6.1\nHow does performance of ADAPT scale\nwith the depth of decomposition?\nSetup.\nTo assess the impact of adaptive decom-\nposition, we study ADAPT under three settings\nwith increasing maximum depth dmax ∈{1, 2, 3}\nfor ALFWorld, WebShop, and TextCraft. Note\nthat dmax = 1 setting corresponds to the iterative\nexecutor-only baseline (ReAct).\nResults.\nFig. 4 shows that across all datasets, per-\nformance of ADAPT scales with increasing the\nmaximum depth dmax. Consistently, we find a sig-\nnificant improvement in success rates as we move\nfrom dmax =1 to dmax =2, i.e., adding the planner\nto decompose a complex task when executor fails\nproves to be effective. Finally, the performance\nincrease from dmax = 2 to dmax = 3 validates our\nhypothesis that some sub-tasks are difficult for the\nLLM to directly execute successfully, and decom-\nposing these further boosts overall performance.\n6.2\nDoes ADAPT cater to different execution\ncapabilities of LLMs?\nSame LLM, different execution capabilities.\nWe run ADAPT on three different executor\nprompts on ALFWorld: (i) task-specific gold tra-\njectories, (ii) atomic skills and common gold-\ntrajectories for 2 tasks used in Sec. 5 (hybrid), and\n(iii) only atomic skills. Using gold trajectories\naligns closely with the task at inference-time and\nthus, should exhibit high performance. In contrast,\n7\nAlfWorld\nWebShop\nTextCraft\n20\n40\n60\n80\n100\nSuccess Rate\nGPT-3.5\nGPT-4\nLemur\nLLaMA\nwith ADaPT\nFigure 6: ADAPT improves (test) performance of GPT-3.5, GPT-4, LLaMA, and Lemur LLMs across datasets.\nexecutor using only atomic skills relies on the in-\nherent composition abilities of the LLM, yielding\nweaker performance. Here we examine if ADAPT\ncan improve success rates for all three settings.\nResults.\nIn Fig. 5, we observe that ADAPT con-\nsistently improves over the executor-only baseline\nfor all diverse executor settings. As expected, the\nexecutor prompted with task-specific trajectories\nperforms the best (left), while the executor with\nonly atomic skills performs the worst (right). No-\ntably, ADAPT substantially improves performance\nof the relatively weak executor, improving success\nrate from 3.3% to 41.7%.\nADAPT with different LLMs.\nWe study the\nability of ADAPT to improve performance across\ndifferent LLMs (as planners and executors): (i)\nGPT-3.5, (ii) GPT-4 (OpenAI, 2023), (iii) LLaMA-\n2 70B (Touvron et al., 2023), and (iv) Lemur\n70B (Xu et al., 2023) on test splits of all datasets.\nResults.\nFig. 6 shows that ADAPT consistently\nimproves downstream performance for all models\nacross all three datasets. Consistent with Liu et al.\n(2023), we find that the gated GPT models outper-\nform the open-source models based on absolute\nsuccess rates. Nevertheless, ADAPT is effective\nacross LLMs and improves performance of GPT-\n4, the strongest LLM, by up to 37%, as well as\nLLaMA, the least performant LLM, by up to 15%\non the TextCraft dataset.\n6.3\nDoes ADAPT handle task complexity?\nSetup.\nBy the compositional design of TextCraft,\ncomplexity of each task in the dataset can be de-\nfined with respect to the depth of the crafting recipe,\ni.e., recipes with higher depth would be more com-\nplex to craft. We evaluate efficacy of ADAPT\nand the ReAct baseline on the test set of TextCraft\nMethod\nRecipe Depth kmax Success Rate\nReAct\n2\n1.0\n26.9\nADAPT (dmax = 4)\n2\n1.9\n78.2\nReAct\n3\n1.0\n1.8\nADAPT (dmax = 4)\n3\n2.8\n38.7\nTable 3: ADAPT improves TextCraft (test) performance\neven as recipe depth increases. The maximum decom-\nposition depth used by ADAPT to succeed at the task\n(kmax) also scales with the recipe depth.\nwith increasing recipe depth.6 Furthermore, while\nwe provide ADAPT with a maximum budget of\ndmax = 4, we study how the maximum decompo-\nsition depth utilized by ADAPT to succeed (kmax)\nvaries with task complexity.\nResults.\nIn Table 3 we observe that ADAPT im-\nproves success rates for games with recipe depth of\n2 from 26.9% to 78.2%, and of depth 3 from 1.8%\nto 38.7% as compared to the ReAct baseline. As\nexpected, the executor alone is unable to handle\ncomplex recipes with depth ≥3, but with the help\nof ADAPT the performance improves significantly.\nAdditionally, given the same budget dmax = 4, as\nthe recipe depth (complexity) increases from 2 to\n3, ADAPT’s level of decomposition (kmax) also\nincreases from 1.9 to 2.8. This showcases that\nADAPT leverages as-needed decomposition in or-\nder to handle task complexity.\n6.4\nCan we use different planner and executor\nLLMs within ADAPT?\nSetup.\nThe planner and executor modules of\nADAPT do not need to necessarily use the same\nunderlying model. Following, Lin et al. (2023) we\nexplore if a relatively smaller LLM can be used to\nperform local actions in the executor and a more\n6As we have only 11 tasks with recipe depth of 4, we\nexclude them from this analysis.\n8\nExecutor LM\nPlanner LM\nSuccess Rate\nGPT-3.5\n−\n38.4\nGPT-3.5\nGPT-3.5\n58.3\nLLaMA-2-70B\n−\n20.4\nLLaMA-2-70B\nGPT-3.5\n43.3\nTable 4: ADAPT improves performance on ALFWorld\n(dev) when using different planner and executor LLMs.\nadvanced LLM be used to devise plans. To this\nend, we explore different combinations of planner\nand executor LLM, with the latter using both gated\nand open-source models on ALFWorld.\nResults.\nTable 4 shows that ADAPT can suc-\ncessfully be used to generate plans from one LLM\nthat are useful to a different, possibly smaller, ex-\necutor LLM, improving success rates by up to\n19.9% compared to the executor-only (ReAct) set-\nting. Interestingly, using an open-source model,\nsuch as LLaMA-2-70B-chat (Touvron et al., 2023)\ncan be used as an executor with a more advanced\nLLMs such as GPT-3.5 to improve success rates\nby 22.9% points. Since the planner LLM is used\nsparingly, open-source executors can dramatically\ndecrease the monetary or computational costs of us-\ning ADAPT. We defer combining knowledge from\nstronger and weaker LMs within ADAPT to future\nwork, as examined in the context of mathematical\nreasoning (Fu et al., 2023; Saha et al., 2023a).\n6.5\nHow does ADAPT compare to baselines in\nterms of LLM calls?\nSetup.\nPerformance of decision-making agents\ncan be enhanced by increasing the number of calls\nallowed to an LLM, e.g., number of retrials in Re-\nflexion. To verify that the gains in ADAPT are\nnot simply due to higher number of LLM calls, we\ncompare the average of number of LLM calls made\nby ADAPT to the baselines.\nResults.\nFig. 7 shows that a ADAPT employs a\ncomparable number of LLM calls w.r.t. Try-Again\nand Reflexion baselines in order to yield perfor-\nmance improvements discussed in Sec. 5 (Tables 1\nand 2). Note that while all methods including Re-\nAct and Plan-and-Execute baselines are offered a\ncomparable computational budget, the actual num-\nber of LLM calls used by the latter is often lower\ndue to their inability to handle intermediate exe-\ncution failures. This strengthens the argument for\neffectiveness of ADAPT as the improvements do\nAlfWorld\nWebShop\nTextCraft\n0\n10\n20\n30\nAvg. #LLM Calls\nReAct\nPlan-&-Exec\nTry Again\nReflexion\nADaPT\nFigure 7: Average number of LLM calls for each ap-\nproach including ADAPT and baselines discussed in\nSec. 4.2 with GPT-3.5 LLM across datasets.\nnot simply stem from using substantially higher\nnumber of calls to the LLM.\n7\nConclusion\nWe introduce ADAPT, a recursive algorithm de-\nsigned to harness the planning capabilities of\nLLMs, dynamically decomposing complex tasks\nwhen the LLM acting as an executor encoun-\nters challenges. Our evaluation across three di-\nverse decision-making tasks, ALFWorld, WebShop,\nand TextCraft, reveals impressive performance of\nADAPT, surpassing existing baselines by substan-\ntial margins of up to 28.3%, 27%, and 33% points,\nrespectively. This not only underscores the effec-\ntiveness of ADAPT but also highlights the signif-\nicance of as-needed decomposition in enhancing\ntask performance. Moreover, our findings demon-\nstrate that ADAPT not only adapts to the capabili-\nties of the underlying executor LLM but also takes\ninto account the complexity of individual task in-\nstances, showcasing its versatility and effectiveness.\nAcknowledgements\nPart of this work was done during internship at\nAI2 and was partially supported at UNC by NSF-\nCAREER Award 1846185, NSF-AI Engage Insti-\ntute DRL-2112635, DARPA Machine Common-\nsense (MCS) Grant N66001-19-2-4031,. We sin-\ncerely thank Bodhisattwa Prasad Majumder, Chris\nCallison-Burch, Shashank Gupta, Peter Jansen, Bill\nYuchen Lin and the Aristo team for their valuable\nfeedback. We also thank Swarnadeep Saha, Elias\nStengel-Eskin, and Peter Hase for their feedback.\n9\nLimitations\nADAPT relies on the success heuristic generated\nby the executor LLM to determine if the model\nis capable of performing a complex task.\nFor\ndecision-making tasks studied in this work, we\nfind that LLMs can reliably determine task success\nbased on past action trajectories and textual feed-\nback from the environment (see Appendix F). How-\never, Huang et al. (2023a); Stechly et al. (2023)\ndiscuss the limits of LLM’s ability to self-evaluate\nand self-refine. In such situations, future works\nmay additionally employ external verifiers (Light-\nman et al., 2023; Shridhar et al., 2023), theory-of-\nmind strategies among multiple LMs (Saha et al.,\n2023a), and other calibration and self-evaluation\ntechniques (Kadavath et al., 2022). These improved\nself-evaluation techniques could be useful to extend\nour framework to non-decision making tasks such\nas question answering.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ADaPT: As-Needed Decomposition and Planning with Language Models.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nADaPT: As-Needed Decomposition and Planning with Language Models\n```\n#### 2. 论文摘要\n```\nLarge Language Models (LLMs) are increasingly being used for interactive\ndecision-making tasks requiring planning and adapting to the environment.\nRecent works employ LLMs-as-agents in broadly two ways: iteratively determining\nthe next action (iterative executors) or generating plans and executing\nsub-tasks using LLMs (plan-and-execute). However, these methods struggle with\ntask complexity, as the inability to execute any sub-task may lead to task\nfailure. To address these shortcomings, we introduce As-Needed Decomposition\nand Planning for complex Tasks (ADaPT), an approach that explicitly plans and\ndecomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute\nthem. ADaPT recursively decomposes sub-tasks to adapt to both task complexity\nand LLM capability. Our results demonstrate that ADaPT substantially\noutperforms established strong baselines, achieving success rates up to 28.3%\nhigher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel\ncompositional dataset that we introduce. Through extensive analysis, we\nillustrate the importance of multilevel decomposition and establish that ADaPT\ndynamically adjusts to the capabilities of the executor LLM as well as to task\ncomplexity.\n```\n\n#### 3. 论文全文\n```\nADAPT: As-Needed Decomposition and Planning with Language Models\nArchiki Prasad♣\nAlexander Koller♡\nMareike Hartmann♡\nPeter Clark♠\nAshish Sabharwal♠\nMohit Bansal♣\nTushar Khot♠\n♣UNC Chapel Hill\n♠Allen Institute for AI\n♡Saarland University\nAbstract\nLarge Language Models (LLMs) are increas-\ningly being used for interactive decision-\nmaking tasks requiring planning and adapt-\ning to the environment.\nRecent works em-\nploy LLMs-as-agents in broadly two ways:\niteratively determining the next action (itera-\ntive executors) or generating plans and execut-\ning sub-tasks using LLMs (plan-and-execute).\nHowever, these methods struggle with task\ncomplexity, as the inability to execute any\nsub-task may lead to task failure.\nTo ad-\ndress these shortcomings, we introduce As-\nNeeded Decomposition and Planning for com-\nplex Tasks (ADAPT), an approach that explic-\nitly plans and decomposes complex sub-tasks\nas-needed, i.e., when the LLM is unable to ex-\necute them. ADAPT recursively decomposes\nsub-tasks to adapt to both task complexity and\nLLM capability. Our results demonstrate that\nADAPT substantially outperforms established\nstrong baselines, achieving success rates up to\n28.3% higher in ALFWorld, 27% in WebShop,\nand 33% in TextCraft – a novel compositional\ndataset that we introduce. Through extensive\nanalysis, we illustrate the importance of multi-\nlevel decomposition and establish that ADAPT\ndynamically adjusts to the capabilities of the\nexecutor LLM as well as to task complexity.1\n1\nIntroduction\nRecent advances in Large Language Models\n(LLMs) have expanded their application beyond\nconventional NLP tasks to more complex tasks\ninvolving mathematical, symbolic, and common-\nsense reasoning (Wei et al., 2022; Huang and\nChang, 2023). Recent models have even been ap-\nplied to decision-making tasks, such as performing\nhousehold chores, navigating a webpage, etc., that\nrequire interactions with external environments or\ntools (Yao et al., 2023b; Qin et al., 2023).\n1Project: https:\/\/allenai.github.io\/adaptllm\nPrior works on using LLMs for decision-making,\nsuch as ReAct (Yao et al., 2023b), iteratively gen-\nerate the next action to be executed in the environ-\nment given the history of actions and observations\n(see Fig. 1; top-left). However, as the tasks become\nmore complex, LLMs struggle due to their limited\ncomposition ability (Dziri et al., 2023) and inability\nto deal with the distractors (Shi et al., 2023) in a\nlong action-observation trajectory.\nTo mitigate this, modular approaches (Khot et al.,\n2023; Yang et al., 2023; Sun et al., 2023) incorpo-\nrate a separate planner module that utilizes an LLM\nto create a high-level plan.2 The planner then dele-\ngates simpler sub-tasks to an executor LLM module\nthereby reducing the compositional complexity and\nlength of action trajectory required by the execu-\ntor. We refer to this category broadly as plan-and-\nexecute approaches (see Fig. 1; top-right). While\nthe plans enable these methods to guide the execu-\ntion and track progress (Wang et al., 2023b), their\nnon-adaptive nature poses a limitation when con-\nfronting unachievable sub-tasks. These approaches\ninherently lack the flexibility to adapt to task com-\nplexity and manage execution failures, as shown\nin Fig. 1(top-right), where just one sub-task that is\ntoo complex results in overall task failure.\nTo address such failures, we propose As-Needed\nDecomposition and Planning for complex Tasks\n(ADAPT), a recursive algorithm that further de-\ncomposes sub-tasks when necessary, to dynami-\ncally accommodate to task complexity. We uti-\nlize separate planner and executor LLM modules\nwithin our framework but only decompose a task\nusing the planner, if the executor LLM detects a\nfailure. As shown in Fig. 1, the overall task of\nputting a clean mug on a desk in an unfamiliar\n2By “planning”, we refer to the colloquial concept of\ndesigning a list of sub-tasks to accomplish a complex task\nrather than its usage in classical AI-planning literature. E.g.,\na “plan” for preparing a lasagna could be to cook the pasta,\nprepare the sauce, layer the ingredients, and then bake it.\narXiv:2311.05772v2  [cs.AI]  8 Apr 2024\n> Go to countertop 1.\nYou reached loc 1...\n> Go to cabinet 12.\n...\n> Think: Mug not found.\nTask failed!\nYou reached loc 20 ...\nIterative Executor (ReAct)\nPlan:\nStep 1: Find and\ntake the mug AND\nStep 2: Clean the\nmug AND\nStep 3: Put the\nclean mug on desk\nPlan-and-Execute\n   ADaPT (Recursive Decomposition, As-needed)\nTask: Put a clean mug on desk.\nNot Executed\nExecute: Task\nPlan: \nStep 1: Find and take the mug AND\nStep 2: Clean the mug AND\nStep 3: Put the clean mug on desk\nExecute: Step 1\nPlan: \nStep 1a: Find and take the mug from countertops OR\nStep 1b: Find and take the mug from cabinets OR\nExecute: Step 1a\nExecute: Step 1b\nExecute: Step 3\nExecute: Step 2\n...\nOn execution failure,\ndecompose further\nSuccessful sub-task allows\nexecution to resume\nExecute:\nStep 1\nExecute:\nStep 2\nExecute:\nStep 3\nOR\nADaPT(Task)\nADaPT(Step1)\nFigure 1: Top-Left: Iterative executors such as Re-\nAct (Yao et al., 2023b) interact directly with the envi-\nronment, performing planning implicitly. Top-Right:\nPlan-and-Execute, e.g., Yang et al. (2023), creates a\nfixed plan for the task, without accounting for complex-\nity in executing step 1. Bottom: ADAPT dynamically\ndecomposes based on success of the executor.\nhousehold is too complex for the model, leading to\nfailure of the iterative executor. While a plan-and-\nexecute-style approach initially breaks down the\ntask into three sub-tasks, it falls short in accounting\nfor the complexity in finding a mug. Moreover, it is\nchallenging to anticipate the difficulty of such a sub-\ntask in advance, as the executor could find a mug\nin the first attempt or in an obscure location. There-\nfore, ADAPT employs its recursive structure to\ndynamically adapt to execution failures (assessed\nby LLMs), by further decomposing the complex\nsub-task of finding a mug via the planner.\nEmpirically, we demonstrate the effectiveness\nof ADAPT on three datasets involving interactive\nenvironments: ALFWorld (Shridhar et al., 2021),\nWebShop (Yao et al., 2022), and a new compo-\nsitional text game for crafting Minecraft recipes\ncalled TextCraft (Sec. 4.1). Using GPT-3.5 as the\nunderlying LLM, ADAPT outperforms strong base-\nlines (discussed in Sec. 4.2) such as ReAct (Yao\net al., 2023b), and Plan-and-Solve (Wang et al.,\n2023b) by up to 28.3%, 27%, and 33% absolute\npoints on ALFWorld, WebShop, and TextCraft re-\nspectively (Sec. 5). Compared to Reflexion (Shinn\net al., 2023), an adaptive approach that addresses\nfailures in the full task trajectory, ADAPT yields\nhigher success rates by 14.1%, 9%, and 20% on\nALFWorld, WebShop, and TextCraft respectively.\nThrough extensive analysis of ADAPT, we es-\ntablish the importance of recursive decomposi-\ntion (Sec. 6.1) and showcase dynamic adaptation\nto the capabilities of the executor LLM includ-\ning open-source models such LLaMA-2 (Touvron\net al., 2023) and Lemur (Xu et al., 2023) in Sec. 6.2.\nLastly, we demonstrate that ADAPT incorporates\ntask complexity (Sec. 6.3), where the extent of re-\ncursive decomposition aligns with the inherent task\ncomplexity. To summarize, our contributions are:\n1. We present ADAPT, a recursive algorithm that\ndynamically decomposes complex sub-tasks on\nan as-needed basis, i.e., intervening only if the\ntask is too complex for the executor.\n2. On three diverse datasets, ALFWorld, WebShop,\nand TextCraft, ADAPT improves success rate\nof GPT-3.5 over previous approaches by up to\n28.3%, 27%, and 33% points respectively.\n3. Analysis of ADAPT underscores the signifi-\ncance of recursive decomposition and the ability\nto adapt dynamically to varying LLM execution\ncapabilities and task complexities.\n2\nRelated Work\nLLMs for Decision-Making.\nLLMs have been\nsuccessfully used as agents to perform a wide vari-\nety of decision-making tasks such as robotic nav-\nigation (Ahn et al., 2022; Huang et al., 2023b;\nSingh et al., 2023), complex multi-modal games\nlike Minecraft (Fan et al., 2022; Wang et al., 2023a),\ntext-based environments (Shridhar et al., 2021; Liu\net al., 2023). While most of these works focus\non learning from trajectories, ReAct (Yao et al.,\n2023b) uses few-shot prompting to build an agent\nthat reasons about the current state (thoughts) and\ngenerates the next action in the environment, given\nprior actions and observations. Their iterative ap-\nproach (shown in Fig. 1; top-left) can handle fail-\nures, but they have to keep track of the entire plan\nimplicitly while deciding every local action (c.f.\nADAPT in Fig. 9 of Appendix A). By incorporat-\ning planning and execution into separate modules\nand enabling dynamic adaptation we are able to\nachieve higher success rates (refer to Sec. 5).\n2\nSeveral follow-up works improve upon the Re-\nAct framework by incorporating feedback in future\ntrials (Madaan et al., 2023; Shinn et al., 2023), or\nusing LLMs to develop heuristics for search (Yao\net al., 2023a; Zhou et al., 2023). In contrast to\nADAPT, they do not employ task decomposition,\nleading to unnecessary computation as they explore\nmultiple trajectories or trials for the whole task,\neven though the LLM struggles with just one sub-\ntask. Such works are complementary to ADAPT\nas they can be incorporated within the planner or\nexecutor modules to strengthen LLM performance\n(just like they are incorporated in ReAct).\nDecomposition and Modularity.\nOur work fol-\nlows extensive literature in NLP on decomposing\ntasks into neural modules (Andreas et al., 2016;\nGupta et al., 2019; Jiang and Bansal, 2019) or\nseq2seq models (Min et al., 2019; Talmor and Be-\nrant, 2018; Khot et al., 2021; Perez et al., 2020;\nSaha et al., 2023b). With the advent of few-shot\nprompted black-box LLMs, this paradigm of pro-\ngrammatic decomposition into LLMs has become\nmore popular (Yao et al., 2023b; Khot et al., 2023;\nWang et al., 2023b, inter alia), referred to as LLM\nPrograms (Schlag et al., 2023; Dohan et al., 2022).\nAdditionally, past works in program synthesis (Mu-\nrali et al., 2018; Nye et al., 2019; Zheng et al., 2023)\nalso employ task decomposition via generating a\n“program sketch” prior to program generation.\nADAPT not only decomposes tasks via the plan-\nner module and delegates them to the executor\nmodule but also automatically adapts to execu-\ntor failures by further decomposing complex tasks\nas-needed. This dynamic capability distinguishes\nADAPT from prior works with a non-adaptive\nstructure. ADAPT extends the recursive and hi-\nerarchical decomposition in Khot et al. (2023), en-\nabling inter-module communications, and robust\nstrategies for execution failures, excelling in real-\nworld textual environments like online shopping.\nHierarchical Problem Solving.\nIn AI problem-\nsolving, there is a longstanding tradition of hi-\nerarchical task decomposition employed in plan-\nning (Ghallab et al., 2004; Georgievski and Aiello,\n2014; Höller et al., 2020), reinforcement learning\n(Sutton et al., 1999; Barto and Mahadevan, 2003;\nNachum et al., 2018; Zhang et al., 2021), and navi-\ngation (She et al., 2014; Sharma et al., 2022; Blukis\net al., 2022; Min et al., 2022; Song et al., 2023).\nThese approaches, such as Hierarchical Task Net-\nworks (Erol et al., 1994), leverage domain knowl-\nedge, e.g., hand-specified library of plans, to break\ncomplex problems into simpler tasks. Our work em-\nbraces this tradition but distinguishes itself by ex-\nploring how LLMs can autonomously decompose\ntasks by leveraging their extensive world knowl-\nedge, without predefined plan libraries. Lastly,\nADAPT performs dynamic hierarchical planning\nby employing its recursive structure.\n3\nMethodology\nWe introduce As-Needed Decomposition and\nPlanning for complex Tasks (ADAPT), a mod-\nular approach for decision-making that integrates\nan LLM as an executor and a planner (Secs. 3.1\nand 3.2) within an LLM program called the con-\ntroller (Sec. 3.3). In Fig. 1, when ADAPT is given\na complex task, it first attempts to accomplish the\nentire task by running the executor iteratively, and\nresorting to the LLM planner for further decom-\nposition into sub-tasks if the executor fails. Sub-\nsequently, ADAPT is recursively called for each\nsub-task to ensure their successful completion, ulti-\nmately leading to overall task success.\n3.1\nLLM as an Executor\nOverview.\nIn a given environment, the executor\nis provided with a concise natural language task\nspecification, as shown in Fig. 2 (left). Following\nYao et al. (2023b), the executor iteratively interacts\nwith the environment via actions generated by the\nLLM. This interaction continues until the task is\neither completed or a preset maximum iteration\nlimit is reached. Consistent with Ahn et al. (2022),\nwe provide the LLM with in-context demonstra-\ntions of low-level “atomic” skills specific to the\nenvironment (listed in Table 5 of Appendix A),\nsuch as knowing how to correctly heat objects in\nALFWorld. This approach offers two advantages:\n(i) it allows us to employ the same executor with\nenvironment-specific knowledge for all baselines\n(Sec. 4.2); and (ii) it enables the planner (discussed\nin Sec. 3.2) to work at a higher level of abstraction,\nleveraging the LLM’s general world knowledge.\nExecution Capabilities of an LLM.\nAt a min-\nimum, the LLM executor should reliably execute\natomic skills. While we provide demonstrations for\nsuccessful execution of atomic skills, LLMs can\nadapt to failures by combining multiple skills to\nperform complex tasks, as discussed in Sec. 6.2.\nFor instance, in Fig. 2 (left), we show the LLM suc-\ncessfully cleaning a mug it’s carrying (an atomic\n3\n[LLM] Think:  Input assumption: I am carrying a\nmug. Now I need to verify this.\n[LLM]> inventory\n[LLM]> go to sinkbasin 1\n[LLM]> clean mug 1 with sinkbasin 1\n[LLM] Think: I cleaned the mug.Task completed!\nADaPT(Task, k)\nExecutor(Task)\nSuccess?\nPlanner(Task)\nStep 2\nStep 1\nStep 3\nADaPT(Step1,k+1)\nADaPT(Step2,k+1)\nADaPT(Step3,k+1)\nTask: Put a clean mug on desk\nTrue\nFalse\nLogic\nTrue\nFalse\nClean the mug\nExecutor(           )\nOK.\nYou are carrying: a mug 1\nYou reached loc 13, you see ...\nYou clean mug 1\n# Think: To do this task, I first need to find a\nmug, then clean, it and put it on the desk. I\nneed to perform these tasks sequentially.\nStep 1: Find and take a mug AND\n# Think: Now I found a mug, I will clean it.\nStep 2: Clean the mug with sinkbasin AND\n# Think: Now I cleaned the mug, I will put\nthe clean mug on the desk.\nStep 3: Put clean mug on desk\n LLM\nPlanner(           )\nPut a clean\nmug on desk\nController\nAND\nFigure 2: Block diagram of the ADAPT pipeline with an example from ALFWorld. Left: Use of LLM as an\nexecutor to interact iteratively with the environment along with an example execution trajectory. Middle: Overall\nrecursive algorithm (depth k ≤dmax) that embeds the executor and planner, refer to Algorithm 1 for details. Right:\nOutline of using LLM as a planner to generate sub-tasks (steps) and logical operators combining them.\nskill). An advanced executor could combine “find-\ning a mug” with the “cleaning” skill to accomplish\n“find a clean mug” without an explicit planner.\nSelf-generated Success Heuristic.\nIn order to\ndecompose based on the abilities of the executor,\nwe need to determine whether the executor is capa-\nble of finishing the given (sub-)task independently\nor if further decomposition is required. To this\nend, we employ the executor LLM to determine\nthe completion of the (sub-)task without relying\non the environment for obtaining gold rewards for\n(sub-)tasks. We include a simple instruction in the\nexecutor prompt to output “task completed” if it de-\ntermines it has succeeded, otherwise output “task\nfailed” in case it cannot proceed. Refer to example\nin Fig. 2 (left). Our success heuristic aligns with bi-\nnary classification models employed in Shinn et al.\n(2023), providing a way to simulate intermediate\nrewards, which complements end-of-task environ-\nment rewards (Rengarajan et al., 2022). We study\nthis LLM-generated heuristic in Appendix F and\nshow that it closely matches the gold reward.\n3.2\nLLM as a Planner\nOverview.\nThe objective of the planner is to\nbreak down complex tasks into smaller sub-tasks.\nTo achieve this, we instruct the LLM to generate\na concise yet comprehensive plan consisting of a\nfew steps, typically 3-5, as shown in Fig. 2 (right).\nWe opt for shorter, more abstract plans because ex-\npecting a detailed, fine-grained plan upfront can be\nimpractical, especially in unexplored environments.\nE.g., devising a 10-step plan to put a clean mug\non a desk without prior knowledge of the mug’s\nlocation can lead to cascading errors due to incor-\nrect assumptions. Therefore, we task the LLM to\ngenerate short plans, with the flexibility to decom-\npose further in subsequent iterations, based on the\nexecutor’s capabilities.\nComposition Logic for Sub-tasks.\nAlong with\nthe sub-tasks, we prompt the planner to generate\nlogical operators to combine various sub-tasks in\nthe plan to accomplish the task. We allow for two\nlogical operators: “AND” and “OR”. Sub-tasks\nare linked using AND when they must be executed\nsequentially for the task to succeed. However, in\ncases requiring exploration, such as finding an item\nin an unknown room, we employ the OR operator\nto simulate conditional checks. Here, the task suc-\nceeds if any of the sub-tasks are successful. For\ninstance, in Fig. 1, the plan to “find a mug” would\nbe to “find a mug on the countertop” OR “find a\nmug in the cabinet”. We execute the latter only\nif the agent has not found the mug yet. While ex-\namples in Figs. 1 and 2 show homogeneous logic,\nADAPT can handle complex logical expressions\nas described in Appendix B.\n3.3\nController – LLM Program\nOverall Pipeline.\nThus far, we describe two\nLLM-based modules that can perform the roles\nof low-level execution and high-level planning. We\nincorporate these modules into ADAPT via the\ncontroller which is a pre-determined and recursive\nalgorithm – making the overall pipeline of ADAPT\nan LLM program (Schlag et al., 2023; Dohan et al.,\n2022), shown in Algorithm 1. The overall flow of\nthe controller program is as follows: (i) given an\ninput task, the controller calls the executor to check\nif it can succeed in performing the task directly; (ii)\n4\nif the executor does not succeed, the controller dele-\ngates decomposing the complex task to the planner\nand recursively calls ADAPT for each sub-task un-\ntil we hit a termination criterion, i.e., if a maximum\ndepth dmax (≥1) is reached.\nFig. 2 (mid) shows the control flow of ADAPT.\nA complex task such as “put a clean mug on the\ndesk” is first assigned to the executor. If the execu-\ntor does not succeed, then ADAPT calls the planner\nto decompose the task into sub-tasks along with a\nlogical operator (AND or OR) indicating how to\ncompose them. Each sub-task (referred to as ‘step’\nin Fig. 2) is then assigned recursively to ADAPT\nand is combined using the logical operator. In the\nend, the success of sub-tasks after recursive decom-\nposition ensures overall task success (unrolled calls\nto planner and executor are shown in Fig. 1).\n4\nExperimental Setup\nWe describe the datasets used in our experiments\nand baselines used for comparison with ADAPT.\n4.1\nDatasets\nWe employ LLMs-as-agents to perform tasks in the\nfollowing three environments and use task success\nrate as our evaluation metric in Secs. 5 and 6.\nALFWorld.\nALFWorld (Shridhar et al., 2021)\nis a text-based game version of the embodied AL-\nFRED benchmark (Shridhar et al., 2020) imple-\nmented in the TextWorld environment (Côté et al.,\n2019). It encompasses 6 distinct task types, where\nan agent is required to accomplish high-level tasks\nthrough navigation and interaction via text-based\nactions in a simulated household that gives textual\nfeedback to an agent (e.g., put a clean mug on desk\ndiscussed earlier in Fig. 2). Following Shridhar\net al. (2021), we present results on 134 unseen eval-\nuation games (test set) with a separate dev set of\n10 games per task from the seen evaluation games\nsplit. Along with atomic skills, we add example\ngold trajectories, following Yao et al. (2023b), for\ntwo tasks: heat and look in the executor prompt.3\nWebShop.\nWebShop (Yao et al., 2022) is an on-\nline shopping website environment featuring 1.18\nmillion real-world products containing 500 user\nqueries in the test set.\nIt serves as a complex\n3Unlike Yao et al. (2023b), we use a standardized executor\nprompt for all ALFWorld tasks, avoiding the agent to know the\ntask-type apriori. Table 6 in Appendix C further demonstrates\nthat ADAPT still improves over task-specific executors.\nCrafting Commands:\ncraft 4 oak planks using 1 oak log\ncraft 1 honeycomb block using 4 honeycomb\ncraft 1 beehive using 6 planks, 3 honeycomb\nGoal: craft beehive\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] get 1 oak log\n[Env] Got 1 oak log\n[Action] craft 4 oak planks using 1 oak log\n[Env] Crafted 4 minecraft:dark_oak_planks\n[Action] craft 1 beehive using 6 oak planks, 3 honeycomb \n[Env] Crafted 1 minecraft:beehive\n...\n1x\n1x\n4x\n4x\n3x\n1x\nFigure 3: Example gold trajectory in TextCraft for a\ntask with recipe depth of 2.\ndecision-making environment with practical appli-\ncations wherein an agent must navigate a website\nthrough a variety of commands to purchase an item\nmatching a user specification (e.g., grey sectional\nsofa priced less than $300 with fast delivery). Fol-\nlowing Shinn et al. (2023), we report performance\non 100 user instructions and use a different subset\nof 40 queries as the dev set.\nTextCraft.\nWe create a new text-only environ-\nment for crafting Minecraft4 items similar to Word-\nCraft (Coenen et al., 2021). Unlike existing agent-\nbased environments, tasks in TextCraft exhibit a\nnatural compositional structure, resembling cook-\ning recipes with steps of varying complexity, where\nsome sub-tasks are more intricate, such as layering\na lasagna, while others are simpler, like baking it.\nTasks in TextCraft are inherently decomposable.\nIn Fig. 3, crafting a beehive necessitates crafting\nits ingredients, like planks and honeycomb, which\nmay require further decomposition. The agent thus\nneeds to identify and adapt to varying task com-\nplexity, e.g., crafting a plank is easier than crafting\na beehive. Moreover, some recipes allow using any\nitem from a particular category. For instance, craft-\ning a beehive uses planks (a category), requiring\nthe agent to use linguistic knowledge for proper\nitem selection (e.g., select oak planks, a specific\nitem in the category planks). We evaluate our ap-\nproach on a test set of 200 tasks where the target\nitems have recipe trees of depth 2, 3, and 4 (exam-\nple tree of depth 2 is shown in Fig. 3). We use the\n4https:\/\/www.minecraft.net\n5\nMethod (dmax = 3)\nPick Clean Heat Cool Look Pick2\nAll\nReAct\n33.3\n67.7\n43.5\n33.3\n55.6\n11.8\n43.3\nPlan-and-Execute\n29.2\n61.3\n47.8\n38.1\n61.1\n11.8\n43.3\nTry Again with ReAct 50.0\n51.6\n60.8\n47.6\n61.1\n5.9\n47.8\nReflexion\n70.8\n61.3\n61.0\n66.7\n61.1\n5.9\n57.5\nADAPT (Ours)\n87.5\n80.6\n60.8\n76.2\n61.1\n52.9\n71.6\nTable 1: ADAPT yields the highest the overall success rates (%)\ncompared to baselines from prior work (discussed in Sec. 4.2) on\nALFWorld (test split). Best (highest) success rates are highlighted\nin bold and second-highest rates are underlined.\nMethod\nWebShop TextCraft\nReAct\n32.0\n19.0\nPlan-and-Execute\n17.0\n27.0\nTry Again with ReAct\n30.0\n15.0\nReflexion\n35.0†\n32.0\nLATS (Zhou et al., 2023)\n38.0†\n−\nADAPT (Ours)\n44.0\n52.0\nTable 2: ADAPT yields the highest success\nrate on WebShop and TextCraft (test split) with\ndmax = 3 and 4 respectively. †Performance\nreported by Zhou et al. (2023)\nitems with recipe tree depth of 3 (123 tasks), depth\nof 4 (11 tasks) and depth of 2 (77 out of 297) in\nour test set, and the rest of depth 2 tasks constitute\nthe dev set. Additional details about creating the\nenvironment are present in Appendix E.\n4.2\nBaseline Approaches\nWe compare ADAPT with four classes of baseline\napproaches described below.\nIterative Executor-Only (ReAct).\nIn this setting,\nwe employ the executor to interact iteratively with\nthe environment, adopting the think-act-observe\nprompting style from ReAct (Yao et al., 2023b). All\nmethods discussed below, including ADAPT, share\nthe same executor, ensuring a standardized impact\nof the executor’s strength and design choices when\ncomparing relative performance in Sec. 5. When\ndmax =1, ADAPT solely relies on this executor.\nPlan-and-Execute.\nAs shown in Fig. 1, in this\nsetting, we generate a plan first and then assign\neach sub-task to the executor. This approach only\nplans once and as a result has a non-adaptive struc-\nture (consistent with Wang et al. (2023b); Yang\net al. (2023); Sun et al. (2023)). To ensure each\nplan step is executable without further decompo-\nsition, we design new prompts with more detailed\nplans. Note that ADAPT with dmax = 2 differs\nfrom plan-and-execute as it is adaptive, i.e., de-\ncomposes only when executor fails and generates\nrelatively shorter plans (refer to Appendix B).\nTry Again with ReAct.\nBy design, ADAPT\nmakes multiple calls to the executor module, al-\nbeit with different (sub-)tasks. Like Yang et al.\n(2023), we design a simple controller that requests\nthe executor to retry the task in a total of dmax\nseparate trials and then uses the trial with the best\nperformance for each task instance.\nReflexion.\nShinn et al. (2023) execute the en-\ntire task first, and if unsuccessful, reflect and store\nfeedback in memory for subsequent dmax−1 trials.\nWhile adaptive, this approach repeats the entire\ntrial even if a single sub-task fails, redundantly\nre-executing previously successful sub-tasks.\nADAPT and Shared Implementation Details.\nFollowing (Yao et al., 2023b; Shinn et al., 2023;\nZhou et al., 2023), by default, we use the GPT-\n3.5 (Ouyang et al., 2022) LLM for both planning\nand execution in ADAPT and other baselines. We\nuse the completion-based models for ALFWorld\nand TextCraft and the chat-based model for Web-\nShop.5 Further, we use ADAPT (and other base-\nlines) with dmax =3 for ALFWorld, and WebShop\nand increase to dmax =4 for TextCraft to accommo-\ndate recipes with a depth of 4 (Sec. 4.1). For addi-\ntional details, refer to Appendix A. We increase the\nmaximum number of iterations for the ReAct base-\nline by a factor of dmax and ensure all baselines\nuse a comparable number of LLM calls (Sec. 6.5).\n5\nMain Results\nUsing GPT-3.5 as the underlying LLM, in this sec-\ntion, we show that ADAPT yields the highest suc-\ncess rate compared to baselines from prior work on\nALFWorld, WebShop, and TextCraft datasets.\nALFWorld.\nIn Table 1, we observe that ADAPT\nachieves the highest overall success rate, while\nusing ReAct alone results in the lowest overall\nperformance. By leveraging adaptive decomposi-\ntion, ADAPT improves over ReAct’s performance\nby 28.3% points (absolute) as well as over Plan-\nand-Execute and Try Again by 28.3% and 23.8%\npoints, respectively. Lastly, we find that ADAPT\n5We use the completion model as chat variants of GPT-3.5\nconsistently underperform their completion counterparts (Liu\net al., 2023; Yang et al., 2023). We discuss the effectiveness\nof ADAPT different LLMs in Sec. 6.2.\n6\n1\n2\n3\nValue of dmax in ADaPT\n20\n40\n60\nSuccess Rate\nALFWorld\nWebShop\nTextCraft\nFigure 4: Success rate of ADAPT increases with the\nmaximum depth dmax for all datasets (dev splits).\nyields 14.1% points higher overall success rate\nthan Reflexion, despite the latter having access\nto dedicated memory and natural language feed-\nback. Specifically, we find baselines yield poor\nresults on ‘pick2’ tasks (<12% success rate) as\nthey require the agent to compose two ‘pick’-style\ntasks involving a longer action history. However,\nADAPT yields significant improvements (by over\na factor of 4×) for this type of tasks.\nWebShop.\nTable 2 shows a similar trend with\nADAPT surpassing all baselines and achieving\nthe highest success rate. ADAPT outperforms Re-\nAct, Plan-and-Execute, and Try-Again baselines\nby up to 27% points. We corroborate the findings\nof Shinn et al. (2023) and observe that natural lan-\nguage feedback offers limited gains in performance,\nas compared to ADAPT (which surpasses Reflex-\nion by 9% points). Additionally, we compare with\na recent search-based baseline LATS (Zhou et al.,\n2023) and find that ADAPT outperforms the suc-\ncess rate of LATS by 6% points.\nTextCraft.\nOur results on TextCraft are summa-\nrized in Table 2. First, we observe that ADAPT\nachieves an improvement of 33% compared to the\nReAct executor. In contrast to Plan-and-Execute,\ni.e., starting with a fixed plan, having the dynamic\nability to adapt to complex sub-tasks (in this case,\ncrafting complex ingredients) in ADAPT improves\nperformance by 25% points. Lastly, ADAPT out-\nperforms Reflexion by 20% points, highlighting\nthe importance of adaptive and as-needed planning.\nWe hypothesize that ADAPT consistently outper-\nforms Reflexion across datasets as the latter relies\non generating feedback based on errors in the entire\ntrajectory. In contrast, due its design, ADAPT of-\nten handle failures of small sub-tasks and redirects\nmore resources in the form of calling the planner\nTask-Specific\nHybrid\nAtomic\nExecutor Setting\n0\n20\n40\n60\nSuccess Rate\nExecutor-Only\nADaPT (dmax=3)\nFigure 5: ADAPT improves success rates across varying\nsettings capturing different executor capabilities (i.e.,\nexecutor-only performance) on ALFWorld (dev).\nand decomposition to the challenging sub-tasks.\n6\nAnalysis and Discussion\nWe analyze ADAPT in detail by addressing the\nfollowing research questions on dev data splits.\n6.1\nHow does performance of ADAPT scale\nwith the depth of decomposition?\nSetup.\nTo assess the impact of adaptive decom-\nposition, we study ADAPT under three settings\nwith increasing maximum depth dmax ∈{1, 2, 3}\nfor ALFWorld, WebShop, and TextCraft. Note\nthat dmax = 1 setting corresponds to the iterative\nexecutor-only baseline (ReAct).\nResults.\nFig. 4 shows that across all datasets, per-\nformance of ADAPT scales with increasing the\nmaximum depth dmax. Consistently, we find a sig-\nnificant improvement in success rates as we move\nfrom dmax =1 to dmax =2, i.e., adding the planner\nto decompose a complex task when executor fails\nproves to be effective. Finally, the performance\nincrease from dmax = 2 to dmax = 3 validates our\nhypothesis that some sub-tasks are difficult for the\nLLM to directly execute successfully, and decom-\nposing these further boosts overall performance.\n6.2\nDoes ADAPT cater to different execution\ncapabilities of LLMs?\nSame LLM, different execution capabilities.\nWe run ADAPT on three different executor\nprompts on ALFWorld: (i) task-specific gold tra-\njectories, (ii) atomic skills and common gold-\ntrajectories for 2 tasks used in Sec. 5 (hybrid), and\n(iii) only atomic skills. Using gold trajectories\naligns closely with the task at inference-time and\nthus, should exhibit high performance. In contrast,\n7\nAlfWorld\nWebShop\nTextCraft\n20\n40\n60\n80\n100\nSuccess Rate\nGPT-3.5\nGPT-4\nLemur\nLLaMA\nwith ADaPT\nFigure 6: ADAPT improves (test) performance of GPT-3.5, GPT-4, LLaMA, and Lemur LLMs across datasets.\nexecutor using only atomic skills relies on the in-\nherent composition abilities of the LLM, yielding\nweaker performance. Here we examine if ADAPT\ncan improve success rates for all three settings.\nResults.\nIn Fig. 5, we observe that ADAPT con-\nsistently improves over the executor-only baseline\nfor all diverse executor settings. As expected, the\nexecutor prompted with task-specific trajectories\nperforms the best (left), while the executor with\nonly atomic skills performs the worst (right). No-\ntably, ADAPT substantially improves performance\nof the relatively weak executor, improving success\nrate from 3.3% to 41.7%.\nADAPT with different LLMs.\nWe study the\nability of ADAPT to improve performance across\ndifferent LLMs (as planners and executors): (i)\nGPT-3.5, (ii) GPT-4 (OpenAI, 2023), (iii) LLaMA-\n2 70B (Touvron et al., 2023), and (iv) Lemur\n70B (Xu et al., 2023) on test splits of all datasets.\nResults.\nFig. 6 shows that ADAPT consistently\nimproves downstream performance for all models\nacross all three datasets. Consistent with Liu et al.\n(2023), we find that the gated GPT models outper-\nform the open-source models based on absolute\nsuccess rates. Nevertheless, ADAPT is effective\nacross LLMs and improves performance of GPT-\n4, the strongest LLM, by up to 37%, as well as\nLLaMA, the least performant LLM, by up to 15%\non the TextCraft dataset.\n6.3\nDoes ADAPT handle task complexity?\nSetup.\nBy the compositional design of TextCraft,\ncomplexity of each task in the dataset can be de-\nfined with respect to the depth of the crafting recipe,\ni.e., recipes with higher depth would be more com-\nplex to craft. We evaluate efficacy of ADAPT\nand the ReAct baseline on the test set of TextCraft\nMethod\nRecipe Depth kmax Success Rate\nReAct\n2\n1.0\n26.9\nADAPT (dmax = 4)\n2\n1.9\n78.2\nReAct\n3\n1.0\n1.8\nADAPT (dmax = 4)\n3\n2.8\n38.7\nTable 3: ADAPT improves TextCraft (test) performance\neven as recipe depth increases. The maximum decom-\nposition depth used by ADAPT to succeed at the task\n(kmax) also scales with the recipe depth.\nwith increasing recipe depth.6 Furthermore, while\nwe provide ADAPT with a maximum budget of\ndmax = 4, we study how the maximum decompo-\nsition depth utilized by ADAPT to succeed (kmax)\nvaries with task complexity.\nResults.\nIn Table 3 we observe that ADAPT im-\nproves success rates for games with recipe depth of\n2 from 26.9% to 78.2%, and of depth 3 from 1.8%\nto 38.7% as compared to the ReAct baseline. As\nexpected, the executor alone is unable to handle\ncomplex recipes with depth ≥3, but with the help\nof ADAPT the performance improves significantly.\nAdditionally, given the same budget dmax = 4, as\nthe recipe depth (complexity) increases from 2 to\n3, ADAPT’s level of decomposition (kmax) also\nincreases from 1.9 to 2.8. This showcases that\nADAPT leverages as-needed decomposition in or-\nder to handle task complexity.\n6.4\nCan we use different planner and executor\nLLMs within ADAPT?\nSetup.\nThe planner and executor modules of\nADAPT do not need to necessarily use the same\nunderlying model. Following, Lin et al. (2023) we\nexplore if a relatively smaller LLM can be used to\nperform local actions in the executor and a more\n6As we have only 11 tasks with recipe depth of 4, we\nexclude them from this analysis.\n8\nExecutor LM\nPlanner LM\nSuccess Rate\nGPT-3.5\n−\n38.4\nGPT-3.5\nGPT-3.5\n58.3\nLLaMA-2-70B\n−\n20.4\nLLaMA-2-70B\nGPT-3.5\n43.3\nTable 4: ADAPT improves performance on ALFWorld\n(dev) when using different planner and executor LLMs.\nadvanced LLM be used to devise plans. To this\nend, we explore different combinations of planner\nand executor LLM, with the latter using both gated\nand open-source models on ALFWorld.\nResults.\nTable 4 shows that ADAPT can suc-\ncessfully be used to generate plans from one LLM\nthat are useful to a different, possibly smaller, ex-\necutor LLM, improving success rates by up to\n19.9% compared to the executor-only (ReAct) set-\nting. Interestingly, using an open-source model,\nsuch as LLaMA-2-70B-chat (Touvron et al., 2023)\ncan be used as an executor with a more advanced\nLLMs such as GPT-3.5 to improve success rates\nby 22.9% points. Since the planner LLM is used\nsparingly, open-source executors can dramatically\ndecrease the monetary or computational costs of us-\ning ADAPT. We defer combining knowledge from\nstronger and weaker LMs within ADAPT to future\nwork, as examined in the context of mathematical\nreasoning (Fu et al., 2023; Saha et al., 2023a).\n6.5\nHow does ADAPT compare to baselines in\nterms of LLM calls?\nSetup.\nPerformance of decision-making agents\ncan be enhanced by increasing the number of calls\nallowed to an LLM, e.g., number of retrials in Re-\nflexion. To verify that the gains in ADAPT are\nnot simply due to higher number of LLM calls, we\ncompare the average of number of LLM calls made\nby ADAPT to the baselines.\nResults.\nFig. 7 shows that a ADAPT employs a\ncomparable number of LLM calls w.r.t. Try-Again\nand Reflexion baselines in order to yield perfor-\nmance improvements discussed in Sec. 5 (Tables 1\nand 2). Note that while all methods including Re-\nAct and Plan-and-Execute baselines are offered a\ncomparable computational budget, the actual num-\nber of LLM calls used by the latter is often lower\ndue to their inability to handle intermediate exe-\ncution failures. This strengthens the argument for\neffectiveness of ADAPT as the improvements do\nAlfWorld\nWebShop\nTextCraft\n0\n10\n20\n30\nAvg. #LLM Calls\nReAct\nPlan-&-Exec\nTry Again\nReflexion\nADaPT\nFigure 7: Average number of LLM calls for each ap-\nproach including ADAPT and baselines discussed in\nSec. 4.2 with GPT-3.5 LLM across datasets.\nnot simply stem from using substantially higher\nnumber of calls to the LLM.\n7\nConclusion\nWe introduce ADAPT, a recursive algorithm de-\nsigned to harness the planning capabilities of\nLLMs, dynamically decomposing complex tasks\nwhen the LLM acting as an executor encoun-\nters challenges. Our evaluation across three di-\nverse decision-making tasks, ALFWorld, WebShop,\nand TextCraft, reveals impressive performance of\nADAPT, surpassing existing baselines by substan-\ntial margins of up to 28.3%, 27%, and 33% points,\nrespectively. This not only underscores the effec-\ntiveness of ADAPT but also highlights the signif-\nicance of as-needed decomposition in enhancing\ntask performance. Moreover, our findings demon-\nstrate that ADAPT not only adapts to the capabili-\nties of the underlying executor LLM but also takes\ninto account the complexity of individual task in-\nstances, showcasing its versatility and effectiveness.\nAcknowledgements\nPart of this work was done during internship at\nAI2 and was partially supported at UNC by NSF-\nCAREER Award 1846185, NSF-AI Engage Insti-\ntute DRL-2112635, DARPA Machine Common-\nsense (MCS) Grant N66001-19-2-4031,. We sin-\ncerely thank Bodhisattwa Prasad Majumder, Chris\nCallison-Burch, Shashank Gupta, Peter Jansen, Bill\nYuchen Lin and the Aristo team for their valuable\nfeedback. We also thank Swarnadeep Saha, Elias\nStengel-Eskin, and Peter Hase for their feedback.\n9\nLimitations\nADAPT relies on the success heuristic generated\nby the executor LLM to determine if the model\nis capable of performing a complex task.\nFor\ndecision-making tasks studied in this work, we\nfind that LLMs can reliably determine task success\nbased on past action trajectories and textual feed-\nback from the environment (see Appendix F). How-\never, Huang et al. (2023a); Stechly et al. (2023)\ndiscuss the limits of LLM’s ability to self-evaluate\nand self-refine. In such situations, future works\nmay additionally employ external verifiers (Light-\nman et al., 2023; Shridhar et al., 2023), theory-of-\nmind strategies among multiple LMs (Saha et al.,\n2023a), and other calibration and self-evaluation\ntechniques (Kadavath et al., 2022). These improved\nself-evaluation techniques could be useful to extend\nour framework to non-decision making tasks such\nas question answering.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | ADaPT：按需分解与规划，提升大型语言模型在复杂任务中的表现\n\n## 📌 背景痛点\/本文动机\n随着大型语言模型（LLMs）在自然语言处理领域的广泛应用，它们也逐渐被应用于需要规划和适应环境的交互式决策任务中。然而，现有的方法在处理复杂任务时面临着挑战，因为LLMs在执行子任务时可能会失败，从而导致整个任务的失败。\n\n## 🚀 核心方法\n为了解决上述问题，本文提出了ADaPT（As-Needed Decomposition and Planning for complex Tasks），一种按需分解和规划复杂任务的方法。ADaPT的核心思想是，当LLM作为执行者无法执行子任务时，将其分解为更小的子任务，并递归地进行分解，以适应任务的复杂性和LLM的能力。\n\n### 💡 创新点1：按需分解\nADaPT通过递归地分解子任务，动态地适应任务的复杂性和LLM的能力。当LLM作为执行者无法执行子任务时，它会调用LLM作为规划者来生成更小的子任务，并递归地调用ADaPT来执行这些子任务。\n\n### 💡 创新点2：多级分解\nADaPT支持多级分解，这意味着它可以进一步分解子任务，直到它们变得足够简单，可以被LLM作为执行者成功执行。这种多级分解的能力使得ADaPT能够处理更复杂的任务，并提高任务的成功率。\n\n## 📈 实验结果\n在ALFWorld、WebShop和TextCraft三个数据集上进行的实验结果表明，ADaPT显著优于现有的强基线方法，在ALFWorld上提高了28.3%的成功率，在WebShop上提高了27%，在TextCraft上提高了33%。\n\n## 💬 可借鉴之处\nADaPT提供了一种有效的方法来处理LLMs在复杂任务中的执行失败问题。它通过按需分解和规划，动态地适应任务的复杂性和LLM的能力，从而提高了任务的成功率。此外，ADaPT的多级分解能力使其能够处理更复杂的任务，并提高任务的成功率。","llm_summary_res_status":200}
{"title":"Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game","authors":"Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, Yali Du","summary":"Multi-agent collaboration with Large Language Models (LLMs) demonstrates\nproficiency in basic tasks, yet its efficiency in more complex scenarios\nremains unexplored. In gaming environments, these agents often face situations\nwithout established coordination protocols, requiring them to make intelligent\ninferences about teammates from limited data. This problem motivates the area\nof ad hoc teamwork, in which an agent may potentially cooperate with a variety\nof teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork\nproblem where the agent operates in an environment driven by natural language.\nOur findings reveal the potential of LLM agents in team collaboration,\nhighlighting issues related to hallucinations in communication. To address this\nissue, we develop CodeAct, a general agent that equips LLM with enhanced memory\nand code-driven reasoning, enabling the repurposing of partial information for\nrapid adaptation to new teammates.","url":"http:\/\/arxiv.org\/abs\/2312.17515v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.17515v1","published":1703838414000,"comment":"Code will release soon","pdf_text":"Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork\nin the Avalon Game\nZijing Shi1, Meng Fang2, Shunfeng Zheng1, Shilong Deng2, Ling Chen1, Yali Du3\n1 AAII, University of Technology Sydney, Australia\n{zijing.shi, shunfeng.zheng}@student.uts.edu.au, ling.chen@uts.edu.au\n2 University of Liverpool, United Kingdom\n{Meng.Fang, shilong.deng}@liverpool.ac.uk\n3 King’s College London, United Kingdom\nyali.du@kcl.ac.uk\nAbstract\nMulti-agent collaboration with Large Language\nModels (LLMs) demonstrates proficiency in\nbasic tasks, yet its efficiency in more complex\nscenarios remains unexplored. In gaming en-\nvironments, these agents often face situations\nwithout established coordination protocols, re-\nquiring them to make intelligent inferences\nabout teammates from limited data. This prob-\nlem motivates the area of ad hoc teamwork,\nin which an agent may potentially cooperate\nwith a variety of teammates to achieve a shared\ngoal. Our study focuses on the ad hoc team-\nwork problem where the agent operates in an\nenvironment driven by natural language. Our\nfindings reveal the potential of LLM agents in\nteam collaboration, highlighting issues related\nto hallucinations in communication. To address\nthis issue, we develop CodeAct, a general agent\nthat equips LLM with enhanced memory and\ncode-driven reasoning, enabling the repurpos-\ning of partial information for rapid adaptation\nto new teammates.\n1\nIntroduction\nLarge Language Models (LLMs) have exhibited im-\npressive abilities in reasoning and generalization,\nshowing their potential in building autonomous\nagents and driving many advancements (Kojima\net al., 2022; Xi et al., 2023).\nRecent research\ninvolving multiple LLM agents working together\nhas shown success in task execution (Wang et al.,\n2023c; Wu et al., 2023; Dong et al., 2023). How-\never, these studies often employ explicit coordina-\ntion protocols for fully cooperative tasks, assum-\ning full information of the teammate and the task.\nWhile some research has explored simulating LLM\nagents in more open-ended settings, these agents\nremain limited by pre-established strategies and\nbehaviors (Park et al., 2023).\nIn dynamic environments, maintaining consis-\ntent coordination protocols among agents is a chal-\nlenge. Such contexts demand an agent capable of\nswift adaptation and on-the-fly cooperation. With-\nout pre-established team strategies, an agent, often\nreferred to as the learner, must observe its team-\nmates’ behaviors and adapt effectively to achieve\ntheir shared goals. This challenge is known as the\nAd Hoc Teamwork (AHT) problem (Stone et al.,\n2010; Macke et al., 2021).\nA significant challenge for the learner is to infer\nthe role of their teammates. Traditional methods\nrely on pre-programmed rules or data-driven al-\ngorithms trained on historical data (Mirsky et al.,\n2022). In contrast, LLM agents can directly com-\nmunicate with their teammates in natural language.\nHowever, the communication dynamics within the\nenvironment may exhibit conflicting and competi-\ntive elements. Moreover, due to the inherent traits\nof LLMs, teammates may provide inaccurate or\nmisleading information.\nIn this study, we explore the AHT abilities of\nLLM agents, utilizing the multi-agent game The\nResistance: Avalon to simulate AHT environments.\nWe introduce the AvalonPlay benchmark, which\nchallenges agents with hidden roles to participate in\nstrategic gameplay across multiple rounds. In each\nround, a learner is tasked with selecting teammates\nwithout prior strategies and full information. Our\nresearch integrates LLM agents into AvalonPlay to\nevaluate their performance in AHT. Rather than de-\nveloping coordination protocols for the entire team,\nwe prioritize designing agents adept at cooperating\nin environments without such explicit protocols.\nA communication channel is designed within the\nAvalonPlay benchmark to enhance collaboration.\nWe observe that LLM agents may sometimes pro-\nduce hallucinations and forget early information.\nTo address this, we introduce CodeAct, a general\nagent leveraging the advanced reasoning capabili-\nties of LLMs, focusing on reasoning about various\npotential teammate types. We begin with a memory\nretrieval system that collects information and uti-\nlize the knowledge gained from these interactions.\narXiv:2312.17515v1  [cs.CL]  29 Dec 2023\nFigure 1: A flowchart of the AvalonPlay benchmark showing team sides and roles on the left and a detailed round\npipeline on the right. Each round includes four stages: leader assignment, team selection, discussion and voting,\nquest execution.\nSubsequently, we incorporate code-driven reason-\ning with action to effectively refer the roles of new\nteammates. With the teammate role information,\nour agent is well-equipped to effectively cooperate\nwith its teammates.\nIn summary, our contributions are as follows:\n• We explore the AHT capabilities of LLM\nagents, utilizing the AvalonPlay benchmark\nwith its multi-round collaborative tasks as a\ntesting ground for these abilities.\n• We develop CodeAct, an LLM agent utilizing\na code-driven prompting approach, aimed at\nboosting the agent’s efficiency in AHT tasks.\n• We conduct comprehensive experiments\nwithin the AvalonPlay benchmark, demon-\nstrating the effectiveness of our proposed\nagent.\n2\nThe AvalonPlay Benchmark\nWe introduce the AvalonPlay benchmark to ex-\nplore the performance of LLM agents on AHT.\nThis benchmark is a language-based, multi-agent\nplatform comprised of multi-round tasks. During\nthe gameplay, agents have limited knowledge about\neach other’s roles. In each round, a team leader,\nalso regarded as the learner, is selected to deduce\nthe roles and intentions of other agents. The leader\nmust adapt their team’s strategies dynamically as\nthe game evolves. This section details the key ele-\nments of the proposed benchmark.\n2.1\nTeammate Role\nWe utilize a setup involving seven players indexed\nby j, each controlled by an LLM agent. In each\ngame, agents are randomly assigned one of six\ndistinct roles, dividing them into two factions: the\njust side (including Merlin, Percival, and two\nLoyal Servants of Arthur) and the evil side\n(including Morgana, Minion, and Assassin).\nEach role possesses unique abilities, as shown\non the left of Figure 1. On the just side, Merlin\nis aware of the factions but not the specific\nroles, while Percival can recognize Merlin and\nMorgana but is unable to distinguish between them.\nThe Loyal Servants of Arthur lack any special\nknowledge. On the evil side, in contrast, all roles\ncan recognize their teammates. This setup puts the\njust side at a disadvantage due to their limited in-\nformation. Our study focuses on scenarios where\nthe just side takes the lead role, aiming to enhance\nthe leader’s ability to deduce the various roles of\nagents.\n2.2\nPipeline\nIn AvalonPlay, a single game is composed of five\nquest rounds indexed by i. Each round, defined as\na full quest cycle, is divided into the following four\nphases, as shown in the right of Figure 1.\n• Leader Assignment: Each round begins with\nthe selection of a leader who nominates team-\nmates for the quest. The first round’s leader\nis randomly chosen, and in subsequent rounds,\nFigure 2: An overview of the proposed CodeAct agent as the leader during team selection. We begins by establishing\na memory retrieval system that distills information from past interactions, enabling the agent to access relevant\ninformation. Then, we integrate code-driven reasoning with action to determine teammate roles effectively. Finally,\nwe employ an interpreter to execute the generated code, equipping the agent with self-debug capabilities.\nleader rotates sequentially to the next player.\nThis approach ensures diverse leader roles while\nincorporating randomness.\n• Team Selection: In this phase, a leader nomi-\nnates ni teammates, with the team size ni vary-\ning per round. Adhering to the game rule, the\nteam sizes are: n1 = 2, n2 = 3, n3 = 3, n4 =\n4, n5 = 4.\nWe align the leader’s response\ninto corresponding actions. For instance, the\nresponse “I choose myself and player 2\nfor the quest team” is mapped to the specific\nselection of teammates.\n• Discussion and Voting: We design a communi-\ncation protocol that enables the agent to acquire\ninformation while accounting for the potential\nof misinformation. Post team selection, agents\nparticipate in discussions to assess the proposed\nteam. Following the discussion, each agent se-\ncretly votes 1j indicates agent j’s approval of\nthe current nominated team. To enhance agents’\ndecision-making, they are required to generate\na JSON containing their reasoning and vote. A\nteam is approved if the total votes reaches a ma-\njority (i.e., P7\nj=1 1j ≥4). Otherwise, the team\nselection is repeated with a new leader. A team is\nautomatically formed if approval is not achieved\nafter five consecutive attempts.\n• Quest Execution: For an approved team, team-\nmates from the just side must vote to approve,\nwhile teammates from the evil side must vote\nagainst. Let Qi denote the total votes of evil\nmembers. The quest is sabotaged if Qi ≥1\nfor rounds i ∈{1, 2, 3}, or Qi ≥2 for rounds\ni ∈{4, 5}. In other cases, it is completed suc-\ncessfully.\nWe adhere to game rule by including an assassi-\nnation phase in the Avalonplay benchmark, where\nthe Assassin identify Merlin after all rounds.\nHowever, our study aims to assess the teamwork\nof LLM agents, not the accuracy of assassinations,\nthough this feature is included for prospective fu-\nture research.\n2.3\nObservation Understanding\nThe AvalonPlay benchmark includes a moderator\nresponsible for overseeing the game’s progress.\nThis involves providing action descriptions to\nagents and recording events that occur during game-\nplay. The moderator’s role is non-interactive with\nthe LLMs but adheres to predefined procedures.\nThe information generated by the moderator is a\nvital part of the agents’ observation.\nWe develop structured prompts to assist LLM\nagents in comprehending both the game’s rules\nand its current state. In general, an LLM agent’s\nobservation is divided into four key variables to\nnavigate the game:\n• Game Rule: We begin with a basic understand-\ning of the game (e.g., “You are playing the\nAvalon game with other players...”).\n• Role Assignment: For each player, we ran-\ndomly assign their roles and create a unique role\nassignment prompt (e.g., Merlin is informed:\n“You are Merlin...Your objective is to\nensure the success of the quests as much\nas possible”).\n• Role Identification: Roles with special abili-\nties get extra information (e.g., Merlin receives\na moderator’s note: “Players 1, 2, and 6\nbelong to the evil side”).\n• Game History: This includes the game trajec-\ntory from previous rounds, as well as earlier time\nsteps of the current round.\n• Action Description: This includes a description\nof the next possible actions for the agent, pro-\nvided by the moderator (e.g., Player 2 receives\na moderator’s note: “Now it’s your turn,\nplayer 2.\nPlease discuss whether to\nagree with the nominated team”).\nThe game rule, role assignment, and role identi-\nfication collectively constitute the global prompt,\nconsistently employed at each time step. Game\nhistory and action description serve as real-time\nprompt, effectively capturing the game’s present\nstate. For additional details, please refer to Ap-\npendix A.\n3\nMethodology\n3.1\nOverview\nReasoning about various potential teammate types\nis a key issue in AHT research. To improve the\nagent’s reasoning abilities, we introduce the Code-\nAct agent, which utilizes the code-based reasoning\ncapabilities of LLMs. Our CodeAct agent does\nnot focus on creating prompts or demonstrations\nfor specific tasks. Instead, it emphasizes a general\nstrategy that transforms complex semantic tasks\ninto flexible code structures.\nOur approach unfolds in three stages, as shown\nin Figure 2. Firstly, we establish a memory retrieval\nsystem to extract information from previous interac-\ntions. Then, we integrate the code-driven reasoning\nwith action to reason teammate roles. Lastly, we\nemploy an interpreter to execute the code and equip\nthe agent with the self-debug capability.\n3.2\nMemory Retrieval\nDuring game playing, we implement global mem-\nory for each LLM agent to prevent the over-\nextension of token inputs. Particularly during the\nteam selection phase, the team leader is equipped\nwith a customized memory module, strategically\ndesigned to aid in deducing the roles of its team-\nmates.\nGlobal Memory\nThe global memory for each\nagent comprises identified key information along\nwith messages from the most recent k steps. This\nkey information includes: 1) Notes from the game\nmoderator; 2) Contributions from agents during dis-\ncussions, which reveal details of personal identities\nor deduce the identities of others.\nLeader-Specific Memory\nLLM agents often\nbase their reasoning on the latest information and\nfrequently experience hallucinations, as detailed in\nSection 4.5. In response, for each game round’s\nteam selection stage, we systematically retrieve fac-\ntual data gathered from earlier interactions to aid\nthe current leader. This includes: 1) Private Infor-\nmation, derived from unique skills tied to an agent’s\nrole, like Percival’s insight: “Among players 1\nand 4, one is good and the other evil, but\nyou can’t discern who is who.” 2) Public In-\nformation, pertaining to historical game outcomes\nand events, accessible to all agents, such as: “In\nthe initial round, players 1 and 2 were\nselected for the quest, which ended in\nfailure.”\nYou are engaged in a game with 7\nplayers , each potentially\ncategorized as 'good ' or 'bad '.\nThere are 4 good players and 3 bad\nplayers among them.\nYou have the fllowing information.\n{PRIVATE_INFORMATION}\n{PUBLIC_INFORMATION}\nPlease generate a code program that\nselects {self._current_team_number}\nplayers most likely to be good\nperson. Please print the final 4\nplayers.'''})\nFigure 3: An example of the CodeAct agent’s prompt.\n3.3\nCode-driven Reasoning with Action\nIn the team selection phase, we utilize a customized\nmemory system, allowing the current leader to de-\nduce the roles of its teammates. Drawing inspi-\nration from ReAct (Yao et al., 2022), our agent is\ndesigned for interactive generation of both thoughts\nand actions through a program in a zero-shot man-\nner. We utilize a program that processes useful\ninformation by reasoning over the agent’s current\nprivate and public data to support future actions.\nAs illustrated in Figure 3, we further enhance this\nprocess by enabling the leader to generate reason-\ning substeps in a code-like format. In this study,\nwe focus on Python, but our approach is applicable\nAction: Python_REPL\nAction Output:\nplayers = [None] * 7\n# Initialize the\nplayers list with None\n# Apply Round 2 information\nplayers [1] = True\n# Player 2 is good\nplayers [3] = True\n# Player 4 is good\nplayers [6] = True\n# Player 7 is good\n# Apply Round 3 information\n# Since Player 2 and Player 4 are good ,\nPlayer 1 must be bad\nplayers [0] = False\n# Player 1 is bad\n# Apply Round 4 information\n# Since Player 2 and Player 7 are good ,\nand we know Player 1 is bad , Player\n3 must be good\nplayers [2] = True\n# Player 3 is good\n# This leaves Player 5 as the only\npossible bad player from this group\nplayers [4] = False\n# Player 5 is bad\n# We already have 2 bad players (Player\n1 and Player 5), so Player 6 must\nbe good\nplayers [5] = True\n# Player 6 is good\n# Print the final 4 players most likely\nto be good\ngood_players = [index + 1 for index ,\nis_good in enumerate(players) if\nis_good]\nprint(good_players)\nObservation: [2, 3, 4, 6, 7]\nFinal Answer: The teammates are Players\n2, 3, 4, and 6.\nFigure 4: An example of the CodeAct agent’s output.\nto any programming language. Figure 4 provide\nan excerpt generated by our CodeAct, with more\ndetails available in Appendix B.\nOur code-driven approach, on the one hand,\nleverages advantages from semantic reasoning that\ninvolve intermediate steps. On the other hand, it\nleverages code-based reasoning, enabling reliable\ninference through the formal yet expressive code\nstructure and the powerful computational tools it\noffers.\n3.4\nCode Execution with Self-Debug\nWe equip the leader with the reflection mechanism\n(Shinn et al., 2023; Chen et al., 2023b), allowing\nthem to refine their programs. The code-based\naction, once formulated, is then processed by a\ncode interpreter. In this study, we equip the agent\nwith a Read-Eval-Print-Loop Python interpreter\n(i.e., Python_REPL). If the code executes success-\nfully, the agent’s decision is informed by the re-\nsulting program output. Conversely, if execution\nfalters, the agent will revise the code, taking into\naccount the issue encountered and previous outputs\nto enhance its generation.\n4\nExperiments\nOur experiments aim to explore the following ques-\ntions: 1) The AHT capabilities of agents employing\ndifferent LLMs as backend models; 2) The effect of\na natural language-based communication protocol\non the AHT capabilities of LLM agents; 3) The\nimpact of diverse prompting strategies on the AHT\ncapabilities of LLM agents. We compare the pro-\nposed CodeAct agent with other semantic-based\nreasoning approaches.\n4.1\nBaselines\nTo tackle question 1, we conduct a preliminary eval-\nuation of three advanced LLMs within the multi-\nagent AvalonPlay benchmark. These models in-\nclude GPT-3.5, GPT-4, and LLaMA 2. Specifically,\nwe utilize the versions gpt-3.5-turbo-0613,\ngpt-4-1106-preview, and Llama2-chat-13B re-\nspectively. These LLM agents, equipped with dif-\nferent backend LLMs, are deployed on both the\ngood and evil sides to compete against one another.\nTo address question 2, we compare scenarios\nin AvalonPlay both with and without communica-\ntion protocols. In scenarios lacking communication\nprotocols, all players proceed to a secret vote im-\nmediately following the team leader’s selection of\nthe team.\nTo tackle question 3, we focus on the team selec-\ntion by the leader, compare semantic reasoning ap-\nproaches (i.e., Chain of Thought (Wei et al., 2022)\nand ReAct (Yao et al., 2022)) with our proposed\nCodeAct agent.\n4.2\nMetrics\nWe design the following three metrics to measure\nan LLM agent’s performance in the AvalonPlay\nbenchmark.\n• Game Win Rate by Quest (Game Win), which\nrefers to the percentage of games where the good\nside wins by completing three out of five quests.\n• Total Quest Win Rate (Quest\nWin), which\nrefers to the percentage of quests where the good\nside wins.\n• Team Selection Accuracy (Team Acc), which\nrefers to the percentage of times the just side,\n(a) Game Win Rate by Quest\n(b) Total Quest Win Rate\n(c) Team Selection Accuracy\nFigure 5: The results of different models acting as the good and evil sides, playing games against each other. Under\neach setting, 30 games were conducted, totaling 150 quests.\n(a) GPT-3.5 as Good Side\n(b) GPT-4 as Good Side\n(c) LLaMA2 as Good Side\nFigure 6: The turn-by-turn quest win rate results when different models assume the role of the good side. Each\nmodel, as the good side, played 90 games, resulting in 450 quests evenly distributed across 5 turns.\nwhen in the role of leader, selects the team for a\nsuccessful quest.\n4.3\nImplementation Details\nFor all LLM agent, we set the sampling temperature\nas 0.7 for text generation and 0 for code generation.\nWe set the window size k to be 15 for the global\nmemory retrieval. During gameplay, all roles are\nrandomly assigned to each player to avoid biasing\nthe data towards certain players based on the turn\norder.\n4.4\nMain Results\nQuestion 1: Model Performance.\nFigure 5 dis-\nplays the performance of agents with three different\nbackend LLMs, playing on different sides. Figure 6\nshows the quest success rates of these LLM agents\nas the good side across different rounds. Our find-\nings indicate that 1) More advanced LLMs achieve\nhigher win rates both in games and in overall tasks\nwhen playing as the good side, with GPT-4 agents\nexhibiting the best performance. 2) Notably, GPT-\n4 agents demonstrate higher quest success rates\nin later rounds compared to earlier ones, indicat-\ning their ability to reason from observations. In\ncontrast, GPT-3.5 and LLaMA agents do not show\nsignificant progress across rounds. This suggests\nthat the GPT-4 agent is more adept at learning and\nadapting, leading us to utilize the GPT-4 model in\nsubsequent experiments.\nQuestion 2: Communication Channel.\nTable 2\ncompares the GPT-4 agent’s performance in self-\nplaying, both with and without communication.\nThe introduction of communication does not sig-\nnificantly enhance the game performance of LLM\nagents. This is possibly due to the fact that, besides\nthe communication protocol, LLM agents also have\naccess to observe and reason based on the outcomes\nof quests in previous rounds. These findings indi-\ncate that communication does not always lead to\nimproved collaborative efficiency in LLM agents.\nWe further explore this phenomenon in Section 4.5.\nQuestion 3: Code-driven Reasoning.\nWe cam-\npare the proposed CodeAct with two semantic rea-\nsoning methods: Chain of Thought (CoT) and Re-\nAct. Table 3 shows that our proposed CodeAgent\nachieves the best performance across three metrics,\nwith a team selection accuracy of 0.830, demon-\nstrating the superiority of code-based reasoning.\n4.5\nQuantitative Analysis\nIn our preliminary analysis of the experimental\nlogs targeting question 1, we observe that employ-\nThe 5th Round Team Selection and Discussion\nModerator:\nPlayer 3, as the team leader, please select 4 team members to undertake the quest. You may also choose yourself.\nPlayer 3:\nI choose player 2, player 3, player 6, and player 7 for the quest. (Team selection by the current leader)\nModerator:\nThe selected team by the leader include [’player 2’, ’player 3’, ’player 6’, ’player 7’]. Now everyone discuss if you agree the team.\nPlayer 1:\nThis team includes players who have been part of successful quests before. It’s a logical choice to maintain a strong team dynamic. I\nagree with the team selection.\nPlayer 2:\nPlayer 3, player 6, and player 7 were on the last successful quest with me, and I believe this consistency will help us ensure another\nvictory. I’m in favor of this team. (In the previous round, players 2, 3, 6, and 7 executed the quest together and succeeded. Player 2\neffectively used this information for reasoning)\nPlayer 3:\nI’ve chosen players who have either been part of successful quests or have not shown any suspicious behavior so far. It’s important\nto maintain a strong team dynamic, and I believe this selection will lead us to victory.\nPlayer 4:\nThe team seems solid with previous successful quest members. Including player 6 could either solidify their trustworthiness or\nprovide us with more information. I agree to proceed.\nPlayer 5:\nThe inclusion of player 6, who hasn’t been on a quest recently, along with proven successful quest members, provides a chance to\nvalidate their loyalty. We should be wary, but I am inclined to give this team a chance. (Player 5, however, produced hallucination,\nwith agent’s statement being inconsistent with the facts shared by everyone)\nPlayer 6:\nThe team includes players who have consistently been part of successful quests, and as a trusted member myself, I feel confident we\ncan achieve success once more. I agree with this selection and am prepared to contribute to the quest’s success.\nPlayer 7:\nThe team selected by player 3 includes myself and other players who have been part of successful quests, which instills confidence\nin this group’s potential for success. Player 6’s inclusion is a variable, but it is essential to evaluate all players. Based on the track\nrecord of success we’ve had, I agree with this team selection and am optimistic about our prospects.\nTable 1: An example of the GPT-4 agent’s participation in discussions during gameplay. In the fifth round, the\nagent’s response is based on previous observations and demonstrates some reasoning ability. However, the agent\nutilizes only limited information and is prone to experiencing hallucinations.\nTable 2: Results from the GPT-4 agent self-playing\nin the AvalonPlay benchmark, both with and without\ncommunication. Under each setting, 30 games were\nconducted, totaling 150 quests.\nAgent\/ Metric\nGame Win\nQuest Win\nTeam Acc\nBase\n0.500\n0.513\n0.640\nBase W\/O Communication\n0.466\n0.480\n0.693\nTable 3: Results of the GPT-4 agent self-playing in\nthe AvalonPlay benchmark using different prompting\nstrageties. Under each setting, 30 games were con-\nducted, totaling 150 quests.\nAgent\/ Metric\nGame Win\nQuest Win\nTeam Acc\nBase with CoT\n0.533\n0.547\n0.707\nBase with ReAct\n0.500\n0.526\n0.634\nCodeAct\n0.633\n0.593\n0.830\ning LLM agents in multi-turn tasks often results in\nmemory forgetting. In cooperative tasks involving\ncommunication, these agents tend to exhibit cas-\ncading hallucinations and logical inconsistencies.\nWe present Table 1 for a more detailed illustration.\nForgetting Early Information.\nPlayers typically\nfocus on information from the last round, failing to\nintegrate key information from the early stages of\nthe game into their later decisions and strategies.\nHallucination Generation.\nIn the game, state-\nments that contradict a player’s actual role are not\nclassified as hallucinations, as deception is a com-\nmon strategy. Similarly, fabricating scenarios to\nfalsely accuse others is also not considered halluci-\nnations, as this mirrors the strategies of human play-\ners. We define inconsistencies within a response\nand counterfactual content in an iteration as hal-\nlucinations. When GPT-3.5 and LLaMa 2 models\nserve as agents, they tend to produce responses in-\nconsistent with the prompt. A notable example is\ntheir role as team leaders, where they often select\na number of teammates that diverges from the re-\nquired amount. Additionally, the final votes they\ngenerate may not correspond with the reasoning\nprocess outlined in the previous step. This problem\nis less pronounced with the GPT-4 agent during\ngameplay. However, all models, including GPT-4,\nare prone to generating responses that contradict\nestablished facts, such as modifying or denying\nevents from previous game rounds. Table 1 pro-\nvides an example of GPT-4 agent’s participation in\ndiscussions. Player 5’s statements are inconsistent\nwith the facts. As player 2 points out, playe 6 had\nalready successfully participated in the quest in the\nprevious round.\nThis suggests that the communication protocol\ndoes not necessarily offer optimal solutions. The\nmost important aspect is the effective verification of\nthe factuality and correctness of communications.\nThis distinguishes it from prior AHT research with\nagents trained from historical data (Mirsky et al.,\n2020). In our study, we address this by utilizing\nmemory retrieval to extract factual information.\nAdditionally, we enhance the agent’s reasoning\nabilities for different types of teammates by in-\ntegrating code-based reasoning with actions.\n5\nRelated Work\nLLMs and Agents.\nThe advent of generative\nagents for interactive tasks marks a significant\nparadigm shift. Unlike traditional methods that re-\nlied on Deep Reinforcement Learning (DRL) (Shi\net al., 2022, 2023; Xu et al., 2020, 2021, 2022),\nwhich confined agents to constrained settings, the\nemergence of LLMs has notably broadened the\nscope of possibilities (OpenAI, 2023). Efforts are\nincreasingly focused on leveraging LLMs for com-\nplex tasks like planning and reasoning (Wang et al.,\n2023c; Wu et al., 2023; Dong et al., 2023). In-\nnovative prompting methods, including ReACT\n(Yao et al., 2022), Chain of Thought (CoT) (Wei\net al., 2022), and Tree of Thought (ToT) (Yao\net al., 2023), significantly enhance the planning\nand reasoning capabilities of LLMs.\nSome re-\nsearch integrates complex human cognitive func-\ntions into LLM agents, such as memory (Wang\net al., 2023b), reflection (Shinn et al., 2023), and\ntool usage (Huang et al., 2023).\nMulti-agent Interaction.\nRecent research has\nexplored that utilizing the interactions among mul-\ntiple LLM agents to achieve several goals (Chen\net al., 2023a; Wu et al., 2023). These studies fo-\ncus on two main areas. First, it involves assigning\ndistinct roles to agents to enhance their task per-\nformance (Wang et al., 2023a). Notable applica-\ntions include using multiple agents in debates to\ncounter self-reflection biases (Liang et al., 2023;\nChan et al., 2023), and diversifying agent roles for\ncomplex projects like software development (Qian\net al., 2023; Hong et al., 2023). Second, researchers\ndevelop multiple LLM agents in sandbox environ-\nments to mimic real human behavior (Park et al.,\n2023; Zhou et al., 2023). Some of these investigate\nthe capabilities that emerge in multi-agent settings,\nsuch as deception and leadership (Xu et al., 2023).\nAd Hoc Teamwork.\nAd Hoc teamwork aims to\ngeneralise the agent’s cooperation capacity with\nnovel agents (Stone et al., 2010; Du et al., 2023)\nThe progress in AHT has shifted from relying on\npreset action-selection rules to a data-driven strat-\negy. This strategy employs methods like proba-\nbilistic models (Rahman et al., 2023), deep learn-\ning (Chen et al., 2020), or reinforcement learning\n(Rahman et al., 2021; Lou et al., 2023; Yan et al.,\n2023) to predict the behaviors of various agents\nin different states, aiming to optimize and refine\nthe learner’s behavior. Recent research has also\nexplored the communication in AHT, considering\nscenarios where teammates follow a shared com-\nmunication protocol (Macke et al., 2021; Mirsky\net al., 2020). However, these studies are often con-\nfined to a limited multi-agent setting. Our work\nstands apart from previous studies by focusing on\ndynamic environments based on natural language\ncommunication. We employ agents powered by\nLLMs to execute AHT, and explore ways to im-\nprove their performance in these settings.\n6\nConclusion and Future Work\nIn this study, we investigate the AHT capabilities\nof LLM agents using the AvalonPlay benchmark.\nLLMs endow agents with powerful reasoning abil-\nities, but challenges such as early memory forget-\nting and hallucination generation also arise. We\naddress these challenges by proposing the CodeAct\nagent. We first employ a memory retrieval system\nto collect factual information from prior interac-\ntions. Subsequently, we integrate a code-driven\nreasoning with action to discern new teammate\nroles, enhancing our agent’s ability to cooperate\nmore effectively. Lastly, we employ an interpreter\nto execute the code and equip the agent with the\nself-debug capability.\nDespite our initial research, there are some un-\nresolved issues in the AHT research with lan-\nguage agents that require further investigation.\nFor intance, our AvalonPlay benchmark involves\nmoderator-guided communication for agents, mak-\ning it crucial to investigate the agent’s ability to\nautonomously decide when to communicate. Fur-\nthermore, significant work lies in reducing the ef-\nfects of hallucinations and enhancing their practical\napplication in real-world scenarios. In this study,\nwe employ a memory retrieval system for factual\ninformation extraction, and another promising re-\nsearch direction involves developing systems for\nevaluating and verifying facts to address these chal-\nlenges.\n7\nLimitations\nRegarding limitations, our study does not incorpo-\nrate experience pools from human players. Future\nresearch will focus on developing robust strategies\nfor leveraging this experience and enhancing com-\nparison with human performance.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nCooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game\n```\n#### 2. 论文摘要\n```\nMulti-agent collaboration with Large Language Models (LLMs) demonstrates\nproficiency in basic tasks, yet its efficiency in more complex scenarios\nremains unexplored. In gaming environments, these agents often face situations\nwithout established coordination protocols, requiring them to make intelligent\ninferences about teammates from limited data. This problem motivates the area\nof ad hoc teamwork, in which an agent may potentially cooperate with a variety\nof teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork\nproblem where the agent operates in an environment driven by natural language.\nOur findings reveal the potential of LLM agents in team collaboration,\nhighlighting issues related to hallucinations in communication. To address this\nissue, we develop CodeAct, a general agent that equips LLM with enhanced memory\nand code-driven reasoning, enabling the repurposing of partial information for\nrapid adaptation to new teammates.\n```\n\n#### 3. 论文全文\n```\nCooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork\nin the Avalon Game\nZijing Shi1, Meng Fang2, Shunfeng Zheng1, Shilong Deng2, Ling Chen1, Yali Du3\n1 AAII, University of Technology Sydney, Australia\n{zijing.shi, shunfeng.zheng}@student.uts.edu.au, ling.chen@uts.edu.au\n2 University of Liverpool, United Kingdom\n{Meng.Fang, shilong.deng}@liverpool.ac.uk\n3 King’s College London, United Kingdom\nyali.du@kcl.ac.uk\nAbstract\nMulti-agent collaboration with Large Language\nModels (LLMs) demonstrates proficiency in\nbasic tasks, yet its efficiency in more complex\nscenarios remains unexplored. In gaming en-\nvironments, these agents often face situations\nwithout established coordination protocols, re-\nquiring them to make intelligent inferences\nabout teammates from limited data. This prob-\nlem motivates the area of ad hoc teamwork,\nin which an agent may potentially cooperate\nwith a variety of teammates to achieve a shared\ngoal. Our study focuses on the ad hoc team-\nwork problem where the agent operates in an\nenvironment driven by natural language. Our\nfindings reveal the potential of LLM agents in\nteam collaboration, highlighting issues related\nto hallucinations in communication. To address\nthis issue, we develop CodeAct, a general agent\nthat equips LLM with enhanced memory and\ncode-driven reasoning, enabling the repurpos-\ning of partial information for rapid adaptation\nto new teammates.\n1\nIntroduction\nLarge Language Models (LLMs) have exhibited im-\npressive abilities in reasoning and generalization,\nshowing their potential in building autonomous\nagents and driving many advancements (Kojima\net al., 2022; Xi et al., 2023).\nRecent research\ninvolving multiple LLM agents working together\nhas shown success in task execution (Wang et al.,\n2023c; Wu et al., 2023; Dong et al., 2023). How-\never, these studies often employ explicit coordina-\ntion protocols for fully cooperative tasks, assum-\ning full information of the teammate and the task.\nWhile some research has explored simulating LLM\nagents in more open-ended settings, these agents\nremain limited by pre-established strategies and\nbehaviors (Park et al., 2023).\nIn dynamic environments, maintaining consis-\ntent coordination protocols among agents is a chal-\nlenge. Such contexts demand an agent capable of\nswift adaptation and on-the-fly cooperation. With-\nout pre-established team strategies, an agent, often\nreferred to as the learner, must observe its team-\nmates’ behaviors and adapt effectively to achieve\ntheir shared goals. This challenge is known as the\nAd Hoc Teamwork (AHT) problem (Stone et al.,\n2010; Macke et al., 2021).\nA significant challenge for the learner is to infer\nthe role of their teammates. Traditional methods\nrely on pre-programmed rules or data-driven al-\ngorithms trained on historical data (Mirsky et al.,\n2022). In contrast, LLM agents can directly com-\nmunicate with their teammates in natural language.\nHowever, the communication dynamics within the\nenvironment may exhibit conflicting and competi-\ntive elements. Moreover, due to the inherent traits\nof LLMs, teammates may provide inaccurate or\nmisleading information.\nIn this study, we explore the AHT abilities of\nLLM agents, utilizing the multi-agent game The\nResistance: Avalon to simulate AHT environments.\nWe introduce the AvalonPlay benchmark, which\nchallenges agents with hidden roles to participate in\nstrategic gameplay across multiple rounds. In each\nround, a learner is tasked with selecting teammates\nwithout prior strategies and full information. Our\nresearch integrates LLM agents into AvalonPlay to\nevaluate their performance in AHT. Rather than de-\nveloping coordination protocols for the entire team,\nwe prioritize designing agents adept at cooperating\nin environments without such explicit protocols.\nA communication channel is designed within the\nAvalonPlay benchmark to enhance collaboration.\nWe observe that LLM agents may sometimes pro-\nduce hallucinations and forget early information.\nTo address this, we introduce CodeAct, a general\nagent leveraging the advanced reasoning capabili-\nties of LLMs, focusing on reasoning about various\npotential teammate types. We begin with a memory\nretrieval system that collects information and uti-\nlize the knowledge gained from these interactions.\narXiv:2312.17515v1  [cs.CL]  29 Dec 2023\nFigure 1: A flowchart of the AvalonPlay benchmark showing team sides and roles on the left and a detailed round\npipeline on the right. Each round includes four stages: leader assignment, team selection, discussion and voting,\nquest execution.\nSubsequently, we incorporate code-driven reason-\ning with action to effectively refer the roles of new\nteammates. With the teammate role information,\nour agent is well-equipped to effectively cooperate\nwith its teammates.\nIn summary, our contributions are as follows:\n• We explore the AHT capabilities of LLM\nagents, utilizing the AvalonPlay benchmark\nwith its multi-round collaborative tasks as a\ntesting ground for these abilities.\n• We develop CodeAct, an LLM agent utilizing\na code-driven prompting approach, aimed at\nboosting the agent’s efficiency in AHT tasks.\n• We conduct comprehensive experiments\nwithin the AvalonPlay benchmark, demon-\nstrating the effectiveness of our proposed\nagent.\n2\nThe AvalonPlay Benchmark\nWe introduce the AvalonPlay benchmark to ex-\nplore the performance of LLM agents on AHT.\nThis benchmark is a language-based, multi-agent\nplatform comprised of multi-round tasks. During\nthe gameplay, agents have limited knowledge about\neach other’s roles. In each round, a team leader,\nalso regarded as the learner, is selected to deduce\nthe roles and intentions of other agents. The leader\nmust adapt their team’s strategies dynamically as\nthe game evolves. This section details the key ele-\nments of the proposed benchmark.\n2.1\nTeammate Role\nWe utilize a setup involving seven players indexed\nby j, each controlled by an LLM agent. In each\ngame, agents are randomly assigned one of six\ndistinct roles, dividing them into two factions: the\njust side (including Merlin, Percival, and two\nLoyal Servants of Arthur) and the evil side\n(including Morgana, Minion, and Assassin).\nEach role possesses unique abilities, as shown\non the left of Figure 1. On the just side, Merlin\nis aware of the factions but not the specific\nroles, while Percival can recognize Merlin and\nMorgana but is unable to distinguish between them.\nThe Loyal Servants of Arthur lack any special\nknowledge. On the evil side, in contrast, all roles\ncan recognize their teammates. This setup puts the\njust side at a disadvantage due to their limited in-\nformation. Our study focuses on scenarios where\nthe just side takes the lead role, aiming to enhance\nthe leader’s ability to deduce the various roles of\nagents.\n2.2\nPipeline\nIn AvalonPlay, a single game is composed of five\nquest rounds indexed by i. Each round, defined as\na full quest cycle, is divided into the following four\nphases, as shown in the right of Figure 1.\n• Leader Assignment: Each round begins with\nthe selection of a leader who nominates team-\nmates for the quest. The first round’s leader\nis randomly chosen, and in subsequent rounds,\nFigure 2: An overview of the proposed CodeAct agent as the leader during team selection. We begins by establishing\na memory retrieval system that distills information from past interactions, enabling the agent to access relevant\ninformation. Then, we integrate code-driven reasoning with action to determine teammate roles effectively. Finally,\nwe employ an interpreter to execute the generated code, equipping the agent with self-debug capabilities.\nleader rotates sequentially to the next player.\nThis approach ensures diverse leader roles while\nincorporating randomness.\n• Team Selection: In this phase, a leader nomi-\nnates ni teammates, with the team size ni vary-\ning per round. Adhering to the game rule, the\nteam sizes are: n1 = 2, n2 = 3, n3 = 3, n4 =\n4, n5 = 4.\nWe align the leader’s response\ninto corresponding actions. For instance, the\nresponse “I choose myself and player 2\nfor the quest team” is mapped to the specific\nselection of teammates.\n• Discussion and Voting: We design a communi-\ncation protocol that enables the agent to acquire\ninformation while accounting for the potential\nof misinformation. Post team selection, agents\nparticipate in discussions to assess the proposed\nteam. Following the discussion, each agent se-\ncretly votes 1j indicates agent j’s approval of\nthe current nominated team. To enhance agents’\ndecision-making, they are required to generate\na JSON containing their reasoning and vote. A\nteam is approved if the total votes reaches a ma-\njority (i.e., P7\nj=1 1j ≥4). Otherwise, the team\nselection is repeated with a new leader. A team is\nautomatically formed if approval is not achieved\nafter five consecutive attempts.\n• Quest Execution: For an approved team, team-\nmates from the just side must vote to approve,\nwhile teammates from the evil side must vote\nagainst. Let Qi denote the total votes of evil\nmembers. The quest is sabotaged if Qi ≥1\nfor rounds i ∈{1, 2, 3}, or Qi ≥2 for rounds\ni ∈{4, 5}. In other cases, it is completed suc-\ncessfully.\nWe adhere to game rule by including an assassi-\nnation phase in the Avalonplay benchmark, where\nthe Assassin identify Merlin after all rounds.\nHowever, our study aims to assess the teamwork\nof LLM agents, not the accuracy of assassinations,\nthough this feature is included for prospective fu-\nture research.\n2.3\nObservation Understanding\nThe AvalonPlay benchmark includes a moderator\nresponsible for overseeing the game’s progress.\nThis involves providing action descriptions to\nagents and recording events that occur during game-\nplay. The moderator’s role is non-interactive with\nthe LLMs but adheres to predefined procedures.\nThe information generated by the moderator is a\nvital part of the agents’ observation.\nWe develop structured prompts to assist LLM\nagents in comprehending both the game’s rules\nand its current state. In general, an LLM agent’s\nobservation is divided into four key variables to\nnavigate the game:\n• Game Rule: We begin with a basic understand-\ning of the game (e.g., “You are playing the\nAvalon game with other players...”).\n• Role Assignment: For each player, we ran-\ndomly assign their roles and create a unique role\nassignment prompt (e.g., Merlin is informed:\n“You are Merlin...Your objective is to\nensure the success of the quests as much\nas possible”).\n• Role Identification: Roles with special abili-\nties get extra information (e.g., Merlin receives\na moderator’s note: “Players 1, 2, and 6\nbelong to the evil side”).\n• Game History: This includes the game trajec-\ntory from previous rounds, as well as earlier time\nsteps of the current round.\n• Action Description: This includes a description\nof the next possible actions for the agent, pro-\nvided by the moderator (e.g., Player 2 receives\na moderator’s note: “Now it’s your turn,\nplayer 2.\nPlease discuss whether to\nagree with the nominated team”).\nThe game rule, role assignment, and role identi-\nfication collectively constitute the global prompt,\nconsistently employed at each time step. Game\nhistory and action description serve as real-time\nprompt, effectively capturing the game’s present\nstate. For additional details, please refer to Ap-\npendix A.\n3\nMethodology\n3.1\nOverview\nReasoning about various potential teammate types\nis a key issue in AHT research. To improve the\nagent’s reasoning abilities, we introduce the Code-\nAct agent, which utilizes the code-based reasoning\ncapabilities of LLMs. Our CodeAct agent does\nnot focus on creating prompts or demonstrations\nfor specific tasks. Instead, it emphasizes a general\nstrategy that transforms complex semantic tasks\ninto flexible code structures.\nOur approach unfolds in three stages, as shown\nin Figure 2. Firstly, we establish a memory retrieval\nsystem to extract information from previous interac-\ntions. Then, we integrate the code-driven reasoning\nwith action to reason teammate roles. Lastly, we\nemploy an interpreter to execute the code and equip\nthe agent with the self-debug capability.\n3.2\nMemory Retrieval\nDuring game playing, we implement global mem-\nory for each LLM agent to prevent the over-\nextension of token inputs. Particularly during the\nteam selection phase, the team leader is equipped\nwith a customized memory module, strategically\ndesigned to aid in deducing the roles of its team-\nmates.\nGlobal Memory\nThe global memory for each\nagent comprises identified key information along\nwith messages from the most recent k steps. This\nkey information includes: 1) Notes from the game\nmoderator; 2) Contributions from agents during dis-\ncussions, which reveal details of personal identities\nor deduce the identities of others.\nLeader-Specific Memory\nLLM agents often\nbase their reasoning on the latest information and\nfrequently experience hallucinations, as detailed in\nSection 4.5. In response, for each game round’s\nteam selection stage, we systematically retrieve fac-\ntual data gathered from earlier interactions to aid\nthe current leader. This includes: 1) Private Infor-\nmation, derived from unique skills tied to an agent’s\nrole, like Percival’s insight: “Among players 1\nand 4, one is good and the other evil, but\nyou can’t discern who is who.” 2) Public In-\nformation, pertaining to historical game outcomes\nand events, accessible to all agents, such as: “In\nthe initial round, players 1 and 2 were\nselected for the quest, which ended in\nfailure.”\nYou are engaged in a game with 7\nplayers , each potentially\ncategorized as 'good ' or 'bad '.\nThere are 4 good players and 3 bad\nplayers among them.\nYou have the fllowing information.\n{PRIVATE_INFORMATION}\n{PUBLIC_INFORMATION}\nPlease generate a code program that\nselects {self._current_team_number}\nplayers most likely to be good\nperson. Please print the final 4\nplayers.'''})\nFigure 3: An example of the CodeAct agent’s prompt.\n3.3\nCode-driven Reasoning with Action\nIn the team selection phase, we utilize a customized\nmemory system, allowing the current leader to de-\nduce the roles of its teammates. Drawing inspi-\nration from ReAct (Yao et al., 2022), our agent is\ndesigned for interactive generation of both thoughts\nand actions through a program in a zero-shot man-\nner. We utilize a program that processes useful\ninformation by reasoning over the agent’s current\nprivate and public data to support future actions.\nAs illustrated in Figure 3, we further enhance this\nprocess by enabling the leader to generate reason-\ning substeps in a code-like format. In this study,\nwe focus on Python, but our approach is applicable\nAction: Python_REPL\nAction Output:\nplayers = [None] * 7\n# Initialize the\nplayers list with None\n# Apply Round 2 information\nplayers [1] = True\n# Player 2 is good\nplayers [3] = True\n# Player 4 is good\nplayers [6] = True\n# Player 7 is good\n# Apply Round 3 information\n# Since Player 2 and Player 4 are good ,\nPlayer 1 must be bad\nplayers [0] = False\n# Player 1 is bad\n# Apply Round 4 information\n# Since Player 2 and Player 7 are good ,\nand we know Player 1 is bad , Player\n3 must be good\nplayers [2] = True\n# Player 3 is good\n# This leaves Player 5 as the only\npossible bad player from this group\nplayers [4] = False\n# Player 5 is bad\n# We already have 2 bad players (Player\n1 and Player 5), so Player 6 must\nbe good\nplayers [5] = True\n# Player 6 is good\n# Print the final 4 players most likely\nto be good\ngood_players = [index + 1 for index ,\nis_good in enumerate(players) if\nis_good]\nprint(good_players)\nObservation: [2, 3, 4, 6, 7]\nFinal Answer: The teammates are Players\n2, 3, 4, and 6.\nFigure 4: An example of the CodeAct agent’s output.\nto any programming language. Figure 4 provide\nan excerpt generated by our CodeAct, with more\ndetails available in Appendix B.\nOur code-driven approach, on the one hand,\nleverages advantages from semantic reasoning that\ninvolve intermediate steps. On the other hand, it\nleverages code-based reasoning, enabling reliable\ninference through the formal yet expressive code\nstructure and the powerful computational tools it\noffers.\n3.4\nCode Execution with Self-Debug\nWe equip the leader with the reflection mechanism\n(Shinn et al., 2023; Chen et al., 2023b), allowing\nthem to refine their programs. The code-based\naction, once formulated, is then processed by a\ncode interpreter. In this study, we equip the agent\nwith a Read-Eval-Print-Loop Python interpreter\n(i.e., Python_REPL). If the code executes success-\nfully, the agent’s decision is informed by the re-\nsulting program output. Conversely, if execution\nfalters, the agent will revise the code, taking into\naccount the issue encountered and previous outputs\nto enhance its generation.\n4\nExperiments\nOur experiments aim to explore the following ques-\ntions: 1) The AHT capabilities of agents employing\ndifferent LLMs as backend models; 2) The effect of\na natural language-based communication protocol\non the AHT capabilities of LLM agents; 3) The\nimpact of diverse prompting strategies on the AHT\ncapabilities of LLM agents. We compare the pro-\nposed CodeAct agent with other semantic-based\nreasoning approaches.\n4.1\nBaselines\nTo tackle question 1, we conduct a preliminary eval-\nuation of three advanced LLMs within the multi-\nagent AvalonPlay benchmark. These models in-\nclude GPT-3.5, GPT-4, and LLaMA 2. Specifically,\nwe utilize the versions gpt-3.5-turbo-0613,\ngpt-4-1106-preview, and Llama2-chat-13B re-\nspectively. These LLM agents, equipped with dif-\nferent backend LLMs, are deployed on both the\ngood and evil sides to compete against one another.\nTo address question 2, we compare scenarios\nin AvalonPlay both with and without communica-\ntion protocols. In scenarios lacking communication\nprotocols, all players proceed to a secret vote im-\nmediately following the team leader’s selection of\nthe team.\nTo tackle question 3, we focus on the team selec-\ntion by the leader, compare semantic reasoning ap-\nproaches (i.e., Chain of Thought (Wei et al., 2022)\nand ReAct (Yao et al., 2022)) with our proposed\nCodeAct agent.\n4.2\nMetrics\nWe design the following three metrics to measure\nan LLM agent’s performance in the AvalonPlay\nbenchmark.\n• Game Win Rate by Quest (Game Win), which\nrefers to the percentage of games where the good\nside wins by completing three out of five quests.\n• Total Quest Win Rate (Quest\nWin), which\nrefers to the percentage of quests where the good\nside wins.\n• Team Selection Accuracy (Team Acc), which\nrefers to the percentage of times the just side,\n(a) Game Win Rate by Quest\n(b) Total Quest Win Rate\n(c) Team Selection Accuracy\nFigure 5: The results of different models acting as the good and evil sides, playing games against each other. Under\neach setting, 30 games were conducted, totaling 150 quests.\n(a) GPT-3.5 as Good Side\n(b) GPT-4 as Good Side\n(c) LLaMA2 as Good Side\nFigure 6: The turn-by-turn quest win rate results when different models assume the role of the good side. Each\nmodel, as the good side, played 90 games, resulting in 450 quests evenly distributed across 5 turns.\nwhen in the role of leader, selects the team for a\nsuccessful quest.\n4.3\nImplementation Details\nFor all LLM agent, we set the sampling temperature\nas 0.7 for text generation and 0 for code generation.\nWe set the window size k to be 15 for the global\nmemory retrieval. During gameplay, all roles are\nrandomly assigned to each player to avoid biasing\nthe data towards certain players based on the turn\norder.\n4.4\nMain Results\nQuestion 1: Model Performance.\nFigure 5 dis-\nplays the performance of agents with three different\nbackend LLMs, playing on different sides. Figure 6\nshows the quest success rates of these LLM agents\nas the good side across different rounds. Our find-\nings indicate that 1) More advanced LLMs achieve\nhigher win rates both in games and in overall tasks\nwhen playing as the good side, with GPT-4 agents\nexhibiting the best performance. 2) Notably, GPT-\n4 agents demonstrate higher quest success rates\nin later rounds compared to earlier ones, indicat-\ning their ability to reason from observations. In\ncontrast, GPT-3.5 and LLaMA agents do not show\nsignificant progress across rounds. This suggests\nthat the GPT-4 agent is more adept at learning and\nadapting, leading us to utilize the GPT-4 model in\nsubsequent experiments.\nQuestion 2: Communication Channel.\nTable 2\ncompares the GPT-4 agent’s performance in self-\nplaying, both with and without communication.\nThe introduction of communication does not sig-\nnificantly enhance the game performance of LLM\nagents. This is possibly due to the fact that, besides\nthe communication protocol, LLM agents also have\naccess to observe and reason based on the outcomes\nof quests in previous rounds. These findings indi-\ncate that communication does not always lead to\nimproved collaborative efficiency in LLM agents.\nWe further explore this phenomenon in Section 4.5.\nQuestion 3: Code-driven Reasoning.\nWe cam-\npare the proposed CodeAct with two semantic rea-\nsoning methods: Chain of Thought (CoT) and Re-\nAct. Table 3 shows that our proposed CodeAgent\nachieves the best performance across three metrics,\nwith a team selection accuracy of 0.830, demon-\nstrating the superiority of code-based reasoning.\n4.5\nQuantitative Analysis\nIn our preliminary analysis of the experimental\nlogs targeting question 1, we observe that employ-\nThe 5th Round Team Selection and Discussion\nModerator:\nPlayer 3, as the team leader, please select 4 team members to undertake the quest. You may also choose yourself.\nPlayer 3:\nI choose player 2, player 3, player 6, and player 7 for the quest. (Team selection by the current leader)\nModerator:\nThe selected team by the leader include [’player 2’, ’player 3’, ’player 6’, ’player 7’]. Now everyone discuss if you agree the team.\nPlayer 1:\nThis team includes players who have been part of successful quests before. It’s a logical choice to maintain a strong team dynamic. I\nagree with the team selection.\nPlayer 2:\nPlayer 3, player 6, and player 7 were on the last successful quest with me, and I believe this consistency will help us ensure another\nvictory. I’m in favor of this team. (In the previous round, players 2, 3, 6, and 7 executed the quest together and succeeded. Player 2\neffectively used this information for reasoning)\nPlayer 3:\nI’ve chosen players who have either been part of successful quests or have not shown any suspicious behavior so far. It’s important\nto maintain a strong team dynamic, and I believe this selection will lead us to victory.\nPlayer 4:\nThe team seems solid with previous successful quest members. Including player 6 could either solidify their trustworthiness or\nprovide us with more information. I agree to proceed.\nPlayer 5:\nThe inclusion of player 6, who hasn’t been on a quest recently, along with proven successful quest members, provides a chance to\nvalidate their loyalty. We should be wary, but I am inclined to give this team a chance. (Player 5, however, produced hallucination,\nwith agent’s statement being inconsistent with the facts shared by everyone)\nPlayer 6:\nThe team includes players who have consistently been part of successful quests, and as a trusted member myself, I feel confident we\ncan achieve success once more. I agree with this selection and am prepared to contribute to the quest’s success.\nPlayer 7:\nThe team selected by player 3 includes myself and other players who have been part of successful quests, which instills confidence\nin this group’s potential for success. Player 6’s inclusion is a variable, but it is essential to evaluate all players. Based on the track\nrecord of success we’ve had, I agree with this team selection and am optimistic about our prospects.\nTable 1: An example of the GPT-4 agent’s participation in discussions during gameplay. In the fifth round, the\nagent’s response is based on previous observations and demonstrates some reasoning ability. However, the agent\nutilizes only limited information and is prone to experiencing hallucinations.\nTable 2: Results from the GPT-4 agent self-playing\nin the AvalonPlay benchmark, both with and without\ncommunication. Under each setting, 30 games were\nconducted, totaling 150 quests.\nAgent\/ Metric\nGame Win\nQuest Win\nTeam Acc\nBase\n0.500\n0.513\n0.640\nBase W\/O Communication\n0.466\n0.480\n0.693\nTable 3: Results of the GPT-4 agent self-playing in\nthe AvalonPlay benchmark using different prompting\nstrageties. Under each setting, 30 games were con-\nducted, totaling 150 quests.\nAgent\/ Metric\nGame Win\nQuest Win\nTeam Acc\nBase with CoT\n0.533\n0.547\n0.707\nBase with ReAct\n0.500\n0.526\n0.634\nCodeAct\n0.633\n0.593\n0.830\ning LLM agents in multi-turn tasks often results in\nmemory forgetting. In cooperative tasks involving\ncommunication, these agents tend to exhibit cas-\ncading hallucinations and logical inconsistencies.\nWe present Table 1 for a more detailed illustration.\nForgetting Early Information.\nPlayers typically\nfocus on information from the last round, failing to\nintegrate key information from the early stages of\nthe game into their later decisions and strategies.\nHallucination Generation.\nIn the game, state-\nments that contradict a player’s actual role are not\nclassified as hallucinations, as deception is a com-\nmon strategy. Similarly, fabricating scenarios to\nfalsely accuse others is also not considered halluci-\nnations, as this mirrors the strategies of human play-\ners. We define inconsistencies within a response\nand counterfactual content in an iteration as hal-\nlucinations. When GPT-3.5 and LLaMa 2 models\nserve as agents, they tend to produce responses in-\nconsistent with the prompt. A notable example is\ntheir role as team leaders, where they often select\na number of teammates that diverges from the re-\nquired amount. Additionally, the final votes they\ngenerate may not correspond with the reasoning\nprocess outlined in the previous step. This problem\nis less pronounced with the GPT-4 agent during\ngameplay. However, all models, including GPT-4,\nare prone to generating responses that contradict\nestablished facts, such as modifying or denying\nevents from previous game rounds. Table 1 pro-\nvides an example of GPT-4 agent’s participation in\ndiscussions. Player 5’s statements are inconsistent\nwith the facts. As player 2 points out, playe 6 had\nalready successfully participated in the quest in the\nprevious round.\nThis suggests that the communication protocol\ndoes not necessarily offer optimal solutions. The\nmost important aspect is the effective verification of\nthe factuality and correctness of communications.\nThis distinguishes it from prior AHT research with\nagents trained from historical data (Mirsky et al.,\n2020). In our study, we address this by utilizing\nmemory retrieval to extract factual information.\nAdditionally, we enhance the agent’s reasoning\nabilities for different types of teammates by in-\ntegrating code-based reasoning with actions.\n5\nRelated Work\nLLMs and Agents.\nThe advent of generative\nagents for interactive tasks marks a significant\nparadigm shift. Unlike traditional methods that re-\nlied on Deep Reinforcement Learning (DRL) (Shi\net al., 2022, 2023; Xu et al., 2020, 2021, 2022),\nwhich confined agents to constrained settings, the\nemergence of LLMs has notably broadened the\nscope of possibilities (OpenAI, 2023). Efforts are\nincreasingly focused on leveraging LLMs for com-\nplex tasks like planning and reasoning (Wang et al.,\n2023c; Wu et al., 2023; Dong et al., 2023). In-\nnovative prompting methods, including ReACT\n(Yao et al., 2022), Chain of Thought (CoT) (Wei\net al., 2022), and Tree of Thought (ToT) (Yao\net al., 2023), significantly enhance the planning\nand reasoning capabilities of LLMs.\nSome re-\nsearch integrates complex human cognitive func-\ntions into LLM agents, such as memory (Wang\net al., 2023b), reflection (Shinn et al., 2023), and\ntool usage (Huang et al., 2023).\nMulti-agent Interaction.\nRecent research has\nexplored that utilizing the interactions among mul-\ntiple LLM agents to achieve several goals (Chen\net al., 2023a; Wu et al., 2023). These studies fo-\ncus on two main areas. First, it involves assigning\ndistinct roles to agents to enhance their task per-\nformance (Wang et al., 2023a). Notable applica-\ntions include using multiple agents in debates to\ncounter self-reflection biases (Liang et al., 2023;\nChan et al., 2023), and diversifying agent roles for\ncomplex projects like software development (Qian\net al., 2023; Hong et al., 2023). Second, researchers\ndevelop multiple LLM agents in sandbox environ-\nments to mimic real human behavior (Park et al.,\n2023; Zhou et al., 2023). Some of these investigate\nthe capabilities that emerge in multi-agent settings,\nsuch as deception and leadership (Xu et al., 2023).\nAd Hoc Teamwork.\nAd Hoc teamwork aims to\ngeneralise the agent’s cooperation capacity with\nnovel agents (Stone et al., 2010; Du et al., 2023)\nThe progress in AHT has shifted from relying on\npreset action-selection rules to a data-driven strat-\negy. This strategy employs methods like proba-\nbilistic models (Rahman et al., 2023), deep learn-\ning (Chen et al., 2020), or reinforcement learning\n(Rahman et al., 2021; Lou et al., 2023; Yan et al.,\n2023) to predict the behaviors of various agents\nin different states, aiming to optimize and refine\nthe learner’s behavior. Recent research has also\nexplored the communication in AHT, considering\nscenarios where teammates follow a shared com-\nmunication protocol (Macke et al., 2021; Mirsky\net al., 2020). However, these studies are often con-\nfined to a limited multi-agent setting. Our work\nstands apart from previous studies by focusing on\ndynamic environments based on natural language\ncommunication. We employ agents powered by\nLLMs to execute AHT, and explore ways to im-\nprove their performance in these settings.\n6\nConclusion and Future Work\nIn this study, we investigate the AHT capabilities\nof LLM agents using the AvalonPlay benchmark.\nLLMs endow agents with powerful reasoning abil-\nities, but challenges such as early memory forget-\nting and hallucination generation also arise. We\naddress these challenges by proposing the CodeAct\nagent. We first employ a memory retrieval system\nto collect factual information from prior interac-\ntions. Subsequently, we integrate a code-driven\nreasoning with action to discern new teammate\nroles, enhancing our agent’s ability to cooperate\nmore effectively. Lastly, we employ an interpreter\nto execute the code and equip the agent with the\nself-debug capability.\nDespite our initial research, there are some un-\nresolved issues in the AHT research with lan-\nguage agents that require further investigation.\nFor intance, our AvalonPlay benchmark involves\nmoderator-guided communication for agents, mak-\ning it crucial to investigate the agent’s ability to\nautonomously decide when to communicate. Fur-\nthermore, significant work lies in reducing the ef-\nfects of hallucinations and enhancing their practical\napplication in real-world scenarios. In this study,\nwe employ a memory retrieval system for factual\ninformation extraction, and another promising re-\nsearch direction involves developing systems for\nevaluating and verifying facts to address these chal-\nlenges.\n7\nLimitations\nRegarding limitations, our study does not incorpo-\nrate experience pools from human players. Future\nresearch will focus on developing robust strategies\nfor leveraging this experience and enhancing com-\nparison with human performance.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 探索大型语言模型在动态环境中的协作能力\n\n## 📌 背景痛点\/本文动机\n随着大型语言模型（LLMs）在推理和泛化能力上的不断提升，它们在构建自主代理和推动人工智能领域的发展方面展现出巨大潜力。然而，在多智能体协作中，特别是在没有预先设定的协调协议的动态环境中，LLMs的协作效率仍然是一个未充分探索的领域。本文旨在研究LLMs在动态环境中的协作能力，特别是在没有明确团队策略的情况下，如何与不同的队友进行有效合作。\n\n## 🚀 核心方法\n💡 创新点1：引入AvalonPlay基准\n本文提出了AvalonPlay基准，这是一个基于自然语言的多智能体平台，用于模拟动态环境中的协作任务。在这个基准中，智能体需要在有限的信息和没有预先设定的团队策略的情况下，通过观察队友的行为来推断他们的角色，并动态调整团队策略以实现共同目标。\n\n💡 创新点2：开发CodeAct智能体\n为了解决LLMs在动态环境中协作时可能出现的记忆遗忘和幻觉生成等问题，本文提出了CodeAct智能体。CodeAct利用LLMs的代码驱动推理能力，通过将复杂的语义任务转化为灵活的代码结构，从而提高智能体在动态环境中的协作效率。\n\n## 📈 实验结果\n实验结果表明，GPT-4模型在AvalonPlay基准中表现出最佳的协作能力，而CodeAct智能体在团队选择准确性方面优于其他语义推理方法。此外，实验还发现，引入自然语言通信协议并不总是能显著提高LLMs的协作效率。\n\n## 💬 可借鉴之处\n本文的研究结果表明，LLMs在动态环境中的协作能力仍然面临一些挑战，如记忆遗忘和幻觉生成。为了提高LLMs的协作效率，可以借鉴本文提出的CodeAct智能体的设计思路，利用代码驱动推理和记忆检索系统来增强智能体的推理能力和信息处理能力。此外，还可以进一步研究如何减少幻觉生成的影响，并探索LLMs在现实世界场景中的应用。","llm_summary_res_status":200}
{"title":"Counting to Explore and Generalize in Text-based Games","authors":"Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, Adam Trischler","summary":"We propose a recurrent RL agent with an episodic exploration mechanism that\nhelps discovering good policies in text-based game environments. We show\npromising results on a set of generated text-based games of varying difficulty\nwhere the goal is to collect a coin located at the end of a chain of rooms. In\ncontrast to previous text-based RL approaches, we observe that our agent learns\npolicies that generalize to unseen games of greater difficulty.","url":"http:\/\/arxiv.org\/abs\/1806.11525v2","pdf_url":"http:\/\/arxiv.org\/pdf\/1806.11525v2","published":1530290429000,"comment":null,"pdf_text":"Counting to Explore and Generalize in Text-based Games\nXingdi Yuan * 1 Marc-Alexandre Cˆot´e * 1 Alessandro Sordoni 1 Romain Laroche 1 Remi Tachet des Combes 1\nMatthew Hausknecht 1 Adam Trischler 1\nAbstract\nWe propose a recurrent RL agent with an episodic\nexploration mechanism that helps discovering\ngood policies in text-based game environments.\nWe show promising results on a set of generated\ntext-based games of varying difﬁculty where the\ngoal is to collect a coin located at the end of a\nchain of rooms. In contrast to previous text-based\nRL approaches, we observe that our agent learns\npolicies that generalize to unseen games of greater\ndifﬁculty.\n1. Introduction\nText-based games like Zork (Infocom, 1980) are complex,\ninteractive simulations. They use natural language to de-\nscribe the state of the world, to accept actions from the\nplayer, and to report subsequent changes in the environment.\nThe player works toward goals which are seldom speci-\nﬁed explicitly and must be discovered through exploration.\nThe observation and action spaces in text games are both\ncombinatorial and compositional, and players must contend\nwith partial observability, since descriptive text does not\ncommunicate complete, unambiguous information about\nthe underlying game state.\nIn this paper, we study several methods of exploration in\ntext-based games. Our basic task is a deterministic text-\nbased version of the chain experiment (Osband et al., 2016;\nPlappert et al., 2017) with distractor nodes that are off-chain:\nthe agent must navigate a path composed of discrete loca-\ntions (rooms) to the goal, ideally without revisiting dead\nends. We propose a DQN-based recurrent model for solving\ntext-based games, where the recurrence gives the model\nthe capacity to condition its policy on historical state infor-\nmation. To encourage exploration, we extend count-based\nexploration approaches (Ostrovski et al., 2017; Tang et al.,\n*Equal contribution 1Microsoft Research. Correspondence to:\nEric Yuan <eric.yuan@microsoft.com>, Marc-Alexandre Cˆot´e\n<macote@microsoft.com>.\nPublished at the Exploration in Reinforcement Learning Work-\nshop at the 35 th International Conference on Machine Learning,\nStockholm, Sweden. Copyright 2018 by the author(s).\n2017), which assign an intrinsic reward derived from the\ncount of state visitations during learning, across episodes.\nSpeciﬁcally, we propose an episodic count-based explo-\nration scheme, where state counts are reset at the beginning\nof each episode. This reward plays the role of an episodic\nmemory (Gershman & Daw, 2017) that pushes the agent to\nvisit states not previously encountered within an episode.\nAlthough the recurrent policy architecture has the capacity\nto solve the task by remembering and avoiding previously\nvisited locations, we hypothesize that exploration rewards\nwill help the agent learn to utilize its memory.\nWe generate a set of games of varying difﬁculty (measured\nwith respect to the path length and the number of off-chain\nrooms) with a text-based game generator (Cˆot´e et al., 2018).\nWe observe that, in contrast to a baseline model and stan-\ndard count-based exploration methods, the recurrent model\nwith episodic bonus learns policies that not only complete\nmultiple training games at same time successfully but also\ngeneralize to unseen games of greater difﬁculty.\n2. Text-based Games as POMDPs\nText-based games are sequential decision-making prob-\nlems that can be described naturally by the Reinforce-\nment Learning (RL) setting. Fundamentally, text-based\ngames are partially observable Markov decision processes\n(POMDP) (Kaelbling et al., 1998) where the environment\nstate is never observed directly.\nTo act optimally, an\nagent must keep track of all observations. Formally, a\ntext-based game is a discrete-time POMDP deﬁned by\n(S, T, A, Ω, O, R, γ), where γ ∈[0, 1] is the discount factor.\nEnvironment States (S): The environment state at turn t\nin the game is st ∈S. It contains the complete internal\ninformation of the game, much of which is hidden from the\nagent. When an agent issues a command ct (deﬁned next),\nthe environment transitions to state st+1 with probability\nT(st+1|st, ct).\nActions (A): At each turn t, the agent issues a text com-\nmand ct. The interpreter can accept any sequence of char-\nacters but will only recognize a tiny subset thereof. Further-\nmore, only a fraction of recognized commands will actually\nchange the state of the world. The resulting action space\narXiv:1806.11525v2  [cs.CL]  7 Mar 2019\nCounting to Explore and Generalize in Text-based Games\nis enormous and intractable for existing RL algorithms. In\nthis work, we make the following two simplifying assump-\ntions. (1) Word-level Each command is a two-word se-\nquence where the words are taken from a ﬁxed vocabulary V .\n(2) Command syntax Each command is a (verb, object)\npair (direction words are considered objects).\nObservations (Ω): The text information perceived by the\nagent at a given turn t in the game is the agent’s observation,\not ∈Ω, which depends on the environment state and the\nprevious command with probability O(ot|st, ct−1). Thus,\nthe function O selects from the environment state what\ninformation to show to the agent given the last command.\nReward Function (R): Based on its actions, the agent\nreceives reward signals rt = R(st, at). The goal is to maxi-\nmize the expected discounted sum of rewards E [P\nt γtrt].\n3. Method\n3.1. Model Architecture\nIn this work, we adopt the LSTM-DQN (Narasimhan et al.,\n2015) model as baseline. It has two modules: a representa-\ntion generator ΦR, and an action scorer ΦA. ΦR takes ob-\nservation strings o as input, after a stacked embedding layer\nand LSTM (Hochreiter & Schmidhuber, 1997) encoder, a\nmean-pooling layer produces a vector representation of the\nobservation. This feeds into ΦA, in which two MLPs, shar-\ning a lower layer, predict the Q-values over all verbs wv\nand object words wo independently. The average of the two\nresulting scores gives the Q-values for the composed actions.\nThe LSTM-DQN does not condition on previous actions\nor observations, so it cannot deal with partial observability.\nWe concatenate the previous command ct−1 to the current\nobservation ot to lessen this limitation.\nTo further enhance the agent’s capacity to remember previ-\nous states, we replace the shared MLP in ΦA by an LSTM\ncell. This model is inspired by (Hausknecht & Stone, 2015;\nLample & Chaplot, 2016) and we call it LSTM-DRQN. The\nLSTM cell in ΦA takes the representation generated by ΦR\ntogether with history information ht−1 from the previous\ngame step as input. It generates the state information at the\ncurrent game step, which is then fed into the two MLPs as\nwell as passed forward to next game step. Figure 1 shows\nthe LSTM-DRQN architecture.\n3.2. Discovery Bonus\nTo promote exploration we use an intrinsic reward by count-\ning state visits (Kolter & Ng, 2009; Tang et al., 2017; Martin\net al., 2017; Ostrovski et al., 2017). We investigate two ap-\nproaches to counting rewards. The ﬁrst is inspired by (Kolter\n& Ng, 2009), where we deﬁne the cumulative counting\nbonus as r+(ot) = β · n(ot)−1\/3, where n(ot) is the num-\nFigure 1. LSTM-DRQN processes textual observations word-by-\nword to generate a ﬁxed-length vector representation. This repre-\nsentation is used by the recurrent policy to estimate Q-values for\nall verbs Q(s, v) and objects Q(s, o).\nber of times the agent has observed ot since the beginning\nof training (across episodes), and β is the bonus coefﬁcient.\nDuring training, as the agent observes new states more and\nmore, the cumulative counting bonus gradually converges\nto 0.\nThe second approach is the episodic discovery bonus,\nwhich encourages the agent to discover unseen states by\nassigning a positive reward whenever it sees a new state.\nIt is deﬁned as: r++(ot) =\n\u001a\nβ\nif n(ot) = 1\n0.0\notherwise\n, where n(·)\nis reset to zero at the beginning of each episode. Taking\ninspiration from (Gershman & Daw, 2017), we hope this\nbehavior pushes the agent to visit states not previously en-\ncountered in the current episode and teaches the agent how\nto use its memory for this purpose so it may generalize to\nunseen environments.\n4. Related Work\nRL Applied to Text-based Games: Narasimhan et al.\n(2015) test their LSTM-DQN in two text-based environ-\nments: Home World and Fantasy World. They report the\nquest completion ratio over multiple runs but not how many\nsteps it takes to complete them. He et al. (2015) intro-\nduce the Deep Reinforcement Relevance Network (DRRN)\nfor tackling choice-based (as opposed to parser-based) text\ngames, evaluating the DRRN on one deterministic game and\none larger-scale stochastic game. The DRRN model con-\nverges on both games; however, this model must know in ad-\nvance the valid commands at each state. Fulda et al. (2017)\npropose a method to reduce the action space for parser-\nbased games by training word embeddings to be aware of\nverb-noun affordances. One drawback of this approach is it\nrequires pre-trained embeddings.\nCount-based Exploration:\nThe Model Based Inter-\nval Estimation-Exploration Bonus (MBIE-EB) (Strehl &\nLittman, 2008) derives an intrinsic reward by counting state-\naction pairs with a table n(s, a). Their exploration bonus has\nthe form β\/\np\nn(s, a) to encourage exploring less-visited\npairs. In this work, we use n(s) rather than n(s, a), since\nthe majority of actions leave the agent in the same state\nCounting to Explore and Generalize in Text-based Games\n(i.e., unrecognized commands). Using the latter would re-\nward the agent for trying invalid commands, which is not\nsensible in our setting.\nTang et al. (2017) propose a hashing function for count-\nbased exploration in order to discretize high-dimensional,\ncontinuous state spaces. Their exploration bonus r+ =\nβ\/\np\nn(φ(s)), where φ(·) is a hashing function that can\neither be static or learned. This is similar to the cumulative\ncounting bonus deﬁned above.\nDeep Recurrent Q-Learning: Hausknecht & Stone (2015)\npropose the Deep Recurrent Q-Networks (DRQN), adding\na recurrent neural network (such as an LSTM (Hochreiter\n& Schmidhuber, 1997)) on top of the standard DQN model.\nDRQN estimates Q(ot, ht−1, at) instead of Q(ot, at), so it\nhas the capacity to memorize the state history. Lample &\nChaplot (2016) use a model built on the DRQN architecture\nto learn to play FPS games.\nA major difference between the work presented in this paper\nand the related work is that we test on unseen games and\ntrain on a set of similar (but not identical) games rather than\ntraining and testing on the same game.\n5. Experiments\n5.1. Coin Collector Game Setup\nTo evaluate the two models described above and the pro-\nposed discovery bonus, we designed a set of simple text-\nbased games inspired by the chain experiment (Osband et al.,\n2016; Plappert et al., 2017). Each game contains a given\nnumber of rooms that are randomly connected to each other\nto form a chain (see ﬁgures in Appendix C). The goal is to\nﬁnd and collect a “coin” placed in one of the rooms. The\nplayer’s initial position is at one end of the chain and the\ncoin is at the other. These games have deterministic state\ntransitions.\nGames stop after a set number of steps or after the player\nhas collected the coin. The game interpreter understands\nonly ﬁve commands (go north, go east, go south, go\nwest and take coin), while the action space is twice as\nlarge: {go, take} × {north, south, east, west, coin}.\nSee Figure 12, Appendix C for an example of what the agent\nobserves in-game.\nOur games have 3 modes: easy (mode 0), there are no dis-\ntractor rooms (dead ends) along the path; medium (mode\n1), each room along the optimal trajectory has one distrac-\ntor room randomly connected to it; hard (mode 2), each\nroom on the path has two distractor rooms, i.e., within a\nroom on the optimal trajectory, all 4 directions lead to a\nconnected room. We use difﬁculty levels to indicate the\noptimal trajectory’s length of a game.\nTo solve easy games, the agent must learn to recall its previ-\nous directional action and to issue the command that does\nnot reverse it (e.g., if the agent entered the current room by\ngoing east, do not now go west). Conversely, to solve\nmedium and hard games, the agent must reverse its previous\naction when it enters distractor rooms to return to the chain,\nand also recall farther into the past to track which exits it\nhas already passed through. Alternatively, since there are no\ncycles, it can learn a less memory intensive “wall-following”\nstrategy by, e.g., taking exits in a clockwise order from\nwhere it enters a room.\nWe refer to models with the cumulative counting bonus\nas MODEL+, and models with episodic discovery bonus as\nMODEL++, where MODEL ∈{DQN, DRQN}1 (implemen-\ntation details in Appendix A). In this section we cover part\nof the experiment results, the full extent of our experiment\nresults are provided in Appendix B.\n5.2. Solving Training Games\nWe ﬁrst investigate whether the variant models can learn\nto solve single games with different difﬁculty modes (easy,\nmedium, hard) and levels {L5, L10, L15, L20, L25, L30}2.\nAs shown in Figure 2 (top row), when the games are sim-\nple, vanilla DQN and DRQN already fail to learn. Adding\nthe cumulative bonus helps somewhat and models perform\nsimilarly with and without recurrence. When the games be-\ncome harder, the cumulative bonus helps less, while episodic\nbonus remains very helpful and recurrence in the model be-\ncomes very helpful.\nNext, we are interested to see whether models can learn to\nsolve a distribution of games. Note that each game has its\nown counting memory, i.e., the states visited in one game\ndo not affect the counters for other games. Here, we ﬁx the\ngame difﬁculty level to 10, and randomly generate training\nsets that contain {2, 5, 10, 30, 50, 100} games in each mode.\nAs shown in Figure 2 (bottom row), when the game mode\nbecomes harder, the episodic bonus has an advantage over\nthe cumulative bonus, and recurrence becomes more crucial\nfor memorizing the game distribution. It is also clear that\nthe episodic bonus and recurrence help signiﬁcantly when\nmore training games are provided.\n5.3. Zero-shot Evaluation\nFinally, we want to see if a pre-trained model can general-\nize to unseen games. The generated training set contains\n{1, 2, 5, 10, 30, 50, 100, 500} L10 games for each mode.\nThen, for each corresponding mode the test set contains\n10 unseen {L5, L10, L15, L20, L30} games. There is no\n1Since all models use the LSTM representation generator, we\nomit “LSTM” for abbreviation.\n2We use Lk to indicate level k game.\nCounting to Explore and Generalize in Text-based Games\nFigure 2. Model performance on single games (top row) and multiple games (bottom row).\nFigure 3. Zero-shot evaluation: Average rewards of DQN++ (left) and DRQN++ (right) as a function of the number of games in the\ntraining set.\nFigure 4. Average rewards and steps used corresponding to best\nvalidation performance in hard games.\noverlap between training and test games in either text de-\nscriptions or optimal trajectories. At test time, the counting\nmodules are disabled, the agent is not updated, and its gen-\nerates verb and noun actions based on the argmax of their\nQ-values.\nAs shown in Figure 3, when the game mode is easy, both\nmodels with and without recurrence can generalize well\non unseen games by training on a large training set. It is\nworth noting that by training on 500 L10 easy games, both\nmodels can almost perfectly solve level 30 unseen easy\ngames. We also observe that models with recurrence are\nable to generalize better when trained on fewer games.\nWhen testing on hard mode games, we observe that both\nmodels suffer from overﬁtting (after a certain number of\nepisodes, average test reward starts to decrease while train-\ning reward increases). Therefore, we further generated a\nvalidation set that contains 10 L10 hard games, and report\ntest results corresponding to best validation performance. In\naddition, we investigated what happens when concatenating\nthe previous 4 steps’ history observation into the input. In\nFigure 4, we add H to model names to indicate this variant.\nAs shown in Figure 4, all models can memorize the 500\ntraining games, while DQN++ and DRQN++H are able\nto generalize better on unseen games. In particular, the\nformer performs near perfectly on test games. To inves-\ntigate this, we looked into all the bi-grams of generated\ncommands (i.e., two commands from adjacent game steps)\nfrom DQN++ model. Surprisingly, except for moving back\nfrom dead end rooms, the agent always explores exits in\nanti-clockwise order. This means the agent has learned a\ngeneral strategy that does not require history information\nbeyond the previous command. This strategy generalizes\nperfectly to all possible hard games because there are no\ncycles in the maps.\n6. Final Remarks\nWe propose an RL model with a recurrent component, to-\ngether with an episodic count-based exploration scheme that\npromotes the agent’s discovery of the game environment.\nWe show promising results on a set of generated text-based\ngames of varying difﬁculty. In contrast to baselines, our\napproach learns policies that generalize to unseen games of\ngreater difﬁculty.\nIn future work, we plan to experiment on games with\nmore complex topology, such as cycles (where the “wall-\nfollowing” strategy will not work). We would like to explore\ngames that require multi-word commands (e.g., unlock red\ndoor with red key), necessitating a model that generates\nsequences of words. Other interesting directions include\nagents that learn to map or to deal with stochastic transitions\nin text-based games.\nCounting to Explore and Generalize in Text-based Games","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Counting to Explore and Generalize in Text-based Games.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nCounting to Explore and Generalize in Text-based Games\n```\n#### 2. 论文摘要\n```\nWe propose a recurrent RL agent with an episodic exploration mechanism that\nhelps discovering good policies in text-based game environments. We show\npromising results on a set of generated text-based games of varying difficulty\nwhere the goal is to collect a coin located at the end of a chain of rooms. In\ncontrast to previous text-based RL approaches, we observe that our agent learns\npolicies that generalize to unseen games of greater difficulty.\n```\n\n#### 3. 论文全文\n```\nCounting to Explore and Generalize in Text-based Games\nXingdi Yuan * 1 Marc-Alexandre Cˆot´e * 1 Alessandro Sordoni 1 Romain Laroche 1 Remi Tachet des Combes 1\nMatthew Hausknecht 1 Adam Trischler 1\nAbstract\nWe propose a recurrent RL agent with an episodic\nexploration mechanism that helps discovering\ngood policies in text-based game environments.\nWe show promising results on a set of generated\ntext-based games of varying difﬁculty where the\ngoal is to collect a coin located at the end of a\nchain of rooms. In contrast to previous text-based\nRL approaches, we observe that our agent learns\npolicies that generalize to unseen games of greater\ndifﬁculty.\n1. Introduction\nText-based games like Zork (Infocom, 1980) are complex,\ninteractive simulations. They use natural language to de-\nscribe the state of the world, to accept actions from the\nplayer, and to report subsequent changes in the environment.\nThe player works toward goals which are seldom speci-\nﬁed explicitly and must be discovered through exploration.\nThe observation and action spaces in text games are both\ncombinatorial and compositional, and players must contend\nwith partial observability, since descriptive text does not\ncommunicate complete, unambiguous information about\nthe underlying game state.\nIn this paper, we study several methods of exploration in\ntext-based games. Our basic task is a deterministic text-\nbased version of the chain experiment (Osband et al., 2016;\nPlappert et al., 2017) with distractor nodes that are off-chain:\nthe agent must navigate a path composed of discrete loca-\ntions (rooms) to the goal, ideally without revisiting dead\nends. We propose a DQN-based recurrent model for solving\ntext-based games, where the recurrence gives the model\nthe capacity to condition its policy on historical state infor-\nmation. To encourage exploration, we extend count-based\nexploration approaches (Ostrovski et al., 2017; Tang et al.,\n*Equal contribution 1Microsoft Research. Correspondence to:\nEric Yuan <eric.yuan@microsoft.com>, Marc-Alexandre Cˆot´e\n<macote@microsoft.com>.\nPublished at the Exploration in Reinforcement Learning Work-\nshop at the 35 th International Conference on Machine Learning,\nStockholm, Sweden. Copyright 2018 by the author(s).\n2017), which assign an intrinsic reward derived from the\ncount of state visitations during learning, across episodes.\nSpeciﬁcally, we propose an episodic count-based explo-\nration scheme, where state counts are reset at the beginning\nof each episode. This reward plays the role of an episodic\nmemory (Gershman & Daw, 2017) that pushes the agent to\nvisit states not previously encountered within an episode.\nAlthough the recurrent policy architecture has the capacity\nto solve the task by remembering and avoiding previously\nvisited locations, we hypothesize that exploration rewards\nwill help the agent learn to utilize its memory.\nWe generate a set of games of varying difﬁculty (measured\nwith respect to the path length and the number of off-chain\nrooms) with a text-based game generator (Cˆot´e et al., 2018).\nWe observe that, in contrast to a baseline model and stan-\ndard count-based exploration methods, the recurrent model\nwith episodic bonus learns policies that not only complete\nmultiple training games at same time successfully but also\ngeneralize to unseen games of greater difﬁculty.\n2. Text-based Games as POMDPs\nText-based games are sequential decision-making prob-\nlems that can be described naturally by the Reinforce-\nment Learning (RL) setting. Fundamentally, text-based\ngames are partially observable Markov decision processes\n(POMDP) (Kaelbling et al., 1998) where the environment\nstate is never observed directly.\nTo act optimally, an\nagent must keep track of all observations. Formally, a\ntext-based game is a discrete-time POMDP deﬁned by\n(S, T, A, Ω, O, R, γ), where γ ∈[0, 1] is the discount factor.\nEnvironment States (S): The environment state at turn t\nin the game is st ∈S. It contains the complete internal\ninformation of the game, much of which is hidden from the\nagent. When an agent issues a command ct (deﬁned next),\nthe environment transitions to state st+1 with probability\nT(st+1|st, ct).\nActions (A): At each turn t, the agent issues a text com-\nmand ct. The interpreter can accept any sequence of char-\nacters but will only recognize a tiny subset thereof. Further-\nmore, only a fraction of recognized commands will actually\nchange the state of the world. The resulting action space\narXiv:1806.11525v2  [cs.CL]  7 Mar 2019\nCounting to Explore and Generalize in Text-based Games\nis enormous and intractable for existing RL algorithms. In\nthis work, we make the following two simplifying assump-\ntions. (1) Word-level Each command is a two-word se-\nquence where the words are taken from a ﬁxed vocabulary V .\n(2) Command syntax Each command is a (verb, object)\npair (direction words are considered objects).\nObservations (Ω): The text information perceived by the\nagent at a given turn t in the game is the agent’s observation,\not ∈Ω, which depends on the environment state and the\nprevious command with probability O(ot|st, ct−1). Thus,\nthe function O selects from the environment state what\ninformation to show to the agent given the last command.\nReward Function (R): Based on its actions, the agent\nreceives reward signals rt = R(st, at). The goal is to maxi-\nmize the expected discounted sum of rewards E [P\nt γtrt].\n3. Method\n3.1. Model Architecture\nIn this work, we adopt the LSTM-DQN (Narasimhan et al.,\n2015) model as baseline. It has two modules: a representa-\ntion generator ΦR, and an action scorer ΦA. ΦR takes ob-\nservation strings o as input, after a stacked embedding layer\nand LSTM (Hochreiter & Schmidhuber, 1997) encoder, a\nmean-pooling layer produces a vector representation of the\nobservation. This feeds into ΦA, in which two MLPs, shar-\ning a lower layer, predict the Q-values over all verbs wv\nand object words wo independently. The average of the two\nresulting scores gives the Q-values for the composed actions.\nThe LSTM-DQN does not condition on previous actions\nor observations, so it cannot deal with partial observability.\nWe concatenate the previous command ct−1 to the current\nobservation ot to lessen this limitation.\nTo further enhance the agent’s capacity to remember previ-\nous states, we replace the shared MLP in ΦA by an LSTM\ncell. This model is inspired by (Hausknecht & Stone, 2015;\nLample & Chaplot, 2016) and we call it LSTM-DRQN. The\nLSTM cell in ΦA takes the representation generated by ΦR\ntogether with history information ht−1 from the previous\ngame step as input. It generates the state information at the\ncurrent game step, which is then fed into the two MLPs as\nwell as passed forward to next game step. Figure 1 shows\nthe LSTM-DRQN architecture.\n3.2. Discovery Bonus\nTo promote exploration we use an intrinsic reward by count-\ning state visits (Kolter & Ng, 2009; Tang et al., 2017; Martin\net al., 2017; Ostrovski et al., 2017). We investigate two ap-\nproaches to counting rewards. The ﬁrst is inspired by (Kolter\n& Ng, 2009), where we deﬁne the cumulative counting\nbonus as r+(ot) = β · n(ot)−1\/3, where n(ot) is the num-\nFigure 1. LSTM-DRQN processes textual observations word-by-\nword to generate a ﬁxed-length vector representation. This repre-\nsentation is used by the recurrent policy to estimate Q-values for\nall verbs Q(s, v) and objects Q(s, o).\nber of times the agent has observed ot since the beginning\nof training (across episodes), and β is the bonus coefﬁcient.\nDuring training, as the agent observes new states more and\nmore, the cumulative counting bonus gradually converges\nto 0.\nThe second approach is the episodic discovery bonus,\nwhich encourages the agent to discover unseen states by\nassigning a positive reward whenever it sees a new state.\nIt is deﬁned as: r++(ot) =\n\u001a\nβ\nif n(ot) = 1\n0.0\notherwise\n, where n(·)\nis reset to zero at the beginning of each episode. Taking\ninspiration from (Gershman & Daw, 2017), we hope this\nbehavior pushes the agent to visit states not previously en-\ncountered in the current episode and teaches the agent how\nto use its memory for this purpose so it may generalize to\nunseen environments.\n4. Related Work\nRL Applied to Text-based Games: Narasimhan et al.\n(2015) test their LSTM-DQN in two text-based environ-\nments: Home World and Fantasy World. They report the\nquest completion ratio over multiple runs but not how many\nsteps it takes to complete them. He et al. (2015) intro-\nduce the Deep Reinforcement Relevance Network (DRRN)\nfor tackling choice-based (as opposed to parser-based) text\ngames, evaluating the DRRN on one deterministic game and\none larger-scale stochastic game. The DRRN model con-\nverges on both games; however, this model must know in ad-\nvance the valid commands at each state. Fulda et al. (2017)\npropose a method to reduce the action space for parser-\nbased games by training word embeddings to be aware of\nverb-noun affordances. One drawback of this approach is it\nrequires pre-trained embeddings.\nCount-based Exploration:\nThe Model Based Inter-\nval Estimation-Exploration Bonus (MBIE-EB) (Strehl &\nLittman, 2008) derives an intrinsic reward by counting state-\naction pairs with a table n(s, a). Their exploration bonus has\nthe form β\/\np\nn(s, a) to encourage exploring less-visited\npairs. In this work, we use n(s) rather than n(s, a), since\nthe majority of actions leave the agent in the same state\nCounting to Explore and Generalize in Text-based Games\n(i.e., unrecognized commands). Using the latter would re-\nward the agent for trying invalid commands, which is not\nsensible in our setting.\nTang et al. (2017) propose a hashing function for count-\nbased exploration in order to discretize high-dimensional,\ncontinuous state spaces. Their exploration bonus r+ =\nβ\/\np\nn(φ(s)), where φ(·) is a hashing function that can\neither be static or learned. This is similar to the cumulative\ncounting bonus deﬁned above.\nDeep Recurrent Q-Learning: Hausknecht & Stone (2015)\npropose the Deep Recurrent Q-Networks (DRQN), adding\na recurrent neural network (such as an LSTM (Hochreiter\n& Schmidhuber, 1997)) on top of the standard DQN model.\nDRQN estimates Q(ot, ht−1, at) instead of Q(ot, at), so it\nhas the capacity to memorize the state history. Lample &\nChaplot (2016) use a model built on the DRQN architecture\nto learn to play FPS games.\nA major difference between the work presented in this paper\nand the related work is that we test on unseen games and\ntrain on a set of similar (but not identical) games rather than\ntraining and testing on the same game.\n5. Experiments\n5.1. Coin Collector Game Setup\nTo evaluate the two models described above and the pro-\nposed discovery bonus, we designed a set of simple text-\nbased games inspired by the chain experiment (Osband et al.,\n2016; Plappert et al., 2017). Each game contains a given\nnumber of rooms that are randomly connected to each other\nto form a chain (see ﬁgures in Appendix C). The goal is to\nﬁnd and collect a “coin” placed in one of the rooms. The\nplayer’s initial position is at one end of the chain and the\ncoin is at the other. These games have deterministic state\ntransitions.\nGames stop after a set number of steps or after the player\nhas collected the coin. The game interpreter understands\nonly ﬁve commands (go north, go east, go south, go\nwest and take coin), while the action space is twice as\nlarge: {go, take} × {north, south, east, west, coin}.\nSee Figure 12, Appendix C for an example of what the agent\nobserves in-game.\nOur games have 3 modes: easy (mode 0), there are no dis-\ntractor rooms (dead ends) along the path; medium (mode\n1), each room along the optimal trajectory has one distrac-\ntor room randomly connected to it; hard (mode 2), each\nroom on the path has two distractor rooms, i.e., within a\nroom on the optimal trajectory, all 4 directions lead to a\nconnected room. We use difﬁculty levels to indicate the\noptimal trajectory’s length of a game.\nTo solve easy games, the agent must learn to recall its previ-\nous directional action and to issue the command that does\nnot reverse it (e.g., if the agent entered the current room by\ngoing east, do not now go west). Conversely, to solve\nmedium and hard games, the agent must reverse its previous\naction when it enters distractor rooms to return to the chain,\nand also recall farther into the past to track which exits it\nhas already passed through. Alternatively, since there are no\ncycles, it can learn a less memory intensive “wall-following”\nstrategy by, e.g., taking exits in a clockwise order from\nwhere it enters a room.\nWe refer to models with the cumulative counting bonus\nas MODEL+, and models with episodic discovery bonus as\nMODEL++, where MODEL ∈{DQN, DRQN}1 (implemen-\ntation details in Appendix A). In this section we cover part\nof the experiment results, the full extent of our experiment\nresults are provided in Appendix B.\n5.2. Solving Training Games\nWe ﬁrst investigate whether the variant models can learn\nto solve single games with different difﬁculty modes (easy,\nmedium, hard) and levels {L5, L10, L15, L20, L25, L30}2.\nAs shown in Figure 2 (top row), when the games are sim-\nple, vanilla DQN and DRQN already fail to learn. Adding\nthe cumulative bonus helps somewhat and models perform\nsimilarly with and without recurrence. When the games be-\ncome harder, the cumulative bonus helps less, while episodic\nbonus remains very helpful and recurrence in the model be-\ncomes very helpful.\nNext, we are interested to see whether models can learn to\nsolve a distribution of games. Note that each game has its\nown counting memory, i.e., the states visited in one game\ndo not affect the counters for other games. Here, we ﬁx the\ngame difﬁculty level to 10, and randomly generate training\nsets that contain {2, 5, 10, 30, 50, 100} games in each mode.\nAs shown in Figure 2 (bottom row), when the game mode\nbecomes harder, the episodic bonus has an advantage over\nthe cumulative bonus, and recurrence becomes more crucial\nfor memorizing the game distribution. It is also clear that\nthe episodic bonus and recurrence help signiﬁcantly when\nmore training games are provided.\n5.3. Zero-shot Evaluation\nFinally, we want to see if a pre-trained model can general-\nize to unseen games. The generated training set contains\n{1, 2, 5, 10, 30, 50, 100, 500} L10 games for each mode.\nThen, for each corresponding mode the test set contains\n10 unseen {L5, L10, L15, L20, L30} games. There is no\n1Since all models use the LSTM representation generator, we\nomit “LSTM” for abbreviation.\n2We use Lk to indicate level k game.\nCounting to Explore and Generalize in Text-based Games\nFigure 2. Model performance on single games (top row) and multiple games (bottom row).\nFigure 3. Zero-shot evaluation: Average rewards of DQN++ (left) and DRQN++ (right) as a function of the number of games in the\ntraining set.\nFigure 4. Average rewards and steps used corresponding to best\nvalidation performance in hard games.\noverlap between training and test games in either text de-\nscriptions or optimal trajectories. At test time, the counting\nmodules are disabled, the agent is not updated, and its gen-\nerates verb and noun actions based on the argmax of their\nQ-values.\nAs shown in Figure 3, when the game mode is easy, both\nmodels with and without recurrence can generalize well\non unseen games by training on a large training set. It is\nworth noting that by training on 500 L10 easy games, both\nmodels can almost perfectly solve level 30 unseen easy\ngames. We also observe that models with recurrence are\nable to generalize better when trained on fewer games.\nWhen testing on hard mode games, we observe that both\nmodels suffer from overﬁtting (after a certain number of\nepisodes, average test reward starts to decrease while train-\ning reward increases). Therefore, we further generated a\nvalidation set that contains 10 L10 hard games, and report\ntest results corresponding to best validation performance. In\naddition, we investigated what happens when concatenating\nthe previous 4 steps’ history observation into the input. In\nFigure 4, we add H to model names to indicate this variant.\nAs shown in Figure 4, all models can memorize the 500\ntraining games, while DQN++ and DRQN++H are able\nto generalize better on unseen games. In particular, the\nformer performs near perfectly on test games. To inves-\ntigate this, we looked into all the bi-grams of generated\ncommands (i.e., two commands from adjacent game steps)\nfrom DQN++ model. Surprisingly, except for moving back\nfrom dead end rooms, the agent always explores exits in\nanti-clockwise order. This means the agent has learned a\ngeneral strategy that does not require history information\nbeyond the previous command. This strategy generalizes\nperfectly to all possible hard games because there are no\ncycles in the maps.\n6. Final Remarks\nWe propose an RL model with a recurrent component, to-\ngether with an episodic count-based exploration scheme that\npromotes the agent’s discovery of the game environment.\nWe show promising results on a set of generated text-based\ngames of varying difﬁculty. In contrast to baselines, our\napproach learns policies that generalize to unseen games of\ngreater difﬁculty.\nIn future work, we plan to experiment on games with\nmore complex topology, such as cycles (where the “wall-\nfollowing” strategy will not work). We would like to explore\ngames that require multi-word commands (e.g., unlock red\ndoor with red key), necessitating a model that generates\nsequences of words. Other interesting directions include\nagents that learn to map or to deal with stochastic transitions\nin text-based games.\nCounting to Explore and Generalize in Text-based Games\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 基于计数探索和泛化的文本游戏强化学习\n\n## 📌 背景痛点\/本文动机\n文本游戏是一种复杂的交互式模拟，使用自然语言描述游戏状态、接受玩家动作并报告环境变化。玩家必须通过探索来发现游戏目标，而游戏中的观察和动作空间都是组合和复合的，玩家必须应对部分可观察性，因为描述性文本并不提供关于底层游戏状态的完整、明确的信息。\n\n## 🚀 核心方法\n💡 创新点1：提出了一种基于循环的强化学习代理，具有情节探索机制，有助于在基于文本的游戏环境中发现良好的策略。\n💡 创新点2：提出了一种情节计数探索方案，其中状态计数在每个情节开始时重置。这种奖励充当情节记忆，推动代理访问当前情节中尚未遇到的状态。\n\n## 📈 实验结果\n在一系列生成的基于文本的游戏中，该代理在收集位于一系列房间末尾的硬币的目标上取得了有希望的结果。与之前的基于文本的强化学习方法相比，观察到该代理学习的策略可以泛化到未见的更具挑战性的游戏。\n\n## 💬 可借鉴之处\n该论文提出的基于计数探索和泛化的文本游戏强化学习方法，为解决文本游戏中的探索和泛化问题提供了一种新的思路。该方法可以应用于其他需要探索和记忆能力的强化学习任务，例如迷宫探索、路径规划等。","llm_summary_res_status":200}
{"title":"Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4","authors":"Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo","summary":"Unlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities of\ndecision-making under uncertain or incomplete information. GPT-4, the recent\nbreakthrough in large language models (LLMs) trained on massive passive data,\nis notable for its knowledge retrieval and reasoning abilities. This paper\ndelves into the applicability of GPT-4's learned knowledge for imperfect\ninformation games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an\ninnovative agent that leverages GPT-4's capabilities for performing in\nimperfect information games. With proper prompt engineering to achieve\ndifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Importantly,\nGPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it\ncan understand others and intentionally impact others' behavior. Leveraging\nthis, we design a planning strategy that enables GPT-4 to competently play\nagainst different opponents, adapting its gameplay style as needed, while\nrequiring only the game rules and descriptions of observations as input. In the\nexperiments, we qualitatively showcase the capabilities of Suspicion-Agent\nacross three different imperfect information games and then quantitatively\nevaluate it in Leduc Hold'em. The results show that Suspicion-Agent can\npotentially outperform traditional algorithms designed for imperfect\ninformation games, without any specialized training or examples. In order to\nencourage and foster deeper insights within the community, we make our\ngame-related data publicly available.","url":"http:\/\/arxiv.org\/abs\/2309.17277v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2309.17277v3","published":1695997803000,"comment":null,"pdf_text":"Published as a conference paper at COLM 2024\nSuspicion-Agent: Playing Imperfect Information Games with\nTheory of Mind Aware GPT-4\nJiaxian Guo ∗†1\nBo Yang * 1\nPaul Yoo 1\nBill Yuchen Lin 2\nYusuke Iwasawa 1\nYutaka Matsuo 1\nThe University of Tokyo 1\nAllen Institute for AI 2\nDemo: https:\/\/shorturl.at\/dmxzD\nGame Data Replay: https:\/\/shorturl.at\/fhuCI\nCodes: https:\/\/github.com\/CR-Gjx\/Suspicion-Agent\nAbstract\nUnlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities\nof decision-making under uncertain or incomplete information. GPT-4, the\nrecent breakthrough in large language models (LLMs) trained on massive\npassive data, is notable for its knowledge retrieval and reasoning abilities.\nThis paper delves into the applicability of GPT-4’s learned knowledge for\nimperfect information games. To achieve this, we introduce Suspicion-\nAgent, an innovative agent that leverages GPT-4’s capabilities for imperfect\ninformation games. With proper prompt engineering to achieve differ-\nent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Impor-\ntantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity,\nmeaning it can understand others and intentionally impact others’ behav-\nior. Leveraging this, we design a planning strategy that enables GPT-4\nto competently play against different opponents, adapting its gameplay\nstyle as needed, while requiring only the game rules and descriptions of\nobservations as input. In the experiments, we qualitatively showcase the\ncapabilities of Suspicion-Agent across three different imperfect information\ngames and then quantitatively evaluate it in Leduc Hold’em. As an explo-\nration study, we show that Suspicion-Agent can potentially outperform\ntraditional algorithms without any specialized training or examples, but\nstill cannot beat Nash-Equilibrium algorithms. In order to encourage and\nfoster deeper insights within the community, we make our game-related\ndata publicly available.\n1\nIntroduction\nRecently, large language models (LLMs) (Brown et al., 2020b; Chowdhery et al., 2022;\nTouvron et al., 2023), which are trained extensively on text corpora and code datasets and\naligned with instructions (Ouyang et al., 2022; Wei et al., 2021; Longpre et al., 2023), have\ndemonstrated remarkable knowledge retrieval and reasoning capabilities (Kojima et al.,\n2022; Wei et al., 2022b;a) on natural language benchmarks and exams (Hendrycks et al.,\n2020; Cobbe et al., 2021). Given few-shot examples or specific instructions as prompts,\nthese models, especially GPT-4 (OpenAI, 2023), can understand human intentions and\nmake informed decisions in open-ended scenarios, and tackle intricate tasks by gathering\nobservations and utilizing the learned prior knowledge, such as Voyager (Wang et al., 2023a),\nReAct (Yao et al., 2022) and SwiftSage (Lin et al., 2023).\nHowever, most of these methods typically assume that the agent has access to all relevant\ninformation, an assumption that is often unrealistic in real-world settings. Take diplomacy\n(Team et al., 2022; Gray et al., 2020) as an example: representatives must discern the veiled\n∗Equal Contribution\n†Corresponding to jiaxian.guo@weblab.t.u-tokyo.ac.jp. (Guo is now working at Google)\n1\narXiv:2309.17277v3  [cs.AI]  31 Aug 2024\nPublished as a conference paper at COLM 2024\nintentions of other countries based on incomplete information and decide accordingly\nto maximize benefits for their own nation. This challenge is not unique to diplomacy\nbut extends to other domains as well, such as poker (Moravˇc´ık et al., 2017; Brown &\nSandholm, 2018) and economic simulations (Holmstr¨om & Myerson, 1983; Harsanyi, 1968).\nThe inherent unpredictability in these games makes it impractical for a learned agent to\nadopt a single, optimal strategy for every scenario (Brown et al., 2019). This necessitates\npredictive capabilities for handling incomplete information, along with a theory of mind\n(ToM) ability (Frith & Frith, 2005) to comprehend decisions from others’ perspectives. Such\ncomplexities, both strategic and cognitive, represent challenges in the field of AI research.\nFurthermore, recent advancements in imperfect information games, such as ReBel (Brown\net al., 2020a), DeepStack (Moravˇc´ık et al., 2017) and Libratus (Brown & Sandholm, 2018),\ntypically start training from scratch, and thus they normally need millions of data to\nunderstand the game rules and learn the adequate strategies for each new game. Such\na high sample complexity hampers their ability to generalize across different games and\nposes challenges when applying them into complex and open-ended imperfect information\nscenarios. By contrast, as alluded to previously, LLMs have undergone training on massive\npassive datasets. This leads to an intriguing proposition: Can we harness pre-trained LLMs’\nknowledge and reasoning capabilities to navigate imperfect information games without\nadditional training or data?\nTo achieve this, we propose Suspicion-Agent, an innovative autonomous agent based on\nGPT-4. This agent harnesses its extensive prior knowledge and cognitive adaptability to\neffective strategies against a range of adversaries without any specialized training. Con-\ncretely, we first decompose the process of solving such games into multiple sub-modules\nlike observation interpreter and planning module to understand the game rules and game\nstates (as Figure 1 shows) so that GPT-4 can make decisions accordingly. Each module\nemploys different prompts to enable GPT-4 to fulfill specific functions. However, unlike\nperfect information games, planning strategies in imperfect information games can have\nvarying effectiveness depending on the opponent’s behavior (Brown et al., 2020a; Moravˇc´ık\net al., 2017; Brown & Sandholm, 2018). To tackle these challenges, we introduce a theory\nof mind (ToM) aware planning approach that leverages the higher-order ToM capability\n(Frith & Frith, 2005) present in LLMs. Specifically, the model utilizes its understanding of\nhuman cognition to predict opponents’ thought processes, susceptibilities, and actions. This\naligns with the idea that individuals use their own minds as models to understand and\naffect others (Montes et al., 2022). For instance, the model might consider, ”If I execute Plan\n1, how would this influence my opponent’s beliefs about my cards, and what actions might\nthey take based on their behavioral patterns?”\nConcretely, given the gameplay history as the input, we find that GPT-4 can identify an\nopponent’s strategic tendencies and analyze how our actions influence the opponent’s\nbehavior, e.g. if Suspicion-Agent identifies a weak hand held by the opponent, coupled\nwith the cautious strategy, it might strategically raise the bet to encourage the opponent\nto fold, even when Suspicion-Agent itself holds a similarly weak hand (as illustrated in\nFigure 9 and A.11). Remarkably, by using some simple prompts, e.g., GPT-4 can even self-\nexamine its behavior through the lens of the opponent (Refer to A.7). Leveraging its ToM\ncapabilities, GPT-4 can predict and even influence an opponent’s actions effectively (R¨oska-\nHardy, 2008). Integrating these simulated actions into our planning module can mitigate\nthe information asymmetry inherent in imperfect information games and more accurately\nassess the effectiveness of various strategies. As a result, our Suspicion-Agent can adjust\nits strategy to play effectively against a range of opponents, as shown in Section 4.1. In the\nexperiments, we first conduct a qualitative assessment of Suspicion-Agent’s in 3 two-player\nimperfect information games, aiming to showcase the generalization capabilities of our\nmethod. Subsequently, we perform a quantitative analysis in Leduc Hold’em (Southey et al.,\n2012). The results reveal that Suspicion-Agent exhibits varying behaviors when interacting\nwith previous works such as CFR+ (Tammelin, 2014) and NFSP (Heinrich & Silver, 2016)\nwhile outperforming some of them in terms of overall performance. In summary, our\ncontributions are as follows:\n1. We introduce Suspicion-Agent, the first agent framework designed to empower GPT-4\nwith theory of mind (ToM) ability to compete in various imperfect information games\n2\nPublished as a conference paper at COLM 2024\nSuspicion-Agent\nObservation \nInterpreter\n(LLM)\nPlanning\n(LLM)\nUnknown\nOpponent\n'hand': 'SJ',\n'public_card': 'SK',\n'all_chips’: [6, 6],\nGame State\nGame Rule\nObservation \nRule\nIn this round , you \nare holding a Jack \nof Spades and the \npublic card is a \nKing of Hearts. So \nfar, you have put 6 \nchips into the pot \nand your \nopponent has put \nin 6 chips. \nAction\nAction\nPlanning with First-Order Theory of Mind\nPlan \nProposals\n(LLM)\nObservation\nInterpretation\nAll Game \nPlay History\nPattern Analysis \n& Reflexion\n(LLM)\nSelf-Evaluate\n(LLM)\nThe opponent tends \nto raise (80%) when \nhe has King …\nOpponent \nCurrent \nInformation \nPrediction (LLM)\nPlan1: Raise\nPlan2: Check\n….\nThe opponent has \nKing (80%), Jack \n(10%), Queen (10%)\nReadable \nCurrent \nGame \nHistory\nPlan1: Based on the \nestimation of the \nopponent’s and my \ncard,\nPlan 1’s winning rate \nis 80%.  \nPlan2: \n…\nBased on the winning \nrate comparison, I will \nselect {Plan 1}\nAction\nFigure 1: Left Figure. The illustration of the Suspicion-Agent when playing against an\nunknown opponent. Right Figure. The illustration about the first-order ToM planning\nmethod, where the texts in yellow blocks are outputs, and green blocks are inputs.\nby understanding game rules and observational data without requiring any specialized\ntraining or examples. By incorporating the ToM capability into the planning process,\nSuspicion-Agent captures the inherent uncertainty of opponent behavior in our strategic\ndeliberations. This enables Suspicion-Agent to adapt its tactics dynamically when facing\nopponents with differing behavioral patterns.\n2. We are the first to demonstrate that an agent based on GPT-4 can potentially outperform\ntraditional algorithms in imperfect-information games, such as Leduc Hold’em (Southey\net al., 2012), when compared to established learning-based methods like NFSP (Heinrich &\nSilver, 2016) and DMC (Zha et al., 2021), but still unperformed Nash-Equilibrium algorithms\nlike CFR+ (Tammelin, 2014) which may inspire more subsequent use of LLMs in imperfect-\ninformation games.\n3. We make all interaction data between Suspicion-Agent and traditional algorithms for\nimperfect-information games in Leduc Hold’em publicly available. This will enable the\nresearch community to scrutinize the capabilities of GPT-4-based agents and inspire further\nwork, particularly in fine-tuning smaller language models.\n2\nProblem Definition\nTwo-Player Imperfect Information Game\nIn this paper, we propose to employ LLMs to\nplay imperfect information games. As a preliminary exploration, we concentrate primarily\non two-player imperfect information games, such as Leduc Hold’em (Southey et al., 2012),\nwhich involves two players, denoted by N = {1, 2}, who share the same action space, A.\nLet a1 ∈A and a2 ∈A represent the actions chosen by player 1 and player 2, respectively.\nEach player has access to two types of observations: a private observation, denoted as\nSpri(i) where i ∈N is the player index, and a public observation, shared among both\nplayers, denoted as Spub.\nAs the game progresses in discrete timesteps indexed by j, each player i observes a\nhistory h of the game. This history comprises the series of public and private obser-\nvations and actions up to timestep j −1 and the result of game rj, formally given as\nh = (S0\npub, S0\npri(i), a0\ni , a0\n¬i, r0 . . . , Sj−1\npub, Sj−1\npri(i), aj−1\ni\n, aj−1\n¬i , rj−1). Simultaneously, players re-\nceive the current private and public observations, Sj\npri(i) and Sj\npub, and select the next action\naj\ni according to a policy πi. All game histories are constructed as a dataset D, denoted as\nD = (h1, h2, . . . , hM), where M indexes individual games. The goal of each player is to\nselect the next action aj\ni with the imperfect observation according to the game rules, aiming\nfor victory over many games. Specifically, the order of players is not fixed and depends on\nthe game rule for each game. For example, the role of the small blind rotates among players\nin Texas Holed’em, dictating the order of play.\n3\nMethod\nTo enable LLMs to play various imperfect information games without specialized training,\nwe break down the overall task into several modules shown in Figure 1, such as the observa-\n3\nPublished as a conference paper at COLM 2024\nVanilla Planning\nPlanning with First-Order \nTheory of Mind\nPlanning with Second-Order \nTheory of Mind\nBased on my card, \nI would like to \ndo {action1} to \nwin the game or \nminimize the \nloss\nI know his behavior \npattern, and I can \nestimate his card  \nand his action. Based \non my card and his \ncard, I would like to \ndo {action1} to win \nthe game\nI know his \nbehavior \npattern, and I \ncan estimate \nhis card and \naction\nI know his \nbehavior \npattern, and \nI can \nestimate his \ncard and \naction based \non his action\nI know he knows \nmy behavior \npattern, and I \ncan adjust my \naction to win him\nFigure 2: Left Figure. The decision-making of the vanilla planning of Suspicion-Agent.\nMiddle Figure. The decision-making of the planning with first-order ToM of Suspicion-\nAgent. Right Figure. The decision-making of the planning with second-order ToM of\nSuspicion-Agent.\ntion interpreter, game pattern analysis, and planning module. In the following sections, we\nwill demonstrate how we craft specific prompts to guide LLMs to use its prior knowledge,\nreasoning ability, and psychological ability in performing these modular functions and\nexplain how we combine these functions to equip the model with the capability to navigate\nthe intricacies of imperfect information games. All prompts and codes will be made public\non our codebase (Please refer to our supplementary material).\n3.1\nGame Rule & Observation Understanding\nWhile LLMs excel in processing text data, it can be misled in imperfect information games\nbecause they normally provide only brief, low-level descriptions. To mitigate this issue, we\ninitially develop structured prompts that assist LLMs in comprehending both the game’s\nrules and its current state. For each type of imperfect information game, one can write a\nstructured rule description as follows:\n• General Rules: A brief game introduction, the number of rounds, and betting rules;\n• Action Descriptions: {Description of Action 1}, {Description of Action 2}, ...;\n• Single Win\/Loss Rule: The conditions for winning, losing, or drawing in a single game;\n• Win\/Loss Payoff Rule: The rewards or penalties for winning or losing a single game;\n• Whole Win\/Loss Rule: The number of games and the overall win\/loss conditions.\nIn most imperfect information game environments (Zha et al., 2019), game states are often\nrepresented as low-level numerical values, such as one-hot vectors, to facilitate machine\nlearning. Leveraging LLMs, we can convert these low-level game states into natural lan-\nguage text (Wu et al., 2023; Wang et al., 2023a; Guo et al., 2022; Lin et al., 2023), thereby\naiding the model’s understanding. For each game, it is also essential to define an observation\nconversion rule. Similar to structuring game rules, we organize the observation conversion\nrule as follows:\n• Input Explanation: The type of inputs received, such as dictionaries, lists, or other\nformats, and describes the number of elements in the game state along with the name of\neach element;\n• Element Descriptions: {Description of Element 1}, {Description of Element 2}, ...;\n• Conversion Tips: More guidelines for transforming the low-level game states into text.\nBy leveraging both the game rule and the observation conversion rule, we can effi-\nciently transform low-level game states into readable text, denoted as Obsr. This read-\nable text serves as the input for LLMs. Using the prompts Promptobs, the conditional\ndistribution for each element Obsr[i] in the generated text can be modeled as: Obsr ∼\n∏M\ni=1 Fθ(Obsr[i]|Promptobs, Rule, Ruleobs, Obsr[1, . . . , i −1]).\nHere, Fθ represents the lan-\nguage model parameterized by θ; M is the length of the generated text Obsr. The concrete\ndefinition can be found in Appendix A.2. We name this module an Observation Interpreter.\nThis formulation allows for a more understandable interaction with the model in imperfect\ninformation games.\n3.2\nVanilla Planning Module and Reflexion\nAfter understanding the game rules and converting the game states into a readable format,\nwe can craft prompts to guide LLMs in formulating strategies. Inspired by advancements in\n4\nPublished as a conference paper at COLM 2024\nLLMs-agent and prompt engineering (Ganguli et al., 2023; Wang et al., 2023e; Liu et al., 2023b;\nShinn et al., 2023), we introduce a vanilla planning method which features a Reflexion\nmodule aimed at automatically scrutinizing game history to enable LLMs to learn and\nimprove planning from the experience of the history, as well as a separate planning module\ndedicated to making decisions accordingly.\nReflexion The Reflexion module takes as input the history of games played against\nthe current opponent and outputs a Reflexion.\nIn the j-th round of the i-th game,\nwe gather all prior game histories, denoted as Di = (h1, h2, . . . , hi−1), and prompt\nLLMs to carry out these analytical functions to get the Reflexion output Oi\nf\n∼\n∏M\ni=1 Fθ(Of [i]|PromptRe f lexion, Rule, Di, Of [1, . . . , i −1]), i.e., Oi\nf ∼FRe f lexion\nθ\n, which covers\nwhy we won or lost in specific previous games, and suggests how to improve strategies for\nfuture games. Importantly, the Reflexion module empowers Suspicion-Agent to enhance its\nstrategies during gameplay, even without previous examples.\nPlanning After obtaining the Reflexion Of , we proceed to use the game rules, the cur-\nrent game history hi, the current readable observation Obsr, and the set of valid ac-\ntions {a} in the current game as inputs.\nWe then prompt LLMs to formulate multi-\nple textual plans based on its understanding of the imperfect information, i.e., Oplan ∼\n∏M\ni=1 Fθ(Oplan[i]|Promptplan, Rule, Obsr, hj−1, Of , Oplan[1, . . . , i −1]), Oplan ∼Fplans\nθ\n. Specifi-\ncally, the vanilla planning method assumes the marginal distribution of the actions of the\nopponent is uniform, and thus it can be regarded as a special case of planning with the\nzero-order ToM. In this way, we can further denote Fplans\nθ\nas Fzero−plan\nθ\n.\nEvaluator To assess the likely success of each plan, we introduce an evaluation module. This\nmodule takes into account factors such as the game’s current state, i.e., readable observation\nObsr, the Reflexion ORe f lexion, the game rule Rule and estimated plans Oplan as the input,\nto estimate the win rates for each of the proposed plans and output the next action by\nprompting LLMs, i.e., the next action aj = Fzero−eval\nθ\n(Obsr, ORe f lexion, Rule, Oplan).\n3.3\nPlanning with Theory of Mind (ToM)\nHowever, the vanilla planning method often struggles against the inherent uncertainties\nthat typify imperfect information games, particularly when faced with opponents skilled at\nexploiting others’ strategies. Inspired by this adaptability, we seek to devise a new planning\nmethod that capitalizes on LLMs’ ToM capabilities (Frith & Frith, 2005; Kosinski, 2023) to\nunderstand the opponent’s behavior and thus can adjust the strategy accordingly. In the\nfollowing sections, we will detail how we employ LLMs to analyze the behavior patterns\nof other agents and predict their subsequent actions in response to various plans using\ndifferent orders of ToM (results are shown in Table 3), thereby facilitating more informed\ndecision-making. Note that all sample outputs are given in Section A.11 and A.7.\nPlanning with First-Order ToM Modelling: In the first-order ToM modeling approach (as\nFigure 2 shows), Suspicion-Agent goes a step further by inferring the probable hand of\nthe opponent based on their actions to that point, e.g., if the opponent raised, they likely\nhave a strong hand. Consequently, Suspicion-Agent can adapt their strategy to maximize\nwinnings when holding a strong hand and minimize losses with a weak hand. To forecast\nthe opponent’s actions, we first introduce a behavior pattern analysis process. In this process,\nwe feed the game history Di = (h1, h2, . . . , hi−1) and the game rules into LLMs, prompting\nit to analyze the opponent’s behavioral pattern. The formulation and the prompts can be\nexpressed as: Obp ∼∏M\ni=1 Fθ(Obp[i]|Promptpattern, Rule, Di, Obp[1, . . . , i −1]).\nSample Prompts for First-Order Behaviour Pattern Analysis (Incomplete) :\nFrom my\nperspective, please infer several beliefs about the opponent’s game pattern\/preference for each round\nwhen holding different cards and the public card (if have).\nThrough this approach, we can deduce the opponent’s behavior pattern. Notably, since\nthe input for behavior pattern analysis is the same as that for the Reflexion module, we\nhave integrated them into a single module to reduce inference time, as shown in Figure 1.\n5\nPublished as a conference paper at COLM 2024\nAfter identifying the opponent’s behavior pattern, LLMs can be prompted to predict the\nstrength of the opponent’s current hand or observations in the game. This is expressed as:\nOcard pred ∼∏M\ni=1 Fθ(Ocard pred[i]|Promptcard pred, Rule, hj−1, Obp, Obsj\nr, Ocard pred[1, . . . , i −\n1]).\nSample Prompts for First-Order Cards Prediction (Incomplete) :\nUnderstanding the game\nrule, your observation, progress summarization in the current game, the estimated behaviour pattern\nof the opponent, and your knowledge about the game, please infer the probabilities about the cards of\nthe opponent (number 100% in total) step by step.\nWith these predictions, we can further augment the previous Planning module and Evalu-\nator module with Ocard pred as the additional input, so that we can further propose better\nplans considering the opponent’s card and estimate the winning rate of each plan, so that\nwe can better make the decision. Because the input of Planning module and Evaluator\nmodule are highly overlapped and our budgets are limited, we combine these two modules\ntogether to save the costs:\nSample Prompts for Planning and Evaluator (Incomplete):\nMake Reasonable Plans: Please\nplan several strategies according to actions you can play now to win the whole game step by step.\nNote that you can say something or keep silent to confuse your opponent.\nPotential opponent’s actions and Estimate Winning\/Lose\/Draw Rate: From the perspective of the\nopponent, please infer what the action opponent with probability would do when the opponent holds\ndifferent cards based on his behaviour pattern, and then calculate the winning\/lose\/draw rates when\nopponent holds different cards step by step. Output in a tree structure:\nThe sample outputs are shown in Figure 8 and A.7.1.\nPlanning with Second-Order ToM Modelling: However, elite players in imperfect in-\nformation games like poker are also adept at dynamically adjusting their strategies, and\nthey may employ ”bluffing” as a tactic, feigning a strong hand when they actually hold a\nweak one to deceive their opponent. Relying solely on a first-order ToM in such situations\ncould lead to incorrect assumptions and potentially costly mistakes. Recognizing this, we\nintroduce a planning method that incorporates a second-order ToM. In this enhanced model,\nSuspicion-Agent engages in even more intricate reasoning, where Suspicion-Agent not only\nconsiders what the opponent might do (as in first-order ToM) but also what the opponent\nbelieves Suspicion-Agent will do as Figure 2 shows. This level of strategic thinking allows\nSuspicion-Agent to gain an advantage in situations involving tactics like bluffing.\nTo implement this, Suspicion-Agent needs to not only just consider the current state from\nits own perspective, but also be capable of role-switching to think his own observation from\nthe opponent’s viewpoint. In traditional methods (De Weerd et al., 2013; Tatarchenko et al.,\n2016), they need to iteratively call the first-order ToM function to estimate the action of the\nopponent. However, we surprisingly find that we can just add the prompts like below, and\nget the outputs in Sec A.7.2.\nSample Prompts for Second-Order Behaviour Pattern Analysis (Incomplete):\nFrom my\nperspective, please infer under what circumstances is the opponent likely to be influenced by my\nactions? Additionally, in what situations would the opponent make decisions based solely on their\nown hand?\nFrom the perspective of the opponent (he cannot observe my card but only action), please infer several\nbeliefs about my game pattern\/preference when holding different cards.\nWith this, LLMs are able to automatically generate insights into whether the opponent’s\nbehavior is likely to be reactive to Suspicion-Agent’s actions, or if they are likely to act\nindependently. Then, we can directly reuse the prompts of the first-order Tom to predict\nthe opponent’s cards based on the behavior pattern estimated from the second-order ToM,\nand we can get sample results in Figure 9 and Section A.7.2. In this way, we can utilize the\nPlanning with second-order ToM to make decisions and adapt the strategies accordingly.\nThe concrete algorithms are given in Section A.9. Without mentioning otherwise, we use\nthe second-order ToM and GPT-4-0613 by default.\n6\nPublished as a conference paper at COLM 2024\n4\nExperiments\nWe conduct experiments to answer the following questions:\n• Can Suspicion-Agent achieve comparable performance with traditional imperfect informa-\ntion algorithms without any specialized training? (Section 4.1)\n• Can Suspicion-Agent adapt its strategies when playing with different opponents? (Section\n4.1)\n• Can Suspicion-Agent play different imperfect information games without any specialized\ntraining? (Section A.11.1)\n• How different orders of ToM improve the performance of Suspicion-Agent? (Section 4.3\nand A.8)\n4.1\nQuantitative Evaluation\nEnvironments\nTo quantitatively assess the performance of LLMs in imperfect information\ngames, we chose the RLCard environment (Zha et al., 2019). Due to budget limits, our\nquantitative evaluation focuses on Leduc Hold’em, a simplified version of Limit Texas\nHold’em. The game rules of Leduc Hold’em can be found in Appendix A.3. Following\n(Southey et al., 2012), we also add the opponent’s observation into the single game history\nh after the end of each game, which also conforms with the real-world experience, but we\nalso perform the ablation study about it in Section 4.3 and A.10.\nCompeting Methods\nWe have selected a range of methods commonly used in decision-\nmaking, such as NFSP (Heinrich & Silver, 2016), DQN (Mnih et al., 2015), DMC (Deep Monte\nCarlo Search for imperfect information games) (Zha et al., 2021) and CFR+ (Zinkevich et al.,\n2007). Among these, NFSP and DMC are specifically designed for imperfect information\ngames and are based on self-play, while CFR+ is grounded in game theory. These algorithms\ntypically show different strategies in the imperfect information games, allowing us to\nevaluate the adaptability of each method. Note that, our Suspicion-Agent does not have\nany specialized training when compared with these methods.\nEvaluation Methods\nTo ensure the robustness of our evaluation metrics, we meticulously\ndesigned a dual-method evaluation framework aimed at mitigating the randomness intrinsic\nto imperfect information games. (1) Variable Random Seeds: Suspicion-Agent play against\ndifferent baselines for 100 games utilizing varying random seeds for each game. This tactic\nis intended to dampen the stochastic variability introduced by the random seed settings.\nThe results are shown in Table 1. (2) Same Cards with Exchange Position: We ran a series of\n50 games with a fixed random seed, thereby keeping the sequence of cards constant across\nthese games. Suspicion-Agent initially played at position 0 for the first 50 games, then we\nrerun the 50 games but switched the position of Suspicion-Agent and the baseline model.\nIn this way, Suspicion-Agent and the baseline should have the same card strength over\n100 games, and thus we can better evaluate the performance of each. The results of these\nexperiments are presented in Table 2.\nResults Analysis\n(1) Suspicion-Agent outperforms most baselines: As illustrated in\nTable 1, it is evident that our GPT-4-based Suspicion-Agent outperforms most other algo-\nrithms specifically trained on Leduc Hold’em environments except CFR+. Notably, it not\nonly defeats most of these methods but also secures the highest average chip count in the\ncomparisons. Our approach surpasses the second-best method by an impressive margin\nof approximately 25%. Even if the current design cannot achieve Nash-Equilibrium, these\nfindings compellingly showcase the potential of employing large language models in the\nrealm of imperfect information games, as well as affirm the effectiveness of our proposed\nframework. (2) The gap between GPT-3.5 and GPT-4 is large: While GPT-4 delivers perfor-\nmance that either matches or outperforms other baselines, agents using GPT-3.5 experience\na significant drop in performance. Specifically, the winning probability for agents built\non GPT-3.5 stands at just 50%, as opposed to 100% for GPT-4-based agents. Additionally,\nthe average chip payoff for GPT-3.5 agents is negative, underlining the stark performance\ndisparity between the two versions of the language model. The further reason analysis can\nbe found in Appendix A.4. (3) Suspicion-Agent outperforms baselines in both positions:\nUtilizing identical card sequences for both positions, Suspicion-Agent exhibits a consistent\nwinning pattern against various baselines, as evidenced in Table 2. This robust performance\n7\nPublished as a conference paper at COLM 2024\nserves as compelling evidence to substantiate the claim that Suspicion-Agent outperforms\nthe baseline models when card strength is held constant.\nTable 1: The comparison results of Suspicion-Agent when playing with different algorithms\ntrained on Leduc Hold’em environments. The results are the win\/lose chips after 100 games\nwith different seeds, and the number of win\/lose chips ranges from 1 to 14.\nOpponent Model\nNFSP\nDQN\nDMC\nCFR+\nOurs\n(GPT-3.5)\nOurs\n(GPT-4)\nAvg.\nNFSP\n-\n-33\n-22\n-21\n-3\n-142\n-61.25\nDQN\n+33\n-\n-55\n-8\n+200\n-44\n+22.8\nDMC\n+22\n+55\n-\n-15\n-49\n-24\n+4\nCFR+\n+21\n+8\n+15\n-\n+126\n+22\n+38.4\nOurs (GPT-3.5)\n+3\n-200\n+49\n-126\n-\n-\n-55\nOurs (GPT-4)\n+142\n+45\n+24\n-22\n-\n-\n+47.25\nTable 2: The comparison results of Suspicion-Agent when playing with CFR+ and DMC\ntrained in Leduc Hold’em environments. These results are quantified over 50 games, and\npos denote the position of the opponent model. For example, CFR+ (pos 0) denotes the\nopponent model is in the position 0, and the model is located in the position 1.\nOpponent Model\nCFR+\n(pos 0)\nCFR+\n(pos 1)\nDMC\n(pos 0)\nDMC\n(pos 1)\nOurs\n(pos 0)\nOurs\n(pos 1)\nAvg.\nDMC\n-21\n-6\n-10\n+10\n-36\n-4\n-11.17\nCFR+\n+35\n-35\n+6\n+21\n+16\n-1\n+7\nOurs\n+1\n-16\n+11\n+36\n-\n-\n+8\nBehaviour Pattern Analysis\nWe illustrate the action percentages of Suspicion-Agent and\nbaselines in Figure 3. We can observe that (1) Suspicion-Agent vs CFR+: CFR+ algorithm\n(Zinkevich et al., 2007) exhibits a mixed strategy, characterized by a conservative approach\nwhere it tends to fold in the face of a weak hand, especially when this hand does not align\nwell with the public cards. Our agent, recognizing this pattern, strategically opts to raise\nmore frequently. This tactic effectively applies pressure on the CFR+ algorithm, increasing\nthe likelihood of it folding under uncertain conditions. However, Over-reliance on bluffing\ncan lead to significant losses, especially when facing opponents capable of recognizing\nand exploiting this pattern. Additionally, it’s noteworthy that CFR+ itself is not averse\nto bluffing. Although it typically plays conservatively, CFR+ does occasionally employ\nbluffing tactics, which can result in losses for our agent. (2) Suspicion-Agent vs DMC: DMC\nalgorithm (Zha et al., 2021) based on the search algorithm DMC employs a more diversified\nstrategy that includes bluffing. It often raises both when it has the weakest and the strongest\nhands. In response, Suspicion-Agent adapts by raising less frequently and opting to call or\nfold more often based on its own hand and the observed behavior of DMC. (3) Suspicion-\nAgent vs DQN: DQN appears to have a more aggressive stance, almost always raising\nwith strong or mid-level hands and never folding. Suspicion-Agent identifies this and, in\nturn, minimizes its own raises (the lowest percentage among all matchups), opting more\noften to call or fold based on the public cards and DQN’s actions. (4) Suspicion-Agent\nvs NFSP: NFSP exhibits a follow-through strategy, opting to always call and never fold.\nSuspicion-Agent responds to this by raising less frequently (compared to matches against\nCFR+) and choosing to call more (compared to matches against CFR+) based on the public\ncard and NFSP’s observed actions. The analysis clearly shows that Suspicion-Agent is\nhighly adaptable and capable of exploiting the weaknesses in the strategies employed by\nvarious other algorithms. This speaks volumes about the large language model’s capability\nto reason and adapt in imperfect information games.\n4.2\nQualitative Evaluation\nIn the qualitative evaluation, we assess Suspicion-Agent on three imperfect information\ngames: Coup, Texas Hold’em Limit, and Leduc Hold’em (Southey et al., 2012). For each\ngame, we provide only the rules and observation rules as described in Section 3.1. Im-\nportantly, Suspicion-Agent is able to play these games without any additional training or\nsampling. Qualitative examples from these games are presented in the subsequent sections.\n8\nPublished as a conference paper at COLM 2024\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparison of Action Percentage when Playing Against Different Algorithms\nvs CFR\nvs DQN\nvs DMC\nvs NFSP\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparisons of Action Percentage of Different Algorithms\nCFR\nDQN\nDMC\nNFSP\nFigure 3: Left Figure. The illustration of the action percentage of Suspicion-Agent when\nplaying against different algorithms. Right Figure. The illustration of the action percentage\nof different algorithms when playing against Suspicion-Agent.\n.\nAction: Call (Holds Jack or Queen)\nObs:In this round, you are holding a Jack of Hearts. So far, you have put 2 chips into the pot, while your opponent \nhas put 2 chips.\nVanilla Planning\nBelief on GoodGuy's cards: GoodGuy chose to 'call' in the first round. According to his behavior pattern, when he \nholds a King, he would like to raise in the first round. Therefore, it is unlikely that he holds a King. When he \nholds a Queen or a Jack, he would like to call in the first round. Therefore, he is likely to hold a Queen or a \nJack. So, the probabilities are: King (0%), Queen (50%), Jack (50%).\nPotential GoodGuy's current believes about your cards:  I have not taken any action in the first round yet. \nAccording to GoodGuy's guess on my game pattern, he believes that I would like to raise when I hold a King, Queen, \nor Jack. Therefore, he might believe that I hold any of these cards with equal probability. So, the probabilities \nare: King (33.33%), Queen (33.33%), Jack (33.33%).\nMake Plans: Plan 1: Raise\n- If GoodGuy holds a Queen (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\n- If GoodGuy holds a Jack (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\nPlan 2: Fold\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). I will lose the pot, which \nwill be 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate \nis 0%.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). I will lose the pot, which will \nbe 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate is \n0%.\nPlan 3: Check\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\nEstimate Expected Chips Gain for Each Plan:\n- Plan 1: Expected Chips Gain = 50% * 3 chips - 0% * 0 chips = 1.5 chips\n- Plan 2: Expected Chips Gain = 0% * 0 chips - 50% * 2 chips = -1 chips\n- Plan 3: The Expected Chips Gain cannot be determined at this stage.\nPlan Selection: Plan 1: raise\nAction: Raise\nAction: Fold\nFigure 4:\nThe qualitative sample of planning with second-order ToM Suspicion-Agent\nabout Strategic Bluffing on Leduc Hold’em. More samples are given in Appendix A.11.\nLeduc Hold’em We present qualitative samples showcasing Suspicion-Agent’s behaviour\nunder different strategies: Vanilla Planning, Planning with First-Order ToM, and Planning\nwith Second-Order ToM in Leduc Hold’em. These samples can be viewed in Figure 6, 2, 8,\nand 9, respectively, and the concrete analysis is given in Appendix A.11.1.\nGame Coup and Texas Hold’em Limit As illustrated in Figure 12 and 11 in Appendix,\nwhen provided solely with the rules and observation guidelines of Texas Hold’em Limit and\nCoup, and keeping the agent prompts consistent, Suspicion-Agentis still adept at discerning\nthe opponent’s game patterns. It analyzes the strength of its hand and subsequently makes\ninformed decisions to accumulate chips. This is the strong evidence to demonstrate the\ngeneralization ability of Suspicion-Agent based on GPT-4 on different imperfect information\ngames, and thus outperforming the algorithms need to be re-trained for every new imperfect\ninformation game. Specifically, In the game of Coup, without any prior training, Suspicion-\nAgent rapidly discerns which character the opponent lacks. It then strategically bluffs\nas that character to block the opponent’s actions. This ability to bluff successfully gives\nSuspicion-Agent a consistent advantage throughout multiple rounds.\n4.3\nAblation Study and Component Analysis\nAblation Study on Orders of ToM\nIn order to explore how different orders of ToM-\naware planning methods affect the behavior of the large language model. We perform the\n9\nPublished as a conference paper at COLM 2024\nexperiments and comparisons on Leduc Hold’em and play against CFR+. We illustrate\nthe action percentages of Suspicion-Agent with planning with different levels of ToM in\nFigure 5 and the chips gain results in Table 3. We can observe that (1) Vanilla Planning:\nBased on Reflexion module, vanilla planning tends to call and check more (the highest\ncall and check percentage when playing against both CFR+ and DMC) during the game,\nwhich cannot push the pressure to make the opponent fold and result in many unnecessary\nlosses. In this way, vanilla planning has the lowest chip gain as Table 3 shows. (2) Planning\nwith First-Order ToM: Utilizing First-Order ToM, Suspicion-Agent is capable of making\ndecisions based on its own and estimates of the opponent’s card strength. As a result, it will\nraise more than vanilla planning but it tends to fold more frequently than other strategies,\naiming to minimize unnecessary losses. However, this cautious approach can be exploited\nby savvy opponent models. For example, DMC often raises when holding the weakest\nhand, and CFR+ may occasionally raise even with a mid-level hand to exert pressure on\nSuspicion-Agent. In these instances, Suspicion-Agent’s tendency to fold can lead to losses.\n(3) Planning with Second-Order ToM: In contrast, Suspicion-Agent excels at identifying\nand capitalizing on the behavioural patterns of opponent models. Specifically, when DMC\nchecks—suggesting its hand doesn’t align with the public cards—Suspicion-Agent will raise\nas a bluff to induce folds from the opponents. As a result, Suspicion-Agent exhibits the\nhighest raise rate among the three planning methods evaluated. This aggressive strategy\nallows Suspicion-Agent to accumulate more chips even when holding a weak hand, thereby\nmaximizing its chip gains.\nTable 3:\nThe comparison re-\nsults of Suspicion-Agent when\nplaying with different levels of\nToM against CFR+ on Leduc\nHold’em environments.\nThe\nresults are quantified after 100\ngames.\nOpponent Model\nCFR+\nDMC\nOurs (vanilla plan)\n-89\n-72\nOurs (w\/ First ToM)\n-67\n-26\nOurs (w\/ Second ToM) -22\n+24\nTable 4: The comparison results indicate the impact of\nincluding opponent observations in the game history\nin Leduc Hold’em environments. These results are\nquantified over 50 games, and pos denote the position\nof the opponent model.\nOpponent Model\nCFR+\n(pos 1)\nDMC\n(pos 1)\nAvg.\nWin\nProbability\nDMC\n-6\n+10\n+2\n50%\nCFR+\n-35\n+21\n-14\n50%\nOurs (w\/o hind obs)\n-45\n+18\n+17\n100%\nOurs (w\/ hind obs)\n-16\n+36\n+36.5\n100%\nAblation Study on the Effect of Hindsight Observation\nFollowing (Southey et al., 2012),\nwe assume that Suspicion-Agent has access to observations of the opponent after the end of\neach game, i.e., Hindsight Observation. To assess the impact of it, we conduct an ablation\nstudy in which hindsight observations are not incorporated into the current game. Without\nhindsight observations, we augment the Reflexion module with additional prompts to\nenable it to infer the opponent’s cards based on game outcomes and imperfect observations.\nAs demonstrated in Table 5 and 4, Suspicion-Agent retains its performance advantage over\nthe baseline methods without the benefit of hindsight observations. Specifically, we observe\nthat Suspicion-Agent adopts a more conservative strategy under the increased uncertainty\nthat comes without hindsight observations. This leads to reduced bluffing, resulting in fewer\ngains when playing against CFR+. However, it also minimizes the risk of over-bluffing\nwhen facing DMC, thus yielding higher chip earnings.\n5\nConclusion\nIn this paper, we introduce Suspicion-Agent, the first prompting system designed to en-\nable large language models to engage in various imperfect information games using only\nthe game rules and observations for interpretation. By incorporating first-order ToM and\nsecond-order ToM capabilities, we show that a GPT-4-based Suspicion-Agent can outper-\nform traditional algorithms such as NFSP, even without specialized training or examples.\nAdditionally, we identify and discuss the current limitations of utilizing LLMs in the con-\ntext of imperfect information games. We make all our code and interactive data publicly\navailable to the research community. This will help in better understanding the capabilities\nof large language models, particularly GPT-4, and we hope our data will encourage the\n10\nPublished as a conference paper at COLM 2024\ndevelopment of more efficient models for imperfect information games. In addition, we\nalso present the limitations of Suspicion-Agent in Appendix A.5.","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nSuspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4\n```\n#### 2. 论文摘要\n```\nUnlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities of\ndecision-making under uncertain or incomplete information. GPT-4, the recent\nbreakthrough in large language models (LLMs) trained on massive passive data,\nis notable for its knowledge retrieval and reasoning abilities. This paper\ndelves into the applicability of GPT-4's learned knowledge for imperfect\ninformation games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an\ninnovative agent that leverages GPT-4's capabilities for performing in\nimperfect information games. With proper prompt engineering to achieve\ndifferent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Importantly,\nGPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it\ncan understand others and intentionally impact others' behavior. Leveraging\nthis, we design a planning strategy that enables GPT-4 to competently play\nagainst different opponents, adapting its gameplay style as needed, while\nrequiring only the game rules and descriptions of observations as input. In the\nexperiments, we qualitatively showcase the capabilities of Suspicion-Agent\nacross three different imperfect information games and then quantitatively\nevaluate it in Leduc Hold'em. The results show that Suspicion-Agent can\npotentially outperform traditional algorithms designed for imperfect\ninformation games, without any specialized training or examples. In order to\nencourage and foster deeper insights within the community, we make our\ngame-related data publicly available.\n```\n\n#### 3. 论文全文\n```\nPublished as a conference paper at COLM 2024\nSuspicion-Agent: Playing Imperfect Information Games with\nTheory of Mind Aware GPT-4\nJiaxian Guo ∗†1\nBo Yang * 1\nPaul Yoo 1\nBill Yuchen Lin 2\nYusuke Iwasawa 1\nYutaka Matsuo 1\nThe University of Tokyo 1\nAllen Institute for AI 2\nDemo: https:\/\/shorturl.at\/dmxzD\nGame Data Replay: https:\/\/shorturl.at\/fhuCI\nCodes: https:\/\/github.com\/CR-Gjx\/Suspicion-Agent\nAbstract\nUnlike perfect information games, where all elements are known to every\nplayer, imperfect information games emulate the real-world complexities\nof decision-making under uncertain or incomplete information. GPT-4, the\nrecent breakthrough in large language models (LLMs) trained on massive\npassive data, is notable for its knowledge retrieval and reasoning abilities.\nThis paper delves into the applicability of GPT-4’s learned knowledge for\nimperfect information games. To achieve this, we introduce Suspicion-\nAgent, an innovative agent that leverages GPT-4’s capabilities for imperfect\ninformation games. With proper prompt engineering to achieve differ-\nent functions, Suspicion-Agent based on GPT-4 demonstrates remarkable\nadaptability across a range of imperfect information card games. Impor-\ntantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity,\nmeaning it can understand others and intentionally impact others’ behav-\nior. Leveraging this, we design a planning strategy that enables GPT-4\nto competently play against different opponents, adapting its gameplay\nstyle as needed, while requiring only the game rules and descriptions of\nobservations as input. In the experiments, we qualitatively showcase the\ncapabilities of Suspicion-Agent across three different imperfect information\ngames and then quantitatively evaluate it in Leduc Hold’em. As an explo-\nration study, we show that Suspicion-Agent can potentially outperform\ntraditional algorithms without any specialized training or examples, but\nstill cannot beat Nash-Equilibrium algorithms. In order to encourage and\nfoster deeper insights within the community, we make our game-related\ndata publicly available.\n1\nIntroduction\nRecently, large language models (LLMs) (Brown et al., 2020b; Chowdhery et al., 2022;\nTouvron et al., 2023), which are trained extensively on text corpora and code datasets and\naligned with instructions (Ouyang et al., 2022; Wei et al., 2021; Longpre et al., 2023), have\ndemonstrated remarkable knowledge retrieval and reasoning capabilities (Kojima et al.,\n2022; Wei et al., 2022b;a) on natural language benchmarks and exams (Hendrycks et al.,\n2020; Cobbe et al., 2021). Given few-shot examples or specific instructions as prompts,\nthese models, especially GPT-4 (OpenAI, 2023), can understand human intentions and\nmake informed decisions in open-ended scenarios, and tackle intricate tasks by gathering\nobservations and utilizing the learned prior knowledge, such as Voyager (Wang et al., 2023a),\nReAct (Yao et al., 2022) and SwiftSage (Lin et al., 2023).\nHowever, most of these methods typically assume that the agent has access to all relevant\ninformation, an assumption that is often unrealistic in real-world settings. Take diplomacy\n(Team et al., 2022; Gray et al., 2020) as an example: representatives must discern the veiled\n∗Equal Contribution\n†Corresponding to jiaxian.guo@weblab.t.u-tokyo.ac.jp. (Guo is now working at Google)\n1\narXiv:2309.17277v3  [cs.AI]  31 Aug 2024\nPublished as a conference paper at COLM 2024\nintentions of other countries based on incomplete information and decide accordingly\nto maximize benefits for their own nation. This challenge is not unique to diplomacy\nbut extends to other domains as well, such as poker (Moravˇc´ık et al., 2017; Brown &\nSandholm, 2018) and economic simulations (Holmstr¨om & Myerson, 1983; Harsanyi, 1968).\nThe inherent unpredictability in these games makes it impractical for a learned agent to\nadopt a single, optimal strategy for every scenario (Brown et al., 2019). This necessitates\npredictive capabilities for handling incomplete information, along with a theory of mind\n(ToM) ability (Frith & Frith, 2005) to comprehend decisions from others’ perspectives. Such\ncomplexities, both strategic and cognitive, represent challenges in the field of AI research.\nFurthermore, recent advancements in imperfect information games, such as ReBel (Brown\net al., 2020a), DeepStack (Moravˇc´ık et al., 2017) and Libratus (Brown & Sandholm, 2018),\ntypically start training from scratch, and thus they normally need millions of data to\nunderstand the game rules and learn the adequate strategies for each new game. Such\na high sample complexity hampers their ability to generalize across different games and\nposes challenges when applying them into complex and open-ended imperfect information\nscenarios. By contrast, as alluded to previously, LLMs have undergone training on massive\npassive datasets. This leads to an intriguing proposition: Can we harness pre-trained LLMs’\nknowledge and reasoning capabilities to navigate imperfect information games without\nadditional training or data?\nTo achieve this, we propose Suspicion-Agent, an innovative autonomous agent based on\nGPT-4. This agent harnesses its extensive prior knowledge and cognitive adaptability to\neffective strategies against a range of adversaries without any specialized training. Con-\ncretely, we first decompose the process of solving such games into multiple sub-modules\nlike observation interpreter and planning module to understand the game rules and game\nstates (as Figure 1 shows) so that GPT-4 can make decisions accordingly. Each module\nemploys different prompts to enable GPT-4 to fulfill specific functions. However, unlike\nperfect information games, planning strategies in imperfect information games can have\nvarying effectiveness depending on the opponent’s behavior (Brown et al., 2020a; Moravˇc´ık\net al., 2017; Brown & Sandholm, 2018). To tackle these challenges, we introduce a theory\nof mind (ToM) aware planning approach that leverages the higher-order ToM capability\n(Frith & Frith, 2005) present in LLMs. Specifically, the model utilizes its understanding of\nhuman cognition to predict opponents’ thought processes, susceptibilities, and actions. This\naligns with the idea that individuals use their own minds as models to understand and\naffect others (Montes et al., 2022). For instance, the model might consider, ”If I execute Plan\n1, how would this influence my opponent’s beliefs about my cards, and what actions might\nthey take based on their behavioral patterns?”\nConcretely, given the gameplay history as the input, we find that GPT-4 can identify an\nopponent’s strategic tendencies and analyze how our actions influence the opponent’s\nbehavior, e.g. if Suspicion-Agent identifies a weak hand held by the opponent, coupled\nwith the cautious strategy, it might strategically raise the bet to encourage the opponent\nto fold, even when Suspicion-Agent itself holds a similarly weak hand (as illustrated in\nFigure 9 and A.11). Remarkably, by using some simple prompts, e.g., GPT-4 can even self-\nexamine its behavior through the lens of the opponent (Refer to A.7). Leveraging its ToM\ncapabilities, GPT-4 can predict and even influence an opponent’s actions effectively (R¨oska-\nHardy, 2008). Integrating these simulated actions into our planning module can mitigate\nthe information asymmetry inherent in imperfect information games and more accurately\nassess the effectiveness of various strategies. As a result, our Suspicion-Agent can adjust\nits strategy to play effectively against a range of opponents, as shown in Section 4.1. In the\nexperiments, we first conduct a qualitative assessment of Suspicion-Agent’s in 3 two-player\nimperfect information games, aiming to showcase the generalization capabilities of our\nmethod. Subsequently, we perform a quantitative analysis in Leduc Hold’em (Southey et al.,\n2012). The results reveal that Suspicion-Agent exhibits varying behaviors when interacting\nwith previous works such as CFR+ (Tammelin, 2014) and NFSP (Heinrich & Silver, 2016)\nwhile outperforming some of them in terms of overall performance. In summary, our\ncontributions are as follows:\n1. We introduce Suspicion-Agent, the first agent framework designed to empower GPT-4\nwith theory of mind (ToM) ability to compete in various imperfect information games\n2\nPublished as a conference paper at COLM 2024\nSuspicion-Agent\nObservation \nInterpreter\n(LLM)\nPlanning\n(LLM)\nUnknown\nOpponent\n'hand': 'SJ',\n'public_card': 'SK',\n'all_chips’: [6, 6],\nGame State\nGame Rule\nObservation \nRule\nIn this round , you \nare holding a Jack \nof Spades and the \npublic card is a \nKing of Hearts. So \nfar, you have put 6 \nchips into the pot \nand your \nopponent has put \nin 6 chips. \nAction\nAction\nPlanning with First-Order Theory of Mind\nPlan \nProposals\n(LLM)\nObservation\nInterpretation\nAll Game \nPlay History\nPattern Analysis \n& Reflexion\n(LLM)\nSelf-Evaluate\n(LLM)\nThe opponent tends \nto raise (80%) when \nhe has King …\nOpponent \nCurrent \nInformation \nPrediction (LLM)\nPlan1: Raise\nPlan2: Check\n….\nThe opponent has \nKing (80%), Jack \n(10%), Queen (10%)\nReadable \nCurrent \nGame \nHistory\nPlan1: Based on the \nestimation of the \nopponent’s and my \ncard,\nPlan 1’s winning rate \nis 80%.  \nPlan2: \n…\nBased on the winning \nrate comparison, I will \nselect {Plan 1}\nAction\nFigure 1: Left Figure. The illustration of the Suspicion-Agent when playing against an\nunknown opponent. Right Figure. The illustration about the first-order ToM planning\nmethod, where the texts in yellow blocks are outputs, and green blocks are inputs.\nby understanding game rules and observational data without requiring any specialized\ntraining or examples. By incorporating the ToM capability into the planning process,\nSuspicion-Agent captures the inherent uncertainty of opponent behavior in our strategic\ndeliberations. This enables Suspicion-Agent to adapt its tactics dynamically when facing\nopponents with differing behavioral patterns.\n2. We are the first to demonstrate that an agent based on GPT-4 can potentially outperform\ntraditional algorithms in imperfect-information games, such as Leduc Hold’em (Southey\net al., 2012), when compared to established learning-based methods like NFSP (Heinrich &\nSilver, 2016) and DMC (Zha et al., 2021), but still unperformed Nash-Equilibrium algorithms\nlike CFR+ (Tammelin, 2014) which may inspire more subsequent use of LLMs in imperfect-\ninformation games.\n3. We make all interaction data between Suspicion-Agent and traditional algorithms for\nimperfect-information games in Leduc Hold’em publicly available. This will enable the\nresearch community to scrutinize the capabilities of GPT-4-based agents and inspire further\nwork, particularly in fine-tuning smaller language models.\n2\nProblem Definition\nTwo-Player Imperfect Information Game\nIn this paper, we propose to employ LLMs to\nplay imperfect information games. As a preliminary exploration, we concentrate primarily\non two-player imperfect information games, such as Leduc Hold’em (Southey et al., 2012),\nwhich involves two players, denoted by N = {1, 2}, who share the same action space, A.\nLet a1 ∈A and a2 ∈A represent the actions chosen by player 1 and player 2, respectively.\nEach player has access to two types of observations: a private observation, denoted as\nSpri(i) where i ∈N is the player index, and a public observation, shared among both\nplayers, denoted as Spub.\nAs the game progresses in discrete timesteps indexed by j, each player i observes a\nhistory h of the game. This history comprises the series of public and private obser-\nvations and actions up to timestep j −1 and the result of game rj, formally given as\nh = (S0\npub, S0\npri(i), a0\ni , a0\n¬i, r0 . . . , Sj−1\npub, Sj−1\npri(i), aj−1\ni\n, aj−1\n¬i , rj−1). Simultaneously, players re-\nceive the current private and public observations, Sj\npri(i) and Sj\npub, and select the next action\naj\ni according to a policy πi. All game histories are constructed as a dataset D, denoted as\nD = (h1, h2, . . . , hM), where M indexes individual games. The goal of each player is to\nselect the next action aj\ni with the imperfect observation according to the game rules, aiming\nfor victory over many games. Specifically, the order of players is not fixed and depends on\nthe game rule for each game. For example, the role of the small blind rotates among players\nin Texas Holed’em, dictating the order of play.\n3\nMethod\nTo enable LLMs to play various imperfect information games without specialized training,\nwe break down the overall task into several modules shown in Figure 1, such as the observa-\n3\nPublished as a conference paper at COLM 2024\nVanilla Planning\nPlanning with First-Order \nTheory of Mind\nPlanning with Second-Order \nTheory of Mind\nBased on my card, \nI would like to \ndo {action1} to \nwin the game or \nminimize the \nloss\nI know his behavior \npattern, and I can \nestimate his card  \nand his action. Based \non my card and his \ncard, I would like to \ndo {action1} to win \nthe game\nI know his \nbehavior \npattern, and I \ncan estimate \nhis card and \naction\nI know his \nbehavior \npattern, and \nI can \nestimate his \ncard and \naction based \non his action\nI know he knows \nmy behavior \npattern, and I \ncan adjust my \naction to win him\nFigure 2: Left Figure. The decision-making of the vanilla planning of Suspicion-Agent.\nMiddle Figure. The decision-making of the planning with first-order ToM of Suspicion-\nAgent. Right Figure. The decision-making of the planning with second-order ToM of\nSuspicion-Agent.\ntion interpreter, game pattern analysis, and planning module. In the following sections, we\nwill demonstrate how we craft specific prompts to guide LLMs to use its prior knowledge,\nreasoning ability, and psychological ability in performing these modular functions and\nexplain how we combine these functions to equip the model with the capability to navigate\nthe intricacies of imperfect information games. All prompts and codes will be made public\non our codebase (Please refer to our supplementary material).\n3.1\nGame Rule & Observation Understanding\nWhile LLMs excel in processing text data, it can be misled in imperfect information games\nbecause they normally provide only brief, low-level descriptions. To mitigate this issue, we\ninitially develop structured prompts that assist LLMs in comprehending both the game’s\nrules and its current state. For each type of imperfect information game, one can write a\nstructured rule description as follows:\n• General Rules: A brief game introduction, the number of rounds, and betting rules;\n• Action Descriptions: {Description of Action 1}, {Description of Action 2}, ...;\n• Single Win\/Loss Rule: The conditions for winning, losing, or drawing in a single game;\n• Win\/Loss Payoff Rule: The rewards or penalties for winning or losing a single game;\n• Whole Win\/Loss Rule: The number of games and the overall win\/loss conditions.\nIn most imperfect information game environments (Zha et al., 2019), game states are often\nrepresented as low-level numerical values, such as one-hot vectors, to facilitate machine\nlearning. Leveraging LLMs, we can convert these low-level game states into natural lan-\nguage text (Wu et al., 2023; Wang et al., 2023a; Guo et al., 2022; Lin et al., 2023), thereby\naiding the model’s understanding. For each game, it is also essential to define an observation\nconversion rule. Similar to structuring game rules, we organize the observation conversion\nrule as follows:\n• Input Explanation: The type of inputs received, such as dictionaries, lists, or other\nformats, and describes the number of elements in the game state along with the name of\neach element;\n• Element Descriptions: {Description of Element 1}, {Description of Element 2}, ...;\n• Conversion Tips: More guidelines for transforming the low-level game states into text.\nBy leveraging both the game rule and the observation conversion rule, we can effi-\nciently transform low-level game states into readable text, denoted as Obsr. This read-\nable text serves as the input for LLMs. Using the prompts Promptobs, the conditional\ndistribution for each element Obsr[i] in the generated text can be modeled as: Obsr ∼\n∏M\ni=1 Fθ(Obsr[i]|Promptobs, Rule, Ruleobs, Obsr[1, . . . , i −1]).\nHere, Fθ represents the lan-\nguage model parameterized by θ; M is the length of the generated text Obsr. The concrete\ndefinition can be found in Appendix A.2. We name this module an Observation Interpreter.\nThis formulation allows for a more understandable interaction with the model in imperfect\ninformation games.\n3.2\nVanilla Planning Module and Reflexion\nAfter understanding the game rules and converting the game states into a readable format,\nwe can craft prompts to guide LLMs in formulating strategies. Inspired by advancements in\n4\nPublished as a conference paper at COLM 2024\nLLMs-agent and prompt engineering (Ganguli et al., 2023; Wang et al., 2023e; Liu et al., 2023b;\nShinn et al., 2023), we introduce a vanilla planning method which features a Reflexion\nmodule aimed at automatically scrutinizing game history to enable LLMs to learn and\nimprove planning from the experience of the history, as well as a separate planning module\ndedicated to making decisions accordingly.\nReflexion The Reflexion module takes as input the history of games played against\nthe current opponent and outputs a Reflexion.\nIn the j-th round of the i-th game,\nwe gather all prior game histories, denoted as Di = (h1, h2, . . . , hi−1), and prompt\nLLMs to carry out these analytical functions to get the Reflexion output Oi\nf\n∼\n∏M\ni=1 Fθ(Of [i]|PromptRe f lexion, Rule, Di, Of [1, . . . , i −1]), i.e., Oi\nf ∼FRe f lexion\nθ\n, which covers\nwhy we won or lost in specific previous games, and suggests how to improve strategies for\nfuture games. Importantly, the Reflexion module empowers Suspicion-Agent to enhance its\nstrategies during gameplay, even without previous examples.\nPlanning After obtaining the Reflexion Of , we proceed to use the game rules, the cur-\nrent game history hi, the current readable observation Obsr, and the set of valid ac-\ntions {a} in the current game as inputs.\nWe then prompt LLMs to formulate multi-\nple textual plans based on its understanding of the imperfect information, i.e., Oplan ∼\n∏M\ni=1 Fθ(Oplan[i]|Promptplan, Rule, Obsr, hj−1, Of , Oplan[1, . . . , i −1]), Oplan ∼Fplans\nθ\n. Specifi-\ncally, the vanilla planning method assumes the marginal distribution of the actions of the\nopponent is uniform, and thus it can be regarded as a special case of planning with the\nzero-order ToM. In this way, we can further denote Fplans\nθ\nas Fzero−plan\nθ\n.\nEvaluator To assess the likely success of each plan, we introduce an evaluation module. This\nmodule takes into account factors such as the game’s current state, i.e., readable observation\nObsr, the Reflexion ORe f lexion, the game rule Rule and estimated plans Oplan as the input,\nto estimate the win rates for each of the proposed plans and output the next action by\nprompting LLMs, i.e., the next action aj = Fzero−eval\nθ\n(Obsr, ORe f lexion, Rule, Oplan).\n3.3\nPlanning with Theory of Mind (ToM)\nHowever, the vanilla planning method often struggles against the inherent uncertainties\nthat typify imperfect information games, particularly when faced with opponents skilled at\nexploiting others’ strategies. Inspired by this adaptability, we seek to devise a new planning\nmethod that capitalizes on LLMs’ ToM capabilities (Frith & Frith, 2005; Kosinski, 2023) to\nunderstand the opponent’s behavior and thus can adjust the strategy accordingly. In the\nfollowing sections, we will detail how we employ LLMs to analyze the behavior patterns\nof other agents and predict their subsequent actions in response to various plans using\ndifferent orders of ToM (results are shown in Table 3), thereby facilitating more informed\ndecision-making. Note that all sample outputs are given in Section A.11 and A.7.\nPlanning with First-Order ToM Modelling: In the first-order ToM modeling approach (as\nFigure 2 shows), Suspicion-Agent goes a step further by inferring the probable hand of\nthe opponent based on their actions to that point, e.g., if the opponent raised, they likely\nhave a strong hand. Consequently, Suspicion-Agent can adapt their strategy to maximize\nwinnings when holding a strong hand and minimize losses with a weak hand. To forecast\nthe opponent’s actions, we first introduce a behavior pattern analysis process. In this process,\nwe feed the game history Di = (h1, h2, . . . , hi−1) and the game rules into LLMs, prompting\nit to analyze the opponent’s behavioral pattern. The formulation and the prompts can be\nexpressed as: Obp ∼∏M\ni=1 Fθ(Obp[i]|Promptpattern, Rule, Di, Obp[1, . . . , i −1]).\nSample Prompts for First-Order Behaviour Pattern Analysis (Incomplete) :\nFrom my\nperspective, please infer several beliefs about the opponent’s game pattern\/preference for each round\nwhen holding different cards and the public card (if have).\nThrough this approach, we can deduce the opponent’s behavior pattern. Notably, since\nthe input for behavior pattern analysis is the same as that for the Reflexion module, we\nhave integrated them into a single module to reduce inference time, as shown in Figure 1.\n5\nPublished as a conference paper at COLM 2024\nAfter identifying the opponent’s behavior pattern, LLMs can be prompted to predict the\nstrength of the opponent’s current hand or observations in the game. This is expressed as:\nOcard pred ∼∏M\ni=1 Fθ(Ocard pred[i]|Promptcard pred, Rule, hj−1, Obp, Obsj\nr, Ocard pred[1, . . . , i −\n1]).\nSample Prompts for First-Order Cards Prediction (Incomplete) :\nUnderstanding the game\nrule, your observation, progress summarization in the current game, the estimated behaviour pattern\nof the opponent, and your knowledge about the game, please infer the probabilities about the cards of\nthe opponent (number 100% in total) step by step.\nWith these predictions, we can further augment the previous Planning module and Evalu-\nator module with Ocard pred as the additional input, so that we can further propose better\nplans considering the opponent’s card and estimate the winning rate of each plan, so that\nwe can better make the decision. Because the input of Planning module and Evaluator\nmodule are highly overlapped and our budgets are limited, we combine these two modules\ntogether to save the costs:\nSample Prompts for Planning and Evaluator (Incomplete):\nMake Reasonable Plans: Please\nplan several strategies according to actions you can play now to win the whole game step by step.\nNote that you can say something or keep silent to confuse your opponent.\nPotential opponent’s actions and Estimate Winning\/Lose\/Draw Rate: From the perspective of the\nopponent, please infer what the action opponent with probability would do when the opponent holds\ndifferent cards based on his behaviour pattern, and then calculate the winning\/lose\/draw rates when\nopponent holds different cards step by step. Output in a tree structure:\nThe sample outputs are shown in Figure 8 and A.7.1.\nPlanning with Second-Order ToM Modelling: However, elite players in imperfect in-\nformation games like poker are also adept at dynamically adjusting their strategies, and\nthey may employ ”bluffing” as a tactic, feigning a strong hand when they actually hold a\nweak one to deceive their opponent. Relying solely on a first-order ToM in such situations\ncould lead to incorrect assumptions and potentially costly mistakes. Recognizing this, we\nintroduce a planning method that incorporates a second-order ToM. In this enhanced model,\nSuspicion-Agent engages in even more intricate reasoning, where Suspicion-Agent not only\nconsiders what the opponent might do (as in first-order ToM) but also what the opponent\nbelieves Suspicion-Agent will do as Figure 2 shows. This level of strategic thinking allows\nSuspicion-Agent to gain an advantage in situations involving tactics like bluffing.\nTo implement this, Suspicion-Agent needs to not only just consider the current state from\nits own perspective, but also be capable of role-switching to think his own observation from\nthe opponent’s viewpoint. In traditional methods (De Weerd et al., 2013; Tatarchenko et al.,\n2016), they need to iteratively call the first-order ToM function to estimate the action of the\nopponent. However, we surprisingly find that we can just add the prompts like below, and\nget the outputs in Sec A.7.2.\nSample Prompts for Second-Order Behaviour Pattern Analysis (Incomplete):\nFrom my\nperspective, please infer under what circumstances is the opponent likely to be influenced by my\nactions? Additionally, in what situations would the opponent make decisions based solely on their\nown hand?\nFrom the perspective of the opponent (he cannot observe my card but only action), please infer several\nbeliefs about my game pattern\/preference when holding different cards.\nWith this, LLMs are able to automatically generate insights into whether the opponent’s\nbehavior is likely to be reactive to Suspicion-Agent’s actions, or if they are likely to act\nindependently. Then, we can directly reuse the prompts of the first-order Tom to predict\nthe opponent’s cards based on the behavior pattern estimated from the second-order ToM,\nand we can get sample results in Figure 9 and Section A.7.2. In this way, we can utilize the\nPlanning with second-order ToM to make decisions and adapt the strategies accordingly.\nThe concrete algorithms are given in Section A.9. Without mentioning otherwise, we use\nthe second-order ToM and GPT-4-0613 by default.\n6\nPublished as a conference paper at COLM 2024\n4\nExperiments\nWe conduct experiments to answer the following questions:\n• Can Suspicion-Agent achieve comparable performance with traditional imperfect informa-\ntion algorithms without any specialized training? (Section 4.1)\n• Can Suspicion-Agent adapt its strategies when playing with different opponents? (Section\n4.1)\n• Can Suspicion-Agent play different imperfect information games without any specialized\ntraining? (Section A.11.1)\n• How different orders of ToM improve the performance of Suspicion-Agent? (Section 4.3\nand A.8)\n4.1\nQuantitative Evaluation\nEnvironments\nTo quantitatively assess the performance of LLMs in imperfect information\ngames, we chose the RLCard environment (Zha et al., 2019). Due to budget limits, our\nquantitative evaluation focuses on Leduc Hold’em, a simplified version of Limit Texas\nHold’em. The game rules of Leduc Hold’em can be found in Appendix A.3. Following\n(Southey et al., 2012), we also add the opponent’s observation into the single game history\nh after the end of each game, which also conforms with the real-world experience, but we\nalso perform the ablation study about it in Section 4.3 and A.10.\nCompeting Methods\nWe have selected a range of methods commonly used in decision-\nmaking, such as NFSP (Heinrich & Silver, 2016), DQN (Mnih et al., 2015), DMC (Deep Monte\nCarlo Search for imperfect information games) (Zha et al., 2021) and CFR+ (Zinkevich et al.,\n2007). Among these, NFSP and DMC are specifically designed for imperfect information\ngames and are based on self-play, while CFR+ is grounded in game theory. These algorithms\ntypically show different strategies in the imperfect information games, allowing us to\nevaluate the adaptability of each method. Note that, our Suspicion-Agent does not have\nany specialized training when compared with these methods.\nEvaluation Methods\nTo ensure the robustness of our evaluation metrics, we meticulously\ndesigned a dual-method evaluation framework aimed at mitigating the randomness intrinsic\nto imperfect information games. (1) Variable Random Seeds: Suspicion-Agent play against\ndifferent baselines for 100 games utilizing varying random seeds for each game. This tactic\nis intended to dampen the stochastic variability introduced by the random seed settings.\nThe results are shown in Table 1. (2) Same Cards with Exchange Position: We ran a series of\n50 games with a fixed random seed, thereby keeping the sequence of cards constant across\nthese games. Suspicion-Agent initially played at position 0 for the first 50 games, then we\nrerun the 50 games but switched the position of Suspicion-Agent and the baseline model.\nIn this way, Suspicion-Agent and the baseline should have the same card strength over\n100 games, and thus we can better evaluate the performance of each. The results of these\nexperiments are presented in Table 2.\nResults Analysis\n(1) Suspicion-Agent outperforms most baselines: As illustrated in\nTable 1, it is evident that our GPT-4-based Suspicion-Agent outperforms most other algo-\nrithms specifically trained on Leduc Hold’em environments except CFR+. Notably, it not\nonly defeats most of these methods but also secures the highest average chip count in the\ncomparisons. Our approach surpasses the second-best method by an impressive margin\nof approximately 25%. Even if the current design cannot achieve Nash-Equilibrium, these\nfindings compellingly showcase the potential of employing large language models in the\nrealm of imperfect information games, as well as affirm the effectiveness of our proposed\nframework. (2) The gap between GPT-3.5 and GPT-4 is large: While GPT-4 delivers perfor-\nmance that either matches or outperforms other baselines, agents using GPT-3.5 experience\na significant drop in performance. Specifically, the winning probability for agents built\non GPT-3.5 stands at just 50%, as opposed to 100% for GPT-4-based agents. Additionally,\nthe average chip payoff for GPT-3.5 agents is negative, underlining the stark performance\ndisparity between the two versions of the language model. The further reason analysis can\nbe found in Appendix A.4. (3) Suspicion-Agent outperforms baselines in both positions:\nUtilizing identical card sequences for both positions, Suspicion-Agent exhibits a consistent\nwinning pattern against various baselines, as evidenced in Table 2. This robust performance\n7\nPublished as a conference paper at COLM 2024\nserves as compelling evidence to substantiate the claim that Suspicion-Agent outperforms\nthe baseline models when card strength is held constant.\nTable 1: The comparison results of Suspicion-Agent when playing with different algorithms\ntrained on Leduc Hold’em environments. The results are the win\/lose chips after 100 games\nwith different seeds, and the number of win\/lose chips ranges from 1 to 14.\nOpponent Model\nNFSP\nDQN\nDMC\nCFR+\nOurs\n(GPT-3.5)\nOurs\n(GPT-4)\nAvg.\nNFSP\n-\n-33\n-22\n-21\n-3\n-142\n-61.25\nDQN\n+33\n-\n-55\n-8\n+200\n-44\n+22.8\nDMC\n+22\n+55\n-\n-15\n-49\n-24\n+4\nCFR+\n+21\n+8\n+15\n-\n+126\n+22\n+38.4\nOurs (GPT-3.5)\n+3\n-200\n+49\n-126\n-\n-\n-55\nOurs (GPT-4)\n+142\n+45\n+24\n-22\n-\n-\n+47.25\nTable 2: The comparison results of Suspicion-Agent when playing with CFR+ and DMC\ntrained in Leduc Hold’em environments. These results are quantified over 50 games, and\npos denote the position of the opponent model. For example, CFR+ (pos 0) denotes the\nopponent model is in the position 0, and the model is located in the position 1.\nOpponent Model\nCFR+\n(pos 0)\nCFR+\n(pos 1)\nDMC\n(pos 0)\nDMC\n(pos 1)\nOurs\n(pos 0)\nOurs\n(pos 1)\nAvg.\nDMC\n-21\n-6\n-10\n+10\n-36\n-4\n-11.17\nCFR+\n+35\n-35\n+6\n+21\n+16\n-1\n+7\nOurs\n+1\n-16\n+11\n+36\n-\n-\n+8\nBehaviour Pattern Analysis\nWe illustrate the action percentages of Suspicion-Agent and\nbaselines in Figure 3. We can observe that (1) Suspicion-Agent vs CFR+: CFR+ algorithm\n(Zinkevich et al., 2007) exhibits a mixed strategy, characterized by a conservative approach\nwhere it tends to fold in the face of a weak hand, especially when this hand does not align\nwell with the public cards. Our agent, recognizing this pattern, strategically opts to raise\nmore frequently. This tactic effectively applies pressure on the CFR+ algorithm, increasing\nthe likelihood of it folding under uncertain conditions. However, Over-reliance on bluffing\ncan lead to significant losses, especially when facing opponents capable of recognizing\nand exploiting this pattern. Additionally, it’s noteworthy that CFR+ itself is not averse\nto bluffing. Although it typically plays conservatively, CFR+ does occasionally employ\nbluffing tactics, which can result in losses for our agent. (2) Suspicion-Agent vs DMC: DMC\nalgorithm (Zha et al., 2021) based on the search algorithm DMC employs a more diversified\nstrategy that includes bluffing. It often raises both when it has the weakest and the strongest\nhands. In response, Suspicion-Agent adapts by raising less frequently and opting to call or\nfold more often based on its own hand and the observed behavior of DMC. (3) Suspicion-\nAgent vs DQN: DQN appears to have a more aggressive stance, almost always raising\nwith strong or mid-level hands and never folding. Suspicion-Agent identifies this and, in\nturn, minimizes its own raises (the lowest percentage among all matchups), opting more\noften to call or fold based on the public cards and DQN’s actions. (4) Suspicion-Agent\nvs NFSP: NFSP exhibits a follow-through strategy, opting to always call and never fold.\nSuspicion-Agent responds to this by raising less frequently (compared to matches against\nCFR+) and choosing to call more (compared to matches against CFR+) based on the public\ncard and NFSP’s observed actions. The analysis clearly shows that Suspicion-Agent is\nhighly adaptable and capable of exploiting the weaknesses in the strategies employed by\nvarious other algorithms. This speaks volumes about the large language model’s capability\nto reason and adapt in imperfect information games.\n4.2\nQualitative Evaluation\nIn the qualitative evaluation, we assess Suspicion-Agent on three imperfect information\ngames: Coup, Texas Hold’em Limit, and Leduc Hold’em (Southey et al., 2012). For each\ngame, we provide only the rules and observation rules as described in Section 3.1. Im-\nportantly, Suspicion-Agent is able to play these games without any additional training or\nsampling. Qualitative examples from these games are presented in the subsequent sections.\n8\nPublished as a conference paper at COLM 2024\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparison of Action Percentage when Playing Against Different Algorithms\nvs CFR\nvs DQN\nvs DMC\nvs NFSP\nraise\ncall\ncheck\nfold\nAction Category\n0\n10\n20\n30\n40\n50\n60\nAction Percentage (%)\nComparisons of Action Percentage of Different Algorithms\nCFR\nDQN\nDMC\nNFSP\nFigure 3: Left Figure. The illustration of the action percentage of Suspicion-Agent when\nplaying against different algorithms. Right Figure. The illustration of the action percentage\nof different algorithms when playing against Suspicion-Agent.\n.\nAction: Call (Holds Jack or Queen)\nObs:In this round, you are holding a Jack of Hearts. So far, you have put 2 chips into the pot, while your opponent \nhas put 2 chips.\nVanilla Planning\nBelief on GoodGuy's cards: GoodGuy chose to 'call' in the first round. According to his behavior pattern, when he \nholds a King, he would like to raise in the first round. Therefore, it is unlikely that he holds a King. When he \nholds a Queen or a Jack, he would like to call in the first round. Therefore, he is likely to hold a Queen or a \nJack. So, the probabilities are: King (0%), Queen (50%), Jack (50%).\nPotential GoodGuy's current believes about your cards:  I have not taken any action in the first round yet. \nAccording to GoodGuy's guess on my game pattern, he believes that I would like to raise when I hold a King, Queen, \nor Jack. Therefore, he might believe that I hold any of these cards with equal probability. So, the probabilities \nare: King (33.33%), Queen (33.33%), Jack (33.33%).\nMake Plans: Plan 1: Raise\n- If GoodGuy holds a Queen (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\n- If GoodGuy holds a Jack (50% probability), he will fold (100% probability). I will win the pot, which will \nbe 6 chips (4 from me, 2 from GoodGuy). The winning rate is 50%, the losing rate is 0%, and the draw rate is \n0%.\nPlan 2: Fold\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). I will lose the pot, which \nwill be 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate \nis 0%.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). I will lose the pot, which will \nbe 4 chips (2 from me, 2 from GoodGuy). The winning rate is 0%, the losing rate is 50%, and the draw rate is \n0%.\nPlan 3: Check\n- If GoodGuy holds a Queen (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\n- If GoodGuy holds a Jack (50% probability), he will check (100% probability). The game will proceed to the \nsecond round. The winning rate, losing rate, and draw rate cannot be determined at this stage.\nEstimate Expected Chips Gain for Each Plan:\n- Plan 1: Expected Chips Gain = 50% * 3 chips - 0% * 0 chips = 1.5 chips\n- Plan 2: Expected Chips Gain = 0% * 0 chips - 50% * 2 chips = -1 chips\n- Plan 3: The Expected Chips Gain cannot be determined at this stage.\nPlan Selection: Plan 1: raise\nAction: Raise\nAction: Fold\nFigure 4:\nThe qualitative sample of planning with second-order ToM Suspicion-Agent\nabout Strategic Bluffing on Leduc Hold’em. More samples are given in Appendix A.11.\nLeduc Hold’em We present qualitative samples showcasing Suspicion-Agent’s behaviour\nunder different strategies: Vanilla Planning, Planning with First-Order ToM, and Planning\nwith Second-Order ToM in Leduc Hold’em. These samples can be viewed in Figure 6, 2, 8,\nand 9, respectively, and the concrete analysis is given in Appendix A.11.1.\nGame Coup and Texas Hold’em Limit As illustrated in Figure 12 and 11 in Appendix,\nwhen provided solely with the rules and observation guidelines of Texas Hold’em Limit and\nCoup, and keeping the agent prompts consistent, Suspicion-Agentis still adept at discerning\nthe opponent’s game patterns. It analyzes the strength of its hand and subsequently makes\ninformed decisions to accumulate chips. This is the strong evidence to demonstrate the\ngeneralization ability of Suspicion-Agent based on GPT-4 on different imperfect information\ngames, and thus outperforming the algorithms need to be re-trained for every new imperfect\ninformation game. Specifically, In the game of Coup, without any prior training, Suspicion-\nAgent rapidly discerns which character the opponent lacks. It then strategically bluffs\nas that character to block the opponent’s actions. This ability to bluff successfully gives\nSuspicion-Agent a consistent advantage throughout multiple rounds.\n4.3\nAblation Study and Component Analysis\nAblation Study on Orders of ToM\nIn order to explore how different orders of ToM-\naware planning methods affect the behavior of the large language model. We perform the\n9\nPublished as a conference paper at COLM 2024\nexperiments and comparisons on Leduc Hold’em and play against CFR+. We illustrate\nthe action percentages of Suspicion-Agent with planning with different levels of ToM in\nFigure 5 and the chips gain results in Table 3. We can observe that (1) Vanilla Planning:\nBased on Reflexion module, vanilla planning tends to call and check more (the highest\ncall and check percentage when playing against both CFR+ and DMC) during the game,\nwhich cannot push the pressure to make the opponent fold and result in many unnecessary\nlosses. In this way, vanilla planning has the lowest chip gain as Table 3 shows. (2) Planning\nwith First-Order ToM: Utilizing First-Order ToM, Suspicion-Agent is capable of making\ndecisions based on its own and estimates of the opponent’s card strength. As a result, it will\nraise more than vanilla planning but it tends to fold more frequently than other strategies,\naiming to minimize unnecessary losses. However, this cautious approach can be exploited\nby savvy opponent models. For example, DMC often raises when holding the weakest\nhand, and CFR+ may occasionally raise even with a mid-level hand to exert pressure on\nSuspicion-Agent. In these instances, Suspicion-Agent’s tendency to fold can lead to losses.\n(3) Planning with Second-Order ToM: In contrast, Suspicion-Agent excels at identifying\nand capitalizing on the behavioural patterns of opponent models. Specifically, when DMC\nchecks—suggesting its hand doesn’t align with the public cards—Suspicion-Agent will raise\nas a bluff to induce folds from the opponents. As a result, Suspicion-Agent exhibits the\nhighest raise rate among the three planning methods evaluated. This aggressive strategy\nallows Suspicion-Agent to accumulate more chips even when holding a weak hand, thereby\nmaximizing its chip gains.\nTable 3:\nThe comparison re-\nsults of Suspicion-Agent when\nplaying with different levels of\nToM against CFR+ on Leduc\nHold’em environments.\nThe\nresults are quantified after 100\ngames.\nOpponent Model\nCFR+\nDMC\nOurs (vanilla plan)\n-89\n-72\nOurs (w\/ First ToM)\n-67\n-26\nOurs (w\/ Second ToM) -22\n+24\nTable 4: The comparison results indicate the impact of\nincluding opponent observations in the game history\nin Leduc Hold’em environments. These results are\nquantified over 50 games, and pos denote the position\nof the opponent model.\nOpponent Model\nCFR+\n(pos 1)\nDMC\n(pos 1)\nAvg.\nWin\nProbability\nDMC\n-6\n+10\n+2\n50%\nCFR+\n-35\n+21\n-14\n50%\nOurs (w\/o hind obs)\n-45\n+18\n+17\n100%\nOurs (w\/ hind obs)\n-16\n+36\n+36.5\n100%\nAblation Study on the Effect of Hindsight Observation\nFollowing (Southey et al., 2012),\nwe assume that Suspicion-Agent has access to observations of the opponent after the end of\neach game, i.e., Hindsight Observation. To assess the impact of it, we conduct an ablation\nstudy in which hindsight observations are not incorporated into the current game. Without\nhindsight observations, we augment the Reflexion module with additional prompts to\nenable it to infer the opponent’s cards based on game outcomes and imperfect observations.\nAs demonstrated in Table 5 and 4, Suspicion-Agent retains its performance advantage over\nthe baseline methods without the benefit of hindsight observations. Specifically, we observe\nthat Suspicion-Agent adopts a more conservative strategy under the increased uncertainty\nthat comes without hindsight observations. This leads to reduced bluffing, resulting in fewer\ngains when playing against CFR+. However, it also minimizes the risk of over-bluffing\nwhen facing DMC, thus yielding higher chip earnings.\n5\nConclusion\nIn this paper, we introduce Suspicion-Agent, the first prompting system designed to en-\nable large language models to engage in various imperfect information games using only\nthe game rules and observations for interpretation. By incorporating first-order ToM and\nsecond-order ToM capabilities, we show that a GPT-4-based Suspicion-Agent can outper-\nform traditional algorithms such as NFSP, even without specialized training or examples.\nAdditionally, we identify and discuss the current limitations of utilizing LLMs in the con-\ntext of imperfect information games. We make all our code and interactive data publicly\navailable to the research community. This will help in better understanding the capabilities\nof large language models, particularly GPT-4, and we hope our data will encourage the\n10\nPublished as a conference paper at COLM 2024\ndevelopment of more efficient models for imperfect information games. In addition, we\nalso present the limitations of Suspicion-Agent in Appendix A.5.\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 利用GPT-4的“心智理论”能力玩不完美信息游戏\n\n## 📌 背景痛点\/本文动机\n在现实世界中，决策往往是在信息不完整或不确定的情况下进行的。然而，大多数现有的AI算法都是在完美信息游戏中训练的，即所有玩家都能看到所有信息。这限制了它们在现实世界中的应用。本文旨在探索如何利用大型语言模型（LLM）的知识和推理能力来处理不完美信息游戏，从而更好地模拟现实世界的决策过程。\n\n## 🚀 核心方法\n本文提出了一个名为Suspicion-Agent的创新型自主代理，它基于GPT-4，并利用其强大的知识检索和推理能力来玩不完美信息游戏。Suspicion-Agent的核心创新点包括：\n\n💡 创新点1：利用GPT-4的“心智理论”（ToM）能力，即理解他人并有意影响他人行为的能力。这使得Suspicion-Agent能够预测对手的行为，并根据对手的行为调整自己的策略。\n\n💡 创新点2：将游戏过程分解为多个子模块，如观察解释器、游戏模式分析和规划模块。每个模块都使用不同的提示来引导GPT-4执行特定的功能，从而实现更有效的决策。\n\n## 📈 实验结果\n在实验中，Suspicion-Agent在三个不同的不完美信息游戏中展示了其能力，并在Leduc Hold'em游戏中进行了定量评估。结果表明，Suspicion-Agent可以潜在地超越传统算法，而无需任何专门的训练或示例。\n\n## 💬 可借鉴之处\n本文提出的Suspicion-Agent框架为利用LLM在不完美信息游戏中进行决策提供了一个新的思路。其核心思想是将LLM的知识和推理能力与ToM能力相结合，从而实现更有效的决策。此外，本文还公开了所有与游戏相关的数据，这将有助于研究人员更好地理解LLM的能力，并开发更有效的模型。","llm_summary_res_status":200}
{"title":"Enhancing Agent Learning through World Dynamics Modeling","authors":"Zhiyuan Sun, Haochen Shi, Marc-Alexandre Côté, Glen Berseth, Xingdi Yuan, Bang Liu","summary":"Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.","url":"http:\/\/arxiv.org\/abs\/2407.17695v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.17695v2","published":1721871161000,"comment":null,"pdf_text":"Enhancing Agent Learning through World Dynamics Modeling\nZhiyuan Sun1* †, Haochen Shi1*\nMarc-Alexandre Côté2, Glen Berseth1,3, Xingdi Yuan2‡, Bang Liu1,3‡\n1 Université de Montréal & Mila, Montréal, Canada\n2 Microsoft Research, Montréal, Canada\n3 Canada CIFAR AI Chair\nAbstract\nLarge language models (LLMs) have been in-\ncreasingly applied to tasks in language under-\nstanding and interactive decision-making, with\ntheir impressive performance largely attributed\nto the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of\nthis knowledge can vary across domains. Many\nexisting approaches assume that LLMs pos-\nsess a comprehensive understanding of their\nenvironment, often overlooking potential gaps\nin their grasp of actual world dynamics. To\naddress this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discov-\ners world dynamics from a small number of\ndemonstrations, verifies the accuracy of these\ndynamics, and evolves new, advanced dynam-\nics tailored to the current situation. Through\nextensive evaluations, we assess the impact of\neach component on performance and compare\nthe dynamics generated by DiVE to human-\nannotated dynamics. Our results show that\nLLMs guided by DiVE make more informed\ndecisions, achieving rewards comparable to hu-\nman players in the Crafter environment and sur-\npassing methods that require prior task-specific\ntraining in the MiniHack environment1.\n1\nIntroduction\nBy absorbing internet-scale knowledge autoregres-\nsively, large language models (LLMs) develop a\nbroad understanding of the world (Achiam et al.,\n2023; Team et al., 2023; Brown et al., 2020; Tou-\nvron et al., 2023). This understanding enables them\nto perform well across a variety of tasks (Brown\net al., 2020; Huang et al., 2022; Yao et al., 2022,\n2024). However, achieving this level of compre-\nhension requires a training dataset that is diverse,\n* Equal contribution.\n† zhiyuan.sun@umontreal.ca\n‡ Equal advising.\n1 The\ncode\nis\navailable\nat\nhttps:\/\/github.com\/\nZhiyuuanS\/DiVE\n: LLMs’ Knowledge\n: Domain Knowledge\n: Overlapping Knowledge\nFigure 1: The knowledge gap between LLMs and down-\nstream domains. Although LLMs have a broad under-\nstanding of the world, they may struggle to grasp the\ncomplex dynamics of specific downstream domains.\nprecise, and in-depth to cover essential domain\ninformation. Otherwise, a knowledge gap may\nemerge between LLMs and the target domain, as\nillustrated in Figure 1.\nSince LLMs tend to follow the most common\npatterns in the dataset (Gunasekar et al., 2023) and\nare prone to hallucination, it is crucial that the nec-\nessary information not only appears frequently on\nthe internet but is also reliable (Dziri et al., 2022).\nFor tasks and environments that are newly emerg-\ning (Hafner, 2021; Samvelyan et al., 2021), these\nrequirements are often difficult to meet due to the\nnoisy nature of internet data, leading to potential\nknowledge gaps.\nWhile LLMs may have a general understanding\nof a domain, optimal decision-making requires in-\ndepth, state-specific knowledge. Providing LLMs\nwith such tailored information enhances their grasp\nof both the environment and the current state. For\nexample, mastering Go involves not just knowing\nthe rules but also applying strategies for specific\nboard states. This specialized knowledge varies\nacross domains and states, making offline collec-\ntion impractical.\nTo address these challenges,\nwe propose\nDiscover, Verify, and Evolve (DiVE). Building on\narXiv:2407.17695v2  [cs.AI]  15 Oct 2024\nthe concept of the World Model (Ha and Schmid-\nhuber, 2018), DiVE not only discovers and veri-\nfies primitive world dynamics from demonstrations\nbut also evolves state-specific knowledge for the\ndownstream domain. By providing LLMs with this\ncomprehensive set of dynamics, DiVE bridges the\nknowledge gap between LLMs and the downstream\ndomain, thereby enhancing their decision-making\nabilities.\nDiVE consists of three distinct components: The\nDiscoverer: this component iteratively uncovers\nthe environment’s dynamics from demonstrations\nusing a curriculum learning approach. The Veri-\nfier: this component eliminates unreliable dynam-\nics caused by LLMs’ tendency to hallucinate. The\nEvolver: this component reasons through in-depth,\nstate-specific strategies tailored to the current situa-\ntion based on the learned dynamics.\nIn the Crafter environment (Hafner, 2021) and\nthe MiniHack environment (Samvelyan et al.,\n2021), DiVE learns comprehensive and reliable\ndynamics from demonstrations, guiding the agent’s\ndecision-making process by evolving in-depth\nstrategies. This enables the agent to outperform all\nbaselines, achieving rewards comparable to human\nplayers in the Crafter environment and surpassing\nthe performance of methods requiring task-specific\ntraining in the MiniHack environment. To gain\ndeeper insights into DiVE’s behavior, we provide\nboth quantitative and qualitative analyses.\nIn summary, our primary contribution is a frame-\nwork that learns world dynamics from demonstra-\ntions, guiding LLMs in the decision-making pro-\ncess by online evolving contextual strategies. This\napproach bridges potential knowledge gaps, result-\ning in a more optimal decision-making process for\nLLMs.\n2\nThe Knowledge Gap\nIn this work, we consider a setting where a pre-\ntrained LLM-based agent is employed to solve\ntasks in downstream domains. Conceptually, we\ndefine KLLM as the set of knowledge embedded\nin the LLMs through their training process, and\nKtarget as the universal set of knowledge relevant to\nthe downstream domains.\nTo ensure the effectiveness of the LLMs, we aim\nfor Krelevant, the subset of KLLM that is relevant to\nKtarget, to cover the broadest possible scope. Ad-\nditionally, we seek for Krelevant to contain more\nreliable knowledge K+ than inaccurate knowledge\nK−, where Krelevant = K+ ∪K−. Thus, we de-\nfine three desirable properties to guide our system\ndesign:\n• Recall R =\n|K+|\n|Ktarget| measures the extent to which\nthe knowledge required to solve tasks in the tar-\nget domain is covered by the LLMs. A low recall\ntypically indicates a significant knowledge gap\nbetween the LLMs’ training data and the target\ndomain.\n• Precision P =\n|K+|\n|Krelevant| measures the accuracy of\nthe LLMs’ knowledge when applied to the target\ndomain. An example of inaccurate dynamics is\nshown in Figure 2 (left), where the LLMs hallu-\ncinate that defeating a skeleton will drop items,\nwhich is not true in Crafter. This may be due to\nthe LLMs memorizing Minecraft-specific data.\n• Depth D measures the abstraction levels of\nknowledge representations. Knowledge can be\nrepresented at varying levels of abstraction, from\nbasic game rules to higher-level strategies.\nWe recognize that precisely quantifying the\nknowledge overlap between LLMs and a down-\nstream domain is challenging. In Section 3, we pro-\nvide a mathematical analysis showing how DiVE\nimproves across all three dimensions.\nIn Sec-\ntion 5.4, we measure the knowledge overlap to\ndemonstrate that DiVE effectively learns useful\ngame dynamics and, to some extent, bridges the\nknowledge gap.\n3\nDiVE: Discover, Verify, and Evolve\nIn an ideal scenario, one could bridge the knowl-\nedge gap by fine-tuning the LLM to adapt to the\ntarget domain. However, this approach is often less\npractical due to its reliance on large amounts of\nannotated data and significant computational over-\nhead (Hu et al., 2021; Zheng et al., 2024; Carta\net al., 2023; Ouyang et al., 2022). Our frame-\nwork, DiVE, is designed to address the knowledge\ngap while considering all three desirable proper-\nties—recall, precision, and depth—without requir-\ning extensive data collection from the target do-\nmain. It is a prompt-based method that learns world\ndynamics W directly from the environment.\nAs shown in Figure 2, DiVE is initially boot-\nstrapped using a small set of human demonstra-\ntion trajectories H, each consisting of observations\not, actions at, and rewards rt at each timestep\nt.\nWe then transform each observation ot into\nIt is daytime\\nState description: \\n-\nEast: immediate (grass); nearby \n(unexplored_area); \\n- North: \nimmediate (grass); nearby \n(unexplored_area); \\n- North East: \nimmediate (grass); nearby (cow, \nunexplored_area); \\n- North West: \nimmediate (grass); nearby \n(unexplored_area, tree); \\n- South: \nimmediate (grass); nearby \n(unexplored_area); cow: North East 5 \nblocks away (nearby) (objects in \nbetween: {'grass'}) \\n- Facing grass on \nthe south.\\nYour status:\\n- health: \n9\/9\\n- food: 9\/9\\n- drink: 9\/9\\n- energy: \n9\/9\\nYou have nothing in your \ninventory.\nIt is daytime\\nState description: \\n-\nEast: immediate (grass); nearby \n(unexplored_area); \\n- North: \nimmediate (grass); nearby \n(unexplored_area); \\n- North East: \nimmediate (grass); nearby (cow, \nunexplored_area); \\n- North West: \nimmediate (grass); nearby \n(unexplored_area, tree); \\n- South: \nimmediate (grass); nearby \n(unexplored_area); cow: North East 5 \nblocks away (nearby) (objects in \nbetween: {'grass'}) \\n- Facing grass on \nthe south.\\nYour status:\\n- health: \n9\/9\\n- food: 9\/9\\n- drink: 9\/9\\n- energy: \n9\/9\\nYou have nothing in your \ninventory.\nIt is nighttime\\nState description: \\n-\nEast: immediate (sand); nearby (path, \ngrass); distant (tree, unexplored_area);\n\\n- North: immediate (zombie); nearby \n(path, sand, grass); distant (tree, \nunexplored_area); \\n…… - zombie: \nNorth 1 blocks away (immediate) \n(objects in between: None) \\n- table: \nSouth East 9 blocks away (nearby) \n(objects in between: {'path', 'sand', \n'water', 'grass'}) \\n- Facing zombie on \nthe north.\\nYour status:\\n- health: \n2\/9\\n- food: 8\/9\\n- drink: 8\/9\\n- energy: \n3\/9\\nYour inventory:\\n- wood: 1\\n-\nstone: 9\\n- coal: 4\\n- wood_pickaxe: \n1\\n- stone_pickaxe: 1\\n- wood_sword: \n1\\n- stone_sword: 1\\n\nIt is nighttime\\nState description: \\n-\nEast: immediate (sand); nearby (path, \ngrass); distant (tree, unexplored_area);\n\\n- North: immediate (zombie); nearby \n(path, sand, grass); distant (tree, \nunexplored_area); \\n …… - zombie: \nNorth 1 blocks away (immediate) \n(objects in between: None) \\n- table: \nSouth East 9 blocks away (nearby) \n(objects in between: {'path', 'sand', \n'water', 'grass'}) \\n- Facing zombie on \nthe north.\\nYour status:\\n- health: \n2\/9\\n- food: 8\/9\\n- drink: 8\/9\\n- energy: \n3\/9\\nYour inventory:\\n- wood: 1\\n-\nstone: 9\\n- coal: 4\\n- wood_pickaxe: \n1\\n- stone_pickaxe: 1\\n- wood_sword: \n1\\n- stone_sword: 1\\n\nVerbalize\nDiscover\nVerify\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\nskeleton turn into path after defeat_skeleton, \nincrease bone by 1 and arrow by 1.\n……\nzombie turn into grass after defeat_skeleton, \nincrease rotten_fresh by 1 and bone by 1.\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\nskeleton turn into path after defeat_skeleton, \nincrease bone by 1 and arrow by 1.\n……\nzombie turn into grass after defeat_skeleton, \nincrease rotten_fresh by 1 and bone by 1.\n(verbalized\ndemonstration)\n(discovered game dynamics)\n(verified game dynamics)\n(demonstration)\n𝑜𝑡\n𝑜𝑡+1\n𝑎𝑡\nCrafter Env\nLLM Agent\n+\nEvolve\nUse collected stones to build a \nprotective barrier around the safe \nlocation.\n……\n(situational strategies)\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\n……\n(verified game dynamics)\nFigure 2: Overall pipeline of DiVE. Left: Learning basic game dynamics from offline demonstrations (Section 3.1).\nWe want to highlight the incorrect game dynamics being identified by the Verifier (labeled by ×), they are evidence\nof the LLMs hallucinate false facts perhaps because of memorizing Minecraft data. Right: Learning situational\nstrategies from online interactions (Section 3.2). For simplicity, we omit the verbalization process in the right figure.\nthe language space as ˜ot using a Verbalizer, re-\nsulting in trajectories represented by ˜ot, at, and\nrt at each timestep t. Subsequently, the Discov-\nerer extracts a set of world dynamic candidates,\n˜\nW = { ˜W+, ˜W−}, from human demonstrations H,\nwhere ˜W+ and ˜W−represent the correct and inac-\ncurate world dynamic sets, respectively.\nEmpirically, we find that the inclusion of ˜W−in\n˜\nW is often unavoidable, either due to LLM’s diffi-\nculties in extracting meaningful knowledge from\ntrajectory data or its tendency to hallucinate. To\naddress this, we employ the Verifier to filter out\npotentially invalid and conflicting world dynamic\ncandidates from ˜\nW, leaving only the valid dynam-\nics W. Lastly, we use the Evolver, designed to\nderive advanced dynamics I tailored to the ver-\nbalized observation ˜ot based on the filtered world\ndynamics W.\nThe final decision-making process on primitive\nactions at ∈A is hierarchically decomposed as\nplanning tasks on sub-goals SG, sub-tasks ST,\nand actions A. The planning procedure is further\nguided by both W and I. In cases where W ̸=\n∅, Recall, Precision and Depth are guaranteed to\nincrease as formulated below:\nRecall :\n|K+|\n|Ktarget|\nDiscoverer\n======⇒\nH\n|K+| + |W|\n|Ktarget|\nPrecision :\n|K+|\n|Krelevant|\nVerifier\n====⇒\n˜\nW,H\n|K+| + |W|\n|Krelevant| + |W|\nDepth : ∅Discoverer\n======⇒\nVerifier\nW Evolver\n====⇒\nW,H\nI ∪W\nThe DiVE framework can be divided into two\nstages: an offline dynamics learning stage and an\nonline strategy learning stage.\n3.1\nOffline Dynamics Learning\nThe offline dynamics learning procedure aims to\nbridge the knowledge gap between LLMs’ under-\nstanding and the basic dynamics of downstream\ndomains by learning the world dynamics W as a\nprior for the decision-making process. Instead of\nrelying on human-authored game manuals or hand-\nbooks to extract world dynamics, as in (Wu et al.,\n2024b,a), which are not only difficult to obtain in\nmany real-world scenarios but also often lack criti-\ncal details (as demonstrated in Table 6), we propose\nlearning world dynamics W directly from experi-\nences H, which are more accessible and provide\nricher information.\nHierarchical Curriculum Learning\nGiven the\nvarying complexities of learning the dynamics of\ndifferent elements in downstream domains, we\nadopt a curriculum learning approach (Bengio et al.,\n2009). Our method follows a sequential learning\nstrategy that progresses from simpler to more com-\nplex dynamics, thereby enabling more effective\nlearning. Specifically, we propose a method for\nlearning the dynamics of each element within the\ntask decomposition hierarchy, denoted as TD =\n{A ∪O, ST, SG}, where O represents the set of\nobjects in downstream domains.\nOur approach begins with elements of lower\nabstraction, such as actions a ∈A and objects\no ∈O, and gradually progresses to higher-level\nelements, such as sub-tasks sti ∈ST. The sub-\ntasks ST are represented as nodes in the achieve-\nment graph G = (V, E) within the downstream\ndomains, i.e., ST = V . Finally, we transition to\nthe subgoals sgi ∈SG. The subgoal sequence\nSG = [sg1, sg2, . . .] is an ordered list used to un-\nlock achievements in the achievement graph G,\nwhere SG = TopologicalSort(G), and each sgi\ncorresponds to a vertex in V .\nWe leverage the Discoverer to extract this order\nfrom human demonstrations H. Achieving a sub-\ngoal sgi may involve completing several sub-tasks\nmultiple times. This approach ensures a logical pro-\ngression through tasks, thereby enabling a deeper\nunderstanding and integration of downstream do-\nmain dynamics.\nDiscoverer\nThe Discoverer is designed to iden-\ntify dynamic candidates ˜\nW related to elements\nwithin the task decomposition hierarchy TD. A\nsingle dynamics discovery step for an element\nE ∈TD involves the following three main steps:\n1. Construction of the Semantic Experience\nBank: For each element E, we construct a se-\nmantic experience bank BE using demonstra-\ntions H. This bank stores experiences that are\ntransformed from H into a suitable granular-\nity for analyzing dynamics related to E. The\ntransformation process involves chunking and\nsummarizing the verbalized demonstrations to\ncapture essential semantic details.\n2. Sampling of Relevant Experiences: For each\nattribute of an instance e ∈E, a subset of expe-\nriences BE\ne that are relevant to the instance e is\nsampled from BE.\n3. Identification of Dynamic Candidates: A dy-\nnamic candidate ˜w is identified from the subset\nBE\ne by recognizing patterns that are consistent\nacross all experiences within BE\ne .\nThe action-level semantic experience bank, de-\nnoted as BA, stores transition tuples derived from\nverbal demonstrations and is represented as: BA =\n{{˜ot, at, ˜ot+1}i}|BA|\ni=1 . Similarly, the object-level\nsemantic experience bank, denoted as BO, gath-\ners individual observations contain an specific\nobject and is represented as: BO = {˜oi}|BO|\ni=1 .\nThe sub-task-level semantic experience bank, de-\nnoted as BST , aggregates trajectory segments rep-\nresenting the completion of sub-tasks and is for-\nmatted as: BST = {{˜ot, . . . , atst, ˜otst+1}i}|BST |\ni=1\n,\nwhere tst denotes the timestep at which a sub-\ntask st ∈ST is completed. For sub-goals, the\nsub-goal-level experience bank, denoted as BSG,\nrecords sequences of sub-tasks that culminate in\nthe completion of sub-goals, expressed as: BSG =\n{{stt, . . . , sttsg}i}|BSG|\ni=1\n, where tsg is the timestep\nat which the sub-goal sg is achieved.\nFor action-level dynamics learning, the relevant\nexperiences, denoted as BA\na , are compiled by ran-\ndomly sampling transition tuples from BA where\nthe action a has been successfully executed. A sim-\nilar approach is employed for dynamics learning\nof other elements within the task decomposition\nhierarchy TD.\nFor action-level dynamics discovery, we iden-\ntify the prerequisites and outcomes of each action\n(e.g., The action MakeWoodPickaxe requires\n1 wood). For object-level dynamics, we focus on\nco-occurrence relationships between objects and\ntheir temporal patterns. The attribute set for a sub-\ntask generally encompasses the necessary steps for\ncompletion, as well as its prerequisites, outcomes,\nand termination conditions. In contrast, the primary\nattribute of interest for a sub-goal is its correct po-\nsition within the sub-goal sequence SG.\nVerifier\nDynamic discovery processes are vul-\nnerable to noise from various sources, including\nconfounders, hallucinations by the LLMs, and dif-\nficulties in the LLMs’ ability to extract meaning-\nful insights from trajectory data. To address these\nchallenges, we introduce a dynamic verifier de-\nsigned to filter out noisy dynamic candidates from\n˜\nW. For each dynamic candidate w ∈˜\nW, the ver-\nifier begins by sampling a subset of relevant se-\nmantic experiences, denoted as BE\ne , from the cor-\nresponding semantic experience bank BE. Here,\nw represents a dynamic candidate associated with\na specific attribute of the instance e ∈E, where\nE ∈{A ∪O, ST, SG} corresponds to an element\nof the task decomposition hierarchy TD. The veri-\nfication of w is conducted as follows: w is deemed\ninaccurate and filtered out if it does not consistently\nhold across experiences within BE\ne or if it conflicts\nwith any established dynamics. The dynamics that\npass this verification process are classified as veri-\nfied dynamics and are denoted as W.\n3.2\nOnline Strategy Learning\nTo effectively incorporate the learned world dynam-\nics W into the downstream domains, we deploy an\nLLM-based agent defined by π : S × W →P(A).\nHere, S represents the state space, A denotes the\naction space, and P symbolizes the probability dis-\ntribution over the action space. Instead of directly\nmapping the world dynamics W and the current\nstate observation ot to the action at, we tackle the\nchallenge of long-horizon planning by integrating\nan online strategy learning method. This approach\ndecomposes the planning process into three distinct\nstages: sub-goal planning, sub-task planning, and\naction planning.\nSub-goal Planning\nGiven that the sub-goal se-\nquence SG = [sg1, sg2, . . .] is derived from hu-\nman demonstrations H and treated as a fixed se-\nquence, we utilize a straightforward heuristic for\nsub-goal planning. When a sub-goal is completed,\nthe current sub-goal is updated to the first uncom-\npleted sub-goal in SG.\nSub-task Planning\nFor a given current sub-goal\nsgi, we have developed an LLM-based sub-task\nplanner. This planner evaluates and ranks all sub-\ntasks st ∈ST based on the learned world dynam-\nics W, the verbalized current observation ˜ot, and\nthe most recently executed sub-task stt−1. The\nhighest-ranked sub-task is then designated as the\ncurrent sub-task stt. To ensure accurate execution,\nthe completion of a sub-task st is contingent upon\nsatisfying its specific termination condition. This\ncondition is verified by querying an LLM using the\ncurrent verbalized observation, the observation at\nthe time the sub-task began, and the termination\nconditions of the current sub-task.\nLearning Strategies\nIn addition to learning the\nfundamental rules of the downstream domains, we\nalso focus on developing advanced game-playing\nstrategies based on these dynamics. Unlike world\ndynamics learning, the strategy space is often too\nexpansive for exhaustive exploration. To address\nthis challenge, we propose evolving the dynam-\nics into strategies, denoted as I, using an online\nlearning approach.\nThis method reduces the search space by condi-\ntioning not only on the dynamics W, but also on the\nverbalized current observation ˜ot and the sub-task\nsti. This targeted approach enables the generation\nof strategies that are more contextually grounded\nand responsive to current game scenarios compared\nto those developed through offline methods. To sup-\nport this process, we have designed an LLM-based\nEvolver that generates strategy candidates ˜I by ap-\nplying deductive reasoning to the learned dynamics\nW. Specifically, the Evolver derives strategy can-\ndidates using rules of inference, such as modus\nponens. These strategy candidates, denoted as ˜I,\nare then evaluated for validity and ranked based on\ntheir utility by an LLM-based critic. Finally, the\nvalid and useful candidates are incorporated into\nthe situational strategy set I.\nAction Planning\nThe final action selection pro-\ncess is executed in two main steps:\n1. Invalid Action Masking: This step involves fil-\ntering out actions that are infeasible under the\ncurrent situation, based on the verified dynamics\nW and current verbalized observation ˜ot.\n2. Action Selection: From the set of valid actions,\na specific primitive action a is chosen based on\nmultiple factors: the current sub-task sti, the\nverbalized current observation ˜ot, the world dy-\nnamics W, a windowed history of previously\nplanned actions and observations, and the de-\nrived strategies I.\n4\nExperiment Setup\nTo demonstrate the effectiveness of DiVE in bridg-\ning the knowledge gap, we evaluate its performance\nwithin the Crafter and MiniHack environments. For\na fair comparison, all LLM-based agents, includ-\ning ours, utilize the GPT-4o model and the same\nenvironment seed in the Crafter setting. Further\ndetails on the setups can be found in Appendix A.1\nand B.1.\n4.1\nCrafter\nCrafter (Hafner, 2021) is an open-world survival\ngame set on 64 × 64 grid-based maps, featuring a\ndiverse array of materials such as tree, stone, and\ncoal, as well as entities including cow, zombie,\nand skeleton semi-randomly spawn on the maps.\nThe games include an achievement graph with 22\nunique achievements across 7 levels. The agent\nperceives its surroundings through a local 7 × 9\nobservation window and maintains awareness of its\nstatus within the game environment.\nThe text description generated by the verbalizer\nincludes: the nearest object of each type within\nthe accumulated observations, the objects situated\nbetween these nearest objects, the objects in each\ndirection, as well as the agent’s current inventory\nand status. An example of the verbalization process\nis provided in Appendix B.5.\nThe agent is evaluated using two primary met-\nrics: reward and score. Agents receive a +1 reward\nfor each new achievement unlocked (e.g., make\nwood pickaxe, place furnace) and a ±0.1 re-\nward for every health point gained or lost. The\nscore is calculated by aggregating the success rates\nsi for each achievement and is formulated as:\nS .= exp\n \n1\nN\nN\nX\ni=1\nln (1 + si)\n!\n−1.\nWe compare DiVE against the following baselines:\n• LLM-based approaches:\nSPRING (Wu\net al., 2024b), ELLM (Du et al., 2023), and\nChain-of-Thought (CoT)(Wei et al., 2022).\n• Reinforcement Learning (RL) approaches:\nDreamerV3(Hafner et al., 2023), PPO (Schul-\nman et al., 2017), and AD (Moon et al., 2024).\n• Human Players: Expert performance on the\nCrafter environment.\n4.2\nMiniHack\nMiniHack (Samvelyan et al., 2021) is a grid-based\nenvironment built on the video game NetHack (Küt-\ntler et al., 2020). Unlike the Crafter environment,\nit supports the creation of tasks that target specific\nagent capabilities. In our work, we focus on the\nSkill Acquisition Tasks subset, which evaluates\nthe agent’s ability to leverage the rich diversity of\nNetHack’s objects, monsters, and dungeon features,\nas well as their interactions. These tasks introduce\ncomplex world dynamics, where actions are fac-\ntorized autoregressively and require executing a\nsequence of follow-up actions for the initial action\nto produce the desired effect.\nWe use the Lava Crossing, Wand of Death, and\nQuest tasks as testbeds to evaluate DiVE. We pro-\nvide detailed descriptions of these task in Ap-\npendix A.2. The agent is rewarded for unlocking\nachievements within each task. To facilitate in-\nteraction with LLMs, we use the NLE language\nwrapper (Goodger et al., 2023) to verbalize both\nobservations and actions from the environment.\nWe compare DiVE against SSO (Nottingham\net al., 2024) and Reflexion (Shinn et al., 2024), both\nof which require prior training on the tasks. Follow-\ning the official guidelines, we run each method for\n30 iterations, evaluating performance after every\n10 iterations by attempting the task 10 times using\na fixed set of skills or reflections. The final perfor-\nmance is determined based on the results from the\nlast 10 evaluation attempts.\nMethod\nScore\nReward\nHuman Experts\n50.5 ± 6.8%\n14.3 ± 2.3\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nSPRING∗\n8.2 ± 2.3%\n6.9 ± 1.8\nCoT\n1.3 ± 0.3%\n2.5 ± 0.5\nAD\n21.79 ± 1.4%\n12.6 ± 0.3\nELLM\nN\/A\n6.0 ± 0.4\nDreamerV3\n14.5 ± 1.6%\n11.7 ± 1.9\nPPO\n4.6 ± 0.3%\n4.2 ± 1.2\nRandom\n1.6 ± 0.0%\n2.1 ± 1.3\nTable 1: Performance comparison of DiVE against base-\nline models in the Crafter environment. Methods with ∗\nindicate that they were obtained using the official code\nimplementation, executed with the same five random\nseeds and model configurations as used for DiVE.\n5\nExperimental Results\nWe evaluate the performance of DiVE in the Crafter\nand MiniHack environments. In Section 5.1, we\npresent the overall results to demonstrate DiVE’s\neffectiveness in bridging the knowledge gap in\nthese tasks. Section 5.2 provides a detailed analy-\nsis of the contributions of individual components\nthrough controlled experiments.\nThen, in Sec-\ntion 5.3, we evaluate the effectiveness of the dy-\nnamics learned by DiVE in the Crafter environ-\nment. Finally, in Section 5.4, we further analyze\nthe learned dynamics in both the Crafter environ-\nment and a modified MiniHack setting with altered\ndynamics.\n5.1\nDiVE’s Performance\nTable 1 and 2 demonstrate that DiVE surpasses\nall other baselines in the Crafter and MiniHack\nenvironments. In the Crafter environment, DiVE\nexceeds the previous state-of-the-art (SOTA) LLM-\nbased method, SPRING, by a substantial margin,\nachieving a 337.8% relative improvement in score\nand a 110.1% enhancement in reward. Addition-\nally, DiVE also surpasses the prior SOTA RL-based\napproach, DreamerV3, with a 21.4% absolute im-\nprovement in score and a 2.8 absolute increase in\nreward. Notably, DiVE achieves rewards compara-\nble to human players using 10 demonstrations.\nIn the MiniHack environment, with only a single\ndemonstration, DiVE matches the performance of\nSSO and Reflexion (both of which require 30 iter-\nations of training) on the Lava Crossing task, and\noutperforms both baselines on the Wand of Death\nand Quest tasks. Specifically, DiVE achieves a 68%\nimprovement over Reflexion on the Wand of Death\ntask and a 30% and 36% improvement over SSO\nand Reflexion, respectively, on the Quest task.\nMethod\nLava Crossing\nWand of Death\nQuest\nReflexion\n0.8±0.0\n0.32±0.15\n0.87±0.43\nSSO∗\n0.8±0.0\n0.52±0.31\n0.91±0.29\nDiVE\n0.8±0.0\n0.54±0.34\n1.18±0.38\nTable 2: Comparison of DiVE performance against\nbaseline models in the MiniHack environment. Results\nmarked with ∗were obtained using the official code\nimplementation and evaluated over 10 runs.\nMethods\nScore\nReward\nComponent analysis\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nw\/o E\n21.1 ± 9.7%\n11.3 ± 4.3\nw\/o V\n9.8 ± 1.0%\n10.1 ± 0.7\nw\/o V&E\n11.5 ± 4.9%\n8.3 ± 3.8\nw\/o D&V&E\n0.9 ± 0.1%\n2.5 ± 1.3\nCoT\n1.3 ± 0.3%\n2.5 ± 0.5\nCoT + D&V\n3.6 ± 0.9%\n3.9 ± 2.3\nDynamics from distinct sources\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nw\/o D&V+S†\n15.7 ± 5.3%\n8.9 ± 5.1\nw\/o D&V&E+S†\n12.1 ± 4.6%\n8.7 ± 3.0\nw\/o D&V&E+H†\n34.2 ± 2.8%\n14.5 ± 0.9\nTable 3: Impact of different components on perfor-\nmance: Crafter. D, V, and E represent Discover, Verifier,\nand Evolver, respectively; S† refers to dynamics derived\nfrom the game manual, H† refers to human-annotated\ndynamics.\n5.2\nContribution of Individual Components\nWe conduct a series of ablation studies to clar-\nify the contribution of each individual element to\nDiVE’s overall performance. We report the results\non Crafter in the first section of Table 3, and the\nresults on MiniHack in Table 4.\nCrafter\nThe significant performance gap be-\ntween DiVE and its variant without the Evolver\ncomponent empirically demonstrates the Evolver’s\neffectiveness in developing gameplay strategies\nbased on world dynamics W, thereby enhancing\nthe agent’s overall proficiency in this environment.\nSimilarly, the performance decline observed in the\nvariant without the Verifier underscores the impor-\ntance of formulating strategies I based on accurate\nworld dynamics W. Moreover, the further perfor-\nmance drop in the version lacking both the Verifier\nand Evolver components highlights their comple-\nmentary roles—the Verifier ensures precision in\ncapturing dynamics, while the Evolver focuses on\nstrategy development.\nThe performance of DiVE without the Discov-\nerer, Verifier, and Evolver components reverts to\nthe CoT baseline, indicating that simply decom-\nposing the task according to the hierarchy H with-\nout integrating domain knowledge Ktarget provides\nComponents\nLava Crossing\nWand of Death\nQuest\nDiVE\n0.80±0.00\n0.54±0.34\n1.18±0.38\nw\/o E\n0.72±0.24\n0.52±0.30\n1.16±0.48\nw\/o V\n0.69±0.25\n0.52±0.38\n1.18±0.38\nw\/o V&E\n0.72±0.24\n0.40±0.40\n1.11±0.48\nw\/o D&V&E\n0.72±0.17\n0.20±0.00\n0.41±0.29\nTable 4: Impact of different components on perfor-\nmance: MiniHack. D, V, and E represent Discover,\nVerifier, and Evolver, respectively.\nno performance benefit. The substantial gap be-\ntween CoT + D&V and DiVE w\/o E further demon-\nstrates that an LLM-based agent struggles with\nlong-horizon planning tasks in the absence of task\ndecomposition, underscoring the importance of the\ndecomposition hierarchy H.\nMiniHack\nSince MiniHack is built on the popu-\nlar video game NetHack (Goodger et al., 2023),\nLLMs already possess a certain level of under-\nstanding of the environment. For example, DiVE\nwithout the Discover, Verifier, and Evolver com-\nponents (CoT baseline) can successfully solve the\nLava Crossing task. However, as the complexity\nof the tasks increases, using LLMs alone is insuf-\nficient for solving more challenging scenarios like\nthe Wand of Death and Quest tasks, highlighting\na potential knowledge gap for these more intricate\nproblems.\nThe primary performance gains of DiVE in the\nMiniHack environment can be attributed to the\nDiscover component, which accurately identifies\nthe underlying environment dynamics required for\neach task. Due to the embedded knowledge of\nLLMs about the MiniHack environment, they can\nidentify these dynamics effectively, as shown by\nthe minimal performance drop when DiVE is used\nwithout the Verifier. However, in the modified Mini-\nHack setting described in Section 5.4, we highlight\nthe critical role of the Verifier in maintaining relia-\nbility. Additionally, since the tasks have a relatively\nshort horizon, the dynamics uncovered by the Dis-\ncover component are sufficient to complete them\nsuccessfully.\n5.3\nEvaluation of Learned Dynamics\nWe investigate the performance of DiVE leveraging\nworld dynamics derived from different sources in\nCrafter. As shown in the second section of Table 3,\nDiVE significantly outperforms variants that uti-\nlize the dynamics S† from the game manual (Wu\net al., 2024b). This performance improvement in-\ndicates that the learned dynamics W are more ad-\nFigure 3: Recall of learned dynamics over discovery\nsteps, presented with mean and standard deviation, in\nthe Crafter environment.\nvantageous than S†, likely because S† lacks certain\nbeneficial details that are captured in W. The per-\nformance gap between methods using S† with and\nwithout the Evolver further highlights the impor-\ntance of strategy evolution, whose effectiveness is\nclosely tied to the quality of the underlying world\ndynamics.\nIn addition to dynamics learned from human\ndemonstrations and game manual, we have ex-\nplored a third source: human-annotated dynamics.\nThe results show that DiVE performs comparably\nto the variants using human-annotated dynamics,\ndemonstrating the robustness and effectiveness of\nDiVE’s approach to dynamic learning.\nAs previously mentioned, it is difficult to quan-\ntify the desired properties because we cannot pre-\ncisely measure the domain-relevant information\nKrelevant in LLMs or the exact amount of domain\nknowledge Ktarget required. However, by using\nhuman-annotated dynamics H† as a reference\nbenchmark for Ktarget, we can estimate the preci-\nsion and recall of the learned dynamics W, en-\nabling us to effectively assess the progress of LLM-\nbased dynamic learning. Specifically, we define re-\ncall as R = |W∩H†|\n|H†|\nand precision as P = |W∩H†|\n|W|\n.\nAs illustrated in Figure 3, both the discovered\ndynamics ˜\nW and the verified dynamics W exhibit\nan increase in recall as the number of discovery\nsteps progresses, indicating that the richness of the\nlearned dynamics improves over time. Moreover,\nthe narrowing gap in recall between ˜\nW and W sug-\ngests that the Verifier effectively filters out ’noisy’\ndynamic candidates while preserving those that\ngeneralize across different trajectory segments.\nTo evaluate whether the Verifier preserves cor-\nrect world dynamic candidates ˜\nW+ while filtering\nout unreliable ones ˜\nW−, we analyze the precision\nCorrectness\nOutcome\nCorrect (✓)\nnone\nConfounder (✗)\n1 health\nIn-domain Hallucination (✗)\n1 wood\nOut-domain Hallucination (✗)\n1 bone\nTable 5: The dynamics underlying the outcome of de-\nfeating zombie in the Crafter environment.\nof both the discovered dynamics ˜\nW and the verified\ndynamics W. As shown in Figure 4, the precision\nof the verified dynamics consistently and signif-\nicantly exceeds that of the discovered dynamics,\ndemonstrating the Verifier’s effectiveness in identi-\nfying and eliminating inaccurate candidates. This\nconfirms the Verifier’s role in enhancing the relia-\nbility of the dynamics used for decision-making.\n5.4\nAnalysis of Learned Dynamics\nThe correctness of the learned and verified dynam-\nics is classified as either correct or erroneous, with\nerrors stemming from confounders, in-domain hal-\nlucinations, or out-of-domain hallucinations. As\nshown in Table 5, an example of a confounder-\nrelated mistake occurs in Crafter when a simulta-\nneous increase in health points is incorrectly at-\ntributed to the act of defeating a zombie. In this\nscenario, the Discoverer misclassifies the health\nincrease as a direct result of defeating the zombie.\nIn the case of in-domain hallucinations, the Dis-\ncoverer incorrectly associates an increase in wood\nwith defeating the zombie, even though it is not\npossible for wood to increase during this event,\ndespite its presence in the observation. Lastly, out-\nof-domain hallucinations involve the discovery of\ndynamics that reference nonexistent objects in the\nobservation or elements not present in the Crafter\nenvironment.\nCompared to the dynamics from the game man-\nual, as shown in Table 6, we found that DiVE’s\ndynamics are not only more precise but also more\ndetailed. For instance, while SPRING only iden-\ntified that placing a stone requires stones, DiVE\ndetermined that it specifically requires exactly one\nstone and the precise facing condition needed for\nsuccessful placement. Moreover, using this infor-\nmation, the Evolver can infer advanced dynamics\nfor placing a stone, such as its potential to serve as a\nbarrier between the agent and dangerous creatures.\nIn a customized MiniHack Lava Crossing setting,\nwe modified the task such that it requires the agent\nSources\nDynamics\nManual\nPlace stone requires stones\nDiscoverer Place stone requires 1 stone and\nfaces paths, grass, sand, water,\nand lava\nEvolver\nPlace stone to block zombies\nand skeletons, preventing them\nfrom reaching the player\nTable 6: Comparing SPRING and DiVE on place stone’s\ndynamics in the Crafter environment.\nto use the Wand of Death to freeze the lava for\ncrossing, even though the wand’s original purpose\nis to zap monsters. Based on the demonstration,\nthe Discoverer mistakenly identifies the precon-\ndition for using the wand as The player must\nbe adjacent to a wall or obstacle that\ncan be destroyed or altered by the wand\nof death, with the outcome being The wand of\ndeath was used, altering the environment\nby removing walls and revealing a dark\narea to the east, southeast, and south.\nThis unreliable dynamic is successfully identified\nand filtered out by the Verifier, highlighting its cru-\ncial role in maintaining the reliability of learned\ndynamics.\n6\nRelated Work\nLanguage Models\nLanguage models (LLMs) are\ntrained autoregressively in a left-to-right sequence,\npredicting each token based on its preceding con-\ntext from an internet-scale corpus. Through this\ntraining, LLMs develop a comprehensive under-\nstanding of both language and the world it repre-\nsents (Achiam et al., 2023; Touvron et al., 2023;\nDubey et al., 2024; The, 2024), enabling them\nto perform competently across a wide range of\ntasks (Yao et al., 2024; Shinn et al., 2024).\nEmbodied Agent\nBuilding an embodied agent\nusing LLMs is challenging because LLMs lack em-\nbodied experience (Valmeekam et al., 2022; Wang\net al., 2024) in downstream environments (Weir\net al., 2022; Shridhar et al., 2020; Côté et al.,\n2019). However, LLMs can still provide founda-\ntional world knowledge that serves as a prior for the\nagent (Shi et al., 2024; Colas et al., 2023; Zhong\net al., 2024; Fu et al., 2024a). A notable distinction\nof our work is that we do not assume LLMs have\nthe necessary knowledge to solve tasks in specific\ndomains.\nFigure 4: Precision of learned dynamics before and after\nverification in the Crafter environment.\nDiscover Dynamics\nLLMs can discover knowl-\nedge by inducing and deducing rules for reasoning\ntasks (Zhu et al., 2023) and by extracting under-\nlying domain knowledge from prior trajectories\nand interactive experiences (Colas et al., 2023;\nMajumder et al., 2023; Fu et al., 2024b; Zhao\net al., 2024). However, the knowledge obtained\nthrough these methods is often unstructured, not\nsuited for addressing long-horizon planning prob-\nlems, and lacks verification for reliability, as it\noverlooks LLMs’ tendency to hallucinate (Zhang\net al., 2023b).\nEvolve\nDynamics\nLLMs\ncan\nrefine\ntheir\ndecision-making process by reflecting on past\ntrajectories (Shinn et al., 2024).\nLeveraging\nthis ability, studies such as (Wang et al., 2023;\nStengel-Eskin et al., 2024; Zhang et al., 2023a;\nNottingham et al., 2024) focus on evolving\nnew and advanced skills from pre-defined ones.\nHowever, these approaches often assume that\nLLMs already possess comprehensive domain\nknowledge and can derive new dynamics based\nsolely on their understanding.\n7\nConclusion\nWe introduce DiVE, a framework that bridges the\nknowledge gap between LLMs and downstream\ndomains. The Discoverer extracts world dynamics,\nwhile the Verifier filters unreliable candidates. In\nan online setting, the Evolver reasons strategies\nthrough interaction. Our experiments demonstrate\nthat DiVE effectively bridges the knowledge gap\nbetween LLMs and downstream domains.\nLimitations\nEvaluating DiVE in embodied environments may\nnot fully capture the diversity and complexity of\nreal-world dynamics that an agent might encounter\nin practice. If the environment’s dynamics change\nacross episodes, the offline-learned dynamics from\nDiVE may struggle to adapt, introducing potential\nbiases in the agent’s understanding and hindering\nthe development of advanced strategies. Moreover,\nacquiring these dynamics from human demonstra-\ntions can be difficult or impractical in certain sce-\nnarios.\nEthical Concerns\nWe do not anticipate any immediate ethical or so-\ncietal impact from our work. This study aims to\nbridge the knowledge gap between LLMs and the\ntarget domain. However, despite our efforts, DiVE\nmay still exhibit hallucinations due to the inherent\ntendency of LLMs to hallucinate.\nAcknowledgements\nThis work is supported by the Canada CIFAR AI\nChair Program and the Canada NSERC Discovery\nGrant (RGPIN-2021-03115).","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Enhancing Agent Learning through World Dynamics Modeling.pdf","query_prompt":"### 任务\n你是一个人工智能领域的专家，你能够快速阅读arxiv上的各种AI前沿论文，并给出非常好的论文总结。\n现在请你阅读以下论文，并给出你对这篇论文的详细介绍，从而放到你的博客上，让更多人了解这篇论文。\n\n### 论文信息\n###  1. 论文标题\n```\nEnhancing Agent Learning through World Dynamics Modeling\n```\n#### 2. 论文摘要\n```\nLarge language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.\n```\n\n#### 3. 论文全文\n```\nEnhancing Agent Learning through World Dynamics Modeling\nZhiyuan Sun1* †, Haochen Shi1*\nMarc-Alexandre Côté2, Glen Berseth1,3, Xingdi Yuan2‡, Bang Liu1,3‡\n1 Université de Montréal & Mila, Montréal, Canada\n2 Microsoft Research, Montréal, Canada\n3 Canada CIFAR AI Chair\nAbstract\nLarge language models (LLMs) have been in-\ncreasingly applied to tasks in language under-\nstanding and interactive decision-making, with\ntheir impressive performance largely attributed\nto the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of\nthis knowledge can vary across domains. Many\nexisting approaches assume that LLMs pos-\nsess a comprehensive understanding of their\nenvironment, often overlooking potential gaps\nin their grasp of actual world dynamics. To\naddress this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discov-\ners world dynamics from a small number of\ndemonstrations, verifies the accuracy of these\ndynamics, and evolves new, advanced dynam-\nics tailored to the current situation. Through\nextensive evaluations, we assess the impact of\neach component on performance and compare\nthe dynamics generated by DiVE to human-\nannotated dynamics. Our results show that\nLLMs guided by DiVE make more informed\ndecisions, achieving rewards comparable to hu-\nman players in the Crafter environment and sur-\npassing methods that require prior task-specific\ntraining in the MiniHack environment1.\n1\nIntroduction\nBy absorbing internet-scale knowledge autoregres-\nsively, large language models (LLMs) develop a\nbroad understanding of the world (Achiam et al.,\n2023; Team et al., 2023; Brown et al., 2020; Tou-\nvron et al., 2023). This understanding enables them\nto perform well across a variety of tasks (Brown\net al., 2020; Huang et al., 2022; Yao et al., 2022,\n2024). However, achieving this level of compre-\nhension requires a training dataset that is diverse,\n* Equal contribution.\n† zhiyuan.sun@umontreal.ca\n‡ Equal advising.\n1 The\ncode\nis\navailable\nat\nhttps:\/\/github.com\/\nZhiyuuanS\/DiVE\n: LLMs’ Knowledge\n: Domain Knowledge\n: Overlapping Knowledge\nFigure 1: The knowledge gap between LLMs and down-\nstream domains. Although LLMs have a broad under-\nstanding of the world, they may struggle to grasp the\ncomplex dynamics of specific downstream domains.\nprecise, and in-depth to cover essential domain\ninformation. Otherwise, a knowledge gap may\nemerge between LLMs and the target domain, as\nillustrated in Figure 1.\nSince LLMs tend to follow the most common\npatterns in the dataset (Gunasekar et al., 2023) and\nare prone to hallucination, it is crucial that the nec-\nessary information not only appears frequently on\nthe internet but is also reliable (Dziri et al., 2022).\nFor tasks and environments that are newly emerg-\ning (Hafner, 2021; Samvelyan et al., 2021), these\nrequirements are often difficult to meet due to the\nnoisy nature of internet data, leading to potential\nknowledge gaps.\nWhile LLMs may have a general understanding\nof a domain, optimal decision-making requires in-\ndepth, state-specific knowledge. Providing LLMs\nwith such tailored information enhances their grasp\nof both the environment and the current state. For\nexample, mastering Go involves not just knowing\nthe rules but also applying strategies for specific\nboard states. This specialized knowledge varies\nacross domains and states, making offline collec-\ntion impractical.\nTo address these challenges,\nwe propose\nDiscover, Verify, and Evolve (DiVE). Building on\narXiv:2407.17695v2  [cs.AI]  15 Oct 2024\nthe concept of the World Model (Ha and Schmid-\nhuber, 2018), DiVE not only discovers and veri-\nfies primitive world dynamics from demonstrations\nbut also evolves state-specific knowledge for the\ndownstream domain. By providing LLMs with this\ncomprehensive set of dynamics, DiVE bridges the\nknowledge gap between LLMs and the downstream\ndomain, thereby enhancing their decision-making\nabilities.\nDiVE consists of three distinct components: The\nDiscoverer: this component iteratively uncovers\nthe environment’s dynamics from demonstrations\nusing a curriculum learning approach. The Veri-\nfier: this component eliminates unreliable dynam-\nics caused by LLMs’ tendency to hallucinate. The\nEvolver: this component reasons through in-depth,\nstate-specific strategies tailored to the current situa-\ntion based on the learned dynamics.\nIn the Crafter environment (Hafner, 2021) and\nthe MiniHack environment (Samvelyan et al.,\n2021), DiVE learns comprehensive and reliable\ndynamics from demonstrations, guiding the agent’s\ndecision-making process by evolving in-depth\nstrategies. This enables the agent to outperform all\nbaselines, achieving rewards comparable to human\nplayers in the Crafter environment and surpassing\nthe performance of methods requiring task-specific\ntraining in the MiniHack environment. To gain\ndeeper insights into DiVE’s behavior, we provide\nboth quantitative and qualitative analyses.\nIn summary, our primary contribution is a frame-\nwork that learns world dynamics from demonstra-\ntions, guiding LLMs in the decision-making pro-\ncess by online evolving contextual strategies. This\napproach bridges potential knowledge gaps, result-\ning in a more optimal decision-making process for\nLLMs.\n2\nThe Knowledge Gap\nIn this work, we consider a setting where a pre-\ntrained LLM-based agent is employed to solve\ntasks in downstream domains. Conceptually, we\ndefine KLLM as the set of knowledge embedded\nin the LLMs through their training process, and\nKtarget as the universal set of knowledge relevant to\nthe downstream domains.\nTo ensure the effectiveness of the LLMs, we aim\nfor Krelevant, the subset of KLLM that is relevant to\nKtarget, to cover the broadest possible scope. Ad-\nditionally, we seek for Krelevant to contain more\nreliable knowledge K+ than inaccurate knowledge\nK−, where Krelevant = K+ ∪K−. Thus, we de-\nfine three desirable properties to guide our system\ndesign:\n• Recall R =\n|K+|\n|Ktarget| measures the extent to which\nthe knowledge required to solve tasks in the tar-\nget domain is covered by the LLMs. A low recall\ntypically indicates a significant knowledge gap\nbetween the LLMs’ training data and the target\ndomain.\n• Precision P =\n|K+|\n|Krelevant| measures the accuracy of\nthe LLMs’ knowledge when applied to the target\ndomain. An example of inaccurate dynamics is\nshown in Figure 2 (left), where the LLMs hallu-\ncinate that defeating a skeleton will drop items,\nwhich is not true in Crafter. This may be due to\nthe LLMs memorizing Minecraft-specific data.\n• Depth D measures the abstraction levels of\nknowledge representations. Knowledge can be\nrepresented at varying levels of abstraction, from\nbasic game rules to higher-level strategies.\nWe recognize that precisely quantifying the\nknowledge overlap between LLMs and a down-\nstream domain is challenging. In Section 3, we pro-\nvide a mathematical analysis showing how DiVE\nimproves across all three dimensions.\nIn Sec-\ntion 5.4, we measure the knowledge overlap to\ndemonstrate that DiVE effectively learns useful\ngame dynamics and, to some extent, bridges the\nknowledge gap.\n3\nDiVE: Discover, Verify, and Evolve\nIn an ideal scenario, one could bridge the knowl-\nedge gap by fine-tuning the LLM to adapt to the\ntarget domain. However, this approach is often less\npractical due to its reliance on large amounts of\nannotated data and significant computational over-\nhead (Hu et al., 2021; Zheng et al., 2024; Carta\net al., 2023; Ouyang et al., 2022). Our frame-\nwork, DiVE, is designed to address the knowledge\ngap while considering all three desirable proper-\nties—recall, precision, and depth—without requir-\ning extensive data collection from the target do-\nmain. It is a prompt-based method that learns world\ndynamics W directly from the environment.\nAs shown in Figure 2, DiVE is initially boot-\nstrapped using a small set of human demonstra-\ntion trajectories H, each consisting of observations\not, actions at, and rewards rt at each timestep\nt.\nWe then transform each observation ot into\nIt is daytime\\nState description: \\n-\nEast: immediate (grass); nearby \n(unexplored_area); \\n- North: \nimmediate (grass); nearby \n(unexplored_area); \\n- North East: \nimmediate (grass); nearby (cow, \nunexplored_area); \\n- North West: \nimmediate (grass); nearby \n(unexplored_area, tree); \\n- South: \nimmediate (grass); nearby \n(unexplored_area); cow: North East 5 \nblocks away (nearby) (objects in \nbetween: {'grass'}) \\n- Facing grass on \nthe south.\\nYour status:\\n- health: \n9\/9\\n- food: 9\/9\\n- drink: 9\/9\\n- energy: \n9\/9\\nYou have nothing in your \ninventory.\nIt is daytime\\nState description: \\n-\nEast: immediate (grass); nearby \n(unexplored_area); \\n- North: \nimmediate (grass); nearby \n(unexplored_area); \\n- North East: \nimmediate (grass); nearby (cow, \nunexplored_area); \\n- North West: \nimmediate (grass); nearby \n(unexplored_area, tree); \\n- South: \nimmediate (grass); nearby \n(unexplored_area); cow: North East 5 \nblocks away (nearby) (objects in \nbetween: {'grass'}) \\n- Facing grass on \nthe south.\\nYour status:\\n- health: \n9\/9\\n- food: 9\/9\\n- drink: 9\/9\\n- energy: \n9\/9\\nYou have nothing in your \ninventory.\nIt is nighttime\\nState description: \\n-\nEast: immediate (sand); nearby (path, \ngrass); distant (tree, unexplored_area);\n\\n- North: immediate (zombie); nearby \n(path, sand, grass); distant (tree, \nunexplored_area); \\n…… - zombie: \nNorth 1 blocks away (immediate) \n(objects in between: None) \\n- table: \nSouth East 9 blocks away (nearby) \n(objects in between: {'path', 'sand', \n'water', 'grass'}) \\n- Facing zombie on \nthe north.\\nYour status:\\n- health: \n2\/9\\n- food: 8\/9\\n- drink: 8\/9\\n- energy: \n3\/9\\nYour inventory:\\n- wood: 1\\n-\nstone: 9\\n- coal: 4\\n- wood_pickaxe: \n1\\n- stone_pickaxe: 1\\n- wood_sword: \n1\\n- stone_sword: 1\\n\nIt is nighttime\\nState description: \\n-\nEast: immediate (sand); nearby (path, \ngrass); distant (tree, unexplored_area);\n\\n- North: immediate (zombie); nearby \n(path, sand, grass); distant (tree, \nunexplored_area); \\n …… - zombie: \nNorth 1 blocks away (immediate) \n(objects in between: None) \\n- table: \nSouth East 9 blocks away (nearby) \n(objects in between: {'path', 'sand', \n'water', 'grass'}) \\n- Facing zombie on \nthe north.\\nYour status:\\n- health: \n2\/9\\n- food: 8\/9\\n- drink: 8\/9\\n- energy: \n3\/9\\nYour inventory:\\n- wood: 1\\n-\nstone: 9\\n- coal: 4\\n- wood_pickaxe: \n1\\n- stone_pickaxe: 1\\n- wood_sword: \n1\\n- stone_sword: 1\\n\nVerbalize\nDiscover\nVerify\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\nskeleton turn into path after defeat_skeleton, \nincrease bone by 1 and arrow by 1.\n……\nzombie turn into grass after defeat_skeleton, \nincrease rotten_fresh by 1 and bone by 1.\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\nskeleton turn into path after defeat_skeleton, \nincrease bone by 1 and arrow by 1.\n……\nzombie turn into grass after defeat_skeleton, \nincrease rotten_fresh by 1 and bone by 1.\n(verbalized\ndemonstration)\n(discovered game dynamics)\n(verified game dynamics)\n(demonstration)\n𝑜𝑡\n𝑜𝑡+1\n𝑎𝑡\nCrafter Env\nLLM Agent\n+\nEvolve\nUse collected stones to build a \nprotective barrier around the safe \nlocation.\n……\n(situational strategies)\nstone can be found near iron, coal, but it is \nnot associated with cow, zombie, skeleton. \nyou cannot walk directly through stone.\nstone turn into path after collect_stone.\nyou cannot walk directly through zombie.\n……\n(verified game dynamics)\nFigure 2: Overall pipeline of DiVE. Left: Learning basic game dynamics from offline demonstrations (Section 3.1).\nWe want to highlight the incorrect game dynamics being identified by the Verifier (labeled by ×), they are evidence\nof the LLMs hallucinate false facts perhaps because of memorizing Minecraft data. Right: Learning situational\nstrategies from online interactions (Section 3.2). For simplicity, we omit the verbalization process in the right figure.\nthe language space as ˜ot using a Verbalizer, re-\nsulting in trajectories represented by ˜ot, at, and\nrt at each timestep t. Subsequently, the Discov-\nerer extracts a set of world dynamic candidates,\n˜\nW = { ˜W+, ˜W−}, from human demonstrations H,\nwhere ˜W+ and ˜W−represent the correct and inac-\ncurate world dynamic sets, respectively.\nEmpirically, we find that the inclusion of ˜W−in\n˜\nW is often unavoidable, either due to LLM’s diffi-\nculties in extracting meaningful knowledge from\ntrajectory data or its tendency to hallucinate. To\naddress this, we employ the Verifier to filter out\npotentially invalid and conflicting world dynamic\ncandidates from ˜\nW, leaving only the valid dynam-\nics W. Lastly, we use the Evolver, designed to\nderive advanced dynamics I tailored to the ver-\nbalized observation ˜ot based on the filtered world\ndynamics W.\nThe final decision-making process on primitive\nactions at ∈A is hierarchically decomposed as\nplanning tasks on sub-goals SG, sub-tasks ST,\nand actions A. The planning procedure is further\nguided by both W and I. In cases where W ̸=\n∅, Recall, Precision and Depth are guaranteed to\nincrease as formulated below:\nRecall :\n|K+|\n|Ktarget|\nDiscoverer\n======⇒\nH\n|K+| + |W|\n|Ktarget|\nPrecision :\n|K+|\n|Krelevant|\nVerifier\n====⇒\n˜\nW,H\n|K+| + |W|\n|Krelevant| + |W|\nDepth : ∅Discoverer\n======⇒\nVerifier\nW Evolver\n====⇒\nW,H\nI ∪W\nThe DiVE framework can be divided into two\nstages: an offline dynamics learning stage and an\nonline strategy learning stage.\n3.1\nOffline Dynamics Learning\nThe offline dynamics learning procedure aims to\nbridge the knowledge gap between LLMs’ under-\nstanding and the basic dynamics of downstream\ndomains by learning the world dynamics W as a\nprior for the decision-making process. Instead of\nrelying on human-authored game manuals or hand-\nbooks to extract world dynamics, as in (Wu et al.,\n2024b,a), which are not only difficult to obtain in\nmany real-world scenarios but also often lack criti-\ncal details (as demonstrated in Table 6), we propose\nlearning world dynamics W directly from experi-\nences H, which are more accessible and provide\nricher information.\nHierarchical Curriculum Learning\nGiven the\nvarying complexities of learning the dynamics of\ndifferent elements in downstream domains, we\nadopt a curriculum learning approach (Bengio et al.,\n2009). Our method follows a sequential learning\nstrategy that progresses from simpler to more com-\nplex dynamics, thereby enabling more effective\nlearning. Specifically, we propose a method for\nlearning the dynamics of each element within the\ntask decomposition hierarchy, denoted as TD =\n{A ∪O, ST, SG}, where O represents the set of\nobjects in downstream domains.\nOur approach begins with elements of lower\nabstraction, such as actions a ∈A and objects\no ∈O, and gradually progresses to higher-level\nelements, such as sub-tasks sti ∈ST. The sub-\ntasks ST are represented as nodes in the achieve-\nment graph G = (V, E) within the downstream\ndomains, i.e., ST = V . Finally, we transition to\nthe subgoals sgi ∈SG. The subgoal sequence\nSG = [sg1, sg2, . . .] is an ordered list used to un-\nlock achievements in the achievement graph G,\nwhere SG = TopologicalSort(G), and each sgi\ncorresponds to a vertex in V .\nWe leverage the Discoverer to extract this order\nfrom human demonstrations H. Achieving a sub-\ngoal sgi may involve completing several sub-tasks\nmultiple times. This approach ensures a logical pro-\ngression through tasks, thereby enabling a deeper\nunderstanding and integration of downstream do-\nmain dynamics.\nDiscoverer\nThe Discoverer is designed to iden-\ntify dynamic candidates ˜\nW related to elements\nwithin the task decomposition hierarchy TD. A\nsingle dynamics discovery step for an element\nE ∈TD involves the following three main steps:\n1. Construction of the Semantic Experience\nBank: For each element E, we construct a se-\nmantic experience bank BE using demonstra-\ntions H. This bank stores experiences that are\ntransformed from H into a suitable granular-\nity for analyzing dynamics related to E. The\ntransformation process involves chunking and\nsummarizing the verbalized demonstrations to\ncapture essential semantic details.\n2. Sampling of Relevant Experiences: For each\nattribute of an instance e ∈E, a subset of expe-\nriences BE\ne that are relevant to the instance e is\nsampled from BE.\n3. Identification of Dynamic Candidates: A dy-\nnamic candidate ˜w is identified from the subset\nBE\ne by recognizing patterns that are consistent\nacross all experiences within BE\ne .\nThe action-level semantic experience bank, de-\nnoted as BA, stores transition tuples derived from\nverbal demonstrations and is represented as: BA =\n{{˜ot, at, ˜ot+1}i}|BA|\ni=1 . Similarly, the object-level\nsemantic experience bank, denoted as BO, gath-\ners individual observations contain an specific\nobject and is represented as: BO = {˜oi}|BO|\ni=1 .\nThe sub-task-level semantic experience bank, de-\nnoted as BST , aggregates trajectory segments rep-\nresenting the completion of sub-tasks and is for-\nmatted as: BST = {{˜ot, . . . , atst, ˜otst+1}i}|BST |\ni=1\n,\nwhere tst denotes the timestep at which a sub-\ntask st ∈ST is completed. For sub-goals, the\nsub-goal-level experience bank, denoted as BSG,\nrecords sequences of sub-tasks that culminate in\nthe completion of sub-goals, expressed as: BSG =\n{{stt, . . . , sttsg}i}|BSG|\ni=1\n, where tsg is the timestep\nat which the sub-goal sg is achieved.\nFor action-level dynamics learning, the relevant\nexperiences, denoted as BA\na , are compiled by ran-\ndomly sampling transition tuples from BA where\nthe action a has been successfully executed. A sim-\nilar approach is employed for dynamics learning\nof other elements within the task decomposition\nhierarchy TD.\nFor action-level dynamics discovery, we iden-\ntify the prerequisites and outcomes of each action\n(e.g., The action MakeWoodPickaxe requires\n1 wood). For object-level dynamics, we focus on\nco-occurrence relationships between objects and\ntheir temporal patterns. The attribute set for a sub-\ntask generally encompasses the necessary steps for\ncompletion, as well as its prerequisites, outcomes,\nand termination conditions. In contrast, the primary\nattribute of interest for a sub-goal is its correct po-\nsition within the sub-goal sequence SG.\nVerifier\nDynamic discovery processes are vul-\nnerable to noise from various sources, including\nconfounders, hallucinations by the LLMs, and dif-\nficulties in the LLMs’ ability to extract meaning-\nful insights from trajectory data. To address these\nchallenges, we introduce a dynamic verifier de-\nsigned to filter out noisy dynamic candidates from\n˜\nW. For each dynamic candidate w ∈˜\nW, the ver-\nifier begins by sampling a subset of relevant se-\nmantic experiences, denoted as BE\ne , from the cor-\nresponding semantic experience bank BE. Here,\nw represents a dynamic candidate associated with\na specific attribute of the instance e ∈E, where\nE ∈{A ∪O, ST, SG} corresponds to an element\nof the task decomposition hierarchy TD. The veri-\nfication of w is conducted as follows: w is deemed\ninaccurate and filtered out if it does not consistently\nhold across experiences within BE\ne or if it conflicts\nwith any established dynamics. The dynamics that\npass this verification process are classified as veri-\nfied dynamics and are denoted as W.\n3.2\nOnline Strategy Learning\nTo effectively incorporate the learned world dynam-\nics W into the downstream domains, we deploy an\nLLM-based agent defined by π : S × W →P(A).\nHere, S represents the state space, A denotes the\naction space, and P symbolizes the probability dis-\ntribution over the action space. Instead of directly\nmapping the world dynamics W and the current\nstate observation ot to the action at, we tackle the\nchallenge of long-horizon planning by integrating\nan online strategy learning method. This approach\ndecomposes the planning process into three distinct\nstages: sub-goal planning, sub-task planning, and\naction planning.\nSub-goal Planning\nGiven that the sub-goal se-\nquence SG = [sg1, sg2, . . .] is derived from hu-\nman demonstrations H and treated as a fixed se-\nquence, we utilize a straightforward heuristic for\nsub-goal planning. When a sub-goal is completed,\nthe current sub-goal is updated to the first uncom-\npleted sub-goal in SG.\nSub-task Planning\nFor a given current sub-goal\nsgi, we have developed an LLM-based sub-task\nplanner. This planner evaluates and ranks all sub-\ntasks st ∈ST based on the learned world dynam-\nics W, the verbalized current observation ˜ot, and\nthe most recently executed sub-task stt−1. The\nhighest-ranked sub-task is then designated as the\ncurrent sub-task stt. To ensure accurate execution,\nthe completion of a sub-task st is contingent upon\nsatisfying its specific termination condition. This\ncondition is verified by querying an LLM using the\ncurrent verbalized observation, the observation at\nthe time the sub-task began, and the termination\nconditions of the current sub-task.\nLearning Strategies\nIn addition to learning the\nfundamental rules of the downstream domains, we\nalso focus on developing advanced game-playing\nstrategies based on these dynamics. Unlike world\ndynamics learning, the strategy space is often too\nexpansive for exhaustive exploration. To address\nthis challenge, we propose evolving the dynam-\nics into strategies, denoted as I, using an online\nlearning approach.\nThis method reduces the search space by condi-\ntioning not only on the dynamics W, but also on the\nverbalized current observation ˜ot and the sub-task\nsti. This targeted approach enables the generation\nof strategies that are more contextually grounded\nand responsive to current game scenarios compared\nto those developed through offline methods. To sup-\nport this process, we have designed an LLM-based\nEvolver that generates strategy candidates ˜I by ap-\nplying deductive reasoning to the learned dynamics\nW. Specifically, the Evolver derives strategy can-\ndidates using rules of inference, such as modus\nponens. These strategy candidates, denoted as ˜I,\nare then evaluated for validity and ranked based on\ntheir utility by an LLM-based critic. Finally, the\nvalid and useful candidates are incorporated into\nthe situational strategy set I.\nAction Planning\nThe final action selection pro-\ncess is executed in two main steps:\n1. Invalid Action Masking: This step involves fil-\ntering out actions that are infeasible under the\ncurrent situation, based on the verified dynamics\nW and current verbalized observation ˜ot.\n2. Action Selection: From the set of valid actions,\na specific primitive action a is chosen based on\nmultiple factors: the current sub-task sti, the\nverbalized current observation ˜ot, the world dy-\nnamics W, a windowed history of previously\nplanned actions and observations, and the de-\nrived strategies I.\n4\nExperiment Setup\nTo demonstrate the effectiveness of DiVE in bridg-\ning the knowledge gap, we evaluate its performance\nwithin the Crafter and MiniHack environments. For\na fair comparison, all LLM-based agents, includ-\ning ours, utilize the GPT-4o model and the same\nenvironment seed in the Crafter setting. Further\ndetails on the setups can be found in Appendix A.1\nand B.1.\n4.1\nCrafter\nCrafter (Hafner, 2021) is an open-world survival\ngame set on 64 × 64 grid-based maps, featuring a\ndiverse array of materials such as tree, stone, and\ncoal, as well as entities including cow, zombie,\nand skeleton semi-randomly spawn on the maps.\nThe games include an achievement graph with 22\nunique achievements across 7 levels. The agent\nperceives its surroundings through a local 7 × 9\nobservation window and maintains awareness of its\nstatus within the game environment.\nThe text description generated by the verbalizer\nincludes: the nearest object of each type within\nthe accumulated observations, the objects situated\nbetween these nearest objects, the objects in each\ndirection, as well as the agent’s current inventory\nand status. An example of the verbalization process\nis provided in Appendix B.5.\nThe agent is evaluated using two primary met-\nrics: reward and score. Agents receive a +1 reward\nfor each new achievement unlocked (e.g., make\nwood pickaxe, place furnace) and a ±0.1 re-\nward for every health point gained or lost. The\nscore is calculated by aggregating the success rates\nsi for each achievement and is formulated as:\nS .= exp\n \n1\nN\nN\nX\ni=1\nln (1 + si)\n!\n−1.\nWe compare DiVE against the following baselines:\n• LLM-based approaches:\nSPRING (Wu\net al., 2024b), ELLM (Du et al., 2023), and\nChain-of-Thought (CoT)(Wei et al., 2022).\n• Reinforcement Learning (RL) approaches:\nDreamerV3(Hafner et al., 2023), PPO (Schul-\nman et al., 2017), and AD (Moon et al., 2024).\n• Human Players: Expert performance on the\nCrafter environment.\n4.2\nMiniHack\nMiniHack (Samvelyan et al., 2021) is a grid-based\nenvironment built on the video game NetHack (Küt-\ntler et al., 2020). Unlike the Crafter environment,\nit supports the creation of tasks that target specific\nagent capabilities. In our work, we focus on the\nSkill Acquisition Tasks subset, which evaluates\nthe agent’s ability to leverage the rich diversity of\nNetHack’s objects, monsters, and dungeon features,\nas well as their interactions. These tasks introduce\ncomplex world dynamics, where actions are fac-\ntorized autoregressively and require executing a\nsequence of follow-up actions for the initial action\nto produce the desired effect.\nWe use the Lava Crossing, Wand of Death, and\nQuest tasks as testbeds to evaluate DiVE. We pro-\nvide detailed descriptions of these task in Ap-\npendix A.2. The agent is rewarded for unlocking\nachievements within each task. To facilitate in-\nteraction with LLMs, we use the NLE language\nwrapper (Goodger et al., 2023) to verbalize both\nobservations and actions from the environment.\nWe compare DiVE against SSO (Nottingham\net al., 2024) and Reflexion (Shinn et al., 2024), both\nof which require prior training on the tasks. Follow-\ning the official guidelines, we run each method for\n30 iterations, evaluating performance after every\n10 iterations by attempting the task 10 times using\na fixed set of skills or reflections. The final perfor-\nmance is determined based on the results from the\nlast 10 evaluation attempts.\nMethod\nScore\nReward\nHuman Experts\n50.5 ± 6.8%\n14.3 ± 2.3\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nSPRING∗\n8.2 ± 2.3%\n6.9 ± 1.8\nCoT\n1.3 ± 0.3%\n2.5 ± 0.5\nAD\n21.79 ± 1.4%\n12.6 ± 0.3\nELLM\nN\/A\n6.0 ± 0.4\nDreamerV3\n14.5 ± 1.6%\n11.7 ± 1.9\nPPO\n4.6 ± 0.3%\n4.2 ± 1.2\nRandom\n1.6 ± 0.0%\n2.1 ± 1.3\nTable 1: Performance comparison of DiVE against base-\nline models in the Crafter environment. Methods with ∗\nindicate that they were obtained using the official code\nimplementation, executed with the same five random\nseeds and model configurations as used for DiVE.\n5\nExperimental Results\nWe evaluate the performance of DiVE in the Crafter\nand MiniHack environments. In Section 5.1, we\npresent the overall results to demonstrate DiVE’s\neffectiveness in bridging the knowledge gap in\nthese tasks. Section 5.2 provides a detailed analy-\nsis of the contributions of individual components\nthrough controlled experiments.\nThen, in Sec-\ntion 5.3, we evaluate the effectiveness of the dy-\nnamics learned by DiVE in the Crafter environ-\nment. Finally, in Section 5.4, we further analyze\nthe learned dynamics in both the Crafter environ-\nment and a modified MiniHack setting with altered\ndynamics.\n5.1\nDiVE’s Performance\nTable 1 and 2 demonstrate that DiVE surpasses\nall other baselines in the Crafter and MiniHack\nenvironments. In the Crafter environment, DiVE\nexceeds the previous state-of-the-art (SOTA) LLM-\nbased method, SPRING, by a substantial margin,\nachieving a 337.8% relative improvement in score\nand a 110.1% enhancement in reward. Addition-\nally, DiVE also surpasses the prior SOTA RL-based\napproach, DreamerV3, with a 21.4% absolute im-\nprovement in score and a 2.8 absolute increase in\nreward. Notably, DiVE achieves rewards compara-\nble to human players using 10 demonstrations.\nIn the MiniHack environment, with only a single\ndemonstration, DiVE matches the performance of\nSSO and Reflexion (both of which require 30 iter-\nations of training) on the Lava Crossing task, and\noutperforms both baselines on the Wand of Death\nand Quest tasks. Specifically, DiVE achieves a 68%\nimprovement over Reflexion on the Wand of Death\ntask and a 30% and 36% improvement over SSO\nand Reflexion, respectively, on the Quest task.\nMethod\nLava Crossing\nWand of Death\nQuest\nReflexion\n0.8±0.0\n0.32±0.15\n0.87±0.43\nSSO∗\n0.8±0.0\n0.52±0.31\n0.91±0.29\nDiVE\n0.8±0.0\n0.54±0.34\n1.18±0.38\nTable 2: Comparison of DiVE performance against\nbaseline models in the MiniHack environment. Results\nmarked with ∗were obtained using the official code\nimplementation and evaluated over 10 runs.\nMethods\nScore\nReward\nComponent analysis\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nw\/o E\n21.1 ± 9.7%\n11.3 ± 4.3\nw\/o V\n9.8 ± 1.0%\n10.1 ± 0.7\nw\/o V&E\n11.5 ± 4.9%\n8.3 ± 3.8\nw\/o D&V&E\n0.9 ± 0.1%\n2.5 ± 1.3\nCoT\n1.3 ± 0.3%\n2.5 ± 0.5\nCoT + D&V\n3.6 ± 0.9%\n3.9 ± 2.3\nDynamics from distinct sources\nDiVE\n35.9 ± 3.2%\n14.5 ± 2.4\nw\/o D&V+S†\n15.7 ± 5.3%\n8.9 ± 5.1\nw\/o D&V&E+S†\n12.1 ± 4.6%\n8.7 ± 3.0\nw\/o D&V&E+H†\n34.2 ± 2.8%\n14.5 ± 0.9\nTable 3: Impact of different components on perfor-\nmance: Crafter. D, V, and E represent Discover, Verifier,\nand Evolver, respectively; S† refers to dynamics derived\nfrom the game manual, H† refers to human-annotated\ndynamics.\n5.2\nContribution of Individual Components\nWe conduct a series of ablation studies to clar-\nify the contribution of each individual element to\nDiVE’s overall performance. We report the results\non Crafter in the first section of Table 3, and the\nresults on MiniHack in Table 4.\nCrafter\nThe significant performance gap be-\ntween DiVE and its variant without the Evolver\ncomponent empirically demonstrates the Evolver’s\neffectiveness in developing gameplay strategies\nbased on world dynamics W, thereby enhancing\nthe agent’s overall proficiency in this environment.\nSimilarly, the performance decline observed in the\nvariant without the Verifier underscores the impor-\ntance of formulating strategies I based on accurate\nworld dynamics W. Moreover, the further perfor-\nmance drop in the version lacking both the Verifier\nand Evolver components highlights their comple-\nmentary roles—the Verifier ensures precision in\ncapturing dynamics, while the Evolver focuses on\nstrategy development.\nThe performance of DiVE without the Discov-\nerer, Verifier, and Evolver components reverts to\nthe CoT baseline, indicating that simply decom-\nposing the task according to the hierarchy H with-\nout integrating domain knowledge Ktarget provides\nComponents\nLava Crossing\nWand of Death\nQuest\nDiVE\n0.80±0.00\n0.54±0.34\n1.18±0.38\nw\/o E\n0.72±0.24\n0.52±0.30\n1.16±0.48\nw\/o V\n0.69±0.25\n0.52±0.38\n1.18±0.38\nw\/o V&E\n0.72±0.24\n0.40±0.40\n1.11±0.48\nw\/o D&V&E\n0.72±0.17\n0.20±0.00\n0.41±0.29\nTable 4: Impact of different components on perfor-\nmance: MiniHack. D, V, and E represent Discover,\nVerifier, and Evolver, respectively.\nno performance benefit. The substantial gap be-\ntween CoT + D&V and DiVE w\/o E further demon-\nstrates that an LLM-based agent struggles with\nlong-horizon planning tasks in the absence of task\ndecomposition, underscoring the importance of the\ndecomposition hierarchy H.\nMiniHack\nSince MiniHack is built on the popu-\nlar video game NetHack (Goodger et al., 2023),\nLLMs already possess a certain level of under-\nstanding of the environment. For example, DiVE\nwithout the Discover, Verifier, and Evolver com-\nponents (CoT baseline) can successfully solve the\nLava Crossing task. However, as the complexity\nof the tasks increases, using LLMs alone is insuf-\nficient for solving more challenging scenarios like\nthe Wand of Death and Quest tasks, highlighting\na potential knowledge gap for these more intricate\nproblems.\nThe primary performance gains of DiVE in the\nMiniHack environment can be attributed to the\nDiscover component, which accurately identifies\nthe underlying environment dynamics required for\neach task. Due to the embedded knowledge of\nLLMs about the MiniHack environment, they can\nidentify these dynamics effectively, as shown by\nthe minimal performance drop when DiVE is used\nwithout the Verifier. However, in the modified Mini-\nHack setting described in Section 5.4, we highlight\nthe critical role of the Verifier in maintaining relia-\nbility. Additionally, since the tasks have a relatively\nshort horizon, the dynamics uncovered by the Dis-\ncover component are sufficient to complete them\nsuccessfully.\n5.3\nEvaluation of Learned Dynamics\nWe investigate the performance of DiVE leveraging\nworld dynamics derived from different sources in\nCrafter. As shown in the second section of Table 3,\nDiVE significantly outperforms variants that uti-\nlize the dynamics S† from the game manual (Wu\net al., 2024b). This performance improvement in-\ndicates that the learned dynamics W are more ad-\nFigure 3: Recall of learned dynamics over discovery\nsteps, presented with mean and standard deviation, in\nthe Crafter environment.\nvantageous than S†, likely because S† lacks certain\nbeneficial details that are captured in W. The per-\nformance gap between methods using S† with and\nwithout the Evolver further highlights the impor-\ntance of strategy evolution, whose effectiveness is\nclosely tied to the quality of the underlying world\ndynamics.\nIn addition to dynamics learned from human\ndemonstrations and game manual, we have ex-\nplored a third source: human-annotated dynamics.\nThe results show that DiVE performs comparably\nto the variants using human-annotated dynamics,\ndemonstrating the robustness and effectiveness of\nDiVE’s approach to dynamic learning.\nAs previously mentioned, it is difficult to quan-\ntify the desired properties because we cannot pre-\ncisely measure the domain-relevant information\nKrelevant in LLMs or the exact amount of domain\nknowledge Ktarget required. However, by using\nhuman-annotated dynamics H† as a reference\nbenchmark for Ktarget, we can estimate the preci-\nsion and recall of the learned dynamics W, en-\nabling us to effectively assess the progress of LLM-\nbased dynamic learning. Specifically, we define re-\ncall as R = |W∩H†|\n|H†|\nand precision as P = |W∩H†|\n|W|\n.\nAs illustrated in Figure 3, both the discovered\ndynamics ˜\nW and the verified dynamics W exhibit\nan increase in recall as the number of discovery\nsteps progresses, indicating that the richness of the\nlearned dynamics improves over time. Moreover,\nthe narrowing gap in recall between ˜\nW and W sug-\ngests that the Verifier effectively filters out ’noisy’\ndynamic candidates while preserving those that\ngeneralize across different trajectory segments.\nTo evaluate whether the Verifier preserves cor-\nrect world dynamic candidates ˜\nW+ while filtering\nout unreliable ones ˜\nW−, we analyze the precision\nCorrectness\nOutcome\nCorrect (✓)\nnone\nConfounder (✗)\n1 health\nIn-domain Hallucination (✗)\n1 wood\nOut-domain Hallucination (✗)\n1 bone\nTable 5: The dynamics underlying the outcome of de-\nfeating zombie in the Crafter environment.\nof both the discovered dynamics ˜\nW and the verified\ndynamics W. As shown in Figure 4, the precision\nof the verified dynamics consistently and signif-\nicantly exceeds that of the discovered dynamics,\ndemonstrating the Verifier’s effectiveness in identi-\nfying and eliminating inaccurate candidates. This\nconfirms the Verifier’s role in enhancing the relia-\nbility of the dynamics used for decision-making.\n5.4\nAnalysis of Learned Dynamics\nThe correctness of the learned and verified dynam-\nics is classified as either correct or erroneous, with\nerrors stemming from confounders, in-domain hal-\nlucinations, or out-of-domain hallucinations. As\nshown in Table 5, an example of a confounder-\nrelated mistake occurs in Crafter when a simulta-\nneous increase in health points is incorrectly at-\ntributed to the act of defeating a zombie. In this\nscenario, the Discoverer misclassifies the health\nincrease as a direct result of defeating the zombie.\nIn the case of in-domain hallucinations, the Dis-\ncoverer incorrectly associates an increase in wood\nwith defeating the zombie, even though it is not\npossible for wood to increase during this event,\ndespite its presence in the observation. Lastly, out-\nof-domain hallucinations involve the discovery of\ndynamics that reference nonexistent objects in the\nobservation or elements not present in the Crafter\nenvironment.\nCompared to the dynamics from the game man-\nual, as shown in Table 6, we found that DiVE’s\ndynamics are not only more precise but also more\ndetailed. For instance, while SPRING only iden-\ntified that placing a stone requires stones, DiVE\ndetermined that it specifically requires exactly one\nstone and the precise facing condition needed for\nsuccessful placement. Moreover, using this infor-\nmation, the Evolver can infer advanced dynamics\nfor placing a stone, such as its potential to serve as a\nbarrier between the agent and dangerous creatures.\nIn a customized MiniHack Lava Crossing setting,\nwe modified the task such that it requires the agent\nSources\nDynamics\nManual\nPlace stone requires stones\nDiscoverer Place stone requires 1 stone and\nfaces paths, grass, sand, water,\nand lava\nEvolver\nPlace stone to block zombies\nand skeletons, preventing them\nfrom reaching the player\nTable 6: Comparing SPRING and DiVE on place stone’s\ndynamics in the Crafter environment.\nto use the Wand of Death to freeze the lava for\ncrossing, even though the wand’s original purpose\nis to zap monsters. Based on the demonstration,\nthe Discoverer mistakenly identifies the precon-\ndition for using the wand as The player must\nbe adjacent to a wall or obstacle that\ncan be destroyed or altered by the wand\nof death, with the outcome being The wand of\ndeath was used, altering the environment\nby removing walls and revealing a dark\narea to the east, southeast, and south.\nThis unreliable dynamic is successfully identified\nand filtered out by the Verifier, highlighting its cru-\ncial role in maintaining the reliability of learned\ndynamics.\n6\nRelated Work\nLanguage Models\nLanguage models (LLMs) are\ntrained autoregressively in a left-to-right sequence,\npredicting each token based on its preceding con-\ntext from an internet-scale corpus. Through this\ntraining, LLMs develop a comprehensive under-\nstanding of both language and the world it repre-\nsents (Achiam et al., 2023; Touvron et al., 2023;\nDubey et al., 2024; The, 2024), enabling them\nto perform competently across a wide range of\ntasks (Yao et al., 2024; Shinn et al., 2024).\nEmbodied Agent\nBuilding an embodied agent\nusing LLMs is challenging because LLMs lack em-\nbodied experience (Valmeekam et al., 2022; Wang\net al., 2024) in downstream environments (Weir\net al., 2022; Shridhar et al., 2020; Côté et al.,\n2019). However, LLMs can still provide founda-\ntional world knowledge that serves as a prior for the\nagent (Shi et al., 2024; Colas et al., 2023; Zhong\net al., 2024; Fu et al., 2024a). A notable distinction\nof our work is that we do not assume LLMs have\nthe necessary knowledge to solve tasks in specific\ndomains.\nFigure 4: Precision of learned dynamics before and after\nverification in the Crafter environment.\nDiscover Dynamics\nLLMs can discover knowl-\nedge by inducing and deducing rules for reasoning\ntasks (Zhu et al., 2023) and by extracting under-\nlying domain knowledge from prior trajectories\nand interactive experiences (Colas et al., 2023;\nMajumder et al., 2023; Fu et al., 2024b; Zhao\net al., 2024). However, the knowledge obtained\nthrough these methods is often unstructured, not\nsuited for addressing long-horizon planning prob-\nlems, and lacks verification for reliability, as it\noverlooks LLMs’ tendency to hallucinate (Zhang\net al., 2023b).\nEvolve\nDynamics\nLLMs\ncan\nrefine\ntheir\ndecision-making process by reflecting on past\ntrajectories (Shinn et al., 2024).\nLeveraging\nthis ability, studies such as (Wang et al., 2023;\nStengel-Eskin et al., 2024; Zhang et al., 2023a;\nNottingham et al., 2024) focus on evolving\nnew and advanced skills from pre-defined ones.\nHowever, these approaches often assume that\nLLMs already possess comprehensive domain\nknowledge and can derive new dynamics based\nsolely on their understanding.\n7\nConclusion\nWe introduce DiVE, a framework that bridges the\nknowledge gap between LLMs and downstream\ndomains. The Discoverer extracts world dynamics,\nwhile the Verifier filters unreliable candidates. In\nan online setting, the Evolver reasons strategies\nthrough interaction. Our experiments demonstrate\nthat DiVE effectively bridges the knowledge gap\nbetween LLMs and downstream domains.\nLimitations\nEvaluating DiVE in embodied environments may\nnot fully capture the diversity and complexity of\nreal-world dynamics that an agent might encounter\nin practice. If the environment’s dynamics change\nacross episodes, the offline-learned dynamics from\nDiVE may struggle to adapt, introducing potential\nbiases in the agent’s understanding and hindering\nthe development of advanced strategies. Moreover,\nacquiring these dynamics from human demonstra-\ntions can be difficult or impractical in certain sce-\nnarios.\nEthical Concerns\nWe do not anticipate any immediate ethical or so-\ncietal impact from our work. This study aims to\nbridge the knowledge gap between LLMs and the\ntarget domain. However, despite our efforts, DiVE\nmay still exhibit hallucinations due to the inherent\ntendency of LLMs to hallucinate.\nAcknowledgements\nThis work is supported by the Canada CIFAR AI\nChair Program and the Canada NSERC Discovery\nGrant (RGPIN-2021-03115).\n```\n\n请输出论文总结\n### 输出格式（输出语言用中文）\n```\n## 🌟 论文解读 | <想一个，宣传该论文的文案标题>\n\n## 📌 背景痛点\/本文动机\n...(介绍论文的背景或痛点，或者本文的动机)\n\n## 🚀 核心方法（介绍本文的几个创新点）\n💡 创新点1\n...\n\n💡 创新点2\n...\n\n## 📈 实验结果\n...\n\n## 💬 可借鉴之处\n...(介绍论文的可借鉴之处)\n\n\n.....\n```\n","llm_summary_res":"## 🌟 论文解读 | 通过世界动态建模增强智能体学习\n\n## 📌 背景痛点\/本文动机\n大型语言模型（LLMs）在语言理解和交互式决策任务中表现出色，这主要归功于它们嵌入的广泛领域知识。然而，这种知识的深度和广度在不同领域之间可能存在差异。许多现有方法假设LLMs对其环境有全面的理解，但往往忽视了它们对实际世界动态的掌握可能存在差距。为了解决这个问题，本文提出了Discover, Verify, and Evolve (DiVE)框架，该框架从少量演示中发现世界动态，验证这些动态的准确性，并根据当前情况演化新的、先进的动态。\n\n## 🚀 核心方法\n💡 创新点1：DiVE框架\nDiVE框架由三个主要组件组成：\n- Discoverer：使用课程学习方法从演示中迭代地发现环境动态。\n- Verifier：消除LLMs由于幻觉倾向而导致的不可靠动态。\n- Evolver：根据学习的动态，推理出针对当前情况的深入、特定于状态的知识。\n\n💡 创新点2：层次课程学习\nDiVE采用层次课程学习方法，从简单到复杂的动态逐步学习，从而更有效地学习。具体来说，它从任务分解层次结构中的元素（如动作、对象、子任务和子目标）开始，逐步学习它们的动态。\n\n💡 创新点3：动态验证\n为了确保动态的准确性，DiVE引入了动态验证器，它可以过滤掉可能无效和冲突的动态候选者，从而提高决策过程的可靠性。\n\n💡 创新点4：在线策略学习\nDiVE不仅学习基本规则，还专注于根据这些动态开发高级游戏策略。它通过在线学习方法将动态演化为策略，从而生成更符合当前游戏场景的策略。\n\n## 📈 实验结果\n在Crafter和MiniHack环境中进行的实验表明，DiVE在性能方面优于所有其他基线模型。在Crafter环境中，DiVE在分数和奖励方面分别比SOTA LLM方法SPRING提高了337.8%和110.1%，并且超过了SOTA RL方法DreamerV3。在MiniHack环境中，DiVE在Lava Crossing任务上与SSO和Reflexion（都需要30次迭代训练）的性能相当，并且在Wand of Death和Quest任务上超过了这两个基线。\n\n## 💬 可借鉴之处\nDiVE框架为解决LLMs在特定领域中的知识差距问题提供了一种有效的方法。它通过发现、验证和演化世界动态，提高了LLMs的决策能力。此外，DiVE的层次课程学习和动态验证方法可以应用于其他需要长期规划和决策的任务中。","llm_summary_res_status":200}
