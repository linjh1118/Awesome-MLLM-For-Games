# Paper List of method_papers.md
- [25/03] **JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse**  
[[Paper](http://arxiv.org/pdf/2503.16365v1)] [[Code/Page](https://craftjarvis.github.io/JarvisVLA)] [[TLDR/Notes](#jarvis-vla--post-training-large-scale-vision-language-models-to-play-visual-games-with-keyboards-and-mouse)]

- [25/02] **Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy**  
[[Paper](http://arxiv.org/pdf/2502.19902v2)] [[Code/Page](https://cybertronagent.github.io/Optimus-2.github.io)] [[TLDR/Notes](#optimus-2--multimodal-minecraft-agent-with-goal-observation-action-conditioned-policy)]

- [24/12] **MineStudio: A Streamlined Package for Minecraft AI Agent Development**  
[[Paper](http://arxiv.org/pdf/2412.18293v2)] [[Code/Page](https://github.com/CraftJarvis/MineStudio)] [[TLDR/Notes](#minestudio--a-streamlined-package-for-minecraft-ai-agent-development)]

- [23/10] **GROOT: Learning to Follow Instructions by Watching Gameplay Videos**  
[[Paper](http://arxiv.org/pdf/2310.08235v2)] [[Code/Page](https://craftjarvis-groot.github.io)] [[TLDR/Notes](#groot--learning-to-follow-instructions-by-watching-gameplay-videos)]

- [25/03] **ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment**  
[[Paper](http://arxiv.org/pdf/2503.02505v1)] [[Code/Page]()] [[TLDR/Notes](#rocket-2--steering-visuomotor-policy-via-cross-view-goal-alignment)]

- [24/10] **Open-World Reinforcement Learning over Long Short-Term Imagination**  
[[Paper](http://arxiv.org/pdf/2410.03618v2)] [[Code/Page]()] [[TLDR/Notes](#open-world-reinforcement-learning-over-long-short-term-imagination)]

- [18/06] **VirtualHome: Simulating Household Activities via Programs**  
[[Paper](http://arxiv.org/pdf/1806.07011v1)] [[Code/Page]()] [[TLDR/Notes](#virtualhome--simulating-household-activities-via-programs)]

- [24/03] **EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents**  
[[Paper](http://arxiv.org/pdf/2403.12014v2)] [[Code/Page]()] [[TLDR/Notes](#envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents)]

- [22/01] **Multi-Stage Episodic Control for Strategic Exploration in Text Games**  
[[Paper](http://arxiv.org/pdf/2201.01251v3)] [[Code/Page]()] [[TLDR/Notes](#multi-stage-episodic-control-for-strategic-exploration-in-text-games)]

- [21/07] **Pre-trained Language Models as Prior Knowledge for Playing Text-based Games**  
[[Paper](http://arxiv.org/pdf/2107.08408v2)] [[Code/Page]()] [[TLDR/Notes](#pre-trained-language-models-as-prior-knowledge-for-playing-text-based-games)]

- [23/02] **Guiding Pretraining in Reinforcement Learning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2302.06692v2)] [[Code/Page](https://github.com/yuqingd/ellm)] [[TLDR/Notes](#guiding-pretraining-in-reinforcement-learning-with-large-language-models)]

- [24/01] **PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas Hold'em via Large Language Model**  
[[Paper](http://arxiv.org/pdf/2401.06781v1)] [[Code/Page]()] [[TLDR/Notes](#pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model)]

- [24/02] **Enhance Reasoning for Large Language Models in the Game Werewolf**  
[[Paper](http://arxiv.org/pdf/2402.02330v2)] [[Code/Page]()] [[TLDR/Notes](#enhance-reasoning-for-large-language-models-in-the-game-werewolf)]

- [23/11] **ADaPT: As-Needed Decomposition and Planning with Language Models**  
[[Paper](http://arxiv.org/pdf/2311.05772v2)] [[Code/Page]()] [[TLDR/Notes](#adapt--as-needed-decomposition-and-planning-with-language-models)]

- [23/12] **Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game**  
[[Paper](http://arxiv.org/pdf/2312.17515v1)] [[Code/Page]()] [[TLDR/Notes](#cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game)]

- [18/06] **Counting to Explore and Generalize in Text-based Games**  
[[Paper](http://arxiv.org/pdf/1806.11525v2)] [[Code/Page]()] [[TLDR/Notes](#counting-to-explore-and-generalize-in-text-based-games)]

- [23/09] **Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4**  
[[Paper](http://arxiv.org/pdf/2309.17277v3)] [[Code/Page]()] [[TLDR/Notes](#suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4)]

- [24/07] **Enhancing Agent Learning through World Dynamics Modeling**  
[[Paper](http://arxiv.org/pdf/2407.17695v2)] [[Code/Page]()] [[TLDR/Notes](#enhancing-agent-learning-through-world-dynamics-modeling)]



# TLDR/Notes
## jarvis-vla--post-training-large-scale-vision-language-models-to-play-visual-games-with-keyboards-and-mouse
### Abstract
Recently, action-based decision-making in open-world environments has gained
significant attention. Visual Language Action (VLA) models, pretrained on
large-scale web datasets, have shown promise in decision-making tasks. However,
previous work has primarily focused on action post-training, often neglecting
enhancements to the foundational model itself. In response, we introduce a
novel approach, Act from Visual Language Post-Training, which refines Visual
Language Models (VLMs) through visual and linguistic guidance in a
self-supervised manner. This enhancement improves the models' capabilities in
world knowledge, visual recognition, and spatial grounding in open-world
environments. Following the above post-training paradigms, we obtain the first
VLA models in Minecraft that can follow human instructions on over 1k different
atomic tasks, including crafting, smelting, cooking, mining, and killing. Our
experiments demonstrate that post-training on non-trajectory tasks leads to a
significant 40% improvement over the best agent baseline on a diverse set of
atomic tasks. Furthermore, we demonstrate that our approach surpasses
traditional imitation learning-based policies in Minecraft, achieving
state-of-the-art performance. We have open-sourced the code, models, and
datasets to foster further research. The project page can be found in
https://craftjarvis.github.io/JarvisVLA.
### ğŸŒŸ è®ºæ–‡è§£è¯» | JARVIS-VLAï¼šé€šè¿‡è§†è§‰è¯­è¨€åè®­ç»ƒæå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼ŒåŸºäºåŠ¨ä½œçš„å†³ç­–åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤§å‹ç½‘ç»œæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨å†³ç­–ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åŠ¨ä½œåè®­ç»ƒä¸Šï¼Œå¾€å¾€å¿½ç•¥äº†åŸºç¡€æ¨¡å‹æœ¬èº«çš„æ”¹è¿›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³â€œä»è§†è§‰è¯­è¨€åè®­ç»ƒä¸­å­¦ä¹ åŠ¨ä½œâ€ï¼Œé€šè¿‡è§†è§‰å’Œè¯­è¨€æŒ‡å¯¼ä»¥è‡ªç›‘ç£çš„æ–¹å¼å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒã€‚è¿™ç§å¢å¼ºæé«˜äº†æ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„ä¸–ç•ŒçŸ¥è¯†ã€è§†è§‰è¯†åˆ«å’Œç©ºé—´å®šä½èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè§†è§‰è¯­è¨€åè®­ç»ƒï¼ˆActVLPï¼‰
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼â€”â€”è§†è§‰è¯­è¨€åè®­ç»ƒï¼ˆActVLPï¼‰ï¼Œå°†è§†è§‰è¯­è¨€ä»»åŠ¡æ•´åˆåˆ°VLAæ¨¡å‹çš„åè®­ç»ƒé˜¶æ®µã€‚ActVLPåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼š
1. å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œä½¿ç”¨ä¸ä¸‹æ¸¸ç¯å¢ƒç›¸å…³çš„ä¸–ç•ŒçŸ¥è¯†æ–‡æœ¬æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚
2. å¯¹è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œä½¿ç”¨å¤šæ¨¡æ€è§†è§‰è¯­è¨€å¯¹é½å’Œç©ºé—´å®šä½æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚
3. åœ¨è½¨è¿¹æ•°æ®é›†ä¸Šè¿›è¡Œæ¨¡ä»¿å­¦ä¹ ï¼Œè¦æ±‚æ¨¡å‹æ¨¡ä»¿ä¸“å®¶åŠ¨ä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šJARVIS-VLAæ¨¡å‹ç»“æ„
JARVIS-VLAé‡‡ç”¨äº†ä¸€ç§ç±»ä¼¼äºLlavaçš„æ¶æ„ï¼Œä½†è¿›è¡Œäº†è½»å¾®çš„ä¿®æ”¹ã€‚å®ƒåŒ…æ‹¬è§†è§‰ç¼–ç å™¨ã€å›¾åƒæŠ•å½±æ¨¡å—ã€è¯­è¨€æ¨¡å‹Transformerå’ŒåŠ¨ä½œè§£ç å™¨ç­‰å…³é”®ç»„ä»¶ã€‚æ­¤å¤–ï¼ŒJARVIS-VLAè¿˜é‡‡ç”¨äº†éé©¬å°”å¯å¤«æ¶æ„ï¼Œé€šè¿‡åœ¨æç¤ºä¸­åŒ…å«è§‚å¯Ÿå›¾åƒçš„å†å²æ¥ç¡®ä¿æ¨¡å‹ä¿ç•™æ—¶é—´ä¸Šä¸‹æ–‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒJARVIS-VLAåœ¨Minecraftæ¸¸æˆä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿéµå¾ªäººç±»æŒ‡ä»¤å®Œæˆè¶…è¿‡1kä¸ªä¸åŒçš„åŸå­ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ¶ä½œã€ç†”ç‚¼ã€çƒ¹é¥ªã€é‡‡çŸ¿å’Œæ€æˆ®ç­‰ã€‚ä¸æœ€ä½³ä»£ç†åŸºçº¿ç›¸æ¯”ï¼Œåœ¨éè½¨è¿¹ä»»åŠ¡ä¸Šè¿›è¡Œåè®­ç»ƒå¯¼è‡´åœ¨ä¸€ç³»åˆ—åŸå­ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†40%ã€‚æ­¤å¤–ï¼ŒJARVIS-VLAåœ¨Minecraftä¸­è¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºæ¨¡ä»¿å­¦ä¹ çš„ç­–ç•¥ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ActVLPè®­ç»ƒèŒƒå¼å’ŒJARVIS-VLAæ¨¡å‹ç»“æ„ä¸ºè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›æå‡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†VLAæ¨¡å‹çš„ç¼©æ”¾è§„å¾‹ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚

## optimus-2--multimodal-minecraft-agent-with-goal-observation-action-conditioned-policy
### Abstract
Building an agent that can mimic human behavior patterns to accomplish
various open-world tasks is a long-term goal. To enable agents to effectively
learn behavioral patterns across diverse tasks, a key challenge lies in
modeling the intricate relationships among observations, actions, and language.
To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a
Multimodal Large Language Model (MLLM) for high-level planning, alongside a
Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP
contains (1) an Action-guided Behavior Encoder that models causal relationships
between observations and actions at each timestep, then dynamically interacts
with the historical observation-action sequence, consolidating it into
fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with
open-ended language instructions to predict actions auto-regressively.
Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}
dataset, which contains 25,000 videos across 8 atomic tasks, providing about
30M goal-observation-action pairs. The automated construction method, along
with the MGOA dataset, can contribute to the community's efforts to train
Minecraft agents. Extensive experimental results demonstrate that Optimus-2
exhibits superior performance across atomic tasks, long-horizon tasks, and
open-ended instruction tasks in Minecraft. Please see the project page at
https://cybertronagent.github.io/Optimus-2.github.io/.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Optimus-2ï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„Minecraftæ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºèƒ½å¤Ÿæ¨¡ä»¿äººç±»è¡Œä¸ºæ¨¡å¼å¹¶å®Œæˆå„ç§ä»»åŠ¡çš„æ™ºèƒ½ä½“ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œè¦ä½¿æ™ºèƒ½ä½“æœ‰æ•ˆåœ°å­¦ä¹ è·¨ä»»åŠ¡çš„è¡Œä¸ºæ¨¡å¼ï¼Œå…³é”®æŒ‘æˆ˜åœ¨äºå»ºæ¨¡è§‚å¯Ÿã€åŠ¨ä½œå’Œè¯­è¨€ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ç°æœ‰çš„æ™ºèƒ½ä½“åœ¨å¤„ç†å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å¤šæ ·åŒ–ä»»åŠ¡æ—¶ï¼Œé€šå¸¸é‡‡ç”¨ä»»åŠ¡è§„åˆ’å™¨å’Œç›®æ ‡æ¡ä»¶ç­–ç•¥çš„æ¡†æ¶ã€‚å°½ç®¡ç°æœ‰çš„æ™ºèƒ½ä½“åœ¨åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè§„åˆ’å™¨æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç›®æ ‡æ¡ä»¶ç­–ç•¥çš„æ€§èƒ½ç“¶é¢ˆä»ç„¶å­˜åœ¨ã€‚ç°æœ‰çš„ç­–ç•¥é€šå¸¸å¿½ç•¥äº†è§‚å¯Ÿå’ŒåŠ¨ä½œä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”éš¾ä»¥å»ºæ¨¡å¼€æ”¾å¼çš„å­ç›®æ ‡å’Œè§‚å¯Ÿ-åŠ¨ä½œåºåˆ—ä¹‹é—´çš„å…³ç³»ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Optimus-2ï¼Œä¸€ä¸ªæ–°é¢–çš„Minecraftæ™ºèƒ½ä½“ï¼Œå®ƒç»“åˆäº†MLLMè¿›è¡Œé«˜çº§è§„åˆ’ï¼Œå¹¶é‡‡ç”¨ç›®æ ‡-è§‚å¯Ÿ-åŠ¨ä½œæ¡ä»¶ç­–ç•¥ï¼ˆGOAPï¼‰è¿›è¡Œä½çº§æ§åˆ¶ã€‚GOAPåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨ä½œå¼•å¯¼çš„è¡Œä¸ºç¼–ç å™¨
åŠ¨ä½œå¼•å¯¼çš„è¡Œä¸ºç¼–ç å™¨ç”¨äºå»ºæ¨¡è§‚å¯Ÿ-åŠ¨ä½œåºåˆ—ã€‚å®ƒé¦–å…ˆä½¿ç”¨å› æœæ„ŸçŸ¥å™¨å°†åŠ¨ä½œåµŒå…¥åˆ°è§‚å¯Ÿç‰¹å¾ä¸­ï¼Œåˆ©ç”¨ä»»åŠ¡ç›¸å…³çš„åŠ¨ä½œä¿¡æ¯ä½œä¸ºæŒ‡å¯¼æ¥è°ƒæ•´è§‚å¯Ÿç‰¹å¾ï¼Œä»è€Œä¸ºåŠ¨ä½œé¢„æµ‹æä¾›ç»†ç²’åº¦çš„è§‚å¯Ÿ-åŠ¨ä½œä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†åœ¨ä¸è¶…å‡ºè¾“å…¥é•¿åº¦é™åˆ¶çš„æƒ…å†µä¸‹å¯¹é•¿æœŸè§‚å¯Ÿ-åŠ¨ä½œåºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œå¼•å…¥äº†å†å²èšåˆå™¨ï¼Œå°†å½“å‰è§‚å¯Ÿ-åŠ¨ä½œä¿¡æ¯ä¸å†å²åºåˆ—åŠ¨æ€åœ°æ•´åˆæˆå›ºå®šé•¿åº¦çš„è¡Œä¸ºæ ‡è®°ã€‚è¡Œä¸ºæ ‡è®°å¯ä»¥ä»¥å›ºå®šä¸”é€‚å½“çš„é•¿åº¦æ•è·è§‚å¯Ÿ-åŠ¨ä½œåºåˆ—çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé¢„æµ‹ä¸è§‚å¯Ÿ-åŠ¨ä½œåºåˆ—é€»è¾‘ä¸€è‡´çš„åŠ¨ä½œï¼Œè€Œä¸æ˜¯ä»…åŸºäºå½“å‰è§‚å¯Ÿè¿›è¡Œå­¤ç«‹çš„åŠ¨ä½œé¢„æµ‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹
ä¸ºäº†æ˜ç¡®ç¼–ç å­ç›®æ ‡çš„è¯­ä¹‰ï¼Œå¼•å…¥äº†MLLMä½œä¸ºGOAPçš„éª¨å¹²ç½‘ç»œã€‚å®ƒå°†å­ç›®æ ‡ä¸è¡Œä¸ºæ ‡è®°å¯¹é½ï¼Œä»¥è‡ªå›å½’æ–¹å¼é¢„æµ‹åç»­åŠ¨ä½œã€‚åˆ©ç”¨MLLMçš„è¯­è¨€ç†è§£å’Œå¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ï¼Œå®ƒå¯ä»¥æ›´å¥½åœ°æ•´åˆå¼€æ”¾å¼å­ç›®æ ‡å’Œè§‚å¯Ÿ-åŠ¨ä½œåºåˆ—çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºç­–ç•¥çš„åŠ¨ä½œé¢„æµ‹èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Minecraftçš„å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜Optimus-2åœ¨åŸå­ä»»åŠ¡ã€é•¿æœŸä»»åŠ¡å’Œå¼€æ”¾å¼æŒ‡ä»¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¸ä¹‹å‰çš„SOTAç›¸æ¯”ï¼ŒOptimus-2åœ¨åŸå­ä»»åŠ¡ã€é•¿æœŸä»»åŠ¡å’Œå¼€æ”¾å¼å­ç›®æ ‡ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†å¹³å‡27%ã€10%å’Œ18%çš„æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Optimus-2æ™ºèƒ½ä½“åŠå…¶GOAPç­–ç•¥ä¸ºå¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ã€‚åŠ¨ä½œå¼•å¯¼çš„è¡Œä¸ºç¼–ç å™¨å’ŒMLLMçš„å¼•å…¥æœ‰æ•ˆåœ°è§£å†³äº†è§‚å¯Ÿã€åŠ¨ä½œå’Œè¯­è¨€ä¹‹é—´çš„å¤æ‚å…³ç³»å»ºæ¨¡é—®é¢˜ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œå¼€æ”¾å¼æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„MGOAæ•°æ®é›†ä¸ºè®­ç»ƒMinecraftæ™ºèƒ½ä½“æä¾›äº†é«˜è´¨é‡çš„æ•°æ®èµ„æºï¼Œæœ‰åŠ©äºæ¨åŠ¨ç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚

## minestudio--a-streamlined-package-for-minecraft-ai-agent-development
### Abstract
Minecraft has emerged as a valuable testbed for embodied intelligence and
sequential decision-making research, yet the development and validation of
novel agents remains hindered by significant engineering challenges. This paper
presents MineStudio, an open-source software package designed to streamline
embodied policy development in Minecraft. MineStudio represents the first
comprehensive integration of seven critical engineering components: simulator,
data, model, offline pretraining, online finetuning, inference, and benchmark,
thereby allowing users to concentrate their efforts on algorithm innovation. We
provide a user-friendly API design accompanied by comprehensive documentation
and tutorials. The complete codebase is publicly available at
https://github.com/CraftJarvis/MineStudio.
### ğŸŒŸ è®ºæ–‡è§£è¯» | â€œMineStudioï¼šæ‰“é€ Minecraft AIæ™ºèƒ½ä½“å¼€å‘çš„åˆ©å™¨â€

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
Minecraftå› å…¶å¼€æ”¾ä¸–ç•Œå’Œä¸°å¯Œçš„æ¸¸æˆæ€§ï¼Œå·²æˆä¸ºç ”ç©¶å…·èº«æ™ºèƒ½å’Œé¡ºåºå†³ç­–çš„é‡è¦å¹³å°ã€‚ç„¶è€Œï¼Œæ–°å‹æ™ºèƒ½ä½“çš„å¼€å‘å’ŒéªŒè¯é¢ä¸´ç€å·¨å¤§çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†MineStudioï¼Œä¸€ä¸ªå¼€æºè½¯ä»¶åŒ…ï¼Œæ—¨åœ¨ç®€åŒ–Minecraftä¸­å…·èº«ç­–ç•¥çš„å¼€å‘æµç¨‹ã€‚MineStudioé›†æˆäº†ä¸ƒä¸ªå…³é”®çš„å·¥ç¨‹ç»„ä»¶ï¼šæ¨¡æ‹Ÿå™¨ã€æ•°æ®ã€æ¨¡å‹ã€ç¦»çº¿é¢„è®­ç»ƒã€åœ¨çº¿å¾®è°ƒã€æ¨ç†å’ŒåŸºå‡†æµ‹è¯•ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé›†ä¸­ç²¾åŠ›è¿›è¡Œç®—æ³•åˆ›æ–°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•ï¼ˆä»‹ç»æœ¬æ–‡çš„å‡ ä¸ªåˆ›æ–°ç‚¹ï¼‰
ğŸ’¡ åˆ›æ–°ç‚¹1
MineStudioæä¾›äº†ä¸€ä¸ªé’©å­å¼çš„MinecraftåŒ…è£…å™¨ï¼Œå…è®¸ç”¨æˆ·é«˜åº¦å®šåˆ¶ç¯å¢ƒï¼ŒåŒ…æ‹¬ç›‘æ§æ¸²æŸ“å¸§ç‡ã€å‘å‡ºä½œå¼Šå‘½ä»¤ã€æ¨¡æ‹Ÿå¿«é€Ÿé‡ç½®ç­‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†ä¸€å¥—å¸¸ç”¨çš„å›è°ƒå‡½æ•°ï¼Œå‡å°‘ç”¨æˆ·çš„å·¥ä½œé‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
è¯¥è½¯ä»¶åŒ…å¼•å…¥äº†ä¸€ç§çµæ´»é«˜æ•ˆçš„æ•°æ®ç»“æ„æ¥å¤„ç†ç¦»çº¿è½¨è¿¹æ•°æ®ï¼Œæ”¯æŒå¯¹è¶…é•¿è½¨è¿¹çš„åˆ†å¸ƒå¼æ‰¹é‡é‡‡æ ·ï¼Œæœ‰åŠ©äºè®­ç»ƒéœ€è¦é•¿æœŸè®°å¿†çš„æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3
æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ç­–ç•¥æ¨¡å‹æ¨¡æ¿ï¼Œä»¥åŠä¸“é—¨ä¸ºMinecrafté¢†åŸŸè®¾è®¡çš„åŠ¨ä½œå’Œä»·å€¼å¤´ï¼Œå…è®¸ç”¨æˆ·ä¸“æ³¨äºæ¨¡å‹æ¶æ„è®¾è®¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4
é›†æˆäº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ç®¡é“ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ï¼Œä»¥åŠä¸€ä¸ªè‡ªåŠ¨åŒ–çš„è¯„ä¼°ç®¡é“ï¼Œå¯ä»¥åˆ†æä»»åŠ¡è§†é¢‘å¹¶æä¾›æ‰¹é‡ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡é€šè¿‡ä¸ç°æœ‰Minecraftå¼€å‘æ¡†æ¶çš„æ¯”è¾ƒï¼Œå±•ç¤ºäº†MineStudioåœ¨ç¯å¢ƒå®šåˆ¶ã€æ™ºèƒ½ä½“åˆ›å»ºã€è®­ç»ƒã€è¯„ä¼°ç­‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚å®ƒä¸ä»…æä¾›äº†æ— ç¼çš„ç®¡é“æ¥é›†æˆæ„å»ºã€è®­ç»ƒå’Œè¯„ä¼°æ™ºèƒ½ä½“ï¼Œè¿˜æä¾›äº†åˆ†å¸ƒå¼æ¨ç†æ¡†æ¶å’Œæœ€å…ˆè¿›çš„åŸºçº¿å®ç°ï¼Œä¸ºMinecraftæ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°èŒƒå¼ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MineStudioé€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œé™ä½äº†å¼€å‘Minecraftæ™ºèƒ½ä½“æ—¶çš„å·¥ç¨‹éš¾åº¦ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥æ›´å¤šåœ°å…³æ³¨ç®—æ³•åˆ›æ–°è€Œä¸æ˜¯å·¥ç¨‹ç»†èŠ‚ã€‚å®ƒçš„å¼€æºç‰¹æ€§å’Œä¸°å¯Œçš„æ–‡æ¡£å’Œæ•™ç¨‹ï¼Œä¸ºå¿«é€Ÿä¸Šæ‰‹æä¾›äº†ä¾¿åˆ©ã€‚æ­¤å¤–ï¼Œå…¶é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ç®¡é“ï¼Œä»¥åŠé›†æˆçš„åŸºå‡†æµ‹è¯•ï¼Œä¸ºMinecraftæ™ºèƒ½ä½“çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚

## groot--learning-to-follow-instructions-by-watching-gameplay-videos
### Abstract
We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GROOTï¼šé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘å­¦ä¹ æŒ‡ä»¤éµå¾ª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é•¿æœŸç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ§åˆ¶å™¨é€šå¸¸åªèƒ½å®Œæˆé¢„å®šä¹‰çš„ã€æœ‰é™çš„ç¨‹åºæ€§ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è§‚çœ‹æ¸¸æˆè§†é¢‘æ¥å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå°†ç›®æ ‡æŒ‡å®šä¸ºå‚è€ƒæ¸¸æˆè§†é¢‘ç‰‡æ®µï¼Œä»è€Œæä¾›ä¸°å¯Œçš„ç›®æ ‡è§„èŒƒï¼ŒåŒæ—¶æ¶ˆé™¤å¯¹æ˜‚è´µçš„æ–‡æœ¬-æ¸¸æˆæ³¨é‡Šçš„éœ€æ±‚ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼•å…¥äº†ä¸€ç§æ–°çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶äº§ç”Ÿä¸€ä¸ªç›®æ ‡ç©ºé—´å’Œä¸€ä¸ªè§†é¢‘æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ï¼Œä»è€Œå®ç°ä»æ¸¸æˆè§†é¢‘ä¸­å­¦ä¹ æŒ‡ä»¤éµå¾ªæ§åˆ¶å™¨ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Minecraft SkillForgeåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGROOTåœ¨æ•´ä½“Eloè¯„åˆ†æ¯”è¾ƒä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå¹¶ä¸”åœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„è·å–é’»çŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å­¦ä¹ æ¡†æ¶å’ŒGROOTä»£ç†çš„æ¶æ„è®¾è®¡ä¸ºæ„å»ºèƒ½å¤Ÿéµå¾ªå¼€æ”¾æŒ‡ä»¤çš„æ§åˆ¶å™¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†ç›®æ ‡ç©ºé—´å’Œæ§åˆ¶å™¨ç­–ç•¥çš„æ½œåœ¨åº”ç”¨ï¼Œä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„å¤æ‚ä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## rocket-2--steering-visuomotor-policy-via-cross-view-goal-alignment
### Abstract
We aim to develop a goal specification method that is semantically clear,
spatially sensitive, and intuitive for human users to guide agent interactions
in embodied environments. Specifically, we propose a novel cross-view goal
alignment framework that allows users to specify target objects using
segmentation masks from their own camera views rather than the agent's
observations. We highlight that behavior cloning alone fails to align the
agent's behavior with human intent when the human and agent camera views differ
significantly. To address this, we introduce two auxiliary objectives:
cross-view consistency loss and target visibility loss, which explicitly
enhance the agent's spatial reasoning ability. According to this, we develop
ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an
improvement in the efficiency of inference 3x to 6x. We show ROCKET-2 can
directly interpret goals from human camera views for the first time, paving the
way for better human-agent interaction.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ROCKET-2ï¼šé€šè¿‡è·¨è§†å›¾ç›®æ ‡å¯¹é½å¼•å¯¼è§†è§‰è¿åŠ¨ç­–ç•¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨æœºå™¨äººå­¦ä¹ å’Œè™šæ‹Ÿç©å®¶é¢†åŸŸï¼Œè®©æ™ºèƒ½ä½“å®ç°äººç±»æœŸæœ›çš„ç›®æ ‡æ˜¯ä¸€ä¸ªé•¿æœŸæŒ‘æˆ˜ã€‚å…³é”®åœ¨äºæ‰¾åˆ°æ—¢èƒ½è®©äººç±»ç”¨æˆ·è½»æ¾æŒ‡å®šï¼Œåˆèƒ½ç²¾ç¡®æ•æ‰å¤šç§ä»»åŠ¡çš„è¡¨ç¤ºæ–¹æ³•ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€åªå…³æ³¨å…¶ä¸­ä¸€ä¸ªæ–¹é¢ï¼Œä¾‹å¦‚è¯­è¨€æŒ‡ä»¤ç›´è§‚ä½†è¡¨è¾¾ç©ºé—´å…³ç³»æ¨¡ç³Šï¼Œè§†è§‰æ¨¡æ€åˆ™å®¹æ˜“å—åˆ°å…‰ç…§ã€ç‰©ä½“å¤–è§‚ç­‰å› ç´ çš„å½±å“ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç”¨æˆ·å‹å¥½çš„è·¨è§†å›¾ç›®æ ‡æŒ‡å®šæ–¹æ³•
ROCKET-2 å…è®¸ç”¨æˆ·ä½¿ç”¨è‡ªå·±çš„ç›¸æœºè§†å›¾ä¸­çš„åˆ†å‰²æ©ç æ¥æŒ‡å®šç›®æ ‡å¯¹è±¡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ™ºèƒ½ä½“çš„è§‚å¯Ÿç»“æœã€‚è¿™ç§æ–¹æ³•å°†ç›®æ ‡æŒ‡å®šä¸æ™ºèƒ½ä½“çš„ç›¸æœºè§†å›¾è§£è€¦ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†äººæœºäº¤äº’çš„æ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè·¨è§†å›¾ä¸€è‡´æ€§æŸå¤±å’Œç›®æ ‡å¯è§æ€§æŸå¤±
ä¸ºäº†è§£å†³å¼€æ”¾ä¸–ç•Œä¸­éƒ¨åˆ†å¯è§‚å¯Ÿæ€§çš„æŒ‘æˆ˜ï¼ŒROCKET-2 å¼•å…¥äº†ä¸¤ä¸ªè¾…åŠ©ç›®æ ‡å‡½æ•°ï¼šè·¨è§†å›¾ä¸€è‡´æ€§æŸå¤±å’Œç›®æ ‡å¯è§æ€§æŸå¤±ã€‚è¿™äº›æŸå¤±å‡½æ•°å¯ä»¥æ˜¾å¼åœ°å¢å¼ºæ™ºèƒ½ä½“åœ¨è·¨è§†å›¾ç›®æ ‡å¯¹é½æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æé«˜å…¶å¯æ“æ§æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ Minecraft ç¯å¢ƒä¸­ï¼ŒROCKET-2 å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ¨ç†æ•ˆç‡æ–¹é¢æ¯” ROCKET-1 æé«˜äº† 3 å€åˆ° 6 å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROCKET-2 å¯ä»¥ç›´æ¥ä»äººç±»çš„ç›¸æœºè§†å›¾ä¸­è§£é‡Šç›®æ ‡ï¼Œä¸ºæ›´å¥½çš„äººæœºäº¤äº’é“ºå¹³äº†é“è·¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ROCKET-2 çš„è·¨è§†å›¾ç›®æ ‡å¯¹é½æ–¹æ³•ä¸ºæ›´ç›´è§‚çš„äººæœºäº¤äº’æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶å¼•å…¥çš„è¾…åŠ©ç›®æ ‡å‡½æ•°å’Œæ¶æ„è®¾è®¡ä¹Ÿä¸ºè§£å†³å¼€æ”¾ä¸–ç•Œä¸­éƒ¨åˆ†å¯è§‚å¯Ÿæ€§çš„æŒ‘æˆ˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

## open-world-reinforcement-learning-over-long-short-term-imagination
### Abstract
Training visual reinforcement learning agents in a high-dimensional open
world presents significant challenges. While various model-based methods have
improved sample efficiency by learning interactive world models, these agents
tend to be "short-sighted", as they are typically trained on short snippets of
imagined experiences. We argue that the primary challenge in open-world
decision-making is improving the exploration efficiency across a vast state
space, especially for tasks that demand consideration of long-horizon payoffs.
In this paper, we present LS-Imagine, which extends the imagination horizon
within a limited number of state transition steps, enabling the agent to
explore behaviors that potentially lead to promising long-term feedback. The
foundation of our approach is to build a $\textit{long short-term world
model}$. To achieve this, we simulate goal-conditioned jumpy state transitions
and compute corresponding affordance maps by zooming in on specific areas
within single images. This facilitates the integration of direct long-term
values into behavior learning. Our method demonstrates significant improvements
over state-of-the-art techniques in MineDojo.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Open-World Reinforcement Learning over Long Short-Term Imagination

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨å¼€æ”¾ä¸–ç•Œä¸­è®­ç»ƒè§†è§‰å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å„ç§åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šè¿‡å­¦ä¹ äº¤äº’å¼ä¸–ç•Œæ¨¡å‹æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œä½†è¿™äº›æ™ºèƒ½ä½“å¾€å¾€â€œç›®å…‰çŸ­æµ…â€ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸æ˜¯åœ¨ç”±ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„çŸ­æœŸæƒ³è±¡ç»éªŒç‰‡æ®µä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œå¼€æ”¾ä¸–ç•Œå†³ç­–ä¸­çš„ä¸»è¦æŒ‘æˆ˜æ˜¯åœ¨å¹¿é˜”çš„çŠ¶æ€ç©ºé—´ä¸­æé«˜æ¢ç´¢æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦è€ƒè™‘é•¿æœŸå›æŠ¥çš„ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé•¿çŸ­æœŸä¸–ç•Œæ¨¡å‹
æœ¬æ–‡æå‡ºäº†LS-Imagineï¼Œå®ƒæ‰©å±•äº†æƒ³è±¡èŒƒå›´ï¼Œåœ¨æœ‰é™çš„çŠ¶æ€è½¬æ¢æ­¥éª¤å†…ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ¢ç´¢å¯èƒ½å¸¦æ¥æœ‰å¸Œæœ›é•¿æœŸåé¦ˆçš„è¡Œä¸ºã€‚LS-Imagineçš„åŸºç¡€æ˜¯æ„å»ºä¸€ä¸ªé•¿çŸ­æœŸä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿç›®æ ‡æ¡ä»¶ä¸‹çš„è·³è·ƒçŠ¶æ€è½¬æ¢ï¼Œå¹¶é€šè¿‡å¯¹å•ä¸ªå›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸè¿›è¡Œç¼©æ”¾æ¥è®¡ç®—ç›¸åº”çš„å¯åˆ©ç”¨æ€§å›¾ï¼Œä»è€Œå°†ç›´æ¥é•¿æœŸä»·å€¼æ•´åˆåˆ°è¡Œä¸ºå­¦ä¹ ä¸­ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¯åˆ©ç”¨æ€§å›¾å’Œå†…åœ¨å¥–åŠ±
ä¸ºäº†æé«˜å¼€æ”¾ä¸–ç•Œä¸­åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡ï¼Œæœ¬æ–‡ä½¿ç”¨è§†è§‰è§‚å¯Ÿå’Œæ–‡æœ¬ä»»åŠ¡å®šä¹‰æ¥ç”Ÿæˆå¯åˆ©ç”¨æ€§å›¾ã€‚å¯åˆ©ç”¨æ€§å›¾çªå‡ºäº†è§‚å¯ŸåŒºåŸŸä¸ä»»åŠ¡æè¿°çš„ç›¸å…³æ€§ï¼Œä½œä¸ºç©ºé—´å…ˆéªŒï¼Œæœ‰æ•ˆåœ°å¼•å¯¼æ™ºèƒ½ä½“çš„æ¢ç´¢æœç€æ„Ÿå…´è¶£çš„åŒºåŸŸè¿›è¡Œã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†åŸºäºå¯åˆ©ç”¨æ€§å›¾çš„å†…åœ¨å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ™ºèƒ½ä½“å‘ç›®æ ‡ç§»åŠ¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ··åˆé•¿çŸ­æœŸæƒ³è±¡è·¯å¾„
LS-Imagineé‡‡ç”¨æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ï¼Œæ ¹æ®ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„æœ‰é™åºåˆ—çš„é•¿æœŸå’ŒçŸ­æœŸæƒ³è±¡æ¥ä¼˜åŒ–æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚æ™ºèƒ½ä½“ä»ç¼–ç çš„åˆå§‹çŠ¶æ€å¼€å§‹ï¼Œæ ¹æ®è·³è·ƒæ ‡å¿—åŠ¨æ€é€‰æ‹©é•¿æœŸæˆ–çŸ­æœŸçŠ¶æ€è½¬æ¢æ¨¡å‹æ¥é¢„æµ‹åç»­çŠ¶æ€ã€‚å¯¹äºé•¿æœŸæƒ³è±¡é¢„æµ‹çš„è·³è·ƒçŠ¶æ€ï¼Œé—´éš”é¢„æµ‹å™¨ä¼°è®¡ä»Ë†stâˆ’1åˆ°Ë†stçš„æ­¥éª¤æ•°Ë†âˆ†tå’ŒË†âˆ†tæ—¶é—´é—´éš”å†…çš„æ½œåœ¨æŠ˜æ‰£ç´¯ç§¯å¥–åŠ±Ë†Gtã€‚å¦åˆ™ï¼Œå¯¹äºé€šè¿‡çŸ­æœŸæƒ³è±¡è·å¾—çš„çŠ¶æ€ï¼Œå¯¹åº”äºç¯å¢ƒä¸­çš„å•æ­¥è½¬æ¢ï¼Œæˆ‘ä»¬è®¾ç½®Ë†âˆ†t = 1å’ŒË†Gt = Ë†rtã€‚å› æ­¤ï¼Œåœ¨ä¸€ä¸ªæƒ³è±¡é›†ä¸­ï¼Œæˆ‘ä»¬è·å¾—ä¸€ä¸ªæ­¥éª¤é—´éš”åºåˆ—Ë†âˆ†2:Lå’Œä¸€ä¸ªé¢„æµ‹å¥–åŠ±åºåˆ—Ë†G2:Lï¼Œè¿™äº›å¥–åŠ±åœ¨è¿ç»­çš„æƒ³è±¡çŠ¶æ€ä¹‹é—´ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨MineDojoåŸºå‡†æµ‹è¯•ä¸­å¯¹LS-Imagineè¿›è¡Œäº†è¯„ä¼°ï¼ŒMineDojoæ˜¯ä¸€ä¸ªåŸºäºMinecraftæ¸¸æˆçš„ç»¼åˆæ¨¡æ‹Ÿå¹³å°ï¼Œå…·æœ‰å„ç§å¼€æ”¾æ€§ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼ŒLS-Imagineåœ¨å®ŒæˆæŒ‘æˆ˜æ€§ä»»åŠ¡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰è§†è§‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–ç›®æ ‡åˆ†å¸ƒçš„ä»»åŠ¡ä¸­ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LS-Imagineæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ•è·å³æ—¶å’Œè·³è·ƒçŠ¶æ€è½¬æ¢ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬åœ¨è¡Œä¸ºå­¦ä¹ ä¸­æé«˜å¼€æ”¾ä¸–ç•Œçš„æ¢ç´¢æ•ˆç‡ã€‚è¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

*   é•¿çŸ­æœŸä¸–ç•Œæ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿé•¿æœŸå’ŒçŸ­æœŸçŠ¶æ€è½¬æ¢ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬è¿›è¡Œè¡Œä¸ºå­¦ä¹ ã€‚
*   é€šè¿‡å›¾åƒç¼©æ”¾ç”Ÿæˆå¯åˆ©ç”¨æ€§å›¾çš„æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å¼•å¯¼æ™ºèƒ½ä½“çš„æ¢ç´¢ã€‚
*   åŸºäºå¯åˆ©ç”¨æ€§å›¾çš„å†…åœ¨å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å‘ç›®æ ‡ç§»åŠ¨ã€‚
*   æ··åˆé•¿çŸ­æœŸæƒ³è±¡è·¯å¾„ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£é•¿æœŸä»·å€¼ï¼Œæé«˜å†³ç­–èƒ½åŠ›ã€‚

LS-Imagineä¸ºå¼€æ”¾ä¸–ç•Œå¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†çµæ„Ÿã€‚

## virtualhome--simulating-household-activities-via-programs
### Abstract
In this paper, we are interested in modeling complex activities that occur in
a typical household. We propose to use programs, i.e., sequences of atomic
actions and interactions, as a high level representation of complex tasks.
Programs are interesting because they provide a non-ambiguous representation of
a task, and allow agents to execute them. However, nowadays, there is no
database providing this type of information. Towards this goal, we first
crowd-source programs for a variety of activities that happen in people's
homes, via a game-like interface used for teaching kids how to code. Using the
collected dataset, we show how we can learn to extract programs directly from
natural language descriptions or from videos. We then implement the most common
atomic (inter)actions in the Unity3D game engine, and use our programs to
"drive" an artificial agent to execute tasks in a simulated household
environment. Our VirtualHome simulator allows us to create a large activity
video dataset with rich ground-truth, enabling training and testing of video
understanding models. We further showcase examples of our agent performing
tasks in our VirtualHome based on language descriptions.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VirtualHomeï¼šé€šè¿‡ç¨‹åºæ¨¡æ‹Ÿå®¶åº­æ´»åŠ¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯çš„å‘å±•ï¼Œè®©æœºå™¨äººæ‰§è¡Œå¤æ‚çš„å®¶åº­æ´»åŠ¨æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æè¿°å’Œæ‰§è¡Œè¿™äº›æ´»åŠ¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º VirtualHome çš„æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨é€šè¿‡ç¨‹åºæ¥æ¨¡æ‹Ÿå®¶åº­æ´»åŠ¨ï¼Œä»è€Œä¸ºæœºå™¨äººæ‰§è¡Œå¤æ‚ä»»åŠ¡æä¾›ä¸€ç§æ–°çš„æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå®¶åº­æ´»åŠ¨çŸ¥è¯†åº“
æœ¬æ–‡é¦–å…ˆé€šè¿‡ä¼—åŒ…çš„æ–¹å¼æ”¶é›†äº†å¤§é‡çš„å®¶åº­æ´»åŠ¨æè¿°ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºç¨‹åºå½¢å¼ã€‚è¿™äº›ç¨‹åºåŒ…å«äº†æ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„å…¨éƒ¨æ­¥éª¤ï¼ŒåŒ…æ‹¬ä¸€äº›å¸¸è¯†æ€§æ­¥éª¤ï¼Œä»è€Œä¸ºæœºå™¨äººæä¾›äº†æ¸…æ™°çš„æ‰§è¡ŒæŒ‡å—ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘ VirtualHome æ¨¡æ‹Ÿå™¨
æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªåä¸º VirtualHome çš„ 3D æ¨¡æ‹Ÿå™¨ï¼Œå¯ä»¥æ¨¡æ‹Ÿå®¶åº­ç¯å¢ƒä¸­çš„å„ç§æ´»åŠ¨ã€‚é€šè¿‡å°†ç¨‹åºè¾“å…¥åˆ° VirtualHome ä¸­ï¼Œå¯ä»¥ç”Ÿæˆä¸°å¯Œçš„æ´»åŠ¨è§†é¢‘æ•°æ®é›†ï¼Œå¹¶ç”¨äºè®­ç»ƒå’Œæµ‹è¯•è§†é¢‘ç†è§£æ¨¡å‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šä»æ–‡æœ¬å’Œè§†é¢‘ä¸­ç”Ÿæˆç¨‹åº
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäº seq2seq æ¨¡å‹çš„æ–¹æ³•ï¼Œå¯ä»¥ä»è‡ªç„¶è¯­è¨€æè¿°æˆ–è§†é¢‘æ¼”ç¤ºä¸­è‡ªåŠ¨ç”Ÿæˆç¨‹åºã€‚è¿™ä½¿å¾—æœºå™¨äººå¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æˆ–è§†é¢‘æ¼”ç¤ºæ¥å­¦ä¹ æ‰§è¡Œæ–°çš„ä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ VirtualHome æ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä»æ–‡æœ¬å’Œè§†é¢‘ä¸­ç”Ÿæˆçš„ç¨‹åºå¯ä»¥æœ‰æ•ˆåœ°é©±åŠ¨æœºå™¨äººæ‰§è¡Œå„ç§å®¶åº­æ´»åŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†ä¸€é¡¹äººç±»ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„ç¨‹åºä¸äººç±»å¯¹æ´»åŠ¨çš„ç†è§£å…·æœ‰è¾ƒé«˜çš„ç›¸å…³æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ VirtualHome æ¨¡æ‹Ÿå™¨å’Œç¨‹åºç”Ÿæˆæ–¹æ³•ä¸ºæœºå™¨äººæ‰§è¡Œå¤æ‚ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ”¶é›†çš„å®¶åº­æ´»åŠ¨çŸ¥è¯†åº“å’Œè§†é¢‘æ•°æ®é›†ä¹Ÿä¸ºç›¸å…³ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚

## envgen--generating-and-adapting-environments-via-llms-for-training-embodied-agents
### Abstract
Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller RL agents learn useful skills that they are weak at? We propose EnvGen,
a novel framework to address this question. We first prompt an LLM to generate
training environments by giving it the task description and simulator
objectives that the agents should learn and then asking it to generate a set of
environment configurations (e.g., different terrains, items initially given to
agents, etc). Next, we train a small RL agent in a mixture of the original and
LLM-generated environments. Then, we enable the LLM to continuously adapt the
generated environments to progressively improve the skills that the agent is
weak at, by providing feedback to the LLM in the form of the agent's
performance. We demonstrate the usefulness of EnvGen with comprehensive
experiments in Crafter and Heist environments. We find that a small RL agent
trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and
learns long-horizon tasks significantly faster. We also show that using an LLM
to adapt environments dynamically outperforms curriculum learning approaches
and how the environments are adapted to help improve RL agents' weaker skills
over time. Additionally, EnvGen is substantially more efficient as it only uses
a small number of LLM calls (e.g., 4 in total), whereas LLM agents require
thousands of calls. Lastly, we present detailed ablation studies for EnvGen
design choices.
### ğŸŒŸ è®ºæ–‡è§£è¯» | EnvGenï¼šåˆ©ç”¨LLMç”Ÿæˆå’Œé€‚åº”ç¯å¢ƒï¼Œè®­ç»ƒå…·èº«æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å…·èº«æ™ºèƒ½ä½“åœ¨å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­çš„å…´èµ·ï¼Œå¦‚ä½•è®©æ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ å¹¶æŒæ¡å„ç§æŠ€èƒ½æˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å¤„ç†é•¿æ—¶ä»»åŠ¡æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ™ºèƒ½ä½“è™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†è°ƒç”¨æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶EnvGenï¼Œæ—¨åœ¨åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥ç”Ÿæˆå’Œé€‚åº”è®­ç»ƒç¯å¢ƒï¼Œå¸®åŠ©å°å‹RLæ™ºèƒ½ä½“å­¦ä¹ å®ƒä»¬ä¸æ“…é•¿çš„æŠ€èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLMç”Ÿæˆç¯å¢ƒ
EnvGené¦–å…ˆé€šè¿‡å‘LLMæä¾›ä»»åŠ¡æè¿°å’Œæ¨¡æ‹Ÿå™¨ç›®æ ‡ï¼Œè®©LLMç”Ÿæˆä¸€ç³»åˆ—ç¯å¢ƒé…ç½®ï¼Œä¾‹å¦‚ä¸åŒçš„åœ°å½¢ã€åˆå§‹ç‰©å“ç­‰ã€‚è¿™äº›ç¯å¢ƒå¯ä»¥å¹¶è¡Œè®­ç»ƒæ™ºèƒ½ä½“ï¼Œä½¿å…¶å¿«é€Ÿå­¦ä¹ ä¸åŒçš„æŠ€èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLLMé€‚åº”ç¯å¢ƒ
EnvGené€šè¿‡å°†æ™ºèƒ½ä½“åœ¨åŸå§‹ç¯å¢ƒä¸­çš„è¡¨ç°åé¦ˆç»™LLMï¼Œè®©LLMä¸æ–­è°ƒæ•´ç”Ÿæˆçš„ç¯å¢ƒï¼Œä½¿å…¶æ›´åŠ ä¸“æ³¨äºæ™ºèƒ½ä½“ä¸æ“…é•¿çš„æŠ€èƒ½ã€‚è¿™ç§åŠ¨æ€é€‚åº”è¿‡ç¨‹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“é€æ­¥æé«˜å…¶æŠ€èƒ½æ°´å¹³ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Crafterå’ŒHeistæ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨EnvGenè®­ç»ƒçš„å°å‹RLæ™ºèƒ½ä½“åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†åŒ…æ‹¬GPT-4åœ¨å†…çš„SOTAæ–¹æ³•ï¼Œå¹¶ä¸”å­¦ä¹ é•¿æ—¶ä»»åŠ¡çš„é€Ÿåº¦æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼ŒEnvGençš„æ•ˆç‡ä¹Ÿè¿œé«˜äºç›´æ¥ä½¿ç”¨LLMä½œä¸ºæ™ºèƒ½ä½“çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒåªéœ€è¦å¾ˆå°‘çš„LLMè°ƒç”¨æ¬¡æ•°ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
EnvGenæä¾›äº†ä¸€ç§åˆ©ç”¨LLMæ¨ç†èƒ½åŠ›æ¥æé«˜RLæ™ºèƒ½ä½“æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§å¼€æ”¾ä¸–ç•Œæ¸¸æˆå’Œæ¨¡æ‹Ÿå™¨ï¼Œå¸®åŠ©æ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ å¹¶æŒæ¡å„ç§æŠ€èƒ½ã€‚æ­¤å¤–ï¼ŒEnvGençš„åŠ¨æ€é€‚åº”æœºåˆ¶ä¹Ÿä¸ºRLæ™ºèƒ½ä½“çš„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚

## multi-stage-episodic-control-for-strategic-exploration-in-text-games
### Abstract
Text adventure games present unique challenges to reinforcement learning
methods due to their combinatorially large action spaces and sparse rewards.
The interplay of these two factors is particularly demanding because large
action spaces require extensive exploration, while sparse rewards provide
limited feedback. This work proposes to tackle the explore-vs-exploit dilemma
using a multi-stage approach that explicitly disentangles these two strategies
within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins
each episode using an exploitation policy that imitates a set of promising
trajectories from the past, and then switches over to an exploration policy
aimed at discovering novel actions that lead to unseen state spaces. This
policy decomposition allows us to combine global decisions about which parts of
the game space to return to with curiosity-based local exploration in that
space, motivated by how a human may approach these games. Our method
significantly outperforms prior approaches by 27% and 11% average normalized
score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in
both deterministic and stochastic settings, respectively. On the game of Zork1,
in particular, XTX obtains a score of 103, more than a 2x improvement over
prior methods, and pushes past several known bottlenecks in the game that have
plagued previous state-of-the-art methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šé˜¶æ®µç­–ç•¥æ¢ç´¢ï¼šæ–‡æœ¬æ¸¸æˆä¸­çš„å¼ºåŒ–å­¦ä¹ æ–°çªç ´

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬å†’é™©æ¸¸æˆä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›äº†ç‹¬ç‰¹çš„æµ‹è¯•å¹³å°ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™äº›æ¸¸æˆå…·æœ‰ç»„åˆçˆ†ç‚¸å¼çš„åŠ¨ä½œç©ºé—´å’Œç¨€ç–çš„å¥–åŠ±ï¼Œè¿™ä½¿å¾—æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡å˜å¾—å°¤ä¸ºå›°éš¾ã€‚å¤§åŠ¨ä½œç©ºé—´éœ€è¦å¹¿æ³›çš„æ¢ç´¢ï¼Œè€Œç¨€ç–çš„å¥–åŠ±åˆ™æä¾›äº†æœ‰é™çš„åé¦ˆã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å•ä¸€ç­–ç•¥å’ŒåŠ¨ä½œé€‰æ‹©ç­–ç•¥ï¼Œéš¾ä»¥åœ¨æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´æ‰¾åˆ°åˆé€‚çš„å¹³è¡¡ç‚¹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†åä¸º eXploit-Then-eXplore (XTX) çš„å¤šé˜¶æ®µæ§åˆ¶ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ¯ä¸ªå›åˆä¸­æ˜ç¡®åœ°å°†æ¢ç´¢å’Œåˆ©ç”¨ç­–ç•¥åˆ†ç¦»ã€‚XTX ç®—æ³•åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š

* **åˆ©ç”¨é˜¶æ®µ**ï¼šè¯¥é˜¶æ®µä½¿ç”¨ä¸€ä¸ªæ¨¡ä»¿è¿‡å»æˆåŠŸè½¨è¿¹çš„ç­–ç•¥ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè¿”å›åˆ°æ¸¸æˆç©ºé—´ä¸­å·²æ¢ç´¢çš„å‰æ²¿çŠ¶æ€ã€‚
* **æ¢ç´¢é˜¶æ®µ**ï¼šè¯¥é˜¶æ®µä½¿ç”¨ä¸€ä¸ªåŸºäºå¥½å¥‡å¿ƒé©±åŠ¨çš„ç­–ç•¥ï¼Œæ—¨åœ¨å‘ç°æ–°é¢–çš„åŠ¨ä½œï¼Œå¹¶æ¢ç´¢æœªçŸ¥çš„æ¸¸æˆçŠ¶æ€ç©ºé—´ã€‚

è¿™ç§ç­–ç•¥åˆ†è§£å…è®¸æ™ºèƒ½ä½“ç»“åˆå…¨å±€å†³ç­–å’Œå±€éƒ¨æ¢ç´¢ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹ç¨€ç–å¥–åŠ±å’ŒåŠ¨æ€åŠ¨ä½œç©ºé—´çš„æŒ‘æˆ˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ Jericho åŸºå‡†æµ‹è¯•çš„ 12 ä¸ªæ¸¸æˆä¸­ï¼ŒXTX ç®—æ³•åœ¨ç¡®å®šæ€§å’Œéšæœºæ€§è®¾ç½®ä¸‹åˆ†åˆ«æ¯”ç°æœ‰æ–¹æ³•æé«˜äº† 27% å’Œ 11% çš„å¹³å‡å½’ä¸€åŒ–åˆ†æ•°ã€‚ç‰¹åˆ«æ˜¯åœ¨ Zork1 æ¸¸æˆä¸­ï¼ŒXTX ç®—æ³•å–å¾—äº† 103 åˆ†çš„æˆç»©ï¼Œæ¯”ç°æœ‰æ–¹æ³•æé«˜äº† 2 å€ä»¥ä¸Šï¼Œå¹¶å…‹æœäº†æ¸¸æˆä¸­ä¸€äº›å·²çŸ¥çš„ç“¶é¢ˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **å¤šé˜¶æ®µç­–ç•¥**ï¼šå°†æ¢ç´¢å’Œåˆ©ç”¨ç­–ç•¥åˆ†ç¦»ï¼Œå¯ä»¥æ›´å¥½åœ°å¹³è¡¡ä¸¤è€…ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚
* **æ¨¡ä»¿å­¦ä¹ **ï¼šåˆ©ç”¨è¿‡å»æˆåŠŸçš„ç»éªŒæ¥æŒ‡å¯¼æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œå¯ä»¥åŠ å¿«å­¦ä¹ é€Ÿåº¦ã€‚
* **å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢**ï¼šé€šè¿‡å¥–åŠ±æ–°é¢–çš„åŠ¨ä½œï¼Œå¯ä»¥é¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢æœªçŸ¥çš„æ¸¸æˆçŠ¶æ€ç©ºé—´ã€‚
* **æ··åˆç­–ç•¥**ï¼šä½¿ç”¨æ··åˆç­–ç•¥å¯ä»¥æä¾›æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ä¸åŒçš„æ¸¸æˆç¯å¢ƒã€‚

### ğŸŒŸ æ€»ç»“
XTX ç®—æ³•ä¸ºæ–‡æœ¬å†’é™©æ¸¸æˆä¸­çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œé€šè¿‡å¤šé˜¶æ®µç­–ç•¥å’Œæ··åˆç­–ç•¥ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œå¹¶åœ¨å®é™…æ¸¸æˆä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

## pre-trained-language-models-as-prior-knowledge-for-playing-text-based-games
### Abstract
Recently, text world games have been proposed to enable artificial agents to
understand and reason about real-world scenarios. These text-based games are
challenging for artificial agents, as it requires an understanding of and
interaction using natural language in a partially observable environment.
Agents observe the environment via textual descriptions designed to be
challenging enough for even human players. Past approaches have not paid enough
attention to the language understanding capability of the proposed agents.
Typically, these approaches train from scratch, an agent that learns both
textual representations and the gameplay online during training using a
temporal loss function. Given the sample-inefficiency of RL approaches, it is
inefficient to learn rich enough textual representations to be able to
understand and reason using the textual observation in such a complicated game
environment setting. In this paper, we improve the semantic understanding of
the agent by proposing a simple RL with LM framework where we use
transformer-based language models with Deep RL models. We perform a detailed
study of our framework to demonstrate how our model outperforms all existing
agents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6
higher than the state-of-the-art model. Overall, our proposed approach
outperforms 4 games out of the 14 text-based games, while performing comparable
to the state-of-the-art models on the remaining games.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æå‡æ–‡æœ¬æ¸¸æˆä¸­çš„æ™ºèƒ½ä½“è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬ä¸–ç•Œæ¸¸æˆä¸ºäººå·¥æ™ºèƒ½ä½“æä¾›äº†ç†è§£å’Œæ¨ç†ç°å®ä¸–ç•Œåœºæ™¯çš„æœºä¼šã€‚ç„¶è€Œï¼Œè¿™äº›æ¸¸æˆå¯¹æ™ºèƒ½ä½“æ¥è¯´æå…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­ç†è§£å’Œäº¤äº’è‡ªç„¶è¯­è¨€ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†æ™ºèƒ½ä½“çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸”é€šå¸¸ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¯¼è‡´æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥æå‡æ™ºèƒ½ä½“çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä»è€Œåœ¨æ–‡æœ¬æ¸¸æˆä¸­å–å¾—æ›´å¥½çš„è¡¨ç°ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„RLä¸LMæ¡†æ¶ï¼Œä½¿ç”¨åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚DistilBERTï¼‰ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ç›¸ç»“åˆã€‚é€šè¿‡åœ¨å¤§å‹é€šç”¨è‹±è¯­è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åé’ˆå¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¸ºæ™ºèƒ½ä½“æä¾›ä¸°å¯Œçš„è¯­è¨€ç†è§£å’Œå…ˆéªŒçŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¸¸æˆæ„ŸçŸ¥çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
ä¸ºäº†ä½¿é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ›´å¥½åœ°é€‚åº”æ¸¸æˆç¯å¢ƒï¼Œæœ¬æ–‡ä½¿ç”¨ç‹¬ç«‹çš„äººç±»æ¸¸æˆæ’­æ”¾è½¨è¿¹æ•°æ®é›†å¯¹DistilBERTè¿›è¡Œå¾®è°ƒï¼Œä»è€Œä½¿å…¶å…·å¤‡æ¸¸æˆæ„ŸçŸ¥èƒ½åŠ›ã€‚è¿™ç§å¾®è°ƒè¿‡ç¨‹æœ‰åŠ©äºå°†è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œä¸–ç•Œæ„ŸçŸ¥èƒ½åŠ›è½¬ç§»åˆ°ä¸åŒçš„æ¸¸æˆå’Œæ™ºèƒ½ä½“ä¸­ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨14ä¸ªæ–‡æœ¬æ¸¸æˆä¸­å¯¹æ‰€æå‡ºçš„æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼Œåœ¨Zork1æ¸¸æˆä¸­ï¼Œæ¨¡å‹å–å¾—äº†44.7åˆ†çš„æˆç»©ï¼Œæ¯”ç°æœ‰æœ€ä½³æ¨¡å‹é«˜å‡º1.6åˆ†ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶åœ¨4ä¸ªæ¸¸æˆä¸­è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨å…¶ä»–æ¸¸æˆä¸­è¡¨ç°ä¸ç°æœ‰æ¨¡å‹ç›¸å½“ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºå…ˆéªŒçŸ¥è¯†çš„æ–¹æ³•ï¼Œä¸ºæ–‡æœ¬æ¸¸æˆä¸­çš„æ™ºèƒ½ä½“è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œä¸–ç•Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œæ¨ç†æ¸¸æˆç¯å¢ƒï¼Œä»è€Œåœ¨æ¸¸æˆä¸­å–å¾—æ›´å¥½çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼ºè°ƒäº†å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥é€‚åº”ç‰¹å®šæ¸¸æˆç¯å¢ƒçš„é‡è¦æ€§ï¼Œè¿™ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å¯ç¤ºã€‚

## guiding-pretraining-in-reinforcement-learning-with-large-language-models
### Abstract
Reinforcement learning algorithms typically struggle in the absence of a
dense, well-shaped reward function. Intrinsically motivated exploration methods
address this limitation by rewarding agents for visiting novel states or
transitions, but these methods offer limited benefits in large environments
where most discovered novelty is irrelevant for downstream tasks. We describe a
method that uses background knowledge from text corpora to shape exploration.
This method, called ELLM (Exploring with LLMs) rewards an agent for achieving
goals suggested by a language model prompted with a description of the agent's
current state. By leveraging large-scale language model pretraining, ELLM
guides agents toward human-meaningful and plausibly useful behaviors without
requiring a human in the loop. We evaluate ELLM in the Crafter game environment
and the Housekeep robotic simulator, showing that ELLM-trained agents have
better coverage of common-sense behaviors during pretraining and usually match
or improve performance on a range of downstream tasks. Code available at
https://github.com/yuqingd/ellm.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„é¢„è®­ç»ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ç¼ºä¹å¯†é›†ã€è‰¯å¥½å½¢çŠ¶çš„å¥–åŠ±å‡½æ•°æ—¶é€šå¸¸ä¼šé‡åˆ°å›°éš¾ã€‚å†…åœ¨åŠ¨æœºæ¢ç´¢æ–¹æ³•é€šè¿‡å¥–åŠ±ä»£ç†è®¿é—®æ–°é¢–çŠ¶æ€æˆ–è½¬æ¢æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œä½†åœ¨å¤§å¤šæ•°å‘ç°çš„æ–°é¢–æ€§å¯¹ä¸‹æ¸¸ä»»åŠ¡æ— å…³ç´§è¦çš„å¤§å‹ç¯å¢ƒä¸­ï¼Œè¿™äº›æ–¹æ³•æä¾›çš„ç›Šå¤„æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ¥è‡ªæ–‡æœ¬è¯­æ–™åº“çš„èƒŒæ™¯çŸ¥è¯†æ¥å¡‘é€ æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºELLMï¼ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¢ç´¢ï¼‰ï¼Œå®ƒå¥–åŠ±ä»£ç†å®ç°ç”±è¯­è¨€æ¨¡å‹æå‡ºçš„ä¸ä»£ç†å½“å‰çŠ¶æ€æè¿°ç›¸å…³çš„ç›®æ ‡ã€‚é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼ŒELLMå¼•å¯¼ä»£ç†æœç€äººç±»æœ‰æ„ä¹‰ä¸”å¯èƒ½æœ‰ç”¨çš„è¡Œä¸ºå‘å±•ï¼Œè€Œæ— éœ€äººå·¥å¹²é¢„ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒŒæ™¯çŸ¥è¯†æ¥å¡‘é€ æ¢ç´¢ã€‚LLMæ˜¯æ¦‚ç‡æ–‡æœ¬æ¨¡å‹ï¼Œå…¶é¢„æµ‹ç¼–ç äº†ä¸°å¯Œçš„å…³äºäººç±»å¸¸è¯†çŸ¥è¯†å’Œæ–‡åŒ–ä¹ ä¿—çš„ä¿¡æ¯ã€‚ELLMé€šè¿‡æŸ¥è¯¢LLMæ¥è·å–å¯èƒ½çš„ç›®æ ‡ï¼Œå¹¶å¥–åŠ±ä»£ç†å®ç°è¿™äº›å»ºè®®ï¼Œä»è€Œå¼•å¯¼æ¢ç´¢æœç€å®Œæˆå¤šæ ·åŒ–ã€ä¸Šä¸‹æ–‡æ•æ„Ÿå’Œäººç±»æœ‰æ„ä¹‰çš„ç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨LLMç”Ÿæˆçš„ç›®æ ‡ä½œä¸ºå†…åœ¨å¥–åŠ±å‡½æ•°ã€‚ELLMé€šè¿‡æµ‹é‡LLMç”Ÿæˆçš„ç›®æ ‡ä¸ç¯å¢ƒä¸­ä»£ç†è½¬æ¢çš„æè¿°ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥è®¡ç®—å¥–åŠ±ã€‚å½“è½¬æ¢çš„æè¿°ä¸ç›®æ ‡æè¿°è¶³å¤Ÿæ¥è¿‘æ—¶ï¼Œä»£ç†å°†è·å¾—ä¸ç›¸ä¼¼åº¦æˆæ¯”ä¾‹çš„å¥–åŠ±ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨Crafteræ¸¸æˆç¯å¢ƒå’ŒHousekeepæœºå™¨äººæ¨¡æ‹Ÿå™¨ä¸­è¯„ä¼°äº†ELLMã€‚ç»“æœè¡¨æ˜ï¼ŒELLMè®­ç»ƒçš„ä»£ç†åœ¨é¢„è®­ç»ƒæœŸé—´å¯¹å¸¸è¯†è¡Œä¸ºçš„è¦†ç›–èŒƒå›´æ›´å¥½ï¼Œå¹¶ä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½é€šå¸¸ä¸åŸºçº¿ç›¸å½“æˆ–æœ‰æ‰€æé«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥ç”¨äºå¼•å¯¼å¼ºåŒ–å­¦ä¹ ä»£ç†åœ¨ç¼ºä¹å¤–éƒ¨å®šä¹‰çš„å¥–åŠ±çš„æƒ…å†µä¸‹å­¦ä¹ æœ‰ç”¨çš„è¡Œä¸ºã€‚é€šè¿‡åˆ©ç”¨LLMçš„èƒŒæ™¯çŸ¥è¯†ï¼ŒELLMå¯ä»¥å¼•å¯¼ä»£ç†æœç€äººç±»æœ‰æ„ä¹‰ä¸”å¯èƒ½æœ‰ç”¨çš„è¡Œä¸ºå‘å±•ï¼Œä»è€Œæé«˜å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†LLMæ€§èƒ½å¯¹æç¤ºé€‰æ‹©ã€çŠ¶æ€å’Œè½¬æ¢æè¿°çš„æ•æ„Ÿæ€§ï¼Œå¹¶æå‡ºäº†æ”¹è¿›LLMæ€§èƒ½çš„æ½œåœ¨æ–¹æ³•ã€‚

## pokergpt--an-end-to-end-lightweight-solver-for-multi-player-texas-hold-em-via-large-language-model
### Abstract
Poker, also known as Texas Hold'em, has always been a typical research target
within imperfect information games (IIGs). IIGs have long served as a measure
of artificial intelligence (AI) development. Representative prior works, such
as DeepStack and Libratus heavily rely on counterfactual regret minimization
(CFR) to tackle heads-up no-limit Poker. However, it is challenging for
subsequent researchers to learn CFR from previous models and apply it to other
real-world applications due to the expensive computational cost of CFR
iterations. Additionally, CFR is difficult to apply to multi-player games due
to the exponential growth of the game tree size. In this work, we introduce
PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number
of players and gaining high win rates, established on a lightweight large
language model (LLM). PokerGPT only requires simple textual information of
Poker games for generating decision-making advice, thus guaranteeing the
convenient interaction between AI and humans. We mainly transform a set of
textual records acquired from real games into prompts, and use them to
fine-tune a lightweight pre-trained LLM using reinforcement learning human
feedback technique. To improve fine-tuning performance, we conduct prompt
engineering on raw data, including filtering useful information, selecting
behaviors of players with high win rates, and further processing them into
textual instruction using multiple prompt engineering techniques. Through the
experiments, we demonstrate that PokerGPT outperforms previous approaches in
terms of win rate, model size, training time, and response speed, indicating
the great potential of LLMs in solving IIGs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | PokerGPTï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è½»é‡çº§å¤šç©å®¶å¾·å·æ‰‘å…‹è§£å†³æ–¹æ¡ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¾·å·æ‰‘å…‹ä½œä¸ºä¸€ç§å…¸å‹çš„éå®Œç¾ä¿¡æ¯æ¸¸æˆï¼ˆIIGï¼‰ï¼Œä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶çš„é‡è¦ç›®æ ‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚DeepStackå’ŒLibratusï¼Œä¸»è¦ä¾èµ–äºåäº‹å®åæ‚”æœ€å°åŒ–ï¼ˆCFRï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è®¡ç®—æˆæœ¬å’Œæ‰©å±•æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚CFRç®—æ³•çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥åº”ç”¨äºå¤šç©å®¶æ¸¸æˆï¼Œä¸”éš¾ä»¥ä»ç°æœ‰æ¨¡å‹ä¸­å­¦ä¹ å¹¶åº”ç”¨äºå…¶ä»–ç°å®ä¸–ç•Œåº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†PokerGPTï¼Œä¸€ç§åŸºäºè½»é‡çº§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«¯åˆ°ç«¯å¾·å·æ‰‘å…‹è§£å†³æ–¹æ¡ˆã€‚PokerGPTé€šè¿‡ä»¥ä¸‹åˆ›æ–°ç‚¹å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶
PokerGPTé‡‡ç”¨ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ï¼Œé¿å…äº†å¤æ‚çš„ç‰¹å¾å·¥ç¨‹å’Œä¸­é—´æ­¥éª¤ã€‚å®ƒä»…éœ€è¦ç®€å•çš„æ–‡æœ¬ä¿¡æ¯å³å¯ç”Ÿæˆå†³ç­–å»ºè®®ï¼Œå®ç°äº†äººæœºäº¤äº’çš„ä¾¿æ·æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè½»é‡çº§LLM
PokerGPTåŸºäºè½»é‡çº§LLMï¼Œå…·æœ‰æ›´å°‘çš„å‚æ•°å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´ä¹Ÿæ›´çŸ­ï¼Œå®ç°äº†èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šé«˜æ•ˆçš„æ•°æ®å¤„ç†
PokerGPTé‡‡ç”¨æ•°æ®æ¸…æ´—å’Œæç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå°†çœŸå®æ¸¸æˆæ•°æ®è½¬æ¢ä¸ºå¯ç†è§£çš„æ–‡æœ¬æç¤ºï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆæŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒPokerGPTåœ¨èƒœç‡ã€æ¨¡å‹å¤§å°ã€è®­ç»ƒæ—¶é—´å’Œå“åº”é€Ÿåº¦ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒPokerGPTèƒ½å¤Ÿå¤„ç†ä»»æ„æ•°é‡çš„ç©å®¶ï¼Œå¹¶å±•ç°å‡ºå‡ºè‰²çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
PokerGPTçš„æˆåŠŸè¡¨æ˜ï¼ŒLLMåœ¨è§£å†³IIGæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å…¶ç«¯åˆ°ç«¯å­¦ä¹ æ¡†æ¶ã€è½»é‡çº§æ¨¡å‹å’Œé«˜æ•ˆçš„æ•°æ®å¤„ç†æŠ€æœ¯ä¸ºå…¶ä»–IIGç ”ç©¶æä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒã€‚æ­¤å¤–ï¼ŒPokerGPTçš„äº¤äº’å¼ç‰¹æ€§ä½¿å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚

## enhance-reasoning-for-large-language-models-in-the-game-werewolf
### Abstract
This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ï¼šä»¥ç‹¼äººæ€æ¸¸æˆä¸ºæ¡ˆä¾‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šçš„çªç ´ï¼Œå…¶åœ¨æ¨ç†ã€è§„åˆ’å’Œå†³ç­–ç­‰é¢†åŸŸçš„æ½œåŠ›ä¹Ÿé€æ¸æ˜¾ç°ã€‚ç„¶è€Œï¼ŒLLMsåœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¢†åŸŸç‰¹å®šçŸ¥è¯†å’Œæ·±åº¦é€»è¾‘åˆ†æçš„ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥å¤–éƒ¨æ¨ç†æ¨¡å—ï¼Œå³â€œæ€è€ƒè€…â€ï¼ˆThinkerï¼‰ï¼Œæ¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŒç³»ç»Ÿæ¨ç†æ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œå°†LLMsä¸å¤–éƒ¨Thinkeræ¨¡å—ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªæ¨ç†å±‚æ¬¡ç»“æ„ã€‚LLMsè´Ÿè´£å¤„ç†ç›´è§‚çš„System-1ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¸¸è¯†æ¨ç†ï¼Œè€ŒThinkeråˆ™ä¸“æ³¨äºéœ€è¦å¤æ‚é€»è¾‘åˆ†æå’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†çš„System-2ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šThinkeræ¨¡å—çš„è®¾è®¡ä¸è®­ç»ƒ
Thinkeræ¨¡å—ç›´æ¥ä»æ•°æ®åº“ä¸­è·å–çŸ¥è¯†ï¼Œå¹¶é‡‡ç”¨å„ç§ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚å®ƒé€šè¿‡æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’ŒåŸºäºç¾¤ä½“çš„è®­ç»ƒç­‰æ–¹æ³•ï¼Œå­¦ä¹ ç”Ÿæˆåˆç†çš„æ¸¸æˆåŠ¨ä½œå’ŒLLMçš„è¯­éŸ³æŒ‡ä»¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•°æ®é›†è´¡çŒ®
æœ¬æ–‡æ”¶é›†äº†18,800åœºçœŸå®äººç±»æ¸¸æˆä¼šè¯æ•°æ®ï¼Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç¤¾äº¤æ¨ç†æ¸¸æˆæ•°æ®é›†ï¼Œä¸ºç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥Thinkeræ¨¡å—æ˜¾è‘—æé«˜äº†LLMsçš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚åœ¨ç‹¼äººæ€æ¸¸æˆä¸­ï¼ŒThinkeræ¨¡å—åœ¨æ¨ç†ã€è¯­éŸ³ç”Ÿæˆå’Œåœ¨çº¿æ¸¸æˆè¯„ä¼°æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†Thinkerä¸ä¸€ä¸ªè¾ƒå°çš„LLMæ¨¡å‹ï¼ˆ6Bï¼‰è¿›è¡Œå¾®è°ƒï¼Œå…¶æ€§èƒ½ç”šè‡³è¶…è¿‡äº†GPT4ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡å°†LLMsä¸å¤–éƒ¨æ¨ç†æ¨¡å—ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆæå‡LLMsåœ¨ç‰¹å®šé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æ›´å¤šå®é™…åº”ç”¨ä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†ä¹Ÿä¸ºç¤¾äº¤æ¨ç†æ¸¸æˆçš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®åŸºç¡€ã€‚

## adapt--as-needed-decomposition-and-planning-with-language-models
### Abstract
Large Language Models (LLMs) are increasingly being used for interactive
decision-making tasks requiring planning and adapting to the environment.
Recent works employ LLMs-as-agents in broadly two ways: iteratively determining
the next action (iterative executors) or generating plans and executing
sub-tasks using LLMs (plan-and-execute). However, these methods struggle with
task complexity, as the inability to execute any sub-task may lead to task
failure. To address these shortcomings, we introduce As-Needed Decomposition
and Planning for complex Tasks (ADaPT), an approach that explicitly plans and
decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute
them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity
and LLM capability. Our results demonstrate that ADaPT substantially
outperforms established strong baselines, achieving success rates up to 28.3%
higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel
compositional dataset that we introduce. Through extensive analysis, we
illustrate the importance of multilevel decomposition and establish that ADaPT
dynamically adjusts to the capabilities of the executor LLM as well as to task
complexity.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ADaPTï¼šæŒ‰éœ€åˆ†è§£ä¸è§„åˆ’ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬ä¹Ÿé€æ¸è¢«åº”ç”¨äºéœ€è¦è§„åˆ’å’Œé€‚åº”ç¯å¢ƒçš„äº¤äº’å¼å†³ç­–ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºLLMsåœ¨æ‰§è¡Œå­ä»»åŠ¡æ—¶å¯èƒ½ä¼šå¤±è´¥ï¼Œä»è€Œå¯¼è‡´æ•´ä¸ªä»»åŠ¡çš„å¤±è´¥ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ADaPTï¼ˆAs-Needed Decomposition and Planning for complex Tasksï¼‰ï¼Œä¸€ç§æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’å¤æ‚ä»»åŠ¡çš„æ–¹æ³•ã€‚ADaPTçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå°†å…¶åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è¿›è¡Œåˆ†è§£ï¼Œä»¥é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæŒ‰éœ€åˆ†è§£
ADaPTé€šè¿‡é€’å½’åœ°åˆ†è§£å­ä»»åŠ¡ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ã€‚å½“LLMä½œä¸ºæ‰§è¡Œè€…æ— æ³•æ‰§è¡Œå­ä»»åŠ¡æ—¶ï¼Œå®ƒä¼šè°ƒç”¨LLMä½œä¸ºè§„åˆ’è€…æ¥ç”Ÿæˆæ›´å°çš„å­ä»»åŠ¡ï¼Œå¹¶é€’å½’åœ°è°ƒç”¨ADaPTæ¥æ‰§è¡Œè¿™äº›å­ä»»åŠ¡ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šçº§åˆ†è§£
ADaPTæ”¯æŒå¤šçº§åˆ†è§£ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥è¿›ä¸€æ­¥åˆ†è§£å­ä»»åŠ¡ï¼Œç›´åˆ°å®ƒä»¬å˜å¾—è¶³å¤Ÿç®€å•ï¼Œå¯ä»¥è¢«LLMä½œä¸ºæ‰§è¡Œè€…æˆåŠŸæ‰§è¡Œã€‚è¿™ç§å¤šçº§åˆ†è§£çš„èƒ½åŠ›ä½¿å¾—ADaPTèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ALFWorldã€WebShopå’ŒTextCraftä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒADaPTæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ–¹æ³•ï¼Œåœ¨ALFWorldä¸Šæé«˜äº†28.3%çš„æˆåŠŸç‡ï¼Œåœ¨WebShopä¸Šæé«˜äº†27%ï¼Œåœ¨TextCraftä¸Šæé«˜äº†33%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
ADaPTæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å¤„ç†LLMsåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ‰§è¡Œå¤±è´¥é—®é¢˜ã€‚å®ƒé€šè¿‡æŒ‰éœ€åˆ†è§£å’Œè§„åˆ’ï¼ŒåŠ¨æ€åœ°é€‚åº”ä»»åŠ¡çš„å¤æ‚æ€§å’ŒLLMçš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†ä»»åŠ¡çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼ŒADaPTçš„å¤šçº§åˆ†è§£èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¹¶æé«˜ä»»åŠ¡çš„æˆåŠŸç‡ã€‚

## cooperation-on-the-fly--exploring-language-agents-for-ad-hoc-teamwork-in-the-avalon-game
### Abstract
Multi-agent collaboration with Large Language Models (LLMs) demonstrates
proficiency in basic tasks, yet its efficiency in more complex scenarios
remains unexplored. In gaming environments, these agents often face situations
without established coordination protocols, requiring them to make intelligent
inferences about teammates from limited data. This problem motivates the area
of ad hoc teamwork, in which an agent may potentially cooperate with a variety
of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork
problem where the agent operates in an environment driven by natural language.
Our findings reveal the potential of LLM agents in team collaboration,
highlighting issues related to hallucinations in communication. To address this
issue, we develop CodeAct, a general agent that equips LLM with enhanced memory
and code-driven reasoning, enabling the repurposing of partial information for
rapid adaptation to new teammates.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¸æ–­æå‡ï¼Œå®ƒä»¬åœ¨æ„å»ºè‡ªä¸»ä»£ç†å’Œæ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ™ºèƒ½ä½“åä½œä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰é¢„å…ˆè®¾å®šçš„åè°ƒåè®®çš„åŠ¨æ€ç¯å¢ƒä¸­ï¼ŒLLMsçš„åä½œæ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ˜ç¡®å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä¸ä¸åŒçš„é˜Ÿå‹è¿›è¡Œæœ‰æ•ˆåˆä½œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥AvalonPlayåŸºå‡†
æœ¬æ–‡æå‡ºäº†AvalonPlayåŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€çš„å¤šæ™ºèƒ½ä½“å¹³å°ï¼Œç”¨äºæ¨¡æ‹ŸåŠ¨æ€ç¯å¢ƒä¸­çš„åä½œä»»åŠ¡ã€‚åœ¨è¿™ä¸ªåŸºå‡†ä¸­ï¼Œæ™ºèƒ½ä½“éœ€è¦åœ¨æœ‰é™çš„ä¿¡æ¯å’Œæ²¡æœ‰é¢„å…ˆè®¾å®šçš„å›¢é˜Ÿç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è§‚å¯Ÿé˜Ÿå‹çš„è¡Œä¸ºæ¥æ¨æ–­ä»–ä»¬çš„è§’è‰²ï¼Œå¹¶åŠ¨æ€è°ƒæ•´å›¢é˜Ÿç­–ç•¥ä»¥å®ç°å…±åŒç›®æ ‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¼€å‘CodeActæ™ºèƒ½ä½“
ä¸ºäº†è§£å†³LLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­åä½œæ—¶å¯èƒ½å‡ºç°çš„è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CodeActæ™ºèƒ½ä½“ã€‚CodeActåˆ©ç”¨LLMsçš„ä»£ç é©±åŠ¨æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å°†å¤æ‚çš„è¯­ä¹‰ä»»åŠ¡è½¬åŒ–ä¸ºçµæ´»çš„ä»£ç ç»“æ„ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œæ•ˆç‡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4æ¨¡å‹åœ¨AvalonPlayåŸºå‡†ä¸­è¡¨ç°å‡ºæœ€ä½³çš„åä½œèƒ½åŠ›ï¼Œè€ŒCodeActæ™ºèƒ½ä½“åœ¨å›¢é˜Ÿé€‰æ‹©å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–è¯­ä¹‰æ¨ç†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜å‘ç°ï¼Œå¼•å…¥è‡ªç„¶è¯­è¨€é€šä¿¡åè®®å¹¶ä¸æ€»æ˜¯èƒ½æ˜¾è‘—æé«˜LLMsçš„åä½œæ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åä½œèƒ½åŠ›ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚è®°å¿†é—å¿˜å’Œå¹»è§‰ç”Ÿæˆã€‚ä¸ºäº†æé«˜LLMsçš„åä½œæ•ˆç‡ï¼Œå¯ä»¥å€Ÿé‰´æœ¬æ–‡æå‡ºçš„CodeActæ™ºèƒ½ä½“çš„è®¾è®¡æ€è·¯ï¼Œåˆ©ç”¨ä»£ç é©±åŠ¨æ¨ç†å’Œè®°å¿†æ£€ç´¢ç³»ç»Ÿæ¥å¢å¼ºæ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›å’Œä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•å‡å°‘å¹»è§‰ç”Ÿæˆçš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚

## counting-to-explore-and-generalize-in-text-based-games
### Abstract
We propose a recurrent RL agent with an episodic exploration mechanism that
helps discovering good policies in text-based game environments. We show
promising results on a set of generated text-based games of varying difficulty
where the goal is to collect a coin located at the end of a chain of rooms. In
contrast to previous text-based RL approaches, we observe that our agent learns
policies that generalize to unseen games of greater difficulty.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºè®¡æ•°æ¢ç´¢å’Œæ³›åŒ–çš„æ–‡æœ¬æ¸¸æˆå¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
æ–‡æœ¬æ¸¸æˆæ˜¯ä¸€ç§å¤æ‚çš„äº¤äº’å¼æ¨¡æ‹Ÿï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¸¸æˆçŠ¶æ€ã€æ¥å—ç©å®¶åŠ¨ä½œå¹¶æŠ¥å‘Šç¯å¢ƒå˜åŒ–ã€‚ç©å®¶å¿…é¡»é€šè¿‡æ¢ç´¢æ¥å‘ç°æ¸¸æˆç›®æ ‡ï¼Œè€Œæ¸¸æˆä¸­çš„è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´éƒ½æ˜¯ç»„åˆå’Œå¤åˆçš„ï¼Œç©å®¶å¿…é¡»åº”å¯¹éƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œå› ä¸ºæè¿°æ€§æ–‡æœ¬å¹¶ä¸æä¾›å…³äºåº•å±‚æ¸¸æˆçŠ¶æ€çš„å®Œæ•´ã€æ˜ç¡®çš„ä¿¡æ¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºäº†ä¸€ç§åŸºäºå¾ªç¯çš„å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œå…·æœ‰æƒ…èŠ‚æ¢ç´¢æœºåˆ¶ï¼Œæœ‰åŠ©äºåœ¨åŸºäºæ–‡æœ¬çš„æ¸¸æˆç¯å¢ƒä¸­å‘ç°è‰¯å¥½çš„ç­–ç•¥ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæå‡ºäº†ä¸€ç§æƒ…èŠ‚è®¡æ•°æ¢ç´¢æ–¹æ¡ˆï¼Œå…¶ä¸­çŠ¶æ€è®¡æ•°åœ¨æ¯ä¸ªæƒ…èŠ‚å¼€å§‹æ—¶é‡ç½®ã€‚è¿™ç§å¥–åŠ±å……å½“æƒ…èŠ‚è®°å¿†ï¼Œæ¨åŠ¨ä»£ç†è®¿é—®å½“å‰æƒ…èŠ‚ä¸­å°šæœªé‡åˆ°çš„çŠ¶æ€ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸€ç³»åˆ—ç”Ÿæˆçš„åŸºäºæ–‡æœ¬çš„æ¸¸æˆä¸­ï¼Œè¯¥ä»£ç†åœ¨æ”¶é›†ä½äºä¸€ç³»åˆ—æˆ¿é—´æœ«å°¾çš„ç¡¬å¸çš„ç›®æ ‡ä¸Šå–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœã€‚ä¸ä¹‹å‰çš„åŸºäºæ–‡æœ¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè§‚å¯Ÿåˆ°è¯¥ä»£ç†å­¦ä¹ çš„ç­–ç•¥å¯ä»¥æ³›åŒ–åˆ°æœªè§çš„æ›´å…·æŒ‘æˆ˜æ€§çš„æ¸¸æˆã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
è¯¥è®ºæ–‡æå‡ºçš„åŸºäºè®¡æ•°æ¢ç´¢å’Œæ³›åŒ–çš„æ–‡æœ¬æ¸¸æˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¸ºè§£å†³æ–‡æœ¬æ¸¸æˆä¸­çš„æ¢ç´¢å’Œæ³›åŒ–é—®é¢˜æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦æ¢ç´¢å’Œè®°å¿†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚è¿·å®«æ¢ç´¢ã€è·¯å¾„è§„åˆ’ç­‰ã€‚

## suspicion-agent--playing-imperfect-information-games-with-theory-of-mind-aware-gpt-4
### Abstract
Unlike perfect information games, where all elements are known to every
player, imperfect information games emulate the real-world complexities of
decision-making under uncertain or incomplete information. GPT-4, the recent
breakthrough in large language models (LLMs) trained on massive passive data,
is notable for its knowledge retrieval and reasoning abilities. This paper
delves into the applicability of GPT-4's learned knowledge for imperfect
information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an
innovative agent that leverages GPT-4's capabilities for performing in
imperfect information games. With proper prompt engineering to achieve
different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable
adaptability across a range of imperfect information card games. Importantly,
GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it
can understand others and intentionally impact others' behavior. Leveraging
this, we design a planning strategy that enables GPT-4 to competently play
against different opponents, adapting its gameplay style as needed, while
requiring only the game rules and descriptions of observations as input. In the
experiments, we qualitatively showcase the capabilities of Suspicion-Agent
across three different imperfect information games and then quantitatively
evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can
potentially outperform traditional algorithms designed for imperfect
information games, without any specialized training or examples. In order to
encourage and foster deeper insights within the community, we make our
game-related data publicly available.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€èƒ½åŠ›ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå†³ç­–å¾€å¾€æ˜¯åœ¨ä¿¡æ¯ä¸å®Œæ•´æˆ–ä¸ç¡®å®šçš„æƒ…å†µä¸‹è¿›è¡Œçš„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„AIç®—æ³•éƒ½æ˜¯åœ¨å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è®­ç»ƒçš„ï¼Œå³æ‰€æœ‰ç©å®¶éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰ä¿¡æ¯ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç´¢å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ¥å¤„ç†ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆï¼Œä»è€Œæ›´å¥½åœ°æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å†³ç­–è¿‡ç¨‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSuspicion-Agentçš„åˆ›æ–°å‹è‡ªä¸»ä»£ç†ï¼Œå®ƒåŸºäºGPT-4ï¼Œå¹¶åˆ©ç”¨å…¶å¼ºå¤§çš„çŸ¥è¯†æ£€ç´¢å’Œæ¨ç†èƒ½åŠ›æ¥ç©ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆã€‚Suspicion-Agentçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåˆ©ç”¨GPT-4çš„â€œå¿ƒæ™ºç†è®ºâ€ï¼ˆToMï¼‰èƒ½åŠ›ï¼Œå³ç†è§£ä»–äººå¹¶æœ‰æ„å½±å“ä»–äººè¡Œä¸ºçš„èƒ½åŠ›ã€‚è¿™ä½¿å¾—Suspicion-Agentèƒ½å¤Ÿé¢„æµ‹å¯¹æ‰‹çš„è¡Œä¸ºï¼Œå¹¶æ ¹æ®å¯¹æ‰‹çš„è¡Œä¸ºè°ƒæ•´è‡ªå·±çš„ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå°†æ¸¸æˆè¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªå­æ¨¡å—ï¼Œå¦‚è§‚å¯Ÿè§£é‡Šå™¨ã€æ¸¸æˆæ¨¡å¼åˆ†æå’Œè§„åˆ’æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½ä½¿ç”¨ä¸åŒçš„æç¤ºæ¥å¼•å¯¼GPT-4æ‰§è¡Œç‰¹å®šçš„åŠŸèƒ½ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å®éªŒä¸­ï¼ŒSuspicion-Agentåœ¨ä¸‰ä¸ªä¸åŒçš„ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­å±•ç¤ºäº†å…¶èƒ½åŠ›ï¼Œå¹¶åœ¨Leduc Hold'emæ¸¸æˆä¸­è¿›è¡Œäº†å®šé‡è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSuspicion-Agentå¯ä»¥æ½œåœ¨åœ°è¶…è¶Šä¼ ç»Ÿç®—æ³•ï¼Œè€Œæ— éœ€ä»»ä½•ä¸“é—¨çš„è®­ç»ƒæˆ–ç¤ºä¾‹ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Suspicion-Agentæ¡†æ¶ä¸ºåˆ©ç”¨LLMåœ¨ä¸å®Œç¾ä¿¡æ¯æ¸¸æˆä¸­è¿›è¡Œå†³ç­–æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†LLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸ToMèƒ½åŠ›ç›¸ç»“åˆï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å†³ç­–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å…¬å¼€äº†æ‰€æœ‰ä¸æ¸¸æˆç›¸å…³çš„æ•°æ®ï¼Œè¿™å°†æœ‰åŠ©äºç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£LLMçš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘æ›´æœ‰æ•ˆçš„æ¨¡å‹ã€‚

## enhancing-agent-learning-through-world-dynamics-modeling
### Abstract
Large language models (LLMs) have been increasingly applied to tasks in
language understanding and interactive decision-making, with their impressive
performance largely attributed to the extensive domain knowledge embedded
within them. However, the depth and breadth of this knowledge can vary across
domains. Many existing approaches assume that LLMs possess a comprehensive
understanding of their environment, often overlooking potential gaps in their
grasp of actual world dynamics. To address this, we introduce Discover, Verify,
and Evolve (DiVE), a framework that discovers world dynamics from a small
number of demonstrations, verifies the accuracy of these dynamics, and evolves
new, advanced dynamics tailored to the current situation. Through extensive
evaluations, we assess the impact of each component on performance and compare
the dynamics generated by DiVE to human-annotated dynamics. Our results show
that LLMs guided by DiVE make more informed decisions, achieving rewards
comparable to human players in the Crafter environment and surpassing methods
that require prior task-specific training in the MiniHack environment.
### ğŸŒŸ è®ºæ–‡è§£è¯» | é€šè¿‡ä¸–ç•ŒåŠ¨æ€å»ºæ¨¡å¢å¼ºæ™ºèƒ½ä½“å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ç†è§£å’Œäº¤äº’å¼å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿™ä¸»è¦å½’åŠŸäºå®ƒä»¬åµŒå…¥çš„å¹¿æ³›é¢†åŸŸçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™ç§çŸ¥è¯†çš„æ·±åº¦å’Œå¹¿åº¦åœ¨ä¸åŒé¢†åŸŸä¹‹é—´å¯èƒ½å­˜åœ¨å·®å¼‚ã€‚è®¸å¤šç°æœ‰æ–¹æ³•å‡è®¾LLMså¯¹å…¶ç¯å¢ƒæœ‰å…¨é¢çš„ç†è§£ï¼Œä½†å¾€å¾€å¿½è§†äº†å®ƒä»¬å¯¹å®é™…ä¸–ç•ŒåŠ¨æ€çš„æŒæ¡å¯èƒ½å­˜åœ¨å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Discover, Verify, and Evolve (DiVE)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»å°‘é‡æ¼”ç¤ºä¸­å‘ç°ä¸–ç•ŒåŠ¨æ€ï¼ŒéªŒè¯è¿™äº›åŠ¨æ€çš„å‡†ç¡®æ€§ï¼Œå¹¶æ ¹æ®å½“å‰æƒ…å†µæ¼”åŒ–æ–°çš„ã€å…ˆè¿›çš„åŠ¨æ€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šDiVEæ¡†æ¶
DiVEæ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼š
- Discovererï¼šä½¿ç”¨è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ä»æ¼”ç¤ºä¸­è¿­ä»£åœ°å‘ç°ç¯å¢ƒåŠ¨æ€ã€‚
- Verifierï¼šæ¶ˆé™¤LLMsç”±äºå¹»è§‰å€¾å‘è€Œå¯¼è‡´çš„ä¸å¯é åŠ¨æ€ã€‚
- Evolverï¼šæ ¹æ®å­¦ä¹ çš„åŠ¨æ€ï¼Œæ¨ç†å‡ºé’ˆå¯¹å½“å‰æƒ…å†µçš„æ·±å…¥ã€ç‰¹å®šäºçŠ¶æ€çš„çŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå±‚æ¬¡è¯¾ç¨‹å­¦ä¹ 
DiVEé‡‡ç”¨å±‚æ¬¡è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œä»ç®€å•åˆ°å¤æ‚çš„åŠ¨æ€é€æ­¥å­¦ä¹ ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»ä»»åŠ¡åˆ†è§£å±‚æ¬¡ç»“æ„ä¸­çš„å…ƒç´ ï¼ˆå¦‚åŠ¨ä½œã€å¯¹è±¡ã€å­ä»»åŠ¡å’Œå­ç›®æ ‡ï¼‰å¼€å§‹ï¼Œé€æ­¥å­¦ä¹ å®ƒä»¬çš„åŠ¨æ€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŠ¨æ€éªŒè¯
ä¸ºäº†ç¡®ä¿åŠ¨æ€çš„å‡†ç¡®æ€§ï¼ŒDiVEå¼•å…¥äº†åŠ¨æ€éªŒè¯å™¨ï¼Œå®ƒå¯ä»¥è¿‡æ»¤æ‰å¯èƒ½æ— æ•ˆå’Œå†²çªçš„åŠ¨æ€å€™é€‰è€…ï¼Œä»è€Œæé«˜å†³ç­–è¿‡ç¨‹çš„å¯é æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šåœ¨çº¿ç­–ç•¥å­¦ä¹ 
DiVEä¸ä»…å­¦ä¹ åŸºæœ¬è§„åˆ™ï¼Œè¿˜ä¸“æ³¨äºæ ¹æ®è¿™äº›åŠ¨æ€å¼€å‘é«˜çº§æ¸¸æˆç­–ç•¥ã€‚å®ƒé€šè¿‡åœ¨çº¿å­¦ä¹ æ–¹æ³•å°†åŠ¨æ€æ¼”åŒ–ä¸ºç­–ç•¥ï¼Œä»è€Œç”Ÿæˆæ›´ç¬¦åˆå½“å‰æ¸¸æˆåœºæ™¯çš„ç­–ç•¥ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Crafterå’ŒMiniHackç¯å¢ƒä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒDiVEåœ¨æ€§èƒ½æ–¹é¢ä¼˜äºæ‰€æœ‰å…¶ä»–åŸºçº¿æ¨¡å‹ã€‚åœ¨Crafterç¯å¢ƒä¸­ï¼ŒDiVEåœ¨åˆ†æ•°å’Œå¥–åŠ±æ–¹é¢åˆ†åˆ«æ¯”SOTA LLMæ–¹æ³•SPRINGæé«˜äº†337.8%å’Œ110.1%ï¼Œå¹¶ä¸”è¶…è¿‡äº†SOTA RLæ–¹æ³•DreamerV3ã€‚åœ¨MiniHackç¯å¢ƒä¸­ï¼ŒDiVEåœ¨Lava Crossingä»»åŠ¡ä¸Šä¸SSOå’ŒReflexionï¼ˆéƒ½éœ€è¦30æ¬¡è¿­ä»£è®­ç»ƒï¼‰çš„æ€§èƒ½ç›¸å½“ï¼Œå¹¶ä¸”åœ¨Wand of Deathå’ŒQuestä»»åŠ¡ä¸Šè¶…è¿‡äº†è¿™ä¸¤ä¸ªåŸºçº¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DiVEæ¡†æ¶ä¸ºè§£å†³LLMsåœ¨ç‰¹å®šé¢†åŸŸä¸­çš„çŸ¥è¯†å·®è·é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å‘ç°ã€éªŒè¯å’Œæ¼”åŒ–ä¸–ç•ŒåŠ¨æ€ï¼Œæé«˜äº†LLMsçš„å†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒDiVEçš„å±‚æ¬¡è¯¾ç¨‹å­¦ä¹ å’ŒåŠ¨æ€éªŒè¯æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–éœ€è¦é•¿æœŸè§„åˆ’å’Œå†³ç­–çš„ä»»åŠ¡ä¸­ã€‚

