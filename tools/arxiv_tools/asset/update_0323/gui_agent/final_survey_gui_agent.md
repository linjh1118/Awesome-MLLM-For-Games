# Paper List of gui_agent.md
- [23/09] **Agents: An Open-source Framework for Autonomous Language Agents**  
[[Paper](http://arxiv.org/pdf/2309.07870v3)] [[Code/Page](https://github.com/aiwaves-cn/agents.)] [[TLDR/Notes](#agents--an-open-source-framework-for-autonomous-language-agents)]

- [24/10] **DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents**  
[[Paper](http://arxiv.org/pdf/2410.14803v5)] [[Code/Page]()] [[TLDR/Notes](#distrl--an-asynchronous-distributed-reinforcement-learning-framework-for-on-device-control-agents)]

- [21/12] **WebGPT: Browser-assisted question-answering with human feedback**  
[[Paper](http://arxiv.org/pdf/2112.09332v3)] [[Code/Page]()] [[TLDR/Notes](#webgpt--browser-assisted-question-answering-with-human-feedback)]

- [18/02] **Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration**  
[[Paper](http://arxiv.org/pdf/1802.08802v1)] [[Code/Page]()] [[TLDR/Notes](#reinforcement-learning-on-web-interfaces-using-workflow-guided-exploration)]

- [23/01] **Neuro-Symbolic World Models for Adapting to Open World Novelty**  
[[Paper](http://arxiv.org/pdf/2301.06294v1)] [[Code/Page]()] [[TLDR/Notes](#neuro-symbolic-world-models-for-adapting-to-open-world-novelty)]

- [22/07] **WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents**  
[[Paper](http://arxiv.org/pdf/2207.01206v4)] [[Code/Page]()] [[TLDR/Notes](#webshop--towards-scalable-real-world-web-interaction-with-grounded-language-agents)]

- [22/10] **ReAct: Synergizing Reasoning and Acting in Language Models**  
[[Paper](http://arxiv.org/pdf/2210.03629v3)] [[Code/Page](https://react-lm.github.io)] [[TLDR/Notes](#react--synergizing-reasoning-and-acting-in-language-models)]

- [23/10] **Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions**  
[[Paper](http://arxiv.org/pdf/2310.15780v1)] [[Code/Page]()] [[TLDR/Notes](#make-llm-a-testing-expert--bringing-human-like-interaction-to-mobile-gui-testing-via-functionality-aware-decisions)]

- [24/04] **VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?**  
[[Paper](http://arxiv.org/pdf/2404.05955v1)] [[Code/Page]()] [[TLDR/Notes](#visualwebbench--how-far-have-multimodal-llms-evolved-in-web-page-understanding-and-grounding-)]

- [24/11] **AutoGLM: Autonomous Foundation Agents for GUIs**  
[[Paper](http://arxiv.org/pdf/2411.00820v1)] [[Code/Page]()] [[TLDR/Notes](#autoglm--autonomous-foundation-agents-for-guis)]

- [22/12] **Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing**  
[[Paper](http://arxiv.org/pdf/2212.04732v1)] [[Code/Page]()] [[TLDR/Notes](#fill-in-the-blank--context-aware-automated-text-input-generation-for-mobile-gui-testing)]

- [24/08] **OmniParser for Pure Vision Based GUI Agent**  
[[Paper](http://arxiv.org/pdf/2408.00203v1)] [[Code/Page]()] [[TLDR/Notes](#omniparser-for-pure-vision-based-gui-agent)]

- [24/02] **WebLINX: Real-World Website Navigation with Multi-Turn Dialogue**  
[[Paper](http://arxiv.org/pdf/2402.05930v2)] [[Code/Page](https://mcgill-nlp.github.io/weblinx)] [[TLDR/Notes](#weblinx--real-world-website-navigation-with-multi-turn-dialogue)]

- [24/08] **Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions**  
[[Paper](http://arxiv.org/pdf/2408.02544v1)] [[Code/Page]()] [[TLDR/Notes](#caution-for-the-environment--multimodal-agents-are-susceptible-to-environmental-distractions)]

- [23/11] **GAIA: a benchmark for General AI Assistants**  
[[Paper](http://arxiv.org/pdf/2311.12983v1)] [[Code/Page](https://huggingface.co/gaia-benchmark.)] [[TLDR/Notes](#gaia--a-benchmark-for-general-ai-assistants)]

- [24/11] **DynaSaur: Large Language Agents Beyond Predefined Actions**  
[[Paper](http://arxiv.org/pdf/2411.01747v1)] [[Code/Page](https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.)] [[TLDR/Notes](#dynasaur--large-language-agents-beyond-predefined-actions)]

- [24/07] **MobileFlow: A Multimodal LLM For Mobile GUI Agent**  
[[Paper](http://arxiv.org/pdf/2407.04346v3)] [[Code/Page]()] [[TLDR/Notes](#mobileflow--a-multimodal-llm-for-mobile-gui-agent)]

- [23/12] **AutoTask: Executing Arbitrary Voice Commands by Exploring and Learning from Mobile GUI**  
[[Paper](http://arxiv.org/pdf/2312.16062v1)] [[Code/Page]()] [[TLDR/Notes](#autotask--executing-arbitrary-voice-commands-by-exploring-and-learning-from-mobile-gui)]

- [24/06] **WebCanvas: Benchmarking Web Agents in Online Environments**  
[[Paper](http://arxiv.org/pdf/2406.12373v3)] [[Code/Page]()] [[TLDR/Notes](#webcanvas--benchmarking-web-agents-in-online-environments)]

- [24/08] **Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents**  
[[Paper](http://arxiv.org/pdf/2408.07199v1)] [[Code/Page]()] [[TLDR/Notes](#agent-q--advanced-reasoning-and-learning-for-autonomous-ai-agents)]

- [24/11] **WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning**  
[[Paper](http://arxiv.org/pdf/2411.02337v3)] [[Code/Page]()] [[TLDR/Notes](#webrl--training-llm-web-agents-via-self-evolving-online-curriculum-reinforcement-learning)]

- [24/09] **NaviQAte: Functionality-Guided Web Application Navigation**  
[[Paper](http://arxiv.org/pdf/2409.10741v1)] [[Code/Page]()] [[TLDR/Notes](#naviqate--functionality-guided-web-application-navigation)]

- [24/12] **Falcon-UI: Understanding GUI Before Following User Instructions**  
[[Paper](http://arxiv.org/pdf/2412.09362v1)] [[Code/Page]()] [[TLDR/Notes](#falcon-ui--understanding-gui-before-following-user-instructions)]

- [23/11] **TaskBench: Benchmarking Large Language Models for Task Automation**  
[[Paper](http://arxiv.org/pdf/2311.18760v4)] [[Code/Page]()] [[TLDR/Notes](#taskbench--benchmarking-large-language-models-for-task-automation)]

- [23/12] **ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation**  
[[Paper](http://arxiv.org/pdf/2312.13108v2)] [[Code/Page]()] [[TLDR/Notes](#assistgui--task-oriented-desktop-graphical-user-interface-automation)]

- [24/10] **Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents**  
[[Paper](http://arxiv.org/pdf/2410.05243v2)] [[Code/Page]()] [[TLDR/Notes](#navigating-the-digital-world-as-humans-do--universal-visual-grounding-for-gui-agents)]

- [24/11] **Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents**  
[[Paper](http://arxiv.org/pdf/2411.06559v2)] [[Code/Page]()] [[TLDR/Notes](#is-your-llm-secretly-a-world-model-of-the-internet--model-based-planning-for-web-agents)]

- [23/07] **A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis**  
[[Paper](http://arxiv.org/pdf/2307.12856v4)] [[Code/Page]()] [[TLDR/Notes](#a-real-world-webagent-with-planning--long-context-understanding--and-program-synthesis)]

- [24/01] **WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models**  
[[Paper](http://arxiv.org/pdf/2401.13919v4)] [[Code/Page]()] [[TLDR/Notes](#webvoyager--building-an-end-to-end-web-agent-with-large-multimodal-models)]

- [23/12] **CogAgent: A Visual Language Model for GUI Agents**  
[[Paper](http://arxiv.org/pdf/2312.08914v3)] [[Code/Page](https://github.com/THUDM/CogVLM,)] [[TLDR/Notes](#cogagent--a-visual-language-model-for-gui-agents)]

- [24/02] **Understanding the planning of LLM agents: A survey**  
[[Paper](http://arxiv.org/pdf/2402.02716v1)] [[Code/Page]()] [[TLDR/Notes](#understanding-the-planning-of-llm-agents--a-survey)]

- [24/01] **WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models**  
[[Paper](http://arxiv.org/pdf/2401.13919v4)] [[Code/Page]()] [[TLDR/Notes](#webvoyager--building-an-end-to-end-web-agent-with-large-multimodal-models)]

- [24/10] **VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks**  
[[Paper](http://arxiv.org/pdf/2410.19100v3)] [[Code/Page]()] [[TLDR/Notes](#videowebarena--evaluating-long-context-multimodal-agents-with-video-understanding-web-tasks)]

- [24/03] **Tur[k]ingBench: A Challenge Benchmark for Web Agents**  
[[Paper](http://arxiv.org/pdf/2403.11905v4)] [[Code/Page]()] [[TLDR/Notes](#tur[k]ingbench--a-challenge-benchmark-for-web-agents)]

- [21/03] **Grounding Open-Domain Instructions to Automate Web Support Tasks**  
[[Paper](http://arxiv.org/pdf/2103.16057v2)] [[Code/Page]()] [[TLDR/Notes](#grounding-open-domain-instructions-to-automate-web-support-tasks)]

- [24/10] **AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents**  
[[Paper](http://arxiv.org/pdf/2410.13825v1)] [[Code/Page]()] [[TLDR/Notes](#agentoccam--a-simple-yet-strong-baseline-for-llm-based-web-agents)]

- [23/10] **Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models**  
[[Paper](http://arxiv.org/pdf/2310.03965v3)] [[Code/Page]()] [[TLDR/Notes](#thought-propagation--an-analogical-approach-to-complex-reasoning-with-large-language-models)]

- [24/11] **Large Language Model-Brained GUI Agents: A Survey**  
[[Paper](http://arxiv.org/pdf/2411.18279v10)] [[Code/Page]()] [[TLDR/Notes](#large-language-model-brained-gui-agents--a-survey)]

- [23/12] **AppAgent: Multimodal Agents as Smartphone Users**  
[[Paper](http://arxiv.org/pdf/2312.13771v2)] [[Code/Page]()] [[TLDR/Notes](#appagent--multimodal-agents-as-smartphone-users)]

- [24/07] **MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices**  
[[Paper](http://arxiv.org/pdf/2407.03913v1)] [[Code/Page]()] [[TLDR/Notes](#mobileexperts--a-dynamic-tool-enabled-agent-team-in-mobile-devices)]

- [24/03] **Exploring the Privacy Protection Capabilities of Chinese Large Language Models**  
[[Paper](http://arxiv.org/pdf/2403.18205v1)] [[Code/Page]()] [[TLDR/Notes](#exploring-the-privacy-protection-capabilities-of-chinese-large-language-models)]

- [24/08] **WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration**  
[[Paper](http://arxiv.org/pdf/2408.15978v1)] [[Code/Page]()] [[TLDR/Notes](#webpilot--a-versatile-and-autonomous-multi-agent-system-for-web-task-execution-with-strategic-exploration)]

- [24/04] **MMInA: Benchmarking Multihop Multimodal Internet Agents**  
[[Paper](http://arxiv.org/pdf/2404.09992v1)] [[Code/Page](https://mmina.cliangyu.com)] [[TLDR/Notes](#mmina--benchmarking-multihop-multimodal-internet-agents)]

- [23/09] **An In-depth Survey of Large Language Model-based Artificial Intelligence Agents**  
[[Paper](http://arxiv.org/pdf/2309.14365v1)] [[Code/Page]()] [[TLDR/Notes](#an-in-depth-survey-of-large-language-model-based-artificial-intelligence-agents)]

- [23/10] **Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models**  
[[Paper](http://arxiv.org/pdf/2310.04406v3)] [[Code/Page](https://github.com/lapisrocks/LanguageAgentTreeSearch)] [[TLDR/Notes](#language-agent-tree-search-unifies-reasoning-acting-and-planning-in-language-models)]

- [23/07] **WebArena: A Realistic Web Environment for Building Autonomous Agents**  
[[Paper](http://arxiv.org/pdf/2307.13854v4)] [[Code/Page]()] [[TLDR/Notes](#webarena--a-realistic-web-environment-for-building-autonomous-agents)]

- [24/10] **MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation**  
[[Paper](http://arxiv.org/pdf/2410.13757v2)] [[Code/Page]()] [[TLDR/Notes](#moba--multifaceted-memory-enhanced-adaptive-planning-for-efficient-mobile-task-automation)]

- [24/06] **VGA: Vision GUI Assistant -- Minimizing Hallucinations through Image-Centric Fine-Tuning**  
[[Paper](http://arxiv.org/pdf/2406.14056v3)] [[Code/Page]()] [[TLDR/Notes](#vga--vision-gui-assistant----minimizing-hallucinations-through-image-centric-fine-tuning)]

- [25/03] **AppAgentX: Evolving GUI Agents as Proficient Smartphone Users**  
[[Paper](http://arxiv.org/pdf/2503.02268v1)] [[Code/Page]()] [[TLDR/Notes](#appagentx--evolving-gui-agents-as-proficient-smartphone-users)]

- [25/03] **Towards Trustworthy GUI Agents: A Survey**  
[[Paper](http://arxiv.org/pdf/2503.23434v1)] [[Code/Page]()] [[TLDR/Notes](#towards-trustworthy-gui-agents--a-survey)]



# TLDR/Notes
## agents--an-open-source-framework-for-autonomous-language-agents
### Abstract
Recent advances on large language models (LLMs) enable researchers and
developers to build autonomous language agents that can automatically solve
various tasks and interact with environments, humans, and other agents using
natural language interfaces. We consider language agents as a promising
direction towards artificial general intelligence and release Agents, an
open-source library with the goal of opening up these advances to a wider
non-specialist audience. Agents is carefully engineered to support important
features including planning, memory, tool usage, multi-agent communication, and
fine-grained symbolic control. Agents is user-friendly as it enables
non-specialists to build, customize, test, tune, and deploy state-of-the-art
autonomous language agents without much coding. The library is also
research-friendly as its modularized design makes it easily extensible for
researchers. Agents is available at https://github.com/aiwaves-cn/agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¼€æºæ¡†æ¶ Agentsï¼šæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†çš„åˆ©å™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…èƒ½å¤Ÿæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿè‡ªåŠ¨è§£å†³å„ç§ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢ä¸ç¯å¢ƒã€äººç±»å’Œå…¶ä»–ä»£ç†è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­è¨€ä»£ç†æ¡†æ¶å¾€å¾€ç¼ºä¹æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œéš¾ä»¥æ»¡è¶³éä¸“ä¸šäººå£«çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Agentsï¼Œä¸€ä¸ªå¼€æºçš„è‡ªä¸»è¯­è¨€ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨è®©æ›´å¹¿æ³›çš„éä¸“ä¸šäººå£«èƒ½å¤Ÿè½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²æœ€å…ˆè¿›çš„è‡ªä¸»è¯­è¨€ä»£ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ”¯æŒå…³é”®åŠŸèƒ½
Agents æ¡†æ¶ç²¾å¿ƒè®¾è®¡ï¼Œæ”¯æŒè§„åˆ’ã€è®°å¿†ã€å·¥å…·ä½¿ç”¨ã€å¤šä»£ç†é€šä¿¡å’Œç»†ç²’åº¦ç¬¦å·æ§åˆ¶ç­‰å…³é”®åŠŸèƒ½ã€‚è¿™ä½¿å¾—è¯­è¨€ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å„ç§ä»»åŠ¡å’Œç¯å¢ƒã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§
Agents æ¡†æ¶çš„ç”¨æˆ·å‹å¥½æ€§ä½“ç°åœ¨å…¶å…è®¸éä¸“ä¸šäººå£«è½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²è‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè€Œæ— éœ€å¤§é‡ç¼–ç ã€‚åŒæ—¶ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾æ‰©å±•æ¡†æ¶ï¼Œä»¥æ»¡è¶³ä»–ä»¬çš„ç ”ç©¶éœ€æ±‚ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAgent Hub å¹³å°
Agents æ¡†æ¶å¼•å…¥äº† Agent Hub å¹³å°ï¼Œå…è®¸ç”¨æˆ·åˆ†äº«ä»–ä»¬å¾®è°ƒçš„è¯­è¨€ä»£ç†ï¼Œå¹¶æœç´¢/ä¸‹è½½å…¶ä»–ç”¨æˆ·åˆ†äº«çš„æœ‰ç”¨è¯­è¨€ä»£ç†ã€‚è¿™å¤§å¤§é™ä½äº†ä»å¤´å¼€å§‹è®¾è®¡å’Œè°ƒæ•´è¯­è¨€ä»£ç†çš„éš¾åº¦ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè‡ªåŠ¨åˆ›å»ºä»£ç†ç³»ç»Ÿ
ä¸ºäº†å‡å°‘ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®š SOP çš„ç¹çå·¥ä½œï¼ŒAgents æ¡†æ¶å®ç°äº†ä¸€ä¸ªè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹ã€‚è¯¥æµç¨‹åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)ï¼Œå¯ä»¥è‡ªåŠ¨åˆ›å»ºå…¶ä»–ä»£ç†å’Œå¤šä»£ç†ç³»ç»Ÿã€‚

### ğŸ“ˆ å®éªŒç»“æœ
è®ºæ–‡å±•ç¤ºäº†ä½¿ç”¨ Agents æ¡†æ¶æ„å»ºçš„å•ä»£ç†ç³»ç»Ÿå’Œå¤šä»£ç†ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒåŒ…æ‹¬é—²èŠæœºå™¨äººã€åŸºäºçŸ¥è¯†åº“å’Œæœç´¢å¼•æ“çš„å®¢æˆ·æœåŠ¡ä»£ç†ã€è´­ç‰©åŠ©æ‰‹ä»£ç†å’Œé”€å”®ä»£ç†ç­‰ã€‚è¿™äº›æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº† Agents æ¡†æ¶çš„æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠæ„å»ºå„ç§ç”¨ä¾‹çš„è¯­è¨€ä»£ç†çš„å¯èƒ½æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
Agents æ¡†æ¶ä¸ºæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå…¶æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…çš„ç†æƒ³é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒAgent Hub å¹³å°å’Œè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹è¿›ä¸€æ­¥æé«˜äº†æ¡†æ¶çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚

## distrl--an-asynchronous-distributed-reinforcement-learning-framework-for-on-device-control-agents
### Abstract
On-device control agents, especially on mobile devices, are responsible for
operating mobile devices to fulfill users' requests, enabling seamless and
intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)
into these agents enhances their ability to understand and execute complex
commands, thereby improving user experience. However, fine-tuning MLLMs for
on-device control presents significant challenges due to limited data
availability and inefficient online training processes. This paper introduces
DistRL, a novel framework designed to enhance the efficiency of online RL
fine-tuning for mobile device control agents. DistRL employs centralized
training and decentralized data acquisition to ensure efficient fine-tuning in
the context of dynamic online interactions. Additionally, the framework is
backed by our tailor-made RL algorithm, which effectively balances exploration
with the prioritized utilization of collected data to ensure stable and robust
training. Our experiments show that, on average, DistRL delivers a 3X
improvement in training efficiency and enables training data collection 2.4X
faster than the leading synchronous multi-machine methods. Notably, after
training, DistRL achieves a 20% relative improvement in success rate compared
to state-of-the-art methods on general Android tasks from an open benchmark,
significantly outperforming existing approaches while maintaining the same
training time. These results validate DistRL as a scalable and efficient
solution, offering substantial improvements in both training efficiency and
agent performance for real-world, in-the-wild device control tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DistRLï¼šæå‡ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ•ˆç‡çš„åˆ†å¸ƒå¼æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…´èµ·ï¼Œå°†å®ƒä»¬é›†æˆåˆ°ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†ä¸­ï¼Œå¯ä»¥æ˜¾è‘—æå‡ä»£ç†ç†è§£å’Œæ‰§è¡Œå¤æ‚å‘½ä»¤çš„èƒ½åŠ›ï¼Œä»è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿›è¡ŒMLLMsçš„å¾®è°ƒé¢ä¸´ç€æ•°æ®å¯ç”¨æ€§æœ‰é™å’Œåœ¨çº¿è®­ç»ƒè¿‡ç¨‹æ•ˆç‡ä½ä¸‹ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DistRLï¼Œä¸€ä¸ªæ—¨åœ¨æé«˜ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæ•ˆç‡çš„åˆ†å¸ƒå¼æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¯æ‰©å±•çš„å¼‚æ­¥æ•°æ®é‡‡é›†æ¶æ„
DistRLé‡‡ç”¨è§£è€¦å’Œå¼‚æ­¥çš„æ¡†æ¶ï¼Œå°†RLä»£ç†éƒ¨ç½²åœ¨å¼‚æ„çš„workerè®¾å¤‡ä¸Šè¿›è¡Œè¿œç¨‹æ•°æ®é‡‡é›†ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…ˆè¿›çš„é›†ä¸­å¼è®­ç»ƒRLç®—æ³•
DistRLå¼€å‘äº†A-RIDEç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åˆ†å¸ƒå¼å’Œå¼‚æ­¥æ•°æ®åˆ©ç”¨çš„ç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒä¼˜å…ˆè€ƒè™‘é‡è¦çš„ç»éªŒï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡ï¼ŒåŒæ—¶é¼“åŠ±æ¢ç´¢ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒDistRLåœ¨è®­ç»ƒæ•ˆç‡æ–¹é¢å¹³å‡æé«˜äº†3å€ï¼Œå¹¶ä¸”æ¯”é¢†å…ˆçš„åŒæ­¥å¤šæœºæ–¹æ³•å¿«2.4å€ã€‚åœ¨è®­ç»ƒåï¼ŒDistRLåœ¨å¼€æ”¾åŸºå‡†æµ‹è¯•ä¸­çš„ä¸€èˆ¬Androidä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†20%ï¼Œåœ¨ä¿æŒç›¸åŒè®­ç»ƒæ—¶é—´çš„åŒæ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DistRLæ¡†æ¶ä¸ºç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚å…¶å¼‚æ­¥æ•°æ®é‡‡é›†å’Œåˆ†å¸ƒå¼è®­ç»ƒçš„è®¾è®¡ï¼Œä»¥åŠA-RIDEç®—æ³•çš„åº”ç”¨ï¼Œä¸ºè§£å†³ç§»åŠ¨è®¾å¤‡ä¸ŠRLå¾®è°ƒçš„æŒ‘æˆ˜æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒDistRLæ¡†æ¶çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶é€‚ç”¨äºå„ç§ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## webgpt--browser-assisted-question-answering-with-human-feedback
### Abstract
We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. By setting up the task so that it can be performed by humans, we are able
to train models on the task using imitation learning, and then optimize answer
quality with human feedback. To make human evaluation of factual accuracy
easier, models must collect references while browsing in support of their
answers. We train and evaluate our models on ELI5, a dataset of questions asked
by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior
cloning, and then performing rejection sampling against a reward model trained
to predict human preferences. This model's answers are preferred by humans 56%
of the time to those of our human demonstrators, and 69% of the time to the
highest-voted answer from Reddit.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebGPTï¼šæµè§ˆå™¨è¾…åŠ©çš„é—®é¢˜å›ç­”ä¸äººç±»åé¦ˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å‘å±•ï¼Œé•¿ç¯‡é—®é¢˜å›ç­”ï¼ˆLFQAï¼‰æˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç±»ç³»ç»Ÿæœ‰æ½œåŠ›æˆä¸ºäººä»¬äº†è§£ä¸–ç•Œçš„ä¸»è¦æ–¹å¼ï¼Œä½†ç›®å‰å…¶æ€§èƒ½ä»è½åäºäººç±»ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æ–¹æ³•ï¼Œæå‡é•¿ç¯‡é—®é¢˜å›ç­”çš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1
æœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„æµè§ˆå™¨ç¯å¢ƒï¼Œä½¿å¾—ç»è¿‡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¸ä¹‹äº’åŠ¨ã€‚è¿™å…è®¸æˆ‘ä»¬ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç­‰é€šç”¨æ–¹æ³•ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼æ”¹è¿›æ£€ç´¢å’Œç”Ÿæˆã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2
æ¨¡å‹åœ¨æµè§ˆè¿‡ç¨‹ä¸­ç”Ÿæˆå¸¦æœ‰å¼•ç”¨çš„å›ç­”ï¼Œå³ä»ç½‘é¡µä¸­æå–çš„æ–‡æœ¬ç‰‡æ®µã€‚è¿™ä¸ºè¯„ä¼°ç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§æä¾›äº†ä¾¿åˆ©ï¼Œæ— éœ€è¯„ä¼°è€…è¿›è¡Œç‹¬ç«‹è€Œå›°éš¾çš„ç ”ç©¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ELI5æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚æœ€ä½³æ¨¡å‹é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’ŒåŸºäºäººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œæ‹’ç»æŠ½æ ·å¾—åˆ°ã€‚åœ¨ä¸‰ç§è¯„ä¼°æ–¹å¼ä¸­ï¼Œè¯¥æ¨¡å‹çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸”åœ¨äº‹å®æ€§å’Œä¿¡æ¯æ€§æ–¹é¢æœ‰æ‰€æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„æ–¹æ³•è¡¨æ˜ï¼Œé€šè¿‡ç»“åˆå¼ºå¤§çš„æœç´¢å·¥å…·å’Œæ¨¡ä»¿å­¦ä¹ ï¼Œå¯ä»¥æ˜¾è‘—æå‡é•¿ç¯‡é—®é¢˜å›ç­”çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œå¼•å…¥äººç±»åé¦ˆè¿›è¡Œä¼˜åŒ–ï¼Œæœ‰åŠ©äºæ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´æœ‰ç”¨çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œå¸¦å¼•ç”¨çš„å›ç­”æ–¹å¼ä¸ºè¯„ä¼°ç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚

## reinforcement-learning-on-web-interfaces-using-workflow-guided-exploration
### Abstract
Reinforcement learning (RL) agents improve through trial-and-error, but when
reward is sparse and the agent cannot discover successful action sequences,
learning stagnates. This has been a notable problem in training deep RL agents
to perform web-based tasks, such as booking flights or replying to emails,
where a single mistake can ruin the entire sequence of actions. A common remedy
is to "warm-start" the agent by pre-training it to mimic expert demonstrations,
but this is prone to overfitting. Instead, we propose to constrain exploration
using demonstrations. From each demonstration, we induce high-level "workflows"
which constrain the allowable actions at each time step to be similar to those
in the demonstration (e.g., "Step 1: click on a textbox; Step 2: enter some
text"). Our exploration policy then learns to identify successful workflows and
samples actions that satisfy these workflows. Workflows prune out bad
exploration directions and accelerate the agent's ability to discover rewards.
We use our approach to train a novel neural policy designed to handle the
semi-structured nature of websites, and evaluate on a suite of web tasks,
including the recent World of Bits benchmark. We achieve new state-of-the-art
results, and show that workflow-guided exploration improves sample efficiency
over behavioral cloning by more than 100x.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ä½¿ç”¨å·¥ä½œæµå¼•å¯¼æ¢ç´¢çš„Webç•Œé¢å¼ºåŒ–å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†é€šè¿‡è¯•é”™æ¥æé«˜æ€§èƒ½ï¼Œä½†åœ¨å¥–åŠ±ç¨€ç–ä¸”ä»£ç†æ— æ³•å‘ç°æˆåŠŸçš„åŠ¨ä½œåºåˆ—æ—¶ï¼Œå­¦ä¹ ä¼šåœæ»ä¸å‰ã€‚è¿™åœ¨è®­ç»ƒæ·±åº¦RLä»£ç†æ‰§è¡ŒåŸºäºWebçš„ä»»åŠ¡ï¼ˆå¦‚é¢„è®¢èˆªç­æˆ–å›å¤ç”µå­é‚®ä»¶ï¼‰æ—¶æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„é—®é¢˜ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªé”™è¯¯å¯èƒ½ä¼šç ´åæ•´ä¸ªåŠ¨ä½œåºåˆ—ã€‚ä¸€ä¸ªå¸¸è§çš„è¡¥æ•‘æªæ–½æ˜¯é€šè¿‡é¢„è®­ç»ƒä»£ç†æ¥æ¨¡ä»¿ä¸“å®¶æ¼”ç¤ºæ¥â€œé¢„çƒ­â€ä»£ç†ï¼Œä½†è¿™å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ç›¸åï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨æ¼”ç¤ºæ¥çº¦æŸæ¢ç´¢ã€‚ä»æ¯ä¸ªæ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬è¯±å¯¼å‡ºé«˜çº§â€œå·¥ä½œæµâ€ï¼Œè¿™äº›å·¥ä½œæµå°†æ¯ä¸ªæ—¶é—´æ­¥å…è®¸çš„åŠ¨ä½œé™åˆ¶ä¸ºä¸æ¼”ç¤ºä¸­çš„åŠ¨ä½œç›¸ä¼¼ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­¥éª¤1ï¼šç‚¹å‡»æ–‡æœ¬æ¡†ï¼›æ­¥éª¤2ï¼šè¾“å…¥ä¸€äº›æ–‡æœ¬â€ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„æ¢ç´¢ç­–ç•¥å­¦ä¹ è¯†åˆ«æˆåŠŸçš„å·¥ä½œæµï¼Œå¹¶é‡‡æ ·æ»¡è¶³è¿™äº›å·¥ä½œæµçš„åŠ¨ä½œã€‚å·¥ä½œæµå‰ªé™¤äº†ä¸è‰¯çš„æ¢ç´¢æ–¹å‘ï¼Œå¹¶åŠ é€Ÿäº†ä»£ç†å‘ç°å¥–åŠ±çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•æ¥è®­ç»ƒä¸€ä¸ªæ–°é¢–çš„ç¥ç»ç­–ç•¥ï¼Œæ—¨åœ¨å¤„ç†ç½‘ç«™çš„åŠç»“æ„åŒ–æ€§è´¨ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—Webä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬æœ€è¿‘çš„World of BitsåŸºå‡†ã€‚æˆ‘ä»¬å–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶è¡¨æ˜å·¥ä½œæµå¼•å¯¼çš„æ¢ç´¢æ¯”è¡Œä¸ºå…‹éš†çš„æ ·æœ¬æ•ˆç‡æé«˜äº†100å€ä»¥ä¸Šã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå·¥ä½œæµå¼•å¯¼æ¢ç´¢ï¼ˆWGEï¼‰æ¡†æ¶
1. ä»æ¯ä¸ªæ¼”ç¤ºä¸­æå–ä¸æ¼”ç¤ºä¸­è§‚å¯Ÿåˆ°çš„åŠ¨ä½œä¸€è‡´çš„å·¥ä½œæµæ ¼ç½‘ã€‚
2. å®šä¹‰ä¸€ä¸ªå·¥ä½œæµæ¢ç´¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡é¦–å…ˆé€‰æ‹©ä¸€ä¸ªå·¥ä½œæµï¼Œç„¶åé‡‡æ ·ç¬¦åˆå·¥ä½œæµçš„åŠ¨ä½œæ¥è¿›è¡Œæ¢ç´¢ã€‚è¯¥ç­–ç•¥é€šè¿‡å¼ºåŒ–å­¦ä¹ é€æ¸å­¦ä¹ é€‰æ‹©å“ªä¸ªå·¥ä½œæµã€‚
3. åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­å‘ç°çš„å¥–åŠ±èµšå–çš„å‰§é›†è¿›å…¥é‡æ”¾ç¼“å†²åŒºï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªæ›´å¼ºå¤§å’Œè¡¨è¾¾æ€§æ›´å¼ºçš„ç¥ç»ç½‘ç»œç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDOMNETç¥ç»ç½‘ç»œç­–ç•¥
ä¸ºäº†å¤„ç†ç½‘ç«™çš„åŠç»“æ„åŒ–æ€§è´¨ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¥ç»ç½‘ç»œç­–ç•¥ï¼ˆDOMNETï¼‰ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿçµæ´»åœ°å¯¹ç½‘ç«™çš„æ ‘çŠ¶HTMLè¡¨ç¤ºè¿›è¡Œå…³ç³»æ¨ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸€ç³»åˆ—Webäº¤äº’ä»»åŠ¡ä¸Šè¯„ä¼°äº†å·¥ä½œæµå¼•å¯¼çš„æ¢ç´¢å’ŒDOMNETï¼ŒåŒ…æ‹¬MiniWoBåŸºå‡†ã€é˜¿æ‹‰æ–¯åŠ èˆªç©ºå…¬å¸çš„èˆªç­é¢„è®¢ç•Œé¢ä»¥åŠä¸€ç³»åˆ—æ–°æ„å»ºçš„ä»»åŠ¡ã€‚ä¸å…ˆå‰åœ¨MiniWoBä¸Šçš„ç»“æœç›¸æ¯”ï¼Œæœ¬æ–‡çš„ç³»ç»Ÿåœ¨æ¯é¡¹ä»»åŠ¡ä¸Šä»…ä½¿ç”¨3-10ä¸ªæ¼”ç¤ºå°±å–å¾—äº†æ›´é«˜çš„æˆåŠŸç‡ï¼Œå¹¶å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„å·¥ä½œæµå¼•å¯¼çš„æ¢ç´¢æ¡†æ¶å’ŒDOMNETç¥ç»ç½‘ç»œç­–ç•¥ä¸ºåœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è®­ç»ƒæ·±åº¦RLä»£ç†æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ¼”ç¤ºæ¥çº¦æŸæ¢ç´¢ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„ç¥ç»ç½‘ç»œç­–ç•¥æ¥å¤„ç†ç½‘ç«™çš„åŠç»“æ„åŒ–æ€§è´¨ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚

## neuro-symbolic-world-models-for-adapting-to-open-world-novelty
### Abstract
Open-world novelty--a sudden change in the mechanics or properties of an
environment--is a common occurrence in the real world. Novelty adaptation is an
agent's ability to improve its policy performance post-novelty. Most
reinforcement learning (RL) methods assume that the world is a closed, fixed
process. Consequentially, RL policies adapt inefficiently to novelties. To
address this, we introduce WorldCloner, an end-to-end trainable neuro-symbolic
world model for rapid novelty adaptation. WorldCloner learns an efficient
symbolic representation of the pre-novelty environment transitions, and uses
this transition model to detect novelty and efficiently adapt to novelty in a
single-shot fashion. Additionally, WorldCloner augments the policy learning
process using imagination-based adaptation, where the world model simulates
transitions of the post-novelty environment to help the policy adapt. By
blending ''imagined'' transitions with interactions in the post-novelty
environment, performance can be recovered with fewer total environment
interactions. Using environments designed for studying novelty in sequential
decision-making problems, we show that the symbolic world model helps its
neural policy adapt more efficiently than model-based and model-based
neural-only reinforcement learning methods.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WorldClonerï¼šå¿«é€Ÿé€‚åº”å¼€æ”¾ä¸–ç•Œæ–°å¥‡çš„ç¥ç»ç¬¦å·ä¸–ç•Œæ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå¼€æ”¾ä¸–ç•Œçš„æ–°å¥‡æ€§â€”â€”å³ç¯å¢ƒæœºåˆ¶æˆ–å±æ€§çš„çªç„¶å˜åŒ–â€”â€”æ˜¯ä¸€ç§å¸¸è§çš„ç°è±¡ã€‚æ–°å¥‡æ€§é€‚åº”æ˜¯æŒ‡ä»£ç†åœ¨æ–°å¥‡æ€§ä¹‹åæé«˜å…¶ç­–ç•¥æ€§èƒ½çš„èƒ½åŠ›ã€‚å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å‡è®¾ä¸–ç•Œæ˜¯å°é—­çš„ã€å›ºå®šçš„è¿‡ç¨‹ã€‚å› æ­¤ï¼ŒRLç­–ç•¥å¯¹æ–°å¥‡æ€§çš„é€‚åº”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†WorldClonerï¼Œä¸€ä¸ªç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ç¥ç»ç¬¦å·ä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿæ–°å¥‡æ€§é€‚åº”ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç¬¦å·ä¸–ç•Œæ¨¡å‹
WorldClonerå­¦ä¹ ä¸€ä¸ªé«˜æ•ˆçš„ç¬¦å·è¡¨ç¤ºï¼Œç”¨äºè¡¨ç¤ºæ–°å¥‡æ€§ä¹‹å‰çš„ç¯å¢ƒè½¬æ¢ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªè½¬æ¢æ¨¡å‹æ¥æ£€æµ‹æ–°å¥‡æ€§ï¼Œå¹¶ä»¥å•æ¬¡æ–¹å¼é«˜æ•ˆåœ°é€‚åº”æ–°å¥‡æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºæƒ³è±¡çš„é€‚åº”
WorldClonerä½¿ç”¨åŸºäºæƒ³è±¡çš„é€‚åº”æ¥å¢å¼ºç­–ç•¥å­¦ä¹ è¿‡ç¨‹ï¼Œå…¶ä¸­ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿæ–°å¥‡æ€§ä¹‹åçš„ç¯å¢ƒè½¬æ¢ï¼Œä»¥å¸®åŠ©ç­–ç•¥é€‚åº”ã€‚é€šè¿‡å°†â€œæƒ³è±¡â€çš„è½¬æ¢ä¸æ–°å¥‡æ€§ä¹‹åçš„ç¯å¢ƒä¸­çš„äº¤äº’ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨æ›´å°‘çš„æ€»ç¯å¢ƒäº¤äº’ä¸­æ¢å¤æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ä¸“é—¨ä¸ºç ”ç©¶åºåˆ—å†³ç­–é—®é¢˜ä¸­çš„æ–°å¥‡æ€§è€Œè®¾è®¡çš„ç¯å¢ƒä¸­ï¼Œæœ¬æ–‡å±•ç¤ºäº†ç¬¦å·ä¸–ç•Œæ¨¡å‹å¸®åŠ©å…¶ç¥ç»ç­–ç•¥æ¯”åŸºäºæ¨¡å‹å’ŒåŸºäºæ¨¡å‹çš„ç¥ç»å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ›´æœ‰æ•ˆåœ°é€‚åº”ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„WorldCloneræ¨¡å‹ä¸ºå¼€æ”¾ä¸–ç•Œå­¦ä¹ ä¸­çš„æ–°å¥‡æ€§é€‚åº”æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å…¶ç¬¦å·ä¸–ç•Œæ¨¡å‹å’ŒåŸºäºæƒ³è±¡çš„é€‚åº”æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜ä»£ç†å¯¹æ–°å¥‡æ€§çš„é€‚åº”æ•ˆç‡ï¼Œå‡å°‘å¯¹çœŸå®ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼Œç¬¦å·è¡¨ç¤ºæ˜¯é€‚åº”å¼ºåŒ–å­¦ä¹ ä»£ç†æ–°å¥‡æ€§çš„æœ‰æ•ˆè¡¥å……ã€‚

## webshop--towards-scalable-real-world-web-interaction-with-grounded-language-agents
### Abstract
Existing benchmarks for grounding language in interactive environments either
lack real-world linguistic elements, or prove difficult to scale up due to
substantial human involvement in the collection of data or feedback signals. To
bridge this gap, we develop WebShop -- a simulated e-commerce website
environment with $1.18$ million real-world products and $12,087$ crowd-sourced
text instructions. Given a text instruction specifying a product requirement,
an agent needs to navigate multiple types of webpages and issue diverse actions
to find, customize, and purchase an item. WebShop provides several challenges
for language grounding including understanding compositional instructions,
query (re-)formulation, comprehending and acting on noisy text in webpages, and
performing strategic exploration. We collect over $1,600$ human demonstrations
for the task, and train and evaluate a diverse range of agents using
reinforcement learning, imitation learning, and pre-trained image and language
models. Our best model achieves a task success rate of $29\%$, which
outperforms rule-based heuristics ($9.6\%$) but is far lower than human expert
performance ($59\%$). We also analyze agent and human trajectories and ablate
various model components to provide insights for developing future agents with
stronger language understanding and decision making abilities. Finally, we show
that agents trained on WebShop exhibit non-trivial sim-to-real transfer when
evaluated on amazon.com and ebay.com, indicating the potential value of WebShop
in developing practical web-based agents that can operate in the wild.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebShopï¼šè¿ˆå‘å¯æ‰©å±•çš„åŸºäºè¯­è¨€äº¤äº’çš„Webç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰çš„è¯­è¨€äº¤äº’ç¯å¢ƒåœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œè¯­è¨€å…ƒç´ æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæˆ–è€…ç”±äºå¤§é‡äººå·¥å‚ä¸æ•°æ®æ”¶é›†æˆ–åé¦ˆä¿¡å·è€Œéš¾ä»¥æ‰©å±•ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†WebShopï¼Œä¸€ä¸ªå…·æœ‰118ä¸‡çœŸå®ä¸–ç•Œäº§å“å’Œ12087ä¸ªä¼—åŒ…æ–‡æœ¬æŒ‡ä»¤çš„æ¨¡æ‹Ÿç”µå­å•†åŠ¡ç½‘ç«™ç¯å¢ƒã€‚WebShopä¸ºè¯­è¨€æ¥åœ°æä¾›äº†å‡ ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç†è§£ç»„åˆæŒ‡ä»¤ã€æŸ¥è¯¢ï¼ˆé‡æ–°ï¼‰åˆ¶å®šã€ç†è§£å’Œåœ¨ç½‘é¡µä¸­æ‰§è¡Œå™ªå£°æ–‡æœ¬ï¼Œä»¥åŠæ‰§è¡Œæˆ˜ç•¥æ¢ç´¢ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWebShopç¯å¢ƒ
WebShopæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿç”µå­å•†åŠ¡ç½‘ç«™çš„ç¯å¢ƒï¼Œå…·æœ‰è¶…è¿‡118ä¸‡çœŸå®ä¸–ç•Œäº§å“å’Œ12087ä¸ªä¼—åŒ…æ–‡æœ¬æŒ‡ä»¤ã€‚ç»™å®šä¸€ä¸ªæŒ‡å®šäº§å“è¦æ±‚çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œä»£ç†éœ€è¦å¯¼èˆªå¤šç§ç±»å‹çš„ç½‘é¡µå¹¶å‘å‡ºå¤šç§åŠ¨ä½œæ¥æŸ¥æ‰¾ã€å®šåˆ¶å’Œè´­ä¹°å•†å“ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç§å­¦ä¹ æ–¹æ³•
æœ¬æ–‡æ”¶é›†äº†è¶…è¿‡1600ä¸ªäººç±»æ¼”ç¤ºï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ã€æ¨¡ä»¿å­¦ä¹ å’Œé¢„è®­ç»ƒçš„å›¾åƒå’Œè¯­è¨€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°äº†å„ç§ä»£ç†ã€‚æœ€ä½³æ¨¡å‹çš„ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ°äº†29%ï¼Œè¶…è¿‡äº†åŸºäºè§„åˆ™çš„å¯å‘å¼æ–¹æ³•ï¼ˆ9.6%ï¼‰ï¼Œä½†è¿œä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ï¼ˆ59%ï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡å‹åˆ†æå’Œæ¶ˆèç ”ç©¶
æœ¬æ–‡åˆ†æäº†ä»£ç†å’Œäººç±»è½¨è¿¹ï¼Œå¹¶æ¶ˆèäº†å„ç§æ¨¡å‹ç»„ä»¶ï¼Œä»¥æä¾›æœ‰å…³å¼€å‘å…·æœ‰æ›´å¼ºè¯­è¨€ç†è§£å’Œå†³ç­–èƒ½åŠ›çš„æœªæ¥ä»£ç†çš„è§è§£ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šSim-to-realè¿ç§»
æœ€åï¼Œæœ¬æ–‡è¡¨æ˜ï¼Œåœ¨WebShopä¸Šè®­ç»ƒçš„ä»£ç†åœ¨amazon.comå’Œebay.comä¸Šè¿›è¡Œè¯„ä¼°æ—¶è¡¨ç°å‡ºéå¹³å‡¡çš„Sim-to-realè¿ç§»ï¼Œè¡¨æ˜WebShopåœ¨å¼€å‘èƒ½å¤Ÿåœ¨é‡å¤–æ“ä½œçš„å®ç”¨Webä»£ç†æ–¹é¢çš„æ½œåœ¨ä»·å€¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€ä½³æ¨¡å‹çš„ä»»åŠ¡æˆåŠŸç‡ä¸º29%ï¼Œè¶…è¿‡äº†åŸºäºè§„åˆ™çš„å¯å‘å¼æ–¹æ³•ï¼ˆ9.6%ï¼‰ï¼Œä½†è¿œä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ï¼ˆ59%ï¼‰ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨amazon.comå’Œebay.comä¸Šçš„è¡¨ç°ä¸åœ¨WebShopä¸Šçš„è¡¨ç°ç›¸ä¼¼ï¼Œè¡¨æ˜äº†Sim-to-realè¿ç§»çš„æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebShopä¸ºå¼€å‘èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•ŒWebç¯å¢ƒä¸­æ“ä½œçš„å®ç”¨ä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•å’Œæ¨¡å‹å¯ä»¥ç”¨äºå¼€å‘å…·æœ‰æ›´å¼ºè¯­è¨€ç†è§£å’Œå†³ç­–èƒ½åŠ›çš„æœªæ¥ä»£ç†ã€‚æ­¤å¤–ï¼ŒWebShopç¯å¢ƒå¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å„ç§è¯­è¨€äº¤äº’ä»»åŠ¡ä¸­çš„ä»£ç†æ€§èƒ½ã€‚

## react--synergizing-reasoning-and-acting-in-language-models
### Abstract
While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help the model induce, track, and update
action plans as well as handle exceptions, while actions allow it to interface
with external sources, such as knowledge bases or environments, to gather
additional information. We apply our approach, named ReAct, to a diverse set of
language and decision making tasks and demonstrate its effectiveness over
state-of-the-art baselines, as well as improved human interpretability and
trustworthiness over methods without reasoning or acting components.
Concretely, on question answering (HotpotQA) and fact verification (Fever),
ReAct overcomes issues of hallucination and error propagation prevalent in
chain-of-thought reasoning by interacting with a simple Wikipedia API, and
generates human-like task-solving trajectories that are more interpretable than
baselines without reasoning traces. On two interactive decision making
benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and
reinforcement learning methods by an absolute success rate of 34% and 10%
respectively, while being prompted with only one or two in-context examples.
Project site with code: https://react-lm.github.io
ä¹‹å¤„ï¼Œæœ¬æ–‡æå‡ºäº†ReActæ–¹æ³•ï¼Œæ—¨åœ¨å°†æ¨ç†å’Œè¡ŒåŠ¨ç›¸ç»“åˆï¼Œä»¥è§£å†³è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œè¡ŒåŠ¨æ–¹é¢çš„ä¸è¶³ã€‚ReActæ–¹æ³•é€šè¿‡äº¤æ›¿ç”Ÿæˆæ¨ç†è½¨è¿¹å’Œç‰¹å®šä»»åŠ¡çš„æ“ä½œï¼Œå®ç°äº†æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´çš„ååŒä½œç”¨ã€‚æ¨ç†è½¨è¿¹å¸®åŠ©æ¨¡å‹è¯±å¯¼ã€è·Ÿè¸ªå’Œæ›´æ–°æ“ä½œè®¡åˆ’ï¼Œå¹¶å¤„ç†å¼‚å¸¸æƒ…å†µï¼›è€Œæ“ä½œåˆ™å…è®¸æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†åº“æˆ–ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

### ğŸŒŸ è®ºæ–‡äº®ç‚¹
* **æ¨ç†ä¸è¡ŒåŠ¨ååŒ**ï¼šReActæ–¹æ³•å°†æ¨ç†å’Œè¡ŒåŠ¨ç›¸ç»“åˆï¼Œå®ç°äº†æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚
* **å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦**ï¼šReActæ–¹æ³•ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å’Œæ“ä½œè®¡åˆ’æ›´åŠ å¯è§£é‡Šå’Œå¯ä¿¡ï¼Œæœ‰åŠ©äºäººç±»ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚
* **å¤šä»»åŠ¡é€‚ç”¨æ€§**ï¼šReActæ–¹æ³•åœ¨é—®ç­”ã€äº‹å®æ ¸æŸ¥ã€äº¤äº’å¼å†³ç­–ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶å¤šä»»åŠ¡é€‚ç”¨æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
* **é—®ç­”å’Œäº‹å®æ ¸æŸ¥**ï¼šReActæ–¹æ³•åœ¨HotpotQAå’ŒFeveræ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå…‹æœäº†é“¾å¼æ€ç»´æ¨ç†ä¸­å¸¸è§çš„å¹»è§‰å’Œé”™è¯¯ä¼ æ’­é—®é¢˜ã€‚
* **äº¤äº’å¼å†³ç­–**ï¼šReActæ–¹æ³•åœ¨ALFWorldå’ŒWebShopæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

### ğŸŒˆ æœªæ¥å±•æœ›
* **å¤šä»»åŠ¡è®­ç»ƒ**ï¼šæœªæ¥å¯ä»¥æ¢ç´¢å°†ReActæ–¹æ³•åº”ç”¨äºæ›´å¤šä»»åŠ¡ï¼Œå¹¶è¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
* **æ¨¡å‹å¯è§£é‡Šæ€§**ï¼šæœªæ¥å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•æé«˜ReActæ–¹æ³•ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å’Œæ“ä½œè®¡åˆ’çš„å¯è§£é‡Šæ€§ï¼Œä»¥ä¾¿äººç±»æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚
* **æ¨¡å‹é²æ£’æ€§**ï¼šæœªæ¥å¯ä»¥ç ”ç©¶å¦‚ä½•æé«˜ReActæ–¹æ³•çš„é²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å‘æŒ¥ä½œç”¨ã€‚

### ğŸ“š å‚è€ƒæ–‡çŒ®
...

### ğŸ¯ æ€»ç»“
ReActæ–¹æ³•æ˜¯ä¸€ç§å¾ˆæœ‰æ½œåŠ›çš„æ–¹æ³•ï¼Œå®ƒå°†æ¨ç†å’Œè¡ŒåŠ¨ç›¸ç»“åˆï¼Œä¸ºè¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œè¡ŒåŠ¨æ–¹é¢å¸¦æ¥äº†æ–°çš„çªç ´ã€‚ReActæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ç­‰ä¼˜ç‚¹ã€‚æœªæ¥ï¼ŒReActæ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®ã€‚

## make-llm-a-testing-expert--bringing-human-like-interaction-to-mobile-gui-testing-via-functionality-aware-decisions
### Abstract
Automated Graphical User Interface (GUI) testing plays a crucial role in
ensuring app quality, especially as mobile applications have become an integral
part of our daily lives. Despite the growing popularity of learning-based
techniques in automated GUI testing due to their ability to generate human-like
interactions, they still suffer from several limitations, such as low testing
coverage, inadequate generalization capabilities, and heavy reliance on
training data. Inspired by the success of Large Language Models (LLMs) like
ChatGPT in natural language understanding and question answering, we formulate
the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM
to chat with the mobile apps by passing the GUI page information to LLM to
elicit testing scripts, and executing them to keep passing the app feedback to
LLM, iterating the whole process. Within this framework, we have also
introduced a functionality-aware memory prompting mechanism that equips the LLM
with the ability to retain testing knowledge of the whole process and conduct
long-term, functionality-based reasoning to guide exploration. We evaluate it
on 93 apps from Google Play and demonstrate that it outperforms the best
baseline by 32% in activity coverage, and detects 31% more bugs at a faster
rate. Moreover, GPTDroid identify 53 new bugs on Google Play, of which 35 have
been confirmed and fixed.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤§å‹è¯­è¨€æ¨¡å‹æˆä¸ºæµ‹è¯•ä¸“å®¶ï¼šé€šè¿‡åŠŸèƒ½æ„ŸçŸ¥å†³ç­–å°†ç±»ä¼¼äººç±»çš„äº¤äº’å¼•å…¥ç§»åŠ¨GUIæµ‹è¯•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç§»åŠ¨åº”ç”¨ç¨‹åºåœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œç¡®ä¿åº”ç”¨ç¨‹åºè´¨é‡å˜å¾—è‡³å…³é‡è¦ã€‚è‡ªåŠ¨åŒ–å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æµ‹è¯•åœ¨ç¡®ä¿åº”ç”¨ç¨‹åºè´¨é‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå­¦ä¹ çš„è‡ªåŠ¨åŒ–GUIæµ‹è¯•æŠ€æœ¯ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚æµ‹è¯•è¦†ç›–ç‡ä½ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³ä»¥åŠå¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–æ€§å¤§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGPTDroidçš„è‡ªåŠ¨åŒ–GUIæµ‹è¯•æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç§»åŠ¨GUIæµ‹è¯•é—®é¢˜è§†ä¸ºä¸€ä¸ªé—®ç­”ä»»åŠ¡ã€‚GPTDroidé€šè¿‡å°†GUIé¡µé¢ä¿¡æ¯ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥ç”Ÿæˆæµ‹è¯•è„šæœ¬ï¼Œå¹¶æ‰§è¡Œè¿™äº›è„šæœ¬ï¼Œç„¶åå°†åº”ç”¨ç¨‹åºçš„åé¦ˆä¼ é€’ç»™LLMï¼Œä»è€Œå®ç°ä¸ç§»åŠ¨åº”ç”¨ç¨‹åºçš„äº¤äº’ã€‚æ­¤å¤–ï¼ŒGPTDroidè¿˜å¼•å…¥äº†ä¸€ç§åŠŸèƒ½æ„ŸçŸ¥è®°å¿†æç¤ºæœºåˆ¶ï¼Œä½¿LLMèƒ½å¤Ÿä¿ç•™æ•´ä¸ªæµ‹è¯•è¿‡ç¨‹ä¸­çš„æµ‹è¯•çŸ¥è¯†ï¼Œå¹¶æŒ‡å¯¼æ¢ç´¢ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Google Playä¸Šçš„93ä¸ªåº”ç”¨ç¨‹åºä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒGPTDroidåœ¨æ´»åŠ¨è¦†ç›–ç‡æ–¹é¢æ¯”æœ€ä½³åŸºçº¿é«˜å‡º32%ï¼Œå¹¶ä»¥æ›´å¿«çš„é€Ÿåº¦æ£€æµ‹åˆ°31%çš„æ›´å¤šé”™è¯¯ã€‚æ­¤å¤–ï¼ŒGPTDroidåœ¨Google Playä¸Šå‘ç°äº†53ä¸ªæ–°é”™è¯¯ï¼Œå…¶ä¸­35ä¸ªå·²å¾—åˆ°ç¡®è®¤å’Œä¿®å¤ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GPTDroidçš„æˆåŠŸè¡¨æ˜ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨åŒ–GUIæµ‹è¯•å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æµ‹è¯•è¦†ç›–ç‡ï¼Œè¿˜æé«˜äº†é”™è¯¯æ£€æµ‹ç‡ã€‚æ­¤å¤–ï¼ŒGPTDroidçš„åŠŸèƒ½æ„ŸçŸ¥è®°å¿†æç¤ºæœºåˆ¶ä¸ºLLMæä¾›äº†é•¿æœŸæ¨ç†çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åº”ç”¨ç¨‹åºçš„åŠŸèƒ½å¹¶æŒ‡å¯¼æ¢ç´¢ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–GUIæµ‹è¯•å·¥å…·æä¾›äº†æ–°çš„æ€è·¯ã€‚

## visualwebbench--how-far-have-multimodal-llms-evolved-in-web-page-understanding-and-grounding-
### Abstract
Multimodal Large Language models (MLLMs) have shown promise in web-related
tasks, but evaluating their performance in the web domain remains a challenge
due to the lack of comprehensive benchmarks. Existing benchmarks are either
designed for general multimodal tasks, failing to capture the unique
characteristics of web pages, or focus on end-to-end web agent tasks, unable to
measure fine-grained abilities such as OCR, understanding, and grounding. In
this paper, we introduce \bench{}, a multimodal benchmark designed to assess
the capabilities of MLLMs across a variety of web tasks. \bench{} consists of
seven tasks, and comprises 1.5K human-curated instances from 139 real websites,
covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3
series, and GPT-4V(ision) on \bench{}, revealing significant challenges and
performance gaps. Further analysis highlights the limitations of current MLLMs,
including inadequate grounding in text-rich environments and subpar performance
with low-resolution image inputs. We believe \bench{} will serve as a valuable
resource for the research community and contribute to the creation of more
powerful and versatile MLLMs for web-related applications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VisualWebBenchï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç½‘é¡µç†è§£å’Œå®šä½æ–¹é¢çš„è¿›å±•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†ç½‘ç»œç›¸å…³ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç½‘ç»œé¢†åŸŸçš„æ€§èƒ½å´é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•è¦ä¹ˆæ˜¯ä¸ºé€šç”¨å¤šæ¨¡æ€ä»»åŠ¡è®¾è®¡çš„ï¼Œæ— æ³•æ•æ‰ç½‘é¡µçš„ç‹¬ç‰¹ç‰¹å¾ï¼Œè¦ä¹ˆä¸“æ³¨äºç«¯åˆ°ç«¯çš„ç½‘ç»œä»£ç†ä»»åŠ¡ï¼Œæ— æ³•è¡¡é‡ç»†ç²’åº¦çš„èƒ½åŠ›ï¼Œå¦‚OCRã€ç†è§£å’Œå®šä½ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†VisualWebBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsåœ¨å„ç§ç½‘ç»œä»»åŠ¡ä¸­èƒ½åŠ›çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šVisualWebBenchç”±ä¸ƒä¸ªä»»åŠ¡ç»„æˆï¼ŒåŒ…æ‹¬ç½‘é¡µæ ‡é¢˜OCRã€ç½‘é¡µQAã€å…ƒç´ OCRã€å…ƒç´ å®šä½ã€åŠ¨ä½œé¢„æµ‹å’ŒåŠ¨ä½œå®šä½ï¼Œæ¶µç›–äº†ç½‘é¡µã€å…ƒç´ å’Œç”¨æˆ·åŠ¨ä½œä¸‰ä¸ªä¸åŒå±‚æ¬¡çš„ç†è§£å’Œå®šä½èƒ½åŠ›ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ¥è‡ª139ä¸ªçœŸå®ç½‘ç«™çš„1.5Kä¸ªäººå·¥ç­–åˆ’çš„å®ä¾‹ï¼Œæ¶µç›–äº†87ä¸ªå­åŸŸï¼Œç¡®ä¿äº†è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨VisualWebBenchä¸Šè¯„ä¼°äº†14ä¸ªå¼€æºMLLMsã€Gemini Proã€Claude-3ç³»åˆ—å’ŒGPT-4V(ision)ï¼Œç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯GPT-4Vå’ŒClaude Sonnetç­‰æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œåœ¨VisualWebBenchä¸Šçš„å¹³å‡å¾—åˆ†ä¹Ÿåªæœ‰64.6å’Œ65.8ï¼Œè¡¨æ˜ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ­¤å¤–ï¼Œå¼€æºMLLMsä¸GPT-4Vå’ŒClaudeç³»åˆ—ç­‰ä¸“æœ‰æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
VisualWebBenchä¸ºè¯„ä¼°MLLMsåœ¨ç½‘ç»œç†è§£å’Œå®šä½æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–åŸºå‡†ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å¼ºå¤§ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ã€è‡ªä¸»ç½‘ç»œä»£ç†å’Œç½‘ç»œç›¸å…³åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†æµ‹è¯•è¿˜æ­ç¤ºäº†å½“å‰MLLMsçš„å±€é™æ€§ï¼ŒåŒ…æ‹¬åœ¨æ–‡æœ¬ä¸°å¯Œçš„ç¯å¢ƒä¸­å®šä½èƒ½åŠ›ä¸è¶³ä»¥åŠå¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒè¾“å…¥çš„æ€§èƒ½ä¸ä½³ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶å’Œæ¨¡å‹å¼€å‘æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚

## autoglm--autonomous-foundation-agents-for-guis
### Abstract
We present AutoGLM, a new series in the ChatGLM family, designed to serve as
foundation agents for autonomous control of digital devices through Graphical
User Interfaces (GUIs). While foundation models excel at acquiring human
knowledge, they often struggle with decision-making in dynamic real-world
environments, limiting their progress toward artificial general intelligence.
This limitation underscores the importance of developing foundation agents
capable of learning through autonomous environmental interactions by
reinforcing existing models. Focusing on Web Browser and Phone as
representative GUI scenarios, we have developed AutoGLM as a practical
foundation agent system for real-world GUI interactions. Our approach
integrates a comprehensive suite of techniques and infrastructures to create
deployable agent systems suitable for user delivery. Through this development,
we have derived two key insights: First, the design of an appropriate
"intermediate interface" for GUI control is crucial, enabling the separation of
planning and grounding behaviors, which require distinct optimization for
flexibility and accuracy respectively. Second, we have developed a novel
progressive training framework that enables self-evolving online curriculum
reinforcement learning for AutoGLM. Our evaluations demonstrate AutoGLM's
effectiveness across multiple domains. For web browsing, AutoGLM achieves a
55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second
attempt) and 96.2% on OpenTable evaluation tasks. In Android device control,
AutoGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7% on
common tasks in popular Chinese APPs.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AutoGLMï¼šè‡ªä¸»æ§åˆ¶æ•°å­—è®¾å¤‡çš„GUIåŸºç¡€æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è·å–äººç±»çŸ¥è¯†å’Œè¯­è¨€èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŠ¨æ€ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿›æ­¥ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†AutoGLMï¼Œä¸€ä¸ªåŸºäºChatGLMæ¨¡å‹å®¶æ—çš„ç³»åˆ—åŸºç¡€æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIsï¼‰è‡ªä¸»æ§åˆ¶æ•°å­—è®¾å¤‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸­é—´æ¥å£è®¾è®¡
AutoGLMé‡‡ç”¨äº†ä¸€ç§ä¸­é—´æ¥å£è®¾è®¡ï¼Œå°†è§„åˆ’è¡Œä¸ºå’Œæ¥åœ°è¡Œä¸ºåˆ†ç¦»ã€‚è§„åˆ’è¡Œä¸ºè´Ÿè´£åˆ¶å®šè¡ŒåŠ¨ç­–ç•¥ï¼Œè€Œæ¥åœ°è¡Œä¸ºè´Ÿè´£è¯†åˆ«å’Œæ“ä½œGUIå…ƒç´ ã€‚è¿™ç§åˆ†ç¦»ä½¿å¾—è§„åˆ’è¡Œä¸ºå¯ä»¥æ›´åŠ çµæ´»ï¼Œè€Œæ¥åœ°è¡Œä¸ºå¯ä»¥æ›´åŠ å‡†ç¡®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªè¿›åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ 
AutoGLMé‡‡ç”¨äº†ä¸€ç§è‡ªè¿›åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨çº¿æ–¹å¼é€æ­¥å¢åŠ ä»»åŠ¡çš„éš¾åº¦ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚è¿™ç§æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ä»»åŠ¡æ•°æ®ç¨€ç¼ºå’Œæ”¿ç­–åˆ†å¸ƒæ¼‚ç§»çš„é—®é¢˜ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AutoGLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œæµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚åœ¨Webæµè§ˆä»»åŠ¡ä¸­ï¼ŒAutoGLMåœ¨VAB-WebArena-Liteä¸Šå–å¾—äº†55.2%çš„æˆåŠŸç‡ï¼Œåœ¨OpenTableä¸Šå–å¾—äº†96.2%çš„æˆåŠŸç‡ã€‚åœ¨Androidè®¾å¤‡æ§åˆ¶ä»»åŠ¡ä¸­ï¼ŒAutoGLMåœ¨AndroidLabä¸Šå–å¾—äº†36.2%çš„æˆåŠŸç‡ï¼Œåœ¨å¸¸è§ä»»åŠ¡ä¸­å–å¾—äº†89.7%çš„æˆåŠŸç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AutoGLMçš„è®¾è®¡å’Œå®ç°ä¸ºå¼€å‘GUIåŸºç¡€æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å‚è€ƒã€‚å…¶ä¸­é—´æ¥å£è®¾è®¡å’Œè‡ªè¿›åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒAutoGLMçš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œé€šè¿‡è‡ªä¸»ç¯å¢ƒäº¤äº’æ¥å¼ºåŒ–ç°æœ‰æ¨¡å‹æ˜¯å¼€å‘åŸºç¡€æ™ºèƒ½ä½“çš„æœ‰æ•ˆé€”å¾„ã€‚

## fill-in-the-blank--context-aware-automated-text-input-generation-for-mobile-gui-testing
### Abstract
Automated GUI testing is widely used to help ensure the quality of mobile
apps. However, many GUIs require appropriate text inputs to proceed to the next
page which remains a prominent obstacle for testing coverage. Considering the
diversity and semantic requirement of valid inputs (e.g., flight departure,
movie name), it is challenging to automate the text input generation. Inspired
by the fact that the pre-trained Large Language Model (LLM) has made
outstanding progress in text generation, we propose an approach named QTypist
based on LLM for intelligently generating semantic input text according to the
GUI context. To boost the performance of LLM in the mobile testing scenario, we
develop a prompt-based data construction and tuning method which automatically
extracts the prompts and answers for model tuning. We evaluate QTypist on 106
apps from Google Play and the result shows that the passing rate of QTypist is
87%, which is 93% higher than the best baseline. We also integrate QTypist with
the automated GUI testing tools and it can cover 42% more app activities, 52%
more pages, and subsequently help reveal 122% more bugs compared with the raw
tool.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¡«ç©ºå¼ï¼šåŸºäºä¸Šä¸‹æ–‡çš„ç§»åŠ¨GUIæµ‹è¯•è‡ªåŠ¨åŒ–æ–‡æœ¬è¾“å…¥ç”Ÿæˆ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç§»åŠ¨åº”ç”¨ï¼ˆAppï¼‰åœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­æ‰®æ¼”ç€ä¸å¯æˆ–ç¼ºçš„è§’è‰²ï¼Œç„¶è€Œï¼Œä¿è¯Appçš„è´¨é‡å´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æµ‹è¯•æ˜¯ç¡®ä¿AppåŠŸèƒ½æ­£ç¡®æ€§çš„é‡è¦æ‰‹æ®µï¼Œä½†è®¸å¤šGUIé¡µé¢éœ€è¦ç‰¹å®šçš„æ–‡æœ¬è¾“å…¥æ‰èƒ½è¿›å…¥ä¸‹ä¸€é¡µï¼Œè¿™æˆä¸ºäº†è‡ªåŠ¨åŒ–æµ‹è¯•è¦†ç›–ç‡çš„éšœç¢ã€‚ç”±äºæœ‰æ•ˆè¾“å…¥çš„å¤šæ ·æ€§å’Œè¯­ä¹‰è¦æ±‚ï¼Œè‡ªåŠ¨åŒ–æ–‡æœ¬è¾“å…¥ç”Ÿæˆä¸€ç›´æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºQTypistçš„æ–¹æ³•ï¼ŒåŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ™ºèƒ½åœ°ç”Ÿæˆè¯­ä¹‰è¾“å…¥æ–‡æœ¬ï¼Œä»¥é€‚åº”GUIä¸Šä¸‹æ–‡ã€‚ä¸ºäº†æé«˜LLMåœ¨ç§»åŠ¨æµ‹è¯•åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæç¤ºçš„æ•°æ®æ„å»ºå’Œå¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è‡ªåŠ¨æå–æ¨¡å‹å¾®è°ƒçš„æç¤ºå’Œç­”æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æˆ‘ä»¬åœ¨Google Playä¸Šå¯¹106ä¸ªåº”ç”¨ç¨‹åºè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºQTypistçš„é€šè¿‡ç‡ä¸º87%ï¼Œæ¯”æœ€ä½³åŸºçº¿é«˜å‡º93%ã€‚æˆ‘ä»¬è¿˜å°†QTypistä¸è‡ªåŠ¨åŒ–GUIæµ‹è¯•å·¥å…·é›†æˆï¼Œå®ƒå¯ä»¥è¦†ç›–42%æ›´å¤šçš„åº”ç”¨ç¨‹åºæ´»åŠ¨ï¼Œ52%æ›´å¤šçš„é¡µé¢ï¼Œå¹¶å¸®åŠ©æ­ç¤º122%æ›´å¤šçš„é”™è¯¯ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
QTypistä¸ºè‡ªåŠ¨åŒ–GUIæµ‹è¯•æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„æ–‡æœ¬è¾“å…¥ï¼Œä»è€Œæé«˜æµ‹è¯•è¦†ç›–ç‡ã€‚æ­¤å¤–ï¼ŒQTypistçš„æ•°æ®æ„å»ºå’Œå¾®è°ƒæ–¹æ³•ä¹Ÿä¸ºå…¶ä»–é¢†åŸŸæä¾›äº†å¯ç¤ºï¼Œä¾‹å¦‚å¦‚ä½•åˆ©ç”¨ç°æœ‰æ•°æ®æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

## omniparser-for-pure-vision-based-gui-agent
### Abstract
The recent success of large vision language models shows great potential in
driving the agent system operating on user interfaces. However, we argue that
the power multimodal models like GPT-4V as a general agent on multiple
operating systems across different applications is largely underestimated due
to the lack of a robust screen parsing technique capable of: 1) reliably
identifying interactable icons within the user interface, and 2) understanding
the semantics of various elements in a screenshot and accurately associate the
intended action with the corresponding region on the screen. To fill these
gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user
interface screenshots into structured elements, which significantly enhances
the ability of GPT-4V to generate actions that can be accurately grounded in
the corresponding regions of the interface. We first curated an interactable
icon detection dataset using popular webpages and an icon description dataset.
These datasets were utilized to fine-tune specialized models: a detection model
to parse interactable regions on the screen and a caption model to extract the
functional semantics of the detected elements. \textsc{OmniParser}
significantly improves GPT-4V's performance on ScreenSpot benchmark. And on
Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input
outperforms the GPT-4V baselines requiring additional information outside of
screenshot.
### ğŸŒŸ è®ºæ–‡è§£è¯» | OmniParserï¼šåŸºäºçº¯è§†è§‰çš„GUIæ™ºèƒ½ä½“è§£æå™¨

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æˆåŠŸï¼Œå®ƒä»¬åœ¨ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ä¸Šæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4Våœ¨è·¨å¹³å°å’Œè·¨åº”ç”¨ç¨‹åºçš„é€šç”¨æ€§æ–¹é¢è¢«ä½ä¼°äº†ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹ä¸€ç§èƒ½å¤Ÿå¯é åœ°è¯†åˆ«ç”¨æˆ·ç•Œé¢ä¸­å¯äº¤äº’å›¾æ ‡å¹¶ç†è§£å±å¹•æˆªå›¾ä¸­çš„å„ç§å…ƒç´ è¯­ä¹‰çš„å±å¹•è§£ææŠ€æœ¯ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†OmniParserï¼Œä¸€ç§å°†ç”¨æˆ·ç•Œé¢å±å¹•æˆªå›¾è§£æä¸ºç»“æ„åŒ–å…ƒç´ çš„ç»¼åˆæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†GPT-4Vç”Ÿæˆå¯ä»¥å‡†ç¡®æ˜ å°„åˆ°ç•Œé¢ç›¸åº”åŒºåŸŸä¸Šçš„åŠ¨ä½œçš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤äº’åŒºåŸŸæ£€æµ‹
OmniParseré¦–å…ˆä½¿ç”¨ä»æµè¡Œç½‘é¡µçš„DOMæ ‘ä¸­æå–çš„è¾¹ç•Œæ¡†åˆ›å»ºäº†ä¸€ä¸ªå¯äº¤äº’åŒºåŸŸæ£€æµ‹æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†ç”¨äºå¾®è°ƒä¸€ä¸ªæ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è§£æå±å¹•ä¸Šçš„å¯äº¤äº’åŒºåŸŸã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠŸèƒ½è¯­ä¹‰æå–
ä¸ºäº†ç†è§£æ£€æµ‹åˆ°çš„å…ƒç´ çš„åŠŸèƒ½è¯­ä¹‰ï¼ŒOmniParserä½¿ç”¨äº†ä¸€ä¸ªå¾®è°ƒçš„å›¾æ ‡æè¿°æ¨¡å‹æ¥æå–æ£€æµ‹åˆ°çš„å…ƒç´ çš„åŠŸèƒ½æè¿°ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨OCRæ¨¡å—æ¥æå–æ–‡æœ¬çš„è¾¹ç•Œæ¡†ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯ä¸å›¾æ ‡æ£€æµ‹æ¨¡å—çš„ç»“æœåˆå¹¶ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»“åˆæœ¬åœ°è¯­ä¹‰
OmniParserå°†æœ¬åœ°è¯­ä¹‰ï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œå›¾æ ‡æè¿°ï¼‰ä¸å±å¹•æˆªå›¾è§†è§‰æç¤ºç›¸ç»“åˆï¼Œä»¥å¸®åŠ©GPT-4Væ›´å‡†ç¡®åœ°è¯†åˆ«è¦æ“ä½œçš„å…ƒç´ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
OmniParseråœ¨ScreenSpotåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†GPT-4Vçš„æ€§èƒ½ã€‚åœ¨Mind2Webå’ŒAITWåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨å±å¹•æˆªå›¾è¾“å…¥çš„OmniParserçš„æ€§èƒ½ä¼˜äºéœ€è¦å±å¹•æˆªå›¾ä¹‹å¤–é¢å¤–ä¿¡æ¯çš„GPT-4VåŸºçº¿ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
OmniParseræä¾›äº†ä¸€ç§é€šç”¨çš„å±å¹•è§£æå·¥å…·ï¼Œå¯ä»¥ä»UIå±å¹•æˆªå›¾ä¸­æå–ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ï¼Œä»è€Œå¢å¼ºGPT-4Våœ¨å„ç§ç”¨æˆ·ä»»åŠ¡ä¸­çš„åŠ¨ä½œé¢„æµ‹æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•ä¸ä¾èµ–äºé¢å¤–çš„ä¿¡æ¯ï¼Œå¦‚HTMLå’ŒAndroidè§†å›¾å±‚æ¬¡ç»“æ„ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªé€šç”¨çš„ã€æ˜“äºä½¿ç”¨çš„å·¥å…·ï¼Œå¯ä»¥åœ¨PCå’Œç§»åŠ¨å¹³å°ä¸Šè§£æä¸€èˆ¬çš„ç”¨æˆ·å±å¹•ã€‚

## weblinx--real-world-website-navigation-with-multi-turn-dialogue
### Abstract
We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebLINXï¼šåŸºäºå¤šè½®å¯¹è¯çš„çœŸå®ä¸–ç•Œç½‘ç«™å¯¼èˆª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€èŠå¤©æœºå™¨äººå¦‚ChatGPTç­‰æŠ€æœ¯çš„å‘å±•ï¼Œå®ƒä»¬å·²ç»èƒ½å¤Ÿé€šè¿‡æ’ä»¶æµè§ˆç½‘ç«™ï¼Œæ‰§è¡Œæ“ä½œå¹¶æä¾›æ›´æœ‰ç”¨çš„å“åº”ã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›æ˜¯æœ‰é™çš„ï¼Œå› ä¸ºæ’ä»¶å¿…é¡»ä¸ºæ¯ä¸ªç½‘ç«™å•ç‹¬å¼€å‘ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•æ¶µç›–ç½‘ç«™çš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦åˆ©ç”¨è¿™äº›åŠ©æ‰‹èƒŒåçš„æ¨¡å‹ï¼Œç›´æ¥åœ¨ç”¨æˆ·çš„æµè§ˆå™¨ä¸­å¯¼èˆªç½‘ç«™ï¼ŒåŒæ—¶ä¿ç•™å®ƒä»¬çš„å¯¹è¯èƒ½åŠ›ï¼Ÿ

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWEBLINXåŸºå‡†æ•°æ®é›†
ä¸ºäº†æ”¯æŒè¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WEBLINXï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2337ä¸ªä¸“å®¶æ¼”ç¤ºçš„å¤§å‹åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†150å¤šä¸ªçœŸå®ä¸–ç•Œç½‘ç«™ä¸Šçš„100Kä¸ªäº¤äº’ã€‚è¿™ä¸ªæ•°æ®é›†å¯ä»¥ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åœ¨å¤šç§åœºæ™¯ä¸­å¯¼èˆªçš„å¯¹è¯å¼ä»£ç†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDense Markup Ranking (DMR) æ¨¡å‹
ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ— æ³•å®æ—¶å¤„ç†æ•´ä¸ªç½‘é¡µï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªæ£€ç´¢å¯å‘çš„æ¨¡å‹ï¼Œé€šè¿‡æ’åç›¸å…³å…ƒç´ æ¥æœ‰æ•ˆåœ°å‰ªæHTMLé¡µé¢ã€‚ä½¿ç”¨é€‰å®šçš„å…ƒç´ ã€å±å¹•æˆªå›¾å’Œæ“ä½œå†å²ï¼Œè¯„ä¼°äº†å„ç§æ¨¡å‹åœ¨å¤åˆ¶äººç±»è¡Œä¸ºæ—¶çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œè¾ƒå°çš„å¾®è°ƒè§£ç å™¨ä¼˜äºæœ€ä½³é›¶æ ·æœ¬LLMsï¼ˆåŒ…æ‹¬GPT-4Vï¼‰ï¼Œä½†æ‰€æœ‰å¾®è°ƒæ¨¡å‹éƒ½éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„ç½‘ç«™ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ï¼Œå³æ„å»ºèƒ½å¤Ÿæ¨å¹¿åˆ°æ–°åœºæ™¯çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„Dense Markup Ranking (DMR) æ¨¡å‹ä¸ºå¤„ç†å¤§å‹ç½‘é¡µæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥ç”¨äºå…¶ä»–éœ€è¦å¤„ç†å¤§é‡æ–‡æœ¬æ•°æ®çš„ä»»åŠ¡ã€‚

## caution-for-the-environment--multimodal-agents-are-susceptible-to-environmental-distractions
### Abstract
This paper investigates the faithfulness of multimodal large language model
(MLLM) agents in the graphical user interface (GUI) environment, aiming to
address the research question of whether multimodal GUI agents can be
distracted by environmental context. A general setting is proposed where both
the user and the agent are benign, and the environment, while not malicious,
contains unrelated content. A wide range of MLLMs are evaluated as GUI agents
using our simulated dataset, following three working patterns with different
levels of perception. Experimental results reveal that even the most powerful
models, whether generalist agents or specialist GUI agents, are susceptible to
distractions. While recent studies predominantly focus on the helpfulness
(i.e., action accuracy) of multimodal agents, our findings indicate that these
agents are prone to environmental distractions, resulting in unfaithful
behaviors. Furthermore, we switch to the adversarial perspective and implement
environment injection, demonstrating that such unfaithfulness can be exploited,
leading to unexpected risks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€æ™ºèƒ½ä½“æ˜“å—ç¯å¢ƒå¹²æ‰°ï¼Œéœ€è­¦æƒ•ï¼

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨è§£å†³å¤æ‚äº¤äº’ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ™ºèƒ½ä½“åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ç€ç¯å¢ƒå¹²æ‰°çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå¹¿å‘Šã€é€šçŸ¥ç­‰æ— å…³å†…å®¹å¯èƒ½ä¼šå¹²æ‰°æ™ºèƒ½ä½“çš„ç›®æ ‡ï¼Œå¯¼è‡´å…¶è¡Œä¸ºåç¦»é¢„æœŸï¼Œç”šè‡³æ‰§è¡Œé”™è¯¯æ“ä½œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨GUIç¯å¢ƒä¸­çš„å¿ è¯šåº¦é—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶æå‡ºäº†ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰äº†ç¯å¢ƒå¹²æ‰°é—®é¢˜ï¼Œå¹¶æ„å»ºäº†åŒ…å«å››ç§åœºæ™¯ï¼ˆå¼¹å‡ºæ¡†ã€æœç´¢ã€æ¨èã€èŠå¤©ï¼‰çš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨ä¸åŒç¯å¢ƒä¸‹çš„è¡Œä¸ºã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†ä¸‰ç§å·¥ä½œæ¨¡å¼ï¼ˆç›´æ¥æç¤ºã€æ€ç»´é“¾æç¤ºã€åŠ¨ä½œæ ‡æ³¨ï¼‰ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒæ„ŸçŸ¥æ°´å¹³çš„æ™ºèƒ½ä½“ï¼Œå¹¶è¯„ä¼°å…¶å¯¹ç¯å¢ƒå¹²æ‰°çš„æ•æ„Ÿæ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºäº†ç¯å¢ƒæ³¨å…¥æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹ç¯å¢ƒå†…å®¹æ¥å¹²æ‰°æ™ºèƒ½ä½“ï¼Œä»è€ŒéªŒè¯å…¶æ˜“å—æ”»å‡»æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯åŠŸèƒ½å¼ºå¤§çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œä¹Ÿå®¹æ˜“å—åˆ°ç¯å¢ƒå¹²æ‰°ï¼Œå¯¼è‡´å…¶å¿ è¯šåº¦å’Œæœ‰æ•ˆæ€§ä¸‹é™ã€‚æ­¤å¤–ï¼Œç¯å¢ƒæ³¨å…¥æ”»å‡»æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹²æ‰°æ™ºèƒ½ä½“ï¼Œä½¿å…¶æ‰§è¡Œé”™è¯¯æ“ä½œã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦è€ƒè™‘ç¯å¢ƒå¹²æ‰°çš„å½±å“ï¼Œå¹¶é‡‡å–ç›¸åº”çš„æªæ–½æ¥æé«˜å…¶é²æ£’æ€§ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡é¢„è®­ç»ƒæ¥å¢å¼ºæ™ºèƒ½ä½“çš„å¿ è¯šåº¦ï¼Œæˆ–è€…å¼•å…¥äººç±»äº¤äº’æ¥è¾…åŠ©æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡ã€‚

### ğŸ“š å‚è€ƒæ–‡çŒ®
[1] Ma, X., Wang, Y., Yao, Y., Yuan, T., Zhang, A., Zhang, Z., & Zhao, H. (2024). Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions. arXiv preprint arXiv:2408.02544.

## gaia--a-benchmark-for-general-ai-assistants
### Abstract
We introduce GAIA, a benchmark for General AI Assistants that, if solved,
would represent a milestone in AI research. GAIA proposes real-world questions
that require a set of fundamental abilities such as reasoning, multi-modality
handling, web browsing, and generally tool-use proficiency. GAIA questions are
conceptually simple for humans yet challenging for most advanced AIs: we show
that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins.
This notable performance disparity contrasts with the recent trend of LLMs
outperforming humans on tasks requiring professional skills in e.g. law or
chemistry. GAIA's philosophy departs from the current trend in AI benchmarks
suggesting to target tasks that are ever more difficult for humans. We posit
that the advent of Artificial General Intelligence (AGI) hinges on a system's
capability to exhibit similar robustness as the average human does on such
questions. Using GAIA's methodology, we devise 466 questions and their answer.
We release our questions while retaining answers to 300 of them to power a
leader-board available at https://huggingface.co/gaia-benchmark.
### ğŸŒŸ è®ºæ–‡è§£è¯» | GAIAï¼šè¯„ä¼°é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹çš„åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„AIåŸºå‡†æµ‹è¯•å·²ç»æ— æ³•æ»¡è¶³è¯„ä¼°è¿™äº›æ¨¡å‹çš„éœ€æ±‚ã€‚è®¸å¤šLLMså·²ç»åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†äººç±»çš„è¡¨ç°ï¼Œä¾‹å¦‚åœ¨æ³•å¾‹æˆ–åŒ–å­¦ç­‰ä¸“ä¸šé¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›ä»»åŠ¡å¯¹äºäººç±»æ¥è¯´å¯èƒ½éå¸¸å›°éš¾ï¼Œè€Œå¯¹äºLLMsæ¥è¯´å´ç›¸å¯¹å®¹æ˜“ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMsåœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
GAIAæ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼ˆAGIï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒæå‡ºäº†éœ€è¦ä¸€ç³»åˆ—åŸºæœ¬èƒ½åŠ›çš„é—®é¢˜ï¼Œä¾‹å¦‚æ¨ç†ã€å¤šæ¨¡æ€å¤„ç†ã€ç½‘ç»œæµè§ˆå’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚GAIAçš„é—®é¢˜å¯¹äºäººç±»æ¥è¯´æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†å¯¹äºå¤§å¤šæ•°å…ˆè¿›çš„AIæ¥è¯´å´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäººç±»å—è®¿è€…è·å¾—äº†92%çš„å‡†ç¡®ç‡ï¼Œè€Œé…å¤‡äº†æ’ä»¶çš„GPT-4ä»…è·å¾—äº†15%çš„å‡†ç¡®ç‡ã€‚

GAIAçš„å“²å­¦ä¸å½“å‰AIåŸºå‡†æµ‹è¯•çš„è¶‹åŠ¿ä¸åŒï¼Œåè€…å»ºè®®é’ˆå¯¹å¯¹äººç±»æ¥è¯´è¶Šæ¥è¶Šå›°éš¾çš„ä»»åŠ¡ã€‚GAIAè®¤ä¸ºï¼Œé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‡ºç°å–å†³äºç³»ç»Ÿèƒ½å¦åœ¨è¿™äº›é—®é¢˜ä¸Šå±•ç°å‡ºä¸æ™®é€šäººç±»ç›¸ä¼¼çš„é²æ£’æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä½¿ç”¨GAIAçš„æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†466ä¸ªé—®é¢˜å’Œç­”æ¡ˆã€‚ä»–ä»¬å‘å¸ƒäº†è¿™äº›é—®é¢˜ï¼Œå¹¶ä¿ç•™äº†300ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œä»¥æ”¯æŒä¸€ä¸ªæ’è¡Œæ¦œï¼Œå¯åœ¨https://huggingface.co/gaia-benchmarkä¸Šè®¿é—®ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
GAIAçš„åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚å®ƒå¼ºè°ƒäº†ç°å®ä¸–ç•Œé—®é¢˜çš„é‡è¦æ€§ï¼Œå¹¶è¦æ±‚AIç³»ç»Ÿå…·å¤‡ä¸€ç³»åˆ—åŸºæœ¬èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGAIAçš„è¯„ä¼°æ–¹æ³•ç®€å•ã€å¿«é€Ÿä¸”æ˜“äºç†è§£ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°AIç³»ç»Ÿçš„ä¸€ç§æœ‰ç”¨å·¥å…·ã€‚

### ğŸŒŸ æ€»ç»“
GAIAæ˜¯ä¸€ä¸ªé‡è¦çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸ºè¯„ä¼°é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ã€‚å®ƒå¼ºè°ƒäº†ç°å®ä¸–ç•Œé—®é¢˜çš„é‡è¦æ€§ï¼Œå¹¶è¦æ±‚AIç³»ç»Ÿå…·å¤‡ä¸€ç³»åˆ—åŸºæœ¬èƒ½åŠ›ã€‚éšç€GAIAçš„ä¸æ–­å‘å±•ï¼Œå®ƒæœ‰æœ›æˆä¸ºè¯„ä¼°AIç³»ç»Ÿçš„ä¸€ç§é‡è¦å·¥å…·ï¼Œå¹¶æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚

## dynasaur--large-language-agents-beyond-predefined-actions
### Abstract
Existing LLM agent systems typically select actions from a fixed and
predefined set at every step. While this approach is effective in closed,
narrowly-scoped environments, we argue that it presents two major challenges
when deploying LLM agents in real-world scenarios: (1) selecting from a fixed
set of actions significantly restricts the planning and acting capabilities of
LLM agents, and (2) this approach requires substantial human effort to
enumerate and implement all possible actions, which becomes impractical in
complex environments with a vast number of potential actions. In this work, we
propose an LLM agent framework that enables the dynamic creation and
composition of actions in an online manner. In this framework, the agent
interacts with the environment by generating and executing programs written in
a general-purpose programming language at each step. Furthermore, generated
actions are accumulated over time for future reuse. Our extensive experiments
on the GAIA benchmark demonstrate that this framework offers significantly
greater flexibility and outperforms previous methods. Notably, it allows an LLM
agent to recover in scenarios where no relevant action exists in the predefined
set or when existing actions fail due to unforeseen edge cases. At the time of
writing, we hold the top position on the GAIA public leaderboard. Our code can
be found in
\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.
### ğŸŒŸ è®ºæ–‡è§£è¯» | DynaSaurï¼šè¶…è¶Šé¢„å®šä¹‰åŠ¨ä½œçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
ç°æœ‰çš„LLMä»£ç†ç³»ç»Ÿé€šå¸¸åœ¨æ¯ä¸ªæ­¥éª¤ä»å›ºå®šå’Œé¢„å®šä¹‰çš„åŠ¨ä½œé›†ä¸­é€‰æ‹©åŠ¨ä½œã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨å°é—­ã€èŒƒå›´ç‹­çª„çš„ç¯å¢ƒä¸­æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ä½œè€…è®¤ä¸ºï¼Œå½“åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­éƒ¨ç½²LLMä»£ç†æ—¶ï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š
1. ä»å›ºå®šåŠ¨ä½œé›†ä¸­é€‰æ‹©åŠ¨ä½œæ˜¾è‘—é™åˆ¶äº†LLMä»£ç†çš„è§„åˆ’å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚
2. è¿™ç§æ–¹æ³•éœ€è¦å¤§é‡çš„äººåŠ›æ¥æšä¸¾å’Œå®ç°æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œè¿™åœ¨å…·æœ‰å¤§é‡æ½œåœ¨åŠ¨ä½œçš„å¤æ‚ç¯å¢ƒä¸­å˜å¾—ä¸åˆ‡å®é™…ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†DynaSaurï¼Œä¸€ä¸ªLLMä»£ç†æ¡†æ¶ï¼Œå®ƒå…è®¸ä»¥åœ¨çº¿æ–¹å¼åŠ¨æ€åˆ›å»ºå’Œç»„åˆåŠ¨ä½œã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œä»£ç†é€šè¿‡åœ¨æ¯ä¸ªæ­¥éª¤ç”Ÿæˆå’Œæ‰§è¡Œç”¨é€šç”¨ç¼–ç¨‹è¯­è¨€ç¼–å†™çš„ç¨‹åºæ¥ä¸ç¯å¢ƒäº¤äº’ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„åŠ¨ä½œä¼šéšç€æ—¶é—´çš„æ¨ç§»ç§¯ç´¯èµ·æ¥ï¼Œä»¥ä¾¿å°†æ¥é‡ç”¨ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€åŠ¨ä½œåˆ›å»º
DynaSaurå°†æ¯ä¸ªåŠ¨ä½œå»ºæ¨¡ä¸ºPythonå‡½æ•°ï¼Œä»£ç†åœ¨æ¯ä¸ªæ­¥éª¤é€šè¿‡ç”ŸæˆPythonä»£ç ç‰‡æ®µæ¥æ‰§è¡ŒåŠ¨ä½œã€‚è¿™äº›ä»£ç ç‰‡æ®µè¦ä¹ˆå®šä¹‰æ–°çš„å‡½æ•°ï¼Œè¦ä¹ˆé‡ç”¨å½“å‰åŠ¨ä½œé›†ä¸­çš„ç°æœ‰å‡½æ•°ã€‚ç”Ÿæˆçš„ä»£ç é€šè¿‡Pythonè§£é‡Šå™¨æ‰§è¡Œï¼Œå¹¶å°†ç»“æœè§‚å¯Ÿè¿”å›ç»™ä»£ç†ã€‚æ‰€æœ‰ç”±ä»£ç†ç”Ÿæˆçš„åŠ¨ä½œéƒ½ä¼šç§¯ç´¯èµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªå¯é‡ç”¨çš„å‡½æ•°åº“ï¼Œä¾›å°†æ¥ä½¿ç”¨ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨ä½œæ£€ç´¢
ä¸ºäº†è§£å†³å°†æ‰€æœ‰ç”Ÿæˆçš„åŠ¨ä½œä½œä¸ºæç¤ºçš„ä¸€éƒ¨åˆ†å¯èƒ½å¯¼è‡´ä¸Šä¸‹æ–‡é™åˆ¶çš„é—®é¢˜ï¼ŒDynaSaurå°†åŠ¨ä½œé›†åˆ†è§£ä¸ºä¸¤ä¸ªå­é›†ï¼šäººç±»è®¾è®¡çš„åŠ¨ä½œé›†å’Œç”Ÿæˆçš„åŠ¨ä½œé›†ã€‚åªæœ‰äººç±»è®¾è®¡çš„åŠ¨ä½œé›†é»˜è®¤åŒ…å«åœ¨æç¤ºä¸­ã€‚ä¸ºäº†æä¾›ä»£ç†å¯¹ç”ŸæˆåŠ¨ä½œé›†çš„è®¿é—®ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŠ¨ä½œæ£€ç´¢å‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®æŸ¥è¯¢å’Œæ•´æ•°kè¿”å›æœ€ç›¸ä¼¼çš„kä¸ªåŠ¨ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDynaSauræ¡†æ¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ï¼Œå¹¶ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒå…è®¸LLMä»£ç†åœ¨é¢„å®šä¹‰é›†ä¸­ä¸å­˜åœ¨ç›¸å…³åŠ¨ä½œæˆ–ç°æœ‰åŠ¨ä½œç”±äºæ„å¤–çš„è¾¹ç¼˜æƒ…å†µè€Œå¤±è´¥çš„æƒ…å†µä¸‹æ¢å¤ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
DynaSauræ¡†æ¶ä¸ºLLMä»£ç†åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡åŠ¨æ€åˆ›å»ºå’Œç»„åˆåŠ¨ä½œï¼Œä»£ç†å¯ä»¥æ›´å¥½åœ°é€‚åº”å¤æ‚å’Œä¸ç¡®å®šçš„ç¯å¢ƒï¼Œå¹¶ä»è¿‡å»çš„ç»éªŒä¸­å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒDynaSauræ¡†æ¶è¿˜å¯ä»¥ä¸å…¶ä»–å·¥å…·å’Œåº“æ— ç¼é›†æˆï¼Œè¿›ä¸€æ­¥æé«˜ä»£ç†çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚

## mobileflow--a-multimodal-llm-for-mobile-gui-agent
### Abstract
Currently, the integration of mobile Graphical User Interfaces (GUIs) is
ubiquitous in most people's daily lives. And the ongoing evolution of
multimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly
bolstered the capabilities of GUI comprehension and user action analysis,
showcasing the potentiality of intelligent GUI assistants. However, current GUI
Agents often need to access page layout information through calling system
APIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a
certain low resolution might result in the loss of fine-grained image details.
At the same time, the multimodal large models built for GUI Agents currently
have poor understanding and decision-making abilities for Chinese GUI
interfaces, making them difficult to apply to a large number of Chinese apps.
This paper introduces MobileFlow, a multimodal large language model
meticulously crafted for mobile GUI agents. Transforming from the open-source
model Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21
billion parameters and is equipped with novel hybrid visual encoders, making it
possible for variable resolutions of image inputs and good support for
multilingual GUI. By incorporating Mixture of Experts (MoE) expansions and
pioneering alignment training strategies, MobileFlow has the capacity to fully
interpret image data and comprehend user instructions for GUI interaction
tasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task
execution by GUI agents on both public and our proposed evaluation metrics, and
has been successfully deployed in real-world business contexts, proving its
effectiveness for practical applications.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MobileFlowï¼šç§»åŠ¨GUIæ™ºèƒ½åŠ©æ‰‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ—¥ç›Šæ™®åŠï¼Œæ™ºèƒ½GUIåŠ©æ‰‹çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„GUIåŠ©æ‰‹åœ¨å¤„ç†ä¸­æ–‡GUIç•Œé¢æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”åœ¨è·å–é¡µé¢å¸ƒå±€ä¿¡æ¯æ—¶å¯èƒ½æ¶‰åŠéšç§é£é™©ã€‚æ­¤å¤–ï¼Œå›ºå®šGUIåˆ†è¾¨ç‡å¯èƒ½å¯¼è‡´å›¾åƒç»†èŠ‚ä¸¢å¤±ï¼Œé™åˆ¶äº†GUIåŠ©æ‰‹çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
MobileFlowæ˜¯ä¸€ä¸ªä¸“ä¸ºç§»åŠ¨GUIåŠ©æ‰‹è®¾è®¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ··åˆè§†è§‰ç¼–ç å™¨
MobileFlowé‡‡ç”¨äº†æ··åˆè§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒè¾“å…¥ï¼Œå¹¶æ”¯æŒå¤šè¯­è¨€GUIã€‚é€šè¿‡ä½¿ç”¨LayoutLMv3ä½œä¸ºUIç¼–ç å™¨çš„åŸºç¡€ç»“æ„ï¼ŒMobileFlowèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–å’Œç†è§£ä¸åŒGUIç•Œé¢çš„ä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMoEæ‰©å±•å’ŒCoTè®­ç»ƒç­–ç•¥
MobileFlowå¼•å…¥äº†MoEæ‰©å±•ï¼Œé€šè¿‡éšæœºæ¿€æ´»å¤šä¸ªä¸“å®¶ç½‘ç»œï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼ŒMobileFlowé‡‡ç”¨äº†CoTè®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸€ç³»åˆ—ä¸­é—´æ­¥éª¤æˆ–è§£é‡Šæ€§é™ˆè¿°ï¼Œä»è€Œæé«˜æ¨ç†å’Œå†³ç­–çš„å‡†ç¡®æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
MobileFlowåœ¨å…¬å…±å’Œè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºQwen-VL-Maxå’ŒGPT-4vï¼Œå¹¶åœ¨å®é™…ä¸šåŠ¡åœºæ™¯ä¸­æˆåŠŸéƒ¨ç½²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MobileFlowçš„åˆ›æ–°æ–¹æ³•ä¸ºå¼€å‘æ™ºèƒ½GUIåŠ©æ‰‹æä¾›äº†æ–°çš„æ€è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸­æ–‡GUIç•Œé¢å’Œè§£å†³éšç§é£é™©æ–¹é¢ã€‚æ­¤å¤–ï¼ŒMoEæ‰©å±•å’ŒCoTè®­ç»ƒç­–ç•¥ä¹Ÿä¸ºæé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## autotask--executing-arbitrary-voice-commands-by-exploring-and-learning-from-mobile-gui
### Abstract
Voice command interfaces (VCIs) have gained increasing importance, enabling
hands-free and eyes-free interaction with digital devices. However, the
inherent complexity in constructing effective voice interfaces has limited the
VCIs' functionalities to only a small fraction of GUI applications and tasks.
This paper presents AutoTask, a VCI capable of automating any task in any
mobile application without configuration or modification from developers or end
users. The primary challenge for AutoTask is the lack of knowledge, as it needs
to accomplish unknown tasks (e.g., user commands) within an unknown environment
(e.g., GUI). To address this challenge, AutoTask employs two strategies: (1)
trial and error: AutoTask explores the GUI, attempts potential operation
sequences, and recovers from errors through backtracking; (2) learning from the
environment: AutoTask accumulates experiences during exploration and summarizes
correct knowledge from these experiences. We implemented AutoTask on Android
devices and conducted an evaluation study, which proved the feasibility of
AutoTask.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AutoTaskï¼šæ¢ç´¢ä¸å­¦ä¹ ï¼Œè®©è¯­éŸ³å‘½ä»¤åœ¨ç§»åŠ¨GUIä¸­è‡ªç”±æ‰§è¡Œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€è¯­éŸ³äº¤äº’æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè¯­éŸ³å‘½ä»¤ç•Œé¢ï¼ˆVCIï¼‰åœ¨æ•°å­—è®¾å¤‡ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œå®ƒå…è®¸ç”¨æˆ·åœ¨æ— éœ€åŠ¨æ‰‹å’Œç”¨çœ¼çš„æƒ…å†µä¸‹ä¸è®¾å¤‡è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œæ„å»ºæœ‰æ•ˆçš„è¯­éŸ³ç•Œé¢é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ç°æœ‰çš„VCIåŠŸèƒ½ä»…é™äºå°‘æ•°GUIåº”ç”¨ç¨‹åºå’Œä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†AutoTaskï¼Œä¸€ä¸ªæ— éœ€å¼€å‘äººå‘˜æˆ–æœ€ç»ˆç”¨æˆ·é…ç½®æˆ–ä¿®æ”¹å³å¯è‡ªåŠ¨æ‰§è¡Œä»»ä½•ç§»åŠ¨åº”ç”¨ç¨‹åºä¸­ä»»ä½•ä»»åŠ¡çš„VCIã€‚AutoTaskçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºç¼ºä¹çŸ¥è¯†ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨æœªçŸ¥ç¯å¢ƒï¼ˆä¾‹å¦‚GUIï¼‰ä¸­å®ŒæˆæœªçŸ¥ä»»åŠ¡ï¼ˆä¾‹å¦‚ç”¨æˆ·å‘½ä»¤ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒAutoTaské‡‡ç”¨äº†ä¸¤ç§ç­–ç•¥ï¼šï¼ˆ1ï¼‰è¯•é”™ï¼šAutoTaskæ¢ç´¢GUIï¼Œå°è¯•æ½œåœ¨çš„æ“ä½œåºåˆ—ï¼Œå¹¶é€šè¿‡å›æº¯ä»é”™è¯¯ä¸­æ¢å¤ï¼›ï¼ˆ2ï¼‰ä»ç¯å¢ƒä¸­å­¦ä¹ ï¼šAutoTaskåœ¨æ¢ç´¢è¿‡ç¨‹ä¸­ç§¯ç´¯ç»éªŒï¼Œå¹¶ä»è¿™äº›ç»éªŒä¸­æ€»ç»“æ­£ç¡®çš„çŸ¥è¯†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢ä¸å­¦ä¹ ç­–ç•¥
AutoTaské‡‡ç”¨â€œæ¢ç´¢-å­¦ä¹ â€ç­–ç•¥ï¼Œé€šè¿‡è¯•é”™å’Œä»ç¯å¢ƒä¸­å­¦ä¹ æ¥å®Œæˆä»»åŠ¡ã€‚è¯•é”™é˜¶æ®µï¼ŒAutoTaskæ¢ç´¢GUIï¼Œå°è¯•å¯èƒ½çš„æ“ä½œåºåˆ—ï¼Œå¹¶åœ¨å¿…è¦æ—¶é€šè¿‡å›æº¯æ¥çº æ­£é”™è¯¯ã€‚å­¦ä¹ é˜¶æ®µï¼ŒAutoTaskå°†æ¢ç´¢è¿‡ç¨‹ä¸­çš„ç»éªŒæ€»ç»“ä¸ºçŸ¥è¯†ï¼ŒåŒ…æ‹¬ç¯å¢ƒçŸ¥è¯†ã€ä»»åŠ¡çŸ¥è¯†å’Œæ‰§è¡ŒçŸ¥è¯†ï¼Œä»¥å¢å¼ºå…¶æ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
AutoTaskåˆ©ç”¨LLMæ¥ç†è§£ç”¨æˆ·å‘½ä»¤å’ŒGUIè¯­ä¹‰ï¼Œå¹¶æ ¹æ®ç»éªŒæ€»ç»“çŸ¥è¯†ã€‚LLMå¯ä»¥å¸®åŠ©AutoTaskæ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾å’Œå‚æ•°ï¼Œä»è€Œæé«˜å‘½ä»¤ç†è§£çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒLLMè¿˜å¯ä»¥å¸®åŠ©AutoTaskç”Ÿæˆæ›´æœ‰æ•ˆçš„æ“ä½œåºåˆ—ï¼Œå¹¶é¿å…æ‰§è¡Œé”™è¯¯ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
AutoTaskåœ¨Androidè®¾å¤‡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶å¯è¡Œæ€§ã€‚AutoTaskåœ¨æ‰§è¡Œç”¨æˆ·æŒ‡ä»¤æ–¹é¢çš„æˆåŠŸç‡æ˜¾è‘—é«˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”éšç€çŸ¥è¯†çš„ç§¯ç´¯ï¼Œå…¶æ‰§è¡Œæ•ˆç‡ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AutoTaskçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–VCIçš„å¼€å‘æä¾›å€Ÿé‰´ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†AutoTaskçš„â€œæ¢ç´¢-å­¦ä¹ â€ç­–ç•¥åº”ç”¨äºå…¶ä»–ç±»å‹çš„äº¤äº’ç•Œé¢ï¼Œå¦‚ç½‘é¡µæµè§ˆå™¨æˆ–å‘½ä»¤è¡Œç•Œé¢ã€‚æ­¤å¤–ï¼ŒAutoTaskè¿˜å¯ä»¥ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆï¼Œä¾‹å¦‚å¤æ‚ä»»åŠ¡åˆ†è§£ç³»ç»Ÿï¼Œä»¥æ»¡è¶³ç”¨æˆ·æ›´å¤æ‚çš„äº¤äº’éœ€æ±‚ã€‚

## webcanvas--benchmarking-web-agents-in-online-environments
### Abstract
For web agents to be practically useful, they must adapt to the continuously
evolving web environment characterized by frequent updates to user interfaces
and content. However, most existing benchmarks only capture the static aspects
of the web. To bridge this gap, we introduce WebCanvas, an innovative online
evaluation framework for web agents that effectively addresses the dynamic
nature of web interactions. WebCanvas contains three main components to
facilitate realistic assessments: (1) A novel evaluation metric which reliably
capture critical intermediate actions or states necessary for task completions
while disregarding noise caused by insignificant events or changed
web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version
of original Mind2Web static dataset containing 542 tasks with 2439 intermediate
evaluation states; (3) Lightweight and generalizable annotation tools and
testing pipelines that enables the community to collect and maintain the
high-quality, up-to-date dataset. Building on WebCanvas, we open-source an
agent framework with extensible modules for reasoning, providing a foundation
for the community to conduct online inference and evaluations. Our
best-performing agent achieves a task success rate of 23.1% and a task
completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we
analyze the performance discrepancies across various websites, domains, and
experimental environments. We encourage the community to contribute further
insights on online agent evaluation, thereby advancing this field of research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebCanvasï¼šåœ¨çº¿ç¯å¢ƒä¸­Webä»£ç†çš„åŸºå‡†æµ‹è¯•æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äº’è”ç½‘çš„å¿«é€Ÿå‘å±•ï¼ŒWebä»£ç†åœ¨å¯¼èˆªå’Œä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Webä»£ç†è¯„ä¼°æ¡†æ¶å¤§å¤šåªå…³æ³¨é™æ€çš„Webé¡µé¢ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°ä»£ç†åœ¨åŠ¨æ€ã€ä¸æ–­å˜åŒ–çš„Webç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WebCanvasï¼Œä¸€ä¸ªåˆ›æ–°çš„åœ¨çº¿è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ›´çœŸå®åœ°è¯„ä¼°Webä»£ç†åœ¨åŠ¨æ€Webäº¤äº’ä¸­çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…³é”®èŠ‚ç‚¹æ ‡æ³¨çš„è¿›åº¦æ„ŸçŸ¥è¯„ä¼°
WebCanvaså¼•å…¥äº†â€œå…³é”®èŠ‚ç‚¹â€çš„æ¦‚å¿µï¼Œå³å®Œæˆç‰¹å®šWebä»»åŠ¡æ‰€å¿…éœ€çš„æ­¥éª¤ã€‚é€šè¿‡æ ‡æ³¨å…³é”®èŠ‚ç‚¹ï¼Œå¯ä»¥æ›´è¯¦ç»†åœ°åˆ†æä»£ç†çš„è¡Œä¸ºï¼Œä»è€Œæ·±å…¥äº†è§£å…¶å†³ç­–çš„ä¼˜ç¼ºç‚¹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç¤¾åŒºé©±åŠ¨çš„æ ‡æ³¨å¹³å°
WebCanvasæ”¯æŒé€šè¿‡é«˜çº§è®°å½•æµè§ˆå™¨æ’ä»¶è®°å½•å’Œæ ‡æ³¨Webä»»åŠ¡åŠå…¶å¯¹åº”çš„å…³é”®èŠ‚ç‚¹è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿˜å¼€æºäº†ä¸€ä¸ªå…·æœ‰å¯æ‰©å±•æ¨ç†æ¨¡å—çš„ä»£ç†æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºæˆå‘˜åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­è¿›è¡Œåœ¨çº¿è¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæˆæœ¬æ•ˆç›Šçš„æ•°æ®ç»´æŠ¤
WebCanvasé‡‡ç”¨å®šæœŸç›‘æ§å’Œè‡ªåŠ¨è­¦æŠ¥çš„ç»´æŠ¤ç­–ç•¥ï¼Œä»¥å¿«é€Ÿè¯†åˆ«æ— æ•ˆçš„åŠ¨ä½œåºåˆ—å’Œå…³é”®èŠ‚ç‚¹ã€‚å½“æ•°æ®å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæµ‹è¯•æŠ¥å‘Šä¼šæä¾›é”™è¯¯ä¿¡æ¯ï¼ŒæŒ‡å¯¼æ•°æ®æ‰€æœ‰è€…è¿›è¡Œå¿«é€Ÿæœ‰æ•ˆçš„æ•°æ®ä¿®æ­£ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åŸºäºWebCanvasæ¡†æ¶ï¼Œæœ¬æ–‡åˆ›å»ºäº†Mind2Web-Liveæ•°æ®é›†ï¼ŒåŒ…å«542ä¸ªä»»åŠ¡å’Œ2439ä¸ªå…³é”®èŠ‚ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4-turboæ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸç‡å’Œä»»åŠ¡å®Œæˆç‡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œåˆ†åˆ«ä¸º23.1%å’Œ48.8%ã€‚æ­¤å¤–ï¼Œåœ¨çº¿è¯„ä¼°ä¸ç¦»çº¿è¯„ä¼°çš„ç»“æœå­˜åœ¨å·®å¼‚ï¼Œè¡¨æ˜åœ¨åŠ¨æ€åœ¨çº¿ç¯å¢ƒä¸­ï¼Œæ¨¡å‹çš„è¡¨ç°å¯èƒ½ä¸å¦‚åœ¨é™æ€ç¦»çº¿ç¯å¢ƒä¸­ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebCanvasæ¡†æ¶ä¸ºWebä»£ç†çš„åœ¨çº¿è¯„ä¼°æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºç¤¾åŒºæˆå‘˜æä¾›äº†ä¸€ä¸ªå¹³å°æ¥æ„å»ºæ•°æ®é›†å’Œè¯„ä¼°Webä»£ç†æ¡†æ¶å’Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å…³é”®èŠ‚ç‚¹æ ‡æ³¨ä½œä¸ºä¸­é—´å¥–åŠ±çš„ä½¿ç”¨ï¼Œå¹¶å‘ç°Webä»£ç†å¯ä»¥ä»äººç±»æä¾›çš„å…³é”®èŠ‚ç‚¹æ ‡æ³¨ä¸­å—ç›Šã€‚

## agent-q--advanced-reasoning-and-learning-for-autonomous-ai-agents
### Abstract
Large Language Models (LLMs) have shown remarkable capabilities in natural
language tasks requiring complex reasoning, yet their application in agentic,
multi-step reasoning within interactive environments remains a difficult
challenge. Traditional supervised pre-training on static datasets falls short
in enabling autonomous agent capabilities needed to perform complex
decision-making in dynamic settings like web navigation. Previous attempts to
bridge this ga-through supervised fine-tuning on curated expert
demonstrations-often suffer from compounding errors and limited exploration
data, resulting in sub-optimal policy outcomes. To overcome these challenges,
we propose a framework that combines guided Monte Carlo Tree Search (MCTS)
search with a self-critique mechanism and iterative fine-tuning on agent
interactions using an off-policy variant of the Direct Preference Optimization
(DPO) algorithm. Our method allows LLM agents to learn effectively from both
successful and unsuccessful trajectories, thereby improving their
generalization in complex, multi-step reasoning tasks. We validate our approach
in the WebShop environment-a simulated e-commerce platform where it
consistently outperforms behavior cloning and reinforced fine-tuning baseline,
and beats average human performance when equipped with the capability to do
online search. In real-world booking scenarios, our methodology boosts Llama-3
70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340%
relative increase) after a single day of data collection and further to 95.4%
with online search. We believe this represents a substantial leap forward in
the capabilities of autonomous agents, paving the way for more sophisticated
and reliable decision-making in real-world settings.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Agent Qï¼šè‡ªä¸»AIä»£ç†çš„é«˜çº§æ¨ç†ä¸å­¦ä¹ 

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦å¤æ‚æ¨ç†çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„å¤šæ­¥æ¨ç†åº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç›‘ç£é¢„è®­ç»ƒæ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œå¤æ‚å†³ç­–æ—¶æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¼•å¯¼è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’Œè‡ªæˆ‘æ‰¹è¯„æœºåˆ¶ï¼Œä»¥åŠä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®—æ³•è¿›è¡Œè¿­ä»£å¾®è°ƒçš„æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å¯¼MCTSæœç´¢
ä½¿ç”¨MCTSç®—æ³•æ¥æŒ‡å¯¼ä»£ç†åœ¨ç½‘é¡µä¸Šçš„æ¢ç´¢ï¼Œä»¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œå¹¶é€æ­¥æ”¹è¿›ç­–ç•¥ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªæˆ‘æ‰¹è¯„æœºåˆ¶
é€šè¿‡AIåé¦ˆå’Œè‡ªæˆ‘æ‰¹è¯„ï¼Œä»£ç†åœ¨æ¯ä¸ªèŠ‚ç‚¹æä¾›è‡ªæˆ‘è¯„ä¼°åé¦ˆï¼Œä½œä¸ºä¸­é—´å¥–åŠ±ï¼Œå¸®åŠ©å¼•å¯¼æœç´¢æ­¥éª¤ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿­ä»£å¾®è°ƒ
ä½¿ç”¨DPOç®—æ³•ä»ä»£ç†äº¤äº’ä¸­å­¦ä¹ ï¼ŒåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥çš„è½¨è¿¹ï¼Œä»¥æé«˜ä»£ç†åœ¨å¤æ‚å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨WebShopç¯å¢ƒä¸­ï¼ŒAgent Qæ–¹æ³•å§‹ç»ˆä¼˜äºè¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒåŸºçº¿ï¼Œå¹¶åœ¨é…å¤‡åœ¨çº¿æœç´¢åŠŸèƒ½æ—¶å‡»è´¥äº†å¹³å‡äººç±»æ€§èƒ½ã€‚åœ¨ç°å®ä¸–ç•Œçš„é¢„è®¢åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•å°†Llama-3 70Bæ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ä»18.6%æé«˜åˆ°81.7%ï¼Œå¹¶åœ¨é…å¤‡åœ¨çº¿æœç´¢åŠŸèƒ½åè¿›ä¸€æ­¥æé«˜åˆ°95.4%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Agent Qæ¡†æ¶ä¸ºè‡ªä¸»AIä»£ç†çš„å‘å±•æä¾›äº†é‡è¦çš„ä¸€æ­¥ï¼Œé€šè¿‡å…¶æœç´¢å’Œè‡ªæˆ‘æ‰¹è¯„èƒ½åŠ›ï¼Œä¸ºäº¤äº’å¼ç¯å¢ƒä¸­çš„å¯é å¤šæ­¥å†³ç­–åˆ¶å®šäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚

## webrl--training-llm-web-agents-via-self-evolving-online-curriculum-reinforcement-learning
### Abstract
Large language models (LLMs) have shown remarkable potential as autonomous
agents, particularly in web-based tasks. However, existing LLM web agents
heavily rely on expensive proprietary LLM APIs, while open LLMs lack the
necessary decision-making capabilities. This paper introduces WebRL, a
self-evolving online curriculum reinforcement learning framework designed to
train high-performance web agents using open LLMs. WebRL addresses three key
challenges in building LLM web agents, including the scarcity of training
tasks, sparse feedback signals, and policy distribution drift in online
learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that
generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised
reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure
consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4
models into proficient web agents. On WebArena-Lite, WebRL improves the success
rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.
These open models significantly surpass the performance of GPT-4-Turbo (17.6%)
and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained
on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's
effectiveness in bridging the gap between open and proprietary LLM-based web
agents, paving the way for more accessible and powerful autonomous web
interaction systems.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebRLï¼šåŸºäºè‡ªæ¼”åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ çš„LLMç½‘ç»œä»£ç†è®­ç»ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç°æœ‰çš„LLMç½‘ç»œä»£ç†ä¸»è¦ä¾èµ–äºæ˜‚è´µçš„ä¸“æœ‰LLM APIï¼Œè€Œå¼€æºLLMsåˆ™ç¼ºä¹å¿…è¦çš„å†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†WebRLï¼Œä¸€ä¸ªè‡ªæ¼”åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨å¼€æºLLMsè®­ç»ƒé«˜æ€§èƒ½çš„ç½‘ç»œä»£ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªæ¼”åŒ–è¯¾ç¨‹
WebRLé€šè¿‡ä»å¤±è´¥çš„å°è¯•ä¸­ç”Ÿæˆæ–°ä»»åŠ¡ï¼Œæ„å»ºäº†ä¸€ä¸ªè‡ªæ¼”åŒ–è¯¾ç¨‹ï¼Œä»è€Œè§£å†³äº†è®­ç»ƒä»»åŠ¡ç¨€ç¼ºçš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“æœç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰
WebRLå¼•å…¥äº†ä¸€ä¸ªå¥å£®çš„ORMæ¥è¯„ä¼°ä»»åŠ¡æˆåŠŸï¼Œè§£å†³äº†åé¦ˆä¿¡å·ç¨€ç–çš„é—®é¢˜ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ç­–ç•¥
WebRLé‡‡ç”¨è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬KLçº¦æŸç­–ç•¥æ›´æ–°å’Œç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œä»¥é˜²æ­¢åœ¨çº¿å­¦ä¹ ä¸­çš„ç­–ç•¥åˆ†å¸ƒæ¼‚ç§»ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
WebRLåœ¨WebArena-Liteä¸Šæ˜¾è‘—æé«˜äº†Llama-3.1å’ŒGLM-4æ¨¡å‹çš„æˆåŠŸç‡ï¼Œåˆ†åˆ«ä»4.8%æé«˜åˆ°42.4%å’Œä»6.1%æé«˜åˆ°43%ã€‚è¿™äº›å¼€æºæ¨¡å‹æ˜¾è‘—ä¼˜äºGPT-4-Turboå’ŒGPT-4oï¼Œå¹¶è¶…è¿‡äº†ä¹‹å‰åœ¨å¼€æºLLMsä¸Šè®­ç»ƒçš„æœ€å…ˆè¿›çš„ç½‘ç»œä»£ç†ï¼ˆAutoWebGLMï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebRLä¸ºè®­ç»ƒé«˜æ€§èƒ½çš„ç½‘ç»œä»£ç†æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ¡†æ¶ï¼Œå…¶è‡ªæ¼”åŒ–è¯¾ç¨‹ã€ORMå’Œè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯ä»¥åº”ç”¨äºå…¶ä»–LLMä»£ç†è®­ç»ƒä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒWebRLçš„æˆåŠŸè¡¨æ˜ï¼Œå¼€æºLLMsåœ¨ç½‘ç»œä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æ›¿ä»£æ˜‚è´µçš„ä¸“æœ‰LLM APIã€‚

## naviqate--functionality-guided-web-application-navigation
### Abstract
End-to-end web testing is challenging due to the need to explore diverse web
application functionalities. Current state-of-the-art methods, such as
WebCanvas, are not designed for broad functionality exploration; they rely on
specific, detailed task descriptions, limiting their adaptability in dynamic
web environments. We introduce NaviQAte, which frames web application
exploration as a question-and-answer task, generating action sequences for
functionalities without requiring detailed parameters. Our three-phase approach
utilizes advanced large language models like GPT-4o for complex decision-making
and cost-effective models, such as GPT-4o mini, for simpler tasks. NaviQAte
focuses on functionality-guided web application navigation, integrating
multi-modal inputs such as text and images to enhance contextual understanding.
Evaluations on the Mind2Web-Live and Mind2Web-Live-Abstracted datasets show
that NaviQAte achieves a 44.23% success rate in user task navigation and a
38.46% success rate in functionality navigation, representing a 15% and 33%
improvement over WebCanvas. These results underscore the effectiveness of our
approach in advancing automated web application testing.
### ğŸŒŸ è®ºæ–‡è§£è¯» | NaviQAteï¼šåŸºäºåŠŸèƒ½çš„Webåº”ç”¨ç¨‹åºå¯¼èˆª

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€Webåº”ç”¨ç¨‹åºçš„æ—¥ç›Šæ™®åŠï¼Œç¡®ä¿å…¶è´¨é‡å’ŒåŠŸèƒ½å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ‰‹åŠ¨æµ‹è¯•æ–¹æ³•è€—æ—¶ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€Œç°æœ‰çš„è‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·å¯èƒ½æ— æ³•å…¨é¢è¦†ç›–æ‰€æœ‰åŠŸèƒ½ï¼Œå¯¼è‡´æ½œåœ¨çš„é”™è¯¯å’Œå¯ç”¨æ€§é—®é¢˜è¢«å¿½è§†ã€‚æ­¤å¤–ï¼ŒWebåº”ç”¨ç¨‹åºçš„åŠ¨æ€æ€§å’Œå¤æ‚æ€§ä½¿å¾—è‡ªåŠ¨åŒ–æµ‹è¯•å˜å¾—æ›´åŠ å›°éš¾ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
NaviQAteæå‡ºäº†ä¸€ç§åŸºäºåŠŸèƒ½çš„Webåº”ç”¨ç¨‹åºå¯¼èˆªæ–¹æ³•ï¼Œå°†Webåº”ç”¨ç¨‹åºæ¢ç´¢è§†ä¸ºä¸€ä¸ªé—®ç­”ä»»åŠ¡ï¼Œæ— éœ€è¯¦ç»†å‚æ•°å³å¯ç”ŸæˆåŠŸèƒ½æ€§çš„æ“ä½œåºåˆ—ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

1. **è¡ŒåŠ¨è§„åˆ’**ï¼šä½¿ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯å°†æŠ½è±¡ä»»åŠ¡è½¬æ¢ä¸ºå¯æ“ä½œçš„æè¿°ï¼Œå¹¶ä»å¤šæ¨¡æ€æ¥æºï¼ˆå¦‚å…ƒæ ‡ç­¾ã€å…ˆå‰æ“ä½œå’Œå±å¹•æˆªå›¾ï¼‰ä¸­æå–ç½‘é¡µä¸Šä¸‹æ–‡ï¼Œä»¥åˆ›å»ºå½“å‰çŠ¶æ€çš„æŠ½è±¡è¡¨ç¤ºã€‚
2. **é€‰æ‹©æå–**ï¼šè¯†åˆ«ç½‘é¡µä¸Šçš„å¯æ“ä½œå…ƒç´ ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„é€‰æ‹©æ’åç³»ç»Ÿå¯¹å…¶è¿›è¡Œæ’åºã€‚å…ƒç´ æ ¹æ®å…¶ä¸é¢„æµ‹ä¸‹ä¸€æ­¥çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œæ’åï¼Œå¹¶ç»“åˆé™„è¿‘çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æä¾›é¢å¤–ä¸Šä¸‹æ–‡ã€‚
3. **å†³ç­–åˆ¶å®š**ï¼šç»“åˆä»»åŠ¡å†å²ã€æ³¨é‡Šå±å¹•æˆªå›¾å’Œæ’åçš„å¯æ“ä½œå…ƒç´ æ¥é€‰æ‹©æœ€ä½³è¡ŒåŠ¨ã€‚è§†è§‰æç¤ºå¸®åŠ©æ¨¡å‹è§£é‡Šç©ºé—´å¸ƒå±€å’Œä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°å‡†ç¡®å’Œé«˜æ•ˆçš„å¯¼èˆªã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨Mind2Web-Liveå’ŒMind2Web-Live-Abstractedæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒNaviQAteåœ¨ç”¨æˆ·ä»»åŠ¡å¯¼èˆªå’ŒåŠŸèƒ½å¯¼èˆªæ–¹é¢åˆ†åˆ«å–å¾—äº†44.23%å’Œ38.46%çš„æˆåŠŸç‡ï¼Œæ¯”WebCanvasåˆ†åˆ«æé«˜äº†15%å’Œ33%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
NaviQAteçš„åŸºäºåŠŸèƒ½çš„å¯¼èˆªæ–¹æ³•ä¸ºè‡ªåŠ¨åŒ–Webåº”ç”¨ç¨‹åºæµ‹è¯•æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚å…¶å¤šé˜¶æ®µã€å¤šæ¨¡å‹çš„æ–¹æ³•è®ºä»¥åŠå¤šæ¨¡æ€è¾“å…¥çš„é›†æˆï¼Œä¸ºæé«˜Webå¯¼èˆªçš„å‡†ç¡®æ€§å’Œæ•ˆç‡æä¾›äº†å®è´µçš„ç»éªŒã€‚æ­¤å¤–ï¼ŒNaviQAteçš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–Webåº”ç”¨ç¨‹åºæµ‹è¯•ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## falcon-ui--understanding-gui-before-following-user-instructions
### Abstract
Pursuing human-like interaction for Graphical User Interface (GUI) agents
requires understanding the GUI context and following user instructions.
However, existing works typically couple these two aspects and focus more on
instruct-following abilities, while ignoring the importance of understanding
the GUI context. In this paper, we introduce an instruction-free GUI navigation
dataset, termed Insight-UI Dataset, to enhance model comprehension of GUI
environments. Insight-UI Dataset is automatically generated from the Common
Crawl corpus, simulating various platforms -- including iOS, Android, Windows,
and Linux -- across multiple resolutions on 312K domains. Although GUI
interactions vary by context, diverse interfaces share common internal
patterns, such as clicking an item to view its details. It implies the
feasibility of independent GUI operation learning, followed by joint
optimization with instruction tuning. Thereby, we develop the GUI agent model
Falcon-UI, which is initially pretrained on Insight-UI Dataset and subsequently
fine-tuned on Android and Web GUI datasets, including AITW, AITZ, Android
Control, and Mind2Web. With 7 billion parameters, Falcon-UI achieves accuracy
comparable to the 72 billion-parameter Qwen2VL on AITZ, validating the
alignment between GUI context comprehension and agent performance. Our code and
dataset will be open-sourced.
### ğŸŒŸ è®ºæ–‡è§£è¯» | Falcon-UIï¼šç†è§£GUIç¯å¢ƒï¼Œå®ç°æ›´æ™ºèƒ½çš„ç”¨æˆ·äº¤äº’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åœ¨æ“ä½œç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œè‡ªåŠ¨ä¸GUIäº¤äº’çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç°æœ‰çš„GUIä»£ç†æ¨¡å‹é€šå¸¸ä¾èµ–äºç³»ç»ŸAPIæˆ–ç»“æ„åŒ–è¾“å…¥ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæ„å»ºåŒ…å«å¤šæ ·åŒ–ã€é«˜è´¨é‡ç”¨æˆ·æŒ‡ä»¤çš„æ•°æ®é›†æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†GUIä»£ç†æ¨¡å‹çš„å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†Falcon-UIï¼Œä¸€ä¸ªåŸºäºè§†è§‰è¾“å…¥çš„GUIä»£ç†æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°æ›´æ™ºèƒ½çš„ç”¨æˆ·äº¤äº’ã€‚Falcon-UIçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæŒ‡ä»¤æ— å…³çš„GUIå¯¼èˆªæ•°æ®é›†ï¼ˆInsight-UI Datasetï¼‰
ä¸ºäº†è§£å†³æ„å»ºå¤šæ ·åŒ–ã€é«˜è´¨é‡ç”¨æˆ·æŒ‡ä»¤æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Insight-UI Datasetï¼Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨ç”Ÿæˆçš„ã€æŒ‡ä»¤æ— å…³çš„GUIå¯¼èˆªæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¨¡æ‹Ÿäº†å¤šç§å¹³å°ï¼ˆiOSã€Androidã€Windowsã€Linuxï¼‰å’Œå¤šç§åˆ†è¾¨ç‡ï¼Œæ¶µç›–äº†312Kä¸ªåŸŸåï¼Œä¸ºGUIä»£ç†æ¨¡å‹æä¾›äº†ä¸°å¯Œçš„GUIç¯å¢ƒçŸ¥è¯†ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šGUIç¯å¢ƒç†è§£ä¸æŒ‡ä»¤è·Ÿéšçš„è§£è€¦
Falcon-UIé‡‡ç”¨äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œå°†GUIç¯å¢ƒç†è§£ä¸æŒ‡ä»¤è·Ÿéšè§£è€¦ã€‚é¦–å…ˆï¼ŒFalcon-UIåœ¨Insight-UI Datasetä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ GUIç¯å¢ƒä¸­çš„å¸¸è§æ¨¡å¼å’Œæ“ä½œé€»è¾‘ã€‚ç„¶åï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒFalcon-UIæ ¹æ®ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼Œå®ç°æ›´å‡†ç¡®çš„GUIäº¤äº’ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªGUIä»£ç†æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFalcon-UIå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨Android in the Wild (AITW)å’ŒAndroid in the Zoo (AITZ)æ•°æ®é›†ä¸Šï¼ŒFalcon-UIçš„å‡†ç¡®ç‡ä¸72äº¿å‚æ•°çš„Qwen2VLæ¨¡å‹ç›¸å½“ï¼ŒéªŒè¯äº†GUIç¯å¢ƒç†è§£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„Falcon-UIæ¨¡å‹å’ŒInsight-UI Datasetä¸ºGUIä»£ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚Falcon-UIçš„è§£è€¦è®­ç»ƒèŒƒå¼å’ŒInsight-UI Datasetçš„è‡ªåŠ¨ç”Ÿæˆæœºåˆ¶ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½ã€æ›´é€šç”¨çš„GUIä»£ç†æ¨¡å‹æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚

## taskbench--benchmarking-large-language-models-for-task-automation
### Abstract
In recent years, the remarkable progress of large language models (LLMs) has
sparked interest in task automation, which involves decomposing complex tasks
described by user instructions into sub-tasks and invoking external tools to
execute them, playing a central role in autonomous agents. However, there is a
lack of systematic and standardized benchmarks to promote the development of
LLMs in task automation. To address this, we introduce TaskBench, a
comprehensive framework to evaluate the capability of LLMs in task automation.
Specifically, task automation can be divided into three critical stages: task
decomposition, tool selection, and parameter prediction. To tackle the
complexities inherent in these stages, we introduce the concept of Tool Graph
to represent decomposed tasks and adopt a back-instruct method to generate
high-quality user instructions. We propose TaskEval, a multi-faceted evaluation
methodology that assesses LLM performance across these three stages. Our
approach combines automated construction with rigorous human verification,
ensuring high consistency with human evaluation. Experimental results
demonstrate that TaskBench effectively reflects the capabilities of various
LLMs in task automation. It provides insights into model performance across
different task complexities and domains, pushing the boundaries of what current
models can achieve. TaskBench offers a scalable, adaptable, and reliable
benchmark for advancing LLM-based autonomous agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | TaskBenchï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä»»åŠ¡è‡ªåŠ¨åŒ–çš„å…¨é¢è¯„ä¼°æ¡†æ¶

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¿€å‘äº†äººä»¬å¯¹ä»»åŠ¡è‡ªåŠ¨åŒ–çš„å…´è¶£ã€‚ä»»åŠ¡è‡ªåŠ¨åŒ–æ¶‰åŠå°†ç”¨æˆ·æŒ‡ä»¤æè¿°çš„å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥æ‰§è¡Œå®ƒä»¬ï¼Œåœ¨è‡ªä¸»ä»£ç†ä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ç³»ç»ŸåŒ–å’Œæ ‡å‡†åŒ–çš„åŸºå‡†æ¥ä¿ƒè¿›LLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TaskBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥å·¥å…·å›¾ï¼ˆTool Graphï¼‰çš„æ¦‚å¿µ
ä¸ºäº†åº”å¯¹ä»»åŠ¡åˆ†è§£ã€å·¥å…·é€‰æ‹©å’Œå‚æ•°é¢„æµ‹ç­‰é˜¶æ®µçš„å¤æ‚æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†å·¥å…·å›¾çš„æ¦‚å¿µã€‚å·¥å…·å›¾å¯ä»¥è¡¨ç¤ºåˆ†è§£åçš„ä»»åŠ¡ä¹‹é—´çš„å…³ç³»å’Œä¾èµ–æ€§ï¼Œå…‹æœäº†ç°æœ‰åŸºå‡†ä¸­ç®€å•APIæ–‡æ¡£æˆ–åŸºäºæ¨¡æ¿æ–¹æ³•çš„å±€é™æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡‡ç”¨åå‘æŒ‡ä»¤ï¼ˆBack-Instructï¼‰æ–¹æ³•
æœ¬æ–‡é‡‡ç”¨åå‘æŒ‡ä»¤æ–¹æ³•æ¥ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤ã€‚é€šè¿‡ä»å·¥å…·å›¾ä¸­é‡‡æ ·å­å›¾ï¼Œå¹¶ä½¿ç”¨LLMsç”Ÿæˆç›¸åº”çš„æŒ‡ä»¤ï¼Œå¯ä»¥ç¡®ä¿ç”Ÿæˆçš„æŒ‡ä»¤æ—¢è‡ªç„¶åˆç¬¦åˆå®é™…å·¥å…·ä½¿ç”¨æ¨¡å¼ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºTaskEvalè¯„ä¼°æ–¹æ³•
ä¸ºäº†å…¨é¢è¯„ä¼°LLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†TaskEvalè¯„ä¼°æ–¹æ³•ã€‚TaskEvalåŒ…æ‹¬ä¸‰ä¸ªå…³é”®é˜¶æ®µçš„è¯„ä¼°ï¼šä»»åŠ¡åˆ†è§£ã€å·¥å…·é€‰æ‹©å’Œå‚æ•°é¢„æµ‹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è‡ªåŠ¨æ„å»ºå’Œä¸¥æ ¼çš„äººå·¥éªŒè¯ï¼Œç¡®ä¿ä¸äººå·¥è¯„ä¼°çš„é«˜åº¦ä¸€è‡´æ€§ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒTaskBenchèƒ½å¤Ÿæœ‰æ•ˆåœ°åæ˜ ä¸åŒLLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒæä¾›äº†å¯¹æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å¤æ‚æ€§å’Œé¢†åŸŸä¸­çš„æ€§èƒ½çš„è§è§£ï¼Œæ¨åŠ¨äº†å½“å‰æ¨¡å‹æ‰€èƒ½è¾¾åˆ°çš„è¾¹ç•Œã€‚TaskBenchæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯é€‚åº”å’Œå¯é çš„åŸºå‡†ï¼Œç”¨äºæ¨è¿›åŸºäºLLMçš„è‡ªä¸»ä»£ç†çš„å‘å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TaskBenchä¸ºLLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„è¯„ä¼°æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œå…¶åˆ›æ–°çš„æ•°æ®ç”ŸæˆæŠ€æœ¯å’Œä¸¥æ ¼çš„è¯„ä¼°æ–¹æ³•ä¸ºLLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚æ­¤å¤–ï¼ŒTaskBenchçš„å¼•å…¥ä¹Ÿä¸ºLLMsåœ¨å¤æ‚ä»»åŠ¡è‡ªåŠ¨åŒ–åœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚

## assistgui--task-oriented-desktop-graphical-user-interface-automation
### Abstract
Graphical User Interface (GUI) automation holds significant promise for
assisting users with complex tasks, thereby boosting human productivity.
Existing works leveraging Large Language Model (LLM) or LLM-based AI agents
have shown capabilities in automating tasks on Android and Web platforms.
However, these tasks are primarily aimed at simple device usage and
entertainment operations. This paper presents a novel benchmark, AssistGUI, to
evaluate whether models are capable of manipulating the mouse and keyboard on
the Windows platform in response to user-requested tasks. We carefully
collected a set of 100 tasks from nine widely-used software applications, such
as, After Effects and MS Word, each accompanied by the necessary project files
for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied
Agent framework, which incorporates a sophisticated GUI parser driven by an
LLM-agent and an enhanced reasoning mechanism adept at handling lengthy
procedural tasks. Our experimental results reveal that our GUI Parser and
Reasoning mechanism outshine existing methods in performance. Nevertheless, the
potential remains substantial, with the best model attaining only a 46% success
rate on our benchmark. We conclude with a thorough analysis of the current
methods' limitations, setting the stage for future breakthroughs in this
domain.
### ğŸŒŸ è®ºæ–‡è§£è¯» | ASSISTGUIï¼šåŸºäºä»»åŠ¡çš„æ¡Œé¢å›¾å½¢ç”¨æˆ·ç•Œé¢è‡ªåŠ¨åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–åœ¨æé«˜äººç±»ç”Ÿäº§åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–åŸºäºLLMçš„AIä»£ç†åœ¨Androidå’ŒWebå¹³å°ä¸Šè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œä½†è¿™äº›ä»»åŠ¡å¤§å¤šé’ˆå¯¹ç®€å•çš„è®¾å¤‡ä½¿ç”¨å’Œå¨±ä¹æ“ä½œã€‚ç„¶è€Œï¼Œå¯¹äºå¤æ‚çš„æ¡Œé¢åº”ç”¨ç¨‹åºï¼Œç”¨æˆ·å¾€å¾€é¢ä¸´ç€é™¡å³­çš„å­¦ä¹ æ›²çº¿ï¼Œè¿™é™åˆ¶äº†ä»–ä»¬çš„åˆ›é€ åŠ›å’Œç”Ÿäº§åŠ›ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä»»åŠ¡å¯¼å‘çš„æ¡Œé¢GUIè‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨åˆ©ç”¨ç”Ÿäº§åŠ›è½¯ä»¶æ–¹é¢çš„æ€§èƒ½ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šASSISTGUIåŸºå‡†
æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºASSISTGUIçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨Windowså¹³å°ä¸Šæ ¹æ®ç”¨æˆ·è¯·æ±‚çš„ä»»åŠ¡æ“çºµé¼ æ ‡å’Œé”®ç›˜çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«ä»ä¹ä¸ªå¹¿æ³›ä½¿ç”¨çš„è½¯ä»¶åº”ç”¨ç¨‹åºä¸­ç²¾å¿ƒæ”¶é›†çš„100ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é™„å¸¦å¿…è¦çš„é¡¹ç›®æ–‡ä»¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¿›è¡Œè¯„ä¼°ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šActor-Critic Embodied Agentæ¡†æ¶
æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºActor-Critic Embodied Agentï¼ˆACEï¼‰çš„å…ˆè¿›æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç”±LLMä»£ç†é©±åŠ¨çš„å¤æ‚GUIè§£æå™¨å’Œä¸€ç§å¢å¼ºçš„æ¨ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿå¤„ç†å†—é•¿çš„è¿‡ç¨‹æ€§ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šGUIè§£æå™¨ã€è¯„ä¼°æ¨¡å—å’Œæ‰§è¡Œæ¨¡å—ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„GUIè§£æå™¨å’Œæ¨ç†æœºåˆ¶åœ¨æ€§èƒ½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œå³ä½¿æœ€ä½³æ¨¡å‹åœ¨åŸºå‡†ä¸Šçš„æˆåŠŸç‡ä¹Ÿåªæœ‰46%ï¼Œè¿™è¡¨æ˜è¯¥é¢†åŸŸä»æœ‰å¾ˆå¤§çš„å‘å±•æ½œåŠ›ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ASSISTGUIåŸºå‡†å’ŒActor-Critic Embodied Agentæ¡†æ¶ä¸ºæ¡Œé¢GUIè‡ªåŠ¨åŒ–é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚ASSISTGUIåŸºå‡†å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œè€ŒActor-Critic Embodied Agentæ¡†æ¶åˆ™ä¸ºå¤„ç†å¤æ‚çš„æ¡Œé¢GUIè‡ªåŠ¨åŒ–ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ·±å…¥åˆ†æäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚

## navigating-the-digital-world-as-humans-do--universal-visual-grounding-for-gui-agents
### Abstract
Multimodal large language models (MLLMs) are transforming the capabilities of
graphical user interface (GUI) agents, facilitating their transition from
controlled simulations to complex, real-world applications across various
platforms. However, the effectiveness of these agents hinges on the robustness
of their grounding capability. Current GUI agents predominantly utilize
text-based representations such as HTML or accessibility trees, which, despite
their utility, often introduce noise, incompleteness, and increased
computational overhead. In this paper, we advocate a human-like embodiment for
GUI agents that perceive the environment entirely visually and directly perform
pixel-level operations on the GUI. The key is visual grounding models that can
accurately map diverse referring expressions of GUI elements to their
coordinates on the GUI across different platforms. We show that a simple
recipe, which includes web-based synthetic data and slight adaptation of the
LLaVA architecture, is surprisingly effective for training such visual
grounding models. We collect the largest dataset for GUI visual grounding so
far, containing 10M GUI elements and their referring expressions over 1.3M
screenshots, and use it to train UGround, a strong universal visual grounding
model for GUI agents. Empirical results on six benchmarks spanning three
categories (grounding, offline agent, and online agent) show that 1) UGround
substantially outperforms existing visual grounding models for GUI agents, by
up to 20% absolute, and 2) agents with UGround outperform state-of-the-art
agents, despite the fact that existing agents use additional text-based input
while ours only uses visual perception. These results provide strong support
for the feasibility and promises of GUI agents that navigate the digital world
as humans do.
### ğŸŒŸ è®ºæ–‡è§£è¯» | äººç±»èˆ¬çš„è§†è§‰å¯¼èˆªï¼šGUI ä»£ç†çš„é€šç”¨è§†è§‰å®šä½

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œä½¿å…¶ä»å—æ§çš„æ¨¡æ‹Ÿç¯å¢ƒè¿‡æ¸¡åˆ°å„ç§å¹³å°ä¸Šçš„å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†çš„æœ‰æ•ˆæ€§å–å†³äºå…¶å®šä½èƒ½åŠ›çš„é²æ£’æ€§ã€‚å½“å‰çš„ GUI ä»£ç†ä¸»è¦ä½¿ç”¨åŸºäºæ–‡æœ¬çš„è¡¨ç¤ºï¼Œå¦‚ HTML æˆ–å¯è®¿é—®æ€§æ ‘ï¼Œè¿™è™½ç„¶æœ‰ç”¨ï¼Œä½†å¾€å¾€ä¼šå¼•å…¥å™ªå£°ã€ä¸å®Œæ•´æ€§å’Œå¢åŠ è®¡ç®—å¼€é”€ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç±»ä¼¼äºäººç±»çš„ GUI ä»£ç†ï¼Œå®ƒå®Œå…¨é€šè¿‡è§†è§‰æ„ŸçŸ¥ç¯å¢ƒï¼Œå¹¶ç›´æ¥åœ¨ GUI ä¸Šæ‰§è¡Œåƒç´ çº§æ“ä½œã€‚å…³é”®åœ¨äºè§†è§‰å®šä½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°å°† GUI å…ƒç´ çš„å¤šç§å¼•ç”¨è¡¨è¾¾å¼æ˜ å°„åˆ°ä¸åŒå¹³å°ä¸Šçš„ GUI åæ ‡ã€‚æœ¬æ–‡å±•ç¤ºäº†åŸºäºç½‘ç»œåˆæˆæ•°æ®å’Œ LLaVA æ¶æ„çš„ç®€å•æ–¹æ³•ï¼Œå¯¹äºè®­ç»ƒæ­¤ç±»è§†è§‰å®šä½æ¨¡å‹éå¸¸æœ‰æ•ˆã€‚æœ¬æ–‡æ”¶é›†äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ GUI è§†è§‰å®šä½æ•°æ®é›†ï¼ŒåŒ…å« 10M GUI å…ƒç´ åŠå…¶åœ¨ 1.3M æˆªå›¾ä¸­çš„å¼•ç”¨è¡¨è¾¾å¼ï¼Œå¹¶ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒäº† UGroundï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„é€šç”¨è§†è§‰å®šä½æ¨¡å‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨æ¶µç›–ä¸‰ä¸ªç±»åˆ«ï¼ˆå®šä½ã€ç¦»çº¿ä»£ç†å’Œåœ¨çº¿ä»£ç†ï¼‰çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼š
1. UGround åœ¨ GUI ä»£ç†çš„è§†è§‰å®šä½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†è§‰å®šä½æ¨¡å‹ï¼Œç»å¯¹å€¼æé«˜äº† 20%ã€‚
2. ä½¿ç”¨ UGround çš„ä»£ç†åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„ä»£ç†ï¼Œå°½ç®¡ç°æœ‰çš„ä»£ç†ä½¿ç”¨é¢å¤–çš„æ–‡æœ¬è¾“å…¥ï¼Œè€Œæˆ‘ä»¬çš„ä»£ç†ä»…ä½¿ç”¨è§†è§‰æ„ŸçŸ¥ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„ SeeAct-V æ¡†æ¶å’Œ UGround æ¨¡å‹ä¸º GUI ä»£ç†çš„è§†è§‰å®šä½æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
- **è§†è§‰æ„ŸçŸ¥**ï¼šé€šè¿‡å®Œå…¨ä¾èµ–è§†è§‰æ„ŸçŸ¥ï¼ŒGUI ä»£ç†å¯ä»¥é¿å…åŸºäºæ–‡æœ¬è¡¨ç¤ºçš„å™ªå£°å’Œä¸å®Œæ•´æ€§é—®é¢˜ã€‚
- **åƒç´ çº§æ“ä½œ**ï¼šç›´æ¥åœ¨ GUI ä¸Šæ‰§è¡Œåƒç´ çº§æ“ä½œï¼Œå¯ä»¥æ›´ç²¾ç¡®åœ°æ§åˆ¶ä»£ç†çš„è¡Œä¸ºã€‚
- **é€šç”¨æ€§**ï¼šUGround æ¨¡å‹å…·æœ‰è‰¯å¥½çš„è·¨å¹³å°æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„ GUI ç¯å¢ƒã€‚
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šSeeAct-V æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—ä»£ç†çš„å„ä¸ªç»„ä»¶å¯ä»¥ç‹¬ç«‹ç ”ç©¶å’Œæ”¹è¿›ã€‚

### ğŸŒŸ æ€»ç»“
æœ¬æ–‡æå‡ºçš„ SeeAct-V æ¡†æ¶å’Œ UGround æ¨¡å‹ä¸º GUI ä»£ç†çš„è§†è§‰å®šä½æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œä¸ºæ„å»ºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·å¯¼èˆªæ•°å­—ä¸–ç•Œçš„ GUI ä»£ç†å¥ å®šäº†åŸºç¡€ã€‚

## is-your-llm-secretly-a-world-model-of-the-internet--model-based-planning-for-web-agents
### Abstract
Language agents based on large language models (LLMs) have demonstrated great
promise in automating web-based tasks. Recent work has shown that incorporating
advanced planning algorithms, e.g., tree search, is advantageous over reactive
planning for web agents. However, unlike simulated sandbox environments,
real-world environments such as the web are rife with irreversible actions.
This undermines the feasibility of backtracking, a cornerstone of (tree)
search. Overly relying on test-time search also hurts efficiency. We advocate
model-based planning for web agents that employs a world model to simulate and
deliberate over the outcome of each candidate action before committing to one.
We systematically explore this paradigm by (1) Proposing a model-based planning
framework, WebDreamer, which employs LLMs to serve as both world models and
value functions; (2) Training specialized LLMs as world models with a scalable
data synthesis pipeline. Empirical results demonstrate that WebDreamer achieves
substantial performance improvements over reactive baselines. It is
competitive, while being 4-5 times more efficient, with tree search in sandbox
environments (VisualWebArena) and also works effectively on real-world websites
(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,
Dreamer-7B, performs comparable to GPT-4o, highlighting the potential of
specialized world models for efficient and effective planning in complex web
environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºäº’è”ç½‘ä¸–ç•Œæ¨¡å‹ï¼Œå®ç°é«˜æ•ˆç½‘ç»œä»£ç†è§„åˆ’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–ç½‘ç»œä»»åŠ¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¦‚ä½•è®©ç½‘ç»œä»£ç†åœ¨å¤æ‚ä¸”ä¸æ–­å˜åŒ–çš„ç½‘ç»œç¯å¢ƒä¸­åšå‡ºæœ‰æ•ˆçš„å†³ç­–ï¼Œæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¯¾é¢˜ã€‚ä¼ ç»Ÿçš„åŸºäºæœç´¢çš„è§„åˆ’ç®—æ³•åœ¨ç½‘ç»œç¯å¢ƒä¸­é¢ä¸´ç€ä¸å¯é€†åŠ¨ä½œå’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„è§„åˆ’æ–¹æ³•ï¼Œåˆ©ç”¨LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹æ¥æ¨¡æ‹Ÿå’Œè¯„ä¼°å€™é€‰åŠ¨ä½œçš„åæœï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç½‘ç»œä»£ç†è§„åˆ’ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„è§„åˆ’æ¡†æ¶WebDreamerï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹å’Œä»·å€¼å‡½æ•°ï¼Œé€šè¿‡æ¨¡æ‹Ÿå’Œè¯„ä¼°å€™é€‰åŠ¨ä½œçš„åæœæ¥åšå‡ºå†³ç­–ã€‚
ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨å¯æ‰©å±•çš„æ•°æ®åˆæˆç®¡é“è®­ç»ƒä¸“é—¨çš„LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWebDreameråœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸åŸºäºæœç´¢çš„è§„åˆ’æ–¹æ³•ç›¸æ¯”ï¼ŒWebDreameråœ¨æ²™ç›’ç¯å¢ƒï¼ˆVisualWebArenaï¼‰ä¸­æ•ˆç‡æé«˜äº†4-5å€ï¼Œå¹¶ä¸”åœ¨çœŸå®ä¸–ç•Œç½‘ç«™ï¼ˆOnline-Mind2Webå’ŒMind2Web-Liveï¼‰ä¸Šä¹Ÿèƒ½æœ‰æ•ˆå·¥ä½œã€‚æ­¤å¤–ï¼Œè®­ç»ƒå‡ºçš„ä¸–ç•Œæ¨¡å‹Dreamer-7Båœ¨ä¸¤ä¸ªåœ¨çº¿åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸GPT-4oç›¸å½“ï¼Œè¯æ˜äº†ä¸“é—¨çš„ä¸–ç•Œæ¨¡å‹åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œé«˜æ•ˆå’Œæœ‰æ•ˆè§„åˆ’çš„å¯èƒ½æ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„åŸºäºæ¨¡å‹çš„è§„åˆ’æ–¹æ³•ä¸ºç½‘ç»œä»£ç†åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„é«˜æ•ˆå†³ç­–æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚åˆ©ç”¨LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿå’Œè¯„ä¼°å€™é€‰åŠ¨ä½œçš„åæœï¼Œä»è€Œæé«˜ç½‘ç»œä»£ç†çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ•°æ®åˆæˆç®¡é“ä¸ºè®­ç»ƒä¸“é—¨çš„ä¸–ç•Œæ¨¡å‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚

## a-real-world-webagent-with-planning--long-context-understanding--and-program-synthesis
### Abstract
Pre-trained large language models (LLMs) have recently achieved better
generalization and sample efficiency in autonomous web automation. However, the
performance on real-world websites has still suffered from (1) open domainness,
(2) limited context length, and (3) lack of inductive bias on HTML. We
introduce WebAgent, an LLM-driven agent that learns from self-experience to
complete tasks on real websites following natural language instructions.
WebAgent plans ahead by decomposing instructions into canonical
sub-instructions, summarizes long HTML documents into task-relevant snippets,
and acts on websites via Python programs generated from those. We design
WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new
pre-trained LLMs for long HTML documents using local and global attention
mechanisms and a mixture of long-span denoising objectives, for planning and
summarization. We empirically demonstrate that our modular recipe improves the
success on real websites by over 50%, and that HTML-T5 is the best model to
solve various HTML understanding tasks; achieving 18.7% higher success rate
than the prior method on MiniWoB web automation benchmark, and SoTA performance
on Mind2Web, an offline task planning evaluation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebAgentï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç°å®ä¸–ç•Œç½‘ç»œè‡ªåŠ¨åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªä¸»ç½‘ç»œè‡ªåŠ¨åŒ–æ–¹é¢å–å¾—äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ï¼Œä½†åœ¨ç°å®ä¸–ç•Œç½‘ç«™ä¸Šçš„æ€§èƒ½ä»ç„¶å—åˆ°ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢çš„æŒ‘æˆ˜ï¼š
1. å¼€æ”¾åŸŸæ€§ï¼šç°å®ä¸–ç•Œç½‘ç«™å…·æœ‰å¼€æ”¾æ€§å’ŒåŠ¨æ€æ€§ï¼Œéš¾ä»¥é¢„å…ˆå®šä¹‰åˆé€‚çš„åŠ¨ä½œç©ºé—´ã€‚
2. æœ‰é™ä¸Šä¸‹æ–‡é•¿åº¦ï¼šç°å®ä¸–ç•Œç½‘ç«™çš„HTMLæ–‡æ¡£é€šå¸¸æ¯”æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„è§‚å¯Ÿç»“æœæ›´é•¿ï¼Œè¶…å‡ºäº†å¤§å¤šæ•°LLMsçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚
3. ç¼ºä¹å¯¹HTMLçš„å½’çº³åç½®ï¼šç°æœ‰çš„LLMsç¼ºä¹å¯¹HTMLæ–‡æ¡£ç»“æ„çš„ç†è§£ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°å¤„ç†ç°å®ä¸–ç•Œç½‘ç«™ä¸Šçš„å¤æ‚ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†WebAgentï¼Œä¸€ä¸ªåŸºäºLLMsçš„è‡ªä¸»ä»£ç†ï¼Œå®ƒé€šè¿‡è‡ªæˆ‘ç»éªŒå­¦ä¹ ï¼Œéµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨ç°å®ä¸–ç•Œç½‘ç«™ä¸Šå®Œæˆä»»åŠ¡ã€‚WebAgentçš„æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š
1. **è§„åˆ’**ï¼šå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºè§„èŒƒå­æŒ‡ä»¤ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œä»»åŠ¡ã€‚
2. **é•¿ä¸Šä¸‹æ–‡ç†è§£**ï¼šä½¿ç”¨HTML-T5æ¨¡å‹å¯¹é•¿HTMLæ–‡æ¡£è¿›è¡Œæ‘˜è¦ï¼Œæå–ä¸ä»»åŠ¡ç›¸å…³çš„ç‰‡æ®µï¼Œä»è€Œå…‹æœLLMsä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚
3. **ç¨‹åºåˆæˆ**ï¼šå°†å­æŒ‡ä»¤å’ŒHTMLç‰‡æ®µè½¬æ¢ä¸ºå¯æ‰§è¡Œçš„Pythonä»£ç ï¼Œé€šè¿‡Selenium WebDriveråœ¨ç½‘ç«™ä¸Šæ‰§è¡Œæ“ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWebAgentåœ¨ç°å®ä¸–ç•Œç½‘ç«™ä¸Šçš„æˆåŠŸç‡æ¯”å•ä¸€LLMæ–¹æ³•æé«˜äº†50%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒHTML-T5æ¨¡å‹åœ¨MiniWoBç½‘ç»œè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†18.7%æ›´é«˜çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨Mind2Webç¦»çº¿ä»»åŠ¡è§„åˆ’è¯„ä¼°ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebAgentçš„è®¾è®¡æ€è·¯ä¸ºç°å®ä¸–ç•Œç½‘ç»œè‡ªåŠ¨åŒ–æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š
1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†ç½‘ç»œè‡ªåŠ¨åŒ–ä»»åŠ¡åˆ†è§£ä¸ºè§„åˆ’ã€æ‘˜è¦å’Œç¨‹åºåˆæˆä¸‰ä¸ªæ¨¡å—ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„LLMsè¿›è¡Œå¤„ç†ï¼Œæé«˜äº†ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚
2. **è‡ªæˆ‘ç»éªŒç›‘ç£**ï¼šé€šè¿‡è‡ªæˆ‘ç»éªŒç›‘ç£ï¼Œå°†é¢†åŸŸä¸“å®¶è¯­è¨€æ¨¡å‹ä¸çœŸå®ä¸–ç•Œç½‘ç«™è¿›è¡Œå¯¹é½ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚
3. **HTML-T5æ¨¡å‹**ï¼šHTML-T5æ¨¡å‹é€šè¿‡å±€éƒ¨å’Œå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠé•¿è·¨åº¦å»å™ªç›®æ ‡ï¼Œæ›´å¥½åœ°æ•æ‰HTMLæ–‡æ¡£çš„ç»“æ„å’Œè¯­ä¹‰ï¼Œä¸ºç½‘ç»œè‡ªåŠ¨åŒ–ä»»åŠ¡æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚

### ğŸ¯ æœªæ¥å±•æœ›
WebAgentçš„æˆåŠŸä¸ºç°å®ä¸–ç•Œç½‘ç»œè‡ªåŠ¨åŒ–å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š
1. **æ›´å¼ºå¤§çš„è§„åˆ’æ¨¡å—**ï¼šå¼€å‘æ›´å¼ºå¤§çš„è§„åˆ’æ¨¡å—ï¼Œä»¥æ›´å‡†ç¡®åœ°åˆ†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆæ›´æœ‰æ•ˆçš„å­æŒ‡ä»¤åºåˆ—ã€‚
2. **æ›´å¹¿æ³›çš„æ³›åŒ–èƒ½åŠ›**ï¼šæ”¶é›†æ›´å¤šçœŸå®ä¸–ç•Œç½‘ç«™ä¸Šçš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨æ›´å¤§çš„é¢†åŸŸä¸“å®¶æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜WebAgentçš„æ³›åŒ–èƒ½åŠ›ã€‚
3. **æ›´æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶**ï¼šå°†åé¦ˆæœºåˆ¶é›†æˆåˆ°æ›´å¤§çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æ›´å¥½åœ°åæ˜ ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå¹¶æé«˜ç¨‹åºåˆæˆçš„è´¨é‡ã€‚
4. **æ›´è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ–¹æ³•**ï¼šå¼€å‘æ›´è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥å‡å°‘äººå·¥å¹²é¢„ï¼Œå¹¶æé«˜WebAgentçš„å¯æ‰©å±•æ€§ã€‚

## webvoyager--building-an-end-to-end-web-agent-with-large-multimodal-models
### Abstract
The rapid advancement of large language models (LLMs) has led to a new era
marked by the development of autonomous applications in real-world scenarios,
which drives innovation in creating advanced web agents. Existing web agents
typically only handle one input modality and are evaluated only in simplified
web simulators or static web snapshots, greatly limiting their applicability in
real-world scenarios. To bridge this gap, we introduce WebVoyager, an
innovative Large Multimodal Model (LMM) powered web agent that can complete
user instructions end-to-end by interacting with real-world websites. Moreover,
we establish a new benchmark by compiling real-world tasks from 15 popular
websites and introduce an automatic evaluation protocol leveraging multimodal
understanding abilities of GPT-4V to evaluate open-ended web agents. We show
that WebVoyager achieves a 59.1% task success rate on our benchmark,
significantly surpassing the performance of both GPT-4 (All Tools) and the
WebVoyager (text-only) setups, underscoring the exceptional capability of
WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement
with human judgment, indicating its effectiveness in providing reliable and
accurate assessments of web agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebVoyagerï¼šåŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ç«¯åˆ°ç«¯ç½‘ç»œä»£ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªä¸»åº”ç”¨åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨åˆ›æ–°æ—¥ç›Šå…´èµ·ï¼Œæ¨åŠ¨äº†é«˜çº§ç½‘ç»œä»£ç†çš„åˆ›å»ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç½‘ç»œä»£ç†é€šå¸¸åªèƒ½å¤„ç†ä¸€ç§è¾“å…¥æ¨¡æ€ï¼Œå¹¶ä¸”ä»…åœ¨ç®€åŒ–çš„ç½‘ç»œæ¨¡æ‹Ÿå™¨æˆ–é™æ€ç½‘ç»œå¿«ç…§ä¸­è¿›è¡Œè¯„ä¼°ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡ä»‹ç»äº†WebVoyagerï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ã€ç”±å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰é©±åŠ¨çš„ç½‘ç»œä»£ç†ï¼Œå®ƒå¯ä»¥é€šè¿‡ä¸çœŸå®ä¸–ç•Œç½‘ç«™çš„äº¤äº’æ¥å®Œæˆç”¨æˆ·æŒ‡ä»¤çš„ç«¯åˆ°ç«¯å¤„ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWebVoyageræ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç½‘ç»œä»£ç†ï¼Œå®ƒé€šè¿‡è§‚å¯Ÿå±å¹•æˆªå›¾å’Œäº¤äº’å¼ç½‘ç»œå…ƒç´ çš„æ–‡æœ¬å†…å®¹æ¥å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šåˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ï¼Œç„¶åæ‰§è¡Œç›¸åº”çš„æ“ä½œï¼ˆå¦‚ç‚¹å‡»ã€è¾“å…¥æˆ–æ»šåŠ¨ç­‰ï¼‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œè¯¥åŸºå‡†ç”±ä»15ä¸ªæµè¡Œç½‘ç«™æ”¶é›†çš„çœŸå®ä¸–ç•Œä»»åŠ¡ç»„æˆï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ï¼Œåˆ©ç”¨GPT-4Vçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›æ¥è¯„ä¼°å¼€æ”¾å¼çš„ç½‘ç»œä»£ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼ŒWebVoyageråœ¨æ–°çš„åŸºå‡†ä¸Šå®ç°äº†59.1%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºGPT-4ï¼ˆAll Toolsï¼‰å’ŒWebVoyagerï¼ˆä»…æ–‡æœ¬ï¼‰è®¾ç½®ï¼Œè¿™çªå‡ºäº†WebVoyagerçš„å“è¶Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæå‡ºçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§è¾¾åˆ°äº†85.3%ï¼Œè¡¨æ˜å…¶åœ¨æä¾›å¯é å’Œå‡†ç¡®çš„ç½‘ç»œä»£ç†è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebVoyagerçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ„å»ºæ™ºèƒ½ç½‘ç»œä»£ç†æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚WebVoyagerçš„è®¾è®¡å’Œè¯„ä¼°æ–¹æ³•ä¸ºæœªæ¥å¼€å‘æ›´é€šç”¨å’Œå¼ºå¤§çš„ç½‘ç»œåŠ©æ‰‹æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## cogagent--a-visual-language-model-for-gui-agents
### Abstract
People are spending an enormous amount of time on digital devices through
graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large
language models (LLMs) such as ChatGPT can assist people in tasks like writing
emails, but struggle to understand and interact with GUIs, thus limiting their
potential to increase automation levels. In this paper, we introduce CogAgent,
an 18-billion-parameter visual language model (VLM) specializing in GUI
understanding and navigation. By utilizing both low-resolution and
high-resolution image encoders, CogAgent supports input at a resolution of
1120*1120, enabling it to recognize tiny page elements and text. As a
generalist visual language model, CogAgent achieves the state of the art on
five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,
Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using
only screenshots as input, outperforms LLM-based methods that consume extracted
HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,
advancing the state of the art. The model and codes are available at
https://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220
available at https://github.com/THUDM/CogAgent.
### ğŸŒŸ è®ºæ–‡è§£è¯» | CogAgentï¼šä¸ºGUIäº¤äº’è€Œç”Ÿçš„æ–°å‹è§†è§‰è¯­è¨€æ¨¡å‹

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººä»¬è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸æ•°å­—è®¾å¤‡äº’åŠ¨ï¼Œä¾‹å¦‚è®¡ç®—æœºæˆ–æ™ºèƒ½æ‰‹æœºå±å¹•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTåœ¨è¾…åŠ©äººä»¬å®Œæˆè¯¸å¦‚æ’°å†™ç”µå­é‚®ä»¶ç­‰ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼ŒLLMsåœ¨ç†è§£å’Œä¸GUIäº¤äº’æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æé«˜è‡ªåŠ¨åŒ–æ°´å¹³çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†CogAgentï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºGUIç†è§£å’Œå¯¼èˆªçš„18äº¿å‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚CogAgenté€šè¿‡åˆ©ç”¨ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒç¼–ç å™¨ï¼Œæ”¯æŒ1120*1120çš„è¾“å…¥åˆ†è¾¨ç‡ï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«å¾®å°çš„é¡µé¢å…ƒç´ å’Œæ–‡æœ¬ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCogAgenté‡‡ç”¨äº†ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒç¼–ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚è¿™ç§è®¾è®¡å…è®¸CogAgentåœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—å†…æé«˜æ¨¡å‹çš„å¯æ¥å—åˆ†è¾¨ç‡ï¼Œä»è€Œè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCogAgentåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†å¤§è§„æ¨¡çš„GUIå’ŒOCRæ•°æ®é›†ï¼Œä»¥åŠä¸“é—¨ä¸ºGUIåœºæ™¯è®¾è®¡çš„GUI groundingä»»åŠ¡ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹GUIå…ƒç´ çš„ç†è§£èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
CogAgentåœ¨å¤šä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬VQAv2ã€OK-VQAã€Text-VQAã€ST-VQAã€ChartQAã€infoVQAã€DocVQAã€MM-Vetå’ŒPOPEã€‚æ­¤å¤–ï¼ŒCogAgentåœ¨PCå’ŒAndroid GUIå¯¼èˆªä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨å±å¹•æˆªå›¾ä½œä¸ºè¾“å…¥ï¼Œå°±ä¼˜äºäº†åŸºäºLLMçš„æ–¹æ³•ï¼Œå¦‚Mind2Webå’ŒAITWï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
CogAgentçš„è®¾è®¡å’Œè®­ç»ƒæ–¹æ³•ä¸ºæ„å»ºGUIæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†èƒ½åŠ›å’Œå¯¹GUIå…ƒç´ çš„ç†è§£èƒ½åŠ›ï¼Œä½¿å…¶åœ¨GUIäº¤äº’ä»»åŠ¡ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒCogAgentçš„é¢„è®­ç»ƒæ•°æ®æ„å»ºæ–¹æ³•ä¹Ÿä¸ºå…¶ä»–è§†è§‰è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†å‚è€ƒã€‚

## understanding-the-planning-of-llm-agents--a-survey
### Abstract
As Large Language Models (LLMs) have shown significant intelligence, the
progress to leverage LLMs as planning modules of autonomous agents has
attracted more attention. This survey provides the first systematic view of
LLM-based agents planning, covering recent works aiming to improve planning
ability. We provide a taxonomy of existing works on LLM-Agent planning, which
can be categorized into Task Decomposition, Plan Selection, External Module,
Reflection and Memory. Comprehensive analyses are conducted for each direction,
and further challenges for the field of research are discussed.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±å…¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è§„åˆ’èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºæ˜¾è‘—çš„æ™ºèƒ½ï¼Œå°†LLMä½œä¸ºè‡ªä¸»ä»£ç†çš„è§„åˆ’æ¨¡å—çš„è¿›å±•å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è§„åˆ’æ–¹æ³•ï¼Œå¦‚ç¬¦å·æ–¹æ³•å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œå­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦å°†çµæ´»çš„è‡ªç„¶è¯­è¨€æè¿°çš„é—®é¢˜è½¬æ¢ä¸ºç¬¦å·å»ºæ¨¡ï¼Œæˆ–è€…éœ€è¦å¤§é‡çš„æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ä¸ªç³»ç»Ÿæ€§çš„è§†è§’ï¼Œä»¥äº†è§£LLMä»£ç†çš„è§„åˆ’èƒ½åŠ›ï¼Œå¹¶æ¢è®¨å¦‚ä½•åˆ©ç”¨LLMæ¥æé«˜ä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡å°†ç°æœ‰çš„LLMä»£ç†è§„åˆ’å·¥ä½œåˆ†ä¸ºäº”ä¸ªä¸»è¦æ–¹å‘ï¼Œå¹¶å¯¹æ¯ä¸ªæ–¹å‘è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æï¼š

1. **ä»»åŠ¡åˆ†è§£**ï¼šå°†å¤æ‚çš„ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œç„¶åä¾æ¬¡ä¸ºæ¯ä¸ªå­ä»»åŠ¡è¿›è¡Œè§„åˆ’ã€‚
2. **å¤šè®¡åˆ’é€‰æ‹©**ï¼šç”Ÿæˆå¤šä¸ªæ›¿ä»£è®¡åˆ’ï¼Œç„¶åä½¿ç”¨ä»»åŠ¡ç›¸å…³çš„æœç´¢ç®—æ³•é€‰æ‹©ä¸€ä¸ªè®¡åˆ’æ¥æ‰§è¡Œã€‚
3. **å¤–éƒ¨æ¨¡å—è¾…åŠ©è§„åˆ’**ï¼šä½¿ç”¨å¤–éƒ¨è§„åˆ’å™¨æ¥æå‡è§„åˆ’è¿‡ç¨‹ï¼ŒåŒæ—¶LLMä¸»è¦æ‰®æ¼”ä»»åŠ¡å½¢å¼åŒ–çš„è§’è‰²ã€‚
4. **åæ€ä¸ç»†åŒ–**ï¼šé€šè¿‡åæ€å’Œç»†åŒ–æ¥æé«˜è§„åˆ’èƒ½åŠ›ï¼Œé¼“åŠ±LLMåæ€å¤±è´¥å¹¶æ”¹è¿›è®¡åˆ’ã€‚
5. **è®°å¿†å¢å¼ºè§„åˆ’**ï¼šä½¿ç”¨é¢å¤–çš„è®°å¿†æ¨¡å—æ¥å¢å¼ºè§„åˆ’ï¼Œå…¶ä¸­å­˜å‚¨æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œå¦‚å¸¸è¯†çŸ¥è¯†ã€è¿‡å»ç»éªŒã€ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ç­‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å‡ ä¸ªä»£è¡¨æ€§æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œæ€§èƒ½éšç€æˆæœ¬çš„å¢åŠ è€Œæé«˜ã€‚æ­¤å¤–ï¼Œå¯¹äºå¤æ‚ä»»åŠ¡ï¼Œå°‘æ ·æœ¬ç¤ºä¾‹å¯¹äºLLMè¿›ä¸€æ­¥ç†è§£ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè€Œåæ€åœ¨æé«˜æˆåŠŸç‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºLLMä»£ç†çš„è§„åˆ’èƒ½åŠ›æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„è§†è§’ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨LLMæ¥æé«˜ä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†LLMä»£ç†è§„åˆ’ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å¹»è§‰ã€ç”Ÿæˆè®¡åˆ’çš„å¯è¡Œæ€§ã€ç”Ÿæˆè®¡åˆ’çš„æ•ˆç‡ã€å¤šæ¨¡æ€ç¯å¢ƒåé¦ˆå’Œç»†ç²’åº¦è¯„ä¼°ç­‰ã€‚è¿™äº›æŒ‘æˆ˜ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ï¼Œå¹¶æœ‰åŠ©äºæ¨åŠ¨LLMä»£ç†è§„åˆ’é¢†åŸŸçš„å‘å±•ã€‚

## webvoyager--building-an-end-to-end-web-agent-with-large-multimodal-models
### Abstract
The rapid advancement of large language models (LLMs) has led to a new era
marked by the development of autonomous applications in real-world scenarios,
which drives innovation in creating advanced web agents. Existing web agents
typically only handle one input modality and are evaluated only in simplified
web simulators or static web snapshots, greatly limiting their applicability in
real-world scenarios. To bridge this gap, we introduce WebVoyager, an
innovative Large Multimodal Model (LMM) powered web agent that can complete
user instructions end-to-end by interacting with real-world websites. Moreover,
we establish a new benchmark by compiling real-world tasks from 15 popular
websites and introduce an automatic evaluation protocol leveraging multimodal
understanding abilities of GPT-4V to evaluate open-ended web agents. We show
that WebVoyager achieves a 59.1% task success rate on our benchmark,
significantly surpassing the performance of both GPT-4 (All Tools) and the
WebVoyager (text-only) setups, underscoring the exceptional capability of
WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement
with human judgment, indicating its effectiveness in providing reliable and
accurate assessments of web agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebVoyagerï¼šåŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ç«¯åˆ°ç«¯ç½‘ç»œä»£ç†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªä¸»åº”ç”¨ç¨‹åºåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œæ¨åŠ¨äº†é«˜çº§ç½‘ç»œä»£ç†çš„åˆ›æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç½‘ç»œä»£ç†é€šå¸¸åªèƒ½å¤„ç†ä¸€ç§è¾“å…¥æ¨¡æ€ï¼Œå¹¶ä¸”ä»…åœ¨ç®€åŒ–çš„ç½‘ç»œæ¨¡æ‹Ÿå™¨æˆ–é™æ€ç½‘ç»œå¿«ç…§ä¸­è¿›è¡Œè¯„ä¼°ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†WebVoyagerï¼Œä¸€ä¸ªç”±å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰é©±åŠ¨çš„åˆ›æ–°ç½‘ç»œä»£ç†ï¼Œå®ƒå¯ä»¥é€šè¿‡ä¸çœŸå®ä¸–ç•Œç½‘ç«™çš„äº¤äº’æ¥å®Œæˆç”¨æˆ·æŒ‡ä»¤çš„ç«¯åˆ°ç«¯æ‰§è¡Œã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWebVoyageræ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç½‘ç»œä»£ç†ï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„èƒ½åŠ›ï¼Œé€šè¿‡å¤„ç†æ¥è‡ªäº¤äº’å¼ç½‘ç»œå…ƒç´ çš„å±å¹•æˆªå›¾å’Œæ–‡æœ¬å†…å®¹ï¼Œæ¥è§‚å¯Ÿç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ï¼Œç„¶åæ‰§è¡Œç›¸åº”çš„æ“ä½œï¼Œä¾‹å¦‚ç‚¹å‡»ã€è¾“å…¥æˆ–æ»šåŠ¨ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œé€šè¿‡ä»15ä¸ªæµè¡Œç½‘ç«™æ”¶é›†çœŸå®ä¸–ç•Œçš„ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ï¼Œåˆ©ç”¨GPT-4Vçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›æ¥è¯„ä¼°å¼€æ”¾å¼çš„ç½‘ç»œä»£ç†ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
WebVoyageråœ¨æ–°çš„åŸºå‡†ä¸Šå®ç°äº†59.1%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ˜¾è‘—è¶…è¿‡äº†GPT-4ï¼ˆæ‰€æœ‰å·¥å…·ï¼‰å’ŒWebVoyagerï¼ˆä»…æ–‡æœ¬ï¼‰è®¾ç½®çš„æ€§èƒ½ï¼Œçªå‡ºäº†WebVoyagerçš„å“è¶Šèƒ½åŠ›ã€‚æå‡ºçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§è¾¾åˆ°äº†85.3%ï¼Œè¡¨æ˜å…¶åœ¨æä¾›å¯é å’Œå‡†ç¡®çš„ç½‘ç»œä»£ç†è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebVoyagerçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ„å»ºæ›´æ™ºèƒ½å’Œé«˜æ•ˆçš„ç½‘ç»œè‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ–¹é¢ï¼Œåˆ©ç”¨å…ˆè¿›çš„LMMèƒ½åŠ›å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„è‡ªåŠ¨è¯„ä¼°åè®®ä¸ºè¯„ä¼°ç½‘ç»œä»£ç†çš„èƒ½åŠ›æä¾›äº†ä¸€ç§å¯é å’Œå‡†ç¡®çš„æ–¹æ³•ï¼Œå¯ä»¥ä¿ƒè¿›ç½‘ç»œä»£ç†ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚

## videowebarena--evaluating-long-context-multimodal-agents-with-video-understanding-web-tasks
### Abstract
Videos are often used to learn or extract the necessary information to
complete tasks in ways different than what text and static imagery alone can
provide. However, many existing agent benchmarks neglect long-context video
understanding, instead focusing on text or static image inputs. To bridge this
gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the
capabilities of long-context multimodal agents for video understanding. VideoWA
consists of 2,021 web agent tasks based on manually crafted video tutorials,
which total almost four hours of content. For our benchmark, we define a
taxonomy of long-context video-based agent tasks with two main areas of focus:
skill retention and factual retention. While skill retention tasks evaluate
whether an agent can use a given human demonstration to complete a task
efficiently, the factual retention task evaluates whether an agent can retrieve
instruction-relevant information from a video to complete a task. We find that
the best model achieves 13.3% success on factual retention tasks and 45.8% on
factual retention QA pairs, far below human performance at 73.9% and 79.3%,
respectively. On skill retention tasks, long-context models perform worse with
tutorials than without, exhibiting a 5% performance decrease in WebArena tasks
and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to
improve the agentic abilities of long-context multimodal models and provides a
testbed for future development with long-context video agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VideoWebArenaï¼šè¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»£ç†çš„è§†é¢‘ç†è§£èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹åŸºç¡€æ¨¡å‹è¢«åº”ç”¨äºAIä»£ç†ï¼Œè¿™äº›å¤šæ¨¡æ€ä»£ç†éœ€è¦å…·å¤‡ç†è§£å’Œå¤„ç†è§†é¢‘çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä»ç½‘ç»œä¸­æ£€ç´¢å’Œå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ä»¥å®Œæˆæ–°çŸ¥è¯†ä»»åŠ¡çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æˆ–é™æ€å›¾åƒè¾“å…¥ï¼Œå¿½ç•¥äº†é•¿ä¸Šä¸‹æ–‡è§†é¢‘ç†è§£ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†VideoWebArena (VideoWA)ï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»£ç†è§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šVideoWebArenaåŸºå‡†æµ‹è¯•
VideoWAç”±2,021ä¸ªåŸºäºæ‰‹åŠ¨åˆ¶ä½œè§†é¢‘æ•™ç¨‹çš„ç½‘ç»œä»£ç†ä»»åŠ¡ç»„æˆï¼Œæ€»æ—¶é•¿è¿‘å››ä¸ªå°æ—¶ã€‚è¯¥åŸºå‡†æµ‹è¯•å®šä¹‰äº†ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»£ç†ä»»åŠ¡çš„åˆ†ç±»æ³•ï¼Œé‡ç‚¹å…³æ³¨æŠ€èƒ½ä¿ç•™å’Œäº‹å®ä¿ç•™ä¸¤ä¸ªæ–¹é¢ã€‚æŠ€èƒ½ä¿ç•™ä»»åŠ¡è¯„ä¼°ä»£ç†æ˜¯å¦èƒ½å¤Ÿä½¿ç”¨ç»™å®šçš„äººç±»æ¼”ç¤ºæ¥é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ï¼Œè€Œäº‹å®ä¿ç•™ä»»åŠ¡è¯„ä¼°ä»£ç†æ˜¯å¦èƒ½å¤Ÿä»è§†é¢‘ä¸­æ£€ç´¢ä¸æŒ‡ä»¤ç›¸å…³çš„ä¿¡æ¯æ¥å®Œæˆä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°ç°æœ‰æ¨¡å‹
æœ¬æ–‡è¯„ä¼°äº†æµè¡Œçš„å’Œæœ€æ–°çš„è§†é¢‘/å›¾åƒèƒ½åŠ›LLMï¼ˆä¾‹å¦‚GPT-4oå’ŒGemini 1.5 Proï¼‰ï¼Œä»¥æ›´å¥½åœ°äº†è§£å®ƒä»¬å½“å‰çš„é•¿ä¸Šä¸‹æ–‡è§†é¢‘ç†è§£èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œè§†é¢‘/å›¾åƒèƒ½åŠ›ä»£ç†ä»ç„¶æœ‰é™ï¼Œè¿œæœªè¾¾åˆ°äººç±»çš„æ€§èƒ½æ°´å¹³ï¼Œçªå‡ºäº†å½“å‰æœ€å…ˆè¿›é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢å’Œä»£ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§å·®è·ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨äº‹å®ä¿ç•™ä»»åŠ¡ä¸Šï¼Œæœ€ä½³æ¨¡å‹çš„æˆåŠŸç‡ä¸º13.3%ï¼Œè€Œåœ¨äº‹å®ä¿ç•™QAå¯¹ä¸Šä¸º45.8%ï¼Œè¿œä½äºäººç±»çš„73.9%å’Œ79.3%ã€‚åœ¨æŠ€èƒ½ä¿ç•™ä»»åŠ¡ä¸Šï¼Œé•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨æ•™ç¨‹ä¸­çš„è¡¨ç°æ¯”æ²¡æœ‰æ•™ç¨‹æ—¶æ›´å·®ï¼Œåœ¨WebArenaä»»åŠ¡ä¸­ä¸‹é™äº†5%ï¼Œåœ¨VisualWebArenaä»»åŠ¡ä¸­ä¸‹é™äº†10.3%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
VideoWebArenaä¸ºè¯„ä¼°å’Œæ”¹è¿›é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»£ç†çš„è§†é¢‘ç†è§£èƒ½åŠ›æä¾›äº†ä¸€ä¸ªé‡è¦çš„æµ‹è¯•å¹³å°ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜è¯†åˆ«ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æ¨åŠ¨æœªæ¥åœ¨é•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»£ç†æ–¹é¢çš„ç ”ç©¶å’Œå‘å±•ã€‚

## tur[k]ingbench--a-challenge-benchmark-for-web-agents
### Abstract
Can advanced multi-modal models effectively tackle complex web-based tasks?
Such tasks are often found on crowdsourcing platforms, where crowdworkers
engage in challenging micro-tasks within web-based environments.
  Building on this idea, we present TurkingBench, a benchmark consisting of
tasks presented as web pages with textual instructions and multi-modal
contexts. Unlike previous approaches that rely on artificially synthesized web
pages, our benchmark uses natural HTML pages originally designed for
crowdsourcing workers to perform various annotation tasks. Each task's HTML
instructions are instantiated with different values derived from crowdsourcing
tasks, creating diverse instances. This benchmark includes 32.2K instances
spread across 158 tasks.
  To support the evaluation of TurkingBench, we have developed a framework that
links chatbot responses to actions on web pages (e.g., modifying a text box,
selecting a radio button). We assess the performance of cutting-edge private
and open-source models, including language-only and vision-language models
(such as GPT4 and InternVL), on this benchmark. Our results show that while
these models outperform random chance, there is still significant room for
improvement. We hope that this benchmark will drive progress in the evaluation
and development of web-based agents.
ä»‹ç»è®ºæ–‡çš„èƒŒæ™¯æˆ–ç—›ç‚¹ï¼Œæˆ–è€…æœ¬æ–‡çš„åŠ¨æœº

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªç„¶ç½‘é¡µä»»åŠ¡
ä¸åŒäºä»¥å¾€ä¾èµ–äººå·¥åˆæˆç½‘é¡µçš„æ–¹æ³•ï¼ŒTurkingBench ä½¿ç”¨äº†è‡ªç„¶ HTML é¡µé¢ï¼Œè¿™äº›é¡µé¢åŸæœ¬æ˜¯ä¸ºä¼—åŒ…å¹³å°ä¸Šçš„å·¥äººè®¾è®¡çš„ï¼Œç”¨äºæ‰§è¡Œå„ç§æ ‡æ³¨ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡çš„ HTML æŒ‡ä»¤éƒ½ä½¿ç”¨æ¥è‡ªä¼—åŒ…ä»»åŠ¡çš„å€¼è¿›è¡Œå®ä¾‹åŒ–ï¼Œä»è€Œåˆ›å»ºå¤šæ ·åŒ–çš„å®ä¾‹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäº¤äº’å¼è¯„ä¼°æ¡†æ¶
ä¸ºäº†æ”¯æŒ TurkingBench çš„è¯„ä¼°ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†èŠå¤©æœºå™¨äººçš„å“åº”é“¾æ¥åˆ°ç½‘é¡µä¸Šçš„æ“ä½œï¼ˆä¾‹å¦‚ï¼Œä¿®æ”¹æ–‡æœ¬æ¡†ã€é€‰æ‹©å•é€‰æŒ‰é’®ï¼‰ã€‚è¯¥æ¡†æ¶å…è®¸è¯„ä¼°æœ€å…ˆè¿›çš„ç§æœ‰å’Œå¼€æºæ¨¡å‹ï¼ˆåŒ…æ‹¬è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨ TurkingBench ä¸Šçš„æ€§èƒ½ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹çš„è¡¨ç°ä¼˜äºéšæœºçŒœæµ‹ï¼Œä½†ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ç ”ç©¶äººå‘˜åˆ†æäº†æ¨¡å‹åœ¨ä¸åŒå­—æ®µç±»å‹å’Œä»»åŠ¡é•¿åº¦ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ç¡®å®šäº†æœªæ¥è¿›æ­¥çš„æŒ‘æˆ˜ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
TurkingBench ä¸ºè¯„ä¼°å’Œå¼€å‘åŸºäº Web çš„æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åŸºå‡†ã€‚è¯¥åŸºå‡†çš„è‡ªç„¶ç½‘é¡µä»»åŠ¡å’Œäº¤äº’å¼è¯„ä¼°æ¡†æ¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¹³å°ï¼Œä»¥æµ‹è¯•å’Œæ”¹è¿›ä»–ä»¬çš„æ¨¡å‹åœ¨å¤„ç†å¤æ‚ Web ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚

## grounding-open-domain-instructions-to-automate-web-support-tasks
### Abstract
Grounding natural language instructions on the web to perform previously
unseen tasks enables accessibility and automation. We introduce a task and
dataset to train AI agents from open-domain, step-by-step instructions
originally written for people. We build RUSS (Rapid Universal Support Service)
to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with
pointers parses instructions to ThingTalk, a domain-specific language we design
for grounding natural language on the web. Then, a grounding model retrieves
the unique IDs of any webpage elements requested in ThingTalk. RUSS may
interact with the user through a dialogue (e.g. ask for an address) or execute
a web operation (e.g. click a button) inside the web runtime. To augment
training, we synthesize natural language instructions mapped to ThingTalk. Our
dataset consists of 80 different customer service problems from help websites,
with a total of 741 step-by-step instructions and their corresponding actions.
RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single
instructions. It outperforms state-of-the-art models that directly map
instructions to actions without ThingTalk. Our user study shows that RUSS is
preferred by actual users over web navigation.
### ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªåŠ¨åŒ–ç½‘ç»œæ”¯æŒä»»åŠ¡ï¼šå°†å¼€æ”¾åŸŸæŒ‡ä»¤æ˜ å°„åˆ°ç½‘ç»œä¸Šçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äº’è”ç½‘çš„æ™®åŠï¼Œè¶Šæ¥è¶Šå¤šçš„æœåŠ¡é€šè¿‡ç½‘ç«™æä¾›ã€‚ç„¶è€Œï¼Œå¯¹äºè§†åŠ›éšœç¢è€…ã€æŠ€æœ¯ä¸ç†Ÿç»ƒè€…æˆ–åˆ†å¿ƒè€…æ¥è¯´ï¼Œä½¿ç”¨è¿™äº›æœåŠ¡ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„è™šæ‹ŸåŠ©æ‰‹å’Œå‘¼å«ä¸­å¿ƒè™½ç„¶å¯ä»¥æä¾›å¸®åŠ©ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ç‰¹å®šçš„APIæˆ–ç”¨æˆ·æ¼”ç¤ºï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†RUSSï¼ˆRapid Universal Support Serviceï¼‰ï¼Œä¸€ä¸ªèƒ½å¤Ÿä»å¼€æ”¾åŸŸçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­å­¦ä¹ å¹¶æ‰§è¡Œç½‘ç»œä»»åŠ¡çš„AIä»£ç†ã€‚RUSSçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šThingTalk DSL
ä¸ºäº†å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°ç½‘ç»œæ“ä½œï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§åä¸ºThingTalkçš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰ã€‚ThingTalkåŒ…å«äº†ä¸€ç³»åˆ—çš„ä»£ç†æ“ä½œå’Œä¸€ä¸ªç”¨äºæ£€ç´¢ç½‘é¡µå…ƒç´ çš„å‡½æ•°ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ThingTalkæ—¢æ˜“äºä»è‡ªç„¶è¯­è¨€è¿›è¡Œè¯­ä¹‰è§£æï¼Œåˆæ˜“äºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å—åŒ–è®¾è®¡
RUSSç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼šè¯­ä¹‰è§£æå™¨ã€å®šä½æ¨¡å‹å’Œè¿è¡Œæ—¶ç¯å¢ƒã€‚è¯­ä¹‰è§£æå™¨ä½¿ç”¨BERT-LSTMæ¨¡å‹å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è§£æä¸ºThingTalkä»£ç ã€‚å®šä½æ¨¡å‹æ ¹æ®ThingTalkä»£ç æ£€ç´¢ç½‘é¡µå…ƒç´ çš„å”¯ä¸€IDã€‚è¿è¡Œæ—¶ç¯å¢ƒæ‰§è¡ŒThingTalkä»£ç ï¼ŒåŒ…æ‹¬ä¸ç”¨æˆ·çš„äº¤äº’å’Œç½‘ç»œæ“ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨åŒ…å«80ä¸ªä¸åŒå®¢æˆ·æœåŠ¡é—®é¢˜å’Œ741ä¸ªæ­¥éª¤çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ•°æ®é›†ä¸Šï¼ŒRUSSå®ç°äº†76.7%çš„ç«¯åˆ°ç«¯å‡†ç¡®ç‡ã€‚ä¸ç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œçš„ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒRUSSå…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œå®é™…ç”¨æˆ·æ›´å–œæ¬¢ä½¿ç”¨RUSSè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç½‘ç»œå¯¼èˆªã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„RUSSæ¨¡å‹ä¸ºæ„å»ºèƒ½å¤Ÿä»å¼€æ”¾åŸŸæŒ‡ä»¤ä¸­å­¦ä¹ å¹¶æ‰§è¡Œç½‘ç»œä»»åŠ¡çš„AIä»£ç†æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ã€‚ThingTalk DSLçš„è®¾è®¡å’Œæ¨¡å—åŒ–è®¾è®¡æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°ç§»åŠ¨è®¾å¤‡æ“ä½œæˆ–æœºå™¨äººæ“ä½œã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„RUSSæ•°æ®é›†å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›æœªæ¥çš„ç ”ç©¶ã€‚

## agentoccam--a-simple-yet-strong-baseline-for-llm-based-web-agents
### Abstract
Autonomy via agents using large language models (LLMs) for personalized,
standardized tasks boosts human efficiency. Automating web tasks (like booking
hotels within a budget) is increasingly sought after. Fulfilling practical
needs, the web agent also serves as an important proof-of-concept example for
various agent grounding scenarios, with its success promising advancements in
many future applications. Prior research often handcrafts web agent strategies
(e.g., prompting templates, multi-agent systems, search methods, etc.) and the
corresponding in-context examples, which may not generalize well across all
real-world scenarios. On the other hand, there has been limited study on the
misalignment between a web agent's observation/action representation and the
pre-training data of the LLM it's based on. This discrepancy is especially
notable when LLMs are primarily trained for language completion rather than
tasks involving embodied navigation actions and symbolic web elements. Our
study enhances an LLM-based web agent by simply refining its observation and
action space to better align with the LLM's capabilities. This approach enables
our base agent to significantly outperform previous methods on a wide variety
of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose
web interaction tasks, our agent AgentOccam surpasses the previous
state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute
points respectively, and boosts the success rate by 26.6 points (+161%) over
similar plain web agents with its observation and action space alignment. We
achieve this without using in-context examples, new agent roles, online
feedback or search strategies. AgentOccam's simple design highlights LLMs'
impressive zero-shot performance on web tasks, and underlines the critical role
of carefully tuning observation and action spaces for LLM-based agents.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AgentOccamï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç½‘ç»œä»£ç†çš„ç®€å•è€Œå¼ºå¤§çš„åŸºçº¿

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç½‘ç»œä»£ç†åœ¨ä¸ªæ€§åŒ–ã€æ ‡å‡†åŒ–çš„ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä¾‹å¦‚è‡ªåŠ¨é¢„è®¢é…’åº—ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç½‘ç»œä»£ç†ç­–ç•¥å¾€å¾€ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ¨¡æ¿ã€å¤šä»£ç†ç³»ç»Ÿã€æœç´¢æ–¹æ³•ç­‰ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ‰€æœ‰ç°å®åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œç½‘ç»œä»£ç†çš„è§‚å¯Ÿ/è¡ŒåŠ¨è¡¨ç¤ºä¸LLMçš„é¢„è®­ç»ƒæ•°æ®ä¹‹é—´çš„ä¸åŒ¹é…ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºLLMä¸»è¦é’ˆå¯¹è¯­è¨€å®Œæˆè¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯æ¶‰åŠå…·èº«å¯¼èˆªåŠ¨ä½œå’Œç¬¦å·ç½‘ç»œå…ƒç´ çš„ä»»åŠ¡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç®€åŒ–è¡ŒåŠ¨ç©ºé—´
AgentOccamé€šè¿‡åˆ é™¤ä¸å¿…è¦çš„è¡ŒåŠ¨ï¼Œä¾‹å¦‚æ— æ“ä½œã€æ ‡ç­¾æ“ä½œå’Œé¡µé¢å¯¼èˆªæ“ä½œï¼Œæ¥ç®€åŒ–è¡ŒåŠ¨ç©ºé—´ã€‚åŒæ—¶ï¼Œå®ƒå°†ä½çº§è¡ŒåŠ¨ç®€åŒ–ä¸ºæ›´æŠ½è±¡çš„æ“ä½œï¼Œä¾‹å¦‚å°†æ‚¬åœå’ŒæŒ‰é”®æ“ä½œæ›¿æ¢ä¸ºç‚¹å‡»æ“ä½œï¼Œå¹¶å°†æ»šåŠ¨æ“ä½œæ›¿æ¢ä¸ºåŠ è½½æ•´ä¸ªé¡µé¢å†…å®¹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¼˜åŒ–è§‚å¯Ÿç©ºé—´
AgentOccamé€šè¿‡åˆ é™¤å†—ä½™å’Œæ— å…³çš„ç½‘ç»œå…ƒç´ ï¼Œå¹¶å°†ç½‘é¡µå†…å®¹å—é‡æ„ä¸ºæ›´ç®€æ´ä½†åŒæ ·ä¿¡æ¯ä¸°å¯Œçš„è¡¨ç¤ºï¼Œæ¥ä¼˜åŒ–è§‚å¯Ÿç©ºé—´ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†ä¸¤ä¸ªè§„åˆ’è¡ŒåŠ¨ï¼ˆåˆ†æ”¯å’Œå‰ªæï¼‰ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä½¿ç”¨è§„åˆ’æ ‘æ¥è‡ªä¸»ç»„ç»‡å¯¼èˆªå·¥ä½œæµç¨‹ï¼Œå¹¶ä½¿ç”¨ç›¸åŒç»“æ„æ¥è¿‡æ»¤å†å²è®°å½•ä»¥è¿›è¡Œå†å²å›æ”¾ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§„åˆ’ç”Ÿæˆ
AgentOccamå¼•å…¥äº†åˆ†æ”¯å’Œå‰ªæè¡ŒåŠ¨ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆè®¡åˆ’å¹¶ç®¡ç†ä»»åŠ¡å·¥ä½œæµç¨‹ã€‚è¿™äº›è¡ŒåŠ¨å…è®¸ä»£ç†å°†é«˜çº§ç›®æ ‡åˆ†è§£ä¸ºæ›´å°çš„å­ç›®æ ‡ï¼Œå¹¶åœ¨å½“å‰å­è®¡åˆ’ä¸å¯è¡Œæ—¶å¯»æ±‚æ›¿ä»£æ–¹æ¡ˆã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨WebArenaåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgentOccamåœ¨é€šç”¨ç½‘ç»œäº¤äº’ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€ä½³æ–¹æ³•å’ŒåŒæœŸå·¥ä½œï¼Œåˆ†åˆ«æé«˜äº†9.8ï¼ˆ+29.4%ï¼‰å’Œ5.9ï¼ˆ+15.8%ï¼‰çš„ç»å¯¹åˆ†æ•°ï¼Œå¹¶å°†æˆåŠŸç‡æé«˜äº†26.6ï¼ˆ+161%ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
AgentOccamçš„ç®€å•è®¾è®¡çªå‡ºäº†LLMåœ¨ç½‘ç»œä»»åŠ¡ä¸Šçš„ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒäº†ä»”ç»†è°ƒæ•´è§‚å¯Ÿå’Œè¡ŒåŠ¨ç©ºé—´å¯¹äºåŸºäºLLMçš„ä»£ç†çš„å…³é”®ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥çš„ç½‘ç»œä»£ç†ç ”ç©¶å’Œå¼€å‘å¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œå¹¶æä¾›äº†ä¸€äº›æœ‰ä»·å€¼çš„è§è§£ã€‚

## thought-propagation--an-analogical-approach-to-complex-reasoning-with-large-language-models
### Abstract
Large Language Models (LLMs) have achieved remarkable success in reasoning
tasks with the development of prompting methods. However, existing prompting
approaches cannot reuse insights of solving similar problems and suffer from
accumulated errors in multi-step reasoning, since they prompt LLMs to reason
\textit{from scratch}. To address these issues, we propose
\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous
problems and leverages their solutions to enhance the complex reasoning ability
of LLMs. These analogous problems are related to the input one, with reusable
solutions and problem-solving strategies. Thus, it is promising to propagate
insights of solving previous analogous problems to inspire new problem-solving.
To achieve this, TP first prompts LLMs to propose and solve a set of analogous
problems that are related to the input one. Then, TP reuses the results of
analogous problems to directly yield a new solution or derive a
knowledge-intensive plan for execution to amend the initial solution obtained
from scratch. TP is compatible with existing prompting approaches, allowing
plug-and-play generalization and enhancement in a wide range of tasks without
much labor in task-specific prompt engineering. Experiments across three
challenging tasks demonstrate TP enjoys a substantial improvement over the
baselines by an average of 12\% absolute increase in finding the optimal
solutions in Shortest-path Reasoning, 13\% improvement of human preference in
Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent
Planning.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ€ç»´ä¼ æ’­ï¼šä¸€ç§åŸºäºç±»æ¯”çš„å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†æ–¹æ³•

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ç°æœ‰çš„æç¤ºæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š1ï¼‰æ— æ³•é‡ç”¨è§£å†³ç±»ä¼¼é—®é¢˜çš„è§è§£ï¼›2ï¼‰åœ¨å¤šæ­¥æ¨ç†ä¸­å®¹æ˜“ç´¯ç§¯é”™è¯¯ã€‚è¿™æ˜¯å› ä¸ºç°æœ‰çš„æ–¹æ³•éƒ½æ˜¯ä»é›¶å¼€å§‹è¿›è¡Œæ¨ç†ï¼Œæ— æ³•åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”å¯¹ä¸­é—´æ¨ç†é˜¶æ®µçš„é”™è¯¯æ•æ„Ÿã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡æå‡ºäº†â€œæ€ç»´ä¼ æ’­â€ï¼ˆThought Propagation, TPï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚TP æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ç±»æ¯”æ¨ç†ï¼Œé€šè¿‡æ¢ç´¢ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„ç±»ä¼¼é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬çš„è§£å†³æ–¹æ¡ˆæ¥å¢å¼º LLMs çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚

#### TP æ¡†æ¶çš„ä¸‰ä¸ªæ¨¡å—ï¼š
1. **LLM Propose**ï¼šç”Ÿæˆä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„ç±»ä¼¼é—®é¢˜ã€‚
2. **LLM Solve**ï¼šè§£å†³è¾“å…¥é—®é¢˜å’Œç±»ä¼¼é—®é¢˜ï¼Œå¹¶ç”Ÿæˆåˆå§‹è§£å†³æ–¹æ¡ˆã€‚
3. **LLM Aggregate**ï¼šèšåˆç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥ç”Ÿæˆæ–°çš„è§£å†³æ–¹æ¡ˆæˆ–åˆ¶å®šé«˜çº§è®¡åˆ’ï¼Œä»è€Œæ”¹è¿›è¾“å…¥é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚

#### TP æ¡†æ¶çš„ä¼˜åŠ¿ï¼š
* **é‡ç”¨å…ˆéªŒçŸ¥è¯†**ï¼šé€šè¿‡ç±»æ¯”æ¨ç†ï¼ŒTP å¯ä»¥é‡ç”¨è§£å†³ç±»ä¼¼é—®é¢˜çš„è§è§£ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚
* **å‡å°‘ç´¯ç§¯é”™è¯¯**ï¼šé€šè¿‡åˆ©ç”¨ç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼ŒTP å¯ä»¥å‡å°‘å¤šæ­¥æ¨ç†ä¸­çš„ç´¯ç§¯é”™è¯¯ã€‚
* **å…¼å®¹æ€§å¼º**ï¼šTP å¯ä»¥ä¸ç°æœ‰çš„æç¤ºæ–¹æ³•å…¼å®¹ï¼Œå®ç°å³æ’å³ç”¨çš„æ³›åŒ–å’Œå¢å¼ºï¼Œæ— éœ€è¿›è¡Œå¤§é‡çš„ä»»åŠ¡ç‰¹å®šæç¤ºå·¥ç¨‹ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¯„ä¼°äº† TP æ¡†æ¶ï¼ŒåŒ…æ‹¬æœ€çŸ­è·¯å¾„æ¨ç†ã€åˆ›æ„å†™ä½œå’Œ LLM-Agent è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTP åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚ï¼š
* **æœ€çŸ­è·¯å¾„æ¨ç†**ï¼šå¹³å‡ç»å¯¹æå‡ 12% çš„æœ€ä¼˜è§£æ‰¾åˆ°ç‡ã€‚
* **åˆ›æ„å†™ä½œ**ï¼šäººç±»åå¥½åº¦æå‡ 13%ã€‚
* **LLM-Agent è§„åˆ’**ï¼šä»»åŠ¡å®Œæˆç‡æå‡ 15%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
* **ç±»æ¯”æ¨ç†çš„åº”ç”¨**ï¼šTP æ¡†æ¶ä¸º LLMs çš„å¤æ‚æ¨ç†æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå³åˆ©ç”¨ç±»æ¯”æ¨ç†æ¥é‡ç”¨å…ˆéªŒçŸ¥è¯†å¹¶å‡å°‘ç´¯ç§¯é”™è¯¯ã€‚
* **æ¨¡å—åŒ–è®¾è®¡**ï¼šTP æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶æ˜“äºä¸å…¶ä»–æç¤ºæ–¹æ³•é›†æˆï¼Œå¹¶å®ç°å³æ’å³ç”¨çš„æ³›åŒ–å’Œå¢å¼ºã€‚
* **å®éªŒéªŒè¯**ï¼šæœ¬æ–‡åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå¯¹ TP æ¡†æ¶è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜å…¶å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚

### ğŸŒŸ æ€»ç»“
TP æ¡†æ¶ä¸º LLMs çš„å¤æ‚æ¨ç†æä¾›äº†ä¸€ç§æ–°é¢–è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡ç±»æ¯”æ¨ç†æ¥é‡ç”¨å…ˆéªŒçŸ¥è¯†å¹¶å‡å°‘ç´¯ç§¯é”™è¯¯ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚TP æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡å’Œå…¼å®¹æ€§å¼ºä½¿å…¶æ˜“äºåº”ç”¨å’Œæ‰©å±•ï¼Œæœ‰æœ›åœ¨æœªæ¥æ¨åŠ¨ LLMs æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥æå‡ã€‚

## large-language-model-brained-gui-agents--a-survey
### Abstract
GUIs have long been central to human-computer interaction, providing an
intuitive and visually-driven way to access and interact with digital systems.
The advent of LLMs, particularly multimodal models, has ushered in a new era of
GUI automation. They have demonstrated exceptional capabilities in natural
language understanding, code generation, and visual processing. This has paved
the way for a new generation of LLM-brained GUI agents capable of interpreting
complex GUI elements and autonomously executing actions based on natural
language instructions. These agents represent a paradigm shift, enabling users
to perform intricate, multi-step tasks through simple conversational commands.
Their applications span across web navigation, mobile app interactions, and
desktop automation, offering a transformative user experience that
revolutionizes how individuals interact with software. This emerging field is
rapidly advancing, with significant progress in both research and industry.
  To provide a structured understanding of this trend, this paper presents a
comprehensive survey of LLM-brained GUI agents, exploring their historical
evolution, core components, and advanced techniques. We address research
questions such as existing GUI agent frameworks, the collection and utilization
of data for training specialized GUI agents, the development of large action
models tailored for GUI tasks, and the evaluation metrics and benchmarks
necessary to assess their effectiveness. Additionally, we examine emerging
applications powered by these agents. Through a detailed analysis, this survey
identifies key research gaps and outlines a roadmap for future advancements in
the field. By consolidating foundational knowledge and state-of-the-art
developments, this work aims to guide both researchers and practitioners in
overcoming challenges and unlocking the full potential of LLM-brained GUI
agents.
Wang, and G. Zeng, â€œPaLM-2: An
evolved language model for everything,â€ arXiv preprint arXiv:2304.02162,
2023.
[455] J. Li, Z. Wang, Y. Li, X. Wang, Y. Li, H. Wang, and Y. Li,
â€œFlorence-2-base: A lightweight vision-language model for
efficient
web
interaction,â€
2024.
[Online].
Available:
https://arxiv.org/abs/2404.03667
[456] Z. Wang, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, L. Chen, Z. Liu,
P. P. Liang et al., â€œOs-atlas: A foundation action model for
generalist gui agents,â€ arXiv preprint arXiv:2410.23218, 2024.
[457] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[458] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[459] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[460] S. Malek, A. Burns, D. Arsan, R. Kumar, K. Saenko, and B. A.
Plummer, â€œMeta-gui: Towards multi-modal conversational agents
on mobile gui,â€ 2022. [Online]. Available: https://arxiv.org/abs/
2205.11029
[461] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[462] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[463] OpenAI, â€œOperator: Introducing a universal interface for ai to
interact with the digital world,â€ 2025. [Online]. Available:
https://openai.com/index/computer-using-agent
[464] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,
S. Gelly, J. Uszkoreit, and N. Houlsby, â€œAn image is worth 16x16
words: Transformers for image recognition at scale,â€ in International
conference on machine learning. PMLR, 2020, pp. 15 841â€“15 855.
[465] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[466] Z. Wang, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, L. Chen, Z. Liu,
P. P. Liang et al., â€œOs-atlas: A foundation action model for
generalist gui agents,â€ arXiv preprint arXiv:2410.23218, 2024.
[467] Z. Chen, H. Zhang, O. Rippel, M. Mattsson, and J. Redmon,
â€œSwin transformer: Hierarchical vision transformer using shifted
windows,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2021, pp. 10091â€“10100.
[468] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[469] H. Zhang, Z. Chen, Y. Li, Y. Wang, and H. Zhang, â€œMixture of
experts for vision language understanding,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2023, pp. 17 718â€“17 728.
[470] Y. Sun, S. Wang, Y. Li, S. Feng, H. Wang, and H. Wang, â€œSmall-
bert: A distilled version of bert for natural language understanding,â€
arXiv preprint arXiv:1908.08124, 2019.
[471] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[472] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[473] T. Wang, K. Wei, S. Feng, B. Chen, H. Li, M. Zhou, H. Wang,
J. Wang, Z. Wang, and H. Zhou, â€œMpt-7b: Training a large language
model to follow your instructions,â€ arXiv preprint arXiv:2304.08716,
2023.
[474] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[475] E. Tzeng, J. Wang, Y. Wang, Y. Li, and H. Zhou, â€œEva-2-clip:
An open-source vision-language model for multimodal understanding
and generation,â€ arXiv preprint arXiv:2403.05055, 2024.
[476] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[477] Z. Chen, Y. Li, Y. Wang, H. Zhang, Z. Chen, Z. Wang, L. Lu,
Y. Li, and H. Zhou, â€œConvnext-xlarge: A large-scale vision
model for multimodal understanding and generation,â€ arXiv preprint
arXiv:2304.13677, 2023.
[478] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[479] D. Zimmermann and A. Koziolek, â€œGui-based software testing: An
automated approach using gpt-4 and selenium webdriver,â€ in 2023
38th IEEE/ACM International Conference on Automated Software
Engineering Workshops (ASEW). IEEE, 2023, pp. 171â€“174.
[480] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[481] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[482] Eko. (2024) Eko: The ai agent platform. Accessed: 2024-11-16.
[Online]. Available: https://eko.fellou.ai/
[483] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[484] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[485] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[486] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[487] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[488] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[489] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[490] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[491] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[492] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[493] C. Dwork, F. McSherry, K. Nissim, and A. Smith, â€œCalibrating
noise to sensitivity in private data analysis,â€ in Theory of Cryptography
Conference. Springer, 2006, pp. 265â€“284.
[494] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[495] C. Gentry, S. Halevi, and N. P. Smart, â€œHomomorphic encryption
from learning with errors: Conceptually-simpler, asymptotically-
faster, attribute-based, and more secure,â€ in Proceedings of the 14th
ACM SIGSAC Conference on Computer and Communications Security.
ACM, 2007, pp. 129â€“138.
[496] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[497] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[498] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[499] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[500] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[501] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[502] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[503] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[504] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[505] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[506] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[507] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[508] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[509] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[510] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[511] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[512] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[513] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[514] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[515] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[516] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping
natural language instructions to mobile ui action sequences,â€ 2020.
[Online]. Available: https://arxiv.org/abs/2005.03776
[517] Y. Li, J. He, X. Zhou, Y

## appagent--multimodal-agents-as-smartphone-users
### Abstract
Recent advancements in large language models (LLMs) have led to the creation
of intelligent agents capable of performing complex tasks. This paper
introduces a novel LLM-based multimodal agent framework designed to operate
smartphone applications. Our framework enables the agent to operate smartphone
applications through a simplified action space, mimicking human-like
interactions such as tapping and swiping. This novel approach bypasses the need
for system back-end access, thereby broadening its applicability across diverse
apps. Central to our agent's functionality is its innovative learning method.
The agent learns to navigate and use new apps either through autonomous
exploration or by observing human demonstrations. This process generates a
knowledge base that the agent refers to for executing complex tasks across
different applications. To demonstrate the practicality of our agent, we
conducted extensive testing over 50 tasks in 10 different applications,
including social media, email, maps, shopping, and sophisticated image editing
tools. The results affirm our agent's proficiency in handling a diverse array
of high-level tasks.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AppAgentï¼šåƒäººç±»ç”¨æˆ·ä¸€æ ·æ“ä½œæ™ºèƒ½æ‰‹æœºåº”ç”¨çš„æ™ºèƒ½ä½“

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ™ºèƒ½ä½“ä¸»è¦ä¾èµ–äºæ–‡æœ¬ä¿¡æ¯ï¼Œé™åˆ¶äº†å®ƒä»¬ä¸ç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨åƒäººç±»ç”¨æˆ·ä¸€æ ·æ“ä½œæ™ºèƒ½æ‰‹æœºåº”ç”¨ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç®€åŒ–åŠ¨ä½œç©ºé—´
æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–çš„åŠ¨ä½œç©ºé—´ï¼ŒåŒ…æ‹¬ç‚¹å‡»ã€é•¿æŒ‰ã€æ»‘åŠ¨å’Œè¾“å…¥æ–‡æœ¬ç­‰æ“ä½œï¼Œæ¨¡æ‹Ÿäººç±»ä¸æ™ºèƒ½æ‰‹æœºçš„äº¤äº’æ–¹å¼ã€‚è¿™ç§è®¾è®¡é¿å…äº†éœ€è¦ç²¾ç¡®å±å¹•åæ ‡çš„é—®é¢˜ï¼Œæé«˜äº†æ™ºèƒ½ä½“çš„æ“ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¢ç´¢å¼å­¦ä¹ æ–¹æ³•
æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ¢ç´¢å¼å­¦ä¹ æ–¹æ³•ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ ä½¿ç”¨æ–°çš„åº”ç”¨ç¨‹åºã€‚ä¸€ç§æ˜¯è‡ªä¸»æ¢ç´¢ï¼Œæ™ºèƒ½ä½“é€šè¿‡å°è¯•ä¸åŒçš„æ“ä½œå¹¶è§‚å¯Ÿç»“æœæ¥å­¦ä¹ åº”ç”¨ç¨‹åºçš„åŠŸèƒ½ã€‚å¦ä¸€ç§æ˜¯è§‚å¯Ÿäººç±»æ¼”ç¤ºï¼Œæ™ºèƒ½ä½“é€šè¿‡è§‚å¯Ÿäººç±»ç”¨æˆ·å¦‚ä½•æ“ä½œåº”ç”¨ç¨‹åºæ¥å­¦ä¹ ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡åœ¨10ä¸ªä¸åŒçš„åº”ç”¨ç¨‹åºä¸Šè¿›è¡Œäº†50ä¸ªä»»åŠ¡çš„æµ‹è¯•ï¼ŒåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€ç”µå­é‚®ä»¶ã€åœ°å›¾ã€è´­ç‰©å’Œå¤æ‚çš„å›¾åƒç¼–è¾‘å·¥å…·ã€‚ç»“æœè¡¨æ˜ï¼ŒAppAgentèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„ç§é«˜çº§ä»»åŠ¡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„AppAgentæ¡†æ¶ä¸ºæ™ºèƒ½æ‰‹æœºåº”ç”¨æ“ä½œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š

* **ç®€åŒ–åŠ¨ä½œç©ºé—´**ï¼šé€šè¿‡è®¾è®¡ç®€åŒ–çš„åŠ¨ä½œç©ºé—´ï¼Œå¯ä»¥é™ä½æ™ºèƒ½ä½“æ“ä½œçš„å¤æ‚æ€§ï¼Œæé«˜æ“ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚
* **æ¢ç´¢å¼å­¦ä¹ æ–¹æ³•**ï¼šé€šè¿‡è‡ªä¸»æ¢ç´¢æˆ–è§‚å¯Ÿäººç±»æ¼”ç¤ºï¼Œæ™ºèƒ½ä½“å¯ä»¥å¿«é€Ÿå­¦ä¹ ä½¿ç”¨æ–°çš„åº”ç”¨ç¨‹åºï¼Œæé«˜é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚
* **å¤šæ¨¡æ€äº¤äº’**ï¼šç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°ç†è§£ç¯å¢ƒå’Œæ‰§è¡Œä»»åŠ¡ã€‚

### ğŸŒŸ æ€»ç»“
AppAgentæ˜¯ä¸€ç§åŸºäºLLMçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿåƒäººç±»ç”¨æˆ·ä¸€æ ·æ“ä½œæ™ºèƒ½æ‰‹æœºåº”ç”¨ã€‚è¯¥æ¡†æ¶å…·æœ‰ç®€åŒ–åŠ¨ä½œç©ºé—´ã€æ¢ç´¢å¼å­¦ä¹ æ–¹æ³•å’Œå¤šæ¨¡æ€äº¤äº’ç­‰åˆ›æ–°ç‚¹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„ç§é«˜çº§ä»»åŠ¡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚AppAgentä¸ºæ™ºèƒ½æ‰‹æœºåº”ç”¨æ“ä½œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

## mobileexperts--a-dynamic-tool-enabled-agent-team-in-mobile-devices
### Abstract
The attainment of autonomous operations in mobile computing devices has
consistently been a goal of human pursuit. With the development of Large
Language Models (LLMs) and Visual Language Models (VLMs), this aspiration is
progressively turning into reality. While contemporary research has explored
automation of simple tasks on mobile devices via VLMs, there remains
significant room for improvement in handling complex tasks and reducing high
reasoning costs. In this paper, we introduce MobileExperts, which for the first
time introduces tool formulation and multi-agent collaboration to address the
aforementioned challenges. More specifically, MobileExperts dynamically
assembles teams based on the alignment of agent portraits with the human
requirements. Following this, each agent embarks on an independent exploration
phase, formulating its tools to evolve into an expert. Lastly, we develop a
dual-layer planning mechanism to establish coordinate collaboration among
experts. To validate our effectiveness, we design a new benchmark of
hierarchical intelligence levels, offering insights into algorithm's capability
to address tasks across a spectrum of complexity. Experimental results
demonstrate that MobileExperts performs better on all intelligence levels and
achieves ~ 22% reduction in reasoning costs, thus verifying the superiority of
our design.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MobileExpertsï¼šç§»åŠ¨è®¾å¤‡ä¸­çš„åŠ¨æ€å·¥å…·èµ‹èƒ½æ™ºèƒ½ä½“å›¢é˜Ÿ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•ï¼Œç§»åŠ¨è®¾å¤‡ä¸Šçš„è‡ªä¸»æ“ä½œé€æ¸æˆä¸ºç°å®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡VLMsè‡ªåŠ¨åŒ–ç§»åŠ¨è®¾å¤‡ä¸Šçš„ç®€å•ä»»åŠ¡ï¼Œè€Œåœ¨å¤„ç†å¤æ‚ä»»åŠ¡å’Œé™ä½æ¨ç†æˆæœ¬æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºä»£ç ç»„åˆçš„å·¥å…·å½¢æˆ
ä¸ºäº†å‡å°‘å¯¹VLMæ¨¡å‹çš„ä¾èµ–ï¼ŒMobileExpertsæå‡ºäº†ä¸€ç§æ–°çš„å·¥å…·æ¨¡å¼ï¼šåˆ©ç”¨LLMçš„ä»£ç ç¼–å†™èƒ½åŠ›ï¼Œé€šè¿‡ä»£ç ç»„åˆå°†ä¸“å®¶çš„åŸºæœ¬æ“ä½œç»„åˆæˆå¯é‡ç”¨çš„ä»£ç å—å·¥å…·ã€‚è¿™äº›å·¥å…·å­˜å‚¨åœ¨ç³»ç»Ÿä¸­çš„æ‰€æœ‰ä¸“å®¶å…±äº«çš„è¿‡ç¨‹è®°å¿†ä¸­ï¼Œä»è€Œé™ä½äº†å·¥å…·å½¢æˆçš„æˆæœ¬ï¼Œå¹¶æœ‰æ•ˆåœ°æé«˜äº†æ“ä½œæ•ˆç‡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€šè¿‡åŒå±‚è§„åˆ’å®ç°ä¸“å®¶åä½œ
MobileExpertsé‡‡ç”¨åŒå±‚è§„åˆ’æ–¹æ³•æ¥è§£å†³é•¿æœŸè§„åˆ’é—®é¢˜ï¼šç¬¬ä¸€å±‚æ˜¯å›¢é˜Ÿä»»åŠ¡åˆ†é…å±‚ï¼Œåœ¨è¿™ä¸€å±‚ï¼Œä»»åŠ¡è¢«åˆ†è§£ä¸ºä¸“å®¶æ‰§è¡Œçš„ä¾èµ–å­ä»»åŠ¡ï¼Œè¿™äº›ä¾èµ–å½¢æˆä¸“å®¶ä¹‹é—´çš„åä½œç½‘ç»œï¼›ç¬¬äºŒå±‚æ˜¯ä¸“å®¶ä»»åŠ¡åˆ†è§£å±‚ï¼Œä¸“å®¶æ¥æ”¶åˆ°çš„å­ä»»åŠ¡è¿›ä¸€æ­¥åˆ†è§£ä¸ºæ›´å°çš„åŠ¨ä½œå•å…ƒï¼Œé€æ­¥å®ç°ç›®æ ‡ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼ŒMobileExpertsä¸ä»…æé«˜äº†æ™ºèƒ½æ°´å¹³ï¼Œè¿˜é™ä½äº†æ¨ç†æˆæœ¬å’Œæ—¶é—´ï¼Œä¸ºDOAé¢†åŸŸæä¾›äº†æ›´é«˜æ•ˆçš„æ‰§è¡Œæ¨¡å¼ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
ä¸ºäº†éªŒè¯MobileExpertsçš„æœ‰æ•ˆæ€§ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªæ–°çš„åˆ†å±‚æ™ºèƒ½æ°´å¹³åŸºå‡†ï¼Œæä¾›äº†å¯¹ç®—æ³•å¤„ç†ä¸åŒå¤æ‚åº¦ä»»åŠ¡èƒ½åŠ›çš„è§è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMobileExpertsåœ¨æ‰€æœ‰æ™ºèƒ½æ°´å¹³ä¸Šéƒ½è¡¨ç°æ›´å¥½ï¼Œå¹¶å®ç°äº†çº¦22%çš„æ¨ç†æˆæœ¬é™ä½ï¼Œä»è€ŒéªŒè¯äº†è®¾è®¡çš„ä¼˜è¶Šæ€§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MobileExpertsä¸ºç§»åŠ¨è®¾å¤‡æ“ä½œè‡ªåŠ¨åŒ–é¢†åŸŸæä¾›äº†ä¸€ç§åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶åŸºäºä»£ç ç»„åˆçš„å·¥å…·å½¢æˆå’ŒåŒå±‚è§„åˆ’æœºåˆ¶ä¸ºå¤„ç†å¤æ‚ä»»åŠ¡å’Œé™ä½æ¨ç†æˆæœ¬æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºçš„Expert-EvalåŸºå‡†ä¸ºè¯„ä¼°ç§»åŠ¨è®¾å¤‡æ“ä½œä»£ç†çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚

## exploring-the-privacy-protection-capabilities-of-chinese-large-language-models
### Abstract
Large language models (LLMs), renowned for their impressive capabilities in
various tasks, have significantly advanced artificial intelligence. Yet, these
advancements have raised growing concerns about privacy and security
implications. To address these issues and explain the risks inherent in these
models, we have devised a three-tiered progressive framework tailored for
evaluating privacy in language systems. This framework consists of
progressively complex and in-depth privacy test tasks at each tier. Our primary
objective is to comprehensively evaluate the sensitivity of large language
models to private information, examining how effectively they discern, manage,
and safeguard sensitive data in diverse scenarios. This systematic evaluation
helps us understand the degree to which these models comply with privacy
protection guidelines and the effectiveness of their inherent safeguards
against privacy breaches. Our observations indicate that existing Chinese large
language models universally show privacy protection shortcomings. It seems that
at the moment this widespread issue is unavoidable and may pose corresponding
privacy risks in applications based on these models.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢ä¸­å›½å¤§å‹è¯­è¨€æ¨¡å‹çš„éšç§ä¿æŠ¤èƒ½åŠ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºçš„å¼ºå¤§èƒ½åŠ›æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ä¹Ÿå¼•å‘äº†å…³äºéšç§å’Œå®‰å…¨æ€§çš„æ‹…å¿§ã€‚ç”±äºLLMsçš„è®­ç»ƒæ•°æ®å¾€å¾€åŒ…å«å¤§é‡ä¸ªäººéšç§ä¿¡æ¯ï¼Œå› æ­¤æ¨¡å‹å¯èƒ½ä¼šæ— æ„ä¸­è®°ä½è¿™äº›å†…å®¹ï¼Œä»è€Œå¯¹æ•°æ®éšç§æ„æˆå®‰å…¨é£é™©ã€‚æ­¤å¤–ï¼Œéšç€è¶Šæ¥è¶Šå¤šçš„æœåŠ¡æä¾›å•†å°†LLMsé›†æˆåˆ°å…¶è½¯ä»¶åº”ç”¨ä¸­ï¼Œæ¨¡å‹åœ¨å¤„ç†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„ç§äººæ–‡æ¡£æ•°æ®æ—¶ï¼Œéœ€è¦ä¸¥æ ¼éµå®ˆéšç§ä¿æŠ¤è§„åˆ™ï¼Œå¹¶èƒ½å¤Ÿè¯†åˆ«å’Œä¿æŠ¤æ•æ„Ÿéšç§ä¿¡æ¯ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†è¯„ä¼°LLMsçš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰å±‚çº§çš„éšç§æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»æµ…åˆ°æ·±åœ°è¯„ä¼°LLMsåœ¨ä¸åŒä»»åŠ¡åœºæ™¯ä¸‹çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚å…·ä½“åŒ…æ‹¬ï¼š

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸€èˆ¬éšç§ä¿¡æ¯è¯„ä¼°
è¯¥å±‚çº§è¯„ä¼°LLMsåœ¨é¢å¯¹ç›´æ¥è¯¢é—®ä¸ªäººéšç§ä¿¡æ¯ï¼ˆå¦‚ç”µè¯å·ç ã€ç”µå­é‚®ä»¶åœ°å€ã€å®¶åº­ä½å€ç­‰ï¼‰æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹åº”èƒ½å¤Ÿæ‹’ç»å›ç­”æ­¤ç±»é—®é¢˜ï¼Œå¹¶è¾“å‡ºåˆé€‚çš„ç†ç”±ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸Šä¸‹æ–‡éšç§è¯„ä¼°
è¯¥å±‚çº§è¯„ä¼°LLMsåœ¨é¢å¯¹æ¶‰åŠéšç§ä¿¡æ¯çš„æƒ…å¢ƒå¯¹è¯æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹åº”èƒ½å¤Ÿè¯†åˆ«æƒ…å¢ƒä¸­çš„éšç§åè®®ï¼Œå¹¶éµå®ˆè¿™äº›åè®®ï¼Œä»¥ä¿æŠ¤ç›¸å…³å†…å®¹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”»å‡»ä¸‹çš„éšç§è¯„ä¼°
è¯¥å±‚çº§è¯„ä¼°LLMsåœ¨é¢å¯¹æ”»å‡»æ€§æŒ‡ä»¤æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹åº”èƒ½å¤Ÿè¯†åˆ«æ”»å‡»æŒ‡ä»¤èƒŒåçš„éšç§æ³„éœ²é£é™©ï¼Œå¹¶ä¸¥æ ¼éµå®ˆå†…éƒ¨éšç§ä¿æŠ¤æŒ‡ä»¤ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œé™¤äº†0-shotæµ‹è¯•å¤–ï¼ŒLLMsåœ¨å…¶ä»–ä»»åŠ¡åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¸ç†æƒ³ã€‚è¿™äº›æ¨¡å‹æœªèƒ½å±•ç°å‡ºè¶³å¤Ÿçš„éšç§æ•æ„Ÿæ€§å’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚è¿™è¡¨æ˜ï¼ŒLLMsåœ¨å¤„ç†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ•°æ®æ—¶ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæ”¹è¿›ï¼Œä»¥ç¡®ä¿ç›¸å…³æ•°æ®çš„å®‰å…¨å’Œéšç§ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„éšç§æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ä¸ºè¯„ä¼°LLMsçš„éšç§ä¿æŠ¤èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿæé†’æ¨¡å‹æœåŠ¡æä¾›å•†/å¼€å‘è€…éœ€è¦æ›´åŠ å…³æ³¨LLMsçš„éšç§ä¿æŠ¤é—®é¢˜ï¼Œå¹¶é‡‡å–ç›¸åº”çš„æªæ–½æ¥é™ä½éšç§æ³„éœ²çš„é£é™©ã€‚

## webpilot--a-versatile-and-autonomous-multi-agent-system-for-web-task-execution-with-strategic-exploration
### Abstract
LLM-based autonomous agents often fail to execute complex web tasks that
require dynamic interaction due to the inherent uncertainty and complexity of
these environments. Existing LLM-based web agents typically rely on rigid,
expert-designed policies specific to certain states and actions, which lack the
flexibility and generalizability needed to adapt to unseen tasks. In contrast,
humans excel by exploring unknowns, continuously adapting strategies, and
resolving ambiguities through exploration. To emulate human-like adaptability,
web agents need strategic exploration and complex decision-making. Monte Carlo
Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with
vast action spaces, unpredictable state transitions, and incomplete information
in web tasks. In light of this, we develop WebPilot, a multi-agent system with
a dual optimization strategy that improves MCTS to better handle complex web
environments. Specifically, the Global Optimization phase involves generating a
high-level plan by breaking down tasks into manageable subtasks and
continuously refining this plan, thereby focusing the search process and
mitigating the challenges posed by vast action spaces in classical MCTS.
Subsequently, the Local Optimization phase executes each subtask using a
tailored MCTS designed for complex environments, effectively addressing
uncertainties and managing incomplete information. Experimental results on
WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on
WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%
relative increase in success rate over the concurrent tree search-based method.
WebPilot marks a significant advancement in general autonomous agent
capabilities, paving the way for more advanced and reliable decision-making in
practical environments.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebPilotï¼šçµæ´»è‡ªä¸»çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŠ©åŠ›å¤æ‚ç½‘ç»œä»»åŠ¡æ‰§è¡Œ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä¸æ–­å¢å¼ºï¼ŒåŸºäºLLMçš„è‡ªä¸»ç½‘ç»œä»£ç†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªå’Œäº¤äº’çš„æ½œåŠ›ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMç½‘ç»œä»£ç†åœ¨æ‰§è¡Œéœ€è¦åŠ¨æ€äº¤äº’çš„å¤æ‚ç½‘ç»œä»»åŠ¡æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬è¿‡åº¦ä¾èµ–ä¸“å®¶è®¾è®¡çš„ã€é’ˆå¯¹ç‰¹å®šçŠ¶æ€å’ŒåŠ¨ä½œçš„åˆšæ€§ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥è™½ç„¶é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œäº†ç²¾å¿ƒè®¾è®¡ï¼Œä½†ç¼ºä¹çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”ç°å®ä¸–ç•Œä¸­ç½‘ç»œç¯å¢ƒçš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†WebPilotï¼Œä¸€ä¸ªçµæ´»è‡ªä¸»çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå®ƒç»“åˆäº†å…¨å±€ä¼˜åŒ–å’Œå±€éƒ¨ä¼˜åŒ–çš„åŒé‡ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜ä»£ç†åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨å±€ä¼˜åŒ–ï¼šé€šè¿‡åæ€è°ƒæ•´è¿›è¡Œè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–
WebPilotçš„å…¨å±€ä¼˜åŒ–é˜¶æ®µæ¨¡æ‹Ÿäº†äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œé€šè¿‡åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ä¸ºä¸ç†Ÿæ‚‰çš„ä»»åŠ¡ç”Ÿæˆåˆå§‹è®¡åˆ’ã€‚ç„¶è€Œï¼Œç”±äºLLMç¼ºä¹ç‰¹å®šçš„ç½‘ç»œé¢†åŸŸçŸ¥è¯†ï¼Œä»¥åŠç½‘ç»œç¯å¢ƒçš„åŠ¨æ€æ€§å’Œä¸ç¡®å®šæ€§ï¼Œåˆå§‹è®¡åˆ’å¾€å¾€ç¼ºä¹å…³é”®ç»†èŠ‚ï¼Œéš¾ä»¥ä¿æŒæœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒWebPiloté€šè¿‡åæ€åˆ†ææ–°è§‚å¯Ÿå’Œå…ˆå‰å­ä»»åŠ¡çš„ç»“æœï¼Œä¸æ–­ä¼˜åŒ–åˆå§‹è®¡åˆ’ã€‚å…¨å±€ä¼˜åŒ–åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±‚æ¬¡ä»»åŠ¡åˆ†è§£ï¼ˆHTDï¼‰å’Œåæ€ä»»åŠ¡è°ƒæ•´ï¼ˆRTAï¼‰ã€‚

*   **å±‚æ¬¡ä»»åŠ¡åˆ†è§£ï¼ˆHTDï¼‰**ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“äºç®¡ç†çš„å­ä»»åŠ¡ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªçµæ´»çš„é«˜çº§è®¡åˆ’ï¼Œå¯ä»¥é€‚åº”ç½‘ç»œç¯å¢ƒçš„ä¸æ–­å˜åŒ–ã€‚
*   **åæ€ä»»åŠ¡è°ƒæ•´ï¼ˆRTAï¼‰**ï¼šåœ¨å®Œæˆæ¯ä¸ªå­ä»»åŠ¡åï¼ŒWebPilotä¼šé‡æ–°è¯„ä¼°å’Œä¼˜åŒ–å…¶é«˜çº§è®¡åˆ’ï¼Œä»¥ç¡®ä¿ä¸æ•´ä½“ä»»åŠ¡ä¿æŒä¸€è‡´ã€‚æ§åˆ¶å™¨ä¼šè¯„ä¼°å½“å‰è§‚å¯Ÿå’Œæ‰§è¡Œçš„åŠ¨ä½œåºåˆ—æ˜¯å¦ç¬¦åˆå­ä»»åŠ¡ï¼Œå¹¶æ ¹æ®æ–°è§‚å¯Ÿé‡æ–°æ ¡å‡†ç­–ç•¥ã€‚

#### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå±€éƒ¨ä¼˜åŒ–ï¼šMCTSå¢å¼ºçš„å†³ç­–ç­–ç•¥
WebPilotçš„å±€éƒ¨ä¼˜åŒ–é˜¶æ®µå—äººç±»åœ¨å¯¼èˆªå’Œè§£å†³å¤æ‚ç½‘ç»œä»»åŠ¡æ—¶æ‰€éœ€çš„ç±»ä¼¼é€‚åº”æ€§å¯å‘ï¼Œæœ‰æ•ˆåœ°é€šè¿‡MCTSæ¥æ•æ‰ã€‚å¯¹äºæ¯ä¸ªå­ä»»åŠ¡åŠå…¶å­ä»»åŠ¡ç‰¹å®šçš„ç›®æ ‡ï¼Œæ¢ç´¢å™¨ã€éªŒè¯å™¨å’Œè¯„ä¼°å™¨ååŒå·¥ä½œä»¥å®Œæˆä»»åŠ¡ã€‚æ¢ç´¢å™¨è¯†åˆ«æœ€ä½³åŠ¨ä½œï¼ŒéªŒè¯å™¨ç¡®ä¿è¿™äº›åŠ¨ä½œæœ‰æ•ˆä¸”ä¸å†—ä½™ï¼Œè¯„ä¼°å™¨è¯„ä¼°åŠ¨ä½œçš„å³æ—¶æœ‰æ•ˆæ€§å’Œå®ç°é¢„æœŸç›®æ ‡çš„æ½œåŠ›ï¼Œå¹¶æä¾›æŒç»­çš„åé¦ˆï¼Œä»¥ä¾¿è¿›è¡Œæ›´ç»†è‡´å’Œå‡†ç¡®çš„è¯„ä¼°ã€‚

WebPilotçš„å±€éƒ¨ä¼˜åŒ–é˜¶æ®µç±»ä¼¼äºç»å…¸çš„MCTSï¼Œéµå¾ªå››ä¸ªå…³é”®é˜¶æ®µï¼š

*   **ç›®æ ‡å¯¼å‘é€‰æ‹©ï¼ˆGOSï¼‰**ï¼šåˆ©ç”¨LLMçš„åˆå§‹ç›´è§‰ï¼Œå¼•å¯¼WebPilotæœç€å­ä»»åŠ¡å®Œæˆçš„æœ€ä½³è·¯å¾„å‰è¿›ã€‚
*   **åæ€å¢å¼ºèŠ‚ç‚¹æ‰©å±•ï¼ˆRENEï¼‰**ï¼šåœ¨æ¯æ¬¡èŠ‚ç‚¹æ‰©å±•åé›†æˆåæ€åé¦ˆï¼Œä½¿WebPilotèƒ½å¤ŸåŠ¨æ€åœ°é‡æ–°è¯„ä¼°å’Œä¼˜åŒ–å…¶ç­–ç•¥ã€‚
*   **åŠ¨æ€è¯„ä¼°å’Œæ¨¡æ‹Ÿï¼ˆDESï¼‰**ï¼šé€šè¿‡åˆ†ææ‰§è¡Œçš„åŠ¨ä½œå’Œæ¨¡æ‹Ÿæ½œåœ¨ç»“æœæ¥è¯„ä¼°å½“å‰çŠ¶æ€ï¼Œä»è€Œæ¨¡æ‹Ÿäººç±»çš„è¿œè§ã€‚
*   **æœ€å¤§å€¼åå‘ä¼ æ’­ï¼ˆMVBï¼‰**ï¼šé€šè¿‡æŒç»­æ›´æ–°åŸºäºæœ€å¤§æœªæ¥å¥–åŠ±çš„ä»·å€¼ä¼°è®¡æ¥ä¼˜å…ˆè€ƒè™‘æœ€æœ‰æ½œåŠ›çš„è·¯å¾„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨WebArenaå’ŒMiniWoB++ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒWebPilotåœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨WebArenaä¸Šï¼ŒWebPilotä¸GPT-4é…åˆä½¿ç”¨ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸å¹¶å‘çš„åŸºäºæ ‘çš„æœç´¢æ–¹æ³•ç›¸æ¯”ï¼ŒæˆåŠŸç‡æé«˜äº†93%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebPilotçš„è®¾è®¡ä¸ºå¼€å‘æ›´çµæ´»ã€æ›´è‡ªä¸»çš„ç½‘ç»œä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚å…¶åŒé‡ä¼˜åŒ–ç­–ç•¥å’ŒMCTSå¢å¼ºçš„å†³ç­–ç­–ç•¥å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦åŠ¨æ€äº¤äº’å’Œå¤æ‚å†³ç­–çš„ç½‘ç»œä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒWebPilotçš„å±‚æ¬¡åæ€æœºåˆ¶å’Œç»†ç²’åº¦åŒé¢è‡ªæˆ‘å¥–åŠ±æœºåˆ¶ä¸ºæé«˜ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚

## mmina--benchmarking-multihop-multimodal-internet-agents
### Abstract
Autonomous embodied agents live on an Internet of multimedia websites. Can
they hop around multimodal websites to complete complex user tasks? Existing
benchmarks fail to assess them in a realistic, evolving environment for their
embodiment across websites. To answer this question, we present MMInA, a
multihop and multimodal benchmark to evaluate the embodied agents for
compositional Internet tasks, with several appealing properties: 1) Evolving
real-world multimodal websites. Our benchmark uniquely operates on evolving
real-world websites, ensuring a high degree of realism and applicability to
natural user tasks. Our data includes 1,050 human-written tasks covering
various domains such as shopping and travel, with each task requiring the agent
to autonomously extract multimodal information from web pages as observations;
2) Multihop web browsing. Our dataset features naturally compositional tasks
that require information from or actions on multiple websites to solve, to
assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.
We propose a novel protocol for evaluating an agent's progress in completing
multihop tasks. We experiment with both standalone (multimodal) language models
and heuristic-based web agents. Extensive experiments demonstrate that while
long-chain multihop web tasks are easy for humans, they remain challenging for
state-of-the-art web agents. We identify that agents are more likely to fail on
the early hops when solving tasks of more hops, which results in lower task
success rates. To address this issue, we propose a simple memory augmentation
approach replaying past action trajectories to reflect. Our method
significantly improved both the single-hop and multihop web browsing abilities
of agents. See our code and data at https://mmina.cliangyu.com
### ğŸŒŸ è®ºæ–‡è§£è¯» | MMInAï¼šè¯„ä¼°å¤šè·³å¤šæ¨¡æ€äº’è”ç½‘ä»£ç†çš„åŸºå‡†

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œè‡ªä¸»ä½“ä»£ç†åœ¨äº’è”ç½‘ä¸Šçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æ— æ³•åœ¨çœŸå®ã€åŠ¨æ€çš„ç¯å¢ƒä¸­è¯„ä¼°è¿™äº›ä»£ç†åœ¨è·¨ç½‘ç«™ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MMInAï¼Œä¸€ä¸ªå¤šè·³å’Œå¤šæ¨¡æ€çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ä»£ç†åœ¨å®Œæˆç»„åˆå¼äº’è”ç½‘ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçœŸå®ä¸–ç•Œå¤šæ¨¡æ€ç½‘ç«™
MMInAåŸºå‡†æµ‹è¯•åœ¨çœŸå®ä¸–ç•Œå¤šæ¨¡æ€ç½‘ç«™ä¸Šè¿è¡Œï¼Œç¡®ä¿äº†é«˜åº¦çš„çœŸå®æ€§å’Œé€‚ç”¨æ€§ã€‚æ•°æ®åŒ…æ‹¬1050ä¸ªç”±äººç±»ç¼–å†™çš„ä»»åŠ¡ï¼Œæ¶µç›–è´­ç‰©ã€æ—…è¡Œç­‰å„ä¸ªé¢†åŸŸï¼Œæ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦ä»£ç†è‡ªä¸»åœ°ä»ç½‘é¡µä¸­æå–å¤šæ¨¡æ€ä¿¡æ¯ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šè·³ç½‘é¡µæµè§ˆ
MMInAæ•°æ®é›†å…·æœ‰è‡ªç„¶ç»„åˆçš„ä»»åŠ¡ï¼Œéœ€è¦ä»å¤šä¸ªç½‘ç«™è·å–ä¿¡æ¯æˆ–æ‰§è¡Œæ“ä½œæ‰èƒ½è§£å†³ï¼Œä»¥è¯„ä¼°åœ¨ç½‘é¡µä»»åŠ¡ä¸Šçš„é•¿è·ç¦»æ¨ç†èƒ½åŠ›ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•´ä½“è¯„ä¼°
æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åè®®ï¼Œç”¨äºè¯„ä¼°ä»£ç†åœ¨å®Œæˆå¤šè·³ä»»åŠ¡æ–¹é¢çš„è¿›å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶é•¿é“¾å¤šè·³ç½‘é¡µä»»åŠ¡å¯¹äººç±»æ¥è¯´å¾ˆå®¹æ˜“ï¼Œä½†å¯¹äºæœ€å…ˆè¿›çš„ç½‘é¡µä»£ç†æ¥è¯´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè®°å¿†å¢å¼ºæ–¹æ³•
ä¸ºäº†è§£å†³ä»£ç†åœ¨æ—©æœŸè·³è½¬ä¸­å¤±è´¥çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„è®°å¿†å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡é‡æ”¾è¿‡å»çš„è¡Œä¸ºè½¨è¿¹æ¥åæ˜ ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†ä»£ç†çš„å•è·³å’Œå¤šè·³ç½‘é¡µæµè§ˆèƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†ç®€å•æ–‡æœ¬ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†MMInAä¸­ä»»åŠ¡çš„é›†æˆå’Œé¡ºåºæ€§è´¨ä»ç„¶æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œè¡¨ç°æœ€å¥½çš„ç‹¬ç«‹æ¨¡å‹GPT-4Våœ¨ä»»åŠ¡ä¸­çš„æ•´ä½“æˆåŠŸç‡ä»…ä¸º21.8%ï¼Œè¿™æ¯”æ–‡æœ¬ä»£ç†åŸºçº¿æœ‰äº†æ˜¾è‘—æé«˜ï¼Œä½†ä»è½åäºäººç±»è¡¨ç°ï¼ˆ96.3%ï¼‰ã€‚å®éªŒè¿˜å‘ç°ï¼Œä»£ç†åœ¨è§£å†³æ›´å¤šè·³è½¬çš„ä»»åŠ¡æ—¶æ›´å®¹æ˜“åœ¨æ—©æœŸè·³è½¬ä¸­å¤±è´¥ï¼Œå¯¼è‡´ä»»åŠ¡æˆåŠŸç‡é™ä½ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MMInAåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å¤šè·³å’Œå¤šæ¨¡æ€äº’è”ç½‘ä»£ç†æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å®ƒå¼ºè°ƒäº†åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¯„ä¼°ä»£ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„è®°å¿†å¢å¼ºæ–¹æ³•ä¸ºæé«˜ä»£ç†æ€§èƒ½æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºæ›´å¤šçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚

## an-in-depth-survey-of-large-language-model-based-artificial-intelligence-agents
### Abstract
Due to the powerful capabilities demonstrated by large language model (LLM),
there has been a recent surge in efforts to integrate them with AI agents to
enhance their performance. In this paper, we have explored the core differences
and characteristics between LLM-based AI agents and traditional AI agents.
Specifically, we first compare the fundamental characteristics of these two
types of agents, clarifying the significant advantages of LLM-based agents in
handling natural language, knowledge storage, and reasoning capabilities.
Subsequently, we conducted an in-depth analysis of the key components of AI
agents, including planning, memory, and tool use. Particularly, for the crucial
component of memory, this paper introduced an innovative classification scheme,
not only departing from traditional classification methods but also providing a
fresh perspective on the design of an AI agent's memory system. We firmly
believe that in-depth research and understanding of these core components will
lay a solid foundation for the future advancement of AI agent technology. At
the end of the paper, we provide directional suggestions for further research
in this field, with the hope of offering valuable insights to scholars and
researchers in the field.
### ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„AIæ™ºèƒ½ä½“ï¼šæ¢ç´¢ä¸å±•æœ›

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€çŸ¥è¯†å­˜å‚¨å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„å¼ºå¤§è¡¨ç°ï¼Œå°†LLMä¸AIæ™ºèƒ½ä½“ç›¸ç»“åˆä»¥æé«˜å…¶æ€§èƒ½çš„ç ”ç©¶æ—¥ç›Šå¢å¤šã€‚ç„¶è€Œï¼ŒLLM-based AIæ™ºèƒ½ä½“ä¸ä¼ ç»ŸAIæ™ºèƒ½ä½“ä¹‹é—´å­˜åœ¨ç€æ ¸å¿ƒå·®å¼‚å’Œç‰¹ç‚¹ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶å’Œç†è§£ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLM-based AIæ™ºèƒ½ä½“ä¸ä¼ ç»ŸAIæ™ºèƒ½ä½“çš„æ¯”è¾ƒ
æœ¬æ–‡é¦–å…ˆæ¯”è¾ƒäº†è¿™ä¸¤ç§ç±»å‹æ™ºèƒ½ä½“çš„åŸºæœ¬ç‰¹æ€§ï¼Œé˜æ˜äº†LLM-based AIæ™ºèƒ½ä½“åœ¨å¤„ç†è‡ªç„¶è¯­è¨€ã€çŸ¥è¯†å­˜å‚¨å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAIæ™ºèƒ½ä½“å…³é”®ç»„ä»¶çš„æ·±å…¥åˆ†æ
æœ¬æ–‡å¯¹AIæ™ºèƒ½ä½“çš„å…³é”®ç»„ä»¶è¿›è¡Œäº†æ·±å…¥åˆ†æï¼ŒåŒ…æ‹¬è§„åˆ’ã€è®°å¿†å’Œå·¥å…·ä½¿ç”¨ã€‚ç‰¹åˆ«æ˜¯å¯¹äºè®°å¿†è¿™ä¸€å…³é”®ç»„ä»¶ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„åˆ†ç±»æ–¹æ¡ˆï¼Œä¸ä»…ä¸ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•ä¸åŒï¼Œè¿˜ä¸ºAIæ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†æ–°çš„è§†è§’ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAIæ™ºèƒ½ä½“çš„åº”ç”¨åœºæ™¯
æœ¬æ–‡æ¢è®¨äº†LLM-based AIæ™ºèƒ½ä½“çš„åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬èŠå¤©æœºå™¨äººã€æ¸¸æˆã€è®¾è®¡ã€ç ”ç©¶ã€ç¼–ç ã€åä½œå’Œé€šç”¨ç›®çš„ç­‰ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šAIæ™ºèƒ½ä½“çš„è¯„ä¼°åŸºå‡†
æœ¬æ–‡è¿˜ä»‹ç»äº†é’ˆå¯¹LLM-based AIæ™ºèƒ½ä½“è®¾è®¡çš„è¯„ä¼°åŸºå‡†ï¼Œä»¥è¯„ä¼°å…¶æ€§èƒ½å’Œæ•ˆæœã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¹¶æœªæä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯å¯¹LLM-based AIæ™ºèƒ½ä½“çš„ç ”ç©¶ç°çŠ¶è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°å’Œåˆ†æã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡ä¸ºLLM-based AIæ™ºèƒ½ä½“çš„ç ”ç©¶æä¾›äº†å®è´µçš„å‚è€ƒå’Œå¯ç¤ºï¼Œæœ‰åŠ©äºè¯»è€…å¿«é€Ÿäº†è§£è¯¥é¢†åŸŸçš„ç ”ç©¶å†å²å’Œåº”ç”¨ç°çŠ¶ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘æ€§çš„å»ºè®®ã€‚

## language-agent-tree-search-unifies-reasoning-acting-and-planning-in-language-models
### Abstract
While language models (LMs) have shown potential across a range of
decision-making tasks, their reliance on simple acting processes limits their
broad deployment as autonomous agents. In this paper, we introduce Language
Agent Tree Search (LATS) -- the first general framework that synergizes the
capabilities of LMs in reasoning, acting, and planning. By leveraging the
in-context learning ability of LMs, we integrate Monte Carlo Tree Search into
LATS to enable LMs as agents, along with LM-powered value functions and
self-reflections for proficient exploration and enhanced decision-making. A key
feature of our approach is the incorporation of an environment for external
feedback, which offers a more deliberate and adaptive problem-solving mechanism
that surpasses the constraints of existing techniques. Our experimental
evaluation across diverse domains, including programming, interactive
question-answering (QA), web navigation, and math, validates the effectiveness
and generality of LATS in decision-making while maintaining competitive or
improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1
accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates
gradient-free performance (average score of 75.9) comparable to gradient-based
fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at
https://github.com/lapisrocks/LanguageAgentTreeSearch
### ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹å†³ç­–æ ‘æœç´¢ï¼šç»Ÿä¸€æ¨ç†ã€è¡ŒåŠ¨å’Œè§„åˆ’

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨å†³ç­–ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶ç®€å•çš„è¡ŒåŠ¨è¿‡ç¨‹é™åˆ¶äº†å…¶ä½œä¸ºè‡ªä¸»ä»£ç†çš„å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†è¯­è¨€æ¨¡å‹å†³ç­–æ ‘æœç´¢ï¼ˆLATSï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†LMsåœ¨æ¨ç†ã€è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ç›¸ç»“åˆçš„é€šç”¨æ¡†æ¶ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLATSåˆ©ç”¨LMsçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰é›†æˆåˆ°LATSä¸­ï¼Œä½¿LMsèƒ½å¤Ÿä½œä¸ºä»£ç†ï¼Œå¹¶ä½¿ç”¨LMé©±åŠ¨çš„ä»·å€¼å‡½æ•°å’Œè‡ªæˆ‘åæ€æ¥è¿›è¡Œç†Ÿç»ƒçš„æ¢ç´¢å’Œå¢å¼ºçš„å†³ç­–ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLATSçš„å…³é”®ç‰¹å¾æ˜¯é›†æˆäº†å¤–éƒ¨åé¦ˆçš„ç¯å¢ƒï¼Œè¿™æä¾›äº†ä¸€ä¸ªæ›´æ·±æ€ç†Ÿè™‘å’Œé€‚åº”æ€§æ›´å¼ºçš„è§£å†³é—®é¢˜çš„æœºåˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯çš„é™åˆ¶ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨ç¼–ç¨‹ã€äº¤äº’å¼é—®ç­”ï¼ˆQAï¼‰ã€ç½‘ç»œå¯¼èˆªå’Œæ•°å­¦ç­‰ä¸åŒé¢†åŸŸçš„å®éªŒè¯„ä¼°ä¸­ï¼ŒéªŒè¯äº†LATSåœ¨å†³ç­–ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰ç«äº‰åŠ›çš„æˆ–æ”¹è¿›çš„æ¨ç†æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLATSåœ¨HumanEvalä¸Šä½¿ç”¨GPT-4å®ç°äº†æœ€å…ˆè¿›çš„pass@1å‡†ç¡®ç‡ï¼ˆ92.7%ï¼‰ï¼Œå¹¶åœ¨WebShopä¸Šä½¿ç”¨GPT-3.5å®ç°äº†ä¸åŸºäºæ¢¯åº¦çš„å¾®è°ƒç›¸å½“çš„æ¢¯åº¦æ— å…³æ€§èƒ½ï¼ˆå¹³å‡åˆ†æ•°ä¸º75.9%ï¼‰ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
LATSæ¡†æ¶ä¸ºLMsåœ¨å†³ç­–å’Œæ¨ç†æ–¹é¢çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶ç»“åˆäº†MCTSã€å¤–éƒ¨åé¦ˆå’Œè‡ªæˆ‘åæ€ï¼Œä¸ºLMsä½œä¸ºé€šç”¨ä»£ç†çš„åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## webarena--a-realistic-web-environment-for-building-autonomous-agents
### Abstract
With advances in generative AI, there is now potential for autonomous agents
to manage daily tasks via natural language commands. However, current agents
are primarily created and tested in simplified synthetic environments, leading
to a disconnect with real-world scenarios. In this paper, we build an
environment for language-guided agents that is highly realistic and
reproducible. Specifically, we focus on agents that perform tasks on the web,
and create an environment with fully functional websites from four common
domains: e-commerce, social forum discussions, collaborative software
development, and content management. Our environment is enriched with tools
(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage
human-like task-solving. Building upon our environment, we release a set of
benchmark tasks focusing on evaluating the functional correctness of task
completions. The tasks in our benchmark are diverse, long-horizon, and designed
to emulate tasks that humans routinely perform on the internet. We experiment
with several baseline agents, integrating recent techniques such as reasoning
before acting. The results demonstrate that solving complex tasks is
challenging: our best GPT-4-based agent only achieves an end-to-end task
success rate of 14.41%, significantly lower than the human performance of
78.24%. These results highlight the need for further development of robust
agents, that current state-of-the-art large language models are far from
perfect performance in these real-life tasks, and that WebArena can be used to
measure such progress.
### ğŸŒŸ è®ºæ–‡è§£è¯» | WebArenaï¼šæ„å»ºè‡ªä¸»ä»£ç†çš„çœŸå®ç½‘ç»œç¯å¢ƒ

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€ç”Ÿæˆå¼AIçš„è¿›æ­¥ï¼Œè‡ªä¸»ä»£ç†ç°åœ¨å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤æ¥ç®¡ç†æ—¥å¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä»£ç†ä¸»è¦æ˜¯åœ¨ç®€åŒ–çš„åˆæˆç¯å¢ƒä¸­åˆ›å»ºå’Œæµ‹è¯•çš„ï¼Œè¿™å¯¼è‡´äº†ä¸ç°å®ä¸–ç•Œåœºæ™¯çš„è„±èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WebArenaï¼Œä¸€ä¸ªé«˜åº¦çœŸå®å’Œå¯å¤åˆ¶çš„ç½‘ç»œç¯å¢ƒï¼Œç”¨äºæ„å»ºå’Œæµ‹è¯•è‡ªä¸»ä»£ç†ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜åº¦çœŸå®çš„ç½‘ç»œç¯å¢ƒ
WebArenaåŒ…å«å››ä¸ªå®Œå…¨åŠŸèƒ½çš„ã€è‡ªæ‰˜ç®¡çš„ç½‘ç»œåº”ç”¨ç¨‹åºï¼Œæ¯ä¸ªåº”ç”¨ç¨‹åºä»£è¡¨ä¸€ä¸ªåœ¨äº’è”ç½‘ä¸Šæ™®éå­˜åœ¨çš„ä¸åŒé¢†åŸŸï¼šç”µå­å•†åŠ¡ã€ç¤¾äº¤è®ºå›è®¨è®ºã€åä½œè½¯ä»¶å¼€å‘å’Œå†…å®¹ç®¡ç†ã€‚æ­¤å¤–ï¼ŒWebArenaè¿˜é›†æˆäº†å¤šç§å®ç”¨å·¥å…·ï¼ˆä¾‹å¦‚åœ°å›¾ï¼‰å’Œå¤–éƒ¨çŸ¥è¯†åº“ï¼ˆä¾‹å¦‚ç”¨æˆ·æ‰‹å†Œï¼‰ï¼Œä»¥é¼“åŠ±ç±»ä¼¼äººç±»çš„ä»»åŠ¡è§£å†³ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·åŒ–çš„åŸºå‡†ä»»åŠ¡
åŸºäºWebArenaç¯å¢ƒï¼Œæœ¬æ–‡å‘å¸ƒäº†ä¸€ç³»åˆ—åŸºå‡†ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨è¯„ä¼°ä»»åŠ¡å®Œæˆçš„å‡½æ•°æ­£ç¡®æ€§ã€‚è¿™äº›ä»»åŠ¡å…·æœ‰å¤šæ ·æ€§ã€é•¿æ—¶ç¨‹ï¼Œå¹¶æ—¨åœ¨æ¨¡æ‹Ÿäººç±»åœ¨äº’è”ç½‘ä¸Šå¸¸è§„æ‰§è¡Œçš„ä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºç»“æœçš„è¯„ä¼°
æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºç»“æœçš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡ç¨‹åºåŒ–éªŒè¯æ¯ä¸ªä»»åŠ¡çš„æˆåŠŸæ¥è¯„ä¼°ä»»åŠ¡çš„æˆåŠŸã€‚è¿™ç§æ–¹æ³•æ¯”æ¯”è¾ƒé¢„æµ‹çš„åŠ¨ä½œåºåˆ—ä¸å‚è€ƒåŠ¨ä½œåºåˆ—çš„æ–‡æœ¬è¡¨é¢å½¢å¼æ›´å¯é ï¼Œå¹¶ä¸”å¯ä»¥å®¹çº³å®ç°ç›¸åŒç›®æ ‡çš„æ½œåœ¨æœ‰æ•ˆè·¯å¾„ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡ä½¿ç”¨WebArenaåŸºå‡†æµ‹è¯•äº†å‡ ä¸ªåŸºçº¿ä»£ç†ï¼Œè¿™äº›ä»£ç†é›†æˆäº†è¯¸å¦‚æ¨ç†å‰è¡ŒåŠ¨ç­‰æœ€æ–°æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§£å†³å¤æ‚ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼šæœ€å¥½çš„åŸºäºGPT-4çš„ä»£ç†çš„ç«¯åˆ°ç«¯ä»»åŠ¡æˆåŠŸç‡ä»…ä¸º14.41%ï¼Œè¿œä½äºäººç±»çš„78.24%ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
WebArenaä¸ºæ„å»ºå’Œæµ‹è¯•è‡ªä¸»ä»£ç†æä¾›äº†ä¸€ä¸ªé«˜åº¦çœŸå®å’Œå¯å¤åˆ¶çš„ç½‘ç»œç¯å¢ƒã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å¥å£®å’Œæœ‰æ•ˆçš„ä»£ç†ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWebArenaè¿˜å¯ä»¥ç”¨äºæµ‹è¯•å’Œæ”¹è¿›ç°æœ‰çš„äº¤äº’å¼å†³ç­–ä»£ç†æ–¹æ³•ï¼Œä¾‹å¦‚åˆ†å±‚è§„åˆ’ã€çŠ¶æ€è·Ÿè¸ªå’Œé”™è¯¯æ¢å¤ã€‚

## moba--multifaceted-memory-enhanced-adaptive-planning-for-efficient-mobile-task-automation
### Abstract
Existing Multimodal Large Language Model (MLLM)-based agents face significant
challenges in handling complex GUI (Graphical User Interface) interactions on
devices. These challenges arise from the dynamic and structured nature of GUI
environments, which integrate text, images, and spatial relationships, as well
as the variability in action spaces across different pages and tasks. To
address these limitations, we propose MobA, a novel MLLM-based mobile assistant
system. MobA introduces an adaptive planning module that incorporates a
reflection mechanism for error recovery and dynamically adjusts plans to align
with the real environment contexts and action module's execution capacity.
Additionally, a multifaceted memory module provides comprehensive memory
support to enhance adaptability and efficiency. We also present MobBench, a
dataset designed for complex mobile interactions. Experimental results on
MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI
environments and perform complex mobile task.
### ğŸŒŸ è®ºæ–‡è§£è¯» | MobAï¼šå¤šé¢è®°å¿†å¢å¼ºè‡ªé€‚åº”è§„åˆ’ï¼ŒåŠ©åŠ›é«˜æ•ˆç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨å¤„ç†å¤æ‚GUIäº¤äº’å’Œæ»¡è¶³å¤šæ ·åŒ–ç”¨æˆ·éœ€æ±‚æ–¹é¢é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ã€‚GUIç¯å¢ƒå…·æœ‰åŠ¨æ€æ€§å’Œç»“æ„æ€§ï¼Œèåˆäº†æ–‡æœ¬ã€å›¾åƒå’Œç©ºé—´å…³ç³»ï¼Œä¸”ä¸åŒé¡µé¢å’Œä»»åŠ¡çš„åŠ¨ä½œç©ºé—´å­˜åœ¨å·®å¼‚ã€‚ç°æœ‰çš„MLLM-basedæ™ºèƒ½ä½“åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶ï¼Œå¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªé€‚åº”è§„åˆ’æ¨¡å—
MOBAå¼•å…¥äº†è‡ªé€‚åº”è§„åˆ’æ¨¡å—ï¼Œè¯¥æ¨¡å—åŒ…å«ä¸€ä¸ªåæ€æœºåˆ¶ï¼Œç”¨äºä»å¤±è´¥çš„å­è®¡åˆ’ä¸­æ¢å¤ä»»åŠ¡æ‰§è¡Œï¼Œå¹¶é€šè¿‡é‡æ–°è¯„ä¼°ç›®æ ‡æˆ–å°†ä»»åŠ¡åˆ†è§£ä¸ºæ›´ç»†ç²’åº¦çš„å­ç›®æ ‡æ¥åŠ¨æ€è°ƒæ•´è®¡åˆ’ã€‚è¿™ç§æ¨¡å—èƒ½å¤Ÿæ ¹æ®å½“å‰çš„GUIç¯å¢ƒå’ŒåŠ¨ä½œæ‰§è¡Œå™¨çš„å®¹é‡ï¼Œç”Ÿæˆå¤šç²’åº¦çš„ä»»åŠ¡è®¡åˆ’ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç¯å¢ƒå¹¶å®Œæˆä»»åŠ¡ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé¢è®°å¿†æ¨¡å—
MOBAè¿˜æå‡ºäº†ä¸€ä¸ªå¤šé¢è®°å¿†æ¨¡å—ï¼Œè¯¥æ¨¡å—æä¾›åˆ†å±‚è®°å¿†æ”¯æŒï¼Œä»¥å¢å¼ºä»»åŠ¡çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¨¡å—åŒ…æ‹¬ä»»åŠ¡è®°å¿†ã€åº”ç”¨è®°å¿†ã€é¡µé¢è®°å¿†ã€åŠ¨ä½œè®°å¿†å’Œç”¨æˆ·è®°å¿†ï¼Œèƒ½å¤Ÿå­˜å‚¨å†å²æ•°æ®ï¼Œå¢å¼ºå†³ç­–èƒ½åŠ›ï¼Œå¹¶å‡å°‘å†—ä½™åŠ¨ä½œã€‚

### ğŸ“ˆ å®éªŒç»“æœ
MOBAåœ¨MOBBENCHå’ŒAndroidArenaæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼ŒMOBAåœ¨å¤„ç†åŠ¨æ€GUIç¯å¢ƒå’Œæ‰§è¡Œå¤æ‚ç§»åŠ¨ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMOBAåœ¨ä»»åŠ¡å®Œæˆç‡ã€é‡Œç¨‹ç¢‘å¾—åˆ†å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
MOBAçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¸ºç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶è‡ªé€‚åº”è§„åˆ’å’Œå¤šé¢è®°å¿†æ¨¡å—çš„è®¾è®¡ï¼Œä¸ºè§£å†³å¤æ‚GUIäº¤äº’å’Œå¤šæ ·åŒ–ç”¨æˆ·éœ€æ±‚æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼ŒMOBAçš„å®éªŒç»“æœä¹Ÿè¡¨æ˜ï¼ŒMLLMåœ¨ç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚

## vga--vision-gui-assistant----minimizing-hallucinations-through-image-centric-fine-tuning
### Abstract
Recent advances in Large Vision-Language Models (LVLMs) have significantly
improve performance in image comprehension tasks, such as formatted charts and
rich-content images. Yet, Graphical User Interface (GUI) pose a greater
challenge due to their structured format and detailed textual information.
Existing LVLMs often overly depend on internal knowledge and neglect image
content, resulting in hallucinations and incorrect responses in GUI
comprehension. To address these issues, we introduce VGA, a fine-tuned model
designed for comprehensive GUI understanding. Our model aims to enhance the
interpretation of visual data of GUI and reduce hallucinations. We first
construct a Vision Question Answering (VQA) dataset of 63.8k high-quality
examples with our propose Referent Method, which ensures the model's responses
are highly depend on visual content within the image. We then design a
two-stage fine-tuning method called Foundation and Advanced Comprehension (FAC)
to enhance both the model's ability to extract information from image content
and alignment with human intent. Experiments show that our approach enhances
the model's ability to extract information from images and achieves
state-of-the-art results in GUI understanding tasks. Our dataset and
fine-tuning script will be released soon.
### ğŸŒŸ è®ºæ–‡è§£è¯» | VGAï¼šåŸºäºå›¾åƒçš„å¾®è°ƒï¼Œå‡å°‘è§†è§‰ç•Œé¢ç†è§£ä¸­çš„å¹»è§‰

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å›¾åƒç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ç†è§£å´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„LVLMså¾€å¾€è¿‡åº¦ä¾èµ–å†…éƒ¨çŸ¥è¯†ï¼Œè€Œå¿½è§†å›¾åƒå†…å®¹ï¼Œå¯¼è‡´åœ¨GUIç†è§£ä»»åŠ¡ä¸­äº§ç”Ÿå¹»è§‰å’Œé”™è¯¯å“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VGAï¼Œä¸€ä¸ªä¸“é—¨ä¸ºGUIç†è§£è€Œè®¾è®¡çš„å¾®è°ƒæ¨¡å‹ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡GUIæ•°æ®é›†
ä¸ºäº†å¾®è°ƒæ¨¡å‹ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«63.8ké«˜è´¨é‡ç¤ºä¾‹çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§ç§°ä¸ºâ€œå‚ç…§æ–¹æ³•â€çš„æ•°æ®æ„å»ºæ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹çš„å“åº”é«˜åº¦ä¾èµ–äºå›¾åƒä¸­çš„è§†è§‰å†…å®¹ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•
æœ¬æ–‡è®¾è®¡äº†ä¸€ç§åä¸ºâ€œåŸºç¡€å’Œé«˜çº§ç†è§£â€ï¼ˆFACï¼‰çš„ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼Œä»¥å¢å¼ºæ¨¡å‹ä»å›¾åƒå†…å®¹ä¸­æå–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ä½¿å…¶ä¸äººç±»æ„å›¾ä¿æŒä¸€è‡´ã€‚åŸºç¡€é˜¶æ®µå¢å¼ºæ¨¡å‹å¯¹GUIå›¾åƒçš„ç†è§£ï¼Œè€Œé«˜çº§é˜¶æ®µåˆ™æé«˜æ¨¡å‹æ ¹æ®å¯¹GUIçš„ç†è§£æ¥å›ç­”å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹ä»å›¾åƒä¸­æå–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶åœ¨GUIç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒVGAåœ¨GUIç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥æ—¶ä»ç„¶è¡¨ç°å‡ºè‰²ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„VGAæ¨¡å‹åŠå…¶å¾®è°ƒæ–¹æ³•ä¸ºLVLMsåœ¨GUIç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡GUIæ•°æ®é›†å’Œè®¾è®¡ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘LVLMsåœ¨GUIç†è§£ä¸­çš„å¹»è§‰ç°è±¡ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚

## appagentx--evolving-gui-agents-as-proficient-smartphone-users
### Abstract
Recent advancements in Large Language Models (LLMs) have led to the
development of intelligent LLM-based agents capable of interacting with
graphical user interfaces (GUIs). These agents demonstrate strong reasoning and
adaptability, enabling them to perform complex tasks that traditionally
required predefined rules. However, the reliance on step-by-step reasoning in
LLM-based agents often results in inefficiencies, particularly for routine
tasks. In contrast, traditional rule-based systems excel in efficiency but lack
the intelligence and flexibility to adapt to novel scenarios. To address this
challenge, we propose a novel evolutionary framework for GUI agents that
enhances operational efficiency while retaining intelligence and flexibility.
Our approach incorporates a memory mechanism that records the agent's task
execution history. By analyzing this history, the agent identifies repetitive
action sequences and evolves high-level actions that act as shortcuts,
replacing these low-level operations and improving efficiency. This allows the
agent to focus on tasks requiring more complex reasoning, while simplifying
routine actions. Experimental results on multiple benchmark tasks demonstrate
that our approach significantly outperforms existing methods in both efficiency
and accuracy. The code will be open-sourced to support further research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | AppAgentXï¼šè®©GUIæ™ºèƒ½ä½“æˆä¸ºç†Ÿç»ƒçš„æ‰‹æœºç”¨æˆ·

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥å‚¬ç”Ÿäº†èƒ½å¤Ÿä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIsï¼‰äº¤äº’çš„æ™ºèƒ½LLMæ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ‰§è¡Œä¼ ç»Ÿä¸Šéœ€è¦é¢„å®šä¹‰è§„åˆ™æ‰èƒ½å®Œæˆçš„å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒLLMæ™ºèƒ½ä½“ä¾èµ–äºé€æ­¥æ¨ç†ï¼Œè¿™å¾€å¾€å¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨ä¾‹è¡Œä»»åŠ¡ä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„ç³»ç»Ÿåœ¨æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æ™ºèƒ½å’Œçµæ´»æ€§ï¼Œæ— æ³•é€‚åº”æ–°æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºGUIæ™ºèƒ½ä½“ï¼Œä»¥æé«˜æ“ä½œæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ™ºèƒ½å’Œçµæ´»æ€§ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®°å¿†æœºåˆ¶
æœ¬æ–‡æå‡ºäº†ä¸€ç§è®°å¿†æœºåˆ¶ï¼Œç”¨äºè®°å½•æ™ºèƒ½ä½“çš„ä»»åŠ¡æ‰§è¡Œå†å²ã€‚é€šè¿‡åˆ†æå†å²è®°å½•ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè¯†åˆ«é‡å¤çš„åŠ¨ä½œåºåˆ—ï¼Œå¹¶è¿›åŒ–å‡ºé«˜çº§åŠ¨ä½œï¼Œä½œä¸ºå¿«æ·æ–¹å¼ï¼Œæ›¿æ¢ä½çº§æ“ä½œï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚è¿™å…è®¸æ™ºèƒ½ä½“ä¸“æ³¨äºéœ€è¦æ›´å¤æ‚æ¨ç†çš„ä»»åŠ¡ï¼ŒåŒæ—¶ç®€åŒ–ä¾‹è¡Œæ“ä½œã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¿›åŒ–æœºåˆ¶
æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåŸºäºé“¾çš„çŸ¥è¯†æ¡†æ¶ï¼Œç”¨äºè®°å½•å’Œä¼˜åŒ–æ™ºèƒ½ä½“çš„æ‰§è¡Œè¡Œä¸ºã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“ä»å…¶è¿‡å»çš„äº¤äº’ä¸­å­¦ä¹ ï¼Œå¹¶åŠ¨æ€åœ°è¿›åŒ–å‡ºæ›´æŠ½è±¡çš„é«˜çº§åŠ¨ä½œï¼Œæ¶ˆé™¤äº†é‡å¤çš„ä½çº§æ“ä½œçš„éœ€æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæ™ºèƒ½ä½“ä¼šåˆ†æå…¶æ‰§è¡Œå†å²ï¼Œä»¥è¯†åˆ«é‡å¤çš„ä½æ™ºèƒ½åŠ¨ä½œï¼Œä¾‹å¦‚ä¾‹è¡Œä»»åŠ¡ä¸­æ¶‰åŠçš„åŠ¨ä½œã€‚ä»è¿™ç§åˆ†æä¸­ï¼Œæ™ºèƒ½ä½“å¯ä»¥ç”Ÿæˆä¸€ä¸ªé«˜çº§åŠ¨ä½œï¼Œå°è£…ä¸€ç³»åˆ—ä½çº§åŠ¨ä½œï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•å°†å¹³å‡æ­¥éª¤æ•°ä»9.1å‡å°‘åˆ°5.7ï¼Œå°†æ­¥éª¤æ‰§è¡Œæ—¶é—´ä»23ç§’å‡å°‘åˆ°16ç§’ï¼Œå¹¶å°†å¹³å‡ä»¤ç‰Œæ¶ˆè€—ä»9.26kå‡å°‘åˆ°4.94kã€‚æ­¤å¤–ï¼Œå¹³å‡æˆåŠŸç‡ä»70.8%æé«˜åˆ°71.4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å¼€é”€ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡æå‡ºçš„è¿›åŒ–æ¡†æ¶ä¸ºGUIæ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡å¼•å…¥è®°å¿†æœºåˆ¶å’Œè¿›åŒ–æœºåˆ¶ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿä»è¿‡å»çš„äº¤äº’ä¸­å­¦ä¹ ï¼Œå¹¶åŠ¨æ€åœ°è¿›åŒ–å‡ºæ›´æŠ½è±¡çš„é«˜çº§åŠ¨ä½œï¼Œä»è€Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŸºäºé“¾çš„çŸ¥è¯†æ¡†æ¶ä¹Ÿä¸ºæ™ºèƒ½ä½“çš„è¡Œä¸ºä¼˜åŒ–æä¾›äº†æ–°çš„æ–¹æ³•ã€‚è¿™äº›åˆ›æ–°ç‚¹å¯¹äºGUIæ™ºèƒ½ä½“çš„å‘å±•å…·æœ‰é‡è¦çš„æ„ä¹‰ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚

## towards-trustworthy-gui-agents--a-survey
### Abstract
GUI agents, powered by large foundation models, can interact with digital
interfaces, enabling various applications in web automation, mobile navigation,
and software testing. However, their increasing autonomy has raised critical
concerns about their security, privacy, and safety. This survey examines the
trustworthiness of GUI agents in five critical dimensions: security
vulnerabilities, reliability in dynamic environments, transparency and
explainability, ethical considerations, and evaluation methodologies. We also
identify major challenges such as vulnerability to adversarial attacks,
cascading failure modes in sequential decision-making, and a lack of realistic
evaluation benchmarks. These issues not only hinder real-world deployment but
also call for comprehensive mitigation strategies beyond task success. As GUI
agents become more widespread, establishing robust safety standards and
responsible development practices is essential. This survey provides a
foundation for advancing trustworthy GUI agents through systematic
understanding and future research.
### ğŸŒŸ è®ºæ–‡è§£è¯» | æ„å»ºå¯ä¿¡çš„GUIæ™ºèƒ½ä½“ï¼šä¸€é¡¹è°ƒæŸ¥

### ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº
éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ™ºèƒ½ä½“å·²ç»ä»ç®€å•çš„é—®ç­”å·¥å…·è½¬å˜ä¸ºèƒ½å¤Ÿä¸æ•°å­—ç•Œé¢äº¤äº’çš„æ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“åœ¨ç½‘é¡µè‡ªåŠ¨åŒ–ã€ç§»åŠ¨å¯¼èˆªå’Œè½¯ä»¶æµ‹è¯•ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œéšç€å®ƒä»¬è‡ªä¸»æ€§çš„æé«˜ï¼Œå…³äºå…¶å®‰å…¨æ€§ã€éšç§å’Œå®‰å…¨çš„æ‹…å¿§ä¹Ÿæ—¥ç›Šå¢åŠ ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦ï¼Œå¹¶ä»äº”ä¸ªå…³é”®ç»´åº¦è¿›è¡Œåˆ†æï¼šå®‰å…¨æ¼æ´ã€åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ã€é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€ä¼¦ç†è€ƒè™‘å’Œè¯„ä¼°æ–¹æ³•ã€‚

### ğŸš€ æ ¸å¿ƒæ–¹æ³•
æœ¬æ–‡çš„æ ¸å¿ƒæ–¹æ³•æ˜¯å¯¹GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦è¿›è¡Œç³»ç»Ÿæ€§çš„è°ƒæŸ¥å’Œåˆ†æã€‚ä½œè€…ä»äº”ä¸ªå…³é”®ç»´åº¦å¯¹GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡é¦–æ¬¡å°†GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦é—®é¢˜è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„è°ƒæŸ¥å’Œåˆ†æï¼Œä»äº”ä¸ªå…³é”®ç»´åº¦è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„GUIæ™ºèƒ½ä½“æä¾›äº†ç†è®ºåŸºç¡€ã€‚

ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡æå‡ºäº†å¤šç§è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æ›´æ™ºèƒ½çš„é˜²å¾¡å·¥å…·ã€ç”¨æˆ·æ§åˆ¶çš„éšç§ã€è¿æ¥çš„é˜²å¾¡å±‚ã€å®æ—¶å¹»è§‰é¢„é˜²ã€è‡ªé€‚åº”å®‰å…¨æ¶æ„ã€ä»å¤±è´¥ä¸­å­¦ä¹ ã€äº¤äº’å¼è§£é‡Šå·¥å…·ã€ä¸Šä¸‹æ–‡è‡ªé€‚åº”è§£é‡Šã€æ–‡åŒ–å’Œç¤¾ä¼šæ„è¯†ã€æ”¿ç­–å½±å“ã€æŒ‡å—å’ŒåŸåˆ™ç­‰ã€‚

### ğŸ“ˆ å®éªŒç»“æœ
æœ¬æ–‡å¹¶æœªè¿›è¡Œå…·ä½“çš„å®éªŒï¼Œè€Œæ˜¯å¯¹ç°æœ‰çš„ç ”ç©¶æˆæœè¿›è¡Œäº†æ€»ç»“å’Œåˆ†æã€‚ä½œè€…é€šè¿‡è°ƒæŸ¥å’Œåˆ†æï¼Œæ­ç¤ºäº†GUIæ™ºèƒ½ä½“åœ¨å¯ä¿¡åº¦æ–¹é¢å­˜åœ¨çš„æŒ‘æˆ˜å’Œé—®é¢˜ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

### ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„
æœ¬æ–‡çš„ç ”ç©¶æˆæœå¯¹äºæ„å»ºå¯ä¿¡çš„GUIæ™ºèƒ½ä½“å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥å€Ÿé‰´æœ¬æ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œè®¾è®¡æ›´å®‰å…¨ã€å¯é ã€é€æ˜å’Œç¬¦åˆä¼¦ç†çš„GUIæ™ºèƒ½ä½“ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿä¸ºGUIæ™ºèƒ½ä½“çš„è¯„ä¼°å’Œæµ‹è¯•æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨åŠ¨GUIæ™ºèƒ½ä½“é¢†åŸŸçš„å‘å±•ã€‚

