
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Agents: An Open-source Framework for Autonomous Language Agents</td>
      <td>Recent advances on large language models (LLMs) enable researchers and<br>developers to build autonomous language agents that can automatically solve<br>various tasks and interact with environments, humans, and other agents using<br>natural language interfaces. We consider language agents as a promising<br>direction towards artificial general intelligence and release Agents, an<br>open-source library with the goal of opening up these advances to a wider<br>non-specialist audience. Agents is carefully engineered to support important<br>features including planning, memory, tool usage, multi-agent communication, and<br>fine-grained symbolic control. Agents is user-friendly as it enables<br>non-specialists to build, customize, test, tune, and deploy state-of-the-art<br>autonomous language agents without much coding. The library is also<br>research-friendly as its modularized design makes it easily extensible for<br>researchers. Agents is available at https://github.com/aiwaves-cn/agents.</td>
      <td>## 🌟 论文解读 | 开源框架 Agents：构建自主语言代理的利器<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）的进步使得研究人员和开发者能够构建自主语言代理，这些代理能够自动解决各种任务，并通过自然语言界面与环境、人类和其他代理进行交互。然而，现有的语言代理框架往往缺乏易用性和可扩展性，难以满足非专业人士的需求。为了解决这个问题，本文提出了 Agents，一个开源的自主语言代理框架，旨在让更广泛的非专业人士能够轻松构建、定制、测试、调整和部署最先进的自主语言代理。<br><br>## 🚀 核心方法<br>💡 创新点1：支持关键功能<br>Agents 框架精心设计，支持规划、记忆、工具使用、多代理通信和细粒度符号控制等关键功能。这使得语言代理能够更好地适应各种任务和环境。<br><br>💡 创新点2：易用性和可扩展性<br>Agents 框架的用户友好性体现在其允许非专业人士轻松构建、定制、测试、调整和部署自主语言代理，而无需大量编码。同时，其模块化设计使得研究人员可以轻松扩展框架，以满足他们的研究需求。<br><br>💡 创新点3：Agent Hub 平台<br>Agents 框架引入了 Agent Hub 平台，允许用户分享他们微调的语言代理，并搜索/下载其他用户分享的有用语言代理。这大大降低了从头开始设计和调整语言代理的难度。<br><br>💡 创新点4：自动创建代理系统<br>为了减少用户手动指定 SOP 的繁琐工作，Agents 框架实现了一个自动 SOP 生成流程。该流程基于检索增强生成 (RAG)，可以自动创建其他代理和多代理系统。<br><br>## 📈 实验结果<br>论文展示了使用 Agents 框架构建的单代理系统和多代理系统的案例研究，包括闲聊机器人、基于知识库和搜索引擎的客户服务代理、购物助手代理和销售代理等。这些案例研究展示了 Agents 框架的易用性和可扩展性，以及构建各种用例的语言代理的可能性。<br><br>## 💬 可借鉴之处<br>Agents 框架为构建自主语言代理提供了一个强大的工具，其易用性和可扩展性使其成为研究人员和开发者的理想选择。此外，Agent Hub 平台和自动 SOP 生成流程进一步提高了框架的实用性和效率。</td>
    </tr>
    <tr>
      <th>1</th>
      <td>DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents</td>
      <td>On-device control agents, especially on mobile devices, are responsible for<br>operating mobile devices to fulfill users' requests, enabling seamless and<br>intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)<br>into these agents enhances their ability to understand and execute complex<br>commands, thereby improving user experience. However, fine-tuning MLLMs for<br>on-device control presents significant challenges due to limited data<br>availability and inefficient online training processes. This paper introduces<br>DistRL, a novel framework designed to enhance the efficiency of online RL<br>fine-tuning for mobile device control agents. DistRL employs centralized<br>training and decentralized data acquisition to ensure efficient fine-tuning in<br>the context of dynamic online interactions. Additionally, the framework is<br>backed by our tailor-made RL algorithm, which effectively balances exploration<br>with the prioritized utilization of collected data to ensure stable and robust<br>training. Our experiments show that, on average, DistRL delivers a 3X<br>improvement in training efficiency and enables training data collection 2.4X<br>faster than the leading synchronous multi-machine methods. Notably, after<br>training, DistRL achieves a 20% relative improvement in success rate compared<br>to state-of-the-art methods on general Android tasks from an open benchmark,<br>significantly outperforming existing approaches while maintaining the same<br>training time. These results validate DistRL as a scalable and efficient<br>solution, offering substantial improvements in both training efficiency and<br>agent performance for real-world, in-the-wild device control tasks.</td>
      <td>## 🌟 论文解读 | DistRL：提升移动设备控制代理在线强化学习效率的分布式框架<br><br>## 📌 背景痛点/本文动机<br>随着多模态大型语言模型（MLLMs）的兴起，将它们集成到移动设备控制代理中，可以显著提升代理理解和执行复杂命令的能力，从而改善用户体验。然而，在移动设备上进行MLLMs的微调面临着数据可用性有限和在线训练过程效率低下等挑战。为了解决这些问题，本文提出了DistRL，一个旨在提高移动设备控制代理在线强化学习（RL）微调效率的分布式框架。<br><br>## 🚀 核心方法<br>💡 创新点1：可扩展的异步数据采集架构<br>DistRL采用解耦和异步的框架，将RL代理部署在异构的worker设备上进行远程数据采集，从而提高训练效率和可扩展性。<br><br>💡 创新点2：先进的集中式训练RL算法<br>DistRL开发了A-RIDE算法，这是一种针对分布式和异步数据利用的离线策略强化学习算法，它优先考虑重要的经验，以提高样本效率，同时鼓励探索。<br><br>## 📈 实验结果<br>实验结果表明，DistRL在训练效率方面平均提高了3倍，并且比领先的同步多机方法快2.4倍。在训练后，DistRL在开放基准测试中的一般Android任务上的成功率比最先进的方法提高了20%，在保持相同训练时间的同时显著优于现有方法。<br><br>## 💬 可借鉴之处<br>DistRL框架为移动设备控制代理的在线强化学习微调提供了一个高效且可扩展的解决方案。其异步数据采集和分布式训练的设计，以及A-RIDE算法的应用，为解决移动设备上RL微调的挑战提供了新的思路。此外，DistRL框架的通用性和可扩展性使其适用于各种移动设备控制任务，具有广泛的应用前景。</td>
    </tr>
    <tr>
      <th>2</th>
      <td>WebGPT: Browser-assisted question-answering with human feedback</td>
      <td>We fine-tune GPT-3 to answer long-form questions using a text-based<br>web-browsing environment, which allows the model to search and navigate the<br>web. By setting up the task so that it can be performed by humans, we are able<br>to train models on the task using imitation learning, and then optimize answer<br>quality with human feedback. To make human evaluation of factual accuracy<br>easier, models must collect references while browsing in support of their<br>answers. We train and evaluate our models on ELI5, a dataset of questions asked<br>by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior<br>cloning, and then performing rejection sampling against a reward model trained<br>to predict human preferences. This model's answers are preferred by humans 56%<br>of the time to those of our human demonstrators, and 69% of the time to the<br>highest-voted answer from Reddit.</td>
      <td>## 🌟 论文解读 | WebGPT：浏览器辅助的问题回答与人类反馈<br><br>## 📌 背景痛点/本文动机<br>随着自然语言处理（NLP）的发展，长篇问题回答（LFQA）成为了一个挑战。这类系统有潜力成为人们了解世界的主要方式，但目前其性能仍落后于人类。本文旨在通过结合信息检索和文本生成的方法，提升长篇问题回答的性能。<br><br>## 🚀 核心方法<br>💡 创新点1<br>本文创建了一个基于文本的浏览器环境，使得经过微调的语言模型能够与之互动。这允许我们使用模仿学习和强化学习等通用方法，以端到端的方式改进检索和生成。<br><br>💡 创新点2<br>模型在浏览过程中生成带有引用的回答，即从网页中提取的文本片段。这为评估答案的事实准确性提供了便利，无需评估者进行独立而困难的研究。<br><br>## 📈 实验结果<br>本文在ELI5数据集上训练和评估模型。最佳模型通过模仿学习和基于人类偏好的奖励模型进行拒绝抽样得到。在三种评估方式中，该模型的表现均优于基线模型，且在事实性和信息性方面有所提升。<br><br>## 💬 可借鉴之处<br>本文的方法表明，通过结合强大的搜索工具和模仿学习，可以显著提升长篇问题回答的性能。同时，引入人类反馈进行优化，有助于模型生成更准确、更有用的答案。此外，带引用的回答方式为评估答案的事实准确性提供了新的思路。</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration</td>
      <td>Reinforcement learning (RL) agents improve through trial-and-error, but when<br>reward is sparse and the agent cannot discover successful action sequences,<br>learning stagnates. This has been a notable problem in training deep RL agents<br>to perform web-based tasks, such as booking flights or replying to emails,<br>where a single mistake can ruin the entire sequence of actions. A common remedy<br>is to "warm-start" the agent by pre-training it to mimic expert demonstrations,<br>but this is prone to overfitting. Instead, we propose to constrain exploration<br>using demonstrations. From each demonstration, we induce high-level "workflows"<br>which constrain the allowable actions at each time step to be similar to those<br>in the demonstration (e.g., "Step 1: click on a textbox; Step 2: enter some<br>text"). Our exploration policy then learns to identify successful workflows and<br>samples actions that satisfy these workflows. Workflows prune out bad<br>exploration directions and accelerate the agent's ability to discover rewards.<br>We use our approach to train a novel neural policy designed to handle the<br>semi-structured nature of websites, and evaluate on a suite of web tasks,<br>including the recent World of Bits benchmark. We achieve new state-of-the-art<br>results, and show that workflow-guided exploration improves sample efficiency<br>over behavioral cloning by more than 100x.</td>
      <td>## 🌟 论文解读 | 使用工作流引导探索的Web界面强化学习<br><br>## 📌 背景痛点/本文动机<br>强化学习（RL）代理通过试错来提高性能，但在奖励稀疏且代理无法发现成功的动作序列时，学习会停滞不前。这在训练深度RL代理执行基于Web的任务（如预订航班或回复电子邮件）时是一个显著的问题，在这些任务中，一个错误可能会破坏整个动作序列。一个常见的补救措施是通过预训练代理来模仿专家演示来“预热”代理，但这容易导致过拟合。相反，本文提出使用演示来约束探索。从每个演示中，我们诱导出高级“工作流”，这些工作流将每个时间步允许的动作限制为与演示中的动作相似（例如，“步骤1：点击文本框；步骤2：输入一些文本”）。然后，我们的探索策略学习识别成功的工作流，并采样满足这些工作流的动作。工作流剪除了不良的探索方向，并加速了代理发现奖励的能力。我们使用我们的方法来训练一个新颖的神经策略，旨在处理网站的半结构化性质，并在一系列Web任务上进行评估，包括最近的World of Bits基准。我们取得了新的最先进的结果，并表明工作流引导的探索比行为克隆的样本效率提高了100倍以上。<br><br>## 🚀 核心方法<br>💡 创新点1：工作流引导探索（WGE）框架<br>1. 从每个演示中提取与演示中观察到的动作一致的工作流格网。<br>2. 定义一个工作流探索策略，该策略通过首先选择一个工作流，然后采样符合工作流的动作来进行探索。该策略通过强化学习逐渐学习选择哪个工作流。<br>3. 在探索过程中发现的奖励赚取的剧集进入重放缓冲区，我们使用它来训练一个更强大和表达性更强的神经网络策略。<br><br>💡 创新点2：DOMNET神经网络策略<br>为了处理网站的半结构化性质，本文提出了一种新颖的神经网络策略（DOMNET），该策略能够灵活地对网站的树状HTML表示进行关系推理。<br><br>## 📈 实验结果<br>本文在一系列Web交互任务上评估了工作流引导的探索和DOMNET，包括MiniWoB基准、阿拉斯加航空公司的航班预订界面以及一系列新构建的任务。与先前在MiniWoB上的结果相比，本文的系统在每项任务上仅使用3-10个演示就取得了更高的成功率，并建立了新的最先进的结果。<br><br>## 💬 可借鉴之处<br>本文提出的工作流引导的探索框架和DOMNET神经网络策略为在稀疏奖励环境中训练深度RL代理提供了一种有效的方法。该方法通过利用演示来约束探索，并使用新颖的神经网络策略来处理网站的半结构化性质，从而提高了样本效率和成功率。</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Neuro-Symbolic World Models for Adapting to Open World Novelty</td>
      <td>Open-world novelty--a sudden change in the mechanics or properties of an<br>environment--is a common occurrence in the real world. Novelty adaptation is an<br>agent's ability to improve its policy performance post-novelty. Most<br>reinforcement learning (RL) methods assume that the world is a closed, fixed<br>process. Consequentially, RL policies adapt inefficiently to novelties. To<br>address this, we introduce WorldCloner, an end-to-end trainable neuro-symbolic<br>world model for rapid novelty adaptation. WorldCloner learns an efficient<br>symbolic representation of the pre-novelty environment transitions, and uses<br>this transition model to detect novelty and efficiently adapt to novelty in a<br>single-shot fashion. Additionally, WorldCloner augments the policy learning<br>process using imagination-based adaptation, where the world model simulates<br>transitions of the post-novelty environment to help the policy adapt. By<br>blending ''imagined'' transitions with interactions in the post-novelty<br>environment, performance can be recovered with fewer total environment<br>interactions. Using environments designed for studying novelty in sequential<br>decision-making problems, we show that the symbolic world model helps its<br>neural policy adapt more efficiently than model-based and model-based<br>neural-only reinforcement learning methods.</td>
      <td>## 🌟 论文解读 | WorldCloner：快速适应开放世界新奇的神经符号世界模型<br><br>## 📌 背景痛点/本文动机<br>在现实世界中，开放世界的新奇性——即环境机制或属性的突然变化——是一种常见的现象。新奇性适应是指代理在新奇性之后提高其策略性能的能力。大多数强化学习（RL）方法假设世界是封闭的、固定的过程。因此，RL策略对新奇性的适应效率低下。为了解决这个问题，本文介绍了WorldCloner，一个端到端可训练的神经符号世界模型，用于快速新奇性适应。<br><br>## 🚀 核心方法<br>💡 创新点1：符号世界模型<br>WorldCloner学习一个高效的符号表示，用于表示新奇性之前的环境转换，并使用这个转换模型来检测新奇性，并以单次方式高效地适应新奇性。<br><br>💡 创新点2：基于想象的适应<br>WorldCloner使用基于想象的适应来增强策略学习过程，其中世界模型模拟新奇性之后的环境转换，以帮助策略适应。通过将“想象”的转换与新奇性之后的环境中的交互相结合，可以在更少的总环境交互中恢复性能。<br><br>## 📈 实验结果<br>在专门为研究序列决策问题中的新奇性而设计的环境中，本文展示了符号世界模型帮助其神经策略比基于模型和基于模型的神经强化学习方法更有效地适应。<br><br>## 💬 可借鉴之处<br>本文提出的WorldCloner模型为开放世界学习中的新奇性适应提供了一个有效的解决方案。其符号世界模型和基于想象的适应方法可以显著提高代理对新奇性的适应效率，减少对真实环境交互的需求。此外，本文的研究结果也表明，符号表示是适应强化学习代理新奇性的有效补充。</td>
    </tr>
    <tr>
      <th>5</th>
      <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
      <td>Existing benchmarks for grounding language in interactive environments either<br>lack real-world linguistic elements, or prove difficult to scale up due to<br>substantial human involvement in the collection of data or feedback signals. To<br>bridge this gap, we develop WebShop -- a simulated e-commerce website<br>environment with \( 1.18 \) million real-world products and \( 12,087 \) crowd-sourced<br>text instructions. Given a text instruction specifying a product requirement,<br>an agent needs to navigate multiple types of webpages and issue diverse actions<br>to find, customize, and purchase an item. WebShop provides several challenges<br>for language grounding including understanding compositional instructions,<br>query (re-)formulation, comprehending and acting on noisy text in webpages, and<br>performing strategic exploration. We collect over \( 1,600 \) human demonstrations<br>for the task, and train and evaluate a diverse range of agents using<br>reinforcement learning, imitation learning, and pre-trained image and language<br>models. Our best model achieves a task success rate of \( 29\% \), which<br>outperforms rule-based heuristics (\( 9.6\% \)) but is far lower than human expert<br>performance (\( 59\% \)). We also analyze agent and human trajectories and ablate<br>various model components to provide insights for developing future agents with<br>stronger language understanding and decision making abilities. Finally, we show<br>that agents trained on WebShop exhibit non-trivial sim-to-real transfer when<br>evaluated on amazon.com and ebay.com, indicating the potential value of WebShop<br>in developing practical web-based agents that can operate in the wild.</td>
      <td>## 🌟 论文解读 | WebShop：迈向可扩展的基于语言交互的Web环境<br><br>## 📌 背景痛点/本文动机<br>现有的语言交互环境在模拟真实世界语言元素方面存在不足，或者由于大量人工参与数据收集或反馈信号而难以扩展。为了弥合这一差距，本文提出了WebShop，一个具有118万真实世界产品和12087个众包文本指令的模拟电子商务网站环境。WebShop为语言接地提供了几个挑战，包括理解组合指令、查询（重新）制定、理解和在网页中执行噪声文本，以及执行战略探索。<br><br>## 🚀 核心方法<br>💡 创新点1：WebShop环境<br>WebShop是一个模拟电子商务网站的环境，具有超过118万真实世界产品和12087个众包文本指令。给定一个指定产品要求的文本指令，代理需要导航多种类型的网页并发出多种动作来查找、定制和购买商品。<br><br>💡 创新点2：多种学习方法<br>本文收集了超过1600个人类演示，并使用强化学习、模仿学习和预训练的图像和语言模型训练和评估了各种代理。最佳模型的任务成功率达到了29%，超过了基于规则的启发式方法（9.6%），但远低于人类专家的表现（59%）。<br><br>💡 创新点3：模型分析和消融研究<br>本文分析了代理和人类轨迹，并消融了各种模型组件，以提供有关开发具有更强语言理解和决策能力的未来代理的见解。<br><br>💡 创新点4：Sim-to-real迁移<br>最后，本文表明，在WebShop上训练的代理在amazon.com和ebay.com上进行评估时表现出非平凡的Sim-to-real迁移，表明WebShop在开发能够在野外操作的实用Web代理方面的潜在价值。<br><br>## 📈 实验结果<br>实验结果表明，最佳模型的任务成功率为29%，超过了基于规则的启发式方法（9.6%），但远低于人类专家的表现（59%）。此外，模型在amazon.com和ebay.com上的表现与在WebShop上的表现相似，表明了Sim-to-real迁移的潜力。<br><br>## 💬 可借鉴之处<br>WebShop为开发能够在真实世界Web环境中操作的实用代理提供了有价值的见解。本文提出的方法和模型可以用于开发具有更强语言理解和决策能力的未来代理。此外，WebShop环境可以用于评估和改进各种语言交互任务中的代理性能。</td>
    </tr>
    <tr>
      <th>6</th>
      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
      <td>While large language models (LLMs) have demonstrated impressive capabilities<br>across tasks in language understanding and interactive decision making, their<br>abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.<br>action plan generation) have primarily been studied as separate topics. In this<br>paper, we explore the use of LLMs to generate both reasoning traces and<br>task-specific actions in an interleaved manner, allowing for greater synergy<br>between the two: reasoning traces help the model induce, track, and update<br>action plans as well as handle exceptions, while actions allow it to interface<br>with external sources, such as knowledge bases or environments, to gather<br>additional information. We apply our approach, named ReAct, to a diverse set of<br>language and decision making tasks and demonstrate its effectiveness over<br>state-of-the-art baselines, as well as improved human interpretability and<br>trustworthiness over methods without reasoning or acting components.<br>Concretely, on question answering (HotpotQA) and fact verification (Fever),<br>ReAct overcomes issues of hallucination and error propagation prevalent in<br>chain-of-thought reasoning by interacting with a simple Wikipedia API, and<br>generates human-like task-solving trajectories that are more interpretable than<br>baselines without reasoning traces. On two interactive decision making<br>benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and<br>reinforcement learning methods by an absolute success rate of 34% and 10%<br>respectively, while being prompted with only one or two in-context examples.<br>Project site with code: https://react-lm.github.io</td>
      <td>之处，本文提出了ReAct方法，旨在将推理和行动相结合，以解决语言模型在推理和行动方面的不足。ReAct方法通过交替生成推理轨迹和特定任务的操作，实现了推理和行动之间的协同作用。推理轨迹帮助模型诱导、跟踪和更新操作计划，并处理异常情况；而操作则允许模型与外部知识库或环境进行交互，以获取更多信息。<br><br>## 🌟 论文亮点<br>* **推理与行动协同**：ReAct方法将推理和行动相结合，实现了推理和行动之间的协同作用，从而提高了模型的性能和可解释性。<br>* **可解释性和可信度**：ReAct方法生成的推理轨迹和操作计划更加可解释和可信，有助于人类理解模型的决策过程。<br>* **多任务适用性**：ReAct方法在问答、事实核查、交互式决策等任务上取得了优异的性能，证明了其多任务适用性。<br><br>## 📈 实验结果<br>* **问答和事实核查**：ReAct方法在HotpotQA和Fever数据集上取得了优异的性能，克服了链式思维推理中常见的幻觉和错误传播问题。<br>* **交互式决策**：ReAct方法在ALFWorld和WebShop数据集上取得了优异的性能，超过了模仿学习和强化学习方法。<br><br>## 🌈 未来展望<br>* **多任务训练**：未来可以探索将ReAct方法应用于更多任务，并进行多任务训练，以提高模型的泛化能力。<br>* **模型可解释性**：未来可以进一步研究如何提高ReAct方法生成的推理轨迹和操作计划的可解释性，以便人类更好地理解模型的决策过程。<br>* **模型鲁棒性**：未来可以研究如何提高ReAct方法的鲁棒性，使其能够在更复杂的环境中发挥作用。<br><br>## 📚 参考文献<br>...<br><br>## 🎯 总结<br>ReAct方法是一种很有潜力的方法，它将推理和行动相结合，为语言模型在推理和行动方面带来了新的突破。ReAct方法在多个任务上取得了优异的性能，并具有可解释性和可信度等优点。未来，ReAct方法有望在更多领域得到应用，并为人工智能的发展做出更大的贡献。</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions</td>
      <td>Automated Graphical User Interface (GUI) testing plays a crucial role in<br>ensuring app quality, especially as mobile applications have become an integral<br>part of our daily lives. Despite the growing popularity of learning-based<br>techniques in automated GUI testing due to their ability to generate human-like<br>interactions, they still suffer from several limitations, such as low testing<br>coverage, inadequate generalization capabilities, and heavy reliance on<br>training data. Inspired by the success of Large Language Models (LLMs) like<br>ChatGPT in natural language understanding and question answering, we formulate<br>the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM<br>to chat with the mobile apps by passing the GUI page information to LLM to<br>elicit testing scripts, and executing them to keep passing the app feedback to<br>LLM, iterating the whole process. Within this framework, we have also<br>introduced a functionality-aware memory prompting mechanism that equips the LLM<br>with the ability to retain testing knowledge of the whole process and conduct<br>long-term, functionality-based reasoning to guide exploration. We evaluate it<br>on 93 apps from Google Play and demonstrate that it outperforms the best<br>baseline by 32% in activity coverage, and detects 31% more bugs at a faster<br>rate. Moreover, GPTDroid identify 53 new bugs on Google Play, of which 35 have<br>been confirmed and fixed.</td>
      <td>## 🌟 论文解读 | 让大型语言模型成为测试专家：通过功能感知决策将类似人类的交互引入移动GUI测试<br><br>## 📌 背景痛点/本文动机<br>随着移动应用程序在我们的日常生活中变得越来越重要，确保应用程序质量变得至关重要。自动化图形用户界面（GUI）测试在确保应用程序质量方面发挥着至关重要的作用。然而，现有的基于学习的自动化GUI测试技术仍然存在一些局限性，例如测试覆盖率低、泛化能力不足以及对训练数据的依赖性大。<br><br>## 🚀 核心方法<br>本文提出了一种名为GPTDroid的自动化GUI测试方法，该方法将移动GUI测试问题视为一个问答任务。GPTDroid通过将GUI页面信息传递给大型语言模型（LLM），以生成测试脚本，并执行这些脚本，然后将应用程序的反馈传递给LLM，从而实现与移动应用程序的交互。此外，GPTDroid还引入了一种功能感知记忆提示机制，使LLM能够保留整个测试过程中的测试知识，并指导探索。<br><br>## 📈 实验结果<br>在Google Play上的93个应用程序上进行的评估表明，GPTDroid在活动覆盖率方面比最佳基线高出32%，并以更快的速度检测到31%的更多错误。此外，GPTDroid在Google Play上发现了53个新错误，其中35个已得到确认和修复。<br><br>## 💬 可借鉴之处<br>GPTDroid的成功表明，将大型语言模型应用于自动化GUI测试具有巨大的潜力。该方法不仅提高了测试覆盖率，还提高了错误检测率。此外，GPTDroid的功能感知记忆提示机制为LLM提供了长期推理的能力，使其能够更好地理解应用程序的功能并指导探索。这些发现为未来的研究提供了有价值的见解，并为开发更有效的自动化GUI测试工具提供了新的思路。</td>
    </tr>
    <tr>
      <th>8</th>
      <td>VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?</td>
      <td>Multimodal Large Language models (MLLMs) have shown promise in web-related<br>tasks, but evaluating their performance in the web domain remains a challenge<br>due to the lack of comprehensive benchmarks. Existing benchmarks are either<br>designed for general multimodal tasks, failing to capture the unique<br>characteristics of web pages, or focus on end-to-end web agent tasks, unable to<br>measure fine-grained abilities such as OCR, understanding, and grounding. In<br>this paper, we introduce \bench{}, a multimodal benchmark designed to assess<br>the capabilities of MLLMs across a variety of web tasks. \bench{} consists of<br>seven tasks, and comprises 1.5K human-curated instances from 139 real websites,<br>covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3<br>series, and GPT-4V(ision) on \bench{}, revealing significant challenges and<br>performance gaps. Further analysis highlights the limitations of current MLLMs,<br>including inadequate grounding in text-rich environments and subpar performance<br>with low-resolution image inputs. We believe \bench{} will serve as a valuable<br>resource for the research community and contribute to the creation of more<br>powerful and versatile MLLMs for web-related applications.</td>
      <td>## 🌟 论文解读 | VisualWebBench：多模态大语言模型在网页理解和定位方面的进展<br><br>## 📌 背景痛点/本文动机<br>随着多模态大语言模型（MLLMs）在处理网络相关任务方面的潜力日益显现，评估这些模型在网络领域的性能却面临挑战。现有的基准测试要么是为通用多模态任务设计的，无法捕捉网页的独特特征，要么专注于端到端的网络代理任务，无法衡量细粒度的能力，如OCR、理解和定位。因此，本文提出了VisualWebBench，一个旨在评估MLLMs在各种网络任务中能力的多模态基准测试。<br><br>## 🚀 核心方法<br>💡 创新点1：VisualWebBench由七个任务组成，包括网页标题OCR、网页QA、元素OCR、元素定位、动作预测和动作定位，涵盖了网页、元素和用户动作三个不同层次的理解和定位能力。<br>💡 创新点2：该基准测试包含来自139个真实网站的1.5K个人工策划的实例，涵盖了87个子域，确保了评估模型的泛化能力。<br><br>## 📈 实验结果<br>在VisualWebBench上评估了14个开源MLLMs、Gemini Pro、Claude-3系列和GPT-4V(ision)，结果表明，即使是GPT-4V和Claude Sonnet等最强大的模型，在VisualWebBench上的平均得分也只有64.6和65.8，表明仍有很大的改进空间。此外，开源MLLMs与GPT-4V和Claude系列等专有模型之间存在显著的性能差距。<br><br>## 💬 可借鉴之处<br>VisualWebBench为评估MLLMs在网络理解和定位方面的能力提供了一个标准化基准，有助于开发更强大、更高效的模型、自主网络代理和网络相关应用。此外，该基准测试还揭示了当前MLLMs的局限性，包括在文本丰富的环境中定位能力不足以及处理低分辨率图像输入的性能不佳。这些发现为未来的研究和模型开发提供了有价值的见解。</td>
    </tr>
    <tr>
      <th>9</th>
      <td>AutoGLM: Autonomous Foundation Agents for GUIs</td>
      <td>We present AutoGLM, a new series in the ChatGLM family, designed to serve as<br>foundation agents for autonomous control of digital devices through Graphical<br>User Interfaces (GUIs). While foundation models excel at acquiring human<br>knowledge, they often struggle with decision-making in dynamic real-world<br>environments, limiting their progress toward artificial general intelligence.<br>This limitation underscores the importance of developing foundation agents<br>capable of learning through autonomous environmental interactions by<br>reinforcing existing models. Focusing on Web Browser and Phone as<br>representative GUI scenarios, we have developed AutoGLM as a practical<br>foundation agent system for real-world GUI interactions. Our approach<br>integrates a comprehensive suite of techniques and infrastructures to create<br>deployable agent systems suitable for user delivery. Through this development,<br>we have derived two key insights: First, the design of an appropriate<br>"intermediate interface" for GUI control is crucial, enabling the separation of<br>planning and grounding behaviors, which require distinct optimization for<br>flexibility and accuracy respectively. Second, we have developed a novel<br>progressive training framework that enables self-evolving online curriculum<br>reinforcement learning for AutoGLM. Our evaluations demonstrate AutoGLM's<br>effectiveness across multiple domains. For web browsing, AutoGLM achieves a<br>55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second<br>attempt) and 96.2% on OpenTable evaluation tasks. In Android device control,<br>AutoGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7% on<br>common tasks in popular Chinese APPs.</td>
      <td>## 🌟 论文解读 | AutoGLM：自主控制数字设备的GUI基础智能体<br><br>## 📌 背景痛点/本文动机<br>随着人工智能的发展，大型语言模型（LLMs）和大型多模态模型（LMMs）在获取人类知识和语言能力方面取得了显著进展。然而，这些模型在动态现实世界环境中的决策能力仍然有限，这限制了它们向通用人工智能（AGI）的进步。为了克服这一限制，本文提出了AutoGLM，一个基于ChatGLM模型家族的系列基础智能体，旨在通过图形用户界面（GUIs）自主控制数字设备。<br><br>## 🚀 核心方法<br>💡 创新点1：中间接口设计<br>AutoGLM采用了一种中间接口设计，将规划行为和接地行为分离。规划行为负责制定行动策略，而接地行为负责识别和操作GUI元素。这种分离使得规划行为可以更加灵活，而接地行为可以更加准确。<br><br>💡 创新点2：自进化在线课程强化学习<br>AutoGLM采用了一种自进化在线课程强化学习框架，通过在线方式逐步增加任务的难度，从而提高智能体的性能。这种框架可以有效地解决任务数据稀缺和政策分布漂移的问题。<br><br>## 📈 实验结果<br>AutoGLM在多个基准测试和真实世界测试中取得了显著的成果。在Web浏览任务中，AutoGLM在VAB-WebArena-Lite上取得了55.2%的成功率，在OpenTable上取得了96.2%的成功率。在Android设备控制任务中，AutoGLM在AndroidLab上取得了36.2%的成功率，在常见任务中取得了89.7%的成功率。<br><br>## 💬 可借鉴之处<br>AutoGLM的设计和实现为开发GUI基础智能体提供了重要的参考。其中间接口设计和自进化在线课程强化学习框架可以有效地提高智能体的性能和鲁棒性。此外，AutoGLM的成功也表明，通过自主环境交互来强化现有模型是开发基础智能体的有效途径。</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing</td>
      <td>Automated GUI testing is widely used to help ensure the quality of mobile<br>apps. However, many GUIs require appropriate text inputs to proceed to the next<br>page which remains a prominent obstacle for testing coverage. Considering the<br>diversity and semantic requirement of valid inputs (e.g., flight departure,<br>movie name), it is challenging to automate the text input generation. Inspired<br>by the fact that the pre-trained Large Language Model (LLM) has made<br>outstanding progress in text generation, we propose an approach named QTypist<br>based on LLM for intelligently generating semantic input text according to the<br>GUI context. To boost the performance of LLM in the mobile testing scenario, we<br>develop a prompt-based data construction and tuning method which automatically<br>extracts the prompts and answers for model tuning. We evaluate QTypist on 106<br>apps from Google Play and the result shows that the passing rate of QTypist is<br>87%, which is 93% higher than the best baseline. We also integrate QTypist with<br>the automated GUI testing tools and it can cover 42% more app activities, 52%<br>more pages, and subsequently help reveal 122% more bugs compared with the raw<br>tool.</td>
      <td>## 🌟 论文解读 | 填空式：基于上下文的移动GUI测试自动化文本输入生成<br><br>## 📌 背景痛点/本文动机<br>移动应用（App）在我们的日常生活中扮演着不可或缺的角色，然而，保证App的质量却是一个挑战。图形用户界面（GUI）测试是确保App功能正确性的重要手段，但许多GUI页面需要特定的文本输入才能进入下一页，这成为了自动化测试覆盖率的障碍。由于有效输入的多样性和语义要求，自动化文本输入生成一直是一个难题。<br><br>## 🚀 核心方法<br>本文提出了一个名为QTypist的方法，基于预训练的大型语言模型（LLM）来智能地生成语义输入文本，以适应GUI上下文。为了提高LLM在移动测试场景中的性能，我们开发了一种基于提示的数据构建和微调方法，该方法自动提取模型微调的提示和答案。<br><br>## 📈 实验结果<br>我们在Google Play上对106个应用程序进行了评估，结果显示QTypist的通过率为87%，比最佳基线高出93%。我们还将QTypist与自动化GUI测试工具集成，它可以覆盖42%更多的应用程序活动，52%更多的页面，并帮助揭示122%更多的错误。<br><br>## 💬 可借鉴之处<br>QTypist为自动化GUI测试提供了一个新的思路，通过利用预训练的语言模型来生成语义丰富的文本输入，从而提高测试覆盖率。此外，QTypist的数据构建和微调方法也为其他领域提供了启示，例如如何利用现有数据来提高模型的性能。</td>
    </tr>
    <tr>
      <th>11</th>
      <td>OmniParser for Pure Vision Based GUI Agent</td>
      <td>The recent success of large vision language models shows great potential in<br>driving the agent system operating on user interfaces. However, we argue that<br>the power multimodal models like GPT-4V as a general agent on multiple<br>operating systems across different applications is largely underestimated due<br>to the lack of a robust screen parsing technique capable of: 1) reliably<br>identifying interactable icons within the user interface, and 2) understanding<br>the semantics of various elements in a screenshot and accurately associate the<br>intended action with the corresponding region on the screen. To fill these<br>gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user<br>interface screenshots into structured elements, which significantly enhances<br>the ability of GPT-4V to generate actions that can be accurately grounded in<br>the corresponding regions of the interface. We first curated an interactable<br>icon detection dataset using popular webpages and an icon description dataset.<br>These datasets were utilized to fine-tune specialized models: a detection model<br>to parse interactable regions on the screen and a caption model to extract the<br>functional semantics of the detected elements. \textsc{OmniParser}<br>significantly improves GPT-4V's performance on ScreenSpot benchmark. And on<br>Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input<br>outperforms the GPT-4V baselines requiring additional information outside of<br>screenshot.</td>
      <td>## 🌟 论文解读 | OmniParser：基于纯视觉的GUI智能体解析器<br><br>## 📌 背景痛点/本文动机<br>随着大型视觉语言模型（VLMs）的成功，它们在用户界面（UI）上执行复杂任务的能力得到了广泛关注。然而，现有的多模态模型如GPT-4V在跨平台和跨应用程序的通用性方面被低估了，这主要是因为缺乏一种能够可靠地识别用户界面中可交互图标并理解屏幕截图中的各种元素语义的屏幕解析技术。为了填补这一空白，本文提出了OmniParser，一种将用户界面屏幕截图解析为结构化元素的综合方法，显著提高了GPT-4V生成可以准确映射到界面相应区域上的动作的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：交互区域检测<br>OmniParser首先使用从流行网页的DOM树中提取的边界框创建了一个可交互区域检测数据集。这些数据集用于微调一个检测模型，该模型可以解析屏幕上的可交互区域。<br><br>💡 创新点2：功能语义提取<br>为了理解检测到的元素的功能语义，OmniParser使用了一个微调的图标描述模型来提取检测到的元素的功能描述。此外，还使用OCR模块来提取文本的边界框，并将这些信息与图标检测模块的结果合并。<br><br>💡 创新点3：结合本地语义<br>OmniParser将本地语义（包括文本和图标描述）与屏幕截图视觉提示相结合，以帮助GPT-4V更准确地识别要操作的元素。<br><br>## 📈 实验结果<br>OmniParser在ScreenSpot基准测试中显著提高了GPT-4V的性能。在Mind2Web和AITW基准测试中，仅使用屏幕截图输入的OmniParser的性能优于需要屏幕截图之外额外信息的GPT-4V基线。<br><br>## 💬 可借鉴之处<br>OmniParser提供了一种通用的屏幕解析工具，可以从UI屏幕截图中提取信息，并将其转换为结构化的边界框和标签，从而增强GPT-4V在各种用户任务中的动作预测性能。这种方法不依赖于额外的信息，如HTML和Android视图层次结构，使其成为一个通用的、易于使用的工具，可以在PC和移动平台上解析一般的用户屏幕。</td>
    </tr>
    <tr>
      <th>12</th>
      <td>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</td>
      <td>We propose the problem of conversational web navigation, where a digital<br>agent controls a web browser and follows user instructions to solve real-world<br>tasks in a multi-turn dialogue fashion. To support this problem, we introduce<br>WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert<br>demonstrations of conversational web navigation. Our benchmark covers a broad<br>range of patterns on over 150 real-world websites and can be used to train and<br>evaluate agents in diverse scenarios. Due to the magnitude of information<br>present, Large Language Models (LLMs) cannot process entire web pages in<br>real-time. To solve this bottleneck, we design a retrieval-inspired model that<br>efficiently prunes HTML pages by ranking relevant elements. We use the selected<br>elements, along with screenshots and action history, to assess a variety of<br>models for their ability to replicate human behavior when navigating the web.<br>Our experiments span from small text-only to proprietary multimodal LLMs. We<br>find that smaller finetuned decoders surpass the best zero-shot LLMs (including<br>GPT-4V), but also larger finetuned multimodal models which were explicitly<br>pretrained on screenshots. However, all finetuned models struggle to generalize<br>to unseen websites. Our findings highlight the need for large multimodal models<br>that can generalize to novel settings. Our code, data and models are available<br>for research: https://mcgill-nlp.github.io/weblinx</td>
      <td>## 🌟 论文解读 | WebLINX：基于多轮对话的真实世界网站导航<br><br>## 📌 背景痛点/本文动机<br>随着聊天机器人如ChatGPT等技术的发展，它们已经能够通过插件浏览网站，执行操作并提供更有用的响应。然而，这种能力是有限的，因为插件必须为每个网站单独开发，并且可能无法涵盖网站的所有功能。本文提出了一个重要的问题：我们能否利用这些助手背后的模型，直接在用户的浏览器中导航网站，同时保留它们的对话能力？<br><br>## 🚀 核心方法<br>💡 创新点1：WEBLINX基准数据集<br>为了支持这个问题，本文提出了WEBLINX，这是一个包含2337个专家演示的大型基准数据集，涵盖了150多个真实世界网站上的100K个交互。这个数据集可以用于训练和评估在多种场景中导航的对话式代理。<br><br>💡 创新点2：Dense Markup Ranking (DMR) 模型<br>由于大型语言模型（LLMs）无法实时处理整个网页，本文设计了一个检索启发的模型，通过排名相关元素来有效地剪枝HTML页面。使用选定的元素、屏幕截图和操作历史，评估了各种模型在复制人类行为时的能力。<br><br>## 📈 实验结果<br>实验结果表明，较小的微调解码器优于最佳零样本LLMs（包括GPT-4V），但所有微调模型都难以推广到未见过的网站。<br><br>## 💬 可借鉴之处<br>本文提出了一个重要的研究方向，即构建能够推广到新场景的大型多模态模型。此外，本文提出的Dense Markup Ranking (DMR) 模型为处理大型网页提供了一种有效的方法，可以用于其他需要处理大量文本数据的任务。</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions</td>
      <td>This paper investigates the faithfulness of multimodal large language model<br>(MLLM) agents in the graphical user interface (GUI) environment, aiming to<br>address the research question of whether multimodal GUI agents can be<br>distracted by environmental context. A general setting is proposed where both<br>the user and the agent are benign, and the environment, while not malicious,<br>contains unrelated content. A wide range of MLLMs are evaluated as GUI agents<br>using our simulated dataset, following three working patterns with different<br>levels of perception. Experimental results reveal that even the most powerful<br>models, whether generalist agents or specialist GUI agents, are susceptible to<br>distractions. While recent studies predominantly focus on the helpfulness<br>(i.e., action accuracy) of multimodal agents, our findings indicate that these<br>agents are prone to environmental distractions, resulting in unfaithful<br>behaviors. Furthermore, we switch to the adversarial perspective and implement<br>environment injection, demonstrating that such unfaithfulness can be exploited,<br>leading to unexpected risks.</td>
      <td>## 🌟 论文解读 | 多模态智能体易受环境干扰，需警惕！<br><br>## 📌 背景痛点/本文动机<br>随着多模态大语言模型（MLLM）的快速发展，多模态智能体在解决复杂交互任务方面展现出巨大潜力，尤其是在图形用户界面（GUI）自动化领域。然而，这些智能体在实际应用中面临着环境干扰的挑战。例如，广告、通知等无关内容可能会干扰智能体的目标，导致其行为偏离预期，甚至执行错误操作。<br><br>## 🚀 核心方法<br>本文针对多模态智能体在GUI环境中的忠诚度问题进行了深入研究，并提出了以下创新点：<br><br>💡 创新点1：定义了环境干扰问题，并构建了包含四种场景（弹出框、搜索、推荐、聊天）的模拟数据集，用于评估智能体在不同环境下的行为。<br><br>💡 创新点2：设计了三种工作模式（直接提示、思维链提示、动作标注），以模拟不同感知水平的智能体，并评估其对环境干扰的敏感性。<br><br>💡 创新点3：提出了环境注入攻击方法，通过修改环境内容来干扰智能体，从而验证其易受攻击性。<br><br>## 📈 实验结果<br>实验结果表明，即使是功能强大的多模态智能体，也容易受到环境干扰，导致其忠诚度和有效性下降。此外，环境注入攻击方法能够有效地干扰智能体，使其执行错误操作。<br><br>## 💬 可借鉴之处<br>本文的研究结果表明，多模态智能体在实际应用中需要考虑环境干扰的影响，并采取相应的措施来提高其鲁棒性。例如，可以通过预训练来增强智能体的忠诚度，或者引入人类交互来辅助智能体完成任务。<br><br>## 📚 参考文献<br>[1] Ma, X., Wang, Y., Yao, Y., Yuan, T., Zhang, A., Zhang, Z., & Zhao, H. (2024). Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions. arXiv preprint arXiv:2408.02544.</td>
    </tr>
    <tr>
      <th>14</th>
      <td>GAIA: a benchmark for General AI Assistants</td>
      <td>We introduce GAIA, a benchmark for General AI Assistants that, if solved,<br>would represent a milestone in AI research. GAIA proposes real-world questions<br>that require a set of fundamental abilities such as reasoning, multi-modality<br>handling, web browsing, and generally tool-use proficiency. GAIA questions are<br>conceptually simple for humans yet challenging for most advanced AIs: we show<br>that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins.<br>This notable performance disparity contrasts with the recent trend of LLMs<br>outperforming humans on tasks requiring professional skills in e.g. law or<br>chemistry. GAIA's philosophy departs from the current trend in AI benchmarks<br>suggesting to target tasks that are ever more difficult for humans. We posit<br>that the advent of Artificial General Intelligence (AGI) hinges on a system's<br>capability to exhibit similar robustness as the average human does on such<br>questions. Using GAIA's methodology, we devise 466 questions and their answer.<br>We release our questions while retaining answers to 300 of them to power a<br>leader-board available at https://huggingface.co/gaia-benchmark.</td>
      <td>## 🌟 论文解读 | GAIA：评估通用人工智能助手的基准<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）能力的快速发展，现有的AI基准测试已经无法满足评估这些模型的需求。许多LLMs已经在某些任务上超越了人类的表现，例如在法律或化学等专业领域。然而，这些任务对于人类来说可能非常困难，而对于LLMs来说却相对容易。因此，需要一种新的基准测试来评估LLMs在解决现实世界问题方面的能力。<br><br>## 🚀 核心方法<br>GAIA是一个针对通用人工智能助手（AGI）的基准测试，它提出了需要一系列基本能力的问题，例如推理、多模态处理、网络浏览和工具使用能力。GAIA的问题对于人类来说概念上很简单，但对于大多数先进的AI来说却具有挑战性。研究表明，人类受访者获得了92%的准确率，而配备了插件的GPT-4仅获得了15%的准确率。<br><br>GAIA的哲学与当前AI基准测试的趋势不同，后者建议针对对人类来说越来越困难的任务。GAIA认为，通用人工智能（AGI）的出现取决于系统能否在这些问题上展现出与普通人类相似的鲁棒性。<br><br>## 📈 实验结果<br>使用GAIA的方法，研究人员设计了466个问题和答案。他们发布了这些问题，并保留了300个问题的答案，以支持一个排行榜，可在https://huggingface.co/gaia-benchmark上访问。<br><br>## 💬 可借鉴之处<br>GAIA的基准测试为评估通用人工智能助手提供了一种新的方法。它强调了现实世界问题的重要性，并要求AI系统具备一系列基本能力。此外，GAIA的评估方法简单、快速且易于理解，使其成为评估AI系统的一种有用工具。<br><br>## 🌟 总结<br>GAIA是一个重要的基准测试，它为评估通用人工智能助手的能力提供了一个新的框架。它强调了现实世界问题的重要性，并要求AI系统具备一系列基本能力。随着GAIA的不断发展，它有望成为评估AI系统的一种重要工具，并推动通用人工智能的发展。</td>
    </tr>
    <tr>
      <th>15</th>
      <td>DynaSaur: Large Language Agents Beyond Predefined Actions</td>
      <td>Existing LLM agent systems typically select actions from a fixed and<br>predefined set at every step. While this approach is effective in closed,<br>narrowly-scoped environments, we argue that it presents two major challenges<br>when deploying LLM agents in real-world scenarios: (1) selecting from a fixed<br>set of actions significantly restricts the planning and acting capabilities of<br>LLM agents, and (2) this approach requires substantial human effort to<br>enumerate and implement all possible actions, which becomes impractical in<br>complex environments with a vast number of potential actions. In this work, we<br>propose an LLM agent framework that enables the dynamic creation and<br>composition of actions in an online manner. In this framework, the agent<br>interacts with the environment by generating and executing programs written in<br>a general-purpose programming language at each step. Furthermore, generated<br>actions are accumulated over time for future reuse. Our extensive experiments<br>on the GAIA benchmark demonstrate that this framework offers significantly<br>greater flexibility and outperforms previous methods. Notably, it allows an LLM<br>agent to recover in scenarios where no relevant action exists in the predefined<br>set or when existing actions fail due to unforeseen edge cases. At the time of<br>writing, we hold the top position on the GAIA public leaderboard. Our code can<br>be found in<br>\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.</td>
      <td>## 🌟 论文解读 | DynaSaur：超越预定义动作的大型语言模型代理<br><br>## 📌 背景痛点/本文动机<br>现有的LLM代理系统通常在每个步骤从固定和预定义的动作集中选择动作。虽然这种方法在封闭、范围狭窄的环境中是有效的，但作者认为，当在现实世界场景中部署LLM代理时，这种方法存在两个主要挑战：<br>1. 从固定动作集中选择动作显著限制了LLM代理的规划和行动能力。<br>2. 这种方法需要大量的人力来枚举和实现所有可能的动作，这在具有大量潜在动作的复杂环境中变得不切实际。<br><br>## 🚀 核心方法<br>为了解决这些限制，本文提出了DynaSaur，一个LLM代理框架，它允许以在线方式动态创建和组合动作。在这个框架中，代理通过在每个步骤生成和执行用通用编程语言编写的程序来与环境交互。此外，生成的动作会随着时间的推移积累起来，以便将来重用。<br><br>### 💡 创新点1：动态动作创建<br>DynaSaur将每个动作建模为Python函数，代理在每个步骤通过生成Python代码片段来执行动作。这些代码片段要么定义新的函数，要么重用当前动作集中的现有函数。生成的代码通过Python解释器执行，并将结果观察返回给代理。所有由代理生成的动作都会积累起来，构建一个可重用的函数库，供将来使用。<br><br>### 💡 创新点2：动作检索<br>为了解决将所有生成的动作作为提示的一部分可能导致上下文限制的问题，DynaSaur将动作集分解为两个子集：人类设计的动作集和生成的动作集。只有人类设计的动作集默认包含在提示中。为了提供代理对生成动作集的访问，引入了一个动作检索函数，该函数根据查询和整数k返回最相似的k个动作。<br><br>## 📈 实验结果<br>在GAIA基准测试上的广泛实验表明，DynaSaur框架提供了更大的灵活性，并优于以前的方法。值得注意的是，它允许LLM代理在预定义集中不存在相关动作或现有动作由于意外的边缘情况而失败的情况下恢复。<br><br>## 💬 可借鉴之处<br>DynaSaur框架为LLM代理在现实世界场景中的应用提供了新的思路。通过动态创建和组合动作，代理可以更好地适应复杂和不确定的环境，并从过去的经验中学习。此外，DynaSaur框架还可以与其他工具和库无缝集成，进一步提高代理的性能和灵活性。</td>
    </tr>
    <tr>
      <th>16</th>
      <td>MobileFlow: A Multimodal LLM For Mobile GUI Agent</td>
      <td>Currently, the integration of mobile Graphical User Interfaces (GUIs) is<br>ubiquitous in most people's daily lives. And the ongoing evolution of<br>multimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly<br>bolstered the capabilities of GUI comprehension and user action analysis,<br>showcasing the potentiality of intelligent GUI assistants. However, current GUI<br>Agents often need to access page layout information through calling system<br>APIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a<br>certain low resolution might result in the loss of fine-grained image details.<br>At the same time, the multimodal large models built for GUI Agents currently<br>have poor understanding and decision-making abilities for Chinese GUI<br>interfaces, making them difficult to apply to a large number of Chinese apps.<br>This paper introduces MobileFlow, a multimodal large language model<br>meticulously crafted for mobile GUI agents. Transforming from the open-source<br>model Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21<br>billion parameters and is equipped with novel hybrid visual encoders, making it<br>possible for variable resolutions of image inputs and good support for<br>multilingual GUI. By incorporating Mixture of Experts (MoE) expansions and<br>pioneering alignment training strategies, MobileFlow has the capacity to fully<br>interpret image data and comprehend user instructions for GUI interaction<br>tasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task<br>execution by GUI agents on both public and our proposed evaluation metrics, and<br>has been successfully deployed in real-world business contexts, proving its<br>effectiveness for practical applications.</td>
      <td>## 🌟 论文解读 | MobileFlow：移动GUI智能助手的多模态大语言模型<br><br>## 📌 背景痛点/本文动机<br>随着移动图形用户界面（GUI）在日常生活中日益普及，智能GUI助手的需求日益增长。然而，现有的GUI助手在处理中文GUI界面时表现不佳，且在获取页面布局信息时可能涉及隐私风险。此外，固定GUI分辨率可能导致图像细节丢失，限制了GUI助手的能力。<br><br>## 🚀 核心方法<br>MobileFlow是一个专为移动GUI助手设计的多模态大语言模型，具有以下创新点：<br><br>💡 创新点1：混合视觉编码器<br>MobileFlow采用了混合视觉编码器，能够处理不同分辨率的图像输入，并支持多语言GUI。通过使用LayoutLMv3作为UI编码器的基础结构，MobileFlow能够有效地提取和理解不同GUI界面的信息。<br><br>💡 创新点2：MoE扩展和CoT训练策略<br>MobileFlow引入了MoE扩展，通过随机激活多个专家网络，提高了模型的性能。同时，MobileFlow采用了CoT训练策略，使模型能够生成一系列中间步骤或解释性陈述，从而提高推理和决策的准确性。<br><br>## 📈 实验结果<br>MobileFlow在公共和自定义评估指标上均优于Qwen-VL-Max和GPT-4v，并在实际业务场景中成功部署，证明了其有效性。<br><br>## 💬 可借鉴之处<br>MobileFlow的创新方法为开发智能GUI助手提供了新的思路，特别是在处理中文GUI界面和解决隐私风险方面。此外，MoE扩展和CoT训练策略也为提高多模态大语言模型的性能提供了新的方向。</td>
    </tr>
    <tr>
      <th>17</th>
      <td>AutoTask: Executing Arbitrary Voice Commands by Exploring and Learning from Mobile GUI</td>
      <td>Voice command interfaces (VCIs) have gained increasing importance, enabling<br>hands-free and eyes-free interaction with digital devices. However, the<br>inherent complexity in constructing effective voice interfaces has limited the<br>VCIs' functionalities to only a small fraction of GUI applications and tasks.<br>This paper presents AutoTask, a VCI capable of automating any task in any<br>mobile application without configuration or modification from developers or end<br>users. The primary challenge for AutoTask is the lack of knowledge, as it needs<br>to accomplish unknown tasks (e.g., user commands) within an unknown environment<br>(e.g., GUI). To address this challenge, AutoTask employs two strategies: (1)<br>trial and error: AutoTask explores the GUI, attempts potential operation<br>sequences, and recovers from errors through backtracking; (2) learning from the<br>environment: AutoTask accumulates experiences during exploration and summarizes<br>correct knowledge from these experiences. We implemented AutoTask on Android<br>devices and conducted an evaluation study, which proved the feasibility of<br>AutoTask.</td>
      <td>## 🌟 论文解读 | AutoTask：探索与学习，让语音命令在移动GUI中自由执行<br><br>## 📌 背景痛点/本文动机<br>随着语音交互技术的不断发展，语音命令界面（VCI）在数字设备中扮演着越来越重要的角色，它允许用户在无需动手和用眼的情况下与设备进行交互。然而，构建有效的语音界面面临着巨大的挑战，导致现有的VCI功能仅限于少数GUI应用程序和任务。本文提出了AutoTask，一个无需开发人员或最终用户配置或修改即可自动执行任何移动应用程序中任何任务的VCI。AutoTask的主要挑战在于缺乏知识，因为它需要在未知环境（例如GUI）中完成未知任务（例如用户命令）。为了解决这个问题，AutoTask采用了两种策略：（1）试错：AutoTask探索GUI，尝试潜在的操作序列，并通过回溯从错误中恢复；（2）从环境中学习：AutoTask在探索过程中积累经验，并从这些经验中总结正确的知识。<br><br>## 🚀 核心方法<br>💡 创新点1：探索与学习策略<br>AutoTask采用“探索-学习”策略，通过试错和从环境中学习来完成任务。试错阶段，AutoTask探索GUI，尝试可能的操作序列，并在必要时通过回溯来纠正错误。学习阶段，AutoTask将探索过程中的经验总结为知识，包括环境知识、任务知识和执行知识，以增强其执行任务的能力。<br><br>💡 创新点2：利用大型语言模型（LLM）<br>AutoTask利用LLM来理解用户命令和GUI语义，并根据经验总结知识。LLM可以帮助AutoTask更好地理解用户意图和参数，从而提高命令理解的准确性。此外，LLM还可以帮助AutoTask生成更有效的操作序列，并避免执行错误。<br><br>## 📈 实验结果<br>AutoTask在Android设备上进行了评估，结果表明其可行性。AutoTask在执行用户指令方面的成功率显著高于基线方法，并且随着知识的积累，其执行效率也得到了显著提高。<br><br>## 💬 可借鉴之处<br>AutoTask的设计理念和方法可以为其他VCI的开发提供借鉴。例如，可以将AutoTask的“探索-学习”策略应用于其他类型的交互界面，如网页浏览器或命令行界面。此外，AutoTask还可以与其他技术相结合，例如复杂任务分解系统，以满足用户更复杂的交互需求。</td>
    </tr>
    <tr>
      <th>18</th>
      <td>WebCanvas: Benchmarking Web Agents in Online Environments</td>
      <td>For web agents to be practically useful, they must adapt to the continuously<br>evolving web environment characterized by frequent updates to user interfaces<br>and content. However, most existing benchmarks only capture the static aspects<br>of the web. To bridge this gap, we introduce WebCanvas, an innovative online<br>evaluation framework for web agents that effectively addresses the dynamic<br>nature of web interactions. WebCanvas contains three main components to<br>facilitate realistic assessments: (1) A novel evaluation metric which reliably<br>capture critical intermediate actions or states necessary for task completions<br>while disregarding noise caused by insignificant events or changed<br>web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version<br>of original Mind2Web static dataset containing 542 tasks with 2439 intermediate<br>evaluation states; (3) Lightweight and generalizable annotation tools and<br>testing pipelines that enables the community to collect and maintain the<br>high-quality, up-to-date dataset. Building on WebCanvas, we open-source an<br>agent framework with extensible modules for reasoning, providing a foundation<br>for the community to conduct online inference and evaluations. Our<br>best-performing agent achieves a task success rate of 23.1% and a task<br>completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we<br>analyze the performance discrepancies across various websites, domains, and<br>experimental environments. We encourage the community to contribute further<br>insights on online agent evaluation, thereby advancing this field of research.</td>
      <td>## 🌟 论文解读 | WebCanvas：在线环境中Web代理的基准测试框架<br><br>## 📌 背景痛点/本文动机<br>随着互联网的快速发展，Web代理在导航和信息检索任务中展现出巨大的潜力。然而，现有的Web代理评估框架大多只关注静态的Web页面，无法有效评估代理在动态、不断变化的Web环境中的表现。为了解决这个问题，本文提出了WebCanvas，一个创新的在线评估框架，旨在更真实地评估Web代理在动态Web交互中的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：关键节点标注的进度感知评估<br>WebCanvas引入了“关键节点”的概念，即完成特定Web任务所必需的步骤。通过标注关键节点，可以更详细地分析代理的行为，从而深入了解其决策的优缺点。<br><br>💡 创新点2：社区驱动的标注平台<br>WebCanvas支持通过高级记录浏览器插件记录和标注Web任务及其对应的关键节点评估。此外，还开源了一个具有可扩展推理模块的代理框架，以促进社区成员在现实世界场景中进行在线评估。<br><br>💡 创新点3：成本效益的数据维护<br>WebCanvas采用定期监控和自动警报的维护策略，以快速识别无效的动作序列和关键节点。当数据发生变化时，测试报告会提供错误信息，指导数据所有者进行快速有效的数据修正。<br><br>## 📈 实验结果<br>基于WebCanvas框架，本文创建了Mind2Web-Live数据集，包含542个任务和2439个关键节点。实验结果表明，GPT-4-turbo模型在任务成功率和任务完成率方面表现最佳，分别为23.1%和48.8%。此外，在线评估与离线评估的结果存在差异，表明在动态在线环境中，模型的表现可能不如在静态离线环境中。<br><br>## 💬 可借鉴之处<br>WebCanvas框架为Web代理的在线评估提供了一个有效的解决方案，并为社区成员提供了一个平台来构建数据集和评估Web代理框架和模型。此外，本文还探讨了关键节点标注作为中间奖励的使用，并发现Web代理可以从人类提供的关键节点标注中受益。</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents</td>
      <td>Large Language Models (LLMs) have shown remarkable capabilities in natural<br>language tasks requiring complex reasoning, yet their application in agentic,<br>multi-step reasoning within interactive environments remains a difficult<br>challenge. Traditional supervised pre-training on static datasets falls short<br>in enabling autonomous agent capabilities needed to perform complex<br>decision-making in dynamic settings like web navigation. Previous attempts to<br>bridge this ga-through supervised fine-tuning on curated expert<br>demonstrations-often suffer from compounding errors and limited exploration<br>data, resulting in sub-optimal policy outcomes. To overcome these challenges,<br>we propose a framework that combines guided Monte Carlo Tree Search (MCTS)<br>search with a self-critique mechanism and iterative fine-tuning on agent<br>interactions using an off-policy variant of the Direct Preference Optimization<br>(DPO) algorithm. Our method allows LLM agents to learn effectively from both<br>successful and unsuccessful trajectories, thereby improving their<br>generalization in complex, multi-step reasoning tasks. We validate our approach<br>in the WebShop environment-a simulated e-commerce platform where it<br>consistently outperforms behavior cloning and reinforced fine-tuning baseline,<br>and beats average human performance when equipped with the capability to do<br>online search. In real-world booking scenarios, our methodology boosts Llama-3<br>70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340%<br>relative increase) after a single day of data collection and further to 95.4%<br>with online search. We believe this represents a substantial leap forward in<br>the capabilities of autonomous agents, paving the way for more sophisticated<br>and reliable decision-making in real-world settings.</td>
      <td>## 🌟 论文解读 | Agent Q：自主AI代理的高级推理与学习<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在需要复杂推理的自然语言任务中表现出色，但在交互式环境中的多步推理应用仍然是一个挑战。传统的监督预训练方法在动态环境中进行复杂决策时效果不佳。为了解决这个问题，本文提出了一种结合引导蒙特卡洛树搜索（MCTS）和自我批评机制，以及使用直接偏好优化（DPO）算法进行迭代微调的框架。<br><br>## 🚀 核心方法<br>💡 创新点1：引导MCTS搜索<br>使用MCTS算法来指导代理在网页上的探索，以平衡探索和利用，并逐步改进策略。<br><br>💡 创新点2：自我批评机制<br>通过AI反馈和自我批评，代理在每个节点提供自我评估反馈，作为中间奖励，帮助引导搜索步骤。<br><br>💡 创新点3：迭代微调<br>使用DPO算法从代理交互中学习，包括成功和失败的轨迹，以提高代理在复杂多步推理任务中的泛化能力。<br><br>## 📈 实验结果<br>在WebShop环境中，Agent Q方法始终优于行为克隆和强化学习微调基线，并在配备在线搜索功能时击败了平均人类性能。在现实世界的预订场景中，该方法将Llama-3 70B模型的零样本性能从18.6%提高到81.7%，并在配备在线搜索功能后进一步提高到95.4%。<br><br>## 💬 可借鉴之处<br>本文提出的Agent Q框架为自主AI代理的发展提供了重要的一步，通过其搜索和自我批评能力，为交互式环境中的可靠多步决策制定了一个新的基准。</td>
    </tr>
    <tr>
      <th>20</th>
      <td>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</td>
      <td>Large language models (LLMs) have shown remarkable potential as autonomous<br>agents, particularly in web-based tasks. However, existing LLM web agents<br>heavily rely on expensive proprietary LLM APIs, while open LLMs lack the<br>necessary decision-making capabilities. This paper introduces WebRL, a<br>self-evolving online curriculum reinforcement learning framework designed to<br>train high-performance web agents using open LLMs. WebRL addresses three key<br>challenges in building LLM web agents, including the scarcity of training<br>tasks, sparse feedback signals, and policy distribution drift in online<br>learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that<br>generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised<br>reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure<br>consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4<br>models into proficient web agents. On WebArena-Lite, WebRL improves the success<br>rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.<br>These open models significantly surpass the performance of GPT-4-Turbo (17.6%)<br>and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained<br>on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's<br>effectiveness in bridging the gap between open and proprietary LLM-based web<br>agents, paving the way for more accessible and powerful autonomous web<br>interaction systems.</td>
      <td>## 🌟 论文解读 | WebRL：基于自演化在线课程强化学习的LLM网络代理训练<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在网络任务中展现出巨大的潜力，但现有的LLM网络代理主要依赖于昂贵的专有LLM API，而开源LLMs则缺乏必要的决策能力。本文提出了WebRL，一个自演化在线课程强化学习框架，旨在使用开源LLMs训练高性能的网络代理。<br><br>## 🚀 核心方法<br>💡 创新点1：自演化课程<br>WebRL通过从失败的尝试中生成新任务，构建了一个自演化课程，从而解决了训练任务稀缺的问题。<br><br>💡 创新点2：结果监督奖励模型（ORM）<br>WebRL引入了一个健壮的ORM来评估任务成功，解决了反馈信号稀疏的问题。<br><br>💡 创新点3：自适应强化学习策略<br>WebRL采用自适应强化学习策略，包括KL约束策略更新和经验回放缓冲区，以防止在线学习中的策略分布漂移。<br><br>## 📈 实验结果<br>WebRL在WebArena-Lite上显著提高了Llama-3.1和GLM-4模型的成功率，分别从4.8%提高到42.4%和从6.1%提高到43%。这些开源模型显著优于GPT-4-Turbo和GPT-4o，并超过了之前在开源LLMs上训练的最先进的网络代理（AutoWebGLM）。<br><br>## 💬 可借鉴之处<br>WebRL为训练高性能的网络代理提供了一个有效的框架，其自演化课程、ORM和自适应强化学习策略可以应用于其他LLM代理训练任务。此外，WebRL的成功表明，开源LLMs在网络任务中具有巨大的潜力，可以替代昂贵的专有LLM API。</td>
    </tr>
    <tr>
      <th>21</th>
      <td>NaviQAte: Functionality-Guided Web Application Navigation</td>
      <td>End-to-end web testing is challenging due to the need to explore diverse web<br>application functionalities. Current state-of-the-art methods, such as<br>WebCanvas, are not designed for broad functionality exploration; they rely on<br>specific, detailed task descriptions, limiting their adaptability in dynamic<br>web environments. We introduce NaviQAte, which frames web application<br>exploration as a question-and-answer task, generating action sequences for<br>functionalities without requiring detailed parameters. Our three-phase approach<br>utilizes advanced large language models like GPT-4o for complex decision-making<br>and cost-effective models, such as GPT-4o mini, for simpler tasks. NaviQAte<br>focuses on functionality-guided web application navigation, integrating<br>multi-modal inputs such as text and images to enhance contextual understanding.<br>Evaluations on the Mind2Web-Live and Mind2Web-Live-Abstracted datasets show<br>that NaviQAte achieves a 44.23% success rate in user task navigation and a<br>38.46% success rate in functionality navigation, representing a 15% and 33%<br>improvement over WebCanvas. These results underscore the effectiveness of our<br>approach in advancing automated web application testing.</td>
      <td>## 🌟 论文解读 | NaviQAte：基于功能的Web应用程序导航<br><br>## 📌 背景痛点/本文动机<br>随着Web应用程序的日益普及，确保其质量和功能变得至关重要。然而，传统的手动测试方法耗时且具有挑战性，而现有的自动化测试工具可能无法全面覆盖所有功能，导致潜在的错误和可用性问题被忽视。此外，Web应用程序的动态性和复杂性使得自动化测试变得更加困难。<br><br>## 🚀 核心方法<br>NaviQAte提出了一种基于功能的Web应用程序导航方法，将Web应用程序探索视为一个问答任务，无需详细参数即可生成功能性的操作序列。该方法分为三个阶段：<br><br>1. **行动规划**：使用检索增强生成技术将抽象任务转换为可操作的描述，并从多模态来源（如元标签、先前操作和屏幕截图）中提取网页上下文，以创建当前状态的抽象表示。<br>2. **选择提取**：识别网页上的可操作元素，并使用新颖的选择排名系统对其进行排序。元素根据其与预测下一步的语义相似性进行排名，并结合附近的视觉和文本信息提供额外上下文。<br>3. **决策制定**：结合任务历史、注释屏幕截图和排名的可操作元素来选择最佳行动。视觉提示帮助模型解释空间布局和上下文，从而实现准确和高效的导航。<br><br>## 📈 实验结果<br>在Mind2Web-Live和Mind2Web-Live-Abstracted数据集上的评估表明，NaviQAte在用户任务导航和功能导航方面分别取得了44.23%和38.46%的成功率，比WebCanvas分别提高了15%和33%。<br><br>## 💬 可借鉴之处<br>NaviQAte的基于功能的导航方法为自动化Web应用程序测试提供了一种新的思路。其多阶段、多模型的方法论以及多模态输入的集成，为提高Web导航的准确性和效率提供了宝贵的经验。此外，NaviQAte的成功也表明，大型语言模型在自动化Web应用程序测试中具有巨大的潜力。</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Falcon-UI: Understanding GUI Before Following User Instructions</td>
      <td>Pursuing human-like interaction for Graphical User Interface (GUI) agents<br>requires understanding the GUI context and following user instructions.<br>However, existing works typically couple these two aspects and focus more on<br>instruct-following abilities, while ignoring the importance of understanding<br>the GUI context. In this paper, we introduce an instruction-free GUI navigation<br>dataset, termed Insight-UI Dataset, to enhance model comprehension of GUI<br>environments. Insight-UI Dataset is automatically generated from the Common<br>Crawl corpus, simulating various platforms -- including iOS, Android, Windows,<br>and Linux -- across multiple resolutions on 312K domains. Although GUI<br>interactions vary by context, diverse interfaces share common internal<br>patterns, such as clicking an item to view its details. It implies the<br>feasibility of independent GUI operation learning, followed by joint<br>optimization with instruction tuning. Thereby, we develop the GUI agent model<br>Falcon-UI, which is initially pretrained on Insight-UI Dataset and subsequently<br>fine-tuned on Android and Web GUI datasets, including AITW, AITZ, Android<br>Control, and Mind2Web. With 7 billion parameters, Falcon-UI achieves accuracy<br>comparable to the 72 billion-parameter Qwen2VL on AITZ, validating the<br>alignment between GUI context comprehension and agent performance. Our code and<br>dataset will be open-sourced.</td>
      <td>## 🌟 论文解读 | Falcon-UI：理解GUI环境，实现更智能的用户交互<br><br>## 📌 背景痛点/本文动机<br>随着图形用户界面（GUI）在操作系统中的广泛应用，自动与GUI交互的需求日益增长。现有的GUI代理模型通常依赖于系统API或结构化输入，限制了其通用性和适应性。此外，构建包含多样化、高质量用户指令的数据集是一个挑战，这限制了GUI代理模型的学习和泛化能力。<br><br>## 🚀 核心方法<br>本文提出了Falcon-UI，一个基于视觉输入的GUI代理模型，旨在实现更智能的用户交互。Falcon-UI的核心创新点包括：<br><br>💡 创新点1：指令无关的GUI导航数据集（Insight-UI Dataset）<br>为了解决构建多样化、高质量用户指令数据集的挑战，本文提出了Insight-UI Dataset，一个完全自动生成的、指令无关的GUI导航数据集。该数据集模拟了多种平台（iOS、Android、Windows、Linux）和多种分辨率，涵盖了312K个域名，为GUI代理模型提供了丰富的GUI环境知识。<br><br>💡 创新点2：GUI环境理解与指令跟随的解耦<br>Falcon-UI采用了一种新的训练范式，将GUI环境理解与指令跟随解耦。首先，Falcon-UI在Insight-UI Dataset上进行预训练，学习GUI环境中的常见模式和操作逻辑。然后，在下游任务中，Falcon-UI根据用户指令进行微调，实现更准确的GUI交互。<br><br>## 📈 实验结果<br>在多个GUI代理模型基准测试中，Falcon-UI取得了优异的性能。例如，在Android in the Wild (AITW)和Android in the Zoo (AITZ)数据集上，Falcon-UI的准确率与72亿参数的Qwen2VL模型相当，验证了GUI环境理解预训练的有效性。<br><br>## 💬 可借鉴之处<br>本文提出的Falcon-UI模型和Insight-UI Dataset为GUI代理模型的研究提供了新的思路和方法。Falcon-UI的解耦训练范式和Insight-UI Dataset的自动生成机制，为构建更智能、更通用的GUI代理模型提供了重要的参考价值。</td>
    </tr>
    <tr>
      <th>23</th>
      <td>TaskBench: Benchmarking Large Language Models for Task Automation</td>
      <td>In recent years, the remarkable progress of large language models (LLMs) has<br>sparked interest in task automation, which involves decomposing complex tasks<br>described by user instructions into sub-tasks and invoking external tools to<br>execute them, playing a central role in autonomous agents. However, there is a<br>lack of systematic and standardized benchmarks to promote the development of<br>LLMs in task automation. To address this, we introduce TaskBench, a<br>comprehensive framework to evaluate the capability of LLMs in task automation.<br>Specifically, task automation can be divided into three critical stages: task<br>decomposition, tool selection, and parameter prediction. To tackle the<br>complexities inherent in these stages, we introduce the concept of Tool Graph<br>to represent decomposed tasks and adopt a back-instruct method to generate<br>high-quality user instructions. We propose TaskEval, a multi-faceted evaluation<br>methodology that assesses LLM performance across these three stages. Our<br>approach combines automated construction with rigorous human verification,<br>ensuring high consistency with human evaluation. Experimental results<br>demonstrate that TaskBench effectively reflects the capabilities of various<br>LLMs in task automation. It provides insights into model performance across<br>different task complexities and domains, pushing the boundaries of what current<br>models can achieve. TaskBench offers a scalable, adaptable, and reliable<br>benchmark for advancing LLM-based autonomous agents.</td>
      <td>## 🌟 论文解读 | TaskBench：大型语言模型任务自动化的全面评估框架<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）取得了显著进展，激发了人们对任务自动化的兴趣。任务自动化涉及将用户指令描述的复杂任务分解为子任务，并调用外部工具来执行它们，在自主代理中发挥着核心作用。然而，目前缺乏系统化和标准化的基准来促进LLMs在任务自动化方面的发展。为了解决这个问题，本文提出了TaskBench，这是一个全面的框架，用于评估LLMs在任务自动化方面的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：引入工具图（Tool Graph）的概念<br>为了应对任务分解、工具选择和参数预测等阶段的复杂性，本文引入了工具图的概念。工具图可以表示分解后的任务之间的关系和依赖性，克服了现有基准中简单API文档或基于模板方法的局限性。<br><br>💡 创新点2：采用反向指令（Back-Instruct）方法<br>本文采用反向指令方法来生成高质量的指令。通过从工具图中采样子图，并使用LLMs生成相应的指令，可以确保生成的指令既自然又符合实际工具使用模式。<br><br>💡 创新点3：提出TaskEval评估方法<br>为了全面评估LLMs在任务自动化方面的性能，本文提出了TaskEval评估方法。TaskEval包括三个关键阶段的评估：任务分解、工具选择和参数预测。该方法结合了自动构建和严格的人工验证，确保与人工评估的高度一致性。<br><br>## 📈 实验结果<br>实验结果表明，TaskBench能够有效地反映不同LLMs在任务自动化方面的能力。它提供了对模型在不同任务复杂性和领域中的性能的见解，推动了当前模型所能达到的边界。TaskBench提供了一个可扩展、可适应和可靠的基准，用于推进基于LLM的自主代理的发展。<br><br>## 💬 可借鉴之处<br>TaskBench为LLMs在任务自动化方面的评估提供了一个全面的框架，其创新的数据生成技术和严格的评估方法为LLMs在任务自动化方面的研究提供了重要的参考。此外，TaskBench的引入也为LLMs在复杂任务自动化场景中的应用提供了新的思路和方向。</td>
    </tr>
    <tr>
      <th>24</th>
      <td>ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation</td>
      <td>Graphical User Interface (GUI) automation holds significant promise for<br>assisting users with complex tasks, thereby boosting human productivity.<br>Existing works leveraging Large Language Model (LLM) or LLM-based AI agents<br>have shown capabilities in automating tasks on Android and Web platforms.<br>However, these tasks are primarily aimed at simple device usage and<br>entertainment operations. This paper presents a novel benchmark, AssistGUI, to<br>evaluate whether models are capable of manipulating the mouse and keyboard on<br>the Windows platform in response to user-requested tasks. We carefully<br>collected a set of 100 tasks from nine widely-used software applications, such<br>as, After Effects and MS Word, each accompanied by the necessary project files<br>for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied<br>Agent framework, which incorporates a sophisticated GUI parser driven by an<br>LLM-agent and an enhanced reasoning mechanism adept at handling lengthy<br>procedural tasks. Our experimental results reveal that our GUI Parser and<br>Reasoning mechanism outshine existing methods in performance. Nevertheless, the<br>potential remains substantial, with the best model attaining only a 46% success<br>rate on our benchmark. We conclude with a thorough analysis of the current<br>methods' limitations, setting the stage for future breakthroughs in this<br>domain.</td>
      <td>## 🌟 论文解读 | ASSISTGUI：基于任务的桌面图形用户界面自动化<br><br>## 📌 背景痛点/本文动机<br>随着人工智能技术的不断发展，图形用户界面（GUI）自动化在提高人类生产力方面展现出巨大的潜力。现有的研究主要集中于利用大型语言模型（LLM）或基于LLM的AI代理在Android和Web平台上自动化任务，但这些任务大多针对简单的设备使用和娱乐操作。然而，对于复杂的桌面应用程序，用户往往面临着陡峭的学习曲线，这限制了他们的创造力和生产力。因此，本文旨在评估模型在任务导向的桌面GUI自动化方面的能力，以评估模型在利用生产力软件方面的性能。<br><br>## 🚀 核心方法<br>💡 创新点1：ASSISTGUI基准<br>本文提出了一个名为ASSISTGUI的基准，用于评估模型在Windows平台上根据用户请求的任务操纵鼠标和键盘的能力。该基准包含从九个广泛使用的软件应用程序中精心收集的100个任务，每个任务都附带必要的项目文件，以便更好地进行评估。<br><br>💡 创新点2：Actor-Critic Embodied Agent框架<br>本文提出了一种名为Actor-Critic Embodied Agent（ACE）的先进框架，该框架结合了由LLM代理驱动的复杂GUI解析器和一种增强的推理机制，能够处理冗长的过程性任务。该框架包括三个主要模块：GUI解析器、评估模块和执行模块。<br><br>## 📈 实验结果<br>实验结果表明，本文提出的GUI解析器和推理机制在性能方面优于现有方法。然而，即使最佳模型在基准上的成功率也只有46%，这表明该领域仍有很大的发展潜力。<br><br>## 💬 可借鉴之处<br>本文提出的ASSISTGUI基准和Actor-Critic Embodied Agent框架为桌面GUI自动化领域的研究提供了重要的参考。ASSISTGUI基准可以帮助研究人员评估和比较不同模型的性能，而Actor-Critic Embodied Agent框架则为处理复杂的桌面GUI自动化任务提供了一种新的思路。此外，本文还深入分析了当前方法的局限性，为未来的研究指明了方向。</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents</td>
      <td>Multimodal large language models (MLLMs) are transforming the capabilities of<br>graphical user interface (GUI) agents, facilitating their transition from<br>controlled simulations to complex, real-world applications across various<br>platforms. However, the effectiveness of these agents hinges on the robustness<br>of their grounding capability. Current GUI agents predominantly utilize<br>text-based representations such as HTML or accessibility trees, which, despite<br>their utility, often introduce noise, incompleteness, and increased<br>computational overhead. In this paper, we advocate a human-like embodiment for<br>GUI agents that perceive the environment entirely visually and directly perform<br>pixel-level operations on the GUI. The key is visual grounding models that can<br>accurately map diverse referring expressions of GUI elements to their<br>coordinates on the GUI across different platforms. We show that a simple<br>recipe, which includes web-based synthetic data and slight adaptation of the<br>LLaVA architecture, is surprisingly effective for training such visual<br>grounding models. We collect the largest dataset for GUI visual grounding so<br>far, containing 10M GUI elements and their referring expressions over 1.3M<br>screenshots, and use it to train UGround, a strong universal visual grounding<br>model for GUI agents. Empirical results on six benchmarks spanning three<br>categories (grounding, offline agent, and online agent) show that 1) UGround<br>substantially outperforms existing visual grounding models for GUI agents, by<br>up to 20% absolute, and 2) agents with UGround outperform state-of-the-art<br>agents, despite the fact that existing agents use additional text-based input<br>while ours only uses visual perception. These results provide strong support<br>for the feasibility and promises of GUI agents that navigate the digital world<br>as humans do.</td>
      <td>## 🌟 论文解读 | 人类般的视觉导航：GUI 代理的通用视觉定位<br><br>## 📌 背景痛点/本文动机<br>随着多模态大型语言模型（MLLMs）的发展，图形用户界面（GUI）代理的能力得到了显著提升，使其从受控的模拟环境过渡到各种平台上的复杂现实世界应用。然而，这些代理的有效性取决于其定位能力的鲁棒性。当前的 GUI 代理主要使用基于文本的表示，如 HTML 或可访问性树，这虽然有用，但往往会引入噪声、不完整性和增加计算开销。<br><br>## 🚀 核心方法<br>本文提出了一种类似于人类的 GUI 代理，它完全通过视觉感知环境，并直接在 GUI 上执行像素级操作。关键在于视觉定位模型，该模型能够准确地将 GUI 元素的多种引用表达式映射到不同平台上的 GUI 坐标。本文展示了基于网络合成数据和 LLaVA 架构的简单方法，对于训练此类视觉定位模型非常有效。本文收集了迄今为止最大的 GUI 视觉定位数据集，包含 10M GUI 元素及其在 1.3M 截图中的引用表达式，并使用该数据集训练了 UGround，这是一个强大的通用视觉定位模型。<br><br>## 📈 实验结果<br>在涵盖三个类别（定位、离线代理和在线代理）的六个基准测试中，实验结果表明：<br>1. UGround 在 GUI 代理的视觉定位方面显著优于现有的视觉定位模型，绝对值提高了 20%。<br>2. 使用 UGround 的代理在性能上优于最先进的代理，尽管现有的代理使用额外的文本输入，而我们的代理仅使用视觉感知。<br><br>## 💬 可借鉴之处<br>本文提出的 SeeAct-V 框架和 UGround 模型为 GUI 代理的视觉定位提供了新的思路和方法，具有以下可借鉴之处：<br>- **视觉感知**：通过完全依赖视觉感知，GUI 代理可以避免基于文本表示的噪声和不完整性问题。<br>- **像素级操作**：直接在 GUI 上执行像素级操作，可以更精确地控制代理的行为。<br>- **通用性**：UGround 模型具有良好的跨平台泛化能力，可以应用于不同的 GUI 环境。<br>- **模块化设计**：SeeAct-V 框架的模块化设计使得代理的各个组件可以独立研究和改进。<br><br>## 🌟 总结<br>本文提出的 SeeAct-V 框架和 UGround 模型为 GUI 代理的视觉定位提供了新的思路和方法，为构建能够像人类一样导航数字世界的 GUI 代理奠定了基础。</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</td>
      <td>Language agents based on large language models (LLMs) have demonstrated great<br>promise in automating web-based tasks. Recent work has shown that incorporating<br>advanced planning algorithms, e.g., tree search, is advantageous over reactive<br>planning for web agents. However, unlike simulated sandbox environments,<br>real-world environments such as the web are rife with irreversible actions.<br>This undermines the feasibility of backtracking, a cornerstone of (tree)<br>search. Overly relying on test-time search also hurts efficiency. We advocate<br>model-based planning for web agents that employs a world model to simulate and<br>deliberate over the outcome of each candidate action before committing to one.<br>We systematically explore this paradigm by (1) Proposing a model-based planning<br>framework, WebDreamer, which employs LLMs to serve as both world models and<br>value functions; (2) Training specialized LLMs as world models with a scalable<br>data synthesis pipeline. Empirical results demonstrate that WebDreamer achieves<br>substantial performance improvements over reactive baselines. It is<br>competitive, while being 4-5 times more efficient, with tree search in sandbox<br>environments (VisualWebArena) and also works effectively on real-world websites<br>(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,<br>Dreamer-7B, performs comparable to GPT-4o, highlighting the potential of<br>specialized world models for efficient and effective planning in complex web<br>environments.</td>
      <td>## 🌟 论文解读 | 利用大型语言模型构建互联网世界模型，实现高效网络代理规划<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自动化网络任务方面的巨大潜力，如何让网络代理在复杂且不断变化的网络环境中做出有效的决策，成为了一个重要的研究课题。传统的基于搜索的规划算法在网络环境中面临着不可逆动作和效率低下的问题。本文提出了一种基于模型的规划方法，利用LLMs作为世界模型来模拟和评估候选动作的后果，从而实现高效的网络代理规划。<br><br>## 🚀 核心方法<br>💡 创新点1：提出了一种基于模型的规划框架WebDreamer，该框架利用LLMs作为世界模型和价值函数，通过模拟和评估候选动作的后果来做出决策。<br>💡 创新点2：使用可扩展的数据合成管道训练专门的LLMs作为世界模型，从而提高模型在复杂网络环境中的规划和决策能力。<br><br>## 📈 实验结果<br>实验结果表明，WebDreamer在三个基准测试中均取得了显著的性能提升，与基于搜索的规划方法相比，WebDreamer在沙盒环境（VisualWebArena）中效率提高了4-5倍，并且在真实世界网站（Online-Mind2Web和Mind2Web-Live）上也能有效工作。此外，训练出的世界模型Dreamer-7B在两个在线基准测试中表现与GPT-4o相当，证明了专门的世界模型在复杂网络环境中进行高效和有效规划的可能性。<br><br>## 💬 可借鉴之处<br>本文提出的基于模型的规划方法为网络代理在复杂网络环境中的高效决策提供了一种新的思路。利用LLMs作为世界模型，可以有效地模拟和评估候选动作的后果，从而提高网络代理的规划和决策能力。此外，本文提出的数据合成管道为训练专门的世界模型提供了一种可扩展的方法，可以进一步提高模型在复杂网络环境中的性能。</td>
    </tr>
    <tr>
      <th>27</th>
      <td>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</td>
      <td>Pre-trained large language models (LLMs) have recently achieved better<br>generalization and sample efficiency in autonomous web automation. However, the<br>performance on real-world websites has still suffered from (1) open domainness,<br>(2) limited context length, and (3) lack of inductive bias on HTML. We<br>introduce WebAgent, an LLM-driven agent that learns from self-experience to<br>complete tasks on real websites following natural language instructions.<br>WebAgent plans ahead by decomposing instructions into canonical<br>sub-instructions, summarizes long HTML documents into task-relevant snippets,<br>and acts on websites via Python programs generated from those. We design<br>WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new<br>pre-trained LLMs for long HTML documents using local and global attention<br>mechanisms and a mixture of long-span denoising objectives, for planning and<br>summarization. We empirically demonstrate that our modular recipe improves the<br>success on real websites by over 50%, and that HTML-T5 is the best model to<br>solve various HTML understanding tasks; achieving 18.7% higher success rate<br>than the prior method on MiniWoB web automation benchmark, and SoTA performance<br>on Mind2Web, an offline task planning evaluation.</td>
      <td>## 🌟 论文解读 | WebAgent：基于大型语言模型的现实世界网络自动化<br><br>## 📌 背景痛点/本文动机<br>随着预训练大型语言模型（LLMs）在自主网络自动化方面取得了更好的泛化能力和样本效率，但在现实世界网站上的性能仍然受到以下三个方面的挑战：<br>1. 开放域性：现实世界网站具有开放性和动态性，难以预先定义合适的动作空间。<br>2. 有限上下文长度：现实世界网站的HTML文档通常比模拟环境中的观察结果更长，超出了大多数LLMs的上下文长度限制。<br>3. 缺乏对HTML的归纳偏置：现有的LLMs缺乏对HTML文档结构的理解，难以有效地处理现实世界网站上的复杂任务。<br><br>## 🚀 核心方法<br>为了解决上述挑战，本文提出了WebAgent，一个基于LLMs的自主代理，它通过自我经验学习，遵循自然语言指令在现实世界网站上完成任务。WebAgent的核心方法包括：<br>1. **规划**：将自然语言指令分解为规范子指令，以便更好地理解和执行任务。<br>2. **长上下文理解**：使用HTML-T5模型对长HTML文档进行摘要，提取与任务相关的片段，从而克服LLMs上下文长度限制。<br>3. **程序合成**：将子指令和HTML片段转换为可执行的Python代码，通过Selenium WebDriver在网站上执行操作。<br><br>## 📈 实验结果<br>实验结果表明，WebAgent在现实世界网站上的成功率比单一LLM方法提高了50%以上。此外，HTML-T5模型在MiniWoB网络自动化基准测试中取得了18.7%更高的成功率，并在Mind2Web离线任务规划评估中取得了最先进的性能。<br><br>## 💬 可借鉴之处<br>WebAgent的设计思路为现实世界网络自动化提供了新的思路，其核心方法具有以下可借鉴之处：<br>1. **模块化设计**：将网络自动化任务分解为规划、摘要和程序合成三个模块，并使用不同的LLMs进行处理，提高了系统的效率和可扩展性。<br>2. **自我经验监督**：通过自我经验监督，将领域专家语言模型与真实世界网站进行对齐，提高了模型的泛化能力和鲁棒性。<br>3. **HTML-T5模型**：HTML-T5模型通过局部和全局注意力机制以及长跨度去噪目标，更好地捕捉HTML文档的结构和语义，为网络自动化任务提供了强有力的支持。<br><br>## 🎯 未来展望<br>WebAgent的成功为现实世界网络自动化开辟了新的可能性，未来可以进一步探索以下方向：<br>1. **更强大的规划模块**：开发更强大的规划模块，以更准确地分解自然语言指令，并生成更有效的子指令序列。<br>2. **更广泛的泛化能力**：收集更多真实世界网站上的数据，并使用更大的领域专家模型进行训练，以提高WebAgent的泛化能力。<br>3. **更有效的反馈机制**：将反馈机制集成到更大的语言模型中，以更好地反映代码生成过程中的错误，并提高程序合成的质量。<br>4. **更自动化的评估方法**：开发更自动化的评估方法，以减少人工干预，并提高WebAgent的可扩展性。</td>
    </tr>
    <tr>
      <th>28</th>
      <td>WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models</td>
      <td>The rapid advancement of large language models (LLMs) has led to a new era<br>marked by the development of autonomous applications in real-world scenarios,<br>which drives innovation in creating advanced web agents. Existing web agents<br>typically only handle one input modality and are evaluated only in simplified<br>web simulators or static web snapshots, greatly limiting their applicability in<br>real-world scenarios. To bridge this gap, we introduce WebVoyager, an<br>innovative Large Multimodal Model (LMM) powered web agent that can complete<br>user instructions end-to-end by interacting with real-world websites. Moreover,<br>we establish a new benchmark by compiling real-world tasks from 15 popular<br>websites and introduce an automatic evaluation protocol leveraging multimodal<br>understanding abilities of GPT-4V to evaluate open-ended web agents. We show<br>that WebVoyager achieves a 59.1% task success rate on our benchmark,<br>significantly surpassing the performance of both GPT-4 (All Tools) and the<br>WebVoyager (text-only) setups, underscoring the exceptional capability of<br>WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement<br>with human judgment, indicating its effectiveness in providing reliable and<br>accurate assessments of web agents.</td>
      <td>## 🌟 论文解读 | WebVoyager：基于大型多模态模型的端到端网络代理<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，自主应用在现实世界场景中的应用创新日益兴起，推动了高级网络代理的创建。然而，现有的网络代理通常只能处理一种输入模态，并且仅在简化的网络模拟器或静态网络快照中进行评估，这极大地限制了它们在现实世界场景中的应用。为了弥补这一差距，本文介绍了WebVoyager，这是一个创新的、由大型多模态模型（LMM）驱动的网络代理，它可以通过与真实世界网站的交互来完成用户指令的端到端处理。<br><br>## 🚀 核心方法<br>💡 创新点1：WebVoyager是一个多模态网络代理，它通过观察屏幕截图和交互式网络元素的文本内容来处理用户查询，并在此基础上制定行动计划，然后执行相应的操作（如点击、输入或滚动等）。<br><br>💡 创新点2：本文建立了一个新的基准，该基准由从15个流行网站收集的真实世界任务组成，并引入了一个自动评估协议，利用GPT-4V的多模态理解能力来评估开放式的网络代理。<br><br>## 📈 实验结果<br>实验结果表明，WebVoyager在新的基准上实现了59.1%的任务成功率，显著优于GPT-4（All Tools）和WebVoyager（仅文本）设置，这突出了WebVoyager的卓越能力。此外，提出的自动评估指标与人类判断的一致性达到了85.3%，表明其在提供可靠和准确的网络代理评估方面的有效性。<br><br>## 💬 可借鉴之处<br>WebVoyager的研究结果表明，大型多模态模型在构建智能网络代理方面具有巨大潜力。WebVoyager的设计和评估方法为未来开发更通用和强大的网络助手提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>29</th>
      <td>CogAgent: A Visual Language Model for GUI Agents</td>
      <td>People are spending an enormous amount of time on digital devices through<br>graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large<br>language models (LLMs) such as ChatGPT can assist people in tasks like writing<br>emails, but struggle to understand and interact with GUIs, thus limiting their<br>potential to increase automation levels. In this paper, we introduce CogAgent,<br>an 18-billion-parameter visual language model (VLM) specializing in GUI<br>understanding and navigation. By utilizing both low-resolution and<br>high-resolution image encoders, CogAgent supports input at a resolution of<br>1120*1120, enabling it to recognize tiny page elements and text. As a<br>generalist visual language model, CogAgent achieves the state of the art on<br>five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,<br>Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using<br>only screenshots as input, outperforms LLM-based methods that consume extracted<br>HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,<br>advancing the state of the art. The model and codes are available at<br>https://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220<br>available at https://github.com/THUDM/CogAgent.</td>
      <td>## 🌟 论文解读 | CogAgent：为GUI交互而生的新型视觉语言模型<br><br>## 📌 背景痛点/本文动机<br>随着人们越来越多地通过图形用户界面（GUI）与数字设备互动，例如计算机或智能手机屏幕，大型语言模型（LLMs）如ChatGPT在辅助人们完成诸如撰写电子邮件等任务方面表现出色。然而，LLMs在理解和与GUI交互方面存在困难，这限制了它们提高自动化水平的潜力。本文提出了CogAgent，一个专门用于GUI理解和导航的18亿参数视觉语言模型（VLM）。CogAgent通过利用低分辨率和高分辨率图像编码器，支持1120*1120的输入分辨率，使其能够识别微小的页面元素和文本。<br><br>## 🚀 核心方法<br>💡 创新点1：CogAgent采用了低分辨率和高分辨率图像编码器，使其能够处理高分辨率图像，同时保持计算效率。这种设计允许CogAgent在有限的计算预算内提高模型的可接受分辨率，从而获得更好的性能。<br><br>💡 创新点2：CogAgent在预训练过程中使用了大规模的GUI和OCR数据集，以及专门为GUI场景设计的GUI grounding任务，从而提高了模型对GUI元素的理解能力。<br><br>## 📈 实验结果<br>CogAgent在多个视觉问答（VQA）基准测试中取得了最先进的性能，包括VQAv2、OK-VQA、Text-VQA、ST-VQA、ChartQA、infoVQA、DocVQA、MM-Vet和POPE。此外，CogAgent在PC和Android GUI导航任务中，仅使用屏幕截图作为输入，就优于了基于LLM的方法，如Mind2Web和AITW，进一步推动了该领域的发展。<br><br>## 💬 可借鉴之处<br>CogAgent的设计和训练方法为构建GUI智能体提供了新的思路。其高分辨率图像处理能力和对GUI元素的理解能力，使其在GUI交互任务中具有很大的潜力。此外，CogAgent的预训练数据构建方法也为其他视觉语言模型的训练提供了参考。</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Understanding the planning of LLM agents: A survey</td>
      <td>As Large Language Models (LLMs) have shown significant intelligence, the<br>progress to leverage LLMs as planning modules of autonomous agents has<br>attracted more attention. This survey provides the first systematic view of<br>LLM-based agents planning, covering recent works aiming to improve planning<br>ability. We provide a taxonomy of existing works on LLM-Agent planning, which<br>can be categorized into Task Decomposition, Plan Selection, External Module,<br>Reflection and Memory. Comprehensive analyses are conducted for each direction,<br>and further challenges for the field of research are discussed.</td>
      <td>## 🌟 论文解读 | 深入理解大型语言模型（LLM）代理的规划能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）展现出显著的智能，将LLM作为自主代理的规划模块的进展引起了越来越多的关注。然而，传统的规划方法，如符号方法和基于强化学习的方法，存在一些局限性，例如需要将灵活的自然语言描述的问题转换为符号建模，或者需要大量的样本进行学习。因此，本文旨在提供一个系统性的视角，以了解LLM代理的规划能力，并探讨如何利用LLM来提高代理的规划能力。<br><br>## 🚀 核心方法<br>本文将现有的LLM代理规划工作分为五个主要方向，并对每个方向进行了详细的分析：<br><br>1. **任务分解**：将复杂的任务分解为多个子任务，然后依次为每个子任务进行规划。<br>2. **多计划选择**：生成多个替代计划，然后使用任务相关的搜索算法选择一个计划来执行。<br>3. **外部模块辅助规划**：使用外部规划器来提升规划过程，同时LLM主要扮演任务形式化的角色。<br>4. **反思与细化**：通过反思和细化来提高规划能力，鼓励LLM反思失败并改进计划。<br>5. **记忆增强规划**：使用额外的记忆模块来增强规划，其中存储有价值的信息，如常识知识、过去经验、特定领域的知识等。<br><br>## 📈 实验结果<br>本文在四个基准测试上评估了几个代表性方法，结果表明，性能随着成本的增加而提高。此外，对于复杂任务，少样本示例对于LLM进一步理解任务至关重要，而反思在提高成功率方面发挥着至关重要的作用。<br><br>## 💬 可借鉴之处<br>本文为LLM代理的规划能力提供了一个系统性的视角，并探讨了如何利用LLM来提高代理的规划能力。此外，本文还讨论了LLM代理规划中存在的挑战，例如幻觉、生成计划的可行性、生成计划的效率、多模态环境反馈和细粒度评估等。这些挑战为未来的研究提供了方向，并有助于推动LLM代理规划领域的发展。</td>
    </tr>
    <tr>
      <th>31</th>
      <td>WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models</td>
      <td>The rapid advancement of large language models (LLMs) has led to a new era<br>marked by the development of autonomous applications in real-world scenarios,<br>which drives innovation in creating advanced web agents. Existing web agents<br>typically only handle one input modality and are evaluated only in simplified<br>web simulators or static web snapshots, greatly limiting their applicability in<br>real-world scenarios. To bridge this gap, we introduce WebVoyager, an<br>innovative Large Multimodal Model (LMM) powered web agent that can complete<br>user instructions end-to-end by interacting with real-world websites. Moreover,<br>we establish a new benchmark by compiling real-world tasks from 15 popular<br>websites and introduce an automatic evaluation protocol leveraging multimodal<br>understanding abilities of GPT-4V to evaluate open-ended web agents. We show<br>that WebVoyager achieves a 59.1% task success rate on our benchmark,<br>significantly surpassing the performance of both GPT-4 (All Tools) and the<br>WebVoyager (text-only) setups, underscoring the exceptional capability of<br>WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement<br>with human judgment, indicating its effectiveness in providing reliable and<br>accurate assessments of web agents.</td>
      <td>## 🌟 论文解读 | WebVoyager：基于大型多模态模型的端到端网络代理<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，自主应用程序在现实世界场景中的应用日益增多，推动了高级网络代理的创新。然而，现有的网络代理通常只能处理一种输入模态，并且仅在简化的网络模拟器或静态网络快照中进行评估，这在很大程度上限制了它们在现实世界场景中的应用。为了填补这一空白，本文提出了WebVoyager，一个由大型多模态模型（LMM）驱动的创新网络代理，它可以通过与真实世界网站的交互来完成用户指令的端到端执行。<br><br>## 🚀 核心方法<br>💡 创新点1：WebVoyager是一个多模态网络代理，它利用大型多模态模型（LMM）的能力，通过处理来自交互式网络元素的屏幕截图和文本内容，来观察用户查询，并制定行动计划，然后执行相应的操作，例如点击、输入或滚动等。<br><br>💡 创新点2：本文建立了一个新的基准，通过从15个流行网站收集真实世界的任务，并引入了一个自动评估协议，利用GPT-4V的多模态理解能力来评估开放式的网络代理。<br><br>## 📈 实验结果<br>WebVoyager在新的基准上实现了59.1%的任务成功率，显著超过了GPT-4（所有工具）和WebVoyager（仅文本）设置的性能，突出了WebVoyager的卓越能力。提出的自动评估指标与人类判断的一致性达到了85.3%，表明其在提供可靠和准确的网络代理评估方面的有效性。<br><br>## 💬 可借鉴之处<br>WebVoyager的研究结果表明，在构建更智能和高效的网络自动化解决方案方面，利用先进的LMM能力具有巨大的潜力。此外，本文提出的自动评估协议为评估网络代理的能力提供了一种可靠和准确的方法，可以促进网络代理研究的进一步发展。</td>
    </tr>
    <tr>
      <th>32</th>
      <td>VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks</td>
      <td>Videos are often used to learn or extract the necessary information to<br>complete tasks in ways different than what text and static imagery alone can<br>provide. However, many existing agent benchmarks neglect long-context video<br>understanding, instead focusing on text or static image inputs. To bridge this<br>gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the<br>capabilities of long-context multimodal agents for video understanding. VideoWA<br>consists of 2,021 web agent tasks based on manually crafted video tutorials,<br>which total almost four hours of content. For our benchmark, we define a<br>taxonomy of long-context video-based agent tasks with two main areas of focus:<br>skill retention and factual retention. While skill retention tasks evaluate<br>whether an agent can use a given human demonstration to complete a task<br>efficiently, the factual retention task evaluates whether an agent can retrieve<br>instruction-relevant information from a video to complete a task. We find that<br>the best model achieves 13.3% success on factual retention tasks and 45.8% on<br>factual retention QA pairs, far below human performance at 73.9% and 79.3%,<br>respectively. On skill retention tasks, long-context models perform worse with<br>tutorials than without, exhibiting a 5% performance decrease in WebArena tasks<br>and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to<br>improve the agentic abilities of long-context multimodal models and provides a<br>testbed for future development with long-context video agents.</td>
      <td>## 🌟 论文解读 | VideoWebArena：评估长上下文多模态代理的视频理解能力<br><br>## 📌 背景痛点/本文动机<br>随着大型基础模型被应用于AI代理，这些多模态代理需要具备理解和处理视频的能力，尤其是在需要从网络中检索和处理多模态信息以完成新知识任务的情况下。然而，现有的代理基准测试主要集中在文本或静态图像输入，忽略了长上下文视频理解。为了填补这一空白，本文提出了VideoWebArena (VideoWA)，一个用于评估长上下文多模态代理视频理解能力的基准测试。<br><br>## 🚀 核心方法<br>💡 创新点1：VideoWebArena基准测试<br>VideoWA由2,021个基于手动制作视频教程的网络代理任务组成，总时长近四个小时。该基准测试定义了一个长上下文视频代理任务的分类法，重点关注技能保留和事实保留两个方面。技能保留任务评估代理是否能够使用给定的人类演示来高效地完成任务，而事实保留任务评估代理是否能够从视频中检索与指令相关的信息来完成任务。<br><br>💡 创新点2：评估现有模型<br>本文评估了流行的和最新的视频/图像能力LLM（例如GPT-4o和Gemini 1.5 Pro），以更好地了解它们当前的长上下文视频理解能力。结果表明，视频/图像能力代理仍然有限，远未达到人类的性能水平，突出了当前最先进长上下文模型在信息检索和代理能力方面的巨大差距。<br><br>## 📈 实验结果<br>在事实保留任务上，最佳模型的成功率为13.3%，而在事实保留QA对上为45.8%，远低于人类的73.9%和79.3%。在技能保留任务上，长上下文模型在教程中的表现比没有教程时更差，在WebArena任务中下降了5%，在VisualWebArena任务中下降了10.3%。<br><br>## 💬 可借鉴之处<br>VideoWebArena为评估和改进长上下文多模态代理的视频理解能力提供了一个重要的测试平台。该基准测试可以帮助研究人员识别现有模型的局限性，并推动未来在长上下文视频代理方面的研究和发展。</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Tur[k]ingBench: A Challenge Benchmark for Web Agents</td>
      <td>Can advanced multi-modal models effectively tackle complex web-based tasks?<br>Such tasks are often found on crowdsourcing platforms, where crowdworkers<br>engage in challenging micro-tasks within web-based environments.<br>  Building on this idea, we present TurkingBench, a benchmark consisting of<br>tasks presented as web pages with textual instructions and multi-modal<br>contexts. Unlike previous approaches that rely on artificially synthesized web<br>pages, our benchmark uses natural HTML pages originally designed for<br>crowdsourcing workers to perform various annotation tasks. Each task's HTML<br>instructions are instantiated with different values derived from crowdsourcing<br>tasks, creating diverse instances. This benchmark includes 32.2K instances<br>spread across 158 tasks.<br>  To support the evaluation of TurkingBench, we have developed a framework that<br>links chatbot responses to actions on web pages (e.g., modifying a text box,<br>selecting a radio button). We assess the performance of cutting-edge private<br>and open-source models, including language-only and vision-language models<br>(such as GPT4 and InternVL), on this benchmark. Our results show that while<br>these models outperform random chance, there is still significant room for<br>improvement. We hope that this benchmark will drive progress in the evaluation<br>and development of web-based agents.</td>
      <td>介绍论文的背景或痛点，或者本文的动机<br><br>## 🚀 核心方法<br>💡 创新点1：自然网页任务<br>不同于以往依赖人工合成网页的方法，TurkingBench 使用了自然 HTML 页面，这些页面原本是为众包平台上的工人设计的，用于执行各种标注任务。每个任务的 HTML 指令都使用来自众包任务的值进行实例化，从而创建多样化的实例。<br><br>💡 创新点2：交互式评估框架<br>为了支持 TurkingBench 的评估，研究人员开发了一个框架，该框架将聊天机器人的响应链接到网页上的操作（例如，修改文本框、选择单选按钮）。该框架允许评估最先进的私有和开源模型（包括语言模型和视觉语言模型）在 TurkingBench 上的性能。<br><br>## 📈 实验结果<br>实验结果表明，尽管这些模型的表现优于随机猜测，但仍有很大的改进空间。研究人员分析了模型在不同字段类型和任务长度上的性能，并确定了未来进步的挑战。<br><br>## 💬 可借鉴之处<br>TurkingBench 为评估和开发基于 Web 的智能体提供了一个有价值的基准。该基准的自然网页任务和交互式评估框架为研究人员提供了一个平台，以测试和改进他们的模型在处理复杂 Web 任务方面的能力。</td>
    </tr>
    <tr>
      <th>34</th>
      <td>Grounding Open-Domain Instructions to Automate Web Support Tasks</td>
      <td>Grounding natural language instructions on the web to perform previously<br>unseen tasks enables accessibility and automation. We introduce a task and<br>dataset to train AI agents from open-domain, step-by-step instructions<br>originally written for people. We build RUSS (Rapid Universal Support Service)<br>to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with<br>pointers parses instructions to ThingTalk, a domain-specific language we design<br>for grounding natural language on the web. Then, a grounding model retrieves<br>the unique IDs of any webpage elements requested in ThingTalk. RUSS may<br>interact with the user through a dialogue (e.g. ask for an address) or execute<br>a web operation (e.g. click a button) inside the web runtime. To augment<br>training, we synthesize natural language instructions mapped to ThingTalk. Our<br>dataset consists of 80 different customer service problems from help websites,<br>with a total of 741 step-by-step instructions and their corresponding actions.<br>RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single<br>instructions. It outperforms state-of-the-art models that directly map<br>instructions to actions without ThingTalk. Our user study shows that RUSS is<br>preferred by actual users over web navigation.</td>
      <td>## 🌟 论文解读 | 自动化网络支持任务：将开放域指令映射到网络上的自然语言指令<br><br>## 📌 背景痛点/本文动机<br>随着互联网的普及，越来越多的服务通过网站提供。然而，对于视力障碍者、技术不熟练者或分心者来说，使用这些服务仍然存在困难。现有的虚拟助手和呼叫中心虽然可以提供帮助，但它们通常需要特定的API或用户演示，限制了其通用性和易用性。<br><br>## 🚀 核心方法<br>本文提出了RUSS（Rapid Universal Support Service），一个能够从开放域的自然语言指令中学习并执行网络任务的AI代理。RUSS的核心创新点包括：<br><br>💡 创新点1：ThingTalk DSL<br>为了将自然语言指令映射到网络操作，本文设计了一种名为ThingTalk的领域特定语言（DSL）。ThingTalk包含了一系列的代理操作和一个用于检索网页元素的函数。这种设计使得ThingTalk既易于从自然语言进行语义解析，又易于生成训练数据。<br><br>💡 创新点2：模块化设计<br>RUSS由三个主要组件组成：语义解析器、定位模型和运行时环境。语义解析器使用BERT-LSTM模型将自然语言指令解析为ThingTalk代码。定位模型根据ThingTalk代码检索网页元素的唯一ID。运行时环境执行ThingTalk代码，包括与用户的交互和网络操作。<br><br>## 📈 实验结果<br>在包含80个不同客户服务问题和741个步骤的自然语言指令的数据集上，RUSS实现了76.7%的端到端准确率。与直接将指令映射到动作的现有模型相比，RUSS具有更高的准确率。用户研究表明，实际用户更喜欢使用RUSS而不是传统的网络导航。<br><br>## 💬 可借鉴之处<br>本文提出的RUSS模型为构建能够从开放域指令中学习并执行网络任务的AI代理提供了一个新的思路。ThingTalk DSL的设计和模块化设计方法可以应用于其他类似的任务，例如将自然语言指令映射到移动设备操作或机器人操作。此外，本文提出的RUSS数据集可以用于评估和改进未来的研究。</td>
    </tr>
    <tr>
      <th>35</th>
      <td>AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents</td>
      <td>Autonomy via agents using large language models (LLMs) for personalized,<br>standardized tasks boosts human efficiency. Automating web tasks (like booking<br>hotels within a budget) is increasingly sought after. Fulfilling practical<br>needs, the web agent also serves as an important proof-of-concept example for<br>various agent grounding scenarios, with its success promising advancements in<br>many future applications. Prior research often handcrafts web agent strategies<br>(e.g., prompting templates, multi-agent systems, search methods, etc.) and the<br>corresponding in-context examples, which may not generalize well across all<br>real-world scenarios. On the other hand, there has been limited study on the<br>misalignment between a web agent's observation/action representation and the<br>pre-training data of the LLM it's based on. This discrepancy is especially<br>notable when LLMs are primarily trained for language completion rather than<br>tasks involving embodied navigation actions and symbolic web elements. Our<br>study enhances an LLM-based web agent by simply refining its observation and<br>action space to better align with the LLM's capabilities. This approach enables<br>our base agent to significantly outperform previous methods on a wide variety<br>of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose<br>web interaction tasks, our agent AgentOccam surpasses the previous<br>state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute<br>points respectively, and boosts the success rate by 26.6 points (+161%) over<br>similar plain web agents with its observation and action space alignment. We<br>achieve this without using in-context examples, new agent roles, online<br>feedback or search strategies. AgentOccam's simple design highlights LLMs'<br>impressive zero-shot performance on web tasks, and underlines the critical role<br>of carefully tuning observation and action spaces for LLM-based agents.</td>
      <td>## 🌟 论文解读 | AgentOccam：基于大型语言模型的网络代理的简单而强大的基线<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）在自动化任务中的广泛应用，网络代理在个性化、标准化的任务中展现出巨大的潜力，例如自动预订酒店。然而，现有的网络代理策略往往依赖于手工设计的模板、多代理系统、搜索方法等，这些方法可能无法很好地泛化到所有现实场景中。此外，网络代理的观察/行动表示与LLM的预训练数据之间的不匹配也是一个问题，因为LLM主要针对语言完成进行训练，而不是涉及具身导航动作和符号网络元素的任务。<br><br>## 🚀 核心方法<br>💡 创新点1：简化行动空间<br>AgentOccam通过删除不必要的行动，例如无操作、标签操作和页面导航操作，来简化行动空间。同时，它将低级行动简化为更抽象的操作，例如将悬停和按键操作替换为点击操作，并将滚动操作替换为加载整个页面内容。<br><br>💡 创新点2：优化观察空间<br>AgentOccam通过删除冗余和无关的网络元素，并将网页内容块重构为更简洁但同样信息丰富的表示，来优化观察空间。此外，它还引入了两个规划行动（分支和剪枝），使代理能够使用规划树来自主组织导航工作流程，并使用相同结构来过滤历史记录以进行历史回放。<br><br>💡 创新点3：规划生成<br>AgentOccam引入了分支和剪枝行动，使代理能够自主生成计划并管理任务工作流程。这些行动允许代理将高级目标分解为更小的子目标，并在当前子计划不可行时寻求替代方案。<br><br>## 📈 实验结果<br>在WebArena基准测试中，AgentOccam在通用网络交互任务上显著优于之前的最佳方法和同期工作，分别提高了9.8（+29.4%）和5.9（+15.8%）的绝对分数，并将成功率提高了26.6（+161%）。<br><br>## 💬 可借鉴之处<br>AgentOccam的简单设计突出了LLM在网络任务上的令人印象深刻的零样本性能，并强调了仔细调整观察和行动空间对于基于LLM的代理的关键作用。这项工作为未来的网络代理研究和开发奠定了坚实的基础，并提供了一些有价值的见解。</td>
    </tr>
    <tr>
      <th>36</th>
      <td>Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models</td>
      <td>Large Language Models (LLMs) have achieved remarkable success in reasoning<br>tasks with the development of prompting methods. However, existing prompting<br>approaches cannot reuse insights of solving similar problems and suffer from<br>accumulated errors in multi-step reasoning, since they prompt LLMs to reason<br>\textit{from scratch}. To address these issues, we propose<br>\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous<br>problems and leverages their solutions to enhance the complex reasoning ability<br>of LLMs. These analogous problems are related to the input one, with reusable<br>solutions and problem-solving strategies. Thus, it is promising to propagate<br>insights of solving previous analogous problems to inspire new problem-solving.<br>To achieve this, TP first prompts LLMs to propose and solve a set of analogous<br>problems that are related to the input one. Then, TP reuses the results of<br>analogous problems to directly yield a new solution or derive a<br>knowledge-intensive plan for execution to amend the initial solution obtained<br>from scratch. TP is compatible with existing prompting approaches, allowing<br>plug-and-play generalization and enhancement in a wide range of tasks without<br>much labor in task-specific prompt engineering. Experiments across three<br>challenging tasks demonstrate TP enjoys a substantial improvement over the<br>baselines by an average of 12\% absolute increase in finding the optimal<br>solutions in Shortest-path Reasoning, 13\% improvement of human preference in<br>Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent<br>Planning.</td>
      <td>## 🌟 论文解读 | 思维传播：一种基于类比的大型语言模型复杂推理方法<br><br>## 📌 背景痛点/本文动机<br>大型语言模型（LLMs）在推理任务中取得了显著的成功，但现有的提示方法存在两个主要问题：1）无法重用解决类似问题的见解；2）在多步推理中容易累积错误。这是因为现有的方法都是从零开始进行推理，无法利用先验知识，并且对中间推理阶段的错误敏感。<br><br>## 🚀 核心方法<br>本文提出了“思维传播”（Thought Propagation, TP）框架，旨在解决上述问题。TP 框架的核心思想是利用类比推理，通过探索与输入问题相关的类似问题，并利用它们的解决方案来增强 LLMs 的复杂推理能力。<br><br>### TP 框架的三个模块：<br>1. **LLM Propose**：生成与输入问题相关的类似问题。<br>2. **LLM Solve**：解决输入问题和类似问题，并生成初始解决方案。<br>3. **LLM Aggregate**：聚合类似问题的解决方案，以生成新的解决方案或制定高级计划，从而改进输入问题的解决方案。<br><br>### TP 框架的优势：<br>* **重用先验知识**：通过类比推理，TP 可以重用解决类似问题的见解，从而提高推理效率。<br>* **减少累积错误**：通过利用类似问题的解决方案，TP 可以减少多步推理中的累积错误。<br>* **兼容性强**：TP 可以与现有的提示方法兼容，实现即插即用的泛化和增强，无需进行大量的任务特定提示工程。<br><br>## 📈 实验结果<br>本文在三个具有挑战性的任务上评估了 TP 框架，包括最短路径推理、创意写作和 LLM-Agent 规划。实验结果表明，TP 在所有任务上都取得了显著的性能提升，例如：<br>* **最短路径推理**：平均绝对提升 12% 的最优解找到率。<br>* **创意写作**：人类偏好度提升 13%。<br>* **LLM-Agent 规划**：任务完成率提升 15%。<br><br>## 💬 可借鉴之处<br>* **类比推理的应用**：TP 框架为 LLMs 的复杂推理提供了一种新的思路，即利用类比推理来重用先验知识并减少累积错误。<br>* **模块化设计**：TP 框架的模块化设计使其易于与其他提示方法集成，并实现即插即用的泛化和增强。<br>* **实验验证**：本文在多个具有挑战性的任务上对 TP 框架进行了实验验证，结果表明其具有显著的性能优势。<br><br>## 🌟 总结<br>TP 框架为 LLMs 的复杂推理提供了一种新颖而有效的方法，通过类比推理来重用先验知识并减少累积错误，从而显著提高推理效率。TP 框架的模块化设计和兼容性强使其易于应用和扩展，有望在未来推动 LLMs 推理能力的进一步提升。</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Large Language Model-Brained GUI Agents: A Survey</td>
      <td>GUIs have long been central to human-computer interaction, providing an<br>intuitive and visually-driven way to access and interact with digital systems.<br>The advent of LLMs, particularly multimodal models, has ushered in a new era of<br>GUI automation. They have demonstrated exceptional capabilities in natural<br>language understanding, code generation, and visual processing. This has paved<br>the way for a new generation of LLM-brained GUI agents capable of interpreting<br>complex GUI elements and autonomously executing actions based on natural<br>language instructions. These agents represent a paradigm shift, enabling users<br>to perform intricate, multi-step tasks through simple conversational commands.<br>Their applications span across web navigation, mobile app interactions, and<br>desktop automation, offering a transformative user experience that<br>revolutionizes how individuals interact with software. This emerging field is<br>rapidly advancing, with significant progress in both research and industry.<br>  To provide a structured understanding of this trend, this paper presents a<br>comprehensive survey of LLM-brained GUI agents, exploring their historical<br>evolution, core components, and advanced techniques. We address research<br>questions such as existing GUI agent frameworks, the collection and utilization<br>of data for training specialized GUI agents, the development of large action<br>models tailored for GUI tasks, and the evaluation metrics and benchmarks<br>necessary to assess their effectiveness. Additionally, we examine emerging<br>applications powered by these agents. Through a detailed analysis, this survey<br>identifies key research gaps and outlines a roadmap for future advancements in<br>the field. By consolidating foundational knowledge and state-of-the-art<br>developments, this work aims to guide both researchers and practitioners in<br>overcoming challenges and unlocking the full potential of LLM-brained GUI<br>agents.</td>
      <td>Wang, and G. Zeng, “PaLM-2: An<br>evolved language model for everything,” arXiv preprint arXiv:2304.02162,<br>2023.<br>[455] J. Li, Z. Wang, Y. Li, X. Wang, Y. Li, H. Wang, and Y. Li,<br>“Florence-2-base: A lightweight vision-language model for<br>efficient<br>web<br>interaction,”<br>2024.<br>[Online].<br>Available:<br>https://arxiv.org/abs/2404.03667<br>[456] Z. Wang, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, L. Chen, Z. Liu,<br>P. P. Liang et al., “Os-atlas: A foundation action model for<br>generalist gui agents,” arXiv preprint arXiv:2410.23218, 2024.<br>[457] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[458] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[459] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[460] S. Malek, A. Burns, D. Arsan, R. Kumar, K. Saenko, and B. A.<br>Plummer, “Meta-gui: Towards multi-modal conversational agents<br>on mobile gui,” 2022. [Online]. Available: https://arxiv.org/abs/<br>2205.11029<br>[461] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[462] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[463] OpenAI, “Operator: Introducing a universal interface for ai to<br>interact with the digital world,” 2025. [Online]. Available:<br>https://openai.com/index/computer-using-agent<br>[464] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,<br>X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,<br>S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16<br>words: Transformers for image recognition at scale,” in International<br>conference on machine learning. PMLR, 2020, pp. 15 841–15 855.<br>[465] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[466] Z. Wang, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, L. Chen, Z. Liu,<br>P. P. Liang et al., “Os-atlas: A foundation action model for<br>generalist gui agents,” arXiv preprint arXiv:2410.23218, 2024.<br>[467] Z. Chen, H. Zhang, O. Rippel, M. Mattsson, and J. Redmon,<br>“Swin transformer: Hierarchical vision transformer using shifted<br>windows,” in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, 2021, pp. 10091–10100.<br>[468] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[469] H. Zhang, Z. Chen, Y. Li, Y. Wang, and H. Zhang, “Mixture of<br>experts for vision language understanding,” in Proceedings of the<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition,<br>2023, pp. 17 718–17 728.<br>[470] Y. Sun, S. Wang, Y. Li, S. Feng, H. Wang, and H. Wang, “Small-<br>bert: A distilled version of bert for natural language understanding,”<br>arXiv preprint arXiv:1908.08124, 2019.<br>[471] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[472] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[473] T. Wang, K. Wei, S. Feng, B. Chen, H. Li, M. Zhou, H. Wang,<br>J. Wang, Z. Wang, and H. Zhou, “Mpt-7b: Training a large language<br>model to follow your instructions,” arXiv preprint arXiv:2304.08716,<br>2023.<br>[474] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[475] E. Tzeng, J. Wang, Y. Wang, Y. Li, and H. Zhou, “Eva-2-clip:<br>An open-source vision-language model for multimodal understanding<br>and generation,” arXiv preprint arXiv:2403.05055, 2024.<br>[476] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[477] Z. Chen, Y. Li, Y. Wang, H. Zhang, Z. Chen, Z. Wang, L. Lu,<br>Y. Li, and H. Zhou, “Convnext-xlarge: A large-scale vision<br>model for multimodal understanding and generation,” arXiv preprint<br>arXiv:2304.13677, 2023.<br>[478] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[479] D. Zimmermann and A. Koziolek, “Gui-based software testing: An<br>automated approach using gpt-4 and selenium webdriver,” in 2023<br>38th IEEE/ACM International Conference on Automated Software<br>Engineering Workshops (ASEW). IEEE, 2023, pp. 171–174.<br>[480] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[481] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[482] Eko. (2024) Eko: The ai agent platform. Accessed: 2024-11-16.<br>[Online]. Available: https://eko.fellou.ai/<br>[483] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[484] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[485] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[486] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[487] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[488] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[489] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[490] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[491] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[492] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[493] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating<br>noise to sensitivity in private data analysis,” in Theory of Cryptography<br>Conference. Springer, 2006, pp. 265–284.<br>[494] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[495] C. Gentry, S. Halevi, and N. P. Smart, “Homomorphic encryption<br>from learning with errors: Conceptually-simpler, asymptotically-<br>faster, attribute-based, and more secure,” in Proceedings of the 14th<br>ACM SIGSAC Conference on Computer and Communications Security.<br>ACM, 2007, pp. 129–138.<br>[496] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[497] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[498] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[499] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[500] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[501] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[502] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[503] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[504] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[505] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[506] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[507] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[508] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[509] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[510] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[511] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[512] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[513] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[514] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[515] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[516] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, “Mapping<br>natural language instructions to mobile ui action sequences,” 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[517] Y. Li, J. He, X. Zhou, Y</td>
    </tr>
    <tr>
      <th>38</th>
      <td>AppAgent: Multimodal Agents as Smartphone Users</td>
      <td>Recent advancements in large language models (LLMs) have led to the creation<br>of intelligent agents capable of performing complex tasks. This paper<br>introduces a novel LLM-based multimodal agent framework designed to operate<br>smartphone applications. Our framework enables the agent to operate smartphone<br>applications through a simplified action space, mimicking human-like<br>interactions such as tapping and swiping. This novel approach bypasses the need<br>for system back-end access, thereby broadening its applicability across diverse<br>apps. Central to our agent's functionality is its innovative learning method.<br>The agent learns to navigate and use new apps either through autonomous<br>exploration or by observing human demonstrations. This process generates a<br>knowledge base that the agent refers to for executing complex tasks across<br>different applications. To demonstrate the practicality of our agent, we<br>conducted extensive testing over 50 tasks in 10 different applications,<br>including social media, email, maps, shopping, and sophisticated image editing<br>tools. The results affirm our agent's proficiency in handling a diverse array<br>of high-level tasks.</td>
      <td>## 🌟 论文解读 | AppAgent：像人类用户一样操作智能手机应用的智能体<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）的快速发展，智能体能够执行复杂的任务。然而，现有的LLM智能体主要依赖于文本信息，限制了它们与环境的交互能力。本文提出了一种基于LLM的多模态智能体框架，旨在像人类用户一样操作智能手机应用。<br><br>## 🚀 核心方法<br>💡 创新点1：简化动作空间<br>本文提出了一种简化的动作空间，包括点击、长按、滑动和输入文本等操作，模拟人类与智能手机的交互方式。这种设计避免了需要精确屏幕坐标的问题，提高了智能体的操作效率和准确性。<br><br>💡 创新点2：探索式学习方法<br>本文提出了两种探索式学习方法，使智能体能够学习使用新的应用程序。一种是自主探索，智能体通过尝试不同的操作并观察结果来学习应用程序的功能。另一种是观察人类演示，智能体通过观察人类用户如何操作应用程序来学习。<br><br>## 📈 实验结果<br>本文在10个不同的应用程序上进行了50个任务的测试，包括社交媒体、电子邮件、地图、购物和复杂的图像编辑工具。结果表明，AppAgent能够有效地处理各种高级任务，并展现出良好的适应性和学习效率。<br><br>## 💬 可借鉴之处<br>本文提出的AppAgent框架为智能手机应用操作提供了一种新的解决方案，具有以下可借鉴之处：<br><br>* **简化动作空间**：通过设计简化的动作空间，可以降低智能体操作的复杂性，提高操作效率和准确性。<br>* **探索式学习方法**：通过自主探索或观察人类演示，智能体可以快速学习使用新的应用程序，提高适应性和灵活性。<br>* **多模态交互**：结合视觉和文本信息，智能体可以更好地理解环境和执行任务。<br><br>## 🌟 总结<br>AppAgent是一种基于LLM的多模态智能体框架，能够像人类用户一样操作智能手机应用。该框架具有简化动作空间、探索式学习方法和多模态交互等创新点，能够有效地处理各种高级任务，并展现出良好的适应性和学习效率。AppAgent为智能手机应用操作提供了一种新的解决方案，具有广泛的应用前景。</td>
    </tr>
    <tr>
      <th>39</th>
      <td>MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices</td>
      <td>The attainment of autonomous operations in mobile computing devices has<br>consistently been a goal of human pursuit. With the development of Large<br>Language Models (LLMs) and Visual Language Models (VLMs), this aspiration is<br>progressively turning into reality. While contemporary research has explored<br>automation of simple tasks on mobile devices via VLMs, there remains<br>significant room for improvement in handling complex tasks and reducing high<br>reasoning costs. In this paper, we introduce MobileExperts, which for the first<br>time introduces tool formulation and multi-agent collaboration to address the<br>aforementioned challenges. More specifically, MobileExperts dynamically<br>assembles teams based on the alignment of agent portraits with the human<br>requirements. Following this, each agent embarks on an independent exploration<br>phase, formulating its tools to evolve into an expert. Lastly, we develop a<br>dual-layer planning mechanism to establish coordinate collaboration among<br>experts. To validate our effectiveness, we design a new benchmark of<br>hierarchical intelligence levels, offering insights into algorithm's capability<br>to address tasks across a spectrum of complexity. Experimental results<br>demonstrate that MobileExperts performs better on all intelligence levels and<br>achieves ~ 22% reduction in reasoning costs, thus verifying the superiority of<br>our design.</td>
      <td>## 🌟 论文解读 | MobileExperts：移动设备中的动态工具赋能智能体团队<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）和视觉语言模型（VLMs）的发展，移动设备上的自主操作逐渐成为现实。然而，现有的研究主要集中在通过VLMs自动化移动设备上的简单任务，而在处理复杂任务和降低推理成本方面仍有很大的改进空间。<br><br>## 🚀 核心方法<br>💡 创新点1：基于代码组合的工具形成<br>为了减少对VLM模型的依赖，MobileExperts提出了一种新的工具模式：利用LLM的代码编写能力，通过代码组合将专家的基本操作组合成可重用的代码块工具。这些工具存储在系统中的所有专家共享的过程记忆中，从而降低了工具形成的成本，并有效地提高了操作效率。<br><br>💡 创新点2：通过双层规划实现专家协作<br>MobileExperts采用双层规划方法来解决长期规划问题：第一层是团队任务分配层，在这一层，任务被分解为专家执行的依赖子任务，这些依赖形成专家之间的协作网络；第二层是专家任务分解层，专家接收到的子任务进一步分解为更小的动作单元，逐步实现目标。通过这些方法，MobileExperts不仅提高了智能水平，还降低了推理成本和时间，为DOA领域提供了更高效的执行模式。<br><br>## 📈 实验结果<br>为了验证MobileExperts的有效性，论文设计了一个新的分层智能水平基准，提供了对算法处理不同复杂度任务能力的见解。实验结果表明，MobileExperts在所有智能水平上都表现更好，并实现了约22%的推理成本降低，从而验证了设计的优越性。<br><br>## 💬 可借鉴之处<br>MobileExperts为移动设备操作自动化领域提供了一种创新的解决方案，其基于代码组合的工具形成和双层规划机制为处理复杂任务和降低推理成本提供了新的思路。此外，论文提出的Expert-Eval基准为评估移动设备操作代理的性能提供了有价值的参考。</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Exploring the Privacy Protection Capabilities of Chinese Large Language Models</td>
      <td>Large language models (LLMs), renowned for their impressive capabilities in<br>various tasks, have significantly advanced artificial intelligence. Yet, these<br>advancements have raised growing concerns about privacy and security<br>implications. To address these issues and explain the risks inherent in these<br>models, we have devised a three-tiered progressive framework tailored for<br>evaluating privacy in language systems. This framework consists of<br>progressively complex and in-depth privacy test tasks at each tier. Our primary<br>objective is to comprehensively evaluate the sensitivity of large language<br>models to private information, examining how effectively they discern, manage,<br>and safeguard sensitive data in diverse scenarios. This systematic evaluation<br>helps us understand the degree to which these models comply with privacy<br>protection guidelines and the effectiveness of their inherent safeguards<br>against privacy breaches. Our observations indicate that existing Chinese large<br>language models universally show privacy protection shortcomings. It seems that<br>at the moment this widespread issue is unavoidable and may pose corresponding<br>privacy risks in applications based on these models.</td>
      <td>## 🌟 论文解读 | 探索中国大型语言模型的隐私保护能力<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）在自然语言处理领域的快速发展，其在各种任务中展现出的强大能力推动了人工智能的进步。然而，这些模型的广泛应用也引发了关于隐私和安全性的担忧。由于LLMs的训练数据往往包含大量个人隐私信息，因此模型可能会无意中记住这些内容，从而对数据隐私构成安全风险。此外，随着越来越多的服务提供商将LLMs集成到其软件应用中，模型在处理包含敏感信息的私人文档数据时，需要严格遵守隐私保护规则，并能够识别和保护敏感隐私信息。<br><br>## 🚀 核心方法<br>为了评估LLMs的隐私保护能力，本文提出了一种三层级的隐私测试和评估框架，该框架从浅到深地评估LLMs在不同任务场景下的隐私保护能力。具体包括：<br><br>💡 创新点1：一般隐私信息评估<br>该层级评估LLMs在面对直接询问个人隐私信息（如电话号码、电子邮件地址、家庭住址等）时的表现。模型应能够拒绝回答此类问题，并输出合适的理由。<br><br>💡 创新点2：上下文隐私评估<br>该层级评估LLMs在面对涉及隐私信息的情境对话时的表现。模型应能够识别情境中的隐私协议，并遵守这些协议，以保护相关内容。<br><br>💡 创新点3：攻击下的隐私评估<br>该层级评估LLMs在面对攻击性指令时的表现。模型应能够识别攻击指令背后的隐私泄露风险，并严格遵守内部隐私保护指令。<br><br>## 📈 实验结果<br>实验结果表明，除了0-shot测试外，LLMs在其他任务场景下的表现均不理想。这些模型未能展现出足够的隐私敏感性和隐私保护能力。这表明，LLMs在处理包含敏感信息的数据时，需要进一步优化和改进，以确保相关数据的安全和隐私。<br><br>## 💬 可借鉴之处<br>本文提出的隐私测试和评估框架为评估LLMs的隐私保护能力提供了一种有效的方法。此外，本文的研究结果也提醒模型服务提供商/开发者需要更加关注LLMs的隐私保护问题，并采取相应的措施来降低隐私泄露的风险。</td>
    </tr>
    <tr>
      <th>41</th>
      <td>WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration</td>
      <td>LLM-based autonomous agents often fail to execute complex web tasks that<br>require dynamic interaction due to the inherent uncertainty and complexity of<br>these environments. Existing LLM-based web agents typically rely on rigid,<br>expert-designed policies specific to certain states and actions, which lack the<br>flexibility and generalizability needed to adapt to unseen tasks. In contrast,<br>humans excel by exploring unknowns, continuously adapting strategies, and<br>resolving ambiguities through exploration. To emulate human-like adaptability,<br>web agents need strategic exploration and complex decision-making. Monte Carlo<br>Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with<br>vast action spaces, unpredictable state transitions, and incomplete information<br>in web tasks. In light of this, we develop WebPilot, a multi-agent system with<br>a dual optimization strategy that improves MCTS to better handle complex web<br>environments. Specifically, the Global Optimization phase involves generating a<br>high-level plan by breaking down tasks into manageable subtasks and<br>continuously refining this plan, thereby focusing the search process and<br>mitigating the challenges posed by vast action spaces in classical MCTS.<br>Subsequently, the Local Optimization phase executes each subtask using a<br>tailored MCTS designed for complex environments, effectively addressing<br>uncertainties and managing incomplete information. Experimental results on<br>WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on<br>WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%<br>relative increase in success rate over the concurrent tree search-based method.<br>WebPilot marks a significant advancement in general autonomous agent<br>capabilities, paving the way for more advanced and reliable decision-making in<br>practical environments.</td>
      <td>## 🌟 论文解读 | WebPilot：灵活自主的多智能体系统，助力复杂网络任务执行<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）的推理能力不断增强，基于LLM的自主网络代理在复杂动态环境中导航和交互的潜力也得到了显著提升。然而，现有的LLM网络代理在执行需要动态交互的复杂网络任务时往往表现不佳，这主要是因为它们过度依赖专家设计的、针对特定状态和动作的刚性策略。这些策略虽然针对特定场景进行了精心设计，但缺乏灵活性和泛化能力，难以适应现实世界中网络环境的复杂性和不确定性。<br><br>## 🚀 核心方法<br>为了克服这些挑战，本文提出了WebPilot，一个灵活自主的多智能体系统，它结合了全局优化和局部优化的双重策略，旨在提高代理在复杂网络环境中的适应性和有效性。<br><br>### 💡 创新点1：全局优化：通过反思调整进行自适应策略优化<br>WebPilot的全局优化阶段模拟了人类的认知过程，通过利用先验知识为不熟悉的任务生成初始计划。然而，由于LLM缺乏特定的网络领域知识，以及网络环境的动态性和不确定性，初始计划往往缺乏关键细节，难以保持有效性。为了解决这个问题，WebPilot通过反思分析新观察和先前子任务的结果，不断优化初始计划。全局优化包括两个关键组件：层次任务分解（HTD）和反思任务调整（RTA）。<br><br>*   **层次任务分解（HTD）**：将复杂任务分解为更小、更易于管理的子任务，从而创建一个灵活的高级计划，可以适应网络环境的不断变化。<br>*   **反思任务调整（RTA）**：在完成每个子任务后，WebPilot会重新评估和优化其高级计划，以确保与整体任务保持一致。控制器会评估当前观察和执行的动作序列是否符合子任务，并根据新观察重新校准策略。<br><br>### 💡 创新点2：局部优化：MCTS增强的决策策略<br>WebPilot的局部优化阶段受人类在导航和解决复杂网络任务时所需的类似适应性启发，有效地通过MCTS来捕捉。对于每个子任务及其子任务特定的目标，探索器、验证器和评估器协同工作以完成任务。探索器识别最佳动作，验证器确保这些动作有效且不冗余，评估器评估动作的即时有效性和实现预期目标的潜力，并提供持续的反馈，以便进行更细致和准确的评估。<br><br>WebPilot的局部优化阶段类似于经典的MCTS，遵循四个关键阶段：<br><br>*   **目标导向选择（GOS）**：利用LLM的初始直觉，引导WebPilot朝着子任务完成的最佳路径前进。<br>*   **反思增强节点扩展（RENE）**：在每次节点扩展后集成反思反馈，使WebPilot能够动态地重新评估和优化其策略。<br>*   **动态评估和模拟（DES）**：通过分析执行的动作和模拟潜在结果来评估当前状态，从而模拟人类的远见。<br>*   **最大值反向传播（MVB）**：通过持续更新基于最大未来奖励的价值估计来优先考虑最有潜力的路径。<br><br>## 📈 实验结果<br>在WebArena和MiniWoB++上的实验结果表明，WebPilot在复杂网络环境中表现出色。在WebArena上，WebPilot与GPT-4配合使用，实现了最先进的性能，与并发的基于树的搜索方法相比，成功率提高了93%。<br><br>## 💬 可借鉴之处<br>WebPilot的设计为开发更灵活、更自主的网络代理提供了有价值的见解。其双重优化策略和MCTS增强的决策策略可以应用于各种需要动态交互和复杂决策的网络任务。此外，WebPilot的层次反思机制和细粒度双面自我奖励机制为提高代理在动态环境中的适应性和有效性提供了新的思路。</td>
    </tr>
    <tr>
      <th>42</th>
      <td>MMInA: Benchmarking Multihop Multimodal Internet Agents</td>
      <td>Autonomous embodied agents live on an Internet of multimedia websites. Can<br>they hop around multimodal websites to complete complex user tasks? Existing<br>benchmarks fail to assess them in a realistic, evolving environment for their<br>embodiment across websites. To answer this question, we present MMInA, a<br>multihop and multimodal benchmark to evaluate the embodied agents for<br>compositional Internet tasks, with several appealing properties: 1) Evolving<br>real-world multimodal websites. Our benchmark uniquely operates on evolving<br>real-world websites, ensuring a high degree of realism and applicability to<br>natural user tasks. Our data includes 1,050 human-written tasks covering<br>various domains such as shopping and travel, with each task requiring the agent<br>to autonomously extract multimodal information from web pages as observations;<br>2) Multihop web browsing. Our dataset features naturally compositional tasks<br>that require information from or actions on multiple websites to solve, to<br>assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.<br>We propose a novel protocol for evaluating an agent's progress in completing<br>multihop tasks. We experiment with both standalone (multimodal) language models<br>and heuristic-based web agents. Extensive experiments demonstrate that while<br>long-chain multihop web tasks are easy for humans, they remain challenging for<br>state-of-the-art web agents. We identify that agents are more likely to fail on<br>the early hops when solving tasks of more hops, which results in lower task<br>success rates. To address this issue, we propose a simple memory augmentation<br>approach replaying past action trajectories to reflect. Our method<br>significantly improved both the single-hop and multihop web browsing abilities<br>of agents. See our code and data at https://mmina.cliangyu.com</td>
      <td>## 🌟 论文解读 | MMInA：评估多跳多模态互联网代理的基准<br><br>## 📌 背景痛点/本文动机<br>随着人工智能的发展，自主体代理在互联网上的应用越来越广泛。然而，现有的基准测试无法在真实、动态的环境中评估这些代理在跨网站任务中的表现。为了解决这个问题，本文提出了MMInA，一个多跳和多模态的基准测试，用于评估代理在完成组合式互联网任务方面的能力。<br><br>## 🚀 核心方法<br>💡 创新点1：真实世界多模态网站<br>MMInA基准测试在真实世界多模态网站上运行，确保了高度的真实性和适用性。数据包括1050个由人类编写的任务，涵盖购物、旅行等各个领域，每个任务都需要代理自主地从网页中提取多模态信息。<br><br>💡 创新点2：多跳网页浏览<br>MMInA数据集具有自然组合的任务，需要从多个网站获取信息或执行操作才能解决，以评估在网页任务上的长距离推理能力。<br><br>💡 创新点3：整体评估<br>本文提出了一种新颖的协议，用于评估代理在完成多跳任务方面的进展。实验结果表明，虽然长链多跳网页任务对人类来说很容易，但对于最先进的网页代理来说仍然具有挑战性。<br><br>💡 创新点4：记忆增强方法<br>为了解决代理在早期跳转中失败的问题，本文提出了一种简单的记忆增强方法，通过重放过去的行为轨迹来反映。这种方法显著提高了代理的单跳和多跳网页浏览能力。<br><br>## 📈 实验结果<br>实验结果表明，虽然最先进的模型在处理简单文本任务方面取得了显著进展，但MMInA中任务的集成和顺序性质仍然构成了重大挑战。例如，表现最好的独立模型GPT-4V在任务中的整体成功率仅为21.8%，这比文本代理基线有了显著提高，但仍落后于人类表现（96.3%）。实验还发现，代理在解决更多跳转的任务时更容易在早期跳转中失败，导致任务成功率降低。<br><br>## 💬 可借鉴之处<br>MMInA基准测试为评估多跳和多模态互联网代理提供了一个有价值的工具。它强调了在真实世界环境中评估代理能力的重要性，并为未来的研究提供了方向。此外，本文提出的记忆增强方法为提高代理性能提供了一种简单而有效的方法，可以应用于更多的大型多模态模型。</td>
    </tr>
    <tr>
      <th>43</th>
      <td>An In-depth Survey of Large Language Model-based Artificial Intelligence Agents</td>
      <td>Due to the powerful capabilities demonstrated by large language model (LLM),<br>there has been a recent surge in efforts to integrate them with AI agents to<br>enhance their performance. In this paper, we have explored the core differences<br>and characteristics between LLM-based AI agents and traditional AI agents.<br>Specifically, we first compare the fundamental characteristics of these two<br>types of agents, clarifying the significant advantages of LLM-based agents in<br>handling natural language, knowledge storage, and reasoning capabilities.<br>Subsequently, we conducted an in-depth analysis of the key components of AI<br>agents, including planning, memory, and tool use. Particularly, for the crucial<br>component of memory, this paper introduced an innovative classification scheme,<br>not only departing from traditional classification methods but also providing a<br>fresh perspective on the design of an AI agent's memory system. We firmly<br>believe that in-depth research and understanding of these core components will<br>lay a solid foundation for the future advancement of AI agent technology. At<br>the end of the paper, we provide directional suggestions for further research<br>in this field, with the hope of offering valuable insights to scholars and<br>researchers in the field.</td>
      <td>## 🌟 论文解读 | 基于大型语言模型的AI智能体：探索与展望<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLM）在自然语言处理、知识存储和推理能力方面的强大表现，将LLM与AI智能体相结合以提高其性能的研究日益增多。然而，LLM-based AI智能体与传统AI智能体之间存在着核心差异和特点，需要深入研究和理解。<br><br>## 🚀 核心方法<br>💡 创新点1：LLM-based AI智能体与传统AI智能体的比较<br>本文首先比较了这两种类型智能体的基本特性，阐明了LLM-based AI智能体在处理自然语言、知识存储和推理能力方面的显著优势。<br><br>💡 创新点2：AI智能体关键组件的深入分析<br>本文对AI智能体的关键组件进行了深入分析，包括规划、记忆和工具使用。特别是对于记忆这一关键组件，本文引入了一种创新的分类方案，不仅与传统分类方法不同，还为AI智能体记忆系统的设计提供了新的视角。<br><br>💡 创新点3：AI智能体的应用场景<br>本文探讨了LLM-based AI智能体的应用场景，包括聊天机器人、游戏、设计、研究、编码、协作和通用目的等。<br><br>💡 创新点4：AI智能体的评估基准<br>本文还介绍了针对LLM-based AI智能体设计的评估基准，以评估其性能和效果。<br><br>## 📈 实验结果<br>本文并未提供具体的实验结果，而是对LLM-based AI智能体的研究现状进行了全面的综述和分析。<br><br>## 💬 可借鉴之处<br>本文为LLM-based AI智能体的研究提供了宝贵的参考和启示，有助于读者快速了解该领域的研究历史和应用现状，并为未来的研究提供了方向性的建议。</td>
    </tr>
    <tr>
      <th>44</th>
      <td>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models</td>
      <td>While language models (LMs) have shown potential across a range of<br>decision-making tasks, their reliance on simple acting processes limits their<br>broad deployment as autonomous agents. In this paper, we introduce Language<br>Agent Tree Search (LATS) -- the first general framework that synergizes the<br>capabilities of LMs in reasoning, acting, and planning. By leveraging the<br>in-context learning ability of LMs, we integrate Monte Carlo Tree Search into<br>LATS to enable LMs as agents, along with LM-powered value functions and<br>self-reflections for proficient exploration and enhanced decision-making. A key<br>feature of our approach is the incorporation of an environment for external<br>feedback, which offers a more deliberate and adaptive problem-solving mechanism<br>that surpasses the constraints of existing techniques. Our experimental<br>evaluation across diverse domains, including programming, interactive<br>question-answering (QA), web navigation, and math, validates the effectiveness<br>and generality of LATS in decision-making while maintaining competitive or<br>improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1<br>accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates<br>gradient-free performance (average score of 75.9) comparable to gradient-based<br>fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at<br>https://github.com/lapisrocks/LanguageAgentTreeSearch</td>
      <td>## 🌟 论文解读 | 语言模型决策树搜索：统一推理、行动和规划<br><br>## 📌 背景痛点/本文动机<br>语言模型（LMs）在决策任务中展现出潜力，但其简单的行动过程限制了其作为自主代理的广泛应用。本文提出了语言模型决策树搜索（LATS），这是第一个将LMs在推理、行动和规划方面的能力相结合的通用框架。<br><br>## 🚀 核心方法<br>💡 创新点1：LATS利用LMs的上下文学习能力，将蒙特卡洛树搜索（MCTS）集成到LATS中，使LMs能够作为代理，并使用LM驱动的价值函数和自我反思来进行熟练的探索和增强的决策。<br><br>💡 创新点2：LATS的关键特征是集成了外部反馈的环境，这提供了一个更深思熟虑和适应性更强的解决问题的机制，超越了现有技术的限制。<br><br>## 📈 实验结果<br>在编程、交互式问答（QA）、网络导航和数学等不同领域的实验评估中，验证了LATS在决策中的有效性和通用性，同时保持了有竞争力的或改进的推理性能。值得注意的是，LATS在HumanEval上使用GPT-4实现了最先进的pass@1准确率（92.7%），并在WebShop上使用GPT-3.5实现了与基于梯度的微调相当的梯度无关性能（平均分数为75.9%）。<br><br>## 💬 可借鉴之处<br>LATS框架为LMs在决策和推理方面的应用提供了新的思路，其结合了MCTS、外部反馈和自我反思，为LMs作为通用代理的应用提供了新的可能性。</td>
    </tr>
    <tr>
      <th>45</th>
      <td>WebArena: A Realistic Web Environment for Building Autonomous Agents</td>
      <td>With advances in generative AI, there is now potential for autonomous agents<br>to manage daily tasks via natural language commands. However, current agents<br>are primarily created and tested in simplified synthetic environments, leading<br>to a disconnect with real-world scenarios. In this paper, we build an<br>environment for language-guided agents that is highly realistic and<br>reproducible. Specifically, we focus on agents that perform tasks on the web,<br>and create an environment with fully functional websites from four common<br>domains: e-commerce, social forum discussions, collaborative software<br>development, and content management. Our environment is enriched with tools<br>(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage<br>human-like task-solving. Building upon our environment, we release a set of<br>benchmark tasks focusing on evaluating the functional correctness of task<br>completions. The tasks in our benchmark are diverse, long-horizon, and designed<br>to emulate tasks that humans routinely perform on the internet. We experiment<br>with several baseline agents, integrating recent techniques such as reasoning<br>before acting. The results demonstrate that solving complex tasks is<br>challenging: our best GPT-4-based agent only achieves an end-to-end task<br>success rate of 14.41%, significantly lower than the human performance of<br>78.24%. These results highlight the need for further development of robust<br>agents, that current state-of-the-art large language models are far from<br>perfect performance in these real-life tasks, and that WebArena can be used to<br>measure such progress.</td>
      <td>## 🌟 论文解读 | WebArena：构建自主代理的真实网络环境<br><br>## 📌 背景痛点/本文动机<br>随着生成式AI的进步，自主代理现在可以通过自然语言命令来管理日常任务。然而，当前的代理主要是在简化的合成环境中创建和测试的，这导致了与现实世界场景的脱节。为了解决这个问题，本文提出了WebArena，一个高度真实和可复制的网络环境，用于构建和测试自主代理。<br><br>## 🚀 核心方法<br>💡 创新点1：高度真实的网络环境<br>WebArena包含四个完全功能的、自托管的网络应用程序，每个应用程序代表一个在互联网上普遍存在的不同领域：电子商务、社交论坛讨论、协作软件开发和内容管理。此外，WebArena还集成了多种实用工具（例如地图）和外部知识库（例如用户手册），以鼓励类似人类的任务解决。<br><br>💡 创新点2：多样化的基准任务<br>基于WebArena环境，本文发布了一系列基准任务，重点关注评估任务完成的函数正确性。这些任务具有多样性、长时程，并旨在模拟人类在互联网上常规执行的任务。<br><br>💡 创新点3：基于结果的评估<br>本文使用了一种基于结果的评估方法，通过程序化验证每个任务的成功来评估任务的成功。这种方法比比较预测的动作序列与参考动作序列的文本表面形式更可靠，并且可以容纳实现相同目标的潜在有效路径。<br><br>## 📈 实验结果<br>本文使用WebArena基准测试了几个基线代理，这些代理集成了诸如推理前行动等最新技术。实验结果表明，解决复杂任务具有挑战性：最好的基于GPT-4的代理的端到端任务成功率仅为14.41%，远低于人类的78.24%。<br><br>## 💬 可借鉴之处<br>WebArena为构建和测试自主代理提供了一个高度真实和可复制的网络环境。它可以帮助研究人员开发更健壮和有效的代理，并评估它们在现实世界任务中的性能。此外，WebArena还可以用于测试和改进现有的交互式决策代理方法，例如分层规划、状态跟踪和错误恢复。</td>
    </tr>
    <tr>
      <th>46</th>
      <td>MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation</td>
      <td>Existing Multimodal Large Language Model (MLLM)-based agents face significant<br>challenges in handling complex GUI (Graphical User Interface) interactions on<br>devices. These challenges arise from the dynamic and structured nature of GUI<br>environments, which integrate text, images, and spatial relationships, as well<br>as the variability in action spaces across different pages and tasks. To<br>address these limitations, we propose MobA, a novel MLLM-based mobile assistant<br>system. MobA introduces an adaptive planning module that incorporates a<br>reflection mechanism for error recovery and dynamically adjusts plans to align<br>with the real environment contexts and action module's execution capacity.<br>Additionally, a multifaceted memory module provides comprehensive memory<br>support to enhance adaptability and efficiency. We also present MobBench, a<br>dataset designed for complex mobile interactions. Experimental results on<br>MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI<br>environments and perform complex mobile task.</td>
      <td>## 🌟 论文解读 | MobA：多面记忆增强自适应规划，助力高效移动任务自动化<br><br>## 📌 背景痛点/本文动机<br>随着多模态大型语言模型（MLLM）的快速发展，其在处理复杂GUI交互和满足多样化用户需求方面面临着重大挑战。GUI环境具有动态性和结构性，融合了文本、图像和空间关系，且不同页面和任务的动作空间存在差异。现有的MLLM-based智能体在处理这些挑战时，往往缺乏足够的适应性和效率。<br><br>## 🚀 核心方法<br>💡 创新点1：自适应规划模块<br>MOBA引入了自适应规划模块，该模块包含一个反思机制，用于从失败的子计划中恢复任务执行，并通过重新评估目标或将任务分解为更细粒度的子目标来动态调整计划。这种模块能够根据当前的GUI环境和动作执行器的容量，生成多粒度的任务计划，从而更好地适应环境并完成任务。<br><br>💡 创新点2：多面记忆模块<br>MOBA还提出了一个多面记忆模块，该模块提供分层记忆支持，以增强任务的适应性和效率。该模块包括任务记忆、应用记忆、页面记忆、动作记忆和用户记忆，能够存储历史数据，增强决策能力，并减少冗余动作。<br><br>## 📈 实验结果<br>MOBA在MOBBENCH和AndroidArena数据集上进行了评估，结果表明，MOBA在处理动态GUI环境和执行复杂移动任务方面表现出色。与现有方法相比，MOBA在任务完成率、里程碑得分和执行效率方面均有显著提升。<br><br>## 💬 可借鉴之处<br>MOBA的设计理念和方法为移动任务自动化领域提供了新的思路。其自适应规划和多面记忆模块的设计，为解决复杂GUI交互和多样化用户需求提供了有效的解决方案。此外，MOBA的实验结果也表明，MLLM在移动任务自动化领域具有巨大的潜力。</td>
    </tr>
    <tr>
      <th>47</th>
      <td>VGA: Vision GUI Assistant -- Minimizing Hallucinations through Image-Centric Fine-Tuning</td>
      <td>Recent advances in Large Vision-Language Models (LVLMs) have significantly<br>improve performance in image comprehension tasks, such as formatted charts and<br>rich-content images. Yet, Graphical User Interface (GUI) pose a greater<br>challenge due to their structured format and detailed textual information.<br>Existing LVLMs often overly depend on internal knowledge and neglect image<br>content, resulting in hallucinations and incorrect responses in GUI<br>comprehension. To address these issues, we introduce VGA, a fine-tuned model<br>designed for comprehensive GUI understanding. Our model aims to enhance the<br>interpretation of visual data of GUI and reduce hallucinations. We first<br>construct a Vision Question Answering (VQA) dataset of 63.8k high-quality<br>examples with our propose Referent Method, which ensures the model's responses<br>are highly depend on visual content within the image. We then design a<br>two-stage fine-tuning method called Foundation and Advanced Comprehension (FAC)<br>to enhance both the model's ability to extract information from image content<br>and alignment with human intent. Experiments show that our approach enhances<br>the model's ability to extract information from images and achieves<br>state-of-the-art results in GUI understanding tasks. Our dataset and<br>fine-tuning script will be released soon.</td>
      <td>## 🌟 论文解读 | VGA：基于图像的微调，减少视觉界面理解中的幻觉<br><br>## 📌 背景痛点/本文动机<br>随着大型视觉语言模型（LVLMs）在图像理解任务中的性能显著提升，图形用户界面（GUI）的理解却仍然是一个挑战。现有的LVLMs往往过度依赖内部知识，而忽视图像内容，导致在GUI理解任务中产生幻觉和错误响应。为了解决这个问题，本文提出了VGA，一个专门为GUI理解而设计的微调模型。<br><br>## 🚀 核心方法<br>💡 创新点1：构建大规模GUI数据集<br>为了微调模型，本文构建了一个包含63.8k高质量示例的视觉问答（VQA）数据集，并采用了一种称为“参照方法”的数据构建方法，确保模型的响应高度依赖于图像中的视觉内容。<br><br>💡 创新点2：两阶段微调方法<br>本文设计了一种名为“基础和高级理解”（FAC）的两阶段微调方法，以增强模型从图像内容中提取信息的能力，并使其与人类意图保持一致。基础阶段增强模型对GUI图像的理解，而高级阶段则提高模型根据对GUI的理解来回答复杂问题的能力。<br><br>## 📈 实验结果<br>实验结果表明，本文的方法显著提高了模型从图像中提取信息的能力，并在GUI理解任务中取得了最先进的结果。与基线模型相比，VGA在GUI理解基准测试中取得了最佳性能，并且在使用低分辨率输入时仍然表现出色。<br><br>## 💬 可借鉴之处<br>本文提出的VGA模型及其微调方法为LVLMs在GUI理解任务中的应用提供了新的思路。通过构建大规模GUI数据集和设计两阶段微调方法，可以有效减少LVLMs在GUI理解中的幻觉现象，提高模型的准确性和实用性。</td>
    </tr>
    <tr>
      <th>48</th>
      <td>AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</td>
      <td>Recent advancements in Large Language Models (LLMs) have led to the<br>development of intelligent LLM-based agents capable of interacting with<br>graphical user interfaces (GUIs). These agents demonstrate strong reasoning and<br>adaptability, enabling them to perform complex tasks that traditionally<br>required predefined rules. However, the reliance on step-by-step reasoning in<br>LLM-based agents often results in inefficiencies, particularly for routine<br>tasks. In contrast, traditional rule-based systems excel in efficiency but lack<br>the intelligence and flexibility to adapt to novel scenarios. To address this<br>challenge, we propose a novel evolutionary framework for GUI agents that<br>enhances operational efficiency while retaining intelligence and flexibility.<br>Our approach incorporates a memory mechanism that records the agent's task<br>execution history. By analyzing this history, the agent identifies repetitive<br>action sequences and evolves high-level actions that act as shortcuts,<br>replacing these low-level operations and improving efficiency. This allows the<br>agent to focus on tasks requiring more complex reasoning, while simplifying<br>routine actions. Experimental results on multiple benchmark tasks demonstrate<br>that our approach significantly outperforms existing methods in both efficiency<br>and accuracy. The code will be open-sourced to support further research.</td>
      <td>## 🌟 论文解读 | AppAgentX：让GUI智能体成为熟练的手机用户<br><br>## 📌 背景痛点/本文动机<br>近年来，大型语言模型（LLMs）的进步催生了能够与图形用户界面（GUIs）交互的智能LLM智能体。这些智能体展现出强大的推理和适应性，能够执行传统上需要预定义规则才能完成的复杂任务。然而，LLM智能体依赖于逐步推理，这往往导致效率低下，尤其是在例行任务中。相比之下，传统的基于规则的系统在效率方面表现出色，但缺乏智能和灵活性，无法适应新情况。为了解决这一挑战，本文提出了一种新的进化框架，用于GUI智能体，以提高操作效率，同时保持智能和灵活性。<br><br>## 🚀 核心方法<br>💡 创新点1：记忆机制<br>本文提出了一种记忆机制，用于记录智能体的任务执行历史。通过分析历史记录，智能体能够识别重复的动作序列，并进化出高级动作，作为快捷方式，替换低级操作，从而提高效率。这允许智能体专注于需要更复杂推理的任务，同时简化例行操作。<br><br>💡 创新点2：进化机制<br>本文设计了一个基于链的知识框架，用于记录和优化智能体的执行行为。该框架允许智能体从其过去的交互中学习，并动态地进化出更抽象的高级动作，消除了重复的低级操作的需求。具体来说，智能体会分析其执行历史，以识别重复的低智能动作，例如例行任务中涉及的动作。从这种分析中，智能体可以生成一个高级动作，封装一系列低级动作，使其能够更有效地执行任务。<br><br>## 📈 实验结果<br>在多个基准任务上的实验结果表明，本文提出的方法在效率和准确性方面都显著优于现有方法。具体来说，与基线模型相比，本文的方法将平均步骤数从9.1减少到5.7，将步骤执行时间从23秒减少到16秒，并将平均令牌消耗从9.26k减少到4.94k。此外，平均成功率从70.8%提高到71.4%。这些结果表明，本文的方法能够有效地提高智能体的性能，同时减少计算开销。<br><br>## 💬 可借鉴之处<br>本文提出的进化框架为GUI智能体的发展提供了新的思路。通过引入记忆机制和进化机制，智能体能够从过去的交互中学习，并动态地进化出更抽象的高级动作，从而提高效率和准确性。此外，本文提出的基于链的知识框架也为智能体的行为优化提供了新的方法。这些创新点对于GUI智能体的发展具有重要的意义，并为未来的研究提供了新的方向。</td>
    </tr>
    <tr>
      <th>49</th>
      <td>Towards Trustworthy GUI Agents: A Survey</td>
      <td>GUI agents, powered by large foundation models, can interact with digital<br>interfaces, enabling various applications in web automation, mobile navigation,<br>and software testing. However, their increasing autonomy has raised critical<br>concerns about their security, privacy, and safety. This survey examines the<br>trustworthiness of GUI agents in five critical dimensions: security<br>vulnerabilities, reliability in dynamic environments, transparency and<br>explainability, ethical considerations, and evaluation methodologies. We also<br>identify major challenges such as vulnerability to adversarial attacks,<br>cascading failure modes in sequential decision-making, and a lack of realistic<br>evaluation benchmarks. These issues not only hinder real-world deployment but<br>also call for comprehensive mitigation strategies beyond task success. As GUI<br>agents become more widespread, establishing robust safety standards and<br>responsible development practices is essential. This survey provides a<br>foundation for advancing trustworthy GUI agents through systematic<br>understanding and future research.</td>
      <td>## 🌟 论文解读 | 构建可信的GUI智能体：一项调查<br><br>## 📌 背景痛点/本文动机<br>随着大型语言模型（LLMs）和大型多模态模型（LMMs）的快速发展，图形用户界面（GUI）智能体已经从简单的问答工具转变为能够与数字界面交互的智能体。这些智能体在网页自动化、移动导航和软件测试等领域有着广泛的应用。然而，随着它们自主性的提高，关于其安全性、隐私和安全的担忧也日益增加。本文旨在探讨GUI智能体的可信度，并从五个关键维度进行分析：安全漏洞、动态环境中的可靠性、透明度和可解释性、伦理考虑和评估方法。<br><br>## 🚀 核心方法<br>本文的核心方法是对GUI智能体的可信度进行系统性的调查和分析。作者从五个关键维度对GUI智能体的可信度进行了深入探讨，并提出了相应的解决方案和未来研究方向。<br><br>💡 创新点1：本文首次将GUI智能体的可信度问题进行了系统性的调查和分析，从五个关键维度进行了深入探讨，为构建可信的GUI智能体提供了理论基础。<br><br>💡 创新点2：本文提出了多种解决方案和未来研究方向，包括更智能的防御工具、用户控制的隐私、连接的防御层、实时幻觉预防、自适应安全架构、从失败中学习、交互式解释工具、上下文自适应解释、文化和社会意识、政策影响、指南和原则等。<br><br>## 📈 实验结果<br>本文并未进行具体的实验，而是对现有的研究成果进行了总结和分析。作者通过调查和分析，揭示了GUI智能体在可信度方面存在的挑战和问题，并提出了相应的解决方案和未来研究方向。<br><br>## 💬 可借鉴之处<br>本文的研究成果对于构建可信的GUI智能体具有重要的参考价值。研究人员和开发者可以借鉴本文提出的解决方案和未来研究方向，设计更安全、可靠、透明和符合伦理的GUI智能体。同时，本文也为GUI智能体的评估和测试提供了新的思路和方法，有助于推动GUI智能体领域的发展。</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        