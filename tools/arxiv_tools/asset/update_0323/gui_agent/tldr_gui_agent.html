
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Sticky Table Headers and First Column</title>
            <style>
            th {
                background-color: #fff;
                position: sticky;
                top: 0;
                z-index: 1;
            }
            th:before {
                content: "";
                position: absolute;
                top: -1px;
                bottom: -1px;
                left: -1px;
                right: -1px;
                border: 1px solid LightGrey;
                z-index: -1;
            }
        </style>
        
            <script id="MathJax-script" async src="https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-chtml.js"></script>
            </head>
            <body>
                <div class="div1">
            <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>summary</th>
      <th>llm_summary_res</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Agents: An Open-source Framework for Autonomous Language Agents</td>
      <td>Recent advances on large language models (LLMs) enable researchers and<br>developers to build autonomous language agents that can automatically solve<br>various tasks and interact with environments, humans, and other agents using<br>natural language interfaces. We consider language agents as a promising<br>direction towards artificial general intelligence and release Agents, an<br>open-source library with the goal of opening up these advances to a wider<br>non-specialist audience. Agents is carefully engineered to support important<br>features including planning, memory, tool usage, multi-agent communication, and<br>fine-grained symbolic control. Agents is user-friendly as it enables<br>non-specialists to build, customize, test, tune, and deploy state-of-the-art<br>autonomous language agents without much coding. The library is also<br>research-friendly as its modularized design makes it easily extensible for<br>researchers. Agents is available at https://github.com/aiwaves-cn/agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¼€æºæ¡†æ¶ Agentsï¼šæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†çš„åˆ©å™¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…èƒ½å¤Ÿæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿè‡ªåŠ¨è§£å†³å„ç§ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€ç•Œé¢ä¸ç¯å¢ƒã€äººç±»å’Œå…¶ä»–ä»£ç†è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­è¨€ä»£ç†æ¡†æ¶å¾€å¾€ç¼ºä¹æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œéš¾ä»¥æ»¡è¶³éä¸“ä¸šäººå£«çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† Agentsï¼Œä¸€ä¸ªå¼€æºçš„è‡ªä¸»è¯­è¨€ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨è®©æ›´å¹¿æ³›çš„éä¸“ä¸šäººå£«èƒ½å¤Ÿè½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²æœ€å…ˆè¿›çš„è‡ªä¸»è¯­è¨€ä»£ç†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ”¯æŒå…³é”®åŠŸèƒ½<br>Agents æ¡†æ¶ç²¾å¿ƒè®¾è®¡ï¼Œæ”¯æŒè§„åˆ’ã€è®°å¿†ã€å·¥å…·ä½¿ç”¨ã€å¤šä»£ç†é€šä¿¡å’Œç»†ç²’åº¦ç¬¦å·æ§åˆ¶ç­‰å…³é”®åŠŸèƒ½ã€‚è¿™ä½¿å¾—è¯­è¨€ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å„ç§ä»»åŠ¡å’Œç¯å¢ƒã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§<br>Agents æ¡†æ¶çš„ç”¨æˆ·å‹å¥½æ€§ä½“ç°åœ¨å…¶å…è®¸éä¸“ä¸šäººå£«è½»æ¾æ„å»ºã€å®šåˆ¶ã€æµ‹è¯•ã€è°ƒæ•´å’Œéƒ¨ç½²è‡ªä¸»è¯­è¨€ä»£ç†ï¼Œè€Œæ— éœ€å¤§é‡ç¼–ç ã€‚åŒæ—¶ï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾æ‰©å±•æ¡†æ¶ï¼Œä»¥æ»¡è¶³ä»–ä»¬çš„ç ”ç©¶éœ€æ±‚ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAgent Hub å¹³å°<br>Agents æ¡†æ¶å¼•å…¥äº† Agent Hub å¹³å°ï¼Œå…è®¸ç”¨æˆ·åˆ†äº«ä»–ä»¬å¾®è°ƒçš„è¯­è¨€ä»£ç†ï¼Œå¹¶æœç´¢/ä¸‹è½½å…¶ä»–ç”¨æˆ·åˆ†äº«çš„æœ‰ç”¨è¯­è¨€ä»£ç†ã€‚è¿™å¤§å¤§é™ä½äº†ä»å¤´å¼€å§‹è®¾è®¡å’Œè°ƒæ•´è¯­è¨€ä»£ç†çš„éš¾åº¦ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè‡ªåŠ¨åˆ›å»ºä»£ç†ç³»ç»Ÿ<br>ä¸ºäº†å‡å°‘ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®š SOP çš„ç¹çå·¥ä½œï¼ŒAgents æ¡†æ¶å®ç°äº†ä¸€ä¸ªè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹ã€‚è¯¥æµç¨‹åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)ï¼Œå¯ä»¥è‡ªåŠ¨åˆ›å»ºå…¶ä»–ä»£ç†å’Œå¤šä»£ç†ç³»ç»Ÿã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>è®ºæ–‡å±•ç¤ºäº†ä½¿ç”¨ Agents æ¡†æ¶æ„å»ºçš„å•ä»£ç†ç³»ç»Ÿå’Œå¤šä»£ç†ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒåŒ…æ‹¬é—²èŠæœºå™¨äººã€åŸºäºçŸ¥è¯†åº“å’Œæœç´¢å¼•æ“çš„å®¢æˆ·æœåŠ¡ä»£ç†ã€è´­ç‰©åŠ©æ‰‹ä»£ç†å’Œé”€å”®ä»£ç†ç­‰ã€‚è¿™äº›æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº† Agents æ¡†æ¶çš„æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠæ„å»ºå„ç§ç”¨ä¾‹çš„è¯­è¨€ä»£ç†çš„å¯èƒ½æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>Agents æ¡†æ¶ä¸ºæ„å»ºè‡ªä¸»è¯­è¨€ä»£ç†æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå…¶æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…çš„ç†æƒ³é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒAgent Hub å¹³å°å’Œè‡ªåŠ¨ SOP ç”Ÿæˆæµç¨‹è¿›ä¸€æ­¥æé«˜äº†æ¡†æ¶çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚</td>
    </tr>
    <tr>
      <th>1</th>
      <td>DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents</td>
      <td>On-device control agents, especially on mobile devices, are responsible for<br>operating mobile devices to fulfill users' requests, enabling seamless and<br>intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)<br>into these agents enhances their ability to understand and execute complex<br>commands, thereby improving user experience. However, fine-tuning MLLMs for<br>on-device control presents significant challenges due to limited data<br>availability and inefficient online training processes. This paper introduces<br>DistRL, a novel framework designed to enhance the efficiency of online RL<br>fine-tuning for mobile device control agents. DistRL employs centralized<br>training and decentralized data acquisition to ensure efficient fine-tuning in<br>the context of dynamic online interactions. Additionally, the framework is<br>backed by our tailor-made RL algorithm, which effectively balances exploration<br>with the prioritized utilization of collected data to ensure stable and robust<br>training. Our experiments show that, on average, DistRL delivers a 3X<br>improvement in training efficiency and enables training data collection 2.4X<br>faster than the leading synchronous multi-machine methods. Notably, after<br>training, DistRL achieves a 20% relative improvement in success rate compared<br>to state-of-the-art methods on general Android tasks from an open benchmark,<br>significantly outperforming existing approaches while maintaining the same<br>training time. These results validate DistRL as a scalable and efficient<br>solution, offering substantial improvements in both training efficiency and<br>agent performance for real-world, in-the-wild device control tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | DistRLï¼šæå‡ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ•ˆç‡çš„åˆ†å¸ƒå¼æ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…´èµ·ï¼Œå°†å®ƒä»¬é›†æˆåˆ°ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†ä¸­ï¼Œå¯ä»¥æ˜¾è‘—æå‡ä»£ç†ç†è§£å’Œæ‰§è¡Œå¤æ‚å‘½ä»¤çš„èƒ½åŠ›ï¼Œä»è€Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿›è¡ŒMLLMsçš„å¾®è°ƒé¢ä¸´ç€æ•°æ®å¯ç”¨æ€§æœ‰é™å’Œåœ¨çº¿è®­ç»ƒè¿‡ç¨‹æ•ˆç‡ä½ä¸‹ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DistRLï¼Œä¸€ä¸ªæ—¨åœ¨æé«˜ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒæ•ˆç‡çš„åˆ†å¸ƒå¼æ¡†æ¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¯æ‰©å±•çš„å¼‚æ­¥æ•°æ®é‡‡é›†æ¶æ„<br>DistRLé‡‡ç”¨è§£è€¦å’Œå¼‚æ­¥çš„æ¡†æ¶ï¼Œå°†RLä»£ç†éƒ¨ç½²åœ¨å¼‚æ„çš„workerè®¾å¤‡ä¸Šè¿›è¡Œè¿œç¨‹æ•°æ®é‡‡é›†ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå…ˆè¿›çš„é›†ä¸­å¼è®­ç»ƒRLç®—æ³•<br>DistRLå¼€å‘äº†A-RIDEç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åˆ†å¸ƒå¼å’Œå¼‚æ­¥æ•°æ®åˆ©ç”¨çš„ç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒä¼˜å…ˆè€ƒè™‘é‡è¦çš„ç»éªŒï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡ï¼ŒåŒæ—¶é¼“åŠ±æ¢ç´¢ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒDistRLåœ¨è®­ç»ƒæ•ˆç‡æ–¹é¢å¹³å‡æé«˜äº†3å€ï¼Œå¹¶ä¸”æ¯”é¢†å…ˆçš„åŒæ­¥å¤šæœºæ–¹æ³•å¿«2.4å€ã€‚åœ¨è®­ç»ƒåï¼ŒDistRLåœ¨å¼€æ”¾åŸºå‡†æµ‹è¯•ä¸­çš„ä¸€èˆ¬Androidä»»åŠ¡ä¸Šçš„æˆåŠŸç‡æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†20%ï¼Œåœ¨ä¿æŒç›¸åŒè®­ç»ƒæ—¶é—´çš„åŒæ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DistRLæ¡†æ¶ä¸ºç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»£ç†çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒæä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚å…¶å¼‚æ­¥æ•°æ®é‡‡é›†å’Œåˆ†å¸ƒå¼è®­ç»ƒçš„è®¾è®¡ï¼Œä»¥åŠA-RIDEç®—æ³•çš„åº”ç”¨ï¼Œä¸ºè§£å†³ç§»åŠ¨è®¾å¤‡ä¸ŠRLå¾®è°ƒçš„æŒ‘æˆ˜æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼ŒDistRLæ¡†æ¶çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶é€‚ç”¨äºå„ç§ç§»åŠ¨è®¾å¤‡æ§åˆ¶ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</td>
    </tr>
    <tr>
      <th>2</th>
      <td>WebGPT: Browser-assisted question-answering with human feedback</td>
      <td>We fine-tune GPT-3 to answer long-form questions using a text-based<br>web-browsing environment, which allows the model to search and navigate the<br>web. By setting up the task so that it can be performed by humans, we are able<br>to train models on the task using imitation learning, and then optimize answer<br>quality with human feedback. To make human evaluation of factual accuracy<br>easier, models must collect references while browsing in support of their<br>answers. We train and evaluate our models on ELI5, a dataset of questions asked<br>by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior<br>cloning, and then performing rejection sampling against a reward model trained<br>to predict human preferences. This model's answers are preferred by humans 56%<br>of the time to those of our human demonstrators, and 69% of the time to the<br>highest-voted answer from Reddit.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebGPTï¼šæµè§ˆå™¨è¾…åŠ©çš„é—®é¢˜å›ç­”ä¸äººç±»åé¦ˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å‘å±•ï¼Œé•¿ç¯‡é—®é¢˜å›ç­”ï¼ˆLFQAï¼‰æˆä¸ºäº†ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç±»ç³»ç»Ÿæœ‰æ½œåŠ›æˆä¸ºäººä»¬äº†è§£ä¸–ç•Œçš„ä¸»è¦æ–¹å¼ï¼Œä½†ç›®å‰å…¶æ€§èƒ½ä»è½åäºäººç±»ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æ–¹æ³•ï¼Œæå‡é•¿ç¯‡é—®é¢˜å›ç­”çš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1<br>æœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„æµè§ˆå™¨ç¯å¢ƒï¼Œä½¿å¾—ç»è¿‡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¸ä¹‹äº’åŠ¨ã€‚è¿™å…è®¸æˆ‘ä»¬ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç­‰é€šç”¨æ–¹æ³•ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼æ”¹è¿›æ£€ç´¢å’Œç”Ÿæˆã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2<br>æ¨¡å‹åœ¨æµè§ˆè¿‡ç¨‹ä¸­ç”Ÿæˆå¸¦æœ‰å¼•ç”¨çš„å›ç­”ï¼Œå³ä»ç½‘é¡µä¸­æå–çš„æ–‡æœ¬ç‰‡æ®µã€‚è¿™ä¸ºè¯„ä¼°ç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§æä¾›äº†ä¾¿åˆ©ï¼Œæ— éœ€è¯„ä¼°è€…è¿›è¡Œç‹¬ç«‹è€Œå›°éš¾çš„ç ”ç©¶ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ELI5æ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚æœ€ä½³æ¨¡å‹é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’ŒåŸºäºäººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œæ‹’ç»æŠ½æ ·å¾—åˆ°ã€‚åœ¨ä¸‰ç§è¯„ä¼°æ–¹å¼ä¸­ï¼Œè¯¥æ¨¡å‹çš„è¡¨ç°å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸”åœ¨äº‹å®æ€§å’Œä¿¡æ¯æ€§æ–¹é¢æœ‰æ‰€æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„æ–¹æ³•è¡¨æ˜ï¼Œé€šè¿‡ç»“åˆå¼ºå¤§çš„æœç´¢å·¥å…·å’Œæ¨¡ä»¿å­¦ä¹ ï¼Œå¯ä»¥æ˜¾è‘—æå‡é•¿ç¯‡é—®é¢˜å›ç­”çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œå¼•å…¥äººç±»åé¦ˆè¿›è¡Œä¼˜åŒ–ï¼Œæœ‰åŠ©äºæ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´æœ‰ç”¨çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œå¸¦å¼•ç”¨çš„å›ç­”æ–¹å¼ä¸ºè¯„ä¼°ç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration</td>
      <td>Reinforcement learning (RL) agents improve through trial-and-error, but when<br>reward is sparse and the agent cannot discover successful action sequences,<br>learning stagnates. This has been a notable problem in training deep RL agents<br>to perform web-based tasks, such as booking flights or replying to emails,<br>where a single mistake can ruin the entire sequence of actions. A common remedy<br>is to "warm-start" the agent by pre-training it to mimic expert demonstrations,<br>but this is prone to overfitting. Instead, we propose to constrain exploration<br>using demonstrations. From each demonstration, we induce high-level "workflows"<br>which constrain the allowable actions at each time step to be similar to those<br>in the demonstration (e.g., "Step 1: click on a textbox; Step 2: enter some<br>text"). Our exploration policy then learns to identify successful workflows and<br>samples actions that satisfy these workflows. Workflows prune out bad<br>exploration directions and accelerate the agent's ability to discover rewards.<br>We use our approach to train a novel neural policy designed to handle the<br>semi-structured nature of websites, and evaluate on a suite of web tasks,<br>including the recent World of Bits benchmark. We achieve new state-of-the-art<br>results, and show that workflow-guided exploration improves sample efficiency<br>over behavioral cloning by more than 100x.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ä½¿ç”¨å·¥ä½œæµå¼•å¯¼æ¢ç´¢çš„Webç•Œé¢å¼ºåŒ–å­¦ä¹ <br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†é€šè¿‡è¯•é”™æ¥æé«˜æ€§èƒ½ï¼Œä½†åœ¨å¥–åŠ±ç¨€ç–ä¸”ä»£ç†æ— æ³•å‘ç°æˆåŠŸçš„åŠ¨ä½œåºåˆ—æ—¶ï¼Œå­¦ä¹ ä¼šåœæ»ä¸å‰ã€‚è¿™åœ¨è®­ç»ƒæ·±åº¦RLä»£ç†æ‰§è¡ŒåŸºäºWebçš„ä»»åŠ¡ï¼ˆå¦‚é¢„è®¢èˆªç­æˆ–å›å¤ç”µå­é‚®ä»¶ï¼‰æ—¶æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„é—®é¢˜ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªé”™è¯¯å¯èƒ½ä¼šç ´åæ•´ä¸ªåŠ¨ä½œåºåˆ—ã€‚ä¸€ä¸ªå¸¸è§çš„è¡¥æ•‘æªæ–½æ˜¯é€šè¿‡é¢„è®­ç»ƒä»£ç†æ¥æ¨¡ä»¿ä¸“å®¶æ¼”ç¤ºæ¥â€œé¢„çƒ­â€ä»£ç†ï¼Œä½†è¿™å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆã€‚ç›¸åï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨æ¼”ç¤ºæ¥çº¦æŸæ¢ç´¢ã€‚ä»æ¯ä¸ªæ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬è¯±å¯¼å‡ºé«˜çº§â€œå·¥ä½œæµâ€ï¼Œè¿™äº›å·¥ä½œæµå°†æ¯ä¸ªæ—¶é—´æ­¥å…è®¸çš„åŠ¨ä½œé™åˆ¶ä¸ºä¸æ¼”ç¤ºä¸­çš„åŠ¨ä½œç›¸ä¼¼ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­¥éª¤1ï¼šç‚¹å‡»æ–‡æœ¬æ¡†ï¼›æ­¥éª¤2ï¼šè¾“å…¥ä¸€äº›æ–‡æœ¬â€ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„æ¢ç´¢ç­–ç•¥å­¦ä¹ è¯†åˆ«æˆåŠŸçš„å·¥ä½œæµï¼Œå¹¶é‡‡æ ·æ»¡è¶³è¿™äº›å·¥ä½œæµçš„åŠ¨ä½œã€‚å·¥ä½œæµå‰ªé™¤äº†ä¸è‰¯çš„æ¢ç´¢æ–¹å‘ï¼Œå¹¶åŠ é€Ÿäº†ä»£ç†å‘ç°å¥–åŠ±çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•æ¥è®­ç»ƒä¸€ä¸ªæ–°é¢–çš„ç¥ç»ç­–ç•¥ï¼Œæ—¨åœ¨å¤„ç†ç½‘ç«™çš„åŠç»“æ„åŒ–æ€§è´¨ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—Webä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬æœ€è¿‘çš„World of BitsåŸºå‡†ã€‚æˆ‘ä»¬å–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶è¡¨æ˜å·¥ä½œæµå¼•å¯¼çš„æ¢ç´¢æ¯”è¡Œä¸ºå…‹éš†çš„æ ·æœ¬æ•ˆç‡æé«˜äº†100å€ä»¥ä¸Šã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå·¥ä½œæµå¼•å¯¼æ¢ç´¢ï¼ˆWGEï¼‰æ¡†æ¶<br>1. ä»æ¯ä¸ªæ¼”ç¤ºä¸­æå–ä¸æ¼”ç¤ºä¸­è§‚å¯Ÿåˆ°çš„åŠ¨ä½œä¸€è‡´çš„å·¥ä½œæµæ ¼ç½‘ã€‚<br>2. å®šä¹‰ä¸€ä¸ªå·¥ä½œæµæ¢ç´¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡é¦–å…ˆé€‰æ‹©ä¸€ä¸ªå·¥ä½œæµï¼Œç„¶åé‡‡æ ·ç¬¦åˆå·¥ä½œæµçš„åŠ¨ä½œæ¥è¿›è¡Œæ¢ç´¢ã€‚è¯¥ç­–ç•¥é€šè¿‡å¼ºåŒ–å­¦ä¹ é€æ¸å­¦ä¹ é€‰æ‹©å“ªä¸ªå·¥ä½œæµã€‚<br>3. åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­å‘ç°çš„å¥–åŠ±èµšå–çš„å‰§é›†è¿›å…¥é‡æ”¾ç¼“å†²åŒºï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªæ›´å¼ºå¤§å’Œè¡¨è¾¾æ€§æ›´å¼ºçš„ç¥ç»ç½‘ç»œç­–ç•¥ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDOMNETç¥ç»ç½‘ç»œç­–ç•¥<br>ä¸ºäº†å¤„ç†ç½‘ç«™çš„åŠç»“æ„åŒ–æ€§è´¨ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¥ç»ç½‘ç»œç­–ç•¥ï¼ˆDOMNETï¼‰ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿçµæ´»åœ°å¯¹ç½‘ç«™çš„æ ‘çŠ¶HTMLè¡¨ç¤ºè¿›è¡Œå…³ç³»æ¨ç†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ä¸€ç³»åˆ—Webäº¤äº’ä»»åŠ¡ä¸Šè¯„ä¼°äº†å·¥ä½œæµå¼•å¯¼çš„æ¢ç´¢å’ŒDOMNETï¼ŒåŒ…æ‹¬MiniWoBåŸºå‡†ã€é˜¿æ‹‰æ–¯åŠ èˆªç©ºå…¬å¸çš„èˆªç­é¢„è®¢ç•Œé¢ä»¥åŠä¸€ç³»åˆ—æ–°æ„å»ºçš„ä»»åŠ¡ã€‚ä¸å…ˆå‰åœ¨MiniWoBä¸Šçš„ç»“æœç›¸æ¯”ï¼Œæœ¬æ–‡çš„ç³»ç»Ÿåœ¨æ¯é¡¹ä»»åŠ¡ä¸Šä»…ä½¿ç”¨3-10ä¸ªæ¼”ç¤ºå°±å–å¾—äº†æ›´é«˜çš„æˆåŠŸç‡ï¼Œå¹¶å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„å·¥ä½œæµå¼•å¯¼çš„æ¢ç´¢æ¡†æ¶å’ŒDOMNETç¥ç»ç½‘ç»œç­–ç•¥ä¸ºåœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­è®­ç»ƒæ·±åº¦RLä»£ç†æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ¼”ç¤ºæ¥çº¦æŸæ¢ç´¢ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„ç¥ç»ç½‘ç»œç­–ç•¥æ¥å¤„ç†ç½‘ç«™çš„åŠç»“æ„åŒ–æ€§è´¨ï¼Œä»è€Œæé«˜äº†æ ·æœ¬æ•ˆç‡å’ŒæˆåŠŸç‡ã€‚</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Neuro-Symbolic World Models for Adapting to Open World Novelty</td>
      <td>Open-world novelty--a sudden change in the mechanics or properties of an<br>environment--is a common occurrence in the real world. Novelty adaptation is an<br>agent's ability to improve its policy performance post-novelty. Most<br>reinforcement learning (RL) methods assume that the world is a closed, fixed<br>process. Consequentially, RL policies adapt inefficiently to novelties. To<br>address this, we introduce WorldCloner, an end-to-end trainable neuro-symbolic<br>world model for rapid novelty adaptation. WorldCloner learns an efficient<br>symbolic representation of the pre-novelty environment transitions, and uses<br>this transition model to detect novelty and efficiently adapt to novelty in a<br>single-shot fashion. Additionally, WorldCloner augments the policy learning<br>process using imagination-based adaptation, where the world model simulates<br>transitions of the post-novelty environment to help the policy adapt. By<br>blending ''imagined'' transitions with interactions in the post-novelty<br>environment, performance can be recovered with fewer total environment<br>interactions. Using environments designed for studying novelty in sequential<br>decision-making problems, we show that the symbolic world model helps its<br>neural policy adapt more efficiently than model-based and model-based<br>neural-only reinforcement learning methods.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WorldClonerï¼šå¿«é€Ÿé€‚åº”å¼€æ”¾ä¸–ç•Œæ–°å¥‡çš„ç¥ç»ç¬¦å·ä¸–ç•Œæ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œå¼€æ”¾ä¸–ç•Œçš„æ–°å¥‡æ€§â€”â€”å³ç¯å¢ƒæœºåˆ¶æˆ–å±æ€§çš„çªç„¶å˜åŒ–â€”â€”æ˜¯ä¸€ç§å¸¸è§çš„ç°è±¡ã€‚æ–°å¥‡æ€§é€‚åº”æ˜¯æŒ‡ä»£ç†åœ¨æ–°å¥‡æ€§ä¹‹åæé«˜å…¶ç­–ç•¥æ€§èƒ½çš„èƒ½åŠ›ã€‚å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å‡è®¾ä¸–ç•Œæ˜¯å°é—­çš„ã€å›ºå®šçš„è¿‡ç¨‹ã€‚å› æ­¤ï¼ŒRLç­–ç•¥å¯¹æ–°å¥‡æ€§çš„é€‚åº”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†WorldClonerï¼Œä¸€ä¸ªç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ç¥ç»ç¬¦å·ä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿæ–°å¥‡æ€§é€‚åº”ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç¬¦å·ä¸–ç•Œæ¨¡å‹<br>WorldClonerå­¦ä¹ ä¸€ä¸ªé«˜æ•ˆçš„ç¬¦å·è¡¨ç¤ºï¼Œç”¨äºè¡¨ç¤ºæ–°å¥‡æ€§ä¹‹å‰çš„ç¯å¢ƒè½¬æ¢ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªè½¬æ¢æ¨¡å‹æ¥æ£€æµ‹æ–°å¥‡æ€§ï¼Œå¹¶ä»¥å•æ¬¡æ–¹å¼é«˜æ•ˆåœ°é€‚åº”æ–°å¥‡æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŸºäºæƒ³è±¡çš„é€‚åº”<br>WorldClonerä½¿ç”¨åŸºäºæƒ³è±¡çš„é€‚åº”æ¥å¢å¼ºç­–ç•¥å­¦ä¹ è¿‡ç¨‹ï¼Œå…¶ä¸­ä¸–ç•Œæ¨¡å‹æ¨¡æ‹Ÿæ–°å¥‡æ€§ä¹‹åçš„ç¯å¢ƒè½¬æ¢ï¼Œä»¥å¸®åŠ©ç­–ç•¥é€‚åº”ã€‚é€šè¿‡å°†â€œæƒ³è±¡â€çš„è½¬æ¢ä¸æ–°å¥‡æ€§ä¹‹åçš„ç¯å¢ƒä¸­çš„äº¤äº’ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨æ›´å°‘çš„æ€»ç¯å¢ƒäº¤äº’ä¸­æ¢å¤æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ä¸“é—¨ä¸ºç ”ç©¶åºåˆ—å†³ç­–é—®é¢˜ä¸­çš„æ–°å¥‡æ€§è€Œè®¾è®¡çš„ç¯å¢ƒä¸­ï¼Œæœ¬æ–‡å±•ç¤ºäº†ç¬¦å·ä¸–ç•Œæ¨¡å‹å¸®åŠ©å…¶ç¥ç»ç­–ç•¥æ¯”åŸºäºæ¨¡å‹å’ŒåŸºäºæ¨¡å‹çš„ç¥ç»å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ›´æœ‰æ•ˆåœ°é€‚åº”ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„WorldCloneræ¨¡å‹ä¸ºå¼€æ”¾ä¸–ç•Œå­¦ä¹ ä¸­çš„æ–°å¥‡æ€§é€‚åº”æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å…¶ç¬¦å·ä¸–ç•Œæ¨¡å‹å’ŒåŸºäºæƒ³è±¡çš„é€‚åº”æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜ä»£ç†å¯¹æ–°å¥‡æ€§çš„é€‚åº”æ•ˆç‡ï¼Œå‡å°‘å¯¹çœŸå®ç¯å¢ƒäº¤äº’çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿè¡¨æ˜ï¼Œç¬¦å·è¡¨ç¤ºæ˜¯é€‚åº”å¼ºåŒ–å­¦ä¹ ä»£ç†æ–°å¥‡æ€§çš„æœ‰æ•ˆè¡¥å……ã€‚</td>
    </tr>
    <tr>
      <th>5</th>
      <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
      <td>Existing benchmarks for grounding language in interactive environments either<br>lack real-world linguistic elements, or prove difficult to scale up due to<br>substantial human involvement in the collection of data or feedback signals. To<br>bridge this gap, we develop WebShop -- a simulated e-commerce website<br>environment with \( 1.18 \) million real-world products and \( 12,087 \) crowd-sourced<br>text instructions. Given a text instruction specifying a product requirement,<br>an agent needs to navigate multiple types of webpages and issue diverse actions<br>to find, customize, and purchase an item. WebShop provides several challenges<br>for language grounding including understanding compositional instructions,<br>query (re-)formulation, comprehending and acting on noisy text in webpages, and<br>performing strategic exploration. We collect over \( 1,600 \) human demonstrations<br>for the task, and train and evaluate a diverse range of agents using<br>reinforcement learning, imitation learning, and pre-trained image and language<br>models. Our best model achieves a task success rate of \( 29\% \), which<br>outperforms rule-based heuristics (\( 9.6\% \)) but is far lower than human expert<br>performance (\( 59\% \)). We also analyze agent and human trajectories and ablate<br>various model components to provide insights for developing future agents with<br>stronger language understanding and decision making abilities. Finally, we show<br>that agents trained on WebShop exhibit non-trivial sim-to-real transfer when<br>evaluated on amazon.com and ebay.com, indicating the potential value of WebShop<br>in developing practical web-based agents that can operate in the wild.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebShopï¼šè¿ˆå‘å¯æ‰©å±•çš„åŸºäºè¯­è¨€äº¤äº’çš„Webç¯å¢ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç°æœ‰çš„è¯­è¨€äº¤äº’ç¯å¢ƒåœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œè¯­è¨€å…ƒç´ æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæˆ–è€…ç”±äºå¤§é‡äººå·¥å‚ä¸æ•°æ®æ”¶é›†æˆ–åé¦ˆä¿¡å·è€Œéš¾ä»¥æ‰©å±•ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†WebShopï¼Œä¸€ä¸ªå…·æœ‰118ä¸‡çœŸå®ä¸–ç•Œäº§å“å’Œ12087ä¸ªä¼—åŒ…æ–‡æœ¬æŒ‡ä»¤çš„æ¨¡æ‹Ÿç”µå­å•†åŠ¡ç½‘ç«™ç¯å¢ƒã€‚WebShopä¸ºè¯­è¨€æ¥åœ°æä¾›äº†å‡ ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç†è§£ç»„åˆæŒ‡ä»¤ã€æŸ¥è¯¢ï¼ˆé‡æ–°ï¼‰åˆ¶å®šã€ç†è§£å’Œåœ¨ç½‘é¡µä¸­æ‰§è¡Œå™ªå£°æ–‡æœ¬ï¼Œä»¥åŠæ‰§è¡Œæˆ˜ç•¥æ¢ç´¢ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWebShopç¯å¢ƒ<br>WebShopæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿç”µå­å•†åŠ¡ç½‘ç«™çš„ç¯å¢ƒï¼Œå…·æœ‰è¶…è¿‡118ä¸‡çœŸå®ä¸–ç•Œäº§å“å’Œ12087ä¸ªä¼—åŒ…æ–‡æœ¬æŒ‡ä»¤ã€‚ç»™å®šä¸€ä¸ªæŒ‡å®šäº§å“è¦æ±‚çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œä»£ç†éœ€è¦å¯¼èˆªå¤šç§ç±»å‹çš„ç½‘é¡µå¹¶å‘å‡ºå¤šç§åŠ¨ä½œæ¥æŸ¥æ‰¾ã€å®šåˆ¶å’Œè´­ä¹°å•†å“ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šç§å­¦ä¹ æ–¹æ³•<br>æœ¬æ–‡æ”¶é›†äº†è¶…è¿‡1600ä¸ªäººç±»æ¼”ç¤ºï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ã€æ¨¡ä»¿å­¦ä¹ å’Œé¢„è®­ç»ƒçš„å›¾åƒå’Œè¯­è¨€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°äº†å„ç§ä»£ç†ã€‚æœ€ä½³æ¨¡å‹çš„ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ°äº†29%ï¼Œè¶…è¿‡äº†åŸºäºè§„åˆ™çš„å¯å‘å¼æ–¹æ³•ï¼ˆ9.6%ï¼‰ï¼Œä½†è¿œä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ï¼ˆ59%ï¼‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ¨¡å‹åˆ†æå’Œæ¶ˆèç ”ç©¶<br>æœ¬æ–‡åˆ†æäº†ä»£ç†å’Œäººç±»è½¨è¿¹ï¼Œå¹¶æ¶ˆèäº†å„ç§æ¨¡å‹ç»„ä»¶ï¼Œä»¥æä¾›æœ‰å…³å¼€å‘å…·æœ‰æ›´å¼ºè¯­è¨€ç†è§£å’Œå†³ç­–èƒ½åŠ›çš„æœªæ¥ä»£ç†çš„è§è§£ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šSim-to-realè¿ç§»<br>æœ€åï¼Œæœ¬æ–‡è¡¨æ˜ï¼Œåœ¨WebShopä¸Šè®­ç»ƒçš„ä»£ç†åœ¨amazon.comå’Œebay.comä¸Šè¿›è¡Œè¯„ä¼°æ—¶è¡¨ç°å‡ºéå¹³å‡¡çš„Sim-to-realè¿ç§»ï¼Œè¡¨æ˜WebShopåœ¨å¼€å‘èƒ½å¤Ÿåœ¨é‡å¤–æ“ä½œçš„å®ç”¨Webä»£ç†æ–¹é¢çš„æ½œåœ¨ä»·å€¼ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæœ€ä½³æ¨¡å‹çš„ä»»åŠ¡æˆåŠŸç‡ä¸º29%ï¼Œè¶…è¿‡äº†åŸºäºè§„åˆ™çš„å¯å‘å¼æ–¹æ³•ï¼ˆ9.6%ï¼‰ï¼Œä½†è¿œä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ï¼ˆ59%ï¼‰ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨amazon.comå’Œebay.comä¸Šçš„è¡¨ç°ä¸åœ¨WebShopä¸Šçš„è¡¨ç°ç›¸ä¼¼ï¼Œè¡¨æ˜äº†Sim-to-realè¿ç§»çš„æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebShopä¸ºå¼€å‘èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•ŒWebç¯å¢ƒä¸­æ“ä½œçš„å®ç”¨ä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•å’Œæ¨¡å‹å¯ä»¥ç”¨äºå¼€å‘å…·æœ‰æ›´å¼ºè¯­è¨€ç†è§£å’Œå†³ç­–èƒ½åŠ›çš„æœªæ¥ä»£ç†ã€‚æ­¤å¤–ï¼ŒWebShopç¯å¢ƒå¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›å„ç§è¯­è¨€äº¤äº’ä»»åŠ¡ä¸­çš„ä»£ç†æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>6</th>
      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
      <td>While large language models (LLMs) have demonstrated impressive capabilities<br>across tasks in language understanding and interactive decision making, their<br>abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.<br>action plan generation) have primarily been studied as separate topics. In this<br>paper, we explore the use of LLMs to generate both reasoning traces and<br>task-specific actions in an interleaved manner, allowing for greater synergy<br>between the two: reasoning traces help the model induce, track, and update<br>action plans as well as handle exceptions, while actions allow it to interface<br>with external sources, such as knowledge bases or environments, to gather<br>additional information. We apply our approach, named ReAct, to a diverse set of<br>language and decision making tasks and demonstrate its effectiveness over<br>state-of-the-art baselines, as well as improved human interpretability and<br>trustworthiness over methods without reasoning or acting components.<br>Concretely, on question answering (HotpotQA) and fact verification (Fever),<br>ReAct overcomes issues of hallucination and error propagation prevalent in<br>chain-of-thought reasoning by interacting with a simple Wikipedia API, and<br>generates human-like task-solving trajectories that are more interpretable than<br>baselines without reasoning traces. On two interactive decision making<br>benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and<br>reinforcement learning methods by an absolute success rate of 34% and 10%<br>respectively, while being prompted with only one or two in-context examples.<br>Project site with code: https://react-lm.github.io</td>
      <td>ä¹‹å¤„ï¼Œæœ¬æ–‡æå‡ºäº†ReActæ–¹æ³•ï¼Œæ—¨åœ¨å°†æ¨ç†å’Œè¡ŒåŠ¨ç›¸ç»“åˆï¼Œä»¥è§£å†³è¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œè¡ŒåŠ¨æ–¹é¢çš„ä¸è¶³ã€‚ReActæ–¹æ³•é€šè¿‡äº¤æ›¿ç”Ÿæˆæ¨ç†è½¨è¿¹å’Œç‰¹å®šä»»åŠ¡çš„æ“ä½œï¼Œå®ç°äº†æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´çš„ååŒä½œç”¨ã€‚æ¨ç†è½¨è¿¹å¸®åŠ©æ¨¡å‹è¯±å¯¼ã€è·Ÿè¸ªå’Œæ›´æ–°æ“ä½œè®¡åˆ’ï¼Œå¹¶å¤„ç†å¼‚å¸¸æƒ…å†µï¼›è€Œæ“ä½œåˆ™å…è®¸æ¨¡å‹ä¸å¤–éƒ¨çŸ¥è¯†åº“æˆ–ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚<br><br>## ğŸŒŸ è®ºæ–‡äº®ç‚¹<br>* **æ¨ç†ä¸è¡ŒåŠ¨ååŒ**ï¼šReActæ–¹æ³•å°†æ¨ç†å’Œè¡ŒåŠ¨ç›¸ç»“åˆï¼Œå®ç°äº†æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚<br>* **å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦**ï¼šReActæ–¹æ³•ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å’Œæ“ä½œè®¡åˆ’æ›´åŠ å¯è§£é‡Šå’Œå¯ä¿¡ï¼Œæœ‰åŠ©äºäººç±»ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚<br>* **å¤šä»»åŠ¡é€‚ç”¨æ€§**ï¼šReActæ–¹æ³•åœ¨é—®ç­”ã€äº‹å®æ ¸æŸ¥ã€äº¤äº’å¼å†³ç­–ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶å¤šä»»åŠ¡é€‚ç”¨æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>* **é—®ç­”å’Œäº‹å®æ ¸æŸ¥**ï¼šReActæ–¹æ³•åœ¨HotpotQAå’ŒFeveræ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå…‹æœäº†é“¾å¼æ€ç»´æ¨ç†ä¸­å¸¸è§çš„å¹»è§‰å’Œé”™è¯¯ä¼ æ’­é—®é¢˜ã€‚<br>* **äº¤äº’å¼å†³ç­–**ï¼šReActæ–¹æ³•åœ¨ALFWorldå’ŒWebShopæ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚<br><br>## ğŸŒˆ æœªæ¥å±•æœ›<br>* **å¤šä»»åŠ¡è®­ç»ƒ**ï¼šæœªæ¥å¯ä»¥æ¢ç´¢å°†ReActæ–¹æ³•åº”ç”¨äºæ›´å¤šä»»åŠ¡ï¼Œå¹¶è¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚<br>* **æ¨¡å‹å¯è§£é‡Šæ€§**ï¼šæœªæ¥å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶å¦‚ä½•æé«˜ReActæ–¹æ³•ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å’Œæ“ä½œè®¡åˆ’çš„å¯è§£é‡Šæ€§ï¼Œä»¥ä¾¿äººç±»æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚<br>* **æ¨¡å‹é²æ£’æ€§**ï¼šæœªæ¥å¯ä»¥ç ”ç©¶å¦‚ä½•æé«˜ReActæ–¹æ³•çš„é²æ£’æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å‘æŒ¥ä½œç”¨ã€‚<br><br>## ğŸ“š å‚è€ƒæ–‡çŒ®<br>...<br><br>## ğŸ¯ æ€»ç»“<br>ReActæ–¹æ³•æ˜¯ä¸€ç§å¾ˆæœ‰æ½œåŠ›çš„æ–¹æ³•ï¼Œå®ƒå°†æ¨ç†å’Œè¡ŒåŠ¨ç›¸ç»“åˆï¼Œä¸ºè¯­è¨€æ¨¡å‹åœ¨æ¨ç†å’Œè¡ŒåŠ¨æ–¹é¢å¸¦æ¥äº†æ–°çš„çªç ´ã€‚ReActæ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ç­‰ä¼˜ç‚¹ã€‚æœªæ¥ï¼ŒReActæ–¹æ³•æœ‰æœ›åœ¨æ›´å¤šé¢†åŸŸå¾—åˆ°åº”ç”¨ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®ã€‚</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions</td>
      <td>Automated Graphical User Interface (GUI) testing plays a crucial role in<br>ensuring app quality, especially as mobile applications have become an integral<br>part of our daily lives. Despite the growing popularity of learning-based<br>techniques in automated GUI testing due to their ability to generate human-like<br>interactions, they still suffer from several limitations, such as low testing<br>coverage, inadequate generalization capabilities, and heavy reliance on<br>training data. Inspired by the success of Large Language Models (LLMs) like<br>ChatGPT in natural language understanding and question answering, we formulate<br>the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM<br>to chat with the mobile apps by passing the GUI page information to LLM to<br>elicit testing scripts, and executing them to keep passing the app feedback to<br>LLM, iterating the whole process. Within this framework, we have also<br>introduced a functionality-aware memory prompting mechanism that equips the LLM<br>with the ability to retain testing knowledge of the whole process and conduct<br>long-term, functionality-based reasoning to guide exploration. We evaluate it<br>on 93 apps from Google Play and demonstrate that it outperforms the best<br>baseline by 32% in activity coverage, and detects 31% more bugs at a faster<br>rate. Moreover, GPTDroid identify 53 new bugs on Google Play, of which 35 have<br>been confirmed and fixed.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è®©å¤§å‹è¯­è¨€æ¨¡å‹æˆä¸ºæµ‹è¯•ä¸“å®¶ï¼šé€šè¿‡åŠŸèƒ½æ„ŸçŸ¥å†³ç­–å°†ç±»ä¼¼äººç±»çš„äº¤äº’å¼•å…¥ç§»åŠ¨GUIæµ‹è¯•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ç§»åŠ¨åº”ç”¨ç¨‹åºåœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œç¡®ä¿åº”ç”¨ç¨‹åºè´¨é‡å˜å¾—è‡³å…³é‡è¦ã€‚è‡ªåŠ¨åŒ–å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æµ‹è¯•åœ¨ç¡®ä¿åº”ç”¨ç¨‹åºè´¨é‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå­¦ä¹ çš„è‡ªåŠ¨åŒ–GUIæµ‹è¯•æŠ€æœ¯ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚æµ‹è¯•è¦†ç›–ç‡ä½ã€æ³›åŒ–èƒ½åŠ›ä¸è¶³ä»¥åŠå¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–æ€§å¤§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGPTDroidçš„è‡ªåŠ¨åŒ–GUIæµ‹è¯•æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç§»åŠ¨GUIæµ‹è¯•é—®é¢˜è§†ä¸ºä¸€ä¸ªé—®ç­”ä»»åŠ¡ã€‚GPTDroidé€šè¿‡å°†GUIé¡µé¢ä¿¡æ¯ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥ç”Ÿæˆæµ‹è¯•è„šæœ¬ï¼Œå¹¶æ‰§è¡Œè¿™äº›è„šæœ¬ï¼Œç„¶åå°†åº”ç”¨ç¨‹åºçš„åé¦ˆä¼ é€’ç»™LLMï¼Œä»è€Œå®ç°ä¸ç§»åŠ¨åº”ç”¨ç¨‹åºçš„äº¤äº’ã€‚æ­¤å¤–ï¼ŒGPTDroidè¿˜å¼•å…¥äº†ä¸€ç§åŠŸèƒ½æ„ŸçŸ¥è®°å¿†æç¤ºæœºåˆ¶ï¼Œä½¿LLMèƒ½å¤Ÿä¿ç•™æ•´ä¸ªæµ‹è¯•è¿‡ç¨‹ä¸­çš„æµ‹è¯•çŸ¥è¯†ï¼Œå¹¶æŒ‡å¯¼æ¢ç´¢ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Google Playä¸Šçš„93ä¸ªåº”ç”¨ç¨‹åºä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒGPTDroidåœ¨æ´»åŠ¨è¦†ç›–ç‡æ–¹é¢æ¯”æœ€ä½³åŸºçº¿é«˜å‡º32%ï¼Œå¹¶ä»¥æ›´å¿«çš„é€Ÿåº¦æ£€æµ‹åˆ°31%çš„æ›´å¤šé”™è¯¯ã€‚æ­¤å¤–ï¼ŒGPTDroidåœ¨Google Playä¸Šå‘ç°äº†53ä¸ªæ–°é”™è¯¯ï¼Œå…¶ä¸­35ä¸ªå·²å¾—åˆ°ç¡®è®¤å’Œä¿®å¤ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GPTDroidçš„æˆåŠŸè¡¨æ˜ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨åŒ–GUIæµ‹è¯•å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†æµ‹è¯•è¦†ç›–ç‡ï¼Œè¿˜æé«˜äº†é”™è¯¯æ£€æµ‹ç‡ã€‚æ­¤å¤–ï¼ŒGPTDroidçš„åŠŸèƒ½æ„ŸçŸ¥è®°å¿†æç¤ºæœºåˆ¶ä¸ºLLMæä¾›äº†é•¿æœŸæ¨ç†çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£åº”ç”¨ç¨‹åºçš„åŠŸèƒ½å¹¶æŒ‡å¯¼æ¢ç´¢ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–GUIæµ‹è¯•å·¥å…·æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>8</th>
      <td>VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?</td>
      <td>Multimodal Large Language models (MLLMs) have shown promise in web-related<br>tasks, but evaluating their performance in the web domain remains a challenge<br>due to the lack of comprehensive benchmarks. Existing benchmarks are either<br>designed for general multimodal tasks, failing to capture the unique<br>characteristics of web pages, or focus on end-to-end web agent tasks, unable to<br>measure fine-grained abilities such as OCR, understanding, and grounding. In<br>this paper, we introduce \bench{}, a multimodal benchmark designed to assess<br>the capabilities of MLLMs across a variety of web tasks. \bench{} consists of<br>seven tasks, and comprises 1.5K human-curated instances from 139 real websites,<br>covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3<br>series, and GPT-4V(ision) on \bench{}, revealing significant challenges and<br>performance gaps. Further analysis highlights the limitations of current MLLMs,<br>including inadequate grounding in text-rich environments and subpar performance<br>with low-resolution image inputs. We believe \bench{} will serve as a valuable<br>resource for the research community and contribute to the creation of more<br>powerful and versatile MLLMs for web-related applications.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | VisualWebBenchï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç½‘é¡µç†è§£å’Œå®šä½æ–¹é¢çš„è¿›å±•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†ç½‘ç»œç›¸å…³ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ç½‘ç»œé¢†åŸŸçš„æ€§èƒ½å´é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•è¦ä¹ˆæ˜¯ä¸ºé€šç”¨å¤šæ¨¡æ€ä»»åŠ¡è®¾è®¡çš„ï¼Œæ— æ³•æ•æ‰ç½‘é¡µçš„ç‹¬ç‰¹ç‰¹å¾ï¼Œè¦ä¹ˆä¸“æ³¨äºç«¯åˆ°ç«¯çš„ç½‘ç»œä»£ç†ä»»åŠ¡ï¼Œæ— æ³•è¡¡é‡ç»†ç²’åº¦çš„èƒ½åŠ›ï¼Œå¦‚OCRã€ç†è§£å’Œå®šä½ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†VisualWebBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsåœ¨å„ç§ç½‘ç»œä»»åŠ¡ä¸­èƒ½åŠ›çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šVisualWebBenchç”±ä¸ƒä¸ªä»»åŠ¡ç»„æˆï¼ŒåŒ…æ‹¬ç½‘é¡µæ ‡é¢˜OCRã€ç½‘é¡µQAã€å…ƒç´ OCRã€å…ƒç´ å®šä½ã€åŠ¨ä½œé¢„æµ‹å’ŒåŠ¨ä½œå®šä½ï¼Œæ¶µç›–äº†ç½‘é¡µã€å…ƒç´ å’Œç”¨æˆ·åŠ¨ä½œä¸‰ä¸ªä¸åŒå±‚æ¬¡çš„ç†è§£å’Œå®šä½èƒ½åŠ›ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ¥è‡ª139ä¸ªçœŸå®ç½‘ç«™çš„1.5Kä¸ªäººå·¥ç­–åˆ’çš„å®ä¾‹ï¼Œæ¶µç›–äº†87ä¸ªå­åŸŸï¼Œç¡®ä¿äº†è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨VisualWebBenchä¸Šè¯„ä¼°äº†14ä¸ªå¼€æºMLLMsã€Gemini Proã€Claude-3ç³»åˆ—å’ŒGPT-4V(ision)ï¼Œç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯GPT-4Vå’ŒClaude Sonnetç­‰æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œåœ¨VisualWebBenchä¸Šçš„å¹³å‡å¾—åˆ†ä¹Ÿåªæœ‰64.6å’Œ65.8ï¼Œè¡¨æ˜ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ­¤å¤–ï¼Œå¼€æºMLLMsä¸GPT-4Vå’ŒClaudeç³»åˆ—ç­‰ä¸“æœ‰æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>VisualWebBenchä¸ºè¯„ä¼°MLLMsåœ¨ç½‘ç»œç†è§£å’Œå®šä½æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–åŸºå‡†ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å¼ºå¤§ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ã€è‡ªä¸»ç½‘ç»œä»£ç†å’Œç½‘ç»œç›¸å…³åº”ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†æµ‹è¯•è¿˜æ­ç¤ºäº†å½“å‰MLLMsçš„å±€é™æ€§ï¼ŒåŒ…æ‹¬åœ¨æ–‡æœ¬ä¸°å¯Œçš„ç¯å¢ƒä¸­å®šä½èƒ½åŠ›ä¸è¶³ä»¥åŠå¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒè¾“å…¥çš„æ€§èƒ½ä¸ä½³ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥çš„ç ”ç©¶å’Œæ¨¡å‹å¼€å‘æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>9</th>
      <td>AutoGLM: Autonomous Foundation Agents for GUIs</td>
      <td>We present AutoGLM, a new series in the ChatGLM family, designed to serve as<br>foundation agents for autonomous control of digital devices through Graphical<br>User Interfaces (GUIs). While foundation models excel at acquiring human<br>knowledge, they often struggle with decision-making in dynamic real-world<br>environments, limiting their progress toward artificial general intelligence.<br>This limitation underscores the importance of developing foundation agents<br>capable of learning through autonomous environmental interactions by<br>reinforcing existing models. Focusing on Web Browser and Phone as<br>representative GUI scenarios, we have developed AutoGLM as a practical<br>foundation agent system for real-world GUI interactions. Our approach<br>integrates a comprehensive suite of techniques and infrastructures to create<br>deployable agent systems suitable for user delivery. Through this development,<br>we have derived two key insights: First, the design of an appropriate<br>"intermediate interface" for GUI control is crucial, enabling the separation of<br>planning and grounding behaviors, which require distinct optimization for<br>flexibility and accuracy respectively. Second, we have developed a novel<br>progressive training framework that enables self-evolving online curriculum<br>reinforcement learning for AutoGLM. Our evaluations demonstrate AutoGLM's<br>effectiveness across multiple domains. For web browsing, AutoGLM achieves a<br>55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second<br>attempt) and 96.2% on OpenTable evaluation tasks. In Android device control,<br>AutoGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7% on<br>common tasks in popular Chinese APPs.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AutoGLMï¼šè‡ªä¸»æ§åˆ¶æ•°å­—è®¾å¤‡çš„GUIåŸºç¡€æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è·å–äººç±»çŸ¥è¯†å’Œè¯­è¨€èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŠ¨æ€ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿›æ­¥ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†AutoGLMï¼Œä¸€ä¸ªåŸºäºChatGLMæ¨¡å‹å®¶æ—çš„ç³»åˆ—åŸºç¡€æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIsï¼‰è‡ªä¸»æ§åˆ¶æ•°å­—è®¾å¤‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸­é—´æ¥å£è®¾è®¡<br>AutoGLMé‡‡ç”¨äº†ä¸€ç§ä¸­é—´æ¥å£è®¾è®¡ï¼Œå°†è§„åˆ’è¡Œä¸ºå’Œæ¥åœ°è¡Œä¸ºåˆ†ç¦»ã€‚è§„åˆ’è¡Œä¸ºè´Ÿè´£åˆ¶å®šè¡ŒåŠ¨ç­–ç•¥ï¼Œè€Œæ¥åœ°è¡Œä¸ºè´Ÿè´£è¯†åˆ«å’Œæ“ä½œGUIå…ƒç´ ã€‚è¿™ç§åˆ†ç¦»ä½¿å¾—è§„åˆ’è¡Œä¸ºå¯ä»¥æ›´åŠ çµæ´»ï¼Œè€Œæ¥åœ°è¡Œä¸ºå¯ä»¥æ›´åŠ å‡†ç¡®ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªè¿›åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ <br>AutoGLMé‡‡ç”¨äº†ä¸€ç§è‡ªè¿›åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨çº¿æ–¹å¼é€æ­¥å¢åŠ ä»»åŠ¡çš„éš¾åº¦ï¼Œä»è€Œæé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚è¿™ç§æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ä»»åŠ¡æ•°æ®ç¨€ç¼ºå’Œæ”¿ç­–åˆ†å¸ƒæ¼‚ç§»çš„é—®é¢˜ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>AutoGLMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œæµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚åœ¨Webæµè§ˆä»»åŠ¡ä¸­ï¼ŒAutoGLMåœ¨VAB-WebArena-Liteä¸Šå–å¾—äº†55.2%çš„æˆåŠŸç‡ï¼Œåœ¨OpenTableä¸Šå–å¾—äº†96.2%çš„æˆåŠŸç‡ã€‚åœ¨Androidè®¾å¤‡æ§åˆ¶ä»»åŠ¡ä¸­ï¼ŒAutoGLMåœ¨AndroidLabä¸Šå–å¾—äº†36.2%çš„æˆåŠŸç‡ï¼Œåœ¨å¸¸è§ä»»åŠ¡ä¸­å–å¾—äº†89.7%çš„æˆåŠŸç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AutoGLMçš„è®¾è®¡å’Œå®ç°ä¸ºå¼€å‘GUIåŸºç¡€æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„å‚è€ƒã€‚å…¶ä¸­é—´æ¥å£è®¾è®¡å’Œè‡ªè¿›åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚æ­¤å¤–ï¼ŒAutoGLMçš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œé€šè¿‡è‡ªä¸»ç¯å¢ƒäº¤äº’æ¥å¼ºåŒ–ç°æœ‰æ¨¡å‹æ˜¯å¼€å‘åŸºç¡€æ™ºèƒ½ä½“çš„æœ‰æ•ˆé€”å¾„ã€‚</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing</td>
      <td>Automated GUI testing is widely used to help ensure the quality of mobile<br>apps. However, many GUIs require appropriate text inputs to proceed to the next<br>page which remains a prominent obstacle for testing coverage. Considering the<br>diversity and semantic requirement of valid inputs (e.g., flight departure,<br>movie name), it is challenging to automate the text input generation. Inspired<br>by the fact that the pre-trained Large Language Model (LLM) has made<br>outstanding progress in text generation, we propose an approach named QTypist<br>based on LLM for intelligently generating semantic input text according to the<br>GUI context. To boost the performance of LLM in the mobile testing scenario, we<br>develop a prompt-based data construction and tuning method which automatically<br>extracts the prompts and answers for model tuning. We evaluate QTypist on 106<br>apps from Google Play and the result shows that the passing rate of QTypist is<br>87%, which is 93% higher than the best baseline. We also integrate QTypist with<br>the automated GUI testing tools and it can cover 42% more app activities, 52%<br>more pages, and subsequently help reveal 122% more bugs compared with the raw<br>tool.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¡«ç©ºå¼ï¼šåŸºäºä¸Šä¸‹æ–‡çš„ç§»åŠ¨GUIæµ‹è¯•è‡ªåŠ¨åŒ–æ–‡æœ¬è¾“å…¥ç”Ÿæˆ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç§»åŠ¨åº”ç”¨ï¼ˆAppï¼‰åœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­æ‰®æ¼”ç€ä¸å¯æˆ–ç¼ºçš„è§’è‰²ï¼Œç„¶è€Œï¼Œä¿è¯Appçš„è´¨é‡å´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æµ‹è¯•æ˜¯ç¡®ä¿AppåŠŸèƒ½æ­£ç¡®æ€§çš„é‡è¦æ‰‹æ®µï¼Œä½†è®¸å¤šGUIé¡µé¢éœ€è¦ç‰¹å®šçš„æ–‡æœ¬è¾“å…¥æ‰èƒ½è¿›å…¥ä¸‹ä¸€é¡µï¼Œè¿™æˆä¸ºäº†è‡ªåŠ¨åŒ–æµ‹è¯•è¦†ç›–ç‡çš„éšœç¢ã€‚ç”±äºæœ‰æ•ˆè¾“å…¥çš„å¤šæ ·æ€§å’Œè¯­ä¹‰è¦æ±‚ï¼Œè‡ªåŠ¨åŒ–æ–‡æœ¬è¾“å…¥ç”Ÿæˆä¸€ç›´æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºQTypistçš„æ–¹æ³•ï¼ŒåŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ™ºèƒ½åœ°ç”Ÿæˆè¯­ä¹‰è¾“å…¥æ–‡æœ¬ï¼Œä»¥é€‚åº”GUIä¸Šä¸‹æ–‡ã€‚ä¸ºäº†æé«˜LLMåœ¨ç§»åŠ¨æµ‹è¯•åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæç¤ºçš„æ•°æ®æ„å»ºå’Œå¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è‡ªåŠ¨æå–æ¨¡å‹å¾®è°ƒçš„æç¤ºå’Œç­”æ¡ˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æˆ‘ä»¬åœ¨Google Playä¸Šå¯¹106ä¸ªåº”ç”¨ç¨‹åºè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºQTypistçš„é€šè¿‡ç‡ä¸º87%ï¼Œæ¯”æœ€ä½³åŸºçº¿é«˜å‡º93%ã€‚æˆ‘ä»¬è¿˜å°†QTypistä¸è‡ªåŠ¨åŒ–GUIæµ‹è¯•å·¥å…·é›†æˆï¼Œå®ƒå¯ä»¥è¦†ç›–42%æ›´å¤šçš„åº”ç”¨ç¨‹åºæ´»åŠ¨ï¼Œ52%æ›´å¤šçš„é¡µé¢ï¼Œå¹¶å¸®åŠ©æ­ç¤º122%æ›´å¤šçš„é”™è¯¯ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>QTypistä¸ºè‡ªåŠ¨åŒ–GUIæµ‹è¯•æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„æ–‡æœ¬è¾“å…¥ï¼Œä»è€Œæé«˜æµ‹è¯•è¦†ç›–ç‡ã€‚æ­¤å¤–ï¼ŒQTypistçš„æ•°æ®æ„å»ºå’Œå¾®è°ƒæ–¹æ³•ä¹Ÿä¸ºå…¶ä»–é¢†åŸŸæä¾›äº†å¯ç¤ºï¼Œä¾‹å¦‚å¦‚ä½•åˆ©ç”¨ç°æœ‰æ•°æ®æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>11</th>
      <td>OmniParser for Pure Vision Based GUI Agent</td>
      <td>The recent success of large vision language models shows great potential in<br>driving the agent system operating on user interfaces. However, we argue that<br>the power multimodal models like GPT-4V as a general agent on multiple<br>operating systems across different applications is largely underestimated due<br>to the lack of a robust screen parsing technique capable of: 1) reliably<br>identifying interactable icons within the user interface, and 2) understanding<br>the semantics of various elements in a screenshot and accurately associate the<br>intended action with the corresponding region on the screen. To fill these<br>gaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing user<br>interface screenshots into structured elements, which significantly enhances<br>the ability of GPT-4V to generate actions that can be accurately grounded in<br>the corresponding regions of the interface. We first curated an interactable<br>icon detection dataset using popular webpages and an icon description dataset.<br>These datasets were utilized to fine-tune specialized models: a detection model<br>to parse interactable regions on the screen and a caption model to extract the<br>functional semantics of the detected elements. \textsc{OmniParser}<br>significantly improves GPT-4V's performance on ScreenSpot benchmark. And on<br>Mind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only input<br>outperforms the GPT-4V baselines requiring additional information outside of<br>screenshot.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | OmniParserï¼šåŸºäºçº¯è§†è§‰çš„GUIæ™ºèƒ½ä½“è§£æå™¨<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æˆåŠŸï¼Œå®ƒä»¬åœ¨ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ä¸Šæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4Våœ¨è·¨å¹³å°å’Œè·¨åº”ç”¨ç¨‹åºçš„é€šç”¨æ€§æ–¹é¢è¢«ä½ä¼°äº†ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹ä¸€ç§èƒ½å¤Ÿå¯é åœ°è¯†åˆ«ç”¨æˆ·ç•Œé¢ä¸­å¯äº¤äº’å›¾æ ‡å¹¶ç†è§£å±å¹•æˆªå›¾ä¸­çš„å„ç§å…ƒç´ è¯­ä¹‰çš„å±å¹•è§£ææŠ€æœ¯ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†OmniParserï¼Œä¸€ç§å°†ç”¨æˆ·ç•Œé¢å±å¹•æˆªå›¾è§£æä¸ºç»“æ„åŒ–å…ƒç´ çš„ç»¼åˆæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†GPT-4Vç”Ÿæˆå¯ä»¥å‡†ç¡®æ˜ å°„åˆ°ç•Œé¢ç›¸åº”åŒºåŸŸä¸Šçš„åŠ¨ä½œçš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šäº¤äº’åŒºåŸŸæ£€æµ‹<br>OmniParseré¦–å…ˆä½¿ç”¨ä»æµè¡Œç½‘é¡µçš„DOMæ ‘ä¸­æå–çš„è¾¹ç•Œæ¡†åˆ›å»ºäº†ä¸€ä¸ªå¯äº¤äº’åŒºåŸŸæ£€æµ‹æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†ç”¨äºå¾®è°ƒä¸€ä¸ªæ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥è§£æå±å¹•ä¸Šçš„å¯äº¤äº’åŒºåŸŸã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠŸèƒ½è¯­ä¹‰æå–<br>ä¸ºäº†ç†è§£æ£€æµ‹åˆ°çš„å…ƒç´ çš„åŠŸèƒ½è¯­ä¹‰ï¼ŒOmniParserä½¿ç”¨äº†ä¸€ä¸ªå¾®è°ƒçš„å›¾æ ‡æè¿°æ¨¡å‹æ¥æå–æ£€æµ‹åˆ°çš„å…ƒç´ çš„åŠŸèƒ½æè¿°ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨OCRæ¨¡å—æ¥æå–æ–‡æœ¬çš„è¾¹ç•Œæ¡†ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯ä¸å›¾æ ‡æ£€æµ‹æ¨¡å—çš„ç»“æœåˆå¹¶ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šç»“åˆæœ¬åœ°è¯­ä¹‰<br>OmniParserå°†æœ¬åœ°è¯­ä¹‰ï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œå›¾æ ‡æè¿°ï¼‰ä¸å±å¹•æˆªå›¾è§†è§‰æç¤ºç›¸ç»“åˆï¼Œä»¥å¸®åŠ©GPT-4Væ›´å‡†ç¡®åœ°è¯†åˆ«è¦æ“ä½œçš„å…ƒç´ ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>OmniParseråœ¨ScreenSpotåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†GPT-4Vçš„æ€§èƒ½ã€‚åœ¨Mind2Webå’ŒAITWåŸºå‡†æµ‹è¯•ä¸­ï¼Œä»…ä½¿ç”¨å±å¹•æˆªå›¾è¾“å…¥çš„OmniParserçš„æ€§èƒ½ä¼˜äºéœ€è¦å±å¹•æˆªå›¾ä¹‹å¤–é¢å¤–ä¿¡æ¯çš„GPT-4VåŸºçº¿ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>OmniParseræä¾›äº†ä¸€ç§é€šç”¨çš„å±å¹•è§£æå·¥å…·ï¼Œå¯ä»¥ä»UIå±å¹•æˆªå›¾ä¸­æå–ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ï¼Œä»è€Œå¢å¼ºGPT-4Våœ¨å„ç§ç”¨æˆ·ä»»åŠ¡ä¸­çš„åŠ¨ä½œé¢„æµ‹æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•ä¸ä¾èµ–äºé¢å¤–çš„ä¿¡æ¯ï¼Œå¦‚HTMLå’ŒAndroidè§†å›¾å±‚æ¬¡ç»“æ„ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªé€šç”¨çš„ã€æ˜“äºä½¿ç”¨çš„å·¥å…·ï¼Œå¯ä»¥åœ¨PCå’Œç§»åŠ¨å¹³å°ä¸Šè§£æä¸€èˆ¬çš„ç”¨æˆ·å±å¹•ã€‚</td>
    </tr>
    <tr>
      <th>12</th>
      <td>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</td>
      <td>We propose the problem of conversational web navigation, where a digital<br>agent controls a web browser and follows user instructions to solve real-world<br>tasks in a multi-turn dialogue fashion. To support this problem, we introduce<br>WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert<br>demonstrations of conversational web navigation. Our benchmark covers a broad<br>range of patterns on over 150 real-world websites and can be used to train and<br>evaluate agents in diverse scenarios. Due to the magnitude of information<br>present, Large Language Models (LLMs) cannot process entire web pages in<br>real-time. To solve this bottleneck, we design a retrieval-inspired model that<br>efficiently prunes HTML pages by ranking relevant elements. We use the selected<br>elements, along with screenshots and action history, to assess a variety of<br>models for their ability to replicate human behavior when navigating the web.<br>Our experiments span from small text-only to proprietary multimodal LLMs. We<br>find that smaller finetuned decoders surpass the best zero-shot LLMs (including<br>GPT-4V), but also larger finetuned multimodal models which were explicitly<br>pretrained on screenshots. However, all finetuned models struggle to generalize<br>to unseen websites. Our findings highlight the need for large multimodal models<br>that can generalize to novel settings. Our code, data and models are available<br>for research: https://mcgill-nlp.github.io/weblinx</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebLINXï¼šåŸºäºå¤šè½®å¯¹è¯çš„çœŸå®ä¸–ç•Œç½‘ç«™å¯¼èˆª<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€èŠå¤©æœºå™¨äººå¦‚ChatGPTç­‰æŠ€æœ¯çš„å‘å±•ï¼Œå®ƒä»¬å·²ç»èƒ½å¤Ÿé€šè¿‡æ’ä»¶æµè§ˆç½‘ç«™ï¼Œæ‰§è¡Œæ“ä½œå¹¶æä¾›æ›´æœ‰ç”¨çš„å“åº”ã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›æ˜¯æœ‰é™çš„ï¼Œå› ä¸ºæ’ä»¶å¿…é¡»ä¸ºæ¯ä¸ªç½‘ç«™å•ç‹¬å¼€å‘ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•æ¶µç›–ç½‘ç«™çš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦åˆ©ç”¨è¿™äº›åŠ©æ‰‹èƒŒåçš„æ¨¡å‹ï¼Œç›´æ¥åœ¨ç”¨æˆ·çš„æµè§ˆå™¨ä¸­å¯¼èˆªç½‘ç«™ï¼ŒåŒæ—¶ä¿ç•™å®ƒä»¬çš„å¯¹è¯èƒ½åŠ›ï¼Ÿ<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWEBLINXåŸºå‡†æ•°æ®é›†<br>ä¸ºäº†æ”¯æŒè¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WEBLINXï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2337ä¸ªä¸“å®¶æ¼”ç¤ºçš„å¤§å‹åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†150å¤šä¸ªçœŸå®ä¸–ç•Œç½‘ç«™ä¸Šçš„100Kä¸ªäº¤äº’ã€‚è¿™ä¸ªæ•°æ®é›†å¯ä»¥ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åœ¨å¤šç§åœºæ™¯ä¸­å¯¼èˆªçš„å¯¹è¯å¼ä»£ç†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šDense Markup Ranking (DMR) æ¨¡å‹<br>ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ— æ³•å®æ—¶å¤„ç†æ•´ä¸ªç½‘é¡µï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªæ£€ç´¢å¯å‘çš„æ¨¡å‹ï¼Œé€šè¿‡æ’åç›¸å…³å…ƒç´ æ¥æœ‰æ•ˆåœ°å‰ªæHTMLé¡µé¢ã€‚ä½¿ç”¨é€‰å®šçš„å…ƒç´ ã€å±å¹•æˆªå›¾å’Œæ“ä½œå†å²ï¼Œè¯„ä¼°äº†å„ç§æ¨¡å‹åœ¨å¤åˆ¶äººç±»è¡Œä¸ºæ—¶çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œè¾ƒå°çš„å¾®è°ƒè§£ç å™¨ä¼˜äºæœ€ä½³é›¶æ ·æœ¬LLMsï¼ˆåŒ…æ‹¬GPT-4Vï¼‰ï¼Œä½†æ‰€æœ‰å¾®è°ƒæ¨¡å‹éƒ½éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„ç½‘ç«™ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ï¼Œå³æ„å»ºèƒ½å¤Ÿæ¨å¹¿åˆ°æ–°åœºæ™¯çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„Dense Markup Ranking (DMR) æ¨¡å‹ä¸ºå¤„ç†å¤§å‹ç½‘é¡µæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥ç”¨äºå…¶ä»–éœ€è¦å¤„ç†å¤§é‡æ–‡æœ¬æ•°æ®çš„ä»»åŠ¡ã€‚</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions</td>
      <td>This paper investigates the faithfulness of multimodal large language model<br>(MLLM) agents in the graphical user interface (GUI) environment, aiming to<br>address the research question of whether multimodal GUI agents can be<br>distracted by environmental context. A general setting is proposed where both<br>the user and the agent are benign, and the environment, while not malicious,<br>contains unrelated content. A wide range of MLLMs are evaluated as GUI agents<br>using our simulated dataset, following three working patterns with different<br>levels of perception. Experimental results reveal that even the most powerful<br>models, whether generalist agents or specialist GUI agents, are susceptible to<br>distractions. While recent studies predominantly focus on the helpfulness<br>(i.e., action accuracy) of multimodal agents, our findings indicate that these<br>agents are prone to environmental distractions, resulting in unfaithful<br>behaviors. Furthermore, we switch to the adversarial perspective and implement<br>environment injection, demonstrating that such unfaithfulness can be exploited,<br>leading to unexpected risks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | å¤šæ¨¡æ€æ™ºèƒ½ä½“æ˜“å—ç¯å¢ƒå¹²æ‰°ï¼Œéœ€è­¦æƒ•ï¼<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨è§£å†³å¤æ‚äº¤äº’ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ™ºèƒ½ä½“åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´ç€ç¯å¢ƒå¹²æ‰°çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå¹¿å‘Šã€é€šçŸ¥ç­‰æ— å…³å†…å®¹å¯èƒ½ä¼šå¹²æ‰°æ™ºèƒ½ä½“çš„ç›®æ ‡ï¼Œå¯¼è‡´å…¶è¡Œä¸ºåç¦»é¢„æœŸï¼Œç”šè‡³æ‰§è¡Œé”™è¯¯æ“ä½œã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨GUIç¯å¢ƒä¸­çš„å¿ è¯šåº¦é—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶æå‡ºäº†ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå®šä¹‰äº†ç¯å¢ƒå¹²æ‰°é—®é¢˜ï¼Œå¹¶æ„å»ºäº†åŒ…å«å››ç§åœºæ™¯ï¼ˆå¼¹å‡ºæ¡†ã€æœç´¢ã€æ¨èã€èŠå¤©ï¼‰çš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨ä¸åŒç¯å¢ƒä¸‹çš„è¡Œä¸ºã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè®¾è®¡äº†ä¸‰ç§å·¥ä½œæ¨¡å¼ï¼ˆç›´æ¥æç¤ºã€æ€ç»´é“¾æç¤ºã€åŠ¨ä½œæ ‡æ³¨ï¼‰ï¼Œä»¥æ¨¡æ‹Ÿä¸åŒæ„ŸçŸ¥æ°´å¹³çš„æ™ºèƒ½ä½“ï¼Œå¹¶è¯„ä¼°å…¶å¯¹ç¯å¢ƒå¹²æ‰°çš„æ•æ„Ÿæ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºäº†ç¯å¢ƒæ³¨å…¥æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹ç¯å¢ƒå†…å®¹æ¥å¹²æ‰°æ™ºèƒ½ä½“ï¼Œä»è€ŒéªŒè¯å…¶æ˜“å—æ”»å‡»æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯åŠŸèƒ½å¼ºå¤§çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œä¹Ÿå®¹æ˜“å—åˆ°ç¯å¢ƒå¹²æ‰°ï¼Œå¯¼è‡´å…¶å¿ è¯šåº¦å’Œæœ‰æ•ˆæ€§ä¸‹é™ã€‚æ­¤å¤–ï¼Œç¯å¢ƒæ³¨å…¥æ”»å‡»æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹²æ‰°æ™ºèƒ½ä½“ï¼Œä½¿å…¶æ‰§è¡Œé”™è¯¯æ“ä½œã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦è€ƒè™‘ç¯å¢ƒå¹²æ‰°çš„å½±å“ï¼Œå¹¶é‡‡å–ç›¸åº”çš„æªæ–½æ¥æé«˜å…¶é²æ£’æ€§ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡é¢„è®­ç»ƒæ¥å¢å¼ºæ™ºèƒ½ä½“çš„å¿ è¯šåº¦ï¼Œæˆ–è€…å¼•å…¥äººç±»äº¤äº’æ¥è¾…åŠ©æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡ã€‚<br><br>## ğŸ“š å‚è€ƒæ–‡çŒ®<br>[1] Ma, X., Wang, Y., Yao, Y., Yuan, T., Zhang, A., Zhang, Z., & Zhao, H. (2024). Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions. arXiv preprint arXiv:2408.02544.</td>
    </tr>
    <tr>
      <th>14</th>
      <td>GAIA: a benchmark for General AI Assistants</td>
      <td>We introduce GAIA, a benchmark for General AI Assistants that, if solved,<br>would represent a milestone in AI research. GAIA proposes real-world questions<br>that require a set of fundamental abilities such as reasoning, multi-modality<br>handling, web browsing, and generally tool-use proficiency. GAIA questions are<br>conceptually simple for humans yet challenging for most advanced AIs: we show<br>that human respondents obtain 92\% vs. 15\% for GPT-4 equipped with plugins.<br>This notable performance disparity contrasts with the recent trend of LLMs<br>outperforming humans on tasks requiring professional skills in e.g. law or<br>chemistry. GAIA's philosophy departs from the current trend in AI benchmarks<br>suggesting to target tasks that are ever more difficult for humans. We posit<br>that the advent of Artificial General Intelligence (AGI) hinges on a system's<br>capability to exhibit similar robustness as the average human does on such<br>questions. Using GAIA's methodology, we devise 466 questions and their answer.<br>We release our questions while retaining answers to 300 of them to power a<br>leader-board available at https://huggingface.co/gaia-benchmark.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | GAIAï¼šè¯„ä¼°é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹çš„åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½åŠ›çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„AIåŸºå‡†æµ‹è¯•å·²ç»æ— æ³•æ»¡è¶³è¯„ä¼°è¿™äº›æ¨¡å‹çš„éœ€æ±‚ã€‚è®¸å¤šLLMså·²ç»åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†äººç±»çš„è¡¨ç°ï¼Œä¾‹å¦‚åœ¨æ³•å¾‹æˆ–åŒ–å­¦ç­‰ä¸“ä¸šé¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›ä»»åŠ¡å¯¹äºäººç±»æ¥è¯´å¯èƒ½éå¸¸å›°éš¾ï¼Œè€Œå¯¹äºLLMsæ¥è¯´å´ç›¸å¯¹å®¹æ˜“ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMsåœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>GAIAæ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼ˆAGIï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒæå‡ºäº†éœ€è¦ä¸€ç³»åˆ—åŸºæœ¬èƒ½åŠ›çš„é—®é¢˜ï¼Œä¾‹å¦‚æ¨ç†ã€å¤šæ¨¡æ€å¤„ç†ã€ç½‘ç»œæµè§ˆå’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚GAIAçš„é—®é¢˜å¯¹äºäººç±»æ¥è¯´æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†å¯¹äºå¤§å¤šæ•°å…ˆè¿›çš„AIæ¥è¯´å´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäººç±»å—è®¿è€…è·å¾—äº†92%çš„å‡†ç¡®ç‡ï¼Œè€Œé…å¤‡äº†æ’ä»¶çš„GPT-4ä»…è·å¾—äº†15%çš„å‡†ç¡®ç‡ã€‚<br><br>GAIAçš„å“²å­¦ä¸å½“å‰AIåŸºå‡†æµ‹è¯•çš„è¶‹åŠ¿ä¸åŒï¼Œåè€…å»ºè®®é’ˆå¯¹å¯¹äººç±»æ¥è¯´è¶Šæ¥è¶Šå›°éš¾çš„ä»»åŠ¡ã€‚GAIAè®¤ä¸ºï¼Œé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‡ºç°å–å†³äºç³»ç»Ÿèƒ½å¦åœ¨è¿™äº›é—®é¢˜ä¸Šå±•ç°å‡ºä¸æ™®é€šäººç±»ç›¸ä¼¼çš„é²æ£’æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ä½¿ç”¨GAIAçš„æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†466ä¸ªé—®é¢˜å’Œç­”æ¡ˆã€‚ä»–ä»¬å‘å¸ƒäº†è¿™äº›é—®é¢˜ï¼Œå¹¶ä¿ç•™äº†300ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œä»¥æ”¯æŒä¸€ä¸ªæ’è¡Œæ¦œï¼Œå¯åœ¨https://huggingface.co/gaia-benchmarkä¸Šè®¿é—®ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>GAIAçš„åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹æä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚å®ƒå¼ºè°ƒäº†ç°å®ä¸–ç•Œé—®é¢˜çš„é‡è¦æ€§ï¼Œå¹¶è¦æ±‚AIç³»ç»Ÿå…·å¤‡ä¸€ç³»åˆ—åŸºæœ¬èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGAIAçš„è¯„ä¼°æ–¹æ³•ç®€å•ã€å¿«é€Ÿä¸”æ˜“äºç†è§£ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°AIç³»ç»Ÿçš„ä¸€ç§æœ‰ç”¨å·¥å…·ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>GAIAæ˜¯ä¸€ä¸ªé‡è¦çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸ºè¯„ä¼°é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ã€‚å®ƒå¼ºè°ƒäº†ç°å®ä¸–ç•Œé—®é¢˜çš„é‡è¦æ€§ï¼Œå¹¶è¦æ±‚AIç³»ç»Ÿå…·å¤‡ä¸€ç³»åˆ—åŸºæœ¬èƒ½åŠ›ã€‚éšç€GAIAçš„ä¸æ–­å‘å±•ï¼Œå®ƒæœ‰æœ›æˆä¸ºè¯„ä¼°AIç³»ç»Ÿçš„ä¸€ç§é‡è¦å·¥å…·ï¼Œå¹¶æ¨åŠ¨é€šç”¨äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>15</th>
      <td>DynaSaur: Large Language Agents Beyond Predefined Actions</td>
      <td>Existing LLM agent systems typically select actions from a fixed and<br>predefined set at every step. While this approach is effective in closed,<br>narrowly-scoped environments, we argue that it presents two major challenges<br>when deploying LLM agents in real-world scenarios: (1) selecting from a fixed<br>set of actions significantly restricts the planning and acting capabilities of<br>LLM agents, and (2) this approach requires substantial human effort to<br>enumerate and implement all possible actions, which becomes impractical in<br>complex environments with a vast number of potential actions. In this work, we<br>propose an LLM agent framework that enables the dynamic creation and<br>composition of actions in an online manner. In this framework, the agent<br>interacts with the environment by generating and executing programs written in<br>a general-purpose programming language at each step. Furthermore, generated<br>actions are accumulated over time for future reuse. Our extensive experiments<br>on the GAIA benchmark demonstrate that this framework offers significantly<br>greater flexibility and outperforms previous methods. Notably, it allows an LLM<br>agent to recover in scenarios where no relevant action exists in the predefined<br>set or when existing actions fail due to unforeseen edge cases. At the time of<br>writing, we hold the top position on the GAIA public leaderboard. Our code can<br>be found in<br>\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | DynaSaurï¼šè¶…è¶Šé¢„å®šä¹‰åŠ¨ä½œçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>ç°æœ‰çš„LLMä»£ç†ç³»ç»Ÿé€šå¸¸åœ¨æ¯ä¸ªæ­¥éª¤ä»å›ºå®šå’Œé¢„å®šä¹‰çš„åŠ¨ä½œé›†ä¸­é€‰æ‹©åŠ¨ä½œã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨å°é—­ã€èŒƒå›´ç‹­çª„çš„ç¯å¢ƒä¸­æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ä½œè€…è®¤ä¸ºï¼Œå½“åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­éƒ¨ç½²LLMä»£ç†æ—¶ï¼Œè¿™ç§æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š<br>1. ä»å›ºå®šåŠ¨ä½œé›†ä¸­é€‰æ‹©åŠ¨ä½œæ˜¾è‘—é™åˆ¶äº†LLMä»£ç†çš„è§„åˆ’å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚<br>2. è¿™ç§æ–¹æ³•éœ€è¦å¤§é‡çš„äººåŠ›æ¥æšä¸¾å’Œå®ç°æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œè¿™åœ¨å…·æœ‰å¤§é‡æ½œåœ¨åŠ¨ä½œçš„å¤æ‚ç¯å¢ƒä¸­å˜å¾—ä¸åˆ‡å®é™…ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†DynaSaurï¼Œä¸€ä¸ªLLMä»£ç†æ¡†æ¶ï¼Œå®ƒå…è®¸ä»¥åœ¨çº¿æ–¹å¼åŠ¨æ€åˆ›å»ºå’Œç»„åˆåŠ¨ä½œã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œä»£ç†é€šè¿‡åœ¨æ¯ä¸ªæ­¥éª¤ç”Ÿæˆå’Œæ‰§è¡Œç”¨é€šç”¨ç¼–ç¨‹è¯­è¨€ç¼–å†™çš„ç¨‹åºæ¥ä¸ç¯å¢ƒäº¤äº’ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„åŠ¨ä½œä¼šéšç€æ—¶é—´çš„æ¨ç§»ç§¯ç´¯èµ·æ¥ï¼Œä»¥ä¾¿å°†æ¥é‡ç”¨ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŠ¨æ€åŠ¨ä½œåˆ›å»º<br>DynaSaurå°†æ¯ä¸ªåŠ¨ä½œå»ºæ¨¡ä¸ºPythonå‡½æ•°ï¼Œä»£ç†åœ¨æ¯ä¸ªæ­¥éª¤é€šè¿‡ç”ŸæˆPythonä»£ç ç‰‡æ®µæ¥æ‰§è¡ŒåŠ¨ä½œã€‚è¿™äº›ä»£ç ç‰‡æ®µè¦ä¹ˆå®šä¹‰æ–°çš„å‡½æ•°ï¼Œè¦ä¹ˆé‡ç”¨å½“å‰åŠ¨ä½œé›†ä¸­çš„ç°æœ‰å‡½æ•°ã€‚ç”Ÿæˆçš„ä»£ç é€šè¿‡Pythonè§£é‡Šå™¨æ‰§è¡Œï¼Œå¹¶å°†ç»“æœè§‚å¯Ÿè¿”å›ç»™ä»£ç†ã€‚æ‰€æœ‰ç”±ä»£ç†ç”Ÿæˆçš„åŠ¨ä½œéƒ½ä¼šç§¯ç´¯èµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªå¯é‡ç”¨çš„å‡½æ•°åº“ï¼Œä¾›å°†æ¥ä½¿ç”¨ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåŠ¨ä½œæ£€ç´¢<br>ä¸ºäº†è§£å†³å°†æ‰€æœ‰ç”Ÿæˆçš„åŠ¨ä½œä½œä¸ºæç¤ºçš„ä¸€éƒ¨åˆ†å¯èƒ½å¯¼è‡´ä¸Šä¸‹æ–‡é™åˆ¶çš„é—®é¢˜ï¼ŒDynaSaurå°†åŠ¨ä½œé›†åˆ†è§£ä¸ºä¸¤ä¸ªå­é›†ï¼šäººç±»è®¾è®¡çš„åŠ¨ä½œé›†å’Œç”Ÿæˆçš„åŠ¨ä½œé›†ã€‚åªæœ‰äººç±»è®¾è®¡çš„åŠ¨ä½œé›†é»˜è®¤åŒ…å«åœ¨æç¤ºä¸­ã€‚ä¸ºäº†æä¾›ä»£ç†å¯¹ç”ŸæˆåŠ¨ä½œé›†çš„è®¿é—®ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŠ¨ä½œæ£€ç´¢å‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®æŸ¥è¯¢å’Œæ•´æ•°kè¿”å›æœ€ç›¸ä¼¼çš„kä¸ªåŠ¨ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDynaSauræ¡†æ¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ï¼Œå¹¶ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒå…è®¸LLMä»£ç†åœ¨é¢„å®šä¹‰é›†ä¸­ä¸å­˜åœ¨ç›¸å…³åŠ¨ä½œæˆ–ç°æœ‰åŠ¨ä½œç”±äºæ„å¤–çš„è¾¹ç¼˜æƒ…å†µè€Œå¤±è´¥çš„æƒ…å†µä¸‹æ¢å¤ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>DynaSauræ¡†æ¶ä¸ºLLMä»£ç†åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡åŠ¨æ€åˆ›å»ºå’Œç»„åˆåŠ¨ä½œï¼Œä»£ç†å¯ä»¥æ›´å¥½åœ°é€‚åº”å¤æ‚å’Œä¸ç¡®å®šçš„ç¯å¢ƒï¼Œå¹¶ä»è¿‡å»çš„ç»éªŒä¸­å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒDynaSauræ¡†æ¶è¿˜å¯ä»¥ä¸å…¶ä»–å·¥å…·å’Œåº“æ— ç¼é›†æˆï¼Œè¿›ä¸€æ­¥æé«˜ä»£ç†çš„æ€§èƒ½å’Œçµæ´»æ€§ã€‚</td>
    </tr>
    <tr>
      <th>16</th>
      <td>MobileFlow: A Multimodal LLM For Mobile GUI Agent</td>
      <td>Currently, the integration of mobile Graphical User Interfaces (GUIs) is<br>ubiquitous in most people's daily lives. And the ongoing evolution of<br>multimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly<br>bolstered the capabilities of GUI comprehension and user action analysis,<br>showcasing the potentiality of intelligent GUI assistants. However, current GUI<br>Agents often need to access page layout information through calling system<br>APIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a<br>certain low resolution might result in the loss of fine-grained image details.<br>At the same time, the multimodal large models built for GUI Agents currently<br>have poor understanding and decision-making abilities for Chinese GUI<br>interfaces, making them difficult to apply to a large number of Chinese apps.<br>This paper introduces MobileFlow, a multimodal large language model<br>meticulously crafted for mobile GUI agents. Transforming from the open-source<br>model Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21<br>billion parameters and is equipped with novel hybrid visual encoders, making it<br>possible for variable resolutions of image inputs and good support for<br>multilingual GUI. By incorporating Mixture of Experts (MoE) expansions and<br>pioneering alignment training strategies, MobileFlow has the capacity to fully<br>interpret image data and comprehend user instructions for GUI interaction<br>tasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task<br>execution by GUI agents on both public and our proposed evaluation metrics, and<br>has been successfully deployed in real-world business contexts, proving its<br>effectiveness for practical applications.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MobileFlowï¼šç§»åŠ¨GUIæ™ºèƒ½åŠ©æ‰‹çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ—¥ç›Šæ™®åŠï¼Œæ™ºèƒ½GUIåŠ©æ‰‹çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„GUIåŠ©æ‰‹åœ¨å¤„ç†ä¸­æ–‡GUIç•Œé¢æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸”åœ¨è·å–é¡µé¢å¸ƒå±€ä¿¡æ¯æ—¶å¯èƒ½æ¶‰åŠéšç§é£é™©ã€‚æ­¤å¤–ï¼Œå›ºå®šGUIåˆ†è¾¨ç‡å¯èƒ½å¯¼è‡´å›¾åƒç»†èŠ‚ä¸¢å¤±ï¼Œé™åˆ¶äº†GUIåŠ©æ‰‹çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>MobileFlowæ˜¯ä¸€ä¸ªä¸“ä¸ºç§»åŠ¨GUIåŠ©æ‰‹è®¾è®¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ··åˆè§†è§‰ç¼–ç å™¨<br>MobileFlowé‡‡ç”¨äº†æ··åˆè§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒè¾“å…¥ï¼Œå¹¶æ”¯æŒå¤šè¯­è¨€GUIã€‚é€šè¿‡ä½¿ç”¨LayoutLMv3ä½œä¸ºUIç¼–ç å™¨çš„åŸºç¡€ç»“æ„ï¼ŒMobileFlowèƒ½å¤Ÿæœ‰æ•ˆåœ°æå–å’Œç†è§£ä¸åŒGUIç•Œé¢çš„ä¿¡æ¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šMoEæ‰©å±•å’ŒCoTè®­ç»ƒç­–ç•¥<br>MobileFlowå¼•å…¥äº†MoEæ‰©å±•ï¼Œé€šè¿‡éšæœºæ¿€æ´»å¤šä¸ªä¸“å®¶ç½‘ç»œï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼ŒMobileFlowé‡‡ç”¨äº†CoTè®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸€ç³»åˆ—ä¸­é—´æ­¥éª¤æˆ–è§£é‡Šæ€§é™ˆè¿°ï¼Œä»è€Œæé«˜æ¨ç†å’Œå†³ç­–çš„å‡†ç¡®æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MobileFlowåœ¨å…¬å…±å’Œè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºQwen-VL-Maxå’ŒGPT-4vï¼Œå¹¶åœ¨å®é™…ä¸šåŠ¡åœºæ™¯ä¸­æˆåŠŸéƒ¨ç½²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MobileFlowçš„åˆ›æ–°æ–¹æ³•ä¸ºå¼€å‘æ™ºèƒ½GUIåŠ©æ‰‹æä¾›äº†æ–°çš„æ€è·¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸­æ–‡GUIç•Œé¢å’Œè§£å†³éšç§é£é™©æ–¹é¢ã€‚æ­¤å¤–ï¼ŒMoEæ‰©å±•å’ŒCoTè®­ç»ƒç­–ç•¥ä¹Ÿä¸ºæé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>17</th>
      <td>AutoTask: Executing Arbitrary Voice Commands by Exploring and Learning from Mobile GUI</td>
      <td>Voice command interfaces (VCIs) have gained increasing importance, enabling<br>hands-free and eyes-free interaction with digital devices. However, the<br>inherent complexity in constructing effective voice interfaces has limited the<br>VCIs' functionalities to only a small fraction of GUI applications and tasks.<br>This paper presents AutoTask, a VCI capable of automating any task in any<br>mobile application without configuration or modification from developers or end<br>users. The primary challenge for AutoTask is the lack of knowledge, as it needs<br>to accomplish unknown tasks (e.g., user commands) within an unknown environment<br>(e.g., GUI). To address this challenge, AutoTask employs two strategies: (1)<br>trial and error: AutoTask explores the GUI, attempts potential operation<br>sequences, and recovers from errors through backtracking; (2) learning from the<br>environment: AutoTask accumulates experiences during exploration and summarizes<br>correct knowledge from these experiences. We implemented AutoTask on Android<br>devices and conducted an evaluation study, which proved the feasibility of<br>AutoTask.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AutoTaskï¼šæ¢ç´¢ä¸å­¦ä¹ ï¼Œè®©è¯­éŸ³å‘½ä»¤åœ¨ç§»åŠ¨GUIä¸­è‡ªç”±æ‰§è¡Œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€è¯­éŸ³äº¤äº’æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè¯­éŸ³å‘½ä»¤ç•Œé¢ï¼ˆVCIï¼‰åœ¨æ•°å­—è®¾å¤‡ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œå®ƒå…è®¸ç”¨æˆ·åœ¨æ— éœ€åŠ¨æ‰‹å’Œç”¨çœ¼çš„æƒ…å†µä¸‹ä¸è®¾å¤‡è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œæ„å»ºæœ‰æ•ˆçš„è¯­éŸ³ç•Œé¢é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ç°æœ‰çš„VCIåŠŸèƒ½ä»…é™äºå°‘æ•°GUIåº”ç”¨ç¨‹åºå’Œä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†AutoTaskï¼Œä¸€ä¸ªæ— éœ€å¼€å‘äººå‘˜æˆ–æœ€ç»ˆç”¨æˆ·é…ç½®æˆ–ä¿®æ”¹å³å¯è‡ªåŠ¨æ‰§è¡Œä»»ä½•ç§»åŠ¨åº”ç”¨ç¨‹åºä¸­ä»»ä½•ä»»åŠ¡çš„VCIã€‚AutoTaskçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºç¼ºä¹çŸ¥è¯†ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨æœªçŸ¥ç¯å¢ƒï¼ˆä¾‹å¦‚GUIï¼‰ä¸­å®ŒæˆæœªçŸ¥ä»»åŠ¡ï¼ˆä¾‹å¦‚ç”¨æˆ·å‘½ä»¤ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒAutoTaské‡‡ç”¨äº†ä¸¤ç§ç­–ç•¥ï¼šï¼ˆ1ï¼‰è¯•é”™ï¼šAutoTaskæ¢ç´¢GUIï¼Œå°è¯•æ½œåœ¨çš„æ“ä½œåºåˆ—ï¼Œå¹¶é€šè¿‡å›æº¯ä»é”™è¯¯ä¸­æ¢å¤ï¼›ï¼ˆ2ï¼‰ä»ç¯å¢ƒä¸­å­¦ä¹ ï¼šAutoTaskåœ¨æ¢ç´¢è¿‡ç¨‹ä¸­ç§¯ç´¯ç»éªŒï¼Œå¹¶ä»è¿™äº›ç»éªŒä¸­æ€»ç»“æ­£ç¡®çš„çŸ¥è¯†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ¢ç´¢ä¸å­¦ä¹ ç­–ç•¥<br>AutoTaské‡‡ç”¨â€œæ¢ç´¢-å­¦ä¹ â€ç­–ç•¥ï¼Œé€šè¿‡è¯•é”™å’Œä»ç¯å¢ƒä¸­å­¦ä¹ æ¥å®Œæˆä»»åŠ¡ã€‚è¯•é”™é˜¶æ®µï¼ŒAutoTaskæ¢ç´¢GUIï¼Œå°è¯•å¯èƒ½çš„æ“ä½œåºåˆ—ï¼Œå¹¶åœ¨å¿…è¦æ—¶é€šè¿‡å›æº¯æ¥çº æ­£é”™è¯¯ã€‚å­¦ä¹ é˜¶æ®µï¼ŒAutoTaskå°†æ¢ç´¢è¿‡ç¨‹ä¸­çš„ç»éªŒæ€»ç»“ä¸ºçŸ¥è¯†ï¼ŒåŒ…æ‹¬ç¯å¢ƒçŸ¥è¯†ã€ä»»åŠ¡çŸ¥è¯†å’Œæ‰§è¡ŒçŸ¥è¯†ï¼Œä»¥å¢å¼ºå…¶æ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰<br>AutoTaskåˆ©ç”¨LLMæ¥ç†è§£ç”¨æˆ·å‘½ä»¤å’ŒGUIè¯­ä¹‰ï¼Œå¹¶æ ¹æ®ç»éªŒæ€»ç»“çŸ¥è¯†ã€‚LLMå¯ä»¥å¸®åŠ©AutoTaskæ›´å¥½åœ°ç†è§£ç”¨æˆ·æ„å›¾å’Œå‚æ•°ï¼Œä»è€Œæé«˜å‘½ä»¤ç†è§£çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒLLMè¿˜å¯ä»¥å¸®åŠ©AutoTaskç”Ÿæˆæ›´æœ‰æ•ˆçš„æ“ä½œåºåˆ—ï¼Œå¹¶é¿å…æ‰§è¡Œé”™è¯¯ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>AutoTaskåœ¨Androidè®¾å¤‡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å…¶å¯è¡Œæ€§ã€‚AutoTaskåœ¨æ‰§è¡Œç”¨æˆ·æŒ‡ä»¤æ–¹é¢çš„æˆåŠŸç‡æ˜¾è‘—é«˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”éšç€çŸ¥è¯†çš„ç§¯ç´¯ï¼Œå…¶æ‰§è¡Œæ•ˆç‡ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AutoTaskçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•å¯ä»¥ä¸ºå…¶ä»–VCIçš„å¼€å‘æä¾›å€Ÿé‰´ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†AutoTaskçš„â€œæ¢ç´¢-å­¦ä¹ â€ç­–ç•¥åº”ç”¨äºå…¶ä»–ç±»å‹çš„äº¤äº’ç•Œé¢ï¼Œå¦‚ç½‘é¡µæµè§ˆå™¨æˆ–å‘½ä»¤è¡Œç•Œé¢ã€‚æ­¤å¤–ï¼ŒAutoTaskè¿˜å¯ä»¥ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆï¼Œä¾‹å¦‚å¤æ‚ä»»åŠ¡åˆ†è§£ç³»ç»Ÿï¼Œä»¥æ»¡è¶³ç”¨æˆ·æ›´å¤æ‚çš„äº¤äº’éœ€æ±‚ã€‚</td>
    </tr>
    <tr>
      <th>18</th>
      <td>WebCanvas: Benchmarking Web Agents in Online Environments</td>
      <td>For web agents to be practically useful, they must adapt to the continuously<br>evolving web environment characterized by frequent updates to user interfaces<br>and content. However, most existing benchmarks only capture the static aspects<br>of the web. To bridge this gap, we introduce WebCanvas, an innovative online<br>evaluation framework for web agents that effectively addresses the dynamic<br>nature of web interactions. WebCanvas contains three main components to<br>facilitate realistic assessments: (1) A novel evaluation metric which reliably<br>capture critical intermediate actions or states necessary for task completions<br>while disregarding noise caused by insignificant events or changed<br>web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version<br>of original Mind2Web static dataset containing 542 tasks with 2439 intermediate<br>evaluation states; (3) Lightweight and generalizable annotation tools and<br>testing pipelines that enables the community to collect and maintain the<br>high-quality, up-to-date dataset. Building on WebCanvas, we open-source an<br>agent framework with extensible modules for reasoning, providing a foundation<br>for the community to conduct online inference and evaluations. Our<br>best-performing agent achieves a task success rate of 23.1% and a task<br>completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we<br>analyze the performance discrepancies across various websites, domains, and<br>experimental environments. We encourage the community to contribute further<br>insights on online agent evaluation, thereby advancing this field of research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebCanvasï¼šåœ¨çº¿ç¯å¢ƒä¸­Webä»£ç†çš„åŸºå‡†æµ‹è¯•æ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äº’è”ç½‘çš„å¿«é€Ÿå‘å±•ï¼ŒWebä»£ç†åœ¨å¯¼èˆªå’Œä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Webä»£ç†è¯„ä¼°æ¡†æ¶å¤§å¤šåªå…³æ³¨é™æ€çš„Webé¡µé¢ï¼Œæ— æ³•æœ‰æ•ˆè¯„ä¼°ä»£ç†åœ¨åŠ¨æ€ã€ä¸æ–­å˜åŒ–çš„Webç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WebCanvasï¼Œä¸€ä¸ªåˆ›æ–°çš„åœ¨çº¿è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ›´çœŸå®åœ°è¯„ä¼°Webä»£ç†åœ¨åŠ¨æ€Webäº¤äº’ä¸­çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…³é”®èŠ‚ç‚¹æ ‡æ³¨çš„è¿›åº¦æ„ŸçŸ¥è¯„ä¼°<br>WebCanvaså¼•å…¥äº†â€œå…³é”®èŠ‚ç‚¹â€çš„æ¦‚å¿µï¼Œå³å®Œæˆç‰¹å®šWebä»»åŠ¡æ‰€å¿…éœ€çš„æ­¥éª¤ã€‚é€šè¿‡æ ‡æ³¨å…³é”®èŠ‚ç‚¹ï¼Œå¯ä»¥æ›´è¯¦ç»†åœ°åˆ†æä»£ç†çš„è¡Œä¸ºï¼Œä»è€Œæ·±å…¥äº†è§£å…¶å†³ç­–çš„ä¼˜ç¼ºç‚¹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç¤¾åŒºé©±åŠ¨çš„æ ‡æ³¨å¹³å°<br>WebCanvasæ”¯æŒé€šè¿‡é«˜çº§è®°å½•æµè§ˆå™¨æ’ä»¶è®°å½•å’Œæ ‡æ³¨Webä»»åŠ¡åŠå…¶å¯¹åº”çš„å…³é”®èŠ‚ç‚¹è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿˜å¼€æºäº†ä¸€ä¸ªå…·æœ‰å¯æ‰©å±•æ¨ç†æ¨¡å—çš„ä»£ç†æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºæˆå‘˜åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­è¿›è¡Œåœ¨çº¿è¯„ä¼°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæˆæœ¬æ•ˆç›Šçš„æ•°æ®ç»´æŠ¤<br>WebCanvasé‡‡ç”¨å®šæœŸç›‘æ§å’Œè‡ªåŠ¨è­¦æŠ¥çš„ç»´æŠ¤ç­–ç•¥ï¼Œä»¥å¿«é€Ÿè¯†åˆ«æ— æ•ˆçš„åŠ¨ä½œåºåˆ—å’Œå…³é”®èŠ‚ç‚¹ã€‚å½“æ•°æ®å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæµ‹è¯•æŠ¥å‘Šä¼šæä¾›é”™è¯¯ä¿¡æ¯ï¼ŒæŒ‡å¯¼æ•°æ®æ‰€æœ‰è€…è¿›è¡Œå¿«é€Ÿæœ‰æ•ˆçš„æ•°æ®ä¿®æ­£ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åŸºäºWebCanvasæ¡†æ¶ï¼Œæœ¬æ–‡åˆ›å»ºäº†Mind2Web-Liveæ•°æ®é›†ï¼ŒåŒ…å«542ä¸ªä»»åŠ¡å’Œ2439ä¸ªå…³é”®èŠ‚ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4-turboæ¨¡å‹åœ¨ä»»åŠ¡æˆåŠŸç‡å’Œä»»åŠ¡å®Œæˆç‡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œåˆ†åˆ«ä¸º23.1%å’Œ48.8%ã€‚æ­¤å¤–ï¼Œåœ¨çº¿è¯„ä¼°ä¸ç¦»çº¿è¯„ä¼°çš„ç»“æœå­˜åœ¨å·®å¼‚ï¼Œè¡¨æ˜åœ¨åŠ¨æ€åœ¨çº¿ç¯å¢ƒä¸­ï¼Œæ¨¡å‹çš„è¡¨ç°å¯èƒ½ä¸å¦‚åœ¨é™æ€ç¦»çº¿ç¯å¢ƒä¸­ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebCanvasæ¡†æ¶ä¸ºWebä»£ç†çš„åœ¨çº¿è¯„ä¼°æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºç¤¾åŒºæˆå‘˜æä¾›äº†ä¸€ä¸ªå¹³å°æ¥æ„å»ºæ•°æ®é›†å’Œè¯„ä¼°Webä»£ç†æ¡†æ¶å’Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å…³é”®èŠ‚ç‚¹æ ‡æ³¨ä½œä¸ºä¸­é—´å¥–åŠ±çš„ä½¿ç”¨ï¼Œå¹¶å‘ç°Webä»£ç†å¯ä»¥ä»äººç±»æä¾›çš„å…³é”®èŠ‚ç‚¹æ ‡æ³¨ä¸­å—ç›Šã€‚</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents</td>
      <td>Large Language Models (LLMs) have shown remarkable capabilities in natural<br>language tasks requiring complex reasoning, yet their application in agentic,<br>multi-step reasoning within interactive environments remains a difficult<br>challenge. Traditional supervised pre-training on static datasets falls short<br>in enabling autonomous agent capabilities needed to perform complex<br>decision-making in dynamic settings like web navigation. Previous attempts to<br>bridge this ga-through supervised fine-tuning on curated expert<br>demonstrations-often suffer from compounding errors and limited exploration<br>data, resulting in sub-optimal policy outcomes. To overcome these challenges,<br>we propose a framework that combines guided Monte Carlo Tree Search (MCTS)<br>search with a self-critique mechanism and iterative fine-tuning on agent<br>interactions using an off-policy variant of the Direct Preference Optimization<br>(DPO) algorithm. Our method allows LLM agents to learn effectively from both<br>successful and unsuccessful trajectories, thereby improving their<br>generalization in complex, multi-step reasoning tasks. We validate our approach<br>in the WebShop environment-a simulated e-commerce platform where it<br>consistently outperforms behavior cloning and reinforced fine-tuning baseline,<br>and beats average human performance when equipped with the capability to do<br>online search. In real-world booking scenarios, our methodology boosts Llama-3<br>70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340%<br>relative increase) after a single day of data collection and further to 95.4%<br>with online search. We believe this represents a substantial leap forward in<br>the capabilities of autonomous agents, paving the way for more sophisticated<br>and reliable decision-making in real-world settings.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Agent Qï¼šè‡ªä¸»AIä»£ç†çš„é«˜çº§æ¨ç†ä¸å­¦ä¹ <br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦å¤æ‚æ¨ç†çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨äº¤äº’å¼ç¯å¢ƒä¸­çš„å¤šæ­¥æ¨ç†åº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç›‘ç£é¢„è®­ç»ƒæ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œå¤æ‚å†³ç­–æ—¶æ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¼•å¯¼è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’Œè‡ªæˆ‘æ‰¹è¯„æœºåˆ¶ï¼Œä»¥åŠä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®—æ³•è¿›è¡Œè¿­ä»£å¾®è°ƒçš„æ¡†æ¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å¯¼MCTSæœç´¢<br>ä½¿ç”¨MCTSç®—æ³•æ¥æŒ‡å¯¼ä»£ç†åœ¨ç½‘é¡µä¸Šçš„æ¢ç´¢ï¼Œä»¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œå¹¶é€æ­¥æ”¹è¿›ç­–ç•¥ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè‡ªæˆ‘æ‰¹è¯„æœºåˆ¶<br>é€šè¿‡AIåé¦ˆå’Œè‡ªæˆ‘æ‰¹è¯„ï¼Œä»£ç†åœ¨æ¯ä¸ªèŠ‚ç‚¹æä¾›è‡ªæˆ‘è¯„ä¼°åé¦ˆï¼Œä½œä¸ºä¸­é—´å¥–åŠ±ï¼Œå¸®åŠ©å¼•å¯¼æœç´¢æ­¥éª¤ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè¿­ä»£å¾®è°ƒ<br>ä½¿ç”¨DPOç®—æ³•ä»ä»£ç†äº¤äº’ä¸­å­¦ä¹ ï¼ŒåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥çš„è½¨è¿¹ï¼Œä»¥æé«˜ä»£ç†åœ¨å¤æ‚å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨WebShopç¯å¢ƒä¸­ï¼ŒAgent Qæ–¹æ³•å§‹ç»ˆä¼˜äºè¡Œä¸ºå…‹éš†å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒåŸºçº¿ï¼Œå¹¶åœ¨é…å¤‡åœ¨çº¿æœç´¢åŠŸèƒ½æ—¶å‡»è´¥äº†å¹³å‡äººç±»æ€§èƒ½ã€‚åœ¨ç°å®ä¸–ç•Œçš„é¢„è®¢åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•å°†Llama-3 70Bæ¨¡å‹çš„é›¶æ ·æœ¬æ€§èƒ½ä»18.6%æé«˜åˆ°81.7%ï¼Œå¹¶åœ¨é…å¤‡åœ¨çº¿æœç´¢åŠŸèƒ½åè¿›ä¸€æ­¥æé«˜åˆ°95.4%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„Agent Qæ¡†æ¶ä¸ºè‡ªä¸»AIä»£ç†çš„å‘å±•æä¾›äº†é‡è¦çš„ä¸€æ­¥ï¼Œé€šè¿‡å…¶æœç´¢å’Œè‡ªæˆ‘æ‰¹è¯„èƒ½åŠ›ï¼Œä¸ºäº¤äº’å¼ç¯å¢ƒä¸­çš„å¯é å¤šæ­¥å†³ç­–åˆ¶å®šäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚</td>
    </tr>
    <tr>
      <th>20</th>
      <td>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</td>
      <td>Large language models (LLMs) have shown remarkable potential as autonomous<br>agents, particularly in web-based tasks. However, existing LLM web agents<br>heavily rely on expensive proprietary LLM APIs, while open LLMs lack the<br>necessary decision-making capabilities. This paper introduces WebRL, a<br>self-evolving online curriculum reinforcement learning framework designed to<br>train high-performance web agents using open LLMs. WebRL addresses three key<br>challenges in building LLM web agents, including the scarcity of training<br>tasks, sparse feedback signals, and policy distribution drift in online<br>learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that<br>generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised<br>reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure<br>consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4<br>models into proficient web agents. On WebArena-Lite, WebRL improves the success<br>rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.<br>These open models significantly surpass the performance of GPT-4-Turbo (17.6%)<br>and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained<br>on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's<br>effectiveness in bridging the gap between open and proprietary LLM-based web<br>agents, paving the way for more accessible and powerful autonomous web<br>interaction systems.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebRLï¼šåŸºäºè‡ªæ¼”åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ çš„LLMç½‘ç»œä»£ç†è®­ç»ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç°æœ‰çš„LLMç½‘ç»œä»£ç†ä¸»è¦ä¾èµ–äºæ˜‚è´µçš„ä¸“æœ‰LLM APIï¼Œè€Œå¼€æºLLMsåˆ™ç¼ºä¹å¿…è¦çš„å†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†WebRLï¼Œä¸€ä¸ªè‡ªæ¼”åŒ–åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨å¼€æºLLMsè®­ç»ƒé«˜æ€§èƒ½çš„ç½‘ç»œä»£ç†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªæ¼”åŒ–è¯¾ç¨‹<br>WebRLé€šè¿‡ä»å¤±è´¥çš„å°è¯•ä¸­ç”Ÿæˆæ–°ä»»åŠ¡ï¼Œæ„å»ºäº†ä¸€ä¸ªè‡ªæ¼”åŒ–è¯¾ç¨‹ï¼Œä»è€Œè§£å†³äº†è®­ç»ƒä»»åŠ¡ç¨€ç¼ºçš„é—®é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šç»“æœç›‘ç£å¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰<br>WebRLå¼•å…¥äº†ä¸€ä¸ªå¥å£®çš„ORMæ¥è¯„ä¼°ä»»åŠ¡æˆåŠŸï¼Œè§£å†³äº†åé¦ˆä¿¡å·ç¨€ç–çš„é—®é¢˜ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ç­–ç•¥<br>WebRLé‡‡ç”¨è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬KLçº¦æŸç­–ç•¥æ›´æ–°å’Œç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œä»¥é˜²æ­¢åœ¨çº¿å­¦ä¹ ä¸­çš„ç­–ç•¥åˆ†å¸ƒæ¼‚ç§»ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>WebRLåœ¨WebArena-Liteä¸Šæ˜¾è‘—æé«˜äº†Llama-3.1å’ŒGLM-4æ¨¡å‹çš„æˆåŠŸç‡ï¼Œåˆ†åˆ«ä»4.8%æé«˜åˆ°42.4%å’Œä»6.1%æé«˜åˆ°43%ã€‚è¿™äº›å¼€æºæ¨¡å‹æ˜¾è‘—ä¼˜äºGPT-4-Turboå’ŒGPT-4oï¼Œå¹¶è¶…è¿‡äº†ä¹‹å‰åœ¨å¼€æºLLMsä¸Šè®­ç»ƒçš„æœ€å…ˆè¿›çš„ç½‘ç»œä»£ç†ï¼ˆAutoWebGLMï¼‰ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebRLä¸ºè®­ç»ƒé«˜æ€§èƒ½çš„ç½‘ç»œä»£ç†æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ¡†æ¶ï¼Œå…¶è‡ªæ¼”åŒ–è¯¾ç¨‹ã€ORMå’Œè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ç­–ç•¥å¯ä»¥åº”ç”¨äºå…¶ä»–LLMä»£ç†è®­ç»ƒä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒWebRLçš„æˆåŠŸè¡¨æ˜ï¼Œå¼€æºLLMsåœ¨ç½‘ç»œä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æ›¿ä»£æ˜‚è´µçš„ä¸“æœ‰LLM APIã€‚</td>
    </tr>
    <tr>
      <th>21</th>
      <td>NaviQAte: Functionality-Guided Web Application Navigation</td>
      <td>End-to-end web testing is challenging due to the need to explore diverse web<br>application functionalities. Current state-of-the-art methods, such as<br>WebCanvas, are not designed for broad functionality exploration; they rely on<br>specific, detailed task descriptions, limiting their adaptability in dynamic<br>web environments. We introduce NaviQAte, which frames web application<br>exploration as a question-and-answer task, generating action sequences for<br>functionalities without requiring detailed parameters. Our three-phase approach<br>utilizes advanced large language models like GPT-4o for complex decision-making<br>and cost-effective models, such as GPT-4o mini, for simpler tasks. NaviQAte<br>focuses on functionality-guided web application navigation, integrating<br>multi-modal inputs such as text and images to enhance contextual understanding.<br>Evaluations on the Mind2Web-Live and Mind2Web-Live-Abstracted datasets show<br>that NaviQAte achieves a 44.23% success rate in user task navigation and a<br>38.46% success rate in functionality navigation, representing a 15% and 33%<br>improvement over WebCanvas. These results underscore the effectiveness of our<br>approach in advancing automated web application testing.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | NaviQAteï¼šåŸºäºåŠŸèƒ½çš„Webåº”ç”¨ç¨‹åºå¯¼èˆª<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€Webåº”ç”¨ç¨‹åºçš„æ—¥ç›Šæ™®åŠï¼Œç¡®ä¿å…¶è´¨é‡å’ŒåŠŸèƒ½å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ‰‹åŠ¨æµ‹è¯•æ–¹æ³•è€—æ—¶ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€Œç°æœ‰çš„è‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·å¯èƒ½æ— æ³•å…¨é¢è¦†ç›–æ‰€æœ‰åŠŸèƒ½ï¼Œå¯¼è‡´æ½œåœ¨çš„é”™è¯¯å’Œå¯ç”¨æ€§é—®é¢˜è¢«å¿½è§†ã€‚æ­¤å¤–ï¼ŒWebåº”ç”¨ç¨‹åºçš„åŠ¨æ€æ€§å’Œå¤æ‚æ€§ä½¿å¾—è‡ªåŠ¨åŒ–æµ‹è¯•å˜å¾—æ›´åŠ å›°éš¾ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>NaviQAteæå‡ºäº†ä¸€ç§åŸºäºåŠŸèƒ½çš„Webåº”ç”¨ç¨‹åºå¯¼èˆªæ–¹æ³•ï¼Œå°†Webåº”ç”¨ç¨‹åºæ¢ç´¢è§†ä¸ºä¸€ä¸ªé—®ç­”ä»»åŠ¡ï¼Œæ— éœ€è¯¦ç»†å‚æ•°å³å¯ç”ŸæˆåŠŸèƒ½æ€§çš„æ“ä½œåºåˆ—ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š<br><br>1. **è¡ŒåŠ¨è§„åˆ’**ï¼šä½¿ç”¨æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯å°†æŠ½è±¡ä»»åŠ¡è½¬æ¢ä¸ºå¯æ“ä½œçš„æè¿°ï¼Œå¹¶ä»å¤šæ¨¡æ€æ¥æºï¼ˆå¦‚å…ƒæ ‡ç­¾ã€å…ˆå‰æ“ä½œå’Œå±å¹•æˆªå›¾ï¼‰ä¸­æå–ç½‘é¡µä¸Šä¸‹æ–‡ï¼Œä»¥åˆ›å»ºå½“å‰çŠ¶æ€çš„æŠ½è±¡è¡¨ç¤ºã€‚<br>2. **é€‰æ‹©æå–**ï¼šè¯†åˆ«ç½‘é¡µä¸Šçš„å¯æ“ä½œå…ƒç´ ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„é€‰æ‹©æ’åç³»ç»Ÿå¯¹å…¶è¿›è¡Œæ’åºã€‚å…ƒç´ æ ¹æ®å…¶ä¸é¢„æµ‹ä¸‹ä¸€æ­¥çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œæ’åï¼Œå¹¶ç»“åˆé™„è¿‘çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æä¾›é¢å¤–ä¸Šä¸‹æ–‡ã€‚<br>3. **å†³ç­–åˆ¶å®š**ï¼šç»“åˆä»»åŠ¡å†å²ã€æ³¨é‡Šå±å¹•æˆªå›¾å’Œæ’åçš„å¯æ“ä½œå…ƒç´ æ¥é€‰æ‹©æœ€ä½³è¡ŒåŠ¨ã€‚è§†è§‰æç¤ºå¸®åŠ©æ¨¡å‹è§£é‡Šç©ºé—´å¸ƒå±€å’Œä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°å‡†ç¡®å’Œé«˜æ•ˆçš„å¯¼èˆªã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨Mind2Web-Liveå’ŒMind2Web-Live-Abstractedæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒNaviQAteåœ¨ç”¨æˆ·ä»»åŠ¡å¯¼èˆªå’ŒåŠŸèƒ½å¯¼èˆªæ–¹é¢åˆ†åˆ«å–å¾—äº†44.23%å’Œ38.46%çš„æˆåŠŸç‡ï¼Œæ¯”WebCanvasåˆ†åˆ«æé«˜äº†15%å’Œ33%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>NaviQAteçš„åŸºäºåŠŸèƒ½çš„å¯¼èˆªæ–¹æ³•ä¸ºè‡ªåŠ¨åŒ–Webåº”ç”¨ç¨‹åºæµ‹è¯•æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚å…¶å¤šé˜¶æ®µã€å¤šæ¨¡å‹çš„æ–¹æ³•è®ºä»¥åŠå¤šæ¨¡æ€è¾“å…¥çš„é›†æˆï¼Œä¸ºæé«˜Webå¯¼èˆªçš„å‡†ç¡®æ€§å’Œæ•ˆç‡æä¾›äº†å®è´µçš„ç»éªŒã€‚æ­¤å¤–ï¼ŒNaviQAteçš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–Webåº”ç”¨ç¨‹åºæµ‹è¯•ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>22</th>
      <td>Falcon-UI: Understanding GUI Before Following User Instructions</td>
      <td>Pursuing human-like interaction for Graphical User Interface (GUI) agents<br>requires understanding the GUI context and following user instructions.<br>However, existing works typically couple these two aspects and focus more on<br>instruct-following abilities, while ignoring the importance of understanding<br>the GUI context. In this paper, we introduce an instruction-free GUI navigation<br>dataset, termed Insight-UI Dataset, to enhance model comprehension of GUI<br>environments. Insight-UI Dataset is automatically generated from the Common<br>Crawl corpus, simulating various platforms -- including iOS, Android, Windows,<br>and Linux -- across multiple resolutions on 312K domains. Although GUI<br>interactions vary by context, diverse interfaces share common internal<br>patterns, such as clicking an item to view its details. It implies the<br>feasibility of independent GUI operation learning, followed by joint<br>optimization with instruction tuning. Thereby, we develop the GUI agent model<br>Falcon-UI, which is initially pretrained on Insight-UI Dataset and subsequently<br>fine-tuned on Android and Web GUI datasets, including AITW, AITZ, Android<br>Control, and Mind2Web. With 7 billion parameters, Falcon-UI achieves accuracy<br>comparable to the 72 billion-parameter Qwen2VL on AITZ, validating the<br>alignment between GUI context comprehension and agent performance. Our code and<br>dataset will be open-sourced.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | Falcon-UIï¼šç†è§£GUIç¯å¢ƒï¼Œå®ç°æ›´æ™ºèƒ½çš„ç”¨æˆ·äº¤äº’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åœ¨æ“ä½œç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œè‡ªåŠ¨ä¸GUIäº¤äº’çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç°æœ‰çš„GUIä»£ç†æ¨¡å‹é€šå¸¸ä¾èµ–äºç³»ç»ŸAPIæˆ–ç»“æ„åŒ–è¾“å…¥ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæ„å»ºåŒ…å«å¤šæ ·åŒ–ã€é«˜è´¨é‡ç”¨æˆ·æŒ‡ä»¤çš„æ•°æ®é›†æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†GUIä»£ç†æ¨¡å‹çš„å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†Falcon-UIï¼Œä¸€ä¸ªåŸºäºè§†è§‰è¾“å…¥çš„GUIä»£ç†æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°æ›´æ™ºèƒ½çš„ç”¨æˆ·äº¤äº’ã€‚Falcon-UIçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæŒ‡ä»¤æ— å…³çš„GUIå¯¼èˆªæ•°æ®é›†ï¼ˆInsight-UI Datasetï¼‰<br>ä¸ºäº†è§£å†³æ„å»ºå¤šæ ·åŒ–ã€é«˜è´¨é‡ç”¨æˆ·æŒ‡ä»¤æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†Insight-UI Datasetï¼Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨ç”Ÿæˆçš„ã€æŒ‡ä»¤æ— å…³çš„GUIå¯¼èˆªæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¨¡æ‹Ÿäº†å¤šç§å¹³å°ï¼ˆiOSã€Androidã€Windowsã€Linuxï¼‰å’Œå¤šç§åˆ†è¾¨ç‡ï¼Œæ¶µç›–äº†312Kä¸ªåŸŸåï¼Œä¸ºGUIä»£ç†æ¨¡å‹æä¾›äº†ä¸°å¯Œçš„GUIç¯å¢ƒçŸ¥è¯†ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šGUIç¯å¢ƒç†è§£ä¸æŒ‡ä»¤è·Ÿéšçš„è§£è€¦<br>Falcon-UIé‡‡ç”¨äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œå°†GUIç¯å¢ƒç†è§£ä¸æŒ‡ä»¤è·Ÿéšè§£è€¦ã€‚é¦–å…ˆï¼ŒFalcon-UIåœ¨Insight-UI Datasetä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ GUIç¯å¢ƒä¸­çš„å¸¸è§æ¨¡å¼å’Œæ“ä½œé€»è¾‘ã€‚ç„¶åï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒFalcon-UIæ ¹æ®ç”¨æˆ·æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼Œå®ç°æ›´å‡†ç¡®çš„GUIäº¤äº’ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨å¤šä¸ªGUIä»£ç†æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFalcon-UIå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨Android in the Wild (AITW)å’ŒAndroid in the Zoo (AITZ)æ•°æ®é›†ä¸Šï¼ŒFalcon-UIçš„å‡†ç¡®ç‡ä¸72äº¿å‚æ•°çš„Qwen2VLæ¨¡å‹ç›¸å½“ï¼ŒéªŒè¯äº†GUIç¯å¢ƒç†è§£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„Falcon-UIæ¨¡å‹å’ŒInsight-UI Datasetä¸ºGUIä»£ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚Falcon-UIçš„è§£è€¦è®­ç»ƒèŒƒå¼å’ŒInsight-UI Datasetçš„è‡ªåŠ¨ç”Ÿæˆæœºåˆ¶ï¼Œä¸ºæ„å»ºæ›´æ™ºèƒ½ã€æ›´é€šç”¨çš„GUIä»£ç†æ¨¡å‹æä¾›äº†é‡è¦çš„å‚è€ƒä»·å€¼ã€‚</td>
    </tr>
    <tr>
      <th>23</th>
      <td>TaskBench: Benchmarking Large Language Models for Task Automation</td>
      <td>In recent years, the remarkable progress of large language models (LLMs) has<br>sparked interest in task automation, which involves decomposing complex tasks<br>described by user instructions into sub-tasks and invoking external tools to<br>execute them, playing a central role in autonomous agents. However, there is a<br>lack of systematic and standardized benchmarks to promote the development of<br>LLMs in task automation. To address this, we introduce TaskBench, a<br>comprehensive framework to evaluate the capability of LLMs in task automation.<br>Specifically, task automation can be divided into three critical stages: task<br>decomposition, tool selection, and parameter prediction. To tackle the<br>complexities inherent in these stages, we introduce the concept of Tool Graph<br>to represent decomposed tasks and adopt a back-instruct method to generate<br>high-quality user instructions. We propose TaskEval, a multi-faceted evaluation<br>methodology that assesses LLM performance across these three stages. Our<br>approach combines automated construction with rigorous human verification,<br>ensuring high consistency with human evaluation. Experimental results<br>demonstrate that TaskBench effectively reflects the capabilities of various<br>LLMs in task automation. It provides insights into model performance across<br>different task complexities and domains, pushing the boundaries of what current<br>models can achieve. TaskBench offers a scalable, adaptable, and reliable<br>benchmark for advancing LLM-based autonomous agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | TaskBenchï¼šå¤§å‹è¯­è¨€æ¨¡å‹ä»»åŠ¡è‡ªåŠ¨åŒ–çš„å…¨é¢è¯„ä¼°æ¡†æ¶<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¿€å‘äº†äººä»¬å¯¹ä»»åŠ¡è‡ªåŠ¨åŒ–çš„å…´è¶£ã€‚ä»»åŠ¡è‡ªåŠ¨åŒ–æ¶‰åŠå°†ç”¨æˆ·æŒ‡ä»¤æè¿°çš„å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥æ‰§è¡Œå®ƒä»¬ï¼Œåœ¨è‡ªä¸»ä»£ç†ä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ç³»ç»ŸåŒ–å’Œæ ‡å‡†åŒ–çš„åŸºå‡†æ¥ä¿ƒè¿›LLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TaskBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå¼•å…¥å·¥å…·å›¾ï¼ˆTool Graphï¼‰çš„æ¦‚å¿µ<br>ä¸ºäº†åº”å¯¹ä»»åŠ¡åˆ†è§£ã€å·¥å…·é€‰æ‹©å’Œå‚æ•°é¢„æµ‹ç­‰é˜¶æ®µçš„å¤æ‚æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†å·¥å…·å›¾çš„æ¦‚å¿µã€‚å·¥å…·å›¾å¯ä»¥è¡¨ç¤ºåˆ†è§£åçš„ä»»åŠ¡ä¹‹é—´çš„å…³ç³»å’Œä¾èµ–æ€§ï¼Œå…‹æœäº†ç°æœ‰åŸºå‡†ä¸­ç®€å•APIæ–‡æ¡£æˆ–åŸºäºæ¨¡æ¿æ–¹æ³•çš„å±€é™æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé‡‡ç”¨åå‘æŒ‡ä»¤ï¼ˆBack-Instructï¼‰æ–¹æ³•<br>æœ¬æ–‡é‡‡ç”¨åå‘æŒ‡ä»¤æ–¹æ³•æ¥ç”Ÿæˆé«˜è´¨é‡çš„æŒ‡ä»¤ã€‚é€šè¿‡ä»å·¥å…·å›¾ä¸­é‡‡æ ·å­å›¾ï¼Œå¹¶ä½¿ç”¨LLMsç”Ÿæˆç›¸åº”çš„æŒ‡ä»¤ï¼Œå¯ä»¥ç¡®ä¿ç”Ÿæˆçš„æŒ‡ä»¤æ—¢è‡ªç„¶åˆç¬¦åˆå®é™…å·¥å…·ä½¿ç”¨æ¨¡å¼ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæå‡ºTaskEvalè¯„ä¼°æ–¹æ³•<br>ä¸ºäº†å…¨é¢è¯„ä¼°LLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†TaskEvalè¯„ä¼°æ–¹æ³•ã€‚TaskEvalåŒ…æ‹¬ä¸‰ä¸ªå…³é”®é˜¶æ®µçš„è¯„ä¼°ï¼šä»»åŠ¡åˆ†è§£ã€å·¥å…·é€‰æ‹©å’Œå‚æ•°é¢„æµ‹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è‡ªåŠ¨æ„å»ºå’Œä¸¥æ ¼çš„äººå·¥éªŒè¯ï¼Œç¡®ä¿ä¸äººå·¥è¯„ä¼°çš„é«˜åº¦ä¸€è‡´æ€§ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒTaskBenchèƒ½å¤Ÿæœ‰æ•ˆåœ°åæ˜ ä¸åŒLLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒæä¾›äº†å¯¹æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å¤æ‚æ€§å’Œé¢†åŸŸä¸­çš„æ€§èƒ½çš„è§è§£ï¼Œæ¨åŠ¨äº†å½“å‰æ¨¡å‹æ‰€èƒ½è¾¾åˆ°çš„è¾¹ç•Œã€‚TaskBenchæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€å¯é€‚åº”å’Œå¯é çš„åŸºå‡†ï¼Œç”¨äºæ¨è¿›åŸºäºLLMçš„è‡ªä¸»ä»£ç†çš„å‘å±•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>TaskBenchä¸ºLLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„è¯„ä¼°æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œå…¶åˆ›æ–°çš„æ•°æ®ç”ŸæˆæŠ€æœ¯å’Œä¸¥æ ¼çš„è¯„ä¼°æ–¹æ³•ä¸ºLLMsåœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–æ–¹é¢çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚æ­¤å¤–ï¼ŒTaskBenchçš„å¼•å…¥ä¹Ÿä¸ºLLMsåœ¨å¤æ‚ä»»åŠ¡è‡ªåŠ¨åŒ–åœºæ™¯ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>24</th>
      <td>ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation</td>
      <td>Graphical User Interface (GUI) automation holds significant promise for<br>assisting users with complex tasks, thereby boosting human productivity.<br>Existing works leveraging Large Language Model (LLM) or LLM-based AI agents<br>have shown capabilities in automating tasks on Android and Web platforms.<br>However, these tasks are primarily aimed at simple device usage and<br>entertainment operations. This paper presents a novel benchmark, AssistGUI, to<br>evaluate whether models are capable of manipulating the mouse and keyboard on<br>the Windows platform in response to user-requested tasks. We carefully<br>collected a set of 100 tasks from nine widely-used software applications, such<br>as, After Effects and MS Word, each accompanied by the necessary project files<br>for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied<br>Agent framework, which incorporates a sophisticated GUI parser driven by an<br>LLM-agent and an enhanced reasoning mechanism adept at handling lengthy<br>procedural tasks. Our experimental results reveal that our GUI Parser and<br>Reasoning mechanism outshine existing methods in performance. Nevertheless, the<br>potential remains substantial, with the best model attaining only a 46% success<br>rate on our benchmark. We conclude with a thorough analysis of the current<br>methods' limitations, setting the stage for future breakthroughs in this<br>domain.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | ASSISTGUIï¼šåŸºäºä»»åŠ¡çš„æ¡Œé¢å›¾å½¢ç”¨æˆ·ç•Œé¢è‡ªåŠ¨åŒ–<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–åœ¨æé«˜äººç±»ç”Ÿäº§åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–åŸºäºLLMçš„AIä»£ç†åœ¨Androidå’ŒWebå¹³å°ä¸Šè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œä½†è¿™äº›ä»»åŠ¡å¤§å¤šé’ˆå¯¹ç®€å•çš„è®¾å¤‡ä½¿ç”¨å’Œå¨±ä¹æ“ä½œã€‚ç„¶è€Œï¼Œå¯¹äºå¤æ‚çš„æ¡Œé¢åº”ç”¨ç¨‹åºï¼Œç”¨æˆ·å¾€å¾€é¢ä¸´ç€é™¡å³­çš„å­¦ä¹ æ›²çº¿ï¼Œè¿™é™åˆ¶äº†ä»–ä»¬çš„åˆ›é€ åŠ›å’Œç”Ÿäº§åŠ›ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä»»åŠ¡å¯¼å‘çš„æ¡Œé¢GUIè‡ªåŠ¨åŒ–æ–¹é¢çš„èƒ½åŠ›ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨åˆ©ç”¨ç”Ÿäº§åŠ›è½¯ä»¶æ–¹é¢çš„æ€§èƒ½ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šASSISTGUIåŸºå‡†<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºASSISTGUIçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨Windowså¹³å°ä¸Šæ ¹æ®ç”¨æˆ·è¯·æ±‚çš„ä»»åŠ¡æ“çºµé¼ æ ‡å’Œé”®ç›˜çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«ä»ä¹ä¸ªå¹¿æ³›ä½¿ç”¨çš„è½¯ä»¶åº”ç”¨ç¨‹åºä¸­ç²¾å¿ƒæ”¶é›†çš„100ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é™„å¸¦å¿…è¦çš„é¡¹ç›®æ–‡ä»¶ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¿›è¡Œè¯„ä¼°ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šActor-Critic Embodied Agentæ¡†æ¶<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºActor-Critic Embodied Agentï¼ˆACEï¼‰çš„å…ˆè¿›æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç”±LLMä»£ç†é©±åŠ¨çš„å¤æ‚GUIè§£æå™¨å’Œä¸€ç§å¢å¼ºçš„æ¨ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿå¤„ç†å†—é•¿çš„è¿‡ç¨‹æ€§ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šGUIè§£æå™¨ã€è¯„ä¼°æ¨¡å—å’Œæ‰§è¡Œæ¨¡å—ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„GUIè§£æå™¨å’Œæ¨ç†æœºåˆ¶åœ¨æ€§èƒ½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œå³ä½¿æœ€ä½³æ¨¡å‹åœ¨åŸºå‡†ä¸Šçš„æˆåŠŸç‡ä¹Ÿåªæœ‰46%ï¼Œè¿™è¡¨æ˜è¯¥é¢†åŸŸä»æœ‰å¾ˆå¤§çš„å‘å±•æ½œåŠ›ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ASSISTGUIåŸºå‡†å’ŒActor-Critic Embodied Agentæ¡†æ¶ä¸ºæ¡Œé¢GUIè‡ªåŠ¨åŒ–é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒã€‚ASSISTGUIåŸºå‡†å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜è¯„ä¼°å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œè€ŒActor-Critic Embodied Agentæ¡†æ¶åˆ™ä¸ºå¤„ç†å¤æ‚çš„æ¡Œé¢GUIè‡ªåŠ¨åŒ–ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ·±å…¥åˆ†æäº†å½“å‰æ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents</td>
      <td>Multimodal large language models (MLLMs) are transforming the capabilities of<br>graphical user interface (GUI) agents, facilitating their transition from<br>controlled simulations to complex, real-world applications across various<br>platforms. However, the effectiveness of these agents hinges on the robustness<br>of their grounding capability. Current GUI agents predominantly utilize<br>text-based representations such as HTML or accessibility trees, which, despite<br>their utility, often introduce noise, incompleteness, and increased<br>computational overhead. In this paper, we advocate a human-like embodiment for<br>GUI agents that perceive the environment entirely visually and directly perform<br>pixel-level operations on the GUI. The key is visual grounding models that can<br>accurately map diverse referring expressions of GUI elements to their<br>coordinates on the GUI across different platforms. We show that a simple<br>recipe, which includes web-based synthetic data and slight adaptation of the<br>LLaVA architecture, is surprisingly effective for training such visual<br>grounding models. We collect the largest dataset for GUI visual grounding so<br>far, containing 10M GUI elements and their referring expressions over 1.3M<br>screenshots, and use it to train UGround, a strong universal visual grounding<br>model for GUI agents. Empirical results on six benchmarks spanning three<br>categories (grounding, offline agent, and online agent) show that 1) UGround<br>substantially outperforms existing visual grounding models for GUI agents, by<br>up to 20% absolute, and 2) agents with UGround outperform state-of-the-art<br>agents, despite the fact that existing agents use additional text-based input<br>while ours only uses visual perception. These results provide strong support<br>for the feasibility and promises of GUI agents that navigate the digital world<br>as humans do.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | äººç±»èˆ¬çš„è§†è§‰å¯¼èˆªï¼šGUI ä»£ç†çš„é€šç”¨è§†è§‰å®šä½<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œä½¿å…¶ä»å—æ§çš„æ¨¡æ‹Ÿç¯å¢ƒè¿‡æ¸¡åˆ°å„ç§å¹³å°ä¸Šçš„å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†çš„æœ‰æ•ˆæ€§å–å†³äºå…¶å®šä½èƒ½åŠ›çš„é²æ£’æ€§ã€‚å½“å‰çš„ GUI ä»£ç†ä¸»è¦ä½¿ç”¨åŸºäºæ–‡æœ¬çš„è¡¨ç¤ºï¼Œå¦‚ HTML æˆ–å¯è®¿é—®æ€§æ ‘ï¼Œè¿™è™½ç„¶æœ‰ç”¨ï¼Œä½†å¾€å¾€ä¼šå¼•å…¥å™ªå£°ã€ä¸å®Œæ•´æ€§å’Œå¢åŠ è®¡ç®—å¼€é”€ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç±»ä¼¼äºäººç±»çš„ GUI ä»£ç†ï¼Œå®ƒå®Œå…¨é€šè¿‡è§†è§‰æ„ŸçŸ¥ç¯å¢ƒï¼Œå¹¶ç›´æ¥åœ¨ GUI ä¸Šæ‰§è¡Œåƒç´ çº§æ“ä½œã€‚å…³é”®åœ¨äºè§†è§‰å®šä½æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°å°† GUI å…ƒç´ çš„å¤šç§å¼•ç”¨è¡¨è¾¾å¼æ˜ å°„åˆ°ä¸åŒå¹³å°ä¸Šçš„ GUI åæ ‡ã€‚æœ¬æ–‡å±•ç¤ºäº†åŸºäºç½‘ç»œåˆæˆæ•°æ®å’Œ LLaVA æ¶æ„çš„ç®€å•æ–¹æ³•ï¼Œå¯¹äºè®­ç»ƒæ­¤ç±»è§†è§‰å®šä½æ¨¡å‹éå¸¸æœ‰æ•ˆã€‚æœ¬æ–‡æ”¶é›†äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ GUI è§†è§‰å®šä½æ•°æ®é›†ï¼ŒåŒ…å« 10M GUI å…ƒç´ åŠå…¶åœ¨ 1.3M æˆªå›¾ä¸­çš„å¼•ç”¨è¡¨è¾¾å¼ï¼Œå¹¶ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒäº† UGroundï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„é€šç”¨è§†è§‰å®šä½æ¨¡å‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨æ¶µç›–ä¸‰ä¸ªç±»åˆ«ï¼ˆå®šä½ã€ç¦»çº¿ä»£ç†å’Œåœ¨çº¿ä»£ç†ï¼‰çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼š<br>1. UGround åœ¨ GUI ä»£ç†çš„è§†è§‰å®šä½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„è§†è§‰å®šä½æ¨¡å‹ï¼Œç»å¯¹å€¼æé«˜äº† 20%ã€‚<br>2. ä½¿ç”¨ UGround çš„ä»£ç†åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„ä»£ç†ï¼Œå°½ç®¡ç°æœ‰çš„ä»£ç†ä½¿ç”¨é¢å¤–çš„æ–‡æœ¬è¾“å…¥ï¼Œè€Œæˆ‘ä»¬çš„ä»£ç†ä»…ä½¿ç”¨è§†è§‰æ„ŸçŸ¥ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„ SeeAct-V æ¡†æ¶å’Œ UGround æ¨¡å‹ä¸º GUI ä»£ç†çš„è§†è§‰å®šä½æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br>- **è§†è§‰æ„ŸçŸ¥**ï¼šé€šè¿‡å®Œå…¨ä¾èµ–è§†è§‰æ„ŸçŸ¥ï¼ŒGUI ä»£ç†å¯ä»¥é¿å…åŸºäºæ–‡æœ¬è¡¨ç¤ºçš„å™ªå£°å’Œä¸å®Œæ•´æ€§é—®é¢˜ã€‚<br>- **åƒç´ çº§æ“ä½œ**ï¼šç›´æ¥åœ¨ GUI ä¸Šæ‰§è¡Œåƒç´ çº§æ“ä½œï¼Œå¯ä»¥æ›´ç²¾ç¡®åœ°æ§åˆ¶ä»£ç†çš„è¡Œä¸ºã€‚<br>- **é€šç”¨æ€§**ï¼šUGround æ¨¡å‹å…·æœ‰è‰¯å¥½çš„è·¨å¹³å°æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„ GUI ç¯å¢ƒã€‚<br>- **æ¨¡å—åŒ–è®¾è®¡**ï¼šSeeAct-V æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—ä»£ç†çš„å„ä¸ªç»„ä»¶å¯ä»¥ç‹¬ç«‹ç ”ç©¶å’Œæ”¹è¿›ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>æœ¬æ–‡æå‡ºçš„ SeeAct-V æ¡†æ¶å’Œ UGround æ¨¡å‹ä¸º GUI ä»£ç†çš„è§†è§‰å®šä½æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œä¸ºæ„å»ºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·å¯¼èˆªæ•°å­—ä¸–ç•Œçš„ GUI ä»£ç†å¥ å®šäº†åŸºç¡€ã€‚</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</td>
      <td>Language agents based on large language models (LLMs) have demonstrated great<br>promise in automating web-based tasks. Recent work has shown that incorporating<br>advanced planning algorithms, e.g., tree search, is advantageous over reactive<br>planning for web agents. However, unlike simulated sandbox environments,<br>real-world environments such as the web are rife with irreversible actions.<br>This undermines the feasibility of backtracking, a cornerstone of (tree)<br>search. Overly relying on test-time search also hurts efficiency. We advocate<br>model-based planning for web agents that employs a world model to simulate and<br>deliberate over the outcome of each candidate action before committing to one.<br>We systematically explore this paradigm by (1) Proposing a model-based planning<br>framework, WebDreamer, which employs LLMs to serve as both world models and<br>value functions; (2) Training specialized LLMs as world models with a scalable<br>data synthesis pipeline. Empirical results demonstrate that WebDreamer achieves<br>substantial performance improvements over reactive baselines. It is<br>competitive, while being 4-5 times more efficient, with tree search in sandbox<br>environments (VisualWebArena) and also works effectively on real-world websites<br>(Online-Mind2Web and Mind2Web-Live). Furthermore, our trained world model,<br>Dreamer-7B, performs comparable to GPT-4o, highlighting the potential of<br>specialized world models for efficient and effective planning in complex web<br>environments.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºäº’è”ç½‘ä¸–ç•Œæ¨¡å‹ï¼Œå®ç°é«˜æ•ˆç½‘ç»œä»£ç†è§„åˆ’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–ç½‘ç»œä»»åŠ¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¦‚ä½•è®©ç½‘ç»œä»£ç†åœ¨å¤æ‚ä¸”ä¸æ–­å˜åŒ–çš„ç½‘ç»œç¯å¢ƒä¸­åšå‡ºæœ‰æ•ˆçš„å†³ç­–ï¼Œæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¯¾é¢˜ã€‚ä¼ ç»Ÿçš„åŸºäºæœç´¢çš„è§„åˆ’ç®—æ³•åœ¨ç½‘ç»œç¯å¢ƒä¸­é¢ä¸´ç€ä¸å¯é€†åŠ¨ä½œå’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„è§„åˆ’æ–¹æ³•ï¼Œåˆ©ç”¨LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹æ¥æ¨¡æ‹Ÿå’Œè¯„ä¼°å€™é€‰åŠ¨ä½œçš„åæœï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç½‘ç»œä»£ç†è§„åˆ’ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„è§„åˆ’æ¡†æ¶WebDreamerï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹å’Œä»·å€¼å‡½æ•°ï¼Œé€šè¿‡æ¨¡æ‹Ÿå’Œè¯„ä¼°å€™é€‰åŠ¨ä½œçš„åæœæ¥åšå‡ºå†³ç­–ã€‚<br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä½¿ç”¨å¯æ‰©å±•çš„æ•°æ®åˆæˆç®¡é“è®­ç»ƒä¸“é—¨çš„LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒWebDreameråœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸åŸºäºæœç´¢çš„è§„åˆ’æ–¹æ³•ç›¸æ¯”ï¼ŒWebDreameråœ¨æ²™ç›’ç¯å¢ƒï¼ˆVisualWebArenaï¼‰ä¸­æ•ˆç‡æé«˜äº†4-5å€ï¼Œå¹¶ä¸”åœ¨çœŸå®ä¸–ç•Œç½‘ç«™ï¼ˆOnline-Mind2Webå’ŒMind2Web-Liveï¼‰ä¸Šä¹Ÿèƒ½æœ‰æ•ˆå·¥ä½œã€‚æ­¤å¤–ï¼Œè®­ç»ƒå‡ºçš„ä¸–ç•Œæ¨¡å‹Dreamer-7Båœ¨ä¸¤ä¸ªåœ¨çº¿åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸GPT-4oç›¸å½“ï¼Œè¯æ˜äº†ä¸“é—¨çš„ä¸–ç•Œæ¨¡å‹åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œé«˜æ•ˆå’Œæœ‰æ•ˆè§„åˆ’çš„å¯èƒ½æ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„åŸºäºæ¨¡å‹çš„è§„åˆ’æ–¹æ³•ä¸ºç½‘ç»œä»£ç†åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„é«˜æ•ˆå†³ç­–æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚åˆ©ç”¨LLMsä½œä¸ºä¸–ç•Œæ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿå’Œè¯„ä¼°å€™é€‰åŠ¨ä½œçš„åæœï¼Œä»è€Œæé«˜ç½‘ç»œä»£ç†çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ•°æ®åˆæˆç®¡é“ä¸ºè®­ç»ƒä¸“é—¨çš„ä¸–ç•Œæ¨¡å‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</td>
    </tr>
    <tr>
      <th>27</th>
      <td>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</td>
      <td>Pre-trained large language models (LLMs) have recently achieved better<br>generalization and sample efficiency in autonomous web automation. However, the<br>performance on real-world websites has still suffered from (1) open domainness,<br>(2) limited context length, and (3) lack of inductive bias on HTML. We<br>introduce WebAgent, an LLM-driven agent that learns from self-experience to<br>complete tasks on real websites following natural language instructions.<br>WebAgent plans ahead by decomposing instructions into canonical<br>sub-instructions, summarizes long HTML documents into task-relevant snippets,<br>and acts on websites via Python programs generated from those. We design<br>WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new<br>pre-trained LLMs for long HTML documents using local and global attention<br>mechanisms and a mixture of long-span denoising objectives, for planning and<br>summarization. We empirically demonstrate that our modular recipe improves the<br>success on real websites by over 50%, and that HTML-T5 is the best model to<br>solve various HTML understanding tasks; achieving 18.7% higher success rate<br>than the prior method on MiniWoB web automation benchmark, and SoTA performance<br>on Mind2Web, an offline task planning evaluation.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebAgentï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç°å®ä¸–ç•Œç½‘ç»œè‡ªåŠ¨åŒ–<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªä¸»ç½‘ç»œè‡ªåŠ¨åŒ–æ–¹é¢å–å¾—äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæ ·æœ¬æ•ˆç‡ï¼Œä½†åœ¨ç°å®ä¸–ç•Œç½‘ç«™ä¸Šçš„æ€§èƒ½ä»ç„¶å—åˆ°ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢çš„æŒ‘æˆ˜ï¼š<br>1. å¼€æ”¾åŸŸæ€§ï¼šç°å®ä¸–ç•Œç½‘ç«™å…·æœ‰å¼€æ”¾æ€§å’ŒåŠ¨æ€æ€§ï¼Œéš¾ä»¥é¢„å…ˆå®šä¹‰åˆé€‚çš„åŠ¨ä½œç©ºé—´ã€‚<br>2. æœ‰é™ä¸Šä¸‹æ–‡é•¿åº¦ï¼šç°å®ä¸–ç•Œç½‘ç«™çš„HTMLæ–‡æ¡£é€šå¸¸æ¯”æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„è§‚å¯Ÿç»“æœæ›´é•¿ï¼Œè¶…å‡ºäº†å¤§å¤šæ•°LLMsçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚<br>3. ç¼ºä¹å¯¹HTMLçš„å½’çº³åç½®ï¼šç°æœ‰çš„LLMsç¼ºä¹å¯¹HTMLæ–‡æ¡£ç»“æ„çš„ç†è§£ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°å¤„ç†ç°å®ä¸–ç•Œç½‘ç«™ä¸Šçš„å¤æ‚ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†WebAgentï¼Œä¸€ä¸ªåŸºäºLLMsçš„è‡ªä¸»ä»£ç†ï¼Œå®ƒé€šè¿‡è‡ªæˆ‘ç»éªŒå­¦ä¹ ï¼Œéµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨ç°å®ä¸–ç•Œç½‘ç«™ä¸Šå®Œæˆä»»åŠ¡ã€‚WebAgentçš„æ ¸å¿ƒæ–¹æ³•åŒ…æ‹¬ï¼š<br>1. **è§„åˆ’**ï¼šå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†è§£ä¸ºè§„èŒƒå­æŒ‡ä»¤ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡Œä»»åŠ¡ã€‚<br>2. **é•¿ä¸Šä¸‹æ–‡ç†è§£**ï¼šä½¿ç”¨HTML-T5æ¨¡å‹å¯¹é•¿HTMLæ–‡æ¡£è¿›è¡Œæ‘˜è¦ï¼Œæå–ä¸ä»»åŠ¡ç›¸å…³çš„ç‰‡æ®µï¼Œä»è€Œå…‹æœLLMsä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚<br>3. **ç¨‹åºåˆæˆ**ï¼šå°†å­æŒ‡ä»¤å’ŒHTMLç‰‡æ®µè½¬æ¢ä¸ºå¯æ‰§è¡Œçš„Pythonä»£ç ï¼Œé€šè¿‡Selenium WebDriveråœ¨ç½‘ç«™ä¸Šæ‰§è¡Œæ“ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒWebAgentåœ¨ç°å®ä¸–ç•Œç½‘ç«™ä¸Šçš„æˆåŠŸç‡æ¯”å•ä¸€LLMæ–¹æ³•æé«˜äº†50%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒHTML-T5æ¨¡å‹åœ¨MiniWoBç½‘ç»œè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†18.7%æ›´é«˜çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨Mind2Webç¦»çº¿ä»»åŠ¡è§„åˆ’è¯„ä¼°ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebAgentçš„è®¾è®¡æ€è·¯ä¸ºç°å®ä¸–ç•Œç½‘ç»œè‡ªåŠ¨åŒ–æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶æ ¸å¿ƒæ–¹æ³•å…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br>1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†ç½‘ç»œè‡ªåŠ¨åŒ–ä»»åŠ¡åˆ†è§£ä¸ºè§„åˆ’ã€æ‘˜è¦å’Œç¨‹åºåˆæˆä¸‰ä¸ªæ¨¡å—ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„LLMsè¿›è¡Œå¤„ç†ï¼Œæé«˜äº†ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚<br>2. **è‡ªæˆ‘ç»éªŒç›‘ç£**ï¼šé€šè¿‡è‡ªæˆ‘ç»éªŒç›‘ç£ï¼Œå°†é¢†åŸŸä¸“å®¶è¯­è¨€æ¨¡å‹ä¸çœŸå®ä¸–ç•Œç½‘ç«™è¿›è¡Œå¯¹é½ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚<br>3. **HTML-T5æ¨¡å‹**ï¼šHTML-T5æ¨¡å‹é€šè¿‡å±€éƒ¨å’Œå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ä»¥åŠé•¿è·¨åº¦å»å™ªç›®æ ‡ï¼Œæ›´å¥½åœ°æ•æ‰HTMLæ–‡æ¡£çš„ç»“æ„å’Œè¯­ä¹‰ï¼Œä¸ºç½‘ç»œè‡ªåŠ¨åŒ–ä»»åŠ¡æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚<br><br>## ğŸ¯ æœªæ¥å±•æœ›<br>WebAgentçš„æˆåŠŸä¸ºç°å®ä¸–ç•Œç½‘ç»œè‡ªåŠ¨åŒ–å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œæœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢ä»¥ä¸‹æ–¹å‘ï¼š<br>1. **æ›´å¼ºå¤§çš„è§„åˆ’æ¨¡å—**ï¼šå¼€å‘æ›´å¼ºå¤§çš„è§„åˆ’æ¨¡å—ï¼Œä»¥æ›´å‡†ç¡®åœ°åˆ†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆæ›´æœ‰æ•ˆçš„å­æŒ‡ä»¤åºåˆ—ã€‚<br>2. **æ›´å¹¿æ³›çš„æ³›åŒ–èƒ½åŠ›**ï¼šæ”¶é›†æ›´å¤šçœŸå®ä¸–ç•Œç½‘ç«™ä¸Šçš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨æ›´å¤§çš„é¢†åŸŸä¸“å®¶æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜WebAgentçš„æ³›åŒ–èƒ½åŠ›ã€‚<br>3. **æ›´æœ‰æ•ˆçš„åé¦ˆæœºåˆ¶**ï¼šå°†åé¦ˆæœºåˆ¶é›†æˆåˆ°æ›´å¤§çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æ›´å¥½åœ°åæ˜ ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå¹¶æé«˜ç¨‹åºåˆæˆçš„è´¨é‡ã€‚<br>4. **æ›´è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ–¹æ³•**ï¼šå¼€å‘æ›´è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥å‡å°‘äººå·¥å¹²é¢„ï¼Œå¹¶æé«˜WebAgentçš„å¯æ‰©å±•æ€§ã€‚</td>
    </tr>
    <tr>
      <th>28</th>
      <td>WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models</td>
      <td>The rapid advancement of large language models (LLMs) has led to a new era<br>marked by the development of autonomous applications in real-world scenarios,<br>which drives innovation in creating advanced web agents. Existing web agents<br>typically only handle one input modality and are evaluated only in simplified<br>web simulators or static web snapshots, greatly limiting their applicability in<br>real-world scenarios. To bridge this gap, we introduce WebVoyager, an<br>innovative Large Multimodal Model (LMM) powered web agent that can complete<br>user instructions end-to-end by interacting with real-world websites. Moreover,<br>we establish a new benchmark by compiling real-world tasks from 15 popular<br>websites and introduce an automatic evaluation protocol leveraging multimodal<br>understanding abilities of GPT-4V to evaluate open-ended web agents. We show<br>that WebVoyager achieves a 59.1% task success rate on our benchmark,<br>significantly surpassing the performance of both GPT-4 (All Tools) and the<br>WebVoyager (text-only) setups, underscoring the exceptional capability of<br>WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement<br>with human judgment, indicating its effectiveness in providing reliable and<br>accurate assessments of web agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebVoyagerï¼šåŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ç«¯åˆ°ç«¯ç½‘ç»œä»£ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªä¸»åº”ç”¨åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨åˆ›æ–°æ—¥ç›Šå…´èµ·ï¼Œæ¨åŠ¨äº†é«˜çº§ç½‘ç»œä»£ç†çš„åˆ›å»ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç½‘ç»œä»£ç†é€šå¸¸åªèƒ½å¤„ç†ä¸€ç§è¾“å…¥æ¨¡æ€ï¼Œå¹¶ä¸”ä»…åœ¨ç®€åŒ–çš„ç½‘ç»œæ¨¡æ‹Ÿå™¨æˆ–é™æ€ç½‘ç»œå¿«ç…§ä¸­è¿›è¡Œè¯„ä¼°ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡ä»‹ç»äº†WebVoyagerï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ã€ç”±å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰é©±åŠ¨çš„ç½‘ç»œä»£ç†ï¼Œå®ƒå¯ä»¥é€šè¿‡ä¸çœŸå®ä¸–ç•Œç½‘ç«™çš„äº¤äº’æ¥å®Œæˆç”¨æˆ·æŒ‡ä»¤çš„ç«¯åˆ°ç«¯å¤„ç†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWebVoyageræ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç½‘ç»œä»£ç†ï¼Œå®ƒé€šè¿‡è§‚å¯Ÿå±å¹•æˆªå›¾å’Œäº¤äº’å¼ç½‘ç»œå…ƒç´ çš„æ–‡æœ¬å†…å®¹æ¥å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šåˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ï¼Œç„¶åæ‰§è¡Œç›¸åº”çš„æ“ä½œï¼ˆå¦‚ç‚¹å‡»ã€è¾“å…¥æˆ–æ»šåŠ¨ç­‰ï¼‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œè¯¥åŸºå‡†ç”±ä»15ä¸ªæµè¡Œç½‘ç«™æ”¶é›†çš„çœŸå®ä¸–ç•Œä»»åŠ¡ç»„æˆï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ï¼Œåˆ©ç”¨GPT-4Vçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›æ¥è¯„ä¼°å¼€æ”¾å¼çš„ç½‘ç»œä»£ç†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼ŒWebVoyageråœ¨æ–°çš„åŸºå‡†ä¸Šå®ç°äº†59.1%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºGPT-4ï¼ˆAll Toolsï¼‰å’ŒWebVoyagerï¼ˆä»…æ–‡æœ¬ï¼‰è®¾ç½®ï¼Œè¿™çªå‡ºäº†WebVoyagerçš„å“è¶Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæå‡ºçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§è¾¾åˆ°äº†85.3%ï¼Œè¡¨æ˜å…¶åœ¨æä¾›å¯é å’Œå‡†ç¡®çš„ç½‘ç»œä»£ç†è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebVoyagerçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ„å»ºæ™ºèƒ½ç½‘ç»œä»£ç†æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚WebVoyagerçš„è®¾è®¡å’Œè¯„ä¼°æ–¹æ³•ä¸ºæœªæ¥å¼€å‘æ›´é€šç”¨å’Œå¼ºå¤§çš„ç½‘ç»œåŠ©æ‰‹æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>29</th>
      <td>CogAgent: A Visual Language Model for GUI Agents</td>
      <td>People are spending an enormous amount of time on digital devices through<br>graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large<br>language models (LLMs) such as ChatGPT can assist people in tasks like writing<br>emails, but struggle to understand and interact with GUIs, thus limiting their<br>potential to increase automation levels. In this paper, we introduce CogAgent,<br>an 18-billion-parameter visual language model (VLM) specializing in GUI<br>understanding and navigation. By utilizing both low-resolution and<br>high-resolution image encoders, CogAgent supports input at a resolution of<br>1120*1120, enabling it to recognize tiny page elements and text. As a<br>generalist visual language model, CogAgent achieves the state of the art on<br>five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,<br>Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using<br>only screenshots as input, outperforms LLM-based methods that consume extracted<br>HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,<br>advancing the state of the art. The model and codes are available at<br>https://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220<br>available at https://github.com/THUDM/CogAgent.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | CogAgentï¼šä¸ºGUIäº¤äº’è€Œç”Ÿçš„æ–°å‹è§†è§‰è¯­è¨€æ¨¡å‹<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººä»¬è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸æ•°å­—è®¾å¤‡äº’åŠ¨ï¼Œä¾‹å¦‚è®¡ç®—æœºæˆ–æ™ºèƒ½æ‰‹æœºå±å¹•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTåœ¨è¾…åŠ©äººä»¬å®Œæˆè¯¸å¦‚æ’°å†™ç”µå­é‚®ä»¶ç­‰ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼ŒLLMsåœ¨ç†è§£å’Œä¸GUIäº¤äº’æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æé«˜è‡ªåŠ¨åŒ–æ°´å¹³çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†CogAgentï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºGUIç†è§£å’Œå¯¼èˆªçš„18äº¿å‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚CogAgenté€šè¿‡åˆ©ç”¨ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒç¼–ç å™¨ï¼Œæ”¯æŒ1120*1120çš„è¾“å…¥åˆ†è¾¨ç‡ï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«å¾®å°çš„é¡µé¢å…ƒç´ å’Œæ–‡æœ¬ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šCogAgenté‡‡ç”¨äº†ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒç¼–ç å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚è¿™ç§è®¾è®¡å…è®¸CogAgentåœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—å†…æé«˜æ¨¡å‹çš„å¯æ¥å—åˆ†è¾¨ç‡ï¼Œä»è€Œè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šCogAgentåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†å¤§è§„æ¨¡çš„GUIå’ŒOCRæ•°æ®é›†ï¼Œä»¥åŠä¸“é—¨ä¸ºGUIåœºæ™¯è®¾è®¡çš„GUI groundingä»»åŠ¡ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹GUIå…ƒç´ çš„ç†è§£èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>CogAgentåœ¨å¤šä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬VQAv2ã€OK-VQAã€Text-VQAã€ST-VQAã€ChartQAã€infoVQAã€DocVQAã€MM-Vetå’ŒPOPEã€‚æ­¤å¤–ï¼ŒCogAgentåœ¨PCå’ŒAndroid GUIå¯¼èˆªä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨å±å¹•æˆªå›¾ä½œä¸ºè¾“å…¥ï¼Œå°±ä¼˜äºäº†åŸºäºLLMçš„æ–¹æ³•ï¼Œå¦‚Mind2Webå’ŒAITWï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>CogAgentçš„è®¾è®¡å’Œè®­ç»ƒæ–¹æ³•ä¸ºæ„å»ºGUIæ™ºèƒ½ä½“æä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†èƒ½åŠ›å’Œå¯¹GUIå…ƒç´ çš„ç†è§£èƒ½åŠ›ï¼Œä½¿å…¶åœ¨GUIäº¤äº’ä»»åŠ¡ä¸­å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼ŒCogAgentçš„é¢„è®­ç»ƒæ•°æ®æ„å»ºæ–¹æ³•ä¹Ÿä¸ºå…¶ä»–è§†è§‰è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Understanding the planning of LLM agents: A survey</td>
      <td>As Large Language Models (LLMs) have shown significant intelligence, the<br>progress to leverage LLMs as planning modules of autonomous agents has<br>attracted more attention. This survey provides the first systematic view of<br>LLM-based agents planning, covering recent works aiming to improve planning<br>ability. We provide a taxonomy of existing works on LLM-Agent planning, which<br>can be categorized into Task Decomposition, Plan Selection, External Module,<br>Reflection and Memory. Comprehensive analyses are conducted for each direction,<br>and further challenges for the field of research are discussed.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ·±å…¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„è§„åˆ’èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºæ˜¾è‘—çš„æ™ºèƒ½ï¼Œå°†LLMä½œä¸ºè‡ªä¸»ä»£ç†çš„è§„åˆ’æ¨¡å—çš„è¿›å±•å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è§„åˆ’æ–¹æ³•ï¼Œå¦‚ç¬¦å·æ–¹æ³•å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œå­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¾‹å¦‚éœ€è¦å°†çµæ´»çš„è‡ªç„¶è¯­è¨€æè¿°çš„é—®é¢˜è½¬æ¢ä¸ºç¬¦å·å»ºæ¨¡ï¼Œæˆ–è€…éœ€è¦å¤§é‡çš„æ ·æœ¬è¿›è¡Œå­¦ä¹ ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ä¸ªç³»ç»Ÿæ€§çš„è§†è§’ï¼Œä»¥äº†è§£LLMä»£ç†çš„è§„åˆ’èƒ½åŠ›ï¼Œå¹¶æ¢è®¨å¦‚ä½•åˆ©ç”¨LLMæ¥æé«˜ä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡å°†ç°æœ‰çš„LLMä»£ç†è§„åˆ’å·¥ä½œåˆ†ä¸ºäº”ä¸ªä¸»è¦æ–¹å‘ï¼Œå¹¶å¯¹æ¯ä¸ªæ–¹å‘è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æï¼š<br><br>1. **ä»»åŠ¡åˆ†è§£**ï¼šå°†å¤æ‚çš„ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡ï¼Œç„¶åä¾æ¬¡ä¸ºæ¯ä¸ªå­ä»»åŠ¡è¿›è¡Œè§„åˆ’ã€‚<br>2. **å¤šè®¡åˆ’é€‰æ‹©**ï¼šç”Ÿæˆå¤šä¸ªæ›¿ä»£è®¡åˆ’ï¼Œç„¶åä½¿ç”¨ä»»åŠ¡ç›¸å…³çš„æœç´¢ç®—æ³•é€‰æ‹©ä¸€ä¸ªè®¡åˆ’æ¥æ‰§è¡Œã€‚<br>3. **å¤–éƒ¨æ¨¡å—è¾…åŠ©è§„åˆ’**ï¼šä½¿ç”¨å¤–éƒ¨è§„åˆ’å™¨æ¥æå‡è§„åˆ’è¿‡ç¨‹ï¼ŒåŒæ—¶LLMä¸»è¦æ‰®æ¼”ä»»åŠ¡å½¢å¼åŒ–çš„è§’è‰²ã€‚<br>4. **åæ€ä¸ç»†åŒ–**ï¼šé€šè¿‡åæ€å’Œç»†åŒ–æ¥æé«˜è§„åˆ’èƒ½åŠ›ï¼Œé¼“åŠ±LLMåæ€å¤±è´¥å¹¶æ”¹è¿›è®¡åˆ’ã€‚<br>5. **è®°å¿†å¢å¼ºè§„åˆ’**ï¼šä½¿ç”¨é¢å¤–çš„è®°å¿†æ¨¡å—æ¥å¢å¼ºè§„åˆ’ï¼Œå…¶ä¸­å­˜å‚¨æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œå¦‚å¸¸è¯†çŸ¥è¯†ã€è¿‡å»ç»éªŒã€ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ç­‰ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å‡ ä¸ªä»£è¡¨æ€§æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œæ€§èƒ½éšç€æˆæœ¬çš„å¢åŠ è€Œæé«˜ã€‚æ­¤å¤–ï¼Œå¯¹äºå¤æ‚ä»»åŠ¡ï¼Œå°‘æ ·æœ¬ç¤ºä¾‹å¯¹äºLLMè¿›ä¸€æ­¥ç†è§£ä»»åŠ¡è‡³å…³é‡è¦ï¼Œè€Œåæ€åœ¨æé«˜æˆåŠŸç‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡ä¸ºLLMä»£ç†çš„è§„åˆ’èƒ½åŠ›æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„è§†è§’ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨LLMæ¥æé«˜ä»£ç†çš„è§„åˆ’èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¨è®ºäº†LLMä»£ç†è§„åˆ’ä¸­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å¹»è§‰ã€ç”Ÿæˆè®¡åˆ’çš„å¯è¡Œæ€§ã€ç”Ÿæˆè®¡åˆ’çš„æ•ˆç‡ã€å¤šæ¨¡æ€ç¯å¢ƒåé¦ˆå’Œç»†ç²’åº¦è¯„ä¼°ç­‰ã€‚è¿™äº›æŒ‘æˆ˜ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ï¼Œå¹¶æœ‰åŠ©äºæ¨åŠ¨LLMä»£ç†è§„åˆ’é¢†åŸŸçš„å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>31</th>
      <td>WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models</td>
      <td>The rapid advancement of large language models (LLMs) has led to a new era<br>marked by the development of autonomous applications in real-world scenarios,<br>which drives innovation in creating advanced web agents. Existing web agents<br>typically only handle one input modality and are evaluated only in simplified<br>web simulators or static web snapshots, greatly limiting their applicability in<br>real-world scenarios. To bridge this gap, we introduce WebVoyager, an<br>innovative Large Multimodal Model (LMM) powered web agent that can complete<br>user instructions end-to-end by interacting with real-world websites. Moreover,<br>we establish a new benchmark by compiling real-world tasks from 15 popular<br>websites and introduce an automatic evaluation protocol leveraging multimodal<br>understanding abilities of GPT-4V to evaluate open-ended web agents. We show<br>that WebVoyager achieves a 59.1% task success rate on our benchmark,<br>significantly surpassing the performance of both GPT-4 (All Tools) and the<br>WebVoyager (text-only) setups, underscoring the exceptional capability of<br>WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement<br>with human judgment, indicating its effectiveness in providing reliable and<br>accurate assessments of web agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebVoyagerï¼šåŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ç«¯åˆ°ç«¯ç½‘ç»œä»£ç†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè‡ªä¸»åº”ç”¨ç¨‹åºåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨æ—¥ç›Šå¢å¤šï¼Œæ¨åŠ¨äº†é«˜çº§ç½‘ç»œä»£ç†çš„åˆ›æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç½‘ç»œä»£ç†é€šå¸¸åªèƒ½å¤„ç†ä¸€ç§è¾“å…¥æ¨¡æ€ï¼Œå¹¶ä¸”ä»…åœ¨ç®€åŒ–çš„ç½‘ç»œæ¨¡æ‹Ÿå™¨æˆ–é™æ€ç½‘ç»œå¿«ç…§ä¸­è¿›è¡Œè¯„ä¼°ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†WebVoyagerï¼Œä¸€ä¸ªç”±å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰é©±åŠ¨çš„åˆ›æ–°ç½‘ç»œä»£ç†ï¼Œå®ƒå¯ä»¥é€šè¿‡ä¸çœŸå®ä¸–ç•Œç½‘ç«™çš„äº¤äº’æ¥å®Œæˆç”¨æˆ·æŒ‡ä»¤çš„ç«¯åˆ°ç«¯æ‰§è¡Œã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šWebVoyageræ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç½‘ç»œä»£ç†ï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„èƒ½åŠ›ï¼Œé€šè¿‡å¤„ç†æ¥è‡ªäº¤äº’å¼ç½‘ç»œå…ƒç´ çš„å±å¹•æˆªå›¾å’Œæ–‡æœ¬å†…å®¹ï¼Œæ¥è§‚å¯Ÿç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶åˆ¶å®šè¡ŒåŠ¨è®¡åˆ’ï¼Œç„¶åæ‰§è¡Œç›¸åº”çš„æ“ä½œï¼Œä¾‹å¦‚ç‚¹å‡»ã€è¾“å…¥æˆ–æ»šåŠ¨ç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œé€šè¿‡ä»15ä¸ªæµè¡Œç½‘ç«™æ”¶é›†çœŸå®ä¸–ç•Œçš„ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°åè®®ï¼Œåˆ©ç”¨GPT-4Vçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›æ¥è¯„ä¼°å¼€æ”¾å¼çš„ç½‘ç»œä»£ç†ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>WebVoyageråœ¨æ–°çš„åŸºå‡†ä¸Šå®ç°äº†59.1%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ˜¾è‘—è¶…è¿‡äº†GPT-4ï¼ˆæ‰€æœ‰å·¥å…·ï¼‰å’ŒWebVoyagerï¼ˆä»…æ–‡æœ¬ï¼‰è®¾ç½®çš„æ€§èƒ½ï¼Œçªå‡ºäº†WebVoyagerçš„å“è¶Šèƒ½åŠ›ã€‚æå‡ºçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§è¾¾åˆ°äº†85.3%ï¼Œè¡¨æ˜å…¶åœ¨æä¾›å¯é å’Œå‡†ç¡®çš„ç½‘ç»œä»£ç†è¯„ä¼°æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebVoyagerçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ„å»ºæ›´æ™ºèƒ½å’Œé«˜æ•ˆçš„ç½‘ç»œè‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæ–¹é¢ï¼Œåˆ©ç”¨å…ˆè¿›çš„LMMèƒ½åŠ›å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„è‡ªåŠ¨è¯„ä¼°åè®®ä¸ºè¯„ä¼°ç½‘ç»œä»£ç†çš„èƒ½åŠ›æä¾›äº†ä¸€ç§å¯é å’Œå‡†ç¡®çš„æ–¹æ³•ï¼Œå¯ä»¥ä¿ƒè¿›ç½‘ç»œä»£ç†ç ”ç©¶çš„è¿›ä¸€æ­¥å‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>32</th>
      <td>VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks</td>
      <td>Videos are often used to learn or extract the necessary information to<br>complete tasks in ways different than what text and static imagery alone can<br>provide. However, many existing agent benchmarks neglect long-context video<br>understanding, instead focusing on text or static image inputs. To bridge this<br>gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the<br>capabilities of long-context multimodal agents for video understanding. VideoWA<br>consists of 2,021 web agent tasks based on manually crafted video tutorials,<br>which total almost four hours of content. For our benchmark, we define a<br>taxonomy of long-context video-based agent tasks with two main areas of focus:<br>skill retention and factual retention. While skill retention tasks evaluate<br>whether an agent can use a given human demonstration to complete a task<br>efficiently, the factual retention task evaluates whether an agent can retrieve<br>instruction-relevant information from a video to complete a task. We find that<br>the best model achieves 13.3% success on factual retention tasks and 45.8% on<br>factual retention QA pairs, far below human performance at 73.9% and 79.3%,<br>respectively. On skill retention tasks, long-context models perform worse with<br>tutorials than without, exhibiting a 5% performance decrease in WebArena tasks<br>and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to<br>improve the agentic abilities of long-context multimodal models and provides a<br>testbed for future development with long-context video agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | VideoWebArenaï¼šè¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»£ç†çš„è§†é¢‘ç†è§£èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹åŸºç¡€æ¨¡å‹è¢«åº”ç”¨äºAIä»£ç†ï¼Œè¿™äº›å¤šæ¨¡æ€ä»£ç†éœ€è¦å…·å¤‡ç†è§£å’Œå¤„ç†è§†é¢‘çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ä»ç½‘ç»œä¸­æ£€ç´¢å’Œå¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ä»¥å®Œæˆæ–°çŸ¥è¯†ä»»åŠ¡çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æˆ–é™æ€å›¾åƒè¾“å…¥ï¼Œå¿½ç•¥äº†é•¿ä¸Šä¸‹æ–‡è§†é¢‘ç†è§£ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†VideoWebArena (VideoWA)ï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»£ç†è§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šVideoWebArenaåŸºå‡†æµ‹è¯•<br>VideoWAç”±2,021ä¸ªåŸºäºæ‰‹åŠ¨åˆ¶ä½œè§†é¢‘æ•™ç¨‹çš„ç½‘ç»œä»£ç†ä»»åŠ¡ç»„æˆï¼Œæ€»æ—¶é•¿è¿‘å››ä¸ªå°æ—¶ã€‚è¯¥åŸºå‡†æµ‹è¯•å®šä¹‰äº†ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»£ç†ä»»åŠ¡çš„åˆ†ç±»æ³•ï¼Œé‡ç‚¹å…³æ³¨æŠ€èƒ½ä¿ç•™å’Œäº‹å®ä¿ç•™ä¸¤ä¸ªæ–¹é¢ã€‚æŠ€èƒ½ä¿ç•™ä»»åŠ¡è¯„ä¼°ä»£ç†æ˜¯å¦èƒ½å¤Ÿä½¿ç”¨ç»™å®šçš„äººç±»æ¼”ç¤ºæ¥é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ï¼Œè€Œäº‹å®ä¿ç•™ä»»åŠ¡è¯„ä¼°ä»£ç†æ˜¯å¦èƒ½å¤Ÿä»è§†é¢‘ä¸­æ£€ç´¢ä¸æŒ‡ä»¤ç›¸å…³çš„ä¿¡æ¯æ¥å®Œæˆä»»åŠ¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¯„ä¼°ç°æœ‰æ¨¡å‹<br>æœ¬æ–‡è¯„ä¼°äº†æµè¡Œçš„å’Œæœ€æ–°çš„è§†é¢‘/å›¾åƒèƒ½åŠ›LLMï¼ˆä¾‹å¦‚GPT-4oå’ŒGemini 1.5 Proï¼‰ï¼Œä»¥æ›´å¥½åœ°äº†è§£å®ƒä»¬å½“å‰çš„é•¿ä¸Šä¸‹æ–‡è§†é¢‘ç†è§£èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œè§†é¢‘/å›¾åƒèƒ½åŠ›ä»£ç†ä»ç„¶æœ‰é™ï¼Œè¿œæœªè¾¾åˆ°äººç±»çš„æ€§èƒ½æ°´å¹³ï¼Œçªå‡ºäº†å½“å‰æœ€å…ˆè¿›é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢å’Œä»£ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§å·®è·ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨äº‹å®ä¿ç•™ä»»åŠ¡ä¸Šï¼Œæœ€ä½³æ¨¡å‹çš„æˆåŠŸç‡ä¸º13.3%ï¼Œè€Œåœ¨äº‹å®ä¿ç•™QAå¯¹ä¸Šä¸º45.8%ï¼Œè¿œä½äºäººç±»çš„73.9%å’Œ79.3%ã€‚åœ¨æŠ€èƒ½ä¿ç•™ä»»åŠ¡ä¸Šï¼Œé•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨æ•™ç¨‹ä¸­çš„è¡¨ç°æ¯”æ²¡æœ‰æ•™ç¨‹æ—¶æ›´å·®ï¼Œåœ¨WebArenaä»»åŠ¡ä¸­ä¸‹é™äº†5%ï¼Œåœ¨VisualWebArenaä»»åŠ¡ä¸­ä¸‹é™äº†10.3%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>VideoWebArenaä¸ºè¯„ä¼°å’Œæ”¹è¿›é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»£ç†çš„è§†é¢‘ç†è§£èƒ½åŠ›æä¾›äº†ä¸€ä¸ªé‡è¦çš„æµ‹è¯•å¹³å°ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜è¯†åˆ«ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æ¨åŠ¨æœªæ¥åœ¨é•¿ä¸Šä¸‹æ–‡è§†é¢‘ä»£ç†æ–¹é¢çš„ç ”ç©¶å’Œå‘å±•ã€‚</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Tur[k]ingBench: A Challenge Benchmark for Web Agents</td>
      <td>Can advanced multi-modal models effectively tackle complex web-based tasks?<br>Such tasks are often found on crowdsourcing platforms, where crowdworkers<br>engage in challenging micro-tasks within web-based environments.<br>  Building on this idea, we present TurkingBench, a benchmark consisting of<br>tasks presented as web pages with textual instructions and multi-modal<br>contexts. Unlike previous approaches that rely on artificially synthesized web<br>pages, our benchmark uses natural HTML pages originally designed for<br>crowdsourcing workers to perform various annotation tasks. Each task's HTML<br>instructions are instantiated with different values derived from crowdsourcing<br>tasks, creating diverse instances. This benchmark includes 32.2K instances<br>spread across 158 tasks.<br>  To support the evaluation of TurkingBench, we have developed a framework that<br>links chatbot responses to actions on web pages (e.g., modifying a text box,<br>selecting a radio button). We assess the performance of cutting-edge private<br>and open-source models, including language-only and vision-language models<br>(such as GPT4 and InternVL), on this benchmark. Our results show that while<br>these models outperform random chance, there is still significant room for<br>improvement. We hope that this benchmark will drive progress in the evaluation<br>and development of web-based agents.</td>
      <td>ä»‹ç»è®ºæ–‡çš„èƒŒæ™¯æˆ–ç—›ç‚¹ï¼Œæˆ–è€…æœ¬æ–‡çš„åŠ¨æœº<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªç„¶ç½‘é¡µä»»åŠ¡<br>ä¸åŒäºä»¥å¾€ä¾èµ–äººå·¥åˆæˆç½‘é¡µçš„æ–¹æ³•ï¼ŒTurkingBench ä½¿ç”¨äº†è‡ªç„¶ HTML é¡µé¢ï¼Œè¿™äº›é¡µé¢åŸæœ¬æ˜¯ä¸ºä¼—åŒ…å¹³å°ä¸Šçš„å·¥äººè®¾è®¡çš„ï¼Œç”¨äºæ‰§è¡Œå„ç§æ ‡æ³¨ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡çš„ HTML æŒ‡ä»¤éƒ½ä½¿ç”¨æ¥è‡ªä¼—åŒ…ä»»åŠ¡çš„å€¼è¿›è¡Œå®ä¾‹åŒ–ï¼Œä»è€Œåˆ›å»ºå¤šæ ·åŒ–çš„å®ä¾‹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šäº¤äº’å¼è¯„ä¼°æ¡†æ¶<br>ä¸ºäº†æ”¯æŒ TurkingBench çš„è¯„ä¼°ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†èŠå¤©æœºå™¨äººçš„å“åº”é“¾æ¥åˆ°ç½‘é¡µä¸Šçš„æ“ä½œï¼ˆä¾‹å¦‚ï¼Œä¿®æ”¹æ–‡æœ¬æ¡†ã€é€‰æ‹©å•é€‰æŒ‰é’®ï¼‰ã€‚è¯¥æ¡†æ¶å…è®¸è¯„ä¼°æœ€å…ˆè¿›çš„ç§æœ‰å’Œå¼€æºæ¨¡å‹ï¼ˆåŒ…æ‹¬è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰åœ¨ TurkingBench ä¸Šçš„æ€§èƒ½ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹çš„è¡¨ç°ä¼˜äºéšæœºçŒœæµ‹ï¼Œä½†ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ç ”ç©¶äººå‘˜åˆ†æäº†æ¨¡å‹åœ¨ä¸åŒå­—æ®µç±»å‹å’Œä»»åŠ¡é•¿åº¦ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ç¡®å®šäº†æœªæ¥è¿›æ­¥çš„æŒ‘æˆ˜ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>TurkingBench ä¸ºè¯„ä¼°å’Œå¼€å‘åŸºäº Web çš„æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åŸºå‡†ã€‚è¯¥åŸºå‡†çš„è‡ªç„¶ç½‘é¡µä»»åŠ¡å’Œäº¤äº’å¼è¯„ä¼°æ¡†æ¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¹³å°ï¼Œä»¥æµ‹è¯•å’Œæ”¹è¿›ä»–ä»¬çš„æ¨¡å‹åœ¨å¤„ç†å¤æ‚ Web ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚</td>
    </tr>
    <tr>
      <th>34</th>
      <td>Grounding Open-Domain Instructions to Automate Web Support Tasks</td>
      <td>Grounding natural language instructions on the web to perform previously<br>unseen tasks enables accessibility and automation. We introduce a task and<br>dataset to train AI agents from open-domain, step-by-step instructions<br>originally written for people. We build RUSS (Rapid Universal Support Service)<br>to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with<br>pointers parses instructions to ThingTalk, a domain-specific language we design<br>for grounding natural language on the web. Then, a grounding model retrieves<br>the unique IDs of any webpage elements requested in ThingTalk. RUSS may<br>interact with the user through a dialogue (e.g. ask for an address) or execute<br>a web operation (e.g. click a button) inside the web runtime. To augment<br>training, we synthesize natural language instructions mapped to ThingTalk. Our<br>dataset consists of 80 different customer service problems from help websites,<br>with a total of 741 step-by-step instructions and their corresponding actions.<br>RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single<br>instructions. It outperforms state-of-the-art models that directly map<br>instructions to actions without ThingTalk. Our user study shows that RUSS is<br>preferred by actual users over web navigation.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è‡ªåŠ¨åŒ–ç½‘ç»œæ”¯æŒä»»åŠ¡ï¼šå°†å¼€æ”¾åŸŸæŒ‡ä»¤æ˜ å°„åˆ°ç½‘ç»œä¸Šçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äº’è”ç½‘çš„æ™®åŠï¼Œè¶Šæ¥è¶Šå¤šçš„æœåŠ¡é€šè¿‡ç½‘ç«™æä¾›ã€‚ç„¶è€Œï¼Œå¯¹äºè§†åŠ›éšœç¢è€…ã€æŠ€æœ¯ä¸ç†Ÿç»ƒè€…æˆ–åˆ†å¿ƒè€…æ¥è¯´ï¼Œä½¿ç”¨è¿™äº›æœåŠ¡ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„è™šæ‹ŸåŠ©æ‰‹å’Œå‘¼å«ä¸­å¿ƒè™½ç„¶å¯ä»¥æä¾›å¸®åŠ©ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦ç‰¹å®šçš„APIæˆ–ç”¨æˆ·æ¼”ç¤ºï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†RUSSï¼ˆRapid Universal Support Serviceï¼‰ï¼Œä¸€ä¸ªèƒ½å¤Ÿä»å¼€æ”¾åŸŸçš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­å­¦ä¹ å¹¶æ‰§è¡Œç½‘ç»œä»»åŠ¡çš„AIä»£ç†ã€‚RUSSçš„æ ¸å¿ƒåˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šThingTalk DSL<br>ä¸ºäº†å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°ç½‘ç»œæ“ä½œï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§åä¸ºThingTalkçš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰ã€‚ThingTalkåŒ…å«äº†ä¸€ç³»åˆ—çš„ä»£ç†æ“ä½œå’Œä¸€ä¸ªç”¨äºæ£€ç´¢ç½‘é¡µå…ƒç´ çš„å‡½æ•°ã€‚è¿™ç§è®¾è®¡ä½¿å¾—ThingTalkæ—¢æ˜“äºä»è‡ªç„¶è¯­è¨€è¿›è¡Œè¯­ä¹‰è§£æï¼Œåˆæ˜“äºç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¨¡å—åŒ–è®¾è®¡<br>RUSSç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼šè¯­ä¹‰è§£æå™¨ã€å®šä½æ¨¡å‹å’Œè¿è¡Œæ—¶ç¯å¢ƒã€‚è¯­ä¹‰è§£æå™¨ä½¿ç”¨BERT-LSTMæ¨¡å‹å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è§£æä¸ºThingTalkä»£ç ã€‚å®šä½æ¨¡å‹æ ¹æ®ThingTalkä»£ç æ£€ç´¢ç½‘é¡µå…ƒç´ çš„å”¯ä¸€IDã€‚è¿è¡Œæ—¶ç¯å¢ƒæ‰§è¡ŒThingTalkä»£ç ï¼ŒåŒ…æ‹¬ä¸ç”¨æˆ·çš„äº¤äº’å’Œç½‘ç»œæ“ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨åŒ…å«80ä¸ªä¸åŒå®¢æˆ·æœåŠ¡é—®é¢˜å’Œ741ä¸ªæ­¥éª¤çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ•°æ®é›†ä¸Šï¼ŒRUSSå®ç°äº†76.7%çš„ç«¯åˆ°ç«¯å‡†ç¡®ç‡ã€‚ä¸ç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œçš„ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒRUSSå…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œå®é™…ç”¨æˆ·æ›´å–œæ¬¢ä½¿ç”¨RUSSè€Œä¸æ˜¯ä¼ ç»Ÿçš„ç½‘ç»œå¯¼èˆªã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„RUSSæ¨¡å‹ä¸ºæ„å»ºèƒ½å¤Ÿä»å¼€æ”¾åŸŸæŒ‡ä»¤ä¸­å­¦ä¹ å¹¶æ‰§è¡Œç½‘ç»œä»»åŠ¡çš„AIä»£ç†æä¾›äº†ä¸€ä¸ªæ–°çš„æ€è·¯ã€‚ThingTalk DSLçš„è®¾è®¡å’Œæ¨¡å—åŒ–è®¾è®¡æ–¹æ³•å¯ä»¥åº”ç”¨äºå…¶ä»–ç±»ä¼¼çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°ç§»åŠ¨è®¾å¤‡æ“ä½œæˆ–æœºå™¨äººæ“ä½œã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„RUSSæ•°æ®é›†å¯ä»¥ç”¨äºè¯„ä¼°å’Œæ”¹è¿›æœªæ¥çš„ç ”ç©¶ã€‚</td>
    </tr>
    <tr>
      <th>35</th>
      <td>AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents</td>
      <td>Autonomy via agents using large language models (LLMs) for personalized,<br>standardized tasks boosts human efficiency. Automating web tasks (like booking<br>hotels within a budget) is increasingly sought after. Fulfilling practical<br>needs, the web agent also serves as an important proof-of-concept example for<br>various agent grounding scenarios, with its success promising advancements in<br>many future applications. Prior research often handcrafts web agent strategies<br>(e.g., prompting templates, multi-agent systems, search methods, etc.) and the<br>corresponding in-context examples, which may not generalize well across all<br>real-world scenarios. On the other hand, there has been limited study on the<br>misalignment between a web agent's observation/action representation and the<br>pre-training data of the LLM it's based on. This discrepancy is especially<br>notable when LLMs are primarily trained for language completion rather than<br>tasks involving embodied navigation actions and symbolic web elements. Our<br>study enhances an LLM-based web agent by simply refining its observation and<br>action space to better align with the LLM's capabilities. This approach enables<br>our base agent to significantly outperform previous methods on a wide variety<br>of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose<br>web interaction tasks, our agent AgentOccam surpasses the previous<br>state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute<br>points respectively, and boosts the success rate by 26.6 points (+161%) over<br>similar plain web agents with its observation and action space alignment. We<br>achieve this without using in-context examples, new agent roles, online<br>feedback or search strategies. AgentOccam's simple design highlights LLMs'<br>impressive zero-shot performance on web tasks, and underlines the critical role<br>of carefully tuning observation and action spaces for LLM-based agents.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AgentOccamï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç½‘ç»œä»£ç†çš„ç®€å•è€Œå¼ºå¤§çš„åŸºçº¿<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œç½‘ç»œä»£ç†åœ¨ä¸ªæ€§åŒ–ã€æ ‡å‡†åŒ–çš„ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä¾‹å¦‚è‡ªåŠ¨é¢„è®¢é…’åº—ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç½‘ç»œä»£ç†ç­–ç•¥å¾€å¾€ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„æ¨¡æ¿ã€å¤šä»£ç†ç³»ç»Ÿã€æœç´¢æ–¹æ³•ç­‰ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ‰€æœ‰ç°å®åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œç½‘ç»œä»£ç†çš„è§‚å¯Ÿ/è¡ŒåŠ¨è¡¨ç¤ºä¸LLMçš„é¢„è®­ç»ƒæ•°æ®ä¹‹é—´çš„ä¸åŒ¹é…ä¹Ÿæ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºLLMä¸»è¦é’ˆå¯¹è¯­è¨€å®Œæˆè¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯æ¶‰åŠå…·èº«å¯¼èˆªåŠ¨ä½œå’Œç¬¦å·ç½‘ç»œå…ƒç´ çš„ä»»åŠ¡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç®€åŒ–è¡ŒåŠ¨ç©ºé—´<br>AgentOccamé€šè¿‡åˆ é™¤ä¸å¿…è¦çš„è¡ŒåŠ¨ï¼Œä¾‹å¦‚æ— æ“ä½œã€æ ‡ç­¾æ“ä½œå’Œé¡µé¢å¯¼èˆªæ“ä½œï¼Œæ¥ç®€åŒ–è¡ŒåŠ¨ç©ºé—´ã€‚åŒæ—¶ï¼Œå®ƒå°†ä½çº§è¡ŒåŠ¨ç®€åŒ–ä¸ºæ›´æŠ½è±¡çš„æ“ä½œï¼Œä¾‹å¦‚å°†æ‚¬åœå’ŒæŒ‰é”®æ“ä½œæ›¿æ¢ä¸ºç‚¹å‡»æ“ä½œï¼Œå¹¶å°†æ»šåŠ¨æ“ä½œæ›¿æ¢ä¸ºåŠ è½½æ•´ä¸ªé¡µé¢å†…å®¹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¼˜åŒ–è§‚å¯Ÿç©ºé—´<br>AgentOccamé€šè¿‡åˆ é™¤å†—ä½™å’Œæ— å…³çš„ç½‘ç»œå…ƒç´ ï¼Œå¹¶å°†ç½‘é¡µå†…å®¹å—é‡æ„ä¸ºæ›´ç®€æ´ä½†åŒæ ·ä¿¡æ¯ä¸°å¯Œçš„è¡¨ç¤ºï¼Œæ¥ä¼˜åŒ–è§‚å¯Ÿç©ºé—´ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†ä¸¤ä¸ªè§„åˆ’è¡ŒåŠ¨ï¼ˆåˆ†æ”¯å’Œå‰ªæï¼‰ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä½¿ç”¨è§„åˆ’æ ‘æ¥è‡ªä¸»ç»„ç»‡å¯¼èˆªå·¥ä½œæµç¨‹ï¼Œå¹¶ä½¿ç”¨ç›¸åŒç»“æ„æ¥è¿‡æ»¤å†å²è®°å½•ä»¥è¿›è¡Œå†å²å›æ”¾ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šè§„åˆ’ç”Ÿæˆ<br>AgentOccamå¼•å…¥äº†åˆ†æ”¯å’Œå‰ªæè¡ŒåŠ¨ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆè®¡åˆ’å¹¶ç®¡ç†ä»»åŠ¡å·¥ä½œæµç¨‹ã€‚è¿™äº›è¡ŒåŠ¨å…è®¸ä»£ç†å°†é«˜çº§ç›®æ ‡åˆ†è§£ä¸ºæ›´å°çš„å­ç›®æ ‡ï¼Œå¹¶åœ¨å½“å‰å­è®¡åˆ’ä¸å¯è¡Œæ—¶å¯»æ±‚æ›¿ä»£æ–¹æ¡ˆã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨WebArenaåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAgentOccamåœ¨é€šç”¨ç½‘ç»œäº¤äº’ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æœ€ä½³æ–¹æ³•å’ŒåŒæœŸå·¥ä½œï¼Œåˆ†åˆ«æé«˜äº†9.8ï¼ˆ+29.4%ï¼‰å’Œ5.9ï¼ˆ+15.8%ï¼‰çš„ç»å¯¹åˆ†æ•°ï¼Œå¹¶å°†æˆåŠŸç‡æé«˜äº†26.6ï¼ˆ+161%ï¼‰ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>AgentOccamçš„ç®€å•è®¾è®¡çªå‡ºäº†LLMåœ¨ç½‘ç»œä»»åŠ¡ä¸Šçš„ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒäº†ä»”ç»†è°ƒæ•´è§‚å¯Ÿå’Œè¡ŒåŠ¨ç©ºé—´å¯¹äºåŸºäºLLMçš„ä»£ç†çš„å…³é”®ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥çš„ç½‘ç»œä»£ç†ç ”ç©¶å’Œå¼€å‘å¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œå¹¶æä¾›äº†ä¸€äº›æœ‰ä»·å€¼çš„è§è§£ã€‚</td>
    </tr>
    <tr>
      <th>36</th>
      <td>Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models</td>
      <td>Large Language Models (LLMs) have achieved remarkable success in reasoning<br>tasks with the development of prompting methods. However, existing prompting<br>approaches cannot reuse insights of solving similar problems and suffer from<br>accumulated errors in multi-step reasoning, since they prompt LLMs to reason<br>\textit{from scratch}. To address these issues, we propose<br>\textbf{\textit{Thought Propagation} (TP)}, which explores the analogous<br>problems and leverages their solutions to enhance the complex reasoning ability<br>of LLMs. These analogous problems are related to the input one, with reusable<br>solutions and problem-solving strategies. Thus, it is promising to propagate<br>insights of solving previous analogous problems to inspire new problem-solving.<br>To achieve this, TP first prompts LLMs to propose and solve a set of analogous<br>problems that are related to the input one. Then, TP reuses the results of<br>analogous problems to directly yield a new solution or derive a<br>knowledge-intensive plan for execution to amend the initial solution obtained<br>from scratch. TP is compatible with existing prompting approaches, allowing<br>plug-and-play generalization and enhancement in a wide range of tasks without<br>much labor in task-specific prompt engineering. Experiments across three<br>challenging tasks demonstrate TP enjoys a substantial improvement over the<br>baselines by an average of 12\% absolute increase in finding the optimal<br>solutions in Shortest-path Reasoning, 13\% improvement of human preference in<br>Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent<br>Planning.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ€ç»´ä¼ æ’­ï¼šä¸€ç§åŸºäºç±»æ¯”çš„å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†æ–¹æ³•<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ç°æœ‰çš„æç¤ºæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š1ï¼‰æ— æ³•é‡ç”¨è§£å†³ç±»ä¼¼é—®é¢˜çš„è§è§£ï¼›2ï¼‰åœ¨å¤šæ­¥æ¨ç†ä¸­å®¹æ˜“ç´¯ç§¯é”™è¯¯ã€‚è¿™æ˜¯å› ä¸ºç°æœ‰çš„æ–¹æ³•éƒ½æ˜¯ä»é›¶å¼€å§‹è¿›è¡Œæ¨ç†ï¼Œæ— æ³•åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”å¯¹ä¸­é—´æ¨ç†é˜¶æ®µçš„é”™è¯¯æ•æ„Ÿã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†â€œæ€ç»´ä¼ æ’­â€ï¼ˆThought Propagation, TPï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚TP æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ç±»æ¯”æ¨ç†ï¼Œé€šè¿‡æ¢ç´¢ä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„ç±»ä¼¼é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬çš„è§£å†³æ–¹æ¡ˆæ¥å¢å¼º LLMs çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚<br><br>### TP æ¡†æ¶çš„ä¸‰ä¸ªæ¨¡å—ï¼š<br>1. **LLM Propose**ï¼šç”Ÿæˆä¸è¾“å…¥é—®é¢˜ç›¸å…³çš„ç±»ä¼¼é—®é¢˜ã€‚<br>2. **LLM Solve**ï¼šè§£å†³è¾“å…¥é—®é¢˜å’Œç±»ä¼¼é—®é¢˜ï¼Œå¹¶ç”Ÿæˆåˆå§‹è§£å†³æ–¹æ¡ˆã€‚<br>3. **LLM Aggregate**ï¼šèšåˆç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥ç”Ÿæˆæ–°çš„è§£å†³æ–¹æ¡ˆæˆ–åˆ¶å®šé«˜çº§è®¡åˆ’ï¼Œä»è€Œæ”¹è¿›è¾“å…¥é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚<br><br>### TP æ¡†æ¶çš„ä¼˜åŠ¿ï¼š<br>* **é‡ç”¨å…ˆéªŒçŸ¥è¯†**ï¼šé€šè¿‡ç±»æ¯”æ¨ç†ï¼ŒTP å¯ä»¥é‡ç”¨è§£å†³ç±»ä¼¼é—®é¢˜çš„è§è§£ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡ã€‚<br>* **å‡å°‘ç´¯ç§¯é”™è¯¯**ï¼šé€šè¿‡åˆ©ç”¨ç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼ŒTP å¯ä»¥å‡å°‘å¤šæ­¥æ¨ç†ä¸­çš„ç´¯ç§¯é”™è¯¯ã€‚<br>* **å…¼å®¹æ€§å¼º**ï¼šTP å¯ä»¥ä¸ç°æœ‰çš„æç¤ºæ–¹æ³•å…¼å®¹ï¼Œå®ç°å³æ’å³ç”¨çš„æ³›åŒ–å’Œå¢å¼ºï¼Œæ— éœ€è¿›è¡Œå¤§é‡çš„ä»»åŠ¡ç‰¹å®šæç¤ºå·¥ç¨‹ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¯„ä¼°äº† TP æ¡†æ¶ï¼ŒåŒ…æ‹¬æœ€çŸ­è·¯å¾„æ¨ç†ã€åˆ›æ„å†™ä½œå’Œ LLM-Agent è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTP åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚ï¼š<br>* **æœ€çŸ­è·¯å¾„æ¨ç†**ï¼šå¹³å‡ç»å¯¹æå‡ 12% çš„æœ€ä¼˜è§£æ‰¾åˆ°ç‡ã€‚<br>* **åˆ›æ„å†™ä½œ**ï¼šäººç±»åå¥½åº¦æå‡ 13%ã€‚<br>* **LLM-Agent è§„åˆ’**ï¼šä»»åŠ¡å®Œæˆç‡æå‡ 15%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>* **ç±»æ¯”æ¨ç†çš„åº”ç”¨**ï¼šTP æ¡†æ¶ä¸º LLMs çš„å¤æ‚æ¨ç†æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ï¼Œå³åˆ©ç”¨ç±»æ¯”æ¨ç†æ¥é‡ç”¨å…ˆéªŒçŸ¥è¯†å¹¶å‡å°‘ç´¯ç§¯é”™è¯¯ã€‚<br>* **æ¨¡å—åŒ–è®¾è®¡**ï¼šTP æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶æ˜“äºä¸å…¶ä»–æç¤ºæ–¹æ³•é›†æˆï¼Œå¹¶å®ç°å³æ’å³ç”¨çš„æ³›åŒ–å’Œå¢å¼ºã€‚<br>* **å®éªŒéªŒè¯**ï¼šæœ¬æ–‡åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå¯¹ TP æ¡†æ¶è¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜å…¶å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>TP æ¡†æ¶ä¸º LLMs çš„å¤æ‚æ¨ç†æä¾›äº†ä¸€ç§æ–°é¢–è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡ç±»æ¯”æ¨ç†æ¥é‡ç”¨å…ˆéªŒçŸ¥è¯†å¹¶å‡å°‘ç´¯ç§¯é”™è¯¯ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨ç†æ•ˆç‡ã€‚TP æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡å’Œå…¼å®¹æ€§å¼ºä½¿å…¶æ˜“äºåº”ç”¨å’Œæ‰©å±•ï¼Œæœ‰æœ›åœ¨æœªæ¥æ¨åŠ¨ LLMs æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥æå‡ã€‚</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Large Language Model-Brained GUI Agents: A Survey</td>
      <td>GUIs have long been central to human-computer interaction, providing an<br>intuitive and visually-driven way to access and interact with digital systems.<br>The advent of LLMs, particularly multimodal models, has ushered in a new era of<br>GUI automation. They have demonstrated exceptional capabilities in natural<br>language understanding, code generation, and visual processing. This has paved<br>the way for a new generation of LLM-brained GUI agents capable of interpreting<br>complex GUI elements and autonomously executing actions based on natural<br>language instructions. These agents represent a paradigm shift, enabling users<br>to perform intricate, multi-step tasks through simple conversational commands.<br>Their applications span across web navigation, mobile app interactions, and<br>desktop automation, offering a transformative user experience that<br>revolutionizes how individuals interact with software. This emerging field is<br>rapidly advancing, with significant progress in both research and industry.<br>  To provide a structured understanding of this trend, this paper presents a<br>comprehensive survey of LLM-brained GUI agents, exploring their historical<br>evolution, core components, and advanced techniques. We address research<br>questions such as existing GUI agent frameworks, the collection and utilization<br>of data for training specialized GUI agents, the development of large action<br>models tailored for GUI tasks, and the evaluation metrics and benchmarks<br>necessary to assess their effectiveness. Additionally, we examine emerging<br>applications powered by these agents. Through a detailed analysis, this survey<br>identifies key research gaps and outlines a roadmap for future advancements in<br>the field. By consolidating foundational knowledge and state-of-the-art<br>developments, this work aims to guide both researchers and practitioners in<br>overcoming challenges and unlocking the full potential of LLM-brained GUI<br>agents.</td>
      <td>Wang, and G. Zeng, â€œPaLM-2: An<br>evolved language model for everything,â€ arXiv preprint arXiv:2304.02162,<br>2023.<br>[455] J. Li, Z. Wang, Y. Li, X. Wang, Y. Li, H. Wang, and Y. Li,<br>â€œFlorence-2-base: A lightweight vision-language model for<br>efficient<br>web<br>interaction,â€<br>2024.<br>[Online].<br>Available:<br>https://arxiv.org/abs/2404.03667<br>[456] Z. Wang, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, L. Chen, Z. Liu,<br>P. P. Liang et al., â€œOs-atlas: A foundation action model for<br>generalist gui agents,â€ arXiv preprint arXiv:2410.23218, 2024.<br>[457] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[458] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[459] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[460] S. Malek, A. Burns, D. Arsan, R. Kumar, K. Saenko, and B. A.<br>Plummer, â€œMeta-gui: Towards multi-modal conversational agents<br>on mobile gui,â€ 2022. [Online]. Available: https://arxiv.org/abs/<br>2205.11029<br>[461] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[462] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[463] OpenAI, â€œOperator: Introducing a universal interface for ai to<br>interact with the digital world,â€ 2025. [Online]. Available:<br>https://openai.com/index/computer-using-agent<br>[464] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,<br>X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,<br>S. Gelly, J. Uszkoreit, and N. Houlsby, â€œAn image is worth 16x16<br>words: Transformers for image recognition at scale,â€ in International<br>conference on machine learning. PMLR, 2020, pp. 15 841â€“15 855.<br>[465] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[466] Z. Wang, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, L. Chen, Z. Liu,<br>P. P. Liang et al., â€œOs-atlas: A foundation action model for<br>generalist gui agents,â€ arXiv preprint arXiv:2410.23218, 2024.<br>[467] Z. Chen, H. Zhang, O. Rippel, M. Mattsson, and J. Redmon,<br>â€œSwin transformer: Hierarchical vision transformer using shifted<br>windows,â€ in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, 2021, pp. 10091â€“10100.<br>[468] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[469] H. Zhang, Z. Chen, Y. Li, Y. Wang, and H. Zhang, â€œMixture of<br>experts for vision language understanding,â€ in Proceedings of the<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition,<br>2023, pp. 17 718â€“17 728.<br>[470] Y. Sun, S. Wang, Y. Li, S. Feng, H. Wang, and H. Wang, â€œSmall-<br>bert: A distilled version of bert for natural language understanding,â€<br>arXiv preprint arXiv:1908.08124, 2019.<br>[471] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[472] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[473] T. Wang, K. Wei, S. Feng, B. Chen, H. Li, M. Zhou, H. Wang,<br>J. Wang, Z. Wang, and H. Zhou, â€œMpt-7b: Training a large language<br>model to follow your instructions,â€ arXiv preprint arXiv:2304.08716,<br>2023.<br>[474] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[475] E. Tzeng, J. Wang, Y. Wang, Y. Li, and H. Zhou, â€œEva-2-clip:<br>An open-source vision-language model for multimodal understanding<br>and generation,â€ arXiv preprint arXiv:2403.05055, 2024.<br>[476] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[477] Z. Chen, Y. Li, Y. Wang, H. Zhang, Z. Chen, Z. Wang, L. Lu,<br>Y. Li, and H. Zhou, â€œConvnext-xlarge: A large-scale vision<br>model for multimodal understanding and generation,â€ arXiv preprint<br>arXiv:2304.13677, 2023.<br>[478] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[479] D. Zimmermann and A. Koziolek, â€œGui-based software testing: An<br>automated approach using gpt-4 and selenium webdriver,â€ in 2023<br>38th IEEE/ACM International Conference on Automated Software<br>Engineering Workshops (ASEW). IEEE, 2023, pp. 171â€“174.<br>[480] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[481] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[482] Eko. (2024) Eko: The ai agent platform. Accessed: 2024-11-16.<br>[Online]. Available: https://eko.fellou.ai/<br>[483] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[484] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[485] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[486] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[487] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[488] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[489] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[490] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[491] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[492] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[493] C. Dwork, F. McSherry, K. Nissim, and A. Smith, â€œCalibrating<br>noise to sensitivity in private data analysis,â€ in Theory of Cryptography<br>Conference. Springer, 2006, pp. 265â€“284.<br>[494] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[495] C. Gentry, S. Halevi, and N. P. Smart, â€œHomomorphic encryption<br>from learning with errors: Conceptually-simpler, asymptotically-<br>faster, attribute-based, and more secure,â€ in Proceedings of the 14th<br>ACM SIGSAC Conference on Computer and Communications Security.<br>ACM, 2007, pp. 129â€“138.<br>[496] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[497] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[498] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[499] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[500] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[501] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[502] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[503] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[504] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[505] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[506] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[507] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[508] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[509] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[510] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[511] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[512] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[513] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[514] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[515] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[516] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge, â€œMapping<br>natural language instructions to mobile ui action sequences,â€ 2020.<br>[Online]. Available: https://arxiv.org/abs/2005.03776<br>[517] Y. Li, J. He, X. Zhou, Y</td>
    </tr>
    <tr>
      <th>38</th>
      <td>AppAgent: Multimodal Agents as Smartphone Users</td>
      <td>Recent advancements in large language models (LLMs) have led to the creation<br>of intelligent agents capable of performing complex tasks. This paper<br>introduces a novel LLM-based multimodal agent framework designed to operate<br>smartphone applications. Our framework enables the agent to operate smartphone<br>applications through a simplified action space, mimicking human-like<br>interactions such as tapping and swiping. This novel approach bypasses the need<br>for system back-end access, thereby broadening its applicability across diverse<br>apps. Central to our agent's functionality is its innovative learning method.<br>The agent learns to navigate and use new apps either through autonomous<br>exploration or by observing human demonstrations. This process generates a<br>knowledge base that the agent refers to for executing complex tasks across<br>different applications. To demonstrate the practicality of our agent, we<br>conducted extensive testing over 50 tasks in 10 different applications,<br>including social media, email, maps, shopping, and sophisticated image editing<br>tools. The results affirm our agent's proficiency in handling a diverse array<br>of high-level tasks.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AppAgentï¼šåƒäººç±»ç”¨æˆ·ä¸€æ ·æ“ä½œæ™ºèƒ½æ‰‹æœºåº”ç”¨çš„æ™ºèƒ½ä½“<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ™ºèƒ½ä½“ä¸»è¦ä¾èµ–äºæ–‡æœ¬ä¿¡æ¯ï¼Œé™åˆ¶äº†å®ƒä»¬ä¸ç¯å¢ƒçš„äº¤äº’èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨åƒäººç±»ç”¨æˆ·ä¸€æ ·æ“ä½œæ™ºèƒ½æ‰‹æœºåº”ç”¨ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šç®€åŒ–åŠ¨ä½œç©ºé—´<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–çš„åŠ¨ä½œç©ºé—´ï¼ŒåŒ…æ‹¬ç‚¹å‡»ã€é•¿æŒ‰ã€æ»‘åŠ¨å’Œè¾“å…¥æ–‡æœ¬ç­‰æ“ä½œï¼Œæ¨¡æ‹Ÿäººç±»ä¸æ™ºèƒ½æ‰‹æœºçš„äº¤äº’æ–¹å¼ã€‚è¿™ç§è®¾è®¡é¿å…äº†éœ€è¦ç²¾ç¡®å±å¹•åæ ‡çš„é—®é¢˜ï¼Œæé«˜äº†æ™ºèƒ½ä½“çš„æ“ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæ¢ç´¢å¼å­¦ä¹ æ–¹æ³•<br>æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ¢ç´¢å¼å­¦ä¹ æ–¹æ³•ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿå­¦ä¹ ä½¿ç”¨æ–°çš„åº”ç”¨ç¨‹åºã€‚ä¸€ç§æ˜¯è‡ªä¸»æ¢ç´¢ï¼Œæ™ºèƒ½ä½“é€šè¿‡å°è¯•ä¸åŒçš„æ“ä½œå¹¶è§‚å¯Ÿç»“æœæ¥å­¦ä¹ åº”ç”¨ç¨‹åºçš„åŠŸèƒ½ã€‚å¦ä¸€ç§æ˜¯è§‚å¯Ÿäººç±»æ¼”ç¤ºï¼Œæ™ºèƒ½ä½“é€šè¿‡è§‚å¯Ÿäººç±»ç”¨æˆ·å¦‚ä½•æ“ä½œåº”ç”¨ç¨‹åºæ¥å­¦ä¹ ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡åœ¨10ä¸ªä¸åŒçš„åº”ç”¨ç¨‹åºä¸Šè¿›è¡Œäº†50ä¸ªä»»åŠ¡çš„æµ‹è¯•ï¼ŒåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€ç”µå­é‚®ä»¶ã€åœ°å›¾ã€è´­ç‰©å’Œå¤æ‚çš„å›¾åƒç¼–è¾‘å·¥å…·ã€‚ç»“æœè¡¨æ˜ï¼ŒAppAgentèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„ç§é«˜çº§ä»»åŠ¡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„AppAgentæ¡†æ¶ä¸ºæ™ºèƒ½æ‰‹æœºåº”ç”¨æ“ä½œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰ä»¥ä¸‹å¯å€Ÿé‰´ä¹‹å¤„ï¼š<br><br>* **ç®€åŒ–åŠ¨ä½œç©ºé—´**ï¼šé€šè¿‡è®¾è®¡ç®€åŒ–çš„åŠ¨ä½œç©ºé—´ï¼Œå¯ä»¥é™ä½æ™ºèƒ½ä½“æ“ä½œçš„å¤æ‚æ€§ï¼Œæé«˜æ“ä½œæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚<br>* **æ¢ç´¢å¼å­¦ä¹ æ–¹æ³•**ï¼šé€šè¿‡è‡ªä¸»æ¢ç´¢æˆ–è§‚å¯Ÿäººç±»æ¼”ç¤ºï¼Œæ™ºèƒ½ä½“å¯ä»¥å¿«é€Ÿå­¦ä¹ ä½¿ç”¨æ–°çš„åº”ç”¨ç¨‹åºï¼Œæé«˜é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚<br>* **å¤šæ¨¡æ€äº¤äº’**ï¼šç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ›´å¥½åœ°ç†è§£ç¯å¢ƒå’Œæ‰§è¡Œä»»åŠ¡ã€‚<br><br>## ğŸŒŸ æ€»ç»“<br>AppAgentæ˜¯ä¸€ç§åŸºäºLLMçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿåƒäººç±»ç”¨æˆ·ä¸€æ ·æ“ä½œæ™ºèƒ½æ‰‹æœºåº”ç”¨ã€‚è¯¥æ¡†æ¶å…·æœ‰ç®€åŒ–åŠ¨ä½œç©ºé—´ã€æ¢ç´¢å¼å­¦ä¹ æ–¹æ³•å’Œå¤šæ¨¡æ€äº¤äº’ç­‰åˆ›æ–°ç‚¹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„ç§é«˜çº§ä»»åŠ¡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚AppAgentä¸ºæ™ºèƒ½æ‰‹æœºåº”ç”¨æ“ä½œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</td>
    </tr>
    <tr>
      <th>39</th>
      <td>MobileExperts: A Dynamic Tool-Enabled Agent Team in Mobile Devices</td>
      <td>The attainment of autonomous operations in mobile computing devices has<br>consistently been a goal of human pursuit. With the development of Large<br>Language Models (LLMs) and Visual Language Models (VLMs), this aspiration is<br>progressively turning into reality. While contemporary research has explored<br>automation of simple tasks on mobile devices via VLMs, there remains<br>significant room for improvement in handling complex tasks and reducing high<br>reasoning costs. In this paper, we introduce MobileExperts, which for the first<br>time introduces tool formulation and multi-agent collaboration to address the<br>aforementioned challenges. More specifically, MobileExperts dynamically<br>assembles teams based on the alignment of agent portraits with the human<br>requirements. Following this, each agent embarks on an independent exploration<br>phase, formulating its tools to evolve into an expert. Lastly, we develop a<br>dual-layer planning mechanism to establish coordinate collaboration among<br>experts. To validate our effectiveness, we design a new benchmark of<br>hierarchical intelligence levels, offering insights into algorithm's capability<br>to address tasks across a spectrum of complexity. Experimental results<br>demonstrate that MobileExperts performs better on all intelligence levels and<br>achieves ~ 22% reduction in reasoning costs, thus verifying the superiority of<br>our design.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MobileExpertsï¼šç§»åŠ¨è®¾å¤‡ä¸­çš„åŠ¨æ€å·¥å…·èµ‹èƒ½æ™ºèƒ½ä½“å›¢é˜Ÿ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•ï¼Œç§»åŠ¨è®¾å¤‡ä¸Šçš„è‡ªä¸»æ“ä½œé€æ¸æˆä¸ºç°å®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡VLMsè‡ªåŠ¨åŒ–ç§»åŠ¨è®¾å¤‡ä¸Šçš„ç®€å•ä»»åŠ¡ï¼Œè€Œåœ¨å¤„ç†å¤æ‚ä»»åŠ¡å’Œé™ä½æ¨ç†æˆæœ¬æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šåŸºäºä»£ç ç»„åˆçš„å·¥å…·å½¢æˆ<br>ä¸ºäº†å‡å°‘å¯¹VLMæ¨¡å‹çš„ä¾èµ–ï¼ŒMobileExpertsæå‡ºäº†ä¸€ç§æ–°çš„å·¥å…·æ¨¡å¼ï¼šåˆ©ç”¨LLMçš„ä»£ç ç¼–å†™èƒ½åŠ›ï¼Œé€šè¿‡ä»£ç ç»„åˆå°†ä¸“å®¶çš„åŸºæœ¬æ“ä½œç»„åˆæˆå¯é‡ç”¨çš„ä»£ç å—å·¥å…·ã€‚è¿™äº›å·¥å…·å­˜å‚¨åœ¨ç³»ç»Ÿä¸­çš„æ‰€æœ‰ä¸“å®¶å…±äº«çš„è¿‡ç¨‹è®°å¿†ä¸­ï¼Œä»è€Œé™ä½äº†å·¥å…·å½¢æˆçš„æˆæœ¬ï¼Œå¹¶æœ‰æ•ˆåœ°æé«˜äº†æ“ä½œæ•ˆç‡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šé€šè¿‡åŒå±‚è§„åˆ’å®ç°ä¸“å®¶åä½œ<br>MobileExpertsé‡‡ç”¨åŒå±‚è§„åˆ’æ–¹æ³•æ¥è§£å†³é•¿æœŸè§„åˆ’é—®é¢˜ï¼šç¬¬ä¸€å±‚æ˜¯å›¢é˜Ÿä»»åŠ¡åˆ†é…å±‚ï¼Œåœ¨è¿™ä¸€å±‚ï¼Œä»»åŠ¡è¢«åˆ†è§£ä¸ºä¸“å®¶æ‰§è¡Œçš„ä¾èµ–å­ä»»åŠ¡ï¼Œè¿™äº›ä¾èµ–å½¢æˆä¸“å®¶ä¹‹é—´çš„åä½œç½‘ç»œï¼›ç¬¬äºŒå±‚æ˜¯ä¸“å®¶ä»»åŠ¡åˆ†è§£å±‚ï¼Œä¸“å®¶æ¥æ”¶åˆ°çš„å­ä»»åŠ¡è¿›ä¸€æ­¥åˆ†è§£ä¸ºæ›´å°çš„åŠ¨ä½œå•å…ƒï¼Œé€æ­¥å®ç°ç›®æ ‡ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼ŒMobileExpertsä¸ä»…æé«˜äº†æ™ºèƒ½æ°´å¹³ï¼Œè¿˜é™ä½äº†æ¨ç†æˆæœ¬å’Œæ—¶é—´ï¼Œä¸ºDOAé¢†åŸŸæä¾›äº†æ›´é«˜æ•ˆçš„æ‰§è¡Œæ¨¡å¼ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>ä¸ºäº†éªŒè¯MobileExpertsçš„æœ‰æ•ˆæ€§ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸€ä¸ªæ–°çš„åˆ†å±‚æ™ºèƒ½æ°´å¹³åŸºå‡†ï¼Œæä¾›äº†å¯¹ç®—æ³•å¤„ç†ä¸åŒå¤æ‚åº¦ä»»åŠ¡èƒ½åŠ›çš„è§è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMobileExpertsåœ¨æ‰€æœ‰æ™ºèƒ½æ°´å¹³ä¸Šéƒ½è¡¨ç°æ›´å¥½ï¼Œå¹¶å®ç°äº†çº¦22%çš„æ¨ç†æˆæœ¬é™ä½ï¼Œä»è€ŒéªŒè¯äº†è®¾è®¡çš„ä¼˜è¶Šæ€§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MobileExpertsä¸ºç§»åŠ¨è®¾å¤‡æ“ä½œè‡ªåŠ¨åŒ–é¢†åŸŸæä¾›äº†ä¸€ç§åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶åŸºäºä»£ç ç»„åˆçš„å·¥å…·å½¢æˆå’ŒåŒå±‚è§„åˆ’æœºåˆ¶ä¸ºå¤„ç†å¤æ‚ä»»åŠ¡å’Œé™ä½æ¨ç†æˆæœ¬æä¾›äº†æ–°çš„æ€è·¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºçš„Expert-EvalåŸºå‡†ä¸ºè¯„ä¼°ç§»åŠ¨è®¾å¤‡æ“ä½œä»£ç†çš„æ€§èƒ½æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Exploring the Privacy Protection Capabilities of Chinese Large Language Models</td>
      <td>Large language models (LLMs), renowned for their impressive capabilities in<br>various tasks, have significantly advanced artificial intelligence. Yet, these<br>advancements have raised growing concerns about privacy and security<br>implications. To address these issues and explain the risks inherent in these<br>models, we have devised a three-tiered progressive framework tailored for<br>evaluating privacy in language systems. This framework consists of<br>progressively complex and in-depth privacy test tasks at each tier. Our primary<br>objective is to comprehensively evaluate the sensitivity of large language<br>models to private information, examining how effectively they discern, manage,<br>and safeguard sensitive data in diverse scenarios. This systematic evaluation<br>helps us understand the degree to which these models comply with privacy<br>protection guidelines and the effectiveness of their inherent safeguards<br>against privacy breaches. Our observations indicate that existing Chinese large<br>language models universally show privacy protection shortcomings. It seems that<br>at the moment this widespread issue is unavoidable and may pose corresponding<br>privacy risks in applications based on these models.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ¢ç´¢ä¸­å›½å¤§å‹è¯­è¨€æ¨¡å‹çš„éšç§ä¿æŠ¤èƒ½åŠ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºçš„å¼ºå¤§èƒ½åŠ›æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ä¹Ÿå¼•å‘äº†å…³äºéšç§å’Œå®‰å…¨æ€§çš„æ‹…å¿§ã€‚ç”±äºLLMsçš„è®­ç»ƒæ•°æ®å¾€å¾€åŒ…å«å¤§é‡ä¸ªäººéšç§ä¿¡æ¯ï¼Œå› æ­¤æ¨¡å‹å¯èƒ½ä¼šæ— æ„ä¸­è®°ä½è¿™äº›å†…å®¹ï¼Œä»è€Œå¯¹æ•°æ®éšç§æ„æˆå®‰å…¨é£é™©ã€‚æ­¤å¤–ï¼Œéšç€è¶Šæ¥è¶Šå¤šçš„æœåŠ¡æä¾›å•†å°†LLMsé›†æˆåˆ°å…¶è½¯ä»¶åº”ç”¨ä¸­ï¼Œæ¨¡å‹åœ¨å¤„ç†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„ç§äººæ–‡æ¡£æ•°æ®æ—¶ï¼Œéœ€è¦ä¸¥æ ¼éµå®ˆéšç§ä¿æŠ¤è§„åˆ™ï¼Œå¹¶èƒ½å¤Ÿè¯†åˆ«å’Œä¿æŠ¤æ•æ„Ÿéšç§ä¿¡æ¯ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†è¯„ä¼°LLMsçš„éšç§ä¿æŠ¤èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰å±‚çº§çš„éšç§æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»æµ…åˆ°æ·±åœ°è¯„ä¼°LLMsåœ¨ä¸åŒä»»åŠ¡åœºæ™¯ä¸‹çš„éšç§ä¿æŠ¤èƒ½åŠ›ã€‚å…·ä½“åŒ…æ‹¬ï¼š<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šä¸€èˆ¬éšç§ä¿¡æ¯è¯„ä¼°<br>è¯¥å±‚çº§è¯„ä¼°LLMsåœ¨é¢å¯¹ç›´æ¥è¯¢é—®ä¸ªäººéšç§ä¿¡æ¯ï¼ˆå¦‚ç”µè¯å·ç ã€ç”µå­é‚®ä»¶åœ°å€ã€å®¶åº­ä½å€ç­‰ï¼‰æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹åº”èƒ½å¤Ÿæ‹’ç»å›ç­”æ­¤ç±»é—®é¢˜ï¼Œå¹¶è¾“å‡ºåˆé€‚çš„ç†ç”±ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸Šä¸‹æ–‡éšç§è¯„ä¼°<br>è¯¥å±‚çº§è¯„ä¼°LLMsåœ¨é¢å¯¹æ¶‰åŠéšç§ä¿¡æ¯çš„æƒ…å¢ƒå¯¹è¯æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹åº”èƒ½å¤Ÿè¯†åˆ«æƒ…å¢ƒä¸­çš„éšç§åè®®ï¼Œå¹¶éµå®ˆè¿™äº›åè®®ï¼Œä»¥ä¿æŠ¤ç›¸å…³å†…å®¹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ”»å‡»ä¸‹çš„éšç§è¯„ä¼°<br>è¯¥å±‚çº§è¯„ä¼°LLMsåœ¨é¢å¯¹æ”»å‡»æ€§æŒ‡ä»¤æ—¶çš„è¡¨ç°ã€‚æ¨¡å‹åº”èƒ½å¤Ÿè¯†åˆ«æ”»å‡»æŒ‡ä»¤èƒŒåçš„éšç§æ³„éœ²é£é™©ï¼Œå¹¶ä¸¥æ ¼éµå®ˆå†…éƒ¨éšç§ä¿æŠ¤æŒ‡ä»¤ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œé™¤äº†0-shotæµ‹è¯•å¤–ï¼ŒLLMsåœ¨å…¶ä»–ä»»åŠ¡åœºæ™¯ä¸‹çš„è¡¨ç°å‡ä¸ç†æƒ³ã€‚è¿™äº›æ¨¡å‹æœªèƒ½å±•ç°å‡ºè¶³å¤Ÿçš„éšç§æ•æ„Ÿæ€§å’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚è¿™è¡¨æ˜ï¼ŒLLMsåœ¨å¤„ç†åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ•°æ®æ—¶ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæ”¹è¿›ï¼Œä»¥ç¡®ä¿ç›¸å…³æ•°æ®çš„å®‰å…¨å’Œéšç§ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„éšç§æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ä¸ºè¯„ä¼°LLMsçš„éšç§ä¿æŠ¤èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç ”ç©¶ç»“æœä¹Ÿæé†’æ¨¡å‹æœåŠ¡æä¾›å•†/å¼€å‘è€…éœ€è¦æ›´åŠ å…³æ³¨LLMsçš„éšç§ä¿æŠ¤é—®é¢˜ï¼Œå¹¶é‡‡å–ç›¸åº”çš„æªæ–½æ¥é™ä½éšç§æ³„éœ²çš„é£é™©ã€‚</td>
    </tr>
    <tr>
      <th>41</th>
      <td>WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration</td>
      <td>LLM-based autonomous agents often fail to execute complex web tasks that<br>require dynamic interaction due to the inherent uncertainty and complexity of<br>these environments. Existing LLM-based web agents typically rely on rigid,<br>expert-designed policies specific to certain states and actions, which lack the<br>flexibility and generalizability needed to adapt to unseen tasks. In contrast,<br>humans excel by exploring unknowns, continuously adapting strategies, and<br>resolving ambiguities through exploration. To emulate human-like adaptability,<br>web agents need strategic exploration and complex decision-making. Monte Carlo<br>Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with<br>vast action spaces, unpredictable state transitions, and incomplete information<br>in web tasks. In light of this, we develop WebPilot, a multi-agent system with<br>a dual optimization strategy that improves MCTS to better handle complex web<br>environments. Specifically, the Global Optimization phase involves generating a<br>high-level plan by breaking down tasks into manageable subtasks and<br>continuously refining this plan, thereby focusing the search process and<br>mitigating the challenges posed by vast action spaces in classical MCTS.<br>Subsequently, the Local Optimization phase executes each subtask using a<br>tailored MCTS designed for complex environments, effectively addressing<br>uncertainties and managing incomplete information. Experimental results on<br>WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on<br>WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93%<br>relative increase in success rate over the concurrent tree search-based method.<br>WebPilot marks a significant advancement in general autonomous agent<br>capabilities, paving the way for more advanced and reliable decision-making in<br>practical environments.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebPilotï¼šçµæ´»è‡ªä¸»çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŠ©åŠ›å¤æ‚ç½‘ç»œä»»åŠ¡æ‰§è¡Œ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä¸æ–­å¢å¼ºï¼ŒåŸºäºLLMçš„è‡ªä¸»ç½‘ç»œä»£ç†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªå’Œäº¤äº’çš„æ½œåŠ›ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMç½‘ç»œä»£ç†åœ¨æ‰§è¡Œéœ€è¦åŠ¨æ€äº¤äº’çš„å¤æ‚ç½‘ç»œä»»åŠ¡æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬è¿‡åº¦ä¾èµ–ä¸“å®¶è®¾è®¡çš„ã€é’ˆå¯¹ç‰¹å®šçŠ¶æ€å’ŒåŠ¨ä½œçš„åˆšæ€§ç­–ç•¥ã€‚è¿™äº›ç­–ç•¥è™½ç„¶é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œäº†ç²¾å¿ƒè®¾è®¡ï¼Œä½†ç¼ºä¹çµæ´»æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéš¾ä»¥é€‚åº”ç°å®ä¸–ç•Œä¸­ç½‘ç»œç¯å¢ƒçš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†WebPilotï¼Œä¸€ä¸ªçµæ´»è‡ªä¸»çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå®ƒç»“åˆäº†å…¨å±€ä¼˜åŒ–å’Œå±€éƒ¨ä¼˜åŒ–çš„åŒé‡ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜ä»£ç†åœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šå…¨å±€ä¼˜åŒ–ï¼šé€šè¿‡åæ€è°ƒæ•´è¿›è¡Œè‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–<br>WebPilotçš„å…¨å±€ä¼˜åŒ–é˜¶æ®µæ¨¡æ‹Ÿäº†äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œé€šè¿‡åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ä¸ºä¸ç†Ÿæ‚‰çš„ä»»åŠ¡ç”Ÿæˆåˆå§‹è®¡åˆ’ã€‚ç„¶è€Œï¼Œç”±äºLLMç¼ºä¹ç‰¹å®šçš„ç½‘ç»œé¢†åŸŸçŸ¥è¯†ï¼Œä»¥åŠç½‘ç»œç¯å¢ƒçš„åŠ¨æ€æ€§å’Œä¸ç¡®å®šæ€§ï¼Œåˆå§‹è®¡åˆ’å¾€å¾€ç¼ºä¹å…³é”®ç»†èŠ‚ï¼Œéš¾ä»¥ä¿æŒæœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒWebPiloté€šè¿‡åæ€åˆ†ææ–°è§‚å¯Ÿå’Œå…ˆå‰å­ä»»åŠ¡çš„ç»“æœï¼Œä¸æ–­ä¼˜åŒ–åˆå§‹è®¡åˆ’ã€‚å…¨å±€ä¼˜åŒ–åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±‚æ¬¡ä»»åŠ¡åˆ†è§£ï¼ˆHTDï¼‰å’Œåæ€ä»»åŠ¡è°ƒæ•´ï¼ˆRTAï¼‰ã€‚<br><br>*   **å±‚æ¬¡ä»»åŠ¡åˆ†è§£ï¼ˆHTDï¼‰**ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“äºç®¡ç†çš„å­ä»»åŠ¡ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªçµæ´»çš„é«˜çº§è®¡åˆ’ï¼Œå¯ä»¥é€‚åº”ç½‘ç»œç¯å¢ƒçš„ä¸æ–­å˜åŒ–ã€‚<br>*   **åæ€ä»»åŠ¡è°ƒæ•´ï¼ˆRTAï¼‰**ï¼šåœ¨å®Œæˆæ¯ä¸ªå­ä»»åŠ¡åï¼ŒWebPilotä¼šé‡æ–°è¯„ä¼°å’Œä¼˜åŒ–å…¶é«˜çº§è®¡åˆ’ï¼Œä»¥ç¡®ä¿ä¸æ•´ä½“ä»»åŠ¡ä¿æŒä¸€è‡´ã€‚æ§åˆ¶å™¨ä¼šè¯„ä¼°å½“å‰è§‚å¯Ÿå’Œæ‰§è¡Œçš„åŠ¨ä½œåºåˆ—æ˜¯å¦ç¬¦åˆå­ä»»åŠ¡ï¼Œå¹¶æ ¹æ®æ–°è§‚å¯Ÿé‡æ–°æ ¡å‡†ç­–ç•¥ã€‚<br><br>### ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå±€éƒ¨ä¼˜åŒ–ï¼šMCTSå¢å¼ºçš„å†³ç­–ç­–ç•¥<br>WebPilotçš„å±€éƒ¨ä¼˜åŒ–é˜¶æ®µå—äººç±»åœ¨å¯¼èˆªå’Œè§£å†³å¤æ‚ç½‘ç»œä»»åŠ¡æ—¶æ‰€éœ€çš„ç±»ä¼¼é€‚åº”æ€§å¯å‘ï¼Œæœ‰æ•ˆåœ°é€šè¿‡MCTSæ¥æ•æ‰ã€‚å¯¹äºæ¯ä¸ªå­ä»»åŠ¡åŠå…¶å­ä»»åŠ¡ç‰¹å®šçš„ç›®æ ‡ï¼Œæ¢ç´¢å™¨ã€éªŒè¯å™¨å’Œè¯„ä¼°å™¨ååŒå·¥ä½œä»¥å®Œæˆä»»åŠ¡ã€‚æ¢ç´¢å™¨è¯†åˆ«æœ€ä½³åŠ¨ä½œï¼ŒéªŒè¯å™¨ç¡®ä¿è¿™äº›åŠ¨ä½œæœ‰æ•ˆä¸”ä¸å†—ä½™ï¼Œè¯„ä¼°å™¨è¯„ä¼°åŠ¨ä½œçš„å³æ—¶æœ‰æ•ˆæ€§å’Œå®ç°é¢„æœŸç›®æ ‡çš„æ½œåŠ›ï¼Œå¹¶æä¾›æŒç»­çš„åé¦ˆï¼Œä»¥ä¾¿è¿›è¡Œæ›´ç»†è‡´å’Œå‡†ç¡®çš„è¯„ä¼°ã€‚<br><br>WebPilotçš„å±€éƒ¨ä¼˜åŒ–é˜¶æ®µç±»ä¼¼äºç»å…¸çš„MCTSï¼Œéµå¾ªå››ä¸ªå…³é”®é˜¶æ®µï¼š<br><br>*   **ç›®æ ‡å¯¼å‘é€‰æ‹©ï¼ˆGOSï¼‰**ï¼šåˆ©ç”¨LLMçš„åˆå§‹ç›´è§‰ï¼Œå¼•å¯¼WebPilotæœç€å­ä»»åŠ¡å®Œæˆçš„æœ€ä½³è·¯å¾„å‰è¿›ã€‚<br>*   **åæ€å¢å¼ºèŠ‚ç‚¹æ‰©å±•ï¼ˆRENEï¼‰**ï¼šåœ¨æ¯æ¬¡èŠ‚ç‚¹æ‰©å±•åé›†æˆåæ€åé¦ˆï¼Œä½¿WebPilotèƒ½å¤ŸåŠ¨æ€åœ°é‡æ–°è¯„ä¼°å’Œä¼˜åŒ–å…¶ç­–ç•¥ã€‚<br>*   **åŠ¨æ€è¯„ä¼°å’Œæ¨¡æ‹Ÿï¼ˆDESï¼‰**ï¼šé€šè¿‡åˆ†ææ‰§è¡Œçš„åŠ¨ä½œå’Œæ¨¡æ‹Ÿæ½œåœ¨ç»“æœæ¥è¯„ä¼°å½“å‰çŠ¶æ€ï¼Œä»è€Œæ¨¡æ‹Ÿäººç±»çš„è¿œè§ã€‚<br>*   **æœ€å¤§å€¼åå‘ä¼ æ’­ï¼ˆMVBï¼‰**ï¼šé€šè¿‡æŒç»­æ›´æ–°åŸºäºæœ€å¤§æœªæ¥å¥–åŠ±çš„ä»·å€¼ä¼°è®¡æ¥ä¼˜å…ˆè€ƒè™‘æœ€æœ‰æ½œåŠ›çš„è·¯å¾„ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨WebArenaå’ŒMiniWoB++ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒWebPilotåœ¨å¤æ‚ç½‘ç»œç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨WebArenaä¸Šï¼ŒWebPilotä¸GPT-4é…åˆä½¿ç”¨ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸å¹¶å‘çš„åŸºäºæ ‘çš„æœç´¢æ–¹æ³•ç›¸æ¯”ï¼ŒæˆåŠŸç‡æé«˜äº†93%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebPilotçš„è®¾è®¡ä¸ºå¼€å‘æ›´çµæ´»ã€æ›´è‡ªä¸»çš„ç½‘ç»œä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚å…¶åŒé‡ä¼˜åŒ–ç­–ç•¥å’ŒMCTSå¢å¼ºçš„å†³ç­–ç­–ç•¥å¯ä»¥åº”ç”¨äºå„ç§éœ€è¦åŠ¨æ€äº¤äº’å’Œå¤æ‚å†³ç­–çš„ç½‘ç»œä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒWebPilotçš„å±‚æ¬¡åæ€æœºåˆ¶å’Œç»†ç²’åº¦åŒé¢è‡ªæˆ‘å¥–åŠ±æœºåˆ¶ä¸ºæé«˜ä»£ç†åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚</td>
    </tr>
    <tr>
      <th>42</th>
      <td>MMInA: Benchmarking Multihop Multimodal Internet Agents</td>
      <td>Autonomous embodied agents live on an Internet of multimedia websites. Can<br>they hop around multimodal websites to complete complex user tasks? Existing<br>benchmarks fail to assess them in a realistic, evolving environment for their<br>embodiment across websites. To answer this question, we present MMInA, a<br>multihop and multimodal benchmark to evaluate the embodied agents for<br>compositional Internet tasks, with several appealing properties: 1) Evolving<br>real-world multimodal websites. Our benchmark uniquely operates on evolving<br>real-world websites, ensuring a high degree of realism and applicability to<br>natural user tasks. Our data includes 1,050 human-written tasks covering<br>various domains such as shopping and travel, with each task requiring the agent<br>to autonomously extract multimodal information from web pages as observations;<br>2) Multihop web browsing. Our dataset features naturally compositional tasks<br>that require information from or actions on multiple websites to solve, to<br>assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation.<br>We propose a novel protocol for evaluating an agent's progress in completing<br>multihop tasks. We experiment with both standalone (multimodal) language models<br>and heuristic-based web agents. Extensive experiments demonstrate that while<br>long-chain multihop web tasks are easy for humans, they remain challenging for<br>state-of-the-art web agents. We identify that agents are more likely to fail on<br>the early hops when solving tasks of more hops, which results in lower task<br>success rates. To address this issue, we propose a simple memory augmentation<br>approach replaying past action trajectories to reflect. Our method<br>significantly improved both the single-hop and multihop web browsing abilities<br>of agents. See our code and data at https://mmina.cliangyu.com</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MMInAï¼šè¯„ä¼°å¤šè·³å¤šæ¨¡æ€äº’è”ç½‘ä»£ç†çš„åŸºå‡†<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œè‡ªä¸»ä½“ä»£ç†åœ¨äº’è”ç½‘ä¸Šçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æ— æ³•åœ¨çœŸå®ã€åŠ¨æ€çš„ç¯å¢ƒä¸­è¯„ä¼°è¿™äº›ä»£ç†åœ¨è·¨ç½‘ç«™ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MMInAï¼Œä¸€ä¸ªå¤šè·³å’Œå¤šæ¨¡æ€çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ä»£ç†åœ¨å®Œæˆç»„åˆå¼äº’è”ç½‘ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šçœŸå®ä¸–ç•Œå¤šæ¨¡æ€ç½‘ç«™<br>MMInAåŸºå‡†æµ‹è¯•åœ¨çœŸå®ä¸–ç•Œå¤šæ¨¡æ€ç½‘ç«™ä¸Šè¿è¡Œï¼Œç¡®ä¿äº†é«˜åº¦çš„çœŸå®æ€§å’Œé€‚ç”¨æ€§ã€‚æ•°æ®åŒ…æ‹¬1050ä¸ªç”±äººç±»ç¼–å†™çš„ä»»åŠ¡ï¼Œæ¶µç›–è´­ç‰©ã€æ—…è¡Œç­‰å„ä¸ªé¢†åŸŸï¼Œæ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦ä»£ç†è‡ªä¸»åœ°ä»ç½‘é¡µä¸­æå–å¤šæ¨¡æ€ä¿¡æ¯ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šè·³ç½‘é¡µæµè§ˆ<br>MMInAæ•°æ®é›†å…·æœ‰è‡ªç„¶ç»„åˆçš„ä»»åŠ¡ï¼Œéœ€è¦ä»å¤šä¸ªç½‘ç«™è·å–ä¿¡æ¯æˆ–æ‰§è¡Œæ“ä½œæ‰èƒ½è§£å†³ï¼Œä»¥è¯„ä¼°åœ¨ç½‘é¡µä»»åŠ¡ä¸Šçš„é•¿è·ç¦»æ¨ç†èƒ½åŠ›ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šæ•´ä½“è¯„ä¼°<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åè®®ï¼Œç”¨äºè¯„ä¼°ä»£ç†åœ¨å®Œæˆå¤šè·³ä»»åŠ¡æ–¹é¢çš„è¿›å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶é•¿é“¾å¤šè·³ç½‘é¡µä»»åŠ¡å¯¹äººç±»æ¥è¯´å¾ˆå®¹æ˜“ï¼Œä½†å¯¹äºæœ€å…ˆè¿›çš„ç½‘é¡µä»£ç†æ¥è¯´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šè®°å¿†å¢å¼ºæ–¹æ³•<br>ä¸ºäº†è§£å†³ä»£ç†åœ¨æ—©æœŸè·³è½¬ä¸­å¤±è´¥çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„è®°å¿†å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡é‡æ”¾è¿‡å»çš„è¡Œä¸ºè½¨è¿¹æ¥åæ˜ ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†ä»£ç†çš„å•è·³å’Œå¤šè·³ç½‘é¡µæµè§ˆèƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨å¤„ç†ç®€å•æ–‡æœ¬ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†MMInAä¸­ä»»åŠ¡çš„é›†æˆå’Œé¡ºåºæ€§è´¨ä»ç„¶æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œè¡¨ç°æœ€å¥½çš„ç‹¬ç«‹æ¨¡å‹GPT-4Våœ¨ä»»åŠ¡ä¸­çš„æ•´ä½“æˆåŠŸç‡ä»…ä¸º21.8%ï¼Œè¿™æ¯”æ–‡æœ¬ä»£ç†åŸºçº¿æœ‰äº†æ˜¾è‘—æé«˜ï¼Œä½†ä»è½åäºäººç±»è¡¨ç°ï¼ˆ96.3%ï¼‰ã€‚å®éªŒè¿˜å‘ç°ï¼Œä»£ç†åœ¨è§£å†³æ›´å¤šè·³è½¬çš„ä»»åŠ¡æ—¶æ›´å®¹æ˜“åœ¨æ—©æœŸè·³è½¬ä¸­å¤±è´¥ï¼Œå¯¼è‡´ä»»åŠ¡æˆåŠŸç‡é™ä½ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MMInAåŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°å¤šè·³å’Œå¤šæ¨¡æ€äº’è”ç½‘ä»£ç†æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ã€‚å®ƒå¼ºè°ƒäº†åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¯„ä¼°ä»£ç†èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„è®°å¿†å¢å¼ºæ–¹æ³•ä¸ºæé«˜ä»£ç†æ€§èƒ½æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºæ›´å¤šçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚</td>
    </tr>
    <tr>
      <th>43</th>
      <td>An In-depth Survey of Large Language Model-based Artificial Intelligence Agents</td>
      <td>Due to the powerful capabilities demonstrated by large language model (LLM),<br>there has been a recent surge in efforts to integrate them with AI agents to<br>enhance their performance. In this paper, we have explored the core differences<br>and characteristics between LLM-based AI agents and traditional AI agents.<br>Specifically, we first compare the fundamental characteristics of these two<br>types of agents, clarifying the significant advantages of LLM-based agents in<br>handling natural language, knowledge storage, and reasoning capabilities.<br>Subsequently, we conducted an in-depth analysis of the key components of AI<br>agents, including planning, memory, and tool use. Particularly, for the crucial<br>component of memory, this paper introduced an innovative classification scheme,<br>not only departing from traditional classification methods but also providing a<br>fresh perspective on the design of an AI agent's memory system. We firmly<br>believe that in-depth research and understanding of these core components will<br>lay a solid foundation for the future advancement of AI agent technology. At<br>the end of the paper, we provide directional suggestions for further research<br>in this field, with the hope of offering valuable insights to scholars and<br>researchers in the field.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„AIæ™ºèƒ½ä½“ï¼šæ¢ç´¢ä¸å±•æœ›<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€çŸ¥è¯†å­˜å‚¨å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„å¼ºå¤§è¡¨ç°ï¼Œå°†LLMä¸AIæ™ºèƒ½ä½“ç›¸ç»“åˆä»¥æé«˜å…¶æ€§èƒ½çš„ç ”ç©¶æ—¥ç›Šå¢å¤šã€‚ç„¶è€Œï¼ŒLLM-based AIæ™ºèƒ½ä½“ä¸ä¼ ç»ŸAIæ™ºèƒ½ä½“ä¹‹é—´å­˜åœ¨ç€æ ¸å¿ƒå·®å¼‚å’Œç‰¹ç‚¹ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶å’Œç†è§£ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLLM-based AIæ™ºèƒ½ä½“ä¸ä¼ ç»ŸAIæ™ºèƒ½ä½“çš„æ¯”è¾ƒ<br>æœ¬æ–‡é¦–å…ˆæ¯”è¾ƒäº†è¿™ä¸¤ç§ç±»å‹æ™ºèƒ½ä½“çš„åŸºæœ¬ç‰¹æ€§ï¼Œé˜æ˜äº†LLM-based AIæ™ºèƒ½ä½“åœ¨å¤„ç†è‡ªç„¶è¯­è¨€ã€çŸ¥è¯†å­˜å‚¨å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šAIæ™ºèƒ½ä½“å…³é”®ç»„ä»¶çš„æ·±å…¥åˆ†æ<br>æœ¬æ–‡å¯¹AIæ™ºèƒ½ä½“çš„å…³é”®ç»„ä»¶è¿›è¡Œäº†æ·±å…¥åˆ†æï¼ŒåŒ…æ‹¬è§„åˆ’ã€è®°å¿†å’Œå·¥å…·ä½¿ç”¨ã€‚ç‰¹åˆ«æ˜¯å¯¹äºè®°å¿†è¿™ä¸€å…³é”®ç»„ä»¶ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„åˆ†ç±»æ–¹æ¡ˆï¼Œä¸ä»…ä¸ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•ä¸åŒï¼Œè¿˜ä¸ºAIæ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†æ–°çš„è§†è§’ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šAIæ™ºèƒ½ä½“çš„åº”ç”¨åœºæ™¯<br>æœ¬æ–‡æ¢è®¨äº†LLM-based AIæ™ºèƒ½ä½“çš„åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬èŠå¤©æœºå™¨äººã€æ¸¸æˆã€è®¾è®¡ã€ç ”ç©¶ã€ç¼–ç ã€åä½œå’Œé€šç”¨ç›®çš„ç­‰ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹4ï¼šAIæ™ºèƒ½ä½“çš„è¯„ä¼°åŸºå‡†<br>æœ¬æ–‡è¿˜ä»‹ç»äº†é’ˆå¯¹LLM-based AIæ™ºèƒ½ä½“è®¾è®¡çš„è¯„ä¼°åŸºå‡†ï¼Œä»¥è¯„ä¼°å…¶æ€§èƒ½å’Œæ•ˆæœã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¹¶æœªæä¾›å…·ä½“çš„å®éªŒç»“æœï¼Œè€Œæ˜¯å¯¹LLM-based AIæ™ºèƒ½ä½“çš„ç ”ç©¶ç°çŠ¶è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°å’Œåˆ†æã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡ä¸ºLLM-based AIæ™ºèƒ½ä½“çš„ç ”ç©¶æä¾›äº†å®è´µçš„å‚è€ƒå’Œå¯ç¤ºï¼Œæœ‰åŠ©äºè¯»è€…å¿«é€Ÿäº†è§£è¯¥é¢†åŸŸçš„ç ”ç©¶å†å²å’Œåº”ç”¨ç°çŠ¶ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘æ€§çš„å»ºè®®ã€‚</td>
    </tr>
    <tr>
      <th>44</th>
      <td>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models</td>
      <td>While language models (LMs) have shown potential across a range of<br>decision-making tasks, their reliance on simple acting processes limits their<br>broad deployment as autonomous agents. In this paper, we introduce Language<br>Agent Tree Search (LATS) -- the first general framework that synergizes the<br>capabilities of LMs in reasoning, acting, and planning. By leveraging the<br>in-context learning ability of LMs, we integrate Monte Carlo Tree Search into<br>LATS to enable LMs as agents, along with LM-powered value functions and<br>self-reflections for proficient exploration and enhanced decision-making. A key<br>feature of our approach is the incorporation of an environment for external<br>feedback, which offers a more deliberate and adaptive problem-solving mechanism<br>that surpasses the constraints of existing techniques. Our experimental<br>evaluation across diverse domains, including programming, interactive<br>question-answering (QA), web navigation, and math, validates the effectiveness<br>and generality of LATS in decision-making while maintaining competitive or<br>improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1<br>accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates<br>gradient-free performance (average score of 75.9) comparable to gradient-based<br>fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at<br>https://github.com/lapisrocks/LanguageAgentTreeSearch</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | è¯­è¨€æ¨¡å‹å†³ç­–æ ‘æœç´¢ï¼šç»Ÿä¸€æ¨ç†ã€è¡ŒåŠ¨å’Œè§„åˆ’<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨å†³ç­–ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶ç®€å•çš„è¡ŒåŠ¨è¿‡ç¨‹é™åˆ¶äº†å…¶ä½œä¸ºè‡ªä¸»ä»£ç†çš„å¹¿æ³›åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†è¯­è¨€æ¨¡å‹å†³ç­–æ ‘æœç´¢ï¼ˆLATSï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†LMsåœ¨æ¨ç†ã€è¡ŒåŠ¨å’Œè§„åˆ’æ–¹é¢çš„èƒ½åŠ›ç›¸ç»“åˆçš„é€šç”¨æ¡†æ¶ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šLATSåˆ©ç”¨LMsçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰é›†æˆåˆ°LATSä¸­ï¼Œä½¿LMsèƒ½å¤Ÿä½œä¸ºä»£ç†ï¼Œå¹¶ä½¿ç”¨LMé©±åŠ¨çš„ä»·å€¼å‡½æ•°å’Œè‡ªæˆ‘åæ€æ¥è¿›è¡Œç†Ÿç»ƒçš„æ¢ç´¢å’Œå¢å¼ºçš„å†³ç­–ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šLATSçš„å…³é”®ç‰¹å¾æ˜¯é›†æˆäº†å¤–éƒ¨åé¦ˆçš„ç¯å¢ƒï¼Œè¿™æä¾›äº†ä¸€ä¸ªæ›´æ·±æ€ç†Ÿè™‘å’Œé€‚åº”æ€§æ›´å¼ºçš„è§£å†³é—®é¢˜çš„æœºåˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯çš„é™åˆ¶ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨ç¼–ç¨‹ã€äº¤äº’å¼é—®ç­”ï¼ˆQAï¼‰ã€ç½‘ç»œå¯¼èˆªå’Œæ•°å­¦ç­‰ä¸åŒé¢†åŸŸçš„å®éªŒè¯„ä¼°ä¸­ï¼ŒéªŒè¯äº†LATSåœ¨å†³ç­–ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰ç«äº‰åŠ›çš„æˆ–æ”¹è¿›çš„æ¨ç†æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLATSåœ¨HumanEvalä¸Šä½¿ç”¨GPT-4å®ç°äº†æœ€å…ˆè¿›çš„pass@1å‡†ç¡®ç‡ï¼ˆ92.7%ï¼‰ï¼Œå¹¶åœ¨WebShopä¸Šä½¿ç”¨GPT-3.5å®ç°äº†ä¸åŸºäºæ¢¯åº¦çš„å¾®è°ƒç›¸å½“çš„æ¢¯åº¦æ— å…³æ€§èƒ½ï¼ˆå¹³å‡åˆ†æ•°ä¸º75.9%ï¼‰ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>LATSæ¡†æ¶ä¸ºLMsåœ¨å†³ç­–å’Œæ¨ç†æ–¹é¢çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼Œå…¶ç»“åˆäº†MCTSã€å¤–éƒ¨åé¦ˆå’Œè‡ªæˆ‘åæ€ï¼Œä¸ºLMsä½œä¸ºé€šç”¨ä»£ç†çš„åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</td>
    </tr>
    <tr>
      <th>45</th>
      <td>WebArena: A Realistic Web Environment for Building Autonomous Agents</td>
      <td>With advances in generative AI, there is now potential for autonomous agents<br>to manage daily tasks via natural language commands. However, current agents<br>are primarily created and tested in simplified synthetic environments, leading<br>to a disconnect with real-world scenarios. In this paper, we build an<br>environment for language-guided agents that is highly realistic and<br>reproducible. Specifically, we focus on agents that perform tasks on the web,<br>and create an environment with fully functional websites from four common<br>domains: e-commerce, social forum discussions, collaborative software<br>development, and content management. Our environment is enriched with tools<br>(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage<br>human-like task-solving. Building upon our environment, we release a set of<br>benchmark tasks focusing on evaluating the functional correctness of task<br>completions. The tasks in our benchmark are diverse, long-horizon, and designed<br>to emulate tasks that humans routinely perform on the internet. We experiment<br>with several baseline agents, integrating recent techniques such as reasoning<br>before acting. The results demonstrate that solving complex tasks is<br>challenging: our best GPT-4-based agent only achieves an end-to-end task<br>success rate of 14.41%, significantly lower than the human performance of<br>78.24%. These results highlight the need for further development of robust<br>agents, that current state-of-the-art large language models are far from<br>perfect performance in these real-life tasks, and that WebArena can be used to<br>measure such progress.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | WebArenaï¼šæ„å»ºè‡ªä¸»ä»£ç†çš„çœŸå®ç½‘ç»œç¯å¢ƒ<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€ç”Ÿæˆå¼AIçš„è¿›æ­¥ï¼Œè‡ªä¸»ä»£ç†ç°åœ¨å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤æ¥ç®¡ç†æ—¥å¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„ä»£ç†ä¸»è¦æ˜¯åœ¨ç®€åŒ–çš„åˆæˆç¯å¢ƒä¸­åˆ›å»ºå’Œæµ‹è¯•çš„ï¼Œè¿™å¯¼è‡´äº†ä¸ç°å®ä¸–ç•Œåœºæ™¯çš„è„±èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†WebArenaï¼Œä¸€ä¸ªé«˜åº¦çœŸå®å’Œå¯å¤åˆ¶çš„ç½‘ç»œç¯å¢ƒï¼Œç”¨äºæ„å»ºå’Œæµ‹è¯•è‡ªä¸»ä»£ç†ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šé«˜åº¦çœŸå®çš„ç½‘ç»œç¯å¢ƒ<br>WebArenaåŒ…å«å››ä¸ªå®Œå…¨åŠŸèƒ½çš„ã€è‡ªæ‰˜ç®¡çš„ç½‘ç»œåº”ç”¨ç¨‹åºï¼Œæ¯ä¸ªåº”ç”¨ç¨‹åºä»£è¡¨ä¸€ä¸ªåœ¨äº’è”ç½‘ä¸Šæ™®éå­˜åœ¨çš„ä¸åŒé¢†åŸŸï¼šç”µå­å•†åŠ¡ã€ç¤¾äº¤è®ºå›è®¨è®ºã€åä½œè½¯ä»¶å¼€å‘å’Œå†…å®¹ç®¡ç†ã€‚æ­¤å¤–ï¼ŒWebArenaè¿˜é›†æˆäº†å¤šç§å®ç”¨å·¥å…·ï¼ˆä¾‹å¦‚åœ°å›¾ï¼‰å’Œå¤–éƒ¨çŸ¥è¯†åº“ï¼ˆä¾‹å¦‚ç”¨æˆ·æ‰‹å†Œï¼‰ï¼Œä»¥é¼“åŠ±ç±»ä¼¼äººç±»çš„ä»»åŠ¡è§£å†³ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šæ ·åŒ–çš„åŸºå‡†ä»»åŠ¡<br>åŸºäºWebArenaç¯å¢ƒï¼Œæœ¬æ–‡å‘å¸ƒäº†ä¸€ç³»åˆ—åŸºå‡†ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨è¯„ä¼°ä»»åŠ¡å®Œæˆçš„å‡½æ•°æ­£ç¡®æ€§ã€‚è¿™äº›ä»»åŠ¡å…·æœ‰å¤šæ ·æ€§ã€é•¿æ—¶ç¨‹ï¼Œå¹¶æ—¨åœ¨æ¨¡æ‹Ÿäººç±»åœ¨äº’è”ç½‘ä¸Šå¸¸è§„æ‰§è¡Œçš„ä»»åŠ¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹3ï¼šåŸºäºç»“æœçš„è¯„ä¼°<br>æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºç»“æœçš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡ç¨‹åºåŒ–éªŒè¯æ¯ä¸ªä»»åŠ¡çš„æˆåŠŸæ¥è¯„ä¼°ä»»åŠ¡çš„æˆåŠŸã€‚è¿™ç§æ–¹æ³•æ¯”æ¯”è¾ƒé¢„æµ‹çš„åŠ¨ä½œåºåˆ—ä¸å‚è€ƒåŠ¨ä½œåºåˆ—çš„æ–‡æœ¬è¡¨é¢å½¢å¼æ›´å¯é ï¼Œå¹¶ä¸”å¯ä»¥å®¹çº³å®ç°ç›¸åŒç›®æ ‡çš„æ½œåœ¨æœ‰æ•ˆè·¯å¾„ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡ä½¿ç”¨WebArenaåŸºå‡†æµ‹è¯•äº†å‡ ä¸ªåŸºçº¿ä»£ç†ï¼Œè¿™äº›ä»£ç†é›†æˆäº†è¯¸å¦‚æ¨ç†å‰è¡ŒåŠ¨ç­‰æœ€æ–°æŠ€æœ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè§£å†³å¤æ‚ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼šæœ€å¥½çš„åŸºäºGPT-4çš„ä»£ç†çš„ç«¯åˆ°ç«¯ä»»åŠ¡æˆåŠŸç‡ä»…ä¸º14.41%ï¼Œè¿œä½äºäººç±»çš„78.24%ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>WebArenaä¸ºæ„å»ºå’Œæµ‹è¯•è‡ªä¸»ä»£ç†æä¾›äº†ä¸€ä¸ªé«˜åº¦çœŸå®å’Œå¯å¤åˆ¶çš„ç½‘ç»œç¯å¢ƒã€‚å®ƒå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å¼€å‘æ›´å¥å£®å’Œæœ‰æ•ˆçš„ä»£ç†ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWebArenaè¿˜å¯ä»¥ç”¨äºæµ‹è¯•å’Œæ”¹è¿›ç°æœ‰çš„äº¤äº’å¼å†³ç­–ä»£ç†æ–¹æ³•ï¼Œä¾‹å¦‚åˆ†å±‚è§„åˆ’ã€çŠ¶æ€è·Ÿè¸ªå’Œé”™è¯¯æ¢å¤ã€‚</td>
    </tr>
    <tr>
      <th>46</th>
      <td>MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation</td>
      <td>Existing Multimodal Large Language Model (MLLM)-based agents face significant<br>challenges in handling complex GUI (Graphical User Interface) interactions on<br>devices. These challenges arise from the dynamic and structured nature of GUI<br>environments, which integrate text, images, and spatial relationships, as well<br>as the variability in action spaces across different pages and tasks. To<br>address these limitations, we propose MobA, a novel MLLM-based mobile assistant<br>system. MobA introduces an adaptive planning module that incorporates a<br>reflection mechanism for error recovery and dynamically adjusts plans to align<br>with the real environment contexts and action module's execution capacity.<br>Additionally, a multifaceted memory module provides comprehensive memory<br>support to enhance adaptability and efficiency. We also present MobBench, a<br>dataset designed for complex mobile interactions. Experimental results on<br>MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI<br>environments and perform complex mobile task.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | MobAï¼šå¤šé¢è®°å¿†å¢å¼ºè‡ªé€‚åº”è§„åˆ’ï¼ŒåŠ©åŠ›é«˜æ•ˆç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨å¤„ç†å¤æ‚GUIäº¤äº’å’Œæ»¡è¶³å¤šæ ·åŒ–ç”¨æˆ·éœ€æ±‚æ–¹é¢é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ã€‚GUIç¯å¢ƒå…·æœ‰åŠ¨æ€æ€§å’Œç»“æ„æ€§ï¼Œèåˆäº†æ–‡æœ¬ã€å›¾åƒå’Œç©ºé—´å…³ç³»ï¼Œä¸”ä¸åŒé¡µé¢å’Œä»»åŠ¡çš„åŠ¨ä½œç©ºé—´å­˜åœ¨å·®å¼‚ã€‚ç°æœ‰çš„MLLM-basedæ™ºèƒ½ä½“åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶ï¼Œå¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè‡ªé€‚åº”è§„åˆ’æ¨¡å—<br>MOBAå¼•å…¥äº†è‡ªé€‚åº”è§„åˆ’æ¨¡å—ï¼Œè¯¥æ¨¡å—åŒ…å«ä¸€ä¸ªåæ€æœºåˆ¶ï¼Œç”¨äºä»å¤±è´¥çš„å­è®¡åˆ’ä¸­æ¢å¤ä»»åŠ¡æ‰§è¡Œï¼Œå¹¶é€šè¿‡é‡æ–°è¯„ä¼°ç›®æ ‡æˆ–å°†ä»»åŠ¡åˆ†è§£ä¸ºæ›´ç»†ç²’åº¦çš„å­ç›®æ ‡æ¥åŠ¨æ€è°ƒæ•´è®¡åˆ’ã€‚è¿™ç§æ¨¡å—èƒ½å¤Ÿæ ¹æ®å½“å‰çš„GUIç¯å¢ƒå’ŒåŠ¨ä½œæ‰§è¡Œå™¨çš„å®¹é‡ï¼Œç”Ÿæˆå¤šç²’åº¦çš„ä»»åŠ¡è®¡åˆ’ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç¯å¢ƒå¹¶å®Œæˆä»»åŠ¡ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šå¤šé¢è®°å¿†æ¨¡å—<br>MOBAè¿˜æå‡ºäº†ä¸€ä¸ªå¤šé¢è®°å¿†æ¨¡å—ï¼Œè¯¥æ¨¡å—æä¾›åˆ†å±‚è®°å¿†æ”¯æŒï¼Œä»¥å¢å¼ºä»»åŠ¡çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚è¯¥æ¨¡å—åŒ…æ‹¬ä»»åŠ¡è®°å¿†ã€åº”ç”¨è®°å¿†ã€é¡µé¢è®°å¿†ã€åŠ¨ä½œè®°å¿†å’Œç”¨æˆ·è®°å¿†ï¼Œèƒ½å¤Ÿå­˜å‚¨å†å²æ•°æ®ï¼Œå¢å¼ºå†³ç­–èƒ½åŠ›ï¼Œå¹¶å‡å°‘å†—ä½™åŠ¨ä½œã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>MOBAåœ¨MOBBENCHå’ŒAndroidArenaæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼ŒMOBAåœ¨å¤„ç†åŠ¨æ€GUIç¯å¢ƒå’Œæ‰§è¡Œå¤æ‚ç§»åŠ¨ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMOBAåœ¨ä»»åŠ¡å®Œæˆç‡ã€é‡Œç¨‹ç¢‘å¾—åˆ†å’Œæ‰§è¡Œæ•ˆç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>MOBAçš„è®¾è®¡ç†å¿µå’Œæ–¹æ³•ä¸ºç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ã€‚å…¶è‡ªé€‚åº”è§„åˆ’å’Œå¤šé¢è®°å¿†æ¨¡å—çš„è®¾è®¡ï¼Œä¸ºè§£å†³å¤æ‚GUIäº¤äº’å’Œå¤šæ ·åŒ–ç”¨æˆ·éœ€æ±‚æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼ŒMOBAçš„å®éªŒç»“æœä¹Ÿè¡¨æ˜ï¼ŒMLLMåœ¨ç§»åŠ¨ä»»åŠ¡è‡ªåŠ¨åŒ–é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</td>
    </tr>
    <tr>
      <th>47</th>
      <td>VGA: Vision GUI Assistant -- Minimizing Hallucinations through Image-Centric Fine-Tuning</td>
      <td>Recent advances in Large Vision-Language Models (LVLMs) have significantly<br>improve performance in image comprehension tasks, such as formatted charts and<br>rich-content images. Yet, Graphical User Interface (GUI) pose a greater<br>challenge due to their structured format and detailed textual information.<br>Existing LVLMs often overly depend on internal knowledge and neglect image<br>content, resulting in hallucinations and incorrect responses in GUI<br>comprehension. To address these issues, we introduce VGA, a fine-tuned model<br>designed for comprehensive GUI understanding. Our model aims to enhance the<br>interpretation of visual data of GUI and reduce hallucinations. We first<br>construct a Vision Question Answering (VQA) dataset of 63.8k high-quality<br>examples with our propose Referent Method, which ensures the model's responses<br>are highly depend on visual content within the image. We then design a<br>two-stage fine-tuning method called Foundation and Advanced Comprehension (FAC)<br>to enhance both the model's ability to extract information from image content<br>and alignment with human intent. Experiments show that our approach enhances<br>the model's ability to extract information from images and achieves<br>state-of-the-art results in GUI understanding tasks. Our dataset and<br>fine-tuning script will be released soon.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | VGAï¼šåŸºäºå›¾åƒçš„å¾®è°ƒï¼Œå‡å°‘è§†è§‰ç•Œé¢ç†è§£ä¸­çš„å¹»è§‰<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å›¾åƒç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ç†è§£å´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„LVLMså¾€å¾€è¿‡åº¦ä¾èµ–å†…éƒ¨çŸ¥è¯†ï¼Œè€Œå¿½è§†å›¾åƒå†…å®¹ï¼Œå¯¼è‡´åœ¨GUIç†è§£ä»»åŠ¡ä¸­äº§ç”Ÿå¹»è§‰å’Œé”™è¯¯å“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VGAï¼Œä¸€ä¸ªä¸“é—¨ä¸ºGUIç†è§£è€Œè®¾è®¡çš„å¾®è°ƒæ¨¡å‹ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæ„å»ºå¤§è§„æ¨¡GUIæ•°æ®é›†<br>ä¸ºäº†å¾®è°ƒæ¨¡å‹ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåŒ…å«63.8ké«˜è´¨é‡ç¤ºä¾‹çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§ç§°ä¸ºâ€œå‚ç…§æ–¹æ³•â€çš„æ•°æ®æ„å»ºæ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹çš„å“åº”é«˜åº¦ä¾èµ–äºå›¾åƒä¸­çš„è§†è§‰å†…å®¹ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•<br>æœ¬æ–‡è®¾è®¡äº†ä¸€ç§åä¸ºâ€œåŸºç¡€å’Œé«˜çº§ç†è§£â€ï¼ˆFACï¼‰çš„ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼Œä»¥å¢å¼ºæ¨¡å‹ä»å›¾åƒå†…å®¹ä¸­æå–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ä½¿å…¶ä¸äººç±»æ„å›¾ä¿æŒä¸€è‡´ã€‚åŸºç¡€é˜¶æ®µå¢å¼ºæ¨¡å‹å¯¹GUIå›¾åƒçš„ç†è§£ï¼Œè€Œé«˜çº§é˜¶æ®µåˆ™æé«˜æ¨¡å‹æ ¹æ®å¯¹GUIçš„ç†è§£æ¥å›ç­”å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹ä»å›¾åƒä¸­æå–ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶åœ¨GUIç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒVGAåœ¨GUIç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥æ—¶ä»ç„¶è¡¨ç°å‡ºè‰²ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„VGAæ¨¡å‹åŠå…¶å¾®è°ƒæ–¹æ³•ä¸ºLVLMsåœ¨GUIç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡GUIæ•°æ®é›†å’Œè®¾è®¡ä¸¤é˜¶æ®µå¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘LVLMsåœ¨GUIç†è§£ä¸­çš„å¹»è§‰ç°è±¡ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚</td>
    </tr>
    <tr>
      <th>48</th>
      <td>AppAgentX: Evolving GUI Agents as Proficient Smartphone Users</td>
      <td>Recent advancements in Large Language Models (LLMs) have led to the<br>development of intelligent LLM-based agents capable of interacting with<br>graphical user interfaces (GUIs). These agents demonstrate strong reasoning and<br>adaptability, enabling them to perform complex tasks that traditionally<br>required predefined rules. However, the reliance on step-by-step reasoning in<br>LLM-based agents often results in inefficiencies, particularly for routine<br>tasks. In contrast, traditional rule-based systems excel in efficiency but lack<br>the intelligence and flexibility to adapt to novel scenarios. To address this<br>challenge, we propose a novel evolutionary framework for GUI agents that<br>enhances operational efficiency while retaining intelligence and flexibility.<br>Our approach incorporates a memory mechanism that records the agent's task<br>execution history. By analyzing this history, the agent identifies repetitive<br>action sequences and evolves high-level actions that act as shortcuts,<br>replacing these low-level operations and improving efficiency. This allows the<br>agent to focus on tasks requiring more complex reasoning, while simplifying<br>routine actions. Experimental results on multiple benchmark tasks demonstrate<br>that our approach significantly outperforms existing methods in both efficiency<br>and accuracy. The code will be open-sourced to support further research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | AppAgentXï¼šè®©GUIæ™ºèƒ½ä½“æˆä¸ºç†Ÿç»ƒçš„æ‰‹æœºç”¨æˆ·<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥å‚¬ç”Ÿäº†èƒ½å¤Ÿä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIsï¼‰äº¤äº’çš„æ™ºèƒ½LLMæ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ‰§è¡Œä¼ ç»Ÿä¸Šéœ€è¦é¢„å®šä¹‰è§„åˆ™æ‰èƒ½å®Œæˆçš„å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒLLMæ™ºèƒ½ä½“ä¾èµ–äºé€æ­¥æ¨ç†ï¼Œè¿™å¾€å¾€å¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨ä¾‹è¡Œä»»åŠ¡ä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„ç³»ç»Ÿåœ¨æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æ™ºèƒ½å’Œçµæ´»æ€§ï¼Œæ— æ³•é€‚åº”æ–°æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿›åŒ–æ¡†æ¶ï¼Œç”¨äºGUIæ™ºèƒ½ä½“ï¼Œä»¥æé«˜æ“ä½œæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ™ºèƒ½å’Œçµæ´»æ€§ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šè®°å¿†æœºåˆ¶<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§è®°å¿†æœºåˆ¶ï¼Œç”¨äºè®°å½•æ™ºèƒ½ä½“çš„ä»»åŠ¡æ‰§è¡Œå†å²ã€‚é€šè¿‡åˆ†æå†å²è®°å½•ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿè¯†åˆ«é‡å¤çš„åŠ¨ä½œåºåˆ—ï¼Œå¹¶è¿›åŒ–å‡ºé«˜çº§åŠ¨ä½œï¼Œä½œä¸ºå¿«æ·æ–¹å¼ï¼Œæ›¿æ¢ä½çº§æ“ä½œï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚è¿™å…è®¸æ™ºèƒ½ä½“ä¸“æ³¨äºéœ€è¦æ›´å¤æ‚æ¨ç†çš„ä»»åŠ¡ï¼ŒåŒæ—¶ç®€åŒ–ä¾‹è¡Œæ“ä½œã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šè¿›åŒ–æœºåˆ¶<br>æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªåŸºäºé“¾çš„çŸ¥è¯†æ¡†æ¶ï¼Œç”¨äºè®°å½•å’Œä¼˜åŒ–æ™ºèƒ½ä½“çš„æ‰§è¡Œè¡Œä¸ºã€‚è¯¥æ¡†æ¶å…è®¸æ™ºèƒ½ä½“ä»å…¶è¿‡å»çš„äº¤äº’ä¸­å­¦ä¹ ï¼Œå¹¶åŠ¨æ€åœ°è¿›åŒ–å‡ºæ›´æŠ½è±¡çš„é«˜çº§åŠ¨ä½œï¼Œæ¶ˆé™¤äº†é‡å¤çš„ä½çº§æ“ä½œçš„éœ€æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæ™ºèƒ½ä½“ä¼šåˆ†æå…¶æ‰§è¡Œå†å²ï¼Œä»¥è¯†åˆ«é‡å¤çš„ä½æ™ºèƒ½åŠ¨ä½œï¼Œä¾‹å¦‚ä¾‹è¡Œä»»åŠ¡ä¸­æ¶‰åŠçš„åŠ¨ä½œã€‚ä»è¿™ç§åˆ†æä¸­ï¼Œæ™ºèƒ½ä½“å¯ä»¥ç”Ÿæˆä¸€ä¸ªé«˜çº§åŠ¨ä½œï¼Œå°è£…ä¸€ç³»åˆ—ä½çº§åŠ¨ä½œï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>åœ¨å¤šä¸ªåŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•å°†å¹³å‡æ­¥éª¤æ•°ä»9.1å‡å°‘åˆ°5.7ï¼Œå°†æ­¥éª¤æ‰§è¡Œæ—¶é—´ä»23ç§’å‡å°‘åˆ°16ç§’ï¼Œå¹¶å°†å¹³å‡ä»¤ç‰Œæ¶ˆè€—ä»9.26kå‡å°‘åˆ°4.94kã€‚æ­¤å¤–ï¼Œå¹³å‡æˆåŠŸç‡ä»70.8%æé«˜åˆ°71.4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ™ºèƒ½ä½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å¼€é”€ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡æå‡ºçš„è¿›åŒ–æ¡†æ¶ä¸ºGUIæ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚é€šè¿‡å¼•å…¥è®°å¿†æœºåˆ¶å’Œè¿›åŒ–æœºåˆ¶ï¼Œæ™ºèƒ½ä½“èƒ½å¤Ÿä»è¿‡å»çš„äº¤äº’ä¸­å­¦ä¹ ï¼Œå¹¶åŠ¨æ€åœ°è¿›åŒ–å‡ºæ›´æŠ½è±¡çš„é«˜çº§åŠ¨ä½œï¼Œä»è€Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„åŸºäºé“¾çš„çŸ¥è¯†æ¡†æ¶ä¹Ÿä¸ºæ™ºèƒ½ä½“çš„è¡Œä¸ºä¼˜åŒ–æä¾›äº†æ–°çš„æ–¹æ³•ã€‚è¿™äº›åˆ›æ–°ç‚¹å¯¹äºGUIæ™ºèƒ½ä½“çš„å‘å±•å…·æœ‰é‡è¦çš„æ„ä¹‰ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</td>
    </tr>
    <tr>
      <th>49</th>
      <td>Towards Trustworthy GUI Agents: A Survey</td>
      <td>GUI agents, powered by large foundation models, can interact with digital<br>interfaces, enabling various applications in web automation, mobile navigation,<br>and software testing. However, their increasing autonomy has raised critical<br>concerns about their security, privacy, and safety. This survey examines the<br>trustworthiness of GUI agents in five critical dimensions: security<br>vulnerabilities, reliability in dynamic environments, transparency and<br>explainability, ethical considerations, and evaluation methodologies. We also<br>identify major challenges such as vulnerability to adversarial attacks,<br>cascading failure modes in sequential decision-making, and a lack of realistic<br>evaluation benchmarks. These issues not only hinder real-world deployment but<br>also call for comprehensive mitigation strategies beyond task success. As GUI<br>agents become more widespread, establishing robust safety standards and<br>responsible development practices is essential. This survey provides a<br>foundation for advancing trustworthy GUI agents through systematic<br>understanding and future research.</td>
      <td>## ğŸŒŸ è®ºæ–‡è§£è¯» | æ„å»ºå¯ä¿¡çš„GUIæ™ºèƒ½ä½“ï¼šä¸€é¡¹è°ƒæŸ¥<br><br>## ğŸ“Œ èƒŒæ™¯ç—›ç‚¹/æœ¬æ–‡åŠ¨æœº<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ™ºèƒ½ä½“å·²ç»ä»ç®€å•çš„é—®ç­”å·¥å…·è½¬å˜ä¸ºèƒ½å¤Ÿä¸æ•°å­—ç•Œé¢äº¤äº’çš„æ™ºèƒ½ä½“ã€‚è¿™äº›æ™ºèƒ½ä½“åœ¨ç½‘é¡µè‡ªåŠ¨åŒ–ã€ç§»åŠ¨å¯¼èˆªå’Œè½¯ä»¶æµ‹è¯•ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œéšç€å®ƒä»¬è‡ªä¸»æ€§çš„æé«˜ï¼Œå…³äºå…¶å®‰å…¨æ€§ã€éšç§å’Œå®‰å…¨çš„æ‹…å¿§ä¹Ÿæ—¥ç›Šå¢åŠ ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦ï¼Œå¹¶ä»äº”ä¸ªå…³é”®ç»´åº¦è¿›è¡Œåˆ†æï¼šå®‰å…¨æ¼æ´ã€åŠ¨æ€ç¯å¢ƒä¸­çš„å¯é æ€§ã€é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€ä¼¦ç†è€ƒè™‘å’Œè¯„ä¼°æ–¹æ³•ã€‚<br><br>## ğŸš€ æ ¸å¿ƒæ–¹æ³•<br>æœ¬æ–‡çš„æ ¸å¿ƒæ–¹æ³•æ˜¯å¯¹GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦è¿›è¡Œç³»ç»Ÿæ€§çš„è°ƒæŸ¥å’Œåˆ†æã€‚ä½œè€…ä»äº”ä¸ªå…³é”®ç»´åº¦å¯¹GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹1ï¼šæœ¬æ–‡é¦–æ¬¡å°†GUIæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦é—®é¢˜è¿›è¡Œäº†ç³»ç»Ÿæ€§çš„è°ƒæŸ¥å’Œåˆ†æï¼Œä»äº”ä¸ªå…³é”®ç»´åº¦è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„GUIæ™ºèƒ½ä½“æä¾›äº†ç†è®ºåŸºç¡€ã€‚<br><br>ğŸ’¡ åˆ›æ–°ç‚¹2ï¼šæœ¬æ–‡æå‡ºäº†å¤šç§è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æ›´æ™ºèƒ½çš„é˜²å¾¡å·¥å…·ã€ç”¨æˆ·æ§åˆ¶çš„éšç§ã€è¿æ¥çš„é˜²å¾¡å±‚ã€å®æ—¶å¹»è§‰é¢„é˜²ã€è‡ªé€‚åº”å®‰å…¨æ¶æ„ã€ä»å¤±è´¥ä¸­å­¦ä¹ ã€äº¤äº’å¼è§£é‡Šå·¥å…·ã€ä¸Šä¸‹æ–‡è‡ªé€‚åº”è§£é‡Šã€æ–‡åŒ–å’Œç¤¾ä¼šæ„è¯†ã€æ”¿ç­–å½±å“ã€æŒ‡å—å’ŒåŸåˆ™ç­‰ã€‚<br><br>## ğŸ“ˆ å®éªŒç»“æœ<br>æœ¬æ–‡å¹¶æœªè¿›è¡Œå…·ä½“çš„å®éªŒï¼Œè€Œæ˜¯å¯¹ç°æœ‰çš„ç ”ç©¶æˆæœè¿›è¡Œäº†æ€»ç»“å’Œåˆ†æã€‚ä½œè€…é€šè¿‡è°ƒæŸ¥å’Œåˆ†æï¼Œæ­ç¤ºäº†GUIæ™ºèƒ½ä½“åœ¨å¯ä¿¡åº¦æ–¹é¢å­˜åœ¨çš„æŒ‘æˆ˜å’Œé—®é¢˜ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚<br><br>## ğŸ’¬ å¯å€Ÿé‰´ä¹‹å¤„<br>æœ¬æ–‡çš„ç ”ç©¶æˆæœå¯¹äºæ„å»ºå¯ä¿¡çš„GUIæ™ºèƒ½ä½“å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼ã€‚ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥å€Ÿé‰´æœ¬æ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆå’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œè®¾è®¡æ›´å®‰å…¨ã€å¯é ã€é€æ˜å’Œç¬¦åˆä¼¦ç†çš„GUIæ™ºèƒ½ä½“ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿä¸ºGUIæ™ºèƒ½ä½“çš„è¯„ä¼°å’Œæµ‹è¯•æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨åŠ¨GUIæ™ºèƒ½ä½“é¢†åŸŸçš„å‘å±•ã€‚</td>
    </tr>
  </tbody>
</table>
                </div>
            </body>
            </html>
        