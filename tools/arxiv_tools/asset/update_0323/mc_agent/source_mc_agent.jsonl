{"title":"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks","authors":"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie","summary":"Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.","url":"http:\/\/arxiv.org\/abs\/2408.03615v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2408.03615v2","published":1723018592000,"comment":"Accepted by NeurIPS 2024","pdf_text":"Optimus-1\n: Hybrid Multimodal Memory\nEmpowered Agents Excel in Long-Horizon Tasks\nZaijing Li1 2, Yuquan Xie1, Rui Shao1∗, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1∗\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016,rshaojimmy,nieliqiang}@gmail.com\nExperience-Driven Reflector\n \nAction Controller\n \nTask: Craft stone sword.\nHybrid Multimodal \nMemory\nHierarchical Directed \nKnowledge Graph\nAbstracted Multimodal \nExperience Pool\n                              \nKnowledge-Guided Planner\nMission Success!!!\nTask: Craft stone pickaxe.\nEnv: forest\nVisual Info: health: full...\nObs:\nRetrieve: Memory|Knowledge\nRetrieve: Memory|Experience\nReflection: Mine stone.\nGoal: Mine stone.\nFigure 1: An illustration of Optimus-1 performing long-horizon tasks in Minecraft. Given the task\n“Craft stone sword”, Knowledge-Guided Planner incorporates knowledge from Hierarchical Directed\nKnowledge Graph into planning, then Action Controller executes these planning sequences step-by-\nstep. During the execution of the task, the Experience-Driven Reflector is periodically activated and\nretrieve experience from Abstracted Multimodal Experience Pool to make reflection.\nAbstract\nBuilding a general-purpose agent is a long-standing vision in the field of artificial\nintelligence. Existing agents have made remarkable progress in many domains, yet\nthey still struggle to complete long-horizon tasks in an open world. We attribute\nthis to the lack of necessary world knowledge and multimodal experience that can\nguide agents through a variety of long-horizon tasks. In this paper, we propose\na Hybrid Multimodal Memory module to address the above challenges. It 1)\ntransforms knowledge into Hierarchical Directed Knowledge Graph that allows\nagents to explicitly represent and learn world knowledge, and 2) summarises histor-\nical information into Abstracted Multimodal Experience Pool that provide agents\n*Corresponding authors\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2408.03615v2  [cs.AI]  21 Oct 2024\nwith rich references for in-context learning. On top of the Hybrid Multimodal\nMemory module, a multimodal agent, Optimus-1, is constructed with dedicated\nKnowledge-guided Planner and Experience-Driven Reflector, contributing to a\nbetter planning and reflection in the face of long-horizon tasks in Minecraft. Exten-\nsive experimental results show that Optimus-1 significantly outperforms all existing\nagents on challenging long-horizon task benchmarks, and exhibits near human-\nlevel performance on many tasks. In addition, we introduce various Multimodal\nLarge Language Models (MLLMs) as the backbone of Optimus-1. Experimental\nresults show that Optimus-1 exhibits strong generalization with the help of the Hy-\nbrid Multimodal Memory module, outperforming the GPT-4V baseline on various\ntasks. Please see the project page at https:\/\/cybertronagent.github.io\/Optimus-\n1.github.io\/.\n1\nIntroduction\nOptimus Prime faces complex tasks alongside humans in Transformers to protect the peace of\nthe planet. Creating an agent [43, 13] like Optimus that can perceive, plan, reflect, and complete\nlong-horizon tasks in an open world has been a longstanding aspiration in the field of artificial\nintelligence [22, 35, 36]. Early research developed simple policy through reinforcement learning\n[7] or imitation learning [1, 25]. A lot of work [46, 49] have utilized Large Language Models\n(LLMs) as action planners for agents, generating executable sub-goal sequences for low-level action\ncontrollers. Further, recent studies [51, 32] employed Multimodal Large Language Models (MLLMs)\n[4, 38, 55] as planner and reflector. Leveraging the powerful instruction-following and logical\nreasoning capabilities of (Multimodal) LLMs [24], LLM-based agents have achieved remarkable\nsuccess across multiple domains [14, 9, 10, 54]. Nevertheless, the ability of these agents to complete\nlong-horizon tasks still falls significantly short of human-level performance.\nAccording to relevant studies [27, 41, 45], the human ability to complete long-horizon tasks in an\nopen world relies on long-term memory storage, which is divided into knowledge and experience. The\nstorage and utilization of knowledge and experience play a crucial role in guiding human behavior\nand enabling humans to adapt flexibly to their environments in order to accomplish long-horizon\ntasks. Inspired by this theory, we summarize the challenges faced by current agents as follows:\nInsufficient Exploration of Structured Knowledge: Structured knowledge, encompassing open\nworld rules, object relationships, and interaction methods with the environment, is essential for agents\nto complete complex tasks [33, 43]. However, MLLMs such as GPT-4V * lack sufficient knowledge\nin Minecraft. Existing agents [1, 25, 7] only learn dispersed knowledge from video data and are\nunable to efficiently represent and learn this structured knowledge, rendering them incapable of\nperforming complex tasks.\nLack of Multimodal Experience: Humans derive successful strategies and lessons from information\non historical experience [8, 31], which assists them in tackling current complex tasks. In a similar\nmanner, agents can benefit from in-context learning with experience demonstrations [42, 53]. How-\never, existing agents [46, 50, 32] only consider unimodal information, which prevents them from\nlearning from multimodal experience as humans do.\nTo address the aforementioned challenges, we propose Hybrid Multimodal Memory module\nthat consists of Hierarchical Directed Knowledge Graph (HDKG) and Abstracted Multimodal\nExperience Pool (AMEP). For HDKG, we map the logical relationships between objects into a\ndirected graph structure, thereby transforming knowledge into high-level semantic representations.\nHDKG efficiently provides the agent with the necessary knowledge for task execution, without\nrequiring any parameter updates. For AMEP, we dynamically summarize and store the multimodal\ninformation (e.g., environment, agent state, task plan, video frames, etc.) from the agent’s task\nexecution process, ensuring that historical information contains both a global overview and local\ndetails. Different from the method of directly storing successful cases as experience [51], AMEP\nconsiders both successful and failed cases as references. This innovative approach of incorporating\nfailure cases into in-context learning significantly enhances the performance of the agent.\n*https:\/\/openai.com\/index\/gpt-4v-system-card\/\n2\nOn top of the Hybrid Multimodal Memory module, we construct a multimodal composable agent,\nOptimus-1. As shown in Figure 1, Optimus-1 consists of Knowledge-Guided Planner, Experience-\nDriven Reflector, and Action Controller. To enhance the ability of agents to cope with complex\nenvironments and long-horizon tasks, Knowledge-Guided Planner incorporates visual observation\ninto the planning phase, leveraging HDKG to capture the knowledge needed. This allows the agent to\nefficiently transform tasks into executable sub-goals. Action Controller takes the sub-goal and the\ncurrent observation as inputs and generates low-level actions, interacting with the game environment\nto update the agent’s state. In open-world complex environments, agents are prone to be erroneous\nwhen performing long-horizon tasks. To address this, we propose Experience-Driven Reflector, which\nis periodically activated to retrieve relevant multimodal experiences from AMEP. This encourages\nthe agent to reflect on its current actions and refine the plan.\nWe validate the performance of Optimus-1 in Minecraft, a popular open-world game environment.\nExperimental results show that Optimus-1 exhibits remarkable performance on long-horizon tasks,\nrepresenting up to 30% improvement over existing agents. Moreover, we introduce various Multi-\nmodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 has a 2 to 6 times performance improvement with the help of Hybrid Multimodal\nMemory, outperforming powerful GPT-4V baseline on lots of long-horizon tasks. Additionally, we\nverified that the plug-and-play Hybrid Multimodal Memory can drive Optimus-1 to incrementally\nimprove its performance in a self-evolution manner. The extensive experimental results show that\nOptimus-1 makes a major step toward a general agent with a human-like level of performance. Main\ncontributions of our paper:\n• We propose Hybrid Multimodal Memory module which is composed of HDKG and AMEP.\nHDKG helps the agent make the planning of long-horizon tasks efficiently. AMEP provides refined\nhistorical experience and guides the agent to reason about the current situation state effectively.\n• On top of the Hybrid Multimodal Memory module, we construct Optimus-1, which consists of\nKnowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Optimus-1\noutperforms all baseline agents on long-horizon task benchmarks, and exhibits capabilities close to\nthe level of human players.\n• Driven by Hybrid Multimodal Memory, various MLLM-based Optimus-1 have demonstrated 2 to 6\ntimes performance improvement, demonstrating the generalization of Hybrid Multimodal Memory.\n2\nOptimus-1\nIn this section, we first elaborate on how to implement the Hybrid Multimodal Memory in Sec 2.1.\nAs a core innovation, it plays a crucial role in enabling Optimus-1 to execute long-horizon tasks.\nNext, we give an overview of Optimus-1 framework (Sec 2.2), which consists of Hybrid Multimodal\nMemory, Knowledge-Guided Planner, Experience-Driven Reflector, and Action Controller. Finally,\nwe introduce a non-parametric learning approach to expand the hybrid multimodal memory (Sec 2.3),\nthereby enhancing the success rate of task execution for Optimus-1.\n2.1\nHybrid Multimodal Memory\nIn order to endow agent with a long-term memory storage mechanism [27, 45], we propose the\nHybrid Multimodal Memory module, which consists of Abstracted Multimodal Experience Pool\n(AMEP) and Hierarchical Directed Knowledge Graph (HDKG).\n2.1.1\nAbstracted Multimodal Experience Pool\nRelevant studies [23, 28, 17, 15] highlight the importance of historical information for agents\ncompleting long-horizon tasks. Minedojo [7] and Voyager [46] employed unimodal storage of\nhistorical information. Jarvis-1 [51] used a multimodal experience mechanism that stores task\nplanning and visual information without summarization, posing challenges to storage capacity and\nretrieval speed. To address this issue, we propose AMEP, which aims to dynamically summarize all\nmultimodal information during task execution. It preserves the integrity of long-horizon data while\nenhancing storage and retrieval efficiency.\n3\nAbstracted Multimodal Experience Pool\nFilter\nHierarchical Directed Knowledge Graph\n(b)\nVideo \n...\nVideo Buffer\n...\n...\nImage Buffer\n0.3\n0.7\n0.6\n0.9\n0.8\nCraft stone \npickaxe\nO1\nO2\nO3\nO4\nupdate\nKnowledge\nvisual\ninfo\nTask: Craft stone pickaxe.\nEnv: forest\nPlan: \n(a)\nObs:                                                            ... \nVisual Info: health: full, food: full,hotbar: empty\nFigure 2: (a) Extraction process of multimodal experience. The frames are filtered through video\nbuffer and image buffer, then MineCLIP [7] is employed to compute the visual and sub-goal sim-\nilarities and finally they are stored in Abstracted Multimodal Experience Pool. (b) Overview of\nHierarchical Directed Knowledge Graph. Knowledge is stored as a directed graph, where its nodes\nrepresent objects, and directed edges point to materials that can be crafted by this object.\nSpecifically, as depicted in Figure 2, to conduct the static visual information abstraction, the video\nstream captured by Optimus-1 during task execution is first input to a video buffer, filtering the\nstream at a fixed frequency of 1 frame per second. Based on the filtered video frames, to further\nperform a dynamic visual information abstraction, these frames are then fed into an image buffer with\na window size of 16, where the image similarity is dynamically computed and final abstracted frames\nare adaptively updated. To align such abstracted visual information with the corresponding textual\nsub-goal, we then utilize MineCLIP [7], a pre-trained video-text alignment model, to calculate their\nmultimodal correlation. When this correlation exceeds a threshold, the corresponding image buffer\nand textual sub-goal are saved as multimodal experience into a pool. Finally, we further incorporate\nenvironment information, agent initial state, and plan generated by Knowledge-Guided Planner, into\nsuch a pool, which forms the AMEP. In this way, we consider the multimodal information of each\nsub-goal, and summarise it to finally compose the multimodal experience of the given task.\n2.1.2\nHierarchical Directed Knowledge Graph\nIn Minecraft, mining and crafting represent a complex knowledge network crucial for effective task\nplanning. For instance, crafting a diamond sword\nrequires two diamonds\nand one wooden stick\n, while mining diamonds requires an iron pickaxe\n, which involving further materials and steps.\nSuch knowledge is essential for an agent’s ability to perform long-horizon complex tasks. Instead of\nimplicit learning through fine-tuning [32, 58], we propose HDKG, which transforms knowledge into\na graph representation. It enables the agent to perform explicit learning by retrieving information\nfrom the knowledge graph.\nAs shown in the Figure 2, we transform knowledge into a graph D(V, E), where nodes set V represent\nobjects, and directed edges set E point to nodes that can be crafted by this object. An edge e ∈E in\nthe D can be represented as e = (u, v), where u, v ∈V. The directed graph efficiently stores and\nupdates knowledge. For a given object x, retrieving the corresponding node allows extraction of a\nsub-graph Dj(Vj, Ej) ∈D, where nodes set Vj and edges set Ej can be formulated as:\nVj = {v ∈V | x} ,\nEj = {e = (u, v) ∈V | u ∈Vj ∪v ∈Vj} ,\n(1)\nThen by topological sorting, we can get all the materials and their relationships needed to complete\nthe task. This knowledge is provided to the Knowledge-Guided Planner as a way to generate a more\n4\nHybrid Multimodal Memory\nAbstracted Multimodal Experience Pool\nHierarchical Directed Knowledge Graph\nEnvironment\n9\n27\n1\n7\n1\n1\n1\n1\n3\n11\n3\n1\nKnowledge-Guided \nPlanner\nSub-Goals\nExperience-Driven \nReflector\n    \n \n   \n   \n   \nReflection\n [replan]\n [continue]\n [complete]\nTask: Craft diamond.\nAction Controller\n...\nTask: Mine cobblestone.\nEnv: forest\nVisual Info: health: full, food: full,hotbar: empty.\nKnowledge：\nSub-Goal: Mine 11 cobblestone.\nCurrent Obs:                                            \nReplan: \nContinue: \nComplete: \nIn-context Cases\nFigure 3: Overview framework of our Optimus-1. Optimus-1 consists of Knowledge-Guided Planner,\nExperience-Driven Reflector, Action Controller, and Hybrid Multimodal Memory architecture. Given\nthe task “craft stone sword”, Optimus-1 incorporates the knowledge from HDKG into Knowledge-\nGuided Planning, then Action Controller generates low-level actions. Experience-Driven Reflector is\nperiodically activated to introduce multimodal experience from AMEP to determine if the current\ntask can be executed successfully. If not, it will ask the Knowledge-Guided Planner to refine the plan.\nreasonable sequence of sub-goals. With HDKG, we can significantly enhance the world knowledge\nof the agent in a train-free manner.\n2.2\nOptimus-1: Framework\nRelevant studies indicate that the human brain is essential for planning and reflection, while the\ncerebellum controls low-level actions, both crucial for complex tasks [39, 40]. Inspired by this, we\ndivide the structure of Optimus-1 into Knowledge-Guided Planner, Experience-Driven Reflector, and\nAction Controller. In a given game environment with a long-horizon task, the Knowledge-Guided\nPlanner senses the environment, retrieves knowledge from HDKG, and decomposes the task into\nexecutable sub-goals. The action controller then sequentially executes these sub-goals. During\nexecution, the Experience-Driven Reflector is activated periodically, leveraging historical experience\nfrom AMEP to assess whether Optimus-1 can complete the current sub-goal. If not, it instructs the\nKnowledge-Guided Planner to revise its plan. Through iterative interaction with the environment,\nOptimus-1 ultimately completes the task.\nKnowledge-Guided Planner. Open-world environments vary greatly, affecting task execution.\nPrevious approaches [50] using LLMs for task planning failed to consider the environment, leading\nto the failure of tasks. For example, an agent in a cave aims to catch fish. It lacks visual information\nto plan conditions on the current situation, such as “leave the cave and find a river”. Therefore, we\nintegrate environmental information into the planning stage. Unlike Jarvis-1 [51] and MP5 [32],\nwhich convert observation to textual descriptions, Optimus-1 directly employs observation as visual\nconditions to generate environment-related plans, i.e., sub-goal sequences. This results in more\ncomprehensive and reasonable planning. More importantly, Knowledge-Guided Planner retrieves\nthe knowledge needed to complete the task from HDKG, allowing task planning to be done once,\nrather than generating the next step in each iteration. Given the task t, observation o, the sub-goals\n5\nsequence g1, g2, g3, ..., gn can be formulated as:\ng1, g2, g3, ..., gn = pθ(o, t, pη(t)),\n(2)\nwhere n is the number of sub-goals, pη denotes sub-graph retrieved from HDKG, pθ denotes MLLM.\nIn this paper, we employ OpenAI’s GPT-4V as Knowledge-Guided Planner and Experience-Driven\nReflector. We also evaluate other alternatives of GPT-4V, such as open-source models like Deepseek-\nVL [26] and InternLM-XComposer2-VL [6] in Section 3.4.\nAction Controller. It takes the sub-goal and the current observation as inputs and then generates\nlow-level actions, which are control signals for the mouse and keyboard. Thus, it can interact with\nthe game environment to update the agent’s state and the observation. The formulation is as follows:\nak = pπ(o, gi),\n(3)\nwhere ak denotes low-level action at time k, pπ denotes action controller. Unlike generating code\n[46, 32, 49], generating control actions for the mouse and keyboard [1, 25, 51, 3] more closely\nresembles human behavior. In this paper, we employ STEVE-1 [25] as our Action Controller.\nExperience-Driven Reflector. The sub-goals generated by Knowledge-Guided Planner are interde-\npendent. The failure of any sub-goal halts the execution of subsequent ones, leading to overall task\nfailure. Therefore, a reflection module is essential to identify and rectify errors promptly. During\ntask execution, the Experience-Driven Reflector activates at regular intervals, retrieving historical\nexperience from AMEP, and then analyzing the current state of Optimus-1. The reflection results\nof Optimus-1 are categorized as COMPLETE, CONTINUE, or REPLAN. COMPLETE indicates successful\nexecution, prompting the action controller to proceed to the next sub-goal. CONTINUE signifies\nongoing execution without additional feedback. REPLAN denotes failure, requiring the Knowledge-\nGuided Planner to revise the plan. The reflection r generated by Experience-Driven Reflector can be\nformulated as:\nr = pθ(o, gi, pϵ(t)),\n(4)\nwhere pϵ denotes multimodal experience retrieved from AMEP. Experimental results in Section\n3.3 demonstrate that the Experience-Driven Reflector significantly enhances the success rate of\nlong-horizon tasks.\nDuring task execution, even in cases where task failure necessitates REPLAN, multimodal experiences\nare stored in AMEP. Thus, during the reflection phase, Optimus-1 can retrieve the most relevant\ncases from each of the three scenarios COMPLETE, CONTINUE, and REPLAN from AMEP as references.\nExperimental Results in Section 3.3 demonstrate the effectiveness of this innovative method of\nincorporating failure cases into in-context learning.\n2.3\nNon-parametric Learning of Hybrid Multimodal Memory\nTo implement the Hybrid Multimodal Memory and enhance Optimus-1’s capacity, we propose a non-\nparametric learning method named “free exploration-teacher guidance”. In the free exploration phase,\nOptimus-1’s equipment and tasks are randomly initialized, and it explores random environments,\nacquiring world knowledge through environmental feedback. For example, it learns that “a stone\nsword\ncan be crafted with a wooden stick\nand two cobblestones\n”, storing this in the HDKG.\nAdditionally, successful and failed cases are stored in the AMEP, providing reference experience for\nthe reflection phase. We initialize multiple Optimus-1, and they share the same HDKG and AMEP.\nThus the memory is filled up efficiently. After free exploration, Optimus-1 has basic world knowledge\nand multimodal experience. In the teacher guidance phase, Optimus-1 needs to learn a small number\nof long-horizon tasks based on extra knowledge. For example, it learns “a diamond sword\nis\nobtained by a stick\nand two diamonds\n” from the teacher, then perform the task “craft diamond\nsword”. During the teacher guidance phase, Optimus-1’s memory is further expanded and it gains the\nexperience of executing complete long-horizon tasks.\nUnlike fine-tuning, this method enhances Optimus-1 incrementally without updating parameters, in\na self-evolution manner. Starting with an empty Hybrid Multimodal Memory, Optimus-1 iterates\nbetween “free exploration-teacher guidance” learning and unseen task inference. With each iteration,\nits memory capacity grows, enabling mastery of tasks from easy to hard.\n6\nTable 1: Main Result of Optimus-1 on long-horizon tasks benchmark. We report the average success\nrate (SR), average number of steps (AS), and average time (AT) on each task group, the results of each\ntask can be found in the Appendix F. Lower AS and AT metrics mean that the agent is more efficient\nat completing the task, while +∞indicates that the agent is unable to complete the task. Overall\nrepresents the average result on the five groups of Iron, Gold, Diamond, Redstone, and Armor.\nGroup\nMetric\nGPT-3.5\nGPT-4V\nDEPS\nJarvis-1\nOptimus-1\nHuman-level\nWood\nSR ↑\n40.16\n41.42\n77.01\n93.76\n98.60\n100.00\nAT ↓\n56.39\n55.15\n85.53\n67.76\n47.09\n31.08\nAS ↓\n1127.78\n1103.04\n1710.61\n1355.25\n841.94\n621.59\nStone\nSR ↑\n20.40\n20.89\n48.52\n89.20\n92.35\n100.00\nAT ↓\n135.71\n132.77\n138.71\n141.50\n129.94\n80.85\nAS ↓\n2714.21\n2655.47\n2574.30\n2830.05\n2518.88\n1617.00\nIron\nSR ↑\n0.00\n0.00\n16.37\n36.15\n46.69\n86.00\nAT ↓\n+∞\n+∞\n944.61\n722.78\n651.33\n434.38\nAS ↓\n+∞\n+∞\n8892.24\n8455.51\n6017.85\n5687.60\nGold\nSR ↑\n0.00\n0.00\n0.00\n7.20\n8.51\n17.31\nAT ↓\n+∞\n+∞\n+∞\n787.37\n726.35\n557.08\nAS ↓\n+∞\n+∞\n+∞\n15747.13\n15527.07\n13141.60\nDiamond\nSR ↑\n0.00\n0.00\n0.60\n8.98\n11.61\n16.98\nAT ↓\n+∞\n+∞\n1296.96\n1255.06\n1150.98\n744.82\nAS ↓\n+∞\n+∞\n23939.30\n25101.25\n23019.64\n16237.54\nRedstone\nSR ↑\n0.00\n0.00\n0.00\n16.31\n25.02\n33.27\nAT ↓\n+∞\n+∞\n+∞\n1070.42\n932.50\n617.89\nAS ↓\n+∞\n+∞\n+∞\n17408.40\n12709.99\n12357.00\nArmor\nSR ↑\n0.00\n0.00\n9.98\n15.82\n19.47\n28.48\nAT ↓\n+∞\n+∞\n997.59\n924.60\n824.53\n551.30\nAS ↓\n+∞\n+∞\n17951.95\n16492.96\n16350.56\n11026.00\nOverall\nSR ↑\n0.00\n0.00\n5.39\n16.89\n22.26\n36.41\n3\nExperiments\n3.1\nExperiments Setting\nEnvironment. To ensure realistic gameplay like human players, we employ MineRL [11] with\nMinecraft 1.16.5 as our simulation environment. The agent operates at a fixed speed of 20 frames per\nsecond and only interacts with the environment via low-level action control signals of the mouse and\nkeyboard. For more information about the detailed descriptions of the observation and action spaces,\nplease refer to the Appendix B.\nBenchmark. We constructed a benchmark of 67 tasks to evaluate the Optimus-1’s ability to complete\nlong-horizon tasks. As illustrated in Table 5, we divide the 67 Minecraft tasks into 7 groups according\nto recommended categories in Minecraft. Please refer to Appendix D for more details.\nBaseline. We compare Optimus-1 with various agents, including GPT-3.5 2, GPT-4V, DEPS [50],\nand Jarvis-1 [51] on the challenging long-horizon tasks benchmark. In addition, we employed 10\nvolunteers to perform the same task on the benchmark, and their average performance served as a\nhuman-level baseline. Please refer to Appendix D.2 for more details about human-level baseline. For\na more comprehensive comparison, we also report Optimus-1’s performances on the benchmark used\nby Voyager [46], MP5 [32], and DEPS [50] in the Appendix F.2. Note that we initialize Optimus-1\nwith an empty inventory, while DEPS [50] and Jarvis-1 [51] have tools in their initial state. This\nmakes it more challenging for Optimus-1 to perform the same tasks.\n2https:\/\/openai.com\/research\/gpt-3.5\n7\nTable 2: Ablation study results. We report average\nsuccess rate (SR) on each task group. P., R., K.,\nE. represent Planning, Reflection, Knowledge, and\nExperience, respectively.\nAblation Setting\nTask Group\nP.\nR.\nK.\nE.\nWood\nStone\nIron\nGold\nDiamond\n14.29\n0.00\n0.00\n0.00\n0.00\n!\n42.95\n25.67\n0.00\n0.00\n0.00\n!\n!\n55.00\n47.37\n18.11\n2.08\n1.11\n!\n!\n!\n73.53\n64.20\n24.19\n3.08\n1.86\n!\n!\n!\n92.37\n69.63\n38.33\n3.49\n2.42\n!\n!\n!\n!\n97.49\n94.26\n53.33\n11.54\n9.59\nTable 3: Ablation study on AMEP. We report\nthe average success rate (SR) on each task group.\nZero, Suc., and Fail. represent retrieving from\nAMEP without getting the case, getting the success\ncase, and getting the failure case, respectively.\nAblation Setting\nTask Group\nZero\nSuc.\nFai.\nWood\nStone\nIron\nGold\nDiamond\n!\n92.00\n79.26\n36.32\n4.25\n3.25\n!\n95.00\n84.29\n46.98\n9.36\n7.89\n!\n95.00\n81.10\n45.47\n7.50\n6.39\n!\n!\n97.49\n94.26\n53.33\n11.54\n9.59\nAbstracted\nMultiModal\nExperience Pool\nfall into \nwater\ndrop in \ncave\nCurrent Goal: chop a tree\nCurrent Goal: go fishing\nAbstracted\nMultiModal\nExperience Pool\n...\nOurs\nSTEVE-1\nSTEVE-1\nOurs\nFigure 4: Illustration of the role of reflection mechanism. Without the help of reflective mechanisms,\nSTEVE-1 [25] often gets into trouble and fails to complete the task. While Optimus-1, with the\nhelp of the Experience-Driven Reflector, leverages the AMEP to retrieve relevant experience, reflect\ncurrent situation and correct errors. This improves Optimus-1’s success rate on long-horizon tasks.\nEvaluation Metrics. The agent always starts in survival mode, with an empty inventory. We\nconducted at least 30 times for each task using different world seeds and reported the average success\nrate to ensure fair and thorough evaluation. Additionally, we add the average steps and average time\nof completing the task as evaluation metrics.\n3.2\nExperimental Results\nThe overall experimental results on benchmark are shown in Table 1, see the accuracy for each task in\nAppendix F. Optimus-1 has a success rate near 100% on the Wood Group\n. Compared with Jarvis-1,\nOptimus-1 has 29.28% and 53.40% improvement on the Diamond Group\nand Redstone Group\n,\nrespectively. Optimus-1 achieves the best performance and the shortest elapsed time among all task\ngroups. It reveals the effectiveness and efficiency of our proposed Optimus-1 framework. Moreover,\ncompared with all baselines, Optimus-1 performance was closer (average 5.37% improvement) to\nhuman levels on long-horizon task groups.\n3.3\nAblation Study\nWe conduct extensive ablation experiments on 18 tasks, experiment setting can be found in Table 6.\nAs shown in Table 2, we first remove Knowledge-Driven Planner and Experience-Driven Reflector,\nthe performance of Optimus-1 on all task groups drops dramatically. It demonstrates the necessity of\nKnowledge-Guided Planner and Experience-Driven Reflector modules for performing long-horizon\ntasks. As for Hybrid Multimodal Memory, we remove HDKG from Optimus-1. Without the help\nof world knowledge, the performance of Optimus-1 decreased by an average of 20% across all task\ngroups. We then removed AMEP, this resulted in the performance of Optimus-1 decreased by an\n8\nSuccess Rate\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nWood Group\nStone Group\nIron Group\nGold Group\nDiamond Group\n97.5 92.4 92.6\n94.3\n79.2\n73.0\n53.3\n43.8\n48.8\n6.1\n4.0\n5.9\n9.6\n3.3 3.3\nGPT-4V w\/ Memory\nGPT-4V w\/o Memory\nDeepseek-VL w\/ Memory\nDeepseek-VL w\/o Memory\nXComposer2-VL w\/ Memory\nXComposer2-VL w\/o Memory\n(a) Generalisation of Hybrid Multimodal Memory\nSuccess Rate\nWood \nGroup\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nEpoch 1\nEpoch 2\nEpoch 3\nEpoch 4\nStone \nGroup\nIron \nGroup\nGold \nGroup\nDiamond \nGroup\n92.50\n83.07\n43.74\n5.75\n4.66\n(b) Self-Evolution\nFigure 5: (a) With the help of Hybrid Multimodal Memory, various MLLM-based Optimus-1 have\ndemonstrated 2 to 6 times performance improvement. (b) Illustration of the change in Optimus-1\nsuccess rate on the unseen task over 4 epochs.\naverage of 12%. Finally, we performed ablation experiments on the way of retrieving cases from\nAMEP. As shown in Table 3, without retrieving cases from AMEP, the success rate shows an average\nof 10% decrease across all groups. It reveals that this reflection mechanism, which considers both\nsuccess and failure cases, has a significant impact on the performance of Optimus-1. To illustrate the\nrole of the reflection mechanism, we have shown some cases in Figure 4.\n3.4\nGeneralization Ability\nIn this section, we explore an interesting issue: whether generic MLLMs can effectively perform\nvarious long-horizon complex tasks in Minecraft using Hybrid Multimodal Memory. As shown in\nFigure 5, We employ Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided\nPlanner and Experience-Driven Reflector. The experimental results show that the original MLLM has\nlow performance on long-horizon tasks due to the lack of knowledge and experience of Minecraft.\nWith the assistance of Hybrid Multimodal Memory, the performance of MLLMs has improved by 2\nto 6 times across various task groups, outperforming the GPT-4V baseline. This encouraging result\ndemonstrates the generalization of the proposed Hybrid Multimodal Memory.\n3.5\nSelf-Evolution via Hybrid Multimodal Memory\nAs shown in Section 2.3, we randomly initialize the Hybrid Multimodal Memory of Optimus-1, then\nupdate it multiple times by using the “free exploration-teacher guidance” learning method. We set\nthe epoch to 4, and the number of learning tasks to 160. At each period, Optimus-1 performs free\nexploration on 150 tasks and teacher guidance learning on the remaining 10 tasks, we then evaluate\nOptimus-1’s learning ability on the task groups same as ablation study. Experimental results are\nshown in Figure 5. It reveals that Optimus-1 keeps getting stronger through the continuous expansion\nof memory during the learning process of multiple periods. Moreover, it demonstrates that MLLM\nwith Hybrid Multimodal Memory can incarnate an expert agent in a self-evolution manner [44].\n4\nRelated Work\n4.1\nAgents in Minecraft\nWe summarise the differences of existing Minecraft agents in the Appendix D.3. Earlier work\n[29, 56, 2, 3] introduced policy models for agents to perform simple tasks in Minecraft. MineCLIP\n[7] used text-video data to train a contrastive video-language model as a reward model for policy,\nwhile VPT [1] pre-trained on unlabelled videos but lacked instruction as input. Building on VPT\nand MineCLIP, STEVE-1 [25] added text input to generate low-level action sequences from human\ninstructions and images. However, these agents struggle with complex tasks due to limitations in\ninstruction comprehension and planning. Recent work [49, 46, 59] incorporated LLMs as planning\nand reflection modules, but lacked visual information integration for adaptive planning. MP5\n9\n[32], MineDreamer [58], and Jarvis-1 [51] enhanced situation-aware planning by obtaining textual\ndescriptions of visual information, yet lacked detailed visual data. Optimus-1 addresses these issues\nby directly using observation as situation-aware conditions in the planning phase, enabling more\nrational, visually informed planning. Additionally, unlike other agents requiring multiple queries\nfor task refinement, Optimus-1 generates a complete and effective plan in one step with the help of\nHDKG. This makes Optimus-1 planning more efficient.\n4.2\nMemory in Agents\nIn the agent-environment interaction process, memory is key to achieving experience accumulation\n[21], environment exploration [16], and knowledge abstraction [57]. There are two forms to represent\nmemory content in LLM-based agents: textual form [17, 15, 30] and parametric form [5, 28, 47, 20].\nIn textual form, the information is explicitly retained and recalled by natural languages. In parametric\nform, the memory information [37] is encoded into parameters and implicitly influences the agent’s\nactions. Recent work [48, 52, 12] has explored the long-term visual information storage [18, 19] and\nsummarisation in MLLM. Our proposed hybrid multimodal memory module is plug-and-play and\ncan provide world knowledge and multimodal experience for Optimus-1 efficiently.\n5\nConclusion\nIn this paper, we propose Hybrid Multimodal Memory module, which consists of two parts: HDKG\nand AMEP. HDKG provides the necessary world knowledge for the planning phase of the agent,\nand AMEP provides the refined historical experience for the reflection phase of the agent. On top\nof the Hybrid Multimodal Memory, we construct the multimodal composable agent, Optimus-1,\nin Minecraft. Extensive experimental results show that Optimus-1 outperforms all existing agents\non long-horizon tasks. Moreover, we validate that general-purpose MLLMs, based on Hybrid\nMultimodal Memory and without additional parameter updates, can exceed the powerful GPT-4V\nbaseline. The extensive experimental results show that Optimus-1 makes a major step toward a\ngeneral agent with a human-like level of performance.\n6\nLimitation and Future Work\nIn the framework of Optimus-1, we are dedicated to leverage proposed Hierarchical Directed Knowl-\nedge Graph and Abstracted Multimodal Experience Pool can be used to enhance the agent’s ability\nto plan and reflect. For Action Controller, we directly introduce STEVE-1 [25] as a generator of\nlow-level actions. However, limited by STEVE-1’s ability to follow instructions and execute complex\nactions, Optimus-1 is weak in completing challenging tasks such as “beat ender dragon” and “build a\nhouse”. Therefore, a potential future research direction is to enhance the instruction following and\naction generation capabilities of action controller.\nIn addition, most of the work, including Optimus-1, utilize a multimodal large language model for\nplanning and reflection, which then drives an action controller to perform the task. Building an\nend-to-end vision-language-action agent will be future work.\n7\nAcknowledgement\nThis study is supported by National Natural Science Foundation of China (Grant No. 62236003\nand 62306090), Shenzhen College Stability Support Plan (Grant No. GXWD20220817144428005),\nNatural Science Foundation of Guangdong Province of China (Grant No. 2024A1515010147), and\nMajor Key Project of Peng Cheng Laboratory (Grant No. PCL2023A08).\n10\nReferences\n[1] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. Advances in Neural Information Processing Systems, 35:24639–24654,\n2022.\n[2] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task con-\ntrol through goal-aware representation learning and adaptive horizon prediction. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pages 13734–13744,\n2023.\n[3] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot:\nLearning to follow instructions by watching gameplay videos. In The Twelfth International\nConference on Learning Representations, 2023.\n[4] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering\nmultimodal large language model with dual-level visual knowledge. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n[5] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npages 6491–6506, 2021.\n[6] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei,\nSongyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering\nfree-form text-image composition and comprehension in vision-language large model. arXiv\npreprint arXiv:2401.16420, 2024.\n[7] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended\nembodied agents with internet-scale knowledge. Advances in Neural Information Processing\nSystems, 35:18343–18362, 2022.\n[8] Mariel K Goddu and Alison Gopnik. The development of human causal learning and reasoning.\nNature Reviews Psychology, pages 1–21, 2024.\n[9] Maitrey Gramopadhye and Daniel Szafir.\nGenerating executable action plans with\nenvironmentally-aware language models. In 2023 IEEE\/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pages 3568–3575. IEEE, 2023.\n[10] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck,\nand Aleksandra Faust. A real-world webagent with planning, long context understanding, and\nprogram synthesis. arXiv preprint arXiv:2307.12856, 2023.\n[11] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv:1907.13440, 2019.\n[12] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav\nShrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for\nlong-term video understanding. arXiv preprint arXiv:2404.05726, 2024.\n[13] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li,\nSong-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world.\narXiv preprint arXiv:2311.12871, 2023.\n[14] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In International\nConference on Machine Learning, pages 9118–9147. PMLR, 2022.\n[15] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sand-\nbox: Transparent and interactive memory management for conversational agents. In Adjunct\nProceedings of the 36th Annual ACM Symposium on User Interface Software and Technology,\npages 1–3, 2023.\n11\n[16] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen,\nLi Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation\nwith multimodal prompts. In NeurIPS 2022 Foundation Models for Decision Making Workshop,\n2022.\n[17] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise?\nIn NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.\n[18] Xiaojie Li, Shaowei He, Jianlong Wu, Yue Yu, Liqiang Nie, and Min Zhang. Mask again:\nMasked knowledge distillation for masked video modeling.\nIn Proceedings of the ACM\nInternational Conference on Multimedia, page 2221–2232. ACM, 2023.\n[19] Xiaojie Li, Jianlong Wu, Shaowei He, Shuo Kang, Yue Yu, Liqiang Nie, and Min Zhang. Fine-\ngrained key-value memory enhanced predictor for video representation learning. In Proceedings\nof the ACM International Conference on Multimedia, page 2264–2274. ACM, 2023.\n[20] Xiaojie Li, Yibo Yang, Xiangtai Li, Jianlong Wu, Yue Yu, Bernard Ghanem, and Min Zhang.\nGenview: Enhancing view quality with pretrained generative model for self-supervised learning.\nIn Proceedings of the European Conference on Computer Vision. Springer, 2024.\n[21] Xiaojie Li, Yibo Yang, Jianlong Wu, Bernard Ghanem, Liqiang Nie, and Min Zhang. Mamba-\nfscil: Dynamic adaptation with selective state space model for few-shot class-incremental\nlearning. arXiv preprint arXiv:2407.06136, 2024.\n[22] Zaijing Li, Fengxiao Tang, Ming Zhao, and Yusen Zhu. Emocaps: Emotion capsule based\nmodel for conversational emotion recognition. In Findings of the Association for Computational\nLinguistics: ACL 2022, pages 1610–1618, 2022.\n[23] Zaijing Li, Ting-En Lin, Yuchuan Wu, Meng Liu, Fengxiao Tang, Ming Zhao, and Yongbin Li.\nUnisa: Unified generative framework for sentiment analysis. In Proceedings of the 31st ACM\nInternational Conference on Multimedia, pages 6132–6142, 2023.\n[24] Zaijing Li, Gongwei Chen, Rui Shao, Dongmei Jiang, and Liqiang Nie. Enhancing the emotional\ngeneration capability of large language models via emotional chain-of-thought. arXiv preprint\narXiv:2401.06836, 2024.\n[25] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A\ngenerative model for text-to-behavior in minecraft. Advances in Neural Information Processing\nSystems, 2023.\n[26] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng\nRen, Zhuoshu Li, Yaofeng Sun, et al.\nDeepseek-vl: towards real-world vision-language\nunderstanding. arXiv preprint arXiv:2403.05525, 2024.\n[27] Simon Makin. The amyloid hypothesis on trial. Nature, 559(7715):S4–S4, 2018.\n[28] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast\nmodel editing at scale. In International Conference on Learning Representations, 2021.\n[29] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generaliza-\ntion with multi-task deep reinforcement learning. In Proceedings of the 34th International\nConference on Machine Learning, pages 2661–2670. PMLR, 2017.\n[30] Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Sid-\ndartha Naidu. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint\narXiv:2308.10882, 2023.\n[31] Eileen Parkes. Scientific progress is built on failure. Nature, 10, 2019.\n[32] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and\nJing Shao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception.\narXiv preprint arXiv:2312.07472, 2023.\n12\n[33] Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton,\nBethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, et al. Scaling instructable\nagents across many simulated worlds. arXiv preprint arXiv:2404.10179, 2024.\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748–8763. PMLR, 2021.\n[35] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen. Multi-adversarial discriminative deep\ndomain generalization for face presentation attack detection. In Proceedings of the IEEE\/CVF\nconference on computer vision and pattern recognition, pages 10023–10031, 2019.\n[36] Rui Shao, Tianxing Wu, and Ziwei Liu. Detecting and grounding multi-modal media manipula-\ntion. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition,\npages 6904–6913, 2023.\n[37] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. Detecting and grounding\nmulti-modal media manipulation and beyond. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2024.\n[38] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and Liqiang Nie. Mome: Mixture of\nmultimodal experts for generalist multimodal large language models. In NeurIPS, 2024.\n[39] Shan H Siddiqi, Konrad P Kording, Josef Parvizi, and Michael D Fox. Causal mapping of\nhuman brain function. Nature reviews neuroscience, pages 361–375, 2022.\n[40] JF Stein. Role of the cerebellum in the visual guidance of movement. Nature, pages 217–221,\n1986.\n[41] Tim Stuart, Andrew Butler, Paul Hoffman, Christoph Hafemeister, Efthymia Papalexi,\nWilliam M Mauck, Yuhan Hao, Marlon Stoeckius, Peter Smibert, and Rahul Satija. Com-\nprehensive integration of single-cell data. cell, 177(7):1888–1902, 2019.\n[42] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang,\nYongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context\nlearners. arXiv preprint arXiv:2312.13286, 2023.\n[43] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong\nXia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A\nmultimodal agent for red dead redemption ii as a case study. arXiv preprint arXiv:2403.03186,\n2024.\n[44] Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei\nHuang, Dacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models.\narXiv preprint arXiv:2404.14387, 2024.\n[45] Deniz Vatansever, Jonathan Smallwood, and Elizabeth Jefferies. Varying demands for cognitive\ncontrol reveals shared neural processes supporting semantic and episodic memory retrieval.\nNature communications, 12(1):2134, 2021.\n[46] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023.\n[47] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen\nZhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via\nknowledge editing. arXiv preprint arXiv:2403.14472, 2024.\n[48] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying\nLi, and Jose M Alvarez. Omnidrive: A holistic llm-agent framework for autonomous driving\nwith 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024.\n13\n[49] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with large language models enables open-world\nmulti-task agents. arXiv preprint arXiv:2302.01560, 2023.\n[50] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with large language models enables open-world\nmulti-task agents. arXiv preprint arXiv:2302.01560, 2023.\n[51] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,\nZhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with\nmemory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023.\n[52] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient\nlong video understanding via large language models. arXiv preprint arXiv:2404.03384, 2024.\n[53] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse\nin-context configurations for image captioning. Advances in Neural Information Processing\nSystems, 36, 2024.\n[54] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.\nAppagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.\n[55] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: Enhancing\nmultimodal large language model to answer questions in dynamic audio-visual scenarios. In\nECCV, 2024.\n[56] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Skill reinforcement learning and planning for open-world long-horizon tasks. arXiv preprint\narXiv:2303.16563, 2023.\n[57] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong,\nand Ji-Rong Wen. A survey on the memory mechanism of large language model based agents.\narXiv preprint arXiv:2404.13501, 2024.\n[58] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao,\nand Jing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for\nsimulated-world control. arXiv preprint arXiv:2403.12037, 2024.\n[59] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin\nLi, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for\nopen-world enviroments via large language models with text-based knowledge and memory.\narXiv preprint arXiv:2305.17144, 2023.\n14\nA\nBroader Impact\nWith the increasing capability level of Multimodal Large Language Models (MLLM) comes many\npotential benefits and also risks. On the positive side, we anticipate that the techniques that used to\ncreate Optimus-1 could be applied to the creation of helpful agents in robotics, video games, and\nthe web. This plug-and-play architecture that we have created can be quickly adapted to different\nMLLMs, and the proposed methods also provide a viable solution for other application areas in the\nagent domain. However, on the negative side, it is imperative to acknowledge the inherent stochastic\nnature of MLLMs in text generation. If not addressed carefully, this could lead to devastating\nconsequences for society. Prior to deploying MLLMs in conjunction with the Hybrid Multimodal\nMemory methodology, a comprehensive assessment of their potential risks must be undertaken. We\nhope that while the stakes are low, works such as ours can improve access to safety research on\ninstruction-following models in multimodal agents domains.\nB\nMinecraft\nMinecraft is an extremely popular sandbox video game developed by Mojang Studios 3. It allows\nplayers to explore a blockly, procedurally generated 3D world with infinite terrain, discover and\nextract raw materials, craft tools and items, and build structures or earthworks (shown in Figure 6).\nMinecraft is a valuable and representative environment for evaluating long-horizon tasks, offering\ngreater diversity and complexity compared to other environments. Unlike web\/app navigation [54] and\nembodied manipulation [16], Minecraft is an open world with a complex and dynamic environment\n(79 biomes, including ocean, plains, forest, desert, etc.). To complete long-horizon tasks, agents must\nachieve multiple sub-goals (e.g., 15 sub-goals to craft a diamond sword), making the construction\nof a Minecraft agent quite challenging. Many studies [46, 32, 51] have chosen Minecraft as the\nenvironment for validating performance on long-horizon tasks. Extensive experimental results in the\npaper show that Optimus-1 outperforms all baselines. Therefore, we chose Minecraft as open-world\nenvironment to evaluate the ability of agents to perform long-horizon tasks.\nB.1\nBasic Rules\nBiomes. The Minecraft world is divided into different areas called “biomes”. Different biomes contain\ndifferent blocks and plants and change how the land is shaped. There are 79 biomes in Minecraft\n1.16.5, including ocean, plains, forest, desert, etc. Diverse environments have high requirements for\nthe generalization of agents.\nTime. Time passes within this world, and a game day lasts for 20 real-world minutes. Nighttime is\nmuch more dangerous than daytime: the game starts at dawn, and agents have 10 minutes of game\ntime before nightfall. Hostile or neutral mobs spawn when night falls, and most of these mobs are\ndangerous, trying to attack agents. How to survive in such a dangerous world is an open problem for\nMinecraft agents research.\nItem. In Minecraft 1.16.5, there are 975 items can be obtained, such as wooden pickaxe\n, iron\nsword\n. Item can be obtained by crafting or destroying blocks or attacking entities. For example,\nagent can attack cows\nto obtain leather\nand beef\n. Agent also can use 1 stick\nand 2 diamonds\nto craft diamond sword\n.\nGameplay progress. Progression primarily involves discovering and utilizing various materials\nand resources, each of which unlocks new capabilities and options. For instance, crafting a wooden\npickaxe\nenables the player to mine stone\n, which can be used to create a stone pickaxe\nand a furnace\n; these, in turn, allow for the mining and smelting of iron ore\n. Subsequently,\nan iron pickaxe\npermits the extraction of diamonds\n, and a diamond pickaxe\ncan mine\nvirtually any block in the game. Similarly, cultivating different crops allows for the breeding of\nvarious animals, each providing distinct resources beyond mere sustenance. Enemy drops also have\nspecific applications, with some being more beneficial than others. By integrating resources from\nmining, farming, and breeding, players can enchant their equipment. The collection and crafting of\nmaterials also facilitate construction, enabling players to build diverse structures. Beyond practical\n3https:\/\/www.minecraft.net\/en-us\/article\/meet-mojang-studios\n15\n(a)\n(b)\n(c)\n(d)\nFigure 6: The screenshots in Minecraft. (a). The world has different complex terrains, including\nplains, river, forest and mine. (b). The agent can use crafting table to craft tools and items with\nrecipes. (c). The agent can use the furnace to smelt ore to obtain precious ingot. (d). The agent can\ngrow wheat near the river.\nconsiderations such as secure bases and farms, the creative aspect of building personalized structures\nconstitutes a significant part of the Minecraft experience.\nFreedom. In Minecraft, player can do anything they can imagine. Player can craft tools, smelt ore,\nbrew potions, trade with villagers and wandering traders, attack mobs, grow crops, raise animals in\ncaptivity, etc. Player even can use redstone\nto build a computer. This is a world of freedom and\ninfinite possibilities.\nMore Challenge than Diamond\n. Progression beyond the Overworld is fairly limited: Eventually,\nyou can build a nether portal to reach the Nether, where you can get materials for more complex\ncrafting, the resources to brew potions, and the top tier of tools and armor. The Nether materials also\nlet you reach the End dimension, where you must defeat the Ender Dragon to unlock the outer End\nIslands, where you can get an elytra that lets you fly, and shulker boxes for more storage.\nB.2\nObservation and Action Spaces\nObservation. Our observation space is completely consistent with human players. The agent only\nreceives an RGB image with dimensions of 640 × 360 during the gameplay process, including the\nhotbar, health indicators, food saturation, and animations of the player’s hands. It is worth helping\nthe agent see more clearly in extremely dark environments, we have added a night vision effect for\nthe agent, which increases the brightness of the environment during the night.\nAction Spaces. Our action space is almost similar to human players, except for craft and smelt\nactions. It consists of two parts: the mouse and the keyboard. The keypresses are responsible for\ncontrolling the movement of agents, such as jumping, forward, back, etc. The mouse movements\nare responsible for controlling the perspective of agents and the cursor movements when the GUI is\nopened. The left and right buttons of the mouse are responsible for attacking and using or placing\nitems. In Minecraft, precise mouse movements are important when completing complex tasks that\nneed open inventory or crafting table. In order to achieve both the same action space with MineDojo\n16\nTable 4: Our action space.\nIndex\nAction\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove back.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current movement.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPlace blocks, entity, open items or other interact actions defined by game.\n10\nhotbar [1-9]\nkeys 1-9\nSelects the appropriate hotbar item.\n11\nOpen\/Close Inventory\nkey E\nOpens the Inventory. Close any open GUI.\n12\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\n[7], we abstract the craft and the smelt action into action space. The detailed action space is described\nin Table 4.\nB.3\nLong-horizon Tasks\nLong-horizon Tasks are complex tasks that require world knowledge to solve and consist of multiple\nindispensable subtask sequences. In Minecraft, technology has six levels, including wood\n, stone\n, iron\n, golden\n, diamond\n, and netherite\n. Wooden tools can mine stone-level blocks, but\ncan’t mine iron-level and upper-level blocks. Stone tools can mine iron-level blocks, but can’t mine\ndiamond-level and upper-level blocks. Iron-level tools can mine diamond-level blocks, but can’t mine\nnetherite-level blocks. Diamond-level tools can mine any level blocks.\nFor example, the agent now wants to complete the task “Craft iron sword\n”. The agent needs to\ncraft wood-level tools to mine stone\n, and craft stone-level tools to mine iron ore\n. In order to\ncraft tools, the agent needs a crafting table\n. To smelt iron ore\ninto iron ingot\n, the agent needs a\nfurnace\n. Moreover, craft crafting table needs 4 planks, and craft furnace needs 8 cobblestone. In\nsummary, the agent needs to obtain many raw materials, wood-level and stone-level tools, 1 crafting\ntable, 1 furnace, and most importantly, 2 iron ingots. The process of this task is shown in Figure 7.\nC\nTheory\nIn this section, we briefly introduce the relevant theory of cognitive science. For more details, please\nrefer to the original articles.\nOur ability to understand and predict the world around us depends on our long-term memory stores,\nwhich have historically been divided into two distinct systems [27, 41, 45]. The semantic memory\nsystem provides a conceptual framework for describing the similar meanings of words and objects\nas they are encountered in different contexts (e.g., a bee is a flying insect with yellow and black\nstripes that produces honey), whereas the episodic memory system records our personal experiences\ncharacterized by the co-occurrence of words and objects at different times and places (e.g., being\nstung by a bee while eating honey at a picnic last weekend). These information stores and the\ninteractions between them play a crucial role in guiding our behaviour and giving us the flexibility to\nadapt to the various demands of our environment.\nIn this paper, inspired by the above theory, we divide the agent memory module into two parts:\nknowledge and experience. Based on this, we propose Hierarchical Directed Knowledge Graph and\nAbstracted Multimodal Experience Pool to enable the agent to acquire, store, and utilize knowledge\nand experience during the execution of tasks. Extensive experimental results demonstrate the\neffectiveness of the proposed methodology\n17\n(a) Chop 7 logs\n(b) Craft 21 planks\n(c) Craft 5 sticks\n(d) Craft 1 crafting table\n(e) Craft 1 wooden pickaxe\n(f) Mine 11 cobblestone\n(g) Craft 1 furnace\n(h) Craft 1 stone pickaxe\n(i) Dig down more deeper to find\niron ore\n(j) Mine 2 iron ores\n(k) Smelt 2 iron ingots\n(l) Craft 1 iron sword\nFigure 7: Processing of task \"Craft 1 iron sword\". Optimus-1 needs thousands of steps to complete\nthis task. To craft and smelt precisely, the mouse movements action can’t have any error.\nD\nBenchmark Suite\nD.1\nBenchmark\nWe constructed a benchmark of 67 tasks to evaluate Optimus-1’s ability to complete long-horizon\ntasks in Minecraft. According to recommended categories in Minecraft, we have classified these tasks\ninto 7 groups: Wood\n, Stone\n, Iron\n, Gold\n, Diamond\n, Redstone\n, Armor\n. The statistics\nfor benchmark are shown in Table 5. Due to the varying complexity of these tasks, we adopt different\nmaximum gameplay steps (Max. Steps) for each task. The maximum steps are determined by the\naverage steps that human players need to complete the task. Due to the randomness of Minecraft, the\nworld and initial spawn point of the agent could vary a lot. In our benchmark setting, We initialize\nthe agent with an empty inventory, which makes it necessary for the agent to complete a series of\nsub-goals (mining materials, crafting tools) in order to perform any tasks. This makes every task\nchallenging, even for human players.\nNote that Diamonds are a very rare item that only spawns in levels 2 to 16 and have a 0.0846%\nchance of spawning in Minecraft 1.16.5. Diamonds are usually found near level 9, or in man-made or\nnatural mines no higher than level 16. In order to reduce the huge impact that diamond generation\n18\nTable 5: Setting of 7 groups encompassing 67 Minecraft long-horizon tasks.\nGroup\nTask Num.\nExample Task\nMax. Steps\nInitial Inventory\nAvg. Sub-goal Num.\nWooden\n10\nCraft a wooden axe\n3600\nEmpty\n4.60\nStone\n9\nCraft one stone pickaxe\n7200\nEmpty\n8.78\nIron\n16\nCraft a iron pickaxe\n12000\nEmpty\n12.75\nGolden\n6\nMine gold and smelt into golden ingot\n36000\nEmpty\n15.83\nRedstone\n6\nCraft a piston\n36000\nEmpty\n17.50\nDiamond\n7\nDig down and mine a diamond\n36000\nEmpty\n15.14\nArmor\n13\nCraft one iron helmet\n36000\nEmpty\n15.85\nTable 6: We evaluate Optimus-1 on these tasks in ablation study which are the subset of our\nbenchmark.\nGroup\nTask\nSub-Goal Num.\nMax. Step\nInitial Inventory\nWooden\nCraft a wooden axe\n5\n3600\nEmpty\nCraft a crafting table\n3\n3600\nEmpty\nStone\nCraft a stone pickaxe\n10\n7200\nEmpty\nCraft a stone axe\n10\n7200\nEmpty\nCraft a furnace\n9\n7200\nEmpty\nIron\nCraft a iron pickaxe\n13\n12000\nEmpty\nCraft a bucket\n13\n12000\nEmpty\nCraft a rail\n13\n12000\nEmpty\nCraft a iron sword\n12\n12000\nEmpty\nCraft a shears\n12\n12000\nEmpty\nGolden\nCraft a golden pickaxe\n16\n36000\nEmpty\nCraft a golden axe\n16\n36000\nEmpty\nSmelt a golden ingot\n15\n36000\nEmpty\nDiamond\nCraft a diamond pickaxe\n15\n36000\nEmpty\nCraft a diamond axe\n16\n36000\nEmpty\nCraft a diamond hoe\n15\n36000\nEmpty\nCraft a diamond sword\n15\n36000\nEmpty\nDig down and mine a diamond\n15\n36000\nEmpty\nprobability has on agent’s likelihood of completing a task, we have adjusted the diamond generation\nprobability to 20%, spawns in levels 2 to 16. This setting applies to human players as well.\nIn the ablation study, we select the subset of our benchmark as the test set (shown in Table 6). The\nenvironment setting is the same as the benchmark.\nD.2\nBaselines\nExisting Baseline. On the one hand, we employ GPT-3.5 and GPT-4V as baseline, which are\nevaluated without integrating hybrid multimodal memory modules. During the planning phase, they\ngenerate a plan for the action controller based on task prompt (and observation). During the reflection\nphase, they generate reflection results in a zero-shot manner. On the other hand, we compare existing\nSOTA Agents [50, 51] in Minecraft.\nHuman-level Baseline. To better demonstrate agent’s performance level in Minecraft, we hired 10\nvolunteers to play the game as a human-level baseline. The volunteers played the game with the same\nenvironment and settings, and every volunteer asked to perform the each task on the benchmark 10\ntimes. Ultimately, we used the average scores of 10 volunteers as the human-level baseline. The\nresults of the human-level baseline are shown in Table 1. To ensure the validity of the experiment,\nwe ensured that each volunteer had at least 20 hours of Minecraft gameplay before conducting the\nexperiment. For each volunteer, we pay $25 as reward.\n19\nTable 7: Statistics for various Minecraft agents.\nAgent\nPub.\nEnv.\nInput\nOutput\nPlanning\nReflection\nKnowledge\nExperience\nVPT [1]\nNeurIPS’ 22\nMineRL\nV\nlow-level action\nMineDOJO [7]\nNeurIPS’ 22\nMineDOJO\nT+V\nlow-level action\nSTEVE-1 [25]\nNeurIPS’ 23\nMineRL\nT+V\nlow-level action\nVoyager [46]\nNeurIPS’ 23\nMineflayer\nT+V\ncode\n!\n!\n!\nDEPS [50]\nNeurIPS’ 23\nMineDOJO\nT+V\ncode\n!\n!\nGROOT [3]\nICLR’ 24\nMineRL\nT+V\nlow-level action\nMP5 [32]\nCVPR’ 24\nMineDOJO\nT+V\ncode\n!\n!\nJarvis-1 [51]\n-\nMineRL\nT+V\nlow-level action\n!\n!\n!\nOptimus-1\n-\nMineRL\nT+V\nlow-level action\n!\n!\n!\n!\nD.3\nMinecraft Agents\nIn this section, we summarise the differences between existing Minecraft agents. As shown in the\nTable 7, earlier work [1, 7, 25, 3] constructed Transformer-based policy network as agent. Recent\nwork [46, 50, 51, 32] introduces the Multimodal Large Language Model, which empowers the agent\nto complete long-horizon tasks by exploiting the powerful language comprehension and planning\ncapabilities of LLM.\nIn the Mineflayer and Minedojo environments, agents [7, 46, 50, 32] can accomplish sub-goals by\ncalling APIs (in the form of codes), which is a different behavioral pattern from humans. In MineRL\n[11], agents [1, 25, 3, 51] must generate low-level actions to perform tasks, which is more challenging\nto accomplish long-horizon tasks.\nMoreover, existing agents lack knowledge and experience, and their performance in Minecraft is\nstill vastly gapped from the human level. In this paper, we introduce Hybrid Multimodal Memory,\nwhich empowers Optimus-1 with hierarchical knowledge and multimodal experience. This makes\nOptimus-1 significantly outperform all existing agents on challenging long-horizon tasks benchmark,\nand exhibits near human-level performance on many tasks.\nE\nImplementation Details\nE.1\nHybrid Multimodal Memory\nE.1.1\nAbstracted Multimodal Experience Pool\nRelevant studies [5, 28, 17, 15] have demonstrated the importance of memory for agents to complete\nlong-horizon tasks. To implement the memory mechanism, Minedojo [7] and Voyager [46] only\nconsidered unimodal storage of historical information. Jarvis-1 [51] considered a multimodal memory\nmechanism to store task planning and visual information as experience, but it stores all historical\ninformation without summarisation. This approach stores all visual images, which poses a huge\nchallenge in storage size and retrieval efficiency. To solve the problem, we propose the Abstracted\nMultimodal Experience Pool structure, which summarizes all historical information during the\nagent’s execution of the task, which maintains the integrity of long sequential information and greatly\nimproves the storage and retrieval efficiency of the experience.\nAs shown in Figure 2, we first input the visual image stream to the video buffer, which filters the\nimage stream at a fixed frequency. It makes the length of the image stream substantially shorter.\nEmpirically, we set the frequency of filtering to 1 second\/frame, meaning that the video buffer takes\none frame per second from the original image stream to compose the filtered image stream. We\nfound that above this frequency makes the visual information redundant (too much similarity between\nimages), and below this frequency does not preserve enough complete visual information.\nThen, we feed the filtered frames into an image buffer with a window size of 16. We dynamically\ncompute the similarity between images in the image buffer, when a new image comes in, we compute\nthe similarity between the new image and the most recent image, and then we remove the image with\nthe highest similarity in order to keep the image buffer’s window size to 16.\nSubsequently, we introduce MineCLIP [7], a pre-trained model of video-text alignment with a\nstructure similar to CLIP [34], as our visual summariser. For a given sub-goal, it calculates the\n20\ncorrelation between the visual content within the current memory bank and the sub-goal, and when\nthis correlation exceeds a pre-set threshold, the frames within the memory bank are saved as the visual\nmemories corresponding to that sub-goal. Finally, we store the visual memories with the sub goal’s\ntextual description into the Abstracted Multimodal Experience Pool. In addition, we incorporate\nthe environment information, agent initial state, plan from Knowledge-Guided Planner, etc. into\nthe experience memory of the given task. In this way, we consider the history information of each\nsub-goal and summaries and summarise it to finally compose the multimodal experience of the given\ntask.\nNote that we also store these visual memories as failure cases when the feedback from the reflection\nphase is REPLAN. Therefore, when Optimus-1 executes a long-horizon task, it can retrieve past\nsuccesses and failures as references and update memory after the task is finished. In the reflection\nphase, Optimus-1 retrieve the most relevant cases from Abstracted Multimodal Experience Pool,\nwhich contains the three scenarios COMPLETE, CONTINUE, and REPLAN, to help the agent better\nassess which state the current situation belongs to. This approach of considering both successful\nand failed cases for in-context learning is inspired by related research [8, 31], and its effectiveness is\nvalidated in Section 3.3.\nE.1.2\nHierarchical Directed Knowledge Graph\nAs shown in the Figure 2, crafting a diamond sword\nrequires two diamonds\nand a wooden stick\n, while mining diamonds requires an iron pickaxe\n, which in turn requires additional raw materials\nand crafting steps. We transform this mine and craft knowledge into a graph structure, where the\nnodes of the graph are objects, and the nodes point to objects that can be crafted or completed by\nthat object. With directed graph, we show that connections between objects are established, and that\nthis knowledge can be stored and updated efficiently. For a given object, we only need to retrieve\nthe corresponding node to extract the corresponding subgraph from the knowledge graph. Then\nby topological sorting, we can get the antecedents and required materials for the object, and this\ninformation is provided to the Knowledge-Guided Planner as a way to generate a more reasonable\nsequence of sub-goals. With Hierarchical Directed Knowledge Graph, we can significantly enhance\nthe world knowledge of the agent in a train-free manner, as shown in the experimental results in\nSection 3.3.\nOur HDKG can be efficiently updated and expanded. When adding new nodes, the HDKG can be\nupdated by simply merging the nodes and relationships into the graph. This method involves local\nlinear modifications to the graph rather than altering the entire graph, making the process efficient\nand time-saving. For example, when M new nodes and N edges are added, the HDKG can be updated\nwith M+N times of operations. Moreover, an HDKG containing 851 objects (nodes) requires less\nthan 1 MB of memory. Thus, the HDKG can be efficiently updated and maintained.\nE.2\nHybrid Multimodal Memory Driven Optimus-1\nIn order to implement the proposed Hybrid Multimodal Memory and to progressively increase the\ncapacity of Optimus-1 in a self-evolution manner, we propose a non-parametric learning method\nnamed “free exploration-teacher guidance”.\nIn the free exploration phase, we randomly initialize the environment, materials, and tasks. For\nthe task “craft a wooden pickaxe”, we provide initial materials (three planks, two sticks), and then\nOptimus-1 (only the action controller activated) attempts to complete the task. If the environment\nfeedback indicates the task is successful, the knowledge {3 planks, 2 sticks →wooden pickaxe} is\nadded to the HDKG. Note that we randomly initialize materials and their quantities, which means\nthat the task may not always succeed. As a result, each free exploration may not acquire the\ncorresponding knowledge, but it can record the relevant experience (whether successful or fail). In\nthe free exploration phase, Optimus-1 learns simple atomic operations, such as crafting sticks in the\nWooden Group and mining diamonds in the Diamond Group.\nIn the teacher guidance phase, Optimus-1 need to learn a small number of long-horizon tasks based\non extra knowledge. For example, during the free exploration phase, Optimus-1 mastered crafting\nstick\nand mining diamond\n, but did not know that “a diamond sword\nis obtained by a stick\nand two diamonds\n”. So we provide some task plans, which will serve as extra knowledge to guide\n21\nOptimus-1 to complete the task of “craft diamond sword”. We built the following automated process\nto get the task plan needed for “free exploration”:\n• We randomly select 5 tasks for each Group (7 groups in total) that are not included in the\nbenchmark.\n• For each selected task, we use a script to automatically obtain the crafting relationships from\nthe Minecraft Wiki 4. Taking the task “craft a wooden sword” as an example, we use the\nscript to automatically obtain the crafting relationships: 1 wooden stick, 2 planks, 1 crafting\ntable →1 wooden sword, 1 log →4 planks, 2 planks →4 sticks, 4 planks →1 crafting\ntable.\n• These relationships are converted into a directed acyclic graph through an automated script.\nBy performing a topological sort, the graph can be converted into tuples of materials and\ntheir quantities: (wooden sword, 1), (crafting table, 1), (wooden stick, 1) (planks, 8), (log,\n2).\n• We prompt GPT-4 to construct a plan in order from basic materials to advanced materials.\n• Finally, we get the plan: 1. Get two logs 2. Craft eight planks 3. Craft a crafting table 4.\nCraft a wooden stick 5. Craft a wooden sword\nDuring the teacher guidance phase, Optimus-1’s memory is further expanded and it gains the\nexperience of executing complete long-horizon tasks. Teacher guidance phase allows Optimus-1 to\nacquire advanced knowledge and learn multimodal experiences through complete long-horizon tasks.\nE.3\nBackbone of Optimus-1\nOptimus-1 consists of Knowledge-Guided Planner, Experience-Driven Reflector, and Action Con-\ntroller. In this paper, we employ OpenAI’s GPT-4V (gpt-4-turbo) 5 as Knowledge-Guided Planner\nand Experience-Driven Reflector, and STEVE-1 [25] as Action Controller. We also employ open-\nsource models like Deepseek-VL [26] and InternLM-XComposer2-VL [6] as Knowledge-Guided\nPlanner and Experience-Driven Reflector.\nAll experiments were implemented on 4x NVIDIA A100 GPUs. We employ multiple Optimus-1\nto perform different tasks at the same time, and this parallelized inference greatly improves our\nexperimental efficiency. In the free exploration and teacher guidance phases, there is no need to\naccess OpenAI’s API, and the learning process takes approximately 16 hours on 4x A100 80G GPUs.\nDuring the inference phase, it takes about 20 hours on 4x A100 80G GPUs.\nThroughout the experiment, we spent about $5, 000 to access the GPT-4V API. However, we also\noffer more cost-effective solutions. As shown in Figure 5, if we employ Deepseek-VL [26] or\nInternLM-XComposer2-VL [6] as Optimus-1’s backbone, we can get comparable performance with\nlow-cost!\nE.4\nPrompt for Optimus-1\nWe show the prompt templates for Experience-Driven Reflector and Action Controller as follows.\nSystem: You are a MineCraft game expert and you can guide agents to complete complex tasks.\nUser: For a given game screen and task, you need to complete <goal inference> and <visual\ninference>.\n<goal inference>: According to the task, you need to infer the weapons, equipment, or\nmaterials required to complete the task.\n<visual inference>: According to the game screen, you need to infer the following aspects:\nhealth bar, food bar, hotbar, environment.\nI will give you an example as follow:\n[Example]\n<task>: craft a stone sword.\n<goal inference>: stone sword\n<visual inference>\nhealth bar: full\nfood bar: full\n4https:\/\/minecraft.wiki\/\n5https:\/\/openai.com\/index\/gpt-4v-system-card\/\n22\nhotbar: empty\nenvironment: forest\nHere is a game screen and task, you MUST output in example format.\n<task>: {task}.\n<game screen>: {image}\nAssistant:\n==============================\nUser: Now you need to make a plan with the help of <visual info> and <craft graph>.\n<visual info>: Consists of the following aspects: health bar, food bar, hotbar, environment.\nBased on the current visual information, you need to consider whether prequel steps\nneeded to ensure that agent can complete the task.\n<craft graph>: a top-down list of all the tools and materials needed to complete the task.\nI will give you an example of planning under specific visual conditions as follow:\n[Example]\n{example}\nHere is a game screen and task, you MUST output in example format. Remember <task planning>\nMUST output in example format.\n<task>: {task}\n<game screen>: {image}\n<craft graph>: {graph}\nAssistant:\nListing 1: Prompt for Knowledge-Guided Planner.\nSystem: You are a MineCraft game expert and you can guide agents to complete complex tasks.\nAgent is executing the task: {task}.\nGiven two images about agent’s state before executing the task and its current state, you\nshould first detection the environment (forest, cave, ocean, etc.,) in which the agent\nis located, then determine whether the agent’s current situation is done, continue, or\nreplan.\n<done>: Comparing the image before the task was performed, the current image reveals that the\ntask is complete.\n<continue>: Current image reveals that the task is NOT complete, but agent is in good state (\ngood health, not hungry) with high likelihood to complete task.\n<replan>: Current image reveals that the task is NOT complete, and agent is in bad state (bad\nhealth, or hungry) or situation (in danger, or in trouble), need for replanning. For\nreplan, you need to further determine whether the agent’s predicament is \"drop_down\" or\n\"in_water\". \"drop_down\" means that the agent has fallen into a cave or is trapped in a\nmountain or river, while \"in_water\" means that the agent is in the ocean and needs to\nreturn to land immediately.\nUser: I’ll give you some examples to illustrate the different situations. Each example\nconsists of two images, where the first image is the state of the agent before\nperforming the task and the second image is the current state of the agent.\n[Examples]\n<done>: {image1},{image2}\n<continue>: {image1},{image2}\n<replan>: {image1},{image2}\nNow given two images about agent’s state before executing the task and its current state, you\nMUST and ONLY output in following format:\nEnviroment: <environment>\nSituation: <situation>\n(if situation is replan) Predicament: <predicament>\nListing 2: Prompt for Experience-Driven Reflector.\nF\nAdditional Experimental Results\nF.1\nFull Results on Our Benchmark\nWe list the results of each task on the benchmark below, with details including task name, sub-goal\nnumbers, success rate (SR), average number of steps (AS), average time (AT), and eval times. All\ntasks are evaluated in Minecraft 1.16.5 Survival Mode. Note that each time Optimus-1 performs a\ntask, we initial it with an empty initial inventory and a random start point. This makes it challenging\nfor Optimus-1 to perform each task.\n23\nTable 8: The results of Optimus-1 on various tasks in the Wood group. SR, AS, AT denote success\nrate, average number of steps, and average time (seconds), respectively.\nTask\nSub-Goal Num.\nSR\nAS\nAT(s)\nEval Times\nCraft a wooden shovel\n6\n95.00\n995.58\n49.78\n40\nCraft a wooden pickaxe\n5\n100.00\n1153.91\n57.70\n30\nCraft a wooden axe\n5\n96.67\n1010.28\n50.51\n30\nCraft a wooden hoe\n5\n100.00\n1042.80\n52.14\n30\nCraft a stick\n4\n97.14\n372.97\n18.65\n70\nCraft a crafting table\n3\n98.55\n448.63\n22.43\n69\nCraft a wooden sword\n5\n100.00\n1214.90\n60.74\n30\nCraft a chest\n4\n100.00\n573.80\n28.69\n30\nCraft a bowl\n4\n100.00\n744.30\n37.21\n30\nTable 9: The results of Optimus-1 on various tasks in the Stone group. SR, AS, AT denote success\nrate, average number of steps, and average time (seconds), respectively.\nTask\nSub-Goal Num.\nSR\nAS\nAT(s)\nEval Times\nCraft a stone shovel\n8\n90.32\n2221.00\n111.05\n31\nCraft a stone pickaxe\n10\n96.77\n2310.09\n115.50\n31\nCraft a stone axe\n10\n96.88\n2112.59\n105.63\n32\nCraft a stone hoe\n8\n94.64\n2684.60\n134.23\n56\nCraft a charcoal\n9\n88.57\n3083.35\n154.17\n35\nCraft a smoker\n9\n90.24\n3118.89\n155.94\n41\nCraft a stone sword\n8\n94.29\n2067.92\n103.40\n35\nCraft a furnace\n9\n93.75\n2842.71\n142.14\n32\nCraft a torch\n8\n85.71\n2109.00\n105.45\n95\nMoreover, in MineRL [11] environment, ’steps’ refers to the number of interactions between the\nagent and the environment, occurring at a frequency of 20 times per second. For example, if an\nagent takes 2 seconds to complete the task “chop a tree”, it interacts with the environment 40 times,\nresulting in a recorded steps number of 40. Experimental results show that Optimus-1’s average task\ncompletion step (AS) is significantly lower than other baselines.\nF.2\nResults on Other Benchmark\nFor a more comprehensive comparison with current Minecraft Agents, we also report Optimus-1’s\nperformances on the benchmark used by Voyager [46], MP5 [32], and DEPS [50] below. Due to\nthe different environments and settings, agents perform tasks with varying degrees of difficulty. For\nexample, Optimus-1 requires low-level action to perform any task in MineRL [11], and we initialize\nits inventory to be empty. While Voyager [46] performs tasks in Mineflayer 6 environment only\nthrough encapsulated code, MP5 [32] performs tasks in MineDOJO [7] environment only needs a\nspecific control signal to craft tools, no low-level actions (mouse movement and click) are needed.\nOptimus-1’s success rate in completing tasks with these baselines is shown in the Table 15 and Table\n16, and Optimus-1’s efficiency in unlocking the tech tree in Minecraft is shown in the Figure 8. These\nresults reveal that Optimus-1 outperforms a variety of powerful baseline agents, even in challenging\nenvironmental settings!\n6https:\/\/github.com\/PrismarineJS\/mineflayer\n24\nTable 10: The results of Optimus-1 on various tasks in the Iron group. SR, AS, AT denote success\nrate, average number of steps, and average time (seconds), respectively.\nTask\nSub Goal Num.\nSR\nAS\nAT(s)\nEval Times\nCraft an iron shovel\n13\n54.79\n5677.35\n637.81\n73\nCraft an iron pickaxe\n13\n59.42\n6157.39\n591.81\n69\nCraft an iron axe\n13\n54.29\n6026.26\n676.97\n70\nCraft a stone_hoe\n13\n52.70\n6650.97\n743.82\n74\nCraft a bucket\n13\n54.29\n6124.61\n591.35\n70\nCraft a hopper\n14\n46.67\n7242.14\n710.17\n60\nCraft a rail\n13\n42.19\n6713.07\n754.48\n64\nCraft an iron sword\n12\n57.14\n5625.49\n633.91\n70\nCraft a shears\n12\n53.62\n5058.00\n570.35\n69\nCraft a smithing table\n12\n44.93\n5317.39\n594.81\n69\nCraft a tripwire hook\n13\n48.57\n4968.74\n562.66\n70\nCraft a chain\n13\n44.93\n5764.42\n645.33\n69\nCraft an iron bars\n12\n42.00\n6508.43\n723.13\n50\nCraft an iron nugget\n12\n30.99\n4697.23\n525.29\n71\nCraft a blast furnace\n14\n25.71\n7760.67\n711.05\n35\nCraft a stonecutter\n13\n34.78\n5993.38\n675.52\n46\nTable 11: The results of Optimus-1 on various tasks in the Gold group. SR, AS, AT denote success\nrate, average number of steps, and average time (seconds), respectively.\nTask\nSub Goal Num.\nSR\nAS\nAT(s)\nEval Times\nCraft a golden shovel\n16\n9.80\n13734.75\n686.74\n51\nCraft a golden pickaxe\n16\n13.75\n9672.00\n783.60\n80\nCraft a golden axe\n16\n4.44\n10158.75\n707.94\n45\nCraft a golden hoe\n16\n3.33\n13120.50\n756.03\n27\nCraft a golden sword\n16\n3.33\n9792.00\n789.60\n26\nSmelt and craft a gold_ingot\n15\n16.42\n9630.27\n681.51\n67\nTable 12: The results of Optimus-1 on various tasks in the Diamond group. SR, AS, AT denote\nsuccess rate, average number of steps, and average time (seconds), respectively.\nTask\nSub Goal Num.\nSR\nAS\nAT(s)\nEval Times\nCraft a diamond shovel\n15\n18.75\n23696.75\n1184.84\n64\nCraft a diamond pickaxe\n15\n15.71\n32189.50\n1609.46\n70\nCraft a diamond axe\n16\n4.00\n21920.50\n1096.03\n75\nCraft a diamond hoe\n15\n4.61\n24031.00\n1201.55\n65\nCraft a diamond sword\n15\n14.52\n27555.50\n1377.78\n62\nDig down and mine a diamond\n15\n9.09\n20782.13\n1039.11\n64\nCraft a jukebox\n15\n14.58\n25056.00\n1252.80\n48\n25\nTable 13: The results of Optimus-1 on various tasks in the Redstone group. SR, AS, AT denote\nsuccess rate, average number of steps, and average time (seconds), respectively.\nLanguage Instruction\nSub-Goal Num.\nSR\nAS\nAT(s)\nEval Times\nCraft a piston\n16\n28.57\n6457.10\n822.85\n35\nCraft a redstone torch\n16\n29.63\n6787.87\n939.39\n27\nCraft an activator rail\n18\n15.68\n8685.62\n934.28\n51\nCraft a compass\n23\n15.00\n14908.67\n845.43\n40\nCraft a dropper\n16\n37.50\n7272.80\n1063.64\n40\nCraft a note block\n16\n24.32\n6727.89\n936.39\n37\nTable 14: The results of Optimus-1 on various tasks in the Armor group. SR, AS, AT denote success\nrate, average number of steps, and average time (seconds), respectively.\nTask\nSub Goal Num.\nSR\nAS\nAT(s)\nEval Times\nCraft shield\n14\n43.33\n7229.00\n861.45\n30\nCraft iron chestplate\n14\n47.22\n7230.24\n851.51\n36\nCraft iron boots\n14\n23.81\n6597.33\n729.87\n42\nCraft iron leggings\n14\n6.67\n9279.00\n763.95\n30\nCraft iron helmet\n14\n58.14\n6287.11\n814.36\n43\nCraft diamond helmet\n17\n2.08\n7342.00\n867.10\n48\nCraft diamond chestplate\n17\n2.70\n7552.00\n777.60\n37\nCraft diamond leggings\n17\n9.68\n7664.67\n883.23\n31\nCraft diamond boots\n17\n16.67\n10065.60\n803.28\n30\nCraft golden helmet\n17\n12.50\n11563.25\n778.16\n32\nCraft golden leggings\n17\n14.60\n10107.33\n805.37\n41\nCraft golden boots\n17\n6.06\n10311.00\n915.55\n33\nCraft golden chestplate\n17\n9.67\n10407.58\n820.38\n31\nMinecraft Tech Tree\nWooden\nStone Iron\nDiamond\nRedstone\nOur\nVoyager\nAutoGPT\nFigure 8: An illustration of Optimus-1 unlocking the tech tree in Minecraft.\n26\nTable 15: Result on Process-Dependent Tasks compared with MP5 [32]. SR, AS, AT denote success\nrate, average number of steps, and average time (seconds), respectively.\nMP5 [32]\nOptimus-1\nTask Level\nSR\nSR\nAS\nAT(s)\nlog\n96.67\n100.00\n586.58\n29.33\nsand\n96.67\n94.32\n1540.33\n77.02\nplanks\n96.67\n100.00\n571.06\n28.55\nstick\n96.67\n97.14\n372.97\n18.65\ncrafting table\n93.33\n98.55\n448.63\n22.43\nBasic Level\nAverage\n96.00\n98.00\n703.91\n35.20\nbowl\n93.33\n100.00\n744.30\n37.21\nboat\n93.33\n92.86\n1170.00\n58.50\nchest\n90.00\n100.00\n573.80\n28.69\nwooden sword\n86.67\n100.00\n1214.90\n60.74\nwooden pickaxe\n80.00\n100.00\n1153.91\n57.70\nWooden Level\nAverage\n88.67\n98.57\n971.38\n48.56\ncobblestone\n80.00\n95.29\n1492.00\n74.60\nfurnace\n80.00\n93.75\n2842.71\n142.14\nstone pickaxe\n80.00\n96.77\n2310.09\n115.50\niron ore\n60.00\n50.00\n3017.00\n150.85\nglass\n80.00\n81.11\n3870.75\n193.54\nStone Level\nAverage\n76.00\n83.38\n2706.51\n135.32\niron ingot\n56.67\n59.42\n4697.23\n634.86\nshield ∗\n56.67\n43.33\n7229.00\n661.45\nbucket\n53.33\n54.29\n6124.61\n606.23\niron pickaxe\n50.00\n59.42\n6157.39\n607.87\niron door\n43.33\n48.28\n5528.00\n676.40\nIron Level\nAverage\n52.00\n52.94\n5947.25\n637.36\ndiamond ore ∗\n30.00\n9.09\n20782.13\n1039.10\nmind redstone\n20.00\n25.12\n6787.87\n739.39\ncompass\n16.67\n15.00\n14908.67\n745.43\ndiamond pickaxe\n23.33\n15.71\n32189.50\n1609.48\npiston\n20.00\n28.57\n6457.10\n622.85\nDiamond Level\nAverage\n22.00\n18.70\n14963.83\n948.19\n27\nTable 16: Results (success rate) on 8 META TASK groups compared with DEPS [49].\nMeta-Task\nTask Object\nInnerMonologue\nCode-as-Policy\nDEPS [49]\nOurs\nBasic\nMT1\nplanks\n83.3\n83.3\n83.3\n100.0\nstick\n83.3\n83.3\n86.7\n97.1\nchest\n0.0\n50.0\n76.7\n100.0\nsign\n0.0\n43.3\n86.7\n94.3\nboat\n26.7\n56.7\n73.3\n92.9\ntrapdoor\n56.7\n56.7\n76.7\n96.2\nbowl\n23.3\n46.7\n80.0\n100.0\nTool(Simple)\nMT2\ncrafting_table\n70.0\n70.0\n90.0\n98.5\nwooden_pickaxe\n80.0\n80.0\n80.0\n100.0\nwooden_sword\n83.3\n83.3\n86.7\n100.0\nwooden_shovel\n76.7\n76.7\n90.0\n95.0\nfurnace\n40.0\n40.0\n66.7\n93.7\nstone_pickaxe\n36.7\n53.3\n73.3\n96.7\nstone_axe\n30.0\n30.0\n70.0\n96.8\nstone_hoe\n36.7\n56.7\n66.7\n94.6\nstone_shovel\n36.7\n36.7\n66.7\n90.3\nstone_sword\n53.3\n36.7\n80.0\n94.2\nHunt and Food\nMT3\nbed\n6.7\n6.7\n43.3\n90.0\npainting\n16.7\n16.7\n86.7\n92.2\ncarpet\n0.0\n13.3\n43.3\n91.3\ncooked_porkchop\n0.0\n0.0\n50.0\n90.0\ncooked_beef\n0.0\n0.0\n63.3\n90.0\ncooked_mutton\n0.0\n0.0\n66.7\n90.0\nDig-down\nMT4\nstone_stairs\n36.7\n16.7\n66.7\n90.3\nstone_slab\n16.7\n33.3\n73.3\n91.2\nlever\n46.7\n46.7\n83.3\n91.0\ncoal\n6.7\n0.0\n20.0\n86.5\ntorch\n6.7\n0.0\n13.3\n85.7\nEquipment\nMT5\nleather_boots\n13.3\n13.3\n60.0\n68.2\nleather_chestplate\n0.0\n6.7\n36.7\n64.2\nleather_helmet\n6.7\n0.0\n70.0\n65.9\nleather_leggings\n20.0\n0.0\n56.7\n65.5\niron_chestplate\n0.0\n0.0\n0.0\n47.2\niron_leggings\n0.0\n0.0\n3.3\n6.6\niron_helmet\n0.0\n0.0\n3.3\n58.1\niron_boots\n0.0\n0.0\n20.0\n23.8\nshield\n0.0\n6.7\n13.3\n43.3\nTool Complex\nMT6\nbucket\n0.0\n3.3\n6.7\n54.3\nshears\n0.0\n0.0\n30.0\n53.6\niron_pickaxe\n6.7\n0.0\n10.0\n59.4\niron_axe\n0.0\n0.0\n16.7\n54.3\niron_hoe\n0.0\n0.0\n13.3\n52.7\niron_shovel\n0.0\n0.0\n13.3\n57.8\niron_sword\n0.0\n3.3\n6.7\n54.7\nIron-Stage\nMT7\niron_bars\n0.0\n0.0\n6.7\n42.0\nhopper\n0.0\n0.0\n6.7\n46.7\niron_door\n0.0\n0.0\n3.3\n48.3\ntripwire_hook\n6.7\n0.0\n30.0\n48.6\nrail\n0.0\n0.0\n6.7\n42.2\nChallenge\nMT8\ndiamond\n0.0\n0.0\n0.6\n9.1\n28\nG\nCase Study\nThis section introduces several cases to comprehensively demonstrate Optimus-1’s capabilities.\nFigures 9, 10, and 11 demonstrate the superiority of our reflection mechanism, which dynamically\nadjusts the plan based on the current game progress.\n• Figure 9 illustrates Optimus-1’s replanning ability. When Optimus-1 realizes it cannot\ncomplete a task (such as a craft failure shown in the figure), it will replan the current task\nand continue execution.\n• Figures 10 and 11 showcase Optimus-1’s ability to make judgments based on visual signals.\nWhen Optimus-1 determines that it has completed a task (such as “kill a cow\n” in Figure\n10), it will finish the current task and move on to the next one. If Optimus-1 discovers that\nit has not yet completed the task and the task has not failed(as shown in Figure 11), it will\ncontinue executing the task.\nFigures 12 and 13 illustrate the advantages of planning with knowledge. With the Hierarchical\nDirected Knowledge Graph, we can generate a high-quality plan in one step and dynamically adjust\nthe plan based on current visual signals.\n• Figure 12 demonstrates the importance of knowledge. For a long-horizon task such as\n“Mine 1 diamond\n,” Optimus-1 first generates a plan based on the Hierarchical Directed\nKnowledge Graph. However, this plan needs to be adjusted based on the current visual\nsignals. For example, in this figure, Optimus-1 appears in a cave, so the primary task is\nnot to “chop a tree” but to “leave the cave” first. Only after exiting the cave can Optimus-1\nproceed with the initial plan.\n• Figure 13 demonstrates the high efficiency of our method. Agents like MP5 [32] and\nVoyager [46] use an iterative planning approach, which is very time-consuming, generating\nthe final plan step by step. During this process, agent does not take any action. As shown in\nFigure 13, a zombie is gradually approaching the agent, but the agent is still iterating on its\nplan. Optimus-1, however, generates the plan in one step based on the Hierarchical Directed\nKnowledge Graph and makes reasonable plans based on the current visual signals.\n1 . Mine 2 logs\n2. Craft 8 planks\n3. Craft 4 sticks\n4. Craft 1  Crafting \ntable\n5. Craft 1  wooden \npickaxe (failed)\n6. Replan: Mine 1  log, Craft 4 \nplanks, Craft 1  wooden pickaxe\nTask: Craft 1 \nwooden pickaxe\nHierarchical Directed\nKnowledge Graph\nPlan\n2\n8\n4\nO1\nO2\nO3\nO4\nO5,1\nO5,2\n1\n1\nFigure 9: The process of completing the task \"Craft 1 wooden pickaxe\". Optimus-1 gives wrong\nplanning. When Optimus-1 realizes it cannot complete the task, it will replan the current task.\n29\n1 . Mine 3 logs\n2. Craft 1 2 planks\n3. Craft 4 sticks\n4. Craft 1  Crafting \ntable\n5. Craft 1  wooden \nsword\n6. Done: Find and kill cow\nTask: Find a cow and \nkill it\nHierarchical Directed\nKnowledge Graph\nPlan\nO1\nO2\nO3\nO4\nO5,1\nO5,2\n3\n1 2\n1\n4\n1\n1\nFigure 10: The process of completing the task \"Find a cow and kill it\". Hierarchical Directed\nKnowledge Graph indicates that having a wooden sword will make the task easier to complete.\nTherefore, Optimus-1 first crafts a wooden sword and then proceeds to find and kill a cow.\n1 . Mine 1 0  logs\n2. Mine 1 0  logs\n3. Mine 1 0  logs\n4. Continue: Mine 1 0  logs\n5. Mine 1 0  logs\n6. Done: Mine 1 0  logs\nTask: Chop tree to \nobtain 10 logs\nHierarchical Directed\nKnowledge Graph\nPlan\nO1\nO2\nO3\nO4\nO5,1\nO5,2\n1 0\nO1,1\nO1,2\nO1,3\nO1,4\nO2\nO2,1\nFigure 11: The process of completing the task \"Chop tree to obtain 10 logs\". Hierarchical Directed\nKnowledge graph indicates that no tools are needed to complete this goal. After finding a tree,\nOptimus-1 starts chopping it down. The task requires a substantial amount of wood, so midway\nthrough, Optimus-1 performs a reflection. The task is not yet complete but is progressing smoothly,\nand the result of the reflection is to continue.\n30\nTask: Mine 1 diamond\nHierarchical Directed\nKnowledge Graph\nPlan\nO1\nO2\nO3\nO4\nO5,1\nO5,2\nO1,1\nO1,2\nO1,3\nO1,4\nO2\nO2,1\nLeave the cave\n1 . Leave the cave\n1 . Leave the cave\nO1,1\nO1,2\n2. Craft 1 wooden pickaxe\n3. Craft 1  stone pickaxe\n4. Craft 1  iron pickaxe\n5. Mine 1  diamond\n+\nO2\nO3\nO4\nO5\nFigure 12: The process of completing the task \"Mine 1 diamond\n\". Mining diamonds\nis a highly\ncomplex task. Diamonds can only be mined with an iron pickaxe\n, so an iron pickaxe\nmust be\ncrafted first. Crafting an iron pickaxe\nrequires iron ingots\n, which are smelted from iron ore\n.\nMining iron ore\nrequires a stone pickaxe\n. Crafting a stone pickaxe\nrequires stone\n, which\nin turn must be mined with a wooden pickaxe\n. Crafting a wooden pickaxe\nrequires wooden\nplanks\nand sticks\n. All these crafting processes require a crafting table\n, and smelting requires\na furnace\n. In this case, the agent spawns at a cave, so Optimus-1 must leave the cave to chop logs.\nQuestion: How to craft iron sword?\nAnswer: Run to a sunny place first. And then\nTask: Craft 1 iron sword\nMP5\nOurs\nQuestion: How to craft iron sword?\nAnswer: Using 2 iron ingot, 1 \nstick to craft iron sword.\nQuestion: How to obtain stick?\nAnswer: Using 2 planks to \ncraft 4 sticks.\nQuestion: How to obtain iron ingot?\nAnswer: Mine iron ore, and smelt it \ninto iron ingot.\nQuestion: How to mine iron ore?\nAnswer: Using stone pickaxe to \nmine iron ore.\nQuestion: How to craft stone pickaxe?\nAnswer: Using 3 cobblestone and 2 \nsticks to craft stone pickaxe.\nHierarchical Directed\nKnowledge Graph\nzombie  is arriving...\nQuestion: How to obtain planks?\nAnswer: Using 1 logs to craft 4 \nplanks.\nFigure 13: In this example, a zombie is slowly approaching the agent. Agents like MP5 [32] and\nVoyager [46] uses an iterative planning strategy to generate the plan, which consumes a great deal of\ntime and puts the agent in danger. While Optimus-1 directly generates a plan in one step based on the\nknowledge graph. Using the current visual information, it makes a plan to \"run to a sunny place,\"\nallowing the agent to avoid danger then begin to achieve sub-goals.\n31\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks.pdf"}
{"title":"Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy","authors":"Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie","summary":"Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.","url":"http:\/\/arxiv.org\/abs\/2502.19902v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2502.19902v2","published":1740647884000,"comment":"Accept to CVPR 2025, Project page:\n  https:\/\/cybertronagent.github.io\/Optimus-2.github.io\/","pdf_text":"Optimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nZaijing Li1 2, Yuquan Xie1, Rui Shao1*, Gongwei Chen1, Dongmei Jiang2, Liqiang Nie1*\n1Harbin Institute of Technology, Shenzhen\n2Peng Cheng Laboratory\n{lzj14011,xieyuquan20016}@gmail.com, {shaorui,nieliqiang}@hit.edu.cn\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/\nAbstract\nBuilding an agent that can mimic human behavior pat-\nterns to accomplish various open-world tasks is a long-\nterm goal.\nTo enable agents to effectively learn behav-\nioral patterns across diverse tasks, a key challenge lies\nin modeling the intricate relationships among observa-\ntions, actions, and language.\nTo this end, we propose\nOptimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-\nlevel planning, alongside a Goal-Observation-Action Con-\nditioned Policy (GOAP) for low-level control. GOAP con-\ntains (1) an Action-guided Behavior Encoder that models\ncausal relationships between observations and actions at\neach timestep, then dynamically interacts with the histori-\ncal observation-action sequence, consolidating it into fixed-\nlength behavior tokens, and (2) an MLLM that aligns be-\nhavior tokens with open-ended language instructions to pre-\ndict actions auto-regressively. Moreover, we introduce a\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset, which contains 25,000 videos across 8 atomic\ntasks, providing about 30M goal-observation-action pairs.\nThe automated construction method, along with the MGOA\ndataset, can contribute to the community’s efforts to train\nMinecraft agents. Extensive experimental results demon-\nstrate that Optimus-2 exhibits superior performance across\natomic tasks, long-horizon tasks, and open-ended instruc-\ntion tasks in Minecraft.\nPlease see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.\n1. Introduction\nEnabling agents to learn human behavioral patterns for\ncompleting complex tasks in open-world environments, is\na long-standing goal in the field of artificial intelligence\n[5, 23, 34, 47]. To effectively handle diverse tasks in an\n*Corresponding authors\nTransformer\nXL\nGoal \nEncoder\nkeyboard: W\nmouse: [0.0, 1.0]\n⊕\nLLM\nMLLM as Planner\nVision \nEncoder\nCausal \nPerceiver \nHistorical Sequence\nHistory\nAggregator \nAction-guided Behavior Encoder\nExsiting Goal-conditioned Policy\nOurs\n  1. chop a tree\n  2. craft four planks\n  3. craft two sticks\n  4. craft a wood sword\nSub-goals\nGoal-conditioned\nPolicy\nAction\nor\nsub-goal: chop a tree\nsub-goal: chop a tree\nI need a \nwooden \nsword\nVision \nEncoder\n...\nFigure 1.\nLeft: General agent framework.\nRight: Compari-\nson between existing goal-conditioned policies and ours. Existing\nTransformer-XL-based policies [3, 25] exhibit limited natural lan-\nguage understanding capabilities and rely solely on combining im-\nplicit goal embeddings with visual embeddings as inputs. In con-\ntrast, our GOAP achieves superior action prediction by 1) employ-\ning an Action-guided behavior encoder to strengthen causal mod-\neling between observations and actions, as well as to improve his-\ntorical sequence modeling capabilities, and 2) leveraging MLLM\nto enhance open-ended language comprehension.\nopen-world environment like Minecraft [20, 32], a promi-\nnent agent framework [24, 32, 41, 42] integrates a task\nplanner with a goal-conditioned policy. As illustrated in\nFigure 1 (left), this framework first utilizes the task plan-\nner’s language comprehension and visual perception abili-\nties to decompose complex task instructions into sequential\nsub-goals. These sub-goals are then processed by a goal-\nconditioned policy to generate actions.\nAlthough existing agents [24, 32, 42] have made promis-\ning progress by using Multimodal Large Language Models\n(MLLM) [4, 37, 45] as planners, the current performance\nbottleneck for agents lies in the improvement of the goal-\narXiv:2502.19902v2  [cs.AI]  11 Mar 2025\nconditioned policy [24]. As the sub-goal serves as a natu-\nral language description of an observation-action sequence,\nthe goal-conditioned policy needs to learn the crucial re-\nlationships among sub-goals, observations, and actions to\npredict actions. However, existing goal-conditioned poli-\ncies exhibit the following limitations: (1) Existing policies\nneglect the modeling of the relationship between observa-\ntions and actions. As shown in Figure 1, they only model\nthe relationship between the sub-goal and the current ob-\nservation by adding the sub-goal embedding to the obser-\nvation features [3, 25, 43]. However, the current observa-\ntion is generated by the previous action interacting with the\nenvironment. This implies a causal relationship between\naction and observation, which is neglected by current poli-\ncies; (2) Existing policies struggle to model the relation-\nship between open-ended sub-goals and observation-action\nsequences. As depicted in Figure 1, existing policies pri-\nmarily rely on either video encoders [3, 43] or conditional\nvariational autoencoders (CVAE) [25] as goal encoder to\nproduce implicit goal embeddings. Such embeddings have\nlimited representation ability [43]. Simply adding it to ob-\nservation features is sub-optimal and unable to handle the\ncomplex relationship between sub-goals and observation-\naction sequences.\nIn this paper, we propose Optimus-2, a novel agent that\nincorporates an MLLM for planning, alongside a Goal-\nObservation-Action Conditioned Policy (GOAP). To ad-\ndress the aforementioned challenges, we propose GOAP,\nwhich can better model the relationship among the obser-\nvations, actions, and sub-goals in two aspects.\nAn Action-guided Behavior Encoder for observation-\naction sequence modeling. To capture the relationship be-\ntween observations and actions, the Action-guided Behav-\nior Encoder first employs a Causal Perceiver to integrate ac-\ntion embeddings into observation features. It utilizes task-\nrelevant action information as guidance to adjust the obser-\nvation features, thereby providing fine-grained observation-\naction information for action prediction. Additionally, to\nmodel a long-term observation-action sequence without ex-\nceeding input length limitations, a History Aggregator is\nintroduced to dynamically integrate current observation-\naction information with the historical sequence into fixed-\nlength behavior tokens. Behavior tokens can capture the\nlong-term dependencies of the observation-action sequence\nwith a fixed and appropriate length. It enables the agent to\npredict actions that align with the logic of the observation-\naction sequence, rather than making isolated action predic-\ntions based solely on the current observation.\nAn MLLM to model the relationship between sub-\ngoal and observation-action sequence. To explicitly en-\ncode the semantics of sub-goals, we introduce an MLLM as\nthe backbone of GOAP. It aligns the sub-goal with behav-\nior tokens to predict subsequent actions auto-regressively.\nLeveraging the MLLM’s language comprehension and mul-\ntimodal perception capabilities, it can better integrate fea-\ntures from open-ended sub-goals and observation-action\nsequences, thereby enhancing the policy’s action predic-\ntion ability. To the best of our knowledge, GOAP is the\nfirst effort to employ MLLM as the core architecture of\na Minecraft policy, which demonstrates strong instruction\ncomprehension capabilities for open-ended sub-goals.\nMoreover, current Minecraft datasets either lack align-\nment among essential elements [10] or are not publicly\naccessible [1], resulting in a significant scarcity of high-\nquality observation-goal-action pairs necessary for policy\ntraining. To this end, we introduce an automated approach\nfor constructing the Minecraft Goal-Observation-Action\n(MGOA) dataset. The MGOA dataset comprises 25,000\nvideos across 8 atomic tasks, providing approximately 30\nmillion aligned observation-goal-action pairs.\nIt will be\nmade openly available to support advancements within\nthe research community.\nWe conducted comprehensive\nevaluations in the open-world environment of Minecraft,\nand the experimental results demonstrate that Optimus-\n2 achieves superior performance.\nCompared to previous\nSOTA, Optimus-2 achieves an average improvements of\n27%, 10%, and 18% on atomic tasks, long-horizon tasks,\nand open-ended sub-goal tasks, respectively.\nIn summary, our contributions are as follows:\n• We propose a novel agent Optimus-2, which consists of\nan MLLM for planning, and a policy for low-level con-\ntrol. The experimental results demonstrate that Optimus-\n2 exhibits superior performance on atomic tasks, long-\nhorizon tasks, and open-ended sub-goal tasks.\n• To better model the relationship among the observations,\nactions, and sub-goals, we propose Goal-Observation-\nAction Conditioned Policy, GOAP. It contains an Action-\nguided Behavior Encoder for observation-action se-\nquence modeling, and an MLLM to model the relation-\nship between sub-goal and observation-action sequence.\n• To address the scarcity of large-scale, high-quality\ndatasets, we introduce the MGOA dataset. It comprises\napproximately 30 million aligned observation-goal-action\npairs and is generated through an automated process with-\nout any manual annotations. The proposed dataset con-\nstruction method and the released MGOA dataset can\ncontribute to the community’s efforts to train agents.\n2. Related Work\nMinecraft Agents. Previous works [2, 8, 13, 31] have con-\nstructed policies in Minecraft using reinforcement learn-\ning or imitation learning. VPT [1] was training on large-\nscale video data recorded by human players, using behavior\ncloning to mimic human behavior patterns. GROOT [3] em-\nploys a video encoder as a goal encoder to learn semantic in-\nformation from videos. However, these policies rely solely\nLarge Language Model\nAction Head\n�t+1\n ViT\nLlama Tokenizer\nSub-goal: \nChop a tree.\nImage Token\nFFN\nCross-Attention\nV\nK\nQ\n...\nCross-Attention\nHistory-Attention\nV\nK\nQ\nV\nK\nQ\nText Token\nAction Token\nBehavior Token\nCausal Perceiver \nHistory\nAggregator \nAction-guided Behavior Encoder\nHistorical Memory Bank \n...\nAction-guided\nBehavior Encoder\n�t\n�t\n�1\n�t−3\n�t−2\n�t−1\n�t+1\n��\n...\n...\n�t\n...\n�t\n�1\n�2\n�3\n��−1\nLoRA\nMLLM-based\nPlanner\nTask: I need a \nwooden sword.\nFigure 2. Overview of Optimus-2. Given a task and the current observation, Optimus-2 first uses an MLLM-based Planner to generate a\nseries of sub-goals. Optimus-2 then sequentially executes these sub-goals through GOAP. GOAP obtains behavior tokens for the current\ntimestep via the Action-guided Behavior Encoder, and these behavior tokens, along with image and text tokens, are fed into the LLM to\npredict subsequent actions.\non visual observations as input and cannot follow human\ninstructions to accomplish specific tasks. MineCLIP [10]\nintroduces a video-text contrastive learning module as a re-\nward model for policy, and STEVE-1 [25] builds on VPT\n[1] by incorporating MineCLIP as goal encoder, enabling\npolicy to follow natural language instructions. Despite these\nadvancements, these policies are constrained by language\nunderstanding and reasoning capabilities. To address this,\ncurrent agents [20, 24, 32, 40, 42, 43] leverage MLLM’s\ninstruction following capabilities to decompose complex\ntasks into executable sub-goal sequences, which are then\nfed into a goal-conditioned policy [3, 25] or formed as exe-\ncutable code [26, 28, 51, 52]. Despite significant progress,\nthe performance of current policies remains constrained by\ntheir limited ability to understand sub-goals. In this paper,\nwe aim to develop an MLLM-based goal-conditioned pol-\nicy to enhance the policy’s comprehension of open-ended\nsub-goals, thereby improving overall performance.\nLong-term Video Modeling. Previous work [1, 3, 10, 25]\nhave segmented videos into multiple clips for training to\nalleviate the challenges posed by long-sequence video in-\nputs. However, this approach prevents the agent from learn-\ning comprehensive behavior representations from the entire\nvideo. To handle long-term video sequences [22, 48, 49],\nexisting studies employ temporal pooling [30], querying\ntransformers [14, 46], or token merging [16, 38, 50] to\nintegrate long-sequence visual tokens. Inspired by previ-\nous works [6, 18, 19, 44], we propose a Q-former [7, 21]\nstructure with a memory bank [14], enabling effective long-\nterm sequence modeling through interactions with histori-\ncal queries. Unlike existing methods that model only the\nobservation sequence, we focus on multimodal learning\n[33, 35, 36]. Moreover, different from previous work [14]\nthat primarily compress video features into fixed-length to-\nkens, our Action-guided Behavior Encoder dynamically in-\nteracts with the historical sequence at each timestep, pro-\nducing behavior tokens corresponding to the observation-\naction sequence from the start to the current timestep.\n3. Preliminaries and Problem Formulation\nIn Minecraft, agents [1, 3, 25] exhibit behavior patterns\nsimilar to humans: at each time step t, the agent receives\na visual observation ot and generates control actions at+1\nusing the mouse and keyboard.\nThese actions interact\nwith the environment, resulting in a new visual observa-\ntion ot+1.\nThrough continuous interactions, a trajectory\nJ = {(o1, a1), (o2, a2), (o3, a3), . . . , (oT , aT )} is formed,\nwhere T represents the length of the trajectory. Previous\nwork primarily trained Minecraft agents using reinforce-\nment learning [10] or behavior cloning [3, 25]. For exam-\nple, in behavior cloning, the goal of the policy pθ(at+1|o1:t)\nis to minimize the negative log-likelihood of the actions at\neach time step t given the trajectory J. Considering that\nsuch trajectories are typically generated under explicit or\nimplicit goals, many recent approaches condition the be-\nhavior on a (implicit or explicit) goal g and learn goal-\nconditioned policy pθ(at+1|o1:t, g) [3, 25]. Generally, for\nboth agents and humans, the explicit goal g is a natural lan-\nguage instruction.\nFormally, given a trajectory J with length T, standard\nbehavior cloning trains the policy pθ(·) with parameters θ\nby minimizing the negative log-likelihood of actions:\nmin\nθ\nT\nX\nt=1\n−log pθ(at+1|o1:t, g)\n(1)\n4. Optimus-2\nIn this section, we first give an overview of our proposed\nagent framework, Optimus-2. As shown in Figure 1 (left),\nit includes a planner for generating a series of executable\nsub-goals and a policy that sequentially executes these sub-\ngoals to complete the task.\nNext, we introduce how to implement Optimus-2’s plan-\nner (Sec. 4.1). Subsequently, we elaborate on how to imple-\nment the proposed GOAP (Sec. 4.2). Finally, in Sec 4.3, we\nintroduce an automated dataset generation method to obtain\na high-quality Minecraft Goal-Observation-Action dataset\n(MGOA) for training GOAP.\n4.1. MLLM-based Task Planner\nIn Minecraft, a complex task consists of multiple interme-\ndiate steps, i.e., sub-goals. For example, the task “I need\na wooden pickaxe” includes five sub-goals: ‘chop a tree to\nget logs\n’, ‘craft four planks\n’, ‘craft a crafting table\n’, ‘craft two sticks\n’, and ‘craft a wooden pickaxe\n’.\nTherefore, a planner is essential for the agent, as it needs to\ndecompose the given complex task into a sequence of exe-\ncutable sub-goals for the policy to execute sequentially. In\nthis paper, we follow Li et al. [24], employing an MLLM\nas the planner, which takes current observation and task in-\nstruction as input to generate sub-goals.\n4.2. Goal-Observation-Action Conditioned Policy\nAccording to Sec 3., a key insight into the relationship\namong observation o, action a, and sub-goal g is: that the\nobservation o and action a at the same time step have a\ncausal relationship; and the sub-goal g is a natural language\ndescription of the observation-action sequence over a cer-\ntain time. To better model the relationships among the three\nelements mentioned above, we propose first integrating the\nrepresentations of observation and action at each time step,\nthen modeling the observation-action sequences along the\ntemporal dimension, and finally aligning the observation-\naction sequences with the sub-goal for action prediction.\nMotivated\nby\nthis,\nwe\npropose\na\nnovel\nGoal-\nObservation-Action\nconditioned\nPolicy,\nGOAP.\nAs\nshown in Figure 2, our GOAP consists of an Action-guided\nBehavior Encoder that dynamically models observation-\naction sequences into fixed-length behavior tokens and an\nTable 1. Comparison of the MGOA dataset with existing datasets.\nO, G, and A represent observation, goal, and action. VPT† in-\ndicates the amount of data that is openly accessible. MineCLIP‡\ndenotes narrated Minecraft videos available on YouTube.\nFormat\nDataset\nO\nG\nA\n# Frames\nImage-Text Pairs\nMP5 [32]\n!\n!\n500K\nOmniJARVIS [43]\n!\n!\n!\n600K\nGameplay Video\nVPT† [1]\n!\n!\n6M\nMineCLIP‡ [10]\n!\n!\n20B\nSTEVE-1 [25]\n!\n!\n!\n32K\nMGOA (Ours)\n!\n!\n!\n30M\nMLLM that aligns such behavior tokens with sub-goal for\naction prediction.\n4.2.1. Action-guided Behavior Encoder\nPrevious policies often overlook the causal relationship be-\ntween observation and action at each timestep. Moreover,\nit remains a challenge to model the long-term observation-\naction sequence without exceeding input length constraints.\nTo this end, we propose an Action-guided Behavior En-\ncoder that integrates the representations of observation and\naction at each time step and then dynamically models the\nhistorical sequences into the fix-length behavior tokens.\nFirstly, for the timestep t, we pass observation ot into a\nvisual encoder VE to obtain the visual features:\nvt ←VE(ot)\n(2)\nwhere vt ∈RP ×d, P is the number of patches for each im-\nage, and d is the dimension of the extracted image feature.\nIn practice, we employ ViT [9] as our visual encoder.\nThen, we introduce a Causal Perceiver module to model\nthe relationship between observations and actions. It takes\nthe visual feature vt as query tokens and the action embed-\nding at as key and value. The module then constructs the\ninformation interaction between action at and vt through a\ncross-attention mechanism:\nQ = vtW Q\nv , K = atW K\na , V = atW V\na\n(3)\nˆvt = CrossAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(4)\nwhere W Q\nv , W K\na , and W V\na represent the weight matrices\nfor the query (Q), key (K), and value (V), respectively.\nCrossAttn(·) denotes the cross-attention layer, and d is\nthe dimension of the image features. In this way, it explic-\nitly assigns action information at at time step t to the visual\nfeatures ˆvt, enhancing the causal relationship between ob-\nservations and actions.\nSubsequently, we introduce a History Aggregator mod-\nule to capture the information of the observation-action se-\nquence along the temporal dimension, serving as the behav-\nior representation. At each timestep t, behavior tokens Bt\nserve as queries, while the sequence of historical behavior\ntokens Ht = [B1, B2, . . . , Bt−1] acts as keys and values.\nThe current behavior tokens interact with the historical se-\nquence through a history-attention layer HisAttn(·):\nˆBt = HisAttn(Q, K, V ) = Softmax(QKT\n√\nd\n)V\n(5)\nwhere Q, K, and V are calculated similarly to Eq 3.\nFinally, another cross-attention layer is introduced, using\nthe behavior tokens ˆBt as queries, and the visual features ˆvt\nas keys and values. In this way, the behavior tokens incor-\nporate the current observation-action information. Follow-\ning the approach of He et al. [14], we construct a memory\nbank for historical behavior tokens Ht, utilizing the simi-\nlarity between adjacent features to aggregate and compress\nthe behavior tokens. This method not only preserves early\nhistorical information but also keeps the historical behav-\nior token sequence Ht at a fixed length to reduce compu-\ntational costs. Leveraging the Action-guided Behavior En-\ncoder, we obtain behavior tokens ˆBt, which correspond to\nthe observation-action sequence from the start to the current\ntime step t.\n4.2.2. MLLM Backbone\nTo model the relationship between the sub-goal and\nobservation-action sequence, we introduce an MLLM that\ntakes the sub-goal g, current observation features vt, and\nbehavior tokens Bt as input to predict subsequent actions\nauto-regressively. To enable the MLLM backbone MLLM\nto predict low-level actions, we employ VPT [1] as action\nhead AH to map output embeddings ¯at+1 of language model\ninto the action space.\n¯at+1 ←MLLM([g, vt, Bt])\n(6)\nat+1 ←AH(¯at+1)\n(7)\nFormally, given a dataset D = {(o1:T , a1:T )}M with M\ncomplete trajectories, we train GOAP to learn the behavior\ndistribution from D via behavioral cloning. Moreover, we\nintroduce a KL-divergence loss to measure the output dis-\ntribution similarity between GOAP and VPT [1]. This helps\nour model effectively learn the knowledge from the teacher\nmodel VPT. The training loss can be formulated as follows:\nLθ = λBC\nT\nX\nt=1\n−log pθ(at+1|o1:t, a1:t, g)\n+λKL\nT\nX\nt=1\nDKL(qϕ(at+1|o1:t) ∥pθ(at+1|o1:t, g))\n(8)\nwhere λBC and λKL are trade off coefficients, pθ is the\nGOAP, qϕ is the teacher model.\nTable 2. Main Result of GOAP on Atomic Tasks. We report the\naverage rewards of each task.\nPolicy\nLogs\nSeeds\nDirt\nStone\nVPT (text) [1]\n2.6\n0.8\n9.2\n0.0\nSTEVE-1 [25]\n11.0\n5.1\n10.0\n3.2\nGROOT [3]\n14.3\n7.3\n19.7\n19.0\nFSQ GROOT [43]\n10.8\n8.2\n20.3\n5.8\nGOAP [MLP ]\n7.2\n4.3\n14.4\n15.5\nGOAP [V P T ]\n15.0\n8.5\n26.7\n25.7\n4.3. MGOA Dataset\nIn Minecraft, there remains a significant lack of high-\nquality goal-observation-action pairs to support behavior\ncloning training.\nPrevious work has primarily relied on\ngameplay videos as training data. These datasets either lack\nnatural language instructions (explicit goals) [1, 3], or use\nactions predicted by IDM models [1] for each observation\nas pseudo-labels [1, 25], which leads to a risk of misalign-\nment between observations and actions. Inspired by Li et\nal. [24], we propose an automated data generation pipeline\nthat enables the creation of aligned goal-observation-action\npairs without the need for manual annotations or human\ncontractors. First, we utilize existing agents [25], provid-\ning them with clear natural language instructions to attempt\ntask completion in Minecraft. We then record the actions\nand corresponding observations during goal execution, gen-\nerating goal-observation-action pairs.\nTo ensure the quality of the generated data, we apply\nthe following filtering criteria: 1) only recording videos\nin which the task is successfully completed, and 2) dis-\ncarding videos where task execution takes an excessive\namount of time.\nFor more details, please refer to Sup.\nC. Through this automated approach, we obtained 25k\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndataset. A comparison of the MGOA dataset with the exist-\ning Minecraft datasets is shown in Table 1. Our automated\ndata generation pipeline offers several key advantages: 1)\nit enables the generation of aligned goal-observation-action\npairs without the need for manual annotation or pseudo-\nlabeling; 2) its construction process is parallelizable, allow-\ning for rapid dataset generation; and 3) it leverages local\nagents for data generation, resulting in low-cost production.\n5. Experiments\n5.1. Experiments Setting\nEnvironment. Following [1, 25], we conduct experiments\nin the complex, open-world environment of Minecraft on\nthe MineRL [12] platform. The agent interacts with the\nMineRL environment at 20 frames per second, generating\nlow-level control signals for the mouse and keyboard. For\neach task execution, the agent is initialized in a randomized\nTable 3. Main Result of Optimus-2 on Long-horizon Tasks. We report the average success rate (SR) on each task group, the results of each\ntask can be found in the Sup. F.1. Pure GPT-4V† denotes the use of GPT-4V in a zero-shot manner to generate executable sub-goals for\nthe policy. Human‡ denotes the human-level baseline, with results sourced from previous work [24].\nMethod\nPolicy\nWood\nStone\nIron\nGold\nDiamond\nRedStone\nArmor\nPure GPT-4V †\nVPT (text)\n0.22\n0.08\n0.00\n0.00\n0.00\n0.00\n0.00\nSTEVE-1\n0.41\n0.20\n0.00\n0.00\n0.00\n0.00\n0.00\nGOAP\n0.50\n0.31\n0.12\n0.02\n0.01\n0.03\n0.03\nDEPS [41]\nSTEVE-1\n0.77\n0.48\n0.16\n0.00\n0.01\n0.00\n0.10\nJarvis-1 [42]\nSTEVE-1\n0.93\n0.89\n0.36\n0.07\n0.08\n0.16\n0.15\nOptimus-1 [24]\nSTEVE-1\n0.98\n0.92\n0.46\n0.08\n0.11\n0.25\n0.19\nOptimus-2\nGOAP\n0.99\n0.93\n0.53\n0.09\n0.13\n0.28\n0.21\nHuman‡ [24]\n-\n1.00\n1.00\n0.86\n0.17\n0.16\n0.33\n0.28\nTable 4.\nMain Result of GOAP on Open-Ended Instruction\nTasks. We report the average success rate (SR) on Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nGROOT [3] and FSQ GROOT [43] were not included as base-\nlines, as they are unable to process language input.\nPlanner\nPolicy\nGLM-4V\nVPT (text)\n0.05\n0\n0\n0\n0\nSTEVE-1\n0.60\n0\n0\n0\n0\nGOAP\n0.71\n0.39\n0.11\n0.14\n0.13\nGPT-4V\nVPT (text)\n0.11\n0\n0\n0\n0\nSTEVE-1\n0.66\n0.10\n0\n0\n0\nGOAP\n0.75\n0.47\n0.13\n0.16\n0.17\nenvironment, allowing us to evaluate the agent’s generaliza-\ntion across diverse environments. Please refer to Sup. B for\nmore details about the Minecraft environment.\nImplementation details. For the planner, we follow Li et\nal. [24], using a hybrid multimodal memory empowered\nGPT-4V 1 as the agent’s planner. As for the policy, we ini-\ntialize GOAP with the weights of DeepSeek-VL-1.3B [29]\nas initialization. We train it on the MGOA dataset and the\npublicly available OpenAI Contractor Dataset [1] through\nbehavior cloning. All experiments were conducted on 8x\nNVIDIA L40 GPUs. Training details and hyperparameter\nsetting can be found in Sup. D.\nEvaluation Tasks & Metrics. Evaluation tasks are catego-\nrized into three types: Atomic Tasks, Long-Horizon Tasks,\nand Open-Ended Instruction Tasks. For each task, the en-\nvironment is randomly reinitialized on each attempt, with a\nminimum of 30 executions per task to ensure robustness.\n• Atomic Tasks represent short-term skills in Minecraft. We\nselect “chop a tree to get logs\n”, “collect seeds\n”,\n“collect dirt\n”, and “mine stone\nwith a pickaxe” as\nevaluation tasks. These tasks evaluate the policy’s basic\ncapabilities in Minecraft. We report the average rewards\n1https:\/\/openai.com\/index\/gpt-4v-system-card\n(number of items obtained) per task execution as an eval-\nuation metric.\n• Long-horizon Tasks consist of an interdependent atomic\ntasks sequence, where the failure of any single atomic\ntask results in the failure of the entire sequence. These\nlong-horizon tasks are designed to evaluate the agent’s ca-\npability to execute a series of diverse tasks continuously\nwithin a complex environment. We follow the setup of Li\net al. [24], conducting experiments on long-horizon tasks\ncomprising 67 tasks grouped into 7 categories. We report\nthe average Success Rate (SR) as an evaluation metric.\n• Open-Ended Instruction Tasks are not limited to prede-\nfined text formats; rather, they involve flexible language\ndirectives that prompt the agent to accomplish long-\nhorizon tasks. These tasks evaluate the agent’s capacity\nto interpret and execute instructions expressed in open-\nended natural language. We selected the Torch\nfrom\nthe Stone Group, Rail\nfrom the Iron Group, Golden\nShovel\nfrom the Gold Group, Diamond Pickaxe\nfrom the Diamond Group, and Compass\nfrom the Red-\nstone Group as evaluation tasks. Given a crafting rela-\ntionship graph, we instructed GPT-4V and GLM-4V [11]\nto generate five open-ended instructions for each task.\nThis allows us to evaluate the policies’ understanding and\nexecution capabilities regarding open-ended instructions.\nTask instructions are provided in the Sup. E.1.\nBaseline. For Atomic Tasks and Open-ended Instruction\nTasks, we compare GOAP with existing goal-conditioned\npolicies, including VPT [1], STEVE-1 [25], GROOT [3]\nand FSQ GROOT [43]. For Long-horizon Tasks, we employ\nGPT-4V, DEPS [41], Jarvis-1 [42], and Optimus-1 [24] as\nbaselines. We also introduce a human-level baseline [24] to\nevaluate the performance gap between existing agents and\nhuman capabilities.\n5.2. Experimental Results\nThe experimental results for Optimus-2 compared to the\nbaselines across Atomic Tasks, Long-horizon Tasks, and\nAgent\nInstruction: I need some iron ores, what should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 3. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need some iron ores,\nwhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nTable 5. Ablation study of Action-guided Behavior Encoder on\nAtomic Tasks. We report average rewards on each task. CP., HA.,\nand MB. represent the Causal Perceiver, History Aggregator, and\nMemory Bank, respectively.\nAblation Setting\nAtomic Task\nCP.\nHA.\nMB.\nLogs\nSeeds\nDirt\nStone\nAverage\n!\n!\n!\n15.0\n8.5\n26.7\n25.7\n19.0\n6.1\n5.4\n12.7\n15.7\n10.0 (↓47.4%)\n!\n10.2\n4.7\n12.8\n21.1\n12.2 (↓35.8%)\n!\n!\n7.4\n6.2\n13.1\n15.5\n10.6 (↓44.2%)\n!\n!\n12.0\n6.8\n22.7\n16.8\n14.6 (↓23.2%)\nOpen-ended Instruction Tasks are presented in Table 2, Ta-\nble 3, and Table 4, respectively.\nGOAP excels in Atomic Tasks. Table 2 shows that pro-\nposed GOAP achieves improvements of 5%, 4%, 31%, and\n35% over the current SOTA on the Logs\n, Seeds\n, Dirt\n, and Stone\n, respectively. These results demonstrate\nthat GOAP has successfully mastered a range of short-term\nskills across diverse environments, and can acquire items\nmore effectively than existing policies.\nOptimus-2 surpasses SOTA in Long-horizon Tasks.\nTable 3 shows that Optimus-2 achieved the highest success\nrates across all seven task groups, particularly excelling in\nthe challenging Diamond Group and Redstone Group with\nsuccess rates of 13% and 28%, respectively. This indicates\nthat Optimus-2 has effectively learned complex behavior\npatterns across atomic tasks, enabling it to sequentially ex-\necute multiple sub-goals and successfully complete long-\nhorizon tasks within complex environments.\nGOAP\noutperforms\nin\nOpen-ended\nInstruction\nTasks. As shown in Table 4, GOAP achieved significantly\nhigher success rates than existing agents across all tasks.\nNotably, on the challenging tasks of Golden Shovel\n, Di-\namond Pickaxe\n, and Compass\n, existing policies fail to\ncomplete these tasks, whereas GOAP achieves success rates\nof 13%, 16%, and 17%, respectively. This advantage stems\nSuccess Rate\n0.0\n5.0\n10.0\n15.0\nGolden \nShovel\nDiamond \nPickaxe\nCompass\nLLM \n20.0\n17.1\n13.1\n16.2\nTransformer-XL\n0.5\n0.5\n0.0\nFigure 4. Ablation of LLM backbone on Open-ended Instruction\nTasks, Golden Shovel\n, Diamond Pickaxe\n, and Compass\n.\nfrom GOAP’s superior comprehension of open-ended nat-\nural language instructions, whereas existing agents exhibit\nweaker instruction-following capabilities. Moreover, Fig-\nure 3 illustrates an example of different policies executing\nan open-ended goal. Due to the limited representation ca-\npability of their goal encoders, VPT [1] and STEVE-1 [25]\nfail to understand the goal, “I need some iron ores, what\nshould I do?” In contrast, GOAP leverages the MLLM’s\nunderstanding of open-ended instructions to effectively ac-\ncomplish the goal (obtaining iron ore\n).\n5.3. Ablation Study\nThere are many unexplored questions around best practices\nfor developing MLLM-based policy in Minecraft. In this\nsection, we conduct an extensive ablation study and sum-\nmarize our key findings.\nThe Action-guided Behavior Encoder plays a crucial\nrole in task execution. As shown in Table 5, the removal\nof the Causal Perceiver leads to an average performance de-\ncline of 42% across all tasks, highlighting the importance\nof capturing the causal relationship between observations\nand actions. Moreover, eliminating the History Aggregator\nAverage Rewards\n0.0\n5.0\n10.0\n15.0\n25.0\n30.0\nLogs\nSeeds\nDirt\nStone\nOCD_MGOA_Mix \n20.0\n26.7\n25.7\n6.0\n15.0\n8.5\n2.3\n14.3\n2.6\nOCD\n2.3\n1.4\n17.1\n22.1\nMGOA\nFigure 5. Ablation study on Training data. OCD refers to the\nOpenAI Contractor Dataset [1]. We report the average rewards on\neach Atomic Task.\nand Memory Bank also results in an average performance\ndecline of 36% across all tasks. This emphasizes the cru-\ncial role of the History Aggregator in modeling observation-\naction sequences and the Memory Bank in dynamically\nstoring long-sequence information.\nLLM significantly enhances policy’s ability to under-\nstand open-ended instructions. As shown in Figure 4, re-\nplacing the LLM backbone with a Transformer-XL leads to\na noticeable decline in performance. We attribute this to\nthe LLM’s pretraining on large-scale textual corpora, which\nendows it with a robust comprehension of open-ended lan-\nguage, a capability that Transformer-XL lacks.\nA pretrained action head improves performance in\nMinecraft. As shown in Table 2, replacing VPT with a\n2-layer MLP projector as the action head leads to a no-\nticeable decline in Optimus-2’s performance. While MLP-\nbased action heads have shown promising results in other\ndomains [17, 27], this substitution is less effective in the\nMinecraft environment. We attribute this to VPT’s exten-\nsive pretraining on large-scale gameplay data, which equips\nit with substantial domain-specific knowledge critical for\neffective task execution in Minecraft.\nThe MGOA datsaset is beneficial for training GOAP.\nWe conducted comparative experiments to evaluate the im-\npact of different training datasets on performance.\nAs\nshown in Figure 5, training only with the current most com-\nmonly used dataset, OpenAI Contractor Dataset (OCD), re-\nsults in suboptimal performance for GOAP on all Atomic\nTasks.\nFor example, compared to training with a mixed\ndataset, its performance on Stone\ndropped by 89%. We\nattribute this to the fact that OCD offers a wide variety of\ntasks but lacks high data quality.\nIn contrast, using our\nMGOA dataset, performance on the four atomic tasks im-\nproved by an average of 70% compared to using only the\nOCD data. We attribute this to the fact that MGOA contains\nhigh-quality aligned goal-observation-action pairs, which\nis beneficial for policy training. Further, we mix the two\ndatasets to train the policy in order to balance task diversity\nLog\nDirt\nSeed\nStone\n(a) ViT\n(c) Action-guided\n          Behavior Encoder\n(b) MineCLIP\nFigure 6. t-SNE visualization of representations extracted by (a)\nViT (b) MineCLIP and (c) Action-guided Behavior Encoder across\nAtomic Tasks. The visualization results show that the represen-\ntations in (a) and (b) cannot distinguish between different tasks,\nwhereas our Action-guided Behavior Encoder clearly differenti-\nates the behavior representations for the four tasks.\nand data quality, leading to improved performance.\n5.4. Visualization of Behavior Representation\nAs shown in Figure 6, we apply t-SNE [39] to visualize\nobservation features extracted by ViT [9], MineCLIP [10],\nand the Action-guided Behavior Encoder for four tasks.\nFrom (a) and (b) in Figure 6, it is evident that the behavior\nrepresentations extracted by ViT and MineCLIP are highly\nmixed, making it challenging to delineate the boundaries\nbetween different tasks. This lack of clear distinction be-\ntween task-specific behavior representations can hinder the\nmodel’s ability to understand the unique behavior patterns\nassociated with each task, potentially leading to task fail-\nure. In contrast, the visualization in (c) of Figure 6 reveals\nclear, distinct clusters for each task, demonstrating that the\nAction-guided Behavior Encoder effectively captures subtle\ndifferences in observation-action sequences, thereby learn-\ning robust behavior representations across tasks.\n6. Conclusion\nIn this paper, we propose a novel agent, Optimus-2, which\ncan excel in various tasks in the open-world environment of\nMinecraft. Optimus-2 integrates an MLLM for high-level\nplanning and a Goal-Observation-Action conditioned Pol-\nicy (GOAP) for low-level control. As a core contribution\nof this paper, GOAP includes an Action-guided Behavior\nEncoder to model the observation-action sequence and an\nMLLM to align the goal with the observation-action se-\nquence for predicting subsequent actions.\nExtensive ex-\nperimental results demonstrate that GOAP has mastered\nvarious atomic tasks and can comprehend open-ended lan-\nguage instructions. This enables Optimus-2 to achieve su-\nperior performance on long-horizon tasks, surpassing ex-\nisting SOTA. Moreover, we introduce a Minecraft Goal-\nObservation-Action dataset to provide the community with\nlarge-scale, high-quality data for training Minecraft agents.\nOptimus-2\n: Multimodal Minecraft Agent with Goal-Observation-Action\nConditioned Policy\nSupplementary Material\nThe supplementary document is organized as follows:\n• Sec. A: Limitation and Future Work.\n• Sec. B: Minecraft Environment.\n• Sec. C: MGOA Dataset.\n• Sec. D: Training Details.\n• Sec. E: Evaluation Benchmark.\n• Sec. F: Experimental Results.\n• Sec. G: Case Study.\nA. Limitation and Future Work\nIn this paper, we aim to explore how agents can mimic\nhuman behavior patterns in Minecraft to accomplish vari-\nous tasks. Experimental results demonstrate that Optimus-\n2 performs exceptionally well in both atomic tasks and\nlong-horizon tasks. However, due to the lack of sufficient\nhigh-quality data for open-ended tasks (such as “building a\nhouse” and “defeating the Ender Dragon”), there remains\nsignificant room for improvement. Once such datasets are\navailable, the ability of Optimus-2 to complete open-ended\ntasks will be enhanced. Moreover, despite showing promis-\ning performance in Minecraft, we have not yet extended our\nexploration to other simulation platforms, which represents\na potential direction for future research.\nB. Minecraft\nMinecraft is an extremely popular sandbox video game de-\nveloped by Mojang Studios 2. It allows players to explore\na blockly, procedurally generated 3D world with infinite\nterrain, discover and extract raw materials, craft tools and\nitems, and build structures or earthworks. In this enviro-\nment, AI agents need to face situations that are highly sim-\nilar to the real world, making judgments and decisions to\ndeal with various environments and problems. As shown\nin Figure 7, both agents and humans are required to receive\nnatural language instructions and current observations as in-\nput, and then output low-level actions, such as mouse and\nkeyboard control commands. Therefore, Minecraft serves\n2https:\/\/www.minecraft.net\/en-us\/article\/meet-mojang-studios\n...\nObservation\n...\nAgent Action\nHuman Action\nspace\nW\nW\nKeyboard:\n {forward}\nMouse:{(2.7, 1.5)}\nKeyboard: \n{forward,jump}\nMouse: {(1.2, 0)}\nMouse: {attack}\nTask: chop a tree to get logs\nLanguage\nFigure 7.\nIllustration of behavior patterns of both human and\nagents in Minecraft.\nas an ideal open-world environment for training agent that\ncan learn human behavior patterns.\nB.1. Basic Rules\nBiomes. The Minecraft world is divided into different areas\ncalled “biomes”. Different biomes contain different blocks\nand plants and change how the land is shaped. There are 79\nbiomes in Minecraft 1.16.5, including ocean, plains, forest,\ndesert, etc. Diverse environments have high requirements\nfor the generalization of agents.\nItem. In Minecraft 1.16.5, there are 975 items can be ob-\ntained, such as wooden pickaxe\n, iron sword\n. Item can\nbe obtained by crafting or destroying blocks or attacking\nentities. For example, agent can attack cows\nto obtain\nleather\nand beef\n. Agent also can use 1 stick\nand 2\ndiamonds\nto craft diamond sword\n.\nTechnology Tree.\nIn Minecraft, the technology hierar-\nchy comprises six levels: wood\n, stone\n, iron\n, gold\n, diamond\n, and redstone\n.\nEach tool level corre-\nsponds to specific mining capabilities. Wooden tools can\nmine stone-level blocks but are incapable of mining iron-\nlevel or higher-level blocks.\nStone tools can mine iron-\nlevel blocks but cannot mine diamond-level or higher-level\nblocks.\nIron tools are capable of mining diamond-level\nblocks. Finally, diamond tools can mine blocks of any level,\nincluding redstone-level.\nGameplay progress. Progression in Minecraft primarily\ninvolves discovering and utilizing various materials and re-\nsources, each unlocking new capabilities and opportunities.\nFor instance, crafting a wooden pickaxe\nenables players\nTable 6. Action space of agent in Minecraft.\nIndex\nAgent Action\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove back.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current movement.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPlace blocks, entity, open items or other interact actions defined by game.\n10\nhotbar [1-9]\nkeys 1-9\nSelects the appropriate hotbar item.\n11\nOpen\/Close Inventory\nkey E\nOpens the Inventory. Close any open GUI.\n12\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nto mine stone\n, which can then be used to create a stone\npickaxe\nand a furnace\n. These tools allow for the mining\nand smelting of iron ore\n. Subsequently, crafting an iron\npickaxe\nenables the extraction of diamonds\n, while a di-\namond pickaxe\ncan mine virtually any block in the game.\nSimilarly, cultivating crops facilitates breeding various an-\nimals, each providing unique resources beyond sustenance.\nDrops from enemies also serve specific purposes, with some\noffering greater utility than others. By integrating resources\nfrom mining, farming, and breeding, players can enchant\ntheir equipment, further enhancing their capabilities. Addi-\ntionally, collecting and crafting materials support construc-\ntion, enabling players to create diverse structures. Beyond\npractical functions, such as building secure bases or farms,\nconstructing personalized structures forms a significant as-\npect of the Minecraft experience. Figure 11 illustrates an\nexample of progression: crafting an iron sword\n.\nB.2. Observation and Action Spaces\nObservation. In this paper, observation space of agent is\ncompletely consistent with human players. The agent only\nreceives an RGB image with dimensions of 640 × 360 dur-\ning the gameplay process, including the hotbar, health in-\ndicators, food saturation, and animations of the player’s\nhands. It is worth helping the agent see more clearly in\nextremely dark environments, we have added a night vision\neffect for the agent, which increases the brightness of the\nenvironment during the night.\nAction Spaces. In MineRL [12] environment, agent’s ac-\ntion space is almost similar to human players. It consists of\ntwo parts: the mouse and the keyboard. The keypresses are\nLog\n48.1%\nSeed\n6.0%\nDirt\n6.0%\nStone\n8.2%\nIron\n8.1%\nGold\n8.0%\nDiamond\n7.9%\nRedstone\n7.7%\nFigure 8. Statistical information on MGOA dataset. It contains 8\nAtomic Tasks: ‘Log\n’, ‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’,\n‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’.\nresponsible for controlling the movement of agents, such as\njumping, forward, back, etc. The mouse movements are re-\nsponsible for controlling the perspective of agents and the\ncursor movements when the GUI is opened. The left and\nright buttons of the mouse are responsible for attacking and\nInstruction\nPool\nFiltering\ninteract\nfeedback\nGPT-4\nScript\nEnvironment\ncollect logs\nkeyboard: W\nmouse: [0.0, 1.0]\nMGOA Dataset\nInstruction\nItems\nFigure 9. The pipeline for generating the MGOA dataset. First, we extracted item names from the Minecraft Wiki and employed GPT-\n4 to generate corresponding instructions. These instructions were then provided as input to STEVE-1, enabling it to interact with the\nenvironment to accomplish the tasks. During task execution, each observation was paired with its corresponding action, resulting in the\ncreation of goal-observation-action pairs.\nusing or placing items. In Minecraft, precise mouse move-\nments are important when completing complex tasks that\nneed open inventory or crafting table. In order to achieve\nboth the same action space with MineDojo [10], we abstract\nthe craft and the smelt action into action space. The detailed\naction space is described in Table 6.\nC. MGOA Dataset\nIn Minecraft, there is still a lack of sufficient high-\nquality goal-observation-action pairs to support the train-\ning of Optimus-2.\nTo address this, we propose an au-\ntomated dataset construction process aimed at creating\nhigh-quality Minecraft Goal-Observation-Action (MGOA)\ndatasets. Through this method. MGOA contains 25,000\nvideos, providing about 30M goal-observation-action pairs.\nIt contains 8 Atomic Tasks across 5 tech levels: ‘Log\n’,\n‘Seed\n’, ‘Dirt\n’, ‘Stone\n’, ‘Iron\n’, ‘Gold\n’, ‘Diamond\n’, ‘Redstone\n’. Note that the Atomic Tasks in MGOA re-\nquire minimal steps and can typically be completed within\n2 ∼3 minutes. For instance, the task ‘Iron\n’ involves min-\ning iron with a stone pickaxe, without the need to gather raw\nmaterials to craft the stone pickaxe. The statistics for the\nMGOA dataset is shown in Figure 8. We provide several ex-\namples of the dataset in the MGOA samples folder within\nthe supplementary materials. We will release this dataset to\ncontribute to the development of open-world agents within\nthe community.\nC.1. Dataset Construction\nPipeline. Inspired by Li et al. [24], we employed a prior\npolicy (STEVE-1 [25] in our work) to perform specific tasks\nin Minecraft, and recorded the corresponding videos and\nactions to generate goal-observation-action pairs. As illus-\ntrated in Figure 9, we employed a custom script to extract\nitem names from the Minecraft Wiki3. Using these item\nnames, we queried GPT-44 with a predefined prompt tem-\nplate to generate task instructions, thereby constructing an\nInstruction Pool. The task instructions from the Instruc-\ntion Pool serve as input to STEVE-1 [25], enabling it to\ninteract with the environment to complete the tasks. During\ntask execution, each frame and corresponding action were\nrecorded and stored. To expedite data generation, we instan-\ntiated multiple policies and used parallelization to quickly\nproduce large amounts of data.\nData Filtering. We judged task success based on environ-\nmental feedback. For example, feedback like “obtained new\nitem, diamond axe” indicated that the task “craft a diamond\naxe” was successfully completed. During the dataset gen-\neration process, we observed a significant amount of low-\nquality video data due to limitations in the policy’s abil-\nity to follow instructions. Examples of low-quality data in-\ncluded task failures or task completion timeouts. To address\nthis issue, we established two filtering criteria to ensure data\nquality: (1) only retaining data from successfully completed\ntasks, and (2) removing data for tasks that lasted longer than\n2 minutes. These criteria allowed us to automatically filter\nout low-quality data, significantly reducing the cost of con-\nstructing the dataset. As a result, we obtained a high-quality\nMGOA dataset consisting of 25,000 samples.\n3https:\/\/minecraft.wiki\/\n4https:\/\/openai.com\/index\/gpt-4-research\/\nC.2. Comparison with Existing Datasets\nPrevious gameplay videos were primarily obtained through\ntwo methods below.\nVideo Platform: For example, MineDojo [10] collected\ngame videos uploaded by human players on platforms such\nas YouTube and Twitter, combining the video content with\ncorresponding titles or subtitles to form video-text pairs.\nHowever, this dataset lacked recorded actions. To address\nthis, VPT [1] used an Inverse Dynamics Model (IDM) to\ngenerate action sequences from the videos. However, the\nactions predicted by the IDM model are only approxima-\ntions, which introduces a potential risk of misalignment be-\ntween the frames and the corresponding actions.\nHuman Contractors: VPT [1] hired human players to\nfreely explore Minecraft and used the frames and actions\nto construct a video-action dataset. However, this dataset\nlacked explicit natural language instructions.\nTo create\ngoal-observation-action pairs, STEVE-1 [25] used GPT-3.5\nto generate specific task descriptions based on the game-\nplay, thereby integrating natural language instructions into\nthe dataset. However, they provide only approximately 32k\naligned goal-observation-action pairs, which remains a rel-\natively scarce amount of data.\nIn addition, some work [32, 43] have utilized GPT-4V\nto generate image captions, task planning, and reflections,\nthereby creating image-text pairs that form instruction-\nfollowing datasets.\nDistinct from the aforementioned datasets, the MGOA\ndataset directly captures agents performing specific tasks,\noffering clear natural language instructions with a one-to-\none correspondence between observations and actions. Fur-\nthermore, through rigorous data filtering, redundant action\nsequences that do not contribute to task completion are ex-\ncluded from MGOA. In addition, compared to the small-\nscale goal-observation-action datasets currently available,\nMGOA offers 25,000 videos, encompassing approximately\n30 million goal-observation-action pairs. This dataset is not\nonly significantly larger but also highly scalable in an auto-\nmated manner.\nD. Training Details\nD.1. Training Pipeline\nOne of the key factors in implementing our proposed\nmethod lies in the efficient alignment of language with the\nobservation-action sequence, and subsequently translating\nlanguage space into the action space. To tackle this prob-\nlem, we adopt a two-phase training approach. First, we\nalign language with the observation-action sequence via be-\nhavior pre-training. Then, we transform the language space\ninto the action space through action fine-tuning.\nBehavior Pre-training: During the pre-training phase, we\nintegrated the Vision-guided Behavior Encoder into the\nTable 7. Hyperparameter setting for pre-training and finetuning.\nHyperparameter\nPre-training\nFine-tuning\nOptimizer\nAdamW\nAdamW\nLearning Rate\n0.0001\n0.00004\nWarmup Steps\n0\n0\nEpochs\n5\n10\nBatch Size\n32\n2048\nNum. Frames\n5M\n80M\nLoRA r\n64\n64\nLoRA alpha\n128\n128\nmodel.\nWe used OpenAI Contractor Dataset [1] and a\nsubset of MGOA as training data, which comprised ap-\nproximately 5,000 videos. To balance efficiency and ef-\nfectiveness, we freeze the visual encoder, then tune the\nVision-guided Behavior Encoder along with a large lan-\nguage model (LoRA [15]). During pre-training, we set the\nlearning rate to 0.0001 and trained for 5 epochs. The hyper-\nparameter settings are shown in Table 7.\nAction Fine-tuning:\nDuring the fine-tuning phase, we\nadapted the general MLLM DeepSeek-VL-1.3B [29] to the\nMinecraft environment, transitioning the model’s output\nspace from language to low-level actions. We fine-tuned\nit using OpenAI Contractor Dataset [1] and MGOA, which\ncomprises approximately 20,000 videos. In this phase, we\nfreeze the Vision-guided Behavior Encoder, visual encoder,\nand large language model (LoRA), and only fine-tuned the\naction head. During fine-tuning, we set the learning rate to\n0.00004 and train for 10 epochs. The hyperparameter set-\ntings are shown in Table 7.\nD.2. Implementation Details\nFor the planner, we follow Li et al. [24], employing Mul-\ntimodal Hybrid Memory empowered GPT-4V for planning\nand reflection. For the policy, we train the GOAP through\nthe above pipeline. All experiments were conducted on 8x\nNVIDIA L40 GPUs. For the MGOA dataset, data collec-\ntion and filtering were conducted in parallel, taking approx-\nimately 7 days.\nTraining required around 2 days, while\ninference and evaluation on atomic tasks, long-horizon\ntasks, and open-ended instruction tasks took approximately\n4 days.\nE. Benchmark\nE.1. Evaluation Tasks\nThe evaluation tasks are divided into three categories:\nAtomic Tasks, Long-horizon Tasks, and Open-ended In-\nstruction Tasks. For each task, the agent’s environment is\n(a) chop a tree to get logs\n(b) mine dirt\n(c) collect seeds\n(d) dig down to mine stone\nFigure 10. Examples of Atomic Task. The agent must follow the instructions to collect resources. These four tasks represent the basic\ncapabilities of the agent. The more resources are collected, the stronger the basic capabilities of the agent will be.\nrandomly initialized each time, and every task is executed\nat least 30 times. For Atomic Tasks, we follow the setting of\nprior work [25, 43], which requires the agent to execute the\ntask within 2 minutes. We then report the average reward\nfor the task, defined as the number of items obtained. For\nOpen-ended Instruction Tasks and Long-horizon Tasks, we\nreport the average success rate (SR) for each task.\nAtomic Tasks. As shown in Figure 10, Atomic Tasks are\nshort-term skills in Minecraft, such as “chop a tree to get\nlogs\n”, “mine dirt\n”, “collect seeds\n”, and “dig down to\nmine stone\n”, etc.\nLong-horizon Tasks.\nAs shown in Figure 11, Long-\nHorizon Tasks are a sequence of Atomic Tasks. For exam-\nple, “craft an iron sword from scratch” requires complet-\ning the atomic tasks of “chop 7 logs”, “craft 21 planks”,\n“craft 5 sticks”, “craft 1 crafting table”, and so on. These\nAtomic Tasks are interdependent, meaning that the failure of\nany single atomic task will result in the failure of the entire\nLong-horizon Task.\nOpen-ended Instruction Tasks. Open-Ended Instruction\nTasks are not limited to predefined text formats; rather, they\ninvolve flexible language directives that prompt the agent\nto accomplish long-horizon tasks. These tasks evaluate the\nagent’s capacity to interpret and execute instructions ex-\npressed in open-ended natural language. We selected Torch\n, Rail\n, Golden Shovel\n, Diamond Pickaxe\n, and\nCompass\nas evaluation tasks. Instruction for each task\nare shown in Table 8, Table 9, Table 10, Table 11 and Table\n12.\nE.2. Baselines\nIn this section, we provide a brief overview of existing\nMinecraft agents and compare them with our proposed\nOptimus-2. Current agents can be broadly categorized into\ntwo types: policy-based agents and planner-policy agents.\nPolicy-based Agents. Policy-based agents [1–3, 10, 25] re-\nfer to those trained through reinforcement learning or imi-\ntation learning, capable of completing atomic tasks within\nMinecraft. However, due to limitations in instruction un-\nderstanding and reasoning abilities, they struggle to accom-\nplish long-horizon tasks.\nPlanner-Policy Agents. Planner-policy agents [20, 24, 32,\n41–43] refer to non-end-to-end architectures that utilize a\nMLLM (Multi-Layered Language Model) as a planner to\ndecompose complex instructions into a sequence of sub-\ngoals executable by a policy. While significant progress has\n(a) Chop 7 logs\n(b) Craft 21 planks\n(c) Craft 5 sticks\n(d) Craft 1 crafting table\n(e) Craft 1 wooden pickaxe\n(f) Mine 11 cobblestone\n(g) Craft 1 furnace\n(h) Craft 1 stone pickaxe\n(i) Dig down more deeper to find iron ore\n(j) Mine 2 iron ores\n(k) Smelt 2 iron ingots\n(l) Craft 1 iron sword\nFigure 11. An example of long-horizon task “crafting an iron sword”. The agent must sequentially complete each atomic task in order to\nsuccessfully craft the iron sword. Failure in any of the atomic tasks will result in the failure of the entire long-horizon task.\nbeen made, the current performance bottleneck stems from\nthe policy’s ability to effectively understand and execute the\nsub-goals generated by the planner.\nComparison with Existing Agents. As a core contribu-\ntion of this work, we propose a novel Goal-Observation-\nAction Conditioned Policy, GOAP. It integrates two key\ncomponents: an Action-Guided Behavior Encoder for mod-\neling observation-action sequences, and an MLLM for\naligning sub-goals with these sequences. Leveraging the\nMLLM’s advanced understanding of open-ended instruc-\ntions, GOAP demonstrates superior instruction-following\ncapabilities compared to existing policies. On top of GOAP,\nthe proposed agent, Optimus-2, exhibits superior perfor-\nmance in long-horizon tasks, outperforming the current\nstate-of-the-art across all seven task groups.\nF. Experimental Results\nIn this section, we report the experimental results of\nOptimus-2 on each Long-horizon task.\nF.1. Results on Long-horizon Task\nIn this section, we report the results of Optimus-2 on each\nLong-horizon Task, with details including task name, num-\nbers of sub-goals, success rate (SR), and eval times. As\nshown in Tables 13 and 14, Optimus-2 demonstrates supe-\nrior performance across all 67 Long-horizon Tasks. Since\nOptimus-2 is randomly initialized in arbitrary environments\nfor each task execution, the experimental results also high-\nlight its generalization capability across diverse environ-\nments.\nG. Case Study\nIn this section, we provide additional cases to illustrate the\ndifferences in the ability of VPT (text) [1], STEVE-1 [25],\nand Optimus-2 to perform Open-ended Instruction Tasks.\nWe provide different open-ended instructions requiring the\nagent to perform tasks across various biomes. As shown in\nFigure 12, Figure 13, and Figure 14, Optimus-2 effectively\ncompletes all tasks, while VPT (text) and STEVE-1 fail due\nto limitations in language understanding and multimodal\nperception capabilities. Moreover, we provide several demo\nvideos of Optimus-2 performing long-horizon tasks in the\nOptimus2 videos folder within the supplementary ma-\nterials.\nTable 8. Open-ended instruction examples of “Craft a torch”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, I need to locate and chop down a couple of trees to collect 4 logs.\n2. Next, I should craft these logs into 13 planks. 3. Then, I need to use some of the\nplanks to craft 3 sticks. 4. I will use some of the planks to craft a crafting table. 5.\nNext, I should craft a wooden pickaxe using some of the planks and sticks. 6. With\nthe wooden pickaxe, I need to find and mine at least 1 coal from a nearby coal deposit\nor cave. 7. Finally, I can use one stick and one coal to craft a torch.\nPlan 2 1. First, I need to find a tree to gather 4 logs. 2. Next, I have to craft the logs\ninto planks. 3. From the planks, I will craft 3 sticks. 4. I need to craft a crafting table\nusing some of the planks. 5. I should then craft a wooden pickaxe to mine coal. 6. I\nwill locate and mine 1 piece of coal using the wooden pickaxe. 7. Finally, I will use\nthe coal and sticks to craft a torch.\nPlan 3 1. I have to find a tree to get 4 logs. 2. Next, I have to craft planks from the\nlogs. 3. Get sticks from some of the planks. 4. I need a crafting table, which requires\nplanks to craft. 5. I should craft a wooden pickaxe using the crafting table. 6. I need\nto find and mine some coal using the wooden pickaxe. 7. Finally, with the stick and\ncoal, I can craft a torch.\nGLM4V\nPlan 1 1. Locate and chop down a tree to gather 4 logs. 2. Convert the logs into 13\nplanks using the crafting menu. 3. Use the planks to craft a crafting table. 4. Craft 3\nsticks from the remaining planks. 5. With the crafting table and sticks, craft a wooden\npickaxe if one is not already available. 6. Use the wooden pickaxe to dig for coal\nor kill some Creepers or zombies for coal drops. 7. Once 1 coal is obtained, use the\ncrafting table to combine it with a stick to craft 1 torch.\nPlan 2 1. Locate and chop down a tree to gather the 4 logs required. 2. Convert the\nlogs into 13 planks using the crafting menu, as this will be the material needed for\ncrafting the crafting table and the wooden pickaxe. 3. Use some of the planks to craft\na crafting table, which is necessary for advanced crafting recipes. 4. Craft a wooden\npickaxe using some of the planks and sticks. The wooden pickaxe will be used to\nmine coal. 5. Gather 3 sticks, which can be obtained from the logs or by crafting them\nfrom planks. 6. Use the wooden pickaxe to mine coal. Only 1 coal is needed to craft\na torch. 7. Once all materials are gathered, open the crafting table and combine 1 coal\nand 1 stick to craft a torch.\nPlan 3 1. Locate and gather 4 logs from the nearest trees. 2. Convert the logs into\n13 planks using the crafting menu. 3. Use some planks to craft a crafting table for\nmore complex recipes. 4. Craft a wooden pickaxe using some planks and sticks to\ngather coal and other resources. 5. Convert some planks into 3 sticks required for\ncrafting torches and the wooden pickaxe. 6. Use the wooden pickaxe to mine coal\nore, ensuring to gather at least 1 coal. 7. With all materials gathered, open the crafting\ntable and craft 1 torch using 1 coal and 1 stick.\nTable 9. Open-ended instruction examples of “Craft a rail”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and punch down a tree to acquire 7 logs. 2. Convert these logs\ninto planks to obtain a total of 25 planks. 3. Use some of these planks to craft 5 sticks.\n4. Create a crafting table using 4 planks. 5. Craft a wooden pickaxe using 3 planks\nand 2 sticks. 6. Use the wooden pickaxe to mine 11 cobblestones. 7. Craft a furnace\nusing 8 cobblestones. 8. Craft a stone pickaxe using 3 cobblestones and 2 sticks. 9.\nUse the stone pickaxe to mine 6 iron ores. 10. Smelt the iron ores in the furnace to\ncreate 6 iron ingots. 11. Finally, craft a rail using the iron ingots and sticks.\nPlan 2 1. I have to find a tree to get 7 logs. 2. Next, I have to craft the logs into 25\nplanks. 3. Craft 5 sticks from the planks. 4. I need to create a crafting table. 5. Use\nthe crafting table to craft a wooden pickaxe. 6. Use the wooden pickaxe to mine 11\npieces of cobblestone. 7. I’ll then craft a furnace using the cobblestone. 8. Craft a\nstone pickaxe with the cobblestone and sticks. 9. Use the stone pickaxe to mine 6\npieces of iron ore. 10. Smelt the iron ore in the furnace to obtain 6 iron ingots. 11.\nFinally, use the crafting table to craft a rail using the iron ingots and sticks.\nPlan 3 1. I have to find a tree to get 7 logs. 2. Next, I need to craft planks from\nthe logs. 3. Get sticks from the planks. 4. Craft a crafting table using some of the\nplanks. 5. Craft a wooden pickaxe with sticks and planks to mine stone. 6. Mine 11\ncobblestone with the wooden pickaxe. 7. Craft a furnace using the cobblestone. 8.\nCraft a stone pickaxe using cobblestone and sticks. 9. Use the stone pickaxe to mine\n6 iron ore. 10. Smelt the iron ore in the furnace to get 6 iron ingots. 11. Finally, use\nthe iron ingots and sticks to craft a rail.\nGLM4V\nPlan 1 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down\na crafting table to access more crafting recipes. 5. Use the crafting table to craft\na wooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones from the\nground or from stone blocks. 7. Craft a furnace using the cobblestones. 8. Upgrade\nthe wooden pickaxe to a stone pickaxe by crafting it at the crafting table with the\ncobblestones. 9. Use the stone pickaxe to mine 6 iron ore from the appropriate rock\nlayers. 10. Smelt the 6 iron ore in the furnace to produce 6 iron ingots. 11. Finally,\nuse the crafting table to craft 1 rail using the 6 iron ingots and 1 stick.\nPlan 2 1. Locate and gather 7 logs from nearby trees. 2. Use the crafting menu\nto convert the logs into 25 planks. 3. Craft 5 sticks using the planks in the crafting\nmenu. 4. Place a crafting table to enable the crafting of more complex items. 5.\nCraft a wooden pickaxe using some of the planks and sticks. 6. Use the wooden\npickaxe to mine 11 cobblestones from stone outcrops. 7. Construct a furnace using\nthe cobblestones. 8. Upgrade to a stone pickaxe by crafting it with the cobblestones\nand sticks. 9. Use the stone pickaxe to mine 6 iron ore from iron veins. 10. Smelt the\n6 iron ore in the furnace to produce 6 iron ingots. 11. Finally, use the crafting table to\ncraft 1 rail using the iron ingots and sticks.\nPlan 3 1. Find and chop down a tree to gather 7 logs. 2. Use the crafting menu to\nconvert the logs into 25 planks. 3. Craft 5 sticks from the planks. 4. Place down a\ncrafting table to access more crafting options. 5. Craft a wooden pickaxe using some\nof the planks and sticks. 6. Use the wooden pickaxe to mine 11 cobblestones from\nstone. 7. Construct a furnace using some of the cobblestones. 8. Upgrade the wooden\npickaxe to a stone pickaxe by crafting it with the cobblestones. 9. Search for and\nmine 6 iron ore blocks with the stone pickaxe. 10. Smelt the iron ore in the furnace to\nproduce 6 iron ingots. 11. Finally, use the crafting table to craft 1 rail using the iron\ningots and sticks.\nTable 10. Open-ended instruction examples of “Craft a golden shovel”\nPlanner Instruction\nGPT4V\nPlan 1 1. Start by finding and chopping down trees to collect 9 logs. 2. Convert the logs into 35 planks\nusing the crafting interface. 3. Use some of the planks to craft 8 sticks. 4. Create a crafting table using\nsome of the planks. 5. Craft a wooden pickaxe. 6. Mine 19 cobblestone. 7. Craft a furnace using 8 of the\ncobblestones. 8. Craft a stone pickaxe. 9. Mine 3 iron ores using the stone pickaxe. 10. Smelt the iron ore\nin the furnace. 11. Craft an iron pickaxe using sticks and iron ingots. 12. Locate and mine a gold block\nusing the iron pickaxe. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Craft a golden\nshovel using the gold ingot and sticks.\nPlan 2 1. Start by finding and chopping down trees to collect 9 logs. 2. Use the logs to craft planks. 3.\nCraft sticks. 4. Use some of the planks to craft a crafting table. 5. Use the crafting table to craft a wooden\npickaxe. 6. Use the wooden pickaxe to mine 19 pieces of cobblestone. 7. Use some of the cobblestone to\ncraft a furnace. 8. Use the remaining cobblestone to craft a stone pickaxe. 9. Mine iron ore with the stone\npickaxe. 10. Smelt the iron ore in the furnace to obtain 3 iron ingots. 11. Use 2 of the iron ingots and sticks\nto craft an iron pickaxe. 12. Use the iron pickaxe to mine gold ore or find a gold block; you need at least 1\ngold ingot. 13. If mining gold ore, smelt it in the furnace to obtain gold ingots. 14. Use the gold ingot and\nstick to craft a golden shovel.\nPlan 3 1. Find and cut down trees to collect 9 logs. 2. Use the logs to craft 35 planks. 3. Craft 8 sticks\nusing the planks. 4. Use some of the planks to craft a crafting table. 5. Craft a wooden pickaxe using planks\nand sticks. 6. Mine cobblestone until you have 19 pieces. 7. Use some cobblestone and the crafting table to\ncraft a furnace. 8. Craft a stone pickaxe using cobblestone and remaining sticks. 9. Use the stone pickaxe\nto mine 3 pieces of iron ore. 10. Smelt the iron ore in the furnace to get 3 iron ingots. 11. Craft an iron\npickaxe. 12. Use the iron pickaxe to mine gold ore to obtain gold blocks. 13. Use 1 gold ingot to craft a\ngolden shovel. 14. use a gold ingot and sticks to craft a golden shovel using the crafting table.\nGLM4V\nPlan 1 1. Find and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to expand the crafting options. 5. Craft\na wooden pickaxe with the planks and sticks. 6. Cine cobblestone until you have a total of 19. 7. Craft a\nfurnace using 8 cobblestone. 8. Use the furnace to smelt the cobblestone into 19 stone, then use the crafting\ntable to craft a stone pickaxe with the stone and sticks. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks at the crafting table. 12. Use the iron pickaxe to mine a gold block. 13. Smelt the gold block in\nthe furnace to get 1 gold ingot. 14. Finally, use the crafting table to craft the golden shovel using 1 gold\ningot and 2 sticks.\nPlan 2 1. Find and gather 9 logs from nearby trees. 2. Convert the logs into 35 planks using a crafting\ntable. 3. Craft 8 sticks from the planks. 4. Build a crafting table. 5. Use the crafting table to craft a wooden\npickaxe with the planks and sticks. 6. Use the wooden pickaxe to mine 19 cobblestones from stone. 7.\nConstruct a furnace to smelt materials. 8. Craft a stone pickaxe using the cobblestones and sticks. 9. Use\nthe stone pickaxe to mine 3 iron ore from iron deposits. 10. Smelt the 3 iron ore in the furnace to produce\n3 iron ingots. 11. Craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine a\ngold block. 13. Smelt the gold block in the furnace to get 1 gold ingot. 14. Finally, craft the golden shovel\nusing the gold ingot and stick in the crafting table.\nPlan 3 1. Find and gather 9 logs from nearby trees. 2. Use the crafting menu to convert the logs into 35\nplanks. 3. Craft 8 sticks from the planks. 4. Craft a crafting table using some of the planks. 5. Use the\ncrafting table to craft a wooden pickaxe with some of the sticks and planks. 6. Use the wooden pickaxe to\nmine 19 cobblestones from stone. 7. Craft a furnace using some of the cobblestones. 8. Use the furnace to\nsmelt the cobblestones into 19 stone, then craft a stone pickaxe with the sticks and stone. 9. Mine 3 iron\nore from the appropriate rock faces. 10. Smelt the iron ore in the furnace to produce 3 iron ingots. 11. Use\nthe crafting table to craft an iron pickaxe with the iron ingots and sticks. 12. Use the iron pickaxe to mine\na gold block. 13. Smelt the gold block in the furnace to produce 1 gold ingot. 14. Finally, use the crafting\ntable to craft the golden shovel with the gold ingot and 2 sticks.\nTable 11. Open-ended instruction examples of “Craft a diamond pickaxe”\nPlanner Instruction\nGPT4V\nPlan 1 1. First, locate and chop down trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some\nof the planks to craft 8 sticks. 4. Create a crafting table using 4 planks. 5. With the crafting table, craft\na wooden pickaxe using planks and sticks. 6. Use the wooden pickaxe to collect at least 11 cobblestones\nfrom stone. 7. With the cobblestones, craft a furnace. 8. Use more cobblestones to craft a stone pickaxe. 9.\nUse the stone pickaxe to mine 3 iron ores. 10. Smelt the iron ores into 3 iron ingots using the furnace. 11.\nCraft an iron pickaxe with iron ingots and sticks. 12. Use the iron pickaxe to mine 3 diamonds. 13. Finally,\ncraft the diamond pickaxe using the diamonds and sticks.\nPlan 2 1. Begin by locating a tree to gather 9 logs. 2. Convert the logs into planks. 3. Use some of the\nplanks to craft sticks. 4. Craft a crafting table to facilitate further crafting. 5. Create a wooden pickaxe to\nstart mining stone. 6. Mine 11 cobblestones. 7. Use the cobblestones to craft a furnace. 8. Craft a stone\npickaxe to advance to mining iron ore. 9. Locate and mine 3 iron ores using the stone pickaxe. 10. Smelt\nthe iron ores in the furnace to produce 3 iron ingots. 11. Use two of the iron ingots and sticks to craft an\niron pickaxe. 12. Locate a diamond ore vein and mine 3 diamonds using the iron pickaxe. 13. Finally, craft\nthe diamond pickaxe with the 3 diamonds and sticks.\nPlan 3 1. I need to find a tree to gather 9 logs. 2. Convert the logs into 35 planks. 3. Use some planks to\ncraft 8 sticks. 4. Craft a crafting table using some of the planks. 5. With the crafting table, make a wooden\npickaxe to mine stone. 6. Mine 11 cobblestones using the wooden pickaxe. 7. Craft a furnace using the\ncollected cobblestones. 8. Make a stone pickaxe to efficiently mine iron ore. 9. Find and mine 3 iron ores\nusing the stone pickaxe. 10. Smelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron\npickaxe using some planks, sticks, and the iron ingots. 12. Locate a source of diamonds and use the iron\npickaxe to mine 3 diamonds. 13. Finally, use the crafting table along with the sticks and diamonds to craft\na diamond pickaxe.\nGLM4V\nPlan 1 1. Start by finding trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to make a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ores. 10.\nSmelt the iron ores in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine 3 diamonds, which may require exploring caves or mining deep underground with the\niron pickaxe. 13. Finally, use the crafting table to combine the diamonds and the sticks to craft a diamond\npickaxe.\nPlan 2 1. Locate and chop down trees to gather a total of 9 logs. 2. Use the logs to craft 35 planks, as this\nwill be the base material for several other items. 3. Craft 8 sticks from the planks, which will be needed to\ncreate various tools. 4. Craft a crafting table using some of the planks, as this is essential for crafting more\ncomplex items. 5. Use the crafting table to craft a wooden pickaxe with the planks and sticks, which will be\nused to mine cobblestone. 6. Mine 11 cobblestone using the wooden pickaxe. 7. Craft a stone pickaxe with\nthe cobblestone and sticks, as this is more durable and efficient for mining. 8. Construct a furnace using the\ncobblestone, which will be required to smelt ores into ingots. 9. Mine 3 iron ore with the stone pickaxe. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe using the iron ingots\nand sticks, as this is necessary to mine diamonds. 12. Find and mine diamond ore with the iron pickaxe\nto gather 3 diamonds. 13. Finally, use the crafting table to craft a diamond pickaxe with the diamonds and\nsticks, completing the task.\nPlan 3 1. Begin by locating trees to gather 9 logs. 2. Convert the logs into 35 planks. 3. Craft 8 sticks\nfrom the planks. 4. Build a crafting table to expand crafting options. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine 11 cobblestones. 7. Craft a stone pickaxe using the\ncobblestones. 8. Construct a furnace to smelt materials. 9. Use the stone pickaxe to mine 3 iron ore. 10.\nSmelt the iron ore in the furnace to produce 3 iron ingots. 11. Craft an iron pickaxe with the iron ingots.\n12. Locate and mine diamond ore with the iron pickaxe to obtain 3 diamonds. 13. Finally, use the crafting\ntable to craft a diamond pickaxe with the diamonds.\nTable 12. Open-ended instruction examples of “Craft a compass”\nPlanner Instruction\nGPT4V\nPlan 1 1. Locate a tree and gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks from the\nplanks. 4. Use some planks to craft a crafting table. 5. With the crafting table, craft a wooden pickaxe. 6.\nMine cobblestone until you have 12 pieces. 7. Upgrade your tools by crafting a stone pickaxe. 8. Construct\na furnace using some of the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron\nore in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots. 12. Find and\nmine redstone using the iron pickaxe to obtain 1 redstone dust. 13. Finally, use the remaining iron ingots\nand redstone dust at the crafting table to craft a compass.\nPlan 2 1. Start by finding and chopping down trees to collect 10 logs. 2. Use the logs to craft 38 planks. 3.\nCraft sticks using the planks to get 8 sticks. 4. Craft a crafting table from the planks. 5. Use the crafting\ntable to create a wooden pickaxe. 6. Locate and mine 12 pieces of cobblestone using the wooden pickaxe.\n7. Craft a stone pickaxe from cobblestone and sticks. 8. Use the cobblestone to craft a furnace. 9. Mine\n7 iron ores using the stone pickaxe. 10. Smelt the iron ore in the furnace to get 7 iron ingots. 11. Craft\nan iron pickaxe using iron ingots and sticks. 12. Search for and mine redstone using the iron pickaxe. 13.\nFinally, use the iron ingots and redstone to craft a compass.\nPlan 3 1. First, I need to find a tree and collect 10 logs. 2. Convert the logs into planks, ensuring I get at\nleast 38 planks. 3. Use some of the planks to craft sticks, collecting at least 8 sticks. 4. Use some of the\nplanks to create a crafting table. 5. Craft a wooden pickaxe. 6. Locate cobblestone and mine at least 12\npieces using the wooden pickaxe. 7. Use some cobblestone to craft a furnace. 8. Craft a stone pickaxe for\nimproved mining. 9. Search for iron ore and mine at least 7 using the stone pickaxe. 10. Smelt the iron ore\nin the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe using some of the iron ingots and sticks.\n12. Locate a redstone ore and mine it using the iron pickaxe to obtain redstone. 13. Finally, use 4 of the\niron ingots and the redstone to craft a compass at the crafting table.\nGLM4V\nPlan 1 1. Locate and gather 10 logs from nearby trees. 2. Convert the logs into 38 planks using the crafting\nmenu. 3. Craft 8 sticks from the planks. 4. Build a crafting table to access more crafting options. 5. Use\nthe crafting table to make a wooden pickaxe. 6. With the wooden pickaxe, mine 12 cobblestones from\nstone. 7. Craft a stone pickaxe using the cobblestones and the crafting table. 8. Construct a furnace to smelt\nmaterials. 9. Use the stone pickaxe to mine 7 iron ores from the appropriate rock formations. 10. Smelt\nthe 7 iron ores in the furnace to produce 7 iron ingots. 11. Craft an iron pickaxe with the iron ingots at the\ncrafting table. 12. Mine for redstone, which can sometimes be found in the ground. 13. Use the crafting\ntable to craft a compass with the iron ingot and redstone.\nPlan 2 1. Locate and chop down trees to gather the required 10 logs. 2. Convert the logs into 38 planks\nusing the crafting menu. 3. Craft 8 sticks from the planks. 4. Use some planks to craft a crafting table.\n5. With the crafting table and the sticks and planks crafted, make a wooden pickaxe. 6. Use the wooden\npickaxe to mine 12 cobblestone. 7. Craft a stone pickaxe using the cobblestone. 8. Construct a furnace\nwith the cobblestone. 9. Use the stone pickaxe to mine 7 iron ore. 10. Smelt the iron ore in the furnace to\nproduce 7 iron ingots. 11. Craft an iron pickaxe using the iron ingots and the crafting table. 12. Search for\nredstone; gather at least 1 redstone. 13. Finally, use the crafting table to combine the redstone with the iron\ningots to craft a compass.\nPlan 3 1. Find and chop down trees to gather 10 logs. 2. Convert the logs into 38 planks. 3. Craft 8 sticks\nfrom the planks. 4. Craft a crafting table using some of the planks. 5. Use the crafting table to craft a\nwooden pickaxe. 6. With the wooden pickaxe, mine cobblestone until you have at least 12. 7. Upgrade the\nwooden pickaxe to a stone pickaxe using the cobblestone. 8. Construct a furnace out of the cobblestone. 9.\nUse the stone pickaxe to mine 7 iron ore. 10. Smelt the 7 iron ore in the furnace to produce 7 iron ingots.\n11. Craft an iron pickaxe using the iron ingots and sticks at the crafting table. 12. Search for redstone; you\nwill need at least 1 piece. 13. Finally, use the crafting table to combine the redstone with the iron ingots to\ncraft a compass.\nTable 13. The results of Optimus-2 on the Wood Group, Stone Group, and Iron Group. SR denotes success rate.\nGroup\nTask\nSub-Goal Num.\nSR\nEval Times\nWood\nCraft a wooden shovel\n6\n100.00\n40\nCraft a wooden pickaxe\n5\n100.00\n30\nCraft a wooden axe\n5\n97.37\n38\nCraft a wooden hoe\n5\n100.00\n30\nCraft a stick\n4\n100\n30\nCraft a crafting table\n3\n93.02\n43\nCraft a wooden sword\n5\n100.00\n30\nCraft a chest\n4\n100.00\n30\nCraft a bowl\n4\n100.00\n30\nCraft a ladder\n4\n100.00\n30\nStone\nCraft a stone shovel\n8\n89.47\n57\nCraft a stone pickaxe\n10\n98.00\n50\nCraft a stone axe\n10\n94.44\n54\nCraft a stone hoe\n8\n95.74\n47\nCraft a charcoal\n9\n85.71\n42\nCraft a smoker\n9\n90.00\n40\nCraft a stone sword\n8\n95.45\n44\nCraft a furnace\n9\n94.44\n36\nCraft a torch\n8\n89.36\n47\nIron\nCraft an iron shovel\n13\n52.08\n48\nCraft an iron pickaxe\n13\n56.00\n50\nCraft an iron axe\n13\n48.15\n54\nCraft an iron hoe\n13\n56.60\n53\nCraft a bucket\n13\n45.10\n51\nCraft a hopper\n14\n54.90\n51\nCraft a rail\n13\n51.02\n49\nCraft an iron sword\n12\n56.52\n46\nCraft a shears\n12\n48.28\n58\nCraft a smithing table\n12\n53.33\n45\nCraft a tripwire hook\n13\n55.56\n45\nCraft a chain\n13\n52.17\n46\nCraft an iron bars\n12\n51.06\n47\nCraft an iron nugget\n12\n54.55\n44\nCraft a blast furnace\n14\n52.27\n44\nCraft a stonecutter\n13\n52.27\n44\nTable 14. The results of Optimus-2 on the Gold group, Diamond Group, Redstone Group, and Armor Group. SR denotes success rate.\nGroup\nTask\nSub Goal Num.\nSR\nEval Times\nGold\nCraft a golden shovel\n16\n8.93\n56\nCraft a golden pickaxe\n16\n11.29\n62\nCraft a golden axe\n16\n8.93\n56\nCraft a golden hoe\n16\n8.96\n67\nCraft a golden sword\n16\n8.20\n61\nSmelt and craft an golden ingot\n15\n9.68\n62\nDiamond\nCraft a diamond shovel\n15\n15.91\n44\nCraft a diamond pickaxe\n15\n11.76\n34\nCraft a diamond axe\n16\n11.00\n36\nCraft a diamond hoe\n15\n15.91\n44\nCraft a diamond sword\n15\n11.11\n36\nDig down and mine a diamond\n15\n11.42\n35\nCraft a jukebox\n15\n13.15\n38\nRedstone\nCraft a piston\n16\n28.33\n60\nCraft a redstone torch\n16\n27.69\n65\nCraft an activator rail\n18\n25.81\n62\nCraft a compass\n23\n28.36\n67\nCraft a dropper\n16\n30.30\n66\nCraft a note block\n16\n25.40\n63\nArmor\nCraft shield\n14\n45.16\n62\nCraft iron chestplate\n14\n43.86\n57\nCraft iron boots\n14\n40.35\n57\nCraft iron leggings\n14\n8.57\n35\nCraft iron helmet\n14\n47.46\n56\nCraft diamond helmet\n17\n9.09\n33\nCraft diamond chestplate\n17\n7.89\n38\nCraft diamond leggings\n17\n5.41\n37\nCraft diamond boots\n17\n12.50\n40\nCraft golden helmet\n17\n13.89\n36\nCraft golden leggings\n17\n12.20\n41\nCraft golden boots\n17\n10.26\n39\nCraft golden chestplate\n17\n10.00\n40\nAgent\nInstruction: I want to get some logs to craft wooden sword, what should I do first?\nSuccess\n❌\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n✔\nFigure 12. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to get some logs\nto craft wooden sword, what should I do first?”. Existing policies are limited by their instruction comprehension abilities and thus fail to\ncomplete the task, whereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I need coal for heating. What should I do?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 13. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I need coal for heating.\nWhat should I do?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task, whereas\nGOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nAgent\nInstruction: I want to collect some seeds, Can you help me?\nSuccess\n❌\n✔\n❌\n�1\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\n�1200\n�200\n�400\n�600\n�800\n�1000\n�0\nFigure 14. An illustration of VPT (text) [1], STEVE-1 [25], and Optimus-2 executing the open-ended instruction, “I want to collect some\nseeds, Can you help me?”. Existing policies are limited by their instruction comprehension abilities and thus fail to complete the task,\nwhereas GOAP leverages the language understanding capabilities of the MLLM, enabling it to accomplish the task.\nReferences\n[1] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampe-\ndro, and Jeff Clune. Video pretraining (vpt): Learning to act\nby watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639–24654, 2022. 2,\n3, 4, 5, 6, 7, 8, 15\n[2] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao\nLiang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. In\nProceedings of the IEEE\/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13734–13744, 2023. 2\n[3] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji\nLiu, and Yitao Liang. Groot: Learning to follow instructions\nby watching gameplay videos. In The Twelfth International\nConference on Learning Representations, 2023. 1, 2, 3, 4, 5,\n6\n[4] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and\nLiqiang Nie. Lion: Empowering multimodal large language\nmodel with dual-level visual knowledge. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 26540–26550, 2024. 1\n[5] Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gong-\nwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu,\nShuai Wang, et al. Spa-bench: A comprehensive benchmark\nfor smartphone agent evaluation. In ICLR, 2025. 1\n[6] Yihong Chen, Yue Cao, Han Hu, and Liwei Wang. Mem-\nory enhanced global-local aggregation for video object de-\ntection. In Proceedings of the IEEE\/CVF conference on com-\nputer vision and pattern recognition, pages 10337–10346,\n2020. 3\n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: towards general-purpose\nvision-language models with instruction tuning. In Proceed-\nings of the 37th International Conference on Neural Infor-\nmation Processing Systems, pages 49250–49267, 2023. 3\n[8] Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang,\nand Zongqing Lu. Clip4mc: An rl-friendly vision-language\nmodel for minecraft.\narXiv preprint arXiv:2303.10571,\n2023. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 4, 8\n[10] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar.\nMinedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems, 35:\n18343–18362, 2022. 2, 3, 4, 8, 5\n[11] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui\nZhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao,\nHanyu Lai, et al.\nChatglm: A family of large language\nmodels from glm-130b to glm-4 all tools.\narXiv preprint\narXiv:2406.12793, 2024. 6\n[12] William H Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan\nSalakhutdinov. Minerl: A large-scale dataset of minecraft\ndemonstrations. arXiv preprint arXiv:1907.13440, 2019. 5,\n2\n[13] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2\n[14] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xue-\nfei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam\nLim. Ma-lmm: Memory-augmented large multimodal model\nfor long-term video understanding. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13504–13514, 2024. 3, 5\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 4\n[16] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao,\nand Li Yuan. Chat-univi: Unified visual representation em-\npowers large language models with image and video un-\nderstanding. In Proceedings of the IEEE\/CVF Conference\non Computer Vision and Pattern Recognition, pages 13700–\n13710, 2024. 3\n[17] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao,\nAshwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan\nFoster, Grace Lam, Pannag Sanketi, et al.\nOpenvla: An\nopen-source vision-language-action model. arXiv preprint\narXiv:2406.09246, 2024. 8\n[18] Sangho Lee, Jinyoung Sung, Youngjae Yu, and Gunhee\nKim.\nA memory network approach for story-based tem-\nporal summarization of 360° videos.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 1410–1419, 2018. 3\n[19] Sangmin Lee, Hak Gu Kim, Dae Hwi Choi, Hyung-Il Kim,\nand Yong Man Ro. Video prediction recalling long-term mo-\ntion context via memory alignment learning. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3054–3063, 2021. 3\n[20] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou,\nYu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and\nJifeng Dai. Auto mc-reward: Automated dense reward de-\nsign with large language models for minecraft. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 16426–16435, 2024. 1, 3, 5\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning, pages 19730–\n19742. PMLR, 2023. 3\n[22] Wei Li, Bing Hu, Rui Shao, Leyang Shen, and Liqiang Nie.\nLion-fs: Fast & slow video-language thinker as online video\nassistant. arXiv preprint arXiv:2503.03663, 2025. 3\n[23] Xiaojie Li, Jianlong Wu, Shaowei He, Shuo Kang, Yue Yu,\nLiqiang Nie, and Min Zhang. Fine-grained key-value mem-\nory enhanced predictor for video representation learning. In\nProceedings of the ACM International Conference on Multi-\nmedia, page 2264–2274. ACM, 2023. 1\n[24] Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dong-\nmei Jiang, and Liqiang Nie.\nOptimus-1:\nHybrid mul-\ntimodal memory empowered agents excel in long-horizon\ntasks. arXiv preprint arXiv:2408.03615, 2024. 1, 2, 3, 4,\n5, 6\n[25] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and\nSheila McIlraith. Steve-1: A generative model for text-to-\nbehavior in minecraft. Advances in Neural Information Pro-\ncessing Systems, 2023. 1, 2, 3, 4, 5, 6, 7, 15\n[26] Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui,\nWenkai Fang, Yuxuan Zheng, Tongya Zheng, and Mingli\nSong. Odyssey: Empowering agents with open-world skills.\narXiv preprint arXiv:2407.15325, 2024. 3\n[27] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan,\nHuayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu.\nRdt-1b: a diffusion foundation model for bimanual manipu-\nlation. arXiv preprint arXiv:2410.07864, 2024. 8\n[28] Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang\nChen, Shu Liu, Zongqing Lu, and Jiaya Jia.\nRl-gpt: In-\ntegrating reinforcement learning and code-as-policy. arXiv\npreprint arXiv:2402.19299, 2024. 3\n[29] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai\nDong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li,\nYaofeng Sun, et al. Deepseek-vl: towards real-world vision-\nlanguage understanding. arXiv preprint arXiv:2403.05525,\n2024. 6, 4\n[30] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video\nunderstanding via large vision and language models. arXiv\npreprint arXiv:2306.05424, 2023. 3\n[31] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet\nKohli.\nZero-shot task generalization with multi-task deep\nreinforcement learning. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning, pages 2661–2670.\nPMLR, 2017. 2\n[32] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu\nSheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: A\nmulti-modal open-ended embodied system in minecraft via\nactive perception. arXiv preprint arXiv:2312.07472, 2023.\n1, 3, 4, 5\n[33] Rui Shao, Xiangyuan Lan, Jiawei Li, and Pong C Yuen.\nMulti-adversarial discriminative deep domain generalization\nfor face presentation attack detection.\nIn Proceedings of\nthe IEEE\/CVF conference on computer vision and pattern\nrecognition, pages 10023–10031, 2019. 3\n[34] Rui Shao, Tianxing Wu, and Ziwei Liu.\nDetecting and\ngrounding multi-modal media manipulation. In Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6904–6913, 2023. 1\n[35] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Zi-\nwei Liu. Detecting and grounding multi-modal media manip-\nulation and beyond. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2024. 3\n[36] Rui Shao, Tianxing Wu, Liqiang Nie, and Ziwei Liu.\nDeepfake-adapter: Dual-level adapter for deepfake detec-\ntion. International Journal of Computer Vision, 2025. 3\n[37] Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, and\nLiqiang Nie. Mome: Mixture of multimodal experts for gen-\neralist multimodal large language models. Advances in neu-\nral information processing systems, 37:42048–42070, 2025.\n1\n[38] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng\nZhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo,\nTian Ye, Yanting Zhang, et al. Moviechat: From dense token\nto sparse memory for long video understanding. In Proceed-\nings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, pages 18221–18232, 2024. 3\n[39] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of Machine Learning Research, 9\n(86):2579–2605, 2008. 8\n[40] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023. 3\n[41] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xi-\naojian Ma, and Yitao Liang.\nDescribe, explain, plan\nand select: Interactive planning with large language mod-\nels enables open-world multi-task agents.\narXiv preprint\narXiv:2302.01560, 2023. 1, 6, 5\n[42] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong\nZheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-\ntask agents with memory-augmented multimodal language\nmodels. arXiv preprint arXiv:2311.05997, 2023. 1, 3, 6\n[43] Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao\nZhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, and Yi-\ntao Liang. Omnijarvis: Unified vision-language-action to-\nkenization enables open-world instruction following agents.\narXiv preprint arXiv:2407.00114, 2024. 2, 3, 4, 5, 6\n[44] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi\nFan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.\nMemvit: Memory-augmented multiscale vision transformer\nfor efficient long-term video recognition. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13587–13597, 2022. 3\n[45] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and\nXiaochun Cao. Cat: Enhancing multimodal large language\nmodel to answer questions in dynamic audio-visual scenar-\nios.\nIn European Conference on Computer Vision, pages\n146–164. Springer, 2024. 1\n[46] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. In Proceedings of the 2023 Conference on Em-\npirical Methods in Natural Language Processing: System\nDemonstrations, pages 543–553, 2023. 3\n[47] Haoyu Zhang, Meng Liu, Yuhong Li, Ming Yan, Zan Gao,\nXiaojun Chang, and Liqiang Nie. Attribute-guided collab-\norative learning for partial person re-identification.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n45(12):14144–14160, 2023. 1\n[48] Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, Yaowei\nWang, and Liqiang Nie. Multi-factor adaptive vision selec-\ntion for egocentric video question answering. In Proceedings\nof the 41st International Conference on Machine Learning,\npages 59310–59328. PMLR, 2024. 3\n[49] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi\nFeng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-\nbased real-time understanding for long video streams. arXiv\npreprint arXiv:2406.08085, 2024. 3\n[50] Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili\nGuan, and Liqiang Nie. Token-level correlation-guided com-\npression for efficient multimodal document understanding.\narXiv preprint arXiv:2407.14439, 2024. 3\n[51] Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi,\nShengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang,\nand Gaoang Wang. See and think: Embodied agent in virtual\nenvironment. arXiv preprint arXiv:2311.15209, 2023. 3\n[52] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-\njie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiao-\ngang Wang, et al. Ghost in the minecraft: Generally capable\nagents for open-world enviroments via large language mod-\nels with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023. 3\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy.pdf"}
{"title":"Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos","authors":"Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune","summary":"Pretraining on noisy, internet-scale datasets has been heavily studied as a\ntechnique for training models with broad, general capabilities for text,\nimages, and other modalities. However, for many sequential decision domains\nsuch as robotics, video games, and computer use, publicly available data does\nnot contain the labels required to train behavioral priors in the same way. We\nextend the internet-scale pretraining paradigm to sequential decision domains\nthrough semi-supervised imitation learning wherein agents learn to act by\nwatching online unlabeled videos. Specifically, we show that with a small\namount of labeled data we can train an inverse dynamics model accurate enough\nto label a huge unlabeled source of online data -- here, online videos of\npeople playing Minecraft -- from which we can then train a general behavioral\nprior. Despite using the native human interface (mouse and keyboard at 20Hz),\nwe show that this behavioral prior has nontrivial zero-shot capabilities and\nthat it can be fine-tuned, with both imitation learning and reinforcement\nlearning, to hard-exploration tasks that are impossible to learn from scratch\nvia reinforcement learning. For many tasks our models exhibit human-level\nperformance, and we are the first to report computer agents that can craft\ndiamond tools, which can take proficient humans upwards of 20 minutes (24,000\nenvironment actions) of gameplay to accomplish.","url":"http:\/\/arxiv.org\/abs\/2206.11795v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2206.11795v1","published":1656000071000,"comment":null,"pdf_text":"Video PreTraining (VPT): Learning to Act by\nWatching Unlabeled Online Videos\nBowen Baker∗†\nbowen@openai.com\nIlge Akkaya∗†\nilge@openai.com\nPeter Zhokhov∗†\npeterz@openai.com\nJoost Huizinga∗†\njoost@openai.com\nJie Tang∗†\njietang@openai.com\nAdrien Ecoffet∗†\nadrien@openai.com\nBrandon Houghton∗†\nbrandon@openai.com\nRaul Sampedro∗†\nraulsamg@gmail.com\nJeff Clune∗†‡\njclune@gmail.com\nAbstract\nPretraining on noisy, internet-scale datasets has been heavily studied as a technique\nfor training models with broad, general capabilities for text, images, and other\nmodalities.1–6 However, for many sequential decision domains such as robotics,\nvideo games, and computer use, publicly available data does not contain the labels\nrequired to train behavioral priors in the same way. We extend the internet-scale\npretraining paradigm to sequential decision domains through semi-supervised\nimitation learning wherein agents learn to act by watching online unlabeled videos.\nSpeciﬁcally, we show that with a small amount of labeled data we can train an\ninverse dynamics model accurate enough to label a huge unlabeled source of online\ndata – here, online videos of people playing Minecraft – from which we can then\ntrain a general behavioral prior. Despite using the native human interface (mouse\nand keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-\nshot capabilities and that it can be ﬁne-tuned, with both imitation learning and\nreinforcement learning, to hard-exploration tasks that are impossible to learn from\nscratch via reinforcement learning. For many tasks our models exhibit human-\nlevel performance, and we are the ﬁrst to report computer agents that can craft\ndiamond tools, which can take proﬁcient humans upwards of 20 minutes (24,000\nenvironment actions) of gameplay to accomplish.\n1\nIntroduction\nWork in recent years has demonstrated the efﬁcacy of pretraining large and general foundation\nmodels7 on noisy internet-scale datasets for use in downstream tasks in natural language1–4 and\ncomputer vision.5,6,8 For sequential decision domains (e.g. robotics, game playing, and computer\nusage) where agents must repeatedly act within an environment, a wealth of data also exists on the\nweb; however, most of this data is in the form of unlabeled video (i.e. without the actions taken\nat each frame), making it much less straightforward to train a behavioral prior in these domains\nthan it is in e.g. natural language. In a few rare settings, such as Chess, Go, and StarCraft, there\n∗This was a large effort by a dedicated team. Each author made huge contributions on many fronts over long\ntime periods. All members were full time on the project for over six months. BB, IA, PZ, and JC were on the\noriginal VPT project team and were thus involved for even longer (over a year). Aside from those original team\nmembers, author order is random. It was also randomized between IA and PZ.\n†OpenAI\n‡University of British Columbia\narXiv:2206.11795v1  [cs.LG]  23 Jun 2022\nalready exist large datasets with action labels from various online platforms that researchers have\nused for imitation learning.9,10 When large labeled datasets do not exist, the canonical strategy\nfor training capable agents is reinforcement learning (RL),11 which can be sample inefﬁcient and\nexpensive for hard-exploration problems.12–18 Many virtual tasks, e.g. navigating websites, using\nPhotoshop, booking ﬂights, etc., can be very hard to learn with RL and do not have large, commonly\navailable sources of labeled data.19,20 In this paper, we seek to extend the paradigm of training\nlarge, general-purpose foundation models to sequential decision domains by utilizing freely available\ninternet-scale unlabeled video datasets with a simple semi-supervised imitation learning method. We\ncall this method Video PreTraining (VPT) and demonstrate its efﬁcacy in the domain of Minecraft.\nExisting semi-supervised imitation learning methods aim to learn with few or no explicit action labels;\nhowever, they generally rely on the policy’s ability to explore the environment throughout training,\nmaking them susceptible to exploration bottlenecks.21–25 Furthermore, most prior semi-supervised\nimitation learning work was tested in the relatively low data regime; because we experiment with far\nmore data (∼70k hours of unlabeled video), we hypothesize that we can achieve good performance\nwith a much simpler method, a trend that has proven true for pretraining in other modalities such\nas text.1 In particular, given a large but unlabeled dataset, we propose generating pseudo-labels by\ngathering a small amount of labeled data to train an inverse dynamics model (IDM) that predicts\nthe action taken at each timestep in a video. Behavioral cloning (BC) can require a large amount\nof data because the model must learn to infer intent and the distribution over future behaviors from\nonly past observations. In contrast, the inverse dynamics modeling task is simpler because it is\nnon-causal, meaning it can look at both past and future frames to infer actions. In most settings,\nenvironment mechanics are far simpler than the breadth of human behavior that can take place within\nthe environment, suggesting that non-causal IDMs could require far less data to train than causal BC\nmodels. Using pseudo-labels generated from the IDM, we then train a model to mimic the distribution\nof behavior in the previously unlabeled dataset with standard behavioral cloning at scale, which does\nnot require any model rollouts and thus does not suffer from any potential exploration bottlenecks\nin the environment. Finally, we show we can ﬁne-tune this model to downstream tasks with either\nbehavioral cloning or reinforcement learning.\nFigure 1:\nExample Minecraft\ncrafting GUI. Agents use the\nmouse and keyboard to navigate\nmenus and drag and drop items.\nWe chose to test our method in Minecraft because (a) it is one\nof the most actively played games in the world26 and thus has\na wealth of commonly available video data online, (b) it is a\nfairly open-ended sandbox game with an extremely wide variety\nof potential things to do, build, and collect, making our results\nmore applicable to real-world applications such as computer us-\nage, which also tends to be varied and open-ended, and (c) it\nhas already garnered interest by the RL community as a research\ndomain due to its complexity and correspondingly difﬁcult ex-\nploration challenges.27–31 In this work we use the native human\ninterface for Minecraft so that we can (1) most accurately model\nthe human behavior distribution and reduce domain shift between\nvideo data and the environment, (2) make data collection easier by allowing our human contractors to\nplay the game without modiﬁcation, and (3) eliminate the need to hand-engineer a custom interface\nfor models to interact with the environment. This choice means that our models play at 20 frames\nper second and must use a mouse and keyboard interface to interact with human GUIs for crafting,\nsmelting, trading, etc., including dragging items to speciﬁc slots or navigating the recipe book with\nthe mouse cursor (Fig. 1). Compared to prior work in Minecraft that uses a lower frame rate and\nconstructs crafting and attacking macros,30,32–34 using the native human interface drastically increases\nthe environment’s exploration difﬁculty, making most simple tasks near impossible with RL from\nscratch. Even the simple task of gathering a single wooden log while already facing a tree takes 60\nconsecutive attack actions with the human interface, meaning the chance for a naive random policy to\nsucceed is 1\n2\n60. While this paper shows results in Minecraft only, the VPT method is general and\ncould be applied to any domain.\nIn Section 4 we show that the VPT foundation model has nontrivial zero-shot performance, accom-\nplishing tasks impossible to learn with RL alone, such as crafting planks and crafting tables (tasks\nrequiring a human proﬁcient in Minecraft a median of 50 seconds or ∼970 consecutive actions).\nThrough ﬁne-tuning with behavioral cloning to smaller datasets that target more speciﬁc behavior\ndistributions, our agent is able to push even further into the technology tree, crafting stone tools\n2\n(taking a human a median of 2.3 minutes or ∼2790 actions). Finally, ﬁne-tuning via RL produces\nthe most dramatic improvements: our agent is able to craft diamond tools, an unprecedented result\nin Minecraft made even more challenging by using the native human interface. This task requires\na proﬁcient human a median upwards of 20 minutes or ∼24000 actions. The main contributions\nof this work are (1) we are the ﬁrst to show promising results applying semi-supervised imitation\nlearning to extremely large, noisy, and freely available video datasets for sequential decision domains,\n(2) we show that such pretraining plus ﬁne-tuning enables agents to solve tasks that were otherwise\nimpossible to learn, (3) we show that labeled contractor data is far more efﬁciently used within\nthe VPT method than it would be by directly training a foundation model from it and (4) we open\nsource our contractor data, trained model weights, and Minecraft environment for future research\ninto learning to act via semi-supervised imitation learning at scale.\n2\nPreliminaries and Related Work\nImitation learning methods35–38 seek to construct a policy that accurately models the distribution of\nbehavior in some dataset D = {(oi, ai)}, i ∈{1...N} of action-observation pairs. In order to roll\nout these policies in an environment, they must be causal, meaning they condition on observations\nfrom the current timestep t and past timesteps only, i.e. π ∼p(at|o1...ot). Imitation learning is\nsimplest when demonstrations are labeled with corresponding actions. Imitating labeled trajectories\nhas seen success in aerial vehicles,39,40 self-driving cars,41,42 board games,9,43 and video games.10,44\nWhen labeled demonstrations are not available, standard behavioral cloning will not work; however,\nthere is a large body of work in imitating behavior from unlabeled demonstrations.22 For instance,\nGAIL23 constructs an adversarial objective incentivizing the trained policy to exhibit behaviors\nindistinguishable from those in the target dataset. Edwards et al. 45 propose to ﬁrst learn a latent\npolicy using unlabeled demonstrations and then map the learned latent actions to real actions with\na small amount of environment interaction. Peng et al. 46 ﬁrst use motion-capture methods to track\nagent positions in videos and then train RL agents to match these waypoints. Similarly, Behbahani\net al. 47 and Aytar et al. 48 task a RL agent to match waypoints; however, they construct waypoints that\nare embeddings from unsupervised feature learning models. Pathak et al. 49 and Nair et al. 50 train\ngoal conditioned policies to take actions that advance the current state towards expert-provided goal\nstates expressed as high dimensional visual waypoints. Most similar to our own work, Torabi et al. 24\nsimultaneously train (1) an inverse dynamics model (IDM),51 which aims to uncover the underlying\naction between timesteps given observations of past and future timesteps, e.g. pIDM(at|ot, ot+1), and\n(2) a behavioral cloning (BC) model on trajectories of observations labeled with the IDM. Data to\ntrain the IDM is collected by rolling out the BC model in the target environment such that both\nmodels improve in tandem. However, at any point in training if there are sequences in the dataset that\nthe IDM performs poorly on, it requires that the BC model perform those or similar sequences in\norder for the IDM to improve and correctly label them. Therefore, if the BC model does not explore\nefﬁciently, it could severely slow down learning. In order to avoid this potential issue we opted for a\nsimpler two-stage approach: we ﬁrst train an IDM on a small number of labeled trajectories collected\nfrom human contractors (they play the game as would normally as we record their keypresses and\nmouse movements). Because human contractors reach most relevant parts of the state space, we can\nhold the IDM ﬁxed throughout BC training.\nCompared to most previous work in semi-supervised imitation learning, we experiment in the much\nmore complex and open-ended environment of Minecraft. Minecraft is a voxel-based 3D video\ngame that, due its popularity and wide variety of mechanics, has attracted a vast amount of RL\nresearch.27,28,30–34,52–60 A large body of work focuses on small, custom-made Minecraft worlds\nwith tasks such as navigation,53,60 block placing,54,55 instruction following,58,59 combat,56 and\nothers.28,31,57 Work operating in the massive, randomly generated environments of Minecraft itself\nhas included hill climbing,52 automated curriculum learning30 and, most closely related to the RL\nexperiments presented in Sec. 4.4, diamond mining.27,32–34 However, to the best of our knowledge,\nthere is no published work that operates in the full, unmodiﬁed human action space, which includes\ndrag-and-drop inventory management and item crafting.\n3\nCollecting “Clean” Data\nTraining the VPT Foundation Model\nvia Behavioral Cloning\nTraining the Inverse Dynamics Model (IDM)\n~270k hours\nunlabeled\nvideo\n~70k hours\nunlabeled\nvideo\n~2k hours\nvideo\nlabeled with\nactions\nFilter for “clean”\nvideo segments\nSearch for relevant\nMinecraft videos\nvia keywords\nContractors\ncollect data \nLabel videos\nwith IDM \n~70k hours\nvideo\nIDM-labeled\nwith actions\nTrain non-causal IDM\nTrain causal\nVPT Foundation Model\na\nd\nspace\nw\na\nd\nspace\nw\nFigure 2: Video Pretraining (VPT) Method Overview.\n3\nMethods\nInverse Dynamics Models (IDM)\nVPT, illustrated in Figure 2, requires we ﬁrst collect a small\namount of labeled contractor data with which to train an inverse dynamics model pIDM(at|o1...T ),\nwhich seeks to minimize the negative log-likelihood of an action at timestep t given a trajectory of T\nobservations ot : t ∈[1...T]. In contrast to an imitation learning policy, the IDM can be non-causal,\nmeaning its prediction for at can be a function of both past and future events, i.e. ot′>t. Compared to\nthe behavioral cloning objective of modeling the distribution of human intent given past frames only,\nwe hypothesize that inverting environment dynamics is easier and more data efﬁcient to learn. Indeed,\nSec. 4.1 will show that the IDM objective is much easier to learn, and furthermore Sec. 4.6 will show\nthat with very little labeled data (as few as 100 hours) we can train a fairly accurate IDM. This IDM\ncan be used to label online videos, providing the large amount of data required for the harder task of\nbehavioral cloning. See appendices D and B for IDM training and data collection details.\nData Filtering\nWe gather a large dataset of Minecraft videos by searching the web for related\nkeywords (Appendix A). Online videos often (1) include overlaid artifacts, such as a video feed\nof the player’s face, channel logos, watermarks, etc., (2) are collected from platforms other than\na computer with different gameplay, or (3) are from different game modes, e.g. in Minecraft we\nonly want \"survival mode\" where players start from scratch and must gather or craft all their items.\nWe call data “clean” if it does not contain visual artifacts and is from survival mode, and call all\nother data “unclean.” With enough data, a large enough model, and enough training compute, a BC\nmodel trained on both unclean and clean videos would likely still perform well in a clean Minecraft\nenvironment. However, for simplicity and training compute efﬁciency, we choose to ﬁlter out unclean\nsegments of video (note that a video may contain both clean and unclean segments). We do this by\ntraining a model to ﬁlter out unclean segments using a small dataset (8800) of images sampled from\nonline videos labeled by contractors as clean or unclean (Appendix A.2).\nVPT Foundation Model\nWe train a foundation model with standard behavioral cloning, i.e. mini-\nmizing the negative log-likelihood of actions predicted by the IDM on clean data. For a particular\ntrajectory of length T we minimize\nmin\nθ\nX\nt∈[1...T ]\n−log πθ(at|o1, . . . , ot), where at ∼pIDM(at|o1, . . . , ot, . . . , oT )\n(1)\nAs we will see in the following sections, this model exhibits nontrivial zero-shot behavior and can be\nﬁne-tuned with both imitation learning and RL to perform even more complex skills.\n4\nResults\n4.1\nPerformance of the Inverse Dynamics Model\nThe IDM architecture is comprised primarily of a temporal convolution layer, a ResNet62 image\nprocessing stack, and residual unmasked attention layers, from which the IDM simultaneously\npredicts keypresses and mouse movements (see Appendix D for IDM architecture and training\ndetails). A key hypothesis behind our work is that IDMs can be trained with a relatively small amount\nof labeled data. While more data improves both mouse movement and keypress predictions, our best\n4\nFigure 3: (Left) IDM keypress accuracy and mouse movement R2 (explained variance61) as a\nfunction of dataset size. (Right) IDM vs. behavioral cloning data efﬁciency.\nIDM trains on only 1962 hours of data (compared to the ∼70k hours of clean data we collected from\nthe internet) and achieves 90.6% keypress accuracy and a 0.97 R2 for mouse movements evaluated\non a held-out validation set of contractor-labeled data (Figure 3 left).\nFigure 3 (right) validates our hypothesis that IDMs are far more data efﬁcient than BC models, likely\nbecause inverting environment mechanics is far easier than modeling the entire distribution of human\nbehavior. The IDM is two orders of magnitude more data efﬁcient than a BC model trained on the\nsame data and improves more quickly with more data. This evidence supports the hypothesis that it is\nmore effective to use contractor data within the VPT pipeline by training an IDM than it is to train a\nfoundation model from contractor data directly (Sections 4.5 and 4.6 provide additional evidence).\n4.2\nVPT Foundation Model Training and Zero-Shot Performance\nFigure 4: (Left) Training and validation loss on the web_clean internet dataset with IDM pseudo-\nlabels, and loss on the main IDM contractor dataset, which has ground-truth labels but is out-of-\ndistribution (see text). (Right) Amount a given item was collected per episode averaged over 2500\n60-minute survival episodes as a function of training epoch, shaded with the standard error of the\nmean. Basic mining refers to collection of dirt, gravel, or sand (all materials that can be gathered\nwithout tools). Logs are obtained by repeatedly hitting trees for three seconds, a difﬁcult feat for an\nRL agent to achieve as we show in Sec. 4.4. Planks can be crafted from logs, and crafting tables\ncrafted from planks. Crafting requires using in-game crafting GUIs, and proﬁcient humans take a\nmedian of 50 seconds (970 consecutive actions) to make a crafting table.\nWe now explore the emergent behavior learned by a behavioral cloning policy trained on an extremely\nlarge, but noisy, internet dataset labeled with our IDM. To collect the unlabeled internet dataset,\nwe searched for publicly available videos of Minecraft play with search terms such as “minecraft\nsurvival for beginners.” These searches resulted in ∼270k hours of video, which we ﬁltered down to\n“clean” video segments yielding an unlabeled dataset of ∼70k hours, which we refer to as web_clean\n(Appendix A has further details on data scraping and ﬁltering). We then generated pseudo-labels\nfor web_clean with our best IDM (Section 3) and then trained the VPT foundation model with\nbehavioral cloning. Preliminary experiments suggested that our model could beneﬁt from 30 epochs\nof training and that a 0.5 billion parameter model was required to stay in the efﬁcient learning\nregime63 for that training duration (Appendix H), which took ∼9 days on 720 V100 GPUs.\nWe evaluate our models by measuring validation loss (Fig. 4, left) and rolling them out in the\nMinecraft environment. Unless otherwise noted, in all environment evaluations we spawn agents in a\nstandard survival mode game where they play for 60 minutes, i.e. 72000 consecutive actions, and we\nplot the mean and shade the standard error of the mean for various game statistics such as crafting\nand collection rates (Fig. 4, right). The VPT foundation model quickly learns to chop down trees\nto collect logs, a task we found near impossible for an RL agent to achieve with the native human\ninterface (Sec. 4.4). It also learns to craft those logs into wooden planks and then use those planks\n5\nto craft a crafting table, which are required to unlock most other technology in the game and take a\nhuman proﬁcient in Minecraft approximately 50 seconds (970 consecutive actions) to collect. While\nthese behaviors are fairly complex in the native human action space, the VPT foundation model crafts\nthese items at a rate far below that of our proﬁcient contractors, e.g. on average our contractors craft\n5.44 crafting tables in 60 minutes of play versus 0.19 for the foundation model. The model also crafts\na non-negligible amount of wooden sticks, which are required to make wooden tools; collects various\nﬂowers and crafts dyes from them; kills zombies that appear during the night; hunts wild animals;\ncollects various berries and mushrooms and eats them; and ﬁnds game-generated villages from which\nto collect various rare items from chests. The model also learned to navigate uneven terrain, swim,\nand pillar jump, which involves the agent repeatedly jumping and quickly placing a block below itself\nsuch that it climbs upward by making a pillar.(iv)\nWhile training and validation loss decrease healthily over training (Fig. 4, left), loss on our contractor\ndataset (which the VPT model does not train on) begins increasing after 7 epochs. Contractor data\ncould be out-of-distribution because our contractors may have a different distribution of play or\nbecause there is some impactful visual domain shift compared to videos from the web. While one\ncould have expected this would be predictive of declining evaluation performance, we do not see\nnotable game statistics from the VPT foundation model rollouts (Figure 4, right) decrease over\ntraining, and in the next section we show that BC ﬁne-tuning performance continually improves as the\nVPT foundation model trains. We provide more insight into this curious phenomenon in Appendix H.\n4.3\nFine-Tuning with Behavioral Cloning\nFoundation models are designed to have a broad behavior proﬁle and be generally capable across a\nwide variety of tasks. To incorporate new knowledge or allow them to specialize on a narrower task\ndistribution, it is common practice to ﬁne-tune these models to smaller, more speciﬁc datasets.1 The\nVPT foundation model trained on the broad web_clean dataset had nontrivial zero-shot performance;\nit was able to craft a crafting table yet unable to go past this in the technology tree. As a case\nstudy into BC ﬁne-tuning, we attempt to improve the VPT foundation model’s ability to collect\nand craft these “early game” items by ﬁne-tuning to two narrower datasets targeted at Minecraft\nbehavior within the ﬁrst few minutes of players starting in a fresh world. In the ﬁrst dataset,\ncontractor_house, contractors have 10 minutes to build a basic house from scratch using primarily\nwood, sand, and dirt. Collecting contractor data can be difﬁcult and expensive, so we also construct a\ndataset earlygame_keyword by searching for videos online with descriptions that match keywords\nsuch as “new world”, “let’s play episode 1”, etc.; this is a subset of web_clean and is labeled with\nthe IDM. See Appendix B.4 and A.3 for full descriptions of both datasets.\nEffect of Foundation Model Quality on BC Fine-Tuning\n59x\n213x\n59x\nFigure 5:\n(Left) Collection and crafting rates for three policies:\nthe zero-shot VPT foun-\ndation model, and the VPT foundation model BC ﬁne-tuned to the earlygame_keyword or\ncontractor_house datasets. BC ﬁne-tuning to either dataset improves performance, including (for\nthe contractor_house dataset) yielding wooden and stone tools. Proﬁcient Minecraft players take\na median of 1.2 minutes (1390 actions) to construct wooden tools and 2.3 minutes (2790 actions)\nto construct stone tools. (Right) Collection and crafting rates for VPT foundation model snapshots\nthroughout training after they are BC ﬁne-tuned to the contractor_house dataset. In general,\ncrafting-related behaviors increase throughout foundation model training. Fig. 4 deﬁnes the other\ntask terms (logs, planks, crafting tables, and total crafting).\n(iv)Sample videos: https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf3U3rSvG_BCWqJ869NdBhcP\n6\nFine-tuning to earlygame_keyword results in a large boost compared to the zero-shot foundation\nmodel: 2.5x more crafting tables, 6.1x more planks, 4.3x more logs, and 5.5x more crafting overall\n(Fig. 5). However, when ﬁne-tuning to this dataset we did not see any new behaviors emerge,\nonly a reﬁnement of existing skills. We saw an even bigger improvement when ﬁne-tuning to the\ncontractor_house dataset: 213x more crafting tables, 59x more wooden planks, 7x more logs,\nand 59x more crafting over all. In addition, we saw the emergence of crafting wooden tools, which\nrequires placing a crafting table on the ground, opening it to reveal a new crafting interface, and then\nusing it to craft wooden tools. This entire sequence takes a proﬁcient human player a median of 1.2\nminutes (1390 consecutive actions) to accomplish. The model goes further and collects cobblestone,\nwhich requires a wooden pickaxe to mine, and crafts stone tools, requiring it to again use a crafting\ntable; this takes a proﬁcient human player a median of 2.3 minutes (2790 consecutive actions). We\nalso saw this model more frequently raiding villages that randomly spawn in the game, hunting\nanimals for food, in addition to many behaviors we saw performed by the foundation model.(v)\nDespite the foundation model’s zero-shot rollout performance plateauing 1\/3 into training (Fig. 4,\nright), ﬁne-tuning performance does continue to increase throughout foundation model training\n(Fig. 5, right). Additionally, there is a stark difference in performance when training from scratch vs.\nﬁne-tuning from the VPT foundation model (Fig. 5 right, comparing the left and rightmost points).\n4.4\nFine-Tuning with Reinforcement Learning\nFigure 6: Typical sequence of items for obtaining a diamond pickaxe. Below each item is the median\ntime and number of actions contractors required to obtain that item and the percentage of contractors\nthat got the item within 10 minutes. The median time to obtain a diamond pickaxe is unknown (except\nthat it is > 20m) because contractors obtained this item in less than 50% of 20-minute episodes.\nTo demonstrate the efﬁcacy of RL ﬁne-tuning, we chose the challenging goal of obtaining a diamond\npickaxe within 10 minutes starting from a fresh Minecraft survival world. Doing so involves acquiring\na sequence of difﬁcult-to-obtain items that require complex skills like mining, inventory management,\ncrafting with and without a crafting table, tool use, operating a furnace, and mining at the lowest\ndepths, where many hazards like enemies and lava exist (Fig. 6). Adding to the difﬁculty, progress\ncan be easily lost by dropping items, destroying items, or dying. Obtaining a diamond pickaxe more\noften than not takes a proﬁcient human over 20 minutes (24,000 actions).\nAgents are rewarded for each item obtained in the sequence, with lower rewards for items that have to\nbe collected in bulk and higher rewards for items near the end of the sequence. Agents are optimized\nwith the phasic policy gradient64 RL algorithm for ∼1.3 million episodes (roughly 1.4×1010 frames).\nEpisodes last for 10 minutes. See Appendix G.1 for reward function and RL training details. Due to\ncomputational constraints, RL experiments use a ∼248 million parameter VPT model (Appendix H).\nA major problem when ﬁne-tuning with RL is catastrophic forgetting65,66 because previously learned\nskills can be lost before their value is realized. For instance, while our VPT foundation model never\nexhibits the entire sequence of behaviors required to smelt iron zero-shot, it did train on examples of\nplayers smelting with furnaces. It therefore may have some latent ability to smelt iron once the many\nprerequisites to do so have been performed. To combat the catastrophic forgetting of latent skills\nsuch that they can continually improve exploration throughout RL ﬁne-tuning, we add an auxiliary\nKullback-Leibler (KL) divergence loss between the RL model and the frozen pretrained policy.10\nTraining from a randomly initialized policy fails to achieve almost any reward, underscoring how\nhard an exploration challenge the diamond pickaxe task is for RL in the native human action space\n(Fig. 7a). The model never learns to reliably collect logs, typically the ﬁrst of many steps to obtaining\na diamond pickaxe (Fig. 7b). RL ﬁne-tuning from the VPT foundation model does substantially\nbetter (Fig. 7a), learning everything up to mining iron ore and crafting furnaces. (Fig. 7c). However,\nthis agent fails at smelting an iron ingot, the next item required to get further into the tech tree, likely\n(v)Sample Videos: https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf2yDSs4AqcoyPv4z_eWUiKm\n7\n0\n5\n10\n15\n20\n25\nReward\nReward over episodes\nRL from Rand. Init. model\nRL from VPT Found. model\nRL from Early-Game model\nNo KL-loss\n(a)\n0\n20\n40\n60\n80\n100\n% episodes\nRL from Rand. Init. model\nLogs\nPlanks\nSticks\nCrafting Tables\nWooden Pickaxe\nCobblestone\nStone Pickaxe\nCoal\nTorch\nFurnace\nIron Ore\nIron Ingot\nIron Pickaxe\nDiamonds\nDiamond Pickaxe\n(b)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nEpisodes\n1e6\n0\n20\n40\n60\n80\n100\n% episodes\nRL from VPT Found. model\n(c)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nEpisodes\n1e6\n0\n20\n40\n60\n80\n100\n% episodes\n2.5%\nRL from Early-Game model\n(d)\nFigure 7: RL Fine-tuning results. (a) RL from a randomly initialized model fails to get almost\nany reward, RL ﬁne-tuning from the VPT foundation model performs substantially better with a\nreward near 13, and RL ﬁne-tuning from the early-game model performs best with a reward of 25.\nWhen training the early-game model without a KL loss to the original policy (No KL-loss) progress\nstalls after 100,000 episodes, suggesting that the skills necessary to make further progress have been\ncatastrophically forgotten. (b) RL from a randomly initialized model occasionally collects sticks by\nbreaking leaves (an easy but inefﬁcient method of getting sticks that does not require logs or planks)\nand never learns to reliably collect logs. (c) RL ﬁne-tuning from the VPT Foundation model learns\neverything in the curriculum up to iron ore and making furnaces, but fails to learn to use the furnace to\nsmelt iron ingots. (d) RL ﬁne-tuning from the early-game model learns to obtain (at human-level) all\nitems in the sequence towards a diamond pickaxe and crafts a diamond pickaxe in 2.5% of episodes.\nbecause the zero-shot probability that the VPT foundation model smelts an iron ingot is too low, even\nwhen given the prerequisite materials.\nResults further improve by ﬁrst BC ﬁne-tuning the VPT Foundation Model to the\nearlygame_keyword dataset (the early-game model, Sec. 4.3) and then ﬁne-tuning with RL\n(Fig. 7a), which in preliminary experiments we found to perform better than ﬁrst ﬁne-tuning to\ncontractor_house followed by ﬁne-tuning with RL (Appendix G.2). The three-phase training\n(pretraining, BC ﬁne-tuning, and then RL ﬁne-tuning) succeeds in learning extremely difﬁcult tasks:\nit achieves over 80% reliability on iron pickaxes, almost 20% reliability on collecting diamonds, and\n2.5% reliability on obtaining a diamond pickaxe (Fig. 7d). For comparison, human players given\nthe objective of obtaining a diamond pickaxe collect these items in 57%, 15%, and 12% of episodes,\nrespectively, meaning our model is human-level for crafting iron pickaxes and mining diamonds.\nOthers have managed to obtain diamonds with ∼0.1% reliability in 15 minutes32,33 but always with a\nsimpliﬁed action space designed to ease exploration. To the best of our knowledge, we are the ﬁrst to\nreport non-zero success rates on crafting a diamond pickaxe. Qualitatively, the model developed\nuseful skills for diamond mining, such as efﬁcient mining patterns, cave exploration, returning to\npreviously placed objects like crafting tables, and advanced techniques like using wooden pickaxes\nas fuel when moving on to iron tools.(vi)\nFinally, we validated the importance of the KL loss to the pretrained model during RL ﬁne-tuning.\nThe treatment without a KL loss obtains only items early in the sequence (logs, planks, sticks, and\ncrafting tables) limiting its reward (Fig. 7a). This failure to progress further into the sequence is\nlikely because, while the initial skills of chopping logs and crafting planks are being learned with RL,\nsubsequent skills like crafting a wooden pickaxe are lost due to catastrophic forgetting.\n4.5\nData Scaling Properties of the Foundation Model\nIn this section we validate a core hypothesis behind this work: that it is far more effective to use\nlabeled contractor data to train an IDM within the VPT method than it is to directly train a BC\nfoundation model from that same small contractor dataset. If we could cheaply collect a labeled\ncontractor dataset of a similar order of magnitude as web_clean, then this would not be important;\nhowever, collecting that scale of data would have cost millions of dollars. Figure 8 compares\nfoundation models trained on increasing orders of magnitude of data from 1 hour up to the full ∼70k\nweb_clean dataset. Foundation models trained up to and including 1k hours are trained on the IDM\n(vi)Videos found at https:\/\/www.youtube.com\/playlist?list=PLNAOIb_agjf3e_UKweM5pQUSfTw8r-Wfc\n8\nTrained on\nContractor Data\nTrained on IDM\nLabeled Web Data\nFigure 8: (Left) Zero-shot rollout performance of foundation models trained on varying amounts\nof data. Models to the left of the dashed black line (points ≤1k hours) were trained on contractor\ndata (ground-truth labels), and models to the right were trained on IDM pseudo-labeled subsets\nof web_clean. Due to compute limitations, this analysis was performed with smaller (71 million\nparameter) models except for the ﬁnal point, which is the 0.5 billion parameter VPT foundation\nmodel. (Right) The corresponding performance of each model after BC ﬁne-tuning each model to\nthe contractor_house dataset.\ncontractor data, and those trained on 5k hours and above are trained on subsets of web_clean, which\ndoes not contain any IDM contractor data. Scaling training data increases log collection, mining, and\ncrafting capabilities. The zero-shot model only begins to start crafting crafting tables at over 5000\nhours of training data. When ﬁne-tuning each foundation model to contractor_house, we see that\ncrafting rates for crafting tables and wooden tools increase by orders of magnitude when using the\nentire ∼70k hour web_clean dataset. We furthermore only see the emergence of crafting stone tools\nat the largest data scale.\n4.6\nEffect of Inverse Dynamics Model Quality on Behavioral Cloning\nFigure 9: Zero-shot performance of BC models\ntrained from scratch on the earlygame_keyword\ndataset labeled with IDMs that were trained on\nincreasing amounts of contractor data.\nThis section investigates how downstream\nBC performance is affected by IDM qual-\nity.\nWe train IDMs on increasingly larger\ndatasets and use each to independently label\nthe earlygame_keyword dataset (this smaller\ndataset was chosen due to a limited compute bud-\nget). We then train a BC model from scratch on\neach dataset and report game statistics for each\nmodel as a function of IDM contractor dataset\nsize (Fig. 9).\nIDMs trained on at least 10 hours of data are\nrequired for any crafting, and the crafting rate\nincreases quickly up until 100 hours of data, after which there are few to no gains and differences are\nlikely due to noise. Similarly, crafting tables are only crafted after 50 or more hours of IDM data, and\nagain gains plateau after 100 hours. While in all previous experiments we use our best IDM trained\non 1962 hours of data, these results suggest we could reduce that number to as low as 100 hours.\n5\nDiscussion and Conclusion\nThe results presented in this paper help pave the path to utilizing the wealth of unlabeled data on the\nweb for sequential decision domains. Compared to generative video modeling or contrastive methods\nthat would only yield representational priors, VPT offers the exciting possibility of directly learning\nto act during pretraining and using these learned behavioral priors as extremely effective exploration\npriors for RL. VPT could even be a better general representation learning method even when the\ndownstream task is not learning to act in that domain—for example, ﬁne-tuning to explain what is\nhappening in a video—because arguably the most important information in any given scene would be\npresent in features trained to correctly predict the distribution over future human actions. We leave\nthis intriguing direction to future work.\nFuture work could improve results with more data (we estimate we could collect >1M hours) and\nlarger, better-tuned models. Furthermore, all the models in this work condition on past observations\nonly; we cannot ask the model to perform speciﬁc tasks. Appendix I presents preliminary experiments\non conditioning our models on closed captions (text transcripts of speech in videos), showing they\n9\nbecome weakly steerable; we believe this a rich direction for future research. Also, loss was not\nconsistently correlated with downstream evaluation metrics (Sec. 4.2), which often made progress\nslow and hard-won. Another fruitful future direction would be to investigate the correlation between\nvarious training metrics and downstream evaluations. Finally, while we do not anticipate any direct\nnegative societal impacts from the models trained in this work, as VPT improves and expands to other\ndomains it will be important to assess and mitigate harms that emerge with other forms of pretraining\non internet datasets, such as emulating inappropriate behavior.67\nIn conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from\nfreely available internet-scale data to sequential decision domains. Our models exhibited impressive\nzero-shot behavior and, when ﬁne-tuned with RL, achieved an unprecedented result of crafting a\ndiamond pickaxe in Minecraft (all the more difﬁcult given the human interface). We further showed\nthat contractor data is far better used within the VPT pipeline than to train a foundation model directly\nand that only a small amount of contractor data (about $2000 USD) was required to unlock massive\namounts of unlabeled online data for use in BC. Finally, learning with the human keyboard and mouse\ninterface is highly general and allows losslessly modeling the entire distribution of human behavior.\nWhile we only experiment in Minecraft, we believe that VPT provides a general recipe for training\nbehavioral priors in hard, yet generic, action spaces in any domain that has a large amount of freely\navailable unlabeled data, such as computer usage.\nReferences\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[3] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[4] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[5] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised\npretraining. In Proceedings of the European conference on computer vision (ECCV), pages\n181–196, 2018.\n[6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748–8763. PMLR, 2021.\n[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n[8] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-\ners. CoRR, abs\/2106.04560, 2021. URL https:\/\/arxiv.org\/abs\/2106.04560.\n[9] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489,\n2016.\n[10] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun-\nyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n10\n[11] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[12] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large\nscale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n[13] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew,\nand Igor Mordatch.\nEmergent tool use from multi-agent autocurricula.\narXiv preprint\narXiv:1909.07528, 2019.\n[14] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia\nCastaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al.\nHuman-level performance in 3d multiplayer games with population-based reinforcement learn-\ning. Science, 364(6443):859–865, 2019.\n[15] Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvit-\nskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human\nbenchmark. In International Conference on Machine Learning, pages 507–517. PMLR, 2020.\n[16] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi\nMunos.\nUnifying count-based exploration and intrinsic motivation.\nAdvances in neural\ninformation processing systems, 29, 2016.\n[17] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random\nnetwork distillation. arXiv preprint arXiv:1810.12894, 2018.\n[18] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return,\nthen explore. Nature, 590(7847):580–586, 2021.\n[19] Peter C Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair\nMuldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, et al. A data-driven\napproach for learning to control computers. arXiv preprint arXiv:2202.08137, 2022.\n[20] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of\nbits: An open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh,\neditors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of\nProceedings of Machine Learning Research, pages 3135–3144. PMLR, 06–11 Aug 2017. URL\nhttps:\/\/proceedings.mlr.press\/v70\/shi17a.html.\n[21] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,\nvolume 1, page 2, 2000.\n[22] Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from\nobservation. arXiv preprint arXiv:1905.13566, 2019.\n[23] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\ninformation processing systems, 29, 2016.\n[24] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv\npreprint arXiv:1805.01954, 2018.\n[25] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation:\nLearning to imitate behaviors from raw video via context translation. In 2018 IEEE International\nConference on Robotics and Automation (ICRA), pages 1118–1125. IEEE, 2018.\n[26] Twinﬁnite\nStaff.\nMost\nplayed\ngames\nin\n2021,\nranked\nby\npeak\nconcur-\nrent\nplayers.\nTwinﬁnite.\nURL\nhttps:\/\/twinfinite.net\/2021\/12\/\nmost-played-games-in-2020-ranked-by-peak-concurrent-players\/.\n[27] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv:1907.13440, 2019.\n11\n[28] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, and Shie Mannor. A deep\nhierarchical approach to lifelong learning in minecraft. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, volume 31, 2017.\n[29] Christian Scheller, Yanick Schraner, and Manfred Vogel. Sample efﬁcient reinforcement\nlearning through learning from demonstrations in minecraft. In NeurIPS 2019 Competition and\nDemonstration Track, pages 67–76. PMLR, 2020.\n[30] Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton,\nRaul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, et al. Multi-task\ncurriculum learning in a complex, visual, hard-exploration domain: Minecraft. arXiv preprint\narXiv:2106.14876, 2021.\n[31] Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, et al. Control of memory, active perception,\nand action in minecraft. In International Conference on Machine Learning, pages 2790–2799.\nPMLR, 2016.\n[32] Vihang P Patil, Markus Hofmarcher, Marius-Constantin Dinu, Matthias Dorfer, Patrick M Blies,\nJohannes Brandstetter, Jose A Arjona-Medina, and Sepp Hochreiter. Align-rudder: Learning\nfrom few demonstrations by reward redistribution. arXiv preprint arXiv:2009.14108, 2020.\n[33] Alexey Skrynnik, Aleksey Staroverov, Ermek Aitygulov, Kirill Aksenov, Vasilii Davydov, and\nAleksandr I Panov. Forgetful experience replay in hierarchical reinforcement learning from\ndemonstrations. arXiv preprint arXiv:2006.09939, 2020.\n[34] Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang.\nJuewu-mc:\nPlaying minecraft with sample-efﬁcient hierarchical reinforcement learning. arXiv preprint\narXiv:2112.04907, 2021.\n[35] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in\nneural information processing systems, 1, 1988.\n[36] Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences,\n3(6):233–242, 1999.\n[37] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot\nlearning from demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.\n[38] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning:\nA survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1–35, 2017.\n[39] Claude Sammut, Scott Hurst, Dana Kedzier, and Donald Michie. Learning to ﬂy. In Machine\nLearning Proceedings 1992, pages 385–393. Elsevier, 1992.\n[40] Alessandro Giusti, Jérôme Guzzi, Dan C Cire¸san, Fang-Lin He, Juan P Rodríguez, Flavio\nFontana, Matthias Faessler, Christian Forster, Jürgen Schmidhuber, Gianni Di Caro, et al. A\nmachine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics\nand Automation Letters, 1(2):661–667, 2015.\n[41] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon\nGoyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning\nfor self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\n[42] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, and Alexey Dosovitskiy.\nEnd-to-end driving via conditional imitation learning. In 2018 IEEE international conference\non robotics and automation (ICRA), pages 4693–4700. IEEE, 2018.\n[43] Rémi Coulom. Computing “elo ratings” of move patterns in the game of go. ICGA journal, 30\n(4):198–208, 2007.\n[44] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,\nJohn Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.\n12\n[45] Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. Imitating latent\npolicies from observation. In International conference on machine learning, pages 1755–1763.\nPMLR, 2019.\n[46] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, and Sergey Levine. Sfv:\nReinforcement learning of physical skills from videos. ACM Transactions On Graphics (TOG),\n37(6):1–14, 2018.\n[47] Feryal Behbahani, Kyriacos Shiarlis, Xi Chen, Vitaly Kurin, Sudhanshu Kasewa, Ciprian Stirbu,\nJoao Gomes, Supratik Paul, Frans A Oliehoek, Joao Messias, et al. Learning from demonstration\nin the wild. In 2019 International Conference on Robotics and Automation (ICRA), pages\n775–781. IEEE, 2019.\n[48] Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando De Fre-\nitas. Playing hard exploration games by watching youtube. Advances in neural information\nprocessing systems, 31, 2018.\n[49] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu,\nEvan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. Zero-shot visual imitation.\nIn ICLR, 2018.\n[50] Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, and Sergey\nLevine. Combining self-supervised learning and imitation for vision-based rope manipulation.\npages 2146–2153, 05 2017. doi: 10.1109\/ICRA.2017.7989247.\n[51] Duy Nguyen-Tuong, Jan Peters, Matthias Seeger, and Bernhard Schölkopf. Learning inverse\ndynamics: a comparison. In European symposium on artiﬁcial neural networks, number CONF,\n2008.\n[52] David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, and Robert E Schapire.\nExploratory gradient boosting for reinforcement learning in complex domains. arXiv preprint\narXiv:1603.04119, 2016.\n[53] Dilip Arumugam, Jun Ki Lee, Sophie Saskin, and Michael L Littman. Deep reinforcement\nlearning from policy-dependent human feedback. arXiv preprint arXiv:1902.04257, 2019.\n[54] Alexander Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. Keeping your dis-\ntance: Solving sparse reward tasks using self-balancing shaped rewards. Advances in Neural\nInformation Processing Systems, 32, 2019.\n[55] Stephan Alaniz. Deep reinforcement learning with model learning and monte carlo tree search\nin minecraft. arXiv preprint arXiv:1803.08456, 2018.\n[56] Hiroto Udagawa, Tarun Narasimhan, and Shim-Young Lee. Fighting zombies in minecraft with\ndeep reinforcement learning. Technical report, Technical report, Technical report, Stanford\nUniversity, 2016.\n[57] Tianmin Shu, Caiming Xiong, and Richard Socher. Hierarchical and interpretable skill acquisi-\ntion in multi-task reinforcement learning. arXiv preprint arXiv:1712.07294, 2017.\n[58] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization\nwith multi-task deep reinforcement learning. In International Conference on Machine Learning,\npages 2661–2670. PMLR, 2017.\n[59] Zhengxiang Shi, Yue Feng, and Aldo Lipani. Learning to execute or ask clariﬁcation questions.\narXiv preprint arXiv:2204.08373, 2022.\n[60] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher–student curriculum\nlearning. IEEE transactions on neural networks and learning systems, 31(9):3732–3740, 2019.\n[61] Robert George Douglas Steel, James Hiram Torrie, et al. Principles and procedures of statistics.\nPrinciples and procedures of statistics., 1960.\n13\n[62] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[63] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\n[64] Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In\nMarina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on\nMachine Learning, volume 139 of Proceedings of Machine Learning Research, pages 2020–\n2027. PMLR, 18–24 Jul 2021. URL https:\/\/proceedings.mlr.press\/v139\/cobbe21a.\nhtml.\n[65] Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas\nBlackiston, Josh Bongard, Andrew P Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune,\net al. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4\n(3):196–210, 2022.\n[66] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.\nOvercoming catastrophic forgetting in neural networks. Proceedings of the national academy of\nsciences, 114(13):3521–3526, 2017.\n[67] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\nthe dangers of stochastic parrots: Can language models be too big???. In Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610–623, 2021.\n[68] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion,\nOlivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-\nlearn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830,\n2011.\n[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[70] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[71] Yann A LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efﬁcient backprop.\nIn Neural networks: Tricks of the trade, pages 9–48. Springer, 2012.\n[72] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[73] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\npages 8024–8035. Curran Associates, Inc., 2019. URL http:\/\/papers.neurips.cc\/paper\/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\n[74] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and\nYoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-\nconvex optimization. Advances in neural information processing systems, 27, 2014.\n[75] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in\ndeep neural networks? Advances in neural information processing systems, 27, 2014.\n14\n[76] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[77] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[78] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation.\narXiv preprint\narXiv:1506.02438, 2015.\n[79] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3):229–256, 1992.\n[80] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-\nment learning. In International conference on machine learning, pages 1928–1937. PMLR,\n2016.\n[81] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience\nreplay. Advances in neural information processing systems, 30, 2017.\n[82] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function ap-\nproximators. In International conference on machine learning, pages 1312–1320. PMLR,\n2015.\n[83] Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck,\nJakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-\nended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021.\n[84] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward\nGrefenstette, Shimon Whiteson, and Tim Rocktäschel.\nA survey of reinforcement learn-\ning informed by natural language. In Proceedings of the Twenty-Eighth International Joint\nConference on Artiﬁcial Intelligence, IJCAI-19, pages 6309–6317. International Joint Confer-\nences on Artiﬁcial Intelligence Organization, 7 2019. doi: 10.24963\/ijcai.2019\/880. URL\nhttps:\/\/doi.org\/10.24963\/ijcai.2019\/880.\n[85] DeepMind Interactive Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico\nCarnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Tim Harley, et al. Creating\nmultimodal interactive agents with imitation and self-supervised learning. arXiv preprint\narXiv:2112.03763, 2021.\n[86] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[87] Dong Yu and Li Deng. Automatic speech recognition, volume 1. Springer, 2016.\n[88] Daulet Nurmanbetov. rpunct, May 25 2021. URL https:\/\/github.com\/Felflare\/rpunct.\naccessed 2022-04-22.\n[89] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek,\nQiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings\nby contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.\nAcknowledgements\nWe thank the following people for helpful discussions and support: Bob McGrew, Ken Stanley,\nJoel Lehman, Ilya Sutskever, Wojciech Zaremba, Ingmar Kanitscheider, David Farhi, Glenn Powell,\nJonathan Gordon, and the OpenAI supercomputing team, especially Christian Gibson, Ben Chess,\nand Christopher Berner.\n15\nSupplementary Information\nA\nCollecting Internet Data\nA.1\nInitial Unclean Dataset Curation\nOur goal was to curate a video dataset of Minecraft gameplay from the survival game mode. Addition-\nally, we prefer the data come from game modes as close as possible to our evaluation environment,\nmeaning preferably coming from Minecraft version 1.16, being on a computer (which uses a mouse\nand keyboard vs. video game controllers with keypads and other buttons), being single- (vs. multi-)\nplayer, and having the default look of the game (vs. modiﬁcations that alter that style, such as to\nmake it look realistic). To try to accomplish these goals, we collect a dataset by performing keyword\nsearches of publicly available videos on the internet. A list of search queries we used are given in\nTable 1.\nminecraft survival longplay\nminecraft gameplay no webcam\nminecraft gameplay survival mode\nminecraft survival tutorial\nminecraft survival guide\nminecraft survival let’s play\nminecraft survival for beginners\nminecraft beginners guide\nultimate minecraft starter guide\nminecraft survival guide 1.16\nminecraft how to start a new survival world\nminecraft survival fresh start\nminecraft survival let’s play episode 1\nlet’s play minecraft episode 1\nminecraft survival 101\nminecraft survival learning to play\nhow to play minecraft survival\nhow to play minecraft\nminecraft survival basic\nminecraft survival for noobs\nminecraft survival for dummies\nhow to play minecraft for beginners\nminecraft survival tutorial series\nminecraft survival new world\nminecraft survival a new beginning\nminecraft survival episodio 1\nminecraft survival epizod 1\nminecraft survival 1. bölüm\ni made a new minecraft survival world\nTable 1: Search terms used for generating the initial web dataset.\nFor videos that have metadata available, we perform an additional step of metadata-based ﬁltering\nto eliminate videos that do not ﬁt our target distribution. In this step, we look for a list of blacklist\nkeywords in the video title and description and reject videos that contain these terms. The blacklist\nkeywords we use are: {ps3, ps4, ps5, xbox 360, playstation, timelapse, multiplayer, minecraft pe,\npocket edition, skyblock, realistic minecraft, how to install, how to download, realmcraft, animation}.\nThis process yielded us ∼270k hours of unlabeled data, which we ﬁlter down to only a “clean” subset\nas described in the next section.\nA.2\nTraining a Model to Filter out Unclean Video Segments\nWe restrict the scope of this work to the Minecraft Survival game mode and therefore limit our\ntraining dataset to clips that are obtained from this mode that are relatively free from visual artifacts.\n16\nTo do so, we asked contractors to label a set of random video frames (images) from Minecraft videos\n(N=8800). These images were from a random subset of the videos we collected toward the beginning\nof the project (Section A.1).\nA.2.1\nLabel Collection\nWe asked 5 workers on Amazon Mechanical Turk (mTurk) that we selected with a sample qualiﬁcation\ntask to label random screen capture images to be used in training the classiﬁer. A sample worker\ninterface that the workers saw on mTurk is given in Figure 10.\nWe asked workers to label videos as being in one of the following three categories (see Figure 11 for\nvisual examples of each class):\n1. Minecraft Survival Mode - No Artifacts: Video frames (images) that correspond\nto the Minecraft Survival game mode that do not contain any non-game visual artifacts (e.g.\nsubscribe buttons, channel logos, advertisements, picture-in-picture of the narrator, etc.).\n2. Minecraft Survival Mode - with Artifacts:\nVideo frames (images) of the\nMinecraft Survival game mode that include such visual artifacts.\n3. None of the Above: Video frames (images) that are not from the Minecraft survival\ngame mode, including those from other Minecraft game modes such as creative mode or\neven other games\/topics entirely.\nThe full set of instructions workers received are as follows (note that we also included multiple image\nexamples from each category in the worker instructions, similar to the sample subset provided in\nFigure 11):\nPlease help us identify screenshots that belong only to the survival mode in Minecraft. Everything\nelse (Minecraft creative mode, other games, music videos, etc.) should be marked as None of the\nabove. Survival mode is identiﬁed by the info at the bottom of the screen:\n• a health bar (row of hearts)\n• a hunger bar (row of chicken drumsticks)\n• a bar showing items held\nSurvival Mode\nValid survival mode videos have health\/hunger bars and an item hotbar at the bottom of the screen.\nCreative Mode\nCreative mode only has an item hotbar and should be classiﬁed as None of the Above.\nLabel Descriptions\n• Minecraft Survival Mode - No Artifacts: These images will be clean screenshots\nfrom the Minecraft survival mode gameplay without any noticeable artifacts.\n• Minecraft Survival Mode - with Artifacts: These images will be valid survival\nmode screenshots, but with some added artifacts. Typical artifacts may include image\noverlays (a logo\/brand), text annotations, a picture-in-picture of the player, etc.\n• None of the Above: Use this category when the image is not a valid Minecraft survival\nscreenshot. It may be a non-Minecraft frame or from a different game mode. In non-survival\ngame modes such as the creative mode, the health\/hunger bars will be missing from the\nimage, the item hotbar may or may not be still present.\nIn total, we spent $319.96 on human labeling experiments on mTurk, of which $159.98 was directly\npaid to workers. The remaining amount was spent towards Amazon platform fees. The workers\nreceived $0.01 per labeled image, at an hourly compensation of $7.20 (based on an estimated labeling\ntime of 5 seconds\/image – in our internal sample run of the same task, we found the average labeling\ntime to be < 3 seconds).\nSince we perform rigorous keyword and metadata based ﬁltering of videos (as described in A.1) from\nwhich we served sample images to be labeled, serving offensive content to workers was extremely\n17\nlow risk and no such images were detected during our manual checks. We only collected labels\nduring our experiment, and the workers were fully anonymized via the mTurk platform, therefore no\npersonally identiﬁable information (PII) was collected.\nFigure 10: Amazon Mechanical Turk worker interface showing an example labeling task\nFigure 11: (Left) Sample image for Class 1: Minecraft Survival Mode - No Artifacts.\n(Middle) Sample image for Class 2: Minecraft Survival Mode - with Artifacts – Image\ncontains annotations and picture-in-picture of the narrator. (Right) Sample image for Class 3: None\nof the Above – Image is missing the hotbar as well as health and armor bars, indicating that it was\nnot captured during survival mode gameplay\nA.2.2\nSVM Training\nWith the image labels collected as described in the previous section, we trained a classiﬁer to extract\nvideo segments that consist of frames from the Minecraft Survival Mode - No Artifacts\ncategory. Given a set of labeled images, we obtain embeddings for each image using the RN50x64\nResNet CLIP Model.6 This is a ResNet-based CLIP model that is scaled up to have approximately\n64x the compute of a ResNet-50. We then train a Support Vector Machine (SVM) using the RBF\nkernel to obtain a frame classiﬁer. We use the Scikit-learn68 SVM implementation with the parameter\nconﬁguration given in Table 2.\nFinally, we apply the classiﬁer to frames of raw video sequences at a rate of 3 frames\/second. We ﬁlter\nfor videos that consist of at least 80% \"clean\" frames at this stage (Classes Minecraft Survival\nMode - with Artifacts and None of the Above are both considered not clean). From this set,\nwe apply a median ﬁlter (with a kernel size of 7) to the labels and segment videos by splitting the\n\"clean\" segments that are at least 5s in duration. The result of this is our ﬁnal web_clean dataset.\nA.3\nearly_game Dataset\nThe early_game dataset is a ∼3000 hour subset of web_clean targeted at “early game” Minecraft\nbehavior, i.e. instances where players start in a fresh world with no items. We obtain the metadata\ntext that accompanies the videos in web_clean and determine whether any of the following regular\nexpressions match:\n18\nCLIP Model Speciﬁcation\nRN50x64 (see text)\nCLIP Input Image Resolution\n448x448x3\nCLIP Embedding Feature Length\n1024\nSVM Parameters\nKernel\nrbf\nC\n20\nGamma\nscale\nSample Size\nClass 1\n2200\nClass 2\n2200\nClass 3\n4400\nTable 2: Feature Extraction Details and SVM Conﬁguration. The parameters are for the SVM\nimplementation in Scikit-learn68.\n• (ep|episode|eps|day|session|sesh|chapter|chap\n.|series|part|parte|pt|round|day|tâ.p|bölüm|episodio|epizod|\u000bpizod)(\n)*(\\.1|#1|1|\\.01|#01|01|one[ˆ0-9]|$)\n• start\n• beginning\n• (new|fresh|clean).*(world|game|play)\n• from scratch\nFrom this set of videos, we take only the ﬁrst 5 minutes of each video.\nB\nContractor Data\nB.1\nRecording Contractor Play\nOur contractors use a custom Minecraft recorder that we built that records their actions and game video\nfeeds as they play. The recorder is implemented using the MCP-Reborn (github.com\/Hexeption\/MCP-\nReborn) modding package. To ensure that the recorder environment is as close as possible to\nthe Minecraft environment used for RL rollouts and evaluations (Appendix C), we use the same\nunderlying game engine for both. The recorder is a Java app that runs in a window mode, with\nconstant resolution of 1280x760. Brightness is set to 0 (the \"gloomy\" setting in Minecraft), which\nis the default setting. Other graphics settings (ﬁeld of view, GUI scale) are ﬁxed to the values used\nin the Minecraft environment (C.1); we explicitly prevented users from changing graphics settings.\nUnlike the environment, the recorder allows all keyboard key presses and continuous (as opposed to\nbinned) mouse actions. On every game step (or “tick”) the frame buffer used to display the game\nwindow is downsized to 640x360 and written into a video ﬁle. In-game actions are recorded in a\nseparate JSONL ﬁle (a text ﬁle where each line is a JSON-formatted string). All recordings are\nchunked into 5 minute clips: after each 5 minute segment of contractor game play the recorder\nautomatically uploads the video ﬁle, the JSONL ﬁle with actions, as well as a Minecraft state ﬁle.\nTo ensure that contractors cannot corrupt each other’s data, we provided every contractor with an\nindividual cloud bucket, as well as with credentials giving write access only to that bucket. Credentials\nalso included adjective-adjective-noun names (e.g. grumpy-amethyst-chipmunk), generated with the\nnamegenerator python package to ensure contractor anonymity when we publish the data.\nB.2\nContractor Contract\nWe recruited contractors by posting the following offer on the UpWork freelancing platform.\n“We are collecting data for training AI models in Minecraft. You’ll need to install\njava, download the modiﬁed version of Minecraft (that collects and uploads your\nplay data), and play Minecraft survival mode! Paid per hour of gameplay. Prior\nexperience in Minecraft not necessary. We do not collect any data that is unrelated\nto Minecraft from your computer.”\n19\nWe had the applications open for a day, and then randomly selected 10 applicants for the ﬁrst round of\ncontractors. Later in the project, as we needed more data and as some contractors asked to terminate\ntheir contracts, we added more applicants from the original pool as well as referrals from the currently\nworking contractors. The contractors were paid $20 per hour (minus Upwork platform fees and\napplicable taxes). All of the results presented in this paper are based on about 4,500 hours of data\n(including data recorded to gather statistics of human play that was not used for training), which cost\nus around $90,000. Over the course of the project, we collected some data we did not use due to\nbugs in the recorder and for some ideas we ultimately did not pursue. In total, we spent about $160k\nfor contractor compensation over the course of the project. However, as we discuss in Sec. 4.6, we\ncould likely obtain most of our results with an IDM trained using only $2000 worth of data, i.e. the\nfoundation VPT model, BC ﬁne-tuning to the earlygame_keyword dataset, and the RL ﬁne-tuning\nresults. Collecting the contractor_house dataset cost about $8000. Because we used the IDM\ntrained on about 2000 hours of contractor data, the actual cost of contractor data for those results was\naround $40,000.\nIn early stages of the project, we were planning to use contractor data solely for the purpose of training\nthe IDM. As such, no speciﬁc tasks were given, other than “play the survival mode of Minecraft like\nyou normally would.” Later in the project, we requested that contractors perform speciﬁc tasks in\nMinecraft, such as:\n• Collect as many units of wood as possible, using only wooden or stone tools (treechop)\n• Start a new world every 30 minutes of game play\n• Build a basic house in 10 minutes using only dirt, wood, sand, and either wooden or stone\ntools (contractor_house, more details below in Appendix B.4).\n• Starting from a new world and an empty inventory, ﬁnd resources and craft a diamond\npickaxe in 20 minutes (obtain_diamond_pickaxe). This dataset was used to obtain\nstatistics for how long it takes humans on average to complete this task (and the subtasks\nrequired to complete it) when obtaining a diamond pickaxe is their goal.\nSince we only recorded in-game events and videos, the data does not include personally identiﬁable\ninformation. That being said, the contractors could theoretically use Minecraft’s open-world property\nto generate personally identiﬁable information and\/or offensive content (e.g. by using Minecraft\nblocks to write their name or offensive messages, then ﬁnding a spot from which the message would\nbe visible). In practice, we have not seen any attempts to do so in the contractor videos that we\nwatched. Of course, we train our BC models on videos from the internet of people playing Minecraft,\nand if such behavior is in those videos our model could also potentially learn it, although we expect\nsuch behavior is rare enough that our model would not be likely to reproduce it.\nB.3\nData for the Inverse Dynamics Model.\nSince the IDM’s task is to infer actions given the video, any labelled data is appropriate for IDM\ntraining. In practice, we included general gameplay as well as the treechop task data described\nin the previous section, which amounted to a total of 1962 hours. Due to collecting datasets like\ncontractor_house only at late stages of the project, they were not included in IDM training.\nB.4\ncontractor_house.\nThe contractor_house contains about 420 hours of data. We asked contractors to build a basic\nhouse in 10 minutes, using only basic dirt, wood, and sand, blocks. Each trajectory starts in a newly\ngenerated world and a timer forcibly ends a trajectory after a 20 minute time limit. For this task, many\ncontractors chose to begin their trajectories by crafting basic tools and building blocks, speciﬁcally\nit was common for the ﬁrst 2 minutes to be spent crafting a wooden pickaxe and then mining stone\nfor an assortment of stone tools before gathering more building blocks and beginning to create their\nstructure.\nC\nMinecraft environment details\nOur Minecraft training environment is a hybrid between MineRL27 and the MCP-Reborn\n(github.com\/Hexeption\/MCP-Reborn) Minecraft modding package. Unlike the regular Minecraft\n20\nFigure 12: (Left) Sample of a Minecraft frame in the original resolution (640x360) with an in-game\nGUI open. The mouse cursor can be seen in the center of the image. This particular GUI shows the\nplayer’s inventory and can be used to craft very basic items. (Middle) We downsample images to\n128x128 for computational reasons. Shown is a downsampled observation with an in-game GUI for\ncrafting. This is the resolution used by our models. (Right) A 128x128 observation as seen by our\nmodels without in-game GUI. The health, hunger, hotbar overlays, and agent hand can be seen in the\nlower part of the image.\ngame, in which the server (or the \"world\") always runs at 20Hz and the client runs as fast as rendering\ncan complete (typically at 60-100Hz), in our version the client and server run in the same thread\nat the same frequency. This allows us to run the environment slower or faster than real time, while\navoiding artifacts like missing chunks of the world. The action and observation spaces are similar\nto those of MineRL environments and are described in more detail in the following subsections.\nThe environment also returns diagnostic information, such as in-game stats, contents of the agent’s\ninventory, whether any in-game GUI is open, etc., which we use for tracking and recording but not as\ninputs to the models. The episode length is 10 minutes for RL experiments and 60 minutes for BC\nmodel evaluations. The agent can \"die\" in a number of ways, such as staying under water for too long\nand drowning, being killed by hostile mobs, or falling from a tall structure. We do not terminate the\nepisode on agent \"death\". Instead, just as for humans in the regular Minecraft game, the agent drops\nall its items when it dies and respawns at a random spot close to the initial spawning spot in the same\nMinecraft world. The policy state is not masked on death, so the model can remember the fact that it\nhas died and act accordingly.\nC.1\nObservation space\nThe environment observations are simply the raw pixels from the Minecraft game that a human\nwould see. Unlike MineRL, we do not remove overlays like the hotbar, health indicators, and the\nanimation of a moving hand shown in response to the attack or “use” actions. The ﬁeld of view is\n70 degrees, which corresponds to the Minecraft default. GUI scale (a parameter controlling the size\nof the in-game GUI) is set to 2, and brightness is set to 2 (which is not a Minecraft default, but is\nvery frequently used in online videos). The rendering resolution is 640x360, which is downsampled\nto 128x128 before being input to the models. We empirically found 128x128 to be the smallest\nresolution for which in-game GUI elements are still discernible, and then chose that to minimize\ncompute costs. Whenever an in-game GUI is open, we additionally render an image of a mouse\ncursor at the appropriate mouse position to match what a human player’s operating system does (Fig.\n12).\nC.2\nAction space\nOur action space includes almost all actions directly available to human players, such as keypresses,\nmouse movements, and clicks. The speciﬁc binary actions we include are shown in Table 3.\nOne difference between the human action space and our agent’s is that we disallow typing arbitrary\nletters, which is only useful for entering text into the search bar of the crafting recipe book. Humans\ncan either do that or browse the recipe book with the mouse, the latter of which our agent can still\ndo. However, because we do allow the agent to press letters that are also shortcuts for actions (e.g.\noutside of the GUI, the \"W\" key triggers the forward action) agents are able to press a few keys\nwithin the GUI (W, A, S, D, E, Q) that produce letters if the recipe book search bar is selected. We\nhave not seen agents attempt to search the recipe book with these letters. Instead, our agents navigate\nthe recipe book with the mouse or craft by dragging items around the crafting window.\n21\nAction\nHuman action\nDescription\nforward\nW key\nMove forward.\nback\nS key\nMove backward.\nleft\nA key\nStrafe left.\nright\nD key\nStrafe right.\njump\nspace key\nJump.\ninventory\nE key\nOpen or close inventory and the 2x2 crafting grid.\nsneak\nshift key\nMove carefully in current direction of motion. In the\nGUI it acts as a modiﬁer key: when used with attack\nit moves item from\/to the inventory to\/from the hot-\nbar, and when used with craft it crafts the maximum\nnumber of items possible instead of just 1.\nsprint\nctrl key\nMove fast in the current direction of motion.\nattack\nleft mouse button\nAttack; In GUI, pick up the stack of items or place the\nstack of items in a GUI cell; when used as a double\nclick (attack - no attack - attack sequence), collect all\nitems of the same kind present in inventory as a single\nstack.\nuse\nright mouse button\nPlace the item currently held or use the block the player\nis looking at. In GUI, pick up the stack of items or place\na single item from a stack held by mouse.\ndrop\nQ key\nDrop a single item from the stack of items the player\nis currently holding. If the player presses ctrl-Q then\nit drops the entire stack. In the GUI, the same thing\nhappens except to the item the mouse is hovering over.\nhotbar.[1-9]\nkeys 1 – 9\nSwitch active item to the one in a given hotbar cell.\nTable 3: Binary actions included in the action space. https:\/\/minecraft.fandom.com\/wiki\/\nControls has more detailed descriptions of each action.\nIn addition to the binary (on\/off) keypress actions, our action space also includes mouse movements.\nAs with human gameplay, when in-game GUIs are not open, mouse X and Y actions change the\nagent’s yaw and pitch, respectively. When a GUI is open, camera actions move the mouse cursor.\nMouse movements are relative (i.e. they move the mouse or camera relative to the current position,\nand thus their effect depends on the current position).\nInventory interaction in Minecraft requires ﬁne-grained mouse movements to achieve tasks such as\ncrafting and smelting, while mining and navigating the world can be achieved with coarser mouse\naction. To be able to achieve both with the same action space, we implemented mouse movements\nas a set of discrete actions with foveated binning along each axis (Fig. 13), which in preliminary\nexperiments we found to improve crafting performance.\nD\nInverse Dynamics Model Training Details\nD.1\nIDM Architecture\nThe IDM model has approximately 0.5 billion trainable weights. The input to the IDM is 128\nconsecutive image frames (128 frames of video), each of which has dimensions 128 × 128 × 3. The\nIDM is tasked with predicting the action at each frame. All image pixel values are ﬁrst divided\nby 255.0 such that they lie within the range [0, 1]. The ﬁrst layer of the IDM is a 3-D convolution\nwith 128 learnable ﬁlters with a temporal kernel width of 5 and spatial kernel widths of 1. This\nconvolution is non-causal, meaning that embeddings at time index t are functions of pixel values at\ntimes t −2, t −1, t, t + 1, and t + 2. We found this layer to be extremely important in IDM training\n22\n−80 −60 −40 −20\n0\n20\n40\n60\n80\nMouse movement (pixels)\n−10\n−5\n0\n5\n10\nCamera angle (deg)\n0\n2\n4\n6\n8\n10\nCamera bin\nFigure 13: Relative camera angle or mouse movement in pixels vs. action bin. The same binning is\nused for both X and Y coordinates. The binning is foveated, meaning that binning is more ﬁne-grained\nfor smaller movements and more coarse-grained for larger movements. There are 11 bins for each\naxis (X and Y). The center of each bin (indicated with green circles) is used when un-discretizing\nmovements (that is, when converting from an action expressed as a bin to a camera angle or mouse\nmovement).\nas it incorporates neighboring temporal information immediately, and we show results comparing\nIDM performance with and without this layer in Figure 14. This comparison was made on the default\n(1962-hour) IDM dataset.\n0\n5\n10\n15\n20\n25\n30\n35\nTraining Progress (Epoch)\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLoss\nWith 3D Conv\nNo 3D Conv\n0\n5\n10\n15\n20\n25\n30\n35\nTraining Progress (Epoch)\n0.0\n0.2\n0.4\n0.6\n0.8\nKeypress Accuracy\nWith 3D Conv\nNo 3D Conv\n0\n5\n10\n15\n20\n25\n30\n35\nTraining Progress (Epoch)\n6\n5\n4\n3\n2\n1\n0\n1\nMouse R2\nWith 3D Conv\nNo 3D Conv\nFigure 14: Effect of 3-D Convolution in the IDM Architecture.\nThis initial temporal convolutional layer is followed by a ResNet62 image processing network. In this\npart of the model, no extra temporal information is shared between neighboring frames; however,\nsince each frame was ﬁrst processed with the temporal convolution, some temporal information\nis present at this stage. The ResNet image processing network is comprised of three subsequent\nstacks with widths W = {64, 128, 128}. Each stack is comprised of, in order, (1) an initial 3x3\nconvolutional layer with 1-pixel zero padding at the embedding boundary (such that the outgoing\nembedding dimensions are the same as the incoming embedding dimension) with W output channels,\n(2) a 3x3 max pooling with stride 2 and padding 1 such that the embedding width and height are\nhalved, and (3) two classic ResNet blocks as deﬁned in He et al. 62 with each layer also having W\noutput channels.\nThe output of the ResNet stack is ﬂattened into a 1-dimensional vector of size 217 = 131072 (one\nvector for each frame in the video) such that at this stage there are 128 vectors of size 131072. Each\nvector is independently processed with two frame-wise dense layers with 256 output activations and\nthen 4096 output activations, respectively. The result is then fed through 4 subsequent non-causal\n(umasked) residual transformer69 blocks. Each block ﬁrst has an unmasked attention layer, i.e. frames\nmay attend to future frames, with 32 attention heads of dimension 128 each and a surrounding residual\nconnection that skips this layer. The embedding is then passed through a frame-wise dense layer\nwith output dimension 16384 and another with output dimension returning to 4096; a single residual\nconnection skips past this pair of frame-wise dense layers (not skipping past each layer separately,\nbut skipping the pair). All dense layers have their weights tied through time, so each frame in the\nvideo is processed with the same weights.\nFinally, independent dense layer heads for each action are pulled from the ﬁnal embedding – a 2\nclass on\/off categorical parameterized with a softmax for each available key as well as a 11-way\n23\ncategorical for both the discretized horizontal and vertical mouse movements (See Appendix C.2 for\ndetails on the action space).\nEach dense layer or convolutional layer in the network is preceded by a layernorm70 and followed by\na ReLU non-linearity. Weights are initialized with Fan-In initialization71 and biases are initialized to\nzero.\nD.2\nIDM Training\nThe total loss for the network is the sum of each independent action prediction loss (one for each\nkey and one for both mouse directions). Each independent loss is the negative log-likelihood of the\ncorrect action. We use the ADAM72 optimizer with a linear learning rate decay. We use an initial\nlearning rate of 0.003, a batch size of 128 (where each item in the batch is a video sequence of 128\nframes), and a weight decay of 0.01. Hyperparameters were tuned in preliminary experiments. The\nIDM is trained on our contractor collected dataset for 20 epochs. This took 4 days on 32 A100 GPUs.\nWe add data augmentation to each video segment; augmentations are randomly sampled once per\nsegment such they are temporally consistent. Using the Pytorch73 transforms library, we adjust the\nhue by a random factor between -0.2 and 0.2, saturation between 0.8 and 1.2, brightness between\n0.8 and 1.2, and contrast between 0.8 and 1.2. We also randomly rotate the image between -2 and 2\ndegrees, scale it by a random factor between 0.98 and 1.02, shear it between -2 and 2 degrees, and\ntranslate it between -2 and 2 pixels in both the x and y dimensions.\nDue the large computational cost of running all of the experiments in this paper, training results\nare from one run of training (for IDM, BC, and RL training): this non-ideal situation is mitigated\nbecause deep learning training tends to be low variance74,75 and because we often have data points\nfrom sweeps (e.g. on dataset size) that suggest overall trends.\nD.3\nGenerating Pseudo Labels with the IDM\nSection 4.1 shows that inverse dynamics modeling is a much easier task than behavioral cloning\nbecause IDMs can be non-causal. The IDM is trained to simultaneously predict all 128 actions for\neach video sequence, so the IDM will effectively be causal for frames at the end of the video clip\nbecause future frames are not included in the sequence. For this reason, we apply the IDM over a\nvideo using a sliding window with stride 64 frames and only use the pseudo-label prediction for\nframes 32 to 96 (the center 64 frames). By doing this, the IDM prediction at the boundary of the\nvideo clip is never used except for the ﬁrst and last frames of a full video.\nE\nFoundation Model Behavioral Cloning\nE.1\nFoundation Model Architecture\nThe behavioral cloning model architecture is the same as the IDM architecture described in Appendix\nD.1 except that we modify the architecture so that it is causal (i.e. cannot see the future when making\npredictions). This means the BC architecture does not have the initial non-causal convolution the\nIDM has (this layer is omitted completely). Furthermore, the residual transformer layers are now\ncausally masked (as is standard in language modeling) and we do Transformer-XL-style76 training\nwhere frames can attend to keys and values from past batches within the same video. We also use a\nTransformer-XL-style relative attention position embedding.\nE.2\nNull Action Filtering\nThe most common action humans take is the null action (no keypresses or mouse movements), which\naccounts for 35% of all actions they take. Among other reasons, a player may take the null action to\nwait for something in the game to ﬁnish, to pause between actions, or to take a break to grab a glass\nof water. Early on in the project we found that the BC model would take a much larger fraction than\n35% of null actions, often upwards of 95%. In order to prevent this behavior we removed frames with\nnull actions from the dataset. We compare a few different treatments: we ﬁlter nulls if there have\nbeen 1, 3, or 21 frames of consecutive null actions, and include a treatment that does not perform\nany null ﬁltering. Null action ﬁltering generally helps, increasing all crafting rates (Figure 15 left).\n24\nbasic mining\nlogs\nplanks\ncrafting tables\ntotal crafting\nnull actions\n10 2\n100\n102\n104\nEffect of Null Action Filtering\nNone\nAll\nGroups of 3\nGroups of 21\nbasic mining\nlogs\nplanks\ncrafting tables\ntotal crafting\nnull actions\n10 3\n10 2\n10 1\n100\n101\n102\n103\nJoint Hierarchical vs Factored Action Spaces\nFactored\nHierarchical Joint\nFigure 15: (Left) Effect of Null Action Filtering during training. We compare environment metrics\nand number of sampled null action during rollouts (rightmost group of columns) for the following\ntreatments: no null action ﬁltering (blue), ﬁltering all null actions (green), ﬁltering only groups of\n3 or more null actions (red), and ﬁltering only groups of 21 or more null actions (purple). (Right)\nHierarchical versus Factored Action Spaces.\nFiltering only groups of 3 performed slightly better than ﬁltering all null action or groups of 21. Initial\nexperiments indicated that ﬁltering all null actions was better; however, after further model tuning\nand after we had already trained our largest models, we found that ﬁltering only groups of 3 or more\nnull actions performed best. Due to compute constraints we were not able to redo all experiments\nwith this setting, but doing so would be a reasonable choice for any future work.\nE.3\nJoint Hierarchical Action Space\nWe originally worked with a factored action space, where each keypress could be independently on\nor off, and this choice was independent of whether the mouse was being moved. This could cause\nissues for modeling the human behavior distribution exactly. Say for a given state, humans either with\n50% probability (a) move forward and attack or with 50% probability (b) move left and drop their\nitem. The best a factored distribution can do is to assign 50% probability to each of the 4 constituent\nactions because it chooses to press each button simultaneously and independently. See Appendix C.2\nfor details on the entire action space.\nFor this reason, we implemented a joint distribution over actions; however, the full joint distribution\nover 20 binary buttons and two mouse movement dimensions discretized into 11 bins each would\nresult in in 220 × 112 ≈1.2 × 108 possible combinations. This is far too large for many reasons, e.g.\nthe ﬁnal layer from the transformer stack with a dimension of 4096 would need to be mapped to each\ncombination resulting in 4096 × 1.2 × 108 ≈5.2 × 1011 parameters for this ﬁnal layer alone. In\norder to reduce this we noted that many buttons in Minecraft have no effect when simultaneously\npressed; for example, if a player tries to move forward and backward at the same time, they remain in\nplace. Below we list the the sets of mutually exclusive actions. Furthermore, the inventory button is\nexclusive with all other buttons and mouse movement.\nMutually Exclusive Actions\nforward, back\nleft, right\nsprint, sneak\nhotbar.[1-9]\nEven reducing the joint action space to reﬂect these mutually exclusive combinations still results in a\nhuge action space when combined with the discretized mouse movements, i.e. 33×10×24×112+1 ≈\n5.2 × 105. This calculation results from 33 for the 3 sets of 2 mutually exclusive keys above where\ntaking neither in the set is an option, ×10 for the 9 hotbar keys or no hotbar keypress, ×24 for the\nremaining binary 4 keys: use, drop, attack, and jump, ×112 for mouse movements, and ﬁnally\n+1 for the inventory button which is mutually exclusive with all other actions. ∼5.2 × 105 is still\nquite large so we chose to implement a hierarchical binary action for camera being moved or not. If\nthis action is on, then there is a secondary discrete action head with 121 classes (the joint distribution\nof mouse movements because each discretized mouse direction has 11 bins) that determines where\n25\nto move the mouse. If the hierarchical action is off, then there is no mouse movement, loss for\nthe secondary mouse movement action is masked during training, and the secondary action head\nneed not be sampled during evaluations. While this no longer models the full joint distribution, it is\nquite a bit better than the factored action space since dependencies between keypresses as well as\nwhether or not to move the mouse (although not which mouse movement) are modeled jointly. The\nresulting action space has dimension 33 × 10 × 24 × 2 + 1 = 8461 (the 112 dimensional multiplier\nfor camera movement has been replaced by a multiplier of 2 here, corresponding to a binary action\nfor whether or not to move the mouse) with an additional 121-dimension head for the joint camera\nmovements. In the future it would be interesting to implement sequential conditional action spaces to\nmore completely model the joint distribution.\nIn Figure 15 (right) we compare environment rollout performance between BC models with the\nhierarchical joint action space and with the factored action space. Environment statistics are fairly\ncomparable; however, we see that the factored action space model samples far more null actions.\nThis is an important example of the factored action space failing to correctly model the distribution in\nthe dataset because, due to null action ﬁltering, there are 0 null actions in the dataset these models\ntrain on. Despite this, the factored model samples many null actions because the prediction for each\nkey is not conditioned on other keypresses.\nE.4\nFoundation Model Training\nThe foundation model training is similar to the IDM training, with the exception of labels being\nIDM-generated pseudo labels. The hyperparameters used for foundation model training are listed in\nTable 4.\nHyperparameter\nValue\nLearning rate\n0.002147\nWeight decay\n0.0625\nEpochs\n30\nBatch size\n880\nTable 4: Hyperparameters for foundation model training\nF\nBehavioral Cloning Fine-Tuning\nBehavior cloning ﬁne-tuning is similar to the foundation model training, except we either use a focused\nsubset of all the videos (early_game dataset, described in A.3) with pseudo labels, or contractor data\n(contractor_house dataset, described in B.4) with ground-truth labels. The hyperparameters used\nfor behavior cloning ﬁne-tuning are listed in Table 5. We used 16 A100 GPUs for about 6 hours when\nﬁne-tuning on contractor_house dataset, and 16 A100 GPUs for about 2 days when ﬁne-tuning\non early_game dataset.\nHyperparameter\nValue\nLearning rate\n0.000181\nWeight decay\n0.039428\nEpochs\n2\nBatch size\n16\nTable 5: Hyperparameters for behavior cloning ﬁne-tuning\nG\nReinforcement Learning Fine-Tuning\nG.1\nReinforcement Learning Fine-Tuning Training Details\nRL experiments were performed with the phasic policy gradient (PPG) algorithm,64 an RL algorithm\nbased on the proximal policy optimization (PPO) algorithm77 that increases sample efﬁciency by\nperforming additional passes over the collected data to optimize the value function as well as an\n26\nauxiliary value function. These algorithms have been described extensively in previous work,64,77\nso here we describe them only brieﬂy. A major inefﬁciency when training on-policy algorithms is\nthat, to remain on-policy, one can only take a single gradient step before new rollout data needs\nto be gathered to continue optimization. To alleviate the potentially destructive effects of taking\nmultiple optimization steps in a single iteration, PPO prevents the policy from changing too much\nin a single step by clipping the loss when the difference between the current policy and the policy\nbefore the update becomes too large.77 We also use generalized advantage estimation (GAE), which\ncan speed-up credit assignment by looking more than 1 step into the future when determining the\nadvantage of an action, with the look-ahead being determined by hyperparameter λ.78\nPPG improves the sample efﬁciency of PPO when the policy and value function share the same\nnetwork by following different optimization processes for the policy, the value function, and their\nshared representation. PPG splits optimization in two phases: a wake phase and a sleep phase. In the\nwake phase, the policy and value function are optimized as in normal PPO training, with the only\nexception being that every sample is used at most once, which prevents the policy from overﬁtting on\nthese samples. In the sleep phase PPG optimizes the value function and an auxiliary value function\n(which is optimized with the exact same loss as the regular value function, but its output is never used\nduring training), while keeping a Kullback-Leibler (KL) divergence loss to the policy before the start\nof the sleep phase to ensure that the policy does not change. Because the policy is not optimized\nin this step, PPG does allow samples to be reused multiple times in this phase. The assumption\nbehind optimizing the value function during the sleep phase is that value function optimization is\nless sensitive to being trained multiple times on the same sample. Optimizing the auxiliary value\nfunction does not directly affect either the value function or the policy, but it can improve the shared\nrepresentation of both functions (the assumption being that predicting the value-function requires\nencoding all features that are important for distinguishing states). The coefﬁcients for the three\nlosses (value function loss, auxiliary value function loss, and KL loss) are listed in Table 6. In our\nexperiments a single iteration consists of two sleep cycles and one wake cycle.\nBecause the value and auxiliary value functions are not optimized during BC pre-training, they\nare initialized at the start of RL ﬁne-tuning. Each value function is implemented as a single, fully\nconnected layer on top of the last residual transformer block of the pretrained model (Appendix D.1).\nThe weights of the auxiliary value function are randomly initialized while the weights of the regular\nvalue function are initialized with zero weights, which appeared to prevent destructive updates early\nin training that could happen with a randomly initialized value function. To prevent the value-function\nloss from having gradients that depend greatly on the magnitude of the reward, we normalize the\nvalue-function target by subtracting the mean and dividing by the standard deviation, which are\nestimated through an exponentially weighted moving average.\nTo prevent catastrophically forgetting the skills of the pretrained network when RL ﬁne-tuning, we\napply an auxiliary KL divergence loss between the RL model and the frozen pretrained policy.10 This\nloss is deﬁned as:\nLklpt = ρKL(πpt, πθ)\n(2)\nWhere πθ is the the policy being trained, πpt is the frozen pretrained policy, KL(πpt, πθ) is the\nKullback-Leibler divergence between the policy being trained and the pretrained policy, and ρ is a\ncoefﬁcient to weight this loss relative to other losses.\nIn the ﬁne-tuning experiments, this KL divergence loss replaces the common entropy maximization\nloss, which is often added to RL experiments to encourage exploration.79,80 The idea behind entropy\nmaximization is that, when all actions appear to have equal value, such as when the agent has not\nlearned about the next reward, it should maximize its entropy to increase the chance that it discovers\nthe next reward. Blindly exploring by maximizing entropy is effective when the state and action\nspaces are sufﬁciently small or the reward is sufﬁciently dense, but becomes infeasible when the\nstate and action spaces are large and rewards are sparse, which is the case in the diamond-pickaxe\ntask. Instead of blindly exploring through uniform-random actions, we assume that the pretrained\npolicy has an action distribution that is much more likely to take sequences of actions that lead\nto interestingly new states, and thus, in states where the agent assigns equal value to each of its\nactions, it should mimic the action-distribution of the pretrained policy instead of a uniform-random\naction distribution. In experiments with a randomly initialized policy we do include the entropy\nmaximization loss with a coefﬁcient of 0.01, which has been an effective setting in other Minecraft\nwork.30 Empirically, we found that a high coefﬁcient ρ for this KL divergence loss would prevent\nthe agent from properly optimizing the reward function while a low coefﬁcient ρ was ineffective at\n27\nHyperparameter\nValue\nLearning rate:\n2 × 10−5\nWeight decay:\n0.04\nBatch size:\n40\nBatches per iteration:\n48\nContext length:\n128\nDiscount factor (γ):\n0.999\nGAE λ:\n0.95\nPPO clip:\n0.2\nMax Grad norm:\n5\nMax Staleness:\n2\nPPG sleep cycles:\n2\nPPG sleep value-function coefﬁcient:\n0.5\nPPG sleep auxiliary value-function coefﬁcient:\n0.5\nPPG sleep KL coefﬁcient:\n1.0\nPPG sleep max Sample Reuse:\n6\nKL divergence coefﬁcient ρ:\n0.2\nCoefﬁcient ρ decay:\n0.9995\nTable 6: Hyperparameters for RL experiments. These are the hyperparameters for all treatments with\ntwo exceptions. First, when ﬁne-tuning from the early-game model without a KL divergence loss,\nin addition to the KL divergence loss being set to 0, the learning rate was set to 3 × 10−6 (the best\nsetting out of a sweep over 5 different learning rates), as we found that performance was substantially\nlower with the standard learning rate of 2 × 10−5 and the agent did not even learn to collect logs.\nWe suspect that the reason that the learning rate needed to be lowered when ﬁne-tuning without a\nKL loss is that the KL loss prevents making optimization steps that change the policy too much in a\nsingle step, especially in early iterations when the value function has not been optimized yet, and\nthe KL loss thus makes it possible to optimize with a higher learning rate. Second, when running\nRL from a randomly initialized policy there is no KL divergence loss or KL divergence decay, but\ninstead we use an entropy bonus of 0.01, which reportedly worked well in previous work.30\nprotecting the learned skills of the pretrained policy and preventing catastrophic forgetting. As such,\nwe start with a relatively high coefﬁcient ρ and decay it by a ﬁxed factor after each iteration (Table 6).\nThis method protects policy skills in early iterations while guaranteeing that the policy can eventually\nmaximize the reward function, regardless of how different its behavior has to be to do so relative to\nthe pretrained policy.\nFor the reward function we estimated the rough quantities of each item that a human player might\ngather when trying to craft a diamond pickaxe, and we reward the model for gathering up to that\nquantity for each item. We started these estimates by iterating over the technology tree backward from\na diamond pickaxe and adding the requirements for each item to the reward function (e.g. ﬁrst we\nadded a diamond pickaxe to the reward function, then we added the 3 diamonds and 2 sticks required\nfor crafting a diamond pickaxe, then we added the 1 iron pickaxe required for mining diamonds,\nand so on). Then we added coal and torches to the reward function, with coal being useful as fuel\nwhen smelting iron and for crafting torches while the torches themselves improve visibility and\nprevent enemies from spawning. Finally, we reward the model for bringing additional logs (5 logs are\nrequired to craft all items in the reward function, but we reward up to 8 logs), which can be used as\nfuel or crafted into a crafting table or sticks if the agent runs out. In practice the agent rarely collects\nthe additional logs, places the torches, or uses coal as fuel when smelting, but the reward function\nwas based on human expectations on what would be useful to execute this task, rather than designed\naround how an RL model behaves after training. Finally, to encourage the agent to keep mining\ndiamonds and crafting diamond pickaxes after it has crafted its ﬁrst diamond pickaxe, we did not put\na limit on the number of diamonds or diamond pickaxes that would be rewarded.\nThe rewards for the different items are separated into 4 tiers, roughly depending on how late a player\nwould usually get the relevant item. The ﬁrst tier consists of all wooden and stone items and has a\nbase reward of 1, the second tier consists of all items requiring coal with a base reward of 2, the third\ntier consists of all items requiring iron with a base reward of 4, and the ﬁnal tier is diamond with a\nbase reward of 8. Thus items later in the sequence of items towards a diamond pickaxe generally\n28\nItem\nQuantity rewarded\nReward per item\nLog\n8\n1\/8\nPlanks\n20\n1\/20\nStick\n16\n1\/16\nCrafting table\n1\n1\nWooden pickaxe\n1\n1\nCobblestone\n11\n1\/11\nStone pickaxe\n1\n1\nFurnace\n1\n1\nCoal\n5\n2\/5\nTorch\n16\n1\/8\nIron ore\n3\n4\/3\nIron ingot\n3\n4\/3\nIron pickaxe\n1\n4\nDiamond\ninf\n8\/3\nDiamond pickaxe\ninf\n8\nTable 7: Reward per item and total quantity rewarded.\ngive a higher reward. To make sure that the agent does not over-value items that are supposed to\nbe gathered in bulk (e.g. the agent is rewarded for up to 20 planks but only up to 1 crafting table,\nwhich can cause the agent to focus on planks at the expense of creating a crafting table), we divide\nthe base reward of each item by the total quantity that the agent gets rewarded for (for the purpose\nof determining the reward, the total quantity for diamonds is 3 and the total quantity for diamond\npickaxes is 1, even though we did not put a limit on the number of these items being rewarded). For\nexample, the agent is rewarded for 3 iron ore, which has a base reward of 4 for being in the iron tier\nand up to 3 blocks of iron ore are rewarded, thus the reward per block of iron ore is 4\/3. The quantity\nand reward for each item are listed in Table 7.\nWhile every item in the sequence towards a diamond pickaxe is rewarded, the reward function is still\nsparse and, in some cases, even deceptive. The sparsity comes from the fact that it can take thousands\nof actions to ﬁnd the next reward, even after the agent has acquired all the necessary prerequisites\n(e.g. human players often take more than 10,000 actions to ﬁnd a diamond after crafting an iron\npickaxe). The reward function can be deceptive when the most efﬁcient method for getting one item\ncan make it far more difﬁcult to get the next item. For example, a good strategy for the agent to\ncraft a stone pickaxe quickly is to mine (i.e. spend a few seconds to pick up) its crafting table after\ncrafting a wooden pickaxe, such that the agent has immediate access to a crafting table as soon as it\nhas collected enough cobblestone. However, the fastest way to get a reward for gathering cobblestone\nis to mine down immediately after crafting a wooden pickaxe, while leaving the crafting table behind.\nThus following the optimal strategy for gathering cobblestone makes it more difﬁcult to learn to craft\na stone pickaxe.\nExperiments ran for approximately 6 days (144 hours) on 80 GPUs (for policy optimization) and\n56,719 CPUs (mostly for collecting rollouts from Minecraft). In this time the algorithm performed\nroughly 4,000 optimization iterations and collected roughly 1.4 million Minecraft episodes consisting\nof 12,000 frames each, for a total of 16.8 billion frames.\nG.2\nReinforcement Learning Fine-Tuning Additional Data\nAdditional ﬁgures that are helpful for understanding the main results of the RL ﬁne-tuning experiments\nare presented in this section. First, we show the items-over-training ﬁgure when RL ﬁne-tuning from\nthe early-game model without a KL loss (Fig. 16). When training without a KL loss, the model\nonly learns to obtain the four items that the early-game model is capable of getting zero-shot, which\nare logs, planks, sticks, and crafting tables. Second, we present preliminary experiments in which\nwe directly compare RL ﬁne-tuning from the house-building model and RL ﬁne-tuning from the\nearly-game model (Fig. 17). These experiments differ from the main experiments in that, for both\ntreatments shown here, the KL loss coefﬁcient was set to 0.4, the learning rate was set to 6 × 10−5,\nand the reward for each item was 1\/quantity for all items (i.e. items closer to the diamond pickaxe\ndid not have an increased reward). While RL ﬁne-tuning from the house-building model initially\n29\nworked better than RL ﬁne-tuning from the early-game model, ﬁne-tuning from the early-game model\nworked better after 800,000 episodes and showed signs of smelting iron ingots, which is why the\nearly-game model was chosen for the main experiments.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nEpisodes\n1e6\n0\n20\n40\n60\n80\n100\n% episodes\nNo KL-loss\nFigure 16: Items obtained when RL ﬁne-tuning from the early-game model without a KL loss. The\nmodel learns to obtain all items that the early-game model can craft zero-shot, which are logs, planks,\nsticks, and a crafting table. In contrast to the treatment with a KL-penalty, it does not learn any items\nbeyond these initial four, likely because skills that are not performed zero-shot, and for which the\nmodel thus does not initially see any reward, are catastrophically forgotten while the ﬁrst four items\nare learned.\n0\n100000\n200000\n300000\n400000\n500000\n600000\n700000\n800000\nEpisodes\n0\n2\n4\n6\n8\nReward\nReward over episodes\nRL from Early-Game model\nRL from House-Builing model\n0\n100000\n200000\n300000\n400000\n500000\n600000\n700000\n800000\nEpisodes\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nIron Ingots Obtained\nIron Ingots Obtained Per Episodes\nRL from Early-Game model\nRL from House-Builing model\nFigure 17: Preliminary experiments when RL ﬁne-tuning from the early-game model compared to\nRL ﬁne-tuning from the house-building model. (Left) While reward initially increases faster when\nﬁne-tuning from the house-building model, ﬁne-tuning form the early-game model eventually obtains\na slightly higher reward. (Right) RL ﬁne-tuning from the early-game model has a higher likelihood\nof smelting an iron-ingot, which is why the early-game model was chosen for future RL ﬁne-tuning\nexperiments.\nH\nFoundation Model Scaling\nIn early experiments we found that increasing model size led to models staying in the efﬁcient\nlearning regime longer into training.63 Here we compare the 0.5B model described in Section 4.2 to\nboth a 248M and 71M parameter model. Both of these models are trained for 15 epochs as compared\nto the 30 epochs the 0.5B model trained for. These models have the same architecture as the 0.5B\nmodel but each layer in the 248M parameter model has 1\/2 the width and each layer in the 71M\nparameter model 1\/3 the width. The 71M model was trained with an initial learning rate of 0.001586,\nbatch size of 480, and weight decay of 0.044506. The 248M model had an initial learning rate of\n0.001831, batch size of 640, and weight decay of 0.051376.\nIn Figure 18 we show validation loss on web_clean with IDM pseudo-labels, loss on the contractor\ndataset used to train the IDM with ground truth labels collected during contractor play, and zero-shot\nenvironment performance for the 71M, 248M, and 0.5B models. While larger models have better\nvalidation loss on web_clean, these results do not tell the clear story that the 0.5B model is better\nthan its smaller counterparts. The 71M model has the lowest contractor dataset loss while having the\nhighest web_clean loss, and it also has the best zero-shot environment performance. In fact, we see\nthat the 71M model even had non-zero wooden tool crafting (Fig. 18 bottom left). The 248M model\nalso appears to be better at crafting than the 0.5B, and also has lower contractor dataset loss.\nWhile the zero-shot results suggest smaller models are better, ﬁne-tuning tells another story. When\nﬁne-tuning to contractor_house, model size rank ordering reverses and now the 0.5B model\nperforms best both in validation loss (Fig. 19 left) and in environment performance (Fig. 19 right)\n30\n10 3\n10 2\n10 1\n100\n101\n~Compute\n2.5\n4\n6\n10\nWeb Clean Loss\nLoss Web Clean Validation Dataset\n71M\n248M\n0.5B\n10 3\n10 2\n10 1\n100\n101\n~Compute\n2.5\n4\n6\n10\nIDM Contractor Dataset Loss\nLoss on IDM Contractor Dataset\n71M\n248M\n0.5B\nbasic\nmining\nlogs\nplanks\ncrafting\ntables\ntotal\ncrafting\n10 2\n10 1\n100\n101\nCollected or Crafted\nZero-Shot Performance vs Model Size\n71M\n248M\n0.5B\n0\n2\n4\n6\n8\n10\n12\n14\n16\nTraining Progress (Epoch)\n0\n10 3\n10 2\n10 1\n100\n101\nCrafting or Collection\nZero-Shot Performance over Training\n(71M Parameter Model)\nbasic mining\nlogs\nplanks\ncrafting tables\ntotal crafting\nwooden tools\n0\n2\n4\n6\n8\n10\n12\n14\n16\nTraining Progress (Epoch)\n0\n10 3\n10 2\n10 1\n100\n101\nZero-Shot Performance over Training\n(248M Parameter Model)\n0\n5\n10\n15\n20\n25\nTraining Progress (Epoch)\n0\n10 3\n10 2\n10 1\n100\n101\nZero-Shot Performance over Training\n(0.5B Parameter Model)\nFigure 18: Training and Zero-Shot Performance versus Model Scale. In the ﬁrst two plots the x-axis\nis compute normalized to that used by the 71M parameter model, such that after 15 epochs of training\nthe 71M model has used 1 \"compute\". The 248M parameter model and the 71M model are trained on\nthe same amount of data (15 epochs), and the 0.5B parameter model is trained on 30 epochs of data.\n(Top Left) Loss on the web_clean validation dataset. (Top Middle) Loss on the IDM contractor\ndataset; note that these models were trained only on web_clean and not on any contractor data.\n(Top Right) Zero-shot environment rollout performance at the end of training. (Bottom) Zero-shot\nenvironment rollout performance over training for the 71M model (bottom left), 248M model (bottom\nmiddle), and 0.5B model (bottom right).\nbasic\nmining\nlogs\nplanks crafting\ntables\ntotal\ncrafting\nwooden\ntools\nstone\ntools\n10 3\n10 2\n10 1\n100\n101\n102\n103\n104\nCollected or Crafted\nFine-Tuning to Contractor House Dataset\n71M\n248M\n0.5B\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFine-Tuning Epoch\n2.2\n2.4\n2.6\n2.8\n3.0\ncontractor_house Validation Loss\nLoss on contractor_house\n71M\n248M\n0.5B\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFine-Tuning Epoch\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3.0\nLoss on IDM Contractor Dataset\nLoss on full IDM Contractor Dataset\n71M\n248M\n0.5B\nFigure 19: contractor_house ﬁne-tuning performance versus model size. (Left) Loss on the\ncontractor_house holdout validation set. (Middle) Loss on the full contractor dataset collected\nto train the IDM; this dataset is disjoint from contractor_house. (Right) Environment rollout\nperformance at the end of ﬁne-tuning.\nfollowed by the 248M model and then the 71M model. Environment model rollouts are performed\nusing the same game engine that we use to collect contractor data, which could be visually distinct\nfrom videos taken from the web. It is plausible that the larger models overfocus on the visual\npeculiarities in web data during pretraining since they have worse contractor data loss (Fig.18 top\nmiddle), and this causes them to perform more poorly in the environment zero-shot. However, we\nhypothesize that because the contractor_house dataset we ﬁne-tune to is collected from our game\nengine, the larger models that are a better overall Minecraft prior (as indicated by lower web_clean\nvalidation loss in Fig.18 top left) can quickly shift their low level features to perform better on data\ncoming from our game engine, resulting in better environment rollout performance. This hypothesis\nis further supported by Fig. 19 (middle) showing loss on the contractor dataset collected for IDM\ntraining, which has no overlap with contractor_house. After just a few steps of ﬁne-tuning to\ncontractor_house, all models quickly improve in loss on the full IDM contractor dataset, with\nlarger models now performing best. While not conclusive, we believe this investigation provides\nsome intuition for future studies of model scaling for sequential decision making problems.\n31\nI\nText Conditioning\nGoal-conditioned policies81,82 make it possible for a single agent to perform a wide variety of\ngoals in a single environment, which is particularly relevant in open-ended environments such as\nMinecraft. In recent work, goal speciﬁcation has increasingly taken the form of domain speciﬁc\nlanguages83, or even natural language84,85. The beneﬁts of language-conditioned agents can be\ntremendous, especially natural-language-conditioned agents, as their goal space contains a wide\nvariety of potentially very complex tasks. Text conditional models have shown an amazing ability to\nperform tasks zero-shot (or learn them few-shot) including generalizing in impressive ways via the\ncompositional and combinatorial possibilities allowed by natural language (e.g. GPT1 and DALL·E\n286). We hypothesize that we should expect similar capabilities to emerge with natural-language-\nconditioned virtual agents, if they are similarly trained on enormous amounts of data (that goes from\na natural language description to a sequence of actions that completes the speciﬁed goal). In this\nsection we take preliminary steps toward that future. Our preliminary experiments provide evidence\nthat it is possible to pretrain a natural-language-conditioned model for Minecraft using the general\napproach presented in this paper (VPT) plus conditioning on the speech that often accompanies\nvideos.\nIn online videos, the human actor sometimes indicates their intent in their verbal commentary\n(e.g. “Let’s go chop some trees to make a wooden axe” or “now let’s learn how to crop photos in\nPhotoshop”). Conditioning on this closed caption data could produce a steerable pre-trained model:\ni.e., it may later be possible to condition the model with text such as “I am going to craft a wooden\npickaxe” or “I am going to build a house,” and have the agent perform those tasks speciﬁcally rather\nthan simply follow typical human behavior (as was investigated in the rest of this paper). An alternate\nway to produce a steerable agent is via RL ﬁne-tuning, which we could have done in Section 4.4\nby adding a bit indicating the task to be completed, as has been done in prior work30. However,\nconditioning on natural language offers many beneﬁts over that approach. First, it is ﬂexible and\npowerful, being able to express any task. Second, one does not need to preconceive of the task to\nbe completed ahead of time. This would allow for general, capable, zero-shot agents like GPT, but\nextending those capabilities to embodied tasks such as completing tasks on computers or in simulated\n3D worlds. Third, text conditioning can be used even when tasks are difﬁcult to specify via reward\nfunctions (e.g. “Let’s build a house” or–if the agent is capable of doing it–more complex things like\n“I will now build a castle surrounded by a moat”). In the limit, VPT+text could conceivably produce\npowerful, capable, natural-language-conditional agents with the powers of GPT to meta-learn, follow\ninstructions, and complete tasks zero or few shot, but in the form of agents that can act in virtual\nworlds, complete tasks on computers, and in other similar embodied sequential decision domains.\nWe do not reach those lofty goals in this work, but we began a ﬁrst step towards exploring in that\ndirection.\nMany Minecraft videos feature audio commentary from the player. This commentary is sometimes\npresent in the form of closed captions for the videos, or could be extracted post-hoc using automated\nspeech recognition (ASR).87 Our dataset features about 17k hours of content with associated closed\ncaptions.\nWe ﬁne-tuned the 220 million parameter VPT foundation model used in the RL-ﬁne-tuning ex-\nperiments (chosen vs. 0.5B for the same reason: to reduce compute costs) with an additional\ntext-conditioning input on the subset of our data for which closed captions are available. To obtain the\nconditioning input, we ﬁrst split videos into 30 second chunks. The same text is associated with every\nframe in a given chunk, and is made up of all the closed captions occurring within that chunk, as well\nas the line of text preceding and following the chunk (if any). Because the vast majority (around\n95%) of our closed caption data lacks capitalization and punctuation, it is punctuated using the rpunct\nlibrary88. We then obtain a text embedding vector of length 4,096 from the OpenAI embedding API89,\nwhich is processed by a randomly initialized multi-layer perceptron (MLP) with two hidden layers of\nsize 2,048. The resulting activations are added for each frame to the pretrained model activations\nbefore the transformer layers (pretransformerActivations += mlp(textEmbedding)). The\nmodel is ﬁne-tuned for four epochs.\nOur model shows evidence of steerability. When conditioned on sentences that incite the agent to\nexplore (such as “I’m going to explore” and “I’m going to ﬁnd water”) the agent travels signiﬁcantly\nfarther from its spawn point (Figure 20a). Additionally, we can steer the agent to preferentially collect\n32\ndig\ndirt\nexplore\nhouse\nseed\nwater\nwood\nConditioning\n120\n130\n140\n150\nTravel distance (blocks)\nTravel Distance with Conditioning\n(a)\ndig\ndirt\nexplore\nhouse\nseed\nwater\nwood\nConditioning\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nWheat seeds collected\nSeed Collection with Conditioning\n(b)\ndig\ndirt\nexplore\nhouse\nseed\nwater\nwood\nConditioning\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nOak logs collected\nLog Collection with Conditioning\n(c)\ndig\ndirt\nexplore\nhouse\nseed\nwater\nwood\nConditioning\n5\n6\n7\n8\n9\n10\nDirt collected\nDirt Collection with Conditioning\n(d)\nFigure 20: Evidence for conditioning. In each plot, the variants expected to stand out are shown in\nbold. The strings corresponding to each variant are shown in Table 8. Statistics are measured over 5\nminute episodes. (a) Distance traveled by the agent . Both “explore” and “water” text strings should\nencourage a steerable agent to move more than when doing other tasks, which is what occurs. Grass\n(which is needed to get seeds) is not present in all biomes, which is likely why the “seed” condition\nproduces more travel (as the agent sometimes needs to move to a biome with grass). The travel\ndistance is the Euclidean distance from the spawn point to the farthest point the agent reached during\nthe episode on the horizontal (x-z) plane. (b) Collection of wheat seeds. The “seed” variant collects\nsubstantially more than other variants, as expected of a steerable agent. (c) Collection of oak (the\nmost common type of wood) logs. The “wood” variant collects signiﬁcantly more oak logs, as is to\nbe expected of a steerable agent (we speculate that the “water” variant collects less because there are\nno trees in water). (d) Collection of dirt. The “dirt” and “dig” variants collect a large amount, and are\nthe variants that are (indirectly in the case of “dig”) conditioned to collect dirt. It is easy to mistakenly\naim at the ground rather than at grass or trees when collecting seeds or wood, which likely explains\nthe slightly higher amount of dirt collected by these variants. In all cases, the error bars are 95%\nconﬁdence intervals of the mean, over 1,000 episodes per conditioning variant. Treatments for which\nthe bars in each bar plot do not overlap are statistically signiﬁcantly different at a p < 0.05 level.\nVariant name\nString\ndig\nI’m going to dig as far as possible\ndirt\nI’m going to collect dirt\nexplore\nI’m going to explore\nhouse\nI’m going to make a house\nseed\nI’m going to collect seeds\nwater\nI’m going to ﬁnd water\nwood\nI’m going to chop wood\nTable 8: Strings corresponding to each conditioning variant.\n33\nearly game items such as seeds, wood, and dirt by conditioning with text such as “I’m going to collect\nseeds\/chop wood\/collect dirt” (Figure 20b,c,d).\nWhile our results show some level of steerability, more work is required to increase it. For example,\nwe were not able to successfully steer agents to gather ﬂowers or to hunt, both of which are possible\nin the early game, but less common (and, in the case of hunting animals, much more difﬁcult) than\ngathering dirt, wood, or seeds. Likewise, an experiment in which the agent is presented with a\ncrafting window and various resources, and conditioned to craft a given item (e.g. “I’m going to\ncraft a wooden axe”) failed to show that the conditioning had a signiﬁcant effect on which items got\ncrafted. Instead, it seemed the agent was more inﬂuenced by the prior, unconditional probability\nof what human players would craft next given the resources available, which is not too surprising\nsince in Minecraft, especially in the early game, there is a relatively consistent path to gathering\nresources in a speciﬁc order go produce more powerful tools (Fig. 6). For example, if the agent had\nthe resources to make a stone pickaxe and we asked it instead to make a (weaker) wooden pickaxe, it\noften would make the stone pickaxe anyway. Finally, looking at videos of agent behaviors failed to\nconvince us that the “house” conditioning causes the agents to take more steps towards building a\nhouse than other variants.\nThus, our results show that it is possible to train a somewhat steerable natural-language-conditioned\nagent. However, its steerability is still too weak to be practically useful, and it is far from what we\nbelieve could be accomplished with more research, data, and training compute. Another exciting\nresearch direction is to have the model predict future text as well as just the next action.\n34\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos.pdf"}
{"title":"GROOT: Learning to Follow Instructions by Watching Gameplay Videos","authors":"Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"We study the problem of building a controller that can follow open-ended\ninstructions in open-world environments. We propose to follow reference videos\nas instructions, which offer expressive goal specifications while eliminating\nthe need for expensive text-gameplay annotations. A new learning framework is\nderived to allow learning such instruction-following controllers from gameplay\nvideos while producing a video instruction encoder that induces a structured\ngoal space. We implement our agent GROOT in a simple yet effective\nencoder-decoder architecture based on causal transformers. We evaluate GROOT\nagainst open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the\nhuman-machine gap as well as exhibiting a 70% winning rate over the best\ngeneralist agent baseline. Qualitative analysis of the induced goal space\nfurther demonstrates some interesting emergent properties, including the goal\ncomposition and complex gameplay behavior synthesis. The project page is\navailable at https:\/\/craftjarvis-groot.github.io.","url":"http:\/\/arxiv.org\/abs\/2310.08235v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.08235v2","published":1697110261000,"comment":null,"pdf_text":"GROOT: Learning to Follow Instructions by\nWatching Gameplay Videos\nShaofei Cai1, Bowei Zhang1, Zihao Wang1, Xiaojian Ma3, Anji Liu2 and Yitao Liang1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis,\nWe study the problem of building a controller that can follow open-ended instructions in open-world\nenvironments. We propose to follow reference videos as instructions, which offer expressive goal\nspecifications while eliminating the need for expensive text-gameplay annotations. A new learning\nframework is derived to allow learning such instruction-following controllers from gameplay videos\nwhile producing a video instruction encoder that induces a structured goal space. We implement our\nagent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers.\nWe evaluate GROOT against open-world counterparts and human players on a proposed Minecraft\nSkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap\nas well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis. The project page is available at https:\n\/\/craftjarvis-groot.github.io.\nFigure 1 | Through the cultivation of extensive gameplay videos, GROOT has grown a rich set of skill fruits\n(number denotes success rate; skills shown above do not mean to be exhaustive; kudos to our artist Haowei).\n1. Introduction\nDeveloping human-level embodied agents that can solve endless tasks in open-world environments,\nsuch as Minecraft (Fan et al., 2022; Johnson et al., 2016), has always been a long-term goal pursued\nin AI. Recent works have explored using Large Language Models (LLMs) to generate high-level plans,\nwhich guide the agent to accomplish challenging long-horizon tasks (Wang et al., 2023a,b; Zhu\net al., 2023). However, a major gap between these LLM-based agents and generalist agents that can\ncomplete endless amounts of tasks is the capability of their low-level controllers, which map the plans\nto motor commands. Recently developed controllers are only capable of completing a predefined and\nnarrow set of programmatic tasks (Baker et al., 2022; Cai et al., 2023; Lin et al., 2021), which hinders\nLLM-based planning agents from unleashing their full potential. We attribute the limitation of these\nlow-level controllers to how the goal is specified. Specifically, existing controllers use task indicator\nCorresponding author(s): Yitao Liang\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>,Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2310.08235v2  [cs.AI]  29 Nov 2023\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(Yu et al., 2019), future outcome (Chen et al., 2021; Lifshitz et al., 2023), and language (Brohan\net al., 2022) to represent the goal. While it is easy to learn a controller with some of these goal\nspecifications, they may not be expressive enough for diverse tasks. Taking future outcome goals as an\nexample, an image of a desired house clearly lacks procedural information on how the house was built.\nOne exception is language, but learning a controller that can receive language goals is prohibitively\nexpensive as it requires a huge number of trajectory-text pairs with text that precisely depicts the full\ndetails of the gameplay, therefore preventing them from scaling up to more open-ended tasks.\nHaving observed the limitations of goal specification in the prior works, this paper seeks to find\na balance between the capacity of goal specification and the cost of controller learning. Concretely,\nwe propose to specify the goal as a reference gameplay video clip. While such video instruction is\nindeed expressive, there are two challenges: 1) How can the controller understand the actual goal\nbeing specified as the video itself can be ambiguous, i.e. a goal space or video instruction encoder\nhas to be learned; 2) How to ultimately map such goal to actual motor commands? To this end, we\nintroduce a learning framework that simultaneously produces a goal space and a video instruction\nfollowing controller from gameplay videos. The fundamental idea is casting the problem as future\nstate prediction based on past observations:\n• The predicting model needs to identify which goal is being pursued from the past observations,\nwhich requires a good goal space (induced by a video instruction encoder);\n• Since the transition dynamics model is fixed, a policy that maps both the state and the recognized\ngoal to action is also needed by the predicting model when rolling the future state predictions.\nEffectively, this results in the goal space and control policy we need. We introduce a variational learning\nobjective for this problem, which leads to a combination of a cloning loss and a KL regularization loss.\nBased on this framework, we implement GROOT, an agent with an encoder-decoder architecture to\nsolve open-ended Minecraft tasks by following video instructions. The video encoder is a non-causal\ntransformer that extracts the semantic information expressed in the video and maps it to the latent\ngoal space. The controller policy is a decoder module implemented by a causal transformer, which\ndecodes the goal information in the latent space and translates it into a sequence of actions in the\ngiven environment states in an autoregressive manner.\nTo comprehensively evaluate an agent’s mastery of skills, we designed a benchmark called\nMinecraft SkillForge. The benchmark covers six common Minecraft task groups: collect, build,\nsurvive, explore, tool, and craft, testing the agent’s abilities in resource collection, structure\nbuilding, environmental understanding, and tool usage, in a total of 30 tasks. We calculate Elo\nratings among GROOT, several counterparts, and human players based on human evaluations. Our\nexperiments showed that GROOT is closing the human-machine gap and outperforms the best\nbaseline by 150 points (or 70% winning rate) in an Elo tournament system. Our qualitative analysis\nof the induced goal space further demonstrates some interesting emergent properties, including the\ngoal composition and complex gameplay behavior synthesis.\nTo sum up, our main contributions are as follows:\n• Start by maximizing the log-likelihood of future states given past ones, we have discovered the\nlearning objectives that lead to a good goal space and ultimately the instruction-following controller\nfrom gameplay videos. It provides theoretical guidance for the agent architecture design and model\noptimization.\n• Based on our proposed learning framework, we implemented a simple yet efficient encoder-\ndecoder agent based on causal transformers. The encoder is responsible for understanding the goal\ninformation in the video instruction while the decoder as the policy emits motor commands.\n• On our newly introduced benchmark, Minecraft SkillForge, GROOT is closing the human-machine\n2\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ngap and surpassing the state-of-the-art baselines by a large margin in the overall Elo rating com-\nparison. GROOT also exhibits several interesting emergent properties, including goal composition\nand complex gameplay behavior synthesis.\n2. Preliminaries and Problem Formulation\nReinforcement Learning (RL) concerns the problem in which an agent interacts with an environment\nat discrete time steps, aiming to maximize its expected cumulative reward (Espeholt et al., 2018; Mnih\net al., 2015; Schulman et al., 2017). Specifically, the environment is defined as a Markov Decision\nProcess (MDP) ⟨S, A, R, P, 𝑑0⟩, where S is the state space, A is the action space, R : S × A →ℝis\nthe reward function, P : S × A →S is the transition dynamics, and 𝑑0 is the initial state distribution.\nOur goal is to learn a policy 𝜋(𝑎|𝑠) that maximizes the expected cumulative reward 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑡], where\n𝛾∈(0, 1] is a discount factor.\nIn goal-conditioned RL (GCRL) tasks, we are additionally provided with a goal 𝑔∈G (Andrychowicz\net al., 2017; Cai et al., 2023; Ding et al., 2019; Jing et al., 2020, 2021; Liu et al., 2022; Yang et al.,\n2019). And the task becomes learning a goal-conditioned policy 𝜋(𝑎|𝑠, 𝑔) that maximizes the expected\nreturn 𝔼[Í∞\n𝑡=0 𝛾𝑡𝑟𝑔\n𝑡], where 𝑟𝑔\n𝑡is the goal-specific reward achieved at time step 𝑡. Apart from being\na new type of RL task, GCRL has been widely studied as a pre-training stage toward conquering\nmore challenging environments\/tasks (Aytar et al., 2018b; Baker et al., 2022; Zhang et al., 2022).\nSpecifically, suppose we are provided with a good goal-condition policy, the goal can be viewed as a\nmeta-action that drives the agent to accomplish various sub-tasks, which significantly simplifies tasks\nthat require an extended horizon to accomplish. Further, when equipped with goal planners, we can\nachieve zero- or few-shot learning on compositional tasks that are beyond the reach of RL algorithms\n(Gong et al., 2023; Huang et al., 2022; Wang et al., 2023a,b; Zhu et al., 2023).\nAt the heart of leveraging such benefits, a key requirement is to have a properly-defined goal\nspace that (i) has a wide coverage of common tasks\/behaviors, and (ii) succinctly describes the task\nwithout including unnecessary information about the state. Many prior works establish goal spaces\nusing guidance from other modalities such as language (Cai et al., 2023; Hong et al., 2020; Stone\net al., 2023) or code (Huang et al., 2023; Wang et al., 2023a). While effective, the requirement on\nlarge-scale trajectory data paired with this auxiliary information could be hard to fulfill in practice.\nInstead, this paper studies the problem of simultaneously learning a rich and coherent goal space\nand the corresponding goal-conditioned policy, given a pre-trained inverse dynamic model and raw\ngameplay videos, i.e. sequences of states {𝑠(𝑖)\n0:𝑇}𝑖collected using unknown policies.\n3. Goal Space Discovery via Future State Prediction\nThis section explains our learning framework: discovering a “good” goal space as well as a video\ninstruction following the controller through the task of predicting future states given previous ones.\nWe start with an illustrative example in Minecraft (Johnson et al., 2016). Imagine that an agent is\nstanding inside a grassland holding an axe that can be used to chop the tree in front of them. Suppose\nin the gameplay video, players either go straight to chop the tree or bypass it to explore the territory.\nIn order to predict future frames, it is sufficient to know (i) which goal (chop tree or bypass tree) is\nbeing pursued by the agent, and (ii) what will happen if the agent chooses a particular option (i.e.\ntransition dynamics). Apart from the latter information that is irrelevant to the past observations, we\nonly need to capture the goal information, i.e. whether the agent decides to chop the tree or bypass\nthe tree. Therefore, the task of establishing a comprehensive yet succinct goal space can be interpreted\nas predicting future states while conditioning on the transition dynamics of the environment.\n3\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFormally, our learning objective is to maximize the log-likelihood of future states given past\nones: log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡). Define 𝑔as a latent variable conditioned on past states (think of it as the\npotential goals the agent is pursuing given past states), the evidence lower-bound of the objective\ngiven variational posterior 𝑞𝜙(𝑔|𝑠0:𝑇) is the following (see Appendix A for the derivation of this and\nthe following equations):\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑔\n𝑝𝜃(𝑠𝑡+1:𝑇, 𝑔|𝑠0:𝑡)\n≥𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇) [log 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔)] −𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n,\nwhere 𝐷KL(·∥·) denotes the KL-divergence. Next, we break down the first term (i.e. 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔))\ninto components contributed by the (unknown) goal-conditioned policy 𝜋𝜃(𝑎|𝑠, 𝑔) and the transition\ndynamics 𝑝𝜃(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡) :\nlog 𝑝𝜃(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑔) =\n𝑇\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) · 𝑝𝜃(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\n≥\n𝑇\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝𝜃(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔) + 𝐶\n\u0003\n,\nwhere the constant 𝐶contains terms that depend solely on the environment dynamics and are\nirrelevant to what we want to learn (i.e. the goal space and the goal-conditioned policy). Bring it\nback to the original objective, we have\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑔∼𝑞𝜙(·|𝑠0:𝑇),𝑎𝜏∼𝑝𝜃(·|𝑠0:𝜏+1) [log 𝜋𝜃(𝑎𝜏|𝑠0:𝜏, 𝑔)]\n|                                                         {z                                                         }\nbehaviour cloning\n−\n𝐷KL\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝑡)\u0001\n|                               {z                               }\ngoal space constraint (KL regularization)\n,\nwhere 𝑞𝜙(·|𝑠0:𝑇) is implemented as a video encoder that maps the whole state sequence into the latent\ngoal space. 𝑝𝜃(·|𝑠0:𝜏+1) is the inverse dynamic model (IDM) that predicts actions required to achieve\na desired change in the states, which is usually a pre-trained model, details are in Appendix C. Thus,\nthe objective can be explained as jointly learning a video encoder and a goal-controller policy through\nbehavior cloning under succinct goal space constraints.\n4. GROOT Architecture Design and Training Strategy\nThis section illustrates how to create an agent (we call it GROOT) that can understand the semantic\nmeaning of a reference video and interact with the environment based on the aforementioned learning\nframework. According to the discussion in Section 3, the learnable parts of GROOT include the\nvideo encoder and the goal-conditioned policy. Recently, Transformer (Vaswani et al., 2017) has\ndemonstrated effectiveness in solving sequential decision-making problems (Brohan et al., 2022; Chen\net al., 2021; Parisotto et al., 2019). Motivated by this, we implement GROOT with transformer-based\nencoder-decoder architecture, as shown in Figure 2. The video encoder is a non-causal transformer\nthat extracts semantic information and generates goal embeddings. The policy is a causal transformer\ndecoder that receives the goal embeddings as the instruction and autoregressively translates the state\nsequence into a sequence of actions. Next, we describe how each module is constructed together with\nthe training strategy.\n4\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nVideo Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑠!\n𝑠\"\n𝑎#\n𝑎$\n𝑎\"\nVideo Encoder\nDecoder as Policy\n𝑠̂#\n𝑠̂%\n𝑠̂\"\n𝑠#\n𝑠%\n𝑠!\n𝑎#\n𝑎%\n𝑎!\n𝑠%\n𝑠!\nBehavior Cloning\nReference Video\nStates\nStates\nRollout Observations\n(a) Training\n(b) Inference\n𝑐̂#\n𝑐̂&\nLearnable Tokens\nLearned Tokens\n1.0\n0.5\n0.0\n𝜇\n𝒒𝒈𝟏:𝑵𝒔𝟏:𝑻)\nsample\n𝑠#\n𝑐#\n𝑐&\n𝑐#\n𝑐&\n𝑐̂#\n𝑐̂&\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n1.0\n0.5\n0.0\n𝜇\n𝒑𝒈𝟏:𝑵𝒔𝟏:𝒕)\n𝑫𝑲𝑳\n⋯\n⋯\n⋯\n⋯\n⋯\n⋯\n𝑠#\n⋯\n𝝁(⋅)\nFigure 2 | Our GROOT agent architecture. Left: In the training stage, a video encoder (non-causal\ntransformer) learns to extract the semantic meaning and transfer the video (state sequence) into the goal\nembedding space. A goal-conditioned policy (causal transformer) is learned to predict actions following the\ngiven instructions. We learn the agent using behavior cloning under a KL constraint. Right: During the\ninference, any reference video is passed into the video encoder to generate the goal embeddings that drive the\npolicy to interact with the environment.\n4.1. Video Encoder\nThe video encoder includes a Convolutional Neural Network (CNN) to extract spatial information\nfrom image states 𝑠1:𝑇and a non-causal transformer to capture temporal information from videos.\nSpecifically, we use a CNN backbone to extract visual embeddings {𝑥1:𝑇} for all frames. Additionally,\nmotivated by Devlin et al. (2019); Dosovitskiy et al. (2020), we construct a set of learnable embeddings\n(or summary tokens), represented as {𝑐1:𝑁}, to capture the semantic information present in the video.\nThe visual embeddings and summary tokens are passed to a non-causal transformer, resulting in the\noutput corresponding to the summary tokens as {ˆ𝑐1:𝑁}\n𝑥1:𝑇←Backbone(𝑠1:𝑇),\nˆ𝑐1:𝑁←Transformer([𝑥1:𝑇, 𝑐1:𝑁]).\n(1)\nSimilar to VAE (Kingma & Welling, 2013), we assume that the latent goal space follows a Gaussian\ndistribution, hence we use two fully connected layers, 𝜇(·) and 𝜎(·), to generate the mean and\nstandard deviation of the distribution, respectively. During training, we use the reparameterization\ntrick to sample a set of embeddings {𝑔1:𝑁} from the distribution, where 𝑔𝑡∼N (𝜇(ˆ𝑐𝑡), 𝜎(ˆ𝑐𝑡)). During\ninference, we use the mean of the distribution as the goal embeddings, i.e. 𝑔𝑡←𝜇(ˆ𝑐𝑡).\n4.2. Decoder as Policy\nTo introduce our policy module, we start with VPT (Baker et al., 2022), a Minecraft foundation model\ntrained with standard behavioral cloning. It is built on Transformer-XL (Dai et al., 2019) that can\nleverage long-horizon historical states and predict the next action seeing the current observation.\nHowever, the vanilla VPT architecture does not support instruction input. To condition the policy\non goal embeddings, we draw the inspiration from Flamingo (Alayrac et al., 2022), that is, to insert\ngated cross-attention dense layers into every Transformer-XL block. The keys and values in these layers\nare obtained from goal embeddings, while the queries are derived from the environment states\nˆ𝑥(𝑙)\n1:𝑡←GatedXATTN(kv = 𝑔1:𝑁, q = 𝑥(𝑙−1)\n1:𝑡\n; 𝜃𝑙),\n𝑥(𝑙)\n1:𝑡←TransformerXL(qkv = ˆ𝑥(𝑙)\n1:𝑡; 𝜃𝑙),\nˆ𝑎𝑡←FeedForward(𝑥(𝑀)\n𝑡\n),\n(2)\n5\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nwhere the policy reuses the visual embeddings extracted by the video encoder, i.e. 𝑥(0)\n1:𝑡= 𝑥1:𝑡, the\npolicy consists of 𝑀transformer blocks, 𝜃𝑙is the parameter of 𝑙-th block, ˆ𝑎𝑡is the predicted action.\nSince our goal space contains information about how to complete a task that is richer than previous\nlanguage-conditioned policy (Cai et al., 2023; Lifshitz et al., 2023), the cross-attention mechanism is\nnecessary. It allows the GROOT to query the task progress from instruction information based on\npast states, and then perform corresponding behaviors to complete the remaining progress.\n4.3. Training and Inference\nThe training dataset can be a mixture of Minecraft gameplay videos and offline trajectories. For\nthose videos without actions, an inverse dynamic model (Baker et al., 2022) can be used to generate\napproximate actions. Limited by the computation resources, we truncated all the trajectories into\nsegments with a fixed length of 𝑇without using any prior. We denote the final dataset as D =\n{(𝑥1:𝑇, 𝑎1:𝑇)}𝑀, where 𝑀is the number of trajectories. We train GROOT in a fully self-supervised\nmanner while the training process can be viewed as self-imitating, that is, training GROOT jointly\nusing behavioral cloning and KL divergence loss\nL(𝜃, 𝜙) = 𝔼(𝑠,𝑎)∼D\n\"\n𝜆𝐵𝐶\n∑︁\n𝑡\n−log 𝜋𝜃(𝑎𝑡|𝑠1:𝑡, 𝑔) + 𝜆𝐾𝐿\n∑︁\n𝜏\n𝐷𝐾𝐿\n\u0000𝑞𝜙(𝑔|𝑠0:𝑇) ∥𝑝𝜃(𝑔|𝑠0:𝜏)\u0001\n#\n,\n(3)\nwhere 𝜆𝐵𝐶, 𝜆𝐾𝐿are tradeoff coefficients, 𝑞𝜙is a posterior visual encoder, 𝑝𝜃is a prior video encoder\nwith the same architecture, 𝑔∼𝑞𝜙(·|𝑠0:𝑇). More details are in the Appendix D.\n5. Result\n5.1. Performance on Mastering Minecraft Skills\nMinecraft SkillForge Benchmark. In order to comprehensively evaluate the mastery of tasks by\nagents in Minecraft, we created a diverse benchmark called Minecraft SkillForge. It covers 30 tasks\nfrom 6 major categories of representative skills in Minecraft, including collect, explore, craft,\ntool, survive, and build. For example, the task “dig three down and fill one up” in the build\ncategory asks the agent to first dig three blocks of dirt, then use the dirt to fill the space above; The\ntask of “building a snow golem” ( ) requires the agent to sequentially stack 2 snow blocks (\n) and\n1 carved pumpkin (\n). We put the details of this benchmark in the Appendix H. Apart from some\nrelatively simple or common tasks such as “collect wood” and “hunt animals”, other tasks require the\nagent to have the ability to perform multiple steps in succession.\nWe compare GROOT with the following baselines: (a) VPT (Baker et al., 2022), a foundation\nmodel pre-trained on large-scale YouTube data, with three variants: VPT (fd), VPT(bc), and VPT(rl),\nindicating vanilla foundation model, behavior cloning finetuned model, and RL finetuned model; (b)\nSTEVE-1 (Lifshitz et al., 2023), an instruction-following agent finetuned from VPT, with two variants:\nSTEVE-1 (visual) and STEVE-1 (text) that receives visual and test instructions. More details are in\nAppendix F.1. It is worth noting that GROOT was trained from scratch.\nHuman Evaluation with Elo Rating. We evaluated the relative strength of agents by running an\ninternal tournament and reporting their Elo ratings, as in Mnih et al. (2015). Before the tournament,\neach agent is required to generate 10 videos of length 600 on each task. Note that, all the reference\nvideos used by GROOT are generated from another biome to ensure generalization. Additionally,\nwe also invited 3 experienced players to do these tasks following the same settings. After the video\ncollection, we asked 10 players to judge the quality of each pair of sampled videos from different\n6\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Elo Rating Comparison\n(b) Winning Rate of GROOT vs. Baselines\n(c) Success Rate Comparison\nFigure 3 | Results on Minecraft SkillForge benchmark. Left: Tournament evaluation of GROOT assessed\nby human players. GROOT performs better than state-of-the-art Minecraft agent STEVE-1. A 150-score gap\ncorresponds to a 70% probability of winning. Middle: Winning rate of GROOT v.s. other agents on specific\ntask categories. Colors from red to blue denote a decrease in the winning rate. Apart from the human player,\nGROOT surpasses all other baselines. Right: Success rate on 9 representative tasks. GROOT champions\nprocess-oriented tasks, such as dig three and fill one (\n) and build snow golems ( ).\nagents. Considering the diversity of tasks, we designed specific evaluation criteria for every task to\nmeasure the quality of rollout trajectories. For example, in the task of “build snow golem”, we rank\nthe completion degree of the task in ascending order: no blocks placed, one type of block placed,\ntwo types of blocks placed, and snow golem built successfully. After 1500 comparisons, the Elo\nrating converged as in Figure 3 (left). Although there is a large performance gap compared with\nhuman players, GROOT has significantly surpassed the current state-of-the-art STEVE-1 series and\ncondition-free VPT series on the overall tasks. Additional details are in Appendix G.\nIn Figure 3 (middle), we compare GROOT with other baselines in winning rate on six task groups.\nWe found that except for the performance on craft tasks, where STEVE-1 (visual) outperforms our\nmodel, GROOT achieves state-of-the-art results. In particular, GROOT greatly outperforms other\nbaselines by a large margin on build and tool. For build, the goal space needs to contain more\ndetailed procedural information, which is the disadvantage of methods that use future outcomes as\nthe goal. Moreover, such tasks are distributed sparsely in the dataset, or even absent in the dataset,\nwhich requires the agent to have strong generalization ability. As for craft group, GROOT is not\nsuperior enough, especially on the “crafting table” task. We attribute this to the wide task distribution\nin the dataset. Thus the future outcomes can prompt STEVE-1 to achieve a high success rate.\nProgrammatic Evaluation. To quantitatively compare the performance of the agents, we selected\n9 representative tasks out of 30 and reported the success rate of GROOT, STEVE-1 (visual), and\nVPT (bc) on these tasks in Figure 3 (right). We found that, based on the success rate on tasks such\nas dye and shear sheep(\n), enchat sword (\n), smelt food (\n), use bow (\n), sleep\n(\n), and lead animals (\n), GROOT has already reached a level comparable to that of human\nplayers (100%). However, the success rates for build snow golems ( ) and build obsidian\n(\n) tasks are only 60% and 50%. By observing the generated videos, we found that GROOT cannot\nprecisely identify the items in Hotbar (such as buckets, lava buckets, snow blocks, and pumpkin\nheads), resulting in a low probability of switching to the correct item. STEVE-1 also has the same\nproblem. This may be due to the current training paradigm lacking strong supervisory signals at the\nimage level. Future work may introduce auxiliary tasks such as vision-question answering (VQA) to\nhelp alleviate this phenomenon. Details are in Appendix F.3.\n7\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\n(a) Random Initialized\n(b) GROOT w\/o KL\n(c) GROOT w\/ KL\n(d) Synthesized Videos\nFigure 4 | t-SNE visualization of the goal space. Each color corresponds to a specific video category. (Left):\nSpace of randomly initialized video encoder. All the videos are entangled together. Middle: Space of GROOT\ntrained with self-supervised learning w\/ and w\/o KL regularization, respectively. The videos are clustered\nbased on their semantics. Visualization shows the subtle differences between the two. Right: Synthesized\nvideos using concatenation manner. The concatenated videos lay on the position between the source videos.\n5.2. Properties of Learned Goal Space\nThis section studies the properties of learned goal space. We used the t-SNE algorithm (van der Maaten\n& Hinton, 2008) to visualize the clustering effect of reference videos encoded in goal space, as in Figure\n4. We select 7 kinds of videos, including craft items, combat enemies, harvest crops,\nhunt animals, chop trees, trade with villagers, and mine ores. These videos are\nsampled from the contractor data (Baker et al., 2022) according to the meta information (details\nare in Appendix F.2). Each category contains 1k video segments. As a control group, in Figure 4\n(left), we showed the initial goal space of the video encoder (with a pre-trained EfficientNet-B0 (Tan\n& Le, 2019) as the backbone) before training. We found that the points are entangled together. After\nbeing trained on offline trajectories, as in Figure 4 (middle), it well understands reference videos and\nclusters them according to their semantics. This proves that it is efficient to learn behavior-relevant\ntask representations using our self-supervised training strategy. The clustering effect is slightly better\nwith KL regularization, though the difference is not very significant. Inevitably, there are still some\nvideos from different categories entangled together. We attribute this to the possibility of overlap\nin the performed behaviors of these videos. For example, chop trees and harvest crops both\nrely on a sequential of “attack” actions.\nCondition on Concatenated Videos. We also study the possibility of conditioning the policy\non concatenated videos. First, we collect 3 kinds of source videos, including chop trees, hunt\nanimals, and trade with villagers. We randomly sampled two videos from sources of chop\ntrees and hunt animals, downsampled and concatenated them into a synthetic video, denoted as\n[chop trees, hunt animals]. By the same token, we can obtain [hunt animals, trade\nwith villagers]. We visualize these videos together with the source videos in Figure 4 (right). We\nfound that the source videos lie far away from each other while the concatenated videos are distributed\nbetween their source videos. Based on this intriguing phenomenon, we infer that concatenated videos\nmay prompt GROOT to solve both tasks simultaneously. To verify this, we evaluate GROOT on\nthree kinds of reference videos, i.e. chop trees, hunt animals, and [chop trees, hunt\nanimals]. We launched GROOT in the forest and in the animal plains, respectively. The collected\nwood and killed mobs are reported in Figure 5. We found that although the concatenated video may\nnot be as effective as raw video in driving an agent to complete a single task (60% of the performance\nof raw video), it does possess the ability to drive the agent to perform multiple tasks. This is an\nimportant ability. As discussed in Wang et al. (2023b), sometimes the high-level planner will propose\nmultiple candidate goals, it will be efficient if the low-level controller can automatically determine\n8\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nchop\nhunt\nconcat\nWood Collected\n11.0\n1.5\n6.5\nchop\nhunt\nconcat\nMobs Killed\n0.5\n2.2\n1.5\nFigure 5 | Comparison on using raw and concate-\nnated reference videos as conditions. Left: Col-\nlected wood in the forest biome. Right: Killed mobs\nin the plains biome. “concat” denotes the reference\nvideo is [chop trees, hunt animals]. Statis-\ntics are measured over 10 episodes.\nw\/ KL\nw\/o KL\nSeagrass Collected\n3.7\n1.8\nw\/ KL\nw\/o KL\nWood Collected\n11.0\n7.3\nw\/ KL\nw\/o KL\nArrows Fired\n10.7\n9.3\nFigure 6 | Ablation study on KL loss.\nAfter being\njointly trained with KL loss, GROOT can collect 2×\nmore seagrass (\n) underwater and 1.5× wood (\n)\nin the forest while the difference is not as impressive\non the use bow (\n) task. Statistics are measured\nover 10 episodes.\ndiamonds distribute in this level\nGROOT\nSTEVE-1\ncondition changed\ncondition changed\ndiamonds distribute in this level\n~𝟏𝟔%\nstuck in the bedrock\nFigure 7 | Results on solving challenging obtain diamond task. The vertical dashed lines represent the\ntime when a certain item is first obtained. Left: GROOT first dug down to the depth of 12 and then mined\nhorizontally to obtain diamonds with an average success rate of 16%. Right: STEVE-1 quickly dug down to the\nspecific depth but struggled to maintain its height.\nwhich to accomplish based on the current observation.\nAblation on KL Divergence Loss. To investigate the role of KL loss in training, we evaluated\nGROOT (w\/ KL) and its variant (w\/o KL) on three tasks: collect seagrass (\n), collect\nwood (\n), and use bow (\n). As shown in Figure 6, we found that introducing the constraint of\nKL loss improved agent performance by 2× and 1.5× in the first two tasks, whereas there was no\nsignificant effect in the use bow task. This may be because the first two tasks require the agent to\ngeneralize the corresponding skills to different terrains (e.g. locating trees in the environment for\ncollecting wood and sinking to specific locations for collecting seagrass). Therefore, it puts higher\ndemands on the agent’s ability to generalize in the goal space, and this is exactly the role played by\nthe KL loss. The use bow task is relatively simple in comparison because it only requires charging\nand shooting the arrow, without considering environmental factors.\n5.3. Combining Skills for Long-horizon Tasks\nIn this section, we explore whether GROOT can combine skills to solve long-horizon tasks, which is\nkey to its integration with a high-level planner. Taking the task of mining diamonds as an example,\nprior knowledge is that diamond ores are generally distributed between the 7th and 14th floors\nunderground, and the probability of appearing in other depths is almost zero. Therefore, the agent\nneeds to first dig down to the specified depth (12) and then maintain horizontal mining. To achieve\nthis, we designed two reference videos, each 128 frames long. One describes the policy of starting\n9\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfrom the surface and digging down, and the other demonstrates the behaviors of horizontal mining.\nWe show an example in Figure 7 (left). In the beginning, GROOT quickly digs down to the specified\ndepth and then switches to horizontal mining mode. It maintains the same height for a long time and\nfound diamonds at 11k steps. In addition, we compared STEVE-1 (visual) under the same setting in\nFigure 7 (right). After switching to the horizontal mining prompt, STEVE-1 maintains its height for a\nshort time before it stuck in the bedrock layer (unbreakable in survival mode), greatly reducing the\nprobability of finding diamonds. This indicates that our goal space is expressive enough to instruct\nthe way of mining, and the policy can follow the instructions persistently and reliably. In contrast,\nSTEVE-1, which relies on future outcomes as a condition, was unable to maintain its depth, despite\nattempts at various visual prompts. We conducted 25 experiments each on GROOT and STEVE-1,\nwith success rates of 16% and 0% for finding diamonds. Additional details are in the Appendix F.4.\n6. Related Works\nPre-train Policy on Offline Data. Pre-training neural networks on web-scale data has been demon-\nstrated as an effective training paradigm in Nature Language Processing (Brown et al., 2020) and\nComputer Vision (Kirillov et al., 2023). Inspired by this, researchers tried to transfer the success\nto the field of decision-making from pre-training visual representations and directly distilling the\npolicy from offline data. As the former, Aytar et al. (2018a); Bruce et al. (2023) leveraged temporal\ninformation present in videos as the supervision signal to learn visual representations. The represen-\ntations are then used to generate intrinsic rewards for boosting downstream policy learning, which\nstill requires expensive online interactions with the environment. Chen et al. (2021); Schmidhuber\n(2019) leveraged scalable offline trajectories to train optimal policy by conditioning it on cumulated\nrewards. Laskin et al. (2022) proposed to learn an in-context policy improvement operator that\ncan distill an RL algorithm in high data efficiency. Reed et al. (2022) learned a multi-task agent\nGato by doing behavior cloning on a large-scale expert dataset. By serializing task data into a flat of\nsequence, they use the powerful transformer architecture to model the behavior distribution. However,\nthese methods either require elaborated reward functions or explicit task definitions. This makes\nit hard to be applied to open worlds, where tasks are infinite while rewards are lacking. Another\ninteresting direction is to use pre-trained language models for reasoning and vision language models\nfor discrimination, to guide the policy in life-long learning in the environment (Di Palo et al., 2023).\nCondition Policy on Goal Space. Researchers have explored many goal modalities, such as\nlanguage (Khandelwal et al., 2021), image (Du et al., 2021), and future video (Xie et al., 2023), to\nbuild a controllable policy. Brohan et al. (2022) collected a large-scale dataset of trajectory-text pairs\nand trained a transformer policy to follow language instructions. Despite the language being a natural\ninstruction interface, the cost of collecting paired training data is expensive. As a solution, Majumdar\net al. (2022) sorted to use hindsight relabeling to first train a policy conditioned on the target image,\nthen aligned text to latent image space, which greatly improves training efficiency. Lifshitz et al.\n(2023) moved a big step on this paradigm by replacing the target image with a 16-frame future video\nand reformulating the modality alignment problem into training a prior of latent goal given text.\nBuild Agents in Minecraft. As a challenging open-world environment, Minecraft is attracting an\nincreasing number of researchers to develop AI agents on it, which can be divided into plan-oriented\n(Wang et al., 2023a,b) and control-oriented methods (Baker et al., 2022; Cai et al., 2023; Lifshitz\net al., 2023) based on their emphasis. Plan-oriented agents aim to reason with Minecraft knowledge\nand decompose the long-horizon task into sub-tasks followed by calling a low-level controller. Control-\noriented works follow the given instructions and directly interact with the environments using low-level\nactions (mouse and keyboard). Baker et al. (2022) pre-trained the first foundation model VPT in\nMinecraft using internet-scale videos. Although it achieves the first obtaining diamond milestone by\n10\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nfine-tuning with RL, it does not support instruction input. Lifshitz et al. (2023) created the first agent\nthat can solve open-ended tasks by bridging VPT and MineCLIP (Fan et al., 2022). However, its goal\nspace is not expressive enough and prevents it from solving multi-step tasks.\n7. Limitation and Conclusion\nAlthough GROOT has demonstrated powerful capabilities in expressing open-ended tasks in the\nform of video instructions, training such a goal space remains highly challenging. We found that\nGROOT is quite sensitive to the selection of reference videos, which we attribute to the fact that\nthe goal space trained from an unsupervised perspective may not be fully aligned with the human\nintention for understanding the semantics of the reference video. Therefore, it would be a promising\nresearch direction in the future to use SFT (supervised fine-tuning, Sanh et al. (2021)) and RLHF\n(Ziegler et al., 2019) to align the pre-trained goal space with human preference.\nIn conclusion, we propose a paradigm for learning to follow instructions by watching gameplay\nvideos. We prove that video instruction is a good form of goal space that not only expresses open-ended\ntasks but can be trained through self-imitation (once the IDM is available to label pseudo actions for raw\ngameplay videos). Based on this, we built an encoder-decoder transformer architecture agent named\nGROOT in Minecraft. Without collecting any text-video data, GROOT demonstrated extraordinary\ninstruction-following ability and crowned the Minecraft SkillForge benchmark. Additionally, we also\nshowed its potential as a planner downstream controller in the challenging obtain diamond task.\nWe believe that this training paradigm can be generalized in other complex open-world environments.\n11\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for\nfew-shot learning. ArXiv, abs\/2204.14198, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:248476411.\nMarcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. ArXiv,\nabs\/1707.01495, 2017. URL https:\/\/api.semanticscholar.org\/CorpusID:3532908.\nYusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyun Wang, and Nando de Freitas. Playing\nhard exploration games by watching youtube. In Neural Information Processing Systems, 2018a.\nURL https:\/\/api.semanticscholar.org\/CorpusID:44061126.\nYusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyun Wang, and Nando de Freitas. Playing\nhard exploration games by watching youtube. In Neural Information Processing Systems, 2018b.\nURL https:\/\/api.semanticscholar.org\/CorpusID:44061126.\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton,\nRaul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled\nonline videos. ArXiv, abs\/2206.11795, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:249953673.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian\nIchter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan C. Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha\nManjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch,\nJornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed,\nJaspiar Singh, Sumedh Anand Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Van-\nhoucke, Steve Vega, Quan Ho Vuong, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna\nZitkovich. Rt-1: Robotics transformer for real-world control at scale. ArXiv, abs\/2212.06817, 2022.\nURL https:\/\/api.semanticscholar.org\/CorpusID:254591260.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. ArXiv, abs\/2005.14165, 2020. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:218971783.\nJake Bruce, Ankit Anand, Bogdan Mazoure, and Rob Fergus. Learning about progress from ex-\nperts.\nIn International Conference on Learning Representations, 2023.\nURL https:\/\/api.\nsemanticscholar.org\/CorpusID:259298702.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. 2023 IEEE\/CVF\n12\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 13734–13744, 2023. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:256194112.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srini-\nvas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.\nIn Neural Information Processing Systems, 2021. URL https:\/\/api.semanticscholar.org\/\nCorpusID:235294299.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a fixed-length context.\nIn Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, Jan 2019.\ndoi:\n10.18653\/v1\/p19-1285. URL http:\/\/dx.doi.org\/10.18653\/v1\/p19-1285.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding.\nArXiv, abs\/1810.04805, 2019.\nURL\nhttps:\/\/api.semanticscholar.org\/CorpusID:52967399.\nNorman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, and\nMartin Riedmiller. Towards a unified agent with foundation models. In Workshop on Reincarnating\nReinforcement Learning at ICLR 2023, 2023.\nYiming Ding, Carlos Florensa, Mariano Phielipp, and P. Abbeel. Goal-conditioned imitation learn-\ning. ArXiv, abs\/1906.05838, 2019. URL https:\/\/api.semanticscholar.org\/CorpusID:\n189762519.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. ArXiv, abs\/2010.11929, 2020. URL https:\/\/api.semanticscholar.org\/CorpusID:\n225039882.\nHeming Du, Xin Yu, and Liang Zheng. Vtnet: Visual transformer network for object goal naviga-\ntion. ArXiv, abs\/2105.09447, 2021. URL https:\/\/api.semanticscholar.org\/CorpusID:\n234790212.\nLasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala: Scalable\ndistributed deep-rl with importance weighted actor-learner architectures. ArXiv, abs\/1802.01561,\n2018. URL https:\/\/api.semanticscholar.org\/CorpusID:3645060.\nLinxi (Jim) Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, An-\ndrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.\nMinedojo: Building open-\nended embodied agents with internet-scale knowledge. ArXiv, abs\/2206.08853, 2022. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:249848263.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-\nChun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv\npreprint arXiv:2309.09971, 2023.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela M. Veloso,\nand Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. In Interna-\ntional Joint Conference on Artificial Intelligence, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:199000710.\n13\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nYicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould.\nVln-bert: A\nrecurrent vision-and-language bert for navigation. 2021 IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 1643–1653, 2020. URL https:\/\/api.semanticscholar.\norg\/CorpusID:227228335.\nWenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models. In Conference on Robot Learning, 2022. URL https:\n\/\/api.semanticscholar.org\/CorpusID:250451569.\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models. ArXiv, abs\/2307.05973,\n2023. URL https:\/\/api.semanticscholar.org\/CorpusID:259837330.\nMingxuan Jing, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Chao Yang, Bin Fang, and Huaping Liu.\nReinforcement learning from imperfect demonstrations under soft expert guidance. In Proceedings\nof the AAAI conference on artificial intelligence, volume 34, pp. 5109–5116, 2020.\nMingxuan Jing, Wenbing Huang, Fuchun Sun, Xiaojian Ma, Tao Kong, Chuang Gan, and Lei Li.\nAdversarial option-aware hierarchical imitation learning. In International Conference on Machine\nLearning, pp. 5097–5106. PMLR, 2021.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial\nintelligence experimentation. In International Joint Conference on Artificial Intelligence, 2016. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:9953039.\nApoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective:\nClip embeddings for embodied ai. 2022 IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 14809–14818, 2021. URL https:\/\/api.semanticscholar.org\/\nCorpusID:244346010.\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs\/1312.6114, 2013.\nURL https:\/\/api.semanticscholar.org\/CorpusID:216078090.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick.\nSegment anything. ArXiv, abs\/2304.02643, 2023. URL https:\/\/api.semanticscholar.\norg\/CorpusID:257952310.\nMichael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald,\nDJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni,\nSatinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm distilla-\ntion. ArXiv, abs\/2210.14215, 2022. URL https:\/\/api.semanticscholar.org\/CorpusID:\n253107613.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila A. McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. ArXiv, abs\/2306.00937, 2023. URL https:\/\/api.\nsemanticscholar.org\/CorpusID:258999563.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang. Juewu-mc: Playing minecraft\nwith sample-efficient hierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021.\n14\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nMinghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems\nand solutions. ArXiv, abs\/2201.08299, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:246063885.\nArjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, and Dhruv Batra. Zson: Zero-\nshot object-goal navigation using multimodal goal embeddings. ArXiv, abs\/2206.12403, 2022. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:250048645.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,\nAlex Graves, Martin A. Riedmiller, Andreas Kirkeby Fidjeland, Georg Ostrovski, Stig Petersen,\nCharlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,\nShane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,\n518:529–533, 2015. URL https:\/\/api.semanticscholar.org\/CorpusID:205242740.\nEmilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Çaglar Gülçehre, Siddhant M.\nJayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick,\nNicolas Manfred Otto Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning.\nIn International Conference on Machine Learning, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:204578308.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\nBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist\nagent. arXiv preprint arXiv:2205.06175, 2022.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman,\nLeo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables\nzero-shot task generalization, 2021.\nJuergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards - just map\nthem to actions. ArXiv, abs\/1912.02875, 2019. URL https:\/\/api.semanticscholar.org\/\nCorpusID:208857600.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. ArXiv, abs\/1707.06347, 2017. URL https:\/\/api.semanticscholar.\norg\/CorpusID:28695052.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman,\nDominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach,\nKoray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural\nnetworks and tree search. Nature, 529:484–489, 2016. URL https:\/\/api.semanticscholar.\norg\/CorpusID:515925.\nAustin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Ho Vuong,\nPaul Wohlhart, Brianna Zitkovich, F. Xia, Chelsea Finn, and Karol Hausman. Open-world object\nmanipulation using pre-trained vision-language models. ArXiv, abs\/2303.00905, 2023. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:257280290.\n15\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nMingxing Tan and Quoc V. Le.\nEfficientnet: Rethinking model scaling for convolutional neu-\nral networks. ArXiv, abs\/1905.11946, 2019. URL https:\/\/api.semanticscholar.org\/\nCorpusID:167217261.\nMaxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. Label Studio: Data\nlabeling software, 2020-2022. URL https:\/\/github.com\/heartexlabs\/label-studio.\nOpen source software available from https:\/\/github.com\/heartexlabs\/label-studio.\nLaurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-sne. Journal of Machine Learn-\ning Research, 9:2579–2605, 2008. URL https:\/\/api.semanticscholar.org\/CorpusID:\n5855042.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. URL https:\n\/\/api.semanticscholar.org\/CorpusID:13756489.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language mod-\nels. ArXiv, abs\/2305.16291, 2023a. URL https:\/\/api.semanticscholar.org\/CorpusID:\n258887849.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and se-\nlect: Interactive planning with large language models enables open-world multi-task agents.\nArXiv, abs\/2302.01560, 2023b.\nURL https:\/\/api.semanticscholar.org\/CorpusID:\n256598146.\nZhihui Xie, Zichuan Lin, Deheng Ye, Qiang Fu, Wei Yang, and Shuai Li. Future-conditioned unsuper-\nvised pretraining for decision transformer. In International Conference on Machine Learning, 2023.\nURL https:\/\/api.semanticscholar.org\/CorpusID:258947476.\nChao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, and Chuang\nGan. Imitation learning from observations by minimizing inverse dynamics disagreement. Advances\nin neural information processing systems, 32, 2019.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan C. Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learn-\ning. ArXiv, abs\/1910.10897, 2019. URL https:\/\/api.semanticscholar.org\/CorpusID:\n204852201.\nQihang Zhang, Zhenghao Peng, and Bolei Zhou. Learning to drive by watching youtube videos:\nAction-conditioned contrastive policy pretraining. In European Conference on Computer Vision, 2022.\nURL https:\/\/api.semanticscholar.org\/CorpusID:250626771.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyuan Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:\nGenerally capable agents for open-world environments via large language models with text-based\nknowledge and memory. ArXiv, abs\/2305.17144, 2023. URL https:\/\/api.semanticscholar.\norg\/CorpusID:258959262.\nDaniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano,\nand Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs\/1909.08593,\n2019. URL https:\/\/api.semanticscholar.org\/CorpusID:202660943.\n16\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nAppendix\nA. Derivation\nIn this section, we detail how we derive the final objective. Recall that the goal is to maximize the\nlog-likelihood of future states given past ones: log 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡). Using Bayes’ theorem and the Jensen’s\ninequality, we have:\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) = log\n∑︁\n𝑧\n𝑝(𝑠𝑡+1:𝑇, 𝑧|𝑠0:𝑡),\n(4)\n= log\n∑︁\n𝑧\n𝑝(𝑠𝑡+1:𝑇, 𝑧|𝑠0:𝑡) 𝑞(𝑧|𝑠0:𝑇)\n𝑞(𝑧|𝑠0:𝑇)\n,\n(5)\n≥𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇, 𝑧|𝑠0:𝑡) −log 𝑞(𝑧|𝑠0:𝑇)\n\u0003\n,\n(6)\n= 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) + log 𝑝(𝑧|𝑠0:𝑡) −log 𝑞(𝑧|𝑠0:𝑇)\n\u0003\n,\n(7)\n= 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧)\n\u0003\n+ 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑧|𝑠0:𝑡)\n𝑞(𝑧|𝑠0:𝑇)\n\u0003\n,\n(8)\n= 𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧)\n\u0003\n−𝐷KL\n\u0000𝑞(𝑧|𝑠0:𝑇) ∥𝑝(𝑧|𝑠0:𝑡)\u0001\n.\n(9)\nWe break down 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) into components: goal-conditioned policy 𝜋(𝑎𝜏|𝑠0:𝜏+1) and the transition\ndynamics 𝑝(𝑠𝑡+1|𝑠0:𝑡, 𝑎𝑡), we have\n𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) =\n𝑇−1\nÖ\n𝜏=𝑡\n\u0000 ∑︁\n𝑎𝜏\n𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) · 𝑝(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏)\u0001\n.\n(10)\nFurthermore, using Jensen’s inequality, log 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) can be written as\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧) =\n𝑇−1\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) · 𝑝(𝑠𝜏+1|𝑠0:𝜏, 𝑎𝜏),\n(11)\n=\n𝑇−1\n∑︁\n𝜏=𝑡\nlog\n∑︁\n𝑎𝜏\n𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) · 𝑝(𝑎𝜏|𝑠0:𝜏, 𝑠𝜏+1) · 𝑝(𝑠𝜏+1|𝑠0:𝜏)\n𝑝(𝑎𝜏|𝑠0:𝜏)\n,\n(12)\n≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏,𝑠𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧) + 𝐶\n\u0003\n,\n(13)\nwhere the constant 𝐶= log 𝑝(𝑠𝜏+1|𝑠0:𝜏)−log 𝑝(𝑎𝜏|𝑠0:𝜏) describes the dataset distribution and is irrelevant\nto what we want to learn (i.e. the goal space and the goal-conditioned policy), we have:\n𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡, 𝑧)\n\u0003\n≥𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇)\n\u0002 𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏,𝑠𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧)\n\u0003\u0003\n,\n(14)\n=\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇),𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏,𝑠𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧)\n\u0003\n.\n(15)\nThus, we derived the evidence lower-bound of log 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) as follows\nlog 𝑝(𝑠𝑡+1:𝑇|𝑠0:𝑡) ≥\n𝑇−1\n∑︁\n𝜏=𝑡\n𝔼𝑧∼𝑞(𝑧|𝑠0:𝑇),𝑎𝜏∼𝑝(𝑎𝜏|𝑠0:𝜏+1)\n\u0002\nlog 𝜋(𝑎𝜏|𝑠0:𝜏, 𝑧)\n\u0003\n−𝐷KL\n\u0000𝑞(𝑧|𝑠0:𝑇) ∥𝑝(𝑧|𝑠0:𝑡)\u0001\n.\n(16)\n17\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFigure 8 | Examples of Minecraft environment. Tasks from top to bottom, from left to right are building houses,\nplanting wheat, fishing, brewing a potion, mining diamond ores, and combating the ender dragon, respectively.\nB. Minecraft Environment\nMinecraft is an extremely popular sandbox game that allows players to freely create and explore their\nworld. This game has infinite freedom, allowing players to change the world and ecosystems through\nbuilding, mining, planting, combating, and other methods (shown in Figure 8). It is precisely because\nof this freedom that Minecraft becomes an excellent AI testing benchmark (Baker et al., 2022; Cai\net al., 2023; Fan et al., 2022; Johnson et al., 2016; Lifshitz et al., 2023; Wang et al., 2023a,b). In this\ngame, AI agents need to face situations that are highly similar to the real world, making judgments\nand decisions to deal with various environments and problems. Therefore, Minecraft is a very suitable\nenvironment to be used as an AI testing benchmark. By using Minecraft, AI researchers can more\nconveniently simulate various complex and diverse environments and tasks, thereby improving the\npractical value and application of AI technology.\nWe use the combination of 1.16.5 version MineRL (Guss et al., 2019) and MCP-Reborn as our\ntesting platform, which is consistent with the environment used by VPT (Baker et al., 2022) and\nSTEVE-1 (Lifshitz et al., 2023). Mainly because this platform preserves observation and action space\nthat is consistent with human players to the fullest extent. On the one hand, this design brings about\nhigh challenges, as agents can only interact with the environment using low-level mouse and keyboard\nactions, and can only observe visual information like human players without any in-game privileged\ninformation. Therefore, the AI algorithms developed on this platform can have higher generalization\nability. On the other hand, this also presents opportunities for us to conduct large-scale pre-training\non internet-scale gameplay videos.\nB.1. Observation Space\nThe visual elements included in our observation space are completely consistent with those seen\nby human players, including the Hotbar, health indicators, player hands, and equipped items. The\nplayer’s perspective is in the first person with a field of view of 70 degrees. The simulator first\ngenerates an RGB image with dimensions of 640 × 360 during the rendering process. Before inputting\nto the agent, we resize the image to 224 × 224 to enable the agent to clearly see item icons in the\ninventory and important details in the environment. When the agent opens the GUI, the simulator\nalso renders the mouse cursor normally. The RGB image is the only observation that the agent can\nobtain from the environment during inference. It is worth noting that to help the agent see more\nhttps:\/\/github.com\/Hexeption\/MCP-Reborn\n18\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 1 | Action space descriptions from Minecraft wiki (https:\/\/minecraft.fandom.com\/wiki\/Controls).\nIndex\nAction\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove backward.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement. When\nused in conjunction with the attack function in the GUI,\nit can swap items between inventory and Hotbar. When\nused with the craft function, it crafts the maximum pos-\nsible number of items instead of just one.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current motion.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nPut down the item being held or interact with the block\nthat the player is currently looking at. Within the GUI,\npick up a stack of items or place a single item from the\nstack that is being held by the mouse.\n10\nhotbar.[1-9]\nkeys 1 - 9\nSelects the appropriate hotbar item. When in the inven-\ntory GUI, swap the contents of the inventory slot under\nthe mouse pointer and the corresponding hotbar slot.\n11\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180\nto +180.\n12\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180\nto +180.\nclearly in extremely dark environments, we have added a night vision effect for the agent, which\nincreases the brightness of the environment during nighttime.\nB.2. Action Space\nOur action space is almost identical to that of humans, except for actions that involve inputting\nstrings. It consists of two parts: the mouse and the keyboard. The mouse movement is responsible for\nchanging the player’s camera perspective and moving the cursor when the GUI is opened. The left and\nright buttons are responsible for attacking and using items. The keyboard is mainly responsible for\ncontrolling the agent’s movement. We list the meaning of each action in the Table 1. To avoid predicting\nnull action, we used the same joint hierarchical action space as Baker et al. (2022), which consists of\nbutton space and camera space. Button space encodes all combinations of keyboard operations and a\nflag indicating whether the mouse is used, resulting in a total of 8461 candidate actions. The camera\nspace discretizes the range of one mouse movement into 121 actions. Therefore, the action head of\nthe agent is a multi-classification network with 8461 dimensions and a multi-classification network\nwith 121 dimensions.\nC. Inverse Dynamic Model\nAccording to the theory in Section 3, we know that our training paradigm relies on the inverse\ndynamic model (IDM) which generates pseudo action labels for raw gameplay videos to calculate the\nbehavior cloning loss. Therefore, in this section, we introduced the background knowledge of IDM.\n19\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nIDM is a non-causal model that aims to uncover the underlying action that caused changes\nin the current step by observing historical and future states, and it can be formally represented as\n𝑝(𝑎𝑡|𝑜𝑡, 𝑜𝑡+1). Compared with traditional policies learned via behavior cloning, IDM is more accurate in\npredicting actions because it can observe the changes between past and future frames. OpenAI (Baker\net al., 2022) developed the first inverse dynamic model in the Minecraft domain. By extending the\nlength of the observable sequence to 128 and modeling 𝑝(𝑎𝑡|𝑜𝑡−64:𝑡+64) with a non-causal transformer,\nthe IDM achieved the accuracy of action prediction to over 95% with only 2k hours of game trajectories.\nThis makes it possible for our training paradigm to utilize the large-scale Minecraft data available on\nthe Internet. Moreover, Zhang et al. (2022) has also trained an accurate IDM model with a small\namount of data in a real autonomous driving environment, which further provides a basic guarantee\nfor our training method to generalize to other complex environments.\nD. Implementation Details\nD.1. Model Architecture\nThe video encoder consists of a convolutional neural network backbone and a non-causal transformer.\nInspired by Brohan et al. (2022), we adopted the EfficientNet (Tan & Le, 2019) as the backbone.\nSpecifically, we use its variant EfficientNet-B0 for efficiency, which takes in images of size 224 × 224\nand extracts a feature vector of shape 7 × 7 × 1280, where 7 × 7 denotes the spatial dimensions. In\norder to adaptively enhance the important visual information, we use a shallow transformer to pool\nthe feature map along spatial channels. To fuse global visual features, we construct another learnable\nembedding [sp], concatenate it with the 49 features in space, and obtain a token sequence of length\n50. After being processed by the transformer, the output for the [sp] token corresponds to the pooled\nvisual feature, whose dimension is 𝑑ℎ𝑖𝑑= 1024. To capture the temporal features of the video, we\nremove the code related to the casual mask in the minGPT and obtain a non-causal transformer. The\npolicy decoder consists of 4 identical blocks, where each block contains a Flamingo gated-attention\ndense layer (Alayrac et al., 2022) and a Transformer-XL block(Dai et al., 2019). The Transformer-XL\nblock maintains a recurrence memory of past 128 key-value pairs to memory long-horizon history\nstates. We directly use the Transformer-XL implementation in Baker et al. (2022) with a simple\nmodification, i.e. before passing states into the policy decoder, we add the previous action to the\nstate embedding at each timestep. Notably, We find this modification very useful especially when we\nneed to train the policy from scratch. As it not only accelerates the training process but makes the\npredicted action more consistent and smooth. Additional hyperparameters can be found in Table 2.\nD.2. Inference\nTo generate reference videos, we invited three human players to play each task according to the task\ndescription. Each person was asked to produce two videos, so we could prepare six videos for each\ntask in total. Then, we selected the most relevant video to the task description from the six videos\nand cropped the first 128 frames into a new video, which was used to instruct GROOT to complete\nthis task. In addition, we selected a 16-frame segment that best expressed the task information as the\nvisual prompt for STEVE-1 (visual) from these six videos. This ensures fairness in comparison.\nDuring inference, we found that in some tasks, such as build obsidian (\n), GROOT’s\nbehavior mixed with the intention of traveling around. We believe this is a bias introduced during\ntraining. We draw the inspiration from STEVE-1 (Lifshitz et al., 2023) and subtract this bias in the\naction logits space before sampling the action. Specifically, we infer two models at the same time,\nwhere one model’s condition is a specific task video and the other model’s condition is a 128-frame\nvideo of traveling freely in the environment. The input observations for the two models are exactly\nhttps:\/\/github.com\/karpathy\/minGPT\n20\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 2 | Hyperparameters for training GROOT.\nHyperparameter\nValue\nOptimizer\nAdamW\nWeight Decay\n0.001\nLearning Rate\n0.0000181\nWarmup Steps\n2000\nNumber of Workers\n4\nParallel Strategy\nddp\nType of GPUs\nNVIDIA RTX 4090Ti, A40\nParallel GPUs\n8\nAccumulate Gradient Batches\n8\nBatch Size\/GPU (Total)\n2 (128)\nTraining Precision\nbf16\nInput Image Size\n224 × 224\nCNN Backbone\nEfficientNet-B0\nEncoder Transformer\nminGPT (w\/o causal mask)\nDecoder Transformer\nTransformerXL\nNumber of Encoder Blocks\n8\nNumber of Decoder Blocks\n4\nHidden Dimension\n1024\nNumber of Condition Slots\n1\nTrajectory Chunk size\n128\nAttention Memory Size\n256\nWeight of BC Loss\n1\nWeight of KL Loss\n0.01\nthe same. At each time step, we use the action logits of the previous model to subtract a certain\nproportion of the action logits predicted by the latter model before using the Gumbel-Softmax trick to\nsample the action. The logits calculation equation is directly borrowed from Lifshitz et al. (2023)\nlogits𝑡= (1 + 𝜆) 𝑓𝜃(𝑜1:𝑡, 𝑔goal) −𝜆𝑓𝜃(𝑜1:𝑡, 𝑔bias)\n(17)\nwhere 𝑓𝜃(𝑜1:𝑡, 𝑔goal) and 𝑓𝜃(𝑜1:𝑡, 𝑔bias) are two kinds of action logits generated by feeding forward two\nreference videos goal and bias to GROOT, 𝜆is a trade-off parameter. As illustrated in Figure 9,\nwe find that this trick can improve the success rate of tasks such as build obsidian (\n), build\nsnow golem ( ), enchant sword (\n), and dig three down and fill one up (\n) with\nthe 𝜆= 1.5. Interestingly, we observe that the effective 𝜆scale (approximately 1.5) in our model\nis much smaller than the scale (approximately 6.5) used in STEVE-1. We speculate that this may\nbe because STEVE-1 fine-tunes the foundation VPT to gain steerability, but VPT does not receive\ngoal conditions for demonstrations during behavior cloning. This may cause VPT to learn overly\nsmooth behavior distributions, requiring the use of larger lambda scales to activate goal-specific\nbehaviors. Although this technique is effective at inference time, it still requires hyperparameter\ntuning in practice. In the future, it will be meaningful to directly remove biased behaviors from the\ntraining process.\nD.3. Ablation on Number of Condition Slots\nIn this section, we explore the impact of the number of condition slots (number of learnable tokens)\non the final performance. We compared the performance of the model on 6 programmatic tasks\n21\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBuild Obsidian\n0.1\n0.2\n0.5\n0.4\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBuild Snow Golem\n0.1\n0.3\n0.6\n0.4\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEnchant Sword\n0.2\n0.6\n0.9\n0.4\nλ = 0.0\nλ = 1.0\nλ = 1.5\nλ = 2.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDig Three Fill One\n0.0\n0.2\n0.3\n0.0\nFigure 9 | Ablation on the condition scale 𝜆.\nwith 𝑁= 1 and 𝑁= 3 condition slots and computed quantitative metrics for each task. As shown\nin Table 3, we find that increasing the number of condition slots leads to a significant decrease\nin the model’s performance on most tasks, except for the “explore run” task. We speculate that\nhaving more condition slots may result in a higher number of dimensions in the goal space, which in\nturn reduces the generalization ability of the learned goal space. Therefore, we suggest that when\napplying GROOT to other environments, the hyperparameters should be carefully chosen based on\nthe characteristics of the environment or using other parameter selection methods.\nTable 3 | Ablation on the number of condition slots.\nTask Name\n(Metric)\nexplore run ↑\n(distance)\nbuild pillar ↓\n(height of pillar)\ncollect grass ↓\n(num of grass)\ncollect seagrass ↓\n(num of seagrass)\ncollect dirt ↓\n(num of dirt)\nmine stone ↓\n(num of stones)\n𝑁= 1\n54.0\n37.6\n23.8\n3.3\n6.2\n12.2\n𝑁= 3\n59.0\n13.3\n5.6\n0.9\n5.4\n11.2\nE. Dataset Details\nE.1. Contractor Data\nThe contractor data is a Minecraft offline trajectory dataset provided by Baker et al. (2022) , which\nis annotated by professional human players and used for training the inverse dynamic model. In\nthis dataset, human players play the game while the system records the image sequence {𝑠1:𝑇}𝑀,\naction sequence {𝑎1:𝑇}𝑀, and metadata {𝑒1:𝑇}𝑀generated by the players. Excluding frames containing\nempty actions, the dataset contains 1.6 billion frames with a duration of approximately 2000 hours.\nThe metadata records the events triggered by the agent in the game at each time step, including\nthree types: craft item, pickup, and mine block, which represent the agent’s activities of\ncrafting items using the GUI, picking up dropped items and destroying blocks at the current time step,\nrespectively. In the process of training GROOT, we use all trajectories provided by the contractor data,\nbut without including any metadata. We only use the metadata to retrieve relevant trajectory segments\nduring the visualization of the goal space.\nF. Experimental Setup Details\nF.1. Baseline Details\nVPT is the first foundation model in the Minecraft domain developed by Baker et al. (2022). Its\narchitecture consists of ImpalaCNN and TransformerXL. Using behavior cloning algorithms to pre-train\nhttps:\/\/github.com\/openai\/Video-Pre-Training\n22\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\non large-scale YouTube demonstrations, they obtained the first checkpoint of VPT(fd) which can\nfreely explore the environment. To further enhance the agent’s abilities in early-game environments,\nthey constructed an “earlygame” dataset and fine-tuned the pre-trained foundation model on that\ndataset, resulting in the VPT(bc) checkpoint. This model significantly improved performance on\nbasic tasks such as “crafting table” and “collecting wood”. Based on VPT(bc), they used online\nreinforcement learning with a carefully designed reward shaping to obtain the checkpoint VPT(rl)\ncapable of obtaining diamonds entirely from scratch. It is noteworthy that the models’ architectures\nof all three checkpoints are consistent and do not support instruction input. That’s why their rankings\non the Minecraft SkillForge benchmark are low. We also observed that the performance of VPT(bc)\nsurpasses that of VPT(rl) due to the “earlygame” dataset’s exploratory nature, making it perform\nbetter on explore tasks. VPT(rl) is tailored specifically for diamond mining tasks and has thus lost\nthe capability of most tasks outside diamond mining path. No matter where you place it, the first\nthing VPT(rl) does is to look for trees and prepare to mine diamonds.\nSTEVE-1 is a Minecraft agent that can follow open-ended text and visual instructions built on\nMineCLIP (Fan et al., 2022) and VPT. It can perform a wide range of short-horizon tasks that can\nbe expressed by a 16-frame future video clip. The training of STEVE-1 can be described in two\nsteps. The first step is to train a future-video conditioned policy with packed hindsight relabeling\ntrick. With the frozen MineCLIP visual encoder to embed the visual instruction, they finetune the\nVPT(bc) on the contractor data to obtain STEVE-1(visual). The second step is to learn a model that\ntranslates textual instruction into visual instruction. By training a conditional variational autoencoder\n(CVAE) on the collected video-text pairs, they created a variant STEVE-1(text) that understands text\ninstructions. This baseline performs well on many simple tasks in the Minecraft SkillForge benchmark,\nsuch as \"explore run,\" \"collect grass,\" and \"collect wood.\" However, it struggles with multi-step and\nless common tasks, like \"build snow golems\" and \"dig three down and fill one up.\"\nPlease note that all baselines, including GROOT, were not fine-tuned for tasks in Minecraft SkillForge.\nF.2. t-SNE Visualization Details\nThis section details how the videos are sampled to do visualization. The selected videos are categorized\ninto seven groups: craft items, combat enemies, harvest crops, hunt animals, chop\ntrees, trade with villagers, and mine ores. Generally, each group contains two types of\nvideos, each with 1000 data points sampled. The sampling method retrieves the time when a certain\nevent occurs in the metadata and goes back 128 frames from that time to obtain a video segment\nthat is 128 frames long. We illustrate video configurations in Table 4. For example, in the combat\nenemies task, taking \"combat zombies\" as an example, we retrieve all the moments when the event\n\"pickup:rotten_flesh\" occurs, because after killing zombies, they will drop rotten flesh, which can\nthen be picked up by players. Through sampling observations, we found that this method can sample\nvideos that are consistent with the descriptions.\nF.3. Programmatic Evaluation Details\nIn this section, we elaborated on how each episode is regarded as successful. For the dye and\nshear sheep (\n) task, dyeing the sheep and shearing its wool must be successfully performed\nto be considered a success. For the use bow (\n) task, firing the arrow after charging it to the\nmaximum degree is required to be successful. For the sleep (\n) task, placing the bed and spending\nthe night on it are required to be successful. For the smelt (\n) task, placing the furnace and\ndragging coal and mutton into the designated slots are required to be successful. For the lead (\n)\ntask, successfully tethering at least one animal is considered a success. For the build obsidian\n23\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 4 | Sample videos from the contractor data (Baker et al., 2022) for the goal space visualization.\nGroup\nVideo Description\nEvent in Metadata\ncraft items\ncraft wodden_pickaxe with crafting_table\ncraft_item:wooden_pickaxe\ncraft items\ncraft iron_pickaxe with crafting_table\ncraft_item:iron_pickaxe\ncombat enemies\ncombat zombies\npickup:rotten_flesh\ncombat enemies\ncombat spiders\npickup:spider_eye\nharvest crops\nharvest wheat\nmine_block:wheat\nharvest crops\nharvest melon\nmine_block:melon\nhunt animals\nhunt sheep\npickup:mutton\nhunt animals\nhunt cow\npickup:beef\nchop trees\nchop oak trees\nmine_block:oak_log\nchop trees\nchop birch trees\nmine_block:birch_log\ntrade with villagers\ntrade with villagers for emerald\ncraft_item:emerald\ntrade with villagers\ntrade with villagers for enchanted_book\ncraft_item:enchanted_book\nmine ores\nmine coal ores with pickaxe\nmine_block:coal_ore\nmine ores\nmine iron ores with pickaxe\nmine_block:iron_ore\n(\n) task, pouring a water bucket and a lava bucket to fuse them is required to be successful. For\nthe enchant (\n) task, placing the enchantment table, putting a diamond sword and lapis lazuli\ninto the slots, and clicking the enchanting option are required to be successful. For the dig down\nthree fill one up (\n) task, the agent must first vertically break three dirt blocks below and\nthen use one dirt block to seal the area above. For the build snow golems ( ) task, placing 2\nsnow blocks and 1 carved pumpkin head in order and triggering the creation of a snow golem are\nrequired to be successful.\nF.4. Combining Skills Experimental Details\nFirst, we introduce the experimental environment selected for our study. The agent is summoned on\nthe plains biome, holding a diamond pickaxe, and granted the night vision status to enable the agent\nto see the various ores underground. At the beginning of each episode, we set the agent’s condition to\ndig down. When the agent descends to a depth below 12 layers, the condition automatically switches\nto horizontal mining. Each round of episodes lasts for 12,000 frames, which is equivalent to 10\nminutes in the real world. For GROOT, both the reference videos of dig down and horizontal\nmining were recorded by a human player. For STEVE-1, we invited the same player to carefully\nrecord the prompt videos. It is worth noting that while we could easily prompt it to dig down, it was\ndifficult to keep it in the horizontal mining condition. This made STEVE-1 prone to falling into the\nbedrock layer and getting stuck. Finally, we did not observe STEVE-1 finding any diamonds in the\n25 experiments, which can be attributed to the inability of its goal space to encode details such as\nhorizontal mining.\nG. Rating System\nG.1. ELO Rating\nThe ELO rating system is widely adopted for evaluating the skill levels of multiple players in two-player\ngames, such as Chess and Go (Silver et al., 2016). In this section, we elaborate on how we introduce\nhuman evaluation and use the ELO Rating system to measure the relative performance of agents on\n24\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nFigure 10 | Example of the annotating system for human evaluation.\nthe Minecraft SkillForge benchmark.\nIn the ELO rating system, each agent’s skill level is represented by a numerical rating. We\nrepeatedly let agents play against each other in pairs. Specifically, in each game, we sample a task\nand two agents, denoted as Agent A and Agent B. Then, we randomly sample a trajectory for each\nagent corresponding to the designated task. The two trajectories are assigned to a human annotator,\nwho selects the most task-relevant one. We implement the annotating system with Label Studio\n(Tkachenko et al., 2020-2022), as shown in Figure 10. We consider the agent that produced this\ntrajectory to be the winner, let’s assume it is Agent A. After each round, we update the scores of Agent\nA and Agent B as follows\n𝑅𝐴←𝑅𝐴+ 𝐾·\n1\n1 + 10(𝑅𝐴−𝑅𝐵)\/400 ,\n𝑅𝐵←𝑅𝐵−𝐾·\n1\n1 + 10(𝑅𝐴−𝑅𝐵)\/400 ,\n(18)\nwhere K is the update factor and we set it to 8. After calculating the score of the agent, we use VPT\n(bc) as 1500 points and shift the scores of other agents accordingly. Based on the ELO ratings, we can\neasily measure the relative winning rate for each paired agent. The win rate of Agent A over Agent B\ncan be represented as\n1\n1+10(𝑅𝐵−𝑅𝐴)\/400 . For example, the win rate ratio between two agents with a score\ndifference of 100 scores is 64% : 36%. A score difference of 200 scores implicit 76% : 24%.\nG.2. TrueSkill Rating\nWe also report the comparison results using TrueSkill rating system, which is used by gamers to\nevaluate their skill level. It was developed by Microsoft Research and is currently used on Xbox LIVE\nfor matchmaking and ranking services. Different from ELO, it can also track the uncertainty of the\nrankings. This system utilizes the Bayesian inference algorithm to quantify a player’s true skill points.\nIn TrueSkill, rating is modeled as a Gaussian distribution which starts from N (25, 25\n3\n2), where 𝜇is an\naverage skill of player, and 𝜎is a confidence of the guessed rating. A real skill of player is between\n𝜇± 2𝜎with 95% confidence. After conducting 1500 updates, the TrueSkill scores converged as in\nTable 5. We found that the ranking order of the baseline methods is consistent with that obtained\nhttps:\/\/trueskill.org\/\n25\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nusing ELO rating: HUMAN ≻GROOT ≻STEVE-1(visual) ≻STEVE-1(text) ≻VPT(bc) ≻VPT(fd) ≻\nVPT(rl).\nTable 5 | TrueSkill rating comparison on the Minecraft SkillForge benchmark.\nBaseline\nHUMAN\nGROOT\nSTEVE-1(visual)\nSTEVE-1(text)\nVPT(bc)\nVPT(fd)\nVPT(rl)\n𝜇± 𝜎\n34.2 ± 1.0\n29.1 ± 0.9\n25.8 ± 0.8\n24.6 ± 0.8\n22.2 ± 0.8\n20.7 ± 0.8\n19.2 ± 0.9\nG.3. Human Participation\nWe recruited 15 students with varying degrees of Minecraft game experience, ranging from a few\nhours to several years, from the Minecraft project group to conduct the evaluation. They are all\nfamiliar with the basic operations of Minecraft. Each employee was asked to label 100 matches for\nELO Rating or TrueSkill Rating, for a total of 1500 matches. For each employee who is required\nto collect or assess gameplay videos, we ask them to first read the description of each task in the\nMinecraft SkillForge Benchmark completely, as well as the evaluation criteria for task completion\nquality, see Appendix H. Taking the task of building a snow golem as an example, the evaluation\ncriteria are as follows: Build a snow golem. ≻Place both kinds of blocks. ≻Place at least one kind\nof block. ≻Place no block. This enables employees to quantify video quality and ensures that all\nemployees evaluate task completion consistently. All these employees were explicitly informed that the\ncollected data would be used for AI research.\nH. Minecraft SkillForge Benchmark\nIn this section, we detail the benchmark titled \"Minecraft SkillForge\" which meticulously incorporates\na wide spectrum of tasks prevalent within Minecraft. Our aim is to ensure that every task provides a\nmeaningful evaluation of a specific skill that an AI agent might possess. We categorize these tasks into\nsix groups: collect, explore, craft, tool, survive, and build. In the following subsections,\nwe will provide a detailed introduction to each of them. The “Description” field provides a brief\ndescription of the task, the “Precondition” field outlines the initial settings of the testing environment\nfor the task, the “SkillAssessed” field indicates which aspect(s) of the agent’s ability are being assessed\nby the task, and the “Evaluation” field describes the quality evaluation metrics for task completion\n(based on which human players judge the quality of two rollout videos).\nH.1. Collect\nThe tasks categorized under the collect section of our benchmark are specifically designed to\nevaluate an AI agent’s capability in resource acquisition proficiency and spatial awareness. This means\nthe agent should not only be adept at identifying and gathering specific resources but also possess\nthe acumen to navigate through varied environments while being aware of its surroundings and the\navailable tools at its disposal.\nTask: collect dirt\nDescription: Collect dirt from the surface.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Basic terrain understanding and the ability to differentiate between surface-level\nblocks.\nEvaluation: Run away. < Look down. < Dig down. < Break the dirt on the surface.\nTask: collect grass\nDescription: Remove weeds on the surface.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Surface navigation and comprehension of vegetation blocks.\n26\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\ncollect dirt from the surface\nremove weeds on the surface\nchop trees to collect wood\ndive into the water and collect seagrass\ndye the sheep then shear the sheep\nFigure 11 | Examples of tasks in collect category.\nEvaluation: Run away. < Break the grass block. < Break a large field of grass blocks.\nTask: collect wood\nDescription: Cut down trees to collect wood.\nPrecondition: Spawn the player in the forest biome with an iron_axe in its hand.\nSkillAssessed: Recognition of tree structures, efficient utilization of tools, and block\nharvesting capability.\nEvaluation: Run away. < Approach trees. < Chop the tree and collect logs.\nTask: collect seagrass\nDescription: Dive into the water and collect seagrass.\nPrecondition: Spawn the player near the sea.\nSkillAssessed: Water navigation, diving mechanics understanding, and underwater block interaction.\nEvaluation: Walk on the land. < Swim on the water < Dive into the water. < Break seagrass blocks.\nTask: collect wool\nDescription: Dye and shear the sheep for wool.\nPrecondition: Spawn the player in the plains biome with a shear (mainhand) and a stack of blue_dye\n(offhand), 5 sheep near the player.\nSkillAssessed: Interaction with entities, tool and item application, and sequential action\nexecution.\nEvaluation: Ignore the sheep. < Dye the sheep. < Shear the sheep. < First dye then shear the sheep.\nPrompt 1 | The environment configuration and evaluation metric for collect series tasks.\nH.2. Explore\nThe tasks encompassed within the explore category of our benchmark are intricately devised to\nevaluate an AI agent’s navigation proficiency, understanding of diverse environments, and intrinsic\nmotivation for exploration. Through these tasks, we gauge an agent’s ability to actively traverse,\nunderstand, and interact with varied elements of the Minecraft world, and its propensity to unravel\nmysteries and challenges posed by the environment.\n27\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nrun and explore \ntravel on a wooden boat through water\nclimb the mountain\nopen a chest and acquire the treasure\nmine horizontally underground\nFigure 12 | Examples of tasks in explore category.\nTask: run and explore\nDescription: Run and explore.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Stamina utilization and distance-based exploration.\nEvaluation: Exploring as far as possible.\nTask: climb the mountain\nDescription: Climb the mountain.\nPrecondition: Spawn the player in the stone shore biome and near the mountain.\nSkillAssessed: Vertical navigation, terrain adaptation, and goal-oriented movement.\nEvaluation: Run away and ignore the mountain. < Approach the mountain. < Climbing the mountain. <\nClimb to the top of the mountain.\nTask: mine horizontally\nDescription: Mine horizontally underground.\nPrecondition: Spawn the player in a deep cave with an iron_pickaxe in the hand.\nSkillAssessed: Underground navigation, tool utilization, and spatial reasoning in confined spaces.\nEvaluation: Run away. < Break the stone. < Dig down. < Mine horizontally.\nTask: travel by boat\nDescription: Travel on a wooden boat through water.\nPrecondition: Spawn the player near the sea with a wooden boat in the hand.\nSkillAssessed: Aquatic travel, tool placement, and boat maneuverability.\nEvaluation: Did not place the boat. < Place the boat on the water. < Board the boat. < Row in the\nwater.\nTask: explore the treasure\nDescription: Rush into a villager’s home and open a chest and acquire the treasure.\nPrecondition: Spawn the player in front of a villager’s house.\nSkillAssessed: Interaction with structures, curiosity-driven exploration, and object acquisition.\nEvaluation: Ignore the house and run away. < Open the door. < Enter the house. < Open the chest. <\nAcquire the treasure.\nPrompt 2 | The environment configuration and evaluation metric for explore series tasks.\n28\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nH.3. Craft\nThe tasks under the craft category in our benchmark have been designed to shed light on an AI\nagent’s prowess in item utilization, the intricacies of Minecraft crafting mechanics, and the nuances\nof various game mechanic interactions. These tasks provide a detailed examination of an agent’s\ncapability to convert materials into functional items and harness the game’s various crafting and\nenhancement mechanics.\nopen inventory and craft a crafting table\nplace the crafting table and open it to craft ladders\nplace a furnace and use it to smelt food\nplace an enchanting table and enchant a diamond sword\nplace a stonecutter and use it to cut the stone\nFigure 13 | Examples of tasks in craft category.\nTask: craft the crafting_table\nDescription: Open inventory and craft a crafting table.\nPrecondition: Spawn the player in the plains biome with a stack of oak_planks in the inventory.\nSkillAssessed: Inventory management and basic crafting.\nEvaluation: Open the inventory. < Click on the recipe button. < Click on the crafting_table. <\nDrag the crafting_table into the inventory.\nTask: craft ladders\nDescription: Place the crafting table and open it to craft ladders.\nPrecondition: Spawn the player in the plains biome with a crafting_table in its main hand and a\nstack of oak_planks in the inventory.\nSkillAssessed: Advanced crafting using crafting stations and recipe navigation.\nEvaluation: Place the crafting_table on the surface. < Open the crafting_tabe. < Click on the\nrecipe book. < Click on the ladder. < Drag the ladder into the inventory.\nTask: enchant sword\nDescription: Place an enchanting table and use it to enchant a diamond sword.\nPrecondition: Spawn the player in the plains biome with an enchanting table in its main hand, 3\ndiamond swords, and 3 stacks of lapis_lazuli in the inventory.\nSkillAssessed: Tool enhancement using enchantment stations and decision-making in choosing\nenchantments.\nEvaluation: Place the enchanting_table on the surface. < Open the enchanting_table. < Place the\nlapis_lazuli or diamond sword. < Place the lapis_lazuli and diamond sword. < Choose any\nenchantment.\nTask: smelt food\nDescription: Place a furnace and use it to smelt food.\n29\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nPrecondition: Spawn the player in the plains biome with a furnace table in its main hand, 3 stacks\nof mutton, and 3 stacks of coal in the inventory.\nSkillAssessed: Food processing using a smelting furnace, raw material to product conversion, and\npatience in awaiting outcomes.\nEvaluation: Place the furnace on the surface. < Open the furnace. < Place raw meat or coal. <\nPlace both raw meat and coal. < Wait for the raw meat to be cooked. < Take out cooked meat.\nTask: cut stone\nDescription: Place a stonecutter and use it to cut stones.\nPrecondition: Spawn the player in the plains biome with a stonecutter in its main hand, 6 stacks\nof stones in the inventory.\nSkillAssessed: Tool enhancement using enchantment stations and decision-making in choosing\nenchantments.\nEvaluation: Place the stonecutter on the surface. < Open the stonecutter. < Place the stones. <\nSelect a target type of stone. < Drag stones to the inventory.\nPrompt 3 | The environment configuration and evaluation metric for craft series tasks.\nH.4. Tool\nThe tasks within the Tool category of our benchmark are designed to deeply investigate an AI agent’s\ncapabilities in tool utilization, precision in tool handling, and contextual application of various tools\nto carry out specific tasks. This category provides insights into the agent’s skill in wielding, using,\nand exploiting tools optimally within different Minecraft scenarios.\ndraw a bow and shoot\nset fires on the forest\nuse rein to tie up the animals\nplace the pumpkins and carve pumpkins with shears\nfly the trident on a rainy day\nFigure 14 | Examples of tasks in tool category.\nTask: use bow\nDescription: Draw a bow and shoot.\nPrecondition: Spawn the player in the plains biome with a bow in the mainhand and a stack of\narrows in the inventory.\nSkillAssessed: Precision, tool handling, and projectile mastery.\nEvaluation: Just run. < Draw the bow and shoot the arrow. < Hold the bow steady and charge up the\nshot before releasing the arrow.\nTask: set fires\n30\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nDescription: Set fires on the trees.\nPrecondition: Spawn the player in the forest biome with a flint_and_steel in its main hand.\nSkillAssessed: Environment manipulation and controlled chaos creation.\nEvaluation: Attack the tree. < Start a fire with the flint_and_steel. < Go wild with the fire.\nTask: lead animals\nDescription: Use rein to tie up the animals.\nPrecondition: Spawn the player in the plains biome with a stack of leads in its main hand. Spawn 5\nsheep and 5 cows near the player’s position.\nSkillAssessed: Entity interaction, tool application on moving entities, and livestock\nEvaluation: Ignore the animals and run away. < Use the rein to tie up animals.\nTask: carve pumpkins\nDescription: Place the pumpkins and carve pumpkins with shears.\nPrecondition: Spawn the player in the plains biome with a shear in its main hand and a stack of\npumpkins in the inventory.\nSkillAssessed: Block placement, block modification, and crafting interaction.\nEvaluation: Just run. < Place the pumpkin on the surface. < Use the shear to carve it. < Get a\ncarved pumpkin.\nTask: use trident\nDescription: Fly the trident on a rainy day.\nPrecondition: Spawn the player in the plains biome with a trident in the main hand, which is\nenchanted with riptide. The weather is rain.\nSkillAssessed: Weather-adaptive tool utilization, motion dynamics, and advanced weapon handling.\nEvaluation: Just run. < Use the trident to break the block. < Use the trident for quick movement.\n< Charge to throw the trident farther.\nPrompt 4 | The environment configuration and evaluation metric for tool series tasks.\nH.5. Survive\nThe tasks embedded within the survive category of our benchmark aim to analyze an AI agent’s\nability to ensure its own survival, adeptness in combat scenarios, and its capability to interact with\nthe environment in order to meet basic needs. Survival, being a core aspect of Minecraft gameplay,\nnecessitates an intricate balance of offensive, defensive, and sustenance-related actions. This category\nis structured to ensure a thorough evaluation of these skills.\nTask: hunt animals\nDescription: Hunt animals on the plains.\nPrecondition: Spawn the player in the plains biome with an iron sword in the main hand. Spawn 5\nsheep and 5 cows near the player’s position.\nSkillAssessed: Predator instincts, combat efficiency, and sustenance acquisition.\nEvaluation: Ignore animals and run away. < Hurt animals. < Kill animals.\nTask: combat enemies\nDescription: Fight the enemy spider.\nPrecondition: Spawn the player in the plains biome with a diamond sword in its main hand and a\nsuite of diamond equipment. Spawn 3 spiders in front of the player.\nSkillAssessed: Self-defense, offensive combat strategy, and equipment utilization.\nEvaluation: Ignore spiders and run away. < Hurt spiders. < Kill spiders.\nTask: use shield\nDescription: Use a shield to ward off zombies.\nPrecondition: Spawn the player in the plains biome with a shield in its main hand and a suite of\ndiamond equipment. Spawn 3 zombies in front of the player.\nSkillAssessed: Defensive tactics, tool application in combat, and strategic protection.\nEvaluation: Ignore zombies and run away. < Use the shield to protect itself.\nTask: plant wheats\nDescription: Use an iron_hoe to till the land and then plant wheat seeds.\nPrecondition: Spawn the player in the plains biome with an iron hoe in its main hand, and a stack\nof wheat seeds in the off hand.\nSkillAssessed: Land cultivation, planting proficiency, and sustainable resource creation.\nEvaluation: Just run away. < Till the land. < Plant the wheats.\nTask: sleep on the bed\nDescription: Place the bed on the surface and sleep.\nPrecondition: Spawn the player in the plains biome with a white bed in its main hand.\n31\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nhunt animals on the plains\nfight the enemy spider\nuse a shield to ward off zombies\nuse the hoe to till the land and plant wheat seeds\nplace the bed on the surface and sleep\nFigure 15 | Examples of tasks in survive category.\nSkillAssessed: Self-preservation, understanding of day-night cycle implications, and use of\nutilities for rest.\nEvaluation: Just run away. < Place the bed on the surface. < Sleep on the bed.\nPrompt 5 | The environment configuration and evaluation metric for survive series tasks.\nH.6. Build\nThe tasks within the build category of our benchmark are devised to evaluate an AI agent’s aptitude\nin structural reasoning, spatial organization, and its capability to interact with and manipulate\nthe environment to create specific structures or outcomes. Building is an integral component of\nMinecraft gameplay, requiring an intricate interplay of planning, creativity, and understanding of\nblock properties.\nTask: build pillar\nDescription: Build a pillar with dirt.\nPrecondition: Spawn the player in the plains biome with a stack of dirt in the main hand.\nSkillAssessed: Vertical construction and basic structure formation.\nEvaluation: Just run away. < Look down. < Jump and place the dirt. < Pile the dirt into a few\npillars. < Make a really high pillar.\nTask: dig three down and fill one up\nDescription: Dig three dirt blocks and fill the hole above.\nPrecondition: Spawn the player in the plains biome.\nSkillAssessed: Ground manipulation and depth perception.\nEvaluation: Just run away. < Look down. < Dig down three dirt blocks. < Raise the head. < Raise\nthe head and use dirt to fill the hole.\nTask: build gate\nDescription: Build an archway gate.\nPrecondition: Spawn the player in the plains biome with a stack of oak_planks in the main hand.\nSkillAssessed: Symmetry, planning, and aesthetic construction.\nEvaluation: Place no plank. < Build 1 pillar. < Build 2 pillars. < Build an archway gate.\n32\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nbuild a pillar with dirt\nbuild an archway gate\nmake obsidian by pouring water and lava buckets\nbuild snow golems by placing snow blocks and pumpkin\ndig three and fill one\nFigure 16 | Examples of tasks in build category.\nTask: build obsidian\nDescription: Make obsidian by pouring a water bucket and a lava bucket.\nPrecondition: Spawn the player in the plains biome with two water buckets and two lava buckets in\nthe Hotbar.\nSkillAssessed: Material transformation, understanding of in-game chemistry, and precise pouring.\nEvaluation: Just run away. < Pour water or lava. < Pour both liquids. < Pour into a mold to make\nobsidian.\nTask: build snow golems\nDescription: Build snow golems by placing two snow blocks and one carved pumpkin.\nPrecondition: Spawn the player in the plains biome with two stacks of snow blocks and two stacks\nof carved pumpkins in the Hotbar.\nSkillAssessed: Entity creation, sequential block placement, and combination of multiple materials.\nEvaluation: Place no block. < Place at least one kind of block. < Place both kinds of blocks. <\nBuild a snow golem.\nPrompt 6 | The environment configuration and evaluation metric for build series tasks.\n33\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nI. Text Conditioning\nText Encoder\nDecoder as Policy\n𝑠!\n𝑠\"\n𝑠#\n𝑎!\n𝑎$\n𝑎#\nDecoder as Policy\n𝑠!\n𝑠%\n𝑠\"\n𝑎!\n𝑎%\n𝑎\"\n𝑠%\n𝑠\"\nBehavior Cloning\nStates\nRollout Observations\n(a) Fine-tuning\n(b) Inference\n𝑠!\nEnvironment\n⋯\n⋯\n⋯\n⋯\n⋯\n“mine block oak_log”\nText Instruction\nText Encoder\n“mine block grass”\nText Instruction\nFigure 17 | Fine-tuning GROOT to understand text instructions. We replace the original video encoder\nwith a text encoder to embed text instructions. The text encoder is fine-tuned to align with the learned goal\nspace. Left: During the fine-tuning, we freeze the learned decoder to provide the supervisory signal to train the\ntext encoder through behavior cloning. Right: During the inference, we can feed forward the text instructions\nto drive the policy to interact with the environment.\nAlthough video instruction has strong expressiveness, it still requires preparing at least one\ngameplay video for a new task. For most common tasks, such as collecting wood or stones, using\nnatural language to specify a goal is a more natural approach. In this section, we explore the possibility\nof aligning the pre-trained goal space with other modal instructions, such as text instructions.\nAligning text instructions with visual instructions in goal space has been validated as feasible\nby Lifshitz et al. (2023). They train a conditional variational autoencoder to project text into video\nspace after collecting 10,000 diversified text-video pairs, similar to what unCLIP did. However, the\nsuccess of this alignment method depends on the pre-alignment of visual and text spaces through\nlarge-scale contrastive pre-training (Fan et al., 2022). During the training process of GROOT, we did\nnot leverage the MineCLIP visual encoder to encode videos, instead trained goal space from scratch.\nOn the one hand, this is because MineCLIP can only handle short videos (only 16 frames); on the\nother hand, it is to free our goal space from the expressiveness bounded by pre-trained VLM.\nAccording to the above discussion, we choose to replace the video encoder in the GROOT\narchitecture with a text encoder, BERT, and directly optimize it through behavior cloning, as shown in\nFigure 17. In order to keep the original goal space, we freeze the decoder and regard it as a gradient\ngenerator that extracts high-level behavioral semantics from the demonstrations. We utilize the meta\ninformation in the contractor data to generate text-demonstration pairs. For example, in the task\nof “collect wood”, we identify the moment 𝑡when event “mine_block:oak_log” is triggered in the\nvideo, and we capture the frames within the range of [𝑡−127, 𝑡] to form a video clip, with “mine\nblock oak log” assigned as its text, thus constructing a sample. Having been fine-tuned on these data,\nour model demonstrated some steerabilities in the text instruction space, as shown in Table 6. We\nfind that the agent fine-tuned on the text-demonstration dataset shows a basic understanding of text\ninstructions. Our method exhibits progress in tasks such as “mine grass”, “mine wood”, “mine stone”,\n“mine seagrass”, “pickup beef” and “mine dirt”. However, it falls short in successfully completing tasks\nsuch as “mine seagrass”. We speculate that this may be related to the distribution of the data, as there\nis much less data available for “mine seagrass” compared to the other tasks (about 300 trajectories).\nWe emphasize that this experiment is very preliminary. In this experiment, the steerability\nof the agent fine-tuned on text instructions is still weak and it is hard to solve practical tasks.\n34\nGROOT: Learning to Follow Instructions by Watching Gameplay Videos\nTable 6 | Text conditioning results on resource collection tasks. Each episode lasts 30 seconds (600 frames).\nStatistics are measured over 10 episodes. The term \"baseline\" refers to the model before being fine-tuned, while\n\"fine-tuned\" refers to the final model after fine-tuning.\nVariant\nmine grass ↑\nmine wood ↑\nmine stone ↑\nmine seagrass ↓\npickup beef ↑\nmine dirt ↑\nbaseline\n3.9\n0.4\n1.8\n1.3\n0.0\n0.0\nfine-tuned\n17.3 (4.4×)\n3.7 (9.3×)\n11.5 (6.4×)\n1.2 (92%)\n0.1\n1.3\nGiven the limited diversity of text instructions in the provided contractor data, we don’t anticipate\nthe model to possess any significant level of generalization with regard to language instructions. To\nfurther verify this point, one needs to collect more diverse and higher-quality text-demonstration\npairs data. Anyway, this experimental result still indicates the possibility of optimizing the upstream\ninstruction generator by leveraging the pre-trained decoder. This creates possibilities for developing\nmore interesting applications on GROOT. Additional discussions on text-conditioning are beyond the\nscope of this paper, and we will leave them for future work.\nJ. Potential Applications and Integration with Planner\nGROOT is specialized in short-horizon instruction-following tasks with its goal being a video clip\nwhile LLM has demonstrated the ability to plan for long-horizon tasks in an open-world environment.\nFor example, DEPS Wang et al. (2023b) utilizes a text-conditioned policy from Cai et al. (2023) to\naccomplish tasks such as mining diamonds from scratch. By integrating GROOT into the DEPS\nframework, it can act as a controller and assist with long-sequence tasks. However, while LLM can\noutput language as the current goal, GROOT requires a specified video clip as its goal. Therefore,\nwhen combining GROOT with DEPS, it is necessary to use the visual language model CLIP to select\nthe most suitable video clip based on the language goal produced by LLM.\nThe proposed approach involves preparing a pre-existing library of video clips 𝑉= {𝑣𝑖} that\ncontains various actions performed by the agent in Minecraft (e.g., “chopping trees” or “mine iron\nore”). When given a long-horizon instruction by LLM’s Planner, it is decomposed into a series of\nshort-horizon language tasks {𝑔𝑖}. During task execution, the CLIP model is utilized to calculate the\nsimilarity between each short-horizon clip 𝑣𝑖in the library 𝑉and task 𝑔𝑖, selecting the most similar\nvideo clip as GROOT’s interaction goal with the environment. Additionally, accessing a video library\nof Minecraft content is effortless due to the abundance of available video data on the internet.\nWhile GROOT mainly relies on videos for input goals, LLM uses both input and output language\nmodalities. These modalities can be aligned using a visual language model, allowing us to combine\nGROOT as a short-horizon control policy with an LLM-based Planner to complete long-sequence\ntasks.\n35\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GROOT: Learning to Follow Instructions by Watching Gameplay Videos.pdf"}
{"title":"MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge","authors":"Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, Anima Anandkumar","summary":"Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https:\/\/minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.","url":"http:\/\/arxiv.org\/abs\/2206.08853v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2206.08853v2","published":1655481185000,"comment":"Outstanding Paper Award at NeurIPS 2022. Project website:\n  https:\/\/minedojo.org","pdf_text":"MINEDOJO: Building Open-Ended\nEmbodied Agents with Internet-Scale Knowledge\nLinxi Fan1, Guanzhi Wang2∗, Yunfan Jiang3∗, Ajay Mandlekar1, Yuncong Yang4,\nHaoyi Zhu5, Andrew Tang4, De-An Huang1, Yuke Zhu1 6†, Anima Anandkumar1 2†\n1NVIDIA, 2Caltech, 3Stanford, 4Columbia, 5SJTU, 6UT Austin\n∗Equal contribution\n†Equal advising\nhttps:\/\/minedojo.org\nAbstract\nAutonomous agents have made great strides in specialist domains like Atari games\nand Go. However, they typically learn tabula rasa in isolated environments with\nlimited and manually conceived objectives, thus failing to generalize across a wide\nspectrum of tasks and capabilities. Inspired by how humans continually learn\nand adapt in the open world, we advocate a trinity of ingredients for building\ngeneralist agents: 1) an environment that supports a multitude of tasks and goals,\n2) a large-scale database of multimodal knowledge, and 3) a ﬂexible and scalable\nagent architecture. We introduce MINEDOJO, a new framework built on the\npopular Minecraft game that features a simulation suite with thousands of diverse\nopen-ended tasks and an internet-scale knowledge base with Minecraft videos,\ntutorials, wiki pages, and forum discussions. Using MINEDOJO’s data, we propose\na novel agent learning algorithm that leverages large pre-trained video-language\nmodels as a learned reward function. Our agent is able to solve a variety of open-\nended tasks speciﬁed in free-form language without any manually designed dense\nshaping reward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https:\/\/minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n1\nIntroduction\nDeveloping autonomous embodied agents that can attain human-level performance across a wide\nspectrum of tasks has been a long-standing goal for AI research. There has been impressive progress\ntowards this goal, most notably in games [80, 85, 126] and robotics [68, 99, 146, 134, 107]. These\nembodied agents are typically trained tabula rasa in isolated worlds with limited complexity and\ndiversity. Although highly performant, they are specialist models that do not generalize beyond a\nnarrow set of tasks. In contrast, humans inhabit an inﬁnitely rich reality, continuously learn from and\nadapt to a wide variety of open-ended tasks, and are able to leverage large amount of prior knowledge\nfrom their own experiences as well as others.\nWe argue that three main pillars are necessary for generalist embodied agents to emerge. First, the\nenvironment in which the agent acts needs to enable an unlimited variety of open-ended goals\n[116, 71, 120, 117]. Natural evolution is able to nurture an ever-expanding tree of diverse life forms\nthanks to the inﬁnitely varied ecological settings that the Earth supports [117, 129]. This process has\nnot stagnated for billions of years. In contrast, today’s agent training algorithms cease to make new\nprogress after convergence in narrow environments [80, 146]. Second, a large-scale database of\nprior knowledge is necessary to facilitate learning in open-ended settings. Just as humans frequently\nlearn from the internet, agents should also be able to harvest practical knowledge encoded in large\namounts of video demos [42, 77], multimedia tutorials [79], and forum discussions [127, 65, 54]. In a\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\narXiv:2206.08853v2  [cs.LG]  22 Nov 2022\nAt\nCraft Glass Bridge\nCombat Zombie\nFish Squid\nBuild Oak House\nMake Ice Igloo\nFarm Sugar Cane\nFind Ocean \nMonument\nTreasure Hunt\nin End City\nExplore \nDesert Temple\nOpen-ended Environments\nGeneralist Agent\nInternet-scale Knowledge Base\nWiki\nYouTube\nReddit\nFigure 1: MINEDOJO is a novel framework for developing open-ended, generally capable agents\nthat can learn and adapt continually to new goals. MINEDOJO features a benchmarking suite with\nthousands of diverse open-ended tasks speciﬁed in natural language prompts, and also provides an\ninternet-scale, multimodal knowledge base of YouTube videos, Wiki pages, and Reddit posts. The\ndatabase captures the collective experience and wisdom of millions of Minecraft gamers for an AI\nagent to learn from. Best viewed zoomed in.\ncomplex world, it would be extremely inefﬁcient for an agent to learn everything from scratch through\ntrial and error. Third, the agent’s architecture needs to be ﬂexible enough to pursue any task in open-\nended environments, and scalable enough to convert large-scale knowledge sources into actionable\ninsights [19, 96]. This motivates the design of an agent that has a uniﬁed observation\/action space,\nconditions on natural language task prompts, and adopts the Transformer pre-training paradigm [27,\n91, 15] to internalize knowledge effectively.\nIn light of these three pillars, we introduce MINEDOJO, a new framework to help the community\ndevelop open-ended, generally-capable agents. It is built on the popular Minecraft game, where a\nplayer explores a procedurally generated 3D world with diverse types of terrains to roam, materials\nto mine, tools to craft, structures to build, and wonders to discover. Unlike most other games\n[80, 85, 126], Minecraft deﬁnes no speciﬁc reward to maximize and no ﬁxed storyline to follow,\nmaking it well suited for developing open-ended environments for embodied AI research. We make\nthe following three major contributions:\n1. Simulation platform with thousands of diverse open-ended tasks.\nMINEDOJO provides\nconvenient APIs on top of Minecraft that standardize task speciﬁcation, world settings, and agent’s\nobservation\/action spaces. We introduce a benchmark suite that consists of thousands of natural\nlanguage-prompted tasks, making it two orders of magnitude larger than prior Minecraft benchmarks\nlike the MineRL Challenge [48, 62]. The suite includes long-horizon, open-ended tasks that cannot\nbe easily evaluated through automated procedures, such as “build an epic modern house with two\nﬂoors and a swimming pool”. Inspired by the Inception score [98] and FID score [55] that are\ncommonly used to assess AI-generated image quality, we introduce a novel agent evaluation protocol\nusing a large video-language model pre-trained on Minecraft YouTube videos. This complements\nhuman scoring [104] that is precise but more expensive. Our learned evaluation metric has good\nagreement with human judgment in a subset of the full task suite considered in the experiments.\n2. Internet-scale multimodal Minecraft knowledge base.\nMinecraft has more than 100 million\nactive players [131], who have collectively generated an enormous wealth of data. They record\ntutorial videos, stream live play sessions, compile recipes, and discuss tips and tricks on forums.\nMINEDOJO features a massive collection of 730K+ YouTube videos with time-aligned transcripts,\n6K+ free-form Wiki pages, and 340K+ Reddit posts with multimedia contents (Fig. 3). We hope that\nthis enormous knowledge base can help the agent acquire diverse skills, develop complex strategies,\ndiscover interesting objectives, and learn actionable representations automatically.\n2\nshear a sheep\ncombat zombie \npigman\nfind a nether \nportal\nput carpets on \nthe floor\nFigure 2: Visualization of our agent’s learned behaviors on four selected tasks. Leftmost texts are the\ntask prompts used in training. Best viewed on a color display.\n3. Novel algorithm for embodied agents with large-scale pre-training.\nWe develop a new\nlearning algorithm for embodied agents that makes use of the internet-scale domain knowledge we\nhave collected from the web. Using the massive volume of YouTube videos from MINEDOJO, we\ntrain a video-text contrastive model in the spirit of CLIP [92], which associates natural language\nsubtitles with their time-aligned video segments. We demonstrate that this learned correlation score\ncan be used effectively as an open-vocabulary, massively multi-task reward function for RL training.\nOur agent solves the majority of 12 tasks in our experiment using the learned reward model (Fig. 2).\nIt achieves competitive performance to agents trained with meticulously engineered dense-shaping\nrewards, and in some cases outperforms them, with up to 73% improvement in success rates. For\nopen-ended tasks that do not have a simple success criterion, our agents also perform well without\nany special modiﬁcations.\nIn summary, this paper proposes an open-ended task suite, internet-scale domain knowledge, and agent\nlearning with recent advances on large pre-trained models [13]. We have open-sourced MINEDOJO’s\nsimulator, knowledge bases, algorithm implementations, pretrained model checkpoints, and task\ncuration tools at https:\/\/minedojo.org\/. We hope that MINEDOJO will serve as an effective\nstarter framework for the community to develop new algorithms and advance towards generally\ncapable embodied agents.\n2\nMINEDOJO Simulator & Benchmark Suite\nMINEDOJO offers a set of simulator APIs help researchers develop generally capable, open-ended\nagents in Minecraft. It builds upon the open-source MineRL codebase [48] and makes the following\nupgrades: 1) We provide uniﬁed observation and action spaces across all tasks, facilitating the\ndevelopment of multi-task and continually learning agents that can constantly adapt to new scenarios\nand novel tasks. This deviates from the MineRL Challenge design that tailors observation and action\nspaces to individual tasks; 2) Our simulation unlocks all three types of worlds in Minecraft, including\nthe Overworld, the Nether, and the End, which substantially expands the possible task space,\nwhile MineRL only supports the Overworld natively; and 3) We provide convenient APIs to conﬁgure\ninitial conditions and world settings to standardize our tasks.\nWith this MINEDOJO simulator, we deﬁne thousands of benchmarking tasks, which are divided into\ntwo categories: 1) Programmatic tasks that can be automatically assessed based on the ground-truth\nsimulator states; and 2) Creative tasks that do not have well-deﬁned or easily-automated success\ncriteria, which motivates our novel evaluation protocol using a learned model (Sec. 4). To scale up\nthe number of Creative tasks, we mine ideas from YouTube tutorials and use OpenAI’s GPT-3 [15]\n3\nservice to generate substantially more task deﬁnitions. Compared to Creative tasks, Programmatic\ntasks are simpler to get started, but tend to have restricted scope, limited language variations, and less\nopen-endedness in general.\n2.1\nTask Suite I: Programmatic Tasks\nWe formalize each programmatic task as a 5-tuple: T = (G, G, I, fS, fR). G is an English\ndescription of the task goal, such as “ﬁnd material and craft a gold pickaxe”. G is a natural\nlanguage guidance that provides helpful hints, recipes, or advice to the agent.\nWe leverage\nOpenAI’s GPT-3-davinci API to automatically generate detailed guidance for a subset of\nthe tasks.\nFor the example goal “bring a pig into Nether”, GPT-3 returns: 1) Find a pig\nin the overworld; 2) Right-click on the pig with a lead; 3) Right-click on\nthe Nether Portal with the lead and pig selected; 4) The pig will be pulled\nthrough the portal! I is the initial conditions of the agent and the world, such as the initial\ninventory, spawn terrain, and weather. fS: st →{0, 1} is the success criterion, a deterministic\nfunction that maps the current world state st to a Boolean success label. fR: st →R is an optional\ndense reward function. We only provide fR for a small subset of the tasks in MINEDOJO due\nto the high costs of meticulously crafting dense rewards. For our current agent implementation\n(Sec. 4.1), we do not use detailed guidance. Inspired by concurrent works SayCan [3] and Socratic\nModels [143], one potential idea is to feed each step in the guidance to our learned reward model\nsequentially so that it becomes a stagewise reward function for a complex multi-stage task.\nMINEDOJO provides 4 categories of programmatic tasks with 1,581 template-generated natural\nlanguage goals to evaluate the agent’s different capabilities systematically and comprehensively:\n1. Survival: surviving for a designated number of days.\n2. Harvest: ﬁnding, obtaining, cultivating, or manufacturing hundreds of materials and objects.\n3. Tech Tree: crafting and using a hierarchy of tools.\n4. Combat: ﬁghting various monsters and creatures that require fast reﬂex and martial skills.\nEach task template has a number of variations based on the terrain, initial inventory, quantity, etc.,\nwhich form a ﬂexible spectrum of difﬁculty. In comparison, the NeurIPS MineRL Diamond challenge\n[48] is a subset of our programmatic task suite, deﬁned by the task goal “obtain 1 diamond\" in\nMINEDOJO.\n2.2\nTask Suite II: Creative Tasks\nWe deﬁne each creative task as a 3-tuple, T = (G, G, I), which differs from programmatic tasks due\nto the lack of straightforward success criteria. Inspired by model-based metrics like the Inception\nscore [98] and FID score [55] for image generation, we design a novel task evaluation metric based\non a pre-trained contrastive video-language model (Sec. 4.1). In the experiments, we ﬁnd that the\nlearned metric exhibits a high level of agreement with human evaluations (see Table 2).\nWe brainstorm and author 216 Creative tasks, such as “build a haunted house with zombie inside” and\n“race by riding a pig”. Nonetheless, such a manual approach is not scalable. Therefore, we develop\ntwo systematic approaches to extend the total number of task deﬁnitions to 1,560. This makes our\nCreative tasks 3 orders of magnitude larger than Minecraft BASALT challenge [104], which has 4\nCreative tasks.\nApproach 1. Task Mining from YouTube Tutorial Videos.\nWe identify our YouTube dataset\nas a rich source of tasks, as many human players demonstrate and narrate creative missions in\nthe tutorial playlists. To collect high-quality tasks and accompanying videos, we design a 3-stage\npipeline that makes it easy to ﬁnd and annotate interesting tasks (see Sec. C.2 for details). Through\nthis pipeline, we extract 1,042 task ideas from the common wisdom of a huge number of veteran\nMinecraft gamers, such as “make an automated mining machine” and “grow cactus up to the sky”.\nApproach 2. Task Creation by GPT-3.\nWe leverage GPT-3’s few-shot capability to generate new\ntask ideas by seeding it with the tasks we manually author or mine from YouTube. The prompt tem-\nplate is: Here are some example creative tasks in Minecraft:\n{a few examples}.\n4\nYouTube\nWiki\nReddit\nFigure 3: MINEDOJO’s internet-scale, multimodal knowledge base.\nLeft, YouTube videos:\nMinecraft gamers showcase the impressive feats they are able to achieve. Clockwise order: an\narchery range, Hogwarts castle, Taj Mahal, a Nether homebase. Middle, Wiki: Wiki pages contain\nmultimodal knowledge in structured layouts, such as comprehensive catalogs of creatures and recipes\nfor crafting. More examples in Fig. A.4 and A.5. Right, Reddit: We create a word cloud from\nReddit posts and comment threads. Gamers ask questions, share achievements, and discuss strategies\nextensively. Sample posts in Fig. A.7. Best viewed zoomed in.\nLet’s brainstorm more detailed while reasonable creative tasks in Minecraft.\nGPT-3 contributes 302 creative tasks after de-duplication, and demonstrates a surprisingly proﬁcient\nunderstanding of Minecraft terminology.\n2.3\nCollection of Starter Tasks\nWe curate a set of 64 core tasks for future researchers to get started more easily. If their agent works\nwell on these tasks, they can more conﬁdently scale to the full benchmark.\n• 32 programmatic tasks: 16 “standard” and 16 “difﬁcult”, spanning all 4 categories (survival,\nharvesting, combat, and tech tree). We rely on our Minecraft knowledge to decide the difﬁculty\nlevel. “Standard” tasks require fewer steps and lower resource dependencies to complete.\n• 32 creative tasks: 16 “standard” and 16 “difﬁcult”. Similarly, tasks labeled with “standard” are\ntypically short-horizon tasks.\nWe recommend that researchers run 100 evaluation episodes for each task and report the percentage\nsuccess rate. The programmatic tasks have ground-truth success, while the creative tasks need our\nnovel evaluation protocol (Sec. 5).\n3\nInternet-scale Knowledge Base\nTwo commonly used approaches [112, 126, 85, 36] to train embodied agents include training agents\nfrom scratch using RL with well-tuned reward functions for each task, or using a large amount of\nhuman-demonstrations to bootstrap agent learning. However, crafting well-tuned reward functions is\nchallenging or infeasible for our task suite (Sec. 2.2), and employing expert gamers to provide large\namounts of demonstration data would also be costly and infeasible [126].\nInstead, we turn to the open web as an ever-growing, virtually unlimited source of learning material\nfor embodied agents. The internet provides a vast amount of domain knowledge about Minecraft,\nwhich we harvest by extensive web scraping and ﬁltering. We collect 33 years worth of YouTube\nvideos, 6K+ Wiki pages, and millions of Reddit comment threads. Instead of hiring a handful of\nhuman demonstrators, we capture the collective wisdom of millions of Minecraft gamers around the\nworld. Furthermore, language is a key and pervasive component of our database that takes the form\nof YouTube transcripts, textual descriptions in Wiki, and Reddit discussions. Language facilitates\nopen-vocabulary understanding, provides grounding for image and video modalities, and unlocks the\npower of large language models [27, 109, 15] for embodied agents. To ensure socially responsible\nmodel development, we take special measures to ﬁlter out low-quality and toxic contents [13, 51]\nfrom our databases, detailed in the Appendix (Sec. D).\n5\n“Shear sheep to \nobtain wool”\nMineCLIP\nCorrelation = 0.95\nRGB\nVoxel\nGPS\nInventory\nObservation space\nMove\nAttack\nCam\nEquip\nAction space\nMineDojo Sim\nR\nTcuK9hpoR1KJs20sZlkSDJCGfoPblwo4tb\/cefmD4EFT0kcDjnXu69J0oZVdpxPqzC2vrG5lZxu7Szu7d\/UD48CpTIJCZtLJiQ3QgpwignbU01I91UEpREjHSiydXc79wTqajgt3qakjBI05jipE2UtBPx3QDMoVx676NfOgY3ue59erhri1hu\nc3oGs7C1TACq1B+b0\/FDhLCNeYIaV6rpPqMEdSU8zIrNTPFEkRnqAR6RnKUJUmC+2ncEzowxhLKT5XMOF+r0jR4lS0yQylQnSY\/Xbm4t\/eb1MxdhTnmacLxclCcMagFnJ8Oh1QSrNnUEIQlNbtCPEYSYW0CKpkQvi6F\/5Ogaru+7d14leblKo4\niOAGn4By4oA6a4Bq0QBtgcAcewBN4toT1aL1Yr8vSgrXqOQY\/YL19AvPQj2g=<\/latexit>φV\nStack the last 16 RGB frames\nφI\nTime\nφI\nφI\nφI\nAggregate\nVideo\nFeature\nPer-frame\nFeature\nFigure 4: Algorithm design. MINECLIP is a contrastive video-language model pre-trained on\nMINEDOJO’s massive Youtube database. It computes the correlation between an open-vocabulary\nlanguage goal string and a 16-frame video snippet. The correlation score can be used as a learned\ndense reward function to train a strong multi-task RL agent.\nYouTube Videos and Transcripts.\nMinecraft is among the most streamed games on YouTube [41].\nHuman players have demonstrated a stunning range of creative activities and sophisticated missions\nthat take hours to complete (examples in Fig. 3). We collect 730K+ narrated Minecraft videos, which\nadd up to 33 years of duration and 2.2B words in English transcripts. In comparison, HowTo100M [77]\nis a large-scale human instructional video dataset that includes 15 years of experience in total – about\nhalf of our volume. The time-aligned transcripts enable the agent to ground free-form natural lan-\nguage in video pixels and learn the semantics of diverse activities without laborious human labeling.\nWe operationalize this insight in our pre-trained video-language model (Sec. 4.1).\nMinecraft Wiki.\nThe Wiki pages cover almost every aspect of the game mechanics, and supply\na rich source of unstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step\ntutorials. We use Selenium [103] to scrape 6,735 pages that interleave text, images, tables, and\ndiagrams. The pages are highly unstructured and do not share any common schema, as the Wiki\nis meant for human consumption rather than AI training. To preserve the layout information, we\nadditionally save the screenshots of entire pages and extract 2.2M bounding boxes of the visual\nelements (visualization in Fig. A.4 and A.5). We do not use Wiki data in our current experiments.\nSince the Wiki contains detailed recipes for all crafted objects, they could be provided as input\nor training data for hierarchical planning methods and policy sketches [8]. Another promising\nfuture direction is to apply document understanding models such as LayoutLM [138, 137] and\nDocFormer [9] to learn actionable knowledge from these unstructured Wiki data.\nReddit.\nWe scrape 340K+ posts along with 6.6M comments under the “r\/Minecraft” subreddit.\nThese posts ask questions on how to solve certain tasks, showcase cool architectures and achievements\nin image\/video snippets, and discuss general tips and tricks for players of all expertise levels. We\ndo not use Reddit data for training in Sec. 5, but a potential idea is to ﬁnetune large language models\n[27, 91] on our Reddit corpus to generate instructions and execution plans that are better grounded\nin the Minecraft domain. Concurrent works [3, 56, 143] have explored similar ideas and showed\nexcellent results on robot learning, which is encouraging for more future research in MINEDOJO.\n4\nAgent Learning with Large-scale Pre-training\nOne of the grand challenges of embodied AI is to build a single agent that can complete a wide range\nof open-world tasks. The MINEDOJO framework aims to facilitate new techniques towards this goal\nby providing an open-ended task suite (Sec. 2) and large-scale internet knowledge base (Sec. 3).\nHere we take an initial step towards this goal by developing a proof of concept that demonstrates\nhow a single language-prompted agent can be trained in MINEDOJO to complete several complex\nMinecraft tasks. To this end, we propose a novel agent learning algorithm that takes advantage of the\nmassive YouTube data offered by MINEDOJO. We note that this is only one of the numerous possible\n6\nTable 1: Our novel MINECLIP reward model is able to achieve competitive performance with\nmanually written dense reward function for Programmatic tasks, and signiﬁcantly outperforms the\nCLIPOpenAI method across all Creative tasks. Entries represent percentage success rates averaged\nover 3 seeds, each tested for 200 episodes. Success conditions are precise in Programmatic tasks, but\nestimated by MineCLIP for Creative tasks.\nGroup\nTasks\nOurs (Attn)\nOurs (Avg)\nManual Reward\nSparse-only\nCLIPOpenAI\nMilk Cow\n64.5 ± 37.1\n6.5 ± 3.5\n62.8 ± 40.1\n0.0 ± 0.0\n0.0 ± 0.0\nHunt Cow\n83.5 ± 7.1\n0.0 ± 0.0\n48.3 ± 35.9\n0.3 ± 0.4\n0.0 ± 0.0\nShear Sheep\n12.1 ± 9.1\n0.6 ± 0.2\n52.3 ± 33.2\n0.0 ± 0.0\n0.0 ± 0.0\nHunt Sheep\n8.1 ± 4.1\n0.0 ± 0.0\n41.9 ± 33.0\n0.3 ± 0.4\n0.0 ± 0.0\nCombat Spider\n80.5 ± 13.0\n60.1 ± 42.5\n87.5 ± 4.6\n47.8 ± 33.8\n0.0 ± 0.0\nCombat Zombie\n47.3 ± 10.6\n72.3 ± 6.4\n49.8 ± 26.9\n8.8 ± 12.4\n0.0 ± 0.0\nCombat Pigman\n1.6 ± 2.3\n0.0 ± 0.0\n13.6 ± 9.8\n0.0 ± 0.0\n0.0 ± 0.0\nCombat Enderman\n0.0 ± 0.0\n0.0 ± 0.0\n0.3 ± 0.2\n0.0 ± 0.0\n0.0 ± 0.0\nFind Nether Portal\n37.4 ± 40.8\n89.8 ± 5.7\nN\/A\nN\/A\n26.3 ± 32.6\nFind Ocean\n33.4 ± 45.6\n54.3 ± 40.7\nN\/A\nN\/A\n9.9 ± 14.1\nDig Hole\n91.6 ± 5.9\n88.1 ± 13.3\nN\/A\nN\/A\n0.0 ± 0.0\nLay Carpet\n97.6 ± 1.9\n98.8 ± 1.0\nN\/A\nN\/A\n0.0 ± 0.0\nways to use MINEDOJO’s internet database — the Wiki and Reddit corpus also hold great potential\nto drive new algorithm discoveries for the community in future works.\nIn this paper, we consider a multi-task reinforcement learning (RL) setting, where an agent is tasked\nwith completing a collection of MINEDOJO tasks speciﬁed by language instructions (Sec. 2). Solving\nthese tasks often requires the agent to interact with the Minecraft world in a prolonged fashion.\nAgents developed in popular RL benchmarks [119, 146] often rely on meticulously crafted dense and\ntask-speciﬁc reward functions to guide random explorations. However, these rewards are hard or even\ninfeasible to deﬁne for our diverse and open-ended tasks in MINEDOJO. To address this challenge, our\nkey insight is to learn a dense, language-conditioned reward function from in-the-wild YouTube\nvideos and their transcripts. Therefore, we introduce MINECLIP, a contrastive video-language\nmodel that learns to correlate video snippets and natural language descriptions (Fig. 4). MINECLIP\nis multi-task by design, as it is trained on open-vocabulary and diverse English transcripts.\nDuring RL training, MINECLIP provides a high-quality reward signal without any domain adaptation\ntechniques, despite the domain gap between noisy YouTube videos and clean simulator-rendered\nframes. MINECLIP eliminates the need to manually engineer reward functions for each and every\nMINEDOJO task. For Creative tasks that lack a simple success criterion (Sec. 2.2), MINECLIP also\nserves the dual purpose of an automatic evaluation metric that agrees well with human judgement\non a subset of tasks we investigate (Sec. 4.2, Table 2). Because the learned reward model incurs\na non-trivial computational overhead, we introduce several techniques to signiﬁcantly improve RL\ntraining efﬁciency, making MINECLIP a practical module for open-ended agent learning in Minecraft\n(Sec. 4.2).\n4.1\nPre-Training MINECLIP on Large-scale Videos\nFormally, the learned reward function can be deﬁned as ΦR : (G, V ) →R that maps a language goal\nG and a video snippet V to a scalar reward. An ideal ΦR should return a high reward if the behavior\ndepicted in the video faithfully follows the language description, and a low reward otherwise. This\ncan be achieved by optimizing the InfoNCE objective [125, 52, 20], which learns to correlate positive\nvideo and text pairs [118, 6, 78, 4, 136].\nSimilar to the image-text CLIP model [92], MINECLIP is composed of a separate text encoder φG\nthat embeds a language goal and a video encoder φV that embeds a moving window of 16 consecutive\nframes with 160 × 256 resolution (Fig. 4). Our neural architecture has a similar design as CLIP4Clip\n[75], where φG reuses OpenAI CLIP’s pretrained text encoder, and φV is factorized into a frame-wise\nimage encoder φI and a temporal aggregator φa that summarizes the sequence of 16 image features\ninto a single video embedding. Unlike CLIP4Clip, we insert two extra layers of residual CLIP\nAdapter [38] after the aggregator φa to produce a better video feature, and ﬁnetune only the last two\nlayers of the pretrained φI and φG.\n7\nTable 2: MINECLIP agrees well with the ground-truth human judgment on the Creative tasks we\nconsider. Numbers are F1 scores between MINECLIP’s binary classiﬁcation of tasks success and\nhuman labels (scaled to the percentage for better readability).\nTasks\nFind Nether Portal\nFind Ocean\nDig Hole\nLay Carpet\nOurs (Attn)\n98.7\n100.0\n99.4\n97.4\nOurs (Avg)\n100.0\n100.0\n100.0\n98.4\nCLIPOpenAI\n48.7\n98.4\n80.6\n54.1\nFrom the MINEDOJO YouTube database, we follow the procedure in VideoCLIP [136] to sample\n640K pairs of 16-second video snippets and time-aligned English transcripts, after applying a keyword\nﬁlter. We train two MINECLIP variants with different types of aggregator φa: (1) MINECLIP[avg]\ndoes simple average pooling, which is fast but agnostic to the temporal ordering; (2) MINECLIP[attn]\nencodes the sequence by two transformer layers, which is relatively slower but captures more temporal\ninformation, and thus produces a better reward signal in general. Details of data preprocessing,\narchitecture, and hyperparameters are listed in the Appendix (Sec. E).\n4.2\nRL with MINECLIP Reward\nWe train a language-conditioned policy network that takes as input raw pixels and predicts discrete\ncontrol. The policy is trained with PPO [102] on the MINECLIP rewards. In each episode, the\nagent is prompted with a language goal and takes a sequence of actions to fulﬁll this goal. When\ncalculating the MINECLIP rewards, we concatenate the agent’s latest 16 egocentric RGB frames in a\ntemporal window to form a video snippet. MINECLIP handles all task prompts zero-shot without any\nfurther ﬁnetuning. In our experiments (Sec. 5), we show that MINECLIP provides effective dense\nrewards out of the box, despite the domain shift between in-the-wild YouTube frames and simulator\nframes. Besides regular video data augmentation, we do not employ any special domain adaptation\nmethods during pre-training. Our ﬁnding is consistent with CLIP’s strong zero-shot performances on\nrobustness benchmarks in object recognition [92].\nCompared to hard-coded reward functions in popular benchmarks [146, 119, 34], the MINECLIP\nmodel has 150M parameters and is thus much more expensive to query. We make several design\nchoices to greatly accelerate RL training with MINECLIP in the loop:\n1. The language goal G is ﬁxed for a speciﬁc task, so the text features φG can be precomputed\nto avoid invoking the text encoder repeatedly.\n2. Our agent’s RGB encoder reuses the pre-trained weights of φI from MINECLIP. We do\nnot ﬁnetune φI during RL training, which saves computation and endows the agent with good\nvisual representations from the beginning.\n3. MINECLIP’s video encoder φV is factorized into an image encoder φI and a light-weight\naggregator φa. This design choice enables efﬁcient image feature caching. Consider two\noverlapping video sequences of 8 frames, V[0:8] and V[1:9]. We can cache the image\nfeatures of the 7 overlapping frames V[1] to V[7] to maximize compute savings. If φV is\na monolithic model like S3D [135] in VideoCLIP [136], then the video features from every\nsliding window must be recomputed, which would incur a much higher cost per time step.\n4. We leverage Self-Imitation Learning [84] to store the trajectories with high MINECLIP\nreward values in a buffer, and alternate between PPO and self-imitation gradient steps. It\nfurther improves sample efﬁciency as shown in the Appendix (Fig. A.8).\n5\nExperiments\nWe evaluate our agent-learning approach (Section 4) on 8 Programmatic tasks and 4 Creative tasks\nfrom the MINEDOJO benchmarking suite. We select these 12 tasks due to the diversity of skills\nrequired to solve them (e.g., harvesting, combat, building, navigation) and domain-speciﬁc entities\n(e.g., animals, resources, monsters, terrains, and structures). We split the tasks into 3 groups\nand train one multi-task agent for each group: Animal-Zoo (4 Programmatic tasks on hunting or\n8\nTable 3: MINECLIP agents have stronger zero-shot visual generalization ability to unseen terrains,\nweathers, and lighting. Numbers outside parentheses are percentage success rates averaged over 3\nseeds (each tested for 200 episodes), while those inside parentheses are relative performance changes.\nTasks\nOurs (Attn), train\nOurs (Attn), unseen test\nCLIPOpenAI, train\nCLIPOpenAI, unseen test\nMilk Cow\n64.5 ± 37.1\n64.8 ± 31.3(+ 0.8%)\n90.0 ± 0.4\n29.2 ± 3.7 (−67.6%)\nHunt Cow\n83.5 ± 7.1\n55.9 ± 7.2 (−32.9%)\n72.7 ± 3.5\n16.7 ± 1.6 (−77.0%)\nCombat Spider\n80.5 ± 13.0\n62.1 ± 29.7(−22.9%)\n79.5 ± 2.5\n54.2 ± 9.6 (−31.8%)\nCombat Zombie\n47.3 ± 10.6\n39.9 ± 25.3(−15.4%)\n50.2 ± 7.5\n30.8 ± 14.4(−38.6%)\nharvesting resource from animals), Mob-Combat (Programmatic, ﬁght 4 types of hostile monsters),\nand Creative (4 tasks).\nIn the experiments, we empirically check the quality of MINECLIP against manually written reward\nfunctions, and quantify how different variants of our learned model affect the RL performance. Table 1\npresents our main results, and Fig. 2 visualizes our learned agent behavior in 4 of the considered tasks.\nPolicy networks of all methods share the same architecture and are trained by PPO + Self-Imitation\n(Sec. 4.2, training details in the Appendix, Sec. F). We compare the following methods:\n• Ours (Attn): our agent trained with the MINECLIP[attn] reward model. For Program-\nmatic tasks, we also add the ﬁnal success condition as a binary reward. For Creative tasks,\nMINECLIP is the only source of reward.\n• Ours (Avg): the average-pooling variant of our method.\n• Manual Reward: hand-engineered dense reward using ground-truth simulator states.\n• Sparse-only: the ﬁnal binary success as a single sparse reward. Note that neither sparse-only\nnor manual reward is available for Creative tasks.\n• CLIPOpenAI: pre-trained OpenAI CLIP model that has not been ﬁnetuned on any MINEDOJO\nvideos.\nMINECLIP is competitive with manual reward.\nFor Programmatic tasks (ﬁrst 8 rows), RL\nagents guided by MINECLIP achieve competitive performance as those trained by manual reward.\nIn three of the tasks, they even outperform the hand-engineered reward functions, which rely on privi-\nleged simulator states unavailable to MINECLIP. For a more statistically sound analysis, we conduct\nthe Paired Student’s t-test to compare the mean success rate of each task (pairing column 3 “Ours\n(Attn)” and column 5 “Manual Reward” in Table 1). The test yields p-value 0.3991 ≫0.05, which\nindicates that the difference between our method and manual reward is not considered statistically\nsigniﬁcant, and therefore they are comparable with each other. Because all tasks require nontrivial ex-\nploration, our approach also dominates the Sparse-only baseline. Note that the original OpenAI CLIP\nmodel fails to achieve any success. We hypothesize that the creatures in Minecraft look dramatically\ndifferent from their real-world counterparts, which causes CLIP to produce misleading signals worse\nthan no shaping reward at all. It implies the importance of ﬁnetuning on MINEDOJO’s YouTube data.\nMINECLIP provides automated evaluation.\nFor Creative tasks (last 4 rows), there are no\nprogrammatic success criteria available. We convert MINECLIP into a binary success classiﬁer\nby thresholding the reward value it outputs for an episode. To test the quality of MINECLIP as\nan automatic evaluation metric, we ask human judges to curate a dataset of 100 successful and\n100 failed trajectories for each task. We then run both MINECLIP variants and CLIPOpenAI on the\ndataset and report the binary F1 score of their judgement against human ground-truth in Table 2.\nThe results demonstrate that both MINECLIP[attn] and MINECLIP[avg] attain a very high degree of\nagreement with human evaluation results on this subset of the Creative task suite that we investigate.\nCLIPOpenAI baseline also achieves nontrivial agreement on Find Ocean and Dig Hole tasks, likely\nbecause real-world oceans and holes have similar texture. We use the attn variant as an automated\nsuccess criterion to score the 4 Creative task results in Table 1. Our proposed method consistently\nlearns better than CLIPOpenAI-guided agents. It shows that MINECLIP is an effective approach to\nsolving open-ended tasks when no straightforward reward signal is available. We provide further\nanalysis beyond these 4 tasks in the Appendix (Sec. G.4).\n9\nTable 4: We train a single multi-task agent for all 12 tasks. All numbers represent percentage success\nrates averaged over 3 seeds, each tested for 200 episodes.\nGroup\nTasks\nSingle Agent on All Tasks\nOriginal\nPerformance Change\nMilk Cow\n91.5 ± 0.7\n64.5 ± 37.1\n↑\nHunt Cow\n46.8 ± 3.7\n83.5 ± 7.1\n↓\nShear Sheep\n73.5 ± 0.8\n12.1 ± 9.1\n↑\nHunt Sheep\n27.0 ± 1.0\n8.1 ± 4.1\n↑\nCombat Spider\n72.1 ± 1.3\n80.5 ± 13.0\n↓\nCombat Zombie\n27.1 ± 2.7\n47.3 ± 10.6\n↓\nCombat Pigman\n6.5 ± 1.2\n1.6 ± 2.3\n↑\nCombat Enderman\n0.0 ± 0.0\n0.0 ± 0.0\n=\nFind Nether Portal\n99.1 ± 0.4\n37.4 ± 40.8\n↑\nFind Ocean\n95.1 ± 1.5\n33.4 ± 45.6\n↑\nDig Hole\n85.8 ± 1.2\n91.6 ± 5.9\n↓\nLay Carpet\n96.5 ± 0.8\n97.6 ± 1.9\n=\nTable 5: We test the open-vocabulary generalization ability to two unseen tasks. All numbers represent\npercentage success rates averaged over 3 seeds, each tested for 200 episodes.\nTasks\nOurs (zero-shot)\nOurs (after RL ﬁnetune)\nBaseline (RL from scratch)\nHunt Pig\n1.3 ± 0.6\n46.0 ± 15.3\n0.0 ± 0.0\nHarvest Spider String\n1.6 ± 1.3\n36.5 ± 16.9\n12.5 ± 12.7\nMINECLIP shows good zero-shot generalization to signiﬁcant visual distribution shift.\nWe\nevaluate the learned policy without ﬁnetuning on a combination of unseen weather, lighting\nconditions, and terrains — 27 scenarios in total. For the baseline, we train agents with the original\nCLIPOpenAI image encoder (not trained on our YouTube videos) by imitation learning. The robustness\nagainst visual shift can be quantitatively measured by the relative performance degradation on\nnovel test scenarios for each task. Table 3 shows that while all methods incur performance drops,\nagents with MINECLIP encoder is more robust to visual changes than the baseline across all\ntasks. Pre-training on diverse in-the-wild YouTube videos is important to boosting zero-shot visual\ngeneralization capability, a ﬁnding consistent with literature [92, 82].\nLearning a Single Agent for All 12 Tasks\nWe have also trained a single agent for all 12 tasks.\nTo reduce the computational burden without loss of generality, the agent is trained by self-imitating\nfrom successful trajectories generated from the self-imitation learning pipeline (Section F.3). We\nsummarize the result in Table 4. Similar to our main experiments, all numbers represent percentage\nsuccess rates averaged over 3 training seeds, each tested for 200 episodes. Compared to the original\nagents, the new 12-multitask agent sees a performance boost in 6 tasks, degradation in 4 tasks,\nand roughly the same success rates in 2 tasks. This result suggests that there are both positive and\nnegative task transfers happening. To improve the multi-task performance, more advanced algorithms\n[141, 133] can be employed to mitigate the negative transfer effects.\nWe also perform Paired Student’s t-test to statistically compare the performance of the 12-multitask\nagent and those separately trained on each task group. We obtain a p-value of 0.3720 ≫0.05, which\nsuggests that the difference between the two training settings is not statistically signiﬁcant.\nGeneralize to Novel Tasks\nTo test the ability to generalize to new open-vocabulary commands,\nwe evaluate the agent on two novel tasks: “harvest spider string” and “hunt pig”. Table 5 shows\nthat the agent struggles in the zero-shot setting because it has not interacted with pigs or spider\nstrings during training, and thus does not know how to interact with them effectively. However, the\nperformance improves substantially by ﬁnetuning with the MINECLIP reward. Here the baseline\nmethods are trained from scratch using RL with the MINECLIP encoders and reward. Therefore,\nthe only difference is whether the policy has been pre-trained on the 12 tasks or not. Given the\n10\nsame environment sampling budget (only around 5% of total samples), it signiﬁcantly outperforms\nbaselines. It suggests that the multitask agent has learned transferable knowledge on hunting and\nresource collection, which enables it to quickly adapt to novel tasks.\n6\nRelated work\nOpen-ended Environments for Decision-making Agents.\nThere are many environments\ndeveloped with the goal of open-ended agent learning. Prior works include maze-style worlds\n[121, 129, 61], purely text-based game [69], grid worlds [21, 16], browser\/GUI-based environments\n[108, 124], and indoor simulators for robotics [1, 107, 114, 34, 110, 99, 89]. Minecraft offers\nan exciting alternative for open-ended agent learning. It is a 3D visual world with procedurally\ngenerated landscapes and extremely ﬂexible game mechanics that support an enormous variety\nof activities. Prior methods in open-ended agent learning [30, 57, 130, 63, 26] do not make use of\nexternal knowledge, but our approach leverages internet-scale database to learn open-vocabulary\nreward models, thanks to Minecraft’s abundance of gameplay data online.\nMinecraft for AI Research.\nThe Malmo platform [60] is the ﬁrst comprehensive release of a\nGym-style agent API [14] for Minecraft. Based on Malmo, MineRL [48] provides a codebase and\nhuman play trajectories for the annual Diamond Challenge at NeurIPS [47, 49, 62]. MINEDOJO’s\nsimulator builds upon the pioneering work of MineRL, but greatly expands the API and benchmarking\ntask suite. Other Minecraft benchmarks exist with different focuses. For example, CraftAssist [44]\nand IGLU [66] study agents with interactive dialogues. BASALT [104] applies human evaluation to 4\nopen-ended tasks. EvoCraft [45] is designed for structure building, and Crafter [50] optimizes for fast\nexperimentation. Unlike prior works, MINEDOJO’s core mission is to facilitate the development of\ngenerally capable embodied agents using internet-scale knowledge. We include a feature comparison\ntable of different Minecraft platforms for AI research in Table A.1.\nInternet-scale Multimodal Knowledge Bases.\nBig dataset such as Common Crawl [24], the Pile\n[37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pre-\ntrained language models [27, 91, 15] and multimodal models [118, 6, 78, 145, 7, 4, 136]. While gener-\nally useful for learning representations, these datasets are not speciﬁcally targeted at embodied agents.\nTo provide agent-centric training data, RoboNet [25] collects video frames from 7 robot platforms,\nand Ego4D [43] recruits volunteers to record egocentric videos of household activities. In comparison,\nMINEDOJO’s knowledge base is constructed without human curation efforts, much larger in volume,\nmore diverse in data modalities, and comprehensively covers all aspects of the Minecraft environment.\nEmbodied Agents with Large-scale Pre-training.\nInspired by the success in NLP, embodied\nagent research [29, 11, 94, 23] has seen a surge in adoption of the large-scale pre-training paradigm.\nThe recent advances can be roughly divided into 4 categories.\n1) Novel agent architecture:\nDecision Transformer [19, 58, 144] applies the powerful self-attention models to sequential decision\nmaking. GATO [95] and Uniﬁed-IO [74] learn a single model to solve various decision-making\ntasks with different control interfaces. VIMA [59] uniﬁes a wide range of robot manipulation\ntasks with multimodal prompting. 2) Pre-training for better representations: R3M [82] trains a\ngeneral-purpose visual encoder for robot perception on Ego4D videos [43]. CLIPort [111] leverages\nthe pre-trained CLIP model [92] to enable free-form language instructions for robot manipulation.\n3) Pre-training for better policies: AlphaStar [126] achieves champion-level performance on\nStarCraft by imitating from numerous human demos. SayCan [3] leverages large language models\n(LMs) to ground value functions in the physical world. [72] and [96] directly reuse pre-trained\nLMs as policy backbone. VPT [10] is a concurrent work that learns an inverse dynamics model from\nhuman contractors to pseudo-label YouTube videos for behavior cloning. VPT is complementary to\nour approach, and can be ﬁnetuned to solve language-conditioned open-ended tasks with our learned\nreward model.\n4) Data-driven reward functions: Concept2Robot [105] and DVD [18] learn a\nbinary classiﬁer to score behaviors from in-the-wild videos [42]. LOReL [81] crowd-sources humans\nlabels to train language-conditioned reward function for ofﬂine RL. AVID [113] and XIRL [142]\nextract reward signals via cycle consistency. MINEDOJO’s task benchmark and internet knowledge\nbase are generally useful for developing new algorithms in all the above categories. In Sec. 4, we\nalso propose an open-vocabulary, multi-task reward model using MINEDOJO YouTube videos.\n11\n7\nConclusion\nIn this work, we introduce the MINEDOJO framework for developing generally capable embodied\nagents. MINEDOJO features a benchmarking suite of thousands of Programmatic and Creative tasks,\nand an internet-scale multimodal knowledge base of videos, wiki, and forum discussions. As an\nexample of the novel research possibilities enabled by MINEDOJO, we propose MINECLIP as an\neffective language-conditioned reward function trained with in-the-wild YouTube videos. MINECLIP\nachieves strong performance empirically and agrees well with human evaluation results, making it a\ngood automatic metric for Creative tasks. We look forward to seeing how MINEDOJO empowers the\ncommunity to make progress on the important challenge of open-ended agent learning.\n8\nAcknowledgement\nWe are extremely grateful to Anssi Kanervisto, Shikun Liu, Zhiding Yu, Chaowei Xiao, Weili Nie,\nJean Kossaiﬁ, Jonathan Raiman, Neel Kant, Saad Godil, Jaakko Haapasalo, Bryan Catanzaro, John\nSpitzer, Zhiyuan “Jerry” Lin, Yingqi Zheng, Chen Tessler, Dieter Fox, Oli Wright, Jeff Clune, Jack\nParker-Holder, and many other colleagues and friends for their helpful feedback and insightful\ndiscussions. We also thank the anonymous reviewers for offering us highly constructive advice\nand kind encouragement during the review and rebuttal period. NVIDIA provides the necessary\ncomputing resource and infrastructure for this project. Guanzhi Wang is supported by the Kortschak\nfellowship in Computing and Mathematical Sciences at Caltech.\nReferences\n[1] Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin,\nRachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia\nGuy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap,\nKory Mathewson, Soˇna Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant\nVarma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating\ninteractive intelligence. arXiv preprint arXiv: Arxiv-2012.05672, 2020.\n[2] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakr-\nishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video\nclassiﬁcation benchmark. arXiv preprint arXiv: Arxiv-1609.08675, 2016.\n[3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey,\nSally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei\nLee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao,\nKanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton\nTan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and\nMengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. arXiv\npreprint arXiv: Arxiv-2204.01691, 2022.\n[4] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and\nBoqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,\naudio and text. arXiv preprint arXiv: Arxiv-2104.11178, 2021.\n[5] Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun hsuan Sung, Brian Strope, and Ray Kurzweil.\nConversational contextual cues: The case of personalization and history for response ranking.\narXiv preprint arXiv: Arxiv-1606.00372, 2016.\n[6] Jean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Rama-\npuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-\nsupervised multimodal versatile networks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia\nHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https:\/\/proceedings.neurips.\ncc\/paper\/2020\/hash\/0060ef47b12160b9198302ebdb144dcf-Abstract.html.\n12\n[7] Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex M. Bronstein. Noise estimation using\ndensity estimation for self-supervised multimodal learning. In Thirty-Fifth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications\nof Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in\nArtiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 6644–6652. AAAI\nPress, 2021. URL https:\/\/ojs.aaai.org\/index.php\/AAAI\/article\/view\/16822.\n[8] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning\nwith policy sketches. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th\nInternational Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017, volume 70 of Proceedings of Machine Learning Research, pages 166–175.\nPMLR, 2017. URL http:\/\/proceedings.mlr.press\/v70\/andreas17a.html.\n[9] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha.\nDocformer: End-to-end transformer for document understanding. arXiv preprint arXiv: Arxiv-\n2106.11539, 2021.\n[10] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by\nwatching unlabeled online videos. arXiv preprint arXiv: Arxiv-2206.11795, 2022.\n[11] Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun,\nSergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, and Hao Su.\nRearrangement: A challenge for embodied ai. arXiv preprint arXiv: Arxiv-2011.01975, 2020.\n[12] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mot-\ntaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. Objectnav revisited: On evaluation\nof embodied agents navigating to objects. arXiv preprint arXiv: Arxiv-2006.13171, 2020.\n[13] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\nvon Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik\nBrynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen,\nKathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn,\nTrevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha,\nTatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti,\nGeoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,\nRohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec,\nIsabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning,\nSuvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak\nNarayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko,\nGiray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance,\nChristopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam,\nAndy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian\nTramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie,\nMichihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang,\nYuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv: Arxiv-2108.07258, 2021.\n[14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv: Arxiv-1606.01540, 2016.\n[15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information processing systems, 33:1877–\n1901, 2020.\n[16] Tianshi Cao, Jingkang Wang, Yining Zhang, and Sivabalan Manivasagam. Babyai++: Towards\ngrounded-language learning beyond memorization. arXiv preprint arXiv: Arxiv-2004.07200,\n2020.\n13\n[17] João Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and\nthe kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 4724–4733. IEEE Computer Society,\n2017. doi: 10.1109\/CVPR.2017.502. URL https:\/\/doi.org\/10.1109\/CVPR.2017.502.\n[18] Annie S. Chen, Suraj Nair, and Chelsea Finn. Learning generalizable robotic reward functions\nfrom in-the-wild human videos. In Dylan A. Shell, Marc Toussaint, and M. Ani Hsieh,\neditors, Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021. doi:\n10.15607\/RSS.2021.XVII.012. URL https:\/\/doi.org\/10.15607\/RSS.2021.XVII.012.\n[19] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin,\nPieter Abbeel, Aravind Srinivas, and Igor Mordatch.\nDecision transformer:\nRein-\nforcement learning via sequence modeling.\nIn Marc’Aurelio Ranzato, Alina Beygelz-\nimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances\nin Neural Information Processing Systems 34: Annual Conference on Neural Informa-\ntion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages\n15084–15097, 2021.\nURL https:\/\/proceedings.neurips.cc\/paper\/2021\/hash\/\n7f489f642a0ddb10272b5c31057f0663-Abstract.html.\n[20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton.\nA simple\nframework for contrastive learning of visual representations. In Proceedings of the 37th\nInternational Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\nvolume 119 of Proceedings of Machine Learning Research, pages 1597–1607. PMLR, 2020.\nURL http:\/\/proceedings.mlr.press\/v119\/chen20j.html.\n[21] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan\nSaharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample\nefﬁciency of grounded language learning. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\nURL https:\/\/openreview.net\/forum?id=rJeXCo0cYX.\n[22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju\nDuke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie\nPellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv: Arxiv-2204.02311, 2022.\n[23] Jack Collins, Shelvin Chand, Anthony Vanderkop, and David Howard. A review of physics\nsimulators for robotic applications. IEEE Access, 9:51416–51431, 2021.\n[24] Common Crawl. Common crawl. https:\/\/commoncrawl.org\/, 2012. Accessed: 2022-06-\n06.\n[25] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeck-\npeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot\nlearning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, 3rd Annual\nConference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019,\nProceedings, volume 100 of Proceedings of Machine Learning Research, pages 885–897.\nPMLR, 2019. URL http:\/\/proceedings.mlr.press\/v100\/dasari20a.html.\n[26] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre M. Bayen, Stuart Russell, An-\ndrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised\nenvironment design. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:\n14\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, 2020. URL https:\/\/proceedings.neurips.cc\/paper\/2020\/hash\/\n985e9a46e10005356bbaf194249f6856-Abstract.html.\n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv preprint arXiv: Arxiv-\n1810.04805, 2018.\n[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv: Arxiv-2010.11929, 2020.\n[29] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied\nAI: from simulators to research tasks. IEEE Trans. Emerg. Top. Comput. Intell., 6(2):230–244,\n2022. doi: 10.1109\/TETCI.2022.3141105. URL https:\/\/doi.org\/10.1109\/TETCI.2022.\n3141105.\n[30] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore:\na new approach for hard-exploration problems. arXiv preprint arXiv: Arxiv-1901.10995, 2019.\n[31] Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, and Charles L. Isbell. Imitating\nlatent policies from observation. arXiv preprint arXiv: Arxiv-1805.07914, 2018.\n[32] William Falcon and The PyTorch Lightning team.\nPyTorch Lightning.\nGithub, 3\n2019. doi: 10.5281\/zenodo.3828935. URL https:\/\/github.com\/PyTorchLightning\/\npytorch-lightning.\n[33] Linxi Fan*, Shyamal Buch*, Guanzhi Wang, Ryan Cao, Yuke Zhu, Juan Carlos Niebles, and\nLi Fei-Fei. Rubiksnet: Learnable 3d-shift for efﬁcient video action recognition. In Proceedings\nof the European Conference on Computer Vision (ECCV), 2020.\n[34] Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke Zhu, and Anima\nAnandkumar. Secant: Self-expert cloning for zero-shot generalization of visual policies. arXiv\npreprint arXiv: Arxiv-2106.09678, 2021.\n[35] Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for\ndeep data-driven reinforcement learning. arXiv preprint arXiv: Arxiv-2004.07219, 2020.\n[36] Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, and Peter Dürr. Super-\nhuman performance in gran turismo sport using deep reinforcement learning. IEEE Robotics\nAutom. Lett., 6(3):4257–4264, 2021. doi: 10.1109\/LRA.2021.3064284. URL https:\/\/doi.\norg\/10.1109\/LRA.2021.3064284.\n[37] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse\ntext for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n[38] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng\nLi, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv\npreprint arXiv: Arxiv-2110.04544, 2021.\n[39] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna M.\nWallach, Hal Daumé III, and Kate Crawford. Datasheets for datasets. Commun. ACM, 64(12):\n86–92, 2021. doi: 10.1145\/3458723. URL https:\/\/doi.org\/10.1145\/3458723.\n[40] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxi-\ncityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:\nArxiv-2009.11462, 2020.\n[41] Jordan\nGerblick.\nMinecraft,\nthe\nmost-watched\ngame\non\nyoutube,\npasses\n1\ntrillion\nviews,\nDec\n2021.\nURL\nhttps:\/\/www.gamesradar.com\/\nminecraft-the-most-watched-game-on-youtube-passes-1-trillion-views\/.\n15\n[42] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne\nWestphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag,\net al. The\" something something\" video database for learning and evaluating visual common\nsense. In Proceedings of the IEEE international conference on computer vision, pages 5842–\n5850, 2017.\n[43] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit\nGirdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar\nNagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma,\nMichael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv\nBatra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph\nFeichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez,\nJames Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik\nKottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya\nMangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will\nPrice, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey\nSoutherland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi,\nZiwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria\nFarinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul\nJoo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg,\nYoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei\nYan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. arXiv\npreprint arXiv: Arxiv-2110.07058, 2021.\n[44] Jonathan Gray, Kavya Srinet, Yacine Jernite, Haonan Yu, Zhuoyuan Chen, Demi Guo, Sid-\ndharth Goyal, C. Lawrence Zitnick, and Arthur Szlam. Craftassist: A framework for dialogue-\nenabled interactive agents. arXiv preprint arXiv: Arxiv-1907.08584, 2019.\n[45] Djordje Grbic, Rasmus Berg Palm, Elias Najarro, Claire Glanois, and Sebastian Risi. EvoCraft:\nA New Challenge for Open-Endedness, pages 325–340. Springer International Publishing,\n2021. doi: 10.1007\/978-3-030-72699-7_21. URL http:\/\/link.springer.com\/content\/\npdf\/10.1007\/978-3-030-72699-7_21.\n[46] Agrim Gupta, Linxi Fan, Surya Ganguli, and Li Fei-Fei. Metamorph: Learning universal\ncontrollers with transformers. In International Conference on Learning Representations, 2022.\nURL https:\/\/openreview.net\/forum?id=Opmqtk_GvYL.\n[47] William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie\nMilani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin,\nManuela Veloso, and Phillip Wang. The minerl 2019 competition on sample efﬁcient rein-\nforcement learning using human priors. arXiv preprint arXiv: Arxiv-1904.10079, 2019.\n[48] William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv: Arxiv-1907.13440, 2019.\n[49] William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno,\nCrissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov,\nJohn Schulman, Shinya Shiroshita, Nicholay Topin, Avinash Ummadisingu, and Oriol Vinyals.\nThe minerl 2020 competition on sample efﬁcient reinforcement learning using human priors.\narXiv preprint arXiv: Arxiv-2101.11071, 2021.\n[50] Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:\nArxiv-2109.06780, 2021.\n[51] Laura Hanu and Unitary team.\nDetoxify.\nGithub. https:\/\/github.com\/unitaryai\/\ndetoxify, 2020.\n[52] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast\nfor unsupervised visual representation learning. In 2020 IEEE\/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9726–\n9735. Computer Vision Foundation \/ IEEE, 2020. doi: 10.1109\/CVPR42600.2020.00975.\nURL https:\/\/doi.org\/10.1109\/CVPR42600.2020.00975.\n16\n[53] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\nautoencoders are scalable vision learners. arXiv preprint arXiv: Arxiv-2111.06377, 2021.\n[54] Matthew Henderson, Paweł Budzianowski, Iñigo Casanueva, Sam Coope, Daniela Gerz, Girish\nKumar, Nikola Mrkši´c, Georgios Spithourakis, Pei-Hao Su, Ivan Vuli´c, and Tsung-Hsien Wen.\nA repository of conversational datasets. arXiv preprint arXiv: Arxiv-1904.06472, 2019.\n[55] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\nwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pages 6626–6637, 2017. URL https:\/\/proceedings.neurips.\ncc\/paper\/2017\/hash\/8a1d694707eb0fefe65871369074926d-Abstract.html.\n[56] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas\nJackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue:\nEmbodied reasoning through planning with language models. arXiv preprint arXiv: Arxiv-\n2207.05608, 2022.\n[57] Joost Huizinga and Jeff Clune. Evolving multimodal robot behavior via many stepping stones\nwith the combinatorial multiobjective evolutionary algorithm. Evolutionary computation, 30\n(2):131–164, 2022.\n[58] Michael Janner, Qiyang Li, and Sergey Levine.\nOfﬂine reinforcement learning as\none big sequence modeling problem.\nIn Marc’Aurelio Ranzato, Alina Beygelz-\nimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Ad-\nvances in Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,\npages 1273–1286, 2021. URL https:\/\/proceedings.neurips.cc\/paper\/2021\/hash\/\n099fe6b0b444c23836c4a5d07346082b-Abstract.html.\n[59] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen,\nLi Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation\nwith multimodal prompts. arXiv preprint arXiv: Arxiv-2210.03094, 2022.\n[60] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for\nartiﬁcial intelligence experimentation. IJCAI, 2016. URL https:\/\/dl.acm.org\/doi\/10.\n5555\/3061053.3061259.\n[61] Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter\nHenry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization\nchallenge in vision, control, and planning. In Sarit Kraus, editor, Proceedings of the Twenty-\nEighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China,\nAugust 10-16, 2019, pages 2684–2691. ijcai.org, 2019. doi: 10.24963\/ijcai.2019\/373. URL\nhttps:\/\/doi.org\/10.24963\/ijcai.2019\/373.\n[62] Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin,\nJunyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue Huang,\nHaicheng Chen, Guangjun Zeng, Yue Lin, Vincent Micheli, Eloi Alonso, François Fleuret,\nAlexander Nikulin, Yury Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond\n2021 competition: Overview, results, and lessons learned. arXiv preprint arXiv: Arxiv-\n2202.10583, 2022.\n[63] Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton,\nRaul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, Oleg Klimov, and\nJeff Clune. Multi-task curriculum learning in a complex, visual, hard-exploration domain:\nMinecraft. arXiv preprint arXiv: Arxiv-2106.14876, 2021.\n[64] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\nnarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and\nAndrew Zisserman. The kinetics human action video dataset. arXiv preprint arXiv: Arxiv-\n1705.06950, 2017.\n17\n[65] Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. Abstractive summarization of red-\ndit posts with multi-level memory networks. In Jill Burstein, Christy Doran, and Thamar\nSolorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages\n2519–2531. Association for Computational Linguistics, 2019. doi: 10.18653\/v1\/n19-1260.\nURL https:\/\/doi.org\/10.18653\/v1\/n19-1260.\n[66] Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha Mohanty, Maartje ter Hoeve,\nMikhail Burtsev, Alexey Skrynnik, Artem Zholus, Aleksandr Panov, Kavya Srinet, Arthur\nSzlam, Yuxuan Sun, Katja Hofmann, Michel Galley, and Ahmed Awadallah. Neurips 2021\ncompetition iglu: Interactive grounded language understanding in a collaborative environment.\narXiv preprint arXiv: Arxiv-2110.06536, 2021.\n[67] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. arXiv preprint arXiv: Arxiv-2205.11916,\n2022.\n[68] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti,\nDaniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d\nenvironment for visual ai. arXiv preprint arXiv: Arxiv-1712.05474, 2017.\n[69] Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici,\nEdward Grefenstette, and Tim Rocktäschel.\nThe nethack learning environment.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances\nin Neural Information Processing Systems, volume 33, pages 7671–7684. Curran As-\nsociates, Inc., 2020.\nURL https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/\n569ff987c643b4bedf504efda8f786c2-Paper.pdf.\n[70] Label Studio. Label studio. https:\/\/labelstud.io\/, 2020. Accessed: 2022-06-06.\n[71] WB Langdon. Pfeiffer–a distributed open-ended evolutionary system. In AISB, volume 5,\npages 7–13. Citeseer, 2005.\n[72] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\nHuang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\nand Yuke Zhu. Pre-trained language models for interactive decision-making. arXiv preprint\narXiv: Arxiv-2202.01771, 2022.\n[73] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts.\nIn 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:\n\/\/openreview.net\/forum?id=Skq89Scxx.\n[74] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUniﬁed-io: A uniﬁed model for vision, language, and multi-modal tasks. arXiv preprint arXiv:\nArxiv-2206.08916, 2022.\n[75] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip:\nAn empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv: Arxiv-\n2104.08860, 2021.\n[76] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David\nGarcía, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao\nWu. Mixed precision training. In 6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.\nOpenReview.net, 2018. URL https:\/\/openreview.net\/forum?id=r1gs9JgRZ.\n[77] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic. Howto100m: Learning a text-video embedding by watching hundred million\nnarrated video clips. arXiv preprint arXiv: Arxiv-1906.03327, 2019.\n18\n[78] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zis-\nserman. End-to-end learning of visual representations from uncurated instructional videos. In\n2020 IEEE\/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle,\nWA, USA, June 13-19, 2020, pages 9876–9886. Computer Vision Foundation \/ IEEE, 2020. doi:\n10.1109\/CVPR42600.2020.00990. URL https:\/\/openaccess.thecvf.com\/content_\nCVPR_2020\/html\/Miech_End-to-End_Learning_of_Visual_Representations_\nFrom_Uncurated_Instructional_Videos_CVPR_2020_paper.html.\n[79] Minecraft Wiki. Minecraft wiki. hhttps:\/\/minecraft.fandom.com\/wiki\/Minecraft_\nWiki, 2016. Accessed: 2022-06-06.\n[80] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv: Arxiv-1312.5602, 2013.\n[81] Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, and Chelsea Finn. Learn-\ning language-conditioned robot behavior from ofﬂine data and crowd-sourced annotation. In\nAleksandra Faust, David Hsu, and Gerhard Neumann, editors, Conference on Robot Learning,\n8-11 November 2021, London, UK, volume 164 of Proceedings of Machine Learning Re-\nsearch, pages 1303–1315. PMLR, 2021. URL https:\/\/proceedings.mlr.press\/v164\/\nnair22a.html.\n[82] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A\nuniversal visual representation for robot manipulation. arXiv preprint arXiv: Arxiv-2203.12601,\n2022.\n[83] Evgenii Nikishin, Max Schwarzer, Pierluca D’Oro, Pierre-Luc Bacon, and Aaron Courville.\nThe primacy bias in deep reinforcement learning. arXiv preprint arXiv: Arxiv-2205.07802,\n2022.\n[84] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In\nInternational Conference on Machine Learning, pages 3878–3887. PMLR, 2018.\n[85] OpenAI, :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław\nD˛ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal\nJózefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P.\nd. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon\nSidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep\nreinforcement learning. arXiv preprint arXiv: Arxiv-1912.06680, 2019.\n[86] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKöpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\nhigh-performance deep learning library. arXiv preprint arXiv: Arxiv-1912.01703, 2019.\n[87] Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu Kuno, Andre\nKramer, Sam Devlin, Raluca D. Gaina, and Daniel Ionita. The multi-agent reinforcement\nlearning in malmÖ (marlÖ) competition. arXiv preprint arXiv: Arxiv-1901.08129, 2019.\n[88] PRAW: The Python Reddit API Wrapper. Praw: The python reddit api wrapper. https:\n\/\/github.com\/praw-dev\/praw, 2010. Accessed: 2022-06-06.\n[89] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Tor-\nralba. Virtualhome: Simulating household activities via programs. In 2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, pages 8494–8502. Computer Vision Foundation \/ IEEE Computer Society, 2018. doi:\n10.1109\/CVPR.2018.00886.\nURL http:\/\/openaccess.thecvf.com\/content_cvpr_\n2018\/html\/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html.\n[90] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. OpenAI, 2018.\n19\n[91] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[92] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748–8763. PMLR, 2021.\n[93] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv: Arxiv-2204.06125,\n2022.\n[94] Harish Ravichandar, Athanasios S Polydoros, Sonia Chernova, and Aude Billard. Recent\nadvances in robot learning from demonstration. Annual review of control, robotics, and\nautonomous systems, 3:297–330, 2020.\n[95] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. arXiv preprint\narXiv: Arxiv-2205.06175, 2022.\n[96] Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help ofﬂine reinforce-\nment learning? arXiv preprint arXiv: Arxiv-2201.12122, 2022.\n[97] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,\nTim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-\nto-image diffusion models with deep language understanding. arXiv preprint arXiv: Arxiv-\n2205.11487, 2022.\n[98] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,\nand Xi Chen.\nImproved techniques for training gans.\nIn Daniel D. Lee, Masashi\nSugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Ad-\nvances in Neural Information Processing Systems 29:\nAnnual Conference on Neu-\nral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,\npages 2226–2234, 2016. URL https:\/\/proceedings.neurips.cc\/paper\/2016\/hash\/\n8a3363abe792db2d8761d6403605aeb7-Abstract.html.\n[99] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana\nJain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra.\nHabitat: A platform for embodied ai research. In Proceedings of the IEEE\/CVF International\nConference on Computer Vision (ICCV), October 2019.\n[100] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-ﬁltered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[101] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation. In Yoshua Bengio\nand Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR\n2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL\nhttp:\/\/arxiv.org\/abs\/1506.02438.\n[102] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv: Arxiv-1707.06347, 2017.\n[103] Selenium WebDriver. Selenium webdriver. https:\/\/www.selenium.dev\/, 2011. Accessed:\n2022-06-06.\n[104] Rohin Shah, Cody Wild, Steven H. Wang, Neel Alex, Brandon Houghton, William Guss,\nSharada Mohanty, Anssi Kanervisto, Stephanie Milani, Nicholay Topin, Pieter Abbeel, Stuart\nRussell, and Anca Dragan. The minerl basalt competition on learning from human feedback.\narXiv preprint arXiv: Arxiv-2107.01969, 2021.\n20\n[105] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot:\nLearning manipulation concepts from instructions and human demonstrations. The Interna-\ntional Journal of Robotics Research, 40(12-14):1419–1434, 2021.\n[106] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv: Arxiv-2002.05202,\n2020.\n[107] Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia\nPérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi,\nKent Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: a simulation environ-\nment for interactive tasks in large realistic scenes. arXiv preprint arXiv: Arxiv-2012.02924,\n2020.\n[108] Tianlin Tim Shi, Andrej Karpathy, Linxi Jim Fan, Jonathan Hernandez, and Percy Liang.\nWorld of bits: an open-domain platform for web-based agents. ICML, 2017. URL https:\n\/\/dl.acm.org\/doi\/10.5555\/3305890.3306005.\n[109] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. arXiv preprint arXiv:1909.08053, 2019.\n[110] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mot-\ntaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded\ninstructions for everyday tasks. In 2020 IEEE\/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10737–10746.\nComputer Vision Foundation \/ IEEE, 2020. doi: 10.1109\/CVPR42600.2020.01075. URL\nhttps:\/\/openaccess.thecvf.com\/content_CVPR_2020\/html\/Shridhar_ALFRED_\nA_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_\nCVPR_2020_paper.html.\n[111] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for\nrobotic manipulation. arXiv preprint arXiv: Arxiv-2109.12098, 2021.\n[112] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap,\nKaren Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general\nreinforcement learning algorithm. arXiv preprint arXiv: Arxiv-1712.01815, 2017.\n[113] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine.\nAvid:\nLearning multi-stage tasks via pixel-level translation of human videos. arXiv preprint arXiv:\nArxiv-1912.04443, 2019.\n[114] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia,\nKent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, C. Karen Liu, Silvio Savarese,\nHyowon Gweon, Jiajun Wu, and Li Fei-Fei. BEHAVIOR: benchmark for everyday household\nactivities in virtual, interactive, and ecological environments. In Aleksandra Faust, David Hsu,\nand Gerhard Neumann, editors, Conference on Robot Learning, 8-11 November 2021, London,\nUK, volume 164 of Proceedings of Machine Learning Research, pages 477–490. PMLR, 2021.\nURL https:\/\/proceedings.mlr.press\/v164\/srivastava22a.html.\n[115] Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv\npreprint arXiv: Arxiv-1703.01703, 2017.\n[116] Russell K Standish. Open-ended artiﬁcial evolution. International Journal of Computational\nIntelligence and Applications, 3(02):167–175, 2003.\n[117] Kenneth O Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge\nyou’ve never heard of. O’Reilly Online,, 2017.\n[118] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video represen-\ntations using contrastive bidirectional transformer. arXiv preprint arXiv: Arxiv-1906.05743,\n2019.\n21\n[119] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David\nBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin\nRiedmiller. Deepmind control suite. arXiv preprint arXiv: Arxiv-1801.00690, 2018.\n[120] Tim Taylor, Mark Bedau, Alastair Channon, David Ackley, Wolfgang Banzhaf, Guillaume\nBeslon, Emily Dolson, Tom Froese, Simon Hickinbotham, Takashi Ikegami, et al. Open-ended\nevolution: Perspectives from the oee workshop in york. Artiﬁcial life, 22(3):408–423, 2016.\n[121] Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck,\nJakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat\nMcAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu,\nSteph Hughes-Fitt, Valentin Dalibard, and Wojciech Marian Czarnecki. Open-ended learning\nleads to generally capable agents. arXiv preprint arXiv: Arxiv-2107.12808, 2021.\n[122] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv\npreprint arXiv: Arxiv-1805.01954, 2018.\n[123] Faraz Torabi, Garrett Warnell, and Peter Stone. Recent advances in imitation learning from\nobservation. arXiv preprint arXiv: Arxiv-1905.13566, 2019.\n[124] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali\nAhmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement\nlearning platform for android. arXiv preprint arXiv: Arxiv-2105.13231, 2021.\n[125] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv: Arxiv-1807.03748, 2018.\n[126] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wo-\njciech M Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al.\nAlphastar: Mastering the real-time strategy game starcraft ii. DeepMind blog, 2, 2019.\n[127] Michael Vølske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit\nto learn automatic summarization. In Proceedings of the Workshop on New Frontiers in\nSummarization, pages 59–63, Copenhagen, Denmark, sep 2017. Association for Computational\nLinguistics. doi: 10.18653\/v1\/W17-4508. URL https:\/\/aclanthology.org\/W17-4508.\n[128] Phil Wang. x-transformers. Github, 2022. URL https:\/\/github.com\/lucidrains\/\nx-transformers.\n[129] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired open-ended trailblazer\n(poet): Endlessly generating increasingly complex and diverse learning environments and their\nsolutions. arXiv preprint arXiv: Arxiv-1901.01753, 2019.\n[130] Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeff Clune, and Kenneth O. Stanley.\nEnhanced poet: Open-ended reinforcement learning through unbounded invention of learning\nchallenges and their solutions. arXiv preprint arXiv: Arxiv-2003.08536, 2020.\n[131] Wikipedia contributors. Minecraft — Wikipedia, the free encyclopedia, 2022. URL https:\n\/\/en.wikipedia.org\/w\/index.php?title=Minecraft&oldid=1092238294. [Online;\naccessed 9-June-2022].\n[132] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transform-\ners: State-of-the-art natural language processing. arXiv preprint arXiv: Arxiv-1910.03771,\n2019.\n[133] Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information\ntransfer in multi-task learning. In 8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:\n\/\/openreview.net\/forum?id=SylzhkBtDB.\n22\n[134] Fei Xia, William B. Shen, Chengshu Li, Priya Kasimbeg, Micael Tchapmi, Alexander Toshev,\nLi Fei-Fei, Roberto Martín-Martín, and Silvio Savarese. Interactive gibson benchmark (igibson\n0.5): A benchmark for interactive navigation in cluttered environments. arXiv preprint arXiv:\nArxiv-1910.14442, 2019.\n[135] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spa-\ntiotemporal feature learning: Speed-accuracy trade-offs in video classiﬁcation. In Proceedings\nof the European conference on computer vision (ECCV), pages 305–321, 2018.\n[136] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze,\nLuke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for\nzero-shot video-text understanding. arXiv preprint arXiv: Arxiv-2109.14084, 2021.\n[137] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei\nFlorencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. Layoutlmv2: Multi-\nmodal pre-training for visually-rich document understanding. arXiv preprint arXiv: Arxiv-\n2012.14740, 2020.\n[138] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm:\nPre-training of text and layout for document image understanding. arXiv preprint arXiv:\nArxiv-1912.13318, 2019.\n[139] Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Learning semantic textual similarity from\nconversations. In Proceedings of The Third Workshop on Representation Learning for NLP,\npages 164–174, Melbourne, Australia, jul 2018. Association for Computational Linguistics.\ndoi: 10.18653\/v1\/W18-3022. URL https:\/\/aclanthology.org\/W18-3022.\n[140] YouTube Data API. Youtube data api. https:\/\/developers.google.com\/youtube\/v3\/,\n2012. Accessed: 2022-06-06.\n[141] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea\nFinn. Gradient surgery for multi-task learning. arXiv preprint arXiv: Arxiv-2001.06782, 2020.\n[142] Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and Debidatta\nDwibedi. Xirl: Cross-embodiment inverse reinforcement learning. arXiv preprint arXiv:\nArxiv-2106.03911, 2021.\n[143] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek\nPurohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.\nSocratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint\narXiv: Arxiv-2204.00598, 2022.\n[144] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. arXiv preprint\narXiv: Arxiv-2202.05607, 2022.\n[145] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. arXiv\npreprint arXiv: Arxiv-2011.07231, 2020.\n[146] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martín-Martín. robosuite: A mod-\nular simulation framework and benchmark for robot learning. arXiv preprint arXiv: Arxiv-\n2009.12293, 2020.\n23\nA\nMinecraft Framework Comparison\nTable A.1: Comparison table of different Minecraft platforms for AI research.\nEnvironment\nSimulator\nTask Suite\nKnowledge Base\nFeatures\nReal\nMinecraft\nNumber\nof Tasks\nLanguage-\ngrounded\nFeatures\nData Scale\nMINEDOJO\nUniﬁed observation and action\nspace;\nunlocks all three types of\nworld (the Overworld, the\nNether, and the End)\n✓\n3, 000+\n✓\nAutomatically scraped from\nthe Internet;\nmultimodal data (videos, im-\nages, texts, tables and dia-\ngrams)\n740K YouTube videos;\n7K Wiki pages;\n350K Reddit posts\nMineRL\nv0.4 [48]\nBuilt on top of Malmo;\nactively maintained\n✓\n11\nAnnotated state-action pairs of\nhuman demonstrations\n60M frames of recorded hu-\nman player data\nMineRL\nv1.0\n(VPT) [10]\nMouse and keyboard control\n✓\n5\nLabeled contractor data;\nunlabeled videos scraped from\nthe Internet\n2K hours of contractor data;\n270K\nhours\nof\nunlabeled\nvideos\nMarLÖ [87]\nCooperative and competitive\nmultiagent tasks;\nparameterizable environments\n✓\n14\nMalmo [60]\nFirst comprehensive release of\na Gym-style agent API for\nMinecraft\n✓\nN\/A\nCraftAssist [44]Bot assistant;\ndialogue interactions\n✓\nN\/A\n✓\nInteractive dialogues;\ncrowd-sourced house building\ndataset\n800K dialogue-action dictio-\nnary pairs;\n2.6K\nhouses\nwith\natomic\nbuilding actions\nIGLU [66]\nInteractive dialogues with hu-\nmans;\naimed at building structures\ndescribed by natural language\n157\n✓\nEvoCraft [45]\nAimed at generating creative\nartifacts;\nallows for direction manipula-\ntion of blocks\nN\/A\nCrafter [50]\n2D clone of Minecraft;\nfast experimentation\n22\nHuman experts dataset\n100 episodes\nB\nMINEDOJO Simulator\nWe design uniﬁed observation and action spaces across all tasks to facilitate the development of\nmulti-tasking and continually learning agents that can adapt to novel tasks and scenarios. The\ncodebase is open sourced at github.com\/MineDojo\/MineDojo.\nB.1\nObservation Space\nOur observation space contains multiple modalities. The agent perceives the world mainly through\nthe RGB screen. To provide the same information as human players receive, we also supplement\nthe agent with observations about its inventory, location, health, surrounding blocks, etc. The full\nobservation space is shown below. We refer readers to see our code documentation for technical\ndetails such as data type for each observable item.\nWe also support a LIDAR sensor that returns the groundtruth type of the blocks that the agent\nsees, however this is considered privileged information and does not go into the benchmarking\nspeciﬁcation. However, it is still useful for hand engineering the dense reward function, which we\nuse in our experiments (Sec. 5). Amounts and directions of LIDAR rays can be arbitrarily conﬁgured\nat the cost of a lower simulation throughput.\n24\nModality\nShape\nDescription\nRGB\n(3, H, W)\nEgo-centric RGB frames.\nEquipment\n(6,)\nNames, quantities, variants, and durabilities of equipped items.\nInventory\n(36,)\nNames, quantities, variants, and durabilities of inventory items.\nVoxel\n(3, 3, 3)\nNames, variants, and properties of 3 × 3 × 3 surrounding blocks.\nLife statistics\n(1,)\nAgent’s health, oxygen, food saturation, etc.\nGPS\n(3,)\nGPS location of the agent.\nCompass\n(2,)\nYaw and pitch of the agent.\nNearby tools\n(2,)\nIndicate if crafting table and furnace are nearby the agent.\nDamage source\n(1,)\nInformation about the damage on the agent.\nLidar\n(Num rays,)\nGround-truth lidar observation.\nB.2\nAction Space\nWe design a compound action space. At each step the agent chooses one movement action (forward,\nbackward, camera actions, etc.) and one optional functional action as listed in the table below. Some\nfunctional actions such as craft take one argument, while others like attack does not take any\nargument. This compound action space can be modelled in an autoregressive manner [126]. We refer\nreaders to our code documentation for example usages of our action space.\nName\nDescription\nArgument\nno_op\nDo nothing.\n∅\nuse\nUse the item held in the main hand.\n∅\ndrop\nDrop the item held in the main hand.\n∅\nattack\nAttack with barehand or tool held in the main hand.\n∅\ncraft\nExecute a crafting recipe to obtain new items.\nIndex of recipe\nequip\nEquip an inventory item.\nSlot index of the item\nplace\nPlace an inventory item on the ground.\nSlot index of the item\ndestroy\nDestroy an inventory item.\nSlot index of the item\nB.3\nCustomizing the Environment\nEnvironments in MINECLIP simulator can be easily and ﬂexibly customized. Through our simulator\nAPI, users can control terrain, weather, day-night condition (different lighting), the spawn rate and\nrange of speciﬁed entities and materials, etc. We support a wide range of terrains, such as desert,\njungle, taiga, and iced plain, and special in-game structures, such as ocean monument, desert temple,\nand End city. Please visit our website for video demonstrations.\nC\nMINEDOJO Task Suite\nIn this section, we explain how we collect the Programmatic (Sec. 2.1) and Creative tasks (Sec. 2.2).\nC.1\nProgrammatic Tasks\nProgrammatic tasks are constructed by ﬁlling manually written templates for four categories of tasks,\nnamely “Survival”, “Harvest”, “Tech Tree”, and “Combat”. The task speciﬁcations are included in\nour codebase. Please refer to Fig. A.1 for a few samples. We brieﬂy explain each task category:\nSurvival.\nThis task group tests the ability to stay alive in the game. It is nontrivial to survive in\nMinecraft, because the agent grows hungry as time passes and the health bar drops gradually. Hostile\nmobs like zombie and skeleton spawn at night, which are very dangerous if the agent does not have\nthe appropriate armor to protect itself or weapons to ﬁght back. We provide two tasks with different\nlevels of difﬁculty for Survival. One is to start from scratch without any assistance. The other is\nto start with initial weapons and food.\n25\n1 survival_sword_food :\n2\ncategory: survival\n3\nprompt: survive as long as possible\ngiven a sword and some food\n4\n5 harvest_wool_with_shears_and_sheep :\n6\ncategory: harvest\n7\nprompt: harvest\nwool from a sheep\nwith\nshears and a sheep\nnearby\n8\n9 techtree_from_barehand_to_wooden_sword :\n10\ncategory: tech -tree\n11\nprompt: find\nmaterial\nand craft a wooden\nsword\n12\n13 combat_zombie_pigman_nether_diamond_armors_diamond_sword_shield :\n14\ncategory: combat\n15\nprompt: combat a zombie\npigman in nether\nwith a diamond sword ,\n16\nshield , and a full\nsuite of diamond\narmors\nFigure A.1: Example speciﬁcations.\nHarvest.\nThis task group tests the agent’s ability to collect useful resources such as minerals (iron,\ndiamond, obsidian), food (beef, pumpkin, carrots, milk), and other useful items (wool, oak wood,\ncoal). We construct these tasks by enumerating the Cartesian product between target items to collect,\ninitial inventory, and world conditions (terrain, weather, lighting, etc.) so that they cover a spectrum\nof difﬁculty. For instance, if the task is to harvest wool, then it is relatively easy if the agent has\na shear in its initial inventory with a sheep nearby, but more difﬁcult if the agent has to craft the\nshear from raw material and explore extensively to ﬁnd a sheep. We ﬁlter out combinations that\nare impossible (such as farming certain plants in the desert) from the Cartesian product.\nTech Tree.\nMinecraft includes several levels of tools and armors with different properties and\ndifﬁculties to unlock. To progress to a higher level of tools and armors, the agent needs to develop\nsystematic and compositional skills to navigate the technology tree (e.g. wood →stone →iron →\ndiamond). In this task group, the agent is asked to craft and use a hierarchy of tools starting from\na less advanced level. For example, some task asks the agent to craft a wooden sword from bare\nhand. Another task may ask the agent to craft a gold helmet. An agent that can successfully complete\nthese tasks should have the ability to transfer similar exploration strategies to different tech levels.\nCombat.\nWe test agent’s reﬂex and martial skills to ﬁght against various monsters and creatures.\nSimilar to how we develop the Harvest task group, we generate these tasks by enumerating the\nCartesian product between the target entity to combat with, initial inventory, and world conditions\nto cover a spectrum of difﬁculty.\nC.2\nCreative Tasks\nWe construct Creative tasks using three approaches: 1) manual brainstorming, 2) mining from\nYouTube tutorial videos, and 3) generate by querying GPT-3 API. We elaborate the second and third\napproaches below.\nTask Mining from YouTube Tutorial Videos.\nOur YouTube dataset serves the dual purpose of\na rich task source, as many human players demonstrate and narrate creative missions in the tutorial\nplaylists. To collect high-quality tasks and accompanying videos, we design a 3-stage pipeline that\nmakes it easy to ﬁnd and annotate interesting tasks.\nStage 1: We search for YouTube playlists with the key phrases, “Minecraft Tutorial” and “Minecraft\nGuide”. Then we apply heuristic rules (see Sec. D.1) to ﬁlter out low-quality videos;\nStage 2: We only show the title of the video to a human annotator through a command-line interface,\nwho makes a binary decision to accept or reject it as a potential task. This step is typically\nvery fast, taking a few seconds on average;\n26\nFigure A.2: Labeling UI to mine tasks from YouTube. A human annotator can choose to reject the\nvideo (Invalid), adjust the timestamps, select the title, or edit and expand the original description to\nbe the new task goal.\nStage 3: For the accepted tasks in stage 2, we design a labeling UI using Label Studio [70] that\ndisplays the full video and YouTube description. A human annotator can choose to reject\nthe video, adjust the timestamps, select the title, or reﬁne the description to be the task goal\n(Fig. A.2). Through this pipeline, we extract 1,042 task ideas from the common wisdom\nof a huge number of veteran Minecraft gamers. Some examples are “make an automated\nmining machine” and “grow cactus up to the sky”.\nC.3\nGPT-3 Guidance\nWe leverage OpenAI’s GPT-3-davinci API to automatically generate detailed guidance for a subset\nof the tasks. Inspired by [67], we adopt the following template to prompt GPT-3: How to {task\ngoal} in Minecraft?\nLet’s think step by step. Here are some examples:\nThe guidance for the task “ﬁnd material and craft a gold pickaxe” is\n1) Find a place with\na lot of trees; 2) Cut down the trees and gather the wood; 3) Find a place\nwith a lot of stone; 4) Mine the stone and gather the cobblestone; 5) Find\na place with a lot of iron; 6) Mine the iron and gather the iron ingots;\n7) Find a place with a lot of gold; 8) Mine the gold and gather the gold\ningots; 9) Craft a gold pickaxe.\nThe guidance for the task “sail on boat with a sheep” is\n1) Find a boat; 2) Place\nthe sheep in the boat; 3) Right-click on the boat with an empty hand to get\nin; 4) Use the WASD keys to move the boat.\nThe sheep should stay in the\nboat.\nC.4\nPlaythrough: Defeat the Ender Dragon\nOur benchmarking suite includes a special task called “Playthrough”. The agent is initialized bare-\nhanded in a freshly created world and aims to defeat the Ender dragon, which is considered the ﬁnal\nboss of Minecraft. This task holds a unique position in our benchmark because killing the dragon\nmeans “beating the game” in the traditional sense of the phrase, and is considered the most signiﬁcant\nachievement for a new player. This boss is optional and plenty of people choose to skip it without\naffecting their open-ended game experience.\n27\n0\n5\n10\n15\n20\n25\n30\nvideo duration (minutes)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nratio of videos\nVideo Duration Histogram\nFigure A.3: Distribution of YouTube video duration. The histogram is trimmed by the 85th percentile\nto hide much longer videos that can run for many hours.\n“Playthrough” is technically a programmatic task, because we can check the simulator state for the\ndefeat of the Ender dragon. However, we decide to create its own category due to the uniqueness as\nwell as the sheer difﬁculty of the task. The mission requires lots of preparation, exploration, agility,\nand trial-and-error, which may take a regular human player many days to complete. It would be\nextremely long horizon (hundreds of thousands of steps) and difﬁcult for an agent to tackle. We\nconsider this one of the moonshot goals in MINEDOJO.\nD\nInternet-Scale Database\nWe upload our databases to zenodo.org, which is an open repository platform operated by CERN.\nThe data DOIs, URLs, and licenses are listed below. In this section, we describe our database\nproperties and data collection process in details.\nDatabase\nDOI\nLicense\nYouTube\n10.5281\/zenodo.6641142\nCreative Commons Attribution 4.0 International (CC BY 4.0)\nWiki\n10.5281\/zenodo.6640448\nCreative Commons Attribution Non Commercial Share Alike 3.0 Unported\nReddit\n10.5281\/zenodo.6641114\nCreative Commons Attribution 4.0 International (CC BY 4.0)\nD.1\nYouTube Videos and Transcripts\nMinecraft is among the most streamed games on YouTube [41]. Human players have demonstrated a\nstunning range of creative activities and sophisticated missions that take hours to complete. We collect\n33 years worth of video and 2.2B words in the accompanying English transcripts. The distribution of\nvideo duration is shown in Fig. A.3. The time-aligned transcripts enable the agent to ground free-form\nnatural language in video pixels and learn the semantics of diverse activities without laborious human\nlabeling.\nWe use the ofﬁcial YouTube Data API [140] to collect our database, following the procedure below:\na) Search for channels that contain Minecraft videos using a list of keywords, e.g., “Minecraft”,\n“Minecraft Guide”, “Minecraft Walkthrough”, “Minecraft Beginner”. We do not directly\nsearch for videos at this step because there is a limit of total results returned by the API;\nb) Search for all the video IDs uploaded by each channel that we obtain at the previous step.\nThere are many false positives at this step because some channels (like gaming news channel)\nmay cover a range of topics other than Minecraft;\nc) To remove the false positives, we rely on the video category chosen by the user when the\nvideo was uploaded and ﬁlter out all the videos that do not belong to the Minecraft category;\n28\nFigure A.4: Wiki dataset examples. Closewise order: Villager trade table, mineral ingredient\ndescriptions, monster gallery, and terrain explanation.\nd) To curate a language-grounded dataset, we favor videos that have English transcripts, which\ncan be manually uploaded by the user, automatically transcribed from audio, or automatically\ntranslated from another language by the YouTube engine. For each video, we ﬁlter it out if\n1) the view count is less than 100; or 2) the aspect ratio is less 1; or 3) the duration is less\nthan 1 minute long; or 4) marked as age-restricted.\ne) To further clean the dataset and remove potentially harmful contents, we employ the Detox-\nify [51] tool to process each video title and description. Detoxify is trained on Wikipedia\ncomments to predict multiple types of toxicity like threats, obscenity, insults, and identity-\nbased hate speech. We delete a video if the toxicity probability in any category is above\n0.5.\nWe release all the video IDs along with metadata such as video titles, view counts, like counts,\nduration, and FPS. In line with prior practice [64], we do not release the actual MP4 ﬁles and\ntranscripts due to legal concerns.\nD.2\nMinecraft Wiki\nThe Wiki pages cover almost every aspect of the game mechanics, and supply a rich source of un-\nstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step tutorials (example\nscreenshots in Fig. A.4 and Fig. A.5). We use Selenium [103] to scrape 6,735 pages that interleave\ntext, images, tables, and diagrams. We elaborate the details of each web element scraped by Selenium:\na) Screenshot. Using Selenium’s built-in function, we take a full screenshot of the rendered\nWiki page in order to preserve the human-readable visual formatting. We also record the\nbounding boxes of each salient web element on the page.\nb) Text. We hand-select several HTML tags that likely contain meaningful text data, such as p,\nh1, h2, ul, dl.\nc) Images and Animations. We download the raw source ﬁle of each image element (JPG,\nPNG, GIF, etc.), as well as the corresponding caption if available. There are also animation\neffects enabled by JavaScript on the Wiki. We save all image frames in the animation.\nd) Sprites. Sprite elements are micro-sized image icons that are typically embedded in text\nto create multimodal tutorials and explanations. We save all the sprites and locate their\nbounding boxes within the text too.\n29\nFigure A.5: More Wiki database examples with bounding boxes (annotated in red). Left: wood block\nintroduction; right: ﬁrst day tutorial.\nimage\n65.7%\nvideo\n15.8%\ntext\n14.7%\nlink\n3.81%\nimage\nvideo\ntext\nlink\nFigure A.6: Distribution of Reddit post types.\ne) Tables. We save the text content and bounding box of each cell that a table element contains.\nWe store the header cells separately as they carry the semantic meaning of each column. A\ntable can be easily reconstructed with the stored text strings and bounding boxes.\nD.3\nReddit\nThere are more than 1M subreddits (i.e., Reddit topics) where people can discuss a wide range of\nthemes and subjects. Prior works use Reddit data for conversational response selection [5, 139, 54]\nand abstractive summarization [127, 65]. The r\/Minecraft subreddit contains free-form discussions\nof game strategies and images\/videos showcases of Minecraft builds and creations (examples in\nFig. A.7). The distribution of post types is shown in Fig. A.6.\nTo scrape the Reddit contents, we use PRAW [88], a Python wrapper on top of the ofﬁcial Reddit\nAPI. Our procedure is as follows:\na) Obtain the ID and metadata (e.g. post title, number of comments, content, score) of every\npost in the “r\/Minecraft” subreddit since it was created. For quality control, we only consider\nposts with scores (upvotes) ≥5 and not marked as NSFW.\nb) Determine each post’s type. There are 4 native post types - text, image\/video, link, and\npoll. We group text and poll posts together as text posts, and store their body text. For\nimage\/video and link posts, we store the source ﬁle URLs on external media hosting sites\nlike Imgur and Gfycat. Based on the URL of each link post, we classify it as an image post,\na video post or a general link post.\n30\nFigure A.7: Examples of posts and comment threads from the Reddit database.\nc) Scrape the comments and store the parent ID of each comment so that we can reconstruct\nthe threaded discussion.\nd) Similarly to our YouTube database (Sec. D.1), we run Detoxify [51] on the scraped Reddit\ncontents to ﬁlter out potentially toxic and harmful posts.\nWe release all post IDs and their corresponding metadata. We also provide a Python function based\non PRAW for researchers to download the post contents after obtaining a license key for the ofﬁcial\nReddit API.\nE\nMINECLIP Algorithm Details\nWe implement all our neural networks in PyTorch v1.11 [86]. Training MINECLIP uses the\nPyTorch-Lightning framework [32], pre-trained models hosted on HuggingFace [132], and the\nx-transformers library for Transformer variants [128].\n31\nTable A.2: Training hyperparameters for MINECLIP.\nHyperparameter\nValue\nLR schedule\nCosine with warmup [73]\nWarmup steps\n500\nPeak LR\n1.5e-4\nFinal LR\n1e-5\nWeight decay\n0.2\nLayerwise LR decay\n0.65\nPre-trained layers LR multiplier\n0.5×\nBatch size per GPU\n64\nParallel GPUs\n8\nVideo resolution\n160 × 256\nNumber of frames\n16\nImage encoder\nViT-B\/16 [28]\nE.1\nVideo-Text Pair Extraction\nSimilar to VideoCLIP [136], we sample 640K pairs of 16-second video snippets and time-aligned\nEnglish transcripts by the following procedure:\n1) Collect a list of keywords corresponding to the supported entities, blocks, and items in\nMinecraft;\n2) Perform string matching over our YouTube video transcripts to obtain 640K text segments;\n3) For each matched transcript segment, randomly grow it to 16 ∼77 tokens (limited by CLIP’s\ncontext length);\n4) Randomly sample a timestamp within the start and end time of the matched transcript as the\ncenter for the video clip;\n5) Randomly grow the video clip from the center timestamp to 8 ∼16 seconds.\nE.2\nArchitecture\nMINECLIP architecture is composed of three parts:\nFrame-wise image encoder φI\nWe use the ViT-B\/16 architecture [28] to compute a 512-D\nembedding for each RGB frame. We initialize the weights from OpenAI CLIP’s public checkpoint\n[92] and only ﬁnetune the last two layers during training. The input resolution is 160 × 256, which is\ndifferent from CLIP’s default 224 × 224 resolution. We adapt the positional embeddings via bicubic\ninterpolation, which does not introduce any new learnable parameters.\nTemporal aggregator φa\nGiven a sequence of frame-wise RGB features, a temporal aggregator\nnetwork summarizes the sequence into one video embedding. After the aggregator, we insert two\nextra layers of residual CLIP Adapter [38]. The residual weight is initialized such that it is very\nclose to an identity function at the beginning of training. We consider two variants of φa:\n1. Average pooling (MINECLIP[avg]): a simple, parameter-free operator. It is fast to execute\nbut loses the temporal information, because average pooling is permutation-invariant.\n2. Self-Attention (MINECLIP[attn]): a 2-layer transformer encoder with 512 embedding size,\n8 attention heads, and Gated Linear Unit variant with Swish activation [106, 22]. The\ntransformer sequence encoder is relatively slower, but captures more temporal information\nand achieves better performance in our experiments (Table 1).\nText encoder φG\nWe use a 12-layer 512-wide GPT model with 8 attention heads [90, 91]. The\ninput string is converted to lower-case byte pair encoding with a 49,152 vocabulary size, and capped\nat 77 tokens. We exactly follow the text encoder settings in CLIP and initialize the weights from\ntheir public checkpoint. Only the last two layers of φG is ﬁnetuned during training.\n32\nAlgorithm 1: PPO-SI Interleaved Training\nInput: policy πθ, value function V F(·), SI buffer threshold ∆, SI frequency ω\n1 Initialize empty SI buffers for all tasks DSI ←{∅, ∀T ∈training tasks};\n2 Initialize a counter for simulator steps counter ←0;\n3 while not done do\n4\nCollect set of trajectories for all tasks {τT , ∀T ∈training tasks} by running policy πθ in\n(parallel) environments;\n5\nforall DSI,T do\n6\nif τT is successful then\n7\nDSI,T ←DSI,T ∪τT\n8\nelse if τT ’s episode return ≥µreturn(DSI,T ) + ∆× σreturn(DSI,T ) then\n9\nDSI,T ←DSI,T ∪τT\n10\nend\n11\nIncrease counter accordingly;\n12\nUpdate πθ following Equation 2;\n13\nFit V F(·) by regression on mean-squared error;\n14\nif 1(counter mod ω = 0) then\n15\nDetermine the number of trajectories to sample from each buffer\n#sample = min({|DSI,T |, ∀T ∈training tasks});\n16\nSample #sample trajectories from each buffer in a prioritized manner to construct DSI;\n17\nUpdate πθ on DSI with supervised objective;\n18 end\nE.3\nTraining\nWe train MINECLIP on the 640K video-text pairs for 2 epochs. We sample 16 RGB frames from each\nvideo uniformly, and apply temporally-consistent random resized crop [17, 33] as data augmentation.\nWe use Cosine learning rate annealing with 500 gradient steps of warming up [73]. We apply a\nlower learning rate (×0.5) on the pre-trained weights and layer-wise learning rate decay for better\nﬁnetuning [53]. Training is performed on 1 node of 8× V100 GPUs with FP16 mixed precision [76]\nvia the PyTorch native amp module. All hyperparameters are listed in Table A.2.\nF\nPolicy Learning Details\nIn this section, we elaborate how a trained MINECLIP can be adapted as a reward function with two\ndifferent formulations. We then discuss the algorithm for policy learning. Finally, we demonstrate\nhow we combine self imitation learning and on-policy learning to further improve sample efﬁciency.\nF.1\nAdapt MINECLIP as Reward Function\nWe investigate two ways to convert MINECLIP output to scalar reward, dubbed DIRECT and DELTA.\nThe ablation results for Animal-Zoo task group are presented in Table A.3.\nDirect.\nFor a task T with the goal description G, MINECLIP outputs the probability PG that the\nobservation video semantically corresponds to G, against a set of negative goal descriptions G−.\nNote that we omit timestep subscript for simplicity. As an example, for the task “shear sheep”, G\nis “shear a sheep” and G−may include negative prompts like “milk a cow”, “hunt a sheep”, “hunt a\ncow”, etc. To compute the DIRECT reward, we further process the raw probability using the formula\nr = max(PG −\n1\nNT , 0) where NT is the number of prompts passed to MINECLIP.\n1\nNT is the\nbaseline probability of randomly guessing which text string corresponds to the video. We threshold\nr at zero to avoid highly uncertain probability estimates below the random baseline. We call the\nvariant without the post-processing DIRECT-Naive: r = PG as the reward signal for every time step.\nDelta.\nThe DIRECT formulation yields strong performance when the task is concerned with moving\ncreatures, e.g. farm animals and monsters that run around constantly. However, we discover that\nDIRECT is suboptimal if the task deals with static objects, e.g., “ﬁnd a nether portal”. Simply using the\n33\nTable A.3: Ablation on different MINECLIP reward formulations.\nGroup\nTasks\nDIRECT\nDIRECT-Naive\nDELTA\nMilk Cow\n64.5 ± 37.1\n8.6 ± 1.2\n7.6 ± 5.2\nHunt Cow\n83.5 ± 7.1\n0.0 ± 0.0\n0.0 ± 0.0\nShear Sheep\n12.1 ± 9.1\n0.8 ± 0.6\n1.8 ± 1.5\nHunt Sheep\n8.1 ± 4.1\n0.1 ± 0.2\n0.0 ± 0.0\nraw probability from MINECLIP as reward can cause the learned agent to stare at the object of interest\nbut fail to move closer and interact. Therefore, we propose to use an alternative formulation, DELTA, to\nremedy this issue. Concretely, the reward value at timestep t becomes rt = PG,t−PG,t−1. We empiri-\ncally validate that this formulation provides better shaped reward for the task group with static entities.\nF.2\nPolicy Network Architecture\nOur policy architecture consists of three parts: an input feature encoder, a policy head, and a value\nfunction. To handle multimodal observations (Sec. B.1), the feature extractor contains several\nmodality-speciﬁc components:\n• RGB frame: we use the frozen frame-wise image encoder φI in MINECLIP to optimize\nfor compute efﬁciency and provide the agent with good visual representations from the\nbeginning (Sec. 4.2).\n• Task goal: φG computes the text embedding of the natural language task goal.\n• Yaw and Pitch: compute sin(·) and cos(·) features respectively, then pass through an MLP.\n• GPS: normalize and featurize via MLP.\n• Voxel: to process the 3 × 3 × 3 surrounding voxels, we embed discrete block names to dense\nvectors, ﬂatten them, and pass through an MLP.\n• Past action: our agent is conditioned on its immediate past action, which is embedded and\nfeaturized by MLP.\nFeatures from all modalities are concatenated, passed through another fusion MLP, and ﬁnally fed\ninto the policy head and value function head. We use an MLP to model the policy head that maps\nfrom the input feature vectors to the action probability distribution. We use another MLP to estimate\nthe value function, conditioned on the same input features.\nF.3\nRL Training\nPPO.\nWe use the popular PPO algorithm [102] (Proximal Policy Optimization) as our RL training\nbackbone. PPO is an on-policy method that optimizes for a surrogate objective while ensuring that\nthe deviation from the previous policy is relatively small. PPO updates the policy network by\nmaximize\nθ\nEs,a∼πθoldL(s, a, θold, θ),\n(1)\nwhere\nL(s, a, θold, θ) = min\n\u0012 πθ(a|s)\nπθold(a|s)Aπθold (s, a), clip\n\u0012 πθ(a|s)\nπθold(a|s), 1 −ϵ, 1 + ϵ\n\u0013\nAπθold (s, a)\n\u0013\n.\n(2)\nA is an estimator of the advantage function (GAE [101] in our case) and ϵ is a hyperparameter that\ncontrols the deviation between the new policy and the old one.\nSelf Imitation Learning.\nWe apply self-imitation learning [84] (SI) to further improve sample efﬁ-\nciency because computing the reward with MINECLIP in the loop makes the training more expensive.\nSelf-imitation learning is essentially supervised learning on a buffer DSI of good trajectories gener-\nated by the agent’s past self. In our case, the trajectories are generated by the behavior policy during\nPPO rollouts, and only added to DSI if it is a successful trial or if the episodic return exceeds a certain\nthreshold. Self imitation optimizes πθ for the objective JSI = Es,a∼DSI log πθ(a|s) with respect to θ.\n34\n(a) “Milk Cow”\n(b) “Shear Sheep”\nFigure A.8: Adding the self imitation technique [84] signiﬁcantly improves the performance of RL\ntraining in MINEDOJO.\nWe alternate between the PPO phase and the SI phase. A pseudocode of our interleaved training\nprocedure is given in Algorithm 1. We use a prioritized strategy to sample trajectories from the buffer\nDSI. Speciﬁcally, we assign equal probability to all successful trajectories. Unsuccessful trajectories\ncan still be sampled but with lower probabilities proportional to their episodic returns.\nIn Fig. A.8, we demonstrate that adding self-imitation dramatically improves the stability, perfor-\nmance, and sample efﬁciency of RL training in MINEDOJO.\nG\nExperiment Details\nG.1\nTask Details\nWe experiment with three task groups with four tasks per group. We train one multi-task agent for\neach group. In this section, we describe each task goals, initial setup, and the manual dense-shaping\nreward function.\nAnimal Zoo:\n4 Programmatic tasks on hunting or harvesting resource from animals. We spawn\nvarious animal types (pig, sheep, and cow) in the same environment to serve as distractors. It is\nconsidered a failure if the agent does not take action on the correct animal speciﬁed by the prompt.\n• Milk Cow: ﬁnd and approach a cow, then obtain milk from it with an empty bucket. The\nprompt is milk a cow. We initialize the agent with an empty bucket to collect milk. We\nalso spawn sheep, cow, and pig nearby the agent. The manual dense reward shaping is\na navigation reward based on geodesic distance obtained from privileged LIDAR. The\ncombined reward passed to PPO can be formulated as rt = λnav max(dmin,t−1 −dmin,t, 0) +\nλsuccess1(milk collected), where λnav = 10 and λsuccess = 200. dmin,t = min(dmin, dt)\nwhere dmin denotes the minimal distance to the cow that the agent has achieved so far in the\nepisode history.\n• Hunt Cow: ﬁnd and approach a cow, then hunt with a sword. The cow will run away so\nthe agent needs to chase after it. The prompt is hunt a cow. We initialize the agent with\na diamond sword. The manual dense reward shaping consists of two parts, a valid attack\nreward and a navigation reward based on geodesic distance obtained from privileged LIDAR.\nMathematically, the reward is rt = λattack1(valid attack) + λnav max(dmin,t−1 −dmin,t, 0) +\nλsuccess1(cow hunted), where λattack = 5, λnav = 1, and λsuccess = 200. We additionally\nreset dmin every time the agent hits the cow to encourage the chasing behavior.\n• Shear Sheep: ﬁnd and approach a sheep, then collect wool from the sheep with a shear. The\nprompt is shear a sheep. We initialize the agent with a shear. The manual dense reward\nshaping is a navigation reward based on geodesic distance obtained from the privileged\nLIDAR sensor, similar to “Milk Cow”.\n• Hunt Sheep: ﬁnd and approach a sheep, then hunt with a sword. The sheep will run away\nso the agent needs to chase after it. An episode will terminate once any entity is hunted. The\nprompt is hunt a sheep. We initialize the agent with a diamond sword. The manual dense\n35\nTable A.4: Hyperparameters in RL experiments. “{state} MLP” refers to MLPs to process\nobservations of compass, GPS, and voxel blocks. “Embed Dim” denotes the same dimension size\nused to embed all discrete observations into dense vectors.\nNN Architecture\nTraining\nHyperparameter\nAnimal-Zoo\nMob-Combat\nCreative\nRGB Feature Size\n512\nLearning Rate\n10−4\n10−4\n10−4\nTask Prompt Feature Size\n512\nCosine Decay Minimal LR\n5 × 10−6\n5 × 10−6\n5 × 10−6\n{state} MLP Hidden Size\n128\nγ\n0.99\n0.99\n0.99\n{state} MLP Output Size\n128\nEntropy Weight (Stage 1)\n5 × 10−3\n5 × 10−3\n5 × 10−3\n{state} MLP Hidden Depth\n2\nEntropy Weight (Stage 2)\n10−2\nN\/A\n10−2\nEmbed Dim\n8\nPPO Optimizer\nAdam\nAdam\nAdam\nNum Feature Fusion Layers\n1\nSI Learning Rate\n10−4\n10−4\n10−4\nFeature Fusion Output Size\n512\nSI Cosine Decay Minimal LR\n10−6\n10−6\n10−6\nPrev Action Conditioning\nTrue\nSI Epoch\n10\n10\n10\nPolicy Head Hidden Size\n256\nSI Frequency (Env Steps)\n100K\n100K\n100K\nPolicy Head Hidden Depth\n3\nSI Optimizer\nAdam\nAdam\nAdam\nVF Hidden Size\n256\nSI Buffer Threshold\n2σ\n2σ\n0.5σ\nVF Hidden Depth\n3\nPPO Buffer Size\n100K\n100K\n100K\nFrame Stack\n1\n1\n1\nVF Loss Weight\n0.5\n0.5\n0.5\nGAE λ\n0.95\n0.95\n0.95\nGradient Clip\n10\n10\n10\nPPO ϵ\n0.2\n0.2\n0.2\nAction Smooth Weight\n10−7\n10−7\n10−7\nAction Smooth Window Size\n3\n3\n3\nMINECLIP Reward Formulation\nDIRECT\nDIRECT\nDELTA\nreward shaping consists of two parts, a valid attack reward and a navigation reward based on\ngeodesic distance obtained from the privileged LIDAR sensor, similar to “Hunt Cow”.\nMob Combat:\nﬁght 4 different types of hostile monsters: Spider, Zombie, Zombie Pigman (a\ncreature in the Nether world), and Enderman (a creature in the End world). The prompt template\nis \"Combat {monster}\". For all tasks within this group, we initialize the agent with a diamond\nsword, a shield, and a full suite of diamond armors. The agent is spawned in the Nether for Zombie\nPigman task, and in the End for Enderman. The manual dense-shaping reward can be expressed\nas rt = λattack1(valid attack) + λsuccess1({monster} hunted) where λattack = 5 and λsuccess = 200.\nCreative:\n4 tasks that do not have manual dense reward shaping or code-deﬁned success criterion.\n• Find Nether Portal: ﬁnd and move close to a Nether Portal, then enter the Nether world\nthrough the portal. The prompt is find a nether portal.\n• Find Ocean: ﬁnd and move close to an ocean. The prompt is find an ocean.\n• Dig Hole: dig holes in the ground. The prompt is dig a hole. We initialize the agent with\nan iron shovel.\n• Lay Carpet: lay down carpets to cover the wooden ﬂoor inside a house. The prompt is put\ncarpets on the floor. We initialize the agent with a number of carpets in its inventory.\nNote that we categorize “Find Nether Portal” and “Find Ocean” as Creative tasks even though they\nseem similar to object navigation [12]. While ﬁnding terrains and other structures is semantically\nwell deﬁned, it is not easy to deﬁne a function to evaluate success automatically because the simulator\ndoes not have the exact location information of these structures given a randomly generated world.\nIn principle, we can make a sweep by querying each chunk of voxels in the world to recognize the\nterrains, but that would be prohibitively expensive. Therefore, we opt to use MineCLIP as the reward\nsignal and treat these tasks as Creative.\nG.2\nObservation and Action Space\nWe use a subset of the full observation and action space listed in Sec. B.1 and B.2, because the\ntasks in our current experiments do not involve actions like crafting or inventory management. Our\nobservation space consists of RGB frame, compass, GPS, and Voxels.\n36\nOur action space is a trimmed version of the full action space. It consists of movement control,\ncamera control, “use” action, and “attack” action, which add up to 89 discrete choices. Concretely, it\nincludes 81 actions for discrete camera control (9 × 9 resulted from the Cartesian product between\nyaw and pitch, each ranges from −60 degree to 60 degree with a discrete interval of 15 degree). It\nalso includes 6 movement actions (forward, forward + jump, jump, back, move left, and move right)\nand 2 functional actions of “use” and “attack”. Note that the “no-op” action is merged into the 81\ncamera actions.\nG.3\nRL Training\nAll hyperparameters used in our RL experiment are listed in Table A.4. We visualize the learned\nbehaviors of 4 tasks in Figure 2. Demos of more tasks can be found on our website https:\n\/\/minedojo.org.\nAction smoothing.\nDue to the stochastic nature of PPO, we observe a lot of action jittering in\nthe agent’s behavior during training. This leads to two negative effects that degrade the learning\nperformance: 1) exploration difﬁculty due to inconsistent action sequence. For example, the agent\nmay be required to take multiple consecutive attack actions in order to complete certain tasks; and 2)\nrapidly switching different movement and camera motions result in videos that are highly non-smooth\nand disorienting. This causes a domain gap from the training data of MINECLIP, which are typically\nsmooth human gameplay videos. Therefore, the reward signal quality deteriorates signiﬁcantly.\nTo remedy the issue, we impose an action smoothing loss to be jointly optimized with the PPO\nobjective (Eq. 2) during training. Concretely, consider a sliding window W with window size |W|\nthat contains |W| consecutive action distributions W = {πt−|W|+1, πt−|W|+2, . . . , πt}, the action\nsmoothing loss is deﬁned as\nLsmooth =\n1\n|W|\n|W|−1\nX\ni=1\nKL(πt∥πt−|W|+i),\n(3)\nwhere KL(·) denotes Kullback–Leibler divergence.\nMulti-stage training for multi-task RL.\nDue to hardware limitations, we are not able to run a\nlarge number of parallel simulators for all tasks in a task group. Therefore, we adopt a multi-stage\nstrategy to split the tasks and train them sequentially with a single policy network. For the task\ngroups Animal-Zoo and Creative, we split the four tasks into two stages of two parallel training\ntasks each. We carry over the self-imitation buffers when switching to the next stage. We also follow\nthe recommended practice in [83] and reset the policy head at the beginning of stage 2 to encourage\nexploration and reduce overﬁtting. We adopt a similar replay buffer balancing strategy as [46] to\nprevent any task from dominating the training.\nG.4\nEvaluation\nIn this section, we elaborate on our human and automatic evaluation procedure for Creative tasks.\nWe ﬁrst ask the human annotators to manually label 100 successful and 100 failure trajectories. This\nproduces a combined dataset of 200 trajectories with groundtruth binary labels to evaluate the learned\nreward functions. On this dataset, we run MINECLIP to produce step-wise rewards and compute a\nscore that averages over each trajectory. We then apply K-means clustering with K = 2 to all scores\nand determine a decision boundary δ from the mean of the two centroids. A trajectory with a score\ngreater than δ is classiﬁed as successful, and vice versa for failure. In this way, we essentially convert\nMINECLIP to a binary classiﬁer. The quality of MINECLIP can be measured by the F1 score of\nits binary classiﬁcation output against the human labels. We demonstrate that MINECLIP has high\nagreements with humans (Table 2), and thus qualiﬁes as an effective automatic evaluation metric for\nCreative tasks in the absence of human judges.\nTo further investigate MINECLIP’s evaluation on more complex Creative tasks, we annotate 50\nYouTube video segments each for 5 more tasks that are much more semantically complex: “build\na farm”, “ build a fence”, “build a house”, “ride a minecart”, and “build a swimming pool”. We\nthen run MINECLIP evaluation on these videos against a negative set. As shown in Table A.5,\nthough not perfect, MINECLIP generally has a positive agreement with human judgment. We note\n37\nTable A.5: MINECLIP’s evaluation on more complex Creative tasks. Numbers represent F1 scores\nbetween MINECLIP’s evaluation on tasks success and human labels. Scaled to percentage for better\nreadability.\nTasks\nBuild a Farm\nBuild a Fence\nBuild a House\nRide a Minecart\nBuild a Swimming Pool\nOurs (Attn)\n78.7\n91.4\n63.7\n95.9\n85.0\nOurs (Avg)\n73.4\n83.1\n37.4\n96.9\n94.7\nCLIPOpenAI\n62.5\n24.5\n52.9\n70.0\n71.7\nthat the current MINECLIP is a proof-of-concept step in leveraging internet data for automated\nevaluation, and further scaling on more training data and parameters may lead to more improvements.\nMeanwhile, human judgment remains a useful and important alternative [93, 97].\nH\nLimitations and Potential Societal Impact\nUnlike human demonstrations [126] or ofﬂine RL datasets [35], our YouTube dataset contains only\nthe video screen observations but not the actual control actions. This allows us to scale up the dataset\ntremendously, but at the same time poses a challenge to imitation learning algorithms that require\nobservation-action pairs to learn. Our proposed algorithm, MINECLIP, side-steps this problem by\nlearning a reward model, but we believe that directly inferring the human expert policy from YouTube\nis another important direction complementary to our approach. There are promising techniques that\ncan potentially overcome this limitation, such as the Learning-from-Observation (LfO) family of\nalgorithms [123, 122, 115, 31].\nOur database is scraped from the internet, which inevitably contains offensive YouTube videos or\ntoxic Reddit posts. While we have made our best effort to ﬁlter out these harmful contents (Sec. D.1),\nthere can still be undesirable biases and toxicity that elude our automatic ﬁlters. Furthermore, we\nadvocate the use of large pre-trained language models in our main paper, and MINECLIP is ﬁnetuned\nfrom the pre-trained weights of OpenAI CLIP [92]. These foundation models are known to contain\nharmful stereotypes and generate hateful commentary [15, 13, 40]. We ask the researchers who will\nuse our code and database to exercise their best judgment during new model development to avoid\nany negative social impact.\nI\nDatasheet\nWe present a Datasheet [39] for documentation and responsible usage of our internet knowledge\ndatabases.\nI.1\nMotivation\nFor what purpose was the dataset created?\nWe create this internet-scale multimodal knowledge\nbase to facilitate research towards open-ended, generally capable embodied agents.\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)?\nThis knowledge base was created by Linxi Fan (Nvidia),\nGuanzhi Wang (Caltech), Yunfan Jiang (Stanford), Ajay Mandlekar (Nvidia), Yuncong Yang\n(Columbia), Haoyi Zhu (SJTU), Andrew Tang (Columbia), De-An Huang (Nvidia), Yuke Zhu\n(Nvidia and UT Austin), and Anima Anandkumar (Nvidia and Caltech).\nI.2\nDistribution\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created?\nYes, the dataset is publicly available\non the internet.\n38\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)?\nAll datasets\ncan be downloaded from https:\/\/zenodo.org\/. Please refer to this table of URL, DOI, and\nlicensing:\nDatabase\nDOI\nLicense\nYouTube\n10.5281\/zenodo.6641142\nCreative Commons Attribution 4.0 International (CC BY 4.0)\nWiki\n10.5281\/zenodo.6640448\nCreative Commons Attribution Non Commercial Share Alike 3.0 Unported\nReddit\n10.5281\/zenodo.6641114\nCreative Commons Attribution 4.0 International (CC BY 4.0)\nHave any third parties imposed IP-based or other restrictions on the data associated with the\ninstances?\nNo.\nDo any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances?\nNo.\nI.3\nMaintenance\nWho will be supporting\/hosting\/maintaining the dataset?\nThe authors will be supporting,\nhosting, and maintaining the dataset.\nHow can the owner\/curator\/manager of the dataset be contacted (e.g., email address)?\nPlease\ncontact Linxi Fan (linxif@nvidia.com), Guanzhi Wang (guanzhi@caltech.edu), and Yunfan\nJiang (yunfanj@cs.stanford.edu).\nIs there an erratum?\nNo. We will make announcements if there is any.\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete\ninstances)?\nYes. New updates will be posted on https:\/\/minedojo.org.\nIf the dataset relates to people, are there applicable limits on the retention of the data\nassociated with the instances (e.g., were the individuals in question told that their data would\nbe retained for a ﬁxed period of time and then deleted)?\nN\/A.\nWill older versions of the dataset continue to be supported\/hosted\/maintained?\nYes, old\nversions will be permanently accessible on zenodo.org.\nIf others want to extend\/augment\/build on\/contribute to the dataset, is there a mechanism for\nthem to do so?\nYes, please refer to https:\/\/minedojo.org.\nI.4\nComposition\nWhat do the instances that comprise the dataset represent?\nFor YouTube videos, our data is in\nJSON format with video URLs and metadata. We do not provide the raw MP4 ﬁles for legal concerns.\nFor Wiki, we provide the text, images, tables, and diagrams embedded on the web pages. For Reddit,\nour data is in JSON format with post IDs and metadata, similar to YouTube. Users can reconstruct\nthe Reddit dataset by running our script after obtaining an ofﬁcial Reddit API license key.\nHow many instances are there in total (of each type, if appropriate)?\nThere are more than\n730K YouTube videos with 2.2B words of transcripts, 6,735 Wiki pages with 2.2M bounding boxes\nof visual elements, and more than 340K Reddit posts with 6.6M comments.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set?\nWe provide all instances in our Zenodo data repositories.\nIs there a label or target associated with each instance?\nNo.\nIs any information missing from individual instances?\nNo.\n39\nAre relationships between individual instances made explicit (e.g., users’ movie ratings, social\nnetwork links)?\nWe provide metadata for each YouTube video link and Reddit post ID.\nAre there recommended data splits (e.g., training, development\/validation, testing)?\nNo. The\nentire database is intended for pre-training.\nAre there any errors, sources of noise, or redundancies in the dataset?\nPlease refer to Sec. D\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)?\nWe follow prior works [64] and only release the video URLs\nof YouTube videos due to legal concerns. Researchers need to acquire the MP4 and transcript ﬁles\nseparately. Similarly, we only release the post IDs for the Minecraft Reddit database, but we also\nprovide a script that can reconstruct the full Reddit dataset given a free ofﬁcial license key.\nDoes the dataset contain data that might be considered conﬁdential?\nNo.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety?\nWe have made our best efforts to detoxify the contents via\nan automated procedure. Please refer to Sec. D.\nI.5\nCollection Process\nThe collection procedure, preprocessing, and cleaning are explained in details in Sec. D.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors)\nand how were they compensated (e.g., how much were crowdworkers paid)?\nAll data\ncollection, curation, and ﬁltering are done by MINEDOJO coauthors.\nOver what timeframe was the data collected?\nThe data was collected between Dec. 2021 and\nMay 2022.\nI.6\nUses\nHas the dataset been used for any tasks already?\nYes, we have used the MINEDOJO YouTube\ndatabase for agent pre-training. Please refer to Sec. 5 and Sec. G for algorithmic and training details.\nWhat (other) tasks could the dataset be used for?\nOur knowledge base is primarily intended\nto facilitate research in open-ended, generally capable embodied agents. However, it can also be\nbroadly applicable to research in video understanding, document understanding, language modeling,\nmultimodal learning, and so on.\nIs there anything about the composition of the dataset or the way it was collected and\npreprocessed\/cleaned\/labeled that might impact future uses?\nNo.\nAre there tasks for which the dataset should not be used?\nWe strongly oppose any research\nthat intentionally generates harmful or toxic contents using our YouTube, Wiki, and Reddit data.\n40\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge.pdf"}
{"title":"Mastering Diverse Domains through World Models","authors":"Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap","summary":"Developing a general algorithm that learns to solve tasks across a wide range\nof applications has been a fundamental challenge in artificial intelligence.\nAlthough current reinforcement learning algorithms can be readily applied to\ntasks similar to what they have been developed for, configuring them for new\napplication domains requires significant human expertise and experimentation.\nWe present DreamerV3, a general algorithm that outperforms specialized methods\nacross over 150 diverse tasks, with a single configuration. Dreamer learns a\nmodel of the environment and improves its behavior by imagining future\nscenarios. Robustness techniques based on normalization, balancing, and\ntransformations enable stable learning across domains. Applied out of the box,\nDreamer is the first algorithm to collect diamonds in Minecraft from scratch\nwithout human data or curricula. This achievement has been posed as a\nsignificant challenge in artificial intelligence that requires exploring\nfarsighted strategies from pixels and sparse rewards in an open world. Our work\nallows solving challenging control problems without extensive experimentation,\nmaking reinforcement learning broadly applicable.","url":"http:\/\/arxiv.org\/abs\/2301.04104v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2301.04104v2","published":1673374336000,"comment":"Website: https:\/\/danijar.com\/dreamerv3","pdf_text":"Mastering Diverse Domains through World Models\nDanijar Hafner,12 Jurgis Pasukonis,1 Jimmy Ba,2 Timothy Lillicrap1\nAbstract\nDeveloping a general algorithm that learns to solve tasks across a wide range of\napplications has been a fundamental challenge in artificial intelligence. Although\ncurrent reinforcement learning algorithms can be readily applied to tasks similar to\nwhat they have been developed for, configuring them for new application domains\nrequires significant human expertise and experimentation. We present DreamerV3, a\ngeneral algorithm that outperforms specialized methods across over 150 diverse tasks,\nwith a single configuration. Dreamer learns a model of the environment and improves its\nbehavior by imagining future scenarios. Robustness techniques based on normalization,\nbalancing, and transformations enable stable learning across domains. Applied out of the\nbox, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without\nhuman data or curricula. This achievement has been posed as a significant challenge in\nartificial intelligence that requires exploring farsighted strategies from pixels and sparse\nrewards in an open world. Our work allows solving challenging control problems without\nextensive experimentation, making reinforcement learning broadly applicable.\n0\n300\n600\n900\nPPO\nRainbow\nMuZero\nDreamer\n57 tasks, 200M steps\nAtari\n10\n30\n50\n70\nPPO\nRainbow\nPPG\nDreamer\n16 tasks, 50M steps\nProcGen\n10\n30\n50\n70\nPPO\nR2D2+\n10x data\nIMPALA\n10x data\nDreamer\n30 tasks, 100M steps\nDMLab\n0\n3\n6\n9\nPPO\nRainbow\nIMPALA\nDreamer\n1 task, 100M steps\nMinecraft\n100K\n1M\n10M\n100M\nEnv steps\n0\n4\n8\n12\nReturn\nMinecraft Diamond\nMax\nMean\n10\n50\n90\n130\nPPO\nTWM\nIRIS\nDreamer\n26 tasks, 400K steps\nAtari100k\n0\n300\n600\n900\nPPO\nD4PG\nDMPO\nDreamer\n18 tasks, 500K steps\nProprio Control\n0\n300\n600\n900\nPPO\nCURL\nDrQ-v2\nDreamer\n20 tasks, 1M steps\nVisual Control\n10\n30\n50\n70\nPPO\nDQN\nBoot DQN\nDreamer\n23 tasks\nBSuite\na\nb\nTuned experts          \nUnified configuration\nFigure 1: Benchmark summary. a, Using fixed hyperparameters across all domains, Dreamer\noutperforms tuned expert algorithms across a wide range of benchmarks and data budgets. Dreamer\nalso substantially outperforms a high-quality implementation of the widely applicable PPO algorithm.\nb, Applied out of the box, Dreamer learns to obtain diamonds in the popular video game Minecraft\nfrom scratch given sparse rewards, a long-standing challenge in artificial intelligence for which\nprevious approaches required human data or domain-specific heuristics.\n1Google DeepMind. 2University of Toronto. Correspondence: mail@danijar.com\n1\narXiv:2301.04104v2  [cs.AI]  17 Apr 2024\n(a) Control Suite\n(b) Atari\n(c) ProcGen\n(d) DMLab\n(e) Minecraft\nFigure 2: Diverse visual domains used in the experiments. Dreamer succeeds across these domains,\nranging from robot locomotion and manipulation tasks over Atari games, procedurally generated\nProcGen levels, and DMLab tasks, that require spatial and temporal reasoning, to the complex and\ninfinite world of Minecraft. We also evaluate Dreamer on non-visual domains.\nIntroduction\nReinforcement learning has enabled computers to solve tasks through interaction, such as surpassing\nhumans in the games of Go and Dota1,2. It is also a key component for improving large language\nmodels beyond what is demonstrated in their pretraining data3,4. While PPO5 has become a standard\nalgorithm in the field of reinforcement learning, more specialized algorithms are often employed\nto achieve higher performance. These specialized algorithms target the unique challenges posed\nby different application domains, such as continuous control6, discrete actions7,8, sparse rewards9,\nimage inputs10, spatial environments11, and board games12. However, applying reinforcement\nlearning algorithms to sufficiently new tasks—such as moving from video games to robotics tasks—\nrequires substantial effort, expertise, and computational resources for tweaking the hyperparameters\nof the algorithm13. This brittleness poses a bottleneck in applying reinforcement learning to new\nproblems and also limits the applicability of reinforcement learning to computationally expensive\nmodels or tasks where tuning is prohibitive. Creating a general algorithm that learns to master new\ndomains without having to be reconfigured has been a central challenge in artificial intelligence and\nwould open up reinforcement learning to a wide range of practical applications.\nWe present Dreamer, a general algorithm that outperforms specialized expert algorithms across a\nwide range of domains while using fixed hyperparameters, making reinforcement learning readily\napplicable to new problems. The algorithm is based on the idea of learning a world model that equips\nthe agent with rich perception and the ability to imagine the future14,15,16. The world model predicts\nthe outcomes of potential actions, a critic neural network judges the value of each outcome, and an\nactor neural network chooses actions to reach the best outcomes. Although intuitively appealing,\nrobustly learning and leveraging world models to achieve strong task performance has been an open\nproblem17. Dreamer overcomes this challenge through a range of robustness techniques based on\nnormalization, balancing, and transformations. We observe robust learning not only across over 150\ntasks from the domains summarized in Figure 2, but also across model sizes and training budgets,\noffering a predictable way to increase performance. Notably, larger model sizes not only achieve\nhigher scores but also require less interaction to solve a task.\nTo push the boundaries of reinforcement learning, we consider the popular video game Minecraft\nthat has become a focal point of research in recent years18,19,20, with international competitions held\nfor developing algorithms that autonomously learn to collect diamonds in Minecraft*. Solving this\n*The MineRL Diamond Competitions were held in 2019, 2020, and 2021 and provided a dataset of human expert\ntrajectories: https:\/\/minerl.io\/diamond. Competitions in the following years focused on a wide range of tasks.\n2\nx1\nx2\nx3\nx̂ 1\nx̂ 2\nx̂ 3\na1\na2\nz1\nz2\nz3\nh3\nh2\nh1\nenc\nenc\nenc\ndec\ndec\ndec\n(a) World Model Learning\nh3\nh2\nh1\na1\na2\nv1\nv2\nr2\nv2\nr2\nx1\nz1\nz2\nz3\nenc\n(b) Actor Critic Learning\nFigure 3: Training process of Dreamer. The world model encodes sensory inputs into discrete\nrepresentations zt that are predicted by a sequence model with recurrent state ht given actions at.\nThe inputs are reconstructed to shape the representations. The actor and critic predict actions at and\nvalues vt and learn from trajectories of abstract representations predicted by the world model.\nproblem without human data has been widely recognized as a substantial challenge for artificial\nintelligence because of the sparse rewards, exploration difficulty, long time horizons, and the\nprocedural diversity of this open world game18. Due to these obstacles, previous approaches resorted\nto using human expert data and domain-specific curricula19,20. Applied out of the box, Dreamer is\nthe first algorithm to collect diamonds in Minecraft from scratch.\nLearning algorithm\nWe present the third generation of the Dreamer algorithm21,22. The algorithm consists of three neural\nnetworks: the world model predicts the outcomes of potential actions, the critic judges the value of\neach outcome, and the actor chooses actions to reach the most valuable outcomes. The components\nare trained concurrently from replayed experience while the agent interacts with the environment. To\nsucceed across domains, all three components need to accommodate different signal magnitudes and\nrobustly balance terms in their objectives. This is challenging as we are not only targeting similar\ntasks within the same domain but aim to learn across diverse domains with fixed hyperparameters.\nThis section introduces the world model, critic, and actor along with their robust loss functions, as\nwell as tools for robustly predicting quantities of unknown orders of magnitude.\nWorld model learning\nThe world model learns compact representations of sensory inputs through autoencoding23 and en-\nables planning by predicting future representations and rewards for potential actions. We implement\nthe world model as a Recurrent State-Space Model (RSSM)24, shown in Figure 3. First, an encoder\nmaps sensory inputs xt to stochastic representations zt. Then, a sequence model with recurrent\nstate ht predicts the sequence of these representations given past actions at−1. The concatenation of\n3\nTrue\nContext Input\nOpen Loop Prediction\nModel\nTrue\nT = 0\nModel\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nFigure 4: Multi-step video predictions of a DMLab maze (top) and a quadrupedal robot (bottom).\nGiven 5 context images and the full action sequence, the model predicts 45 frames into the future\nwithout access to intermediate images. The world model learns an understanding of the underlying\nstructure of each environment.\nht and zt forms the model state from which we predict rewards rt and episode continuation flags\nct ∈{0, 1} and reconstruct the inputs to ensure informative representations:\nRSSM\n\n\n\n\n\nSequence model:\nht = fϕ(ht−1, zt−1, at−1)\nEncoder:\nzt ∼qϕ(zt | ht, xt)\nDynamics predictor:\nˆzt ∼pϕ(ˆzt | ht)\nReward predictor:\nˆrt ∼pϕ(ˆrt | ht, zt)\nContinue predictor:\nˆct ∼pϕ(ˆct | ht, zt)\nDecoder:\nˆxt ∼pϕ(ˆxt | ht, zt)\n(1)\nFigure 4 visualizes long-term video predictions of the world world. The encoder and decoder use\nconvolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for\nvector inputs. The dynamics, reward, and continue predictors are also MLPs. The representations\nare sampled from a vector of softmax distributions and we take straight-through gradients through\nthe sampling step25,22. Given a sequence batch of inputs x1:T, actions a1:T, rewards r1:T, and\ncontinuation flags c1:T, the world model parameters ϕ are optimized end-to-end to minimize the\nprediction loss Lpred, the dynamics loss Ldyn, and the representation loss Lrep with corresponding\nloss weights βpred = 1, βdyn = 1, and βrep = 0.1:\nL(ϕ) .= Eqϕ\nh PT\nt=1(βpredLpred(ϕ) + βdynLdyn(ϕ) + βrepLrep(ϕ))\ni\n.\n(2)\nThe prediction loss trains the decoder and reward predictor via the symlog squared loss described\nlater, and the continue predictor via logistic regression. The dynamics loss trains the sequence model\nto predict the next representation by minimizing the KL divergence between the predictor pϕ(zt | ht)\nand the next stochastic representation qϕ(zt | ht, xt). The representation loss, in turn, trains the\nrepresentations to become more predictable allowing us to use a factorized dynamics predictor for\nfast sampling during imagination training. The two losses differ in the stop-gradient operator sg(·)\nand their loss scale. To avoid a degenerate solution where the dynamics are trivial to predict but fail\n4\nto contain enough information about the inputs, we employ free bits26 by clipping the dynamics and\nrepresentation losses below the value of 1 nat ≈1.44 bits. This disables them while they are already\nminimized well to focus learning on the prediction loss:\nLpred(ϕ) .= −ln pϕ(xt | zt, ht) −ln pϕ(rt | zt, ht) −ln pϕ(ct | zt, ht)\nLdyn(ϕ) .= max\n\u00001, KL\n\u0002\nsg(qϕ(zt | ht, xt))\n\r\r\npϕ(zt | ht)\n\u0003\u0001\nLrep(ϕ) .= max\n\u00001, KL\n\u0002\nqϕ(zt | ht, xt)\n\r\r sg(pϕ(zt | ht))\n\u0003\u0001\n(3)\nPrevious world models require scaling the representation loss differently based on the visual\ncomplexity of the environment21. Complex 3D environments contain details unnecessary for\ncontrol and thus prompt a stronger regularizer to simplify the representations and make them\nmore predictable. In games with static backgrounds and where individual pixels may matter for\nthe task, a weak regularizer is required to extract fine details. We find that combining free bits\nwith a small representation loss resolves this dilemma, allowing for fixed hyperparameters across\ndomains. Moreover, we transform vector observations using the symlog function described later,\nto prevent large inputs and large reconstruction gradients, further stabilizing the trade-off with the\nrepresentation loss.\nWe occasionally observed spikes the in KL losses in earlier experiments, consistent with reports for\ndeep variational autoencoders27. To prevent this, we parameterize the categorical distributions of the\nencoder and dynamics predictor as mixtures of 1% uniform and 99% neural network output, making\nit impossible for them to become deterministic and thus ensuring well-behaved KL losses. Further\nmodel details and hyperparameters are included in the supplementary material.\nCritic learning\nThe actor and critic neural networks learn behaviors purely from abstract trajectories of representa-\ntions predicted by the world model14. For environment interaction, we select actions by sampling\nfrom the actor network without lookahead planning. The actor and critic operate on model states\nst .= {ht, zt} and thus benefit from the Markovian representations learned by the recurrent world\nmodel. The actor aims to maximize the return Rt .= P∞\nτ=0 γτrt+τ with a discount factor γ = 0.997\nfor each model state. To consider rewards beyond the prediction horizon T = 16, the critic learns to\napproximate the distribution of returns28 for each state under the current actor behavior:\nActor:\nat ∼πθ(at | st)\nCritic:\nvψ(Rt | st)\n(4)\nStarting from representations of replayed inputs, the world model and actor generate a trajectory of\nimagined model states s1:T, actions a1:T, rewards r1:T, and continuation flags c1:T. Because the critic\npredicts a distribution, we read out its predicted values vt .= E[vψ( · | st)] as the expectation of the\ndistribution. To estimate returns that consider rewards beyond the prediction horizon, we compute\nbootstrapped λ-returns29 that integrate the predicted rewards and the values. The critic learns to\npredict the distribution of the return estimates Rλ\nt using the maximum likelihood loss:\nL(ψ) .= −PT\nt=1 ln pψ(Rλ\nt | st)\nRλ\nt\n.= rt + γct\n\u0010\n(1 −λ)vt + λRλ\nt+1\n\u0011\nRλ\nT\n.= vT\n(5)\nWhile a simple choice would be to parameterize the critic as a Normal distribution, the return\ndistribution can have multiple modes and vary by orders of magnitude across environments. To\nstabilize and accelerate learning under these conditions, we parameterize the critic as categorical\ndistribution with exponentially spaced bins, decoupling the scale of gradients from the prediction\n5\ntargets as described later. To improve value prediction in environments where rewards are challenging\nto predict, we apply the critic loss both to imagined trajectories with loss scale βval = 1 and to\ntrajectories sampled from the replay buffer with loss scale βrepval = 0.3. The critic replay loss\nuses the imagination returns Rλ\nt at the start states of the imagination rollouts as on-policy value\nannotations for the replay trajectory to then compute λ-returns over the replay rewards.\nBecause the critic regresses targets that depend on its own predictions, we stabilize learning by\nregularizing the critic towards predicting the outputs of an exponentially moving average of its\nown parameters. This is similar to target networks used previously in reinforcement learning7 but\nallows us to compute returns using the current critic network. We further noticed that the randomly\ninitialized reward predictor and critic networks at the start of training can result in large predicted\nrewards that can delay the onset of learning. We thus initialize the output weight matrix of the\nreward predictor and critic to zeros, which alleviates the problem and accelerates early learning.\nActor learning\nThe actor learns to choose actions that maximize return while exploring through an entropy regular-\nizer30. However, the correct scale for this regularizer depends both on the scale and frequency of\nrewards in the environment. Ideally, we would like the agent to explore more if rewards are sparse\nand exploit more if rewards are dense or nearby. At the same time, the exploration amount should\nnot be influenced by arbitrary scaling of rewards in the environment. This requires normalizing the\nreturn scale while preserving information about reward frequency.\nTo use a fixed entropy scale of η = 3 × 10−4 across domains, we normalize returns to be approx-\nimately contained in the interval [0, 1]. In practice, substracting an offset from the returns does\nnot change the actor gradient and thus dividing by the range S is sufficient. Moreover, to avoid\namplifying noise from function approximation under sparse rewards, we only scale down large return\nmagnitudes but leave small returns below the threshold of L = 1 untouched. We use the Reinforce\nestimator31 for both discrete and continuous actions, resulting in the surrogate loss function:\nL(θ) .= −PT\nt=1 sg\n\u0010\u0000Rλ\nt −vψ(st)\n\u0001\n\/ max(1, S)\n\u0011\nlog πθ(at | st) + η H\n\u0002\nπθ(at\n\f\f st)\n\u0003\n(6)\nThe return distribution can be multi-modal and include outliers, especially for randomized environ-\nments where some episodes have higher achievable returns than others. Normalizing by the smallest\nand largest observed returns would then scale returns down too much and may cause suboptimal\nconvergence. To be robust to these outliers, we compute the range from the 5th to the 95th return\npercentile over the return batch and smooth out the estimate using an exponential moving average:\nS .= EMA\n\u0000Per(Rλ\nt , 95) −Per(Rλ\nt , 5), 0.99\n\u0001\n(7)\nPrevious work typically normalizes advantages5 rather than returns, which puts a fixed amount\nof emphasis on maximizing returns over entropy regardless of whether rewards are within reach.\nScaling up advantages when rewards are sparse can amplify noise that outweighs the entropy\nregularizer and stagnates exploration. Normalizing rewards or returns by standard deviation can\nfail under sparse rewards where their standard deviation is near zero, drastically amplifying rewards\nregardless of their size. Constrained optimization targets a fixed entropy on average across states32,33\nregardless of achievable returns, which is robust but explores slowly under sparse rewards and\nconverges lower under dense rewards. We did not find stable hyperparameters across domains\nfor these approaches. Return normalization with a denominator limit overcomes these challenges,\nexploring rapidly under sparse rewards and converging to high performance across diverse domains.\n6\nRobust predictions\nReconstructing inputs and predicting rewards and returns can be challenging because the scale of\nthese quantities can vary across domains. Predicting large targets using a squared loss can lead to\ndivergence whereas absolute and Huber losses7 stagnate learning. On the other hand, normalizing\ntargets based on running statistics5 introduces non-stationarity into the optimization. We suggest\nthe symlog squared error as a simple solution to this dilemma. For this, a neural network f(x, θ)\nwith inputs x and parameters θ learns to predict a transformed version of its targets y. To read out\npredictions ˆy of the network, we apply the inverse transformation:\nL(θ) .= 1\n2\n\u0000f(x, θ) −symlog(y)\n\u00012\nˆy .= symexp\n\u0000f(x, θ)\n\u0001\n(8)\nUsing the logarithm as transformation would not allow us to predict targets that take on negative\nvalues. Therefore, we choose a function from the bi-symmetric logarithmic family34 that we name\nsymlog as the transformation with the symexp function as its inverse:\nsymlog(x) .= sign(x) ln\n\u0000|x| + 1\n\u0001\nsymexp(x) .= sign(x)\n\u0000exp(|x|) −1\n\u0001\n(9)\nThe symlog function compresses the magnitudes of both large positive and negative values. Unlike\nthe logarithm, it is symmetric around the origin while preserving the input sign. This allows the\noptimization process to quickly move the network predictions to large values when needed. The\nsymlog function approximates the identity around the origin so that it does not affect learning of\ntargets that are already small enough.\nFor potentially stochastic targets, such as rewards or returns, we introduce the symexp twohot loss.\nHere, the network outputs the logits for a softmax distribution over exponentially spaced bins bi ∈B.\nPredictions are read out as the weighted average of the bin positions weighted by their predicted\nprobabilities. Importantly, the network can output any continuous value in the interval because the\nweighted average can fall between the buckets:\nˆy .= softmax(f(x))TB\nB .= symexp(\n\u0002\n−20\n...\n+20\n\u0003\n)\n(10)\nThe network is trained on twohot encoded targets8,28, a generalization of onehot encoding to\ncontinuous values. The twohot encoding of a scalar is a vector with |B| entries that are all 0 except\nat the indices k and k + 1 of the two bins closest to the encoded scalar. The two entries sum up\nto 1, with linearly higher weight given to the bin that is closer to the encoded continuous number.\nThe network is then trained to minimize the categorical cross entropy loss for classification with\nsoft targets. Note that the loss only depends on the probabilities assigned to the bins but not on the\ncontinuous values associated with the bin locations, decoupling the size of the gradients from the\nsize of the targets:\nL(θ) .= −twohot(y)T log softmax(f(x, θ))\n(11)\nApplying these principles, Dreamer transforms vector observations using the symlog functions, both\nfor the encoder inputs and the decoder targets and employs the synexp twohot loss for the reward\npredictor and critic. We find that these techniques enable robust and fast learning across many\ndiverse domains. For critic learning, an alternative asymmetric transformation has previously been\nproposed35, which we found less effective on average across domains. Unlike alternatives, symlog\ntransformations avoid truncating large targets7, introducing non-stationary from normalization5, or\nadjusting network weights when new extreme values are detected36.\n7\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nAgents with\n   item (%)   \nIron Ingot\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nIron Pickaxe\n0M\n50M\n100M\nEnv steps\n0\n25\n50\n75\n100\nDiamond\nDreamer\nIMPALA\nRainbow\nPPO\nFigure 5: Fraction of trained agents that discover each of the three latest items in the Minecraft\nDiamond task. Although previous algorithms progress up to the iron pickaxe, Dreamer is the only\ncompared algorithm that manages to discover a diamond, and does so reliably.\nResults\nWe evaluate the generality of Dreamer across 8 domains—with over 150 tasks—under fixed hyper-\nparameters. We designed the experiments to compare Dreamer to the best methods in the literature,\nwhich are often specifically designed and tuned for the benchmark at hand. We further compare to a\nhigh-quality implementation of PPO5, a standard reinforcement learning algorithm that is known for\nits robustness. We run PPO with fixed hyperparameters chosen to maximize performance across\ndomains and that reproduce strong published results of PPO on ProcGen37. To push the boundaries\nof reinforcement learning, we apply Dreamer to the challenging video game Minecraft, comparing\nit to strong previous algorithms. Finally, we analyze the importance of individual components of\nDreamer and its robustness to different model sizes and computational budgets. All Dreamer agents\nare trained on a single Nvidia A100 GPU each, making it reproducible for many research labs. A\npublic implementation of Dreamer that reproduces all results is available on the project website.\nBenchmarks We perform an extensive empirical study across 8 domains that include continuous\nand discrete actions, visual and low-dimensional inputs, dense and sparse rewards, different reward\nscales, 2D and 3D worlds, and procedural generation. Figure 1 summarizes the benchmark results,\nshowing that Dreamer outperforms a wide range of previous expert algorithms across diverse\ndomains. Crucially, Dreamer substantially outperforms PPO across all domains.\n• Atari\nThis established benchmark contains 57 Atari 2600 games with a budget of 200M frames,\nposing a diverse range of challenges38. We use the sticky action simulator setting39. Dreamer\noutperforms the powerful MuZero algorithm8 while using only a fraction of the computational\nresources. Dreamer also outperforms the widely-used expert algorithms Rainbow40 and IQN41.\n• ProcGen\nThis benchmark of 16 games features randomized levels and visual distractions to test\nthe robustness and generalization of agents42. Within the budget of 50M frames, Dreamer matches\nthe tuned expert algorithm PPG37 and outperforms Rainbow42,40. Our PPO agent with fixed\nhyperparameters matches the published score of the highly tuned official PPO implementation37.\n• DMLab\nThis suite of 30 tasks features 3D environments that test spatial and temporal reason-\ning43. In 100M frames, Dreamer exceeds the performance of the scalable IMPALA and R2D2+\nagents35 at 1B environment steps, amounting to a data-efficiency gain of over 1000%. We note\nthat these baselines were not designed for data-efficiency but serve as a valuable comparison point\nfor the performance previously achievable at scale.\n8\n0\n50\n100\nEnv steps (%)\n0\n50\n100\nReturn (%)\n14 task mean\nDreamer\nNo obs symlog\nNo retnorm (advnorm)\nNo symexp twohot (Huber)\nNo KL balance & free bits\nWithout all\n0\n20\n40\nEnv steps (106)\n0\n10\n20\nReturn\nCrafter\n400M\n200M\n100M\n50M\n25M\n12M\n0\n100\n200\nEnv steps (106)\n0\n250\n500\nDMLab Goals\n0\n50\n100\nEnv steps (%)\n0\n50\n100\nReturn (%)\n14 task mean\nDreamer\nNo reward & value grads\nNo reconstruction grads\n0\n10\n20\nEnv steps (106)\n0\n9\n18\nReturn\nCrafter\n64\n32\n16\n8\n4\n2\n1\n0\n100\n200\nEnv steps (106)\n0\n150\n300\n450\nDMLab Goals\nRobustness techniques\nLearning signals\nModel size scaling\nReplay scaling\na\nb\nc\nd\nFigure 6: Ablations and robust scaling of Dreamer. a, All individual robustness techniques\ncontribute to the performance of Dreamer on average, although each individual technique may only\naffect some tasks. Training curves of individual tasks are included in the supplementary material.\nb, The performance of Dreamer predominantly rests on the unsupervised reconstruction loss of its\nworld model, unlike most prior algorithms that rely predominantly on reward and value prediction\ngradients7,5,8. c, The performance of Dreamer increases monotonically with larger model sizes,\nranging from 12M to 400M parameters. Notably, larger models not only increase task performance\nbut also require less environment interaction. d, Higher replay ratios predictably increase the\nperformance of Dreamer. Together with model size, this allows practitioners to improve task\nperformance and data-efficiency by employing more computational resources.\n• Atari100k\nThis data-efficiency benchmark comntains 26 Atari games and a budget of only\n400K frames, amounting to 2 hours of game time17. EfficientZero44 holds the state-of-the-art by\ncombining online tree search, prioritized replay, and hyperparameter scheduling, but also resets\nlevels early to increase data diversity, making a comparison difficult. Without this complexity,\nDreamer outperforms the best remaining methods, including the transformer-based IRIS and\nTWM agents, the model-free SPR, and SimPLe45.\n• Proprio Control\nThis benchmark contains 18 control tasks with continuous actions, proprio-\nceptive vector inputs, and a budget of 500K environment steps46. The tasks range from classical\ncontrol over locomotion to robot manipulation tasks, featuring dense and sparse rewards. Dreamer\nsets a new state-of-the-art on this benchmark, outperforming D4PG, DMPO, and MPO33.\n• Visual Control\nThis benchmark consists of 20 continuous control tasks where the agent receives\nonly high-dimensional images as input and has a budget of 1M environment steps46. Dreamer\nestablishes a new state-of-the-art on this benchmark, outperforming DrQ-v2 and CURL47, which\nare specialized to visual environments and leverage data augmentation.\n9\n• BSuite\nThis benchmark includes 23 environments with a total of 468 configurations that\nare specifically designed to test credit assignment, robustness to reward scale and stochasticity,\nmemory, generalization, and exploration48. Dreamer establishes a new state-of-the-art on this\nbenchmark, outperforming Boot DQN and other methods49. Dreamer improves over previous\nalgorithms especially in the scale robustness category.\nMinecraft Collecting diamonds in the popular game Minecraft has been a long-standing challenge\nin artificial intelligence18,19,20. Every episode in this game is set in a unique randomly generated\nand infinite 3D world. Episodes last until the player dies or up to 36000 steps equaling 30 minutes,\nduring which the player needs to discover a sequence of 12 items from sparse rewards by foraging\nfor resources and crafting tools. It takes about 20 minutes for experienced human players to obtain\ndiamonds20. We follow the block breaking setting of prior work19 because the provided action space\nwould make it challenging for stochastic policies to keep a key pressed for a prolonged time.\nBecause of the training time in this complex domain, extensive tuning would be difficult for\nMinecraft. Instead, we apply Dreamer out of the box with its default hyperparameters. As shown in\nFigures 1 and 5, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without\nusing human data as was required by VPT20 or adaptive curricula19. All the Dreamer agents we\ntrained on Minecraft discover diamonds in 100M environment steps. While several strong baselines\nprogress to advanced items such as the iron pickaxe, none of them discovers a diamond.\nAblations In Figure 6, we ablate the robustness techniques and learning signals on a diverse set of\n14 tasks to understand their importance. The training curves of individual tasks are included in the\nsupplementary material. We observe that all robustness techniques contribute to performance, most\nnotably the KL objective of the world model, followed by return normalization and symexp twohot\nregression for reward and value prediction. In general, we find that each individual technique is\ncritical on a subset of tasks but may not affect performance on other tasks.\nTo investigate the effect of the world model, we ablate the learning signals of Dreamer by stopping\neither the task-specific reward and value prediction gradients or the task-agnostic reconstruction\ngradients from shaping its representations. Unlike previous reinforcement learning algorithms that\noften rely only on task-specific learning signals7,8, Dreamer rests predominantly on the unsupervised\nobjective of its world model. This finding could allow for future algorithm variants that leverage\npretraining on unsupervised data.\nScaling properties To investigate whether Dreamer can scale robustly, we train 6 model sizes\nranging from 12M to 400M parameters, as well as different replay ratios on Crafter50 and a DMLab\ntask43. The replay ratio affects the number of gradient updates performed by the agent. Figure 6\nshows robust learning with fixed hyperparameters across the compared model sizes and replay ratios.\nMoreover, increasing the model size directly translates to both higher task performance and a lower\ndata requirement. Increasing the number of gradient steps further reduces the interactions needed to\nlearn successful behaviors. The results show that Dreamer learns robustly across model sizes and\nreplay ratios and that its performance and provides a predictable way for increasing performance\ngiven computational resources.\n10\nPrevious work\nDeveloping general-purpose algorithms has long been a goal of reinforcement learning research.\nPPO5 is one of the most widely used algorithms and is relatively robust but requires large amounts\nof experience and often yields lower performance than specialized alternatives. SAC32 is a popular\nchoice for continuous control and leverages experience replay for data-efficiency, but in practice\nrequires tuning, especially for its entropy scale, and struggles under high-dimensional inputs51.\nMuZero8 plans using a value prediction model and has been applied to board games and Atari, but the\nauthors did not release an implementation and the algorithm contains several complex components,\nmaking it challenging to reproduce. Gato52 fits one large model to expert demonstrations of multiple\ntasks, but is only applicable when expert data is available. In comparison, Dreamer masters a\ndiverse range of environments with fixed hyperparameters, does not require expert data, and its\nimplementation is open source.\nMinecraft has been a focus of recent research. With MALMO53, Microsoft released a free version\nof the successful game for research purposes. MineRL18 offers several competition environments,\nwhich we rely on as the basis for our experiments. The MineRL competition supports agents in\nexploring and learning meaningful skills through a diverse human dataset18. Voyager obtains items\nat a similar depth in the technology tree as Dreamer using API calls to a language model but operates\non top of the MineFlayer bot scripting layer that was specifically engineered to the game and\nexposes high-level actions54. VPT20 trained an agent to play Minecraft through behavioral cloning\nbased on expert data of keyboard and mouse actions collected by contractors and finetuning using\nreinforcement learning to obtain diamonds using 720 GPUs for 9 days. In comparison, Dreamer\nuses the MineRL competition action space to autonomously learn to collect diamonds from sparse\nrewards using 1 GPU for 9 days, without human data.\nConclusion\nWe present the third generation of the Dreamer algorithm, a general reinforcement learning algorithm\nthat masters a wide range of domains with fixed hyperparameters. Dreamer excels not only across\nover 150 tasks but also learns robustly across varying data and compute budgets, moving reinforce-\nment learning toward a wide range of practical applications. Applied out of the box, Dreamer is\nthe first algorithm to collect diamonds in Minecraft from scratch, achieving a significant milestone\nin the field of artificial intelligence. As a high-performing algorithm that is based on a learned\nworld model, Dreamer paves the way for future research directions, including teaching agents world\nknowledge from internet videos and learning a single world model across domains to allow artificial\nagents to build up increasingly general knowledge and competency.\nAcknowledgements We thank Mohammad Norouzi, Jessy Lin, Abbas Abdolmaleki, John Schul-\nman, Adam Kosiorek, and Oleh Rybkin for insightful discussions. We thank Bobak Shahriari, Denis\nYarats, Karl Cobbe, and Hubert Soyer for sharing training curves of baseline algorithms. We thank\nDaniel Furrer, Andrew Chen, and Dakshesh Garambha for help with Google Cloud infrastructure.\n11\nReferences\n1. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van\nDen Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot,\net al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):\n484, 2016.\n2. OpenAI. OpenAI Five. https:\/\/blog.openai.com\/openai-five\/, 2018.\n3. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in neural information processing systems,\n35:27730–27744, 2022.\n4. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi.\nCoderl: Mastering code generation through pretrained models and deep reinforcement learning.\nAdvances in Neural Information Processing Systems, 35:21314–21328, 2022.\n5. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n6. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\n7. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n8. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,\nSimon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering\natari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265,\n2019.\n9. Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo,\nDavid Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary\ntasks. arXiv preprint arXiv:1611.05397, 2016.\n10. Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, and R Devon\nHjelm. Unsupervised state representation learning in atari. Advances in neural information\nprocessing systems, 32, 2019.\n11. Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, and Marc Toussaint. Reinforcement\nlearning with neural radiance fields. arXiv preprint arXiv:2206.01634, 2022.\n12. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of\ngo without human knowledge. Nature, 550(7676):354, 2017.\n13. Marcin Andrychowicz, Anton Raichuk, Piotr Sta´nczyk, Manu Orsini, Sertan Girgin, Raphael\nMarinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What\nmatters in on-policy reinforcement learning? a large-scale empirical study. arXiv preprint\narXiv:2006.05990, 2020.\n14. Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSIGART Bulletin, 2(4):160–163, 1991.\n12\n15. Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017\nIEEE International Conference on Robotics and Automation (ICRA), pages 2786–2793. IEEE,\n2017.\n16. David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\n17. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad\nCzechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-\nbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.\n18. William H Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie\nMilani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, et al.\nThe minerl competition on sample efficient reinforcement learning using human priors. arXiv\ne-prints, pages arXiv–1904, 2019.\n19. Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton,\nRaul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, et al. Multi-task\ncurriculum learning in a complex, visual, hard-exploration domain: Minecraft. arXiv preprint\narXiv:2106.14876, 2021.\n20. Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. arXiv preprint arXiv:2206.11795, 2022.\n21. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:\nLearning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.\n22. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with\ndiscrete world models. arXiv preprint arXiv:2010.02193, 2020.\n23. Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n24. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee,\nand James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint\narXiv:1811.04551, 2018.\n25. Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n26. Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.\nImproved variational inference with inverse autoregressive flow. Advances in neural information\nprocessing systems, 29, 2016.\n27. Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on\nimages. arXiv preprint arXiv:2011.10650, 2020.\n28. Marc G Bellemare, Will Dabney, and Rémi Munos.\nA distributional perspective on\nreinforcement learning. In International Conference on Machine Learning, pages 449–458.\nPMLR, 2017.\n29. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n30. Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement\nlearning algorithms. Connection Science, 3(3):241–268, 1991.\n13\n31. Ronald J Williams.\nSimple statistical gradient-following algorithms for connectionist\nreinforcement learning. Machine learning, 8(3-4):229–256, 1992.\n32. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018.\n33. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and\nMartin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920,\n2018.\n34. J Beau W Webber. A bi-symmetric log transformation for wide-range data. Measurement\nScience and Technology, 24(2):027001, 2012.\n35. Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent\nexperience replay in distributed reinforcement learning. In International conference on learning\nrepresentations, 2018.\n36. Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado\nvan Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 33, pages 3796–3803, 2019.\n37. Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In\nInternational Conference on Machine Learning, pages 2020–2027. PMLR, 2021.\n38. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artificial Intelligence\nResearch, 47:253–279, 2013.\n39. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and\nMichael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open\nproblems for general agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.\n40. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will\nDabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining\nimprovements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial\nIntelligence, 2018.\n41. Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for\ndistributional reinforcement learning. In International conference on machine learning, pages\n1096–1105. PMLR, 2018.\n42. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation\nto benchmark reinforcement learning. In International conference on machine learning, pages\n2048–2056. PMLR, 2020.\n43. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich\nKüttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv\npreprint arXiv:1612.03801, 2016.\n44. Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari\ngames with limited data. Advances in Neural Information Processing Systems, 34:25476–25488,\n2021.\n45. Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world\nmodels. arXiv preprint arXiv:2209.00588, 2022.\n14\n46. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David\nBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite.\narXiv preprint arXiv:1801.00690, 2018.\n47. Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous\ncontrol: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645,\n2021.\n48. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva,\nKatrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for\nreinforcement learning. arXiv preprint arXiv:1908.03568, 2019.\n49. Olivia Dizon-Paradis, Stephen Wormald, Daniel Capecci, Avanti Bhandarkar, and Damon\nWoodard. Investigating the practicality of existing reinforcement learning algorithms: A\nperformance comparison. Authorea Preprints, 2023.\n50. Danijar Hafner.\nBenchmarking the spectrum of agent capabilities.\narXiv preprint\narXiv:2109.06780, 2021.\n51. Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus.\nImproving sample efficiency in model-free reinforcement learning from images. arXiv preprint\narXiv:1910.01741, 2019.\n52. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.\nA generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n53. Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for\nartificial intelligence experimentation. In IJCAI, pages 4246–4247. Citeseer, 2016.\n54. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023.\n55. Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun\nWang. The 37 implementation details of proximal policy optimization. The ICLR Blog Track\n2023, 2022.\n56. Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani,\nTamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A\nresearch framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979,\n2020.\n57. Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared\nexperience replay. In International Conference on Machine Learning, pages 8545–8554. PMLR,\n2020.\n58. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\narXiv preprint arXiv:1511.05952, 2015.\n59. Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale\nimage recognition without normalization. In International Conference on Machine Learning,\npages 1059–1071. PMLR, 2021.\n60. Liu Ziyin, Zhikang T Wang, and Masahito Ueda. Laprop: Separating momentum and adaptivity\nin adam. arXiv preprint arXiv:2002.04839, 2020.\n15\n61. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n62. Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare,\nand Remi Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement\nlearning. arXiv preprint arXiv:1704.04651, 2017.\n63. Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n64. Matthijs Van Keirsbilck, Alexander Keller, and Xiaodong Yang. Rethinking full connectivity in\nrecurrent neural networks. arXiv preprint arXiv:1905.12340, 2019.\n65. Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and\nMichael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open\nproblems for general agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.\n66. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward,\nYotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl\nwith importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.\n16\nMethods\nBaselines\nWe employ the Proximal Policy Optimization (PPO) algorithm5, which has become a standard\nchoice in the field, to compare Dreamer under fixed hyperparameters across all benchmarks. There\nare a large number of PPO implementations available publicly and they are known to substantially\nvary in task performance55. To ensure a comparison that is representative of the highest performance\nPPO can achieve under fixed hyperparameters across domains, we choose the high-quality PPO\nimplementation available in the Acme framework56 and select its hyperparameters in Table 1\nfollowing recommendations55,13 and additionally tune its epoch batch size to be large enough for\ncomplex environments42, its learning rate, and its entropy scale. We match the discount factor to\nDreamer because it works well across domains and is a common choice in the literature35,8. We\nchoose the IMPALA network architecture that we have found performed better than alternatives42\nand set the minibatch size to the largest possible for one A100 GPU. We verify the performance of\nour PPO implementation and hyperparameters on the ProcGen benchmark, where a highly tuned\nPPO implementation has been reported by the PPO authors37. We find that our implementation\nmatches or slightly outperforms this performance reference.\nParameter\nValue\nObservation normalization\nYes\nReward normalization\nYes\nReward clipping (stddev.)\n10\nEpoch batch\n64 × 256\nNumber of epochs\n3\nMinibatch size\n8\nMinibatch length\n256\nPolicy trust region\n0.2\nValue trust region\nNo\nAdvantage normalization\nYes\nEntropy penalty scale\n0.01\nDiscount factor\n0.997\nGAE lambda\n0.95\nLearning rate\n3 × 10−4\nGradient clipping\n0.5\nAdam epsilon\n10−5\nTable 1: PPO hyperparameters used across all benchmarks.\nFor Minecraft, we additionally tune and run the IMPALA and Rainbow algorithms because not\nsuccessful end-to-end learning from scratch has been reported in the literature18. We use the Acme\nimplementations56 of these algorithms, use the same IMPALA network we used for PPO, and tuned\nthe learning rate and entropy regularizers. For all other benchmarks, we compare to tuned expert\nalgorithms reported in the literature as referenced in the results section.\n17\nImplementation\nExperience replay We implement Dreamer using a uniform replay buffer with an online queue57.\nSpecifically, each minibatch is formed first from non-overlapping online trajectories and then filled\nup with uniformly sampled trajectories from the replay buffer. We store latent states into the replay\nbuffer during data collection to initialize the world model on replayed trajectories, and write the\nfresh latent states of the training rollout back into the buffer. While prioritized replay58 is used by\nsome of the expert algorithms we compare to and we found it to also improve the performance of\nDreamer, we opt for uniform replay in our experiments for ease of implementation.\nWe parameterize the amount of training via the replay ratio. This is the fraction of time steps trained\non per time step collected from the environment, without action repeat. Dividing the replay ratio by\nthe time steps in a minibatch and by action repeat yields the ratio of gradient steps to env steps. For\nexample, a replay ratio of 32 on Atari with action repeat of 4 and batch shape 16 × 64 corresponds\nto 1 gradient step every 128 env steps, or 1.5M gradient steps over 200M env steps.\nOptimizer We employ Adaptive Gradient Clipping (AGC)59, which clips per-tensor gradients if\nthey exceed 30% of the L2 norm of the weight matrix they correspond to, with its default ϵ = 10−3.\nAGC decouples the clipping threshold from the loss scales, allowing to change loss functions or\nloss scales without adjusting the clipping threshold. We apply the clipped gradients using the\nLaProp optimizer60 with ϵ = 10−20 and its default parameters β1 = 0.9 and β2 = 0.99. LaProp\nnormalizes gradients by RMSProp and then smoothes them by momentum, instead of computing\nboth momentum and normalizer on raw gradients as Adam does61. This simple change allows for a\nsmaller epsilon and avoids occasional instabilities that we observed under Adam.\nDistributions The encoder, dynamics predictor, and actor distributions are mixtures of 99% the\npredicted softmax output and 1% of a uniform distribution62 to prevent zero probabilities and\ninfinite log probabilities. The rewards and critic neural networks output a softmax distribution over\nexponentially spaced bins b ∈B and are trained towards twohot encoded targets:\ntwohot(x)i .=\n\n\n\n\n\n|bk+1 −x| \/ |bk+1 −bk|\nif i = k\n|bk\n−x| \/ |bk+1 −bk|\nif i = k + 1\n0\nelse\nk .=\n|B|\nX\nj=1\nδ(bj < x)\n(12)\nThe output weights of twohot distributions are initialized to zero to ensure that the agent does\nnot hallucinate rewards and values at initialization. For computing the expected prediction of the\nsoftmax distribution under bins that span many orders of magnitude, the summation order matters\nand positive and negative bins should be summed up separately, from small to large bins, and then\nadded. Refer to the source code for an implementation.\nNetworks Images are encoded using stride 2 convolutions to resolution 6 × 6 or 4 × 4 and then\nflattened and are decoded using transposed stride 2 convolutions, with sigmoid activation on the\noutput. Vector inputs are symlog transformed and then encoded and decoded using 3-layer MLPs.\nThe actor and critic neural networks are also 3-layer MLPs and the reward and continue predictors\nare 1-layer MLPs. The sequence model is a GRU63 with block-diagonal recurrent weights64 of 8\nblocks to allow for a large number of memory units without quadratic increase in parameters and\nFLOPs. The input to the GRU at each time step is a linear embedding of the sampled latent zt, of\nthe action at, and of the recurrent state to allow mixing between blocks.\n18\nBenchmarks\nProtocols Summarized in Table 2, we follow the standard evaluation protocols for the benchmarks\nwhere established. Atari38 uses 57 tasks with sticky actions65. The random and human reference\nscores used to normalize scores vary across the literature and we chose the most common reference\nvalues, replicated in Table 6. DMLab43 uses 30 tasks66 and we use the fixed action space36,35. We\nevaluate at 100M steps because running for 10B as in some prior work was infeasible. Because\nexisting published baselines perform poorly at 100M steps, we compare to their performance at 1B\nsteps instead, giving them a 10× data advantage. ProcGen uses the hard difficulty setting and the\nunlimited level set42. Prior work compares at different step budgets42,37 and we compare at 50M\nsteps due to computational cost, as there is no action repeat. For Minecraft Diamond purely from\nsparse rewards, we establish the evaluation protocol to report the episode return measured at 100M\nenv steps, corresponding to about 100 days of in-game time. Atari100k17 includes 26 tasks with\na budget of 400K env steps, 100K after action repeat. Prior work has used various environment\nsettings, summarized in Table 10, and we chose the environments as originally introduced. Visual\nControl46,21 spans 20 tasks with an action repeat of 2. Proprioceptive Control follows the same\nprotocol but we exclude the two quadruped tasks because of baseline availability in prior work47.\nBenchmark\nTasks\nEnv\nSteps\nAction\nRepeat\nEnv\nInstances\nReplay\nRatio\nGPU\nDays\nModel\nSize\nMinecraft\n1\n100M\n1\n64\n32\n8.9\n200M\nDMLab\n30\n100M\n4\n16\n32\n2.9\n200M\nProcGen\n16\n50M\n1\n16\n64\n16.1\n200M\nAtari\n57\n200M\n4\n16\n32\n7.7\n200M\nAtari100K\n26\n400K\n4\n1\n128\n0.1\n200M\nBSuite\n23\n—\n1\n1\n1024\n0.5\n200M\nProprio Control\n18\n500K\n2\n16\n512\n0.3\n12M\nVisual Control\n20\n1M\n2\n16\n512\n0.1\n12M\nTable 2: Benchmark overview. All agents were trained on a single Nvidia A100 GPU each.\nEnvironment instances In earlier experiments, we observed that the performance of both Dreamer\nand PPO is robust to the number of environment instances. Based on the CPU resources available\non our training machines, we use 16 environment instance by default. For BSuite, the benchmark\nrequires using a single environment instance. We also use a single environment instance for\nAtari100K because the benchmark has a budget of 400K env steps whereas the maximum episode\nlength in Atari is in principle 432K env steps. For Minecraft, we use 64 environments using remote\nCPU workers to speed up experiments because the environment is slower to step.\nSeeds and error bars We run 5 seeds for each Dreamer and PPO per benchmark, with the exception\nof 1 seed for ProcGen due to computational constraints, 10 seeds for BSuite as required by the\nbenchmark, and 10 seeds for Minecraft to reliably report the fraction of runs that achieve diamonds.\nAll curves show the mean over seeds with one standard deviation shaded.\nComputational choices All Dreamer and PPO agents in this paper were trained on a single Nvidia\nA100 GPU each. Dreamer uses the 200M model size by default. On the two control suitse, Dreamer\nthe same performance using the substantially faster 12M model, making it more accessible to\nresearchers. The replay ratio control the trade-off between computational cost and data efficiency as\nanalyzed in Figure 6 and is chosen to fit the step budget of each benchmark.\n19\nModel sizes\nTo accommodate different computational budgets and analyze robustness to different model sizes,\nwe define a range of models ranging from 12M to 400M parameters shown in Table 3. The sizes\nare parameterized by the model dimension, which approximately increases in multiples of 1.5,\nalternating between powers of two and power of two scaled by 1.5. This yields tensor shapes that\nare multiples of 8 as required for hardware efficiency. Sizes of different network components derive\nfrom the model dimension. The MLPs have the model dimension as the number of hidden units.\nThe sequence model has 8 times the number of recurrent units, split into 8 blocks of the same size as\nthe MLPs. The convolutional encoder and decoder layers closest to the data use 16× fewer channels\nthan the model dimension. Each latent also uses 16× fewer codes than the model dimension. The\nnumber of hidden layers and number of latents is fixed across model sizes. All hyperparamters,\nincluding the learning rate and batch size, are fixed across model sizes.\nParameters\n12M\n25M\n50M\n100M\n200M\n400M\nHidden size (d)\n256\n384\n512\n768\n1024\n1536\nRecurrent units (8d)\n1024\n3072\n4096\n6144\n8192\n12288\nBase CNN channels (d\/16)\n16\n24\n32\n48\n64\n96\nCodes per latent (d\/16)\n16\n24\n32\n48\n64\n96\nTable 3: Dreamer model sizes. The number of MLP hidden units defines the model dimension,\nfrom which recurrent units, convolutional channels, and number of codes per latent are derived. The\nnumber of layers and latents is constant across model sizes.\nPrevious Dreamer generations\nWe present the third generation of the Dreamer line of work. Where the distinction is useful, we refer\nto this algorithm as DreamerV3. The DreamerV1 algorithm21 was limited to continuous control,\nthe DreamerV2 algorithm22 surpassed human performance on Atari, and the DreamerV3 algorithm\nenables out-of-the-box learning across diverse benchmarks.\nWe summarize the changes that DreamerV3 introduces as follows:\n• Robustness techniques: Observation symlog, KL balance and free bits, 1% unimix for all categor-\nicals, percentile return normalization, symexp twohot loss for the reward head and critic.\n• Network architecture: Block GRU, RMSNorm normalization, SiLu activation.\n• Optimizer: Adaptive gradient clipping (AGC), LaProp (RMSProp followed by momentum).\n• Replay buffer: Larger capacity, online queue, storing and updating latent states.\n20\nHyperparameters\nName\nSymbol\nValue\nGeneral\nReplay capacity\n—\n5 × 106\nBatch size\nB\n16\nBatch length\nT\n64\nActivation\n—\nRMSNorm + SiLU\nLearning rate\n—\n4 × 10−5\nGradient clipping\n—\nAGC(0.3)\nOptimizer\n—\nLaProp(ϵ = 10−20)\nWorld Model\nReconstruction loss scale\nβpred\n1\nDynamics loss scale\nβdyn\n1\nRepresentation loss scale\nβrep\n0.1\nLatent unimix\n—\n1%\nFree nats\n—\n1\nActor Critic\nImagination horizon\nH\n15\nDiscount horizon\n1\/(1 −γ)\n333\nReturn lambda\nλ\n0.95\nCritic loss scale\nβval\n1\nCritic replay loss scale\nβrepval\n0.3\nCritic EMA regularizer\n—\n1\nCritic EMA decay\n—\n0.98\nActor loss scale\nβpol\n1\nActor entropy regularizer\nη\n3 × 10−4\nActor unimix\n—\n1%\nActor RetNorm scale\nS\nPer(R, 95) −Per(R, 5)\nActor RetNorm limit\nL\n1\nActor RetNorm decay\n—\n0.99\nTable 4: Dreamer hyperparameters. The same values are used across all benchmarks, including\nproprioceptive and visual inputs, continuous and discrete actions, and 2D and 3D domains. We do\nnot use any hyperparameter annealing, prioritized replay, weight decay, or dropout.\n21\nMinecraft\nGame description With 100M monthly active users, Minecraft is one of the most popular video\ngames worldwide. Minecraft features a procedurally generated 3D world of different biomes,\nincluding plains, forests, jungles, mountains, deserts, taiga, snowy tundra, ice spikes, swamps,\nsavannahs, badlands, beaches, stone shores, rivers, and oceans. The world consists of 1 meter sized\nblocks that the player and break and place. There are about 30 different creatures that the player\ncan interact with or fight. From gathered resources, the player can use over 350 recipes to craft\nnew items and progress through the technology tree, all while ensuring safety and food supply to\nsurvive. There are many conceivable tasks in Minecraft and as a first step, the research community\nhas focused on the salient task of obtaining a diamonds, a rare item found deep underground and\nrequires progressing through the technology tree.\nLearning environment We built the Minecraft Diamond environment on top of MineRL to define a\nflat categorical action space and fix issues we discovered with the original environments via human\nplay testing. For example, when breaking diamond ore, the item sometimes jumps into the inventory\nand sometimes needs to be collected from the ground. The original environment terminates episodes\nwhen breaking diamond ore so that many successful episodes end before collecting the item and thus\nwithout the reward. We remove this early termination condition and end episodes when the player\ndies or after 36000 steps, corresponding to 30 minutes at the control frequency of 20Hz. Another\nissue is that the jump action has to be held for longer than one control step to trigger a jump, which\nwe solve by keeping the key pressed in the background for 200ms. We built the environment on top\nof MineRL v0.4.418, which offers abstract crafting actions. The Minecraft version is 1.11.2.\nObservations and rewards The agent observes a 64 × 64 × 3 first-person image, an inventory\ncount vector for the over 400 items, a vector of maximum inventory counts since episode begin\nto tell the agent which milestones it has achieved, a one-hot vector indicating the equipped item,\nand scalar inputs for the health, hunger, and breath levels. We follow the sparse reward structure of\nthe MineRL competition environment18 that rewards 12 milestones leading up to the diamond, for\nobtaining the items log, plank, stick, crafting table, wooden pickaxe, cobblestone, stone pickaxe,\niron ore, furnace, iron ingot, iron pickaxe, and diamond. The reward for each item is only given\nonce per episode, and the agent has to learn to collect certain items multiple times to achieve the\nnext milestone. To make the return easy to interpret, we give a reward of +1 for each milestone\ninstead of scaling rewards based on how valuable each item is. Additionally, we give −0.01 for each\nlost heart and +0.01 for each restored heart, but did not investigate whether this is helpful.\n22\nSupplementary material\nMinecraft video predictions\nTrue\nContext Input\nOpen Loop Prediction\nModel\nTrue\nModel\nTrue\nModel\nTrue\nModel\nTrue\nT = 0\nModel\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nFigure 7: Multi-step predictions on Minecraft. The world model receives the first 5 frames as\ncontext input and the predicts 45 steps into the future given the action sequence and without access\nto intermediate images.\n23\nMinecraft additional results\nMethod\nReturn\nDreamer\n9.1\nIMPALA\n7.1\nRainbow\n6.3\nPPO\n5.1\nTable 5: Minecraft Diamond scores at 100M\nenvironment steps.\n0\n25M 50M 75M 100M\nEnv steps\n0\n4\n8\n12\nReturn\nMinecraft Diamond\nDreamer\nIMPALA\nRainbow\nPPO\nFigure 8: Minecraft learning curves.\n0\n25\n50\n75\n100\n% of episodes\nLog\n0\n25\n50\n75\n100\nPlanks\n0\n25\n50\n75\n100\nStick\n0\n25\n50\n75\n100\nCrafting Table\n0\n25\n50\n75\n100\n% of episodes\nWooden Pickaxe\n0\n25\n50\n75\n100\nCobblestone\n0\n25\n50\n75\n100\nFurnace\n0\n25\n50\n75\n100\nStone Pickaxe\n100K 1M\n10M 100M\nEnv steps\n0\n20\n40\n60\n80\n% of episodes\nIron Ore\n100K 1M\n10M 100M\nEnv steps\n0\n20\n40\n60\n80\nIron Ingot\n100K 1M\n10M 100M\nEnv steps\n0\n6\n12\n18\n24\nIron Pickaxe\n100K 1M\n10M 100M\nEnv steps\n0\n0.2\n0.5\n0.8\n1\nDiamond\nDreamer\nIMPALA\nRainbow\nPPO\nFigure 9: Item success rates as a percentage of episodes. Dreamer obtains items at substantially\nhigher rates than the baselines and continues to improve until the 100M step budget. At the budget,\nDreamer obtains diamonds in 0.4% of episodes, leaving a challenge for future research. This metric\ndiffers from Figure 5, which shows that over the course of training, 100% of Dreamer agents obtain\none or more diamonds regardless of episode boundaries, compared to 0% of the baseline agents.\n24\nAtari learning curves\n0\n25K\n50K\n75K\nReturn\nAlien\n0\n2K\n4K\nAmidar\n0\n15K\n30K\n45K\nAssault\n0\n300K\n600K\n900K\nAsterix\n0\n150K\n300K\nAsteroids\n0\n500K\n1M\n1.5M\nAtlantis\n0\n400\n800\n1.2K\nReturn\nBank Heist\n0\n150K\n300K\n450K\nBattle Zone\n0\n80K\n160K\nBeam Rider\n0\n8K\n16K\n24K\nBerzerk\n80\n160\n240\nBowling\n0\n30\n60\n90\nBoxing\n0\n250\n500\n750\nReturn\nBreakout\n0\n250K\n500K\n750K\nCentipede\n0\n300K\n600K\n900K\nChopperComm\n0\n60K\n120K\n180K\nCrazy Climber\n0\n200K\n400K\n600K\nDefender\n0\n50K\n100K\n150K\nDemon Attack\n-15\n0\n15\n30\nReturn\nDouble Dunk\n0\n800\n1.6K\n2.4K\nEnduro\n-60\n0\n60\nFishing Derby\n0\n15\n30\nFreeway\n0\n25K\n50K\n75K\nFrostbite\n0\n40K\n80K\n120K\nGopher\n0\n4K\n8K\n12K\nReturn\nGravitar\n0\n15K\n30K\n45K\nHero\n0\n25\n50\n75\nIce Hockey\n0\n8K\n16K\n24K\nJamesbond\n0\n4K\n8K\n12K\nKangaroo\n0\n30K\n60K\n90K\nKrull\n0\n50K\n100K\n150K\nReturn\nKung Fu Master\n0\n1K\n2K\n3K\nMontezumaRev\n0\n20K\n40K\n60K\nMs Pacman\n0\n40K\n80K\n120K\nName This Game\n0\n150K\n300K\nPhoenix\n-300\n-150\n0\nPitfall\n-15\n0\n15\nReturn\nPong\n0\n15K\n30K\n45K\nPrivate Eye\n0\n150K\n300K\nQbert\n0\n40K\n80K\n120K\nRiverraid\n0\n200K\n400K\n600K\nRoad Runner\n0\n40\n80\n120\nRobotank\n0\n300K\n600K\nReturn\nSeaquest\n-32K\n-24K\n-16K\nSkiing\n2K\n4K\n6K\n8K\nSolaris\n0\n10K\n20K\n30K\nSpace Invaders\n0\n250K\n500K\n750K\nStar Gunner\n-8\n0\n8\nSurround\n-24\n-16\n-8\n0\nReturn\nTennis\n0\n150K\n300K\nTime Pilot\n0\n150\n300\nTutankham\n0\n250K\n500K\n750K\nUp N Down\n0\n600\n1.2K\n1.8K\nVenture\n0\n300K\n600K\n900K\nVideo Pinball\n0\n200M\nEnv steps\n0\n40K\n80K\nReturn\nWizard Of Wor\n0\n200M\nEnv steps\n0\n300K\n600K\n900K\nYars Revenge\n0\n200M\nEnv steps\n0\n25K\n50K\n75K\nZaxxon\n0\n200M\nEnv steps\n0\n10\n20\n30\nGamer Mean\n0\n200M\nEnv steps\n0\n4\n8\nGamer Median\n0\n200M\nEnv steps\n0\n0.2\n0.4\nRecord Mean Cap\nDreamer\nMuZero\nPPO\nFigure 10: Atari learning curves.\n25\nAtari scores\nTask\nRandom\nGamer\nRecord\nPPO\nMuZero\nDreamer\nEnvironment steps\n—\n—\n—\n200M\n200M\n200M\nAlien\n228\n7128\n251916\n5476\n56835\n10977\nAmidar\n6\n1720\n104159\n817\n1517\n3612\nAssault\n222\n742\n8647\n6673\n42742\n26010\nAsterix\n210\n8503\n1000000\n47190\n879375\n441763\nAsteroids\n719\n47389\n10506650\n2479\n374146\n348684\nAtlantis\n12850\n29028\n10604840\n539721\n1353617\n1553222\nBank Heist\n14\n753\n82058\n946\n1077\n1083\nBattle Zone\n2360\n37188\n801000\n27816\n167412\n419653\nBeam Rider\n364\n16926\n999999\n7973\n201154\n37073\nBerzerk\n124\n2630\n1057940\n1186\n1698\n10557\nBowling\n23\n161\n300\n118\n133\n250\nBoxing\n0\n12\n100\n98\n100\n100\nBreakout\n2\n30\n864\n299\n799\n384\nCentipede\n2091\n12017\n1301709\n51833\n774421\n554553\nChopper Command\n811\n7388\n999999\n12667\n8945\n802698\nCrazy Climber\n10780\n35829\n219900\n93176\n184394\n193204\nDefender\n2874\n18689\n6010500\n38270\n554492\n579875\nDemon Attack\n152\n1971\n1556345\n8229\n142509\n142109\nDouble Dunk\n–19\n–16\n22\n16\n23\n24\nEnduro\n0\n860\n9500\n1887\n2369\n2166\nFishing Derby\n–92\n–39\n71\n43\n58\n82\nFreeway\n0\n30\n38\n33\n0\n34\nFrostbite\n65\n4335\n454830\n1123\n17087\n41888\nGopher\n258\n2412\n355040\n24792\n122025\n87600\nGravitar\n173\n3351\n162850\n3436\n10301\n12570\nHero\n1027\n30826\n1000000\n31967\n36063\n40677\nIce Hockey\n–11\n1\n36\n12\n26\n57\nJamesbond\n29\n303\n45550\n1019\n14872\n24010\nKangaroo\n52\n3035\n1424600\n7769\n14380\n12229\nKrull\n1598\n2666\n104100\n9193\n11476\n69858\nKung Fu Master\n258\n22736\n1000000\n32335\n148936\n154893\nMontezuma Revenge\n0\n4753\n1219200\n2368\n0\n1852\nMs Pacman\n307\n6952\n290090\n7041\n51310\n24079\nName This Game\n2292\n8049\n25220\n19441\n85331\n77809\nPhoenix\n761\n7243\n4014440\n31412\n105593\n316606\nPitfall\n–229\n6464\n114000\n–2\n0\n0\nPong\n–21\n15\n21\n19\n21\n20\nPrivate Eye\n25\n69571\n101800\n73\n100\n26432\nQbert\n164\n13455\n2400000\n14554\n102129\n201084\nRiverraid\n1338\n17118\n1000000\n14860\n137983\n48080\nRoad Runner\n12\n7845\n2038100\n423995\n604083\n150402\nRobotank\n2\n12\n76\n63\n70\n132\nSeaquest\n68\n42055\n999999\n1927\n399764\n356584\nSkiing\n–17098\n–4337\n–3272\n–29926\n–30000\n–29965\nSolaris\n1236\n12327\n111420\n2368\n5860\n5851\nSpace Invaders\n148\n1669\n621535\n3489\n3639\n15005\nStar Gunner\n664\n10250\n77400\n53439\n127417\n408961\nSurround\n–10\n6\n6\n6\n9\n9\nTennis\n–24\n–8\n21\n–1\n0\n–3\nTime Pilot\n3568\n5229\n65300\n17250\n427209\n314947\nTutankham\n11\n168\n5384\n225\n235\n395\nUp N Down\n533\n11693\n82840\n83743\n522962\n614065\nVenture\n0\n1188\n38900\n953\n0\n0\nVideo Pinball\n16257\n17668\n89218328\n382306\n775304\n940631\nWizard Of Wor\n564\n4756\n395300\n10910\n0\n99136\nYars Revenge\n3093\n54577\n15000105\n137164\n846061\n675774\nZaxxon\n32\n9173\n83700\n13599\n58115\n78443\nGamer median (%)\n0\n100\n3716\n180\n693\n830\nGamer mean (%)\n0\n100\n123001\n892\n3054\n3381\nRecord mean (%)\n0\n13\n100\n21\n66\n74\nRecord mean capped (%)\n0\n13\n100\n21\n34\n38\nTable 6: Atari scores.\n26\nProcGen learning curves\n0\n8\n16\n24\n32\nReturn\nBigfish\n0\n3\n6\n9\n12\nBossfight\n3\n6\n9\nCaveflyer\n2\n4\n6\n8\nChaser\n3\n6\n9\nReturn\nClimber\n6\n8\n10\nCoinrun\n3\n6\n9\nDodgeball\n0\n6\n12\n18\n24\nFruitbot\n2\n4\n6\n8\nReturn\nHeist\n2\n4\n6\n8\nJumper\n2\n4\n6\n8\nLeaper\n2\n4\n6\n8\nMaze\n4\n8\n12\nReturn\nMiner\n0\n25M\n50M\nEnv steps\n2\n4\n6\n8\n10\nNinja\n0\n25M\n50M\nEnv steps\n5\n10\n15\n20\n25\nPlunder\n0\n25M\n50M\nEnv steps\n0\n8\n16\n24\nStarpilot\n0\n25M\n50M\nEnv steps\n0\n0.2\n0.3\n0.5\n0.6\nReturn\nNormalized Mean\nDreamer\nPPG\nPPO\nFigure 11: ProcGen learning curves.\n27\nProcGen scores\nTask\nOriginal PPO\nPPO\nPPG\nDreamer\nEnvironment steps\n50M\n50M\n50M\n50M\nBigfish\n10.92\n12.72\n31.26\n8.62\nBossfight\n10.47\n9.36\n11.46\n11.61\nCaveflyer\n6.03\n6.71\n10.02\n9.42\nChaser\n4.48\n3.54\n8.57\n5.49\nClimber\n7.59\n9.04\n10.24\n11.43\nCoinrun\n7.93\n6.71\n8.98\n9.86\nDodgeball\n4.80\n3.44\n10.31\n10.93\nFruitbot\n20.28\n21.69\n24.32\n11.04\nHeist\n2.25\n6.87\n3.77\n8.51\nJumper\n5.09\n6.13\n5.84\n9.17\nLeaper\n5.90\n4.07\n8.76\n7.05\nMaze\n4.97\n7.86\n7.06\n6.85\nMiner\n7.56\n12.97\n9.08\n5.71\nNinja\n6.16\n3.62\n9.38\n9.82\nPlunder\n11.16\n3.99\n13.44\n23.81\nStarpilot\n17.00\n10.13\n21.57\n28.00\nNormalized mean\n41.16\n42.80\n64.89\n66.01\nTable 7: ProcGen scores. The PPO implementation we use throughout our paper under fixed\nhyperparameters performs on par or better than the original PPO, which its authors describe as\nhighly tuned with near optimal hyperparameters37.\n28\nDMLab learning curves\n0\n50\n100\n150\nReturn\nExplore\nGoals Large\n0\n150\n300\n450\nExplore\nGoals Small\n20\n40\n60\nExplore\nObjects Large\n0\n30\n60\n90\nExplore\nObjects Small\n0\n15\n30\n45\nExplore\nRewards Few\n20\n40\n60\nReturn\nExplore\nRewards Many\n0\n25\n50\n75\nExplore\nObstructed Large\n0\n80\n160\n240\nExplore\nObstructed Small\n0\n80\n160\n240\nLanguage\nQuantitative\n0\n80\n160\n240\nLanguage\nExecute\n0\n250\n500\nReturn\nLanguage\nDescribed\n0\n200\n400\n600\nLanguage\nLocated\n-0.3\n-0.2\n0\nLasertag\nOne Large\n-0.2\n-0.1\n0\n0.1\nLasertag\nOne Small\n0\n4\n8\n12\nLasertag\nThree Large\n0\n8\n16\n24\nReturn\nLasertag\nThree Small\n15\n30\n45\nNatlab\nFixed Large\n16\n24\n32\nNatlab\nRandomized\n5\n10\n15\n20\nNatlab\nRegrowth\n8\n16\n24\n32\nPsychlab\nVisumotor\n24\n28\n32\n36\nReturn\nPsychlab\nContinuous\n0\n15\n30\n45\nPsychlab\nSequential\n0\n25\n50\n75\nPsychlab\nVisual Search\n0\n3\n6\n9\nRooms\nCollect Good\n10\n20\n30\n40\nRooms\nExploit Deferred\n15\n30\n45\nReturn\nRooms\nKeys Doors\n0\n50M 100M\nEnv steps\n0\n25\n50\n75\nRooms\nSelect Nonmatching\n0\n50M 100M\nEnv steps\n8\n16\n24\n32\nRooms\nWatermaze\n0\n50M 100M\nEnv steps\n0\n25\n50\n75\nSkymaze\nHard\n0\n50M 100M\nEnv steps\n25\n50\n75\nSkymaze\nVaried\n0\n50M 100M\nEnv steps\n0.2\n0.4\n0.6\nReturn\nHuman Mean Cap\nDreamer\nIMPALA\nPPO\nFigure 12: DMLab learning curves.\n29\nDMLab scores\nTask\nR2D2+\nIMPALA\nIMPALA\nIMPALA\nPPO\nDreamer\nEnvironment steps\n10B\n10B\n1B\n100M\n100M\n100M\nExplore Goal Locations Large\n174.7\n316.0\n137.8\n64.2\n21.2\n116.7\nExplore Goal Locations Small\n460.7\n482.0\n302.8\n196.1\n115.1\n372.8\nExplore Object Locations Large\n60.6\n91.0\n55.1\n34.3\n22.5\n63.9\nExplore Object Locations Small\n83.7\n100.4\n75.9\n50.6\n38.9\n93.5\nExplore Object Rewards Few\n80.7\n92.6\n46.9\n34.6\n19.0\n40.0\nExplore Object Rewards Many\n75.8\n89.4\n68.5\n53.3\n25.5\n58.1\nExplore Obstructed Goals Large\n95.5\n102.0\n57.9\n23.7\n12.9\n52.8\nExplore Obstructed Goals Small\n311.9\n372.0\n214.9\n118.0\n70.4\n224.6\nLanguage Answer Quantitative Question\n344.4\n362.0\n304.7\n1.0\n0.3\n266.0\nLanguage Execute Random Task\n497.4\n465.4\n140.8\n–44.4\n–2.5\n223.7\nLanguage Select Described Object\n617.6\n664.0\n618.4\n0.2\n526.9\n665.5\nLanguage Select Located Object\n772.8\n731.4\n413.0\n38.2\n397.5\n679.5\nLasertag One Opponent Large\n0.0\n0.0\n0.0\n–0.1\n0.0\n–0.1\nLasertag One Opponent Small\n31.8\n0.0\n0.0\n–0.1\n0.0\n0.0\nLasertag Three Opponents Large\n28.6\n32.2\n10.4\n–0.1\n0.0\n9.0\nLasertag Three Opponents Small\n49.0\n57.2\n37.1\n12.1\n0.4\n18.8\nNatlab Fixed Large Map\n60.6\n63.4\n53.8\n13.0\n15.8\n50.5\nNatlab Varying Map Randomized\n42.4\n47.0\n40.5\n35.3\n29.6\n31.2\nNatlab Varying Map Regrowth\n24.6\n34.0\n25.5\n15.3\n16.3\n16.7\nPsychlab Arbitrary Visuomotor Mapping\n33.1\n38.4\n16.5\n11.9\n16.0\n30.7\nPsychlab Continuous Recognition\n30.0\n28.6\n30.0\n30.1\n30.5\n33.8\nPsychlab Sequential Comparison\n30.0\n29.6\n0.0\n0.0\n30.0\n44.3\nPsychlab Visual Search\n79.9\n80.0\n0.0\n0.0\n76.6\n40.1\nRooms Collect Good Objects Test\n9.9\n10.0\n9.9\n9.3\n9.7\n9.9\nRooms Exploit Deferred Effects Test\n38.1\n62.2\n37.6\n34.5\n39.0\n40.4\nRooms Keys Doors Puzzle\n46.2\n54.6\n36.9\n24.2\n26.0\n42.4\nRooms Select Nonmatching Object\n63.6\n39.0\n63.2\n4.0\n2.7\n73.1\nRooms Watermaze\n49.0\n47.0\n50.1\n23.6\n21.2\n26.1\nSkymaze Irreversible Path Hard\n76.0\n80.0\n46.4\n7.7\n0.0\n80.9\nSkymaze Irreversible Path Varied\n76.0\n100.0\n69.8\n32.7\n31.3\n87.7\nHuman mean capped (%)\n85.4\n85.1\n66.3\n31.0\n35.9\n71.4\nTable 8: DMLab scores at 100M environment steps and larger budgets. The IMPALA agent\ncorresponds to “IMPALA (deep)” presented by Kapturowski et al. 35 who made the learning curves\navailable.\n30\nAtari100k learning curves\n400\n800\n1.2K\nReturn\nAlien\n0\n40\n80\n120\nAmidar\n200\n400\n600\n800\nAssault\n400\n800\n1.2K\nAsterix\n0\n200\n400\n600\nReturn\nBank Heist\n0\n8K\n16K\n24K\nBattle Zone\n0\n30\n60\n90\nBoxing\n3\n6\n9\n12\nBreakout\n800\n1.6K\n2.4K\nReturn\nChopper Command\n0\n40K\n80K\n120K\nCrazy Climber\n0\n400\n800\n1.2K\nDemon Attack\n0\n3\n6\nFreeway\n0\n2K\n4K\n6K\nReturn\nFrostbite\n0\n1.5K\n3K\n4.5K\nGopher\n0\n5K\n10K\n15K\nHero\n0\n250\n500\n750\nJamesbond\n0\n2K\n4K\n6K\nReturn\nKangaroo\n2.5K\n5K\n7.5K\nKrull\n0\n15K\n30K\nKung Fu Master\n600\n1.2K\n1.8K\nMs Pacman\n-16\n-8\n0\nReturn\nPong\n0\n2K\n4K\nPrivate Eye\n0\n1.5K\n3K\nQbert\n0\n8K\n16K\n24K\nRoad Runner\n0\n200K\n400K\nEnv steps\n0\n400\n800\n1.2K\nReturn\nSeaquest\n0\n200K\n400K\nEnv steps\n0\n60K\n120K\n180K\nUp N Down\n0\n200K\n400K\nEnv steps\n0\n0.4\n0.8\n1.2\nGamer Mean\n0\n200K\n400K\nEnv steps\n0\n0.2\n0.3\n0.5\nGamer Median\nDreamer\nPPO\nFigure 13: Atari100k learning curves.\n31\nAtari100k scores\nTask\nRandom Human\nPPO\nSimPLe\nSPR\nTWM\nIRIS\nDreamer\nEnvironment steps\n—\n—\n400K\n400K\n400K\n400K\n400K\n400K\nAlien\n228\n7128\n276\n617\n842\n675\n420\n1118\nAmidar\n6\n1720\n26\n74\n180\n122\n143\n97\nAssault\n222\n742\n327\n527\n566\n683\n1524\n683\nAsterix\n210\n8503\n292\n1128\n962\n1117\n854\n1062\nBank Heist\n14\n753\n14\n34\n345\n467\n53\n398\nBattle Zone\n2360\n37188\n2233\n4031\n14834\n5068\n13074\n20300\nBoxing\n0\n12\n3\n8\n36\n78\n70\n82\nBreakout\n2\n30\n3\n16\n20\n20\n84\n10\nChopper Command\n811\n7388\n1005\n979\n946\n1697\n1565\n2222\nCrazy Climber\n10780\n35829\n14675\n62584\n36700\n71820\n59324\n86225\nDemon Attack\n152\n1971\n160\n208\n518\n350\n2034\n577\nFreeway\n0\n30\n2\n17\n19\n24\n31\n0\nFrostbite\n65\n4335\n127\n237\n1171\n1476\n259\n3377\nGopher\n258\n2412\n368\n597\n661\n1675\n2236\n2160\nHero\n1027\n30826\n2596\n2657\n5859\n7254\n7037\n13354\nJamesbond\n29\n303\n41\n100\n366\n362\n463\n540\nKangaroo\n52\n3035\n55\n51\n3617\n1240\n838\n2643\nKrull\n1598\n2666\n3222\n2205\n3682\n6349\n6616\n8171\nKung Fu Master\n258\n22736\n2090\n14862\n14783\n24555\n21760\n25900\nMs Pacman\n307\n6952\n366\n1480\n1318\n1588\n999\n1521\nPong\n–21\n15\n–20\n13\n–5\n19\n15\n–4\nPrivate Eye\n25\n69571\n100\n35\n86\n87\n100\n3238\nQbert\n164\n13455\n317\n1289\n866\n3331\n746\n2921\nRoad Runner\n12\n7845\n602\n5641\n12213\n9109\n9615\n19230\nSeaquest\n68\n42055\n305\n683\n558\n774\n661\n962\nUp N Down\n533\n11693\n1502\n3350\n10859\n15982\n3546\n46910\nGamer mean (%)\n0\n100\n11\n33\n62\n96\n105\n125\nGamer median (%)\n0\n100\n2\n13\n40\n51\n29\n49\nTable 9: Atari100k scores at 400K environment steps, corresponding to 100k agent steps.\nSetting\nSimPLe\nEffMuZero\nSPR\nIRIS\nTWM\nDreamer\nGamer score (%)\n33\n190\n62\n105\n96\n125\nGamer median (%)\n13\n109\n40\n29\n51\n49\nGPU days\n5.0\n0.6\n0.1\n3.5\n0.4\n0.1\nOnline planning\n—\nX\n—\n—\n—\n—\nData augmentation\n—\n—\nX\n—\n—\n—\nNon-uniform replay\n—\nX\nX\n—\nX\n—\nSeparate hparams\n—\n—\n—\nX\n—\n—\nIncreased resolution\n—\nX\nX\n—\n—\n—\nUses life information\n—\nX\nX\nX\nX\n—\nUses early resets\n—\nX\n—\nX\n—\n—\nSeparate eval episodes\nX\nX\nX\nX\nX\n—\nTable 10: Evaluation protocols for the Atari 100k benchmark. Computational resources are\nconverted to A100 GPU days. EfficientMuZero44 achieves the highest scores but changed the envi-\nronment configuration from the standard17. IRIS uses a separate hyperparameter for its exploration\nstrength on Freeway.\n32\nProprioceptive control learning curves\n0\n60\n120\n180\n240\nReturn\nAcrobot Swingup\n0\n300\n600\n900\nBall In Cup Catch\n400\n600\n800\n1K\nCartpole Balance\n0\n250\n500\n750\n1K\nCartpole Bal. Sparse\n0\n200\n400\n600\n800\nReturn\nCartpole Swingup\n0\n250\n500\n750\nCartpole Sw. Sparse\n0\n200\n400\n600\nCheetah Run\n0\n250\n500\n750\n1K\nFinger Spin\n0\n250\n500\n750\n1K\nReturn\nFinger Turn Easy\n0\n300\n600\n900\nFinger Turn Hard\n0\n40\n80\n120\n160\nHopper Hop\n0\n250\n500\n750\n1K\nHopper Stand\n0\n250\n500\n750\n1K\nReturn\nPendulum Swingup\n0\n250\n500\n750\n1K\nReacher Easy\n0\n250\n500\n750\n1K\nReacher Hard\n0\n200\n400\n600\nWalker Run\n0\n250K\n500K\nEnv steps\n250\n500\n750\n1K\nReturn\nWalker Stand\n0\n250K\n500K\nEnv steps\n0\n250\n500\n750\n1K\nWalker Walk\n0\n250K\n500K\nEnv steps\n200\n400\n600\nTask Mean\n0\n250K\n500K\nEnv steps\n0\n200\n400\n600\n800\nTask Median\nDreamer\nD4PG\nDMPO\nPPO\nFigure 14: DeepMind Control Suite learning curves under proprioceptive inputs.\n33\nProprioceptive control scores\nTask\nPPO\nDDPG\nDMPO\nD4PG\nDreamer\nEnvironment steps\n500K\n500K\n500K\n500K\n500K\nAcrobot Swingup\n6\n100\n103\n124\n134\nBall In Cup Catch\n632\n917\n968\n968\n962\nCartpole Balance\n523\n997\n999\n999\n990\nCartpole Balance Sparse\n930\n992\n999\n974\n990\nCartpole Swingup\n240\n864\n860\n875\n852\nCartpole Swingup Sparse\n7\n703\n438\n752\n491\nCheetah Run\n82\n596\n650\n624\n614\nFinger Spin\n18\n775\n769\n823\n931\nFinger Turn Easy\n281\n499\n620\n612\n793\nFinger Turn Hard\n106\n313\n495\n421\n889\nHopper Hop\n0\n36\n68\n80\n113\nHopper Stand\n3\n484\n549\n762\n576\nPendulum Swingup\n1\n767\n834\n759\n788\nReacher Easy\n494\n934\n961\n960\n954\nReacher Hard\n288\n949\n968\n937\n938\nWalker Run\n31\n561\n493\n616\n649\nWalker Stand\n159\n965\n975\n947\n964\nWalker Walk\n64\n952\n942\n969\n936\nTask mean\n94\n771\n801\n792\n871\nTask median\n215\n689\n705\n733\n754\nTable 11: DeepMind Control Suite scores under proprioceptive inputs.\n34\nVisual control learning curves\n0\n80\n160\n240\nReturn\nAcrobot Swingup\n0\n250\n500\n750\n1K\nBall In Cup Catch\n400\n600\n800\n1K\nCartpole Balance\n0\n250\n500\n750\n1K\nCartpole Bal. Sparse\n0\n200\n400\n600\n800\nReturn\nCartpole Swingup\n0\n250\n500\n750\nCartpole Sw. Sparse\n0\n200\n400\n600\n800\nCheetah Run\n0\n250\n500\n750\n1K\nFinger Spin\n200\n400\n600\n800\nReturn\nFinger Turn Easy\n0\n250\n500\n750\nFinger Turn Hard\n0\n100\n200\n300\nHopper Hop\n0\n250\n500\n750\n1K\nHopper Stand\n0\n250\n500\n750\nReturn\nPendulum Swingup\n150\n300\n450\n600\nQuadruped Run\n0\n200\n400\n600\n800\nQuadruped Walk\n0\n250\n500\n750\n1K\nReacher Easy\n0\n250\n500\n750\nReturn\nReacher Hard\n0\n200\n400\n600\n800\nWalker Run\n0\n500K\n1M\nEnv steps\n250\n500\n750\n1K\nWalker Stand\n0\n500K\n1M\nEnv steps\n0\n250\n500\n750\n1K\nWalker Walk\n0\n500K\n1M\nEnv steps\n200\n400\n600\n800\nReturn\nTask Mean\n0\n500K\n1M\nEnv steps\n0\n200\n400\n600\n800\nTask Median\nDreamer\nDrQ-v2\nCURL\nPPO\nFigure 15: DeepMind Control Suite learning curves under visual inputs.\n35\nVisual control scores\nTask\nPPO\nSAC\nCURL\nDrQ-v2\nDreamer\nEnvironment steps\n1M\n1M\n1M\n1M\n1M\nAcrobot Swingup\n3\n4\n4\n166\n229\nBall In Cup Catch\n829\n176\n970\n928\n972\nCartpole Balance\n516\n937\n980\n992\n993\nCartpole Balance Sparse\n881\n956\n999\n987\n964\nCartpole Swingup\n290\n706\n771\n863\n861\nCartpole Swingup Sparse\n1\n149\n373\n773\n759\nCheetah Run\n95\n20\n502\n716\n836\nFinger Spin\n118\n291\n880\n862\n589\nFinger Turn Easy\n253\n200\n340\n525\n878\nFinger Turn Hard\n79\n94\n231\n247\n904\nHopper Hop\n0\n0\n164\n221\n227\nHopper Stand\n4\n5\n777\n903\n903\nPendulum Swingup\n1\n592\n413\n843\n744\nQuadruped Run\n88\n54\n149\n450\n617\nQuadruped Walk\n112\n49\n121\n726\n811\nReacher Easy\n487\n67\n689\n944\n951\nReacher Hard\n94\n7\n472\n670\n862\nWalker Run\n30\n27\n360\n539\n684\nWalker Stand\n161\n143\n486\n978\n976\nWalker Walk\n87\n40\n822\n768\n961\nTask mean\n94\n81\n479\n770\n861\nTask median\n206\n226\n525\n705\n786\nTable 12: DeepMind Control Suite scores under visual inputs.\n36\nBSuite performance spectrum\nBasic\nCredit Assignment\nExploration\nGeneralization\nMemory\nNoise\nScale\nDreamer\nBoot DQN\nDQN\nPPO\nFigure 16: BSuite scores visualized by category48. Dreamer exceeds previous methods in the\ncategories scale and memory. The scale category measure robustness to reward scales.\n37\nBSuite scores\nTask\nRandom\nPPO\nAC-RNN\nDQN\nBoot DQN Dreamer\nBandit\n0.00\n0.38\n1.00\n0.93\n0.98\n0.96\nBandit Noise\n0.00\n0.61\n0.63\n0.71\n0.80\n0.75\nBandit Scale\n0.00\n0.39\n0.60\n0.74\n0.83\n0.78\nCartpole\n0.04\n0.84\n0.40\n0.85\n0.69\n0.93\nCartpole Noise\n0.04\n0.77\n0.20\n0.82\n0.69\n0.93\nCartpole Scale\n0.04\n0.83\n0.12\n0.72\n0.65\n0.92\nCartpole Swingup\n0.00\n0.00\n0.00\n0.00\n0.15\n0.03\nCatch\n0.00\n0.91\n0.87\n0.92\n0.99\n0.96\nCatch Noise\n0.00\n0.54\n0.27\n0.58\n0.68\n0.53\nCatch Scale\n0.00\n0.90\n0.17\n0.85\n0.65\n0.94\nDeep Sea\n0.00\n0.00\n0.00\n0.00\n1.00\n0.00\nDeep Sea Stochastic\n0.00\n0.00\n0.00\n0.00\n0.90\n0.00\nDiscounting Chain\n0.20\n0.24\n0.39\n0.25\n0.22\n0.40\nMemory Len\n0.00\n0.17\n0.70\n0.04\n0.04\n0.65\nMemory Size\n0.00\n0.47\n0.29\n0.00\n0.00\n0.59\nMnist\n0.05\n0.77\n0.56\n0.85\n0.85\n0.61\nMnist Noise\n0.05\n0.41\n0.22\n0.38\n0.34\n0.34\nMnist Scale\n0.05\n0.76\n0.09\n0.49\n0.31\n0.55\nMountain Car\n0.10\n0.10\n0.10\n0.93\n0.93\n0.92\nMountain Car Noise\n0.10\n0.10\n0.10\n0.89\n0.82\n0.87\nMountain Car Scale\n0.10\n0.10\n0.10\n0.85\n0.56\n0.90\nUmbrella Distract\n0.00\n1.00\n0.09\n0.30\n0.26\n0.74\nUmbrella Length\n0.00\n0.87\n0.43\n0.39\n0.39\n0.78\nBasic\n0.04\n0.60\n0.58\n0.90\n0.89\n0.88\nCredit assignment\n0.03\n0.76\n0.37\n0.59\n0.56\n0.75\nExploration\n0.00\n0.00\n0.00\n0.00\n0.68\n0.01\nGeneralization\n0.06\n0.47\n0.19\n0.68\n0.60\n0.70\nMemory\n0.00\n0.32\n0.49\n0.02\n0.02\n0.62\nNoise\n0.02\n0.54\n0.24\n0.51\n0.61\n0.62\nScale\n0.04\n0.60\n0.22\n0.73\n0.60\n0.82\nTask mean (%)\n3\n49\n32\n54\n60\n66\nCategory mean (%)\n3\n47\n30\n49\n57\n63\nTable 13: BSuite scores for each task averaged over environment configurations, as well as aggre-\ngated performance by category and over all tasks.\n38\nRobustness ablations\n0\n10M\n20M\n0\n400K\n800K\n1.2M\nReturn\nAtari\nAtlantis\n0\n10M\n20M\n0\n100\n200\n300\nAtari\nBreakout\n0\n10M\n20M\n0\n1K\n2K\nAtari\nMontezuma\n0\n2.5M\n5M\n4\n8\n12\n16\nCrafter\nReward\n0\n10M\n20M\n0\n100\n200\n300\nReturn\nDMLab\nGoals Small\n0\n10M\n20M\n0\n25\n50\nDMLab\nNonmatching\n0\n5M\n10M\n0\n600\n1.2K\n1.8K\nPinPad\nFive\n0\n5M\n10M\n0\n500\n1K\n1.5K\nPinPad\nSix\n0\n5M\n10M\n0\n3\n6\n9\nReturn\nProcGen\nBossfight\n0\n5M\n10M\n2\n4\n6\n8\nProcGen\nCaveflyer\n0\n4M\n0\n150\n300\nProprio Control\nDog Run\n0\n5M\n10M\n0\n300\n600\n900\nProprio Control\nReacher Hard\n0\n5M\n10M\nEnv Steps\n0\n80\n160\n240\nReturn\nVisual Control\nAcrobot Sparse\n0\n4M\nEnv Steps\n0\n40\n80\n120\nVisual Control\nHumanoid Run\n0\n50\n100\nEnv Steps (%)\n0\n25\n50\n75\n100\nReturn (%)\nNormalized\nTask Mean\nDreamer\nNo obs symlog\nNo retnorm (advnorm)\nNo symexp twohot (Huber)\nNo KL balance & free bits\nWithout all\nFigure 17: Individual learning curves for the robustness ablation experiment. All robustness\ntechniques contribute to the overall performance of Dreamer, although each individual technique\nmay only improve the performance on a subset of the tasks.\n39\nLearning signal ablations\n0\n20M\n0\n400K\n800K\n1.2M\nReturn\nAtari\nAtlantis\n0\n20M\n0\n100\n200\n300\nAtari\nBreakout\n0\n20M\n0\n800\n1.6K\n2.4K\nAtari\nMontezuma\n0\n5M\n4\n8\n12\n16\nCrafter\nReward\n0\n20M\n0\n100\n200\n300\nReturn\nDMLab\nGoals Small\n0\n20M\n0\n20\n40\n60\nDMLab\nNonmatching\n0\n10M\n0\n600\n1.2K\n1.8K\nPinPad\nFive\n0\n10M\n0\n500\n1K\nPinPad\nSix\n0\n10M\n0\n3\n6\n9\nReturn\nProcGen\nBossfight\n0\n10M\n2\n4\n6\n8\nProcGen\nCaveflyer\n0\n6M\n0\n100\n200\n300\nProprio Control\nDog Run\n0\n10M\n0\n300\n600\n900\nProprio Control\nReacher Hard\n0\n10M\nEnv Steps\n0\n100\n200\n300\nReturn\nVisual Control\nAcrobot Sparse\n0\n6M\nEnv Steps\n0\n25\n50\n75\nVisual Control\nHumanoid Run\n0\n100\nEnv Steps (%)\n0\n25\n50\n75\n100\nReturn (%)\nNormalized\nTask Mean\nDreamer\nNo reward & value grads\nNo reconstruction grads\nFigure 18: Individual learning curves for the learning signal ablation experiment. Dreamer relies\npredominantly on the undersupervised reconstruction objective of its world model and additional\nreward and value gradients further improve performance on a subset of tasks.\n40\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Mastering Diverse Domains through World Models.pdf"}
{"title":"STEVE-1: A Generative Model for Text-to-Behavior in Minecraft","authors":"Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila McIlraith","summary":"Constructing AI models that respond to text instructions is challenging,\nespecially for sequential decision-making tasks. This work introduces a\nmethodology, inspired by unCLIP, for instruction-tuning generative models of\nbehavior without relying on a large dataset of instruction-labeled\ntrajectories. Using this methodology, we create an instruction-tuned Video\nPretraining (VPT) model called STEVE-1, which can follow short-horizon\nopen-ended text and visual instructions in Minecraft. STEVE-1 is trained in two\nsteps: adapting the pretrained VPT model to follow commands in MineCLIP's\nlatent space, then training a prior to predict latent codes from text. This\nallows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and\nall for only $60 of compute. By leveraging pretrained models like VPT and\nMineCLIP and employing best practices from text-conditioned image generation,\nSTEVE-1 sets a new bar for open-ended instruction-following in Minecraft with\nlow-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game\nevaluation suite. We provide experimental evidence highlighting key factors for\ndownstream performance, including pretraining, classifier-free guidance, and\ndata scaling. All resources, including our model weights, training scripts, and\nevaluation tools are made available for further research.","url":"http:\/\/arxiv.org\/abs\/2306.00937v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2306.00937v3","published":1685641181000,"comment":null,"pdf_text":"STEVE-1: A Generative Model for\nText-to-Behavior in Minecraft\nShalev Lifshitz1,2∗\nshalev.lifshitz@mail.utoronto.ca\nKeiran Paster1,2∗\nkeirp@cs.toronto.edu\nHarris Chan1,2†\nhchan@cs.toronto.edu\nJimmy Ba1,2\njba@cs.toronto.edu\nSheila McIlraith1,2\nsheila@cs.toronto.edu\n1Department of Computer Science, University of Toronto, Toronto, Canada.\n2Vector Institute for Artificial Intelligence, Toronto, Canada.\nAbstract\nConstructing AI models that respond to text instructions is challenging, especially\nfor sequential decision-making tasks. This work introduces a methodology, inspired\nby unCLIP, for instruction-tuning generative models of behavior without relying\non a large dataset of instruction-labeled trajectories. Using this methodology, we\ncreate an instruction-tuned Video Pretraining (VPT) model called STEVE-1, which\ncan follow short-horizon open-ended text and visual instructions in Minecraft™.\nSTEVE-1 is trained in two steps: adapting the pretrained VPT model to follow com-\nmands in MineCLIP’s latent space, then training a prior to predict latent codes from\ntext. This allows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and all\nfor only $60 of compute. By leveraging pretrained models like VPT and MineCLIP\nand employing best practices from text-conditioned image generation, STEVE-1\nsets a new bar for open-ended instruction-following in Minecraft with low-level\ncontrols (mouse and keyboard) and raw pixel inputs, far outperforming previous\nbaselines and robustly completing 12 of 13 tasks in our early-game evaluation\nsuite. We provide experimental evidence highlighting key factors for downstream\nperformance, including pretraining, classifier-free guidance, and data scaling. All\nresources, including our model weights, training scripts, and evaluation tools are\nmade available for further research.\n1\nIntroduction\nThe ability to use text instructions to control and interact with powerful AI models has made these\nmodels accessible and customizable for the masses. Such models include ChatGPT [41], which\ncan respond to messages written in natural language and perform a wide array of tasks, and Stable\nDiffusion [50], which turns natural language into an image. While those models cost anywhere from\nhundreds of thousands to hundreds of millions of dollars to train, there has been an equally exciting\ntrend whereby powerful open-source foundation models like LLaMA [59] can be finetuned with\nsurprisingly little compute and data to become instruction-following (e.g., [58, 13]).\nIn this paper, we study whether such an approach could be applicable to sequential decision-making\ndomains. Unlike in text and image domains, diverse data for sequential decision-making is very\nexpensive and often does not come with a convenient “instruction” label like captions for images. We\npropose to instruction-tune pretrained generative models of behavior, mirroring the advancements\nseen in recent instruction-tuned LLMs like Alpaca [58], and without relying on a large dataset of\ninstruction-labeled trajectories.\n∗Equal contribution.\n†Core contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.00937v3  [cs.AI]  4 Feb 2024\nText\nEncoder\n“chop a  tree”\nCVAE\nVideo\nencoder\nCLIP   Objective\nGaussian Prior\nTransformer (VPT)\nMineCLIP \n(Frozen)\nlinear\nPrior\nPolicy\nResNet\nResNet\nResNet\nMouse\/Keyboard\n\/\/\nFigure 1: Like unCLIP [48], our approach involves two models. First, we train the policy by finetuning\nVPT to achieve goals given by pretrained MineCLIP [17] visual embeddings using our gameplay\ndataset. Second, for the prior model, we train a CVAE [54] to sample MineCLIP visual embeddings\ngiven a text prompt. The combination of these two models enables our agent to follow text and visual\ninstructions.\nIn the past year, two foundation models for the popular open-ended video game Minecraft™were\nreleased: a foundation model for behavior called VPT [5] and a model aligning text and video\nclips called MineCLIP [17]. This has opened up an intriguing avenue to explore fine-tuning for\ninstruction-following in the sequential decision-making domain of Minecraft. VPT was trained on\n70k hours of Minecraft gameplay, so the agent already has vast knowledge about the Minecraft\nenvironment. However, just as the massive potential of LLMs was unlocked by aligning them\nto follow instructions, it is likely that the VPT model has the potential for general, controllable\nbehavior if it is finetuned to follow instructions. In particular, our paper demonstrates a method for\nfine-tuning VPT to follow short-horizon text instructions with only $60 of compute and around 2,000\ninstruction-labeled trajectory segments.\nOur method draws inspiration from unCLIP [48], the approach used to create the popular text-to-\nimage model DALL•E 2. We decompose the problem of creating an instruction-following Minecraft\nagent into two models: a VPT model finetuned to achieve visual goals embedded in the MineCLIP\nlatent space, and a prior model that translates text instructions into MineCLIP visual embeddings. We\nfinetune VPT using behavioral cloning with self-supervised data generated with hindsight relabeling\n[3], avoiding the use of expensive text-instruction labels in favor of visual MineCLIP embeddings.\nWe apply unCLIP with classifier-free guidance [23] to create our agent called STEVE-1, which sets\na new bar for open-ended instruction-following in Minecraft with low-level controls (mouse and\nkeyboard) and raw pixel inputs, far outperforming the baseline set by Baker et al. [5].\nOur main contributions are as follows:\n• We introduce a methodology, inspired by unCLIP [48], for instruction-tuning generative\nmodels of behavior without relying on a large dataset of expensive instruction labels.\n• We apply this methodology to create STEVE-1, a Minecraft agent that can follow short-\nhorizon open-ended text and visual instructions with a high degree of accuracy, all for only\n$60 of compute. We perform extensive evaluations of our agent, showing that it can robustly\ncomplete 12 of 13 goal-conditioned control tasks in our early-game evaluation suite in\nMinecraft. For long-horizon tasks1 like crafting and building, we show that a basic version\nof prompt chaining can dramatically improve performance.\n• We provide experimental evidence highlighting key factors for downstream performance,\nincluding pretraining, classifier-free guidance, data scaling, prompt-engineering, and other\ndesign choices. In particular, we show that unCLIP [48] and classifier-free guidance [23]\ntranslate well to sequential decision-making and are essential for strong performance.\n• We release model weights for STEVE-1 as well as training scripts and evaluation code to\nhelp foster more research into instructable, open-ended sequential decision-making agents.2\n1Short-horizon tasks require few steps: e.g., go to a tree and chop it down, dig a hole. Long-horizon tasks\ntake many steps: e.g., craft complex recipes from scratch, build a house.\n2Model weights, training code, videos, and an interactive demo script are hosted on our project webpage at\nhttps:\/\/sites.google.com\/view\/steve-1.\n2\n2\nRelated Work\nMinecraft as a Test-bed for AI\nMinecraft has gained popularity as a benchmark for AI research\ndue to its complex and dynamic environment, making it a rich test-bed for reinforcement learning\nand other AI methods (e.g., [26, 19, 17, 21, 40, 62, 38, 9]). We leverage the MineRL environment\n[19] to research the creation of agents that can follow open-ended instructions in complex visual\nenvironments using only low-level actions (mouse and keyboard). We build STEVE-1 on top of\ntwo recent foundation models. In order to align text and videos, we use MineCLIP [17], a CLIP\n[47] model trained on paired web videos of Minecraft gameplay and associated captions. To train\nSTEVE-1’s policy, we fine-tune VPT [5], a foundation model of Minecraft behavior that is pretrained\non 70k hours of web videos of Minecraft along with estimated mouse and keyboard actions. Several\nprior works [61, 62] have explored the use of LLMs in creating instructable Minecraft agents. These\nworks typically use LLMs to make high-level plans that are then executed by lower-level RL [40, 62]\nor scripted [46] policies. Since STEVE-1 is a far more flexible low-level policy, the combination\nof STEVE-1 with LLMs is a promising direction for future work. Fan et al. [17] introduced an\nagent trained using RL with MineCLIP as a shaping reward on 12 different tasks and conditioned on\nMineCLIP-embedded text-prompts. However, this agent failed to generalize beyond the original set\nof tasks without further RL finetuning using the MineCLIP reward function. Cai et al. [9] proposed\na Goal-Sensitive Backbone architecture for goal-conditioned control in Minecraft which is trained\non a fixed set of goals, while STEVE-1 learns goal-reaching behavior from a large dataset in a\nself-supervised way without training on an explicit set of tasks.\nFoundation Models for Sequential Decision-Making\nFoundation models which are pretrained on\nvast amounts of data and then finetuned for specific tasks have recently shown great promise in a\nvariety of domains including language [8, 14, 59], vision [48, 10, 47], and robotics [7, 53, 25, 39, 65].\nGATO [49] and RT-1 [7] have demonstrated the potential of training transformers to perform both\nsimulated and real-world robotic tasks. With the exception of Kumar et al. [30], which uses Q-\nlearning, the vast majority of cases [32, 7, 49] where deep learning has been scaled to large, multitask\noffline-RL datasets have used supervised RL. Supervised RL (e.g., [42, 18, 12]) works by framing the\nsequential decision-making problem as a prediction problem, where the model is trained to predict the\nnext action conditioned on some future outcome. While these approaches are simple and scale well\nwith large amounts of compute and data, more work is needed to understand the trade-offs between\nsupervised RL and Q-learning or policy gradient-based methods [43, 44, 6, 55]. Recent works explore\nthe use of hindsight relabeling [3] using vision-language models [47, 2] to produce natural language\nrelabeling instructions. DIAL [65] finetunes CLIP [47] on human-labeled trajectories, which is\nthen used to select a hindsight instruction from a candidate set. Sumers et al. [56] uses Flamingo\n[2] zero-shot for hindsight relabeling by framing it as a visual-question answering (VQA) task. In\ncontrast, STEVE-1 relabels goals using future trajectory segment embeddings given by the MineCLIP\n[17] visual embedding.\nText-Conditioned Generative Models\nThere has been a recent explosion of interest in text-to-X\nmodels, including text-to-image (e.g., [48, 51, 50]), text-to-3D (e.g., [27, 35]), and even text-to-music\n(e.g., [1]). These models are typically either autoregressive transformers modeling sequences of\ndiscrete tokens [60, 8] or diffusion models [24]. Most related to our work is unCLIP, the method used\nfor DALL•E 2 [48]. unCLIP works by training a generative diffusion model to sample images from\nCLIP [47] embeddings of those images. By combining this model with a prior that translates text\nto visual CLIP embeddings, unCLIP can produce photorealistic images for arbitrary text prompts.\nunCLIP and many other diffusion-based approaches utilize a technique called classifier-free guidance\n[23], which lets the model trade-off between mode-coverage and sample fidelity post-training. We\nutilize the basic procedure of unCLIP and classifier-free guidance for training STEVE-1.\n3\nMethod\nInspired by the rapid recent progress in instruction-tuning Large Language Models (LLMs), we\nchoose to leverage the recently released Video Pretraining (VPT) [5] model as a starting point for\nour agent. Since VPT was trained on 70k hours of Minecraft gameplay, the agent already has vast\nknowledge about the Minecraft environment. However, just as the massive potential of LLMs was\nunlocked by aligning them to follow instructions, it is likely that the VPT model has the potential\n3\nz1\nz2\nz2\nz3\nz4\nz5\nz6\nz7\nz2\nz7\nz7\nz7\nz7\nz7\nFigure 2: To create goal-conditioned data for finetuning, we randomly select timesteps from episodes\nand use hindsight relabeling to set the intermediate goals for the trajectory segments to those visual\nMineCLIP embeddings. This self-supervised data teaches the agent which actions lead to which states.\nfor general, controllable behavior if it is finetuned to follow instructions. In this work, we present\na method for finetuning VPT to follow natural, open-ended textual and visual instructions, which\nopens the door for a wide range of uses for VPT in Minecraft.\nOur approach is inspired by unCLIP, the method behind the recent text-to-image generation model,\nDALL•E 2 [48]. Our goal is to create a generative model of behavior in Minecraft conditioned on\ntext instructions y. To do so, we utilize a dataset of Minecraft trajectory segments, some of which\ncontain instruction labels y: [(τ1, y1), (τ2, y2), . . . , (τn, ∅)] where τ is a trajectory of observations\nand actions. We also employ a pretrained CLIP model called MineCLIP [17], which generates aligned\nlatent variables zτt:t+16, zy, where zτt:t+16 is an embedding of any 16 consecutive timesteps from\nthe trajectory. MineCLIP is trained using a contrastive objective on pairs of Minecraft videos and\ntranscripts from the web. For simplicity of notation, we refer to the MineCLIP embedding of the\nlast 16 timesteps of a trajectory segment as zτgoal. Like unCLIP [48], we utilize a hierarchical model\nconsisting of a prior and a policy:\n• A prior p(zτgoal|y) that produces a latent variable zτgoal conditioned on a text instruction y.\n• A policy p(τ|zτgoal) that produces a trajectory conditioned on a latent variable zτgoal.\nThese two models can then be combined to produce a generative model of behaviors conditioned on\ntext instructions:\np(τ|y) = p(τ, zτgoal|y) = p(zτgoal|y)p(τ|zτgoal)\n(3.1)\n3.1\nPolicy\nTo learn our policy, we finetune VPT, a foundation model of Minecraft behaviors pθ(τ) trained\non 70k hours of Minecraft gameplay videos. Specifically, VPT consists of a ResNet [22] that\nprocesses frames of dimension 128 × 128 × 3, and a Transformer-XL [15] which processes the frame\nrepresentations and autoregressively predicts the next action using the joint hierarchical action space\ndescribed in Baker et al. [5]. In order to modify the architecture to condition on goal information, we\nadd an affine transformation of zτgoal to the output of the ResNet before passing it to the transformer:\nProcess Frames:\nResNetθ(ot) →xt\n[+ Conditioning on MineCLIP Embedding Goal]:\nxt →xt + Wθzτgoal + bθ\nPredict Actions:\nTransformerXLθ(xt, . . . , xt+T ) →at+T\nIn order to finetune VPT to condition on goals, we finetune the model using a method inspired by\nsupervised RL approaches like Decision Transformer [12], GLAMOR [42], and GCSL [18]. We use\na modification of hindsight relabeling which we call packed hindsight relabeling (see Figure 2) to\ngenerate a new dataset of trajectories with goals pulled from future states that periodically switch. In\ncontrast with hindsight relabeling, packed hindsight relabeling packs multiple relabeled sequences\ninto a single sequence. Specifically, our method to generate this dataset consists of two steps:\n1. Given a trajectory τ with T timesteps, randomly generate indices to select goals from:\ni1, i2, . . . , in. These indices are chosen by starting at the first timestep and repeatedly\nsampling a new timestep by adding a random value to the previous timestep. This ensures\nthat the data reflects that some goals may take longer to achieve than others.\n2. For each chosen goal at timestep ij, set the goals for timesteps ij−1 + 1, . . . , ij to be the\ngoal at timestep ij, denoted zτij .\n4\nOur final dataset Drelabeled consists of observation sequences (o1, . . . , oT ), action sequences\n(a1, . . . , aT ), and packed hindsight relabeled goals (z1, . . . , zT ). We then finetune VPT on this\ndataset using a supervised loss to predict each action autoregressively using a causal attention mask:\nLpolicy(θ) = EDrelabeled[−log pθ(at|o1...t, z1...t)]\n(3.2)\n3.2\nPrior\nIn order to condition not only on embeddings of visual goals but on latent goals, we need the prior, a\nmodel that produces a latent variable zτgoal conditioned on a text instruction y. Our model is a simple\nconditional variational autoencoder (CVAE) [54, 29] with a Gaussian prior and a Gaussian posterior.\nRather than learn to condition directly on text, we choose to condition on frozen text representations\nfrom MineCLIP zy. Thus, the prior learns a function to translate from a text embedding zy to a visual\nembedding zτgoal (see Appendix C.5 for further discussion). Both the encoder and decoder of our\nCVAE are parameterized as two-layer MLPs with 512 hidden units and layer normalization [4]. We\ntrain the model on our dataset, for which we have text labels Dlabels using the following loss:\nLprior(ϕ) = E(zτgoal,zy)∼Dlabels\nh\nKL(qϕ(zτgoal|zy)∥p(zτgoal)) −Ec∼qϕ(zτgoal|zy)\n\u0002\nlog pϕ(zτgoal|c, zy)\n\u0003i\n(3.3)\n3.3\nDatasets\nTo train our policy, we gather a gameplay dataset with 54M frames (≈1 month at 20FPS) of Minecraft\ngameplay along with associated actions from two sources: contractor gameplay and VPT-generated\ngameplay. To train our prior, we use a small dataset of text-video pairs gathered by humans and\naugmented using the OpenAI API gpt-3.5-turbo model [41] and MineCLIP. See Appendix D for\nmore detailed dataset information.\nOpenAI Contractor Dataset\nWe use 39M frames sourced from the contractor dataset which VPT\n[5] used to train its inverse dynamics model and finetune its policy. The dataset was gathered by\nhiring human contractors to play Minecraft and complete tasks such as house building or obtaining a\ndiamond pickaxe. During gameplay, keypresses and mouse movements are recorded. We use the\nsame preprocessing as VPT, including filtering out null actions.\nVPT-Generated Dataset\nWe generate an additional dataset of 15M frames by generating random\ntrajectories using the various pretrained VPT agents. The diversity of this dataset is improved by\nrandomly switching between models during trajectories [44], randomly resetting the agent’s memory,\nand randomly turning the agent to face a new direction.\nText-Video Pair Dataset\nTo train our prior model, which learns a mapping between text embed-\ndings and visual embeddings, we also manually gather a small dataset of 2,000 text instructions\npaired with 16-frame video segments (less than a second) from our gameplay dataset. This dataset\ncorresponds to less than 30 minutes of gameplay and takes just a few hours to collect. We aug-\nment this dataset by using the alignment between text and video embeddings from MineCLIP. For\neach text instruction, we find the top k most similar gameplay segments in our dataset and use the\ncorresponding 16-frame segment as additional training data. For augmentation, we also add 8,000\ntext-instructions generated by the OpenAI API gpt-3.5-turbo model [41], in addition to our 2,000\nhand-labeled instructions.\n3.4\nInference\nAt inference time, we use the prior to sample a latent goal zτgoal from the text instruction y. We then\nuse the policy to autoregressively sample actions at conditioned on the observation history o1...t\nand the latent goal zτgoal. Similar to the observation in Appendix I of Baker et al. [5], even with\nconditioning, the policy often fails to follow its instruction and simply acts according to its prior\nbehavior. To mitigate this, we borrow another trick used in image generation models: classifier-free\nguidance. Specifically, during inference we simultaneously compute logits for the policy conditioned\non the goal f(ot, . . . , ot+1, zτgoal) and for the unconditional policy f(ot, . . . , ot+1). We then compute\na combination of the two logits using a λ parameter to trade-off between the two:\n5\nlogits = (1 + λ) fθ(ot, . . . , ot+1, zτgoal)\n|\n{z\n}\nconditional logits\n−λ fθ(ot, . . . , ot+1)\n|\n{z\n}\nunconditional logits\n(3.4)\nBy setting a higher value of λ, we can encourage the policy to follow actions that are more likely when\nconditioned on the goal and, as demonstrated in Section 4.5, this significantly improves performance.\nAlso, in order to train the policy to generate these unconditional logits, we occasionally dropout the\ngoal embedding zτgoal from the policy’s input (with probability 0.1). This lets us generate both the\nconditional and unconditional logits using the same model with batch processing at inference time.\n3.5\nEvaluation\nEvaluating the performance of our agent is a challenging task due to the wide variety of instructions\nthat are possible and the difficulty of evaluating whether the agent has successfully achieved its\ntask. We use a combination of programmatic evaluation metrics and automatic MineCLIP evaluation\nmetrics to get a sense of the agent’s capability level. We collectively refer to all of our evaluation tasks\nincluding the 11 evaluation tasks from Figure 3 and the two prompt chaining tasks from Section 4.3\nas our early-game evaluation suite.\nProgrammatic Evaluation\nWe compute programmatic evaluation metrics by monitoring the\nMineRL [19] environment state throughout each evaluation episode. As done in VPT [5], we compute\nmultiple programmatic metrics including travel distance and early-game item collection. The travel\ndistance is the maximum displacement of the agent along on the horizontal (X-Z) plane, measured\nfrom the initial spawn point. For early-game inventory counts, we store the maximum number of log,\nseed, and dirt items seen in the agent’s inventory during the episode.\nMineCLIP Evaluation\nWe explore the use of text-visual alignment in MineCLIP latent space\nbetween trajectories and text or visual goals to evaluate our agent over a wider variety of tasks where\nprogrammatic evaluation isn’t practical. To determine the degree to which a task has been completed\nat all during an evaluation episode, we record the minimum cosine distance between the (text or\nvisual) goal embedding and the visual MineCLIP embedding at any timestep during an episode.\n4\nResults\nIn our experiments, we aim to answer the following questions:\n1. How well does STEVE-1 perform at achieving both text and visual goals in Minecraft?\n2. How does our method scale with more data?\n3. What choices are important for the performance of our method?\n4.1\nTraining Setup\nWe base our implementation off of the official VPT codebase3. The main STEVE-1 agent is trained\nusing Pytorch [45] distributed data parallel on four A40 GPUs for 160M frames, or just under three\nepochs of our gameplay dataset. Hyperparameters are selected to match those in Baker et al. [5] with\nthe exception of learning rate, which we set to 4e-5. Our models are optimized using AdamW [37].\nSee Table 1 for a full list of hyperparameters.\n4.2\nPerformance on Textual and Visual Goals\nDue to the hierarchical nature of our model, we can evaluate the performance of our agent at achieving\neither text or visual goals simply by choosing whether to use the prior to condition on text or bypass\nthe prior and condition on a MineCLIP video embedding directly. We first tested our model on a set\nof 11 tasks that are achievable within the first 2.5 minutes of gameplay and which do not require\nmultiple steps to complete (e.g., chop a tree or dig a hole, but not build a house). A complete list\nof the tasks and prompts we used for evaluation can be found in Table 3 in the appendix. To select\nvisual goals for testing each of the evaluation tasks, we implemented a tool that searches through\n3https:\/\/github.com\/openai\/Video-Pre-Training\n6\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n100\n200\n300\n400\nTravel Distance (Blocks)\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nSeeds Collected\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n5\n10\n15\n20\nWooden Logs Collected\nVPT\nVPT\n(Text)*\nSTEVE-1\n(Text)\nSTEVE-1\n(Visual)\n0\n20\n40\n60\nDirt Collected\n(a) Programmatic Evaluation\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nConditioned Prompt\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nEvaluation Prompt\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(b) MineCLIP Evaluation\nFigure 3: Left: In our programmatic evaluations, STEVE-1 performed far better than the unconditional\nVPT agent early-game-2x and the text-conditioned VPT agent when prompted appropriately. The\nasterisk * in the “VPT (Text)*” indicates that this result was taken from Appendix I in [5], which\nhad twice the episode length compared to our setting. On some tasks, visual outperforms text-based\nprompting, creating a gap that can likely be bridged through better prompt engineering. Right:\nAcross our 11 MineCLIP evaluation tasks, STEVE-1 achieves the shortest distance between the\nepisode and the MineCLIP goal embedding when prompted appropriately except for in two cases,\nwhere it mixes up digging and dirt and swimming and going underwater. This shows the strong\ngeneral performance of STEVE-1 across a wide variety of short-horizon tasks. The dashed box marks\nthe minimum element along the row, and the diagonal number signifies the diagonal element’s rank\n(0 means it is the minimum row element). See Figure 14 for sample frames from each of the 11 visual\ngoals and Figure 13 for a success-rate version of this matrix.\n10% of our gameplay dataset by finding the closest 16-frame videos to a given text prompt. We then\nmanually selected a 16-frame video that clearly demonstrates the task being completed and use the\ncorresponding MineCLIP video embedding as the goal embedding for that task. Screenshots of these\nvisual goals can be found in Figure 14 in the appendix.\nIn Figure 3, we compare the performance of our text and visual-conditioned agents with the uncondi-\ntional VPT agent and text-conditioned VPT agent (from Appendix I in [5]) across our programmatic\ntasks. We find that when given the relevant text instruction, STEVE-1 collects 75× more dirt, 4.9×\nmore wood, 22× more seeds, and travels 4.3× farther than the unconditional agent, and STEVE-1\ncollects 3.3× more dirt, 4.4× more wood, 8.1× more seeds, and travels 2.2× farther than the text-\nconditioned VPT agent. This represents a significant improvement over the reported performance\nof text-conditioned VPT, which collects several times fewer resources despite having twice as long\nof an episode to do so. We also run an automatic evaluation using MineCLIP embedding distances\nby measuring the minimum distance of a goal embedding to any frame in the episode. As shown\nin Figure 3b, the distance between the goal and the episode is significantly lower when the agent is\nconditioned on the corresponding visual goal than otherwise. Full results for STEVE-1 with both text\nand visual goals can be found in Appendix F.\nIn addition to our evaluations of STEVE-1, we also recorded several sample interactive sessions we had\nwith the agent (controlling it in real-time by giving it written text instructions or specific visual goals).\nThese sessions demonstrate STEVE-1’s ability to responsively follow instructions in real-time in a\nvariety of situations. We believe that such use-cases, where humans give an agent natural instructions\nthat it can follow to complete tasks, will become increasingly important and have practical uses in the\ncreation of instructable assistants and virtual-world characters. These videos, as well as videos of our\nagent performing our evaluation tasks, can be found at https:\/\/sites.google.com\/view\/steve-1.\n4.3\nPrompt Chaining\nWe also experiment with longer horizon tasks that require multiple steps, such as crafting and building.\nWe explore two different prompting methods: directly prompting with the target goal, and a simple\nform of prompt chaining [11, 64, 16] where the task is decomposed into several subtasks and the\n7\n0\n1000\n2000\n3000\nEpisode Timestep\n0\n5\n10\n15\n20\n25\n30\nDirt in Inventory\n0\n1000\n2000\n3000\nEpisode Timestep\n0\n2\n4\n6\n8\nLogs in Inventory\n20M\n40M\n60M\n80M\n100M\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBuild Tower Success\nPrompt Chaining\nDirect Prompting\n20M\n40M\n60M\n80M\n100M\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWooden Planks\nCrafting Success\n55\n60\n65\n70\n75\n80\n85\nAgent Height Position\nBuild a Tower\n0\n2\n4\n6\n8\n10\n12\n14\nPlanks in Inventory\nMake Wooden Planks\nNumber of Frames\n100\n200\n300\nTravel Distance (Blocks)\n0\n2\n4\n6\nSeeds Collected\nrelevant\nirrelevant\n20M\n40M\n60M\n80M\n100M\n0\n5\n10\n15\n20\nWooden Logs Collected\n20M\n40M\n60M\n80M\n100M\n0\n20\n40\n60\nDirt Collected\nNumber of Frames\nFigure 4: Top left: By sequentially chaining visual prompts like “get dirt” and “build a tower”,\nSTEVE-1 successfully gathers dirt and then uses this dirt to build a tower. The prompts switch at\nthe dotted vertical line. Bottom left: The success rates of the chained prompts improve steadily as\nwe train STEVE-1 on more data. Right: The performance of STEVE-1 on different tasks scales in\ndifferent ways when conditioning on a relevant visual prompt for the metric versus other irrelevant\nvisual prompts (e.g., the break wood prompt is the relevant prompt for the “Wooden Logs Collected”\nmetric, while the other prompts are “irrelevant”). For instance, in the wood-collection and dirt-\ncollection tasks, performance starts increasing after training on 60M frames of gameplay. See\nFigure 14 for sample frames from each visual prompt.\nprompts are given sequentially for a fixed number of steps. We explore prompt chaining with visual\ngoals for two tasks: 1) building a tower and 2) making wooden planks. When using prompt chaining,\nwe first prompt STEVE-1 to gather dirt before building a tower, and to gather wooden logs before\ncrafting wooden planks. Figure 4 shows that directly prompting STEVE-1 with the final tasks results\nin near-zero success rates. However, prompt chaining allows STEVE-1 to build a tower 50% of the\ntime and craft wooden planks 70% of the time. For the tower building task, STEVE-1 immediately\nstarts collecting dirt until the prompt switches, at which point its average height starts increasing\nrapidly and its dirt decreases as it builds a tower. Similarly, for the crafting wooden planks task,\nSTEVE-1 immediately starts collecting a large amount of wooden logs until the prompt switches and\nit rapidly converts these wooden logs into wooden planks (causing the amount of wooden logs in its\ninventory to immediately decrease and the number of wooden planks to increase as it crafts more).\nFigure 4 visualizes the average item counts and agent height for the prompt chaining episodes. See\nFigure 18 and Figure 19 in the appendix for visualizations of specific prompt chaining episodes.\n4.4\nScaling\nRecent works in language modeling have found that scaling up pretraining FLOPs, by training on\nmore data or by training a model with more parameters, can improve performance on downstream\ntasks [28, 57, 63]. In certain cases when measuring performance with metrics such as exact-match\n[52], performance improvement may appear to be “emergent” [63], appearing suddenly as the model\nis trained with more compute. Here, we aim to gain a basic understanding of how the performance\nof STEVE-1 on various tasks scales by training with more data (learning rate schedule is chosen\nappropriately).\nTo assess performance gain, we isolate the performance of the policy from the prior, measuring\nperformance of the agent on programmatic tasks (travel distance, seeds, logs, dirt) with visual goals.\nDue to compute constraints, we chose to use the 2x VPT model, which has 248M parameters. We\nfound that both seed collection and travel distance did not improve significantly past 20M frames.\nFrom inspecting gameplay, we suspect that travel distance is a relatively easy task since it is close to\nVPT’s default behavior of running around and exploring. For seed collection, performance remains\nsuboptimal, suggesting that further scaling may be beneficial. This hypothesis is supported by the\nobservation that performance on log and dirt collection remained roughly level until 60M frames when\nit began to rapidly improve. Figure 4 shows the scaling curves for STEVE-1 on each programmatic\ntask when conditioning on relevant vs. irrelevant visual prompts for that task. Since we do not\nobserve regression on any tasks as we train the model with more compute, we expect the model to\ncontinue to perform better as we train larger models on larger datasets.\n8\nWe also evaluated the scaling properties of STEVE-1 for our multi-step tasks with and without prompt\nchaining. Without prompt chaining, the tasks remain challenging for STEVE-1 throughout training.\nHowever, we note that after 60M frames, STEVE-1 sometimes gathers wooden logs and then builds\na small tower when directly prompted to build a tower. This is likely because our visual prompt\nfor tower building shows a video of a tower being built out of wooden logs. With prompt chaining,\nthe performance of STEVE-1 steadily increases with more data. We conjecture that this is because\nthe success of a chained prompt requires the success of each element in the chain. Since different\nabilities emerge at different scales, one would expect chained prompts to steadily get more reliable\nas these subgoals become more reliably completed. In the case of crafting wooden planks, we note\nthat crafting is one such task that gets significantly more reliable as the agent is trained on more data.\nFigure 4 shows the scaling curves for STEVE-1 on the prompt chaining tasks.\nIn summary, we see evidence of tasks that do not require much data for STEVE-1 to learn, tasks\nthat steadily get more reliable as the agent is trained longer, and tasks where capability suddenly\nspikes after the agent reaches some threshold. Put together, this suggests that further scaling would\nlikely significantly improve the agent, although we leave the task of predicting exactly how much\nperformance there is to gain to future studies.\n4.5\nWhat Matters for Downstream Performance?\nPretraining\nBaker et al. [5] finds that by pretraining a behavioral prior with imitation learning on\ninternet-scale datasets for Minecraft, the learned policy can be effectively finetuned to accomplish\ntasks that are impossible without pretraining. In this section, we demonstrate that pretraining is\nalso massively beneficial for instruction-tuning in Minecraft. We hypothesize that due to the strong\nperformance of STEVE-1 and the relatively small amount of compute (≈1% additional compute)\nused for instruction finetuning, most of the capabilities of our agent come from the pretraining rather\nthan the finetuning. To test this hypothesis, we finetune several varients of STEVE-1 from various\npretrained weights: foundation-2x, bc-early-game-2x, rl-from-foundation-2x, and with\nrandomly initialized weights. In this experiment, each model was finetuned on 100M frames.\nFigure 5 shows the performance of these models on our programmatic tasks with visual goals. Note\nthat while an agent trained on our dataset from scratch can accomplish basic tasks like dirt collection\nfairly well, it is unable to find and chop down trees, in contrast to the pretrained agents. This demon-\nstrates that the abilities present in the agent due to pretraining are successfully transferred to the fine-\ntuned agent. Out of all the pretrained weights we tried, we noticed that rl-from-foundation-2x\nperformed the best, having qualitatively better performance at tasks like crafting and chopping down\ntrees. Indeed, Figure 5 shows that this model has strong performance, likely due to the massive\namount of compute it was trained with during its RL training [5].\nClassifier-Free Guidance\nBaker et al. [5] observed that when conditioning the agent on text, it\ntended to ignore its instruction and instead perform the prior behavior learned during pretraining.\nAs discussed in Section 3.4, classifier-free guidance [23] gives a knob for trading off between goal-\nconditioned and prior behaviors. Figure 5 shows the effect of this parameter λ on the log and dirt\ncollection tasks. The performance of the agent reaches its maximum around λ = 5.0 to λ = 7.0,\nafter which it starts to drop off. These results demonstrate the importance of classifier-free guidance,\nwhich improves the performance of STEVE-1 by orders of magnitude.\nPrompt Engineering\nPrompt engineering as a discipline has rapidly emerged over the last year\ndue to the observation that the quality of the output of text-to-X models can dramatically change\ndepending on the prompt [67]. For example, Table 5 in the appendix shows how a prompt for Stable\nDiffusion [50] might be written. By listing out the various attributes of the image such as visual\nmedium, style, and the phrase “trending on ArtStation”, the user is able to get a higher quality image\n[20, 36]. In this section, we explore how this same style of prompt engineering can improve the\nperformance of STEVE-1. Figure 6 shows how a simple prompt of “get dirt” might be changed in\norder to more accurately specify the type of behavior that is desired. Just like in image generation\nmodels, the performance of STEVE-1 significantly improves by modifying the prompt in this fashion.\nBy changing to more complicated prompts, STEVE-1 is able to collect 1.6× more wood, 2× more\ndirt, and 3.3× more seeds.\n9\nscratch\nfd\nbc\nrl\n0\n5\n10\nWooden Logs Collected\nscratch\nfd\nbc\nrl\n0\n25\n50\n75\nDirt Collected\n0\n2\n4\n6\n8\n10\n0\n10\n20\nWooden Logs Collected\n0\n2\n4\n6\n8\n10\n20\n40\n60\nDirt Collected\nConditional Scale λ\nPretrained Weights\nFigure 5: Left: We trained STEVE-1 on 100M frames starting from four different pretrained\nweights: random initialization (scratch), foundation-2x (fd), bc-early-game-2x (bc), and\nrl-from-foundation-2x (rl). The rl-from-foundation-2x agent is generally the most per-\nformant after fine-tuning. Using pretrained weights performs better than training from scratch,\nespecially for more complicated tasks like collecting wood. Right: By using classifier-free guidance\n[23], STEVE-1 collects 7.5× more dirt and 15× more wood than when λ = 0 (no guidance). See\nFigure 17 in the Appendix for similar results with other programmatic tasks.\nPrompt\nDirt Collected\n“break a flower”\n0.7 (-0.2, 1.6)\n“collect seeds”\n2.7 (0.9, 4.5)\n“dig as far as possible”\n3.9 (2.8, 5.0)\n“get dirt”\n9.2 (5.7, 12.7)\n“get dirt, dig hole, dig dirt, gather a ton of dirt, collect dirt”\n26.7 (19.9, 33.5)\nFigure 6: Similar to text-to-image generation, switching to a longer, more specific prompt dramatically\nimproves the performance of STEVE-1. Values in parentheses are 95% confidence intervals.\n5\nLimitations and Conclusion\nIn this paper, we present a methodology for creating instruction-following foundation models of\nbehavior. Specifically, by leveraging two existing pretrained foundation models: a behavioral prior\n(VPT [5]) and a domain-specific CLIP model (MineCLIP [17]), we create a powerful Minecraft agent\nthat can follow short-horizon open-ended text and visual instructions, all for only $60 of compute.\nThe resulting foundation model, STEVE-1, sets a new bar for open-ended instruction-following in\nMinecraft with low-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game evaluation suite. We note\nthat generalist agents such as STEVE-1 can have potential negative effects on society. We include a\nthorough discussion of these issues in Appendix A.\nSTEVE-1 is a significant advancement in creating generative models of text-to-behavior, but it has\nseveral limitations, as described in Appendix B. First, STEVE-1 is mostly proficient at achieving\nshort-horizon tasks while struggling with longer-horizon tasks. While prompt chaining is a promising\napproach for improving performance on complex tasks, more can be done in future work to improve\nperformance. Another limitation we observe is that prompt engineering, as with other generative\nmodels, can be unintuitive and time-consuming. Future work should investigate improving the\nsteerability of STEVE-1 through a better understanding of natural language prompts. Additionally,\nwe note that evaluating and describing the capabilities of open-ended generalist agents is an open\nresearch problem itself since capability depends strongly on preconditions, prompt engineering, and\nour own ability to come up with varied and challenging tasks. Finally, since our approach is not\nspecific to the Minecraft domain, we hope that the method used to create STEVE-1 can inspire future\nwork in creating powerful generalist agents in other domains and environments.\n10\nAcknowledgements\nAll of the authors gratefully acknowledge funding for this research from the Natural Sciences and\nEngineering Research Council of Canada (NSERC) and the Canada CIFAR AI Chairs Program\n(Vector Institute for Artificial Intelligence). SL is supported by a Vector Institute internship and by\nan NSERC Discovery Grant. KP is supported by an NSERC PGS-D award. HC is supported by an\nNSERC CGS-D award. JB acknowledges funding from the Canada CIFAR AI Chairs program, Fujitsu\nJapan, and an Amazon Research Award. In addition to NSERC and CIFAR (Vector Institute), SM\nacknowledges funding from Microsoft Research. We thank Silviu Pitis, Romi Lifshitz, Forest Yang,\nand Yongchao Zhou for their helpful comments; Alisa Wu and Ziming Chen for their contributions\nto the text-video pair dataset; and Finn Paster for the logo and graphic for the website. Resources\nused in preparing this research were provided, in part, by the Province of Ontario, the Government\nof Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence\n(www.vectorinstitute.ai\/partners).\nReferences\n[1] Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse H. Engel, Mauro Verzetti, Antoine\nCaillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi,\nNeil Zeghidour, and Christian Havnø Frank. Musiclm: Generating music from text. CoRR,\nabs\/2301.11325, 2023. doi: 10.48550\/arXiv.2301.11325. URL https:\/\/doi.org\/10.48550\/\narXiv.2301.11325.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems, 35:\n23716–23736, 2022.\n[3] Marcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.\nIn Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.\nVishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pages 5048–5058, 2017. URL https:\/\/proceedings.neurips.cc\/\npaper\/2017\/hash\/453fadbd8a1a3af50a9df4df899537b5-Abstract.html.\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[5] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. Advances in Neural Information Processing Systems, 35:24639–24654,\n2022.\n[6] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna.\nWhen does return-conditioned supervised learning work for offline reinforcement learning?\narXiv preprint arXiv:2206.01079, 2022.\n[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics\ntransformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in\nNeural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates,\nInc., 2020. URL https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2020\/file\/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n11\n[9] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task\ncontrol through goal-aware representation learning and adaptive horizon prediction. arXiv\npreprint arXiv:2301.10034, 2023.\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings\nof the International Conference on Computer Vision (ICCV), 2021.\n[11] Harrison Chase. Langchain, 2022. URL https:\/\/github.com\/hwchase17\/langchain.\n[12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084–15097,\n2021.\n[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL\nhttps:\/\/lmsys.org\/blog\/2023-03-30-vicuna\/.\n[14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,\nNoam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope,\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,\nAnselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant\nMisra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language\nmodeling with pathways. CoRR, abs\/2204.02311, 2022. doi: 10.48550\/arXiv.2204.02311. URL\nhttps:\/\/doi.org\/10.48550\/arXiv.2204.02311.\n[15] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhut-\ndinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Anna\nKorhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, pages 2978–2988. Association for Computational Linguistics,\n2019. doi: 10.18653\/v1\/p19-1285. URL https:\/\/doi.org\/10.18653\/v1\/p19-1285.\n[16] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo\nLopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy,\nand Charles Sutton. Language model cascades, 2022.\n[17] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,\nAndrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.\nMinedojo: Build-\ning open-ended embodied agents with internet-scale knowledge.\nIn S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neu-\nral Information Processing Systems, volume 35, pages 18343–18362. Curran Associates,\nInc., 2022. URL https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2022\/file\/\n74a67268c5cc5910f64938cac4526a90-Paper-Datasets_and_Benchmarks.pdf.\n[18] Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Manon Devin, Benjamin\nEysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. In\nInternational Conference on Learning Representations, 2021. URL https:\/\/openreview.\nnet\/forum?id=rALA0Xo6yNJ.\n[19] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv:1907.13440, 2019.\n12\n[20] Gustavosta. Stable-diffusion-prompts. Hugging Face, 2023. URL https:\/\/huggingface.\nco\/datasets\/Gustavosta\/Stable-Diffusion-Prompts. Hugging Face Datasets.\n[21] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016.\ndoi: 10.1109\/CVPR.2016.90. URL https:\/\/doi.org\/10.1109\/CVPR.2016.90.\n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs\/2207.12598, 2022.\ndoi: 10.48550\/arXiv.2207.12598. URL https:\/\/doi.org\/10.48550\/arXiv.2207.12598.\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\nHugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-\nTien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-\n12, 2020, virtual, 2020. URL https:\/\/proceedings.neurips.cc\/paper\/2020\/hash\/\n4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.\n[25] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen,\nLi Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation\nwith multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.\n[26] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for\nartificial intelligence experimentation. In Ijcai, pages 4246–4247, 2016.\n[27] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. CoRR,\nabs\/2305.02463, 2023. doi: 10.48550\/arXiv.2305.02463. URL https:\/\/doi.org\/10.48550\/\narXiv.2305.02463.\n[28] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. CoRR, abs\/2001.08361, 2020. URL https:\/\/arxiv.org\/abs\/2001.08361.\n[29] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio\nand Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL\nhttp:\/\/arxiv.org\/abs\/1312.6114.\n[30] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline\nq-learning on diverse multi-task data both scales and generalizes. In The Eleventh International\nConference on Learning Representations, 2023. URL https:\/\/openreview.net\/forum?\nid=4-k7kUavAj.\n[31] Lauro Langosco Di Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger.\nGoal misgeneralization in deep reinforcement learning. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of\nthe 39th International Conference on Machine Learning, volume 162 of Proceedings of\nMachine Learning Research, pages 12004–12019. PMLR, 17–23 Jul 2022. URL https:\n\/\/proceedings.mlr.press\/v162\/langosco22a.html.\n[32] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio\nGuadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game\ndecision transformers. Advances in Neural Information Processing Systems, 35:27921–27936,\n2022.\n[33] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following\nmodels. https:\/\/github.com\/tatsu-lab\/alpaca_eval, 2023.\n13\n[34] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang\nYuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré,\nDiana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda\nRong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng,\nMert Yüksekgönül, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab,\nPeter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli,\nTatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen\nLi, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. CoRR,\nabs\/2211.09110, 2022. doi: 10.48550\/ARXIV.2211.09110. URL https:\/\/doi.org\/10.\n48550\/arXiv.2211.09110.\n[35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d\ncontent creation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2023.\n[36] Vivian Liu and Lydia B. Chilton. Design guidelines for prompt engineering text-to-image\ngenerative models. In Simone D. J. Barbosa, Cliff Lampe, Caroline Appert, David A. Shamma,\nSteven Mark Drucker, Julie R. Williamson, and Koji Yatani, editors, CHI ’22: CHI Conference\non Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022,\npages 384:1–384:23. ACM, 2022. doi: 10.1145\/3491102.3501825. URL https:\/\/doi.org\/\n10.1145\/3491102.3501825.\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019. URL https:\/\/openreview.net\/forum?id=Bkg6RiCqY7.\n[38] Federico Malato, Florian Leopold, Amogh Raut, Ville Hautamäki, and Andrew Melnik. Be-\nhavioral cloning via search in video pretraining latent space. arXiv preprint arXiv:2212.13326,\n2022.\n[39] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A\nuniversal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n[40] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi,\nSameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep?: Embodied\ndecision making using language guided world modelling. arXiv preprint arXiv:2301.12050,\n2023.\n[41] OpenAI. Introducing ChatGPT, Nov 2022. URL https:\/\/openai.com\/blog\/chatgpt.\n[42] Keiran Paster, Sheila A McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics\nmodels. arXiv preprint arXiv:2012.02419, 2020.\n[43] Keiran Paster, Sheila A. McIlraith, and Jimmy Ba. You can’t count on luck: Why decision\ntransformers and rvs fail in stochastic environments. In Alice H. Oh, Alekh Agarwal, Danielle\nBelgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems,\n2022. URL https:\/\/openreview.net\/forum?id=atb3yifRtX.\n[44] Keiran Paster, Silviu Pitis, Sheila A. McIlraith, and Jimmy Ba. Return augmentation gives\nsupervised RL temporal compositionality. In NeurIPS 2022 Foundation Models for Decision\nMaking Workshop, 2022. URL https:\/\/openreview.net\/forum?id=q5olkWCt7nl.\n[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKöpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\nhigh-performance deep learning library.\nIn Hanna M. Wallach, Hugo Larochelle, Alina\nBeygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances\nin Neural Information Processing Systems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 8024–8035, 2019. URL https:\/\/proceedings.neurips.cc\/paper\/2019\/hash\/\nbdbca288fee7f92f2bfa9f7012727740-Abstract.html.\n14\n[46] PrismarineJS and Others. Mineflayer. https:\/\/github.com\/PrismarineJS\/mineflayer,\n2023. URL https:\/\/github.com\/PrismarineJS\/mineflayer. GitHub repository.\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748–8763. PMLR, 2021.\n[48] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[49] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al.\nA generalist agent. arXiv preprint arXiv:2205.06175, 2022.\n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022.\n[51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L.\nDenton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol\nAyan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi.\nPho-\ntorealistic text-to-image diffusion models with deep language understanding.\nIn\nNeurIPS, 2022.\nURL http:\/\/papers.nips.cc\/paper_files\/paper\/2022\/hash\/\nec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html.\n[52] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language\nmodels a mirage?\nCoRR, abs\/2304.15004, 2023. doi: 10.48550\/arXiv.2304.15004. URL\nhttps:\/\/doi.org\/10.48550\/arXiv.2304.15004.\n[53] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\nmanipulation. In Conference on Robot Learning, pages 894–906. PMLR, 2022.\n[54] Kihyuk Sohn, Honglak Lee, and Xinchen Yan.\nLearning structured output rep-\nresentation using deep conditional generative models.\nIn Corinna Cortes, Neil D.\nLawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances\nin Neural Information Processing Systems 28:\nAnnual Conference on Neural Infor-\nmation Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada,\npages 3483–3491, 2015. URL https:\/\/proceedings.neurips.cc\/paper\/2015\/hash\/\n8d55a249e6baa5c06772297520da2051-Abstract.html.\n[55] Miroslav Strupl, Francesco Faccio, Dylan R. Ashley, Jürgen Schmidhuber, and Rupesh Kumar\nSrivastava. Upside-down reinforcement learning can diverge in stochastic environments with\nepisodic resets. CoRR, abs\/2205.06595, 2022. doi: 10.48550\/arXiv.2205.06595. URL https:\n\/\/doi.org\/10.48550\/arXiv.2205.06595.\n[56] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distilling\ninternet-scale vision-language models into embodied agents. arXiv preprint arXiv:2301.12507,\n2023.\n[57] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challeng-\ning big-bench tasks and whether chain-of-thought can solve them. CoRR, abs\/2210.09261, 2022.\ndoi: 10.48550\/arXiv.2210.09261. URL https:\/\/doi.org\/10.48550\/arXiv.2210.09261.\n[58] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps:\/\/github.com\/tatsu-lab\/stanford_alpaca, 2023.\n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. CoRR, abs\/2302.13971, 2023. doi: 10.48550\/arXiv.2302.13971. URL\nhttps:\/\/doi.org\/10.48550\/arXiv.2302.13971.\n15\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\nGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,\npages 5998–6008, 2017. URL https:\/\/proceedings.neurips.cc\/paper\/2017\/hash\/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n[61] Ryan Volum, Sudha Rao, Michael Xu, Gabriel DesGarennes, Chris Brockett, Benjamin\nVan Durme, Olivia Deng, Akanksha Malhotra, and Bill Dolan. Craft an iron sword: Dy-\nnamically generating interactive game characters by prompting large language models tuned on\ncode. In Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Word-\nplay 2022), pages 25–43, Seattle, United States, July 2022. Association for Computational\nLinguistics. doi: 10.18653\/v1\/2022.wordplay-1.3. URL https:\/\/aclanthology.org\/2022.\nwordplay-1.3.\n[62] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023.\n[63] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels. CoRR, abs\/2206.07682, 2022. doi: 10.48550\/arXiv.2206.07682. URL https:\/\/doi.\norg\/10.48550\/arXiv.2206.07682.\n[64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\nmodels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\nAdvances in Neural Information Processing Systems, 2022. URL https:\/\/openreview.net\/\nforum?id=_VjQlMeSB_J.\n[65] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman,\nSergey Levine, and Jonathan Tompson. Robotic skill acquisition via instruction augmentation\nwith vision-language models. arXiv preprint arXiv:2211.11736, 2022.\n[66] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n[67] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\nand Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https:\/\/openreview.\nnet\/forum?id=92gvk82DE-.\n16\nA\nBroader Impact\nWith the increasing capability level of artificial intelligence comes many potential benefits and also\nrisks. On the positive side, we anticipate that the techniques that used to create STEVE-1 could be\napplied to the creation of helpful agents in other sequential decision making domains, including\nrobotics, video games, and the web. Our demonstration of such a low cost approach to creating a\npowerful, instruction-following model also has the potential to improve the democratization of AI.\nHowever, on the negative side, agents pretrained on large internet datasets reflect the biases of the\ninternet and, as suggested by our experiments, these pretraining biases can potentially remain after\ninstruction-tuning. If not addressed carefully, this could lead to devastating consequences for society.\nWe hope that while the stakes are low, works such as ours can improve access to safety research on\ninstruction-following models in sequential decision-making domains.\nB\nLimitations and Future Work\nB.1\nGoal Misgeneralization\nOne of the most common mistakes that STEVE-1 makes during evaluation is to overgeneralize. For\ninstance, if we prompt STEVE-1 with a video of someone punching a cow, it may simply run to the\nnearest animal and punch that animal instead. This is related to the concept of goal misgeneralization\n[31]. Generalization can be helpful when the task we assign the agent is impossible to achieve from\nthe current state and the agent instead performs a closely related action, but harmful when the task\nis achievable. We note two things: first, we believe the powerful generalization ability of STEVE-1\nprobably comes from the MineCLIP embeddings and it especially improves the ability of STEVE-1\nto follow visual instructions when the exact items or blocks nearby are not available in the current\nenvironment, which is an extremely common scenario. Second, we notice that the tendency of the\nagent to misgeneralize decreases with scale. For example, with a model trained on less data, we find\nthat asking the agent to look up and punch a tree to get a wooden log often resulted in the agent\nlooking in the air and punching nothing; training the model on more data results in the agent first\nwalking over to a nearby tree and looking up to get a wooden log. Future work should look to measure\nmore closely the relationship between misgeneralization and scale.\nB.2\nRandom Selection of Hindsight Goals\nIn this work, we choose to randomly select future episode timesteps as hindsight goals, rather than\nuse a more sophisticated strategy. This is primarily due to the simplicity of the approach, but also to\nensure a diverse and unbiased coverage of potential goals achievable within the short horizon. Future\nworks can investigate the effects of alternative approaches that filter for semantically interesting\ntimesteps as goals.\nB.3\nDifference in Performance Between Text and Visual Goals\nIn our experiments, we observed that STEVE-1 often performed better when conditioned with visual\ngoals compared to text goals converted through the prior. There are several potential factors that\ncould contribute to this performance gap between text and visual conditioning. First, the prior\nmodel may not be accurately capturing the full meaning of the text prompt. Training the prior on\nmore data or using a more powerful model architecture could potentially improve the quality of the\nsampled latent goals. Second, the visual goals can provide more precise demonstrations of the desired\nbehavior. Text goals are inherently more ambiguous. Providing additional information such as the\nobservation context to the prior and further prompt-engineering to make text prompts more detailed\nand less ambiguous, could help close this gap. Because text conditioning provides more flexibility\nand potential for generalization, closing the performance gap between text and visual conditioning is\nan important direction for future work.\nB.4\nChallenges in Evaluating STEVE-1\nSee Table 4 for a non-exhaustive list of tasks that the STEVE-1 agent is able to achieve. It is worth\nmentioning that evaluating and describing the capabilities of open-ended generalist agents is an open\n17\nresearch problem itself since capability depends strongly on preconditions, prompt engineering, and\nour own ability to come up with varied and challenging tasks. For example, there are many recent\nworks on the evaluation of LLMs (e.g., [34, 33, 66]) which highlight these challenges.\nThat being said, there are a number of tasks which STEVE-1 is unable to accomplish. As previously\nmentioned, long-horizon tasks such as obtaining a diamond or building a house are currently beyond\nthe capability of STEVE-1. Further, STEVE-1 also struggles with more complex crafting tasks like\ncrafting an enchanting table, bookcase, or boat. Again, the virtually limitless and open-ended nature\nof tasks in Minecraft makes it very difficult to test generalist agents in this domain. We hope future\nworks develop more sophisticated methods to evaluate the performance of generalist agents on short\nand long-horizon tasks (potentially through an extension of our MineCLIP evaluation method).\nIt is also worth noting that it is currently not possible to test STEVE-1 or VPT [5] in the MineDojo\n[17] environment, which is meant for generalist agent evaluation, since the action spaces are not\ncompatible. We believe that bridging this gap could be greatly beneficial to the generalist agent\ncommunity and we hope future works investigate this further.\nB.5\nTowards Improved Long-Horizon Performance\nSTEVE-1 is a significant advancement in creating generative models of text-to-behavior, but it\nhas several limitations. First, STEVE-1 is mostly proficient at achieving short-horizon tasks while\nstruggling on longer-horizon tasks like obtaining a diamond. Solving long-horizon tasks while taking\nactions using low-level mouse\/keyboard controls is a very challenging and exciting research direction\nand, while prompt chaining is a promising approach for improving performance on complex tasks,\nmore can be done in future work to improve performance.\nOne potential bottleneck is the fact that during packed hindsight relabeling, the hindsight goals\nare limited to at most 200 timesteps in the future (∼10 seconds). Thus, tasks which require more\nthan 200 timesteps to complete are technically out-of-distribution for STEVE-1. Although sampling\nhindsight goals from farther into the future could theoretically enhance long-horizon performance,\nour experiments in Appendix C.4 indicate that the performance tends to decrease if we increase\nthis hyperparameter too much. We suspect that while increasing this hyperparameter may be able\nto improve long-horizon performance, it also increases noise and comes at the cost of reducing\nperformance on short-horizon goals. Investigating whether it is possible to achieve a better tradeoff is\nan important avenue for future work. We also suspect that the long-horizon capabilities of STEVE-1\ncould be improved through scaling or finetuning with reinforcement learning, or leveraging LLMs or\nVLMs to automatically provide prompt chains to the STEVE-1 agent.\nB.6\nApplying the STEVE-1 Approach to Other Domains\nWe designed STEVE-1 for Minecraft due to the availability of two key ingredients: (1) a strong\nbehavioral prior (VPT [5]), and (2) a powerful visual-language model which maps text and video\nto a joint embedding space (MineCLIP [17]). However, the method used to create STEVE-1 is not\nspecific to the Minecraft domain. Given the rapid development of generative models, we expect\nthat similar models to VPT and MineCLIP will become available in many other domains. As these\nmodels become available, future work could investigate the applicability of the STEVE-1 approach to\nthese other domains.\nC\nAdditional Ablations\nIn this section, we describe additional ablations on design choices for our method, including the use of\nclassifier-free guidance during training, text augmentation strategies, different VAE variants, varying\nchunk sizes during finetuning, and more. We use programmatic evaluation metrics to compare the\nperformance of the various ablations.\nC.1\nClassifier-Free Guidance During Training\nWe examine the importance of using classifier-free guidance during training by finetuning a model\nwith no guidance which does not drop out the goal embedding zτgoal from the policy’s input (i.e.,\npuncond = 0.0) and comparing it to the version which uses guidance (puncond = 0.1). The chunk\n18\nno guidance (λ = 0)\nguidance (λ = 0)\nguidance (λ = 3)\n0\n100\n200\n300\nTravel Distance (Blocks)\nno guidance (λ = 0)\nguidance (λ = 0)\nguidance (λ = 3)\n0\n1\n2\n3\nSeeds Collected\nno guidance (λ = 0)\nguidance (λ = 0)\nguidance (λ = 3)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nWooden Logs Collected\nno guidance (λ = 0)\nguidance (λ = 0)\nguidance (λ = 3)\n0\n20\n40\n60\nDirt Collected\nFigure 7: Ablation on Guidance. In the “no guidance” variant, we set puncond = 0, meaning\nthat we do not drop any zτgoal from the policy’s input during training. The “guidance” variants set\npuncond = 0.1, dropping 10% of the time during training. Whereas the “no guidance” model is\nonly compatible with λ = 0 at inference, the “guidance” model can use λ > 0, allowing for better\nperformance.\nsize is set to the range 15 to 50 and we train each policy for 100M frames. In Figure 7, we compare\nthe performance of using visual goals (MineCLIP video embedding) on the no guidance model using\nconditional scale λ = 0 and the guidance model using conditional scales λ = 0 and λ = 3. We\nobserve that while the no guidance model slightly outperforms the guidance model at λ = 0 across a\nfew metrics, the agent with guidance outperforms the no guidance agent by a factor of 2 to 3 times\nfor the inventory collection tasks when we increase the conditional scale to λ = 3 (which we cannot\ndo for the no guidance model). For the travel distance metric, both of the guidance versions perform\nsimilarly to the no guidance version.\nC.2\nText Augmentation\nDuring finetuning, instead of using only self-supervision with future MineCLIP video embedding\nas the goal, we considered using the text embeddings from the 2,000 human labeled trajectory\nsegments as goal embeddings, either solely or in addition to the self-supervised video embeddings.\nIn order to more fairly compare with the CVAE prior approach, we augment the human-labeled\ndata with additional text-gameplay pairs generated as described in Appendix E.2. We implement\nthis experiment by replacing the visual embeddings used for relabeling in Algorithm 1 with text\nembeddings, when available, with a 90% probability. To experiment with not using visual embeddings\nat all, we can replace the visual embeddings with zeros in the same way. In Figure 8, we observe that\nusing only the visual embeddings during training, in combination with the CVAE, can outperform\nusing MineCLIP text embeddings directly in the other two baselines. In this experiment, the chunk\nsize is set to the range 15 to 50 and we train each policy for 100M frames.\nC.3\nVAE Variants\nWe study the dataset used to train the CVAE prior model. In Figure 9, we observe that augmentation\nhelps in some programmatic tasks, including the dirt and seed collection tasks, but slightly hurts the\nwooden log collection and travel distance metrics. In this experiment, we use the same policy with\neach CVAE variant and we tune the conditional scale λ for each variant. The chunk size is set to the\nrange 15 to 200 and we train the policy for 100M frames.\n19\ntext (raw text)\ntext + visual (raw text)\nvisual (text VAE)\n0\n100\n200\n300\nTravel Distance (Blocks)\ntext (raw text)\ntext + visual (raw text)\nvisual (text VAE)\n0\n2\n4\n6\nSeeds Collected\ntext (raw text)\ntext + visual (raw text)\nvisual (text VAE)\n0\n2\n4\n6\n8\n10\nWooden Logs Collected\ntext (raw text)\ntext + visual (raw text)\nvisual (text VAE)\n0\n2\n4\n6\n8\n10\nDirt Collected\nFigure 8: Ablation on Text Augmentation. In the “text (raw text)” ablation, we train the model\nusing only the text labels from human labelled trajectory segments, and directly use the MineCLIP\ntext embedding of the text label as the goal embedding during training and at inference. For the “text\n+ visual (raw text)” ablation, we use both the visual embedding in self-supervised manner and the\ntext embedding from the human labelled trajectory segments during training and use the MineCLIP\ntext embedding during inference. Even with augmentation, the dataset only contained around 2% text\nembeddings. The “visual (text VAE)” version is as reported in the main method, using the CVAE to\nconvert MineCLIP text embedding to visual embedding during inference.\nHuman\nHuman + Aug\n0\n100\n200\n300\nTravel Distance (Blocks)\nHuman\nHuman + Aug\n0\n5\n10\n15\nSeeds Collected\nHuman\nHuman + Aug\n0\n5\n10\n15\n20\nWooden Logs Collected\nHuman\nHuman + Aug\n0\n10\n20\n30\nDirt Collected\nFigure 9: Ablation on VAE Training Data. “Human” baseline uses only the 2,000 human-labelled\ntrajectory segments (text-video pairs), as training example for the CVAE prior model. “Human +\nAug” baseline adds additional pairs of text-video examples as described in Section 3.3.\nC.4\nChunk Size\nDuring\nfinetuning,\nwe\ncompare\ndifferent\ngoal\nchunk\nsizes\nby\nvarying\nthe\nmax_btwn_goals=[100,200,300,400], while keeping the min_btwn_goals=15.\nSee Al-\ngorithm 1 for more details. A larger max_btwn_goals introduces more noise, with actions that led\nto achieving the further away goal being less correlated to the actions present in that goal chunk. In\nFigure 10, we observe that the best max_btwn_goals chunk size is around 200, and increasing the\nchunk size beyond that causes a drop in performance. We train each policy for 160M frames and tune\nthe conditional scale for each.\n100\n200\n300\n400\nChunk Size\n150\n200\n250\n300\n350\nTravel Distance (Blocks)\n100\n200\n300\n400\nChunk Size\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nSeeds Collected\n100\n200\n300\n400\nChunk Size\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nWooden Logs Collected\n100\n200\n300\n400\nChunk Size\n20\n30\n40\n50\n60\n70\nDirt Collected\nFigure 10: Ablation on Segment Chunk Size. We vary the max_btwn_goals parameter in Algo-\nrithm 1. The performance is roughly the best at around 200, beginning to decline with greater values.\n20\nnoDirtorDig\nnormal\n0\n100\n200\n300\nTravel Distance (Blocks)\nnoDirtorDig\nnormal\n0\n2\n4\n6\nSeeds Collected\nnoDirtorDig\nnormal\n0\n5\n10\n15\nWooden Logs Collected\nnoDirtorDig\nnormal\n0\n10\n20\n30\n40\n50\nDirt Collected\nFigure 11: Even without training the prior on the concept of dirt or digging at all, STEVE-1 can still\nbe instructed to dig holes and get dirt. This demonstrates that STEVE-1 can generalize to unseen text\ninstructions.\nC.5\nGeneralization to Novel Text Instructions\nWe train the prior on a dataset of human and GPT-generated instructions designed to be representative\nof the tasks that appear in our gameplay dataset. Here, we have performed a set of simple generaliza-\ntion experiments to measure the degree to which the prior can generalize to unseen instructions.\nInstruction Training Set Contamination\nThe “Text Prompt” column in Table 3 shows which of\nthe prompts used in evaluation show up in our training dataset. Among our evaluation instructions,\nthe bolded and italicized instructions show up in the instruction-trajectory dataset. While some\ninstructions do show up, most of the instructions do not show up in our training set (verbatim).\nTraining Set Decontamination\nTo measure the effect on performance of removing a concept from\nthe training set, we ran an experiment where we removed every instruction with the words “dirt” or\n“dig” in them and retrained the VAE model. This corresponds to around 10% of the instructions.\nAs shown in Figure 11, we found that even without training on the concept of dirt or digging at all,\nSTEVE-1 can still be instructed to dig holes and get dirt. This demonstrates clearly that STEVE-1 can\ngeneralize to unseen text instructions — likely because most of the text-understanding comes from\nthe pretrained MineCLIP model which was trained on a highly diverse dataset of YouTube videos\nand captions. The prior VAE only needs to learn a mapping between the text and visual MineCLIP\nembeddings. Note that there is a slight decrease in performance across all tasks likely due to the\nsmaller VAE training set (∼10% less).\nThe instruction-following capability of STEVE-1 is shared between: the policy, which learns to follow\ninstructions in the visual MineCLIP embedding space; the MineCLIP text-encoder, which is trained\nto align well with the visual embeddings and performs most of the text-understanding; and our prior\nVAE model, which learns a simple function to translate between text and visual embeddings. This\nmodeling setup lets us fully exploit pretrained models such as MineCLIP to gain impressive language\nunderstanding without relying on having our own large datasets or compute.\nD\nDataset Details\nD.1\nGameplay Dataset\nOur gameplay dataset consists of two types of episodes: 7,854 episodes (38.94M frames) of a\ncontractor dataset made available from Baker et al. [5] and 2,267 episodes (14.96M frames) of\ngameplay generated by running various pretrained VPT agents.\nOpenAI Contractor Dataset\nThe majority of our data comes from the contractor data used to train\nVPT [5]. OpenAI released five subsets of contractor data: 6.x, 7.x, 8.x, 9.x, and 10.x. We use an equal\n21\nmix of 8.x, 9.x, and 10.x, which correspond to “house building from scratch”, “house building from\nrandom starting materials”, and “obtain diamond pickaxe”. Contractors were given anywhere from\n10 to 20 minutes to accomplish these goals to the best of their abilities while their screen, mouse, and\nkeyboard were recorded.\nVPT-Generated Dataset\nWe generated additional data by generating episodes using various pre-\ntrained VPT agents. In order to increase the diversity of data as well as to get data of the agent\nswitching tasks randomly throughout the middle of episodes, we added random switching between\nthe different pretrained agents during episodes. Specifically, at the beginning of an episode we ran-\ndomly sample two VPT agents from (foundation_model_2x, bc_early_game_2x, bc_house_3x,\nrl_from_foundation_2x, rl_from_house_2x) and switch between them at each timestep with a\nprobability of 1\/1000. Since the RL agents all act quite similarly, we avoid sampling two RL agents\nat once. Additionally, with a probability of 1\/750 each timestep, we cause the agent to spin a random\nnumber of degrees. This adds more data where the agent spontaneously changes tasks, increasing\ndownstream steerability.\nD.2\nText-Video Pair Dataset\nWe gathered a small dataset of 2,000 human-labelled trajectory segments (text-video pairs) by\nmanually labeling gameplay from our datasets. We used a simple web app that presented a video of\n16 frames (less than a second) to the user from a randomly sampled episode. This only corresponds to\n32,000 frames of labeled data, which corresponds to labeling 0.06% of the full dataset, or 27 minutes\nof labeled data. However, as discussed in Appendix E.2, combining this with automatically labeled\ndata using gpt-3.5-turbo and MineCLIP results in a strong prior model.\nD.3\nPrompt Design\nIn our experiments we used both short and longer prompts. The short prompts are either taken from\nprevious literature (e.g., the language-conditioning experiment in the VPT appendix [5]) or they were\nsimply the first prompt we tried. The longer prompts were created by taking inspiration from the\nprompt engineering methods used with text-to-image models such as Stable Diffusion [50]. To design\nthese prompts, we simply strung together a lot of terms related to our task in order to increase the\nspecificity of the prompts. We were excited to discover that this style of prompt design inspired by\nthe prompt-engineering community works well in STEVE-1.\nE\nTraining Details\nE.1\nPolicy Training\nSTEVE-1 was trained using distributed data parallel in PyTorch [45]. During training, segments of 640\ntimesteps were sampled from the dataset. Due to memory constraints, these segments were further\nbroken up into chunks of 64, which are processed sequentially. Since VPT uses a Transformer-XL\n[15], this sequential processing lets the policy attend to previous batches up to the limit of its context\nlength. We optimized the weights using AdamW [37] with a maximum learning rate of 4e-5 and a\nlinear warmup for the first 10M frames followed by a cosine learning rate decay schedule that decays\nto 10% of the original learning rate. See Table 1 for an exhaustive list of hyperparameters used during\ntraining.\nDuring training, we sample data using packed hindsight relabeling (Figure 2). This involves sampling\na segment of an episode, randomly selecting some timesteps at which to change goals, and then\nfilling in the corresponding goal embeddings for the entire episode with the embeddings from the\ncorresponding goal segments. See Algorithm 1 for a detailed explanantion of packed hindsight\nrelabelling.\nE.2\nPrior Training\nThe prior model is a simple CVAE [54] that conditions on MineCLIP [17] text embeddings and models\nthe conditional distribution of visual embeddings given the corresponding text embedding. This\nmodel is trained on a combination of around 2,000 hand-labeled trajectory segments and augmented\n22\nHyperparameter Name\nValue\ntrunc_t\n64\nT\n640\nbatch_size\n12\nnum_workers\n4\nweight_decay\n0.039428\nn_frames\n160M\nlearning_rate\n4e-5\noptimizer\nAdamW [37]\nwarmup_frames\n10M\np_uncond\n0.1\nmin_btwn_goals\n15\nmax_btwn_goals\n200\nvpt_architecture\n2x\nTable 1: Policy Hyperparameters\nHyperparameter Name\nValue\narchitecture\nMLP\nhidden_dim\n512\nlatent_dim\n512\nhidden_layers\n2\nbatch_size\n256\nlearning_rate\n1e-4\nβ\n0.001\nn_epochs\n50\nn_search_episodes\n2000\nk\n5\noffset\n8\nTable 2: Prior Hyperparameters\nAlgorithm 1: Sampling Episode Segments with Packed Hindsight Relabeling\nFunction sample_episode_segment(T, min_btwn_goals, max_btwn_goals)\nsegment = sampleSegment(episode, T)\ncurr_timestep = segment.start\ngoal_switching_indices = []\nwhile curr_timestep < segment.end do\ncurr_timestep += uniform(min_btwn_goals, max_btwn_goals)\ngoal_switching_indices.append(curr_timestep)\nrelabeled_goal_embeds = []\nfor n in range(1, len(goal_switching_indices)) do\nrelabeled_goal_embeds[in−1:in] = segment.goal_embeddings[in]\nreturn segment.obs, segment.actions, relabeled_goal_embeds\nwith additional data by automatically searching for text-gameplay pairs from our gameplay dataset.\nThis is done using the following steps:\n1. Combine the 2,000 text labels with 8,000 additional labels generated by querying\ngpt-3.5-turbo.\n2. For each of these 10,000 text labels, search through 1,000 episodes sampled from the\ngameplay dataset to find the top 5 closest visual MineCLIP embeddings to the text embedding\nof the text label.\nThese 50,000 automatically-mined text-video pairs are added to the original 2,000 hand-labeled\nexamples to form the final dataset used for prior training.\nWe noticed when prompting STEVE-1 using visual goals that when the visual goal showed the agent\nhitting a block but not following through and breaking it that STEVE-1 actually avoided breaking\nblocks. Unfortunately, many of the automatically discovered text-gameplay clips include gameplay\nof this kind. In order to prevent this issue, we added an offset to the embeddings found in this manner.\nBy selecting embeddings from a timestep offset steps after the originally-selected timestep, the\nagent is much more likely to follow through with breaking blocks.\nWe trained our prior model for 50 epochs on this dataset and used early-stopping with a small\nvalidation set. An exhaustive list of hyperparameters used for creating the prior model can be found\nat Table 2.\n23\nE.3\nTraining Costs\nThe $60 cost we report corresponds to the cost of renting a 8xA10g node using spot instances on\nAWS for 12 hours using our instances prices in May 2023.\nF\nAdditional Visualizations\nF.1\nMineCLIP Evaluation\nWe ran MineCLIP evaluation on both text and visual prompts. The MineCLIP evaluation results can\nbe found in Figure 12.\nF.2\nSteerability with Programmatic Metrics\nSimilar to Figure 20 in the VPT appendix [5], we plot the programmatic metric performances (mean\nand 95% confidence intervals) across the different goal prompt conditioning, both using visual\nprompts (Figure 15) and text prompts with CVAE prior (Figure 16) conditioning, on our policy\ntrained with hyperparameters in Table 1 and using conditional scaling λ = 7 (for visual prompts)\nand λ = 6.0 (for text prompts with CVAE prior). Each conditioning variant is run with 10 trials,\neach trial with a different environmental seed and with an episode length of 3000 timesteps (2.5\nminutes gameplay). Across the conditioning variant, we use the same set of environmental seeds.\nFor comparison, we also plot the metrics for an unconditional VPT (early_game) agent (“VPT\n(uncond)”) and the text-conditioned agent investigated in VPT appendix [5] (“VPT (text)*”) when\nconditioned on the relevant text. When using visual goal conditioning, we the use MineCLIP video\nencoder to embed a 16-frame clip of the agent performing the desired task taken from our training\ndataset. An example frame from each of the visual goals is illustrated in Figure 14. When using text\nVAE goal conditioning, we use the MineCLIP text encoder to encode the text prompts (Table 3) and\nuse the CVAE prior to sample the goal embedding from the MineCLIP text embedding.\nWe note several differences in our experimental setup compared to that in VPT [5]. We only run\nour evaluation episodes for 3000 timesteps, equivalent to 2.5 minutes of gameplay, compared to\n5 minutes in the VPT paper. Due to a limited computational budget, we generate 10 episodes per\nconditioning variant, and 110 episodes for the unconditional (“VPT (uncond)”), compared to VPT’s\n1000 episodes. Lastly, when measuring the inventory count, we log the maximum inventory count\nseen throughout the episode, which is a lower bound on the potential number of items collected since\nthe agent can later throw out, place, or use these items to craft. As a result of these caveats, we denote\nthe “VPT (text)*” legend in Figure 15 and Figure 16 with an asterisk as we use the results reported in\n[5] directly for comparison.\nWe make several observations. First, we observe that our agents is more steerable: when conditioned\nto collect certain items (in bold), the agent collects (relatively) many more of those items than when\nconditioned on other instructions unrelated to that item, as well as compared to the unconditional VPT.\nWhen conditioned on tasks unrelated to the item (e.g. break a flower when interested in measuring\nlogs collected), we also observe that the agent pursues that item less than the unconditional agent.\nSecond, we observe that for the bolded instructions which we expect to stand out, we outperform\nVPT performance (dashed blue line) [5], even with half the amount of time in the episode rollout.\nThis suggests that our agent is both more steerable relative to the unconditioned VPT agent and the\ntext-conditioned VPT agent investigated in the VPT appendix [5].\nF.3\nPrompt Chaining Visualization\nWe visualize two specific episodes from the prompt chaining experiments in Section 4.3 in Figure 18\n(building a tower) and Figure 19 (crafting wooden planks).\n24\n(a) MineCLIP Text Evaluation\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nConditioned Prompt\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nEvaluation Prompt\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n(b) MineCLIP Visual Evaluation\nFigure 12: MineCLIP Evaluation. We measure the cosine distance between the goal embedding\ngiven to the agent and the MineCLIP video embeddings throughout the episode and record the\nminimum across the episode. Dashed box indicates the minimum along the row, and the number in\nthe diagonal box indicates the rank of the diagonal element in the row (0 specifies that the diagonal\nis the minimum element in the row). The ideal performance would be where the minimum values\nof each row lie on the diagonal. That is, the agent performs a specific task best when it is asked\nto perform that specific task. Left: We use the prior to convert the text into the goal embedding.\nAcross our 11 text MineCLIP evaluation tasks, STEVE-1 achieves the shortest distance between the\nepisode and the MineCLIP goal embedding when prompted appropriately for most cases. This shows\nthe strong general performance of STEVE-1 across a wide variety of short-horizon tasks. Right:\nWe embed the visual goals (Figure 14) with the MineCLIP video encoder. Across our 11 visual\nMineCLIP evaluation tasks, STEVE-1 achieves the shortest distance between the episode and the\nMineCLIP goal embedding when prompted appropriately except for in two cases, where it mixes up\ndigging and dirt and swimming and going underwater.\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nConditioned Prompt\ndig\ndirt\nsky\nleaves\nwood\nseeds\nflower\nexplore\nswim\nunderwater\ninventory\nEvaluation Prompt\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n1.00\n1.00\n0.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.90\n1.00\n0.10\n0.10\n0.10\n0.90\n0.00\n0.00\n0.00\n0.20\n0.00\n0.00\n0.00\n1.00\n0.80\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.50\n0.00\n0.20\n0.10\n0.70\n0.90\n0.30\n0.00\n0.20\n0.10\n0.10\n0.00\n0.00\n0.30\n0.00\n0.40\n1.00\n0.10\n0.00\n0.00\n0.00\n0.10\n0.00\n0.00\n0.20\n0.00\n0.10\n0.20\n0.50\n0.30\n0.00\n0.10\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.00\n0.10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.00\n0.10\n1.00\n1.00\n0.70\n0.00\n0.00\n0.50\n0.30\n0.40\n0.20\n0.30\n0.70\n0.90\n0.80\n0.70\n0.00\n0.00\n0.10\n0.00\n0.00\n0.00\n0.20\n0.10\n0.20\n0.50\n0.40\n0.00\n0.10\n0.30\n1.00\n0.30\n0.20\n0.10\n0.20\n0.20\n0.20\n0.10\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 13: Visual Evaluation Success-Rate Matrix. We manually reviewed the same videos used\nfor the MineCLIP Visual Evaluation matrix in Figure 3b in order to verify that the MineCLIP scores\ncorrespond well to human judgment. Thus, the values in this matrix are subject to human error\nand subjectivity. Each cell value shows how often the agent achieves the Evaluation Prompt when\nconditioned on the Conditioned Prompt (success-rate). The dotted cell(s) is\/are the maximum value\nin the row. Across the tasks, STEVE-1 achieves the highest success-rate when prompted appropriately\nexcept in three cases, where it breaks wood more than leaves, explores more than it swims, and swims\nmore than it goes underwater. This shows the strong general performance of STEVE-1 across a wide\nvariety of early-game short-horizon tasks.\n25\nFigure 14: Sample frames from each of the 11 visual goals. Note that the text overlaid on the frame\nis not present when we encode the 16-frame video with the MineCLIP video encoder, and is only\npresent for the figure visualization.\nShortened Name\nConditioning Variant Name\nText Prompt\ndig\ndig as far as possible\ndig as far as possible\ndirt\nget dirt\nget dirt\nsky\nlook at the sky\nlook at the sky\nleaves\nbreak leaves\nbreak leaves\nwood\nchop a tree\nchop a tree\nseeds\ncollect seeds\ncollect seeds\nflower\nbreak a flower\nbreak a flower\nexplore\ngo explore\ngo explore\nswim\ngo swimming\ngo swimming\nunderwater\ngo underwater\ngo underwater\ninventory\nopen inventory\nopen inventory\ndirt (engineered)\nget dirt ...\nget dirt, dig hole, dig dirt,\ngather a ton of dirt, collect dirt\nwood (engineered)\nchop down the tree ...\nchop down the tree, gather\nwood, pick up wood, chop it\ndown, break tree\nseeds (engineered)\nbreak tall grass ...\nbreak tall grass, break grass,\ncollect seeds, punch the\nground, run around in circles\ngetting seeds from bushes\nTable 3: A summary of the different ways we refer to the 11 direct-prompting (not prompt-chaining)\nearly-game evaluation task prompts. “Shortened Name” is the way we refer to the prompts in any\nsuccess-rate matrix or MineCLIP matrix. This is also how we refer to the visual prompts which\nhave no actual text. “Conditioning Variant Name” is the name used to refer to the “Text Prompts”\nin Figure 16, since not all text prompts fit in the figure. A “Conditioning Variant Name” with “...”\nindicates that this is an engineered text prompt that does not fit in the figure. Also, in reference to the\nexperiments Appendix C.5, the bolded and italicized prompts in the “Text Prompt” column are those\nthat were present verbatim in the text-video pair dataset.\n26\ndig a hole\nget dirt\nlook at the sky\nbreak leaves\nbreak wood\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nConditioning (Visual)\n0\n50\n100\n150\n200\n250\n300\nTravel Distance (Blocks)\nTravel Distance with Conditioning\nVPT (text)*\nVPT (uncond)\n(a)\ndig a hole\nget dirt\nlook at the sky\nbreak leaves\nbreak wood\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nConditioning (Visual)\n0\n2\n4\n6\n8\nSeeds Collected\nSeed Collection with Conditioning\nVPT (text)*\nVPT (uncond)\n(b)\ndig a hole\nget dirt\nlook at the sky\nbreak leaves\nbreak wood\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nConditioning (Visual)\n0\n5\n10\n15\n20\nWooden Logs Collected\nLog Collection with Conditioning\nVPT (text)*\nVPT (uncond)\n(c)\ndig a hole\nget dirt\nlook at the sky\nbreak leaves\nbreak wood\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nConditioning (Visual)\n0\n10\n20\n30\n40\n50\n60\n70\nDirt Collected\nDirt Collection with Conditioning\nVPT (text)*\nVPT (uncond)\n(d)\nFigure 15: Conditioning with Visual Goals. We plot the performance of the programmatic metrics,\nincluding their mean values and 95% confidence intervals, across different goal conditioning. See\nFigure 14 for visualization of these visual prompts. Plots are similar to Figure 20 in the VPT appendix\n[5]. Each conditioning variant is run with 10 trials, each with a different environmental seed and with\nan episode length of 3000 time steps (2.5 minutes gameplay). We use the policy that was trained using\nthe hyperparameters specified in Table 1, and with a conditional scaling value λ = 7. For comparison,\nthe dashed horizontal lines refer to the performance of the unconditional VPT agent (“VPT (uncond)”)\nand the text-conditioned VPT agent from Appendix I in [5] (“VPT (text)*”) conditioned on the\nrelevant text.\n27\ndig as far as possible\nget dirt\nlook at the sky\nbreak leaves\nchop a tree\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nget dirt ...\nchop down the tree ...\nbreak tall grass ...\nConditioning (Text)\n0\n50\n100\n150\n200\n250\n300\nTravel Distance (Blocks)\nTravel Distance with Conditioning\nVPT (text)*\nVPT (uncond)\n(a)\ndig as far as possible\nget dirt\nlook at the sky\nbreak leaves\nchop a tree\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nget dirt ...\nchop down the tree ...\nbreak tall grass ...\nConditioning (Text)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nSeeds Collected\nSeed Collection with Conditioning\nVPT (text)*\nVPT (uncond)\n(b)\ndig as far as possible\nget dirt\nlook at the sky\nbreak leaves\nchop a tree\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nget dirt ...\nchop down the tree ...\nbreak tall grass ...\nConditioning (Text)\n0\n2\n4\n6\n8\n10\n12\nWooden Logs Collected\nLog Collection with Conditioning\nVPT (text)*\nVPT (uncond)\n(c)\ndig as far as possible\nget dirt\nlook at the sky\nbreak leaves\nchop a tree\ncollect seeds\nbreak a ﬂower\ngo explore\ngo swimming\ngo underwater\nopen inventory\nget dirt ...\nchop down the tree ...\nbreak tall grass ...\nConditioning (Text)\n0\n5\n10\n15\n20\n25\n30\nDirt Collected\nDirt Collection with Conditioning\nVPT (text)*\nVPT (uncond)\n(d)\nFigure 16: Conditioning with Text goals. See Table 3 for the exact text string used for each\nconditioning variant. We use the same policy model but with a conditional scaling value λ = 6. We\nobserve strong steerability which outperforms the text-conditioned VPT in Appendix I of [5], and we\nobserve that prompt-engineering can improve performance.\n0\n2\n4\n6\n8\n10\n100\n200\n300\nTravel Distance (Blocks)\n0\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\nSeeds Collected\n0\n2\n4\n6\n8\n10\n0\n5\n10\n15\n20\nWooden Logs Collected\n0\n2\n4\n6\n8\n10\n20\n40\n60\nDirt Collected\nConditional Scale λ\nFigure 17: The conditional scale λ in classifier-free guidance [23] can be tuned to improve perfor-\nmance in each of the programmatic tasks. By tuning λ to use classifier-free guidance at inference\ntime, STEVE-1 is able to collect 7.5× more dirt, 15× more wood, 23× more seeds, and travel 1.7×\nfurther than at λ = 0 (no guidance).\n28\nTask\nPrompts\nPrecondition\ndig a hole, get dirt, look at the\nsky, break leaves, get wood,\nget seeds, break a flower, go\nexplore, go swimming, go\nunderwater, open inventory\nUse the best-performing\nprompts in Table 3 in the\nAppendix.\nFor grass and flowers, it works\nbest when grass and flowers\nare in the current biome. We\nfound breaking flowers to be\nless reliable than the others.\nmake a tower\n“build a tower”\nbuilding blocks in hotbar. To\nobtain building blocks, you\ncan use the dig a hole or get\nwood prompts from Table 3.\ncraft wooden planks\n“make wooden planks, craft\nwooden planks” (*)\nwooden logs. To obtain\nwooden logs, use the get\nwood prompt from Table 3.\nplace torches, place a crafting\ntable, place wooden planks\n“place [torches\/a crafting\ntable\/wooden planks]”\n[torches\/crafting table\/wooden\nplanks] in the hotbar\nmake a crafting table\n“make a crafting table”\nwooden planks.\nbreak stone\n“mine stone, go mining, get\nstone” (*)\nget cobblestone\n“mine stone, go mining, get\ncobblestone” (*)\npickaxe in hotbar.\ncreate a wooden pickaxe\n“craft a wooden pickaxe,\nmake a wooden pickaxe” (*)\nalready looking at a placed\ncrafting table, has necessary\nmaterials.\nhit a sheep, hit a pig, hit a cow\n“kill a [sheep\/pig\/cow]”\nagent is close to and looking\nat the sheep\/pig\/cow. (See\nAppendix B.1).\nTable 4: Examples of tasks that STEVE-1 is able to achieve. (*) denotes that this is a single\nprompt-engineered prompt. Note that this list represents only a subset of the capabilities of STEVE-\n1 due to the difficulty associated with evaluating and describing the capabilities of open-ended\nmodels, which is an open research problem itself since capability depends strongly on preconditions,\nprompt engineering, and our own ability to come up with varied and challenging tasks. Please see\nAppendix B.4 for further discussion.\nModel\nSimple Prompt\nComplex Prompt\nStable Diffusion [50]\nsteampunk market interior\nsteampunk market interior, colorful,\n3D scene, Greg Rutkowski, Zabrocki,\nKarlkka, Jayison Devadas, trending\non ArtStation, 8K, ultra-wide-angle,\nzenith view, pincushion lens effect\n[20]\nSTEVE-1\ncollect seeds\nbreak tall grass, break grass, collect\nseeds, punch the ground, run around\nin circles getting seeds from bushes\nTable 5: Example of evolving simple prompts into more complex ones for various models.\n29\n0\n500\n1000\n1500\n2000\n2500\n3000\nEpisode Timestep\n0\n5\n10\n15\n20\n25\n30\n35\nDirt in Inventory\n1\n2\n3\n4\n70\n75\n80\n85\n90\n95\n100\n105\nAgent Height Position\nBuild a Tower\n1: Frame #52\n3: Frame #2015\n2: Frame #1500\n4: Frame #2540\nFigure 18: Build a Tower task. (Left) We track the amount of dirt in the inventory and the agent’s\nheight position (y-axis) throughout the episode. In the first 1500 timesteps, the agent is conditioned\non the visual get dirt goal, then the agent is conditioned on the visual build a tower goal for the final\n1500 timesteps. Vertical dotted lines with numbers indicate the corresponding frames on the right.\n(Right) The agent’s observation frames at 4 different points in the episode. First the agent collects\ndirt, then begins to build the tower using the dirt blocks.\n0\n500\n1000\n1500\n2000\n2500\n3000\nEpisode Timestep\n0\n1\n2\n3\n4\n5\n6\n7\nLogs in Inventory\n1\n2\n3\n4\n0\n5\n10\n15\n20\n25\n30\n35\nPlanks in Inventory\nMake Wooden Planks\n1: Frame #220\n3: Frame #1500\n2: Frame #876\n4: Frame #1540\nFigure 19: Make Wooden Planks task. (Left) We track the number of logs and planks in the inventory.\nIn the first 1500 timesteps, the agent is conditioned on the visual break wood goal, then the agent is\nconditioned on the visual craft wooden planks goal for the final 1500 timesteps. Vertical dotted lines\nwith numbers indicate the corresponding frames on the right. (Right) The agent’s observation frames\nat 4 different points in the episode. First the agent breaks trees to collect wooden logs, then opens the\ninventory and crafts wooden planks.\n30\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.pdf"}
{"title":"Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction","authors":"Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"We study the problem of learning goal-conditioned policies in Minecraft, a\npopular, widely accessible yet challenging open-ended environment for\ndeveloping human-level multi-task agents. We first identify two main challenges\nof learning such policies: 1) the indistinguishability of tasks from the state\ndistribution, due to the vast scene diversity, and 2) the non-stationary nature\nof environment dynamics caused by partial observability. To tackle the first\nchallenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage\nthe emergence of goal-relevant visual state representations. To tackle the\nsecond challenge, the policy is further fueled by an adaptive horizon\nprediction module that helps alleviate the learning uncertainty brought by the\nnon-stationary dynamics. Experiments on 20 Minecraft tasks show that our method\nsignificantly outperforms the best baseline so far; in many of them, we double\nthe performance. Our ablation and exploratory studies then explain how our\napproach beat the counterparts and also unveil the surprising bonus of\nzero-shot generalization to new scenes (biomes). We hope our agent could help\nshed some light on learning goal-conditioned, multi-task agents in challenging,\nopen-ended environments like Minecraft.","url":"http:\/\/arxiv.org\/abs\/2301.10034v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2301.10034v3","published":1674288938000,"comment":"This paper is accepted by CVPR2023","pdf_text":"Open-World Multi-Task Control Through\nGoal-Aware Representation Learning and Adaptive Horizon Prediction\nShaofei Cai1,2, Zihao Wang1,2, Xiaojian Ma3, Anji Liu3, Yitao Liang1,4\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2School of Intelligence Science and Technology, Peking University\n3Computer Science Department, University of California, Los Angeles\n4Beijing Institute for General Artificial Intelligence (BIGAI)\n{caishaofei,zhwang}@stu.pku.edu.cn,xiaojian.ma@ucla.edu\nliuanji@cs.ucla.edu,yitaol@pku.edu.cn\nAbstract\nWe study the problem of learning goal-conditioned poli-\ncies in Minecraft, a popular, widely accessible yet challeng-\ning open-ended environment for developing human-level\nmulti-task agents. We first identify two main challenges of\nlearning such policies: 1) the indistinguishability of tasks\nfrom the state distribution, due to the vast scene diversity,\nand 2) the non-stationary nature of environment dynamics\ncaused by partial observability. To tackle the first challenge,\nwe propose Goal-Sensitive Backbone (GSB) for the policy\nto encourage the emergence of goal-relevant visual state\nrepresentations. To tackle the second challenge, the pol-\nicy is further fueled by an adaptive horizon prediction mod-\nule that helps alleviate the learning uncertainty brought by\nthe non-stationary dynamics. Experiments on 20 Minecraft\ntasks show that our method significantly outperforms the\nbest baseline so far; in many of them, we double the perfor-\nmance. Our ablation and exploratory studies then explain\nhow our approach beat the counterparts and also unveil the\nsurprising bonus of zero-shot generalization to new scenes\n(biomes). We hope our agent could help shed some light on\nlearning goal-conditioned, multi-task agents in challeng-\ning, open-ended environments like Minecraft. The code is\nreleased at https:\/\/github.com\/CraftJarvis\/\nMC-Controller.\n1. Introduction\nBuilding agents that can accomplish a vast and diverse\nsuite of tasks in an open-ended world is considered a key\nchallenge towards devising generally capable artificial in-\ntelligence [2, 3, 6, 35]. In recent years, environments like\nMinecraft have drawn much attention from the related re-\ncombat pig\nharvest poppy\nharvest wood\ncombat sheep\npick-place\nwindow-open\nbox-close\nstick-push\nMinecraft\nMeta-world\nFigure 1. Comparison of states between Meta-world [49] (left) and\nMinecraft [24] (right) based on t-SNE visualization. The points\nwith the same color represent states from the trajectories that com-\nplete the same task. It can be seen that the states are much more\ndistinguishable in terms of tasks in Meta-world than in Minecraft,\nimplying the higher diversity of states and tasks in open worlds\nlike Minecraft over traditional multi-task agent learning environ-\nments like Meta-world.\nsearch communities [16,18–20,26], since they are not only\npopular, and widely accessible, but also offer an open-\nended universe with myriad of tasks, making them great\nplatforms for developing human-level multi-task agents.\nAlthough groundbreaking successes have been observed\nin many challenging sequential decision-making problems\nsuch as Atari[32], Go[39], and MOBA games[13, 44, 45],\nsuch successes have not been transferred to those open\nworlds. To understand the gap and design corresponding so-\nlutions, we need to first understand the distinct challenges\n1\narXiv:2301.10034v3  [cs.AI]  12 Oct 2023\nbrought by these environments. Let’s take Minecraft [24]\nas an example: there are over twenty types of landscapes\nranging from flat lands like Savannah and desert to rough\nmountains with forests and caves.\nThese diverse land-\nscapes also enable countless tasks that could be achieved by\nthe agents: mining, harvesting, farming, combating, con-\nstructing, etc. Compared to canonical agent learning en-\nvironments like Go [39], Atari [32], and robotic control\nsuite [41, 43, 48], Minecraft provides a substantially more\ndiverse distribution of states thanks to the rich scenes and\ntasks built with the game, making it exceptionally diffi-\ncult to extract the pivotal task-relevant visual state repre-\nsentations for goal-conditioned policies. To help our read-\ners understand the significance of this challenge, we visual-\nize the states from trajectories that complete some tasks in\nMinecraft and Meta-world [48] (a popular multi-task learn-\ning environment but with fewer states and tasks) in Fig. 1.\nStates of different tasks are annotated with different colors.\nClearly, the states in Minecraft are much less distinguish-\nable in terms of tasks than in Meta-world. Therefore goal-\nconditioned policies are more likely to struggle in mapping\nthose states and tasks (served as goals) to actions.\nAnother grand challenge in an open-ended environment\nlike Minecraft hails from the setting of such games, where\nan agent can only have very limited observations of the\nworld.\nFor example, in MineDoJo [16] (a recent agent\nbenchmark built on Minecraft), the observation space com-\nprises a first-person view image and a list of possessed\nitems. However, many more aspects of the surroundings re-\nmain hidden from the agents. That is, the agent now has to\nwork with a partially observable environment. A plague\nembedded with such an environment is non-stationary dy-\nnamics, which makes it almost impossible to predict what\nwill happen next. Therefore, the distances from states to the\ncurrent goal become much less clear due to the world un-\ncertainty, leading to less distinguishable states in terms of\ngoal completeness and more faulty decisions emitted by the\ngoal-conditioned policies.\nThis paper aims at mitigating both aforementioned chal-\nlenges that emerge from most open-world environments.\nFirst, we observe that the architecture of the policy network\nis crucial to learning goal-relevant visual state representa-\ntions that allow goal-conditioned actions in domains with\nlow inter-goal state diversity (cf. Fig. 1). To this end, we\npropose Goal-Sensitive Backbone (GSB), which enables ef-\nfective learning goal-conditioned policies over 20 tasks in\nthe Minecraft domain. Next, to mitigate the challenge posed\nby the partially observed and non-stationary environment,\nwe introduce horizon as an extra condition for the policy\nand a corresponding horizon prediction module. Specifi-\ncally, the policy is also explicitly conditioned on the remain-\ning time steps till achieving certain goals (i.e., distance-to-\ngoal). We find it significantly boosts the performance of\nour agents in open-world multi-task domains. However, the\nground-truth distance-to-goal is unavailable during evalu-\nation. To fix this problem, we train a horizon prediction\nmodule and feed the estimated distance-to-goal to the hori-\nzon commanding policy in evaluation. This leads to a 27%\ngain in average success rate under the multi-task settings.\nWe evaluate the proposed approaches based on the sim-\nple yet effective behavior cloning algorithm [10]. The ex-\nperiments are conducted in three common biomes. In multi-\ntask settings, our proposed method outperforms the base-\nline in terms of success rate and precision by a large mar-\ngin. It also achieves consistent improvement in single-task\nsettings. Our ablation and exploratory studies then explain\nhow our approach beat the counterparts and also unveil the\nsurprising bonus of zero-shot generalization to new scenes\n(biomes).\nTo summarize, targeting two identified challenges dis-\ntinct to open worlds, our contributions are threefold:\n• We propose Goal-Sensitive Backbone (GSB), a neural\nnetwork that enables effective learning goal-relevant vi-\nsual state representations at multiple levels for goal-\nconditioned policies, aiming at addressing the challenge\nof diverse state distribution in open-ended environments.\n• We further introduce adaptive horizon prediction to ex-\nplicitly condition the policy on the distance from the cur-\nrent state to the goal, yielding much better performances\nin a partially observable open-ended environment with\nnon-stationary dynamics.\n• We conduct extensive studies on the popular yet challeng-\ning Minecraft domain with baselines and our proposed\nmethod. The results demonstrate superior advantages of\nour approach over the counterparts in terms of both suc-\ncess rate and precision of task completion.\n2. Preliminaries\nGoal-conditioned policy, as its name suggests, is a type of\nagent’s policy π for decision-making that is conditioned on\ngoals besides states. Specifically, we denote π(a|s, g) as\na goal-conditioned policy that maps the current state s and\ngoal g to an action a. Compared to the canonical formula-\ntion of policy where the goal is absent, the goal-conditioned\npolicy offers flexibility of learning multi-task agent as it al-\nlows different behaviors for different tasks by simply alter-\ning the goal. There are multiple ways to specify the goal,\ne.g., natural language instructions [2] and goal images [36].\nGoal-conditioned imitation learning is a simple yet ef-\nfective way to learn goal-conditioned policies. Specifically,\nπ(a|s, g) is optimized by imitating the demonstrations D,\nwhere D = {τ 1, τ 2, τ 3, . . . } is a collection of trajectories\nτ i. A trajectory is a sequence of states, actions, and goals,\ndefined as τ i = {(si\nt, ai\nt, gi)}T\nt=0, where T is the trajectory\nlength. The imitation learning objective is to maximize the\n2\nCross-biome\nEnvironment\nSingle-biome\nEnvironment\nkill sheep in Snowy Plains\nchop tree in Plains\nkill sheep in Plains\nFigure 2. Demonstrations of the cross-biome environment and\nthe more challenging single-biome environment. The challenge\ncomes from the fact that the agent needs to learn diverse behaviors\nin similar states conditioned on different goals.\nlikelihood of the action in demonstrations when attempting\nto reach the desired goal\nJIL(π) = Eτ∼D\n\u0002 XT\nt=0 log π(at|st, g)\n\u0003\n.\n(1)\nNotation. At each timestep, our architecture takes in a tu-\nple (st, at, ht, g, at−1) as the input, where st = {oI\nt , oE\nt },\noI\nt is the raw image observation, oE\nt is the extra observation\nprovides by the environments. ht comes from the demon-\nstration. ˜ht and ˜at are the predicted horizon and action,\nrespectively. For simplicity, we also use the same symbols\n(oE\nt , g, at−1) to represent their embeddings.\n3. Method\nIn this section, we describe the proposed algorithm for\nlearning goal-conditioned policies that are capable of com-\npleting various preliminary tasks in open-world domains.\nFirst, we revisit and provide a detailed illustration of the\nidentified challenges in open-world domains (§3.1). Aim-\ning at solving these challenges, we proceed to introduce\nthe proposed goal-sensitive backbone (§3.2) and adaptive\nhorizon prediction module (§3.3). Finally, we provide an\noverview of the proposed method in Section 3.4.\n3.1. Challenges\nAs demonstrated in Section 1, the first major challenge\nof open-world environments is the indistinguishability of\nstates in terms of different goals (cf. Fig. 1). That is, it is\noften hard to identify the task\/goal by looking at individual\nstates. Compared to environments with clear goal indicators\nin their states, agents in open-world domains need to learn\ngoal-conditioned diverse behaviors under similar states.\nThis challenge can be reflected by the illustrative exper-\niment in Fig. 2. Two multi-task environments are created\nbased on the Minecraft domain. Both environments consist\nof two preliminary tasks: collect logs and hunt sheep, where\nthe former can be done by chopping trees and the latter re-\nquires the agent to slaughter sheep. Both tasks require the\nagent to first locate and approach the corresponding target.\nAs shown in Fig. 2 (center), in the single-biome environ-\nment (blue blob in Fig. 2), the agent is tasked to collect\nlogs and hunt sheep both inside a randomly generated plain\narea with grass, trees, and various mobs. In contrast, in\nthe cross-biome environment (red blob in Fig. 2), whenever\nthe agent is tasked to hunt sheep, it is spawned randomly\nin a snowy plain. Although different in visual appearance,\nsnowy plains and plains have very similar terrains, so the\ndifficulty of each task in the cross-biome environment is\nsimilar to its counterpart in the single-biome environment.\nThe main consequence of this change is that the agent can\ndetermine its goal by solely looking at the current state,\nwhich mimics the setting of Meta-World in Fig. 1(left).\nWe collect demonstrations by filtering successful trajec-\ntories played by VPT [4] (see §4.1 for more details) and use\nbehavior cloning to train multi-task policies on both envi-\nronments. Perhaps surprisingly, as shown in Fig. 2, despite\nthe minor difference, performance in the single-biome envi-\nronment is significantly weaker than in the cross-biome one.\nThis clearly demonstrates that the common practice of di-\nrectly concatenating observation features and goal features\nsuffer from learning diverse actions (e.g., locate trees, find\nsheep) given similar observations. In contrast, in the cross-\nbiome environment, the difficulty of the two tasks funda-\nmentally remains the same, yet the agent only needs to learn\na consistent behavior in each biome (i.e., plains and snow\nfields). This alleviates the need to learn goal-conditioned\ndiverse behaviors in similar states and leads to a better suc-\ncess rate.\nThe second key challenge comes from the partial ob-\nservability of the game and non-stationary environment dy-\nnamics.\nSpecifically, in Minecraft, the biome and mobs\nsurrounding the agent are generated procedurally and ran-\ndomly after each reset. Further, only a small fraction of the\nwhole terrain is visible to the agent in one observation, lead-\ning to more uncertainty of the world. From the perspective\nof learning goal-conditioned policies, the distances from\nstates to the current goal will become much less clear com-\npared to canonical learning environments like Atari [12].\nWe refer to Appendix B for more discussion on this. Since\nthe goal-conditioned policies also rely on distinguishable\nstates in terms of goal completeness, they’re more likely to\nmake wrong decisions as a result of world uncertainty.\n3\nGoal Space\nHunt a cow\nShear a sheep\n…\nChop Trees\nAction Space\nExtra Observation\nMove\nCam\nAttack\nUse\nImage Observation\nCompass GPS\nBiome\nVoxels\n𝒐!\n\"\n𝒈\n𝒐!\n#\n𝒂!$%\nEmbed\nEmbed\nEmbed\nℎ%!\nℎ!\nEmbed\n𝒂&!\n𝒂!\nℒ!\nℒ\"\nGSB\n𝑰!\n&\n𝒇!\nTraining\n𝜇\n𝜋!\nConcantenate\nHorizon Loss\n𝒐!\n\"\n𝒈\n𝒐!\n#\n𝒂!$%\nEmbed\nEmbed\nEmbed\nℎ%!\nℎ)!\nEmbed\n𝒂!\nGSB\n𝑰!\n&\n𝒇!\nEvaluation\n𝜇\n𝜋!\nConcantenate\nAdjust\nAdaptive Horizon \nPrediction\nGoal-Sensitive Backbone (GSB)\n𝒙(()\n𝒈\nFC\nReLU\nFC\nConv\nReLU\nConv\n𝒙((*%)\nReLU\nSigmoid\n×\n𝒐!\n\"\n𝒈\nConv\nMax\nG-Conv\nBlock\nG-Conv\nBlock\n×𝟑\n+\nFigure 3. Our Goal-conditioned Policy Architecture. Our contributions are in red and purple. Right: The goal-sensitive backbone\n(GSB) is a key component to incentivize goal-condition behaviors. It consists of a stack of g-conv blocks. It takes the image observation\noI\nt and the goal embedding g as input, and outputs the goal-attended visual representation Ig\nt . The multimodal joint representation f t is\nthe concatenation of visual representation Ig\nt , goal embedding g, extra observation embedding oE\nt and previous action embedding at−1.\nThe horizon prediction module µ uses it to predict the horizon ˜ht while the horizon commanding policy πθ uses it to predict the action ˜at.\nTop: During the training, the predicted horizon ˜ht is only used to compute the horizon loss Lh. The policy is conditioned on ht that comes\nfrom the demonstration. Bottom: During the evaluation, the policy is conditioned on the predicted horizon ˜ht which needs to be adjusted.\n3.2. Incentivize Goal-Conditioned Behavior with\nStacked Goal-Sensitive Backbone\nAs elaborated in Section 3.1, learning goal-conditioned\npolicies becomes extremely hard when states collected from\ntrajectories that accomplish different tasks are indistin-\nguishable. While certain algorithmic design choices could\nimprove multi-task performance in such open-world envi-\nronments, we find that the structure of the policy network\nis a key factor towards higher episode reward.\nSpecifi-\ncally, we observe that existing CNN-based backbones can\nexcel at completing many single tasks (e.g., hunt cow, col-\nlect stone), but struggle to learn goal-conditioned behavior\nwhen training on the tasks in a goal-conditioned manner.\nThis motivates the need to properly fuse goal information\ninto the network. Despite the existence of various feature\nfusion approaches such as concatenation and Bilinear lay-\ners [27], they all perform poorly even with a moderate num-\nber of tasks. This motivates the need to carry goal informa-\ntion into multiple layers of the network. Specifically, we\npropose goal-sensitive backbone (GSB), which effectively\nblends goal information to the state features at multiple lev-\nels. As shown in Fig. 3 (right), GSB is composed with mul-\ntiple goal convolution blocks (g-conv block), which are ob-\ntained by augmenting the vanilla convolution block with a\ngoal branch. Functionally, it can provide deep feature fu-\nsion between multi-level visual features and the goal infor-\nmation. As we will proceed to show in Section 4.3, adding\nGSB can lead to significant performance boost in multi-task\nenvironments. The g-conv block processes its input visual\nfeatures x(l) ∈RC×H×W with two convolution layers\nˆx(l) = ReLU(Conv(ReLU(Conv(x(l))))).\n(2)\nMeanwhile, it maps the goal embedding g to the same fea-\nture space as the intermediate features ˆx(l) with two fully-\nconnected layers, decribed as\nˆg(l) = FC(ReLU(FC(g))).\n(3)\nThe goal feature ˆg(l) is then used to modulate the interme-\ndiate features ˆx(l) channel-wise. By adding a residual con-\nnection [21], the output feature x(l+1) is expressed by\nx(l+1) = σ(ˆg(l)) ⊙ˆx(l) + x(l),\n(4)\nwhere σ(·) is the sigmoid function and ⊙is the element-\nwise product.\nThis channel-wise modulation encourages\nthe module to focus on goal-specific regions and discard the\nbackground information by adaptively weighing the chan-\nnel importance.\nWe highlight that the g-conv block can\nbe plugged into any convolution backbone to improve its\ncapability of extracting goal-aware visual features.\nThe\nproposed goal-sensitive backbone is constructed by replac-\ning 6 convolution blocks of the widely-adopted Impala\nCNN [14] to g-conv blocks. In our experiments, a GSB\nis used to compute goal-conditioned state features Ig\nt =\nGSB(oI\nt , g). Such an idea of fusing condition information\ninto the backbone layer by layer was also used by some\nprior works [5, 22, 33, 34]. Here, we demonstrate that it\nworks in a critical role for open-world multi-task control.\n4\n3.3. Combat World Uncertainty with Adaptive\nHorizon Prediction\nTo address the challenge brought by the uncertainty of\nthe world, we need to ensure the goal-conditioned policies\nto be more aware of goal-completeness given the current\nstate.\nWe observe that conditioning the policy addition-\nally on the number of remaining steps toward achieving a\ngoal, i.e., distance-to-goal, or horizon, can significantly im-\nprove the accuracy of predicted actions on held-out offline\ndatasets [17,37]. Here, we define the horizon ht := T −t,\nwhere T is the trajectory length, as the remaining time steps\nto complete the given goal. This motivates the design of a\nhorizon commanding policy πθ : S × G × H →A that\ntakes a state s, a goal g, and a horizon h as inputs and out-\nputs an action a. A key problem of the horizon commanding\npolicy is that it cannot be directly used for evaluation: dur-\ning gameplay, horizon is unknown as it requires completing\nthe whole trajectory. To fix this problem, we introduce an\nadditional horizon prediction module, which estimates the\nhorizon given a state s and a goal g. Combining the two\nmodules together, we can apply the fruitful horizon com-\nmanding policy during gameplay.\nBoth modules can be trained efficiently with dense su-\npervision. Specifically, the horizon commanding policy πθ\ncan be learned by any policy loss specified by RL algo-\nrithms. For example, when behavior cloning is used, πθ\ncan be optimized by minimizing the loss\nLa = −log πθ(at|ht, f t),\n(5)\nwhere f t is the joint representation of the state and goal\nembedded by a neural network (see §3.4). The horizon pre-\ndiction module is trained by a supervised learning loss\nLh = −log µ(ht|f t),\n(6)\nwhere µ is a network that predicts the horizon.\nDuring the evaluation, after computing the embedding\nf t for st and g, the horizon prediction module µ is first in-\nvoked to compute an estimated horizon ˜ht = µ(f t). This\npredicted horizon can then be fed to the horizon command-\ning policy to compute the action distribution πθ(at|˜ht, f t).\nIn practice, we observe that feeding an adaptive version of\n˜ht, defined as ˆht := max(˜ht −c, 0) (c is a hyperparame-\nter), to πθ leads to better performance. We hypothesize that\nthis advantageous behavior comes from the fact that by sup-\nplying the adaptive horizon ˆht, the agent is encouraged to\nchoose actions that lead to speedy completion of the goal.\nThe effectiveness of the adaptive horizon will be demon-\nstrated in Section 4.3.\n3.4. Model Summary\nAs shown in Fig. 3, our model sequentially connects the\nproposed goal-sensitive backbone, horizon prediction mod-\nule, and horizon commanding policy. At each time step\n(a) Flat\n(b) Plains\n(c) Forest\nFigure 4. Snapshots of the RGB camera view in three biomes.\nt, the image observation and goal information are first fed\nforward into the goal-sensitive backbone to compute goal-\naware visual feature Ig\nt . The visual feature is then fused\nwith additional input information including the extra obser-\nvation embedding oE\nt , the goal embedding g, and the pre-\nvious action embedding at−1 by concatenation and a feed-\nforward network:\nf t = FFN(\n\u0002\nIg\nt ∥oE\nt ∥g ∥at−1\n\u0003\n).\n(7)\nThen, f t is input to the horizon prediction module to predict\nhorizon ˜ht = µ(f t). And the horizon commanding policy\ntakes in the horizon and features f t to compute the action.\nWhen trained with behavior cloning, the overall objective\nfunction is L = La + Lh. During the evaluation, the adap-\ntive horizon ˆht is fed to the horizon commanding policy in\nreplacement of ˜ht.\n4. Experiments\nThis section analyzes and evaluates the proposed goal-\nsensitive backbone and the adaptive horizon prediction\nmodule in the open-world domain Minecraft. To minimize\nperformance variation caused by the design choices in RL\nalgorithms, we build the proposed method on top of the sim-\nple yet effective behavior cloning algorithm. In Section 4.1,\nwe first introduce three suites of tasks; the agent is asked\nto collect and combat various target objects\/mobs with in-\ndistinguishable states conditioned on different goals (chal-\nlenge #1) and non-stationary environment dynamics (chal-\nlenge #2). Single-task and multi-task performance on the\nbenchmarks is evaluated and analyzed in Section 4.2, and\nablation studies are conducted in Section 4.3. Finally, we\nunveil the surprising bonus of zero-shot generalization to\nnew scenes and tasks in Section 4.4.\n4.1. Experimental Setup\nEnvironment and task. To best expose the challenges de-\nscribed in Sections 1 and 3.1, a key design principle of our\nbenchmark environments is to task the agent to complete\nmultiple preliminary tasks in similar yet highly random-\nized scenes. By specifying the biome that surrounds the\nagent, Minecraft provides a perfect way to create such en-\nvironments. Specifically, as shown in Fig. 4, every biome\nhas unique and consistent observations; randomness comes\nfrom the fact that the terrain is generated randomly in each\nepisode. To evaluate the scalability of the proposed method\nin terms of the number of tasks, we choose Plains and\n5\nForest, the two most common biomes that contain a large\nnumber of resources and mobs.\nIn addition to the two challenges, Plains and Forest\nalso add unique difficulties to learning goal-conditioned\npolicies.\nSpecifically, although we have better views in\nPlains, the resources\/targets are located further away\nfrom the agent and require more exploration. In contrast,\nthere exist more occlusions and obstacles in Forest.\nThe Plains benchmark consists of four tasks: har-\nvest oak wood (\n), and Combat sheep (\n), cow (\n),\npig (\n).\nIn the Forest benchmark, the agent is\ntasked to complete thirteen tasks: combat sheep (\n),\ncow (\n), pig (\n), harvest dirt (\n), sand (\n),\noak wood (\n), birch wood (\n), oak leaves (\n),\nbirch leaves (\n), wool (\n), grass (\n),\npoppy ( ), orange tulip ( ).\nIn addition to the above two benchmarks, we also\ntest the agent on a “hunt animals” benchmark based\non the Flat biome, which contains a flattened world.\nSpecifically, the agent needs to combat sheep (\n),\ncow (\n), pig (\n), spider (\n), polar bear (\n),\nchicken (\n), donkey (\n), horse (\n), wolf (\n),\nllama (\n), mushroom cow (\n) in the Flat environ-\nment.\nCompared to other benchmarks, the challenge of\nFlat comes from the fact that the mobs are constantly\nwondering around, which makes it hard to locate and ap-\nproach the correct target.\nWe adopt the original observation space provided by\nMineDoJo [16], which includes a RGB camera-view,\nyaw\/pitch angle, GPS location, and the type of 3 × 3 blocks\nsurrounding the agent.\nWe discretize the original multi-\ndiscrete action space provided by MineDojo into 42 discrete\nactions. Details are included in Appendix A.1.\nData collection pipeline.\nOne significant downside of\nbehavior cloning algorithms is the need for high-quality\nand densely-labeled trajectories, which often requires enor-\nmous human effort to collect. To mitigate this problem,\nwe collect goal-conditioned demonstrations by filtering suc-\ncessful trajectories from gameplays by pretrained non-goal-\nconditioned policies.\nSpecifically, we adopt Video Pre-\nTraining (VPT) [4], which is trained on tremendous amount\nof non-goal-conditioned gameplays. We rollout the VPT\npolicy in the three benchmarks and record all episodes that\naccomplishes any of the defined goals.\nThese trajecto-\nries are then converted to a goal-conditioned demonstration\ndataset. Please refer to Appendix A.2 for detailed settings\nand efficiency analysis of our data collection pipeline.\nEvaluation. During the evaluation, the maximum episode\nlength is set to 600, 600, and 300 on the Flat, Plains\nand Forest benchmarks, respectively.\nPlains and\nForest are given more time steps since, in these environ-\nments, the agent needs more time to locate and approach the\ntarget. We use Success Rate and Precision as our evaluation\nmetrics. A gameplay is successful if the agent completes the\ngoal within the episode. Precision is defined as the number\nof times the specified goal is achieved divided by the total\nnumber of goals completed in an episode. It measures how\nwell the agent can be aware of the specified goal, instead of\nsimply accomplishing any goal during gameplay.\n4.2. Experimental Results\nWe first focus on the simpler single-task learning set-\nting in order to isolate the challenge introduced by non-\nstationary dynamics and partial observability (§4.2.1). We\nthen examine whether the proposed method can better ad-\ndress both challenges by examining its multi-task perfor-\nmance (§4.2.2).\n4.2.1\nSingle task experiments\nWe select three typical tasks, i.e., harvest log, hunt cow,\nand hunt sheep, from the Plains benchmark for single-\ntask training. We compare the proposed method against\nthe following baselines. First, MineAgent [16] is an online\nRL algorithm that leverages pretrained state representations\nand dense reward functions to boost training. BC (VPT) [4],\nBC (CLIP) [16], and BC (I-CNN) [14] are variants of the be-\nhavior cloning algorithm that use different backbone mod-\nels (indicated in the corresponding brackets) for state fea-\nture extraction. The backbones are finetuned with the BC\nloss (see Appendix A.3 for more details).\nResults are reported in Table 1. First, we observe that\neven the individual tasks are extremely challenging for on-\nline RL algorithms such as MineAgent, even its networks\nare pretrained on Minecraft data.\nWe attribute this fail-\nure to its inconsistent dense reward when facing a hard-\nexploration task (e.g., the additional provided reward is not\nconsistently higher when the agent is moving closer to a\ntarget object).\nNext, compared to BC (I-CNN) that uses\na randomly initialized impala CNN model, the Minecraft-\npretrained backbones in BC (VPT) and BC (CLIP) do not\nbring any benefit. This could be caused by the lack of plas-\nticity, i.e., the ability to learn in these well-trained models,\nechoing similar findings in computer vision and RL [11].\nFinally, our approach outperforms all baseline methods, es-\npecially in terms of precision. This demonstrates that our\nmethod is more robust against non-stationary dynamics and\npartially observable observations.\n4.2.2\nMulti-task experiments\nWe move on to evaluate the proposed method on the three\nmulti-task benchmarks introduced in Section 4.1. The base-\nline includes three behavior cloning methods (we use “MT-\nBC” as an abbreviation of multi-task behavior cloning). We\nalso include two variations of our method: one without the\ngoal-sensitive backbone, and the other without the adaptive\n6\nTable 1. Results of single-goal tasks (§4.2.1) on Plains.\nMethod\nSuccess Rate (%)\nPrecision (%)\nMineAgent [16] 00±00 01±00 01±00\n–\n–\n–\nBC (CLIP) [16]\n18±06 26±05 25±06 51±08 43±08 44±05\nBC (VPT) [4]\n22±08 27±06 22±06 58±09 46±05 42±05\nBC (I-CNN) [14] 45±05 46±04 48±07 86±05 55±12 45±07\nOurs\n50±07 58±10 60±08 83±10 75±10 75±06\nTable 2. Results of multi-goal tasks (§4.2.2) on three biomes.\nMethod\nAvg. Success Rate (%) Avg. Precision (%)\nPlains\nFlat\nForest\nPlains\nFlat\nForest\nMT-BC (VPT) [4]\n25±06 17±05 15±04 22±05 17±03 14±04\nMT-BC (CLIP) [16] 22±05 14±03 14±03 23±04 15±03 13±03\nMT-BC (I-CNN) [14] 25±02 18±02 15±03 23±04 14±02 13±03\nMT-BC (w\/ GSB)\n32±05 36±03 19±05 43±06 36±02 17±03\nOurs (I-CNN)\n31±06 31±04 18±02 22±03 28±04 15±04\nOurs (w\/ GSB)\n55±09 57±09 30±06 70±09 50±06 29±06\nhorizon prediction module. Results on the Plains, Flat,\nand Forest environments are reported in Table 2, respec-\ntively. First, we observe that our method significantly out-\nperforms all baselines in terms of both success rate and pre-\ncision in all three benchmarks. Moreover, scaling up the\nnumber of tasks does not necessarily deteriorate the per-\nformance of our method. Specifically, we compare the av-\nerage success rate on the Plains and Flat benchmark,\nwhich contain 4 and 9 tasks, respectively. While the base-\nlines struggle to maintain their success rate on the Flat\nenvironment, our approach is capable of maintaining high\nperformance despite the increased number of tasks. Putting\ntogether, results on multi-task benchmarks clearly demon-\nstrate the superiority of our method when facing open-world\nenvironments with the two elaborated challenges (cf. §3.1).\n4.3. Ablation Study\nAblation study on goal-sensitive backbone. To examine\nthe effectiveness of our proposed goal-sensitive backbone,\nwe compare the following two groups of architectures: 1)\nOurs (I-CNN) v.s.\nOurs (w\/ GSB), 2) MT-BC (I-CNN) v.s.\nMT-BC (w\/ GSB). The key distinction between the groups is\nwhether the backbone employs a standard Impala CNN or\na goal-sensitive backbone. As depicted in Table 2, our find-\nings indicate that the goal-sensitive backbone consistently\nenhances performance in terms of both success rate and pre-\ncision across all environments. Remarkably, in the Flat\nbiome, our approach with the goal-sensitive backbone at-\ntains a 26% and 22% performance improvement in success\nrate and precision, respectively. This demonstrates that the\ngoal-sensitive backbone effectively fuses the goal informa-\ntion into visual features and leads to goal-aware behavior.\nTable 3. Additional ablation experiments on Plains biome.\n# Method\nAvg. SR (%) Avg. P (%)\n1 Ours (GSB + horizon pred)\n55±09\n70±09\n2 Ours + RNN\n65±07\n67±08\n3 Ours −horizon pred + RNN\n39±08\n51±08\n4 Ours −horizon pred\n35±08\n45±15\n5 w\/o horizon loss\n47±06\n54±08\n6 w\/o extra obs\n50±07\n69±07\n7 w\/o language condition\n25±03\n26±05\nTable 4. The success rate (SR) under condition-free policy.\nGoal\nAvg.\nSuccess Rate (%) 44±19 24±06 23±11 11±07 25±03\nParameter sensitivity on horizon prediction. To investi-\ngate the sensitivity of the horizon-based control policy to\nthe constant c (outlined in §3.3), we perform experiments\nwith c values ranging from 0 to 14. We train and evaluate\nthe model using the multi-task setting on the Flat bench-\nmark, shown in Figure 5. Our findings indicate that within\nthe 0 to 10 range, decreasing c enhances performance, while\nfurther reduction leads to decline. This implies that sub-\ntracting a small constant from the predicted horizon-to-goal\nyields a more effective policy. However, subtracting a larger\nvalue results in performance deterioration, as attaining the\ngoal within such a limited horizon may be unfeasible.\nComparision with recurrent architecture. We built two\nrecurrent variants ( “Ours + RNN”, “Ours −horizon pred +\nRNN”) by using a GRU module to fuse the joint representa-\ntion ft and optionally also removing the horizon prediction\nmodule. During training, the batch size, frame number, and\nskipping frame are set to 8, 16, and 5, respectively. Ta-\nble 3 (exp1 vs. exp3) shows that “Ours −horizon pred +\nRNN” becomes significantly worse, likely due to the par-\ntial observability issue (−26% SR). However, when com-\nbining RNN and horizon module (exp2), the performance\ngains significantly more than our original method (+10%\nSR). To sum up, while RNNs can aid in addressing partial\nobservability, our findings indicate that in our open-world\nscenario, they are considerably more effective when com-\nbined with our horizon prediction module.\nAblation on horizon loss, extra observation, and lan-\nguage condition.\nTable 3 demonstrates that excluding\nhorizon loss (exp5) and extra observation (exp6) can result\nin a decrease of success rate by 8% and 5%, respectively.\nFurthermore, as depicted in Table 4, when the language con-\ndition is removed from the input (exp7), the policy primar-\nily accomplishes the “chopping tree” task (44% SR) while\nscarcely completing the “hunting pig” task (11% SR). The\ntasks “hunting sheep” and “hunting cow” are executed fairly\nevenly (around 24% SR). This is likely due to trees appear-\ning more frequently than animals in the environment.\n7\n0\n2\n4\n6\n8\n10\n12\n14\nSubtract Constant\n0.40\n0.45\n0.50\n0.55\n0.60\nSuccess Rate\n0\n2\n4\n6\n8\n10\n12\n14\nSubtract Constant\n0.55\n0.60\n0.65\n0.70\n0.75\nPrecision\nFigure 5. Multi-task performance as a function of subtracting the\nhorizon constant c. Results show that setting c to a small constant\nlead to better overall performance as it incentivizes the agent to\nexhibit behaviors that lead to faster task completion.\n4.4. Generalization Performance\nIn the open-ended Minecraft environment, which fea-\ntures a variety of biomes with distinct appearances, a de-\ncent agent should be capable of generalizing across these\ndiverse biomes. To evaluate the agent’s zero-shot gener-\nalization ability in a new biome, we initially train the agent\nusing data exclusively from the Plains biome. Subsequently,\nwe test it in the Flat biome, where it faces the challenge\nof combatting sheep, cows, and pigs. Complicating\nthe task, numerous distracting mobs, such as wolves and\nmushroom cows, appear in the testing biome but not in\nthe training biome. The results are presented in Table 5.\nOur zero-shot agent demonstrates success rates comparable\nto those of an agent trained directly on the Flat biome. The\nhigh precision of our zero-shot agent also indicates its ro-\nbust performance, even amidst numerous novel distracting\nmobs in the new testing biome. Therefore, we believe that\nour agent displays a degree of zero-shot generalization to\nnew environments, achieved through goal-aware represen-\ntation learning and adaptive horizon prediction.\n5. Related Works\nOpen-ended Environments.\nA variety of environments\nhave been developed for open-ended agent training, such\nas grid worlds [8, 9], maze worlds [25, 42, 46], and indoor\nworlds [1,15,38,40]. Although these benchmarks have ad-\nvanced agent development, they generally lack complexity\nin perception and task domains. This paper concentrates on\nMinecraft, a voxel-based 3D, first-person, open-world game\ncentered around survival and creation. Microsoft introduced\nthe first Gym-style API platform called Malmo [24] for\nMinecraft, which has spawned numerous secondary devel-\nopment variants.\nBuilding on Malmo, MineRL [20] of-\nfers a human-interface simulator and a dataset of human\nplay demonstrations for the annual Diamond Challenge at\nNeurIPS [18,19,26]. MineDoJo [16], an extension of Min-\neRL, broadens the APIs for customizing tasks and provides\nthousands of pre-defined compositional tasks aimed at de-\nveloping a generally capable embodied agent, which we use\nto evaluate our method.\nTable 5. Quantitive results on generalization to a novel biome.\nTrain →Eval\nSuccess Rate (%)\nPrecision (%)\nAvg.\nAvg.\nFlat→Flat\n72\n60\n57\n63\n44\n48\n54\n49\nPlains→Flat\n67\n47\n60\n58\n89\n89\n70\n83\nEmbodied Agents in Minecraft.\nSome prior studies\nhave utilized a hierarchical reinforcement learning frame-\nwork to develop sophisticated embodied agents.\nFor in-\nstance, SEIHAI [31] divides a long-horizon task into sev-\neral subtasks, training an appropriate agent for each sub-\ntask and designing a scheduler to manage the execution of\nthese agents. Similarly, JueWu-MC [28] adopts this con-\ncept but enhances the agent with action-aware representa-\ntion learning capabilities. In recent times, the internet-scale\npretraining paradigm has made a significant impact on em-\nbodied research in open-ended environments. VPT [4], for\nexample, undergoes pretraining on an extensive collection\nof online gameplay videos using imitation learning. How-\never, it lacks the ability to process any command input.\nMineAgent [16] takes a different approach by pretraining a\nlanguage-conditioned reward function using online video-\ntranscript pairs, which is then utilized to support multi-task\nreinforcement learning.\nProgress Monitor.\nThe horizon-to-goal prediction tech-\nnology has already been employed as a progress moni-\ntor in the Vision-Language Navigation (VLN) communi-\nties [29, 30, 47]. This technology aids in understanding the\ntask structure and expediting the training procedure. Gen-\nerally, current progress monitors primarily function as sup-\nplementary objectives. Their estimated progress is utilized\nto reassess actions or execute beam search. In contrast, our\nestimated horizon is explicitly incorporated into the policy\nnetwork to guide agent behaviors. During inference, the\nhorizon input can be adjusted for enhanced performance.\n6. Conclusion\nIn this paper, we explore the issue of learning goal-\noriented policies in open-world environments. We pinpoint\ntwo major challenges unique to such settings: 1) the diffi-\nculty in distinguishing tasks from the state distribution due\nto immense scene variety, and 2) the non-stationary nature\nof environmental dynamics resulting from partial observ-\nability. We propose a goal-sensitive backbone and an adap-\ntive horizon prediction module to overcome both. Our ex-\nperiments on challenging Minecraft confirm the advantages\nof our proposed methods over baselines in terms of both\nsuccess rate and precision of task completeness.\nAcknowledgement. This work was supported by the Na-\ntional Key R&D Program of China 2022ZD0160301, and\nin part by the NSF grants #IIS-1943641, #IIS-1956441,\n#CCF-1837129, Samsung, CISCO, and a Sloan Fellowship.\nWe thank Hongming Xu for his engineering support.\n8\nReferences\n[1] Josh Abramson, Arun Ahuja, Arthur Brussee, Federico\nCarnevale, Mary Cassin, Stephen R. Clark, Andrew Dudzik,\nPetko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden\nHung, Zachary Kenton, Jessica Landon, Timothy P. Lil-\nlicrap, Kory Wallace Mathewson, Alistair Muldal, Adam\nSantoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\nNathaniel Wong, Chen Yan, and Rui Zhu. Imitating inter-\nactive intelligence. arXiv: Learning, 2020. 8\n[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Cheb-\notar, Omar Cortes, Byron David, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i\ncan, not as i say: Grounding language in robotic affordances.\narXiv preprint arXiv:2204.01691, 2022. 1, 2\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatie Millican, Malcolm Reynolds, et al. Flamingo: a vi-\nsual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022. 1\n[4] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampe-\ndro, and Jeff Clune.\nVideo pretraining (VPT): learn-\ning to act by watching unlabeled online videos.\nCoRR,\nabs\/2206.11795, 2022. 3, 6, 7, 8, 12\n[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-\nishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\nRt-1: Robotics transformer for real-world control at scale.\narXiv preprint arXiv:2212.06817, 2022. 4\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877–1901, 2020. 1\n[7] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl:\nReduce memory, not parameters for efficient on-device\nlearning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 11285–11297. Curran\nAssociates, Inc., 2020. 12\n[8] Tianshi Cao, Jingkang Wang, Yining Zhang, and Sivabalan\nManivasagam.\nBabyai++:\nTowards grounded-language\nlearning beyond memorization.\nCoRR, abs\/2004.07200,\n2020. 8\n[9] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem\nLahlou, Lucas Willems, Chitwan Saharia, Thien Huu\nNguyen, and Yoshua Bengio. Babyai: A platform to study\nthe sample efficiency of grounded language learning. Learn-\ning, 2018. 8\n[10] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano\nPhielipp. Goal-conditioned imitation learning. Advances in\nneural information processing systems, 32, 2019. 2\n[11] Shibhansh Dohare, A Rupam Mahmood, and Richard S Sut-\nton. Continual backprop: Stochastic gradient descent with\npersistent randomness.\narXiv preprint arXiv:2108.06325,\n2021. 6\n[12] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O\nStanley, and Jeff Clune. First return, then explore. Nature,\n590(7847):580–586, 2021. 3\n[13] Islam Elnabarawy, Kristijana Arroyo, and Donald C. Wun-\nsch.\nStarcraft ii build order optimization using deep re-\ninforcement learning and monte-carlo tree search.\narXiv:\nLearning, 2020. 1\n[14] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Si-\nmonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu,\nTim Harley, Iain Dunning, et al.\nImpala: Scalable dis-\ntributed deep-rl with importance weighted actor-learner ar-\nchitectures. In ICML, pages 1407–1416. PMLR, 2018. 4, 6,\n7\n[15] Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li\nFei-Fei, Yuke Zhu, and Animashree Anandkumar. Secant:\nSelf-expert cloning for zero-shot generalization of visual\npolicies. arXiv: Learning, 2021. 8\n[16] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar.\nMinedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\narXiv preprint arXiv:2206.08853, 2022. 1, 2, 6, 7, 8, 12\n[17] Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu,\nColine Manon Devin, Benjamin Eysenbach, and Sergey\nLevine. Learning to reach goals via iterated supervised learn-\ning. In International Conference on Learning Representa-\ntions, 2021. 5\n[18] William H. Guss, Mario Ynocente Castro, Sam Devlin,\nBrandon Houghton, Noboru Sean Kuno, Crissman Loomis,\nStephanie Milani, Sharada P. Mohanty, Keisuke Nakata,\nRuslan Salakhutdinov, John Schulman, Shinya Shiroshita,\nNicholay Topin, Avinash Ummadisingu, and Oriol Vinyals.\nThe minerl 2020 competition on sample efficient reinforce-\nment learning using human priors. arXiv: Learning, 2021.\n1, 8\n[19] William H Guss, Cayden Codel, Katja Hofmann, Brandon\nHoughton, Noboru Kuno, Stephanie Milani, Sharada Mo-\nhanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay\nTopin, et al. Neurips 2019 competition: the minerl competi-\ntion on sample efficient reinforcement learning using human\npriors. arXiv preprint arXiv:1904.10079, 2019. 1, 8\n[20] William H. Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan\nSalakhutdinov. Minerl: A large-scale dataset of minecraft\ndemonstrations. international joint conference on artificial\nintelligence, 2019. 1, 8\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 4\n[22] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Fred-\nerik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn.\nBc-z: Zero-shot task generalization with robotic imitation\nlearning.\nIn Conference on Robot Learning, pages 991–\n1002. PMLR, 2022. 4\n[23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge J. Belongie, Bharath Hariharan, and Ser-Nam Lim.\n9\nVisual prompt tuning. In Shai Avidan, Gabriel J. Brostow,\nMoustapha Ciss´e, Giovanni Maria Farinella, and Tal Hass-\nner, editors, Computer Vision - ECCV 2022 - 17th European\nConference, Tel Aviv, Israel, October 23-27, 2022, Proceed-\nings, Part XXXIII, volume 13693 of Lecture Notes in Com-\nputer Science, pages 709–727. Springer, 2022. 12\n[24] Matthew Johnson, Katja Hofmann, Tim J. Hutton, and\nDavid Michael Bignell. The malmo platform for artificial\nintelligence experimentation. international joint conference\non artificial intelligence, 2016. 1, 2, 8\n[25] Arthur Juliani, Ahmed Khalifa, Vincent Pierre Berges,\nJonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi,\nJulian Togelius, and Danny Lange. Obstacle tower: A gen-\neralization challenge in vision, control, and planning. inter-\nnational joint conference on artificial intelligence, 2019. 8\n[26] Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas,\nNicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, De-\nheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue\nHuang, Haicheng Chen, Guangjun Zeng, Yue Lin, Vincent\nMicheli, Eloi Alonso, Fran\nc{c}ois Fleuret, Alexander Nikulin, Yury Belousov, Oleg\nSvidchenko, and Aleksei Shpilman. Minerl diamond 2021\ncompetition: Overview, results, and lessons learned. neural\ninformation processing systems, 2022. 1, 8\n[27] Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee\nKim, Jung-Woo Ha, and Byoung-Tak Zhang.\nHadamard\nproduct for low-rank bilinear pooling.\narXiv preprint\narXiv:1610.04325, 2016. 4\n[28] Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu,\nand Wei Yang. Juewu-mc: Playing minecraft with sample-\nefficient hierarchical reinforcement learning. arXiv preprint\narXiv:2112.04907, 2021. 8\n[29] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib,\nZsolt Kira, Richard Socher, and Caiming Xiong.\nSelf-\nmonitoring navigation agent via auxiliary progress estima-\ntion. In 7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net, 2019. 8\n[30] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming\nXiong, and Zsolt Kira. The regretful agent: Heuristic-aided\nnavigation through progress estimation. In Proceedings of\nthe IEEE\/CVF conference on Computer Vision and Pattern\nRecognition, pages 6732–6740, 2019. 8\n[31] Hangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yim-\ning Lu, Chengjie Wu, Jianye Hao, Dong Li, and Pingzhong\nTang. Seihai: A sample-efficient hierarchical ai for the min-\nerl competition. In International Conference on Distributed\nArtificial Intelligence, pages 38–51. Springer, 2021. 8\n[32] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin\nRiedmiller. Playing atari with deep reinforcement learning.\narXiv: Learning, 2013. 1, 2\n[33] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet\nKohli.\nZero-shot task generalization with multi-task deep\nreinforcement learning. In International Conference on Ma-\nchine Learning, pages 2661–2670. PMLR, 2017. 4\n[34] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 32, 2018. 4\n[35] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez\nColmenarejo, Alexander Novikov, Gabriel Barth-Maron,\nMai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin-\ngenberg, et al.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022. 1\n[36] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv\nBatra. Habitat: A platform for embodied ai research. inter-\nnational conference on computer vision, 2019. 2\n[37] Juergen Schmidhuber. Reinforcement learning upside down:\nDon’t predict rewards–just map them to actions.\narXiv\npreprint arXiv:1912.02875, 2019. 5\n[38] Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart´ın-\nMart´ın, Linxi Fan, Guanzhi Wang, Shyamal Buch, Clau-\ndia D’Arpino, Sanjana Srivastava, Lyne P. Tchapmi, Micael\nTchapmi, Kent Vainio, Li Fei-Fei, and Silvio Savarese. igib-\nson, a simulation environment for interactive tasks in large\nrealistic scenes. intelligent robots and systems, 2020. 8\n[39] David Silver, Julian Schrittwieser, Karen Simonyan, Ioan-\nnis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert,\nLucas Baker, Matthew Lai, Adrian Bolton, et al.\nMas-\ntering the game of go without human knowledge. nature,\n550(7676):354–359, 2017. 1, 2\n[40] Sanjana Srivastava, Chengshu Li, Michael Lingelbach,\nRoberto Mart´ın-Mart´ın,\nFei Xia,\nKent Vainio,\nZheng\nLian, Cem Gokmen, Shyamal Buch, C. Karen Liu, Silvio\nSavarese, Hyowon Gweon, Jiajun Wu, and Li Fei-Fei. Be-\nhavior: Benchmark for everyday household activities in vir-\ntual, interactive, and ecological environments. Conference\non Robot Learning, 2021. 8\n[41] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez,\nYazhe Li, Diego de Las Casas, David Budden, Abbas Ab-\ndolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind\ncontrol suite. arXiv preprint arXiv:1801.00690, 2018. 2\n[42] Open Ended Learning Team, Adam Stooke, Anuj Maha-\njan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub\nSygnowski, Maja Trebacz, Max Jaderberg, Micha¨el Math-\nieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel\nWong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt,\nValentin Dalibard, and Wojciech Marian Czarnecki. Open-\nended learning leads to generally capable agents.\nCoRR,\nabs\/2107.12808, 2021. 8\n[43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A\nphysics engine for model-based control. intelligent robots\nand systems, 2012. 2\n[44] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,\nMicha¨el Mathieu,\nAndrew Dudzik,\nJunyoung Chung,\nDavid H Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, et al. Grandmaster level in starcraft ii using multi-\nagent reinforcement learning. Nature, 575(7782):350–354,\n2019. 1\n[45] Oriol Vinyals,\nTimo Ewalds,\nSergey Bartunov,\nPetko\nGeorgiev, Alexander Vezhnevets, Michelle Yeo, Alireza\nMakhzani, Heinrich K¨uttler, John P. Agapiou, Julian Schrit-\ntwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen\n10\nSimonyan, Tom Schaul, Hado van Hasselt, David Silver,\nTimothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony\nBrunasso, David Lawrence, Anders Ekermo, Jacob Repp,\nand Rodney Tsing. Starcraft ii: A new challenge for rein-\nforcement learning. arXiv: Learning, 2017. 1\n[46] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley.\nPaired open-ended trailblazer (poet): Endlessly generating\nincreasingly complex and diverse learning environments and\ntheir solutions. arXiv: Neural and Evolutionary Computing,\n2019. 8\n[47] Joel Ye, Dhruv Batra, Abhishek Das, and Erik Wijmans.\nAuxiliary tasks and exploration enable objectnav.\narXiv\npreprint arXiv:2104.04112, 2021. 8\n[48] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nKarol Hausman, Chelsea Finn, and Sergey Levine. Meta-\nworld: A benchmark and evaluation for multi-task and meta\nreinforcement learning. 2019. 2\n[49] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nKarol Hausman, Chelsea Finn, and Sergey Levine. Meta-\nworld: A benchmark and evaluation for multi-task and meta\nreinforcement learning.\nIn Conference on robot learning,\npages 1094–1100. PMLR, 2020. 1, 13\n[50] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models. CoRR, abs\/2106.10199, 2021. 12\n11\nA. Experimental Details\nA.1. Observation and Action Space\nThe agent receives identical information as human play-\ners do.\nThe observation space primarily comprises four\ncomponents: 1) ego-centric RGB frames, 2) voxels (sur-\nrounding blocks), 3) GPS locations (the agent’s three-\ndimensional coordinates), and 4) compass (pitch\/yaw an-\ngles). These are shaped as (3, 480, 640), (3, 3, 3), (3, ), and\n(2, ), respectively. It is important to note that the agent does\nnot know the precise location of the target object. Instead,\nthe agent can only obtain information about the target ob-\nject by examining the pixel image. The RGB frames are\nresized to a shape of (3, 128, 128) using bilinear interpola-\ntion before being fed into the networks. At each step, the\nagent must execute a movement action, camera action, and\nfunctional action. A compound action space is employed,\nconsisting of a multi-discrete space with six dimensions:\n1) forward and backward, 2) move left and right, 3) jump,\nsneak and sprint, 4) camera delta pitch, 5) camera delta\nyaw, and 6) functional actions (attack and use). The origi-\nnal delta camera degree, which ranges from -180 to 180, is\ndiscretized into 11 bins. As this paper’s primary focus is on\nresource collection rather than item crafting, actions related\nto crafting are omitted.\nA.2. Data Collection Pipeline\nOur data collection pipeline collects high-quality goal-\nconditioned demonstrations with actions. The core idea is to\ntrain a proxy policy with non-goal demonstrations and roll\nout in customized environments, then filter the demonstra-\ntions according to the achievement. Generally, the pipeline\nconsists of six steps: 1) collect online videos, 2) clean and\nlabel the videos, 3) train a proxy policy, 4) customize the\nenvironments, 5) roll out the proxy policy, and 6) filter by\nthe accomplishments.\nVideo-Pretraining [4] is ideally suited for stages 1-3.\nIt begins by amassing a vast dataset of Minecraft videos,\nsourced from the web using relevant keywords. Given that\ncollected videos often feature overlaid artifacts, the process\nfilters out videos without visual artifacts and those from sur-\nvival mode. Next, an Inverse Dynamics Model (IDM) is\ntrained to label these videos with actions, yielding demon-\nstrations for proxy policy training. We directly employ the\npretrained VPT[4] as our proxy policy. In stage 4, we utilize\nAPIs supplied by MineDojo[16] to create environments tai-\nlored to each task’s success criteria. During stage 5, we de-\nploy the proxy policy, recording successful trajectories and\ntheir corresponding achieved goals. The environment is re-\nset once the episode concludes or the goal is accomplished,\nensuring trajectory independence.\nNotably, we execute the proxy policy rollout in parallel\nusing 16 processes on 4 A40 GPUs, generating 0.5GB of\ndemonstrations per minute (without leveraging video com-\npression algorithm during storing frames). This approach\nminimizes human intervention and enhances data collection\nefficiency. In total, we have gathered 215GB, 289GB, and\n446GB of goal-conditioned demonstrations from Plains,\nFlat, and Forest environments, respectively.\nA.3. Implementation\nHorizon discretization.\nAs the horizon illustrates the\nnumber of steps required to attain the desired objective, it\nis infeasible to precisely determine the exact value. In prac-\ntice, we suggest dividing the original horizon into 16 dis-\ntinct segments: [0, 10) →0, [10, 20) →1, [20, 30) →2,\n· · · , [90, 100) →9, [100, 120) →10, [120, 140) →11,\n· · · , [180, 200) →14, and [200, ∞) →15. In this ap-\nproach, each segment inherently represents a phase that sig-\nnifies the level of task completion. Consequently, the hori-\nzon prediction issue can be framed as a multi-class problem.\nIt is important to note that the method of discretization is not\nsingular and merits further exploration in the future.\nTraining. The observation of RGB image is scaled into\n128×128 where no data augmentation is adopted. We train\nthe policy with the AdamW optimizer and a linear learn-\ning rate decay. We use an initial learning rate of 0.0001, a\nbatch size of 32, and a weight decay of 0.0001. Besides,\nwe also use a warmup trick that the learning rate linearly\nincreases from 0 to 0.0001 in 10k iterations. The policy\nis trained for 500k iterations on our collected dataset. It\ntakes one day on a single A40 GPU. To train the baseline\npolicies BC (VPT) and BC (CLIP), we only finetune the bias\nterms of their backbones, which is widely adopted by pre-\nvious works [7, 23, 50]. Also note that, to keep the archi-\ntecture comparable, we only transfer model and weights of\nthe backbone from vpt model and MineCLIP model while\nreplace their transformer architecture with ours.\nEvaluation. During the evaluation, the maximum episode\nlength is empirically set to 600, 600, and 300 for the Flat,\nPlains, and Forest biomes, respectively. In most in-\nstances, the agent is able to complete the assigned tasks\nwithin these limits. Furthermore, in our adaptive horizon\nprediction module, the hyperparameter c is empirically set\nto 3.\nThe model is evaluated every 10,000 gradient up-\ndates. During each evaluation round, each goal is assessed\n10 times to compute the Success Rate and Precision met-\nrics. For the ablation study, we utilize the checkpoint after\n500,000 training iterations, evaluate each goal 200 times,\nand report the average metrics in Table 5 and Figure 5.\nB. Horizon Distribution Analysis\nTo further emphasize the importance of our adaptive\nhorizon prediction module, we have visualized the distri-\nbution of successful trajectory lengths for various tasks in\nMinecraft, as shown in Figure 6. These successful trajec-\n12\nHarvest log\nCombat sheep\nCombat cow\nSuccessful trajectory length\nDensity\nFigure 6. Successful trajectory distribution of different tasks in open-ended Minecraft. The distribution is long-tailed, making it hard\nto learn goal-conditioned policies with a fixed horizon.\ntories were gathered from agents trained using single-task\nbehavior cloning (with a randomly initialized Impala CNN\nas the backbone) in the Plains biome.\nAs depicted in Figure 6, the distribution of successful tra-\njectory lengths in the open-world setting exhibits a long tail,\nmaking it challenging to train a policy with a fixed horizon.\nThis can be attributed to Minecraft’s extensive explorable\nspace, partial observation properties, and non-stationary dy-\nnamics, which set it apart from other popular multi-task,\nclosed-ended environments like Meta-World [49].\nConsequently, the minimum number of steps needed for\nan agent to achieve its goal varies across different environ-\nments and episodes. The episode length typically hinges\non the relative position and terrain constraints between the\ntarget object and the agent’s initial position.\nAn added\nlayer of complexity arises when no target objects are near\nthe agent’s starting location, necessitating large-scale explo-\nration (i.e., a larger horizon). Once the agent locates the\ntarget object, it must track it until the relevant skill can be\nexecuted on the object (e.g., killing or harvesting). This de-\nmands that the agent remain aware of its current stage.\nOur proposed adaptive horizon prediction module incor-\nporates the horizon as an additional condition for the policy.\nThe policy explicitly takes into account the remaining time\nsteps needed to achieve specific goals. Our experiments in\nSection 4.3 demonstrate that the adaptive horizon predic-\ntion module and the horizon loss Lh effectively enhance\nthe success rate in open-world environments with such dis-\ntributions.\nC. Limitation and Future Work\nIn essence, our approach hinges on trajectories labeled\nwith goals, which enables it to generalize across various\ndomains, provided that such data is accessible. When only\nvideo segments labeled with actions are available, we can\nemploy a goal predictor to assign goal labels to these clips.\nThis can also be achieved by utilizing zero-shot models,\nsuch as CLIP. Moreover, if action labels are absent in these\nclips, we can resort to training an inverse dynamics model,\nas demonstrated in VPT. Undoubtedly, these present in-\ntriguing avenues for future exploration\n13\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction.pdf"}
{"title":"MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control","authors":"Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, Jing Shao","summary":"It is a long-lasting goal to design a generalist-embodied agent that can\nfollow diverse instructions in human-like ways. However, existing approaches\noften fail to steadily follow instructions due to difficulties in understanding\nabstract and sequential natural language instructions. To this end, we\nintroduce MineDreamer, an open-ended embodied agent built upon the challenging\nMinecraft simulator with an innovative paradigm that enhances\ninstruction-following ability in low-level control signal generation.\nSpecifically, MineDreamer is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs) and diffusion models, and we employ a\nChain-of-Imagination (CoI) mechanism to envision the step-by-step process of\nexecuting instructions and translating imaginations into more precise visual\nprompts tailored to the current state; subsequently, the agent generates\nkeyboard-and-mouse actions to efficiently achieve these imaginations, steadily\nfollowing the instructions at each step. Extensive experiments demonstrate that\nMineDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent's imaginative ability\nreveals its generalization and comprehension of the open world.","url":"http:\/\/arxiv.org\/abs\/2403.12037v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2403.12037v2","published":1710784782000,"comment":"Project page: https:\/\/sites.google.com\/view\/minedreamer\/main","pdf_text":"MineDreamer: Learning to Follow Instructions\nvia Chain-of-Imagination for\nSimulated-World Control\nEnshen Zhou1,2∗, Yiran Qin1,3∗,\nZhenfei Yin1,4, Yuzhou Huang3, Ruimao Zhang3†, Lu Sheng2†,\nYu Qiao1, Jing Shao1‡\n1 Shanghai Artificial Intelligence Laboratory\n2 Beihang University\n3 The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen)\n4 The University of Sydney\nzhouenshen@buaa.edu.cn\nyiranqin@link.cuhk.edu.cn\nhttps:\/\/sites.google.com\/view\/minedreamer\/main\nAbstract. It is a long-lasting goal to design a generalist-embodied agent\nthat can follow diverse instructions in human-like ways. However, exist-\ning approaches often fail to steadily follow instructions due to difficul-\nties in understanding abstract and sequential natural language instruc-\ntions. To this end, we introduce MineDreamer, an open-ended embodied\nagent built upon the challenging Minecraft simulator with an innovative\nparadigm that enhances instruction-following ability in low-level con-\ntrol signal generation. Specifically, MineDreamer is developed on top of\nrecent advances in Multimodal Large Language Models (MLLMs) and\ndiffusion models, and we employ a Chain-of-Imagination (CoI) mecha-\nnism to envision the step-by-step process of executing instructions and\ntranslating imaginations into more precise visual prompts tailored to the\ncurrent state; subsequently, the agent generates keyboard-and-mouse ac-\ntions to efficiently achieve these imaginations, steadily following the in-\nstructions at each step. Extensive experiments demonstrate that Mine-\nDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent’s imaginative\nability reveals its generalization and comprehension of the open world.\nKeywords: Chain-of-Imagination · multimodal large language model ·\ninstruction following · low-level control\n1\nIntroduction\nOne of the core objectives of current embodied intelligence is to develop a gener-\nalist low-level control agent that can follow diverse instructions to solve endless\nopen-world embodied tasks [4, 5, 8, 42, 57]. Recent studies [4, 5, 8, 42] success-\nfully unlock the instruction-following ability of foundation models [3, 12, 15] in\n∗Equal contribution\n† Corresponding author\n‡ Project leader\narXiv:2403.12037v2  [cs.CV]  19 Mar 2024\n2\nE. Zhou et al.\nMineDreamer\nPrevious Studies\n“Chop a tree.” \n…\n…\n“Chop a tree.” \n“Chop a tree.” \n“Chop a tree.” \nFig. 1: Comparison between MineDreamer and previous studies. In “Chop\na tree”\ntask, MineDreamer employs a Chain-of-Imagination mechanism, where it\nimagines step by step what to do next tailored to the current state. Imaginations\ncontain environmental understanding and physical rules (e.g., perspective-based size\nchanges). These can serve as more precise visual prompts to steadily guide the agent in\ngenerating actions to achieve these imaginations as effectively as possible at each step.\nPrevious approaches have seen a tree, but missed the opportunity to chop it down.\nthe sequential decision-making domain [8, 11, 33, 57, 63, 69, 73]. However, these\nmethods [5,42] struggle to enable agents to follow textual instructions steadily,\ndue to the: (1) Many textual instructions are abstract for low-level control and\nmodels struggle to effectively understand. They should be transformed into more\neffective prompts that consider how to execute instructions based on the current\nstate. Hence, simple textual instructions cannot provide a precise demonstration\nof the desired behavior. (2) Many textual instructions are sequential, and exe-\ncuting them may require considering the current state and breaking down the\ntask into multiple stages for step-by-step completion. Therefore, steady action\ngeneration driven by single-text instructions often fails.\nTo address the above issues, this work aims to explore how to unlock the\nsituation-aware reasoning ability for a pre-trained decision-making foundation\nmodel. We introduce a simple yet effective mechanism called Chain-of-Imagination\n(CoI), which enables the agent to imagine and act upon the next stage step by\nstep according to the instructions. Our method is motivated by two ideas: (1)\nWhen solving complex problems, humans often envision the goal of the next\nstage based on the current state. If we can break down the sequential instructions\ninto multiple stages according to the current state, step by step, we can enable\nagents to follow instructions steadily. (2) Inspired by prompt tuning [34,78,79],\nif we can provide visual prompts containing physical rules and environmental\nunderstanding for each imagined step, tailored to optimally describe the desired\nbehavior in the current state, which are more intuitive and efficient than task\ninstructions, we can better guide the foundation model in predicting actions.\nTo this end, we propose MineDreamer within Minecraft, which generates a\nseries of “imagined” sub-steps based on the textual instructions and current state.\nThese visual sub-steps are then fed into a pre-trained decision-making founda-\ntion model to generate low-level control actions aimed at achieving the sub-steps.\nSpecifically, MineDreamer comprises three modules: (1) An Imaginator, a dif-\nfusion model enhanced by a Multimodal Large Language Model (MLLM), can\nbetter generate imaginations that contain the physical rules and environmental\nMineDreamer\n3\nunderstanding. (2) A Prompt Generator, the bridge between Imaginator and\nPolicyNet, can convert future imaginations into latent visual prompts that offer\nmore logical and precise demonstrations of the desired behavior. (3) A PolicyNet,\na foundation model, can use latent prompts as guidance to predict actions for\nagents in an open-world environment.\nNotably, as shown in Fig. 1, MineDreamer leverages a Chain-of-Imagination\nmechanism through multi-turn interaction between the Imaginator and the Pol-\nicyNet and cyclically generates latent visual prompts that better align with the\ncurrent state to guide the PolicyNet in following instructions steadily in action\ngeneration. This mechanism represents an attempt to implement “self multi-turn\ninteraction” in the sequential decision-making domain. Training an Imaginator\nin an open-world environment to envision the image of the next step requires\nextensive data. We employ the Goal Drift Collection method to gather a large\namount of egocentric embodied data, which helps the Imaginator to understand\nhow to achieve the instruction sequentially and how to achieve it repeatedly.\nOur main contributions are as follows:\n– We introduce the Chain-of-Imagination(CoI) method, which introduces “self\nmulti-turn interaction” to the sequential decision-making domain and en-\nables the agent to follow human instructions steadily in action generation.\n– We propose the Goal Drift Collection method and an MLLM-enhanced dif-\nfusion model that can generate imaginations adhering to physical rules and\nenvironmental understanding, providing more precise visual prompts rele-\nvant to the current state and instructions.\n– Leveraging these methods, we create an embodied agent in Minecraft named\nMineDreamer that has achieved nearly double the performance of the best\ngeneralist agent baseline in executing single and multi-step instructions steadily.\n2\nRelated Work\n2.1\nBuild Instruction-Following Agents in Minecraft\nResearch on generalist agents in Minecraft’s complex and dynamic environment\nis increasingly popular in AI. Despite the exploration of Large Language Mod-\nels [7,14,48,54,65,66] as high-level task planners that guide agents in executing\nlong-horizon tasks [25,50,70–72,81] like Voyager [70] and MP5 [50], we still re-\nquire lower-level controllers [3,8,20,27,42] to execute the generated plans. In the\nsequential decision-making domain, DreamerV3 [27] trains agents using a world\nmodel, while VPT [3] builds a large foundational model to generate actions by\nlearning from extensive video data. However, neither can follow instructions.\nGROOT [8] is developed to follow video instructions but fails to follow text\ninstructions. STEVE-1 [42], an evolution of VPT [3], is built for text instruc-\ntions but struggles to understand natural language prompts, despite extensive\nprompt engineering. Therefore, we create MineDreamer, which, leveraging the\nChain-of-Imagination mechanism, generates more precise visual prompts step-\nby-step, enabling it to follow instructions steadily in action generation.\n4\nE. Zhou et al.\n2.2\nConditioned Diffusion Models in Embodied Scenario\nWith the development of the text-to-image diffusion model [18,30,46,55,58,60],\nthe instruction-based diffusion methods [6, 9, 21, 23, 29, 32, 35, 67, 76] have re-\ncently marked considerable progress in generative tasks, especially in embod-\nied scenarios. UniPi [19] and HiP [1] integrate video diffusion with inverse dy-\nnamics to generate robot control signals for specific tasks. SkillDiffuser [41]\napplies interpretable hierarchical planning via skill abstractions in diffusion-\nbased task execution. While existing methods can only handle embodied tasks\nlimited to fixed environments, the emergence of Multimodal Large Language\nModels (MLLMs) [13, 22, 43, 49, 62, 74, 75, 80] has showcased superior reasoning\nand perceptual abilities in open-world environment. Inspired by this, we create\nan MLLM-enhanced diffusion model, focusing on the model’s understanding of\nphysics rules and environmental understanding, and its ability to create high-\nquality egocentric images for guiding low-level action generation.\n3\nMethod\nIn this section, we first provide an overview (Sec. 3.1) of our MineDreamer, in-\ncluding its mechanisms and features. Next, we introduce the purpose and work-\nflow of the Chain-of-Imagination (CoI) mechanism (Sec. 3.2) regarding Fig. 2.\nTo implement CoI and collect extensive embodied data to train Imaginator, we\nelaborate on the dataset construction (Sec. 3.3), including Goal Drift Collec-\ntion method. Finally, we provide the necessary details of each part, including\nImaginator (Sec. 3.4), Prompt Generator, and PolicyNet (Sec. 3.5).\n3.1\nOverview\nOur MineDreamer comprises three modules, i.e., Imaginator, Prompt Genera-\ntor, and PolicyNet. Our objective is to empower agents, especially foundation\nmodels in the sequential decision-making domain, to follow human instructions\nsteadily and act accordingly. The Imaginator is a parameter-efficiently fine-tuned\ndiffusion model specific to Minecraft utilizing the visual reasoning ability of\na Multimodal Large Language Model (MLLM). The Prompt Generator recon-\nstructs latent visual prompts from the current observations, future imaginations,\nand instructions. PolicyNet is the existing Video Pretraining (VPT) [3] model,\ntrained on 70k hours of Minecraft gameplay.\nWhy future goal imagination? Given a pre-trained model that can predict\nactions, the intuitive approach is to input the current state and instructions to\nguide it directly. So why the future goal imagination? In practice, we find that\nfuture goal imagination proves more interpretable for humans, easing debugging,\nand improving interaction and safety assessment [40, 51, 56, 77]. Furthermore,\nimages yield flexible, explicit representations, facilitating natural language goal\ndecomposition into clearer stages by learned physical rules and environmental\nunderstanding, helping the low-level control model “plan” what to do now.\nMineDreamer\n5\nImaginator\n…\nVPT\nVPT\nVPT\n𝑓𝑡\n𝑝𝑡\n𝑓𝑡+1\n𝑝𝑡+1\n𝑓𝑡+𝑇−1\n𝑝𝑡+𝑇−1\n𝑬𝑽\n𝑬𝑽\n𝑬𝑽\n𝑮\n𝑮\n𝑮\n𝑬𝑽\n𝑮\nVisual \nEncoder\nPrompt\nGenerator\nVisual &\nVisual Only\nImaginator\nImaginator\nText Instruction(𝑦) \nChop a tree\nCurrent Observation\nGoal Imagination\nInstruction\nFig. 2: The Overview of Chain-of-Imagination. The Imaginator imagines a goal\nimagination based on the instruction and current observation. The Prompt Generator\ntransforms this into a precise visual prompt, considering both the instruction and\nobserved image. The Visual Encoder encodes the current observation, integrates it\nwith this prompt, and inputs this into VPT. VPT then determines the agent’s next\naction, leading to a new observation, and the cycle continues. Note that VPT’s input is\nhistorical observations, so the figure cannot fully represent the autoregressive process.\nMore details about VPT as PolicyNet can be found in Sec. 3.5.\nWhy can MineDreamer follow instructions more steadily? Firstly, Mine-\nDreamer employs a Chain-of-Imagination (CoI) mechanism for incremental goal\nachievement via self-multi-turn interactions, enabling the agent to appropriately\nrespond to the current state. In addition, with the help of this mechanism, the\nPrompt Generator crafts logical latent visual prompts that provide clear demon-\nstrations of desired behaviors, ensuring that the agent steadily follows instruc-\ntions. Furthermore, the enhanced Imaginator not only comprehends open-ended\nvisual concepts, enabling it to imagine images of novel instructions it has never\nseen before but also ensures these images adhere to physical rules and envi-\nronmental understanding, thereby sharpening the precision of prompts. Thus,\nMineDreamer can follow instructions steadily in an open-world environment.\n3.2\nChain-of-Imagination\nChain-of-Imagination (CoI) enables the agent to envision the steps needed to\nachieve a goal iteratively. As shown in Fig. 2, it is an example to demonstrate\nhow CoI works. First, the Imaginator takes in the user’s instructions y and\ncurrent observations Ot and imagines a future image It+1 depicting a moment\nwithin the process of completing the given instruction y, which is closely related\nto the current observation Ot. Next, the Prompt Generator progressively creates\na more precise latent visual prompt pt in awareness of the current observation Ot,\ninstruction y and future imagination It+1, aligning with the visual input space of\nthe Video Pretraining (VPT) [3] model. The Visual Encoder then processes Ot\ninto a representation ft, which is combined with pt and fed into VPT [3]. Finally,\nVPT [3] progressively predicts an action (i.e., keyboard and mouse) from the\nobservation history, interacts with the environment, gathers a new observation\nOt+1, and repeats the cycle later.\n3.3\nDatasets\nWe train the Imaginator with the Goal Drift Dataset, which includes 500k\ntriplets (current observation, future goal imagination, instruction) from the Ope-\nnAI Contractor Gameplay Dataset [3], using the Goal Drift Collection method.\n6\nE. Zhou et al.\n𝑡∗\n𝑡𝑏1\nBackward\nForward\nBackward\nDrift\nForward\nDrift\n…\n…\n…\n…\n𝑡𝑏2\n𝑡𝑏𝑛\n𝑡𝑏𝑛+1\n𝑡𝑓𝑛\n𝑡𝑓𝑚\nInstruction(𝑦): “ Chop a tree\/Collect wood\/…”\nFig. 3: Goal Drift Collection. For each timestamp t∗, we form many triplets com-\nprising (current observation, goal imagination, instruction) associated with the game\nevent-related instructions completed by contractors. Each pair of linked images forms\na training triplet with its instruction for the Imaginator in this figure.\nOpenAI Contractor Gameplay Dataset. OpenAI Contractor Gameplay\nDataset [3] is created by hiring human contractors to play Minecraft and com-\nplete tasks like house building. Game events, like “mine_block”, noting the type\nof block broken, are logged with timestamps. These timestamps (t∗) provide\nprecise progress tracking and align with completed event-related instructions.\nGoal Drift Collection. The Gameplay Dataset allows us to construct numer-\nous embodied data by using specific event-related instructions achieved at each\ntimestamp t∗. Yet, directly pairing images from these timestamps t∗as future\ngoal imaginations Ot∗with images from a fixed timestep T earlier as current\nobservations Ot∗−T , along with instruction y, could lead to certain problems:\n(1) Goal Illusion: The Imaginator edits the observation to depict the\ncompleted instruction. Training the Imaginator on such data may reduce it\nto an image editor, as it generates imaginations without regard to the environ-\nment because all goal imaginations in the dataset represent the moment when\ninstruction is completed. For instance, given the instruction “Break dirt”\nwhile facing the sky, the Imaginator may unrealistically insert a broken dirt\nblock\ninto the sky. (2) Imagination Stagnation: The Imaginator fails\nto conceive repeated task completion. The Imaginator is trained to envision\nthe instructions’ fulfillment once, not recognizing the need for repetition, as all\ncurrent observations precede the achievement of instructions. For instance, given\n“Chop a tree”\n, after cutting the uppermost wood\nby looking up, the agent\nwill not look down for more trees\n, impeding continuous task performance.\nTo address the aforementioned issues, we propose the Goal Drift Collection\nmethod to gather Goal Drift Dataset. From the Gameplay Dataset, we form\nmany triplets (current observation, goal imagination, instruction) at each times-\ntamp t∗, all associated with the same event-related instructions y completed\nby the contractors. Fig. 3 shows that a pair of linked images with instructions\ny constitutes a training triplet. Our approach has both Backward Drift, which\nhelps the model understand the step-by-step completion of tasks to mitigate\nGoal Illusion, and Forward Drift, which enables the model to learn how to ac-\ncomplish instructions repeatedly to reduce Imagination Stagnation. The details\nof collecting three kinds of data samples corresponding to each t∗are as follows:\n1. Backward Drift 1: We set tb1 as t∗backward by fixed Tb time steps and\nthen select m −2 random timestamps between tb1 and t∗to form the se-\nquence tb1, . . . , tbm , where t∗is tbm. At each time step, the current and next\nobservations are paired as the current observations and goal imagination,\nrespectively, which can form m −1 samples.\nMineDreamer\n7\nLarge Language Model (LLaMA) \nLoRA\nImage Encoder\nText Encoder\nLearnable Goal Tokens\nDiffusion\nModel\nGoal \nQ-former\n[GOAL0]\n[GOAL1]\n[GOAL2]\n[GOALN]\n…\nText Instruction (𝑦): Chop a tree\nCurrent Observation         \n𝑓∗\nLearnable \nDream Query\nGoal Imagination\nFinetune\/Train\nFrozen\nFig. 4: The Overall Framework of Imaginator. For the goal understanding, we\nadd k [GOAL] tokens to the end of instruction y and input them with current obser-\nvation Ot into LLaVA [43]. Then LLaVA [43] generates hidden states for the [GOAL]\ntokens, which the Q-Former processes to produce the feature f ∗. Subsequently, the\nimage encoder Ev combines its output with f ∗in the diffusion models for instruction-\nbased future goal imagination generation.\n2. Backward Drift 2: In tb1, . . . , tbm, the observations at each timestamp except\nfor tbm are used as the current observations, and the observation at t∗serve\nas the goal imagination, which can form m −1 samples.\n3. Forward Drift: We set tfm as t∗forward by fixed Tf time steps and randomly\nselect m −2 timestamps between t∗and tfm , where t∗is tf1. The observa-\ntion at t∗serves as the current observation, and the observations at future\ntimestamps serve as the goal imaginations, which can form m −1 samples.\nFor more details about the dataset and collection method, please check Supp. B.\n3.4\nImaginator\nInspired by prompt tuning [34, 78, 79], we introduce Imaginator, an MLLM-\nenhanced diffusion model that imagines step by step what to do next based on\nthe current state and instruction, enabling the creation of more precise visual\nprompts for improved low-level control demonstrations of the desired behavior.\nImaginator’s training data utilizes the Goal Drift Dataset from Sec 3.3, consisting\nof (current observation, goal imagination, instruction) triplets.\nGoal Understanding via Task Instruction Following. Given a current ob-\nservation Ot and a textual instruction y, the Imaginator generates a future goal\nimagination It+1 for the PromptGenerator’s visual prompt. In Fig. 4, current\nobservation Ot is encoded by a frozen image encoder Ev into Ev(Ot), textual in-\nstruction y is tokenized into (x1, ..., xT ), they are sent to the LLM together. Imag-\ninator now can acquire a goal imagination of the instruction intention but are\nlimited to the language modality. Inspired by GILL [37], we bridge the language-\nvision modalities gap by extending the LLM’s vocabulary with k Learnable Goal\nTokens [GOAL1], . . . , [GOALk], appending them to instruction y. Specifically, a\ntrainable matrix Eg, representing these [GOAL] embeddings, is added to the\nLLM’s embedding matrix. We aim to minimize the negative log-likelihood of\npredicting the next [GOAL] token given previously generated [GOAL] tokens:\n  \\m a t\nh\nc\nal \n{L} _{\\mathrm {LLM}}=-\\ s um _{i= 1}^ k \\l og  p_{\\left  \\ { \\ theta _{L} \\cup \\theta _{l} \\cup \\mathbf {E}_{g}\\right \\}} ([\\operatorname {GOAL}_{i}] \\mid \\mathbf {E}_{v}(O_{t}),\\nonumber x_{1}, ..., x_{T}, [\\operatorname {GOAL}_{1}], \\ldots ,[\\operatorname {GOAL}_{i-1}]) \\tag {1} \n(1)\n8\nE. Zhou et al.\nWe add LoRA [31] parameters θl into the LLM’s self-attention projection layers\nfor efficient fine-tuning while keeping all LLM parameters θL frozen. During\ntraining, only the LoRA [31] parameters θl and the Learnable Goal Tokens Eg\nare updated. The hidden states h[GOAL] corresponding to Eg tokens are used to\ngenerate imaginations in the following module.\nGoal Imagination Generation via Latent Imagination. To address the\ndisparity between the LLM’s hidden states and the CLIP [53] text encoder’s\nfeature spaces, we must transform the LLM’s sequential goal tokens into seman-\ntically relevant representations for guiding goal imagination generation. Inspired\nby BLIP2 [39] and InstructBLIP [16], we employ a Goal Q-Former Q with several\nLearnable Dream Query, to derive the goal imagination representation f ∗:\n  f ^\n*\n=\\mathc\nal {Q}\\left (h_{[\\operatorname {GOAL}]}\\right ) \\tag {2} \n(2)\nTo enhance goal imagination with representation f ∗to guide imagination gen-\neration, we utilize a latent diffusion model combining a variational autoen-\ncoder (VAE) [36] for latent space denoising diffusion. Drawing from Instruct-\nPix2Pix’s [6] latent diffusion approach, a cornerstone in instruction-based image\nediting, our model introduces noise to the latent encoding z = E(It+1) of the goal\nimagination It+1 through encoder E, yielding a noisy latent zs across timesteps\ns ∈S. A U-Net [59] ϵδ is trained to estimate this noise, conditional on the cur-\nrent observation co = E(Ot) and text instruction cT , by merging co with zs. The\nspecific process can be formulated as follows:\n  \\mat h cal {L}_{\\mathrm {dre am}}=\\math b b {E} _{\\mathcal  {E}(\\ m a thca\nl {I}_{t+1}), \\mathcal {E}(O_{t}), c_{T}, \\epsilon \\sim \\mathcal {N}(0,1), s}[\\| \\epsilon \\nonumber -\\epsilon _\\delta (s, \\mathrm {concat}[z_s, \\mathcal {E}(O_{t})]+f^*) \\|_2^2] \\tag {3} \\label {eq:diffusion} \n(3)\nwhere ϵ is unscaled noise, s is the sampling step, zs is latent noise at step\ns, E(Otn) is the current observation condition, and cT is the text instruction\ncondition. The concat corresponds to the concatenation operation.\n3.5\nPrompt Generator and PolicyNet\nTo transform goal imaginations into precise latent visual prompts that the Poli-\ncyNet can understand, we require a Prompt Generator to serve as the bridge be-\ntween the Imaginator and the PolicyNet. Inspired by STEVE-1 [42], our prompt\ngenerator is a conditional variational autoencoder (CVAE) [36,64] model trained\non the Goal Drift subset dataset. It encodes the current observations, goal imagi-\nnations, and instructions by MineCLIP [20] to produce three embeddings. These\nembeddings are then reconstructed into a latent visual embedding within the\nMineCLIP [20] visual space and a linear layer then projects it into the visual\ninput space of our PolicyNet.\nIn our PolicyNet, we utilize the architecture of the existing model named\nVPT [3] and the training parameters of STEVE-1 [42]. Specifically, as shown\nin Fig. 2, we first process the current observation with a Visual Encoder (i.e.,\nResNet [28]) of VPT [3] and get representation ft. After adding it with the\nlatent visual prompts pt generated by the Prompt Generator, the sum result ot is\nthen fed into the PolicyNet. PolicyNet, whose backbone is Transformer-XL [17],\nprocesses the current input representations ot and autoregressively predicts the\nnext action at. We can describe the process where the Prompt Generator creates\nMineDreamer\n9\nlatent visual prompts pt and PolicyNet predicts the next action at based on them\nand historical observations using the following simple notation:\n p _ {t} \\ lefta rro\nw \\ mathca\nl { G} ( \\ma\nth c a l {O} _ { t } ,  \\mathcal {I}_{t+1}, y) , ~~~ f_{t} \\leftarrow \\mathcal {V}(\\mathcal {O}_{t}),~~~ o_{t} \\leftarrow f_{t} + p_{t}, ~~~ {a}_{t} \\leftarrow \\mathcal {T}(o_{t-T}, \\ldots , o_{t})\\tag {4}\n(4)\nwhere G is PromptGenerator, V is VisualEncoder, and T is TransformerXL [17].\n4\nExperiments\n4.1\nExperimental Setup\nTraining Process. The training process of Imaginator is divided into three\nmain stages. In the first stage, the MLLM is aligned with the CLIP [54] text\nencoder [53] using the QFormer [39]. In the second stage, we apply Instruct-\nPix2Pix [6] to warm up the weights for the diffusion model in Minecraft. In the\nthird stage, we optimize Imaginator in an end-to-end manner. To be specific, the\nweights of LLaVA [43] are frozen and LoRA [31] is added for efficient fine-tuning.\nFor the diffusion model, we directly use the weights pre-trained in the second\nstage as the initial weights in Imaginator. The CVAE [36,64] within the Prompt\nGenerator features a Gaussian prior and a Gaussian posterior, with its encoder\nand decoder, parameterized as three-layer MLPs, each with 512 hidden units\nand layer normalization [2], similar to the architecture of STEVE-1’s [42] prior.\nMore training details can be found in Supp. C.\nTraining Datasets. In the first stage of Imaginator, we use the extensive corpus\nCC12M [10], and our Goal Drift Dataset is used in the second and third stages.\nWe follow STEVE-1’s [42] approach for CVAE [36,64] training, curating a subset\nof approximately 10k quadruplets from the Goal Drift Dataset for our test tasks.\nThis subset includes current observations, goal imaginations, and instructions\nthat match the Goal Drift Dataset. We use the MineCLIP [20] video encoder to\ntransform the goal imagination and the previous 16 frames into a visual prompt\nembedding, which acts as the ground truth. More details can be found in Supp. B.\nEnvironment Setting. We employ MineRL [26] as the Minecraft simulation.\nThe observation space is limited to RGB images, and the action space is confined\nto keyboard and mouse controls, which are consistent with human interaction.\nFor more details about the simulator, please check Supp. A.\nBaseline. We compare MineDreamer with three baseline:\n1. VPT [3], a foundation model pretrained on 70k hours gameplay. Here, we se-\nlect the VPT(rl), which is finetuned by reinforcement learning on the original\nVPT [3] foundation model but cannot follow instructions.\n2. STEVE-1 [42], an instruction-following agent finetuned from VPT(rl). Here,\nwe select STEVE-1(text), which uses a simple prior to aligning the text with\nthe visual space, without considering the current observation.\n3. Multi-Modal Memory, a substitute for the Imaginator and Prompt Genera-\ntor in MineDreamer, efficiently searches through extensive instruction-video\npairs to find the most relevant video as a visual prompt based on the given\ninstruction and the current observation, which effectively leverages the\ncurrent observation and incorporates a CoI mechanism.\n10\nE. Zhou et al.\nFig. 5: Performance on Programmatic Evaluation. MineDreamer surpasses the\nunconditional VPT [3], the text-conditioned STEVE-1 [42] that ignores current state,\nand the Multi-Modal Memory that utilizes current state with a CoI mechanism.\nFor more details about the baseline, please check Supp. D.1.\nEvaluation. We utilize STEVE-1’s [42] early-game evaluation suite, which com-\nprises two evaluations: (1) Programmatic Evaluation, a quantitative evaluation\nused to evaluate an agent’s ability to execute single-step instruction steadily.\nWe track the states provided by the simulator to calculate metrics (e.g., wooden\nlog collection, travel distance). (2) Command-Switching Evaluation, a quanti-\ntative evaluation designed to assess whether the agent can successfully execute\nmulti-step instructions in sequence to complete long-horizon tasks (e.g., ob-\ntaining diamond\n). We use the success rate as the metric for evaluation. More\nevaluation details can be found in Supp. D.2 and Supp. D.3.\n4.2\nPerformance on Textul Instructions Control\nProgrammatic Evaluation. We quantitatively evaluate all agents on 5 tasks\nand plot the programmatic metric performances(mean and 95% confidence in-\ntervals). Each task runs 10 trials with distinct environment seeds, limiting 3,000\nframes (i.e., 2.5 minutes of gameplay) which are consistent with STEVE-1 [42].\nUnlike STEVE-1 [42], we condition all agents with the most suitable biome.\nFig. 5 compares the performance of our MineDreamer with the uncondi-\ntional VPT [3], the text-conditioned STEVE-1 [42] and MineDreamer using\nMulti-Modal Memory. With appropriate text instructions, MineDreamer signif-\nicantly outperforms the unconditional VPT [3], collecting 64× more seeds\n,\n7× more wood\n, 41× more dirt\n, traveling 2.7× further\n, and digging 22×\ndeeper\n. It also surpasses the STEVE-1 [42], collecting 1.7× more seeds\n,\n1.4× more wood\n, 2.1× more dirt\n, traveling 1.2× further\n, and digging\n1.9× deeper\n. Compared to Multi-Modal Memory, MineDreamer collects 1.8×\nmore seeds\n, 1.5× more wood\n, 1.8× more dirt\n, travels 1.3× further\n, and\ndigs 1.1× deeper\n. This demonstrates that our CoI mechanism, which breaks\ndown instructions into multiple stages and executes them step by step, leads to\nsteadier instruction following compared to STEVE-1 [42] which uses direct text\ninstruction guidance. Unlike Multi-Modal Memory, which also features the CoI\nmechanism, our method generates future imaginations that closely resemble the\ncurrent state at each stage, resulting in providing more precise visual prompts\nof the desired behavior, thus enhancing the stability of action generation.\nWe also observe an interesting phenomenon: while Multi-Modal Memory,\nusing the CoI mechanism and current observations, outperforms unconditional\nMineDreamer\n11\n4\n8\n16\n32\n64\nHorizontal   Altitude    (Blocks)\nSwitch instructions when reaching 13th level\n(Dig down -> Mine horizontally)\nSuccess Rate: 10%\nFailed to maintain \nhorizontal altitude Success Rate: 0%\nFig. 6: Performance on Command-Switching Evaluation. (Left) MineDreamer\nswiftly adapts to instructions and follows them steadily, achieving a higher success rate\nthan the unconditional VPT [3], the text-conditioned STEVE-1 [42], and the Multi-\nModal Memory with CoI mechanism. (Right) MineDreamer can dig down\nto a\ndepth of 13 and steadily mine horizontally\nto obtain diamonds\nwith an average\nsuccess rate of 10%, while STEVE-1 [42] struggles to maintain a consistent altitude.\nVPT [3], it sometimes underperforms compared to STEVE-1 [42]. Upon review-\ning the recorded videos and the results of memory retrieval, we find that due\nto the vast diversity of open-world environments, the videos retrieved by Multi-\nModal Memory still exhibit slight differences from the current state. This dis-\ncrepancy misguides the PolicyNet in predicting agent actions, indicating that the\nCoI’s effectiveness hinges on the relevancy and precision of future imaginations\nor visual prompts to the current state.\nCommand-Switching Evaluation for Long-Horizon Tasks. In this part,\nwe explore agents’ ability to solve long-horizon tasks that require executing\nmulti-step instructions in sequence, including (1) collect wood\nand then craft\nplanks\n, (2) gather dirt\nand then build a tower\nand (3) dig down\nand\nthen mine horizontally\nfor diamonds\n, each with 50 trials. Tasks 1 and 2\nlimits 3,000 frames (i.e., 2.5 minutes of gameplay), with instructions changing\nat 1,500 and 2,000 frames. Task 3 limits 12,000 frames (i.e., 10 minutes of game-\nplay), switching instructions upon reaching the 13th floor, as diamonds\nare\ncommonly found between the 7th and 14th floors.\nIn Fig. 6 (Left), MineDreamer consistently surpasses VPT [3] and STEVE-\n1 [42] in Command-Switching tasks. VPT ’s [3] inability to follow instructions\nleads to a complete failure in executing sequential instructions, as evidenced by\na 0% success rate in the evaluation. Although STEVE-1 [42] occasionally com-\npletes Command-Switching tasks, it underperforms compared to MineDreamer.\nFor instance, in the Obtain diamond\ntask, STEVE-1’s [42] success rate is\n0%, while Multi-Modal Memory’s success rate is 2%, notably lower than Mine-\nDreamer’s 10%. As shown in Fig. 6 (Right), we reconstruct an instance where\ntwo agents act in the same environment based on the simulator records. Initially,\nboth MineDreamer and STEVE-1 [42] rapidly dig down\nto the target depth\nand then mine horizontally\nto obtain diamonds\n. Compared to STEVE-\n1 [42], MineDreamer can consistently maintain the specified horizontal level over\nan extended period and successfully obtains diamonds\naround the 10k steps\nin this instance. While STEVE-1 [42] manages to maintain its specified horizon-\ntal level for a long time, it ultimately fails to do so and becomes stuck in the\nbedrock layer (i.e., the agent cannot break any block), resulting in a 0% success\nrate. This demonstrates that, even when instructions are switched rapidly, the\n12\nE. Zhou et al.\nCoI mechanism can still drive the agent to generate future goal imaginations\nthat align with the current state. Visual prompts generated from these imagina-\ntions enable the agent to quickly adapt its actions to correspond with the new\ninstructions while steadily following the instructions in action generation.\nCurrent observation\nInstructPix2Pix\nMineDreamer\nGround Truth\n“Go explore.” \n“Chop a tree, collect the log.” \n“Place a torch on the wall.” \nFig. 7: Qualitative Comparison of Goal Imagination Generation. When compared to\nInstructPix2Pix [6] that have undergone further fine-tuning on our Goal Drift Dataset,\nour approach demonstrates superior goal imagination capabilities in embodied scenar-\nios. See Sec. 4.3 for a more detailed analysis.\n4.3\nQualitative Results of Imaginator\nWe compare Imaginator with the existing state-of-the-art instruction-based im-\nage editing model, namely InstructPix2Pix [6]. Given this model has been trained\non specific datasets, its performance would inevitably be suboptimal if directly\napplied to the Minecraft domain. To facilitate a fair comparison, we fine-tune\nInstructPix2Pix [6] using the same training set employed by the Imaginator and\nassess the performance of the fine-tuned models in addressing tasks in Minecraft.\nFig 7 shows qualitative results in the evaluation set, our methodology exhibits\nenhanced abilities in Goal Imagination Generation within intricate scenarios.\nThe first comparison shows that the Imaginator adeptly captures the agent’s\nperspective shift as it advances, whereas InstructPix2Pix [6] struggles to generate\nimages in alignment with the provided instructions. In the second instance, the\nImaginator specifically visualizes the region with felled trees\n, contrasting with\nInstructPix2Pix [6], which yields an image markedly divergent from the exist-\ning observation background. The third comparison highlights the Imaginator’s\nability to depict enhanced visibility following torch placement, in contrast to\nInstructPix2Pix [6], which merely adds torches without the associated increase\nin illumination. These observations suggest that in scenarios requiring instruc-\ntion reasoning and goal understanding, a simple CLIP [54] text encoder may\nstruggle to guide the diffusion model to generate reasonable goal imagination.\nMineDreamer\n13\nCurrent Observation\nCurrent Observation\nGoal Imagination\nGoal Imagination\nNext Observation\nNext Observation\nFig. 8: The Generalizability of MineDreamer. (Left) Despite excluding data in-\nvolving ‘Dirt’\nor ‘Dig’\nfrom Goal Drift Dataset and retraining, Imaginator can\nstill generate relatively high-quality imaginations aligned with the instruction’s con-\ncept. (Right) The retrained Imaginator remains operational with the CoI mechanism\nand can handle unseen instructions while largely preserving its previous performance.\nHowever, the MLLM can fully utilize its powerful reasoning ability, vast environ-\nmental knowledge, and intrinsic physical rules to correctly understand the goal\nand generate goal imagination. More visual results can be found in Supp. F and\nSupp. G.\n4.4\nDiscussion on Generalization\nIn this part, we will explore the generalizability of MineDreamer, as the agent’s\nability to generalize is key to its behavior in the open world where environments\nare complex and instructions vary widely. Since STEVE-1 [42] has shown its prior\nability to map text to visual prompts effectively, and our Prompt Generator is\nbuilt upon it, we will now concentrate on the generalizability of our Imaginator\nand the entire agent. At first, we exclude data related to the words ‘Dirt’\nor\n‘Dig’\nfrom the Goal Drift Dataset and retrain the model. Then, we observe\nthe images generated in response to the instruction “Collect dirt”\nbased on\nthe current state and the quantity of dirt\ncollected by the agent.\nAs shown in Fig. 8, we find that even after completely removing the concepts\nof ‘Dirt’\nor ‘Dig’\n, Imaginator is still able to generate goal imaginations\nof relatively good quality aligned with the instruction’s concept (i.e., agent\npoints towards the dirt\nand attempt to break it), which can still guide\nthe PolicyNet to follow instructions. The resulting collection of dirt\nis about\n70% of the original amount, which shows that the Imaginator can respond to\nunseen novel instructions while largely maintaining its previous performance.\nWe attribute this to three key factors: (1) The MLLM within Imaginator has\nthe relevant environmental knowledge to map the text ‘Dirt’\nto its corre-\nsponding element in Minecraft images, recognizing its visual counterpart; (2)\ntraining data for related tasks, such as “Collect seeds”\n, enables the MLLM\nTable 1: We study the impact of dataset collection methods on agent performance.\nValues in parentheses represent 95% confidence intervals.\nInstruction\nFixed Timestep\nOnly Backward\nOnly Forward\nNormal\nBackward\nDrift\nDrift\n“Chop a tree”\n7.60(3.84, 11.36)\n10.10(2.82, 5.58)\n4.20(2.82, 5.58)\n24.30(21.71, 26.89)\n“Collect dirt”\n38.60(21.97, 55.23) 30.30(20.71, 39.89) 18.10(6.74, 29.46) 65.20(55.81, 74.59)\n14\nE. Zhou et al.\nto comprehend the meaning of action ‘Collect’ in Minecraft task; (3) The pre-\ntrained Diffusion model can generalize to the Minecraft domain and generate\ngoal imaginations by leveraging the MLLM’s latent representations for under-\nstanding textual semantics mentioned above from the instructions.\nTable 2: We study the impact of the Chain-of-Imagination and diffusion model ability\non agent performance. Values in parentheses represent 95% confidence intervals.\nInstruction\nwo CoI\nRandom\nInstruct-\nNormal\nNoise\nPix2Pix\n“Chop a tree”\n18.70(15.26, 22.14)\n2.70(0.85, 4.55)\n22.90(20.17, 25.63) 24.30(21.71, 26.89)\n“Collect dirt”\n53.50(36.93, 70.07) 10.90(3.95, 17.85) 59.50(54.00, 65.00) 65.20(55.81, 74.59)\n4.5\nWhat Contributes to Performance\nDataset Collection Method. In Tab. 1, we study the impact on agent per-\nformance by training with datasets of equal size collected using fixed Back-\nward timesteps, only Backward Drift, only Forward Drift, and normal Goal Drift\nDataset Collection. Although data collected using the first three methods can\nenable the agent to follow instructions, the Imaginator is affected by Goal Illu-\nsion and Imagination Stagnation, which are discussed in Sec. 3.3. This results\nin the Imaginator’s inability to envision the step-by-step process of completing\nthe instruction and how to steadily complete the instruction multiple times.\nChain-of-Imagination. In Tab. 2, we explore the effect of the CoI mech-\nanism on agent performance, where “wo-CoI” denotes the scenario where the\nagent generates the goal imagination and visual prompt only at the beginning\nand remains unchanged thereafter. Compared to normal performance, “wo-CoI”\nachieves about 77%. This is because the visual prompts generated at the be-\nginning become less capable of providing precise demonstrations of the desired\nbehavior in later stages, resulting in hindering the ability to guide the agent step\nby step more steadily.\nDiffusion Model Ability. In Tab. 2, we explore the impact of diffusion model\nability on performance. Using “random noise” as a goal imagination results in\nvague visual prompts, which drastically reduce performance to merely 10% of its\noriginal level. The performance of InstructPix2Pix [6] and our MLLM-enhanced\ndiffusion model are comparable; however, by leveraging MLLM, our generated\nimages adhere more closely to physical rules and environmental knowledge, as\nshown in Fig. 7. Additionally, as discussed in Sec. 4.2, we discover that the CoI\nmechanism demands a certain quality of goal imagination, suggesting that the\nstronger the Imaginator, the better it can guide agents to follow instructions.\nMore ablation studies can be found in Supp. E.\n5\nConclusion and Limitation\nIn this paper, we introduce an innovative paradigm for enhancing the instruction-\nfollowing ability of agents in simulated-world control. We prove that by employ-\ning a Chain-of-Imagination mechanism to envision the step-by-step process of\nexecuting instructions, and translating imaginations into precise visual prompts\nMineDreamer\n15\ntailored to the current state and instruction, can significantly help the foundation\nmodel follow instructions steadily in action generation. Our Agent, MineDreamer\nin Minecraft, showcases its strong instruction-following ability. Furthermore, we\nshow its potential as a high-level planner’s downstream controller in the chal-\nlenging “Obtain diamond”\ntask. We believe this novel paradigm will inspire\nfuture research and generalize to other domains and open-world environments.\nLimitation.\nFirstly, generating high-quality imagination can take seconds,\nslowing down frequent-use scenarios. Speed enhancements via distillation [61]\nand quantization [24] may mitigate this. Secondly, the Imaginator may pro-\nduce unrealistic hallucinations. Integrating world knowledge via methods such\nas RAG [38] or reducing MLLM hallucinations [45] could mitigate this.\nReferences\n1. Ajay, A., Han, S., Du, Y., Li, S., Gupta, A., Jaakkola, T., Tenenbaum, J., Kaelbling,\nL., Srivastava, A., Agrawal, P.: Compositional foundation models for hierarchical\nplanning. Advances in Neural Information Processing Systems 36 (2024) 4\n2. Ba,\nJ.L.,\nKiros,\nJ.R.,\nHinton,\nG.E.:\nLayer\nnormalization.\narXiv\npreprint\narXiv:1607.06450 (2016) 9\n3. Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton,\nB., Sampedro, R., Clune, J.: Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. Advances in Neural Information Processing Systems 35,\n24639–24654 (2022) 1, 3, 4, 5, 6, 8, 9, 10, 11, 2, 7, 12\n4. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K.,\nDing, T., Driess, D., Dubey, A., Finn, C., et al.: Rt-2: Vision-language-action mod-\nels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818\n(2023) 1\n5. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakr-\nishnan, K., Hausman, K., Herzog, A., Hsu, J., et al.: Rt-1: Robotics transformer\nfor real-world control at scale. arXiv preprint arXiv:2212.06817 (2022) 1, 2\n6. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image\nediting instructions. In: Proceedings of the IEEE\/CVF Conference on Computer\nVision and Pattern Recognition. pp. 18392–18402 (2023) 4, 8, 9, 12, 14, 6, 16, 17\n7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot\nlearners. Advances in neural information processing systems 33, 1877–1901 (2020)\n3\n8. Cai, S., Zhang, B., Wang, Z., Ma, X., Liu, A., Liang, Y.: Groot: Learning to follow\ninstructions by watching gameplay videos. arXiv preprint arXiv:2310.08235 (2023)\n1, 2, 3, 11\n9. Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., Zheng, Y.: Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and editing. arXiv preprint\narXiv:2304.08465 (2023) 4\n10. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail visual concepts. In: Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pattern Recognition. pp.\n3558–3568 (2021) 9\n16\nE. Zhou et al.\n11. Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srini-\nvas, A., Mordatch, I.: Decision transformer: Reinforcement learning via sequence\nmodeling. Advances in neural information processing systems 34, 15084–15097\n(2021) 2\n12. Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz,\nC.R., Goodman, S., Wang, X., Tay, Y., et al.: Pali-x: On scaling up a multilingual\nvision and language model. arXiv preprint arXiv:2305.18565 (2023) 1\n13. Chen, Z., Wang, Z., Wang, Z., Liu, H., Yin, Z., Liu, S., Sheng, L., Ouyang, W.,\nQiao, Y., Shao, J.: Octavius: Mitigating task interference in mllms via moe. arXiv\npreprint arXiv:2311.02684 (2023) 4\n14. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang,\nS., Zhuang, Y., Gonzalez, J.E., et al.: Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https:\/\/vicuna. lmsys. org (accessed 14 April\n2023) (2023) 3\n15. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\nguage modeling with pathways. Journal of Machine Learning Research 24(240),\n1–113 (2023) 1\n16. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,\nS.: Instructblip: Towards general-purpose vision-language models with instruction\ntuning (2023) 8\n17. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.:\nTransformer-xl: Attentive language models beyond a fixed-length context. arXiv\npreprint arXiv:1901.02860 (2019) 8, 9, 12\n18. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems 34, 8780–8794 (2021) 4\n19. Du, Y., Yang, S., Dai, B., Dai, H., Nachum, O., Tenenbaum, J., Schuurmans, D.,\nAbbeel, P.: Learning universal policies via text-guided video generation. Advances\nin Neural Information Processing Systems 36 (2024) 4\n20. Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang,\nD.A., Zhu, Y., Anandkumar, A.: Minedojo: Building open-ended embodied agents\nwith internet-scale knowledge. Advances in Neural Information Processing Systems\n35, 18343–18362 (2022) 3, 8, 9, 2, 5, 7, 14\n21. Fu, T.J., Hu, W., Du, X., Wang, W.Y., Yang, Y., Gan, Z.: Guiding instruction-\nbased image editing via multimodal large language models. arXiv preprint\narXiv:2309.17102 (2023) 4\n22. Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He,\nC., Yue, X., et al.: Llama-adapter v2: Parameter-efficient visual instruction model.\narXiv preprint arXiv:2304.15010 (2023) 4\n23. Geng, Z., Yang, B., Hang, T., Li, C., Gu, S., Zhang, T., Bao, J., Zhang, Z., Hu, H.,\nChen, D., et al.: Instructdiffusion: A generalist modeling interface for vision tasks.\narXiv preprint arXiv:2309.03895 (2023) 4\n24. Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.W., Keutzer, K.: A survey\nof quantization methods for efficient neural network inference (2021) 15\n25. Gong, R., Huang, Q., Ma, X., Vo, H., Durante, Z., Noda, Y., Zheng, Z., Zhu, S.C.,\nTerzopoulos, D., Fei-Fei, L., et al.: Mindagent: Emergent gaming interaction. arXiv\npreprint arXiv:2309.09971 (2023) 3\n26. Guss, W.H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., Salakhutdi-\nnov, R.: Minerl: a large-scale dataset of minecraft demonstrations. In: Proceedings\nof the 28th International Joint Conference on Artificial Intelligence. pp. 2442–2448\n(2019) 9, 1, 3, 4, 10, 11\nMineDreamer\n17\n27. Hafner, D., Pasukonis, J., Ba, J., Lillicrap, T.: Mastering diverse domains through\nworld models. arXiv preprint arXiv:2301.04104 (2023) 3\n28. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition.\npp. 770–778 (2016) 8\n29. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or,\nD.: Prompt-to-prompt image editing with cross attention control. arXiv preprint\narXiv:2208.01626 (2022) 4\n30. Ho,\nJ.,\nSalimans,\nT.:\nClassifier-free\ndiffusion\nguidance.\narXiv\npreprint\narXiv:2207.12598 (2022) 4, 11, 12, 13\n31. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021) 8, 9, 6\n32. Huang, Y., Xie, L., Wang, X., Yuan, Z., Cun, X., Ge, Y., Zhou, J., Dong, C., Huang,\nR., Zhang, R., et al.: Smartedit: Exploring complex instruction-based image editing\nwith multimodal large language models. arXiv preprint arXiv:2312.06739 (2023) 4\n33. Janner, M., Li, Q., Levine, S.: Offline reinforcement learning as one big sequence\nmodeling problem. Advances in neural information processing systems 34, 1273–\n1286 (2021) 2\n34. Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.:\nVisual prompt tuning. In: European Conference on Computer Vision. pp. 709–727.\nSpringer (2022) 2, 7\n35. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,\nM.: Imagic: Text-based real image editing with diffusion models. In: Proceedings\nof the IEEE\/CVF Conference on Computer Vision and Pattern Recognition. pp.\n6007–6017 (2023) 4\n36. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 (2013) 8, 9, 5, 7, 14\n37. Koh, J.Y., Fried, D., Salakhutdinov, R.R.: Generating images with multimodal\nlanguage models. Advances in Neural Information Processing Systems 36 (2024) 7\n38. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H.,\nLewis, M., Yih, W.t., Rocktäschel, T., et al.: Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural Information Processing Systems\n33, 9459–9474 (2020) 15\n39. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597 (2023) 8, 9, 6\n40. Li, L., Dong, B., Wang, R., Hu, X., Zuo, W., Lin, D., Qiao, Y., Shao, J.: Salad-\nbench: A hierarchical and comprehensive safety benchmark for large language mod-\nels. arXiv preprint arXiv:2402.05044 (2024) 4\n41. Liang, Z., Mu, Y., Ma, H., Tomizuka, M., Ding, M., Luo, P.: Skilldiffuser: Inter-\npretable hierarchical planning via skill abstractions in diffusion-based task execu-\ntion. arXiv preprint arXiv:2312.11598 (2023) 4\n42. Lifshitz, S., Paster, K., Chan, H., Ba, J., McIlraith, S.: Steve-1: A generative model\nfor text-to-behavior in minecraft. arXiv preprint arXiv:2306.00937 (2023) 1, 2, 3,\n8, 9, 10, 11, 13, 5, 7, 12\n43. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint\narXiv:2304.08485 (2023) 4, 7, 9, 6\n44. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017) 6\n18\nE. Zhou et al.\n45. Minervini, P., et al.: awesome-hallucination-detection. https:\/\/github.com\/\nEdinburghNLP\/awesome-hallucination-detection (2014) 15\n46. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021) 4\n47. OpenAI: Gpt-4v(ision) system card (2023), https:\/\/openai.com\/research\/gpt-\n4v-system-card 5\n48. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C.,\nAgarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instruc-\ntions with human feedback. Advances in Neural Information Processing Systems\n35, 27730–27744 (2022) 3\n49. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-\n2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824 (2023) 4\n50. Qin, Y., Zhou, E., Liu, Q., Yin, Z., Sheng, L., Zhang, R., Qiao, Y., Shao, J.: Mp5:\nA multi-modal open-ended embodied system in minecraft via active perception.\narXiv preprint arXiv:2312.07472 (2023) 3\n51. Qu, Y., Shen, X., He, X., Backes, M., Zannettou, S., Zhang, Y.: Unsafe diffu-\nsion: On the generation of unsafe images and hateful memes from text-to-image\nmodels. In: Proceedings of the 2023 ACM SIGSAC Conference on Computer and\nCommunications Security. pp. 3403–3417 (2023) 4\n52. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable\nvisual models from natural language supervision. In: ICML (2021) 9\n53. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from\nnatural language supervision. In: International conference on machine learning. pp.\n8748–8763. PMLR (2021) 8, 9, 6\n54. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019) 3, 9, 12\n55. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n1(2), 3 (2022) 4\n56. Rando, J., Paleka, D., Lindner, D., Heim, L., Tramèr, F.: Red-teaming the stable\ndiffusion safety filter. arXiv preprint arXiv:2210.04610 (2022) 4\n57. Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., Barth-Maron,\nG., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.T., et al.: A generalist agent.\narXiv preprint arXiv:2205.06175 (2022) 1, 2\n58. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: Proceedings of the IEEE\/CVF\nconference on computer vision and pattern recognition. pp. 10684–10695 (2022) 4\n59. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference, Munich, Germany, Oc-\ntober 5-9, 2015, Proceedings, Part III 18. pp. 234–241. Springer (2015) 8\n60. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding. Advances in Neural\nInformation Processing Systems 35, 36479–36494 (2022) 4\n61. Salimans, T., Ho, J.: Progressive distillation for fast sampling of diffusion models.\narXiv preprint arXiv:2202.00512 (2022) 15\nMineDreamer\n19\n62. Shi, Z., Wang, Z., Fan, H., Yin, Z., Sheng, L., Qiao, Y., Shao, J.: Chef: A com-\nprehensive evaluation framework for standardized assessment of multimodal large\nlanguage models. arXiv preprint arXiv:2311.02692 (2023) 4\n63. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al.: Master-\ning the game of go with deep neural networks and tree search. nature 529(7587),\n484–489 (2016) 2\n64. Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep\nconditional generative models. Advances in neural information processing systems\n28 (2015) 8, 9, 5, 7, 14\n65. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023) 3\n66. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 3\n67. Tumanyan, N., Geyer, M., Bagon, S., Dekel, T.: Plug-and-play diffusion features\nfor text-driven image-to-image translation. In: Proceedings of the IEEE\/CVF Con-\nference on Computer Vision and Pattern Recognition. pp. 1921–1930 (2023) 4\n68. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nŁ., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017) 6\n69. Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu, M., Dudzik, A., Chung,\nJ., Choi, D.H., Powell, R., Ewalds, T., Georgiev, P., et al.: Grandmaster level in\nstarcraft ii using multi-agent reinforcement learning. Nature 575(7782), 350–354\n(2019) 2\n70. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., Anand-\nkumar, A.: Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291 (2023) 3\n71. Wang, Z., Cai, S., Liu, A., Jin, Y., Hou, J., Zhang, B., Lin, H., He, Z., Zheng, Z.,\nYang, Y., et al.: Jarvis-1: Open-world multi-task agents with memory-augmented\nmultimodal language models. arXiv preprint arXiv:2311.05997 (2023) 3\n72. Wang, Z., Cai, S., Liu, A., Ma, X., Liang, Y.: Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task\nagents. arXiv preprint arXiv:2302.01560 (2023) 3\n73. Wen, M., Lin, R., Wang, H., Yang, Y., Wen, Y., Mai, L., Wang, J., Zhang, H.,\nZhang, W.: Large sequence models for sequential decision-making: a survey. Fron-\ntiers of Computer Science 17(6), 176349 (2023) 2\n74. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,\nShi, Y., et al.: mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178 (2023) 4\n75. Yin, Z., Wang, J., Cao, J., Shi, Z., Liu, D., Li, M., Sheng, L., Bai, L., Huang, X.,\nWang, Z., et al.: Lamm: Language-assisted multi-modal instruction-tuning dataset,\nframework, and benchmark. arXiv preprint arXiv:2306.06687 (2023) 4\n76. Zhang, K., Mo, L., Chen, W., Sun, H., Su, Y.: Magicbrush: A manually anno-\ntated dataset for instruction-guided image editing. arXiv preprint arXiv:2306.10012\n(2023) 4\n77. Zhang, Z., Zhang, Y., Li, L., Gao, H., Wang, L., Lu, H., Zhao, F., Qiao, Y., Shao, J.:\nPsysafe: A comprehensive framework for psychological-based attack, defense, and\nevaluation of multi-agent system safety. arXiv preprint arXiv:2401.11880 (2024) 4\n20\nE. Zhou et al.\n78. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-\nlanguage models. In: Proceedings of the IEEE\/CVF Conference on Computer Vi-\nsion and Pattern Recognition. pp. 16816–16825 (2022) 2, 7\n79. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language\nmodels. International Journal of Computer Vision 130(9), 2337–2348 (2022) 2, 7\n80. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 (2023) 4\n81. Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L.,\nWang, X., et al.: Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory.\narXiv preprint arXiv:2305.17144 (2023) 3\nMineDreamer\n1\nMineDreamer: Learning to Follow Instructions\nvia Chain-of-Imagination for\nSimulated-World Control\nSupplementary Material\nThe supplementary document is organized as follows:\n– Sec. A: Environment Setting, like observation and action space.\n– Sec. B: Dataset composition and collection.\n– Sec. C: Implementation Details, like training details.\n– Sec. D: Experiment Details, like baseline and evaluation details.\n– Sec. E: More Ablation Studies about MineDreamer.\n– Sec. F: More Visual Results about Imagination in MineDreamer.\n– Sec. G: Demo videos about MineDreamer.\nA\nMinecraft Environment\nMinecraft is a widely popular sandbox game that offers players the freedom to\nbuild and explore their worlds without limits, which also extends to AI agents\nas well. Within the game, AI agents encounter situations that closely mirror\nreal-world challenges, requiring them to make decisions and solve endless tasks\nin an open-world setting. Consequently, Minecraft is an ideal platform for AI\nevaluation and stands as an exemplary benchmark for AI testing, due to its vast\nfreedom and open nature. With the help of Minecraft, AI researchers can more\neasily simulate a wide variety of complex and dynamic environments and tasks,\nallowing them to conduct experiments that enhance the practical and applicable\nvalue of AI technologies.\nWe use MineRL [26] v1.0, which corresponds to Minecraft 1.16.5, as our\nsimulation platform, ensuring an environment that is consistent with those used\nby VPT [3] and STEVE-1 [42]. In this version of MineRL [26], a significant\nadvancement over its predecessor (i.e., MineRL v0.4.4), lies in the simulation\nenvironment. The environment now enables AI agents to interact in a manner\nentirely consistent with human players, eschewing primitive actions or script-\nbased APIs. This approach presents a more complex and challenging scenario for\nAI research. More specifically, AI agents experience the environment as humans\ndo, solely through egocentric RGB images, devoid of any privileged in-game\ninformation. Additionally, their interactions with the environment are restricted\nto low-level keyboard and mouse actions. Consequently, AI agents trained in this\nversion of MineRL [26] (i.e., MineRL v1.0) resemble embodied agents capable of\nperforming various tasks in an open-world environment, demonstrating a higher\ndegree of generalization. Furthermore, the abundance of gaming videos available\non the internet (e.g., YouTube), provides AI researchers with the opportunity to\nhttps:\/\/github.com\/minerllabs\/minerl\/releases\/tag\/v1.0\n2\nE. Zhou et al.\nharness these vast datasets for extensive pre-training, enabling the development\nof a foundation model in the sequential decision-making domain.\nA.1\nObservation Space\nOur observation space aligns with that of human players, comprising simply the\nraw pixels from Minecraft. This includes the hotbar, health indicators, player\nhands, equipped items, and the game environment itself. Specifically, the simu-\nlator produces RGB images with a resolution of 640x360. When the agent takes\naction within the environment, the simulator renders the player’s first-person\nperspective with a field of view of 70 degrees. If the agent opens the inventory,\nthe simulator will render the GUI interface along with the mouse cursor.\nNotably, we do not employ privileged information such as voxels and lidar\ninformation available in MineDojo [20], which could be provided to the agent.\nDuring actual inference, the PolicyNet of MineDreamer only accepts the raw\nRGB pixels observations as input that the agent can obtain from the envi-\nronment and generates text-conditioned low-level action controls based on these\nobservations, which are consistent with those used in VPT [3] and STEVE-1 [42].\nA.2\nAction Space\nAs shown in Tab. 3, our action space encompasses a vast array of actions that\nare consistent with those of human players (i.e., keyboard and mouse), including\nkeypresses, mouse movements, and clicks. Excluding the “chat” action, which\nserves to initialize the agent with pre-defined conditions, more details can be\nfound in Supp. A.3. Keyboard presses and mouse clicks are binary functional\nactions (e.g., “Forward”, “Back”, “Left”, “Right” and etc.). Beyond these binary\ninput options, our action space also has mouse cursor movements. While the\nGUI is closed (i.e., activated by pressing “E” for the GUI inventory) and remains\ninactive, the mouse’s horizontal and vertical movements direct the agent’s yaw\nand pitch. Conversely, with GUI open, the same movements are re-purposed to\nnavigate the cursor across the display.\nIt is noteworthy that we have not employed structured APIs such as “craft”\nand “smelt” as seen in MineDojo [20], which replace the need for precise mouse\nmovements that are necessary for interacting with the inventory for certain tasks,\neffectively turning these operations into GUI functional binary actions. During\nactual inference, our MineDreamer’s PolicyNet only outputs keyboard and\nmouse actions to dictate the agent’s movements, aligning these actions with\nthose utilized in VPT [3] and STEVE-1 [42].\nA.3\nEnvironment Settings and Rules\nIn our experiments, the agent’s initial position at the start of the game, as well\nas the seed used to generate the environment, are completely random. This in-\ntroduces an element of unpredictability and variety into the experimental setup,\nensuring that the agent will encounter a wide range of scenarios and challenges.\nMineDreamer\n3\nTable 3: Action Space utilized in the MineRL [26] simulator. The action space\nprimarily consists of 14 keyboard and mouse operations, with detailed descriptions\nsourced from the Minecraft wiki (https:\/\/minecraft.fandom.com\/wiki\/Controls).\nIndex\nAction\nHuman Action Description\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove backward.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nInventory\nkey E\nOpen or close GUI inventory.\n6\nDrop\nkey Q\nDrop a single item from the stack of items the player\nis currently holding.\n7\nJump\nkey Space\nJump. When in the water, it keeps the player afloat.\n8\nSneak\nkey left Shift Move slowly in the current direction of movement.\n9\nSprint\nkey left Ctrl\nMove fast in the current direction of movement.\n10\nAttack\nleft Mouse\nDestroy blocks (hold down); Attack entity (click\nButton\nonce); Pick up the stack of items or place the stack\nof items in the GUI (click once)\n11\nUse\nright mouse\nPlace the item being held or interact with the block\nButton\nthat the player is currently looking at.\n12\nHotbar.[1-9]\nkeys 1 - 9\nSwitch the appropriate hotbar cell.\n13\nYaw\nmove\nTurning; aiming; camera movement.Ranging from\nMouse X\n-180 to +180.\n14\nPitch\nmove\nTurning; aiming; camera movement.Ranging from\nMouse Y\n-180 to +180.\nTo better evaluate the agent’s ability to follow textual instructions for action\nprediction and its ability to rapidly adapt its behavior based on instructions, we\nhave modified MineRL [26] to enable “chat” action operations. This allows for\nthe swift initialization of the agent with predefined conditions through instruc-\ntions. Specifically, for Programmatic Evaluation, we ensure that each experi-\nment for all agents is conducted with the same seed and within the biome most\nconducive to completing the current instruction; across multiple experiments,\ndifferent seeds are used. For Command-Switching Evaluation for Long-Horizon\nTasks, all agents are placed in the same seed and biome optimal for the current\ninstruction as well. In addition, the following rules are applied as aids:\n– \/difficulty peaceful: Set the difficulty of the environment to peaceful mode.\n– \/gamerule doDaylightCycle false: Set the environment to daytime forever.\n– \/gamerule keep inventory true: Set agent to not drop items upon death.\nSpecifically, for the task of “Obtain diamonds”\n, we add two additional rules\non top of the aforementioned ones as assistance:\n– \/effect give @a night_vision 99999 250 true: Help the agent see more clearly\nin extremely dark environments (e.g., at night or underground).\n– \/give @p minecraft:diamond_pickaxe: Provided the agent with a diamond pick-\naxe\n, enabling it to break almost all blocks and mine all ores within Minecraft.\n4\nE. Zhou et al.\nFor details regarding the most suitable biome used in the experiments, please\ncheck Supp. D.2 and Supp. D.3.\nB\nDataset Details\nB.1\nOpenAI Contractor Gameplay Dataset\nAll our raw data are based on the contractor dataset, which consists of offline\ntrajectory data in Minecraft used for training VPT [3]. This dataset is created by\nhiring human contractors to play Minecraft and complete predetermined tasks,\nand it includes video (i.e., image sequences), along with corresponding action\nsequences and metadata. OpenAI releases six subsets of contractor data: 6.x, 7.x,\n8.x, 9.x, 10.x, and the MineRL BASALT 2022 dataset. Our Goal Drift Dataset\nultimately selects three of these subsets as our raw data, including 8.x (house\nbuilding from scratch), 10.x (\n), and the FindCave dataset from the MineRL\nBASALT 2022 dataset. For each video, there is an associated metadata file that\nnot only records the contractor’s actions for every frame but also documents\nevents triggered by the contractor within the simulator; the specific events are\ndetailed in Tab. 4.\nTable 4: The detailed event name and description in MineRL [26] simulator.\nThe simulator records the names of events that occur as well as related information,\nincluding quantities. We can use these events to collect a large amount of data for\ncompleting event-related instruction tasks with clarity.\nEvent Name Description\nmine_block\nThe moment the agent breaks a block, the type of block is recorded.\ncraft_item\nThe moment the agent crafts items, the type and number of items are recorded.\nuse_item\nThe moment the agent uses or places items, the type of item is recorded.\nkill_entity The moment the agent kills an entity, the type of entity is recorded.\nbreak_item\nThe moment the tool of the agent is broken, the type of tool is recorded.\npick_up\nThe moment the agent picks up items, the type and number of items are recorded.\nB.2\nEvent Selection\nIn constructing our dataset, we opt to select events directly from the\nMineRL simulator and supplement them with manually annotated\nevents. Specifically, to train the Imaginator within the constraints of limited re-\nsources, we focus on the following types of events: “mine_block”, “craft_item”,\n“use_item”, “kill_entity” and a manually defined event named “easy_action”.\nDetails of the specific items selected for each event can be found in Tab. 5. The\nsimulator’s built-in events have a clearly defined completion time  t^* , while man-\nually annotated events are marked with a manually labeled completion time.\nhttps:\/\/github.com\/openai\/Video-Pre-Training\nMineDreamer\n5\nTable 5: Details of the specific items selected for each event. We select\nfour built-in events from the simulator, along with a manually defined event called\n“easy_action”. The built-in events have a clearly defined completion moment, while\nthe collection of the “easy_action” event is manually annotated.\nEvent Name mine_block\ncraft_item\nuse_item kill_entity\neasy_action\nDetail items\nWooden Log wooden planks\ntorch\nsheep\nGo explore\nGrass\nDig down\nDirt\nLook at the sky\nGrass Block\nGo Swimming\nSand\nStay underwater\nSnow\nBuild a tower\nStone\nMine horizontally\nCoal Ore\nIron Ore\nRedstone Ore\nDiamond Ore\nB.3\nDataset Collection\nAfter obtaining the completion times  t^* for all events, we employ gpt-4-turbo [47]\nto generate corresponding event-related instructions. Specifically, we provide\ngpt-4-turbo [47] with the event’s name, description, and detailed items, and\nprompt it to generate multiple distinct simple instructions. These instructions\ninclude specific actions, while others mention the items to be obtained upon\ncompleting the action. For instance, for “Grass” in the “mine_block” event,\ngpt-4-turbo [47] would generate instructions like “break grass”, “break tall\ngrass”, “gather seeds”, and “collect seeds”. After gathering instructions for all\nevents, we apply the Goal Drift Collection method described in Sec. 3.3 of the\nmain paper to conduct backward and forward drift on the completion times  t^* of\nevent-related instructions. For each pair (current observation, goal imagination),\nthere are many instructions created by gpt-4-turbo [47] to describe that event.\nThis process results in a substantial collection of triplets (current observation,\ngoal imagination, instruction), which serve as training data for the Imagina-\ntor, forming what we call the Goal Drift Dataset. The final Goal Drift Dataset\ncontains approximately 500,000 triplets (current observation, goal imagination,\ninstruction), with about 400,000 of these triplets derived from events built into\nthe simulator.\nWe follow the method used in STEVE-1 [42] for training the CVAE [36,64]\nand collect a subset of approximately 10,000 quadruplets from the Goal Drift\nDataset for the events we need to test subsequently. This subset consists of\nquadruplets where the current observation, goal imagination, and instruction\nare consistent as conditions with the Goal Drift Dataset. Additionally, there\nis a visual prompt embedding that serves as ground truth. This embedding is\nderived from a video composed of the goal imagination and the preceding 16\nframes, processed through the MineCLIP [20] video encoder.\n6\nE. Zhou et al.\nC\nImplementation Details\nC.1\nImaginator\nThe training process of Imaginator is divided into three main stages. In the\nfirst stage, the MLLM is aligned with the CLIP [53] text encoder using the\nQFormer [39]. In the second stage, we apply InstructPix2Pix [6] to warm up\nthe weights for the diffusion model in Minecraft. In the third stage, we optimize\nImaginator in an end-to-end manner. To be specific, the weights of LLaVA [43]\nare frozen and LoRA [31] is added for efficient fine-tuning. For the diffusion\nmodel, we directly use the weights pre-trained in the second stage as the initial\nweights in Imaginator.\nFor the Large Language Model with visual input (e.g., LLaVA [43]), we choose\nLLaVA-1.1-7b [43] as the base model. During training, the weights of LLaVA are\nfrozen and we add LoRA for efficient fine-tuning. We expand the original LLM\nvocabulary with 32 new tokens. The QFormer is composed of 6 transformer [68]\nlayers and 77 learnable query tokens. We use the AdamW optimizer [44] in all\nthree stages. In the initial stage of training, we configure the learning rate and\nweight decay parameters at 2e-4 and 0, respectively. The training targets for\nthis stage encompass a dual-objective framework, comprising the Mean Squared\nError (MSE) loss between the outputs of LLaVA [43] and the CLIP [53] text en-\ncoder, alongside the language model loss. Both losses are assigned equal weights\nof 1. The training setting in the second is the same as InstructPix2Pix [6]. In\nthe final stage, the settings for the learning rate, weight decay, and warm-up\nratio are adjusted to 1e-5, 0, and 0.001, respectively. During this phase, the loss\nfunction is diffusion loss.\nTable 6: The Hyperparameters of Imaginator.\nHyperparameter Name\nValue\nbase_model\nLLaVA [43]\ninput_image_size\n256 × 256\nexpand_vocabulary_num\n32\ntransformer_layers_num\n6\nQFormer_learnable_query_num\n77\noptimizer\nAdamW [44]\nlearning_rate_initial_stage\n2e-4\nweight_decay_initial_stage\n0\nlearning_rate_final_stage\n1e-5\nweight_decay_final_stage\n0\nwarm-up_ratio_final_stage\n0.001\nn_iterations_initial_stage\n5000\nn_iterations_final_stage\n10000\nMineDreamer\n7\nC.2\nPrompt Generator\nOur Prompt Generator is mainly a conditional variational autoencoder (CVAE) [36,\n64] with a Gaussian prior and a Gaussian posterior similar to STEVE-1 [42].\nBoth the encoder and decoder of CVAE [36,64] are parameterized as three-layer\nMLPs with 512 hidden units and layer normalization. It encodes the current ob-\nservations, goal imaginations, and instructions then reconstructs a latent visual\nembedding, and uses a linear layer to project this embedding into the visual\ninput space of our PolicyNet as the final visual prompt.\nIt is noteworthy that instead of using raw pixel images and natural language\ninstructions directly as conditions to generate pixel-level videos depicting the ex-\necution of an instruction from the current observation to the imagined target, we\nopt to perform reconstruction within the visual space of MineCLIP [20], where\nMineCLIP [20] is a pre-trained CLIP model that employs a contrastive objective\non pairs of Minecraft videos and associated transcripts from the web. Specifically,\nthe process of generating prompts by the Prompt Generator mainly involves\nthree steps. First, we stack the current observation and the goal imagination 16\ntimes each to create two static 16-frame videos. These are then processed through\nMineCLIP [20]’s video encoder to obtain two visual embeddings. Concurrently,\nthe instruction is encoded into a text embedding using MineCLIP [20]’s text en-\ncoder. This ensures that all embeddings are encoded within the MineCLIP [20]\nspace. We then train a CVAE [36,64] using the ELBO loss, which reconstructs\na latent visual embedding from the previous three embeddings. This representa-\ntion is a video embedding that describes the process within the MineCLIP [20]\nvisual space. This representation is a video embedding that captures the process\nwithin the MineCLIP [20] visual space. The ground truth for this is mentioned\nin Supp. B.3 and is derived from the goal imagination and the preceding 16\nframes, which have been processed through the MineCLIP [20] video encoder.\nIn the end, we use a linear layer to project the latent visual embedding into the\nvisual input space of the PolicyNet as the final visual prompt. For each event to\nbe evaluated subsequently, we train a CVAE [36,64] on the dataset, specifically\nfor 150 epochs with early stopping on a small validation set. Notably, the pa-\nrameters of the MineCLIP [20] within Prompt Generator remain unchanged, as\ndo the parameters of the linear layer that maps MineCLIP [20]’s visual space to\nthe visual input space of PolicyNet, whose parameters come from STEVE-1 [42].\nThe hyperparameters used during the training are listed in the following Tab. 7.\nD\nExperiment Details\nIn this section, we first detail the three baselines we select. We then separately\npresent the Programmatic Evaluation details and the Command-Switching Eval-\nuation for Long-Horizon Tasks details.\nD.1\nBaseline Datails\nVideo Pretraining (VPT) [3] is the first foundation model in the Minecraft\ndomain, pre-trained on 70k hours of gameplay by Baker et al. [3]. Its archi-\n8\nE. Zhou et al.\nTable 7: The Hyperparameters of CVAE [36,64] within Prompt Generator.\nHyperparameter Name\nValue\narchitecture\nMLP\nvisual_prompt_dim\n512\ntext_dim\n512\ncurrent_img_dim\n512\ngoal_img_dim\n512\nhidden_layers\n3\nbatch_size\n256\nlearning_rate\n1e-4\nβ\n0.001\nn_epochs\n150\ntecture primarily consists of two parts: ImpalaCNN and TransformerXL [17].\nVPT [3] has three variants: VPT(fd), VPT(bc), and VPT(rl), representing the\nvanilla foundation model, the behavior cloning fine-tuned model, and the RL\nfine-tuned model, respectively. Specifically, they initially pre-trained on a large\ncorpus of YouTube videos using a behavior cloning algorithm to obtain VPT(fd),\nwhich is capable of free exploration within the environment. This model gains\na fundamental understanding of the environment and acquires some environ-\nmental knowledge. To enhance the agent’s capability in completing early-game\ntasks (e.g., “Collect wood”\nand “Craft wooden planks”\n, they collect an\n“Early-Game” video dataset and fine-tune the VPT(fd) to obtain VPT(bc). This\nmodel performs well in early-game tasks but struggles with long-horizon tasks,\nsuch as obtaining diamonds\n. Building on VPT(bc), they employ online re-\ninforcement learning with carefully designed rewards to fine-tune the model,\nenabling it to complete the task of obtaining diamonds\nfrom scratch, ulti-\nmately resulting in the creation of VPT(rl). Hence, it is noteworthy that all\nthree variants of VPT [3] are unable to follow instructions; they must\nfirst be fine-tuned on downstream tasks before they can be completed. Despite\ntheir extensive environmental knowledge, this knowledge cannot be unlocked by\ninstruction-following capabilities. In our experiments, we use VPT(rl) because\nit initially seeks out trees\nand gathers wood\n, a critical step in the pathway\nto obtaining diamonds\n. When set in the appropriate biome, VPT(rl) explores\nfurther\nand collects more wood\ncompared to VPT(fd) and VPT(bc).\nSTEVE-1 [42] is a Minecraft agent that can follow both textual and visual in-\nstructions, built upon MineCLIP [20] and VPT [3]. Drawing from the paradigms\nof instruction tuning in large language models and multimodal large language\nmodels, it successfully unlocks the instruction-following abilities of the founda-\ntion model (i.e., VPT [3]) in the domain of decision-making. STEVE-1 [42] comes\nin two variants, STEVE-1(visual) and STEVE-1(text). The training process is\ndivided into two steps. The first step involves training a policy conditioned on\nfuture video as visual instructions using the packed hindsight relabeling method.\nSpecifically, they utilize the OpenAI Contractor Gameplay Dataset to fine-tune\nMineDreamer\n9\nVPT(rl) to follow visual instructions, resulting in STEVE-1(visual). The sec-\nond step is to train a model that can map text instructions to visual instruc-\ntions. Inspired by UnCLIP, they trained a Conditional Variational Autoencoder\n(CVAE) [36, 64] on a dataset of video-text pairs they collected, thus obtaining\nSTEVE-1(text) which can follow text instructions. It is important to note that\nthe visual or textual instruction variants of STEVE-1 [42] do not con-\nsider the current observation and remain unchanged throughout the\ntask, serving as an initial guide without adapting to environmental\nchanges.\nMulti-Modal Memory serves as a substitute for the Imaginator and Prompt\nGenerator in the MineDreamerframework, essentially functioning by supplying\nPolicyNet with video prompts that best align with the current observations and\ntextual instructions, similar to the approach of STEVE-1 (visual). We construct\na multi-modal memory comprised of numerous video-text pairs. This memory\nis specifically built upon the triplets (current observation, goal imagination, in-\nstruction) from the Goal Drift Dataset. By tracing back 16 frames from the\ntimestamp of the goal imagination, we create a 16-frame video segment, result-\ning in a revised triplet format: (current observation, goal imagination video,\ninstruction). Each event, whether from the MineRL [26] environment or manu-\nally defined, contains 1,000 pairs. The retrieval process is as follows: First, we\nencode the current instruction and all instructions in the multi-modal memory\nusing the OpenCLIP [52] text encoder to obtain embeddings. We then compare\nthese embeddings using cosine similarity. Next, within the memory correspond-\ning to the text instruction with the highest similarity, we find the match where\nthe current observation and the memory’s observation, once encoded through\nthe OpenCLIP [52] Image encoder, have the highest cosine similarity in their\nembeddings. Finally, the video from the final retrieval result is then encoded us-\ning the MineCLIP [20] video encoder, and the resulting visual embedding is used\nas the final visual prompt. Therefore, Multi-Modal Memory leverages the\ncurrent observation and also utilizes the Chain-of-Imagination (CoI)\nmechanism.\nD.2\nProgrammatic Evaluation Datails\nIn this part, we will elaborate on the selection of experimental tasks for Pro-\ngrammatic Evaluation, the methodology for calculating evaluation metrics, and\nthe specific details of the experimental setup.\nFor the Programmatic Evaluation, we evaluate the agents on five single-step\ninstruction tasks derived from the early-game evaluation suite proposed in Table\n3 of the STEVE-1 [42] appendix. The purpose of this evaluation is to quanti-\ntatively measure an agent’s ability to follow instructions with minimal human\nintervention. Specifically, we calculate the programmatic evaluation metrics by\nmonitoring the state of the MineRL [26] environment during each evaluation\nepisode. Consistent with VPT [3] and STEVE-1 [42], we compute multiple pro-\n10\nE. Zhou et al.\ngrammatic metrics, including travel distance, dig depth, and early-game item\ncollection. The calculation is as follows:\n1. Travel Distance (Blocks): The agent’s maximum horizontal displacement, in\nthe X-Z plane, is measured from the initial spawn point.\n2. Dig Depth (Blocks): The agent’s maximum vertical (Y-axis) displacement is mea-\nsured from its initial spawn point.\n3. Early-Game Inventory Counts: The maximum number of log\n, seed\n, and\ndirt\nitems seen in the agent’s inventory during the episode.\nWe test all agents on these five single-step instruction tasks, with each\ntask running 10 episodes of 3000 timesteps (i.e., 2.5 minutes of gameplay). Each\nepisode used a unique environmental seed, yet all agents were tested under the\nsame seed for consistency. It is important to note a key difference in our exper-\nimental setup compared to STEVE-1 [42]: for each task, we initialize the\nagents in the biome most conducive to task completion to enhance the\nreliability of our evaluation metrics. For instance, in the “Chop a tree”\ntask,\nall agents are spawned in a forest biome, rather than a plain, to avoid the added\nrandomness of searching for trees\nbefore chopping them. Due to a limited com-\nputational budget, we do not generate goal imaginations for every frame within\nan episode. In MineRL [26], an agent can perform only one mouse or keyboard\naction per frame, and for tasks such as breaking a block of dirt\n, it requires\napproximately 25 frames of consistently holding down the left mouse button.\nTherefore, we decide to imagine a goal imagination and translate it to\na visual prompt every 25 frames ultimately, which then guides the action\ngeneration for the following 25 frames (i.e., the visual prompt pt will not change\nfor the next 25 frames). This interval is chosen because, aside from the “Chop a\ntree”\ntask, the other four tasks can be achieved within 25 frames (i.e., just\nover 1 second of gameplay), thereby necessitating a new round of imagination to\nguide subsequent actions. The detailed settings for the Programmatic Evaluation\ncan be found in Tab. 8.\nTable 8: The detailed settings for the Programmatic Evaluation.\nId Text Instruction Biome Time Limit Imagination Interval\nMetric\n1\ngo explore\nPlains\n3000 Frames\n25 Frames\nTravel Distance (Blocks)\n2 collect seeds\nPlains\nSeeds Collected\n3\nchop a tree\nForest\nWooden Logs Collected\n4 collect dirt\nPlains\nDirt Collected\n5\ndig down\nPlains\nDig Depth (Blocks)\nD.3\nCommand-Switching Evaluation Datails\nIn this part, we will also detail the selection of experimental tasks for Command-\nSwitching Evaluation for Long-Horizon Tasks, the calculation methods for eval-\nuation metrics, and the specific details of the experimental setup.\nMineDreamer\n11\nThe Command-Switching Evaluation for Long-Horizon Tasks comprises three\nmulti-step instructions tasks sourced from the early-game evaluation suite of\nSTEVE-1 [42], except the “Obtain diamonds”\ntask which originates from\nGROOT [8], designed to steadily follow video instructions. These tasks aim to\nevaluate an agent’s ability to swiftly adapt to new instructions following an in-\nstruction switch, a critical capability for a downstream controller operating under\nan LLM-based high-level planner. We employ success rate as the performance\nmetric, also by monitoring the MineRL [26] environment state throughout each\nevaluation episode. The criteria for determining success across the three different\ntasks are as follows:\n1. collect wood\nand then craft planks\n: Success is defined as successfully\ncrafting at least one wooden log\ninto four wooden planks\nwithin the given\ntime frame.\n2. gather dirt\nand then build a tower\n: Success is defined as successfully\nbuilding a tower\nwith a height of at least 7 blocks within the given time frame.\n3. dig down\nand then mine horizontally\n: Success is obtaining at least one\ndiamond\nwithin the given time frame.\nFor these three multi-step instructions tasks, we run 50 episodes of testing\nper task. The time limit for the first two tasks is set at 3000 frames (i.e., 2.5\nminutes of gameplay), consistent with STEVE-1 [42], while the final task has\nan episode time limit of 12,000 frames (i.e., 10 minutes of gameplay), aligning\nwith what is mentioned in the main paper of GROOT [8]. Each episode utilizes\na unique environmental seed to ensure variability; however, all agents are tested\nwith the same seed for consistency across episodes. It is important to note that\nour experimental setup differs from that of STEVE-1 [42] in that we initialize\nthe agents in the biome most conducive to task completion for each\ntask. Specifically, as mentioned in Supp. A.3, we utilize the “chat” action to ini-\ntialize the agent. For the “Obtain diamonds”\ntask, we equip the agent with\nnight vision and a diamond pickaxe\n, which is consistent with the description\nprovided in the main paper of GROOT [8]. Considering that STEVE-1 [42]\nmay not be explicitly trained on the “mine horizontally”\ninstruc-\ntion, we augment STEVE-1 [42]’s prior original training data with\nthe corresponding text-video pairs from the Goal Drift Dataset and\nretrain the prior. This ensures that the updated prior can map the textual\ninstruction “mine horizontally”\nto the associated visual instructions. The\ndetailed settings for the Command-Switching Evaluation for Long-Horizon Tasks\nexperiment can be found in Tab. 9.\nE\nMore Ablation Studies\nIn this section, we introduce additional ablation studies to explore various con-\ntributors to performance. This includes the use of Classifier-Free Guidance [30]\nduring inference, the selection of Drift Lengths from the Goal Drift Dataset, and\nthe generation strategies for Visual Prompts. We employ the same experimental\n12\nE. Zhou et al.\nTable 9: The detailed settings for the Command-Switching Evaluation.\nId\nText Instruction\nBiome Switch Condition\nTime Limit\nImagination Interval\n1\nchop a tree\nForest Reach 1500 Frames 3000 Frames\n25 Frames\ncraft wooden planks\n2\ncollect dirt\nPlains Reach 2000 Frames 3000 Frames\n25 Frames\nbuild a tower\n3\ndig down\nPlains Reach 13th floors 12000 Frames\n25 Frames\nmine horizontally\nsettings as our Programmatic Evaluation, compare the performance of differ-\nent ablations, and plot the results, showing both the mean and 95% confidence\nintervals of the programmatic metrics.\nE.1\nClassifier-Free Guidance During Inference\nGiven that VPT [3] is a foundation model obtained through behavior cloning\nfrom extensive video demonstrations without instruction guidance during train-\ning, this may lead to a smoother behavior distribution learned by VPT [3].\nConsequently, even after fine-tuning VPT [3] for instruction-following abilities,\nwhen provided with direct instruction as a condition, it tends to act based on\nits previously learned knowledge from behavior cloning. It fails to steadily fol-\nlow the instructions given, similar to the observation in Appendix I of Baker et\nal. [3]. We believe that this bias arises inherently from the training process of\nVPT [3]. Inspired by STEVE-1 [42], we employ classifier-free guidance [30] to\nmitigate this bias as much as possible in the action logits space before sampling\nthe action. Specifically, for each inference, we perform two computations of log-\nits through PolicyNet: one with visual prompt guidance and the other without.\nAt each timestep, we subtract a certain proportion of the action logits\nfrom the unconditioned PolicyNet from those predicted by the visual\nprompt-conditioned PolicyNet. The equation for computing logits is directly\nborrowed from STEVE-1 [42].\n  f _{t} \\\nle f tarrow\n \\math c al {V }( \\math c a l  { O}_\n{\nt}\n)\n,~~~ o_{t} \\lefta\nrr ow  f_{t }  +  p _{t\n}\n, \n~\n~~\\operatorna me {logits} \\leftarrow (1+\\lambda ) \\underbrace {\\mathcal {T}_{\\theta }\\left (o_{t-T}, \\ldots , o_{t}\\right )}_{\\text {conditional logits }}-\\lambda \\underbrace {\\mathcal {T}_{\\theta }\\left (f_{t-T}, \\ldots , f_{t}\\right )}_{\\text {unconditional logits }} \\tag {1} \n(1)\nwhere V is the VisualEncoder and T is the TransformerXL [17], Ot is the current\nobservation, pt is the visual prompt, λ is the trade-off parameter between the\nvisual prompt conditioned logits and unconditioned logits. By setting a suitable\nvalue for λ, we can encourage PolicyNet to follow the instructions in action gen-\neration more steadily. Fig. 9 illustrates how choosing different values affects the\nagent’s performance in Programmatic Evaluation. When the value of λ is less\nthan 6, performance improves with an increase in λ, indicating that classifier-\nfree guidance [30] can significantly reduce the bias introduced by prior behavior.\nThe agent performs optimally when λ is 6 to 8; beyond this range, the perfor-\nmance begins to decline. This decrease is due to excessive guidance disrupting\nthe agent’s original understanding and knowledge of the environment, imped-\ning its ability to act normally. Ultimately, we opt for a value of λ equal to 6.\nMineDreamer\n13\n𝜆= 0\n𝜆= 2\n𝜆= 4\n𝜆= 6\n𝜆= 8\n𝜆= 10\n𝜆= 0\n𝜆= 2\n𝜆= 4\n𝜆= 6\n𝜆= 8\n𝜆= 10\n𝜆= 0\n𝜆= 2\n𝜆= 4\n𝜆= 6\n𝜆= 8\n𝜆= 10\n𝜆= 0\n𝜆= 2\n𝜆= 4\n𝜆= 6\n𝜆= 8\n𝜆= 10\nFig. 9: The impact of different values of condition scale  \\lambda on the perfor-\nmance of the agent by using classifier-free guidance. Selecting the optimal\nparameter \\lambda to balance between visual prompt-conditioned and unconditioned set-\ntings can significantly enhance agent performance, consistently improving its ability\nto follow instructions. By using the best λ (λ = 6), MineDreamer when significantly\noutperforms MineDreamer when λ = 0 (no guidance), collecting 32× more seeds\n,\n5× more wood\n, 7.3× more dirt\n, travelling 2× further\n.\nAfter utilising classifier-free guidance [30] during inference, MineDreamer when\nλ = 6 significantly outperforms MineDreamer when λ = 0 (no guidance), col-\nlecting 32× more seeds\n, 5× more wood\n, 7.3× more dirt\n, travelling 2×\nfurther\n. Therefore, selecting an appropriate value for parameter λ to balance\nthe trade-off between visual prompt-conditioned and unconditioned logits can\nsignificantly enhance the agent’s performance and steadily improve its ability to\nfollow instructions in action generation. Although this technique trick is effec-\ntive during inference, it still needs to find the best hyperparameter in practice.\nIn the future, eliminating biased behaviors directly from the fine-tuning process\ntraining would be meaningful.\nE.2\nSelection of Drift Lengths\nDuring Goal Drift Dataset collection, we utilize fixed values for  T_b and  T_f to\naddress the challenges of “Goal Illusion” and “Imagination Stagnation” by per-\nforming backward and forward drifts around the event occurrence moment  t^* .\nThe specific algorithmic procedure is detailed in Sec. 3.3 of the main paper.\nIt is noteworthy that we observe an inconsistency in the optimal Drift Length\nfor each event. As illustrated in Fig. 10, the optimal drift length for “wood”\nis\napproximately 40, while for ‘dirt”\n, it is around 20. We find that the best drift\nlength correlates with the amount of time required to complete the instruction\ntask once. Also as shown in Fig. 10, selecting appropriate values for  T_b and  T_f can\neffectively address the “Goal Illusion” and “Imagination Stagnation” challenges\nmentioned in Sec. 3.3 of the main paper, enhancing the agent’s ability to steadily\nfollow instructions in action generation. Although this method is effective, it\ndoes require the cumbersome task of selecting the right length for each event.\nWe believe that mitigating or eliminating the interference caused by varying\ndrift lengths during training or fine-tuning in the future will be meaningful.\n14\nE. Zhou et al.\n𝑇𝑏= 𝑇𝑓= 10\n𝑇𝑏= 𝑇𝑓= 20\n𝑇𝑏= 𝑇𝑓= 30\n𝑇𝑏= 𝑇𝑓= 40\n𝑇𝑏= 𝑇𝑓= 50\n𝑇𝑏= 𝑇𝑓= 60\n𝑇𝑏= 𝑇𝑓= 10\n𝑇𝑏= 𝑇𝑓= 20\n𝑇𝑏= 𝑇𝑓= 30\n𝑇𝑏= 𝑇𝑓= 40\n𝑇𝑏= 𝑇𝑓= 50\n𝑇𝑏= 𝑇𝑓= 60\nFig. 10: The influence of different goal drift lengths on agent performance.\nFor each event, there is an optimal goal drift length that is correlated with the duration\nof an instruction task completion one time. Employing the appropriate goal drift length\ncan address the dual challenges of “Goal Illusion” and “Imagination Stagnation”, thereby\nenhancing the agent’s ability to steadily follow instructions in action generation.\nE.3\nGeneration Strategies for Visual Prompts\nWhen the agent acts in the simulator, the Imaginator first creates a goal imag-\nination of the next stage to complete the given instruction based on the cur-\nrent observation and instruction. Then, the Prompt Generator creates a visual\nprompt from this goal imagination, integrating the current observation and in-\nstruction. This part investigates strategies for generating visual prompts. We\nconsider two variants:\n1. Unlike current methods, we can synthesize the imagination into a 16-frame\nvideo by simply stacking it 16 times, and we encode this video with the\nMineCLIP [20] video encoder to project it into the MineCLIP [20] space and\nalign it with the PolicyNet using a linear layer. More specifically, we bypass\nthe reconstruction step mentioned in Supp. C.2 and directly transform the\nonly goal imagination into the required visual prompt for the MineCLIP [20]\nvisual space.\n2. In contrast to current methods, we eliminate the Imaginator and retrain a\nPrompt Generator to directly reconstruct visual prompts from current ob-\nservations and instructions. Specifically, we retrain a CVAE [36,64] without\nusing the goal of Imagination as a guiding condition for visual prompt gen-\neration.\nFrom Fig. 11, it is evident that our approach enables the agent to follow\ninstructions more steadily, as our visual prompt provides a more precise demon-\nstration of the desired behaviour customized to the current environment. One\ndrawback of using goal imagination stacked into a 16-frame video as a visual\nprompt is that the depicted behavior resembles a static state. This can confuse\nPolicyNet, making it unclear whether to remain stationary or to achieve the state\nrepresented in the video. A limitation of reconstructing the current observation\nand instruction into a visual prompt is that the CVAE [36,64]’s ability to model\nMineDreamer\n15\nonly GI\nwo GI \nNormal\nonly GI\nwo GI \nNormal\nonly GI\nwo GI \nNormal\nonly GI\nwo GI \nNormal\nFig. 11: The impact of different visual prompt generation strategies on the\nperformance. “only GI” refers to bypassing the CVAE reconstruction phase in Prompt\nGenerator and directly stacking the goal imagination into a static 16-frame video as\nthe visual prompt. “wo GI” indicates that the CVAE reconstructs the visual prompt\nwithout using goal imagination as a condition, thus skipping the imagination phase of\nthe Imaginator.\nfuture spatiotemporal aspects is subpar. Without relying on goal imagination, it\nstruggles to accurately reconstruct the demonstration of the desired behaviour.\nThis occasionally results in misleading the agent, preventing it from steadily\nfollowing instructions during action generation.\nF\nMore Visual Results\nImagination visual results are all hosted on our project webpage.\nF.1\nImagination Visual Results without Goal Drift\nTo evaluate the efficacy of the Goal Drift data collection method, we carry out\nexperiments comparing various data collection approaches. Fig. 12 illustrates the\nimagination generated by the Imaginator trained on data collected without any\ngoal drift. Due to the absence of backward drift, all imaginations generated by\nthe Imaginator correspond to the moment when the event-related instructions\nare completed. Consequently, this leads to the phenomenon of “Goal Illusion”,\nwhere the Imaginator edits the current observation to depict the completed in-\nstruction. For the instruction “Chop a tree”\n, when the agent faces the sky,\nthe Imaginator may unrealistically insert a broken wooden log\ninto the sky.\nFor the instruction “Collect dirt”\n, even though the agent is pointing at a\nstone\n, the Imaginator still imagines dirt and shatters it, resulting in the agent\neventually attempting to break the stone\n. Fig. 13 shows the imaginations\ngenerated by the Imaginator trained on data collected without forward drift.\nBecause there is no forward drift, all imaginations generated by the Imaginator\nrepresent moments before the completion of event-related instructions. This re-\nsults in the phenomenon of “Imagination Stagnation”, where the Imaginator fails\nto conceive repeated task completion. For the instruction “Chop a tree”\n, af-\nter cutting the uppermost wood\nby looking up, the agent will not look down\nfor more trees\n, which impedes continuous task performance. In contrast, an\n16\nE. Zhou et al.\nImaginator trained with data collected including forward drift is able to under-\nstand that the agent should now look down to find other trees\nto continue the\ntask.\n“Chop a tree.”\n“Collect dirt.”\nWo Goal Drift \nImagination\nCurrent Observation\nWo Goal Drift \nImagination\nCurrent Observation\nFig. 12: Imagination Visual Results without Goal Drift. Due to the absence of\ngoal drift, the imaginations generated by the Imaginator are all related to the moment\nof event-related instruction completion, leading to the phenomenon known as “Goal\nIllusion”, where the Imaginator edits the current observation to represent the executed\ninstruction. In the figure depicted, the agent inserts broken wooden blocks\ninto the\nsky and, facing a stone\n, imagines itself breaking dirt\n.\nF.2\nImagination Visual Results on Evaluation Set\nWe compare Imaginator with the existing state-of-the-art instruction-based im-\nage editing model, namely InstructPix2Pix [6]. Given this model has been trained\non specific datasets, its performance would inevitably be suboptimal if directly\napplied to the Minecraft domain. To facilitate a fair comparison, we fine-tune\nInstructPix2Pix [6] using the same training set employed by the Imaginator\nand evaluate the performance of the fine-tuned models in addressing tasks in\nMinecraft. Fig 14 shows qualitative results in the evaluation set, our methodol-\nogy exhibits enhanced abilities in Goal Imagination Generation within intricate\nscenarios.\nF.3\nImagination Visual Results During Agent Solving Tasks\nWe visualize the agent’s imagination during task execution alongside the next\nobservation in Fig. 15 and Fig. 16 to evaluate the Imaginator’s generalization\ncapability in open scenarios. It is observed that the Imaginator is capable of gen-\nerating high-quality visualizations that closely align with the current scene in an\nopen environment, thereby guiding the subsequent PolicyNet to autoregressively\npredict the next action steadily.\nMineDreamer\n17\n“Chop a tree.”\nWo Forward Drift \nImagination\nCurrent Observation\nNormal Imagination\nFig. 13: Imagination Visual Results without Forward Drift. Due to the lack\nof forward drift, the imaginations produced by the Imaginator are all from moments\nprior to the completion of event-related instructions, resulting in a phenomenon called\n“Imagination Stagnation”. This means the Imaginator fails to anticipate the outcomes of\nrepeated tasks. For example, in the figure provided, after the agent cuts the uppermost\nwood\nby looking up, it will not look down for more trees\nto continue the task.\nF.4\nUser Studies\nTo further evaluate MineDreamer’s efficacy, we conduct a user study. Specifically,\nwe randomly select 15 images from the evaluation set, representing a wide range\nof tasks and scenarios within Minecraft. For each image, we generate results us-\ning both InstructPix2Pix [6] and MineDreamer, then randomly shuffle the order\nof these results. As noted in Sec. 4.3 of the main paper, InstructPix2Pix [6] is\nfine-tuned on the same dataset as MineDreamer. This process yield 15 sets of\nimages in a shuffled sequence. Participants are asked to independently identify\nthe two superior images for each set: the first being the one that best matches\nthe given instructions (named Instruct-Alignment), and the second being the\nimage that most closely mirrors real-world appearances, including perspective\nand physical laws (named Image Quality). A total of 25 individuals participate\nin the study. The findings, illustrated in Fig. 17, reveal that over 69.40% of par-\nticipants find MineDreamer’s outputs to be more aligned with the instructions,\nand more than 70.31% favor the results produced by MineDreamerfor their re-\nalism. These outcomes further underscore MineDreamer’s instruction following\nability and generalization ability.\nG\nDemo Videos\nDemo videos are all hosted on our project webpage.\n18\nE. Zhou et al.\n“Kill a sheep.” \n“Break a sand block.” \nMine Redstone Ore.” \n“Dig down.” \nCurrent observation\nInstructPix2Pix\nMineDreamer\nGround Truth\n“Collect stones.” \nFig. 14: Imagination visual results on Goal Drift Evaluation Set.\nMineDreamer\n19\nCurrent observation\nGoal Imagination\nNext observation\n“Swimming under the water.” \n“Break the grass blocks.” \n“Craft planks.” \n“Chop a tree.” \n“Chop a tree.” \n“Build a dirt tower.” \nFig. 15: Imagination visual results during agent solving tasks.\n20\nE. Zhou et al.\nCurrent observation\nGoal Imagination\nNext observation\n“Break grass.” \n“Collect logs.” \n“Mine diamond ore.” \n“Mine horizontally.” \n“Break snow block.” \n“Look at the sky.” \nFig. 16: Imagination visual results during agent solving tasks.\nMineDreamer\n21\nImage Quality\nInstructPix2Pix\nMineDreamer\nInstruct-Alignment\nInstructPix2Pix\nMineDreamer\n70.31%\n29.69%\n30.60%\n69.40%\nFig. 17: The results of user studies, comparing the results generated by Instruct-\nPix2Pix and MineDreamer. Based on the results from both the Instruction Alignment\nand Image Quality perspectives, MineDreamerdemonstrates superior effectiveness.\nG.1\nProgrammatic Evaluation\nWe demonstrate videos of the four tasks from the Programmatic Evaluation on\nthe aforementioned anonymous project webpage. Of course, you can also view\nthe demo videos for the respective tasks by directly accessing the video URLs.\n– “Go explore”\n: https:\/\/youtu.be\/UdG0ckoGRCY\n– “Collect seeds\n”: https:\/\/youtu.be\/TFchu_YBiuI\n– “Chop a tree”\n: https:\/\/youtu.be\/Sx_NKjq5DTA\n– “Collect dirt”\n: https:\/\/youtu.be\/7TOR0SOFaB8\nG.2\nCommand-Switching Evaluation\nWe demonstrate videos of the three tasks from the Command-Switching Evalu-\nation on the anonymous project webpage mentioned above. Of course, you can\nalso view the demo videos for the respective tasks by accessing the video URLs.\n– “Chop a tree\nto Craft planks\n”: https:\/\/youtu.be\/YtY2M_Hi7OE\n– “Gather dirt\nto Build a tower\n”: https:\/\/youtu.be\/Zy2t2RpeNtQ\n– “Obtain Diamond”\n: https:\/\/youtu.be\/hThbWh0q5EE\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control.pdf"}
{"title":"Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations","authors":"Ziyu Jiang, Yinpeng Chen, Mengchen Liu, Dongdong Chen, Xiyang Dai, Lu Yuan, Zicheng Liu, Zhangyang Wang","summary":"Recently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)\ndemonstrate that self-supervision is powerful to learn good representations.\nHowever, naively combining them is far from success. In this paper, we start by\nmaking the empirical observation that a naive joint optimization of CL and MIM\nlosses leads to conflicting gradient directions - more severe as the layers go\ndeeper. This motivates us to shift the paradigm from combining loss at the end,\nto choosing the proper learning method per network layer. Inspired by\nexperimental observations, we find that MIM and CL are suitable to lower and\nhigher layers, respectively. We hence propose to combine them in a surprisingly\nsimple, \"sequential cascade\" fashion: early layers are first trained under one\nMIM loss, on top of which latter layers continue to be trained under another CL\nloss. The proposed Layer Grafted Pre-training learns good visual\nrepresentations that demonstrate superior label efficiency in downstream\napplications, in particular yielding strong few-shot performance besides linear\nevaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields\n65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B\/16, which\nimproves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. The\ncode is available at\nhttps:\/\/github.com\/VITA-Group\/layerGraftedPretraining_ICLR23.git.","url":"http:\/\/arxiv.org\/abs\/2302.14138v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2302.14138v1","published":1677531130000,"comment":"Accepted by ICLR 2023","pdf_text":"Published as a conference paper at ICLR 2023\nLAYER GRAFTED PRE-TRAINING:\nBRIDGING CON-\nTRASTIVE LEARNING AND MASKED IMAGE MODEL-\nING FOR LABEL-EFFICIENT REPRESENTATIONS\nZiyu Jiang†,‡∗, Yinpeng Chen‡, Mengchen Liu‡, Dongdong Chen‡, Xiyang Dai‡,\nLu Yuan‡, Zicheng Liu‡, Zhangyang Wang§\n†Texas A&M University\n‡Microsoft\n§University of Texas at Austin\njiangziyu@tamu.edu, atlaswang@utexas.edu,\n{yiche,mengcliu,dochen,xidai,luyuan,zliu}@microsoft.com\nABSTRACT\nRecently, both Contrastive Learning (CL) and Mask Image Modeling (MIM)\ndemonstrate that self-supervision is powerful to learn good representations. How-\never, naively combining them is far from success. In this paper, we start by making\nthe empirical observation that a naive joint optimization of CL and MIM losses\nleads to conﬂicting gradient directions - more severe as the layers go deeper. This\nmotivates us to shift the paradigm from combining loss at the end, to choosing\nthe proper learning method per network layer. Inspired by experimental observa-\ntions, we ﬁnd that MIM and CL are suitable to lower and higher layers, respec-\ntively. We hence propose to combine them in a surprisingly simple, “sequential\ncascade” fashion: early layers are ﬁrst trained under one MIM loss, on top of\nwhich latter layers continue to be trained under another CL loss. The proposed\nLayer Grafted Pre-training learns good visual representations that demonstrate\nsuperior label efﬁciency in downstream applications, in particular yielding strong\nfew-shot performance besides linear evaluation. For instance, on ImageNet-1k,\nLayer Grafted Pre-training yields 65.5% Top-1 accuracy in terms of 1% few-shot\nlearning with ViT-B\/16, which improves MIM and CL baselines by 14.4% and\n2.1% with no bells and whistles. The code is available at https:\/\/github.\ncom\/VITA-Group\/layerGraftedPretraining_ICLR23.git.\n1\nINTRODUCTION\nSelf-supervision has demonstrated undoubted power in learning strong visual representation, with\ntwo mainstream representative methods: Contrastive learning (CL) (Chen et al., 2020b; He et al.,\n2020; Chen et al., 2020d; 2021; Grill et al., 2020; Caron et al., 2021), and Mask Image Modeling\n(MIM) (Bao et al., 2021; He et al., 2021; Xie et al., 2022; Dong et al., 2021; 2022). The two methods\nfollow different mechanisms, and often manifest different strengths. Generally, CL performs the\ninstance-level task that pulls augmented views from the same image to be similar while pushing\ndifferent images to distribute diversely, making it versatile at learning semantic-aware clustering\nstructures across images. In contrast, MIM draws inspiration from BERT (Devlin et al., 2018) and\nperforms masked token or pixel reconstruction that facilitates the learning of rich local structures\nwithin the same image. In particular, although the latter one, MIM, has recently surpassed CL on the\nﬁne-tuning performance of many datasets, CL often remains to be a top competitor in data-scarce,\nfew-shot downstream applications (Chen et al., 2020c;d; Tian et al., 2020).\nA natural question then follows: are CL and MIM indeed complementary to each other, and is\nthere a way to best combine their strengths?. One immediate, conceptually simple idea is to refer\nto multiple task learning (MTL) and jointly optimize the two losses on top of the same backbone.\nUnfortunately, our preliminary experiment (See Section 2.2) shows that such a vanilla combination\nfails to improve over either baseline, in fact often compromising the single loss’s performance. A\ndeeper dive reveals that the two losses, when being optimized together, will incur increasingly severe\n∗Part of this work was conducted during a summer internship at Microsoft.\n1\narXiv:2302.14138v1  [cs.CV]  27 Feb 2023\nPublished as a conference paper at ICLR 2023\nconﬂicts in the gradient directions, as the layers go deeper (see Figure 1). That causes considerable\nhurdles for the (pre-)training to effectively proceed.\nWe are then inspired to ask: if the two losses conﬂict when both are placed at the end, how about\nplacing them differently, such as appending them to different layers? Based on experimental ob-\nservations, it appears that lower layers tend to learn better from the MIM loss in order to capture\nlocal spatial details; while higher layers tend to beneﬁt more from the CL loss in order to learn\nsemantically-aware grouping and invariance. Inspired by so, we propose a simple MIM→CL Graft-\ning idea to combine the bests of both worlds: (step i) ﬁrst training the lower layers with MIM loss\nand ﬁxing their weights, on top of which (step ii) higher layer weights continue to be trained un-\nder another CL loss. This simple cascaded training idea neatly separates MIM and CL losses to\navoid their conﬂicts against each other if placed together; each loss is also strategically placed to\npre-training its most suitable portion. Practically, we “‘smooth out” the grafting by allowing lower\nlayers to be slowly tuned in step ii. Our ablation experiments also ﬁnd that the order of graft-\ning matters, i.e., reversing MIM\/CL loss locations and performing CL→MIM will considerably\ndamage the performance. The contributions of this paper are summarized as follows:\n• We propose Layer Grafted Pre-training, a principled framework to merge MIM and CL,\nimproving representation learning beyond both, with no bells and whistles.\n• We investigate the different preferences of lower and higher layers towards CL and MIM\nlosses, and show the order of grafting to matter.\n• Despite its embarrassing simplicity, the proposed Layer Grafted Pre-training demonstrates\nmore desirable representation quality, and consequently superior label efﬁciency in down-\nstream applications, yielding strong few-shot performance besides linear evaluation. For\nexample, we achieve [65.5%, 77.8%, 77.7%] in terms of [1% few-shot, 10% few-shot,\nlinear evaluation] performance, improving over MIM and CL baselines by [14.4%, 4.5%,\n9.7%] and [2.1%, 2.4%, 1.0%], respectively.\n2\nMETHOD\n2.1\nPRELIMINARY AND OVERVIEW\nIn Contrastive Learning (CL), the learning target is to pull the positive pairs together in the feature\nspace while pushing negative pairs apart. Formally, the loss can be deﬁned as:\nM(vi, v+\ni , V −, τ) = 1\nN\nN\nX\ni=1\n−log\nexp\n\u0000vi · v+\ni \/τ\n\u0001\nexp\n\u0000vi · v+\ni \/τ\n\u0001\n+ P\nv−\ni ∈V −exp\n\u0000vi · v−\ni \/τ\n\u0001\n(1)\nwhere (vi, v+\ni ) represents features of the positive pairs while (vi, v−\ni ) means features of negative\npairs. Also, V −is the pool of negative features. τ denotes the temperature. N is the number of\nsamples. In practice, the positive pairs are often the different augmented views from the same image\nwhile the negative pool is composed by all the views from different images (Chen et al., 2021).\nOn the other hand, Mask Image Modeling (MIM) learns to reconstruct a corrupted image where\nsome parts of the image or feature map are masked out. The learning target can be formulated as:\nL(xi, M) = 1\nN\nN\nX\ni=1\nD(d(f(Mxi)), xi)\n(2)\nwhere xi and M are input images and randomly generated masks, respectively. f and d represent\nthe encoding and decoding functions, respectively. d(f(Mxi)) is the generated image conditioned\nby masked image Mxi. D measures the difference between d(f(Mxi)) and the original image xi.\nOverview. In the following parts of this section, we ﬁrst introduce our preliminary exploration on the\nMTL of MIM and CL tasks in Section 2.2, which reveals the existence of the conﬂicting gradient\ndirection. Afterward, in Section 2.3, we provide a simple separating idea towards mitigating the\nconﬂicts, which further leads to the proposed Layer Grafted Pre-training in Section 2.4.\n2\nPublished as a conference paper at ICLR 2023\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.10\n0.05\n0.00\n0.05\n0.10\n0.15\nCMIM, CL(x)\nFigure 1: The box plot of CMIM,CL(x) across different blocks for MTL combination of MIM and CL.\nThis is measured on training datasets when the network is trained to 100 epochs (total 300 epochs).\nThe red dash line indicates the linear regression of median numbers.\n2.2\nCONFLICTS PREVENT MULTI-TASK LEARNING FROM WORKING\nOur ﬁrst step towards answering the question of whether CL and MIM can complement each other\nis to examine the most straightforward and conceptually simple idea - Multi-Task Learning (MTL)\ncombination. Speciﬁcally, each iteration of MTL is composed of two steps. Firstly, the images\nare augmented twice for computing the CL loss following Moco V3 Chen et al. (2021). Then, the\nimage with minimal augmentation would be utilized for computing MIM loss following MAE He\net al. (2021). These two losses share the same encoder and would be added together as the ﬁnal loss.\nAs summarized in Table 1, MTL only yields a marginal performance improvement of 0.4% on linear\nevaluation compared to the MIM baseline. However, it is still much lower that the CL baseline (-\n8.3%). Moreover, on both 1% few-shot and ﬁne-tuning performance, MTL is even inferior to both\nMIM and CL baselines. Similar observations were also made by Wang et al. (2022a).\nWe further conjecture that the conﬂicts between these two targets in the MTL combination is the\ncause of the bad performance. To verify it, we design a gradient surgery experiment by computing\nthe cosine similarity between gradients of two tasks following Yu et al. (2020). Formally, the cosine\nsimilarity is calculated as follows:\nCMIM,CL(x) = ∇θLMIM (x)T\n∥∇θLMIM (x)∥\n∇θLCL (x)\n∥∇θLCL (x)∥\n(3)\nwhere LMIM and LCL denote the losses for MIM and CL, respectively. x is a batch of input samples.\nWe measure the distribution of CMIM,CL(x) across different layers of a pre-trained MTL model. As\nshown in Figure 1, there always exist negative values for CMIM,CL(x), where the MIM and CL are\noptimized in opposite directions. Moreover, the gradient direction varies across layers - more severe\nas layers go deeper.\nAlso, the conﬂicts can be reﬂected in two losses’ contradictory targets to enforce. The MIM loss, for\ninstance, requires that the reconstruction have the same brightness, color distribution, and positions\nas the input image, therefore the model needs to be sensitive to all these augmentations. Conversely,\nCL loss is designed to ensure that the model remains invariant regardless of different augmentations.\n2.3\nADDRESSING THE CONFLICTS VIA SEPARATING\nGiven the conﬂicts of the MTL combination, we ask the following question: if the two losses conﬂict\nwhen both are placed at the end, how about placing them differently, such as appending them to dif-\nferent layers? Fortunately, recent empirical evidence suggests that CL and MIM may favor different\npre-training methods. For MIM, Wang et al. (2022c) points out that, when only the pre-trained lower\n3\nPublished as a conference paper at ICLR 2023\nLayer Grafted Pre-training\nLearning Rate Decay\nCL loss\nMIM     CL Grafting\nCL     MIM Grafting\nStep 1\nStep 2\nMIM loss\nMIM loss\nCL loss\nCL loss\nMIM loss\nFigure 2: The pipelines of the MIM→CL, CL→MIM Grafting, and Layer Grafted Pre-training. The\nformer two are employed for preliminary experiments. The latter one is the ﬁnal adopt pipeline,\nwhich is the ‘smooth out’ version of MIM→CL Grafting.\nTable 1: Illustration of preliminary study experiments’ performance on ViT-B\/16. Linear, 1% and\nFine-tuning denote linear evaluation, 1% few-shot and ﬁne-tuning performance, respectively. The\nperformance of MIM and CL are from MAE (He et al., 2021) and MocoV3 (Chen et al., 2021), re-\nspectively. MTL combination denotes the Muti-Task Learning (MTL) Combination of MIM and CL.\nMTL combination is pretrained for 300 epochs. For step 1 of MIM→CL and CL→MIM Grafting,\nwe directly adopt the pre-trained model of MAE and MoCo V3, respectively. Step 2 of MIM→CL\nand CL→MIM Grafting is trained for 100 epochs.\nMethod\nLinear\n1%\nFine-tuning\nMIM (MAE)\n68.0\n51.1\n83.6\nCL (Moco V3)\n76.7\n63.4\n83.2\nMTL combination\n68.4\n47.6\n81.0\nCL→MIM Grafting\n65.5\n32.5\n82.5\nMIM→CL Grafting\n74.5\n56.5\n83.6\nlayers are retained while the higher layers are reset to random initialization, most of the gain is still\npreserved for downstream ﬁne-tuning tasks. Based on this observation, the lower layers appear to\nbe a key element in MIM. On the other hand, Chen et al. (2021) ﬁnds CL to be ineffective and even\nunstable for training the projection layer, the earliest layer of ViT (Dosovitskiy et al., 2020). Fixing\nits weight to be random initialization can even yield signiﬁcantly higher performance. Additionally,\nCL excels at semantic concepts, which happen often at higher layers of the neural network.\nDriven by the above analysis, we propose a simple MIM→CL Grafting framework. As shown in\nFigure 2, MIM→CL Grafting can be separated into two steps: (step i) the lower layers are ﬁrst\ntrained with MIM and then ﬁxed, on the top of which (step ii) higher layers continue to learn with\nCL. Despite the simplicity of this framework, it yields promising preliminary results as shown in\nTable 1, exceeding the MTL combination by [6.1%, 8.9%, 2.6%] for [linear evaluation, 1% few-\nshot, Fine-tuning] performance, respectively. In contrast, when the order of two tasks is reversed,\nthe resulting CL→MIM Grafting, would suffer a dramatic drop in performance, which is even lower\nthan the MTL combination by [2.9%, 15.1%] in terms of [linear evaluation, 1% few-shot] perfor-\nmance, respectively. The huge gap between CL→MIM and MIM→CL Grafting further conﬁrms\nthe preference of MIM and CL towards lower and higher layers, respectively.\nThe example discussed at the end of Section 2.2 can also explain why this preference difference\nhappens: The two different prior knowledge types requested by MIM and CL, while seemingly at\nodds, may work together at different layers of the model. For example, the sensitivity to augmenta-\ntions can be helpful for recognizing the local feature with strong color patterns (Xiao et al., 2020)\nin the lower layers (i.e. the fur of leopards). Meanwhile, for a consistent semantic understanding,\n4\nPublished as a conference paper at ICLR 2023\nthe inﬂuence of the lightness difference should be eliminated when it comes to understanding the\ncontext feature at higher layers.\n2.4\nLAYER GRAFTED PRE-TRAINING\nTo fully unleash the power of Grafting, we ‘smooth out’ the boundary of MIM→CL grafting to\navoid a sudden change in the feature space. Speciﬁcally, rather than ﬁxing the lower layers, we\nassign them a small learning rate. The resultant method, termed as Layer Grafted Pre-training, is\nshown in Figure 2. In Section 3.3, we also explore other LR choices and our results indicate that\nemploying small and large LR for lower and higher layers, respectively, yields the best performance.\nBy effectively capturing the augmentation-sensitive features (i.e. the colors) in the lower layers with\nMIM while learning semantic alignment in the higher layers with CL. The proposed Layer Grafted\nPre-training enables the learning of strong visual representations. It not only provides strong inter-\nclass variance that helps to cluster but also beneﬁts the intra-class variance by keeping the diversity\nof samples in the early features.\n3\nEXPERIMENT\n3.1\nSETTING\nGeneral. We conduct all the experiments on ImageNet-1k (Deng et al., 2009) with Nvidia V100\nGPUs. The code is implemented with Pytorch (Paszke et al., 2019).\nBackbone. We adopt the standard ViT-B and ViT-L architecture (Dosovitskiy et al., 2020) with the\ntoken size of 16×16. The ViT-B is by default employed unless speciﬁed. When training with CL\nloss, we employ the projection and prediction head following Moco V3 (Chen et al., 2021). The\nsettings for pre-training and evaluation protocols can be found at Appendix A.5.\n3.2\nCOMPARISON WITH STATE-OF-THE-ART METHODS\nWe start by verifying the effectiveness of the proposed Layer Grafted Pre-training by comparing it\nwith state-of-the-art methods. As shown in Table 2, in ViT-B\/16, compared to the employed MIM\nand CL baselines, the proposed Layer Grafted Pre-training leads to a consistent improvement. For\ninstance, it improves MAE and Moco V3 by [9.7%, 14.4%, 4.5%] and [1.0%, 2.1%, 2.4%] for\n[linear evaluation, 1% few-shot, 10% few-shot], respectively.\nCompared to close competitors which also attempt to combine MIM and CL, the proposed Layer\nGrafted Pre-training surpasses iBoT by 1.5% for linear evaluation performance. Compared to SIM,\nthe proposed Layer Grafted Pre-training yields an improvement of 1.1% and 0.2% for linear evalu-\nation and 1% few-shot learning performance, respectively.\nTable 2: Comparison with State-of-The-Arts on ViT-B\/16 and ViT-L\/16. Linear, 1% and 10% denote\nthe top-1 accuracy (%) of linear evaluation, 1% and 10% few-shot learning, respectively. †: We\nemploy the result of iBoT without augmentations from Zhou et al. (2021) for fair comparison.\nBackBone\nMethod\nLinear\n1%\n10%\nViT-B\/16\nMAE (He et al., 2021)\n68.0\n51.1\n73.3\nMoco V3 (Chen et al., 2021)\n76.7\n63.4\n75.4\niBoT† (Zhou et al., 2021)\n76.0\n-\n-\nSIM (Tao et al., 2022)\n76.4\n65.3\n-\nC-MAE (Huang et al., 2022)\n73.9\n65.3\n77.3\nMimCo (Zhou et al., 2022)\n70.2\n62.7\n-\nLayer Grafted Pre-training (Ours)\n77.7\n65.5\n77.8\nViT-L\/16\nMAE (He et al., 2021)\n75.8\n55.2\n78.7\nMoco V3 (Chen et al., 2021)\n77.6\n-\n-\nLayer Grafted Pre-training (Ours)\n81.0\n69.3\n80.1\n5\nPublished as a conference paper at ICLR 2023\n(a) Moco V3\n(b) Layer Grafted Pre-training\nFigure 3: t-SNE (Van der Maaten & Hinton, 2008) visualization for feature distribution of Moco V3\nand Layer Grafted Pre-training. Different colors represent different classes. Best viewed in color.\nOur method also demonstrates good scalability toward larger models size. For instance, when scal-\ning from ViT-B\/16 to ViT-L\/16, Layer Grafted Pre-training further improves accuracy by [3.3%,\n3.8%, 2.3%] in terms of [linear evaluation, 1% few-shot, 10% few-shot], respectively. Remarkably,\nthe gap above Moco V3 in linear evaluation performance also increases from 1.0% to 3.4%.\nWe further qualitatively evaluate the representations learned by the proposed Layer Grafted Pre-\ntraining using t-SNE (Van der Maaten & Hinton, 2008). As shown in Figure 3b, the proposed Layer\nGrafted Pre-training shows better inter-class variance. For example, the categories represented by\npink (•) and light blue (•) points are hard to separate given they are very close with each other in\nFigure 3a. In contrast, for representation of the proposed Layer Grafted Pre-training, they form two\nclusters with a clear boundary in Figure 3b. Besides, the proposed Layer Grafted Pre-training also\n(a) Linear, 3rd Stage LR: 1.5e-4\n(b) Linear, 3rd Stage LR: 1.5e-5\n(c) Linear, 3rd Stage LR: 1.5e-6\n(d) Tuning, 3rd Stage LR: 1.5e-4\n(e) Tuning, 3rd Stage LR: 1.5e-5\n(f) Tuning, 3rd Stage LR: 1.5e-6\nFigure 4: Illustration of the LR grid search results for different stages in terms of linear evaluations\nand ﬁne-tuning performance. The grid is [1.5e-6,1.5e-5,1.5e-4] for each stage. [(a), (b), (c)] and\n[(d), (e), (f)] denotes the linear evaluation and ﬁne-tuning performance with third stage LR of [1.5e-\n4, 1.5e-5, 1.5e-6], respectively. [(a), (b)] and [(e), (f)] share the same color bar with (c) and (g),\nrespectively. That of (d), (e) and (f) are also the same. The tested points are highlighted with blue\ndots in each plot. Best view in color.\n6\nPublished as a conference paper at ICLR 2023\nshows better intra-variance: the red (•), green (•) and yellow (•) points of Moco V3 collapse to a\nsmaller region than the proposed Layer Grafted Pre-training.\n3.3\nLR SEARCH LAYER GRAFTED PRE-TRAINING\nWe further verify if small and large LR work the best for the proposed Layer Grafted Pre-training.\nSpeciﬁcally, we study the performance with different LR settings on the three stages on ViT-B\/16\n(Refer to ﬁne-tuning part of Appendix A.5 for the deﬁnition of stages), where each stage is searched\nwith a grid of [1.5e-6,1.5e-5,1.5e-4]. As demonstrated in Figure 4, when the LR increase from\n1.5e-6 to 1.5e-4 for the third stage, both linear evaluation and ﬁne-tuning performance achieve an\nimprovement. Taking linear evaluation as an example, as shown in Figure 4a, 4b and 4c, when\nthe LR of the third stage increase from 1.5e-6 to 1.5e-5 and 1.5e-4, the performance range could\nimprove from 2.5%-63.0% to 64.8%-70.9% and 73.7%-75.1%, respectively. In contrast, a big LR\nof the ﬁrst stage would lead to a drop in performance. For instance, in terms of the linear evalu-\nation performance with third stage LR of 1.5e-4, as shown in Figure 4a, the performance ranges\nare 74.9%-75.1%, 74.8%-75.0% and 73.7%-73.8% for ﬁrst stage LR of 1.5e-6, 1.5e-5 and 1.5e-\n4, respectively. The LR of the second stage is not as sensitive as that of the ﬁrst or third stage.\n0\n2\n4\n6\n8\n10\n12\nThe number of fixing blocks\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\nTop 1 accuracy (%)\n0.3\n9.5\nMAE\nLayer Grafted Pre-training\nFigure 5: Comparison between the pro-\nposed Layer Grafted Pre-training and\nMAE under different numbers of ﬁx-\ning blocks on ViT-B\/16 in terms of\nﬁne-tuning performance. The training\ndataset is the full ImageNet-1k.\nBest\nview in color.\nThe trend of ﬁne-tuning performance is also similar to\nthat of the linear evaluation performance. The preference\nfor larger LR for higher layers indicates that they can ben-\neﬁt by performing CL. Meanwhile, lower layers prefer a\nsmaller LR means that keeping MIM features for these\nlayers can be helpful.\n3.4\nMORE ABLATIONS\nFine-tuning Performance Comparison. We also com-\npare State-of-The-Art methods for ﬁne-tuning perfor-\nmance. As shown in Table 3, the proposed Layer Grafted\nPre-training yields a competitive ﬁne-tuning performance\nof 83.9%, which is 0.3% and 0.7% higher than the em-\nployed MIM (MAE) and CL (Moco V3) baselines, re-\nspectively.\nMoreover, Layer Grafted Pre-training also\nsurpasses SIM by 0.1%.\nPartial Fine-tuning. Follow MAE (He et al., 2021), we\nevaluate the performance of the proposed Layer Grafted\nPre-training with different number of ﬁxing blocks. As\nillustrated in Figure 5, Layer Grafted Pre-training consistently yields higher performance than MAE.\nAnd this gap continue to grow larger when more layers are ﬁxed, indicating the superiority of the\nrepresentations learned by the proposed method.\nVariance-Invariance-Covariance Analysis. A study of the Variance-Invariance-Covariance pat-\ntern for the output of each block is conducted in order to better understand the Layer Grafted Pre-\ntraining. As illustrated in Figure 6, we ﬁnd that the VIC pattern of Layer Grafted Pre-training tends\nTable 3: Top 1 Fine-tuning performance comparison.\nBackBone\nMethod\nFine-tuning\nViT-B\/16\nMAE (He et al., 2021)\n83.6\nMoco V3 (Chen et al., 2021)\n83.2\nSIM (Tao et al., 2022)\n83.8\nConMIM (Yi et al., 2022)\n83.7\nLayer Grafted Pre-training (Ours)\n83.9\nViT-L\/16\nMAE (He et al., 2021)\n85.9\nMoco V3 (Chen et al., 2021)\n84.1\nLayer Grafted Pre-training (Ours)\n85.9\n7\nPublished as a conference paper at ICLR 2023\n0\n2\n4\n6\n8\n10\nBlock Index\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\n0.038\n0.040\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(a) Variance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.0000\n0.0001\n0.0002\n0.0003\n0.0004\n0.0005\n0.0006\n0.0007\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(b) Invariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nMAE\nMAE Fine-tune\nLayer Grafted Pre-training\n(c) Covariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.026\n0.028\n0.030\n0.032\n0.034\n0.036\n0.038\n0.040\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(d) Variance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.0000\n0.0001\n0.0002\n0.0003\n0.0004\n0.0005\n0.0006\n0.0007\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(e) Invariance\n0\n2\n4\n6\n8\n10\nBlock Index\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\nMoco V3\nMoco V3 Fine-tune\nLayer Grafted Pre-training\n(f) Covariance\nFigure 6: The Variance-Invariance-Covariance (VIC) analysis for different methods. VIC are com-\nputed on ViT-B\/16 following Bardes et al. (2021). The input features are ﬁrst averaged over all\ntokens and then normalized to remove the effect from magnitude. [(a), (d)], [(b), (e)] and [(c), (f)]\nstudy variance, covariance and invariance, respectively. Best view in color.\nTable 4: Layer Grafted Pre-training on ViT-B\/16 with VICReg (Bardes et al., 2021). We train ViT-\nB\/16 with VICReg for 100 epochs as the baseline. For Layer Grafted Pre-training - VICReg, the CL\nloss of stage ii is replaced with VICReg loss.\nMethod\nLinear\nFine-tuning\nVICReg (Bardes et al., 2021)\n70.1%\n81.2%\nLayer Grafted Pre-training - VICReg (Ours)\n74.9%\n83.6%\nto be similar to that of ﬁne-tuning. In the MAE case, the VIC curve of Layer Grafted Pre-training\nclosely matches that of MAE Fine-tune, much closer than MAE itself. The similarity between\nthe proposed Layer Grafted Pre-training and ﬁne-tuning in the VIC pattern also explains the high\nfew-shot performance: weights of the pre-training do not need to change substantially to match the\ndownstream task.\nLayer Grafted Pre-training with VICReg. We further examine the generalizability of the pro-\nposed idea on a different pre-training method - VICRegn (Bardes et al., 2021). As shown in Table 4,\nwhen replacing the CL loss with the VICReg loss, the proposed Layer Grafted Pre-training still\nyields strong performance, surpassing the VICReg baseline by [4.8%, 2.4%] for [linear evaluation,\nﬁne-tuning] performance, respectively.\n4\nRELATED WORKS\nContrastive Learning.\nCL performs instance classiﬁcation tasks by contrasting positive pairs\nagainst negative pairs (Chen et al., 2020b; He et al., 2020; Zhuang et al., 2019; Dosovitskiy et al.,\n2014). Other close works also explore learning without negative samples (Grill et al., 2020; Bardes\net al., 2021; Caron et al., 2021; Zbontar et al., 2021; Chen & He, 2021) and the clustering based\napproachs (Caron et al., 2020).\n8\nPublished as a conference paper at ICLR 2023\nOne common merit of these methods is their strong performance on learning good representa-\ntions, which shows strong clustering pattern (Dwibedi et al., 2021) and leads to state-of-the-art\nfew-shot\/semi-supervised performance (Chen et al., 2020c; Tian et al., 2020; Li et al., 2021; Jiang\net al., 2022; You et al., 2022). However, they contain an implicit assumption that the features should\nbe invariant to heavy augmentations, which, however, could further lead to worse performance when\nthe downstream performance violates it (Xiao et al., 2020). The proposed Layer Grafted Pre-training\naddress this via leveraging MIM for processing the features of lower layers.\nMask Image Modeling. Mask Image Modeling (MIM) is inspired by the success of BERT (Devlin\net al., 2018) in Natural Language Processing (NLP). iGPT (Chen et al., 2020a) begins the exploration\nof this idea in Computer Vision (CV). The emergence of ViT (Dosovitskiy et al., 2020) further\nshrinks the gap of backbones between CV and NLP, motivating more researchers to delve into this\ndirection. Beit (Bao et al., 2021; Peng et al., 2022), MaskFeat (Wei et al., 2022) and Peco (Dong\net al., 2021) focus on predicting tokens. MAE (He et al., 2021) and simMIM (Xie et al., 2022)\nfurther show the possibility of directly reconstructing the original pixels. Following works (Dong\net al., 2022; Chen et al., 2022; Wang et al., 2022b) continue to improve the performance or extend to\nother modalities. However, MIM achieves the most success in ﬁne-tuning with enough data points.\nFor downstream tasks with limited data, MIM fails to surpass CL given the lack of linear separability\nfor its representations (Tao et al., 2022). We address this drawback by employing CL for learning\nthe semantic alignment for higher layers.\nBridging Contrastive Learning and Mask Image Modeling. Only recently, researchers begin\nto explore the potential of combining MIM and CL. iBoT (Zhou et al., 2021), one of the pioneers\nin this direction, proposes to switch the modeling of images to the modeling of features. Some\nconcurrent works also follow this self-distillation paradigm (Tao et al., 2022; Assran et al., 2022).\nHowever, just like CL, this paradigm still relays on the involvement of strong augmentations to avoid\ncollapsing, which could lead to over-suppressing of some features (i.e. color) (Xiao et al., 2020). In\ncontrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong\naugmentations. Besides, previous combination works treat the network as a whole while we provide\na new layer-wise perspective.\nComparison to other multiple-step pre-training tasks. One may relate the proposed method\nwith previous multiple-step pre-training tasks like intermediate ﬁne-tuning ( e.g., ﬁnetuning a MIM\nmodel using ImageNet22k and transfer to ImageNet1k (Bao et al., 2021)) or self-supervised ﬁne-\ntuning like Reed et al. (2022). The main differences lie in two aspects: (1) The key innovation of the\nproposed method is to reveal and utilize the layerwise difference between MIM and CL. In contrast,\nintermediate ﬁnetuning and self-supervised ﬁne-tuning are treating the model as a whole; and (2)\nWhile intermediate ﬁnetuning and self-supervised are designed for the same pretraining methods\nacross different domains, the proposed method is devised for different pretraining methods in\nthe same domain.\n5\nCONCLUSION\nIn this work, we propose Layer Grafted Pre-training, a simple yet principled method for under-\nstanding and bridging two popular types of self-supervised learning methods - Mask Image Mod-\neling (MIM) and Contrastive Learning (CL). Our work provides a simple remedy to the conﬂicts\nbetween MIM and CL and further reveals the different preferences of these two methods toward\ndifferent parts of the neural network. It advances the quality of the self-supervised representations\nand achieves strong performance on linear evaluation and few-shot performance. Potential future\nwork includes assessing or extending the proposed method to real-world unlabeled data with more\nchallenges such as long-tail distribution or imbalance (Jiang et al., 2021).\nREFERENCES\nMahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent,\nArmand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efﬁcient\nlearning. arXiv preprint arXiv:2204.07141, 2022.\nHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254, 2021.\n9\nPublished as a conference paper at ICLR 2023\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\nfor self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. Advances in Neural\nInformation Processing Systems, 33:9912–9924, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE\/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International conference on machine learning, pp. 1691–\n1703. PMLR, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597–1607. PMLR, 2020b.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big\nself-supervised models are strong semi-supervised learners. Advances in neural information pro-\ncessing systems, 33:22243–22255, 2020c.\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han,\nPing Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representa-\ntion learning. arXiv preprint arXiv:2202.03026, 2022.\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021.\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297, 2020d.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\ntransformers. In Proceedings of the IEEE\/CVF International Conference on Computer Vision, pp.\n9640–9649, 2021.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training\ntext encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen,\nFang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transform-\ners. arXiv preprint arXiv:2111.12710, 2021.\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen,\nFang Wen, and Nenghai Yu. Bootstrapped masked autoencoders for vision bert pretraining. In\nECCV, 2022.\nAlexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discrimi-\nnative unsupervised feature learning with convolutional neural networks. Advances in neural\ninformation processing systems, 27, 2014.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n10\nPublished as a conference paper at ICLR 2023\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With\na little help from my friends: Nearest-neighbor contrastive learning of visual representations. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision, pp. 9588–9597,\n2021.\nPriya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\ndrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\nin 1 hour. arXiv preprint arXiv:1706.02677, 2017.\nJean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\net al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural\ninformation processing systems, 33:21271–21284, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning.\nIn Proceedings of the IEEE\/CVF conference on\ncomputer vision and pattern recognition, pp. 9729–9738, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick.\nMasked\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\nZhicheng Huang, Xiaojie Jin, Chengze Lu, Qibin Hou, Ming-Ming Cheng, Dongmei Fu, Xiaohui\nShen, and Jiashi Feng. Contrastive masked autoencoders are stronger vision learners. arXiv\npreprint arXiv:2207.13532, 2022.\nZiyu Jiang, Tianlong Chen, Bobak J Mortazavi, and Zhangyang Wang. Self-damaging contrastive\nlearning. In International Conference on Machine Learning, pp. 4927–4939. PMLR, 2021.\nZiyu Jiang, Tianlong Chen, Xuxi Chen, Yu Cheng, Luowei Zhou, Lu Yuan, Ahmed Awadallah, and\nZhangyang Wang. Dna: Improving few-shot transfer learning with low-rank decomposition and\nalignment. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XX, pp. 239–256. Springer, 2022.\nSuichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei Zhang, Qi Chu, Bin Liu, and Nenghai\nYu. Improve unsupervised pretraining for few-label transfer. In Proceedings of the IEEE\/CVF\nInternational Conference on Computer Vision, pp. 10201–10210, 2021.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the\nIEEE\/CVF International Conference on Computer Vision, pp. 10012–10022, 2021.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran\nAssociates, Inc., 2019.\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling\nwith vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366, 2022.\nColorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard Mao,\nBo Li, Shanghang Zhang, Devin Guillory, Sean Metzger, et al. Self-supervised pretraining im-\nproves self-supervised pretraining. In Proceedings of the IEEE\/CVF Winter Conference on Ap-\nplications of Computer Vision, pp. 2584–2594, 2022.\n11\nPublished as a conference paper at ICLR 2023\nChenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image\nmodeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204,\n2022.\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking\nfew-shot image classiﬁcation: a good embedding is all you need? In European Conference on\nComputer Vision, pp. 266–282. Springer, 2020.\nHugo Touvron, Matthieu Cord, and Herv´e J´egou. Deit iii: Revenge of the vit. arXiv preprint\narXiv:2204.07118, 2022.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008.\nLuya Wang, Feng Liang, Yangguang Li, Wanli Ouyang, Honggang Zhang, and Jing Shao. Repre:\nImproving self-supervised vision transformer with reconstructive pre-training.\narXiv preprint\narXiv:2201.06857, 2022a.\nRui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang\nJiang, Luowei Zhou, and Lu Yuan. Bevt: Bert pretraining of video transformers. In IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR 2022), 2022b.\nShaoru Wang, Jin Gao, Zeming Li, Jian Sun, and Weiming Hu. A closer look at self-supervised\nlightweight vision transformers. arXiv preprint arXiv:2205.14443, 2022c.\nChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichten-\nhofer. Masked feature prediction for self-supervised visual pre-training. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 14668–14678, 2022.\nTete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive in\ncontrastive learning. arXiv preprint arXiv:2008.05659, 2020.\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.\nSimmim: A simple framework for masked image modeling. In Proceedings of the IEEE\/CVF\nConference on Computer Vision and Pattern Recognition, pp. 9653–9663, 2022.\nKun Yi, Yixiao Ge, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, and Xiaohu\nQie. Masked image modeling with denoising contrast. arXiv preprint arXiv:2205.09616, 2022.\nChenyu You, Weicheng Dai, Fenglin Liu, Haoran Su, Xiaoran Zhang, Lawrence Staib, and James S\nDuncan. Mine your own anatomy: Revisiting medical image segmentation with extremely limited\nlabels. arXiv preprint arXiv:2209.13476, 2022.\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\nGradient surgery for multi-task learning. Advances in Neural Information Processing Systems,\n33:5824–5836, 2020.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-supervised\nlearning via redundancy reduction. In International Conference on Machine Learning, pp. 12310–\n12320. PMLR, 2021.\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot:\nImage bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.\nQiang Zhou, Chaohui Yu, Hao Luo, Zhibin Wang, and Hao Li. Mimco: Masked image modeling\npre-training with contrastive teacher. In Proceedings of the 30th ACM International Conference\non Multimedia, pp. 4487–4495, 2022.\nChengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning\nof visual embeddings. In Proceedings of the IEEE\/CVF International Conference on Computer\nVision, pp. 6002–6012, 2019.\n12\nPublished as a conference paper at ICLR 2023\nTable 5: Comparison between MIM (MAE) and CL (Moco V3) for frozen features from blocks\n[6,9,12], on top of which we employ various numbers of blocks (#Blocks) for ﬁne-tuning. 0 block\nindicates that only a linear classiﬁcation head is employed (identical to linear evaluation). Top 1\naccuracy on ImageNet-1K is reported and the best performance under each setting is highlighted\nwith bold text.\n#Blocks\nThe block index of the feature\n6\n9\n12\nMIM\nCL\nMIM\nCL\nMIM\nCL\n0\n38.9%\n43.6%\n59.3%\n65.5%\n68.0%\n76.7%\n1\n70.1%\n72.7%\n77.8%\n78.1%\n78.9%\n79.1%\n2\n76.3%\n76.8%\n80.2%\n79.2%\n80.6%\n79.7%\n4\n78.5%\n78.1%\n81.2%\n79.4%\n81.4%\n79.2%\nA\nAPPENDIX\nThis appendix contains the following details that we could not include in the main paper due to\nspace restrictions.\nA.1\nMORE ANALYSIS FOR THE LAYER-WISE DIFFERENCE BETWEEN MIM AND CL\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nThe blocks index\n40\n50\n60\n70\n80\n90\n100\n110\nMIM\nCL\nLayer Grafted Pre-training\nFigure 7: Demonstration of average attention dis-\ntance (Dosovitskiy et al., 2020) for MIM (MAE),\nCL (Moco V3) and Graft in terms of the across all\nattention heads.\nWe provide more analysis to further understand\nthe layerwise difference between MIM and CL.\nWe start by analyzing the average attention dis-\ntance across different layers. As shown in Fig-\nure 7, on the one hand, for the deep layers (i.e.\nfrom 8th to 12th blocks), the average attention\ndistance of CL would keep increasing, where\nthe aggregation of local features is likely to\nhappen. In contrast, the average attention dis-\ntance of MIM keeps the same level for the deep\nlayers. On the other hand, the average attn dis-\ntance of shallow layers (i.e. 1st and 2nd blocks)\nof CL is much larger than that of MIM, which\nmay distract the model from extracting local\nfeatures. The proposed method combines the\nlower and higher layers’ patterns of MIM and\nCL, respectively, forming a gradually increas-\ning attention distance pattern. Remarkably, this\npattern echos the philosophy of gradually in-\ncreasing receptive ﬁeld for designing network architecture He et al. (2016); Liu et al. (2021).\nSecondly, we study the different properties of features across different layers. Speciﬁcally, in Im-\nageNet 1K, we turn several random initialized blocks on top of features from different layers. As\ndemonstrated in Table 5, when only ﬁne-tuning the classiﬁcation head (#Block = 0), the perfor-\nmance of MIM is much lower than CL, indicating that the feature of CL is more close to semantic\nrepresentation. By contrast, when increasing the turnable blocks, the performance of MIM would\nsigniﬁcantly increase and even surpass CL, demonstrating the features of MIM better encode the\nlocal features. The potential of these local features can be stimulated by adding enough modules to\naggregate them. The proposed Layer Grafted Pre-training employs MIM for producing high-quality\nearly features while utilizing the CL in higher layers for aggregation.\nA.2\nDISCUSSION AND COMPARISON WITH CONCURRENT WORKS\nIn this section, we discuss the difference between the proposed method with four concurrent\nworks (Tao et al., 2022; Huang et al., 2022; Zhou et al., 2022; Yi et al., 2022) and provide more\n13\nPublished as a conference paper at ICLR 2023\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.10\n0.05\n0.00\n0.05\n0.10\nCMIM, CL(x)\n(a) 200 epochs\nBlock 1\nBlock 2\nBlock 3\nBlock 4\nBlock 5\nBlock 6\nBlock 7\nBlock 8\nBlock 9\nBlock 10\nBlock 11\nBlock 12\nBlock Index\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.075\nCMIM, CL(x)\n(b) 300 epochs\nFigure 8: The box plot of CMIM,CL(x) across different blocks for MTL combination of MIM and\nCL. This is measured on training datasets when the network is trained to (a) 200 epochs and (b) 300\nepochs (total 300 epochs). The red dash line indicates the linear regression of median numbers.\ncomparisons. To combine the strength of MIM and CL, SIM (Tao et al., 2022) proposes to pre-\ndict the dense representations of an augmented view for enforcing semantic alignment and spatial\nsensitivity simultaneously. CMAE (Huang et al., 2022) proposes two new components: pixel shift\nand feature decoder. MimCo (Zhou et al., 2022) utilizes the CL pre-trained model as a teacher\nand performs patch-level and image-level reconstruction tasks. ConMIM (Yi et al., 2022) utilizes\ncontrastive constraints to produce a dynamic masked prediction target. The difference between the\nproposed Layer Grafted Pre-training and these works are as follows:\n• While all the concurrent works are treating the network as a whole, we reveal the different\npreferences of MIM and CL towards their internal different layers, which motivates us a\ndesign a novel layerwise method.\n• Our method employs the original design of MIM and CL, which not only is simple but also\nenables an apple-to-apple comparison. In contrast, It’s non-straightforward to tell whether\nthe improvements of the concurrent works are from the newly introduced module or the\noriginal MIM\/CL design.\n• The proposed Layer Grafted Pre-training provides an in-depth analysis of the reason why\nMIM and CL cannot be directly combined together through gradient analysis.\nAlso, here we further analyze why the proposed method fails to surpass the concurrent work C-\nMAE (Huang et al., 2022) in terms of ﬁne-tuning performance. One possible reason lies in whether\nthe masked view is employed for contrastive learning. C-MAE is contrasting a masked and a full\nimage while the proposed method is contrasting two full images following Moco V3. The difference\nin design further leads to different strengths: On the one hand, empirical results highlight that the\n14\nPublished as a conference paper at ICLR 2023\nTable 6: ADE20K semantic segmentation comparison using UperNet with different pre-training\nmethods.\nMethod\nmIoU\nMAE (He et al., 2021)\n48.1\nMoco V3 (Chen et al., 2021)\n47.3\nLayer Grafted Pre-training (Ours)\n48.7\nmasked view can beneﬁt the downstream ﬁne-tuning task (Touvron et al., 2022; He et al., 2021),\nwhich may be because it helps to learn the correlation between sparse patches that cannot be built\nunder full view. On the other hand, contrasting full images leads to a smaller gap with downstream\ntasks and thus beneﬁting the downstream few-shot and linear evaluation tasks.\nA.3\nMORE GRADIENT ANALYSIS RESULTS\nWe measure the distribution of CMIM,CL(x) across different training epochs and conﬁrmed that the\nstatistical results persist the same across the entire training process, rather than just a few speciﬁc\nepochs. As two examples, Figures 8a and 8b show that a considerable part of values is negative in\nthe 200 and 300 epochs.\nA.4\nTRANSFER LEARNING RESULTS\nWe further evaluate the proposed method for the standard transfer semantic segmentation task. As\nshown in Table 6, on ADE20K with UperNet, the proposed method achieves higher performance\nthan both MAE and Moco V3.\nA.5\nMORE SETTINGS\nPre-training. We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our\nMIM and CL frameworks, respectively. For the ﬁrst step of Layer Grafted Pre-training, since it\nidentically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He\net al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained\nmodel and train with Moco V3 (Chen et al., 2021) for 300 epochs. For LR, We ﬁrst split the network\ninto three stages. Each of them contains the same number of blocks. (i.e. 4 and 8 blocks for ViT-B\nand ViT-L, respectively.) Then, the base LR of the ﬁrst and second stages (corresponding to lower\nlayers) is assigned as 1.5e-5 while the third stage is set as 1.5e-4 by default. In the second stage of\nViT-L, we further ensure the early layers of the resultant model are close to the MIM pre-training\nby minimizing the l2 distance between the ﬁrst 12 layers between them (Refer Section A.6 for more\nablations). Other settings are identically followed from Moco V3 (Chen et al., 2021).\nFine-tuning. For ﬁne-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs\nfollowing MAE (He et al., 2021). We employ a base LR of 5e-4 with linear scaling rule (Goyal\net al., 2017). The layer-wise LR decay ratio is set as 0.6 (Clark et al., 2020). For other settings such\nas data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).\nFew-shot Learning. We conduct few-shot learning with 1% or 10% available labels. The sub-\nsampling splits are adopted from Chen et al. (2020c). For 1% few-shot evaluation, following Caron\net al. (2021), we ﬁrst generate frozen features on training images without data augmentation, on top\nof which a logistic regression classiﬁer is trained for prediction. For 10% semi-supervised learning,\nwe train from the ﬁrst layer of the projection head following Chen et al. (2020c). We train for 400\nepochs with an initial base LR of 3e-5. Other settings are identical to the ﬁne-tuning.\nLinear Evaluation. For linear evaluation, we train a linear classiﬁer on top of frozen pre-train\nfeatures to measure the quality of the visual representations following common practices (Chen\net al., 2020b). Following Moco V3 (Chen et al., 2021), the classiﬁer is trained for 90 epochs with\nSGD optimizer and weight decay of 0. The LR is swept for each case.\n15\nPublished as a conference paper at ICLR 2023\nTable 7: Ablation study for l2 regularization on ViT-L. Top 1 accuracy on ImageNet-1K is reported\nand the best performance under each setting is highlighted with bold text.\nl2 regularization\nLinear\n1%\n10%\nFinetune\n\u0017\n80.5\n68.9\n79.8\n85.7\n✓\n81.0\n69.3\n80.1\n85.9\nA.6\nABLATION STUDY FOR l2 REGULARIZATION\nIn this section, we ablation study the effectiveness of l2 regularization mentioned at section A.5. As\nshown in Table 7, employing the l2 regularization can help to preserve the Mask Image Modeling\nfeatures of the lower layers and yield consistent improvement across multiple benchmarks.\n16\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations.pdf"}
{"title":"Vision-Language Models Provide Promptable Representations for Reinforcement Learning","authors":"William Chen, Oier Mees, Aviral Kumar, Sergey Levine","summary":"Humans can quickly learn new behaviors by leveraging background world\nknowledge. In contrast, agents trained with reinforcement learning (RL)\ntypically learn behaviors from scratch. We thus propose a novel approach that\nuses the vast amounts of general and indexable world knowledge encoded in\nvision-language models (VLMs) pre-trained on Internet-scale data for embodied\nRL. We initialize policies with VLMs by using them as promptable\nrepresentations: embeddings that encode semantic features of visual\nobservations based on the VLM's internal knowledge and reasoning capabilities,\nas elicited through prompts that provide task context and auxiliary\ninformation. We evaluate our approach on visually-complex, long horizon RL\ntasks in Minecraft and robot navigation in Habitat. We find that our policies\ntrained on embeddings from off-the-shelf, general-purpose VLMs outperform\nequivalent policies trained on generic, non-promptable image embeddings. We\nalso find our approach outperforms instruction-following methods and performs\ncomparably to domain-specific embeddings. Finally, we show that our approach\ncan use chain-of-thought prompting to produce representations of common-sense\nsemantic reasoning, improving policy performance in novel scenes by 1.5 times.","url":"http:\/\/arxiv.org\/abs\/2402.02651v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.02651v3","published":1707094136000,"comment":null,"pdf_text":"Vision-Language Models Provide Promptable\nRepresentations for Reinforcement Learning\nWilliam Chen\nU.C. Berkeley\nOier Mees\nU.C. Berkeley\nAviral Kumar\nGoogle DeepMind\nSergey Levine\nU.C. Berkeley\nAbstract\nHumans can quickly learn new behaviors by leveraging background world knowl-\nedge. In contrast, agents trained with reinforcement learning (RL) typically learn\nbehaviors from scratch. We thus propose a novel approach that uses the vast\namounts of general and indexable world knowledge encoded in vision-language\nmodels (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize\npolicies with VLMs by using them as promptable representations: embeddings\nthat encode semantic features of visual observations based on the VLM’s internal\nknowledge and reasoning capabilities, as elicited through prompts that provide task\ncontext and auxiliary information. We evaluate our approach on visually-complex,\nlong horizon RL tasks in Minecraft and robot navigation in Habitat. We find that\nour policies trained on embeddings from off-the-shelf, general-purpose VLMs out-\nperform equivalent policies trained on generic, non-promptable image embeddings.\nWe also find our approach outperforms instruction-following methods and performs\ncomparably to domain-specific embeddings. Finally, we show that our approach\ncan use chain-of-thought prompting to produce representations of common-sense\nsemantic reasoning, improving policy performance in novel scenes by 1.5 times.\n1\nIntroduction\nEmbodied decision-making often requires representations informed by world knowledge for per-\nceptual grounding, planning, and control. Humans rapidly learn to perform sensorimotor tasks by\ndrawing on prior knowledge, which might be high-level and abstract (“If I’m cooking something\nthat needs milk, the milk is probably in the refrigerator”) or grounded and low-level (e.g., what\nrefrigerators and milk look like). These capabilities would be highly beneficial for reinforcement\nlearning (RL) too: we aim for our agents to interpret tasks in terms of concepts that can be reasoned\nabout with relevant prior knowledge and grounded with previously-learned representations, thus\nenabling more efficient learning. However, doing so requires a condensed source of vast amounts of\ngeneral-purpose world knowledge, captured in a form that allows us to specifically index into and\naccess task-relevant information. Therefore, we need representations that are contextual, such that\nagents can use a concise task context to draw out relevant background knowledge, abstractions, and\ngrounded features that aid it in acquiring a new behavior.\nAn approach to facilitate this involves integrating RL agents with the prior knowledge and reasoning\nabilities of pre-trained foundation models. Transformer-based language models (LMs) and vision-\nlanguage models (VLMs) are trained on Internet-scale data to enable generalization in downstream\ntasks requiring facts or common sense. Moreover, in-context learning [8], chain-of-thought reason-\ning (CoT) [71], and instruction fine-tuning [50] have provided better ways to index into (V)LMs’\nknowledge and steer their capabilities based on user needs. These successes have seen some transfer\nto embodied control, with (V)LMs being used to reason about goals to produce executable plans\nCorrespondence to: William Chen <verityw@berkeley.edu>. Website: pr2l.github.io.\narXiv:2402.02651v3  [cs.LG]  23 May 2024\nPrompt\n“Spiders in Minecraft \nare black. Is there a \nspider in this image?”\nVLM\nTask \nEnv\nLearned\nPolicy\n“Yes, there is a spider.”\nActions\nObservations\nPrompt\n“Would a toilet be \nfound here? Why or \nwhy not?”\nVLM\nTask \nEnv\nLearned\nPolicy\n“No, as it’s a bedroom. Toilets \nare usually found in bathrooms.”\nActions\nObservations\nMinecraft Task: Combat Spider\nHabitat Task: Find Toilet\nFigure 1: Example instantiations of PR2L for tasks in Minecraft and Habitat. We query a VLM with a\ntask-relevant prompt about observations to produce promptable representations, which we train a policy on\nvia RL. Rather than directly asking for actions or specifying the task, the prompt enables indexing into the\nVLM’s prior world knowledge to access task-relevant information. This prompt also allows us to inject auxiliary\ninformation and elicit chain-of-thought reasoning.\n[2] or as encoders of useful information (like instructions [40] or feedback [61]) that the control\npolicy utilizes. Both these paradigms have major limitations: actions generated by LMs are often not\nappropriately grounded, unless the tasks and scenes are amenable to being expressed or captioned in\nlanguage. Even then, (V)LMs are often only suited to producing subtask plans, not low-level control\nsignals. On the other hand, using (V)LMs to simply encode inputs under-utilizes their knowledge and\nreasoning abilities, instead focusing on producing embeddings that reflect the compositionality of\nlanguage (e.g., so an instruction-following policy may generalize). This motivates the development\nof an algorithm for learning to produce low-level actions that are grounded and leverage (V)LMs’\nknowledge and reasoning.\nTo this end, we introduce Promptable Representations for Reinforcement Learning (PR2L): a flexible\nframework for steering VLMs into producing semantic features, which (i) integrate observations\nwith prior task knowledge and (ii) are grounded into actions via RL (see Figure 1). Specifically,\nwe ask a VLM questions about observations that are related to the given control task, priming it to\nattend to task-relevant features in the image based on both its internal world knowledge, reasoning\ncapabilities, and any supplemental information injected via prompting. The VLM then encodes this\ninformation in decoded text, which is discarded, and associated embeddings, which serve as inputs\nto a learned policy. In contrast to the standard approach of using pre-trained image encoders to\nconvert visual inputs into generic features for downstream learning, our method yields task-specific\nfeatures capturing information particularly conducive to learning a considered task. Thus, the VLM\ndoes not just produce an un-grounded encoding of instructions, but embeddings containing semantic\ninformation relevant to the task, that is both grounded and informed by the VLM’s prior knowledge.\nTo the best our knowledge, we introduce the first approach for initializing RL policies with generative\nVLM representations. We demonstrate our approach on tasks in Minecraft [19] and Habitat [58], as\nthey present semantically-rich problems representative of many practical, realistic, and challenging\napplications of RL. We find that PR2L outperforms equivalent policies trained on vision-only\nembeddings or with instruction-conditioning, popular ways of using pre-trained image models and\nVLMs respectively for control. We also show that promptable representations extracted from general-\npurpose VLMs are competitive with domain-specific representations. Our results highlight how\nvisually-complex control tasks can benefit from accessing the knowledge captured within VLMs via\nprompting in both online and offline RL settings.\n2\nRelated Works\nVision-language models. In this work, we utilize generative VLMs (like [33, 34, 14, 29]): models\nthat generate language in response to an image and a text prompt passed as input. This is in contrast to\nother designs of combining vision and language that either generate images or segmentation [57, 30]\nand contrastive representations [52]. Formally, the VLM enables sampling from p(x1:K|I, c), where\nx1:K represents the K tokens of the output, I is the input image(s), c is the prompt, and p is the\ndistribution over natural language responses produced by the VLM on those inputs. Typically, the\nVLM is pre-trained on tasks that require building association between vision and language such as\ncaptioning. All these tasks require learning to attend to certain semantic features of input images\ndepending on the given prompt. For auto-regressive generative VLMs, this distribution is factorized as\nQ\nt p(xt|I, c, x1:t−1). Typical architectures parameterize these distributions using weights that define\na representation ϕt(I, c, x1:t−1), which depends on the image I, the prompt c, and the previously\nemitted tokens, and a decoder p(xt|ϕt(I, c, x1:t−1)), which defines a distribution over the next token.\n2\nEmbodied (V)LM reasoning. Many recent works have leveraged (V)LMs as priors over effective\nplans for a given goal. These works use the model’s language modeling and auto-regressive generation\ncapabilities to extract such priors as textual subtask sequences [2, 24, 60] or code [36, 64, 77, 68],\nthereby using the (V)LM to decompose long-horizon tasks into executable parts. These systems\noften need grounding mechanisms to ensure plan feasibility (e.g., affordance estimators [2], scene\ncaptioners [77], or trajectory labelers [51]). They also often assume access to low-level policies\nthat can execute these subtasks, such as robot pick-and-place skills [2, 36], which is often a strong\nassumption. These methods generally do not address how such policies can be acquired, nor how\nthese low-level skills can themselves benefit from the prior knowledge in (V)LMs. Even works in\nthis area that use RL still use (V)LMs as state-dependent priors over reasonable high-level goals to\nlearn [17]. This is a key difference from our work: instead of considering priors on plans\/goals, we\nrely on VLM’s implicit knowledge of the world to extract representations which encode task-relevant\ninformation. We train a policy to convert these features into low-level actions via standard RL,\nmeaning the VLM does not need to know how to take actions for a task.\nEmbodied (V)LM pre-training. Other works use (V)LMs to embed useful information like instruc-\ntions [40, 45, 42, 44, 49], feedback [61, 9], reward specifications [19], and data for world modeling\n[39, 47]. These works use (V)LMs as encoders of the compositional semantic structure of input text\nand images, which aids in generalization: an instruction-conditioned model may never have learned\nto grasp apples (but can grasp other objects), but by interacting with them in other ways and receiving\nassociated language descriptions, the model might still be able to grasp them zero-shot. In contrast,\nour method produces embeddings that are informed by world knowledge and reasoning, both from\nprompting and pre-training. Rather than just specifying that the task is to acquire an apple, we ask a\nVLM to parse observations into task-relevant features, like whether there is an apple in the image or\nif the observed location likely contains apples – information that is useful even in single-task RL.\nThus, we use VLMs to help RL solve new tasks, not just to follow instructions.\nThese two categories are not mutually exclusive: Brohan et al. [6] use VLMs to understand instruc-\ntions, but also reasoning (e.g., figuring out the “correct bowl” for a strawberry is one that contains\nfruits); Palo et al. [51] use a LM to reason about goal subtasks and a VLM to know when a trajectory\nmatches a subtask, automating the demonstration collection\/labeling of Ahn et al. [2], while Adeniji\net al. [1] use a similar approach to pretrain a language-conditioned RL policy that is transferable to\nlearning other tasks; and Shridhar et al. [63] use CLIP to merge vision and text instructions directly\ninto a form that a Transporter [76] policy can operationalize. Nevertheless, these works primarily\nfocus on instruction-following for robot manipulation. Our approach instead prompts a VLM to\nsupplement RL with representations of world knowledge, not instructions. In addition, except for\nAdeniji et al. [1], these works focus on behavior cloning (BC), assuming access to demonstrations for\npolicy learning, whereas our framework can be used for both online RL and offline RL\/BC.\n3\nPR2L: Promptable Representations for Reinforcement Learning\nWe adopt the standard framework of partially-observed Markov decision process in deep RL, wherein\nthe objective is to find a policy mapping states to actions that maximizes the expected returns. Our\ngoal is to supplement RL with task-relevant information extracted from VLMs containing general-\npurpose knowledge. One way to index into this information is by prompting the model to get it\nto produce semantic information relevant to a given control task. Therefore, our approach, PR2L,\nqueries a VLM with a task-relevant prompt for each visual observation received by the agent, and\nreceives both the decoded text and, critically, the intermediate representations, which we refer to\nas promptable representations. Even though the decoded text might often not be correct or directly\nusable for choosing the action, our key insight is that these VLM embeddings can still provide\nuseful semantic features for training control policies via RL. This recipe enables us to incorporate\nsemantic information without the need of re-training or fine-tuning a VLM to directly output actions,\nas proposed by Brohan et al. [6]. Note that our method is not an instruction-following method, and\nit does not require a task instruction to perform well. Instead, our approach still learns control via\nRL, while benefiting from the incorporation of background context. In this section, we will describe\nvarious components of our approach, accompanied by practical design choices and considerations.\n3.1\nPromptable Representations\nIn principle, one can directly query a VLM to produce actions for a task given a visual observation.\nWhile this may work when high-level goals or subtasks are sufficient, VLMs are empirically poor at\nyielding the low-level actions used commonly in RL [23]. As VLMs are trained to follow instructions\n3\nPolicy Network\nFrozen LLM Transformer Layers\nImage Encoder\nTokenizer\nDetokenizer\nPrompt\n“Spiders in Minecraft are black. Is \nthere a spider in this image?”\nDecoded Text\n“Yes, there is a spider.”\nLearned Transformer\nEncoder\nCLS\nDecoder\nNon-visual Observations\nAction\nSummary \nEmbed\nVision-Language Model\nPolicy\n…\nFigure 2: Schematic of how we extract task-relevant features from the VLM and use them in a policy\ntrained with RL. These representations can incorporate task context from the prompt, while generic image\nembeddings cannot. As generative VLM’s embeddings can be variable length, the policy has a Transformer layer\nthat takes in these embeddings and a “CLS” token, thereby condensing all inputs into a single summary vector.\nand answer questions about images, it is more appropriate to use these models to extract and reason\nabout semantic features about observations that are conducive to being linked to actions. We thus\nelicit features that are useful for the downstream task by querying these VLMs with task-relevant\nprompts that provide contextual task information, thereby causing the VLM to attend to and interpret\nappropriate parts of observed images. Extracting these features naïvely by only using the VLM’s\ndecoded text has its own challenges: such models often suffer from hallucinations [26] and an inability\nto report what they “know” in language, even when their embeddings contain such information\n[27, 21]. However, even when the text is bad, the underlying representations still contain valuable\ngranular world information that is potentially lost in the projection to language [32, 72, 22, 35]. Thus,\nwe disregard the generated text and instead provide our policy the embeddings produced by the VLM\nin response to prompts asking about relevant semantic features in observations instead.\nWhich parts of the network can be used as promptable representations? The VLMs we consider\nare all based on the Transformer architecture [67], which treats the prompt, input image(s), and\ndecoded text as token sequences. This architecture provides a source of learned representations by\ncomputing embeddings for each token at every layer based on the previous layer’s token embeddings.\nIn terms of the generative VLM formalism introduced prior, a Transformer-based VLM’s repre-\nsentations ϕt(I, c, x1:t−1) consist of N embeddings per token (the outputs of the N self-attention\nlayers) in the input image I, prompt c, and decoded text x1:t−1. The decoder p(xt|ϕt) extracts the\nfinal layer’s embedding of the most recent token xt−1, projecting it to a distribution over the token\nvocabulary and allowing for it to be sampled. When given a visual observation and task prompt,\nthe tokens representing the prompt, image, and answer consequently encode task-relevant semantic\ninformation. Thus, for each observation, we use the VLM to sample a response to the task prompt\nx1:K ∼p(x1:K|I, c). We then use some or all of these token embeddings ϕK(I, c, x1:t−1) as our\npromptable representations and feed them, along with any non-visual observation information, as a\nstate representation into our neural policy trained with RL.\nIn summary, our approach involves creating a task-relevant prompt that provides context and auxiliary\ninformation. This prompt, alongside the current visual observation from the environment, is fed\nto into the VLM to generate tokens. While these tokens are used for decoding, they are ultimately\ndiscarded. Instead, we utilize the representations produced by the VLM (associated with the image,\nprompt, and decoded text) as input for our policy, which is trained via an off-the-shelf online RL\nalgorithm to produce appropriate actions. A schematic of our approach is depicted in Figure 2 and a\ncode snippet example is presented in Appendix I.\n3.2\nDesign Choices for PR2L\nTo instantiate this idea, we need to make some concrete design choices in practice. First, the\nrepresentations of the VLM’s decoded text depend on the chosen decoding scheme: greedy decoding\nis fast and deterministic, but may yield low-probability decoded tokens; beam search improves on this\nby considering multiple “branches” of decoded text, at the cost of requiring more compute time (for\npotentially small improvements); lastly, sampling-based decoding can quickly yield estimates of the\nmaximum likelihood answer, but at the cost of introducing stochasticity, which may increase variance.\nGiven the inherent high-variance of our tasks (due to sparse rewards and partial observability) and\nthe expense of VLM decoding, we opt for greedy decoding or fixed-seed sampling.\nSecond, one must choose which VLM layers’ embeddings to utilize in the policy. While theoretically,\nall layers of the VLM could be used, pre-trained Transformer models tend to encode valuable high-\n4\nlevel semantic information in their later layers [66, 25]. Thus, we opt to only feed the final few\nlayers’ representations into our policy. As these representation sequences are of variable length, we\nincorporate an encoder-decoder Transformer layer in the policy. At each time step in a trajectory,\nthis layer receives variable-length VLM representations, which are attended to and converted into a\nfixed-length summarization by the embeddings of a learned “CLS” token [15] in the decoder (green\nin Figure 2). We also note that this policy can receive the observed image directly (e.g., after being\nembedded by the image encoder), so as to not lose any visual information from being processed\nby the VLM. However, we do not do this in our experiments in order to more clearly isolate and\ndemonstrate the usefulness of the VLM’s representations in particular.\nFinally, while it is possible to fine-tune the VLM for RL end-to-end with the policy [6],this incurs\nsubstantial compute, memory, and time overhead, particularly with larger VLMs. Nonetheless, we\nfind that our approach performs better than not using the language and prompting components of the\nVLM. This holds true even when the VLM is frozen, and only the policy is trained via RL, or when\nthe decoded text occasionally fails to answer the task-specific prompt correctly.\n3.3\nTask-Relevant Prompt Design\nHow do we design good prompts to elicit useful representations from VLMs? As we aim to\nextract good state representations from the VLM for a downstream policy, we do not use instructions\nor task descriptions, but task-relevant prompts: questions that make the VLM attend to and encode\nsemantic features in the image that are useful for the RL policy learning to solve the task [5]. For\ninstance, if the task is to find a toilet within a house, appropriate prompts include “What room is this?”\nand “Would a toilet be found here?” Intuitively, the answers to these questions help determine good\nactions (e.g., look around the room or explore elsewhere), making the corresponding representations\ngood for representing the state for a policy. Answering the questions will require the VLM to attend to\ntask-relevant features in the scene, relying on the model’s internal conception of what things look like\nand common-sense semantic relations. One can also prompt the VLM to use chain of thought [71]\nto explain its generated text, often requiring it to reason about task-relevant features in the image,\nresulting in further enrichment of the state representations. Finally, prompts can provide helpful\nauxiliary information: e.g., one can describe what certain entities of interest look like, aiding the\nVLM in detecting them even if they were not commonly found in the model’s pre-training data.\nNote that prompts based on instructions or task descriptions do not enjoy the above properties: while\nthe goal of those prior methods is to be able to directly query the VLM for the optimal action, the\ngoal of task-relevant prompts is to produce a useful state representation, such that running RL with\nthem can accelerate learning an optimal policy. While the former is not possible without task-specific\ntraining data for the VLM in the control task, the latter proves beneficial with off-the-shelf VLMs.\nEvaluating and designing prompts for RL. Since the specific representations elicited from the VLM\nare determined by the prompt, we want to design prompts that produce promptable representations\nthat maximize performance on the downstream task. The brute-force approach would involve running\nRL with each candidate prompt to measure its efficacy, but this would be computationally very\nexpensive. In lieu of this, we evaluate candidate prompts on a small dataset of observations labeled\nwith semantic features of interest for the considered task. Example features include whether task-\nrelevant entities are in the image, the relative position of said entities, or even actions (if expert\ndemonstrations are available). We test prompts by querying the VLM and checking how well the\nresulting decoded text for each image matches ground truth labels. As this is only practical for\nsmall, discrete spaces that are easily expressed in words, we see how well a small model can fit the\nVLM’s embeddings to the labels (akin to probing in self-supervised learning [62, 4]). While this does\nnot directly optimize for task performance, it does act as a proxy that ensures a prompt’s resulting\nrepresentations encode certain semantic features which are helpful for the task.\n4\nExperimental Setups\nOur experiments analyze whether promptable representations from VLMs provide benefits to down-\nstream control, thus providing an effective vehicle for transferring Internet-scale knowledge to RL.\nWe aim to show that PR2L is a good source of state representations, even with our current VLMs\nthat are bad at reasoning about actions – as such models become more performant, we expect such\nrepresentations to be even better. We thus design experiments to answer the following: (1) Can\npromptable representations obtained via task-specific prompts enable more performant and sample-\nefficient learning than those of non-promptable image encoders pre-trained for vision or control? (2)\nHow does PR2L compare to approaches that directly “ask” the VLM to generate good actions for a\n5\ntask specified in the prompt? (3) How does PR2L fare against other popular learning approaches or\npurely visual features in our domains of interest?\n4.1\nDomain 1: Minecraft\nWe first conduct experiments in Minecraft, which provides control tasks that require associating\nvisual observations with rich semantic information to succeed. Moreover, since these observations\nare distinct from the images in the the pre-training dataset of the VLM, succeeding on these tasks\nrelies crucially on the efficacy of the task-specific prompt in meaningfully affecting the learned\nrepresentation, enabling us to stress-test our method. E.g., while spiders in Minecraft somewhat\nresemble real-life spiders, they exhibit stylistic exaggerations such as bright red eyes and a large black\nbody. If the task-specific prompt is indeed effective in informing the VLM of these facts, it would\nproduce a representation that is more conducive to policy learning and this would be reflected in\ntask performance. For this domain, we use the half-precision Vicuna-7B version of the InstructBLIP\ninstruction-tuned generative VLM [14, 12] to produce promptable representations.\nMinecraft tasks. We consider all programmatic Minecraft tasks evaluated by Fan et al. [19]: combat\nspider, milk cow, shear sheep, combat zombie, combat enderman, and combat pigman1. The\nremaining tasks considered by Fan et al. [19] are creative tasks, which do not have programmatic\nreward functions or success detectors, so we cannot directly train RL agents on them. We follow the\nMineDojo definitions of observation\/action spaces and reward function structures for these tasks: at\neach time step, the policy observes an egocentric RGB image, its pose, and its previously action;\nthe policy can choose a discrete action to turn the agent by changing the agent’s pitch and\/or yaw in\ndiscrete increments, move, attack, or use a held item. These tasks are long horizon, with a maximum\nepisode length of 500 - 1000 and taking roughly 200 steps for a learned policy to complete them. See\nFigure 3 for example observations and Appendix B.1 for more details.\nComparisons. We compare PR2L to five performant classes of approaches for RL in Minecraft: (a)\nMethods using non-promptable representations of visual observations. This does not use prompting\naltogether, instead using task-agnostic embeddings from the VLM’s image encoder (specifically, the\nViT-g\/14 from InstructBLIP – blue in Figure 2). While these representations are still pre-trained, PR2L\nutilizes prompting to produce task-specific representations. For a fair comparison, we use the exact\nsame policy architecture and hyperparameters for this baseline as in PR2L, ensuring that performance\ndifferences come from prompting for better representations from the VLM. (b) Methods that directly\n“asks” the VLM to output actions to execute on the agent. This adapts the approach of Brohan et al.\n[6] to our setting and directly outputs the action from the VLM. While Brohan et al. [6] also fine-tune\nthe VLM backbone, we are unable to do so using our compute resources. To compensate, we do not\njust execute the action from the VLM, but train an RL policy to map this decoded action to a better\none. Note that if the VLM already decodes good action texts, simply copying over this action via RL\nshould be easy. (c) Methods for efficient RL from pixels via model-based approaches. We choose\nDreamer v3, since it has proven to be successful at learning Minecraft tasks from scratch [20]. (d)\nMethods leveraging pretrained representations specifically useful for embodied control, though which\nare non-promptable and non-Minecraft specific. We choose VC-1 and R3M [43, 46]. (e) Methods\nusing models pre-trained on large-scale Minecraft data. These serve as “oracle” comparisons, as\nthese representations are explicitly fine-tuned on Minecraft YouTube videos, whereas our pre-trained\nVLM is both frozen and not trained on any Minecraft video data. We choose MineCLIP, VPT, and\nSTEVE-1 as our sources of Minecraft-specific representations [19, 3, 37].\nWe use PPO [59] as our base RL algorithm for all non-Dreamer Minecraft policies. We also note that\nwe do not compare against non-RL methods, such as Voyager (which uses LLMs to write high-level\ncode skills, abstracting away low-level control to hand-written APIs that use oracle information). See\nAppendix B.2 for training details and E.1 for further discussion of such non-learned systems.\n4.2\nDomain 2: Habitat\nA major advantage of VLMs pre-trained on Internet-scale data is their reasoning and generalization\ncapabilities. To evaluate this, we run offline BC and RL experiments in the Habitat household\nsimulator. In contrast to Minecraft, tasks in this domain require connecting naturalistic images\nwith real-world common sense about the structure and contents of typical home environments. Our\nexperiments evaluate (1) whether PR2L confers the generalization properties of VLMs to our policies,\n1 Fan et al. [19] also consider hunt cow\/sheep. However, we omit them as we were unable to replicate their\nresults on those tasks; all approaches failed to learn them.\n6\nPR2L Prompt\nRT-2-style Baseline Prompt\nChange Auxiliary Text Ablation Prompt\nCombat Spider\nSpiders in Minecraft are black.\nIs there a spider in this image?\nI want to fight a spider. I can attack,\nmove, or turn. What should I do?\nIs there a spider in this image?\nMilk Cow\nIs there a cow in this image?\nI want to milk a cow. I can use my bucket,\nmove, or turn. What should I do?\nCows in Minecraft are black and white.\nIs there a cow in this image?\nShear Sheep\nIs there a sheep in this image?\nI want to shear a sheep. I can use my shears,\nmove, or turn. What should I do?\nSheep in Minecraft are usually white.\nIs there a sheep in this image?\nOther Combat Tasks\nIs there a [target entity] in this image?\nI want to fight a [target entity]. I can attack,\nmove, or turn. What should I do?\n-\nTable 1: Prompts used in Minecraft for querying the VLM with PR2L, comparison (b), and the change auxiliary\ntext ablation. For the last column, we remove the auxiliary text for combat spider, and add it in for the other two.\n(2) whether PR2L-based policies can leverage the semantic reasoning capabilities of the underlying\nVLM (e.g., via chain-of-thought [71]), and (3) whether PR2L can learn entirely from stale, offline\ndata sources. We use a Llama2-7B Prismatic VLM for the Habitat experiments [29].\nHabitat tasks. We consider the ObjectNav task suite in 3D scanned household scenes from the\nHM3D dataset [58, 73, 54]. These tasks involve a simulated robot traversing a home environment to\nfind an instance of a specified object (toilet, bed, sofa, television, plant, or chair) in the shortest path\npossible. The full benchmark consists of 80 household scenes intended to train the agent and 20 for\nvalidation. We change the observation space to consist of just RGB vision, previous action, pose,\nand target object class, omitting depth images to ensure that observed performance differences come\nfrom the quality of promptable representations vs. unpromptable ones. Like with MineDojo, these\ntasks are long horizon, taking 80 steps for a privileged shortest path follower to succeed and 150+\nfor humans. See Figure 3 for example observations and Appendix C for more details.\nComparisons. To see if PR2L can leverage VLM reasoning capabilities, we train two PR2L policies,\none with and one without chain-of-thought prompting (see Section 4.3). We also train a policy\non Prismatic VLM image encoder embeddings (equivalent to Minecraft approach (a), but with\nDino+SigLIP [11, 78]) on a human demonstration dataset collected from the ObjectNav training\nscenes collected with Habitat-Web [55] and used by past works on large-scale BC on pre-trained\nvisual representations [56, 74, 43]. As it previously achieved state-of-the-art performance among\nthose works, we also compare against two policies using VC-1 as an encoder [43], either using just\nits summarizing CLS token or using a learned Transformer layer to condense its patch embeddings.\nWe adopt the same LSTM-based recurrent architecture used by that work, but replace the image\nembeddings with a learned Transformer layer that condenses our input token embeddings (from the\nVLM, VLM image encoder, or VC-1) into a single summary embedding, as done with Minecraft.\nDue to computational constraints, we train all policies on just under a tenth of the full dataset of\n77k trajectories\/12M steps. In contrast, other works using this dataset train on the entire dataset.\nNevertheless, we evaluate on the unseen validation scenes, thereby testing how well PR2L generalizes.\n4.3\nDesigning Task-Specific Prompts for Minecraft and Habitat\nWe now discuss how to design prompts for PR2L. As noted in Section 3.3, these are not instructions\nor task descriptions, but prompts that force the VLM to encode semantic information useful for the\ntask in its representation. The simplest relevant feature for our Minecraft tasks is the presence of the\ntarget entity in an observation. Thus, we choose “Is there a [target entity] in this image?” as the base\nof our chosen prompt. We also pick two alternate prompts per task that prepend different amounts of\nauxiliary information about the target entity. E.g., for combat spider, one candidate is “Spiders in\nMinecraft are black.” To choose between these candidates, we measure how well the VLM is able\nto decode a correct answer to the prompt question of whether or not the target entity is present in\nthe image on a small annotated dataset. Full details of this prompt evaluation scheme for the first\nthree Minecraft tasks are presented in Appendix A and Table 5. We find that auxiliary text only helps\nwith detecting spiders while systematically and significantly degrading the detection of sheep and\ncows. Our ablations show that this detection success rate metric correlates with performance of the\nRL policy. Additionally, the prompts used for comparison (b) follow the prompt structure prescribed\nby Brohan et al. [6], which motivated this comparison. In these prompts, we also provide a list of\nactions that the VLM can choose from to the policy. All chosen prompts are presented in Table 1.\nFor Habitat, we choose the prompt “Would a [target object] be found here? Why or why not?” As\nopposed to the Minecraft prompts, this does not just identify the presence of a target object in the\nimage, but draws on general knowledge from the VLM to determine if the observed location would\ncontain the target object, even if said object is not in view. The second part of the prompt then leads\nthe VLM to provide a chain of thought (CoT) [71] rationale for its final answer. This CoT draws out\n7\nTask\nPR2L (Ours)\nBaselines\nOracles\nVLM Image Encoder\nRT-2-style\nDreamer\nVC-1\nR3M\nMineCLIP\nVPT\nSTEVE-1\nCombat Spider\n97.6 ± 14.9\n51.2 ± 9.3\n71.5 ± 9.7\n5.4 ± 1.1\n72.2 ± 9.3\n72.9 ± 8.7\n176.9 ± 19.8\n137.2 ± 19.2\n88.8 ± 14.0\nMilk Cow\n223.4 ± 35.4\n95.2 ± 18.7\n128.6 ± 28.9\n24.0 ± 1.2\n96.6 ± 16.3\n100.0 ± 14.1\n194.4 ± 33.3\n85.5 ± 14.5\n75.2 ± 15.4\nShear Sheep\n37.0 ± 4.4\n23.0 ± 3.6\n26.2 ± 3.2\n20.9 ± 1.2\n26.5 ± 4.0\n17.5 ± 2.4\n23.1 ± 3.7\n24.1 ± 2.9\n18.2 ± 2.5\nCombat Zombie\n24.6 ± 1.6\n14.8 ± 2.0\n18.2 ± 2.1\n1.8 ± 0.2\n5.6 ± 1.0\n5.8 ± 1.4\n56.6 ± 8.3\n31.2 ± 3.2\n23.6 ± 3.4\nCombat Enderman\n52.2 ± 5.6\n51.9 ± 6.8\n44.6 ± 5.8\n1.6 ± 0.5\n27.2 ± 2.4\n33.8 ± 3.8\n72.1 ± 7.1\n74.4 ± 13.2\n59.3 ± 6.7\nCombat Pigman\n46.4 ± 3.3\n36.8 ± 3.7\n35.1 ± 2.5\n5.8 ± 1.5\n33.7 ± 4.9\n31.4 ± 4.2\n189.0 ± 7.9\n169.0 ± 7.8\n98.3 ± 8.4\nTable 2: Performance of PR2L, baseline, and oracle approaches in Minecraft tasks. Values reported\nare IQM successes and standard errors. PR2L universally outperforms all baselines. As they are trained on\nMinecraft-specific data, the oracles outperform PR2L in half the comparisons (italicized).\nTask (# Episodes)\nPR2L (Ours)\nVLM Image Encoder\nVC-1 + CLS\nVC-1 + Patch Embeds\nWith CoT\nWithout CoT\n40 Epochs\n120 Epochs\n40 Epochs\n120 Epochs\nAverage (2000)\n41.9%\n27.8%\n11.6%\n6.8%\n8.9%\n13.6%\n15.8%\nToilet (398)\n37.2%\n22.9%\n8.8%\n2.8%\n2.0%\n7.0%\n9.3%\nBed (433)\n45.0%\n28.9%\n12.9%\n6.7%\n9.9%\n14.8%\n19.2%\nSofa (376)\n48.1%\n34.3%\n11.7%\n9.8%\n14.4%\n17.0%\n19.4%\nChair (428)\n51.2%\n40.9%\n17.5%\n11.7%\n15.0%\n22.4%\n23.8%\nTelevision (281)\n26.7%\n10.3%\n5.0%\n2.8%\n3.2%\n4.6%\n4.6%\nPlant (84)\n23.8%\n8.3%\n9.1%\n1.2%\n1.2%\n9.5%\n9.5%\nTable 3: Performance of PR2L and baselines on Habitat ObjectNav tasks. Following prior works, values\nreported are average success rates in unseen validation scenes. PR2L (with or without CoT) does better than\nall other approaches. PR2L with CoT does the best, universally achieving more than double the performance\nof all non-PR2L approaches and 14.7% higher average performance than PR2L without CoT. Note that PR2L\nand image encoder policies were trained for 40 epochs, but VC-1 policies’ performance saturated at 120, so we\nreport their performance at both times.\ntask-relevant VLM world knowledge by explicitly reasoning about visual semantic concepts, that are\nuseful to learning a policy (see Table 4; ObjectNav). To investigate if PR2L enables embodied agents\nto benefit from these VLM common-sense reasoning capabilities (even if they do not directly reason\nabout actions), we train PR2L policies both with and without the second part of the prompt.\n5\nResults\nMinecraft results. We report the interquartile mean (IQM) and standard error number of successes\nover 16 seeds for all Minecraft tasks in Table 2. PR2L uniformly outperforms the non-oracle\napproaches of (a) using non-promptable image embeddings, (b) directly asking the VLM for actions,\n(c) learning from scratch Dreamer, and (d) using non-promptable control-specific embeddings.\nPR2L outperforms (a) the VLM image encoder baseline, even though both approaches receive\nthe same visual features, with PR2L simply transforming those features via prompting an LLM\n(with no additional information from the environment), thus supporting that prompting does shape\nrepresentations in a beneficial way for learning control tasks. We provide an analysis of why PR2L\nstates are better than (b) RT-2-style ones in Appendix H.1. We observe that PR2L embeddings are\nbimodally distributed, with transitions leading to high reward clustered at one mode. This structure\nlikely enables more efficient learning, thereby showing how control tasks can benefit from extracting\nprior knowledge encoded in VLMs by prompting them with task context, even when the VLM does\nnot know how to act. For (c) the model-based comparisons, we find that Dreamer is not as conducive\nat learning our Minecraft tasks. We hypothesize this is because our tasks are comparatively shorter\nthan the ones considered by Hafner et al. [20], so learning a model is less beneficial (while PR2L\nprovides immediately-useful representations). Additionally, we note that all our approaches involve\ninteracting with partially-observable, non-stationary entities, which the Dreamer model may have a\nhard time learning. See Appendix E.2 for further discussion. Finally, (e) the oracles outperform PR2L\nin combat enderman\/pigman, all but STEVE-1 do better in combat spider\/zombie, and none do better\nin shear sheep\/milk cow. We hypothesize this is because endermen and pigmen are Minecraft-specific\nentities, giving rise to comparatively poor representations in the VLM (which is trained exclusively\non natural images). In contrast, Minecraft zombies\/spiders are heavily stylized, but still somewhat\nresemble other depictions of such creatures, while Minecraft cows and sheep are the closest to their\nnaturalistic counterparts, making PR2L more effective. Even though our VLM is not trained on\nMinecraft data, its representations yield better policies in half the oracle comparisons.\nWe provide ablations in Table 8 and Appendix F. We find that (1) PR2L performs worse when it is\nunprompted or does not decode text, (2) our prompt evaluation scheme successfully identified cases\nwhere auxiliary text improves\/degrades performance, and (3) a policy with oracle entity detection\ndoes worse than PR2L, suggesting our prompt is not just eliciting that feature from the VLM.\n8\nTarget\nExample VLM Responses to \"Would a [target] be found here? Why or why not?\"\nToilet\n\"No, a toilet is not found in this room as it is a bedroom. Toilets are typically found in bathrooms .\nThe image shows a large bed with a wooden headboard, nightstands, and lamps .\"\n\"Yes, a toilet would be found in this room because it is a bathroom.\nThe presence of a toilet in the image is expected, as bathrooms are designed to accommodate toilets for personal hygiene .\"\nBed\n\"No, a bed would not be found in this room because it is a dining room.\nThe presence of a dining table and chairs suggests that this room is designed for eating meals and socializing, rather than sleeping \"\n\"Yes, a bed would be found here because the room is described as a bedroom.\nIt has a bed , a television, a lamp, and a dresser , all of which are common furnishings in a bedroom .\"\nSofa\n\"No, a sofa is not found here because it is a kitchen area with appliances and cabinets .\nSofas are typically found in living rooms or other common areas .\"\n\" Yes, there is a black leather sofa in the living room. It has a red pillow on it. It is a large sectional couch .\"\nTable 4: Example VLM responses to the Habitat prompt for various images. Beyond just detecting the target,\nprompting the VLM for CoT elicits relevant common sense, which it semantically relates to other useful visual\nfeatures. By using the underlying VLM embeddings as a state representation, the policy thus integrates the\nVLM’s knowledge and reasoning into its decision-making.\nHabitat results. Following prior works, we report success rates on the ObjectNav validation\nepisodes in Table 3. PR2L with CoT outperforms all other policies on all tasks, including an almost\n4× performance increase over the VLM image encoder baselines – again, suggesting that using\npromptable representations for control improves over the base purely-visual embeddings. While\nPR2L without CoT still does better than all baselines, we find CoT prompting improves policy\nperformance (by 1.5×, from 27.8% success rate to 41.9%), likely because it provides the policy with\nuseful generalizable features: e.g., even if the agent comes across an unfamiliar room while searching\nfor a toilet, it still knows to look elsewhere if the VLM reasons that, due to the presence of a bed, the\nroom is likely a bedroom (which is unlikely to contain toilets). Thus, even if the VLM cannot reason\nabout actions, our results indicate that PR2L provides a promising way of using its ability to reason\nabout image semantics and common sense for control. See Table 4 for CoT examples.\nWhile we do not beat VC-1’s reported SOTA BC performance (60.3% success rate when VC-1 is\nfrozen [43]), we note that said performance is achieved with (1) over ten times more training data and\ngradient steps and (2) image augmentations to prevent overfitting. Our VC-1 policies were trained on\nthe same amount of data as our PR2L agent and for 1-3× as many gradient steps, but perform far\nworse, suggesting that PR2L is significantly more sample- and compute-efficient than VC-1 policies.\nAdditionally, PR2L does not use any explicit countermeasures to overfitting, yet still generalizes well\nto unseen ObjectNav scenes (aided by the VLM’s representations of reasoning).\nFinally, we analyze policies trained with offline RL in a simplified Habitat setting in Appendices D,\nH, where we find that VLM representations align well with the returns of an optimal policy.\n6\nConclusion\nWe propose Promptable Representations for Reinforcement Learning, a method for extracting se-\nmantic features from images by prompting VLMs with task context to leverage their extensive\ngeneral-purpose prior knowledge. We demonstrate PR2L in Minecraft and Habitat, domains that\nbenefit from interpreting observations in terms of semantic concepts that can be related to task context.\nThis framework for using VLMs for control opens new directions. For example, other types of\nfoundation models pre-trained with more sophisticated methods could also be used for PR2L: e.g.,\nones trained on physical interactions might yield features which encode physics or action knowledge,\nrather than just common-sense visual semantics. Developing and using such models with PR2L offers\nan exciting way to transfer diverse prior knowledge to a broad range of control applications.\nA limitation of PR2L is that prompts are currently hand-crafted based on the user’s conception of\nuseful task features. While coming up with good prompts for our tasks was not hard, the process of\nevaluating and improving them could be automated, which we leave to future works. We also find that\nthe quality of representations largely depends on the VLM – e.g., InstructBLIP could not reason well\nabout Habitat scenes, but the more recent Prismatic VLMs are more capable in that regard, enabling\nour CoT experiments. Thus, as VLM capabilities are expected to increase, we expect the quality of\ntheir representations to also improve. Lastly, the size and speed of VLMs can limit their applicability.\nOur policies typically achieve 3-5 Hz inference speeds, comparable to those of robot policies built on\nlarge models [7, 6, 49]. Likewise, our VLM sizes are comparable to models used for policies in prior\nworks [6, 65]. While their inference speeds may hinder online policy learning, we find that offline\napproaches (which can parallelize training and data generation) we used for Habitat help remedy this.\n9\nReferences\n[1] A. Adeniji, A. Xie, C. Sferrazza, Y. Seo, S. James, and P. Abbeel. Language reward modulation\nfor pretraining reinforcement learning, 2023.\n[2] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakr-\nishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano,\nK. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine,\nY. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet,\nN. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng.\nDo as i can and not as i say: Grounding language in robotic affordances. 2022.\n[3] B. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro,\nand J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos,\n2022.\n[4] Y. Belinkov and J. Glass. Analysis methods in neural language processing: A survey, 2019.\n[5] J. Borja-Diaz, O. Mees, G. Kalweit, L. Hermann, J. Boedecker, and W. Burgard. Affordance\nlearning from play for sample-efficient policy learning. In Proceedings of the IEEE International\nConference on Robotics and Automation (ICRA), Philadelphia, USA, 2022.\n[6] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess,\nA. Dubey, C. Finn, P. Florence, C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman,\nA. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal,\nL. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao,\nK. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut,\nH. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao,\nP. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-2: Vision-language-action models transfer web\nknowledge to robotic control, 2023.\n[7] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi,\nR. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla, D. Manju-\nnath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao,\nM. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran,\nV. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-1:\nRobotics transformer for real-world control at scale, 2023.\n[8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan-\nguage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and\nH. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–\n1901. Curran Associates, Inc., 2020. URL https:\/\/proceedings.neurips.cc\/paper_\nfiles\/paper\/2020\/file\/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n[9] A. Bucker, L. Figueredo, S. Haddadin, A. Kapoor, S. Ma, S. Vemprala, and R. Bonatti. Latte:\nLanguage trajectory transformer, 2022.\n[10] S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction, 2023.\n[11] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging\nproperties in self-supervised vision transformers, 2021.\n[12] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality, March 2023. URL https:\/\/lmsys.org\/blog\/2023-03-30-vicuna\/.\n[13] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning\nwith quantile regression, 2017.\n10\n[14] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip:\nTowards general-purpose vision-language models with instruction tuning, 2023.\n[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding, 2019.\n[16] Z. Ding, H. Luo, K. Li, J. Yue, T. Huang, and Z. Lu. Clip4mc: An rl-friendly vision-language\nmodel for minecraft, 2023.\n[17] Y. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta, and J. Andreas. Guiding\npretraining in reinforcement learning with large language models, 2023.\n[18] K. Ehsani, T. Gupta, R. Hendrix, J. Salvador, L. Weihs, K.-H. Zeng, K. P. Singh, Y. Kim,\nW. Han, A. Herrasti, R. Krishna, D. Schwenk, E. VanderBilt, and A. Kembhavi. Imitating\nshortest paths in simulation enables effective navigation and manipulation in the real world,\n2023.\n[19] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,\nand A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale\nknowledge. In Neural Information Processing Systems, 2022, 2022.\n[20] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world\nmodels, 2023.\n[21] J. Hu and R. Levy. Prompt-based methods may underestimate large language models’ linguistic\ngeneralizations, 2023.\n[22] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual language maps for robot navigation.\nIn Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),\nLondon, UK, 2023.\n[23] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents, 2022.\n[24] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter.\nInner monologue: Embodied reasoning through planning with language models, 2022.\n[25] G. Jawahar, B. Sagot, and D. Seddah. What does BERT learn about the structure of language?\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\nFlorence, Italy, 2019. Association for Computational Linguistics.\n[26] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung.\nSurvey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38,\nmar 2023.\n[27] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Hatfield-\nDodds, N. DasSarma, E. Tran-Johnson, S. Johnston, S. El-Showk, A. Jones, N. Elhage, T. Hume,\nA. Chen, Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson, J. Kernion,\nS. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer, D. Amodei, T. Brown, J. Clark,\nN. Joseph, B. Mann, S. McCandlish, C. Olah, and J. Kaplan. Language models (mostly) know\nwhat they know, 2022.\n[28] A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang,\nW. Hong, Z. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret, A. Nikulin,\nY. Belousov, O. Svidchenko, and A. Shpilman. Minerl diamond 2021 competition: Overview,\nresults, and lessons learned, 2022.\n[29] S. Karamcheti, S. Nair, A. Balakrishna, P. Liang, T. Kollar, and D. Sadigh. Prismatic vlms:\nInvestigating the design space of visually-conditioned language models, 2024.\n[30] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C.\nBerg, W.-Y. Lo, P. Dollár, and R. Girshick. Segment anything, 2023.\n11\n[31] A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for offline reinforcement\nlearning, 2020.\n[32] B. Z. Li, M. Nye, and J. Andreas. Implicit representations of meaning in neural language\nmodels, 2021.\n[33] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation, 2022.\n[34] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models, 2023.\n[35] K. Li, A. K. Hopkins, D. Bau, F. Viégas, H. Pfister, and M. Wattenberg. Emergent world\nrepresentations: Exploring a sequence model trained on a synthetic task, 2023.\n[36] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as\npolicies: Language model programs for embodied control, 2023.\n[37] S. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith. Steve-1: A generative model for\ntext-to-behavior in minecraft, 2023.\n[38] H. Lin, Z. Wang, J. Ma, and Y. Liang. Mcu: A task-centric framework for open-ended agent\nevaluation in minecraft, 2023.\n[39] J. Lin, Y. Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein, and A. Dragan. Learning to model\nthe world with language. 2023.\n[40] H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with multimodal transformer,\n2023.\n[41] H. Luo, A. Yue, Z.-W. Hong, and P. Agrawal. Stubborn: A strong baseline for indoor object\nnavigation, 2022.\n[42] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data,\n2021.\n[43] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel,\nJ. Malik, D. Batra, Y. Lin, O. Maksymets, A. Rajeswaran, and F. Meier. Where are we in the\nsearch for an artificial visual cortex for embodied intelligence?, 2023.\n[44] O. Mees, J. Borja-Diaz, and W. Burgard. Grounding language with visual affordances over\nunstructured data. In Proceedings of the IEEE International Conference on Robotics and\nAutomation (ICRA), London, UK, 2023.\n[45] V. Myers, A. He, K. Fang, H. Walke, P. Hansen-Estruch, C.-A. Cheng, M. Jalobeanu, A. Kolobov,\nA. Dragan, and S. Levine. Goal representations for instruction following: A semi-supervised\nlanguage interface to control, 2023.\n[46] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation\nfor robot manipulation, 2022.\n[47] K. Narasimhan, R. Barzilay, and T. Jaakkola. Grounding language for transfer in deep reinforce-\nment learning, 2018.\n[48] K. Nottingham, P. Ammanabrolu, A. Suhr, Y. Choi, H. Hajishirzi, S. Singh, and R. Fox. Do\nembodied agents dream of pixelated sheep: Embodied decision making using language guided\nworld modelling, 2023.\n[49] O.M.T., D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, C. Xu, J. Luo,\nT. Kreiman, Y. Tan, D. Sadigh, C. Finn, and S. Levine. Octo: An open-source generalist robot\npolicy. https:\/\/octo-models.github.io, 2023.\n[50] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with\nhuman feedback, 2022.\n12\n[51] N. D. Palo, A. Byravan, L. Hasenclever, M. Wulfmeier, N. Heess, and M. Riedmiller. Towards\na unified agent with foundation models, 2023.\n[52] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from\nnatural language supervision, 2021.\n[53] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann. Stable-baselines3:\nReliable reinforcement learning implementations. Journal of Machine Learning Research, 22\n(268):1–8, 2021. URL http:\/\/jmlr.org\/papers\/v22\/20-1364.html.\n[54] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. Turner, E. Un-\ndersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao, and D. Batra. Habitat-\nmatterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai, 2021.\n[55] R. Ramrakhya, E. Undersander, D. Batra, and A. Das. Habitat-web: Learning embodied\nobject-search strategies from human demonstrations at scale, 2022.\n[56] R. Ramrakhya, D. Batra, E. Wijmans, and A. Das. Pirlnav: Pretraining with imitation and rl\nfinetuning for objectnav, 2023.\n[57] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\nwith latent diffusion models, 2022.\n[58] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun,\nJ. Malik, D. Parikh, and D. Batra. Habitat: A Platform for Embodied AI Research. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision (ICCV), 2019.\n[59] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms, 2017.\n[60] P. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language, 2022.\n[61] P. Sharma, B. Sundaralingam, V. Blukis, C. Paxton, T. Hermans, A. Torralba, J. Andreas, and\nD. Fox. Correcting robot plans with natural language feedback. In Robotics: Science and\nSystems, 2022, 2023.\n[62] X. Shi, I. Padhi, and K. Knight. Does string-based neural MT learn source syntax?\nIn\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,\npages 1526–1534, Nov. 2016.\n[63] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipula-\ntion. In Proceedings of the 5th Conference on Robot Learning (CoRL), 2021.\n[64] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. Progprompt: Generating situated robot task plans using large language models, 2022.\n[65] A. Szot, M. Schwarzer, H. Agrawal, B. Mazoure, W. Talbott, K. Metcalf, N. Mackraz, D. Hjelm,\nand A. Toshev. Large language models as generalizable policies for embodied tasks, 2024.\n[66] I. Tenney, D. Das, and E. Pavlick. Bert rediscovers the classical nlp pipeline, 2019.\n[67] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need, 2017.\n[68] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles and\nmodel abilities. Technical report, Microsoft, 2023.\n[69] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager:\nAn open-ended embodied agent with large language models, 2023.\n[70] Z. Wang, S. Cai, G. Chen, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task agents, 2023.\n13\n[71] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou.\nChain-of-thought prompting elicits reasoning in large language models, 2023.\n[72] G. Wiedemann, S. Remus, A. Chawla, and C. Biemann. Does bert make any sense? interpretable\nword sense disambiguation with contextualized embeddings, 2019.\n[73] K. Yadav, J. Krantz, R. Ramrakhya, S. K. Ramakrishnan, J. Yang, A. Wang, J. Turner,\nA. Gokaslan, V.-P. Berges, R. Mootaghi, O. Maksymets, A. X. Chang, M. Savva, A. Clegg,\nD. S. Chaplot, and D. Batra. Habitat challenge 2023. https:\/\/aihabitat.org\/challenge\/\n2023\/, 2023.\n[74] K. Yadav, A. Majumdar, R. Ramrakhya, N. Yokoyama, A. Baevski, Z. Kira, O. Maksymets, and\nD. Batra. Ovrl-v2: A simple state-of-art baseline for imagenav and objectnav, 2023.\n[75] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforcement\nlearning and planning for open-world minecraft tasks, 2023.\n[76] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin,\nD. Duong, V. Sindhwani, and J. Lee. Transporter networks: Rearranging the visual world for\nrobotic manipulation. Conference on Robot Learning (CoRL), 2020.\n[77] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit,\nM. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, and P. Florence. Socratic models: Composing\nzero-shot multimodal reasoning with language, 2022.\n[78] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training,\n2023.\n[79] B. Zhou, K. Li, J. Jiang, and Z. Lu. Learning from visual observation via offline pretrained\nstate-to-go transformer, 2023.\n[80] M. Zhu, Y. Li, and T. Kong. Integrating map-based method with end-to-end learning, 2022.\nURL https:\/\/www.youtube.com\/watch?v=N-wW3TwEqbU.\n[81] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, Y. Qiao,\nZ. Zhang, and J. Dai. Ghost in the minecraft: Generally capable agents for open-world\nenvironments via large language models with text-based knowledge and memory, 2023.\n14\nTarget Entity\nPrompt\nTrue Positive Rate\nTrue Negative Rate\nSpider\n“Is there a spider in this image?\"\n22.27%\n100.00%\n“Spiders in Minecraft are black.\nIs there a spider in this image?\"\n73.42%\n94.54%\n“Spiders in Minecraft are black\nand have red eyes and long, thin\nlegs. Is there a spider in this image?\"\n50.50%\n99.85%\nCow\n“Is there a cow in this image?\"\n71.00%\n45.41%\n“Cows in Minecraft are black and white.\nIs there a cow in this image?\"\n98.22%\n2.00%\n“Cows in Minecraft are black and white\nand have four legs.\nIs there a cow in this image?\"\n96.67%\n7.35%\nSheep\n“Is there a sheep in this image?\"\n88.00%\n59.83%\n“Sheep in Minecraft are white.\nIs there a sheep in this image?\"\n100.00%\n0.00%\n“Sheep in Minecraft are white and\nhave four legs.\nIs there a sheep in this image?\"\n100.00%\n0.00%\nTable 5: InstructBLIP’s performance at decoding text indicating that it detected the presence of a\ntarget entity when given different prompts. We use this as a proxy metric for prompt engineering for\nRL, allowing us to determine which prompt to use for PR2L.\nA\nPrompt Evaluation for RL in Minecraft\nWe discuss how to evaluate prompts to use with PR2L, by showcasing an example for a Minecraft\ntask. We start by noting that the presence and relative location of the entity of interest for each task\n(i.e., spiders, sheep, or cows) are good features for the policy to have. To evaluate if a prompt elicits\nthese features from the VLM, we collect a small dataset of videos in which each Minecraft entity\nof interest is on the left, right, middle, or not on screen for the entirety of the clip. Each video is\ncollected by a human player screen recording visual observations from Minecraft of the entity from\ndifferent angles for around 30 seconds at 30 frames per second (with the exception of the video where\nthe entity is not present, which is a minute long).\nWe propose prompts that target each of the two features we labeled. First, we evaluate prompts that\nask “Is there a(n) [entity] in this image?” As the answers to these questions are just yes\/no, we see\nhow well the VLM can directly generate the correct answer for each frame in the collected videos.\nThe VLM should answer “yes” for frames in the three videos where the target entity is on the left,\nright, or middle of the screen and “no” for the final video. Second, we evaluate if our prompts can\nextract the entity’s relative position (left, right, or middle) in the videos where it is present. We\nnote that the prompts we tried could not extract this feature in the decoded text (e.g., asking “Is the\n[entity] on the left, right, or middle of the screen?” will always cause the VLM to decode the same\ntext). Thus, we try to see if this feature can be extracted from the decoded texts’ representations. We\nmeasure this by fitting a three-category linear classifier of the entity’s position given the token-wise\nmean of the decoded tokens’ final embeddings. This is an unsophisticated and unexpressive classifier,\ni.e., we do not have to worry about the model potentially memorizing the data, which means that\ngood classification performance corresponds to an easy extractability of said feature.\nWe evaluate three types of prompts per task entity for the first feature: one simply asking if the\nentity is present in the image (e.g., “Is there a spider in this image?”) and two others adding varying\namounts of auxiliary information about visual characteristics of the entity (e.g., “Spiders in Minecraft\nare black. Is there a spider in this image?” and “Spiders in Minecraft are black and have red eyes\nand long, thin legs. Is there a spider in this image?”). We present evaluations of all such prompts in\nTable 5. We find that the VLM benefits greatly from auxiliary information for the spider case only,\nlikely because spiders in Minecraft are the most dissimilar to the ones present in natural images of\nreal spiders, whereas cows and sheep are still comparatively similar, especially in terms of scale and\ncolor. However, adding too much auxiliary information degrades performance, perhaps because the\ninput prompt becomes too long, and therefore is out-of-distribution for the types of prompts that\nthe VLM was pre-trained on. This same argument may explain why auxiliary information degrades\n15\nFigure 3: Example tasks, observations, and task-relevant prompts from MineDojo and Habitat.\nperformance for the other two target entities as well, causing them to almost always answer that\nsaid entities are present, even when they are not. Once more, considering that these targets exhibit a\nhigher degree of visual resemblance to to their real counterparts compared to Minecraft spiders, it is\nreasonable to infer that the VLM would not benefit from auxiliary information. Furthermore, taking\ninto account that the auxiliary information we gave is more common-sense than the information given\nfor the spider, it could imply that the prompts are also more likely to be out-of-distribution (given\nthat “sheep are white” is so obvious that people would not bother expressing it in language), causing\nthe systematic performance degradation.\nFor the probing evaluation, we find that all three prompts reach similar final linear classifiabilities for\neach of their target entities, as shown in Figure 4. While this does not aid in choosing one prompt\nover another, it does confirm that the VLM’s decoded embeddings for each prompt still contains this\nvaluable and granular position information about the target entity, even though the input prompt did\nnot ask for it.\nB\nMineDojo Details\nB.1\nEnvironment Details\nSpaces. The observation space for the Minecraft tasks consists of the following:\n1. RGB: Egocentric RGB images from the agent. (160, 256, 3)-size tensor of integers ∈\n{0, 1, ..., 255}.\n2. Position: Cartesian coordinates of agent in world frame. 3-element vector of floats.\n3. Pitch, Yaw: Orientation of agent in world frame in degrees. Note that we limit the pitch\nto 15◦above the horizon to 75◦below for combat spider, which makes learning easier (as\nthe agent otherwise often spends a significant amount of time looking straight up or down).\nTwo 1-element vectors of floats.\n4. Previous Action: The previous action taken by the agent. Set to no operation at the start\nof each episode. One-hot vector of size |A| = 53 for combat spider and 89 otherwise (see\nbelow).\nThis differs from the simplified observation space used in [19] in that we do not use any nearby voxel\nlabel information and impose pitch limits for combat spider. This observation space is the same for\nall Minecraft experiments.\n16\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClassification Accuracy\nLinear Classifier Accuracy of Relative Position of Spider\nIs there a spider in this image?\nSpiders in Minecraft are black. Is there a spider in this image?\nSpiders in Minecraft are black and have red eyes and long, thin legs.\nIs there a spider in this image?\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClassification Accuracy\nLinear Classifier Accuracy of Relative Position of Cow\nIs there a cow in this image?\nCows in Minecraft are black and white. Is there a cow in this image?\nCows in Minecraft are black and white and have four legs.\nIs there a cow in this image?\n0\n100\n200\n300\n400\n500\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nClassification Accuracy\nLinear Classifier Accuracy of Relative Position of Sheep\nIs there a sheep in this image?\nSheep in Minecraft are white. Is there a sheep in this image?\nSheep in Minecraft are white and have four legs.\nIs there a sheep in this image?\nFigure 4: We train a linear classifier to predict the rela-\ntive position of the target entity (left\/right\/middle) based\non the average VLM embeddings decoded in response to\neach associated candidate prompt. We find that all three\ncandidate prompts per task elicit embeddings that are\nsimilarly highly conducive to this classification scheme.\nThe action space is discrete, consisting of 53 or\n89 different actions:\n1. Turn: Change the yaw and pitch of\nthe agent. The yaw and pitch can be\nchanged up to ±90◦in multiples of\n15◦. As they can both be changed at\nthe same time, there are 9 × 9 = 81 to-\ntal different turning actions. The turn-\ning action where the yaw and pitch\nchanges are both 0◦is the no opera-\ntion action. Note that, since we im-\npose pitch limits for the spider task, we\nalso limit the change in pitch to ±30◦,\nmeaning there are only 45 turning ac-\ntions in that case.\n2. Move: Move forward, backward, left,\nright, jump up, or jump forward for 6\nactions total.\n3. Attack: Swing the held item at what-\never is targeted at the center of the\nagent’s view.\n4. Use Item: Use the held item on what-\never is targeted at the center of the\nagent’s view. This is used to milk cows\nor shear sheep (with an empty bucket\nor shears respectively). If holding a\nsword and shield, this action will block\nattacks with the latter.\nThis non-combat spider action space is the same\nas the simplified one in [19]. All experiments\nfor a given task share the same action space.\nWorld specifications. MineDojo implements\na fast reset functionality that we use. Instead\nof generating an entirely new world for each\nepisode, fast reset simply respawns the player\nand all specified entities in the same world instance, but with fully restored items, health points,\nand other relevant task quantities. This lowers the time overhead of resets significantly, but also\nmeans that some changes to the world (like block destruction) are persistent. However, as breaking\nblocks generally takes multiple time steps of taking the same action (and does not directly lead to any\nreward), the agent empirically does not break many blocks aside from tall grass (which is destroyed\nwith a single strike from any held item). We keep all reset parameters (like the agent respawn radius,\nhow far away entities can spawn from the agent, etc) at their default values provided by MineDojo.\nWe stage all tasks in the same area of the same programmatically-generated world: namely, a\nsunflower plains biome in the world with seed 123. This is the default location for the implementation\nof the spider combat task in MineDojo. We choose this specific world\/location as it represents a\nprototypical Minecraft scene with relatively easily-traversable terrain (thus making learning faster\nand easier).\nAdditional task details and reward functions. We provide additional notes about our Minecraft\ntasks.\nCombat spider: Upon detecting the agent, the spider approaches and attacks; if the agent’s health is\ndepleted, then the episode terminates in failure. The agent receives +1 reward for striking any entity\nand +10 for defeating the spider. We also include several distractor animals (a cow, pig, chicken, and\nsheep) that passively wander the task space; the agent can reward game by striking these animals,\nmaking credit assignment of success rewards and the overall task harder.\n17\nHyperparameter\nTask\nCombat Spider\nMilk Cow\nShear Sheep\nCombat Zombie\nCombat Enderman\nCombat Pigman\nTotal Train Steps\n150000\n100000\nRollout Steps\n2048\nAction Entropy Coefficient\n5e-3\nValue Function Coefficient\n0.5\nMax LR\n5e-5\n1e-4\n1e-4\n5e-5\n1e-4\n5e-5\nMin LR\n5e-6\n1e-4\n1e-4\n5e-6\n1e-4\n5e-6\nBatch Size\n64\nUpdate Epochs\n10\nγ\n0.99\nGAE λ\n0.95\nClip Range\n0.2\nMax Gradient Norm\n0.5\nNormalize Advantage\nTrue\nTable 6: PPO hyperparameters for Minecraft tasks, shared by the baselines, our method, and ablations.\nPolicy Transformer Hyperparameters\nTransformer Token Size\n512 \/ 128\nTransformer Feedforward Dim\n512 \/ 128\nTransformer Number Heads\n2\nTransformer Number Decoder Layers\n1\nTransformer Number Encoder Layers\n1\nTransformer Output Dim\n128\nTransformer Dropout\n0.1\nTransformer Nonlinearity\nReLU\nPolicy MLP Hyperparameters\nNumber Hidden Layers\n1\nHidden Layer Size\n128\nActivation Function\ntanh\nVLM Generation Hyperparameters\nMax Tokens Generated\n6\nMin Tokens Generated\n6\nDecoding Scheme\nGreedy\nTable 7: All policy hyperparameters for all Minecraft tasks. Smaller token sizes and feedforward\ndimensions are used for combat [zombie\/enderman\/pigman].\nMilk cow: The agent also holds wheat in its off hand, which causes the cow to approach the agent\nwhen detected and sufficiently nearby. For each episode, we track the minimum visually-observed\ndistance between the agent and the cow at each time step. The agent receives +0.1|∆dmin| reward for\ndecreasing this minimum distance (where ∆dmin ≤0 is the change in this minimum distance at a\ngiven time step) and +10 for successfully milking the cow.\nShear sheep: As with milk cow, the agent holds wheat in its off hand to cause the sheep to approach\nit. The reward function also has the same structure as that task, albeit with different coefficients:\n+|∆dmin| for decreasing the minimum distance to the sheep and +10 for shearing it.\nCombat zombie: Same as combat spider, but the enemy is a zombie. We increase the episode length\nto 1000, as the zombie has more health points than the spider.\nCombat enderman: Same as combat spider, but the enemy is an Enderman. As with combat zombie,\nwe increase the episode length to 1000. Note that Endermen are non-hostile (until directly looked at\nfor sufficiently long or attacked) and have significantly more health points than other enemies. We\nthus enchant the agent’s sword to deal more damage and decrease the initial spawn distance of the\nenderman from the agent.\nCombat pigman: Same as combat spider, but the enemy is a hostile zombie pigman. As with combat\nzombie, we increase the episode length to 1000.\n18\nB.2\nPolicy and Training Details\nFor our actual RL algorithm, we use the Stable-Baselines3 (version 2.0.0) implementation of clipping-\nbased PPO [53], with hyperparameters presented in Table 6. Many of these parameters are the same\nas the ones presented by [19]. For the spider trials, we use a cosine learning rate schedule:\nLR(current train step) = Min LR + (Max LR −Min LR)\n\n\n1 + cos\n\u0010\nπ current train step\ntotal train steps\n\u0011\n2\n\n\n(1)\nWe also present the policy and VLM hyperparameters in Table 7. The hyperparameters and architec-\nture of the MLP part of the policy are primarily defined by the default values and structure defined by\nthe Stable-Baselines3 ActorCriticPolicy class. Note that the no generation ablation, VLM image\nencoder baseline, and MineCLIP trials do not generate text with the VLM, and so all do not use the\nassociated process’s hyperparameters. The MineCLIP trials also do not use a Transformer layer in\nthe policy, due to not receiving token sequence embeddings. It instead just uses a MLP, but with two\nhidden layers (to supplement the lowered policy expressivity due to the lack of a Transformer layer).\nAdditionally, InstructBLIP’s token embeddings are larger than ViT-g\/14’s (used in the VLM image\nencoder baseline), and so may carry more information. However, the VLM does not receive any\nprivileged information over the image encoder from the task environment – any additional information\nin the VLM’s representations is therefore purely from the model’s prompted internal knowledge. Still,\nto ensure consistent policy expressivity, we include a learned linear layer projecting all representations\nfor this baseline and our approach to the same size (512 dimensions) so that the rest of the policy is\nthe same for both.\nMinecraft training runs were run on 16 A5000 GPUs (to accommodate the 16 seeds).\nC\nHabitat ObjectNav Details\nC.1\nEnvironment Details\nThe spaces and agent\/task specifications are largely the same as the defaults provided by Habitat, as\nspecified in the HM3D ObjectNav configuration file [58].\nSpaces. The observation space for Habitat consists of the following:\n1. RGB: Egocentric RGB images from the agent. (480, 640, 3)-size tensor of integers ∈\n{0, 1, ..., 255}. By default, agents also receive depth images, but we remove them to ensure\nthat state representations are grounded primarily in visual observations.\n2. Position: Horizontal Cartesian coordinates of agent. 2-element vector of floats.\n3. Compass: Yaw of the agent. Single floats.\n4. Previous Action: The previous action taken by the agent. Set to no operation at the start of\neach episode. One-hot vector of size |A| = 4.\n5. Object Goal: Which object the agent is aiming to find. One-hot vector of size 3.\nThe action space is the standard Habitat-Lab action space, though we remove the pitch-changing\nactions, leaving only four:\n1. Turn: Turn left or right, changing the yaw by 30◦.\n2. Move Forward: Move forward a fixed amount or until the agent collides with something.\n3. Stop: Ends the episode, indicating that the agent believes it has found the goal object.\nAll observations, actions, and associated dynamics are deterministic.\nWorld specifications. In ObjectNav, an agent is spawned in a household environment and must find\nand navigate to an instance of a specified target object in as efficient a path as possible. Doing so\neffectively requires a common-sense understanding of where household objects are often found and\nthe structure of standard homes.\n19\nHabitat provides a standardized train-validation split, consisting of 80 household scenes for training\n(from which one can run online RL or collect data for offline RL or BC) and 20 novel scenes\nfor validation, thereby testing policies’ generalization capabilities. These scenes come from the\nHabitat-Matterport 3D v1 dataset [54].\nC.2\nPolicy and Training Details\nIn line with previous work [56, 74, 43], we train our policies with behavior cloning (BC) on the\nHabitat-Web human demonstration dataset of 77k trajectories (12M steps) [55]. We adopt many of\nthe same design choices provided by said prior works, but with a few critical differences:\n1. Due to compute limitations, we were unable to train on the full dataset (as those original\nworks used 512 parallel environments to roll out demo trajectories and collect data). Instead,\nwe used a subset of the dataset, built by dividing the dataset by both target object and scene,\nthen sampling every tenth demo. This would ensure that our training data still contained\nexamples from every training scene + target object combination that existed. In total, our\nsubsampled dataset contains approximately 1.1M steps over 7550 trajectories.\n2. We adopt the same optimizer, scheduler, and associated hyperparameters as Majumdar et al.\n[43], but find a learning rate of 1e −4 to be more effective than their 1e −3.\n3. Rather than sampling partial trajectory rollouts from 512 parallel environments as done by\nMajumdar et al. [43], our batches contain full trajectories, though with the same total number\nof transitions per batch as in that work. This means that our batches potentially contain less\ndiverse data (due to observations from fewer different total scenes being present), but allow\nus to compute up-to-date full trajectory hidden states for the RNN portion of our policy. We\nuse gradient accumulation to achieve this, once again due to compute limitations.\n4. While Majumdar et al. [43] trains for 24k gradient steps (observing approximately 400M\ntransitions.), we find using only approximately a tenth of that (40 epochs through our smaller\ndataset, so around 40M transitions) to reach peak performance for our policy. The scheduler\nstill assumes the full training run will last for 400M transitions, so our LR decays at the\nsame rate as with VC-1. Furthermore, for fairness, we leave our VC-1 baseline policies\n(trained on our subsampled datasets) training beyond 40 epochs, and report their validation\nperformance at both 40 and 120 epochs (when its performance saturates).\n5. For policies that receive visual observations as a sequence of tokens (PR2L, VC-1 with\npatch embeddings), we apply 2D average pooling with kernel sizes of 4 × 4 to reduce down\nto 16 tokens. Then, we pass those tokens through a learned Transformer layer, instead of the\nlearned compression layer used by Majumdar et al. [43]. We do this to ensure that policy\nperformance differences are due to representation quality, not architecture.\n6. We employ inflection upweighting during training, as done by Ramrakhya et al. [56], Yadav\net al. [74], Majumdar et al. [43]. However, we also categorically upweight the cross entropy\nloss of stopping and turning by 1.5 (due to them being uncommon but important), as we\nobserve this increases learning speed for all policies.\n7. We do not employ any image augmentation or loss regularization to prevent overfitting.\nHowever, we find our policy exhibits strong generalization performance in unseen validation\nscenes nonetheless.\nFor PR2L-specific design choices:\n1. Our chosen VLM is the Prismatic VLM [29] with Dino+SigLIP as a vision backbone and\nLlama2-7B-pure as the language backbone. We use the 224px version, which maps images\nto 256 visual tokens (which, as described above, get compressed into 16 via pooling).\n2. To reduce the size of VLM representations for PR2L, we embed one observation (sampled\nuniformly at random) from each trajectory in our subsampled dataset with our VLM, then\ncompute all resulting tokens’ principle component vectors. We then use said vectors to\nlower all tokens’ dimensionality down from 4096 to 1024 (i.e., corresponding approximately\nto their first 1024 principle components).\n3. Like with the Minecraft experiments, we take the VLM’s last two layers’ embeddings and\ntreat them as our promptable representations. However, unlike with Minecraft, we stack\n20\neach VLM token’s two embeddings (forming new embeddings of size 2048), rather than\nconcatenate all of them.\n4. For generating text in response to our task-relevant prompt, we use sample-based decoding\nwith fixed random seed prior to the decoding with temperature 0.4 and 32 −48 new tokens\ngenerated.\n5. The learned Transformer layer of our policy is the same as the one used in the Minecraft\nexperiments, but with token embedding sizes of 1024.\nAll Habitat training was done on an A100 GPU server. Generation of data and evaluations were done\non 16 A5000 GPUs for parallelization.\nD\nSimplified Habitat Offline RL Experiments\nWhile our primary Habitat experiments use behavior cloning to stay consistent with past works, we\nalso run offline RL experiments on a simplified version of ObjectNav to better explore how VLM\nrepresentations aid action learning. We discuss the details of said setting now.\nD.1\nEnvironment Details\nWe pick 32 reconstructed 3D home environments with at least one instance of each of the three target\nobjects (toilet, bed, and sofa) and an annotator quality score of at least 4 out of 5. We choose to\nremove plants and televisions from the goal object set due to finding numerous unlabeled instances\nof them. Additionally, we remove chairs, as they are significantly more common than other goal\nobjects and thus usually can be found in much shorter episodes. This simplified problem formulation\nenables us to remove many of the “tricks” that aid ObjectNav, such as using omnidirectional views or\npolicies with history; our agent makes action decisions purely based on its current visual observation\nand pose, allowing us to do “vanilla” RL to better isolate the effect of PR2L.\nTo generate data, we use Habitat’s built-in greedy shortest geodesic path follower. Imitating such\ndemonstrations allows policies to learn unintuitively emergent and performant navigation behaviors\n[18] at scale. For each defined starting location in our considered households, we autonomously\ncollect data by using the path follower to navigate to each reachable instance of the corresponding\ngoal object. This yields high quality, near-optimal data. We then supplement our dataset by generating\nlower-quality data. Specifically, for each computed near-optimal path from a starting location to a\ngoal object instance, we choose to inject action noise partway through the trajectory (uniformly at\nrandom from 0 −90% of the way through). At that point, all subsequent actions have a 0 −50%\nprobability (again chosen uniformly at random) of being a random action other than the one specified\nby the path follower. To ensure that paths are sufficiently long, we choose to make the probability of\nchoosing the stop action 10% and the other two movement actions 45%. In total, we collect 107518\nobservations over 2364 trajectories.\nReward functions. The ObjectNav challenge evaluates agents based on the average \"success\nweighted by path length\" (SPL) metric [73]: if an agent succeeds at taking the stop action while close\nto an instance of the goal object, it gets SPL(p, l) =\nl\nmax(l,p) points, where l is the actual shortest\npath from the starting point to an instance of the goal object and p is the length of the path that the\nagent actually took during that particular episode. If the agent stops while not close to the target\nobject, the SPL is 0. Thus, taking the most efficient path to the nearest goal object and stopping yields\na maximum SPL of 1.\nWe use this to design our reward function. Specifically, when the agent stops, it receives a reward\nof +10SPL(p, l). Additionally, we add a shaping reward of the change in geodesic distance to the\nnearest goal object instance each time the agent moves (where lowering that distance yields a positive\nreward).\nD.2\nPolicy and Training Details\nFor our offline RL experiments in Habitat, we use Conservative Q-Learning (CQL) on top of\nthe Stable-Baslines3 Contrib codebase’s implementation of Quantile Regression DQN (QR-DQN)\n[31, 13]. We choose to multiply the QR-DQN component of the CQL loss by 0.2. Using the notation\n21\nproposed by Kumar et al. [31], this is equivalent to α = 5, which said work also uses. Other\nhyperparameters are τ = 1, γ = 0.99, fixed learning rate of 1e −4, 100 epochs, and 50 quantiles (no\nexploration hyperparameters are specified, since we do not generate any new online data).\nThe policy architecture used for Habitat experiments are the same as those used for PPO, though the\nfinal network outputs quantile Q-values for each action (rather than just a distribution over actions).\nThe action with the highest mean quantile value is chosen at evaluation time.\nDuring training, we shuffle the data and load full offline trajectories until the buffer has at least\n32×1024 = 32768 transitions or all trajectories have been loaded once that epoch. We then uniformly\nsample and train on batches of size 512 transitions from the buffer until each transition has been\ntrained on once in expectation (e.g., ∼number of transitions in the buffer\n512\nbatches). Each batch is used for 8\ngradient steps before the next is sampled. We choose this data loading scheme to fit the training\ninfrastructure provided by Stable-Baselines3 while not using up too much memory at once.\nD.3\nExperiments and Results\nOur primary comparison is once again between our promptable representations and general-purpose\nnon-promptable ones. We thus repeat the baseline described previously for Minecraft in Section 4.1,\ntraining a single agent for all three ObjectNav tasks using both PR2L and the VLM image encoder\nrepresentations. We empirically note that longer visual embedding sequences tend to perform better in\nHabitat. To control for this, we opt to use InstructBLIP’s Q-Former unprompted embeddings instead\nof the ViT embeddings directly (which are much longer than PR2L’s embedding sequences). As\nInstructBLIP uses the former representations to extract visual features to be projected into language\nembedding space, this serves to close the gap in embedding sequence length between our two\nconditions while still providing us with general visual features that the VLM processes via prompting.\nIn this case, we use the same InstructBLIP model as the Minecraft experiments and choose “What\nroom is this?” as our task-relevant prompt.\nAverage\nToilet\nBed\nSofa\nTarget Object\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nSuccess Rates Per Category\nAverage\nToilet\nBed\nSofa\nTarget Object\n0\n2\n4\n6\n8\n10\nAverage Returns\nAverage Returns Per Category\nPR2L (ours)\nVLM Image Encoder Baseline\nFigure 5: Offline RL performance of PR2L and baselines in Habitat ObjectNav. Plots show final evaluation\nsuccess rates and average returns per target object and overall. PR2L outperforms the baseline in all cases.\nWe report evaluation success rates and average returns for the simplified Habitat ObjectNav setting in\nFigure 5. PR2L achieves nearly double the average success rate of the baseline (60.4% vs. 35.2%),\nsupporting the hypothesis that PR2L works especially well when exploration is not needed. Lastly, in\nAppendix H.2, we find that PR2L causes the VLM to produce highly structured representations that\ncorrelate with an expert policy’s value function: high-value states are typically labeled by the VLM\nas being from a room where one would expect to find the target object.\nE\nExtended Discussion of Tasks and Results\nE.1\nNotes on Task-specific Systems\nWe designed experiments to specifically investigate the use of VLM embeddings as task-specific\npromptable representations for downstream sensorimotor policy learning. As such, we compare with\n22\nTask\nPR2L (Ours)\nVLM Image Encoder\nAblations\nNo Prompt\nNo Generation\nChange Aux. Text\nOracle Detector\nCombat Spider\n97.6 ± 14.9\n51.2 ± 9.3\n72.6 ± 14.2\n66.6 ± 11.8\n80.1 ± 12.6\n58.0 ± 13.4\nMilk Cow\n223.4 ± 35.4\n95.2 ± 18.7\n116.6 ± 25.9\n160.2 ± 23.6\n80.5 ± 17.8\n178.4 ± 42.5\nShear Sheep\n37.0 ± 4.4\n23.0 ± 3.6\n23.8 ± 3.2\n26.1 ± 4.5\n27.8 ± 4.6\n27.4 ± 9.3\nTable 8: Minecraft ablations, VLM image encoder baseline, and our full approach. All achieve worse\nperformance than PR2L. Values are final IQM success counts and intervals are the standard error.\nother works that propose or evaluate either learning from scratch or from pre-trained representations,\nbut not to systems in Minecraft and Habitat that require domain-specific engineered systems beyond\njust policy learning (such as Luo et al. [41], Zhu et al. [80]) or which target learning or producing\nhigher-level plans or abstractions (such as Wang et al. [70]).\nSuch comparisons are not made as these works either aim to investigate other problems in control or\nare aiming to develop highly specialized and task-specific systems (whereas we present a general\napproach for policy learning). For instance, Voyager shows how an LLM can reason about and\ncompose high-level hand-crafted control primitives [69]. Voyager’s ability to complete harder tasks\ncomes from its access to powerful hand-crafted high-level primitives that extensively leverage oracle\ninformation, which are composed into skills by GPT-4 (which does not handle any low-level control).\nSaid hand-coded control primitives used in Voyager are very advanced and do much of the heavy-\nlifting. In particular, Voyager gives GPT-4 access to a dedicated killMob(<entity name>) control\nprimitive function. This function calls a separate bot.pvp.attack(<entity>) (hand-written)\nfunction, which calls a hard-coded oracle pathfinder, aiming controller, and attack function to\nrepeatedly approach and attack the specified entity until it is defeated. Thus, for Voyager, the skill for\nhunting sheep simply fills in the powerful killMob() primitive function with “sheep” as the target,\nabstracting away all low-level control via the oracle hand-written controllers.\nVitally, unlike PR2L, Voyager does not investigate how to use (V)LMs to learn these primitives. It\nthus cannot be applied to settings that lack such primitives (e.g., because oracle path planners are\nnot available, like in Habitat). This makes PR2L complementary: we directly learn a policy to link\nobservations to low-level actions (turning, moving, attacking, etc) via RL with no oracle information,\nwhile Voyager aims to compose pre-existing primitives into skills via LLMs.\nE.2\nNotes on Dreamer v3\nWe note that PR2L just proposes to use VLMs as a source of task-specific representations for RL\ntasks; it does not prescribe which learning algorithm to use. Therefore, in principle, one could\nuse Dreamer in conjunction with PR2L and gain benefits from both the VLM representation and\nthe choice of a strong model-based RL algorithm. However, while we leave this to future works,\nour Minecraft comparison (c) measures how well the approach does on our Minecraft tasks (as the\noriginal paper focuses more on the component subtasks involved in the find diamond task, all of\nwhich do not involve interacting with moving entities).\nWe find that Dreamer v3 is unable to learn our six tasks given the same number of environment interac-\ntions that PR2L+PPO was trained on. We hypothesize that this is due to its visual reconstruction-based\nworld model not being suited for tasks requiring interaction with partially-observable, non-stationary\nautonomous entities (which all our tasks involve). We note that the last two rows of the figure\nvisualizing model reconstructions in the original Dreamer v3 paper shows that its world model\nfails to reconstruct an observed pig [20], supporting our hypothesis. This highlights the need for\nrobust representations that are conducive to world model learning, with PR2L’s capabilities to elicit\ntask-relevant visual semantic features via prompting being one possibility for doing so.\nF\nAblations\nWe run four ablations on combat spider, milk cow, and shear sheep to isolate and understand the\nimportance of various components of PR2L. First, we run PR2L with no prompt to see if prompting\nwith task context actually tailors the VLM’s generated representations favorably towards the target\ntask, improving over an unprompted VLM. Note that this is not the same as just using the image\nencoder (comparison (a)), as this ablation still decodes through the VLM, just with an empty prompt.\nSecond, we run PR2L with our chosen prompt, but no generation of text – i.e., the policy only\n23\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nCombat\nSpider\nMilk\nCow\nShear\nSheep\nBehavior Cloning Success Rate\nPR2L (ours)\nVLM Image Encoder Baseline\nFigure 6: Success rates for BC on either PR2L or VLM image encoder baseline representations for\nall original tasks. PR2L excels at combat spider, even after the policy is trained for a single epoch.\nreceives the embeddings associated with the image and prompt (the left and middle red groupings\nof tokens in Figure 2, but not the right-most group). This tests the hypothesis that representations\nof generated text might make certain task-relevant features more salient: e.g., the embeddings for\n“Is there a cow in this image?”, might not encode the presence of a cow as clearly as if the VLM\ngenerates “Yes” in response, impacting downstream performance. Third, to check if our prompt\nevaluation strategy provides a good proxy for downstream task performance while tuning prompts\nfor P2RL, we run PR2L with alternative prompts that were not predicted to be the best, as per our\ncriterion in Appendix A. We thus remove the auxiliary text from the prompt for combat spider and\nadd it for milk cow and shear sheep. Lastly, to see if PR2L embeddings are just better due to them\nencoding entity detection, we train a VLM image encoder policy with an additional ground truth\noracle target entity detector as a feature.\nResults from these additional experiments are presented in Table 8. In general, all ablations perform\nworse than PR2L. For milk cow, we note the most performant ablation is no generation, perhaps\nbecause the generated text is often wrong; among the chosen prompts, it yields the lowest true\npositive and negative rates for classifying the presence of its corresponding target entity (see Table 5\nin Appendix A), though adding auxiliary text makes it even worse, perhaps explaining why milk cow\nexperienced the largest performance decrease from adding it back in. Based on these overall trends,\nwe conclude that (i) the promptable and generative aspects of VLM representations are important for\nextracting good features for control tasks and (ii) our simple evaluation scheme is an effective proxy\nmeasure of how good a prompt is for PR2L.\nG\nMinecraft Behavior Cloning Experiments\nWe collected expert policy data by training a policy on MineCLIP embeddings to completion on all\nof our original tasks and saving all transitions to create an offline dataset. We then embedded all\ntransitions with either PR2L or the VLM image encoder. Finally, we train policies with behavior\ncloning (BC) on successful trajectories under a specified length (300 for combat spider, 250 for milk\ncow, and 500 for shear sheep) from either set of embeddings for all three tasks, then evaluate their\ntask success rates.\nResults are presented in Figure 6. We first note that, since the expert data was collected from a policy\ntrained on MineCLIP embeddings, the shear sheep policy is not very effective (as we found in Table\n2). Both resulting shear sheep BC policies are likewise not very performant. We find that combat\nspider in particular shows a very large gap in performance: the PR2L agent achieves approximately\ntwice the success rate of the VLM image encoder agent after training for just a single epoch. The\ncomparatively small amount of training and data necessary to achieve near-expert performance for\nthis task supports our hypothesis that promptable representations from general-purpose VLMs do\nnot help with exploration (they work better in offline cases, where exploration is not a problem), but\ninstead are particularly conducive to being linked to appropriate actions even though the VLM is not\nproducing actions itself. Further investigation of this hypothesis is presented in Appendix H.\n24\n0\n5\n10\n15\n20\n10\n5\n0\n5\n10\n15\nPC2\nCombat Spider PR2L Reps PCA\nMovement\nAttack\n15\n10\n5\n0\n5\n10\n15\n15\n10\n5\n0\n5\n10\n15\nMilk Cow PR2L Reps PCA\nMovement\nUse\n20\n15\n10\n5\n0\n5\n10\n15\n10\n5\n0\n5\n10\n15\nShear Sheep PR2L Reps PCA\nMovement\nUse\n15\n10\n5\n0\n5\n10\nPC1\n15\n10\n5\n0\n5\n10\nPC2\nCombat Spider Instruction Reps PCA\nMovement\nAttack\n15\n10\n5\n0\n5\n10\n15\nPC1\n10\n5\n0\n5\n10\n15\nMilk Cow Instruction Reps PCA\nMovement\nUse\n15\n10\n5\n0\n5\n10\n15\nPC1\n10\n5\n0\n5\n10\n15\nShear Sheep Instruction Reps PCA\nMovement\nUse\nFigure 7: PCA of PR2L representations of observations from twenty episode rollouts of expert\npolicies in all three Minecraft tasks. Larger points correspond to transitions where the expert\nreceived > 0.1 reward. We vary the prompt to be either our task-relevant prompt or the RT-2-style\nbaseline instruction prompt. Our prompt’s representations are bi-modal, with the clusters on the left\ncorresponding to the VLM outputting “yes” (the entity is in view). We find that most functional\nactions (orange points) that yielded rewards are located in said clusters. Note that, since these expert\npolicies are trained on top of MineCLIP embeddings, the shear sheep policy is not very performant,\nas seen in Table 2.\nH\nRepresentation Analysis\nWhy do our prompts yield higher performance than one asking for actions or instruction-following?\nIntuitively, despite appropriate responses to our task-relevant prompts not directly encoding actions,\nthere should be a strong correlation: e.g., when fighting a spider, if the spider is in view and the\nVLM detects this, then a good policy should know to attack to get rewards. We therefore wish to\ninvestigate if our representations are conducive to easily deciding when certain rewarding actions\nwould be appropriate for a given task – if it is, then such a policy may be more easily learned by RL,\nwhich would explain PR2L’s improved performance over the baselines.\nH.1\nMinecraft Analysis\nTo investigate this, we use the embeddings of our offline data from the BC experiments (collected\nby training a MineCLIP encoder policy to high performance on all of our original three tasks, as\ndiscussed in Appendix G). We specifically look at the embeddings produced by a VLM when given\nour standard task-relevant prompts and when given the instructions used for our RT-2-style baseline.\nWe then perform principal component analysis (PCA) on the tokenwise average of all embeddings\nfor each observation, thereby projecting the embeddings to a 2D space with maximum variance.\nWe visualize these low-dimensional space in Figure 7 for the final 20 successful observations from\neach task, with the point colors of orange and blue respectively indicating whether the observation\nresults in a functional action (attack or use item) or movement (translation or rotation) by the expert\npolicy. Additionally, we enlarge points corresponding to when the agent received rewards in order to\nrecognize which actions aided in or achieved the task objective.\nWe find that our considered prompts resulted in a bimodal distribution over representations, wherein\nthe left-side cluster corresponds to the VLM outputting “yes (the entity is in view)” and the right-side\none corresponds to “no.” Additionally, observations resulting in functional actions that received\n25\n60\n50\n40\n30\n20\n10\n0\n10\nPC1\n40\n30\n20\n10\n0\n10\n20\n30\nPC2\nFind Toilet PR2L Reps PCA\n60\n50\n40\n30\n20\n10\n0\n10\n20\nPC1\n30\n20\n10\n0\n10\n20\n30\n40\nPC2\nFind Bed PR2L Reps PCA\n20\n10\n0\n10\n20\n30\n40\n50\nPC1\n40\n30\n20\n10\n0\n10\n20\n30\n40\nPC2\nFind Sofa PR2L Reps PCA\n2\n0\n2\n4\n6\n8\n0\n2\n4\n6\n8\n2\n0\n2\n4\n6\n8\nbedroom\nbathroom\nliving room\nhallway\ndining room\nkitchen\nfoyer\nstair\nlaundry\n30\n20\n10\n0\n10\n20\n30\n40\nPC1\n30\n20\n10\n0\n10\n20\n30\n40\nPC2\nFind Toilet Image Encoder Reps PCA\n30\n20\n10\n0\n10\n20\n30\n40\nPC1\n40\n30\n20\n10\n0\n10\n20\n30\nPC2\nFind Bed Image Encoder Reps PCA\n30\n20\n10\n0\n10\n20\n30\n40\nPC1\n30\n20\n10\n0\n10\n20\n30\n40\nPC2\nFind Sofa Image Encoder Reps PCA\n0\n2\n4\n6\n8\n2\n0\n2\n4\n6\n8\n0\n2\n4\n6\n8\nFigure 8: PCA of PR2L (above) and image encoder (below) representations of observations\nfrom thirty episode rollouts of expert policies in all Habitat tasks. The points’ colors correspond\nto their value under Habitat’s built-in oracle shortest path follower (a near-optimal policy). More\nyellow is better. Boxes correspond to points the VLM has labeled as a given household room, in\nresponse to the task prompt of “What room is this?” This analysis aligns with intuition: for find toilet,\nhigh value observations tend to be labeled as bathrooms (orange box), find bed’s tend to be labeled as\nbedrooms (blue), and find sofa’s are labeled as living rooms (red).\nrewards (large orange points in Figure 7) tend to be on the left-side (“yes”) cluster for representations\nelicited by our prompt, but are more widely distributed in the instruction prompt case, in agreement\nwith intuition. This is especially clear in the milk cow plot, wherein nearly all rewarding functional\nactions (using the bucket on the cow to successfully collect milk) are in the lower left corner.\nThis analysis supports that the representations yielded by InstructBLIP in response to our chosen style\nof prompts are more structured than representations from instructions. Such structure is useful in\nidentifying and learning rewarding actions, even when said actions were taken from an expert policy\ntrained on unrelated embeddings. This suggests that such representations may similarly be more\nconducive to being mapped to good actions via RL, which we observe empirically (as our prompt’s\nrepresentations yield more performant policies than the instructions for the RT-2-style baseline).\nH.2\nHabitat Analysis\nLikewise, we conduct a similar analysis on the Habitat data from our simplified setting. Specifically,\nwe wish to see if PR2L produces representations that are conducive to extracting the value function\nof a good policy. Since the chosen Habitat ObjectNav prompt is “What room is this?” we expect the\nstate representations to be clustered based on room categories. Intuitively, states corresponding to the\nroom one is likely to find the target object should have the highest values.\nAs shown in Figure 8, we thus used PCA to project expert trajectories’ PR2L and general image\nencoder state representations (generated with Habitat’s geodesic shortest path follower) to two\ndimensions, then colored each one based on their value under said near-optimal policy. We also\nplotted the mean and standard deviation of all points labeled as each room, visualizing them as\naxis-aligned bounding boxes. Note that each upper subplot in Figure 8 has a cluster of points far from\nall boxes. These correspond to the VLM generating nothing or garbage data with no room label.\nThis visualization qualitatively agrees with intuition. High value states tend to be grouped with the\nroom the corresponding target object is often found in: find toilet corresponds to bathrooms, find bed\nto bedrooms, and find sofa to living rooms. Comparatively, the general image encoder features do\nnot have such semantically meaningful groupings; all observations are clustered together and, within\n26\nthat single grouping, high-value observations are more spread out. This all supports the idea that\nprompting allows representations to take on structures that correlate well to value functions of good\npolicies.\nI\nCode Snippets\nWe provide some code snippets showcasing instantiations of PR2L.\nclass\nPolicy(torch.nn.Module):\ndef\n__init__(self , num_actions , tf_embed_dim =4096):\n\"\"\" Policy\nthat\naccepts\npromptable\nreps as input \"\"\"\nsuper ().__init__ ()\n# Project\ndown VLM embed\ndimensions\nself.embed_fc = torch.nn.Linear(tf_embed_dim , 1024)\n# Predict\nactions\nself.action_fc = torch.nn.Linear (1024 ,\nnum_actions)\n# Transformer\nlayer to condense\npromptable\nreps to 1 token\nself.transformer = torch.nn.Transformer(\n1024,\n1,\nnum_encoder_layers =1,\nnum_decoder_layers =1,\ndim_feedforward =1024 ,\nbatch_first=True ,\n)\nself.cls = torch.nn.Embedding (1, 1024)\n# cls tokens\ndef\nforward(self , x):\nseq , mask = x\nbs , traj_len , num_tokens , _ = seq.shape\n# [batch*traj_len , num tokens , token\nsize]\nseq = seq.reshape(bs * traj_len , num_tokens ,\n-1)\n# [batch*traj_len , num tokens]\nmask = mask.reshape(bs * traj_len , num_tokens)\n# Project\ndown\n# [batch*traj_len , num tokens , tf dim]\nseq = self.embed_fc(seq)\n# Get CLS\nembedding\ncls = self.cls(torch.zeros ([bs * traj_len , 1],\ndevice=seq.device , dtype=int))\n# Get\nsummary\nembedding\n# [batch*traj_len , 1, tf dim]\ncls_embed = self.transformer(\nseq ,\n# Encoder\ninput\ncls ,\n# Decoder\ninput\n# Apply\nmask\nsrc_key_padding_mask =mask ,\nmemory_key_padding_mask =mask ,\n)\n# [batch , traj_len , d_model]\ncls_embed = cls_embed.reshape(bs , traj_len ,\n-1)\n# Predict\nactions\n# [batch , traj_len , actions]\nreturn\nself.action_fc(cls_embed)\nListing 1: Example policy for PR2L.\ndef\nprocess_obs(model , processor , image , prompt , device , last_n =2):\n27\ninputs = processor(images=image , text=prompt , return_tensors =\"pt\")\n.to(device)\n# Generate\ntext in response to prompt and\nextract\nembeddings\noutputs = model.generate(\n**inputs ,\noutput_hidden_states =True ,\nreturn_dict_in_generate =True ,\n# Any other\ngeneration\nparameters (min\/max tokens , temp , etc)\n)\nhs = outputs[\"hidden_states \"]\n# Get image and prompt\ntoken\nembeds\n# Any\nadditional\nprocessing\nshould\nhappen\nhere (eg pooling of\nvisual\ntokens)\n# [last_n , num img + prompt tokens , tf_embed_dim]\nimage_and_prompt_embs = torch.cat(hs[0], dim =0)[-last_n :]\n# Get\ndecoded\ntoken\nembeds\n# [last_n , num\ndecoded\ntokens , tf_embed_dim]\ndec_embs = []\nfor dec_hs in hs [1:]:\n# [last_n , 1, tf_embed_dim]\ndec_hs = torch.cat(dec_hs , dim =0)[-last_n :]\ndec_embs.append(dec_hs)\n# [last_n , num\ndecoded\ntokens , tf_embed_dim]\ndec_embs = torch.cat(dec_embs , dim =1)\n# [last_n , num total\ntokens]\nseq_embs = torch.cat([ image_and_prompt_embs , dec_embs], dim =1)\ntf_embed_dim = seq_embs.shape [-1]\n# [bs=1, seq_len =1, last_n*num total tokens , tf_embed_dim ]\nseq_embs = seq_embs.reshape (1, 1, -1, tf_embed_dim)\nmask = torch.zeros(seq_embs [:-1], type=int)\nreturn\nseq_embs , mask\nListing 2: Example code for extracting promptable representations from a VLM.\n# Create VLM and\nprocessor (InstructBLIP , for\nexample)\nmodel = InstructBlipForConditionalGeneration . from_pretrained (\n\"Salesforce\/instructblip -vicuna -7b\"\n)\nprocessor = InstructBlipProcessor . from_pretrained (\"Salesforce\/\ninstructblip -vicuna -7b\")\n# Set device , can also\nchange\ndtype if desired\ndevice = \"cuda :0\"\nmodel = model.to(device)\n# Create env\nenv = ...\n# Create\npolicy. This can be trained\nvia RL or BC as needed.\npolicy = Policy(env.num_actions).to(device)\n# Define task -relevant\nprompt\nprompt = \"Would a toilet be found\nhere? Why or why not?\"\n# To predict an action , get an RGB obs from the env and\nprocess it\nwith the VLM\nobs = env.reset ()\nseq , mask = process_obs(model , processor , obs , prompt , device)\n28\n# Then , pass it through\nthe policy to get action\nlogits and step env\nact_logits = policy.forward ((seq , mask)).reshape(env.num_actions)\naction = torch.argmax(act_logits)\nobs , _, _, _ = env.step(action)\nListing 3: Example usage of the above function and policy.\nJ\nExtended Literature Review\nLearning in Minecraft. We now consider some current approaches for creating autonomous learning\nsystems for tasks in Minecraft. Such works highlight some of the difficulties prevalent in tasks\nin said environment. For instance, since Minecraft tasks take place in a dynamic open world, it\ncan be difficult for an agent to determine what goal it is attempting to reach and how close it is\nto reaching that goal. [10] tackles these issues by introducing and integrating a training scheme\nfor self-supervised goal-conditioned representations and a horizon predictor. [79] learns a model\nfrom visual observations to discriminate between expert state sequences and non-expert ones, which\nprovides a source of intrinsic rewards for downstream RL tasks (as it pushes the policy to learn\nto match the expert state distribution, which tend to be “good” states for accomplishing tasks in\nMinecraft).\nFoundation Models and Minecraft. Likewise, there has been much interest in applying foundation\nmodels – especially (V)LMs – to Minecraft tasks. [3] pretrains on large scale videos, which enabled\nthe first agent that could learn to acquire diamond tools (thereby completing a longstanding challenge\nin the MineRL competition [28]). LMs have subsequently also been used to produce graphs of\nproposed skills to learn or technology tree advancements to make in the form of structured language\n[48, 81, 75, 70]. Other works propose to use the LLM to generate actions or code submodules\ngiven textual descriptions of observations or agent states [69]. Finally, VLMs have been used\nlargely for language-conditioned reward shaping [19, 16]. In contrast, we use VLMs as a source\nof representations for learning of atomic tasks (as defined by [38]) that have pre-defined reward\nfunctions; the latter works can thus be used in conjunction with our proposed approach for tasks\nwhere these vision-language reward functions are appropriate.\n29\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Vision-Language Models Provide Promptable Representations for Reinforcement Learning.pdf"}
{"title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","authors":"Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, Zongqing Lu","summary":"One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https:\/\/github.com\/PKU-RL\/CLIP4MC.","url":"http:\/\/arxiv.org\/abs\/2303.10571v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2303.10571v2","published":1679203252000,"comment":"ECCV 2024","pdf_text":"Reinforcement Learning Friendly\nVision-Language Model for Minecraft\nHaobin Jiang1⋆, Junpeng Yue1⋆, Hao Luo1 ,\nZiluo Ding2, and Zongqing Lu1,2†\n1 School of Computer Science, Peking University\n2 Beijing Academy of Artificial Intelligence\n{haobin.jiang,zongqing.lu}@pku.edu.cn\nAbstract. One of the essential missions in the AI research community\nis to build an autonomous embodied agent that can achieve high-level\nperformance across a wide spectrum of tasks. However, acquiring or\nmanually designing rewards for all open-ended tasks is unrealistic. In\nthis paper, we propose a novel cross-modal contrastive learning frame-\nwork architecture, CLIP4MC, aiming to learn a reinforcement learn-\ning (RL) friendly vision-language model (VLM) that serves as an in-\ntrinsic reward function for open-ended tasks. Simply utilizing the sim-\nilarity between the video snippet and the language prompt is not RL-\nfriendly since standard VLMs may only capture the similarity at a coarse\nlevel. To achieve RL-friendliness, we incorporate the task completion\ndegree into the VLM training objective, as this information can as-\nsist agents in distinguishing the importance between different states.\nMoreover, we provide neat YouTube datasets based on the large-scale\nYouTube database provided by MineDojo. Specifically, two rounds of\nfiltering operations guarantee that the dataset covers enough essential\ninformation and that the video-text pair is highly correlated. Empiri-\ncally, we demonstrate that the proposed method achieves better perfor-\nmance on RL tasks compared with baselines. The code and datasets are\navailable at https:\/\/github.com\/PKU-RL\/CLIP4MC.\nKeywords: Dataset · Multimodal model · Reinforcement learning\n1\nIntroduction\nTraining reinforcement learning (RL) agents to perform complex tasks in a\nvision-based and open-ended world can be difficult. One main challenge is that\nmanually specifying reward functions in all open-ended tasks is unrealistic [3,21],\nespecially when we cannot access the internal configuration of the environment.\nIn addition, learning a reward model from human feedback is typically expensive.\nTo this end, MineDojo [8] has proposed an internet-scale, multi-modal knowl-\nedge base of YouTube videos to facilitate learning in an open-ended world. With\nthe advent of a such large-scale database, agents are able to harvest practical\n⋆Equal contribution. † Corresponding author.\narXiv:2303.10571v2  [cs.LG]  5 Aug 2024\n2\nH. Jiang et al.\nknowledge encoded in large amounts of media like human beings. Moreover, a\nvision-language model (VLM), MineCLIP [8], is proposed to utilize the internet-\nscale domain knowledge. In more detail, the learned correlation score between\nthe visual observation and the language prompt can be used effectively as an\nopen-vocabulary, massively multi-task reward function for RL training. There-\nfore, no further task-specific reward design is needed for open-ended tasks.\nHowever, the autonomous embodied agent requires the vision-language model\nto provide a more instructive correlation score. Given the partial observations,\ne.g., video snippet, and the language prompt that describes the task, the agent\nneeds to figure out two non-trivial matters to better evaluate the current state.\nOn the one hand, whether the target entities are present within its field of vision?\nMineCLIP tried to address this question. However, the alignment of texts and\nvideos in the YouTube database is totally a catastrophe, which impedes the\nlearning of VLM. On the other hand, what is the relationship between each video\nsnippet and the degree of completion of the task? Normally, in an open-ended\nworld, e.g. Minecraft, the agent explores first, then approaches and interacts\nwith the target object. In other words, the agent requires approaching the target\nobject before it takes trial-and-error, even for the target that needs to be kept\naway. Therefore, it is reasonable to make an assumption, namely, that the higher\nthe level of completion of the task, the closer the agent is to the targets in the\nvideo snippet. We also argue it is important to incorporate the level of completion\nof the task into the reward function.\nIn this paper, we first construct neat YouTube datasets to facilitate the learn-\ning of basic game concepts, mainly the correspondence between the videos and\ntexts. Though a large-scale database is provided by MineDojo, it contains signif-\nicant noise due to its nature as an online resource. In addition, MineDojo only\nclaims the training dataset is randomly sampled from the database, making it\nhard to reproduce. To overcome the catastrophic misalignment of video-text\npairs in the original database, we have done four steps of dataset processing to\nguarantee the dataset is clean. Firstly, transcript cleaning enhances the accuracy\nof transcripts and ensures they are complete sentences. Secondly, keyword filter-\ning ensures that the content of clips is relevant to the key entities in Minecraft,\nthereby facilitating the learning of basic game concepts. Thirdly, video partition-\ning and selection can handle the issue of scene transitions and thereby mitigate\ninterference from other extraneous information. Lastly, correlation filtering can\neffectively address the issue of mismatch between video clips and transcripts.\nWe also propose an upgraded vision-language model, CLIP4MC, to provide a\nmore RL-friendly reward function. In RL, simply utilizing the similarity between\nthe video snippet and the language prompt is not RL-friendly since MineCLIP\ntends to only capture the similarity at the entity level. In other words, VLM\ncan hardly reflect the relationship between each video snippet and the degree of\ncompletion of the task. However, this information can better help agents distin-\nguish the importance between similar states. To achieve this, we incorporate the\ndegree of task completion into the VLM training objective. In more detail, CLIP\nhas exhibited remarkable segmentation capabilities without fine-tuning. After we\nCLIP4MC\n3\nextend pre-trained MineCLIP with modifications inspired by MaskCLIP [35], it\ncan segment the specified object from the image and label the size of the tar-\nget shown in the corresponding video. Intuitively, the closer the agent is to the\ntarget, the larger the target size becomes. During the learning procedure, we\ndynamically control the degree of contrasting positive and negative pairs of in-\nstances based on the target size in this positive video sample. Thus, CLIP4MC\ncan render a more RL-friendly reward signal that instructs the agent to learn\ntasks faster. Our proposed method is trained on our YouTube dataset and eval-\nuated on MineDojo Programmatic tasks, including harvest, hunt, and combat\ntasks. Empirically, our results show that CLIP4MC can provide a more friendly\nreward signal for the RL training procedure.\nTo summarize, our contributions are as follows:\n– Open-sourced datasets: We provide two high-quality datasets. The first\none undergoes data cleaning (Sections 4.1 to 4.3) and global-level correlation\nfiltering (Section 4.4). The VLM trained on this dataset matches the perfor-\nmance of the officially released MineCLIP, which lacks a publicly available\ntraining set.\n– RL-friendly dataset: Our second dataset further incorporates local-level\ncorrelation filtering (Section 4.4), making it more suited for RL. The VLM\ntrained on this dataset outperforms that on the first dataset.\n– RL-friendly VLM: To better evaluate and leverage the advantages of our\nmore RL-friendly dataset, we introduce CLIP4MC, a novel method to train\na VLM that could improve downstream RL performance (Section 5.2).\n2\nRelated Work\nVideo-Text Retrieval. Video-text retrieval plays an essential role in multi-\nmodal research and has been widely used in many real-world web applications.\nRecently, the pre-trained models have dominated this line of research with no-\nticeable results on both zero-shot and fine-tuned retrieval. Especially, BERT [5],\nViT [6], and CLIP [25], are used as the backbones to extract the text or video\nembedding. The cross-modal embeddings are then matched with specific fusion\nnetworks to find the correct video-text pair.\nIn more detail, CLIP4Clip [20] proposes three different similarity modules to\ncalculate the correlation between video and text embeddings. HiT [19] performs\nhierarchical matching at two different levels, i.e. semantic level and feature level.\nNote that semantic level and feature level features are from the transformer\nnetwork’s higher and lower feature layers, respectively. Frozen [1] proposes a dual\nencoder architecture that utilizes the flexibility of a transformer visual encoder\nto train from images or video clips with text captions. Moreover, MDMMT [7]\nadopts several pre-training models as encoders and it shows the CLIP-based\nmodel performs the best. Therefore, our model follows this line of research by\nusing the pre-trained model, CLIP [25], to extract the feature embeddings.\nMinecraft for AI Research. As an open-ended video game with an egocentric\nvision, Minecraft is a noticeable and important domain in RL due to the nature\n4\nH. Jiang et al.\nof the sparse reward, large exploration space, and long-term episodes. Since\nthe release of the Malmo simulator [14] and later the MineDojo simulator [8],\nvarious methods have attempted to train agents to complete tasks in Minecraft\n[10, 12, 18, 30, 31]. Approaches such as model-based RL, hierarchical RL, goal-\nbased RL, and reward shaping have been adopted to alleviate the sparse reward\nand exploration difficulty for the agent in this environment.\nRecently, with the development of large language models (LLM) like GPT-\n4 [24], a series of methods leveraging LLMs for high-level planning in Minecraft\nhave been proposed [16, 22, 32, 34, 36]. These methods have demonstrated re-\nmarkable capabilities in guiding the agent to complete multiple complicated,\nlong-horizon tasks, such as mining diamonds. These LLMs play a crucial role\nin decision-making, determining the sequence of basic skills required to accom-\nplish specific tasks. Their effectiveness is due to their extensive knowledge about\nMinecraft, learned from the Internet, and their ability to reflect on real-time\nfeedback from the game environment.\nIn addition to the use of LLMs, recent research attempts to incorporate Inter-\nnet visual data into basic skill learning in Minecraft, beyond the traditional RL\nmethods. MineRL [11] collected 60M player demonstrations with action labels,\nmotivating some methods [16,29] based on behavior cloning. As well-labeled data\nis limited in quantity, MineCLIP [8] instead uses over 730K narrated Minecraft\nvideos without action labels from YouTube. It aims to learn a vision-language\nmodel providing auxiliary reward signals, utilizing the vast and diverse data\navailable on the Internet. Different from MineCLIP, VPT [2] uses action-labeled\ndata to train an inverse dynamic model to label 70K hours of Internet videos\nand then conduct behavior cloning.\nUnlike the existing approaches, which incorporate the human experience and\nrequire a large number of demonstrations with action labels to train the agent,\nour work follows the line of MineCLIP [8] and focuses on only using the data\nwithout action labels to assist agent learning in Minecraft, which is more friendly\nwith data collection and has the potential to scale in the future.\n3\nBackground\nMineDojo tasks. MineDojo [8] provides thousands of benchmark tasks, which\ncan be used to develop generally capable agents in Minecraft. These tasks can be\ndivided into two categories, Programmatic and Creative tasks. The former has\nground-truth simulator states to assess whether the task has been completed.\nThe latter, however, do not have well-defined success criteria and tend to be\nmore open-ended, but have to be evaluated by humans.\nWe mainly focus on Programmatic tasks since they can be automatically\nassessed. Specifically, MineDojo provides 4 categories of programmatic tasks, in-\ncluding Harvest, Combat, Survival, and Tech Tree, with 1581 template-generated\nnatural language goals to evaluate the agent’s different capabilities. Among these\ntasks, Survival and Tech Tree tasks are harder than Harvest and Combat tasks.\nCurrently, MineCLIP [8] only expresses promising potential in some Harvest and\nCLIP4MC\n5\n... at some point you will lose there\ntoo or also hearts there spider was\nlike being completely blown away I\ndon 't assume anything at ...\n... so here I'm with nick and we've\ngot look at my inventory and his in-\nventory looks similar this is gonna\nbe interesting so four...\n...  give it  I'm a little chicken just\nlaying around by any chance a lot\nof arrows going on down here  I'm \nguessing most of  ...\n irrelavant video content\n mismatched video content\n matched video content\nFig. 1: Illustration of the YouTube video database. The screenshots of video clips\nare on the left and key entities are circled in red. The corresponding transcript clips\nare on the right and key entities are marked in red. We give examples of irrelevant,\nmismatched, and matched video content in the YouTube video database.\nCombat tasks. Harvest means finding, obtaining, cultivating, or manufacturing\nhundreds of materials and objects. Combat means fighting various monsters and\ncreatures that require fast reflexes and martial skills.\nPOMDP. We model the programmatic task as a partially observable Markov\ndecision process (POMDP) [15]. At each timestep t, the agent obtains the partial\nobservation ot from the global state s_t and a language prompt G, takes action\na_{t} following its policy \\pi (a_{t}|o_{t}), and receives a reward r_ { t} = \\Phi (V_{t},G), where Vt\nis the fixed-length sequence of observations till t (thus a video snippet) and \\Phi \nmaps V_t and G to a scalar value. Then the environment transitions to the next\nstate s_{t+1} given the current state and action according to transition probability\nfunction \\ protect \\ mathcal  {T}(s_{t+1}|s_t,a_t). The agent aims to maximize the expected return R =\n\\m\nat\nhbb  {E}_\\pi \\sum _{t=1}^{T}\\gamma ^{t-1}r_{t}, where \\gamma is the discount factor and T is the episode time horizon.\n4\nYouTube Dataset\nThe Internet is rich in Minecraft-related data, containing a wealth of weakly la-\nbeled or even unlabeled Minecraft knowledge, including crucial entities, plausible\nactions, and common-sense event processes. With these multi-modal data, it is\npossible to create dense language-conditioned rewards, making open-ended long-\nterm task learning feasible. MineDojo [8] collected over 730K YouTube videos\nand their corresponding transcripts, totaling 33 years of video content and 2.2B\nwords in transcripts. Around 640K video clips are selected using keyword-based\nfiltering from these videos, and these clips are used to train MineCLIP. However,\nMineDojo open-sourced a 13.8M dataset, and the 640K video clips were ran-\ndomly sampled in their paper. However, as illustrated in Figure 1, most videos\n6\nH. Jiang et al.\nfeature irrelevant game content that is not conducive to learning basic game\nconcepts. Meanwhile, the alignment between the transcripts and videos may not\nalways be precise, leading to temporal or content discrepancies that could hin-\nder the learning of retrieval and RL tasks. Given the low quality of the data, we\nfound it necessary to provide a neat 640k dataset to the community for the train-\ning usage. To address these issues, we adopt the following few steps to acquire\na clean dataset with high quality.\n4.1\nTranscript Cleaning\nDownloading YouTube’s automatic transcripts directly can lead to several issues.\nFirstly, transcript blocks may overlap, resulting in overlapping timestamps in the\ntranscript. Additionally, the caption quality is typically mediocre, with a higher\noccurrence of misidentifications, especially in non-English videos. Moreover, the\ntranscript lacks punctuation, making it less friendly for understanding semantics.\nBased on the aforementioned issues, we implement a pipeline to construct high-\nquality transcripts as follows: (1) Extract audio from the videos and use Whisper\n[26] to obtain high-quality, temporally non-overlapping transcripts. (2) Employ\nFullStop [9] to generate punctuation, resulting in complete sentences of 10-35\nwords in length.\n4.2\nKeyword Filtering\nFollowing MineDojo [8], we also implement keyword-based filtering to ensure\nthat the content in our dataset is pertinent to the key entities in Minecraft,\nthereby facilitating the learning of basic game concepts. As essential components\nof Minecraft, the key entities, such as stones, trees, and sheep, are common across\nmultiple tasks in MineDojo and videos from YouTube. Specifically, we identify\nentity keywords in the transcripts using a keyword list from MineDojo and ex-\ntract transcript clips formed into sentences to encompass as many keywords as\npossible. These extracted transcript clips then serve as the textual component\nof our dataset, determining the location of corresponding video clips.\n4.3\nVideo Partition and Selection\nAfter completing the previous two steps, we obtain transcript clips relevant to\nthe keywords. For each transcript clip, we calculate the central timestamp that\ncorresponds to the clip based on the transcript timestamps and then use this\ncentral timestamp to extract a video clip with a duration of D seconds from\nthe video. This process allows us to obtain temporally-aligned video clips. How-\never, the video clips obtained in this manner still exhibit some issues, including\nscene transitions and discrepancies between video content and transcripts. We\nhandle the former problem through video partition and filtering, while the latter\nproblem is addressed in Section 4.4.\nCLIP4MC\n7\nFig. 2: Examples of how we estimate the size of key entities in video frames. Red\nbounding boxes are generated by our modified MineCLIP visual encoder, following the\napproach proposed in MaskCLIP [35]. These boxes are then used to calculate the size.\nOwing to the informal nature of YouTube content, there is often a lack of\nsemantic congruence between the video clips and their corresponding transcrip-\ntions, as noted in VideoCLIP [33]. Moreover, video clips frequently contain a\nfew different behaviors which cause scene transitions, e.g. Chopping down the\ntree first, then suddenly switching the inventory bar. Since some scene transi-\ntions lead to irrelevant information, we partition the video content into several\nsemantically coherent segments based on the semantic structure of the video.\nThen we select the segment that aligns best with the transcript.\nTo achieve semantic partition of video content, we employ the Bellman k-\nsegmentation algorithm [13]. This algorithm divides a sequence of data points\ninto k distinct and constant-line segments, providing a piece-wise constant ap-\nproximation of the data. To process the video, we first use the officially re-\nleased MineCLIP [8] video encoder to obtain the embedding of each frame, since\nMineCLIP can capture video semantics to some extent. Subsequently, we parti-\ntion these embeddings into k segments and select the segment with the highest\nsimilarity score, as calculated in MineCLIP.\n4.4\nCorrelation Filtering\nWe employ correlation filtering techniques to address disparities that persist\nbetween video content and transcripts. The correlation filtering is done at two\nlevels, the global and the local levels. From the global level, we calculate the co-\nsine similarities between video embeddings and text embeddings via the original\nMineCLIP and then select clips based on these similarities.\nRecent research [17, 35] has demonstrated that CLIP [25], though trained\non whole images, can generate meaningful local features. Inspired by this and\nfollowing MaskCLIP [35], we make modifications to the original MineCLIP visual\nencoder, empowering it with the ability to estimate and label the size of the key\n8\nH. Jiang et al.\nentity, which is mentioned in the corresponding transcript, in each frame of a\nvideo clip without fine-tuning. Then the correlations between video clips and\ntranscripts are calculated as the summation of the sizes across all frames in each\nvideo clip. We consider this correlation at the local level.\nBased on these two criteria, we select the top k% of clips, resulting in the final\ntraining set. We provide an elaboration on our implementation in Appendix A\nand illustrate some examples in Figure 2.\nThe aforementioned four-step approach creates a dataset consisting of 640K\nvideo-text clip pairs, with an additional 4K pairs extracted for validation of\nvideo-text retrieval. Regarding the constants of the approach, D and k are set\nto 16 and 50, respectively. Therefore, our dataset comprises videos with a total\nduration of one week and approximately 0.16B words, significantly smaller in\nscale compared to the original low-quality 13.8M dataset. Importantly, we will\nopen-source our entire YouTube dataset, serving as an upgraded version of the\nunreleased MineCLIP training data.\n5\nCLIP4MC\nGiven a video snippet V and a language prompt G, the vision-language model\noutputs a correlation score, C, that measures the similarity between the video\nsnippet and the language prompt. Ideally, if the agent performs behaviors fol-\nlowing the description of the language prompt, the vision-language model will\ngenerate a higher correlation score, leading to a higher reward. Otherwise, the\nagent will be given a lower reward.\n5.1\nArchitecture\nWe follow the same architecture design as MineCLIP [8], including a video en-\ncoder, a text encoder, and a similarity calculator. All the video frames first go\nthrough the spatial transformer to obtain a sequence of frame features. The tem-\nporal transformer is then utilized to summarize the sequence of frame features\ninto a single video embedding. An adapter further processes the video embedding\nfor better features. Refer to Appendix B for more details about the architecture.\nEmpirically, we found that the video encoder (essentially MineCLIP) can\nprovide a bond between the entities and the language prompts and give a similar\nhigh reward as long as the target entities are present in the video frames, similar\nto observations in [4]. However, such a reward is not instructive enough for RL\ntasks since it does not reflect the behavioral trends of agents.\n5.2\nContrastive Training\nWe aim to minimize the sum of the multi-modal contrastive losses [23], including\nvideo-to-text, and text-to-video:\n  \\\nb\negin {aligne\nd\n} \\ mathcal  {L }  = & -\\sum  _{\n(\nz_v,z_m,z_t)\\in \\mathcal {B}}\\Big (\\log {\\rm NCE}(z_v,z_t)+\\log {\\rm NCE}(z_t,z_v) \\Big ), \\end {aligned} \n(1)\nCLIP4MC\n9\n░\nImage \nEncoder\nText\nEncoder\nv1\nt1\nv2\nv3\nvN\nt2\nt3\ntN\nv1·t1\nv2·t2\nv3·t3\nvN·tN\nv1·t2 v1·t3\nv1·tN\nv2·t3\nv2·t1\nv2·tN\nRandom Swap\nNo Swap\nRandom \nSwap\nRandomly choose a sample \nRandom Swap\nRandom Swap\nNo Swap\nRandom Swap\nswap\nmax ReLU 1(\n)\nb\nb\nH W\nP\nP\nHW\n\n=\n\n−\nswap\nP\nP\n\nswap\nP\nP\n\nv3·t1\nvN·t1\nv3·t2\nvN·t2\nv3·tN\nvN·t3\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\nPull\nPush\nText Emb.\nNeg. Video Emb.\nPos. Video Emb.\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\n... at some point you will \nlose there too or also \nhearts there spider was \nlike being completely \nblown away ...\nVideo Clip (16 seconds)\nTranscript (10-35 words)\nFig. 3: Illustration of CLIP4MC training. The upper part shows the concept of con-\ntrastive learning, while the lower part explains the swapping operation.\nwhere \\protect \\mathcal  {B} is the batch containing sampled video-text pairs, and {\\rm N CE}(\\cdot ,\\cdot ) is the\ncontrastive loss that calculates the similarity of two inputs. To illustrate, the\nvideo-to-text contrastive loss is given by\n  {\\rm NCE }\n(z_v,z _ t)\n = \\\nf\nrac {\n\\ exp\n ( z_v \\c d ot z_t^{+}\/\\lambda )}{\\sum _{z \\in \\{z_t^{+},z_t^{-}\\}} \\exp (z_v \\cdot z\/\\lambda )}, \n(2)\nwhere \\lambda is a temperature hyperparameter, z_\nt^{+} \nis the positive text embedding\nmatching with the video embedding z_v, and \\if\nm mode \\lbrace \\else \\textbraceleft \\fi z_t^{-}\\} are negative text embeddings\nthat are implicitly formed by other text clips in the training batch. Contrastive\nloss of text-to-video is defined in the same way.\nIn addition, we also need to incorporate the degree of task completion into\nthe training objective. Specifically, we hope the similarity score between video\nclip and text embeddings could reflect the task completion degree, i.e., higher\ncompletion leads to higher similarity. The sizes of key entities provided by the\nlocal correlation filtering serve as a surrogate for the task completion degree.\nInitially, we try to dynamically adjust the weight of the positive pairs based on\nthe size of the target object. However, it did not work. Essentially, positive and\nnegative pairs will still be separated, even with smaller sample weights.\nTherefore, we instead forcibly change the labels of the positive and negative\nvideo samples, as illustrated in Figure 3, thereby reducing the model’s confidence\nin certain positive pairs. Specifically, during training, a positive video sample is\nswapped with a random negative video sample from the \\protect \\mathcal  {B} based on probability\n10\nH. Jiang et al.\nPswap. This swap only occurs when the labeled target size HbWb in the positive\nvideo sample is below a certain threshold τ, and the smaller the size, the greater\nthe probability,\n  P_{ \\ rm s w ap} \n=\n P _{\\r\nm m\na\nx} *{\\rm ReLU} \\left ( 1- \\frac {H_b W_b}{\\tau HW}\\right ), \\label {equ:clip4mc} \n(3)\nwhere HW represents the size of the image.\nSince contrastive learning brings positive samples closer and pushes nega-\ntive samples away, converting a positive pair to a negative one with a certain\nprobability, i.e., the aforementioned random swap, during training will lower\nthe similarity score. As the decrease in similarity is directly proportional to the\nswapping probability, we set Pswap to be inversely related to task completion,\nensuing similarity score increases as completion increases.\nIn addition, the upper limit Pmax in Equation (3) is set to 0.5, as the swap\nshould not disrupt severely the normal distinction between positive and negative\npairs in most cases. This constraint enables CLIP4MC to preserve the under-\nstanding capability of MineCLIP for general behaviors that may lack explicit\ntarget entities in Minecraft, as discussed in Appendix F.\n5.3\nRL Training\nFor RL training, the first step is reward generation. At timestep t, we concatenate\nthe agent’s latest 16 egocentric RGB frames in a temporal window to form a\nvideo snippet, V_t. CLIP4MC outputs the probability P_{G,t} that calculates the\nsimilarity of V_t to the task prompt, G, against all other negative prompts. To\ncompute the reward, we further process the raw probability as previous work\n[8]r_t = \\max ( P_{G,t} - 1 \/ N_t,0) r\n, where N_t is the number of prompts passed to\nCLIP4MC. Note that CLIP4MC can handle unseen language prompts without\nany further fine-tuning due to the open-vocabulary ability of CLIP [25].\nThe ultimate goal is to train a policy network that takes as input raw pixels\nand other structural data and outputs discrete actions to accomplish the task\nthat is described by the language prompt, G. We use PPO [28] as our RL training\nbackbone and the policy is trained on the CLIP4MC reward together with the\nsparse task reward if any. The policy input contains several modality-specific\ncomponents and more details can be found in Appendix D.\n6\nExperiments\nIn this section, we comprehensively evaluate and analyze our proposed model\nCLIP4MC, utilizing the open-ended platform MineDojo [8], which comprises\nthousands of diverse, open-ended Minecraft tasks designed for embodied agents.\nWe compare CLIP4MC against two baselines: (1) MineCLIP [official], the\nofficially released MineCLIP model [8]. (2) MineCLIP [ours], using the same\narchitecture as MineCLIP [official] but trained on our cleaned YouTube dataset.\nIt also serves as the ablation of CLIP4MC without the swap operation. We train\nCLIP4MC and MineCLIP [ours] for 20 epochs and select the models with the\nCLIP4MC\n11\nhighest performance on RL tasks. Please refer to Appendix C for more training\ndetails. All results are presented in terms of the mean and standard error of four\nruns with different random seeds.\n6.1\nEnvironment Settings\nWe conduct experiments on eight Programmatic tasks, comprising two harvest\ntasks: milk a cow and shear wool, two combat tasks: combat a spider and combat\na zombie, and four hunt tasks: hunt a cow, hunt a sheep, hunt a pig, and hunt a\nchicken. These tasks are all built-in tasks in the MineDojo benchmark.\nHarvest. Milk a cow requires the agent to obtain milk from a cow with an\nempty bucket. Similarly, shear wool requires the agent to obtain wool from a\nsheep with shears. A harvest task is terminated and considered completed when\nthe target item is obtained by the agent with a specified quantity. The prompts\nused to calculate the reward is “obtain milk from a cow in plains with an empty\nbucket”; for shear wool, it is “shear a sheep in plains with shears”.\nCombat. In these tasks, target animals, spiders, and zombies, are hostile and\nwill actively approach and attack the agent. The agent’s goal is to fight and kill\nthe target animal. The prompt for each combat task is “combat a spider\/zombie\nin plains with a diamond sword”.\nHunt. Hunt tasks consist of hunt a cow, hunt a sheep, hunt a pig, and hunt a\nsheep. For each task, the agent’s goal is to kill the target animal as indicated\nin the task name. Different from combat tasks, the target animals will flee from\nthe agent after being attacked. Therefore, these tasks require the agent to keep\nchasing and attacking the target, making them challenging. As noted in [4],\nthe original MineCLIP reward fails in these tasks since it cannot consistently\nincrease when the agent approaches the agent. This observation aligns with our\nassertion that the original MineCLIP model can hardly capture the degree of\ntask completion. The prompt for each task is “hunt a {target} in plains with a\ndiamond sword” where {target} is replaced with the corresponding animal name.\nMore elaborated introduction of the Minecraft environment and settings of\nthese tasks are available in Appendix D. To guarantee a fair comparison, we\nadopt the same RL hyperparameters for all models and tasks. These hyperpa-\nrameters are listed in Appendix E.\n6.2\nRL Results\nThrough our evaluation of CLIP4MC, MineCLIP [ours], and MineCLIP [official]\nacross eight Minecraft tasks, we want to answer two key questions:\n(1) Whether the YouTube dataset we constructed enables MineCLIP, when\ntrained on it, to provide a more effective reward signal for task learning?\n(2) Whether our upgraded model, CLIP4MC, based on our YouTube dataset,\noffers a reward signal that is further friendly for the RL training procedure?\n12\nH. Jiang et al.\nTable 1: Success rates (%) of RL trained with rewards provided by different models\non eight Minecraft tasks. Each mean and standard error of success rates are calculated\non four models after training 1e6 environment steps with different random seeds.\nModels\nHarvest\nCombat\nmilk a cow\nshear wool\ncombat a spider combat a zombie\nCLIP4MC\n84.5±2.0\n74.6±2.1\n85.8±0.9\n70.4±8.3\nMineCLIP[ours]\n84.4±1.1\n71.6±3.5\n75.4±10.1\n63.6±8.7\nMineCLIP[official]\n84.1±0.5\n73.2±1.8\n82.7±2.5\n57.4±3.7\nModels\nHunt\nhunt a cow\nhunt a sheep\nhunt a pig\nhunt a chicken\nCLIP4MC\n39.8±2.5\n45.9±7.2\n30.6±8.4\n26.1±3.5\nMineCLIP[ours]\n17.3±10.6\n33.0±18.1\n14.1±10.6\n15.3±10.6\nMineCLIP[official]\n11.6±11.1\n28.5±16.7\n1.5±0.6\n0.0±0.0\nThese questions are central to verifying the effectiveness of our dataset and\nCLIP4MC model in Minecraft. Table 1 shows the success rates of all methods\non the eight Minecraft tasks.\nIt is noticeable that, in four hunt tasks, three models demonstrate varying\nperformance on RL. Firstly, MineCLIP [ours] consistently achieves better results\ncompared to MineCLIP [official] across all hunt tasks. The superior performance\nof MineCLIP [ours] provides a positive answer to our first question, suggesting\nthat the YouTube dataset we construct indeed enhances the effectiveness of the\nreward signal in MineCLIP for task learning. Note that this is the dataset we\nplan to release for better training the VLM model for RL tasks on Minecraft.\nSecondly, CLIP4MC shows significantly higher success rates on hunt tasks com-\npared to both MineCLIP [ours] and MineCLIP [official], meaning that the answer\nto the second question is also positive. As CLIP4MC provides a reward signal\ntaking into account a surrogate for the degree of task completion, i.e., the size of\nthe target object in our implementation, it becomes more RL-friendly in these\nchallenging tasks.\nIn addition, we notice a practical example of misalignment in the officially\nreleased MineCLIP model. Specifically, we observe that in hunt a chicken, the\nagent trained with MineCLIP [official] tends to keep looking at the sky, indicat-\ning such behavior can provide a high intrinsic reward. This phenomenon suggests\nthat the officially released MineCLIP indeed suffers from the misalignment prob-\nlem in the YouTube dataset. In contrast, our MineCLIP [official] shows promising\nbehaviors on this task, demonstrating that our dataset processing improves the\nalignment between the transcripts and videos.\nIn contrast to hunt tasks, these methods perform comparably on harvest\ntasks and combat tasks, while CLIP4MC still shows marginal advantage over the\nCLIP4MC\n13\n4\n3\n2\n1\nln(size)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n(a) CLIP4MC\n4\n3\n2\n1\nln(size)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n(b) MineCLIP [ours]\n4\n3\n2\n1\nln(size)\n0.0\n0.1\n0.2\n0.3\n0.4\nReward\n(c) MineCLIP [official]\nFig. 4: Scatter plots illustrating the relationship between the entity size and the in-\ntrinsic reward. The red line indicates a linear fit to the data.\nother two methods. This finding aligns with the results reported in [8], where the\nMineCLIP reward already achieves saturated performance on these easy tasks.\nTherefore, further enhancements in the reward signal, like MineCLIP [ours] and\nCLIP4MC, do not significantly improve performance. Unlike hunt tasks, neither\nharvest nor combat tasks require the agent to take multiple rounds of chasing.\nThis result does not detract from the superiority of CLIP4MC, evidenced by its\nperformance on hunt tasks, which are considered more difficult [2].\n6.3\nReward Analysis\nTo quantitatively verify that CLIP4MC captures the size of the target entity\nspecified in the language prompt, we collect 5000 steps in task hunt a cow and\napply the method described in Appendix A to estimate the maximal size of the\ncow in consecutive 16 frames. Then we use CLIP4MC, MineCLIP [ours], and\nMineCLIP [official] to calculate intrinsic rewards respectively. Before calculating\nthe correlation between the size and intrinsic rewards, we transform the size\nvalue using f(x) = ln (x + e−2), focusing on smaller values. The relationship\nbetween the transformed size and intrinsic rewards is visualized in Figure 4. The\ncorresponding Pearson correlation coefficients from left to right are 0.81, 0.66,\nand 0.62, indicating that CLIP4MC reward has a higher correlation with the\nsize, especially when it is relatively small. This is crucial in RL, as the agent\nneeds dense and distinguishing reward signals to guide the learning process,\nparticularly when the target is distant. Such characteristic is the essential benefit\nof CLIP4MC in RL.\n6.4\nAblation on Dataset Filtering\nTo evaluate the influence of the dataset quality on RL training and verify the\neffectiveness of our proposed correlation filter at the local level, we compare\nMineCLIP [ours] with two ablations: (1) MineCLIP [w\/o LCF] is trained\non the dataset processed with only global correlation filtering, omitting the lo-\ncal correlation filtering. (2) MineCLIP [RS] is trained on 640K video clips\nrandomly sampled from the MineDojo released database, consistent with their\nstated dataset construction method [8]. Note that methods evaluated here do not\napply random swap introduced in Section 5.2 since these datasets cannot provide\n14\nH. Jiang et al.\nTable 2: Success rates (%) of RL trained with rewards provided by different models on\nthree Minecraft tasks. Each mean and standard error of success rates are calculated on\nfour models after training 5e5 and 1e6 environment steps with different random seeds.\nModels\nmilk a cow\nshear wool\nhunt a cow\n5e5\n1e6\n5e5\n1e6\n5e5\n1e6\nours\n73.1±0.9\n84.4±1.1\n47.5±6.3\n71.6±3.5\n15.6±8.7\n17.3±10.6\nw\/o LCF\n71.3±1.8\n80.7±1.1\n53.8±3.3\n73.6±1.5\n0.7±0.7\n12.1±12.0\nRS\n65.1±3.9\n81.7±1.0\n21.7±6.5\n68.5±3.6\n1.0±0.5\n4.0±2.6\nofficial\n69.8±1.3\n84.1±0.5\n47.0±13.5\n73.2±1.8\n2.7±1.1\n11.6±11.1\nentity size. The results are presented in Table 2. Given the overall performance\nacross all three presented tasks, firstly, [RS] does not perfectly reproduce the\nperformance of [official], suggesting that misalignment in the original database\nhinders the reproduction of [official]. Secondly, part of our proposed data filter-\ning makes the model, [w\/o LCF], comparable to [official]. In addition, with all\nof our proposed data filtering techniques, [ours] outperforms [official].\n6.5\nVideo-Text Retrieval Results\nTable 3: Results of video-to-text \/ text-to-video re-\ntrieval on the test set. The best results are high-\nlighted in bold.\nModels\nR@1\nR@5\nR@10\nours\n12.4\/13.1\n27.5\/27.8\n35.3\/35.9\nw\/o LCF 12.7\/14.1 29.4\/29.7 37.9\/37.5\nRS\n11.1\/11.6\n24.8\/25.0\n32.4\/32.2\nTable 3 shows the results\nof video-to-text retrieval and\ntext-to-video on the test set\nwith\nthe\nsame\nMineCLIP\nmodel for a fair comparison.\nWe train these models for 20\nepochs and select the models\nwith the highest R@1 value\non the test set, respectively.\nFrom the results, the model\ntrained on the randomly sam-\npled dataset gets the worst\nperformance than those trained on our YouTube dataset. The results demon-\nstrate that our neat dataset indeed can facilitate the learning of basic game\nconcepts. A notable observation is that [ours] achieves higher success rates on\nRL tasks but lower performance on retrieval tasks compared to [w\/o LCF], sug-\ngesting that the video-text alignment objective does not fully align with RL\nrequirements. [w\/o LCF] is trained on the dataset filtered only by global-level\ncorrelation, which is directly conducted at the video level. In contrast, [ours] is\ntrained on the dataset further filtered by local-level correlation, which is object-\ncentric designing for RL training. Note that [w\/o LCF] is the dataset we plan\nto release for better training the VLM model for video-text retrieval tasks on\nMinecraft.\nCLIP4MC\n15\n7\nConclusion\nWe construct a neat YouTube dataset based on the large-scale YouTube database\nprovided by MineDojo. Moreover, we introduce a novel cross-modal contrastive\nlearning framework architecture, CLIP4MC, to learn an RL-friendly VLM that\nserves as an intrinsic reward function for open-ended tasks. Our findings suggest\nthat our dataset enhances the acquisition of fundamental game concepts and\nCLIP4MC delivers a more effective reward signal for RL training.\nAcknowledgements\nThis work was supported by NSFC under grant 62250068. The authors would\nlike to thank the anonymous reviewers for their valuable comments.\nReferences\n1. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In: IEEE\/CVF International Conference\non Computer Vision (ICCV) (2021) 3\n2. Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton,\nB., Sampedro, R., Clune, J.: Video pretraining (VPT): learning to act by watching\nunlabeled online videos. arXiv preprint arXiv:2206.11795 (2022) 4, 13\n3. Baumli, K., Baveja, S., Behbahani, F., Chan, H., Comanici, G., Flennerhag, S.,\nGazeau, M., Holsheimer, K., Horgan, D., Laskin, M., et al.: Vision-language models\nas a source of rewards. arXiv preprint arXiv:2312.09187 (2023) 1\n4. Cai, S., Wang, Z., Ma, X., Liu, A., Liang, Y.: Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. arXiv\npreprint arXiv:2301.10034 (2023) 8, 11\n5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018) 3\n6. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInternational Conference on Learning Representations (ICLR) (2021) 3\n7. Dzabraev, M., Kalashnikov, M., Komkov, S., Petiushko, A.: MDMMT: multido-\nmain multimodal transformer for video retrieval. In: IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) Workshops (2021) 3\n8. Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A.,\nHuang, D.A., Zhu, Y., Anandkumar, A.: Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In: Neural Information Processing Systems\n(NeurIPS) Datasets and Benchmarks Track (2022) 1, 2, 4, 5, 6, 7, 8, 10, 13, 18,\n20, 21, 22, 23, 25\n9. Guhr, O., Schumann, A.K., Bahrmann, F., Böhme, H.J.: Fullstop: Multilingual\ndeep models for punctuation prediction (June 2021), http:\/\/ceur-ws.org\/Vol-\n2957\/sepp_paper4.pdf 6\n16\nH. Jiang et al.\n10. Guss, W.H., Codel, C., Hofmann, K., Houghton, B., Kuno, N., Milani, S., Mohanty,\nS., Liebana, D.P., Salakhutdinov, R., Topin, N., et al.: Neurips 2019 competition:\nthe minerl competition on sample efficient reinforcement learning using human\npriors. arXiv preprint arXiv:1904.10079 (2019) 4\n11. Guss, W.H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., Salakhutdi-\nnov, R.: Minerl: A large-scale dataset of minecraft demonstrations. In: International\nJoint Conference on Artificial Intelligence (IJCAI) (2019) 4\n12. Hafner, D., Pasukonis, J., Ba, J., Lillicrap, T.P.: Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104 (2023) 4\n13. Haiminen, N., Gionis, A., Laasonen, K.: Algorithms for unimodal segmentation\nwith applications to unimodality detection. Knowledge and information systems\n14, 39–57 (2008) 7\n14. Johnson, M., Hofmann, K., Hutton, T., Bignell, D.: The malmo platform for arti-\nficial intelligence experimentation. In: International Joint Conference on Artificial\nIntelligence (IJCAI) (2016) 4\n15. Kaelbling, L.P., Littman, M.L., Cassandra, A.R.: Planning and acting in partially\nobservable stochastic domains. Artificial intelligence 101(1-2), 99–134 (1998) 5\n16. Kanervisto, A., Milani, S., Ramanauskas, K., Topin, N., Lin, Z., Li, J., Shi, J., Ye,\nD., Fu, Q., Yang, W., Hong, W., Huang, Z., Chen, H., Zeng, G., Lin, Y., Micheli,\nV., Alonso, E., Fleuret, F., Nikulin, A., Belousov, Y., Svidchenko, O., Shpilman,\nA.: Minerl diamond 2021 competition: Overview, results, and lessons learned. arXiv\npreprint arXiv:2202.10583 (2022) 4\n17. Li, Y., Wang, H., Duan, Y., Li, X.: Clip surgery for better explainability with\nenhancement in open-vocabulary tasks. arXiv preprint arXiv:2304.05653 (2023) 7\n18. Lin,\nZ.,\nLi,\nJ.,\nShi,\nJ.,\nYe,\nD.,\nFu,\nQ.,\nYang,\nW.:\nJuewu-mc:\nPlaying\nminecraft with sample-efficient hierarchical reinforcement learning. arXiv preprint\narXiv:2112.04907 (2021) 4\n19. Liu, S., Fan, H., Qian, S., Chen, Y., Ding, W., Wang, Z.: Hit: Hierarchical trans-\nformer with momentum contrast for video-text retrieval. In: IEEE\/CVF Interna-\ntional Conference on Computer Vision (ICCV) (2021) 3\n20. Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An\nempirical study of CLIP for end to end video clip retrieval and captioning. Neuro-\ncomputing 508, 293–304 (2022) 3, 21\n21. Ma, Y.J., Liang, W., Wang, G., Huang, D.A., Bastani, O., Jayaraman, D., Zhu,\nY., Fan, L., Anandkumar, A.: Eureka: Human-level reward design via coding large\nlanguage models. arXiv preprint arXiv:2310.12931 (2023) 1\n22. Nottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y., Hajishirzi, H., Singh, S.,\nFox, R.: Do embodied agents dream of pixelated sheep?: Embodied decision making\nusing language guided world modelling. arXiv preprint arXiv:2301.12050 (2023) 4\n23. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748 (2018) 8\n24. OpenAI: Gpt-4 technical report (2023) 4\n25. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,\nAskell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable\nvisual models from natural language supervision. In: International Conference on\nMachine Learning, ICML (2021) 3, 7, 10, 18\n26. Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever, I.: Robust\nspeech recognition via large-scale weak supervision (2022). https:\/\/doi.org\/10.\n48550\/ARXIV.2212.04356, https:\/\/arxiv.org\/abs\/2212.04356 6\nCLIP4MC\n17\n27. Schulman, J., Moritz, P., Levine, S., Jordan, M., Abbeel, P.: High-dimensional\ncontinuous\ncontrol\nusing\ngeneralized\nadvantage\nestimation.\narXiv\npreprint\narXiv:1506.02438 (2015) 23\n28. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 (2017) 10, 23\n29. Shah, R., Wang, S.H., Wild, C., Milani, S., Kanervisto, A., Goecks, V.G., Way-\ntowich, N.R., Watkins-Valls, D., Prakash, B., Mills, E., Garg, D., Fries, A., Souly,\nA., Chan, J.S., del Castillo, D., Lieberum, T.: Retrospective on the 2021 minerl\nBASALT competition on learning from human feedback. In: Neural Information\nProcessing Systems (NeurIPS) Competitions and Demonstrations Track (2021) 4\n30. Shu, T., Xiong, C., Socher, R.: Hierarchical and interpretable skill acquisition in\nmulti-task reinforcement learning. In: International Conference on Learning Rep-\nresentations (ICLR) (2018) 4\n31. Tessler, C., Givony, S., Zahavy, T., Mankowitz, D., Mannor, S.: A deep hierarchi-\ncal approach to lifelong learning in minecraft. In: AAAI conference on artificial\nintelligence (AAAI) (2017) 4\n32. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., Anand-\nkumar, A.: Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291 (2023) 4\n33. Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettle-\nmoyer, L., Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot\nvideo-text understanding. arXiv preprint arXiv:2109.14084 (2021) 7\n34. Yuan, H., Zhang, C., Wang, H., Xie, F., Cai, P., Dong, H., Lu, Z.: Plan4mc: Skill\nreinforcement learning and planning for open-world minecraft tasks. arXiv preprint\narXiv:2303.16563 (2023) 4\n35. Zhou, C., Loy, C.C., Dai, B.: Extract free dense labels from clip. In: European\nConference on Computer Vision. pp. 696–712. Springer (2022) 3, 7, 18\n36. Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L.,\nWang, X., et al.: Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory.\narXiv preprint arXiv:2305.17144 (2023) 4\n18\nH. Jiang et al.\nA\nDataset Details\nA.1\nKey Entity Size Estimation\nMaskCLIP [35] has shown notable zero-shot segmentation ability of CLIP [25]\nwithout fine-tuning. Given that the architecture and training loss of MineCLIP\n[8] are similar to those of CLIP, we hypothesize that MineCLIP could also\nachieve zero-shot segmentation. Therefore, we modify the vision transformer in\nMineCLIP following MaskCLIP, replacing the output of CLS token with value-\nembeddings of all patches in the final self-attention block. This change enables\nthe vision transformer to output embedding vectors for each image patch rather\nthan a single vector for the entire image. Since there is an additional temporal\ntransformer in MineCLIP visual encoder compared to CLIP, we pass the patch\nembedding vectors through this temporal transformer individually, ensuring the\npatch embeddings are aligned with MineCLIP’s embedding space. With an im-\nage input, we now acquire embedding vectors for each patch in MineCLIP’s\nembedding space. We also generate text embedding vectors for all entities in the\nMineDojo keyword list using the MineCLIP text encoder. The cosine similarities\nbetween each patch embedding vector and the text embedding vectors are then\ncalculated.\n(a) RGB image\n(b) Similarity scores\n(c) Filtering\n(d) Bounding box\nFig. 5: Procedure of generating a bounding box for the key entity. (a) The raw RGB\nimage containing the key entity “horse”. (b) The heatmap illustrating the similarity\nscores of “horse” on each patch. (c) The filtered heatmap. (d) The bounding box of\n“horse”.\nAs our goal is to estimate the size of the key entity mentioned in the corre-\nsponding transcript, we extract the similarity scores of the key entity on each im-\nage patch and create a heatmap for it. For instance, Figure 5b shows a heatmap\ngenerated for the entity “horse”. To refine this heatmap, we apply two filtering\ncriteria: firstly, a patch is retained only if the key entity’s similarity score on it\nis higher than that of any other entity in the keyword list; secondly, we ignore\npatches with similarity scores below a threshold, τpatch = 0.295, to reduce noise.\nAfter filtering, we find the maximum connected region in the filtered heatmap\nand compute the minimum bounding box that covers this region, as depicted in\nFigure 5d. This bounding box allows us to estimate the size of the key entity\nwithin the image, calculated as the area of the bounding box HbWb.\nA.2\nConstruction Process\nThe construction steps of the dataset are as follows:\nCLIP4MC\n19\n1. Obtain YouTube videos along with their transcripts and the keywords list\nfrom the MineDojo database.\n2. Use Transcript Cleaning (Section 4.1) to enhance the accuracy of transcripts\nand ensure they are complete sentences.\n3. For each video with a transcript, annotate all keywords (including different\nforms of keywords such as combined words, plural forms, etc.) appearing in\nthe transcript.\n4. Use Keyword Filtering (Section 4.2) to find all transcript sentences contain-\ning keywords. The length of sentences is adjusted to 10-35 words.\n5. Extract all non-overlapping transcript clips from each video with a transcript\nfollowing step 4.\n6. For each transcript clip, calculate the central timestamp corresponding to\nthe clip based on the transcript timestamps. Use this central timestamp to\nextract a video clip of duration D seconds from the video.\n7. From all the video-clip pairs extracted in the previous steps, extract M\npairs. Utilize Video Partition and Selection (Section 4.3) to select one from\np partitions, in order to handle the issue of scene transitions and thereby\nmitigate interference from other extraneous information.\n8. Use Correlation Filtering (Section 4.4) to select the top k% pairs with the\nhighest correlation from the M pairs as the training set. Specifically, we\nfirstly select pairs following the local correlation filtering criteria, i.e., the\nsize of target entity. If the number of video clips with explicit target en-\ntities is less than k%, we select the remaining training pairs based on the\nglobal correlation filtering criteria, i.e., the cosine similarity calculated via\nMineCLIP.\n9. Randomly select M ′ pairs in addition to the M pairs as the test set.\nThe parameters used in the above process are listed in Table 4. Following this\nprocess, we construct a training set of size 640K and a test set of size 4096. This\nis the dataset for better training the VLM for RL tasks on Minecraft. Our other\ndataset for better training the VLM for video-text retrieval tasks on Minecraft is\nonly filtered by the global correlation filtering criteria to select the top k% pairs\nin step 8. We have released the two datasets by specifying the transcript clips\nand the corresponding timestamps of the videos in the original database. The\nlink is https:\/\/huggingface.co\/datasets\/AnonymousUserCLIP4MC\/CLIP4MC.\nB\nArchitecture\nInput Details. The length of each transcript clip is 25 words, while the length\nof the video is 16 seconds. The resolution of the video stream is 160 × 256, with\n5 fps. In other words, the video stream is 80 frames. As for the video snippet,\nwe further equidistantly sample it to 16 frames for fewer computing resources.\nhttps:\/\/github.com\/MineDojo\/MineDojo\n20\nH. Jiang et al.\nTable 4: Values of parameters used in the dataset construction process.\nParameter\nValue\nD\n16\nM\n1,280,000\np\n3\nk\n50\nM ′\n4,096\nTable 5: Training hyperparameters for CLIP4MC.\nHyperparameter\nValue\nwarmup steps\n320\nlearning rate\n1.5e-4\nlr schedule\ncosine with warmup\nweight decay\n0.2\nlayerwise lr decay\n0.65\nbatch size per GPU\n100\nparallel GPUs\n4\nvideo resolution\n160 × 256\nnumber of frames\n16\nimage encoder\nViT-B\/16\nswap threshold τ\n0.02\nText Encoder Details. Referring to the design of MineCLIP [8], the text en-\ncoder is a 12-layer 512-width GPT model, which has 8 attention heads. The tex-\ntual input is tokenized via the tokenizer used in CLIP and is padded\/truncated\nto 77 tokens. The initial weights of the model use the public checkpoint of CLIP\nand only finetune the last two layers during training.\nSpatial Encoder Details. The Spatial encoder is a frame-wise image encoder\nreferred to the design of MineCLIP [8], which uses the ViT-B\/16 architecture to\ncompute a 512-D embedding for each frame. The initial weights of the model use\nthe public checkpoint of OpenAI CLIP, and only the last two layers are finetuned\nduring training.\nTemporal Transformer Details. The temporal Transformer is used to ag-\ngregate the temporal information from the spatial encoder, which is a 2-depth\n8-head Transformer module whose input dimension is 512 and maximum se-\nquence length is 32.\nAdapter Details. In order to get better embedding, we use an adapter to map\nvideo embedding and text embedding. The adapter models are 2-layer MLP,\nexcept the text adapter which is an identity.\nCLIP4MC\n21\nC\nCLIP4MC Training\nThe training process for CLIP4MC is adapted from the training processes for\nCLIP4Clip [20] and MineCLIP [8]. We trained all models on the 640K training\nset. For each video-text clip pair, we obtain 16 frames of RGB image through\nequidistant sampling and normalize each channel separately. During training,\nwe use random resize crops for data augmentation. We use cosine learning rate\nannealing with 320 gradient steps of warming up. We only fine-tune the last\ntwo layers of pre-trained CLIP encoders, and we apply a module-wise learning\nrate decay (learning rate decays along with the modules) for better fine-tuning.\nTraining is performed on 1 node of 4 × A100 GPUs with FP16 mixed precision\nvia the PyTorch native amp module. All hyperparameters are listed in Table 5.\nD\nEnvironment Details\nD.1\nEnvironment Initialization\nTable 6 outlines how we set up and initialize the environment for each task\nin our RL experiments. SR denotes the spawn range of animals, and Length\nrepresents the length of one episode. All tasks are in the biome plains.\nTable 6: Environment setup in our experiments.\nTask\nMob SR Length\nTool\nPrompt\nmilk a cow\ncow\n10\n200\nempty_bucket obtain milk from a cow in\nplains with a bucket\nshear wool\nsheep\n10\n200\nshears\nshear a sheep in plains with\nshears\ncombat a spider\nspider\n7\n500\ndiamond_sword combat a spider in plains with\na diamond sword\ncombat a zombie zombie\n7\n500\ndiamond_sword combat a zombie in plains with\na diamond sword\nhunt a cow\ncow\n7\n500\ndiamond_sword hunt a cow in plains with a di-\namond sword\nhunt a sheep\nsheep\n7\n500\ndiamond_sword hunt a sheep in plains with a\ndiamond sword\nhunt a pig\npig\n7\n500\ndiamond_sword hunt a pig in plains with a di-\namond sword\nhunt a chicken\nchicken\n7\n500\ndiamond_sword hunt a chicken in plains with\na diamond sword\n22\nH. Jiang et al.\nD.2\nObservation Space\nTo enable the creation of multi-task and continually learning agents that can\nadapt to new scenarios and tasks, MineDojo provides unified observation and\naction spaces [8]. Table 7 provides detailed descriptions of the observation space\nadopted in our experiments.\nTable 7: Observation space in our experiments.\nObservation\nDescriptions\nEgocentric RGB frame\nRGB frames provide an egocentric view of the run-\nning Minecraft client; The shape height (H) and\nwidth (W) are specified by argument image_size.\nIn our experiment, it is (160, 256).\nVoxels\nVoxels observation refers to the 3x3x3 surrounding\nblocks around the agent. This type of observation\nis similar to how human players perceive their sur-\nrounding blocks. It includes names and properties of\nblocks.\nLocation Statistics\nLocation statistics include information about the\nterrain the agent currently occupies. It also includes\nthe agent’s location and compass.\nBiome ID\nIndex of the environmental biome where the agent\nspawns.\nD.3\nAction Space\nWe simplify the original action space of MineDojo [8] into a two-dimensional\nmulti-discrete action space. The first dimension consists of 12 discrete actions:\nNO_OP, forward, backward, left, right, jump, sneak, sprint, camera pitch +30,\ncamera pitch -30, camera yaw +30, and camera yaw -30. The second dimension\ncontains NO_OP, attack, and use.\nE\nRL Training\nE.1\nAgent Network\nLike MineAgent [8], our policy framework is also composed of three components:\nan encoder for input features, a policy head, and a value head. In order to\ndeal with cross-modal observations, the feature extractor includes a variety of\nmodality-specific components:\nCLIP4MC\n23\nRGB frame. To optimize for computational efficiency and equip the agent with\nstrong visual representations from scratch, we use the fixed frame-wise image\nencoder ψI from CLIP4MC to process RGB frames.\nGoal of the task. The text embedding of the natural language task objective\n(prompt) is computed by ψG from CLIP4MC.\nYaw and Pitch. We first compute sin(·) and cos(·) features respectively, then\npass them through CompassMLP as described in Table 8.\nGPS. The position coordinates are normalized and passed through GPSMLP\nas described in Table 8.\nVoxels. To process the 3 × 3 × 3 voxels surrounding the agent, we embed\ndiscrete block names as dense vectors, flatten them, and pass them through\nVoxelEncoder as described in Table 8.\nPrevious action. Our agent relies on its immediate previous action, which is\nembedded and processed through Prev Action Emb as described in Table 8,\nwhich is a conditioning factor.\nBiomeID. To perceive the discrepancy in different environments, we embed\nBiomeID as a vector through an MLP named BiomeIDEmb as described in\nTable 8.\nThe features from each modality are combined by concatenation, passed\nthrough an additional feature fusion network as described in Table 8, and then\nfed into a GRU to integrate historical information. The output of GRU serves\nas the input for both the policy head and the value head. The policy head is\nmodeled using an MLP, as described in Table 8 for Actor & Critic Network,\nwhich transforms the input feature vectors into an action probability distribu-\ntion. Similarly, value MLP is used to estimate the value function, conditioned\non the same input features.\nE.2\nAlgorithm\nIn our experiments, we implement Proximal Policy Optimization (PPO) [28] as\nour base RL algorithm. In detail, PPO updates policies via\n  \\t h eta  _{\nk\n+1} = \\a rg \\m ax  _{ \\th eta } \\mathbb {E}_{s,a\\sim \\pi _{\\theta _k}}\\left [L(s,a,\\theta _k,\\theta )\\right ]. \nHere, L is defined as\n  L( s, a,\\ th e ta \n_ k,\\thet\na ) = \\min \\ lef t ( \\fra\nc  {\\pi _\n\\theta (a | s )} { \\ p\ni\n _{\\ the ta\n \n_k}(a|s)}A^{\\pi _{\\theta _k}}(s,a),\\operatorname {clip}\\left (\\frac {\\pi _\\theta (a|s)}{\\pi _{\\theta _k}(a|s)}, 1-\\epsilon , 1+\\epsilon \\right )A^{\\pi _{\\theta _k}}(s,a)\\right ), \nwhere ϵ is a clip value that limits the deviation between the new policy πθk+1\nand the old πθk. Aπθk (s, a) is the advantage function for the current policy and\nestimated via Generalized Advantage Estimation (GAE) [27]. Unlike MineDojo\n[8], our implementation does not include additional self-imitation learning and\n24\nH. Jiang et al.\nTable 8: Policy Network Details\nNetwork\nDetails\nActor & Critic Net\nhidden_dim: 256, hidden_depth: 2\nGRU\nhidden_dim: 256, layer_num: 1\nCompassMLP\nhidden_dim: 128, output_dim: 128, hidden_depth: 2\nGPSMLP\nhidden_dim: 128, output_dim: 128, hidden_depth: 2\nVoxelEncoder\nembed_dim: 8, hidden_dim: 128, output_dim: 128, hid-\nden_depth: 2\nBiomeIDEmb\nembed_dim: 8\nPrevActionEmb\nembed_dim: 8\nImageEmb\noutput_dim: 512\nFeatureFusion\noutput_dim: 512, hidden_depth: 0\naction smoothing techniques as we found that our adopted PPO is already able\nto achieve high performance. The hyperparameters of our PPO implementation\nare listed in Table 9 and shared across all experiments.\nTable 9: PPO hyperparameters in our experiments.\nHyperparameter\nValue\nnum steps\n1000\nnum envs\n4\nnum minibatches\n4\nnum epochs\n8\nGAE lambda\n0.95\ndiscounted gamma\n0.99\nentropy coef\n0.005\nclip epsilon\n0.02\nlearning rate\n1e-4\noptimizer\nAdam\nGRU data chunk length\n10\ngradient norm\n10.0\nE.3\nCoefficient of MineCLIP Reward\nIn our experiments, the agent is trained with the shaped reward defined as\nrt = renv\nt\n+ c · rmc\nt . Specifically, renv\nt\nrepresents the sparse reward provided by\nCLIP4MC\n25\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess rate\nCLIP4MC 1.0\nCLIP4MC 0.1\nCLIP4MC 0.01\n(a) CLIP4MC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess rate\nMineCLIP [ours] 1.0\nMineCLIP [ours] 0.1\nMineCLIP [ours] 0.01\n(b) MineCLIP [ours]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess rate\nMineCLIP [official] 1.0\nMineCLIP [official] 0.1\nMineCLIP [official] 0.01\n(c) MineCLIP [official]\nFig. 6: Learning curves of CLIP4MC, MineCLIP [ours], and MineCLIP [official] with\ndifferent reward coefficients on task milk a cow.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSuccess rate\nCLIP4MC 1.0\nCLIP4MC 0.1\nCLIP4MC 0.01\n(a) CLIP4MC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSuccess rate\nMineCLIP [ours] 1.0\nMineCLIP [ours] 0.1\nMineCLIP [ours] 0.01\n(b) MineCLIP [ours]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSuccess rate\nMineCLIP [official] 1.0\nMineCLIP [official] 0.1\nMineCLIP [official] 0.01\n(c) MineCLIP [official]\nFig. 7: Learning curves of CLIP4MC, MineCLIP [ours], and MineCLIP [official] with\ndifferent reward coefficients on task hunt a cow.\nthe environment. The environment will provide a +100 reward only when the\nagent successfully accomplishes the task; otherwise, the environment reward is\nzero. The other component, rmc\nt , denotes the MineCLIP reward introduced in\nSection 5.3. The coefficient c controls the weight of the MineCLIP reward. For RL\nalgorithms in sparse-reward environments, the coefficient of the intrinsic reward\nhas a significant influence on the final performance of algorithms. Therefore, we\nconduct preliminary experiments to decide a suitable coefficient of MineCLIP\nreward for our experiments in the paper. We choose two tasks, milk a cow and\nhunt a cow, as our testbed. As shown in Figures 6 and 7, three values, c = 1.0, c =\n0.1, and c = 0.01, are evaluated on CLIP4MC, MineCLIP [ours], and MineCLIP\n[official]. The results demonstrate that c = 0.1 achieves the best performance\nin 5 out of 6 experiments. Based on this observation, we set c = 0.1 for all\nexperiments in this paper.\nE.4\nLearning Curves\nFigure 8 shows the learning curves of all methods on eight Minecraft tasks. The\nresults in Table 1 are averaged on the last five checkpoints in Figure 8.\nF\nCreative Tasks\nAs defined in MineDojo [8], Creative Tasks is a unique task suite, distinct from\nProgrammatic Tasks due to the lack of success criteria. MineCLIP demonstrates\n26\nH. Jiang et al.\n0.70\n0.75\n0.80\n0.85\n0.90\nSuccess rate          \nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n(a) milk a cow\n0.60\n0.65\n0.70\n0.75\n0.80\nSuccess rate          \nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n(b) shear wool\n0.70\n0.75\n0.80\n0.85\n0.90\nSuccess rate          \nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n(c) combat a spider\n0.5\n0.6\n0.7\n0.8\nSuccess rate          \nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n(d) combat a zombie\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nSuccess rate\nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n(e) hunt a cow\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.2\n0.4\n0.6\n0.8\nSuccess rate\nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n(f) hunt a sheep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n0.2\n0.3\n0.4\nSuccess rate\nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n(g) hunt a pig\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps (1e6)\n0.0\n0.1\n0.2\n0.3\n0.4\nSuccess rate\nCLIP4MC\nMineCLIP [ours]\nMineCLIP [official]\n(h) hunt a chicken\nFig. 8: Learning curves of CLIP4MC, MineCLIP [ours], and MineCLIP [official] on\neight Programmatic tasks of MineDojo.\nits effectiveness in learning Creative Tasks, such as dig a hole and lay the carpet,\nand serving as a reliable success classifier. It is noticeable that Creative Tasks\noften lack clear indicators of task completion progress, unlike Programmatic\nTasks. In other words, our adopted approach, where the size of the key entity\nserves as a surrogate for task completion progress, is ineffective. For example, in\ndig a hole, it is not feasible to detect a hole and estimate its size before the agent\ncompletes this task. Therefore, the swap operation in CLIP4MC would bring no\nbenefit to Creative Tasks. However, due to the upper bound of swap probability\nPmax = 0.5 we set, CLIP4MC is expected to preserve the ability of MineCLIP to\nunderstand these creative behaviors without an explicit target entity that exists\nin the Minecraft world in advance.\nTo validate our hypothesis regarding the effectiveness of CLIP4MC in Cre-\native Tasks, we conduct experiments on dig a hole. We train agents using PPO\nwith intrinsic rewards calculated by CLIP4MC or MineCLIP [official]. At the\nbeginning of each episode, the agent is spawned in the biome plains with an\niron shovel. The prompt is “dig a hole” and each episode lasts 200 steps. Due to\nthe lack of success criteria for dig a hole, we run each agent for 100 episodes and\nrecord the average reward per step within each episode, calculated by MineCLIP\n[official]. The resulting reward distributions are shown in Figure 9. The result\nshows that the mean MineCLIP reward of the agent trained with CLIP4MC\nreward is lower than that of the agent trained with MineCLIP reward. This is\nconsistent with expectations, as the agent trained with MineCLIP reward should\noverfit it and thus achieve higher rewards. In addition, the MineCLIP reward of\nthe agent trained with CLIP4MC is significantly higher than that taking ran-\ndom actions, indicating that CLIP4MC successfully guides agents in learning\nbehaviors relevant to dig a hole.\nThe original MineCLIP paper proposes a successful classifier based on the\naverage MineCLIP reward. In detail, a trajectory with a average MineCLIP\nreward greater than the threshold τ is classified as successful. However, the\nCLIP4MC\n27\nspecific threshold values for each task are not provided. Therefore, we evaluate\nsuccess rates at various reward thresholds, as depicted in Figure 9. Considering\nthe high success rate (91.6%) reported for dig a hole in the original paper,\nwe speculate that the agent trained with CLIP4MC would also achieve high\nsuccess rates. This is supported by the narrow performance gap between the\nagent trained with MineCLIP and CLIP4MC rewards when the former’s success\nrate is high (around 90%), as shown in Figure 9.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nReward \/ Threshold\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRelative Frequency\nMineCLIP\nCLIP4MC\nRandom\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nFig. 9: Histogram & left y-axis. The distributions of MineCLIP reward for agents\ntrained with CLIP4MC and MineCLIP reward and the agent taking random actions.\nCurve & right y-axis. The success rates of agents with different reward thresholds.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Reinforcement Learning Friendly Vision-Language Model for Minecraft.pdf"}
{"title":"ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting","authors":"Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"Vision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. One critical issue is bridging the gap between discrete entities in\nlow-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve\nas high-level reasoners that break down tasks into executable sub-tasks,\ntypically specified using language. However, language suffers from the\ninability to communicate detailed spatial information. We propose\nvisual-temporal context prompting, a novel communication protocol between VLMs\nand policy models. This protocol leverages object segmentation from past\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, supported by real-time object\ntracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to\ntackle complex tasks that demand spatial reasoning. Experiments in Minecraft\nshow that our approach enables agents to achieve previously unattainable tasks,\nwith a $\\mathbf{76}\\%$ absolute improvement in open-world interaction\nperformance. Codes and demos are now available on the project page:\nhttps:\/\/craftjarvis.github.io\/ROCKET-1.","url":"http:\/\/arxiv.org\/abs\/2410.17856v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2410.17856v3","published":1729690019000,"comment":null,"pdf_text":"ROCKET-1: Mastering Open-World Interaction\nwith Visual-Temporal Context Prompting\nShaofei Cai1, Zihao Wang1, Kewei Lian1, Zhancun Mu1, Xiaojian Ma3, Anji Liu2 and Yitao Liang B1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis\nVision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied\ndecision-making in open-world environments presents challenges. One critical issue is bridging the\ngap between discrete entities in low-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners\nthat break down tasks into executable sub-tasks, typically specified using language. However, language\nsuffers from the inability to communicate detailed spatial information. We propose visual-temporal\ncontext prompting, a novel communication protocol between VLMs and policy models. This protocol\nleverages object segmentation from past observations to guide policy-environment interactions. Using\nthis approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual\nobservations and segmentation masks, supported by real-time object tracking from SAM-2. Our method\nunlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning.\nExperiments in Minecraft show that our approach enables agents to achieve previously unattainable\ntasks, with a 76% absolute improvement in open-world interaction performance. Codes and demos are\nnow available on the project page: https:\/\/craftjarvis.github.io\/ROCKET-1.\nFigure 1 | Our pipeline solves creative tasks, such as get the obsidian in the original Minecraft version, using the\naction space identical to human players (mouse and keyboard). We present a novel instruction interface, visual-\ntemporal context prompting, under which we learn a spatial-sensitive policy, ROCKET-1. VLMs identify regions\nof interest within each observation and guide ROCKET-1 interactions. Different colors in the segmentation\nrepresent distinct interaction types, for example,\n- use,\n- approach,\n- switch,\n- mine block.\n1. Introduction\nPre-trained foundation vision-language models\n(VLMs) (Achiam et al., 2023; Team et al., 2023)\nhave shown impressive performance in reason-\ning, visual question answering, and task planning\n(Brohan et al., 2023; Cheng et al., 2024; Driess\net al., 2023; Wang et al., 2023b), primarily due\nto training on internet-scale multimodal data. Re-\ncently, there has been growing interest in trans-\nCorresponding author(s): Yitao Liang\nShaofei Cai <caishaofei@stu.pku.edu.cn>, Zihao Wang <zhwang@stu.pku.edu.cn>, Kewei Lian <lkwkwl@stu.pku.edu.cn>, Zhancun Mu\n<muzhancun@stu.pku.edu.cn>, Xiaojian Ma <xiaojian.ma@ucla.edu>, Anji Liu <liuanji@cs.ucla.edu>, Yitao Liang <yitaol@pku.edu.cn>\narXiv:2410.17856v3  [cs.CV]  20 Mar 2025\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 2 | Different pipelines in solving embodied decision-making tasks. (a) End-to-end pipeline modeling\ntoken sequences of language, observations, and actions. (b) Language prompting: VLMs decompose instructions\nfor language-conditioned policy execution. (c) Latent prompting: maps discrete behavior tokens to low-level\nactions. (d) Future-image prompting: fine-tunes VLMs and diffusion models for image-conditioned control. (e)\nVisual-temporal prompting: VLMs generate segmentations and interaction cues to guide ROCKET-1.\nferring these capabilities to embodied decision-\nmaking in open-world environments. Existing\napproaches can be broadly categorized into (i)\nend-to-end and (ii) hierarchical approaches. End-\nto-end approaches, such as RT-2 (Brohan et al.,\n2023), Octo (Octo Model Team et al., 2024), LEO\n(Huang et al., 2023), and OpenVLA (Stone et al.,\n2023), aim to enable VLMs to interact with envi-\nronments by collecting robot manipulation trajec-\ntory data annotated with text. This data is then\ntokenized to fine-tune VLMs into vision-language-\naction models (VLAs) in an end-to-end manner,\nas illustrated in Figure 2(a). However, collecting\nsuch annotated trajectory data is difficult to scale.\nMoreover, introducing the action modality risks\ncompromising the foundational abilities of VLMs.\nHierarchical agent architectures typically con-\nsist of a high-level reasoner and a low-level pol-\nicy, which can be trained independently. In this\narchitecture, the “communication protocol” be-\ntween components defines the capability lim-\nits of the agent. Alternative approaches (Driess\net al., 2023; Wang et al., 2023a,b) leverage VLMs’\nreasoning abilities to zero-shot decompose tasks\ninto language-based sub-tasks, with a separate\nlanguage-conditioned policy executing them in\nthe environment, refer to Figure 2(b). However,\nlanguage instructions often fail to effectively con-\nvey spatial information, limiting the tasks agents\ncan solve. For example, when multiple homony-\nmous objects appear in an observation image, dis-\ntinguishing a specific one using language alone\nmay require extensive spatial descriptors, increas-\ning data collection complexity and learning dif-\nficulty for the language-conditioned policy. To\naddress this issue, approaches like STEVE-1 (Lif-\nshitz et al., 2023), GROOT-1 (Cai et al., 2023b),\nand MineDreamer (Zhou et al., 2024) propose us-\ning a purely vision-based interface to convey task\ninformation to the low-level policy. MineDreamer,\nin particular, uses hindsight relabeling to train an\nimage-conditioned policy (Lifshitz et al., 2023)\nfor interaction, while jointly fine-tuning VLMs\nand diffusion models to generate goal images\nthat guide the policy, shown in Figure 2(d). Al-\nthough replacing language with imagined images\nas the task interface simplifies data collection and\npolicy learning, predicting future observations re-\nquires building a world model, which still faces\nchallenges such as hallucinations, temporal incon-\nsistencies, and limited temporal scope.\nIn human task execution, such as object grasp-\ning, people do not pre-imagine holding an object\nbut maintain focus on the target object while\napproaching its affordance. When the object is\nobscured, humans rely on memory to recall its lo-\ncation and connect past and present visual scenes.\nThis use of visual-temporal context enables hu-\nmans to solve tasks effectively in novel environ-\nments. Building on this idea, we propose a novel\ncommunication protocol called visual-temporal\ncontext prompting, as shown in Figure 2(e). This\nallows users\/reasoners to apply object segmenta-\ntion to highlight regions of interest in past visual\n2\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nobservations and convey interaction-type cues via\na set of skill primitives. Based on this, we learn\nROCKET-1, a low-level policy that uses visual\nobservations and reasoner-provided segmenta-\ntions as task prompts to predict actions causally.\nSpecifically, a transformer (Dai et al., 2019) mod-\nels dependencies between observations, essen-\ntial for representing tasks in partially observable\nenvironments. As a bonus feature, ROCKET-1\ncan enhance its object-tracking capabilities dur-\ning inference by integrating the state-of-the-art\nvideo segmentation model, SAM-2 (Ravi et al.,\n2024), in a plug-and-play fashion. Additionally,\nwe propose a backward trajectory relabeling\nmethod, which efficiently generates segmenta-\ntion annotations in reverse temporal order us-\ning SAM-2, facilitating the creation of training\ndatasets for ROCKET-1. Finally, we develop a\nhierarchical agent architecture leveraging visual-\ntemporal context prompting, which perfectly in-\nherits the vision-language reasoning capabilities\nof foundational VLMs. Experiments in Minecraft\ndemonstrate that our pipeline enables agents to\ncomplete tasks previously unattainable by other\nmethods, while the hierarchical architecture ef-\nfectively solves long-horizon tasks.\nOur main contributions are threefold: (1) We\npresent visual-temporal context prompting, a\nnovel protocol that effectively communicates spa-\ntial and interaction cues in hierarchical agent ar-\nchitecture. (2) We learn ROCKET-1, the first\nsegmentation-conditioned policy in Minecraft, ca-\npable of interacting with nearly all the objects.\n(3) We develop backward trajectory relabel-\ning method that can automatically detect and\nsegment desired objects in collected trajectories\nwith pre-trained SAMs for training ROCKET-1.\n2. Preliminaries\nOffline Reinforcement Learning. We model the\nopen-world interaction problem as a Markov De-\ncision Process (MDP) ⟨O, A, P, C, M, R⟩, where\nO and A represent the observation and action\nspaces, P : O×A×O →ℝ+ describes the environ-\nment dynamics, C is the set of interaction types,\nand M is the segmentation mask space. The bi-\nnary reward function R : O×A ×C×M →{0, 1}\ndetermines whether the policy has completed the\nspecified interaction with the object indicated by\nthe segmentation mask at each time step. The\nobjective of reinforcement learning is to learn a\npolicy that maximizes the expected cumulative\nreward, 𝔼\n\u0002Í𝑇\n𝑡=1 𝑟𝑡\n\u0003\n, where 𝑟𝑡is the reward at time\nstep 𝑡. Our proposed backward trajectory relabel-\ning method ensures that each trajectory attains a\npositive reward based on current object segmen-\ntations. This allows us to discard the rewards and\nlearn a conditioned policy 𝜋(𝑎|𝑜, 𝑐, 𝑚) directly us-\ning behavior cloning. In the offline setting, agents\ndo not interact with the environment but rely on\na fixed, limited dataset of trajectories. This set-\nting is harder as it removes the ability to explore\nthe environment and gather additional feedback.\nVision Language Models. Vision-Language Mod-\nels (VLMs) are machine learning models capable\nof processing both image and language modal-\nities. Recent advances in generative pretrain-\ning have led to the emergence of conversational\nmodels like Gemini (Team et al., 2023), GPT-4o\n(Achiam et al., 2023), and Molmo (Deitke et al.,\n2024), which are trained on large-scale multi-\nmodal data and can reason and generate human-\nlike responses based on text and images. Models\nsuch as Palm-E (Driess et al., 2023) have demon-\nstrated strong abilities in embodied question-\nanswering and task planning. However, stan-\ndalone VLMs cannot often interact directly with\nenvironments. Some approaches use VLMs to\ngenerate language instructions for driving low-\nlevel controllers, but these methods struggle with\nexpressing spatial information. This work focuses\non releasing VLMs’ spatial understanding in em-\nbodied decision-making scenarios. Molmo can\naccurately identify correlated objects in images\nusing a list of (𝑥, 𝑦) coordinates, as demonstrated\nin https:\/\/molmo.allenai.org.\nSegment Anything Models. The Segment Any-\nthing Model (SAM, Kirillov et al. (2023)), intro-\nduced by Meta, is a segmentation model capa-\nble of interactively segmenting objects based on\npoint or bounding box prompts, or segmenting\nall objects in an image at once. It demonstrates\nimpressive zero-shot generalization in both real-\nworld and video game environments. Recently,\nMeta introduced SAM-2 (Ravi et al., 2024), ex-\n3\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 3 | ROCKET-1 architecture. ROCKET-1 processes observations (𝑜), object segmentations (𝑚), and\ninteraction types (𝑐) to predict actions (𝑎) using a causal transformer. Observations and segmentations are\nconcatenated and passed through a visual backbone for deep fusion. Interaction types and segmentations are\nrandomly dropped with a pre-defiened probability during training.\ntending segmentation to the temporal domain.\nWith SAM-2, users can prompt object segmen-\ntation with points or bounding boxes on a sin-\ngle video frame, and the model will track the\nobject forward or backward in time, refer to\nhttps:\/\/ai.meta.com\/sam2.\nRemarkably,\nSAM-2 continues tracking even if the object disap-\npears and reappears, making it well-suited for par-\ntially observable open-world environments. In ad-\ndition, we find the SAM models can be equipped\nwith a text prompt module, enabling them to\nground text-based concepts in visual images, as\nseen in grounded SAM (Liu et al., 2023).\n3. Methods\nOverview. Our work focuses on addressing com-\nplex interactive tasks in open-world environ-\nments like Minecraft. We leverage VLMs’ visual-\nlanguage reasoning capabilities to decompose\ntasks into multiple steps and determine object in-\nteractions based on environmental observations.\nFor example, the “build nether portal” task re-\nquires a sequence of block placements at spe-\ncific locations. A controller is also needed to\nmap these steps into low-level actions. To con-\nvey spatial information accurately, we propose a\nvisual-temporal context prompting protocol and\na low-level policy, ROCKET-1. Pretrained VLMs\nprocess a sequence of frames 𝑜1:𝑡and a language-\nbased task description to generate object segmen-\ntations 𝑚1:𝑡and interaction types 𝑐1:𝑡, represent-\ning the interaction steps. The learned ROCKET-1\n𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡, 𝑐1:𝑡) interprets these outputs to in-\nteract with the environment in real-time. In this\nsection, we outline ROCKET-1 ’s architecture\nand training methods, the dataset collection pro-\ncess, and a pipeline integrating ROCKET-1 with\nstate-of-the-art VLMs.\nROCKET-1 Architecture. To train ROCKET-\n1, we prepare interaction trajectory data in the\nformat: 𝜏= (𝑜1:𝑇, 𝑎1:𝑇, 𝑚1:𝑇, 𝑐1:𝑇), where 𝑜𝑡∈\nℝ3×𝐻×𝑊is the visual observation at time 𝑡, 𝑚𝑡∈\n{0, 1}1×𝐻×𝑊is a binary mask highlighting the ob-\nject in 𝑜𝑡for future interaction, 𝑐𝑡∈ℕdenotes\nthe interaction type, and 𝑎𝑡is the action. If both\n𝑚𝑡and 𝑐𝑡are zeros, no region is highlighted at\n𝑜𝑡. As shown in Figure 3, ROCKET-1 is formal-\nized as a conditioned policy, 𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡, 𝑐1:𝑡),\nwhich takes a sequence of observations and object-\nsegmented interaction regions to causally predict\nactions. To effectively encode spatial information,\ninspired by Zhang et al. (2023), we concatenate\nthe observation and object segmentation pixel-\nwise into a 4-channel image, which is processed\nby a visual backbone for deep fusion, followed by\nan self-attention pooling layer:\nℎ𝑡←Backbone([𝑜𝑡, 𝑚𝑡]),\n(1)\n𝑥𝑡←AttentionPooling(ℎ𝑡).\n(2)\nWe extend the input channels of the first convolu-\ntion in the pre-trained visual backbone from 3 to\n4, initializing the new parameters to 0s to mini-\nmize the gap in early training. A TransformerXL\n4\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 4 | Trajectory relabeling pipeline in Minecraft. A bounding box and point selection are applied to the\nimage center in the frame preceding the interaction event to identify the interacting object. SAM-2 is then run\nin reverse temporal order for a specified duration.\n(Baker et al., 2022; Dai et al., 2019) module is\nthen used to model temporal dependencies be-\ntween observations and incorporate interaction\ntype information to predict the next action ˆ𝑎𝑡:\nˆ𝑎𝑡←TransformerXL(𝑐1, 𝑥1, · · · , 𝑐𝑡, 𝑥𝑡).\n(3)\nWe delay the integration of interaction type infor-\nmation 𝑐𝑡until after fusing 𝑚𝑡and 𝑜𝑡, enabling the\nbackbone to share knowledge across interaction\ntypes and mitigating data imbalance. Behavior\ncloning loss is used for optimization. However,\nthis approach risks making 𝑎𝑡overly dependent\non 𝑚𝑡and 𝑐𝑡, reducing the model’s temporal rea-\nsoning capability. To address this, we propose\nrandomly dropping segmentations with a certain\nprobability, forcing the model to infer user intent\nfrom past inputs (visual-temporal context). The\nfinal optimization objective is:\nL = −\n|𝜏|\n∑︁\n𝑡=1\nlog 𝜋(𝑎𝑡|𝑜1:𝑡, 𝑚1:𝑡⊙𝑤1:𝑡, 𝑐1:𝑡⊙𝑤1:𝑡), (4)\nwhere 𝑤𝑡∼Bernoulli(1 −𝑝) represents a mask,\nwith 𝑝denoting the dropping probability, ⊙de-\nnotes the product operation over time dimension.\nBackward Trajectory Relabeling. We seek to\nbuild a dataset for training ROCKET-1. The col-\nlected trajectory data 𝜏typically contains only\nobservations 𝑜1:𝑇and actions 𝑎1:𝑇. To generate ob-\nject segmentations and interaction types for each\nframe, we propose a novel hindsight relabeling\ntechnique (Andrychowicz et al., 2017) combined\nwith an object tracking model (Ravi et al., 2024)\nfor automatic data labeling. We first abstract a\nset of interactions C and identify frames where\ninteraction events occur, detected using a pre-\ntrained vision-language model, such as Achiam\net al. (2023). Then, we traverse the trajectory in\nreverse order, segmenting interacting objects in\nframe 𝑡via an open-vocabulary grounding model,\nsuch as (Liu et al., 2023). Finally, SAM-2 (Ravi\net al., 2024) is used to track and generate seg-\nmentations for frames 𝑡−1, 𝑡−2, . . . , 𝑡−𝑘, where\n𝑘is the window length.\nFor Minecraft, we use contractor data (Baker\net al., 2022) from OpenAI, consisting of 1.6 billion\nframes of human gameplay. This dataset includes\nmeta information for each frame, recording inter-\naction events such as kill entity, mine block, use\nitem, interact, craft, and switch, eliminating the\nneed for vision-language models to detect events.\nWe observed that interacting objects are often\ncentered in the previous frame, allowing the use\nof a fixed-position bounding box and point with\nthe SAM-2 model for segmentation, replacing\nopen-vocabulary grounding models. We also in-\ntroduced an additional interaction type, navigate.\nIf a player’s movement exceeds a set threshold\nover a period, they are considered to be approach-\ning an object. The object they face in the seg-\nment’s final frame is marked as the target, with\nSAM-2 applied in reverse to identify it in earlier\nframes. As shown in Figure 4, the entire labeling\nprocess can be totally automated.\nIntegration with High-level Reasoner. Complet-\ning complex long-horizon tasks in open-world\n5\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 5 | A hierarchical agent structure based on our proposed visual-temporal context prompting. A\nGPT-4o model decomposes complex tasks into steps based on the current observation, while the Molmo model\nidentifies interactive objects by outputting points. SAM-2 segments these objects based on the point prompts,\nand ROCKET-1 uses the object masks and interaction types to make decisions. GPT-4o and Molmo run at low\nfrequencies, while SAM-2 and ROCKET-1 operate at the same frequency as the environment.\nTable 1 | Hyperparameters for training ROCKET-1.\nHyperparameter\nValue\nInput Image Size\n224 × 224\nVisual Backbone\nEfficientNet-B0 (4 channels)\nPolicy Transformer\nTransformerXL\nNumber of Policy Blocks\n4\nHidden Dimension\n1024\nTrajectory Chunk size\n128\nDropout Rate 𝑝\n0.75\nOptimizer\nAdamW\nLearning Rate\n0.00004\nenvironments requires agents to have strong com-\nmonsense knowledge and do visual-language rea-\nsoning, both of which are strengths of modern\nVLMs. As shown in Figure 5, we design a novel\nhierarchical agent architecture comprising GPT-\n4o (Achiam et al., 2023), Molmo (Deitke et al.,\n2024), SAM-2 (Ravi et al., 2024), and ROCKET-1.\nGPT-4o decomposes tasks into object interactions\nbased on an observation 𝑜𝑡−𝑘, leveraging its ex-\ntensive knowledge and reasoning abilities. Since\nGPT-4o cannot directly output the object masks,\nwe use Molmo to generate (𝑥, 𝑦) coordinates for\nthe described objects. SAM-2 then produces the\nobject mask 𝑚𝑡−𝑘from these coordinates and ef-\nficiently tracks objects 𝑚𝑡−𝑘+1:𝑡in subsequent ob-\nservations. ROCKET-1 uses the generated masks\n𝑚𝑡−𝑘:𝑡and interaction types 𝑐𝑡−𝑘:𝑡from GPT-4o to\nengage with the environment. Due to the high\ncomputational cost, GPT-4o and Molmo run at\nlower frequencies, while SAM-2 and ROCKET-1\noperate at the env’s frequency.\n4. Results and Analysis\nFirst, we provide a detailed overview of the exper-\nimental setup, including the benchmarks, base-\nlines, and implementation details. We then ex-\nplore ROCKET-1 ’s performance on basic open-\nworld interactions and long-horizon tasks. Finally,\nwe conduct comprehensive ablation studies to val-\nidate the rationale behind our design choices.\n4.1. Experimental Setup\nImplementation Details.\nBriefly, we present\nROCKET-1 ’s model architecture, hyperparam-\neters, and optimizer configurations in Table 1.\nDuring training, each complete trajectory is di-\nvided into 128-length segments to reduce mem-\nory requirements. During inference, ROCKET-1\ncan access up to 128 frames of past observations.\nMost training parameters follow the settings from\nprior works such as Baker et al. (2022); Cai et al.\n(2023b, 2024b).\nEnvironment and Benchmark. We use the un-\nmodified Minecraft 1.16.5 (Guss et al., 2019; Lin\net al., 2023) as our testing environment, which\naccepts mouse and keyboard inputs as the action\nspace and outputs a 640 × 360 RGB image as the\nobservation. To comprehensively evaluate the\nagent’s interaction capabilities, as shown in Fig-\nure 6, we introduce the Minecraft Interaction\nBenchmark, consisting of six categories and a\ntotal of 12 tasks, including Hunt, Mine, Interact,\nNavigate, Tool, and Place. This benchmark empha-\nsizes object interaction and spatial localization\n6\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 6 | A benchmark for evaluating open-world interaction capabilities of agents.\nThe benchmark\ncontains six interaction types in Minecraft, totaling 12 tasks. Unlike previous benchmarks, these tasks emphasize\ninteracting with objects at specific spatial locations. For example, in “hunt the sheep in the right fence,” the task\nfails if the agent kills the sheep on the left side. Some tasks, such as “place the oak door on the diamond block,”\nnever appear in the training set. It is also designed to evaluate zero-shot generalization capabilities.\nTable 2 | Results on the Minecraft Interaction benchmark. Each task is tested 32 times, and the average\nsuccess rate is reported as the final result. “Human” indicates instructions provided by a human.\nMethod\nPrompt\nHunt\nMine\nInteract\nNavigate\nTool\nPlace\nAvg\nVPT-bc\nN\/A\n0.13\n0.16\n0.00\n0.13\n0.03\n0.31\n0.00\n0.09\n0.00\n0.00\n0.00\n0.00\n0.07\nSTEVE-1\nHuman\n0.00\n0.06\n0.00\n0.69\n0.00\n0.03\n0.00\n0.31\n0.91\n0.06\n0.16\n0.00\n0.19\nGROOT-1\nHuman\n0.09\n0.22\n0.00\n0.06\n0.03\n0.06\n0.00\n0.03\n0.47\n0.13\n0.03\n0.00\n0.09\nROCKET-1\nMolmo\n0.91\n0.84\n0.78\n0.75\n0.81\n0.50\n0.78\n0.97\n0.94\n0.91\n0.72\n0.91\n0.82\nROCKET-1\nHuman\n0.94\n0.91\n0.91\n0.94\n0.94\n0.91\n0.97\n0.97\n0.97\n0.97\n0.94\n0.97\n0.95\nskills. For example, in the “hunt the sheep in the\nright fence” task, success requires the agent to kill\nsheep within the right fence, while doing so in\nthe left fence results in failure. In the “place the\noak door on the diamond block” task, success is\nachieved only if the oak door is adjacent to the\ndiamond block on at least one side.\nBaselines. We compare our methods with the\nfollowing baselines: (1) VPT (Baker et al., 2022):\nA foundational model pre-trained on large-scale\nYouTube data, with three variants—VPT (fd),\nVPT (bc), and VPT (rl)—representing the vanilla\nfoundational model, behavior-cloning finetuned\nmodel, and RL-finetuned model, respectively. In\nthis study, we utilize the VPT (bc) variant. (2)\nSTEVE-1 (Lifshitz et al., 2023): An instruction-\nfollowing agent finetuned from VPT, capable of\nsolving various short-horizon tasks. We select the\ntext-conditioned version of STEVE-1 for compari-\nson. (3) GROOT-1 (Cai et al., 2023b): A reference-\nvideo conditioned policy designed to perform\nopen-ended tasks, trained on 2,000 hours of long-\nform videos using latent variable models.\n4.2. ROCKET-1 Masters Minecraft Interactions\nWe evaluated ROCKET-1 on the Minecraft In-\nteraction Benchmark, with results as illustrated\nin Table 2. Since ROCKET-1 operates as a low-\nlevel policy, it requires a high-level reasoner to\nprovide prompts within a visual-temporal con-\ntext, driving ROCKET-1 ’s interactions with the\nenvironment. We tested two reasoners: (1) A\nskilled Minecraft human player, who can provide\nprompts to ROCKET-1 at any interaction mo-\nment, serving as an oracle reasoner that demon-\nstrates the upper bound of ROCKET-1 ’s capa-\nbilities. (2) A Molmo 72B model (Deitke et al.,\n2024), where a predefined Molmo prompt is set\nfor each task to periodically select points in the\nobservation as prompts, which are then processed\ninto object segmentations by the SAM-2 model\n7\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nFigure 7 | Screenshots of our hierarchical agent when completing long-horizon tasks.\nTable 3 | Comparison of hierarchical architectures with different communication protocols. All seven\ntasks require complex reasoning capabilities. The diamond task was run 100 times, while other tasks were run\n20 times, with average success rates reported.\nMethod\nCommunication Protocol\nPolicy\nDEPS\nlanguage\nSTEVE-1\n0.95\n0.75\n0.15\n0.02\n0.15\n0.00\n0.00\nMineDreamer∗\nfuture image\nSTEVE-1\n0.95\n-\n-\n-\n0.00\n0.00\n0.00\nOmniJarvis\nlatent code\nGROOT-1\n0.95\n0.90\n0.20\n0.08\n0.40\n0.00\n0.00\nOurs\nvisual-temporal context\nROCKET-1\n1.00\n1.00\n0.45\n0.25\n0.75\n0.50\n0.70\n(Ravi et al., 2024). Between Molmo’s invoca-\ntions, SAM-2’s tracking capabilities offer object\nsegmentations to guide ROCKET-1. For all base-\nlines, humans provide prompts. We found that\nROCKET-1 + Molmo consistently outperformed\nall baselines, notably achieving a 91% success\nrate in the “place oak door on the diamond block”\ntask that no baseline can solve.\n4.3. ROCKET-1 Supports Long-Horizon Tasks\nWe compared hierarchical agent architectures\nbased on different communication protocols: (1)\nlanguage-based approaches, exemplified by DEPS\n(Wang et al., 2023b); (2) future-image-based\nmethods, represented by MineDreamer (Zhou\net al., 2024); (3) latent-code-based methods, as\nin OmniJarvis (Wang et al., 2024a); and (4) our\nproposed approach based on visual-temporal con-\ntext, as illustrated in the Figure 5. For Mine-\nDreamer, we used the planner provided by DEPS\nand MineDreamer as the controller to complete\nthe long-horizon experiment. We evaluated these\nmethods on seven tasks, each requiring long-\nhorizon planning: obtaining a wooden pickaxe\n(3.6k), furnace (6k), shears (12k), diamond\n(24k), steak (6k), obsidian (24k), and pink wool\n(6k), where the numbers in parentheses repre-\nsent the time limit. In the first five tasks, the\nagent starts from scratch, while for the obsidian\ntask, we provide an empty bucket and a diamond\npickaxe in advance, and for the pink wool task,\nwe provide shears. Taking the obsidian task as\nan example, the player must first locate a nearby\nwater source, fill the bucket, find a nearby lava\npool, pour the water to form obsidian, and finally\nswitch to the diamond pickaxe to mine the obsid-\nian. Our approach significantly improved success\nrates on the first five tasks, particularly achieving\na 35% increase in the steak task. For the last two\ntasks, all previous baseline methods failed, while\nour approach achieved a 70% success rate on the\nwool dyeing task. Figure 7 presents screenshots.\n4.4. What Matters for Learning ROCKET-1?\nWe conduct ablation studies on individual tasks\nof Minecraft Interaction benchmark: “Hunt right\nsheep (\n)” and “Mine emerald (\n)”.\n8\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nTable 4 | Comparison of different condition fusion\nmethods.\nFusion Positions\nHunt (\n)↑\nMine (\n) ↑\nFusion in transformer layer\n0.91\n0.78\nFusion in visual backbone\n0.72\n0.69\nTable 5 | Comparison between different SAM-2 vari-\nants. We studied the impact of SAM-2 models of dif-\nferent sizes on the agent’s object-tracking capability\n(metric: success rate) and inference speed (metric:\nframes per second, FPS). “#Pmt” indicates the num-\nber of frames between prompts generated by Molmo.\nVariants\n#Pmt\nFPS ↑\n↑\n↑\nbaseline (w\/o sam2)\n3\n0.9\n0.84\n0.82\nbaseline (w\/o sam2)\n30\n9.2\n0.00\n0.03\n+sam2_tiny\n30\n5.4\n0.84\n0.69\n+sam2_small\n30\n5.1\n0.88\n0.50\n+sam2_base_plus\n30\n3.0\n0.88\n0.63\n+sam2_large\n30\n2.4\n0.91\n0.78\nCondition Fusion Methods. We modified the vi-\nsual backbone’s input layer from 3 to 4 channels,\nallowing ROCKET-1 to integrate object segmen-\ntation information. For fusing interaction-type\ninformation, we explored two approaches: (1)\nkeeping the object segmentation channel binary\nand encoding interaction types via an embedding\nlayer for fusion in TransformerXL, and (2) di-\nrectly encoding interaction types into the object\nsegmentation for fusion within the visual back-\nbone. As shown in Table 4, the first approach sig-\nnificantly outperformed the second, as it allows\nthe visual backbone to share knowledge across\ndifferent interaction types and focus on recogniz-\ning objects of interest without being affected by\nimbalanced interaction-type distributions.\nSAM-2 Models. The SAM-2 model acts as a proxy\nsegmentation generator when the high-level rea-\nsoner fails to provide timely object segmentations.\nWe evaluate the impact of different SAM-2 model\nsizes on task performance and inference speed,\nas shown in Table 5. Results indicate that with\nlow-frequency prompts from the high-level rea-\nsoner (Molmo 72B) at 1.5 (game frequency is\n20), SAM-2 greatly improves task success rates.\nWhile “sam2_hiera_large” is the best, increasing\nthe SAM-2 model size yields performance gains\nat the cost of higher time.\n5. Related Works\nInstructions for Multi-Task Policy. Most current\napproaches (Brohan et al., 2022, 2023; Cai et al.,\n2023a; Huang et al., 2023; Lynch et al., 2023)\nuse natural language to describe task details and\ncollect large amounts of text-demonstration data\npairs to train a language-conditioned policy for\ninteraction with the environment. Although nat-\nural language can express a wide range of tasks,\nit struggles to represent spatial relationships ef-\nfectively. Additionally, gathering text-annotated\ndemonstration data is costly, limiting the scala-\nbility of these methods. Alternatives, such as Lif-\nshitz et al. (2023); Majumdar et al. (2022); Sun-\ndaresan et al. (2024), use images to drive goal-\nconditioned policies, typically learning through\nhindsight relabeling in a self-supervised manner.\nWhile this reduces the need for annotated data,\nfuture images are often insufficiently expressive,\nmaking it difficult to capture detailed task execu-\ntion processes. Methods like Cai et al. (2023b);\nJang et al. (2022) propose using reference videos\nto describe tasks, offering strong expressiveness\nbut suffering from ambiguity, which may lead to\ninconsistencies between policy interpretation and\nhuman understanding, raising safety concerns.\nGu et al. (2023) suggests representing tasks with\nrough robot arm trajectories, enabling novel task\ncompletion but only in fully observable environ-\nments, limiting its applicability in open-world\nsettings. CLIPort (Shridhar et al., 2022), which\naddresses pick-and-place tasks by controlling the\nrobot’s start and end positions using heatmaps,\nbears some resemblance to our proposed visual-\ntemporal context prompting method. However,\nCLIPort focuses solely on the pick-and-place task\nsolutions in a fully observable environment.\nAgents in Minecraft. Minecraft offers a highly\nopen sandbox environment with complex tasks\nand free exploration, ideal for testing AGI’s adapt-\nability and long-term planning abilities. Its rich\ninteractions and dynamic environment simulate\nreal-world challenges, making it an excellent\ntestbed for AGI. One line of research focuses on\nlow-level control policies in Minecraft. Baker et al.\n(2022) annotated a large YouTube Minecraft\nvideo dataset with actions and trained the first\nfoundation agent in the domain using behavior\n9\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\ncloning, but it lacks instruction-following capabil-\nities. Cai et al. (2023a) employs a goal-sensitive\nbackbone and horizon prediction module to en-\nhance multi-task execution in partially observ-\nable environments, but it only solves tasks seen\nduring training. Fan et al. (2022) fine-tunes a\nvision-language alignment model MineCLIP using\nYouTube video data, and incorporates it into a re-\nward shaping mechanism for training a multi-task\nagent, though task transfer still requires extensive\nenvironment interaction. Lifshitz et al. (2023)\nuses hindsight-relabeling to learn an image-goal-\nconditioned policy and aligns image and text\nspaces via MineCLIP, but this approach is lim-\nited to short-horizon tasks. Another research fo-\ncus integrates vision-language models for long-\nhorizon task planning in Minecraft (Liu et al.,\n2024; Qin et al., 2023; Wang et al., 2024b; Yuan\net al., 2023; Zheng et al., 2023). DEPS (Wang\net al., 2023b), the first to apply large language\nmodels in Minecraft, uses a four-step process to\ndecompose tasks, achieving the diamond mining\nchallenge with minimal training. Voyager (Wang\net al., 2023a) highlights LLM-based agents’ au-\ntonomous exploration and skill-learning abilities.\nJarvis-1 (Wang et al., 2023c) extends DEPS with\nmultimodal memory, improving long-horizon task\nsuccess rates by recalling past experiences. Om-\nniJarvis (Wang et al., 2024a) learns a behavior\ncodebook using self-supervised methods to jointly\nmodel language, images, and actions.\nMine-\nDreamer (Zhou et al., 2024) fine-tunes VLMs and\na diffusion model to generate goal images for task\nexecution, though it faces challenges with image\nquality and consistency.\n6. Conclusions and Limitations\nThis paper presents a novel hierarchical agent ar-\nchitecture for open-world interaction. To address\nspatial communication challenges, we introduce\nvisual-temporal context prompting to convey in-\ntent between the high-level reasoner and low-\nlevel policy. We develop ROCKET-1, an object-\nsegmentation-conditioned policy for real-time ob-\nject interaction, enhanced by SAM-2 for plug-and-\nplay object tracking. Experiments in Minecraft\nshow that our approach effectively leverages\nVLMs’ visual-language reasoning, achieving su-\nperior open-world interaction performance over\nbaselines.\nAlthough ROCKET-1 significantly enhances\ninteraction capabilities in Minecraft, it cannot en-\ngage with objects that are outside its field of view\nor have not been previously encountered. For in-\nstance, if the reasoner instructs ROCKET-1 to\neliminate a sheep that it has not yet seen, the rea-\nsoner must indirectly guide ROCKET-1 ’s explo-\nration by providing segmentations of other known\nobjects. This limitation reduces ROCKET-1 ’s\nefficiency in completing simple tasks and neces-\nsitates frequent interventions from the reasoner,\nleading to increased computational overhead. We\nsolve this problem in ROCKET-2 (Cai et al., 2025).\nThis project is implemented using MineStudio\n(Cai et al., 2024a).\n7. Acknoledgements\nThis work was supported by the National Science\nand Technology Major Project #2022ZD0114902\nand the CCF-Baidu Open Fund. We sincerely ap-\npreciate their generous support, which enabled\nus to conduct this research.\n10\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nReferences\nO. J. Achiam, S. Adler, S. Agarwal, L. Ah-\nmad, I. Akkaya, F. L. Aleman, D. Almeida,\nJ.\nAltenschmidt,\nS.\nAltman,\net\nal.\nGpt-4\ntechnical\nreport.\n2023.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:257532815.\nM. Andrychowicz, D. Crow, A. Ray, J. Schneider,\nR. Fong, P. Welinder, B. McGrew, J. Tobin,\nP. Abbeel, and W. Zaremba. Hindsight expe-\nrience replay. ArXiv, abs\/1707.01495, 2017.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:3532908.\nB. Baker, I. Akkaya, P. Zhokhov, J. Huizinga,\nJ. Tang, A. Ecoffet, B. Houghton, R. Sampe-\ndro, and J. Clune.\nVideo pretraining (vpt):\nLearning to act by watching unlabeled online\nvideos. ArXiv, abs\/2206.11795, 2022. URL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:249953673.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebo-\ntar, J. Dabis, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, J. Hsu, J. Ibarz,\nB. Ichter, A. Irpan, T. Jackson, S. Jesmonth,\nN. J. Joshi, R. C. Julian, D. Kalashnikov,\nY. Kuang,\nI. Leal,\nK.-H. Lee,\nS. Levine,\nY. Lu, U. Malla, D. Manjunath, I. Mordatch,\nO. Nachum, C. Parada, J. Peralta, E. Perez,\nK. Pertsch, J. Quiambao, K. Rao, M. S. Ryoo,\nG. Salazar, P. R. Sanketi, K. Sayed, J. Singh,\nS. A. Sontakke, A. Stone, C. Tan, H. Tran,\nV. Vanhoucke, S. Vega, Q. H. Vuong, F. Xia,\nT. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich.\nRt-1:\nRobotics transformer for real-world\ncontrol at scale. ArXiv, abs\/2212.06817, 2022.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:254591260.\nA. Brohan, N. Brown, J. Carbajal, Y. Cheb-\notar, X. Chen, K. Choromanski, T. Ding,\nD. Driess, A. Dubey, C. Finn, et al.\nRt-2:\nVision-language-action models transfer web\nknowledge to robotic control. arXiv preprint\narXiv:2307.15818, 2023.\nS. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang.\nOpen-world multi-task control through goal-\naware representation learning and adaptive\nhorizon prediction. 2023 IEEE\/CVF Conference\non Computer Vision and Pattern Recognition\n(CVPR), pages 13734–13744, 2023a.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:256194112.\nS. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and\nY. Liang. Groot: Learning to follow instructions\nby watching gameplay videos. In The Twelfth\nInternational Conference on Learning Represen-\ntations, 2023b.\nS. Cai, Z. Mu, K. He, B. Zhang, X. Zheng, A. Liu,\nand Y. Liang. Minestudio: A streamlined pack-\nage for minecraft ai agent development. 2024a.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:274992448.\nS. Cai, B. Zhang, Z. Wang, H. Lin, X. Ma, A. Liu,\nand Y. Liang.\nGroot-2: Weakly supervised\nmulti-modal instruction following agents. arXiv\npreprint arXiv:2412.10410, 2024b.\nS. Cai, Z. Mu, A. Liu, and Y. Liang. Rocket-2:\nSteering visuomotor policy via cross-view goal\nalignment. arXiv preprint arXiv:2503.02505,\n2025.\nY. Cheng, C. Zhang, Z. Zhang, X. Meng, S. Hong,\nW. Li, Z. Wang, Z. Wang, F. Yin, J. Zhao,\net al. Exploring large language model based\nintelligent agents: Definitions, methods, and\nprospects. arXiv preprint arXiv:2401.03428,\n2024.\nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le,\nand R. Salakhutdinov. Transformer-xl: Atten-\ntive language models beyond a fixed-length\ncontext.\nIn Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics, Jan 2019.\ndoi: 10.18653\/v1\/\np19-1285. URL http:\/\/dx.doi.org\/10.\n18653\/v1\/p19-1285.\nM. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang,\nJ. S. Park, et al.\nMolmo and pixmo: Open\nweights and open data for state-of-the-art mul-\ntimodal models, 2024. URL https:\/\/arxiv.\norg\/abs\/2409.17146.\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch,\nA. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\n11\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nQ. Vuong, T. Yu, et al. Palm-e: An embodied\nmultimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\nL. J. Fan, G. Wang, Y. Jiang, A. Mandlekar,\nY. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,\nand A. Anandkumar. Minedojo: Building open-\nended embodied agents with internet-scale\nknowledge.\nArXiv, abs\/2206.08853, 2022.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:249848263.\nJ. Gu, S. Kirmani, P. Wohlhart, Y. Lu, M. G. Arenas,\nK. Rao, W. Yu, C. Fu, K. Gopalakrishnan, Z. Xu,\net al. Rt-trajectory: Robotic task generalization\nvia hindsight trajectory sketches. arXiv preprint\narXiv:2311.01977, 2023.\nW. H. Guss, B. Houghton, N. Topin, P. Wang,\nC. Codel, M. M. Veloso, and R. Salakhutdinov.\nMinerl:\nA large-scale dataset of minecraft\ndemonstrations.\nIn International Joint Con-\nference on Artificial Intelligence, 2019.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:199000710.\nJ. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang,\nQ. Li, S.-C. Zhu, B. Jia, and S. Huang.\nAn\nembodied generalist agent in 3d world. arXiv\npreprint arXiv:2311.12871, 2023.\nE. Jang, A. Irpan, M. Khansari, D. Kappler,\nF. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z:\nZero-shot task generalization with robotic imi-\ntation learning. ArXiv, abs\/2202.02005, 2022.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:237257594.\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland,\nL. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,\nW.-Y. Lo, P. Dollár, and R. B. Girshick. Segment\nanything. ArXiv, abs\/2304.02643, 2023. URL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:257952310.\nS. Lifshitz, K. Paster, H. Chan, J. Ba, and\nS. A. McIlraith.\nSteve-1:\nA generative\nmodel\nfor\ntext-to-behavior\nin\nminecraft.\nArXiv,\nabs\/2306.00937,\n2023.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:258999563.\nH. Lin, Z. Wang, J. Ma, and Y. Liang.\nMcu:\nA task-centric framework for open-ended\nagent evaluation in minecraft. arXiv preprint\narXiv:2310.08367, 2023.\nS. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang,\nC. Li, J. Yang, H. Su, J. Zhu, et al. Ground-\ning dino: Marrying dino with grounded pre-\ntraining for open-set object detection. arXiv\npreprint arXiv:2303.05499, 2023.\nS. Liu, H. Yuan, M. Hu, Y. Li, Y. Chen, S. Liu,\nZ. Lu, and J. Jia. RL-GPT: Integrating rein-\nforcement learning and code-as-policy. arXiv\npreprint arXiv:2402.19299, 2024.\nC. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker,\nR. Baruch, T. Armstrong, and P. Florence. Inter-\nactive language: Talking to robots in real time.\nIEEE Robotics and Automation Letters, 2023.\nA. Majumdar, G. Aggarwal, B. Devnani, J. Hoff-\nman, and D. Batra. Zson: Zero-shot object-goal\nnavigation using multimodal goal embed-\ndings.\nArXiv, abs\/2206.12403, 2022.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:250048645.\nOcto Model Team, D. Ghosh, H. Walke, K. Pertsch,\nK. Black, O. Mees, S. Dasari, J. Hejna, C. Xu,\nJ. Luo, T. Kreiman, Y. Tan, L. Y. Chen, P. San-\nketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and\nS. Levine. Octo: An open-source generalist\nrobot policy. In Proceedings of Robotics: Science\nand Systems, Delft, Netherlands, 2024.\nY. Qin, E. Zhou, Q. Liu, Z. Yin, L. Sheng, R. Zhang,\nY. Qiao, and J. Shao. Mp5: A multi-modal open-\nended embodied system in minecraft via active\nperception. arXiv preprint arXiv:2312.07472,\n2023.\nN. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma,\nH. Khedr, R. Rädle, C. Rolland, L. Gustafson,\nE. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y.\nWu, R. Girshick, P. Dollár, and C. Feichtenhofer.\nSam 2: Segment anything in images and videos.\narXiv preprint arXiv:2408.00714, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2408.00714.\nM. Shridhar, L. Manuelli, and D. Fox. Cliport:\nWhat and where pathways for robotic manipu-\n12\nROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting\nlation. In Conference on robot learning, pages\n894–906. PMLR, 2022.\nA. Stone,\nT. Xiao,\nY. Lu,\nK. Gopalakrish-\nnan, K.-H. Lee, Q. H. Vuong, P. Wohlhart,\nB. Zitkovich, F. Xia, C. Finn, and K. Haus-\nman.\nOpen-world object manipulation\nusing\npre-trained\nvision-language\nmod-\nels.\nArXiv, abs\/2303.00905, 2023.\nURL\nhttps:\/\/api.semanticscholar.org\/\nCorpusID:257280290.\nP. Sundaresan, Q. Vuong, J. Gu, P. Xu, T. Xiao,\nS. Kirmani, T. Yu, M. Stark, A. Jain, K. Haus-\nman, et al. Rt-sketch: Goal-conditioned imita-\ntion learning from hand-drawn sketches. 2024.\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B.\nAlayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.\nDai, A. Hauth, et al. Gemini: a family of highly\ncapable multimodal models.\narXiv preprint\narXiv:2312.11805, 2023.\nG. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao,\nY. Zhu, L. J. Fan, and A. Anandkumar. Voyager:\nAn open-ended embodied agent with large lan-\nguage models. ArXiv, abs\/2305.16291, 2023a.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:258887849.\nZ. Wang, S. Cai, G. Chen, A. Liu, X. S. Ma, and\nY. Liang. Describe, explain, plan and select:\ninteractive planning with llms enables open-\nworld multi-task agents. Advances in Neural\nInformation Processing Systems, 36, 2023b.\nZ. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang,\nH. Lin, Z. He, Z. Zheng, Y. Yang, et al. Jarvis-\n1: Open-world multi-task agents with memory-\naugmented multimodal language models. arXiv\npreprint arXiv:2311.05997, 2023c.\nZ. Wang, S. Cai, Z. Mu, H. Lin, C. Zhang, X. Liu,\nQ. Li, A. Liu, X. Ma, and Y. Liang. Omnijarvis:\nUnified vision-language-action tokenization en-\nables open-world instruction following agents.\narXiv preprint arXiv:2407.00114, 2024a.\nZ. Wang, A. Liu, H. Lin, J. Li, X. Ma, and\nY. Liang. Rat: Retrieval augmented thoughts\nelicit context-aware reasoning in long-horizon\ngeneration. arXiv preprint arXiv:2403.05313,\n2024b.\nH. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai,\nH. Dong, and Z. Lu. Plan4mc: Skill reinforce-\nment learning and planning for open-world\nminecraft tasks. ArXiv, abs\/2303.16563, 2023.\nURL\nhttps:\/\/api.semanticscholar.\norg\/CorpusID:257805102.\nL. Zhang, A. Rao, and M. Agrawala. Adding con-\nditional control to text-to-image diffusion mod-\nels. In Proceedings of the IEEE\/CVF International\nConference on Computer Vision (ICCV), pages\n3836–3847, October 2023.\nS. Zheng, Y. Feng, Z. Lu, et al. Steve-eye: Equip-\nping llm-based embodied agents with visual\nperception in open worlds. In The Twelfth In-\nternational Conference on Learning Representa-\ntions, 2023.\nE. Zhou, Y. Qin, Z. Yin, Y. Huang, R. Zhang,\nL. Sheng, Y. Qiao, and J. Shao. Minedreamer:\nLearning to follow instructions via chain-of-\nimagination for simulated-world control. arXiv\npreprint arXiv:2403.12037, 2024.\n13\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting.pdf"}
{"title":"GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents","authors":"Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji Liu, Yitao Liang","summary":"Developing agents that can follow multimodal instructions remains a\nfundamental challenge in robotics and AI. Although large-scale pre-training on\nunlabeled datasets (no language instruction) has enabled agents to learn\ndiverse behaviors, these agents often struggle with following instructions.\nWhile augmenting the dataset with instruction labels can mitigate this issue,\nacquiring such high-quality annotations at scale is impractical. To address\nthis issue, we frame the problem as a semi-supervised learning task and\nintroduce GROOT-2, a multimodal instructable agent trained using a novel\napproach that combines weak supervision with latent variable models. Our method\nconsists of two key components: constrained self-imitating, which utilizes\nlarge amounts of unlabeled demonstrations to enable the policy to learn diverse\nbehaviors, and human intention alignment, which uses a smaller set of labeled\ndemonstrations to ensure the latent space reflects human intentions. GROOT-2's\neffectiveness is validated across four diverse environments, ranging from video\ngames to robotic manipulation, demonstrating its robust multimodal\ninstruction-following capabilities.","url":"http:\/\/arxiv.org\/abs\/2412.10410v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2412.10410v1","published":1733550469000,"comment":null,"pdf_text":"GROOT-2: Weakly Supervised Multi-Modal\nInstruction Following Agents\nShaofei Cai†1, Bowei Zhang†1, Zihao Wang1, Haowei Lin1, Xiaojian Ma3, Anji Liu2 and Yitao Liang B1\n1PKU, 2UCLA, 3BIGAI, All authors are affiliated with Team CraftJarvis\nDeveloping agents that can follow multimodal instructions remains a fundamental challenge in robotics\nand AI. Although large-scale pre-training on unlabeled datasets (no language instruction) has enabled\nagents to learn diverse behaviors, these agents often struggle with following instructions. While augment-\ning the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at\nscale is impractical. To address this issue, we frame the problem as a semi-supervised learning task and\nintroduce GROOT-2, a multimodal instructable agent trained using a novel approach that combines\nweak supervision with latent variable models. Our method consists of two key components: constrained\nself-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn\ndiverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations\nto ensure the latent space reflects human intentions. GROOT-2’s effectiveness is validated across four\ndiverse environments, ranging from video games to robotic manipulation, demonstrating its robust\nmultimodal instruction-following capabilities.\nFigure 1 | By feeding a mixture of demonstrations and some multimodal labels, we learn GROOT-2, a human-\naligned agent capable of understanding multimodal instructions and adaptable to various environments, ranging\nfrom video games to robot manipulation, including Atari, Minecraft, Language Table, and Simpler Env.\n1. Introduction\nDeveloping policies that can follow multimodal instructions to solve open-ended tasks in open-world\nenvironments is a long-standing challenge in robotics and AI research. With the advancement of\nlarge-scale pretraining (Baker et al., 2022; Brohan et al., 2022; Brown et al., 2020), the research\nparadigm for instruction-following policies has shifted from reinforcement learning to supervised\nlearning. In a supervised learning approach, researchers collect large amounts of demonstration data\nand annotate each demonstration with multimodal instructions—such as videos (Duan et al., 2017;\nJang et al., 2022), texts (Lynch et al., 2023; Padalkar et al., 2023), and episode returns (Chen et al.,\n2021)—using hindsight relabeling. In theory, the instruction-following capability of such policies\nCorresponding author(s): Yitao Liang\n† indicates equal contribution\nShaofei Cai<caishaofei@stu.pku.edu.cn>, Bowei Zhang<zhangbowei@stu.pku.edu.cn>, Zihao Wang<zhwang@stu.pku.edu.cn>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Anji Liu<liuanji@cs.ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2412.10410v1  [cs.AI]  7 Dec 2024\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nMechanical Imitation\n𝐵𝐶\n𝐵𝐶+ 𝐾𝐿\nPosterior Collapse\n→→↑… →×\n↑ ←←… ←\n←↑←… ←\n→↑ →… →×\n…\n…\nNaïve Action Copying\nRough Trajectory\nBehavior Semantic\nCondition Agnostic\n0\n1\nFigure 2 | The ELBO Objective of the VAE and Latent Space Spectrum. We define a spectrum based on\n𝑅=\n𝐵𝐶\n𝐵𝐶+𝐾𝐿, where 𝑅= 0 corresponds to “mechanical imitation” and 𝑅= 1 to “posterior collapse.” At low 𝑅,\nlatent vector 𝑧directly outputs action sequences without considering observations ( 𝐵𝐶→0 ). As 𝑅increases,\n𝑧represents high-level task information, such as specific object interactions. At 𝑅= 1, 𝑧provides no beneficial\ninformation for decision-making.\nimproves as the dataset grows. However, annotating demonstrations with high-quality multimodal\nlabels is prohibitively expensive, making it challenging to scale these methods in practice.\nAnother line of work (Ajay et al., 2020; Cai et al., 2023b; Lynch et al., 2020c) avoids the need\nfor additional human annotations by learning from demonstration-only data in a self-supervised\nmanner. These approaches leverage latent variable generative models (Kingma & Welling, 2013) to\njointly learn an encoder and a latent-conditioned policy. The resulting policy is capable of completing\nmultiple tasks specified by a reference video (Cai et al., 2023b). While a reference video is generally\nexpressive enough to represent various tasks, the inherent ambiguity in videos can lead to a learned\nlatent space that is misaligned with human intention. For example, the encoder module may capture\nthe dynamics between adjacent frames in a video, thereby learning a latent representation of the\naction sequence—a process we refer to as “mechanical imitation.” While this latent space accurately\nreconstructs the target action sequence, the resulting latent representation is difficult for human users\nto leverage during policy deployment. Another potential issue is “posterior collapse,” where the latent\nspace collapses to a single point and loses its influence over the policy during inference. We attribute\nthis mismatch between training and inference to the absence of direct supervision for aligning the\nlatent space with human intention. As illustrated in Figure 2, an ideal controllable latent-induced\npolicy space must strike a balance between these two extremes.\nWe present GROOT-2 (refer to Figure 1), a multimodal instructable agent developed using a latent\nvariable model under weak supervision. To unify the training pipeline, we encode instructions from all\nmodalities as distributions over the latent space. The training objectives consist of two key components:\n(1) constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable\nthe latent-conditioned policy to learn diverse behaviors; and (2) human intention alignment, which\nuses relatively small sets of multimodal labels to align the latent space with human intentions.\nSpecifically, we apply the maximum log-likelihood method in the latent space for alignment. The\nunderlying principle is that the latent embedding encoded by multimodal labels should also be sampled\nfrom the distribution learned from the corresponding video. Our approach is both general and flexible,\nas demonstrated through evaluations across four diverse environments—ranging from video games\nto robotic manipulation—including Atari Games (Bellemare et al., 2013), Minecraft (Johnson et al.,\n2016), Language Table Lynch et al. (2023), and Simpler Env (Li et al., 2024). These experiments\nhighlight GROOT-2 ’s robust ability to follow multimodal instructions, with extensive tests showing\nthat scaling up either unlabeled or labeled demonstrations further enhances performance.\n2\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\n2. Background and Problems\n2.1. Latent Variable Models Enable Controllable Behavior Generation\nIn recent years, the GPT series (Brown et al., 2020; Radford, 2018; Radford et al., 2019) has\ndemonstrated impressive capabilities in controllable text generation. Its success can be attributed to\nself-supervised pretraining and the advantageous properties of natural language. A natural language\nparagraph contains rich dependencies between sentences. For instance, the title of an article sets the\ncentral theme for its body, and the response in a question-answer or dialogue is highly correlated\nwith the preceding text. This characteristic enables large language models, trained via next-token\nprediction, to achieve controllable text generation through prompts during inference. Unfortunately,\nsuch strong correlations do not exist between low-level actions. A desired behavior may not have\na necessary preceding trajectory segment. Thus, it isn’t easy to prompt a pre-trained policy model\nto generate a desired behavior. Instead, the generation of actions depends on an underlying latent\nintention variable. A natural approach is to employ latent variable generative models to jointly model\ntrajectory data and the latent variables that drive them, allowing for controllable behavior generation\nby manipulating the latent variables during inference. Next, we will elaborate on how latent variable\nmodels model trajectory data.\nAs a classic latent variable generative model, Variational Autoencoder (VAE, Kingma & Welling\n(2013)) has been widely used in fields such as image generation and text generation. With the\ndevelopment of the offline pretraining paradigm, recent years have seen an increasing number of\nworks utilizing VAE to model trajectory data. Typically, its architectures consist of three components:\na posterior encoder, a prior encoder, and a policy decoder. The posterior encoder, 𝑞(𝑧|𝜏), encodes a\nspecific behavioral trajectory 𝜏= (o1:𝑁, a1:𝑁) and generates a posterior distribution over the latent\nspace. When the action sequence can be accurately inferred from the observation sequence (Baker et al.,\n2022; Zhang et al., 2022)—i.e., when the inverse dynamics model of the environment 𝑝IDM(a1:𝑁|o1:𝑁)\nis easily learned—the action sequence can be excluded from the posterior’s input (Cai et al., 2023b),\nthus reducing the distribution condition to o1:𝑁. The prior encoder, 𝑝(𝑧|o1:𝑘), generates a distribution\nover the latent space based on the history of observations, where 𝑘denotes the length of the observation\nwindow. When 𝑘= 0, the prior distribution is independent of historical observations and is typically\nassumed to follow a standard normal distribution N (0; 1). The decoder, 𝜋(a𝑡|o1:𝑡, 𝑧), is generally a\nlatent-conditioned policy that takes in the environment’s observations along with a specific latent\nvariable to predict the next action to be executed. According to variational inference theory, we can\noptimize the VAE’s modeling capabilities by maximizing the Evidence Lower Bound (ELBO)\nLELBO = 𝔼𝑧∼𝑞(𝑧|o1:𝑁)\n\" 𝑁\n∑︁\n𝑡=𝑘\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n+ 𝐷KL(𝑞(𝑧|o1:𝑁) ∥𝑝(𝑧|o≤𝑘)).\n(1)\nThere are generally three main objectives for using VAE to model trajectory data: (1) Modeling\nmultimodal behaviors (Lynch et al., 2020a; Mees et al., 2022): For instance, when trajectory data\nis collected from different individuals, the variations in action sequences across different behavior\nmodes can be substantial. Directly applying a naive behavior cloning algorithm may result in poor\nmodeling performance. Introducing an additional latent variable to differentiate between behavior\nmodes can help mitigate conflicts between them during training. (2) Skill discovery (Gupta et al.,\n2019; Xu et al., 2023): Complex trajectory data is often composed of various skills. A VAE can\nabstract action sequences in a self-supervised manner, enabling skill reuse in downstream tasks, such\nas accelerating the exploration process in reinforcement learning (Ajay et al., 2020; Pertsch et al.,\n2021). (3) Following reference videos to complete open-ended tasks (also known as one-shot\ndemonstration learning, Cai et al. (2023b)): This approach aims to leverage the learned posterior\n3\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 3 | Comparison of Policies with Different Latent Spaces. The reference video depicts digging for\ndiamonds. A policy that mechanically imitates the trajectory falls into lava, while one aligned with human\nintention avoids lava and successfully reaches the diamonds.\nencoder to recognize the underlying intention behind a reference video and encode it as a latent,\nwhich can then drive a policy to complete the specified task in a novel deployment. It points to a way\nto pre-train instruction-following policies using unlabeled trajectory data. We primarily focus on the\nthird objective in the following paragraphs.\n2.2. Modeling Behaviors with VAE Leads to Ambiguous Latent Space\nSeveral studies on VAE (Abeer et al., 2024; Alemi et al., 2018) have pointed out that the Pareto\nfrontier of the ELBO contains an infinite number of solutions for the latent space, a phenomenon we\nrefer to as latent space ambiguity. To facilitate understanding, we provide an informal illustration in\nFigure 2, which shows several possible latent spaces when a VAE is used to model behaviors, all having\nsimilar ELBO values. We differentiate these latent spaces using the ratio 𝑅=\n𝐵𝐶\n𝐵𝐶+𝐾𝐿, where 𝑅= 0\nand 𝑅= 1 represent two extremes of the latent space. When 𝑅approaches 0, the latent condition\ncontains much information, nearly dictating every action of the policy’s behavior. We refer to this as\nmechanical imitation, where the VAE effectively degenerates into an Autoencoder (AE). Conversely,\nwhen 𝑅approaches 1, the latent loses its ability to control the policy’s output, a phenomenon known\nas posterior collapse (Fang et al., 2019; Pagnoni et al., 2018), in which the VAE reduces to an\nAuto-regressive (AR) model. Intuitively, as 𝑅increases, the information encoded in the latent space\nbecomes more high-level, and the policy relies more on environmental feedback (observations) to\nmake decisions that align with the dataset’s distribution. On the other hand, when 𝑅is smaller, the\npolicy tends to down-weight the environment’s observations.\nNot all latent spaces effectively support following a reference video. As shown in Figure 3, the\ngap between the environment state in the reference video and during policy deployment requires\nthe posterior encoder to extract intentions independent of environmental dynamics. For instance, in\nthe Minecraft task “mining a diamond underground,” a reference video may show a player walking\nforward and mining a diamond. If the latent encodes only the trajectory sketch, the policy might fail\nby colliding with obstacles in the deployment environment. This mismatch occurs because humans\ninterpret the video as “mining the diamond” rather than copying specific actions. Aligning the latent\nspace with human intentions is critical for improving policy steerability.\n4\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nEncoder\nPolicy\nEncoder\n𝑜!:#\n𝔼!∼#(!|&!:#)[−log 𝑒(𝑧|𝑜(:*) ]\nEncoder\nEncoder\n𝐷+,(𝑒𝑧𝑜(:* ∥𝑒(𝑧|𝑜())\nHuman Intention Alignment\nConstrained Self-Imitating\nImage Token\nText \/ Returns Token\nAction Token\nSampled Condition Latent\nCondition Distribution\nStop Gradient Operation\nPast Observations\n𝑤!:$\n𝑜!\n𝑜!:#\n𝑜%\n𝑧&\n𝑜!:%'!\n𝑧(\nPolicy\n𝑎%\n𝑜%\n𝑜!:%'!\n−log 𝜋𝑎- 𝑜(:-, 𝑧.)\n𝑎%\n−log 𝜋𝑎- 𝑜(:-, 𝑧&)\nFigure 4 | Pipeline for Constructing a Training Batch for GROOT-2. Each batch includes two sample\ntypes: (1) demonstration-only samples for learning a latent-conditioned policy (Constrained Self-Imitating);\nand (2) labeled samples (text or expected returns) for aligning the latent space with human intentions (Human\nIntention Alignment). The sample ratio varies by dataset distribution.\n3. Aligning Policy Learners with Weak Supervision\nWe explore the development of instructable agents based on latent variable models. To avoid “latent\nspace ambiguity”, we introduce human intention knowledge into the generative pretraining process\nof the policy model to assist in shaping the latent space. As multimodal labels associated with\ndemonstrations carry rich human intention details, we propose a weakly supervised policy learning\nalgorithm to leverage large amounts of unlabeled demonstration data to learn the latent space while\nusing a small amount of multimodal labeled data to align the latent space with human intention.\nUltimately, this enables instructions from all modalities to be unified within the same latent space.\nNext, we will elaborate on the dataset collection, training pipeline, and inference procedure.\nDataset Collection. We can collect two types of training data from the web: a large set of unlabeled\ndemonstrations Ddem = {(o1:𝑁, a1:𝑁)} and a relatively small set of annotated demonstrations Dlab =\n{(o1:𝑁, a1:𝑁, w1:𝑀)}, where o is the image observation provided by the environment, a is the action\ntaken by the policy, w is the word token, 𝑁is the length of a demonstration, 𝑀is the length of an\nannotation sentence. The annotation sentence can be multimodal, such as a language sentence (with\n𝑀≥1) or a scaler of the episode return (with 𝑀= 1), which explains the behavior or outcome of the\ndemonstration from a human’s perspective. Since the annotation data is expensive to collect, we have\n|Dlab| ≪|Ddem|.\nTraining Pipeline. Our goal is to learn a shared latent space Z, per-modal instruction encoders\n𝑒(𝑧|𝑐), and a latent-conditioned policy 𝜋(a𝑡|o≤𝑡, 𝑧). Leveraging past observations is essential for a\npolicy to make decisions in a partially observable environment such as Minecraft (Johnson et al.,\n2016). We call the learned policy model GROOT-2, whose training pipeline is shown in Figure 4. For\nan unlabeled demonstration (o1:𝑁, a1:𝑁), we use the encoder module to produce a prior distribution\n𝑒(𝑧|o1) and a posterior distribution 𝑒(𝑧|o1:𝑁). Using the reparameterization trick (Kingma & Welling,\n2013), we sample the latent 𝑧from the posterior distribution 𝑒(𝑧|o1:𝑁) and train the policy model,\nconditioned on 𝑧and o1:𝑡, to reconstruct the entire action sequence causally. To limit the information\npresented in the latent space, we introduce an auxiliary KL divergence term in the objective:\nLdem(o, a) = 𝔼𝑧∼𝑒(𝑧|o1:𝑁)\n\" 𝑁\n∑︁\n𝑡=1\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n+ 𝛽1𝐷KL(𝑒(𝑧|o1:𝑁) ∥𝑒(𝑧|o1)).\n(2)\nThis allows the model to leverage demonstration-only data to enhance the complexity of the la-\ntent space, a process we refer to as “constrained self-imitating.” For a labeled demonstration\n5\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 5 | Diverse visual environments used in the experiments. We test our GROOT-2 on both video games\n(simple Atari games and the complex Minecraft game) and robotic manipulation environments (Language\nTable and Simpler Env). Minecraft is a partially observable open-ended environment, while others are fully\nobservable.\n(o1:𝑁, a1:𝑁, w1:𝑀), we pass the label w1:𝑀through the encoder module to obtain a latent distri-\nbution and train the policy model to reconstruct the action sequence based on the latent 𝑧sampled\nfrom this distribution 𝑒(𝑧|w1:𝑀). This allows human knowledge to be modeled in the latent space.\nFurther, to make the encoder understand demonstration o1:𝑁just like humans, we introduce an auxil-\niary MLE term: maximize the log-likelihood of 𝑒(𝑧|o1:𝑁) given the latent 𝑧sampled from 𝑒(𝑧|w1:𝑀).\nUnlike the prior behavior cloning term, the aligning term can be quickly calculated in closed form.\nThis process is referred to as “human intention alignment”:\nLlab(o, a, w) = 𝔼𝑧∼𝑒(𝑧|w1:𝑀)\n\" 𝑁\n∑︁\n𝑡=1\n−log 𝜋(a𝑡|o1:𝑡, 𝑧)\n#\n−𝛽2𝔼𝑧∼sg[𝑒(𝑧|w1:𝑀)] [log 𝑒(𝑧|o1:𝑁)] ,\n(3)\nwhere sg[·] denotes stop gradient operation. The MLE-based alignment objective ensures that the\nlatent sampled from the label-conditioned distribution 𝑒(𝑧|𝑤1:𝑀) can also be sampled from its video-\nconditioned distribution 𝑒(𝑧|𝑜1:𝑁). The final loss function combines the two objectives:\nL(Ddem, Dlab) = 𝔼(o,a)∼Ddem [Ldem(o, a)] + 𝔼(o,a,w)∼Dlab [Llab(o, a, w)] .\n(4)\nSpecific implementation details, such as the model design choice, can be found in the Appendix A.\nInference Procedure. GROOT-2 supports two types of instructions during inference: (1) visual-based\ninstruction – the user can either retrieve a demonstration from the dataset as a reference video or\nmanually record a reference video to serve as the condition for the policy; (2) label-based instruction\n– the user can input a text sentence or specify an expected return as the condition (depending on the\nlabel modality used during the model’s training). We tested them in the following experiments.\n4. Capabilities and Analysis\nWe aim to address the following questions: (1) How does GROOT-2 perform in open-world video\ngames and robotic manipulation? (2) Can GROOT-2 follow instructions beyond language and video?\n(3) What insights can be gained from visualizing the learned latent space? (4) How does GROOT-2\nscale with labeled and unlabeled trajectories? (5) What is the impact of backbone initialization on\nperformance? (6) How do language and video losses influence performance?\nEnvironment and Benchmarks. We conduct experiments across four types of representative envi-\nronments: classical 2D game-playing benchmarks on Atari (Bellemare et al., 2013), 3D open-world\ngameplaying benchmarks on Minecraft (Johnson et al., 2016; Lin et al., 2023), and Robotics bench-\nmarks on Language Table simulator (Lynch et al., 2023) and Simpler Env simulator(Li et al., 2024),\n6\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nTable 1 | Results on the Open-World Minecraft Benchmark. This benchmark includes 8 task families and\n100 tasks. Each task is evaluated 30 times across three seeds, and the average success rate is calculated per\ntask family. For example, Combat (16) indicates 16 tasks in the Combat family.\nMethods\nPrompt\nCombat\nHunt\nRide\nBreed\nCraft\nMine\nInteract\nPlant\n(16)\n(10)\n(4)\n(8)\n(20)\n(20)\n(10)\n(12)\nVPT\nN\/A\n11±3\n20±4\n7±2\n2±0\n4±1\n7±2\n21±6\n22±7\nSTEVE-1\nlang\n12±3\n9±2\n54±8\n4±2\n5±2\n6±3\n53±9\n33±8\nSTEVE-1\nvisual\n15±4\n10±3\n38±9\n6±2\n6±2\n10±4\n40±8\n43±7\nGROOT-1\nvisual\n18±5\n28±8\n26±6\n12±3\n15±4\n22±7\n57±8\n75±6\nGROOT-2\nlang\n40±7\n43±5\n46±6\n22±6\n18±3\n37±5\n55±4\n75±9\nGROOT-2\nvisual\n37±4\n48±7\n51±4\n20±4\n27±3\n36±7\n63±6\n77±7\nTable 2 | Results on the Language Table Benchmark. We reported success rates (in %) within 200 steps for\neach instruction modality, averaging over 250 rollouts. Results are averaged over 3 seeds with mean and stderr.\n“-” indicates missing data. The percentages in parentheses indicate the proportion of labels used.\nTask Family\nBC-Zero\nLAVA\nRT-1\nGROOT-1\nGROOT-2 (50%)\nGROOT-2 (100%)\nlang\nlang\nlang\nvisual\nlang\nvisual\nlang\nvisual\nblock to block\n-\n90±2\n-\n8±2\n84±9\n78±9\n86±8\n82±7\nblock to absolute loc\n-\n72±4\n-\n10±3\n70±8\n68±8\n76±6\n70±8\nblock to block relative loc\n-\n72±3\n-\n4±1\n74±9\n64±7\n76±8\n62±6\nblock to relative loc\n-\n64±4\n-\n8±2\n82±5\n78±6\n84±6\n80±4\nseparate two blocks\n-\n94±2\n-\n12±2\n98±1\n96±2\n98±0\n98±0\nOverall\n72±3\n78±4\n74±13\n8±2\n82±8\n76±7\n84±6\n78±8\nillustrated in Figure 5. These four simulators are used to evaluate whether GROOT-2 can be effec-\ntively steered by returns (Chen et al., 2021; Mnih et al., 2015), reference videos (Cai et al., 2023b;\nJang et al., 2022), and textual instructions (Brohan et al., 2022, 2023).\nResults on the Open-World Minecraft Benchmark. To evaluate policy models in Minecraft, we\nused the contractor dataset from Baker et al. (2022), containing 160M frames. According to the\nmeta information, labeled trajectories account for approximately 35% of the total data. We extended\nthe Minecraft SkillForge Benchmark (Cai et al., 2023b) from 30 to 100 tasks, grouped into eight\nfamilies: Combat, Hunt, Ride, Breed, Craft, Mine, Interact, and Plant. Details are in the Appendix C.\nWe compared GROOT-2 with three baselines: (1) VPT (Baker et al., 2022), a foundational model\ntrained on YouTube data via imitation learning, lacking instruction-following; (2) STEVE-1 (Lifshitz\net al., 2023), which supports text and future image-conditioned instructions; and (3) GROOT-1 (Cai\net al., 2023b), a self-supervised model using reference videos as instructions. Key findings from\nTable 1 are as follows: (1) GROOT-2 (visual) consistently outperforms GROOT-1 across all task\ncategories, with particularly notable gains in mob interaction tasks like Combat and Hunt. Comparing\ntrajectories on Hunt, GROOT-1 mechanically repeats “attack” actions, while GROOT-2 actively\ntracks objects, showing that text data enhances object-centric understanding and better aligns with\nhuman intentions. (2) GROOT-2 (text) performs similarly to GROOT-2 (visual) across most tasks,\ndemonstrating that language and visual modalities share task knowledge. This enables the model to\nleverage both modalities for improved task completion. This highlights the advantage of combining\nmultimodal data for better alignment with human intentions and improved policy performance.\nResults on the Language Table benchmark. To assess GROOT-2’s multimodal instruction following\n7\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nTable 3 | Results on the Simpler Env Benchmark. We report the success rate (in %) of the video-instruction\nand language-instruction following for each model on 3 task families. The percentages in parentheses indicate\nthe proportion of labels used.\nMethods\nPrompt\nPick Coke Can\nMove Near\nOpen\/Close Drawer\nH-Pose\nV-Pose\nS-Pose\nAvg\nAvg\nOpen\nClose\nAvg\nRT-1-X\nlang\n57\n20\n70\n49\n32\n7\n52\n29\nOcto-base\nlang\n5\n0\n1\n1\n3\n0\n2\n1\nGROOT-2 (50%)\nvisual\n42\n18\n52\n37\n35\n29\n30\n30\nlang\n52\n20\n50\n41\n42\n27\n33\n30\nGROOT-2 (100%)\nvisual\n40\n22\n47\n36\n35\n27\n33\n30\nlang\n53\n23\n52\n42\n45\n27\n35\n31\ncapabilities in the context of Robotic Table Manipulation, we utilized the Google Language Table as our\ntesting platform and compared it with methods such as LAVA (Lynch et al., 2023), RT-1 (Brohan et al.,\n2022), GROOT-1 (Cai et al., 2023b). We utilize a dataset provided by Lynch et al. (2023) comprising\n100M trajectories. We removed the text labels from half of the trajectories in the dataset, creating a\n1 : 1 ratio of labeled to unlabeled trajectories. Given that the Language Table environment comes with\nfive task families, all of which are instructed solely through language, we curated five reference videos\nfor each task with relatively clear intentions to evaluate the model’s ability to comprehend video\ninstructions. Detailed specifics are provided in the appendix D. The experimental results are shown\nin the Table 2. We observed that: (1) GROOT-2 leads by an absolute success rate of 4% following\ntext-based instructions compared, likely due to GROOT-2’s more refined model architecture design.\nWe mark the results of RT-2 in gray here, as it uses significantly more training data than ours. (2) The\nperformance of GROOT-2 in following video instructions dropped by approximately 6% compared\nto text instructions, possibly due to the ambiguity of the reference videos, where a “block to block”\ntype video could be interpreted as a “block to relative location” type task. (3) GROOT-1 struggled to\nunderstand the intentions conveyed by the reference videos. We observed that GROOT-1 imitated a\nreference video’s trajectory sketch rather than their colors and shapes. This further underscores the\nimportance of introducing language annotations for some trajectory data as a crucial method to align\nwith human intentions.\nResults on the Simpler Env Benchmark. We utilized the Simpler Env (Li et al., 2024) simulation\nof the Google Robot environment to evaluate the policy’s capability in controlling complex robotic\narms. GROOT-2 is trained on the OpenX dataset (Collaboration et al., 2023). We erased the text\nlabels from half of the dataset’s trajectories, achieving a 1:1 balance between labeled and unlabeled\ndata. We evaluated three types of tasks: Pick Coke Can, Move Near, and Open\/Close Drawer. Following\nBrohan et al. (2023); Li et al. (2024), we set up multiple variants for each task. For instance, the\nPick Coke Can task involved three different poses for the Coke can; in the Move Near task, the layout\nand types of objects varied; and in the Open\/Close Drawer task, the drawer had three layers from\ntop to bottom. We compared GROOT-2 with baseline methods such as RT-1 (Brohan et al., 2022),\nand Octo (Octo Model Team et al., 2024). Among these, RT-1-X is an efficient language-conditioned\ntransformer-based policy trained on the entire OpenX (Collaboration et al., 2023) dataset, which\ncan be considered the performance boundary that GROOT-2 can achieve. As shown in Table 3, we\nfound that GROOT-2 (lang) and GROOT-2 (visual) achieved comparable performance to the RT-1-X\nmodel across all three tasks. This indicates that our method retains language control capabilities and\nimbues the policy with equivalent visual instruction control abilities.\nCan GROOT-2 Follow Instructions Beyond Language and Video, Like Episode Returns?\nWe evaluated GROOT-2 ’s steerability and performance on four Atari games (Breakout, Demon\n8\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 6 | Comparison of Weakly Supervised (WSL) and Self-Supervised (SSL) Learning on 4 Atari Games.\nPolicies are evaluated under return and reference video conditions. For return conditioning, normalized returns\nare input into the encoder, while for video conditioning, videos with similar returns (error < 0.05) are used.\nFigure 7 | t-SNE Visualization of Learned Latent Spaces on Four Atari Games. The first row shows\nresults under self-supervised learning, while the second row displays GROOT-2 ’s performance under weakly\nsupervised learning. Points represent reference videos, with shapes indicating games and colors denoting\nepisode returns. The first four columns compare individual games, and the last column shows a mixed-game\ncomparison.\nAttack, Hero, and Name This Game). Datasets from Agarwal et al. (2020), containing approximately\n10M frames per game, were used. Episode returns were normalized to 𝜇= 0, 𝜎= 1.\nFor training, we constructed a dataset with 30% labeled trajectories (returns) and 70% unlabeled\ndata. Using this dataset, we trained GROOT (wsl) (weakly supervised learning). For comparison,\nGROOT (ssl) was trained on the same dataset without return labels in a fully self-supervised manner.\nBoth models were jointly trained across the four games. During inference, we evaluated policy perfor-\nmance in following reference videos sampled from the test set with normalized returns {−1, 0, +1},\nusing 20 samples per category. Results (Figure 6) show: (1) GROOT (ssl) can recognize behavioral\nquality in reference videos, constructing a rough intention space even without labeled guidance. (2)\nLabeled data significantly improved GROOT (wsl)’s ability to understand video instructions, with the\ngreatest gains in Hero and Name This Game. We also evaluated GROOT (wsl) on return-style instruc-\ntions with normalized rewards {−1, 0, +1}. The similarity between video and reward-conditioned\nperformance suggests the video encoder and reward encoder share the same intention space. The\nAtari experiments aim to evaluate GROOT-2 ’s performance on modalities beyond language and\nvideo, rather than maximizing scores, distinguishing it from traditional offline RL methods.\nWhat Does the Visualization of the Learned Latent Space Reveal?\n9\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 8 | Performance Comparison on Unlabeled Demonstrations. Human-normalized task scores are\naveraged over 20 rollouts across 5 Minecraft tasks to evaluate the agent’s reference-video following ability.\nWe applied t-SNE to visualize embeddings from randomly sampled reference videos. Each point\nin Figure 7 represents a unique video, with shapes denoting game environments and colors indicating\nepisode returns. The first row illustrates results for GROOT (ssl), where videos in Breakout, Demon\nAttack, and Name This Game are classified into two categories based on episode return magnitudes,\nsuggesting that the self-supervised algorithm distinguishes only significant score differences. In\ncontrast, GROOT (ssl) shows poor clustering and limited steerability in the Hero game. The second\nrow shows results for GROOT (wsl), which captures continuous variations in video behavior quality\nacross all games. As shown in the fifth column, embeddings from different environments follow\na continuous pattern aligned with reward labels, indicating a shared latent space that promotes\ncross-environment knowledge transfer.\nHow Does Scaling Up Unlabeled Trajectories Impact Performance?\nWe trained four GROOT-2 variants with 0%, 25%, 50%, and 100% unlabeled data in Minecraft.\nPerformance was tested on five Minecraft tasks (Chop Tree, Hunt Animals, Combat Enemies, Open\nChest, Climb Mountain) and scored relative to skilled human players. For example, if a human\ncollects 20.0 logs in 600 steps and GROOT-2 collects 15.0, the score is 0.75. Results (Figure 8) show\nconsistent improvement with more unlabeled data, with the 100% variant achieving a 5× gain in\nthe Climb Mountain task over the 0% version. It is worth noting that the Climb Mountain and Open\nChest tasks do not have language instructions in the training set.\nHow Does Scaling Up Labeled Trajectories Impact Performance?\nFigure 9 | Ablation Study on Labeled\nTrajectories in the Language Table.\nTo evaluate the impact of labeled trajectory proportions\nin the training set on the instruction-following capabilities of\nGROOT-2, we conducted experiments on the Language Table\nbenchmark. The total number of trajectories remained con-\nstant across different dataset configurations, with only the pro-\nportion of trajectories containing text labels varying. Figure 9\nreports the success rate achieved by GROOT-2 conditioned\non language. At low labeled data proportions (0%−25%), the\nsuccess rate rapidly increased from 10% to 65%, indicating\nthat labeled data significantly influences model performance. However, as the labeled data proportion\nincreased to 50% −80%, the success rate plateaued, rising slightly from 82% to 83%, demonstrating\ndiminishing marginal gains from additional labeled data. Therefore, under resource constraints, a\nlabeled data proportion of 50% may represent the optimal balance between performance and cost.\nHow Does Backbone Initialization Affect Performance?\nWe evaluated different initializations for ViT (random, ImageNet, CLIP) and BERT (random,\nBERT, CLIP) on the Language Table Benchmark. For randomly initialized models, both backbones\n10\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nwere unfrozen during training. According to Table 4, CLIP initialization yielded the best results\nfor ViT, followed by ImageNet, with minimal difference between them, while random initialization\nperformed worst. For BERT, CLIP and standard BERT initialization performed similarly, both surpassing\nrandom initialization. Initializing vision and language encoders with CLIP parameters improves policy\nperformance and reduces training time.\nBackbone\nViT\nViT\nViT\/BERT BERT BERT\nWeights\n-\nImageNet\nCLIP\n-\nBERT\nSR (in %) 76±10\n80±11\n82±8\n79±12 81±8\nTable 4 | Ablation study on the backbone initialization.\nVariants\n−Llab baseline −Ldem baseline\nPrompt\nvision\nvision\nlang\nlang\nSR (in %) 10±2\n76±7\n12±3\n82±8\nTable 5 | Ablation on Llab and Ldem objectives.\nHow Does Language and Video Losses Impact Performance?\nThe Llab loss significantly enhances the model’s understanding of reference videos, as observed\nin the Language Table environment. We compared a variant without Llab loss to the full GROOT-2\nmodel, both trained on the same scale of the Language Table dataset, and tested their ability to follow\nreference videos using standard evaluation scripts. As shown in Table 5, the variant without Llab\nloss failed to complete any tasks. Further analysis of its output videos revealed that it mechanically\nmimicked the arm movement trajectories in the reference videos, completely ignoring object colors\nand shapes, which is inconsistent with human understanding of the reference videos.\nThe Ldem loss is indispensable in the GROOT-2 architecture. Removing Ldem causes the pipeline\nto degrade into an autoencoder when processing unlabeled data. Without constraints on the latent\nencoding, the model tends to learn the video encoder as an inverse dynamics model, encoding\nlow-level action sequences in latent z instead of high-level task information, thereby significantly\nreducing the behavior cloning loss. Additionally, Table 5 show that removing Ldem causes the language\nencoder’s latent z to collapse, leading to a dramatic drop in task success rates.\n5. Related Works\nLearning Policies Across Diverse Domains. Developing policies for sequential control tasks in\nreal and virtual environments poses significant challenges. Research spans domains such as robotic\nmanipulation (Lynch et al., 2023; Yu et al., 2019), video games (Bellemare et al., 2013; Guss et al.,\n2019), and embodied navigation (Hong et al., 2020; Huang et al., 2023; Savva et al., 2019), with\napproaches categorized into reinforcement learning (RL) and imitation learning (IL) based on reward\nfunction reliance. For video games with dense rewards (e.g., ALE platform (Bellemare et al., 2013)),\nonline RL algorithms can achieve superhuman performance (Badia et al., 2020; Mnih et al., 2015) but\nsuffer from low efficiency, risky interactions, and limited generalization. These challenges restrict their\napplicability to physical (Padalkar et al., 2023) or embodied environments (Guss et al., 2019), where\nrewards and cheap interactions are unavailable. IL, as a supervised learning paradigm, addresses\nthese issues through batch efficiency and scalability with large datasets, leveraging Transformer\narchitectures (Jang et al., 2022; Pashevich et al., 2021; Zhang & Chai, 2021). The RT-X series (Brohan\net al., 2022, 2023; Padalkar et al., 2023) advances robotic manipulation by training Transformers\non large expert demonstration datasets, achieving strong zero-shot generalization. Similarly, Baker\net al. (2022) developed a Transformer-based policy for Minecraft using internet-scale gameplay data,\nsolving the diamond challenge. Building on this, Schmidhuber (2019) frames RL as supervised\nlearning, while Chen et al. (2021); Lee et al. (2022) introduce “decision transformers” to model joint\ndistributions of rewards, states, and actions from offline data, highlighting the potential for unified\npolicy learning within Transformers.\nLearning Policies to Follow Instructions. Enabling policies to follow instructions is key to building\n11\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\ngeneral-purpose agents. A common approach involves using language annotations from offline\ndemonstrations to train language-conditioned policies (Abramson et al., 2020; Brohan et al., 2022;\nCai et al., 2023a; Huang et al., 2023; Raad et al., 2024; Reed et al., 2022; Wang et al., 2023a,b),\nleveraging the compositionality of natural language for generalization. However, obtaining high-\nquality annotations is costly. An alternative uses anticipated outcomes as instructions. Majumdar\net al. (2022) trained an image-goal conditioned navigation policy via hindsight relabeling (HER)\n(Andrychowicz et al., 2017) and aligned goal spaces with text. Similarly, Lifshitz et al. (2023) used\nthis strategy for open-ended tasks in Minecraft. Generative latent variable models offer another\nsolution, using label-free demonstrations to train plan-conditioned policies (Ajay et al., 2020; Lynch\net al., 2020b). Extending this, Cai et al. (2023b) applied a posterior encoder to interpret reference\nvideos in Minecraft. Policy learning with weak supervision remains less explored. Lynch & Sermanet\n(2020) proposed a shared latent space conditioned on language and HER-generated goal images,\nwhile Jang et al. (2022) replaced goal images with video labels under full supervision. Jain et al.\n(2024) trained robots using human videos as task representations but required extensive paired\nvideo-trajectory data. Myers et al. (2023) combined labeled and unlabeled trajectories, aligning\nstart-goal pairs with language via contrastive learning, effective for Table Manipulation but limited in\nhandling complex tasks or generalizing to partially observable environments like Minecraft.\n6. Conclusions, Limitations and Future Works\nThis paper investigates the joint learning of a latent intention space and a multimodal instruction-\nfollowing policy under weak supervision. We identify the “latent space ambiguity” issue in latent\nvariable generative models when handling text-free trajectory data, arising from the absence of\ndirect human guidance in shaping the latent space. To address this, we propose a weakly supervised\nalgorithm for training GROOT-2. Evaluations across four diverse environments, from video games\nto robotic manipulation, demonstrate GROOT-2 ’s generality and flexibility in following multimodal\ninstructions. However, GROOT-2 ’s reliance on trajectory data for training limits its applicability to\nvideo data, which lacks action labels. Considering the abundance and diversity of video data available\nonline compared to trajectory data, extending the weak supervision framework to leverage both play\nand trajectory data would be a promising avenue for future work.\n12\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nReferences\nANM Nafiz Abeer, Nathan M Urban, M Ryan Weil, Francis J Alexander, and Byung-Jun Yoon. Multi-\nobjective latent space optimization of generative molecular design models. Patterns, 2024.\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, et al. Imitating interactive intelligence.\narXiv preprint arXiv:2012.05672, 2020.\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline\nreinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR,\n2020.\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive\ndiscovery for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for\nfew-shot learning. ArXiv, abs\/2204.14198, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:248476411.\nAlexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a\nbroken elbo. In International conference on machine learning, pp. 159–168. PMLR, 2018.\nMarcin Andrychowicz, Dwight Crow, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Joshua Tobin, P. Abbeel, and Wojciech Zaremba. Hindsight experience replay. ArXiv,\nabs\/1707.01495, 2017. URL https:\/\/api.semanticscholar.org\/CorpusID:3532908.\nAdrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,\nZhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark.\nIn International conference on machine learning, pp. 507–517. PMLR, 2020.\nBowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton,\nRaul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled\nonline videos. ArXiv, abs\/2206.11795, 2022. URL https:\/\/api.semanticscholar.org\/\nCorpusID:249953673.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment:\nAn evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,\n2013.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian\nIchter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan C. Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha\nManjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch,\nJornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed,\nJaspiar Singh, Sumedh Anand Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Van-\nhoucke, Steve Vega, Quan Ho Vuong, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna\nZitkovich. Rt-1: Robotics transformer for real-world control at scale. ArXiv, abs\/2212.06817, 2022.\nURL https:\/\/api.semanticscholar.org\/CorpusID:254591260.\n13\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. ArXiv, abs\/2005.14165, 2020. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:218971783.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. 2023 IEEE\/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 13734–13744, 2023a. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:256194112.\nShaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning\nto follow instructions by watching gameplay videos. In The Twelfth International Conference on\nLearning Representations, 2023b.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srini-\nvas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.\nIn Neural Information Processing Systems, 2021. URL https:\/\/api.semanticscholar.org\/\nCorpusID:235294299.\nOpen X-Embodiment Collaboration, Abby O’Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek\nGupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain,\nAlbert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta,\nAndrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie,\nAnthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna,\nAyzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian\nIchter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi,\nChenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin,\nDanfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler,\nDinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu,\nFederico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S.\nSukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn,\nGuangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben\nAmor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch,\nIlija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan\nPeters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao,\nJiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy\nWu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan\nTompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka\nRao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra\nByrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani,\nKiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh,\nKuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei,\nLiam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius\nMemmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du,\n14\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nMichael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama,\nMohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J Joshi,\nNiko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver\nKroemer, Osbert Bastani, Pannag R Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng\nXu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu\nChen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart’in-Mart’in, Rohan Baijal,\nRosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah,\nRyan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry\nMoore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar,\nSiddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan\nWelker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae\nPark, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya\nMatsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis\nArmstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan\nZhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng,\nXiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen\nChebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou,\nYoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu,\nYunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan\nMa, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment:\nRobotic learning datasets and RT-X models. https:\/\/arxiv.org\/abs\/2310.08864, 2023.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a fixed-length context.\nIn Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, Jan 2019.\ndoi:\n10.18653\/v1\/p19-1285. URL http:\/\/dx.doi.org\/10.18653\/v1\/p19-1285.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding.\nArXiv, abs\/1810.04805, 2019.\nURL\nhttps:\/\/api.semanticscholar.org\/CorpusID:52967399.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. ArXiv, abs\/2010.11929, 2020. URL https:\/\/api.semanticscholar.org\/CorpusID:\n225039882.\nYan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever,\nPieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. Advances in neural information\nprocessing systems, 30, 2017.\nLe Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent variable\nmodels for text generation. arXiv preprint arXiv:1908.11527, 2019.\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy\nlearning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint\narXiv:1910.11956, 2019.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela M. Veloso,\nand Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. In Interna-\ntional Joint Conference on Artificial Intelligence, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:199000710.\n15\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nYicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould.\nVln-bert: A\nrecurrent vision-and-language bert for navigation. 2021 IEEE\/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 1643–1653, 2020. URL https:\/\/api.semanticscholar.\norg\/CorpusID:227228335.\nJiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-\nChun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv\npreprint arXiv:2311.12871, 2023.\nVidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny Driess, Quan Vuong, Pannag R\nSanketi, Pierre Sermanet, Stefan Welker, Christine Chan, et al. Vid2robot: End-to-end video-\nconditioned policy learning with cross-attention transformers. arXiv preprint arXiv:2403.12943,\n2024.\nEric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey\nLevine, and Chelsea Finn.\nBc-z: Zero-shot task generalization with robotic imitation learn-\ning. ArXiv, abs\/2202.02005, 2022. URL https:\/\/api.semanticscholar.org\/CorpusID:\n237257594.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial\nintelligence experimentation. In International Joint Conference on Artificial Intelligence, 2016. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:9953039.\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs\/1312.6114, 2013.\nURL https:\/\/api.semanticscholar.org\/CorpusID:216078090.\nKuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama,\nIan Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers.\nAdvances in Neural Information Processing Systems, 35:27921–27936, 2022.\nXuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa\nLunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong,\nand Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint\narXiv:2405.05941, 2024.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila A. McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. ArXiv, abs\/2306.00937, 2023. URL https:\/\/api.\nsemanticscholar.org\/CorpusID:258999563.\nHaowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: A task-centric framework for open-ended\nagent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023.\nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem,\nand Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision,\nlanguage, audio, and action. arXiv preprint arXiv:2312.17172, 2023.\nCorey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data.\narXiv preprint arXiv:2005.07648, 2020.\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre\nSermanet. Learning latent plans from play. In Conference on robot learning, pp. 1113–1132. PMLR,\n2020a.\n16\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre\nSermanet. Learning latent plans from play. In Conference on robot learning, pp. 1113–1132. PMLR,\n2020b.\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre\nSermanet. Learning latent plans from play. In Conference on robot learning, pp. 1113–1132. PMLR,\n2020c.\nCorey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis\nArmstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics\nand Automation Letters, 2023.\nArjun Majumdar, Gunjan Aggarwal, Bhavika Devnani, Judy Hoffman, and Dhruv Batra. Zson: Zero-\nshot object-goal navigation using multimodal goal embeddings. ArXiv, abs\/2206.12403, 2022. URL\nhttps:\/\/api.semanticscholar.org\/CorpusID:250048645.\nOier Mees, Lukas Hermann, and Wolfram Burgard. What matters in language conditioned robotic\nimitation learning over unstructured data. IEEE Robotics and Automation Letters, 7(4):11205–11212,\n2022.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,\nAlex Graves, Martin A. Riedmiller, Andreas Kirkeby Fidjeland, Georg Ostrovski, Stig Petersen,\nCharlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,\nShane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,\n518:529–533, 2015. URL https:\/\/api.semanticscholar.org\/CorpusID:205242740.\nVivek Myers, Andre Wang He, Kuan Fang, Homer Rich Walke, Philippe Hansen-Estruch, Ching-An\nCheng, Mihai Jalobeanu, Andrey Kolobov, Anca Dragan, and Sergey Levine. Goal representations\nfor instruction following: A semi-supervised language interface to control. In Conference on Robot\nLearning, pp. 3894–3908. PMLR, 2023.\nOcto Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari,\nJoey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen,\nPannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo:\nAn open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft,\nNetherlands, 2024.\nAbhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander\nKhazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning\ndatasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023.\nArtidoro Pagnoni, Kevin Liu, and Shangyan Li. Conditional variational autoencoder for neural machine\ntranslation. arXiv preprint arXiv:1812.04405, 2018.\nAlexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language\nnavigation. In Proceedings of the IEEE\/CVF International Conference on Computer Vision, pp. 15942–\n15952, 2021.\nKarl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill\npriors. In Conference on robot learning, pp. 188–204. PMLR, 2021.\nMaria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie\nBrownfield, Gavin Buttimore, Max Cant, Sarah Chakera, et al. Scaling instructable agents across\nmany simulated worlds. arXiv preprint arXiv:2404.10179, 2024.\n17\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nAlec Radford. Improving language understanding by generative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In International Conference on Machine\nLearning, 2021. URL https:\/\/api.semanticscholar.org\/CorpusID:231591445.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\nBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist\nagent. arXiv preprint arXiv:2205.06175, 2022.\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian\nStraub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research.\nIn Proceedings of the IEEE\/CVF international conference on computer vision, pp. 9339–9347, 2019.\nJuergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards - just map\nthem to actions. ArXiv, abs\/1912.02875, 2019. URL https:\/\/api.semanticscholar.org\/\nCorpusID:208857600.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu\nSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805, 2023.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents.\nAdvances in Neural Information Processing Systems, 36, 2023a.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He,\nZilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented\nmultimodal language models. arXiv preprint arXiv:2311.05997, 2023b.\nMengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, and Shuran Song. Xskill: Cross embodiment\nskill discovery. In Conference on Robot Learning, pp. 3536–3555. PMLR, 2023.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan C. Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learn-\ning. ArXiv, abs\/1910.10897, 2019. URL https:\/\/api.semanticscholar.org\/CorpusID:\n204852201.\nQihang Zhang, Zhenghao Peng, and Bolei Zhou. Learning to drive by watching youtube videos:\nAction-conditioned contrastive policy pretraining. In European Conference on Computer Vision, 2022.\nURL https:\/\/api.semanticscholar.org\/CorpusID:250626771.\nYichi Zhang and Joyce Chai. Hierarchical task learning from language instructions with unified\ntransformers and self-monitoring. arXiv preprint arXiv:2106.03427, 2021.\n18\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nA. Implementation Details\nA.1. Model Architecture\nThis section outlines the architectural design choices employed in our approach. GROOT-2 utilizes a\nTransformer encoder-decoder architecture, augmented with a probabilistic latent space. We detail\nthe components of the model in a structured sequence: extract representations, encode instructions,\nand decode actions.\nExtract Representations. This paragraph elaborates on the backbone networks used to extract\nrepresentations from various data modalities. We denote the modalities of image observation, language\ninstruction, and expected returns as 𝑜1:𝑁, 𝑤1:𝑀, and 𝑟, respectively. For vision inputs, we utilize a\npre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2020) initialized with CLIP (Radford et al.,\n2021) weights. Specifically, the 𝑡−step image observation 𝑜𝑡is resized to 224 × 224 and processed to\nextract 7×7 patch embeddings 𝑥𝑜\n𝑡=\nD\n𝑥𝑜\n𝑡,[1], · · · , 𝑥𝑜\n𝑡,[49]\nE\n. The video representation 𝑥𝑣is then composed\nof the averages of these embeddings across the video frames, denoted as 𝑥𝑣=\n\navg(𝑥𝑜\n1), · · · , avg(𝑥𝑜\n𝑁)\n\u000b\n,\nwhere avg(·) refers to spatial average pooling to minimize computational overhead and 𝑁represents\nthe video length. Textual inputs are processed using the BERT encoder (Devlin et al., 2019) of the\nCLIP model. Rather than utilizing the [CLS] token as the final representation, we retain all word\nembeddings generated by BERT as 𝑥𝑤=\nD\n𝑥𝑤\n[1], · · ·\nE\n. The BERT model parameters are kept frozen\nduring training. For the scalar-form modality of expected returns, we employ a simple Multi-Layer\nPerceptron (MLP) to process these values, represented as 𝑥𝑟←MLP(𝑟). These embeddings are then\nforwarded to subsequent modules.\nEncode Multimodal Instructions with Non-Causal Transformer. Recent works (Lu et al., 2023;\nReed et al., 2022; Team et al., 2023) have demonstrated the Transformer’s effectiveness in capturing\nboth intra-modal and inter-modal relationships, which inspires us to adopt a unified Transformer\nencoder for encoding multimodal instructions. This approach offers two significant advantages: (1)\nIt eliminates the need for designing separate architectures and tuning hyperparameters for each\nmodality. (2) It promotes the sharing of underlying representations across different modalities.\nInstructions are represented as a sequence of embeddings. Before encoding, each embedding is\naugmented with a modality-specific marker. For instance, video instructions are represented as\n\n𝑥𝑣\n1 + [VID], · · · , 𝑥𝑣\n𝑁+ [VID]\n\u000b\n, where [VID] is a learnable embedding.\nDecode Action with Causal Transformer. Given a latent 𝑧and a temporal sequence of perceptual\nobservations 𝑜1:𝑡, the policy aims to predict the next action 𝑎𝑡. Following prior works (Baker et al.,\n2022; Cai et al., 2023b; Raad et al., 2024), we employ the Transformer-XL model (Dai et al., 2019)\nin our policy network, which enables causal attention to past memory states and facilitates smooth\npredictions. Additionally, we utilize the shared vision backbone to extract vision representations,\nthereby representing perceptual inputs as 𝑥𝑜\n1:𝑡. A significant challenge with this approach is low\nefficiency: each new observation 𝑥𝑜\n𝑡adds up to 49 tokens to the input sequence, substantially\nincreasing memory and computational demands. To address this issue, we introduce a pre-fusion\nmechanism inspired by Abramson et al. (2020); Alayrac et al. (2022); Lynch et al. (2023). Specifically,\nwe deploy a lightweight cross-attention module XATTN(q = ·; kv = ·) to perform spatial pooling on 𝑥𝑜\n𝑡,\nusing 𝑧as the query and\nD\n𝑥𝑜\n𝑡,[1], · · · , 𝑥𝑜\n𝑡,[49]\nE\nas the keys and values:\n𝑥𝑧\n𝑡←XATTN(q = 𝑧; kv = 𝑥𝑜\n𝑡,[1], · · · , 𝑥𝑜\n𝑡,[49]).\n(5)\nThis pre-fusion mechanism not only reduces the sequence length but also enhances the integration\nof perceptual and latent representations. Utilizing the latent-fused representations 𝑥𝑧\n1:𝑡as the input\n19\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nsequence, we articulate the action decoding process in an autoregressive manner:\n𝑎𝑡←TransformerXL(𝑥𝑧\n1, · · · , 𝑥𝑧\n𝑡).\n(6)\nA.2. Hyper-parameters\nHyper-parameters for training GROOT-2 are shown in Table 6.\nTable 6 | Hyperparameters for training GROOT-2.\nHyperparameter\nValue\nOptimizer\nAdamW\nWeight Decay\n0.001\nLearning Rate\n0.0000181\nWarmup Steps\n2000\nNumber of Workers\n4\nParallel Strategy\nddp\nType of GPUs\nNVIDIA A800\nParallel GPUs\n8\nAccumulate Gradient Batches\n1\nBatch Size\/GPU (Total)\n16 (128)\nTraining Precision\nbf16\nInput Image Size\n224 × 224\nVisual Backbone\nViT\/32\nEncoder Transformer\nminGPT (w\/o causal mask)\nDecoder Transformer\nTransformerXL\nNumber of Encoder Blocks\n8\nNumber of Decoder Blocks\n4\nHidden Dimension\n1024\nTrajectory Chunk size\n128\nAttention Memory Size\n256\n𝛽1\n0.1\n𝛽2\n0.1\nB. Atari\nEnvironment Description. Atari 2600 games contain a lot of diverse video games, which is a\nwidespread benchmark to evaluate the decision-making capability of an agent. The Atari games\ndo not inherently support multitasking concepts; agents are typically tasked with optimizing for\nthe highest possible rewards. However, an advanced human player can deliberately control their\ngameplay level and achieve any potential score. The ability to “control scores” is generally considered\na higher intelligence level compared with merely “winning the game”. Therefore, this paper does not\nemphasize the highest absolute score an agent can achieve in the Atari environment. Instead, it focuses\non evaluating the agent’s ability to follow instructions in the form of videos and \"desired cumulative\nrewards\" and to perform at the appropriate level. Especially when videos serve as conditions, the agent\nneeds to infer the player’s level demonstrated in the reference gameplay, which poses a significant\nchallenge for the current agents. To our knowledge, this setting has not been explored by previous\nworks.\n20\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 10 | Distribution of episode returns for each Atari game.\nObservation and Action Spaces. We utilize the popular Arcade Learning Environment (ALE) as\nour testing platform, where the original observation image provided is 210 × 160, and the action\nspace consists of 18 discrete actions defined by the joystick controller. Following previous works, the\nobservation images are typically resized to 84 × 84 grayscale images. In implementing GROOT-v2, we\nemploy the ViT\/32 model initialized with OpenAI’s pre-trained CLIP model. The observation image,\n84x84, is resized to a resolution of 224 × 224 before being fed into the model for unification. During\nthe training process, the ViT backbone is jointly fine-tuned. The TransformerXL architecture used for\nthe decoder has its memory set to a horizon of 128.\nTraining Dataset. We utilize the trajectories from the Replay Buffer generated during the training of\nagents using the DQN algorithm on Atari, provided by Google, as our source of training data. We\naccess this data through the interface provided in the d4rl-atari project at GitHub1. Specifically, the\ntrajectory data for each game consists of three parts: mixed, medium, and expert, representing the\nenvironment interaction data from 0-1M steps, 9M-10M steps, and the final 1M steps of a training\nsession, respectively. We construct training data of 10M steps for each Atari game, with the proportions\nof mixed, medium, and expert data being 2:5:3. During training, we use a single model to fit 35\nselected game environments on Atari. Considering the significant differences in absolute scores across\n1https:\/\/github.com\/takuseno\/d4rl-atari\n21\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\ndifferent games, we standardize the reward scores. Specifically, we calculate the cumulative reward\nscores for each complete trajectory and adjust them to a mean of 0 and a standard deviation of 1 using\nthe formula 𝑅←(𝑅−𝜇)\/𝜎, which represents the game level corresponding to that trajectory. Figure\n10 illustrates the episode return distributions for each Atari game. Subsequently, each trajectory is\nsegmented into 128-step fragments with the same label.\nComplete Results. We selected trajectory data from 35 Atari games, totaling 350 million frames, to\ntrain GROOT-2, with 30% of the data labeled with returns and the remaining 70% containing only\nimage observations and actions, aligning with the setup for weakly supervised training. After the\nmodel converged, we tested GROOT-2’s ability to follow return-format and video-format instructions\nacross these 35 games. When testing return-format instructions, we chose three samples within the\nnormalized returns space: {−1, 0, 1}. For video-format instructions, we randomly sampled a segment\nof 128 frames from the test data with normalized rewards within the range of {−1, 0, 1}, allowing\na deviation of ±0.05. Each instruction was tested 40 times, with the results depicted in Figure 11.\nWe observed the following: (1) In the majority of games, GROOT-2’s performance showed a clear\npositive correlation with the game level corresponding to the instructions. (2) In certain games (such\nas Pong, Seaquest, Skiing, Wizard of Wor), video-format instructions yielded better control over the\nagent than return-format instructions. Conversely, in games like Amidar, Battle Zone, and Zaxxon,\nreturn-format instructions demonstrated significantly superior control compared to video-format\ninstructions.\nC. Minecraft\nEnvironment Description. Minecraft is a 3D sandbox game with a global monthly active user base\nof 100 million. It features procedurally generated worlds of unlimited size and includes dozens\nof biomes such as plains, forests, jungles, and oceans. The game grants players a high degree of\nfreedom to explore the entire world. The mainstream gameplay includes gathering materials, crafting\nitems, constructing structures, farming land, engaging in combat mobs, and treasure hunting, among\nothers. In this game, players need to face situations that are highly similar to the real world, making\njudgments and decisions to deal with various environments and problems. One can easily specify a\ntask with a natural language description or a demonstration video. Therefore, Minecraft is an ideal\nenvironment to test how an agent behaves in an open-world environment.\nObservation and Action Spaces. We use the combination of 1.16.5 version MineRL (Guss et al.,\n2019) and MCP-Reborn2 as our testing platform, which is consistent with the environment used by\nVPT (Baker et al., 2022) STEVE-1 (Lifshitz et al., 2023) and GROOT-1 (Cai et al., 2023b). Mainly\nbecause this platform preserves observation and action space that is consistent with human players\nto the fullest extent. On the one hand, this design brings about high challenges, as agents can only\ninteract with the environment using low-level mouse and keyboard actions, and can only observe\nvisual information like human players without any in-game privileged information. The Minecraft\nsimulator first generates an RGB image with dimensions of 640 × 360 during the rendering process.\nBefore inputting to the agent, we resize the image to 224 × 224 to enable the agent to see item\nicons in the inventory and important details in the environment. When the agent opens the GUI,\nthe simulator also renders the mouse cursor normally. The RGB image is the only observation that\nthe agent can obtain from the environment during inference. It is worth noting that to help the\nagent see more clearly in extremely dark environments, we have added a night vision effect for the\nagent, which increases the brightness of the environment during nighttime. Our action space is\nalmost identical to that of humans, except for actions that involve inputting strings. It consists of two\n2https:\/\/github.com\/Hexeption\/MCP-Reborn\n22\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nparts: the mouse and the keyboard. The mouse movement is responsible for changing the player’s\ncamera perspective and moving the cursor when the GUI is opened. The left and right buttons are\nresponsible for attacking and using items. The keyboard is mainly responsible for controlling the\nagent’s movement. To avoid predicting null action, we used the same joint hierarchical action space\nas Baker et al. (2022), which consists of button space and camera space. Button space encodes all\ncombinations of keyboard operations and a flag indicating whether the mouse is used, resulting in\na total of 8461 candidate actions. The camera space discretizes the range of one mouse movement\ninto 121 actions. Therefore, the action head of the agent is a multi-classification network with 8461\ndimensions and a multi-classification network with 121 dimensions.\nTraining Dataset. The contractor data is a Minecraft offline trajectory dataset provided by Baker\net al. (2022), which is recorded by professional human players. In this dataset, human players play\nthe game while the system records the image sequence 𝑜1:𝑁, action sequence 𝑎1:𝑁, and metadata\n𝑒1:𝑁generated by the players. Excluding frames containing empty actions, the dataset contains 1.6\nbillion frames with a duration of approximately 2000 hours. The metadata records the 7 kinds of\nevents triggered by the agent in the game at each timestep, i.e. craft item, pickup, mine block, drop\nitem, kill entity, use item, and custom. We augment each event with a text description using the\nOpenAI chatGPT service. To construct trajectory data with textual labels, we enumerate all timesteps\nwithin the trajectory where an event occurs. From this point, we count 112 frames backward and\n16 frames forward to form a segment of 128 frames. The textual label for this segment is derived\nfrom the text associated with the event. It is important to note that many events occur frequently;\nfor example, when the player is mining a tunnel, the event \"mine block: cobblestone\" is triggered on\naverage twice per second. To address this issue, if a segment generated by an event overlaps with a\npreviously generated segment, it is skipped. Each event collects a maximum of 2000 segments, and\nacross all 1518 events, 414,387 segments are included. It is noteworthy that a significant amount of\nduplication persists within these segments, as a single segment may encompass multiple events.\nD. Language Table\nEnvironment Description. Language Table (Lynch et al., 2023) is a comprehensive evaluation suite\nproposed by the Google for assessing a robot’s ability to follow natural language instructions to solve\nTable Manipulation tasks. It includes a dataset, environment, benchmarks, and a baseline policy.\nThe evaluation benchmark encompasses over 87, 000 diverse behaviors and more than 600, 000\ntrajectories annotated with text instruction. In addition to data from real environments, the suite also\nprovides a simulator akin to a real environment along with corresponding simulated data.\nObservation and Action Spaces. Language Table’s simulated environment resembles the real-world\ntabletop manipulation scenario, which consists of an xArm6 robot, constrained to move in a 2D plane\nwith a cylindrical end-effector, in front of a smooth wooden board with a fixed set of 8 plastic blocks,\ncomprising 4 colors and 6 shapes. In both simulation and real collection, they use high-rate human\nteleoperation with a 3rd person view (line-of-sight in real). Actions are 2D delta Cartesian setpoints,\nfrom the previous setpoint to the new one. They batch collected training and inference data to 5hz\nobservations and actions.\nTraining Dataset. We use the training trajectories from the official Language Table repository.\nAn oracle script generates the trajectories and covers all 5 task families, each containing 20M\ntrajectories, in a total of 100M trajectories. The dataset names are: language-table-blocktoblock-\noracle-sim, language-table-blocktoblockrelative-oracle-sim, language-table-blocktoabsolute-oracle-sim,\nlanguage-table-blocktorelative-oracle-sim, language-table-separate-oracle-sim.\nTask Definition. The evaluation benchmark consists of 5 task families (block2block, block2abs,\n23\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nTable 7 | Sampled reference videos to build video instruction set.\nTask Family\nVideo Description\nblock to block\nput the red moon to the blue moon\nblock to block\nput the blue moon towards the yellow star\nblock to block\nslide the red pentagon close to the green cube\nblock to block\nslide the green star to the red moon\nblock to block\nput the green cube next to the red pentagon\nblock to absolute location\nslide the blue cube to the upper left corner\nblock to absolute location\npush the blue moon to the top left of the board\nblock to absolute location\nmove the red moon to the bottom left\nblock to absolute location\nslide the yellow star to the right side of the board\nblock to absolute location\npush the yellow pentagon to the left side\nblock to block relative location\nmove the green star to the left side of the yellow pentagon\nblock to block relative location\npush the green star diagonally up and to the right of the green cube\nblock to block relative location\nput the red moon to the bottom left side of the yellow star\nblock to block relative location\nslide the yellow pentagon to the bottom left side of the red pentagon\nblock to block relative location\nslide the blue cube to the top of the blue moon\nblock to relative location\npush the green cube right\nblock to relative location\nslide the yellow pentagon downwards and to the right\nblock to relative location\npush the blue cube somewhat to the left\nblock to relative location\nmove the blue moon to the right\nblock to relative location\nslide the red pentagon up\nseparate\npull the yellow pentagon apart from the blue moon\nseparate\npull the green star apart from the yellow star\nseparate\npull the blue cube apart from the blue moon and red pentagon\nseparate\nmove the blue cube away from the yellow star\nseparate\nmove the green star away from the yellow pentagon\nblock2rel, block2blockrel, separate), totaling 696 distinct task variants. We report the success rate\nof the agent within 200 steps on each task as the final metric. Considering that the Language Table\ninherently includes instructions in the language modality for its 5 task families, we have curated a\nset of reference videos for each task family, each with relatively clear intentions, to serve as a video\ninstruction set. This is done to evaluate the model’s ability to comprehend video instructions. The\ndetails are in Table 7. We visualize some examples when conditioning GROOT-2 on reference videos\nin Figure 12.\nE. Simpler Env\nEnvironment Description. Simpler Env is a physical simulator proposed by Li et al. (2024), efficient,\nscalable, and informative complements to real-world evaluations. It can be used to evaluate diverse\nsets of rigid-body tasks (non-articulated \/ articulated objects, tabletop \/ non-tabletop tasks), with\nmany intra-task variations (e.g., different object combinations; different object\/robot positions and\norientations), for each of two robot embodiments (Google Robot and WidowX).\nObservation and Action Spaces. The observation and action spaces of Simpler Env are the same as\nthe Language Table. The action sequence is expected to be a 6D end-effector pose trajectory with a\ngripper flag indicating the open\/close status. Before feeding the image observation into the policy,\nwe resize the image to a 224 × 224 resolution.\n24\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\nFigure 11 | IQM scores (with 95% confidence interval) of GROOT-2 which is jointly trained on 35 Atari\ngames. GROOT-2 can understand both the returns-format instructions and video-format instructions on\nmost of the games. The performance of the GROOT-2 exhibits a positive correlation with the game level\ncorresponding to the provided instructions.\n25\nGROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents\n“put blue moon next to \nthe blue pentagon”\n“push the green cube \ntowards the blue cube”\n“slide the green star \nnext to the green cube”\nReference Video\nRollout Trajectory\nFigure 12 | GROOT-2 can infer the intention behind the reference video and follow it to complete tasks.\nThe left visualizes three reference videos along with their textual descriptions. The right figure displays the\npolicy’s rollout trajectories when conditioned on the reference videos. The white dashed line represents the\narm’s movement trajectory, and the red dashed circle highlights the arm’s final position.\n26\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents.pdf"}
{"title":"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents","authors":"Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang","summary":"We investigate the challenge of task planning for multi-task embodied agents\nin open-world environments. Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla\nplanners do not consider how easy the current agent can achieve a given\nsub-task when ordering parallel sub-goals within a complicated plan, the\nresulting plan could be inefficient or even infeasible. To this end, we propose\n\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and\n$\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based\non Large Language Models (LLMs). DEPS facilitates better error correction on\ninitial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of\nthe plan execution process and providing self-$\\textit{explanation}$ of\nfeedback when encountering failures during the extended planning phases.\nFurthermore, it includes a goal $\\textit{selector}$, which is a trainable\nmodule that ranks parallel candidate sub-goals based on the estimated steps of\ncompletion, consequently refining the initial plan. Our experiments mark the\nmilestone of the first zero-shot multi-task agent that can robustly accomplish\n70+ Minecraft tasks and nearly double the overall performances. Further testing\nreveals our method's general effectiveness in popularly adopted non-open-ended\ndomains as well (i.e., ALFWorld and tabletop manipulation). The ablation and\nexploratory studies detail how our design beats the counterparts and provide a\npromising update on the $\\texttt{ObtainDiamond}$ grand challenge with our\napproach. The code is released at https:\/\/github.com\/CraftJarvis\/MC-Planner.","url":"http:\/\/arxiv.org\/abs\/2302.01560v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2302.01560v3","published":1675404387000,"comment":"NeurIPS 2023","pdf_text":"Describe, Explain, Plan and Select:\nInteractive Planning with Large Language Models\nEnables Open-World Multi-Task Agents\nZihao Wang1,2, Shaofei Cai1,2, Guanzhou Chen3, Anji Liu4, Xiaojian Ma4, Yitao Liang1,5∗\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2School of Intelligence Science and Technology, Peking University\n3School of Computer Science, Beijing University of Posts and Telecommunications\n4Computer Science Department, University of California, Los Angeles\n5Beijing Institute for General Artificial Intelligence (BIGAI)\n{zhwang,caishaofei}@stu.pku.edu.cn,rayment@bupt.edu.cn\nliuanji@cs.ucla.edu,xiaojian.ma@ucla.edu,yitaol@pku.edu.cn\nAbstract\nWe investigate the challenge of task planning for multi-task embodied agents in\nopen-world environments.2 Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners\ndo not consider how easy the current agent can achieve a given sub-task when\nordering parallel sub-goals within a complicated plan, the resulting plan could be\ninefficient or even infeasible. To this end, we propose “Describe, Explain, Plan\nand Select” (DEPS), an interactive planning approach based on Large Language\nModels (LLMs). DEPS facilitates better error correction on initial LLM-generated\nplan by integrating description of the plan execution process and providing self-\nexplanation of feedback when encountering failures during the extended planning\nphases. Furthermore, it includes a goal selector, which is a trainable module that\nranks parallel candidate sub-goals based on the estimated steps of completion,\nconsequently refining the initial plan. Our experiments mark the milestone of the\nfirst zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks\nand nearly double the overall performances. Further testing reveals our method’s\ngeneral effectiveness in popularly adopted non-open-ended domains as well (i.e.,\nALFWorld and tabletop manipulation). The ablation and exploratory studies detail\nhow our design beats the counterparts and provide a promising update on the\nObtainDiamond grand challenge with our approach. The code is released at\nhttps:\/\/github.com\/CraftJarvis\/MC-Planner.\n1\nIntroduction\nDeveloping multi-task agents that can accomplish a vast and diverse suite of tasks in complex domains\nhas been viewed as one of the key milestones towards generally capable artificial intelligence [36, 1,\n5, 10, 25]. To enable such capabilities, earlier works have suggested employing a hierarchical goal\nexecution architecture [2, 4], where a planner generates action plans that would then be executed by\nlow-level goal-conditioned controllers. This architecture has been delivering promising progress in\n∗Corresponding Author.\n2We borrow the term “open world” from the game community. It highlights that the agent can navigate inside\na diverse environment and accomplish open-ended tasks freely.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2302.01560v3  [cs.AI]  8 Jul 2024\nReachable region \nwithin 3 mins\nUnachievable \nroutes\nOptional routes\nChallenge #2: State-dependent Task Feasibility\nChallenge #1: Complex Sub-task Dependency\nMine diamond in Minecraft environment\n99%\n42%\n23%\n80%\n9%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1\n2\n3\nPlanner w\/ Learned Controller\nPlanner w\/ Oracle Controller\nTabletop Environment\nALFWorld\nMinecraft\nSuccess Rate\nManipulation in Tabletop environment\nPlanning success plummet in open worlds due to new challenges\nFigure 1: Planning success rates plummet in open worlds due to new challenges.\nmany robotics domains, including table-top and mobile manipulation [46, 4], 2D shape drawing [20]\nand table rearrangement [17]. However, whether such success can be transferred to a more open-ended\nworld with unlimited exploration areas and internet-scale knowledge remains open [14, 10, 13, 12, 19].\nTo understand the gap, we run Inner Monologue[17], a general and competitive hierarchical goal\nexecution model on a typical open-world domain Minecraft [18, 14, 10] and two classical robotic en-\nvironments ALFWorld [41] and Tabletop environments [40, 4]. The algorithm uses a Large Language\nModel (LLM) based planner that contains domain-specific knowledge for all three environments. In\nall environments, we use either an Oracle goal-conditioned controller or a learned one. Results are\nshown in the bar plot in Figure 1. First, even when the Oracle controller is used, the success rate of\nexecuting Minecraft tasks is much less than that of the other environments. Next, the task failure rate\nbecomes even higher in Minecraft when the learned controller is substituted. Both failures originate\nfrom unique challenges brought by open-world environments, which we identify in the following.\nFirst, compared to canonical environments (e.g., Atari [29] and robotic control suite [40]), open\nworlds have highly abundant object types with complex dependency and relation. As a result, ground-\ntruth plans typically involve a long sequence of sub-goals with strict dependencies. As Figure 1\nchallenge #1 suggests, it requires at least 13 sub-goals executed in proper order to obtain a diamond\nin Minecraft, while in Tabletop a task is typically no more than a few consecutive sub-goals.\nAnother challenge brought by the complicated tasks in an open-ended world is the feasibility of the\nproduced plans. Consider the example shown in Figure 1 (challenge #2). To craft a bed in Minecraft,\nthe fastest way is by either slaughtering a sheep to obtain wool, which can be used to craft beds, or\ncollecting beds from a village. However, since no sheep or village is reachable by the agent within 3\nminutes of gameplay, to craft a bed efficiently, the agent should choose to slaughter a spider and use\nmaterials (e.g., string) it drops to craft wool, and then a bed. That is, when dealing with a task that\ncan be completed by executing multiple possible sequences of sub-goals, the planner should be able\nto select the best route based on the current state of the agent. However, the complex and diverse\nstate distribution of open-world environments makes state awareness hard to achieve.\nTo tackle these problems, we propose “Describe, Explain, Plan and Select” (DEPS), an interactive\nplanning approach based on Large Language Models (LLMs) to alleviate the aforementioned issues.\nThe key to tackling the first challenge is to effectively adjust the generated plan upon failure.\nSpecifically, whenever the controller fails to complete a sub-goal, a descriptor will summarize the\ncurrent situation as text and send it back to the LLM-based planner. We then prompt the LLM\nas an explainer to locate the errors in the previous plan. Finally, a planner will refine the plan\nusing information from the descriptor and explainer. To improve the feasibility of generated plans\nconditioned on the current state, which is the second identified challenge, we use a learned goal-\nselector to choose the most accessible sub-task based on the proximity to each candidate sub-goal.\nOur experiments are conducted on 71 tasks in open-ended Minecraft without any demonstration.\nGiven the goal-conditioned controller for atom sub-tasks (i.e., mine log and mine stone), our zero-\n2\n(Re-)Planner\nLLM*\nController\nGoal-conditioned Policy\nSelector\nHPM\nDescriptor\nVLM\nExplainer\nLLM*\nInstruction\nplan 𝑃!\ngoal 𝑔!\nfeedback\naction\nobs\ndescription 𝑑!\nexplain\nEnvironment\nobs\nTask instruction: Obtain a diamond\nin Minecraft survival mode step-by-step?\nCandidate goals:\nSelected Goal 𝒈𝟏:\n× 4\nThe agent locates in the birch forest,\nwhich only has birch wood.\nDescription 𝒅𝒕: I succeed on goal 1-5. I\nfail on goal 6, mining 3\nwith\n.\nNow my inventory has 5 planks, …\nInitial Plan 𝑷𝟎:\n4\n16\n1\n8\n3\n3\n1\n1\n1\nExplanation: Because mining       needs to\nuse at least\n, which I do not have.\nSo I need to craft\nfirst.\n…\nFinished Plan  𝑃!: \n3\n3\n1\n1\n1\n1\nSelected Goal\n𝑔$\n× 3\nno other choices\nDescription 𝒅𝒕: I succeed on goal …. I \nfail on smelting 3\nfrom 3\n, on\n.\nMy inventory now has 3 iron ore, …\nUpdated Plan 𝑷𝒕: \n3\n1\n3\n1\n1\n3\nExplanation: Because smelting       needs\nto use\nand\n, which I do not have.\nSo I need to craft\nfirst.\nTask Finished !\nAgent\nEnv\n𝑎%\n𝑠%\nMine acacia wood\nCraft acacia planks\nCraft crafting table\nCraft wood pickaxe\nCraft stick\nMine cobblestone\nCraft stone pickaxe\nMine diamond\nMine coal\nMine iron ore\nCraft furnace\nSmelt iron ingot\nCraft iron pickaxe\nMine birch wood\nMine oak wood\nCraft birch planks\nCraft oak planks\nFigure 2: Overview of our proposed interactive planner architecture.\nshot3 LLM-based planner can finish all tasks within a limited horizon (3000-12000 steps for different\ntasks). We find DEPS outperforms all language planner baselines by nearly doubling the overall\nsuccess rate, with the same initial state and goal-conditioned controller. Our ablation and exploratory\nstudies then explain how our approach beats the counterparts and becomes the first planning-based\nagent that accomplishes the challenging ObtainDiamond task. DEPS does not require any planning\ntraining for the environment. Additionally, DEPS achieves between on-par and more than 50% relative\nimprovement over existing or concurrent LLM-based planning methods on non-open-ended robotics\ndomains such as ALFWorld [41] and Tabletop environments [40].\n2\nBackground\nWe aim to develop an agent capable of solving long-horizon goal-reaching tasks using image\nobservations and language goals. To accomplish this, we propose a combined approach involving\ngoal-conditioned policies (termed controllers) and a planner. The goal-conditioned policies are\ntrained to complete sub-goals, while the planner decomposes long-horizon tasks into a series of\nK short-horizon sub-goals, g1, . . . , gK, to be executed by the controller. At each time step t, the\ngoal-conditioned policy π(at | st, gk) generates an action at based on the current state st and the\nspecified sub-goal gk.\nPlanning with Large Language Models\nPrevious works have shown that LLMs such as Instruct-\nGPT [32] and Codex [8] can be used as zero-shot planners to generate sub-goal sequences for various\ntasks in embodied environments [16, 42]. Formally, given the task description T as prompt p, LLM\nacts as a planner to decode T into K sub-goals, g1, . . . , gK, which are then executed one by one by\nthe low-level controller π(at | st, gk) to accomplish the task.\nHowever, the above pipeline suffers from both challenges identified in Section 1. Regarding the first\nchallenge, the probability of generating a flawless plan directly from the task description decreases\nsignificantly as the required number of sub-goals increases. Moreover, even when the LLM generates\na correct plan, it is very likely that the plan is highly inefficient given the agent’s current state\n(challenge #2). Prior works mostly focus on solving the first challenge by providing environmental\nfeedback to the LLM through affordance functions [4], success detector [20] or scene descriptor [17].\nHowever, although these approaches work well on many non-open-ended domains, they still suffer\nfrom high failure rates in open-world environments.\n3Similar to [5, 16], “zero-shot” here means no gradient updates are performed. However we provide some\nrelated demonstrations as prompts during inference time.\n3\n3\nTowards Reliable Planning in Embodied Open-World Environments\nIn this section, we first give an overview of our proposed interactive planning framework “Descibe,\nExplain, Plan, and Select” (DEPS) for solving complex and long-horizon tasks in open-world\nenvironments (Sec. 3.1). Next, in Section 3.2, we elaborate how DEPS iteratively refines its plan\nto combat the first identified challenge. Section 3.3 introduces the selector module that is used to\nidentify efficient plans in response to the second identified challenge.\n3.1\nDEPS Overview\nAs demonstrated in Figure 2, our agent (DEPS) consists of an event-triggered Descriptor, a Large\nLanguage Model (LLM) as Explainer and Planner, a goal Selector based on horizon prediction and\na goal-conditioned controller. In the following, we use Minecraft as a running example to better\nelaborate our agent. Note that DEPS can be directly applied to other (non-)open-ended tasks.\nWe take a large language model (LLM) as a zero-shot planner of the agent to complete tasks. Given\na goal command (e.g., ObtainDiamond) as task T, the LLM-based planner decomposes this\nhigh-level task into a sequence of sub-goals {g1, . . . , gK}, as the initial plan P0. The goals are\ninstructions in natural language, such as mine oak wood\n(in Minecraft), find two cups (in\nALFWorld), put block A on top of block B (in Tabletop Manipulation).\nAs described in Section 2, a controller is then invoked to execute the provided sub-goals sequentially\nthrough a goal-conditioned policy π(a | s, g). However, the initial plan provided by the planner often\ncontains errors, which results in execution failures of the controller. For example, the goal\ncan\nnot be finished only with a wooden pickaxe\nas shown in Figure 2. When failure pops up, the\ndescriptor will summarize the current state st and execution outcome of the most recent goal into text\ndt and send it to the LLM. The LLM will first try to locate the errors in the previous plan Pt−1 by\nself-explanation, e.g., the goal\nneed to be executed with a stone pickaxe\n. Then it will re-plan\nthe current task T and generate a revised plan Pt according to the explanation. In this process, the\nLLM is also treated as an explainer in addition to the planner role. The Descriptor, Explainer, and\nPlanner will be detailed in Section 3.2.\nDescription : dt = fDESC(st−1),\nExplanation : et = fEX(dt),\nPrompt : pt = CONCAT(pt−1, dt, et),\nPlan : Pt = fLM(pt),\nGoal : gt ∼fS(Pt, st−1),\nAction : at ∼π(at | st−1, gt)\n(1)\nAs shown in Equation (1), DEPS will iteratively update the plan Pt until the task is finished, where\nfDESC is the descriptor model, fLM denotes the language model as explainer and planner, fS is the\nselector model, π is goal-conditioned policies from the controller.\nTo filter out inefficient plans, the selector is trained to predict the number of time steps remaining to\nachieve every goal gk in a set of parallel goals given the current state st. When the generated plan\ncontains alternative routes, the selector uses this information to choose a suitable goal as the current\ngoal gt. For example, the horizon predicted by the selector of goal acacia tree\nis less than\ngoal oak tree\nin Savanna biome, which leads to chop acacia tree as current goal gt.\n3.2\nDescribe, Explain and Plan with LLM Generates Executable Plans\nCurrent LLM-based planners usually query the LLM once at the beginning of every episode and use\nthe output plan throughout the episode [16, 42]. However, as demonstrated by Figure 1, such one-shot\nplanning methods often fail on long-horizon tasks that require many sub-goals. This is caused by two\nmajor issues. First, since the correct plan for long-horizon tasks needs to respect various complex\npreconditions, it is extremely hard for the LLM to generate a flawless plan directly from the task\ninstructions, resulting in failure when simply following the initial plan. Additionally, due to the\nunpredictable transition dynamics, some incidents may happen during the execution and make the\ninitial plan non-executable. To remedy these problems, existing methods introduce feedback (e.g.,\n4\nPrompt 1 Planner prompt template, Python-like code\ndef craft_wooden_axe(initial_inventory={}):\n# step 1: mine 3 logs\nmine(obj = {\"log\":3}, tool = None)\n# step 2: craft 12 planks from 3 logs\ncraft(obj = {\"planks\":12}, materials = {\"log\":3}, tool = None)\n# step 3: craft 4 sticks from 2 planks\ncraft(obj = {\"stick\":4}, materials = {\"planks\":2}, tool = None)\n# step 4: craft 1 crafting_table from 4 planks\ncraft(obj = {\"crafting_table\":1}, materials = {\"planks\":4}, tool = None)\n# step 5: craft 1 wooden_axe from 3 planks and 2 sticks on crafting table\ncraft(obj = {\"wooden_axe\":1}, {\"planks\": 3, \"stick\": 2}, tool = \"crafting_table\")\nreturn \"wooden_axe\"\nfrom success detector or scene descriptor) to reflect on the results of previous executions [17, 20, 4].\nHowever, merely informing the LLM whether a sub-goal is completed is often insufficient to correct\nthe planning error.\nTo remedy this, we propose “describe, explain and plan”, a new interactive planning method to\ngenerate more executable and explainable plans. We start with rewriting the prompt into an interactive\ndialogue format as in ChatGPT [32] so that subsequent feedback can be passed to the LLM effectively.\nThe produced plan is also augmented with the preconditions and effects of each goal. The structured\nprompt improves the readability and interpretability of the plan and facilitates error-locating when\nthe execution fails later, as demonstrated in Prompt 1.\nThe descriptor will then collect the feedback generated by the agent during the execution of the\ntask. The feedback can be practically obtained either by a person (human feedback [4]), or by a\npre-trained vision-language model CLIP [35]. While the previous type of feedback needs intensive\nhuman involvement, the latter from the pre-trained model needs to be fine-tuned for the specific\ndomain, which decreases the automation and generalization of the agent. On the contrary, Minecraft\nreturns the ‘info’ and other high-level observations (such as biome, GPS, and compass), we can easily\ntranslate the unstructured information into structured language. Therefore we take the symbolic\ninformation available in the game and translate it into feedback description dt in this work. To avoid\ncarrying unrelated information in the prompt, we further distill plan-related messages (e.g., inventory\ninformation, biome) as final event-level description dt as demonstrated in Figure 2.\nNotably, we also treat the LLM as an explainer to explain why the previous plans Pt−1 failed.\nSpecifically, by analyzing the current state from description dt and precondition of current goal gt,\nthe explainer can identify the reason why the current goal cannot be executed successfully. As shown\nin Figure 2, the reason may be the current goal requires the use of an iron pickaxe, but the tool is\nnot prepared in advance, or the current goal requires the use of 3 planks, but the currently available\nplanks are not enough. To implement this, we provide few-shot demonstrations to the LLM as in\nchain-of-thoughts prompting [45], as shown in Prompt 1. Finally, the LLM goes back to its role as a\nplanner and re-plans the task with the explicit explanation of existing bugs in the previous plan Pt−1,\nultimately generating an updated plan Pt according to the explanation.\n3.3\nHorizon-Predictive Selector Yields Efficient Plans\nDue to the abundance of objects and the compositional nature of their functionalities, there often exist\nmultiple feasible plans to complete a task, i.e., there are usually multiple paths for the completion of a\nparticular goal. However, despite the feasibility of all such plans, most of them are highly inefficient\nto execute in the current episode. For example, as shown in Figure 2, obtaining a wood can be done\nby chopping oak trees\n, birch trees\n, or acacia trees\n. But only oak trees are available in the\nplains biome. So the planner needs to choose oak trees since it is more efficient, as the agent does\nnot need to travel to another biome.\nOn the other hand, there is no strict sequential requirement for some goals in the plan Pt, i.e.,\ngi, gj ∼Pt enjoy the same precondition, which means gi and gj can be executed in any order. As\nshown in Figure 1, the choice of different paths (sequences) may affect the execution efficiency of\nthe plan Pt as one goal might be closer to the agent. Always choosing the closer goal to execute first\ncould yield more efficient plans and improve the final success rate under a limited episode length.\nMoreover, the dynamic nature of open-world environments further amplifies the impact of efficient\n5\nGoal: Meat*3\nCandidate Skill: Kill Sheep\nOR Cow OR Pig\nSelection: Kill Sheep\nExplanation: Meet sheep first.\nGoal: Log*2\nCandidate Skill: Chop Oak\nOR Birch OR Acacia Tree\nSelection: Chop Acacia Tree\nExplanation: Savanna biome\nonly has Acacia tree.\nGoal: Coal*1 AND Iron_Ore*1\nCandidate Skill: Mine Coal AND\nIron_Ore\nSelection: Mine Iron_Ore\nExplanation: Meet iron_ore first.\nGoal: Survive in Night.\nCandidate Skill: Sleep in bed\nOR Dig down.\nSelection: Sleep_in_bed\nExplanation: Village has beds.\nFigure 3: Selection Demonstration from “Selector”. Given parallel sub-goals, i.e. candidate skills, our Selector\nwill determine the sequence in which to carry out these sub-goals based on their current proximity to the agent\nand modify the original plan produced by the LM planner.\nplans on the success rate. For example, in Minecraft, if the agent chooses to execute a further goal\nlike collect wood first, the much closer target sheep may disappear and be hard to find again.\nIn order to improve the efficiency of our plans, we propose to use a selector that selects the most\nefficient path with the highest execution success rate as the final plan. Specifically, we design a\nstate-aware selector to choose the nearest goal under state st as the current goal gt from the candidate\ngoal sets in plan Pt. It predicts the goal distribution p(gt|st, Pt) under the current state st and plan\nPt, where gt ∈Gt, Gt describes all current executable goals in Pt. A straight way to implement\nthe selector is to leverage the semantic similarity between the current state and the goal text using\na vision-language model (VLM) such as CLIP [35]. Nevertheless, this may not exactly reflect the\ndifficulty of completing the goal since VLM lacks practical experience. For example, an “oak tree” in\nfront of the agent could lead to high semantic similarity for the “chopping tree” goal, but it may be\ninefficient to achieve this goal if a canyon is in the middle between the agent and the oak tree.\nTo mitigate this, we implement a horizon-predictive selector that embeds practical task experience to\naccurately rank the goals based on their efficiency and feasibility. Here, we define the horizon of a\ngoal ht(g) := Tg −t as the remaining time steps to complete the given goal, where Tg is the time of\ncompleting goal g. This metric accurately reflects how quickly we can achieve the given goal from\nthe current state. To estimate the horizon, we learn a neural network µ to fit the offline trajectories by\nminimizing the entropy loss −log µ(ht(g) | st, g), where ht is the ground-truth horizon in trajectories\nof completing goal g. Therefore, the goal distribution can be formulated as follows:\nf(gt | st, Pt) =\nexp(−µ(gt, st))\nP\ng∈Gt exp(−µ(g, st)).\n(2)\nWe set goal-sensitive Impala CNN [6] as the backbone of the selector. In practice, the horizon predic-\ntive selector can be jointly trained with the controller policies and share the backbone parameters [6].\n4\nExperiments\nThis section analyzes and evaluates our proposed “describe, explain, plan, and select\" (DEPS) method.\nTo minimize performance variation caused by the low-level controller, we standardize all experiments\nwith one controller learned by behavior cloning. We refer to the details of this controller in Appendix\nC. In Section 4.1, we introduce our testing environments and our evaluation task set, consisting of the\nhardest 71 tasks from MCU SkillForgeChain [22]. In Section 4.2, we report our performance in the\ncontext of existing LLM-based planners. Ablation studies are conducted in Section 4.3. Finally, we\npay close attention to the hardest task, ObtainDiamond, which is long-hailed as a major challenge\nin the community. The experiments on ALFWorld and Tabletop Manipulation environments are\nshown in Appendix A.\n4.1\nExperimental Setup\nEnvironment and Task Setting\nWe first evaluate our proposed method in Minecraft, a popular\nopen-world environment with both challenges discussed in Section 1. For better reflecting the\nperformance of DEPS, we choose three Minecraft environments with different versions for better\n6\nevaluation, including Minedojo [10] with Minecraft 1.11.2, MineRL [3] with Minecraft 1.16.5, and\nMC-TextWorld [22] with Minecraft 1.19.2. Rules and items have something different in the above\nthree Minecraft environments, which can better evaluate the dynamic and interactive planning abilities\nof DEPS.\nTable 1: Attributes of 8 meta tasks covering Task101: We evaluate the algorithm on Minecraft Task101. We\ngroup the consisted 71 task into 8 different meta groups, with each focusing on testing a different aspect of our\nproposed method.\nMeta\nName\nNumber\nExample Task\nMax. Steps\nInitial Inventory\nGiven Tool\nMT1\nBasic\n14\nMake a wooden door.\n3000\nEmpty\nAxe\nMT2\nTool (Simple)\n12\nMake a stone pickaxe.\n3000\nEmpty\nAxe\nMT3\nHunt and Food\n7\nCook the beef.\n6000\nEmpty\nAxe\nMT4\nDig-Down\n6\nMine coal.\n3000\nEmpty\nAxe\nMT5\nEquipment\n9\nEquip the leather helmet.\n6000\nEmpty\nAxe\nMT6\nTool (Complex)\n7\nMake shears and bucket.\n6000\nEmpty\nAxe\nMT7\nIronStage\n13\nObtain an iron sword.\n6000\nEmpty\nAxe\nMT8\nChallenge\n1\nObtain a diamond!\n12000\nEmpty\nAxe\nWe choose 71 tasks from the Minecraft Universe Benchmark SkillForgeChain [22] for evaluation.\nThese tasks are related to items that can be obtained in the Minecraft overworld. To better present the\nresults, we divide the 71 Minecraft tasks into 8 meta groups according to the ingredients and function\nof the tasks, i.e., MT1-MT8. The instruction for every task is written in natural language, e.g., make\na wooden door in MT1 (Basic group) and obtain a diamond in MT8 (Challenge group),\nas illustrated in Table 1. Considering how long it typically takes human players to complete each\ntask as a ballpark [14], we set different maximum episode steps for different meta tasks from 3000\n(for easiest Basic tasks) to 12000 (for the hardest Challenge tasks). The names, number of required\nskills, and functions of all tasks are listed in Appendix B. We give an empty inventory for every task\nin Survival mode and require the agent to obtain every item from the environment by itself. Note that\nour agent will be summoned in different environments randomly for each evaluation. Biomes and\ninitial positions are also different each time. Following the previous work [18], we take the success\nrate as the evaluation metric.\nBaselines\nWe compare DEPS with other language-based planners, including GPT as Zero-shot\nPlanner(GPT) [16], ProgPrompt(PP) [42], Chain-of-Thought(CoT) [45], Inner Monologue(IM) [17],\nand Code as Policies(CaP) [20]. For all baseline models, we use the same demonstration example\nin the prompt, the same LM model from OpenAI, and the same controller in all tasks for a fair\ncomparison. Since these methods were not originally experimented with Minecraft, we reproduce\nthem to conform to the Minecraft specification based on prompt and feedback template design. All\nplanner methods access the LLM model through OpenAI API (text-davinci-03 model [32]\nfor GPT, CoT, and IM, and code-davinci-02 model [8] for PP, CaP, and Ours). All hyper-\nparameters of LLM (including the temperature and best_of, etc.) are kept as default. We also list the\nfull prompt of all different methods in Appendix G.\n4.2\nMain Results\nEvery task is executed 30 times and the average results in Minedojo [10] for every meta task are listed\nin Table 2. Our approach achieves the best performance with all meta tasks. As the complexity of the\ntask increases from MT1-MT8, the planner usually needs to give more accurate task steps (i.e., longer\ngoal sequence) to achieve the final task. Therefore the success rate of all agents decreases with the\nreasoning steps increasing. Starting from MT6, almost all existing LLM-based planners fail (nearly\n0 success rate). DEP (w\/o Selector) already consistently beats existing LLM-based planners in all\nmeta tasks with a significant margin. This validates that “describe, explain and plan” can estimate the\nreason for current plan failure and correct the original flawed plans. Due to the limited maximum\nepisode length and restricted control success rate for a hard goal (e.g., Mine diamond with\niron_pickaxe), the final success rate is still capped.\n7\nTable 2: Success rates of DEPS and existing LLM planners on Minecraft Task101. The full task-by-task list is in\nAppendix F.\nMethods\nMT1\nMT2\nMT3\nMT4\nMT5\nMT6\nMT7\nMT8\nAVG\nGPT[16, 32]\n25.85±24.8\n47.88±31.5\n10.78±14.6\n7.14±9.0\n1.98±5.9\n0.0±0.0\n0.0±0.0\n0.0±0.0\n15.42\nPP[42]\n30.61±23.6\n40.09±30.6\n17.13±19.1\n16.00±17.3\n3.21±4.9\n0.47±1.3\n0.60±2.2\n0.0±0.0\n16.88\nCoT[45]\n40.24±30.8\n55.21±26.8\n6.82±11.6\n4.76±8.2\n1.73±5.2\n0.0±0.0\n0.0±0.0\n0.0±0.0\n18.89\nIM[17]\n46.89±31.4\n53.73±20.8\n3.64±6.9\n18.41±17.4\n4.57±7.4\n0.64±1.7\n1.02±2.5\n0.0±0.0\n21.64\nCaP[20]\n60.08±17.3\n60.11±20.24\n8.72±9.7\n20.33± 21.0\n2.84±4.6\n0.63±1.3\n0.60±2.2\n0.0±0.0\n25.77\nDEP\n75.70±10.4\n66.13±13.4\n45.69±16.2\n43.35±20.2\n15.93±13.9\n5.71±3.7\n4.60±7.1\n0.50±0.5\n39.36\nDEPS\n79.77±8.5\n79.46±10.6\n62.40±17.9\n53.32±29.3\n29.24±27.3\n13.80±8.0\n12.56±13.3\n0.59±0.5\n48.56\nIn addition, selector also greatly improves the final task success rate of the agent (from DEP w\/o\nSelector to DEPS). Hard meta tasks usually require the completion of multiple sub-goals (up to\ndozens of goals), thus bringing more flexibility and providing more candidate goals for the Selector.\nAt the same time, as the agent conducts experiments with limited episode length, it also places high\ndemands on the efficiency of the plan. Therefore, the Selector brings a significant improvement on\nefficiency-sensitive tasks such as MT7 (up to +2.7 times success rate).\nRobustness on different controller and different Minecraft versions\nWe also evaluate DEPS\non MineRL [3] and MC-Textworld [22]. Note that DEPS is a planning method, which needs to\nequip the goal-conditioned controller for interacting with the Minecraft environments. We choose\nMC-Controller [6] and Steve-1 [21] as controllers to interact with Minedojo and MineRL, respectively.\nThese two methods are all control policies that perceive visual partial observations and produce\nmouse and keyboard actions. While MC-Textworld is a text world, which only keeps the Minecraft\ncrafting recipes and mining rules. So MC-Textworld does not require the controller. The DEPS\nresults of the task set MT1-MT8 on different Minecraft environments are shown in Table 3. The\nresults report that DEPS can generate effective plans in various Minecraft environments. The results\non MC-Textworld [22] also show that the performance drops on more difficult task sets from MT6 to\nMT8 are mainly from the controller limitation.\nTable 3: Success rates of DEPS under different Minecraft environments.\nEnvironment\nVersion\nController\nMT1\nMT2\nMT3\nMT4\nMT5\nMT6\nMT7\nMT8\nMineDojo [10]\n1.11.2\n[6]\n79.77\n79.46\n62.40\n53.32\n29.24\n13.80\n12.56\n0.59\nMineRL [3]\n1.16.5\n[21]\n84.05\n80.32\n24.25\n36.21\n9.16\n17.22\n16.79\n1.84\nMC-Textworld [22]\n1.19.2\n-\n100.00\n90.00\n80.00\n56.25\n64.71\n57.14\n69.57\n50.00\n4.3\nAblation Study\nWe conduct ablation experiments to investigate the number of candidate executable goals for different\nSelector models and the specific impact of the rounds of DEPS.\n4.3.1\nAblation on Selector\nWe verify the robustness of our proposed Selector under different parallel goals. The agent is asked\nto complete 2, 3, and 4 candidate goals (the precondition is consistent for all goals), respectively. The\ngoals of the task correspond to different kinds of mobs or materials.\nWe report the final success rate of our method (DEP) with different selector implementations,\nincluding using a fixed sequence of goals, a random sequence of goals, and selecting a goal based on\nMineCLIP [10], CLIP [35], and our horizon-predictive Selector (HPS). As Figure 4 shows, in one\nround of parallel candidate goals, an improvement of ∆=+22.3%, +29.2%, +32.6% is obtained using\nour horizon-predictive Selector compared to not any selector (i.e., fixed plan), respectively.\nAt a limited episode length, e.g., 1000 steps, goal-model shows a greater advantage, which proves that\ngoal-model can improve the execution efficiency of the plan in embodied environments. In addition,\ncompared to using vision-language models such as CLIP [35] and MineCLIP [10] as a goal model,\nhorizon-predictive has the best performance due to better estimation of the horizon information. The\ncurve trend also demonstrates that agents with Selector scale up under large amounts of goals in an\nopen-world environment.\n8\nMaximum Episode Length\nSuccess Rate\nSelector Model\nHPS\nCLIP\nMineCLIP\nFixed\nRandom\nParallel Goals = 2\nParallel Goals = 3\nParallel Goals = 4\nSuccess Rate\nGoals Setting\nBiome: Plains\n2:\n3:\n4:\nFigure 4: The success rates of DEPS with differ-\nent selectors under varying numbers of parallel\ngoals and maximum episode lengths.\nTable 4: Success Rate of DEPS under different maxi-\nmum rounds of re-planning. Round 0 represents the\nvanilla Planner w\/o the re-planning process. ∞rep-\nresents the re-planning process will not end until task\nsuccess or reaching the maximum horizon, which is still\nlimited by the maximum tokens of LLMs. The maxi-\nmum number of rounds for Codex is around 7-8 rounds.\nRounds\n0\n1\n3\n5\n∞\n∆\n(0 →∞)\nMT1\n28.6\n50.6\n68.1\n79.8\n79.8\n+51.2\nMT2\n37.1\n71.2\n71.4\n79.2\n79.5\n+42.4\nMT3\n15.1\n20.1\n40.3\n40.8\n62.4\n+47.3\nMT4\n15.9\n17.4\n48.3\n50.7\n53.3\n+37.4\nMT5\n3.2\n3.2\n3.2\n15.2\n29.2\n+26.0\nMT6\n0.5\n0.5\n1.1\n1.9\n13.8\n+13.3\nMT7\n0.6\n2.3\n2.9\n2.9\n12.6\n+12.0\nMT8\n0.0\n0.0\n0.0\n0.0\n0.6\n+0.6\n4.3.2\nAblation on Re-Planning Rounds\nWe evaluate our agent on all tasks with increasing maximum rounds of DEPS. The round is defined as\na cycle of interactive LLM-based planning with description, explanation, and planning and selecting,\ni.e., an updated plan. All tasks for every maximum round are executed 30 times and the average\nsuccess rate is reported in Table 4. We take the vanilla LLM planner as the baseline, in which the\nmodel takes the initially generated plan as the final execution plan, without involving any description,\nre-planning, or self-explanation processes during the task execution. Our results in the previous\nsubsection utilize the maximum rounds possible under maximum tokens capped by OpenAI. We\nalso report the success rate increment from vanilla planner to DEPS of every meta task in column\n∆in Table 4. This set of experiments demonstrates that DEPS can iteratively improve its plan in\nopen-world environments. More description, self-explanation, and re-planning rounds produce better\nresults, especially for hard tasks.\n4.4\nObtainDiamond Challenge\nMining diamonds in the open-world game Minecraft, i.e. MT8 in Table 2, has been a long-standing\nchallenge for the community [14]. It is challenging because mining diamonds from scratch in\nMinecraft involves acquiring a sequence of difficult-to-obtain items that require complex planning\non goals like mining, inventory management, crafting with and without a crafting table, tool use,\nsmelting iron ingot in a furnace, and mining at the lowest depths. We take the ObtainDiamond\ntask as a bonus experiment to show the capabilities of our zero-shot planner on complex tasks in\nembodied environments. Previous methods’ success rates on this challenge further vouch for its\ndifficulty. [43, 34] leverages domain-secific reward functions and RL fine-tuning to achieve ∽0.1%\nsuccess rate in 15 minutes of game play. VPT further boosts the success rate to 20% within 20\nminutes of play through pre-training on collects ∽70k hours human demonstrations and finetuning\nwith human-designed reward function [3]. DreamerV3 is trained from scratch to collect diamonds in\na modified Minecraft environment (easier to break blocks) with world models to achieve a success\nrate of 2% [15].\nOur DEPS manages to achieve on-par performance in this grand challenge; our agent achieves a\n0.59% success rate within 10 minutes of gameplay. Note our method does not specifically fine-tune\nfor this challenge. It is designed to be multi-task in its nature. Furthermore, considering our planner\noperates with demonstration prompts on a fixed Large Language Model, it can be straightforwardly\nadapted to other open-ended environments with modifications.\n5\nRelated Works\nTask planning with LLMs\nThere have been some methods leveraging the large language model\nto generate action plans for high-level tasks in embodied environments [46, 9, 11]. [16] decompose\nnatural language commands into sequences of executable actions by text completion and semantic\n9\ntranslation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted\nby skill affordances from value functions [4]. For better executing the plan in embodied environments,\nsome methods use an object detector describing the initial environment into the language prompt to\nproduce environment-suitable plans and adopt success detectors to check that each step is executed\nsuccessfully [17, 20]. [42] and [20] use the pythonic-style prompt to produce more executable plans.\nHowever, all of the above methods assume that the initial plan from the LLM is correct. When there\nare bugs in the initial plan, it’s difficult for the agent to finish the task successfully.\nInteractive Planning with LLMs\nInner Monologue [17] pilots the front of interactive planning\nwith LLMs, which introduces the feedback (including success detection and scene description) to\nthe planner. However, we found it could still suffer from accumulative planning error, especially in\nlong-horizon open-world tasks. Rather, our “Describe, Explain, Plan and Select” (DEPS) method can\nproduce more reliable plans by leveraging chain-of-thought thinking and explanation to locate the\nerrors in previous plans. Moreover, we also propose a goal Selector to further improve the efficiency\nof the plan, thereby yielding much better performances. Readers are encouraged to refer to the\ncomparative results in Section 4.2 between DEPS and these prior arts. There are also some concurrent\nworks on planning with LLMs [39, 26, 23, 33, 47].\nAgents in Minecraft\nSome previous works have employed the hierarchical architecture to solve\nlong-horizon tasks in Minecraft [30, 27, 24]. Recently, based on the internet-scale corpus, [10]\npre-trains a language-conditioned reward function and learns multi-task MineAgent. [3] collects a\nvast amount of human demonstrations to train a behavior cloning agent. More recently, [15] utilized a\nlearned world model to distill a policy that can efficiently explore in Minecraft. There are also some\nworks focus on learning goal-conditioned policies for better instruction-following [6, 7, 21]. While\nthese efforts all focus on improving the low-level controller. Rather, the planner in our architecture\nemphasizes applying domain knowledge to propose and arrange the sub-goals. It significantly\ninfluences the complexity and breadth of tasks that the agent can handle. Moreover, our planner is\nzero-shot, making it possible to generalize to other long-horizon open worlds.\n6\nLimitations\nAlbeit the impressive results of our approach, we believe there are at least two major limitations within\nour approach. First of all, our framework relies on privately-held LLMs like GPT-3 and ChatGPT,\nwhich makes it less accessible to those who cannot afford or access the service. However, we’re fully\ncommitted to ensuring a more democratized method and will explore using open-sourced models\nincluding OPT [48] and BLOOM [38]. Another issue is the explicit step-by-step planning in our\nsystem. Although it brings us superior performances over the baselines, the planning bottleneck can\nalso prevent our model from being further scaled up. A more appealing approach will be amortizing\nthe planning within an end-to-end trainable goal-conditioned policy, which is worth exploring next.\nFurthermore, some previous fundamental challenges in planning (e.g., dead ends) may not prevalent\nin our adopted environments and hence could be inadvertently overlooked by our paper. We are\ndedicated to addressing more fundamental challenges present in building a multi-task generalist agent\nin our series of following work.\n7\nConclusion\nWe investigate the problem of planning in open worlds. We identify two major challenges unique to\nthese environments: 1) long-term planning requires precise and multi-step reasoning, and 2) planning\nefficiency could be compromised since canonical planners do not take the agent’s proximity to parallel\ngoals\/subtasks into consideration. We propose “Describe, Explain, Plan and Select” (DEPS), an\ninteractive approach based on Large Language Models (LLMs) to tackle them both. Our experiments\nin the challenging Minecraft domain verify the advantages of our approach over counterparts by\nmarking the milestone of robustly accomplishing 70+ Minecraft tasks and nearly doubling the overall\nperformances. DEPS also is the first planning-based agent that can reach the diamond in this game.\n10\nAcknowledgements\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301, a\ngrant from CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441,\n#CCF-1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from\nRelationalAI. We thank Dai Zhixiang from NVIDIA and Xu Hongming from BIGAI on training\nLLMs and infrastructure supports, respectively.\nReferences\n[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022. 1\n[2] P.-L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the AAAI\nconference on artificial intelligence, 2017. 1\n[3] B. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro,\nand J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos.\narXiv preprint arXiv:2206.11795, 2022. 7, 8, 9, 10\n[4] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang,\nR. Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In 6th\nAnnual Conference on Robot Learning, 2022. 1, 2, 3, 5, 10, 21\n[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020. 1, 3, 15, 21, 22\n[6] S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023.\n6, 8, 10, 21, 24\n[7] S. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang. Groot: Learning to follow instructions\nby watching gameplay videos. arXiv preprint arXiv:2310.08235, 2023. 10\n[8] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021. 3, 7, 21, 22\n[9] I. Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R. Fergus. Col-\nlaborating with language models for embodied reasoning. In NeurIPS Foundation Models for\nDecision Making Workshop, 2022. 9\n[10] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,\nand A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale\nknowledge. Advances in Neural Information Processing Systems Datasets and Benchmarks,\n2022. 1, 2, 7, 8, 10, 18, 21, 26\n[11] R. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos,\nL. Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971,\n2023. 9\n[12] W. H. Guss, M. Y. Castro, S. Devlin, B. Houghton, N. S. Kuno, C. Loomis, S. Milani, S. P.\nMohanty, K. Nakata, R. Salakhutdinov, J. Schulman, S. Shiroshita, N. Topin, A. Ummadisingu,\nand O. Vinyals. The minerl 2020 competition on sample efficient reinforcement learning using\nhuman priors. arXiv: Learning, 2021. 2\n[13] W. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. Mohanty, D. P. Liebana,\nR. Salakhutdinov, N. Topin, et al. Neurips 2019 competition: the minerl competition on sample\nefficient reinforcement learning using human priors. arXiv preprint arXiv:1904.10079, 2019. 2\n[14] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov.\nMinerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440,\n2019. 2, 7, 9, 24\n[15] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world\nmodels. arXiv preprint arXiv:2301.04104, 2023. 9, 10\n11\n[16] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. ICML, 2022. 3, 4, 7, 8, 9, 15, 16, 17, 21,\n24, 25, 26, 28\n[17] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\nmodels. arXiv preprint arXiv:2207.05608, 2022. 2, 3, 5, 7, 8, 10, 15, 16, 17, 24, 25, 26, 30, 34,\n35\n[18] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial\nintelligence experimentation. In Ijcai, pages 4246–4247. Citeseer, 2016. 2, 7\n[19] A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang,\nW. Hong, Z. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret, A. Nikulin,\nY. Belousov, O. Svidchenko, and A. Shpilman. Minerl diamond 2021 competition: Overview,\nresults, and lessons learned. neural information processing systems, 2022. 2\n[20] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as\npolicies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753,\n2022. 2, 3, 5, 7, 8, 10, 25, 26, 31\n[21] S. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith. Steve-1: A generative model for\ntext-to-behavior in minecraft. arXiv preprint arXiv:2306.00937, 2023. 8, 10\n[22] H. Lin, Z. Wang, J. Ma, and Y. Liang. Mcu: A task-centric framework for open-ended agent\nevaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023. 6, 7, 8, 18\n[23] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language\ninstructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023. 10\n[24] Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, and W. Yang. Juewu-mc: Playing minecraft with sample-\nefficient hierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021. 10\n[25] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang. Sqa3d: Situated question\nanswering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. 1\n[26] J. Mai, J. Chen, B. Li, G. Qian, M. Elhoseiny, and B. Ghanem. Llm as a robotic brain: Unifying\negocentric memory and control. arXiv preprint arXiv:2304.09349, 2023. 10\n[27] H. Mao, C. Wang, X. Hao, Y. Mao, Y. Lu, C. Wu, J. Hao, D. Li, and P. Tang. Seihai: A sample-\nefficient hierarchical ai for the minerl competition. In Distributed Artificial Intelligence: Third\nInternational Conference, DAI 2021, Shanghai, China, December 17–18, 2021, Proceedings 3,\npages 38–51. Springer, 2022. 10\n[28] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer.\nRethinking the role of demonstrations: What makes in-context learning work? arXiv preprint\narXiv:2202.12837, 2022. 23\n[29] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.\nPlaying atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 2\n[30] J. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep\nreinforcement learning. In International Conference on Machine Learning, pages 2661–2670.\nPMLR, 2017. 10\n[31] OpenAI. Gpt-4 technical report, 2023. 21, 22\n[32] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\narXiv preprint arXiv:2203.02155, 2022. 3, 5, 7, 8\n[33] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative\nagents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023. 10\n[34] V. P. Patil, M. Hofmarcher, M.-C. Dinu, M. Dorfer, P. M. Blies, J. Brandstetter, J. A. Arjona-\nMedina, and S. Hochreiter. Align-rudder: Learning from few demonstrations by reward\nredistribution. ICML, 2020. 9\n[35] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\nIn International Conference on Machine Learning, pages 8748–8763. PMLR, 2021. 5, 6, 8\n12\n[36] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,\nY. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175,\n2022. 1\n[37] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084, 2019. 23\n[38] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow, R. Castagné, A. S. Luccioni,\nF. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100, 2022. 10\n[39] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory\nand self-reflection. arXiv preprint arXiv:2303.11366, 2023. 10\n[40] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipula-\ntion. In Conference on Robot Learning. PMLR, 2022. 2, 3, 15, 16, 17, 23\n[41] M. Shridhar, X. Yuan, M.-A. Côté, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning\ntext and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768,\n2020. 2, 3, 15, 16\n[42] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv\npreprint arXiv:2209.11302, 2022. 3, 4, 7, 8, 10, 25, 26, 28\n[43] A. Skrynnik, A. Staroverov, E. Aitygulov, K. Aksenov, V. Davydov, and A. I. Panov. For-\ngetful experience replay in hierarchical reinforcement learning from expert demonstrations.\nKnowledge-Based Systems, 218:106844, 2021. 9\n[44] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 22\n[45] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. 36th Conference on Neural Information\nProcessing Systems (NeurIPS 2022), 2022. 5, 7, 8, 15, 25, 26, 29\n[46] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani,\nJ. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with\nlanguage. arXiv preprint arXiv:2204.00598, 2022. 2, 9\n[47] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu,\net al. Proagent: Building proactive cooperative ai with large language models. arXiv preprint\narXiv:2308.11339, 2023. 10\n[48] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V.\nLin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,\n2022. 10\n13\nAppendix\nContents\nA Additional Experiments\n15\nA.1 ALFWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.1.1\nTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.1.2\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.2 Tabletop Manipulation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.2.1\nTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nA.2.2\nResults\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB\nMinecraft Task Details\n18\nC DEPS Implementation Details\n20\nC.1\nController . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.2\nLLM as Planner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.3\nLLM as Explainer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC.4\nOther modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nD Comparison with other LLM-based Planners\n23\nE\nDiscussion on ObtainDiamond Task\n24\nF\nSuccess Rates of ALL Tasks in Minecraft\n25\nG Prompt for Different Tasks and Different Methods\n27\nG.1\nPrompt for Minecraft Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nG.1.1\nDEPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nG.1.2\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nG.2\nPrompt for ALFWorld Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.2.1\nDEPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nG.2.2\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nG.3\nPrompt for Tabletop Manipulation Tasks . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.3.1\nDEPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.3.2\nBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nH Full Dialogue\n35\n14\nA\nAdditional Experiments\nAdditional experiments are conducted on the ALFWorld [41] and Tabletop Manipulation environ-\nments [40] to showcase the generalization capabilities of DEPS.\nA.1\nALFWorld\nALFWorld [41] is an interactive learning environment that aligns text and embodiment, allowing\nagents to acquire abstract, text-based policies in TextWorld, and subsequently execute goals from the\nALFRED benchmark in a visually rich environment.\nA.1.1\nTasks\nThe ALFWorld framework contains six types (namely Pick & Place, Examine in Light,\nClean & Place, Heat & Place, Cool & Place, Pick Two & Place) of tasks with\nvarious difficulty levels. Tasks involve first finding a particular object, which often requires the agent\nto open and search receptacles like drawers or cabinets. Subsequently, all tasks other than Pick &\nPlace require some interaction with the object such as heating (place the object in a microwave and\nstart it) or cleaning (wash the object in a sink). To complete the task, the object must be placed in the\ndesignated location. We sample 10 tasks from ALFWorld randomly and list all the task names, types,\nand the number of receptacles in Table 5. We classify them into 6 groups based on their functionality.\nFor all tasks, the maximum number of steps is set as 50.\nTable 5: Task list in ALFWorld.\nGroup\nNo.\nTask\nNumber of Receptacles\nPick & Place\n1\nput some soapbottle on garbagecan\n13\n2\nput a tissuebox in dresser\n26\n3\nput some soapbar on drawer\n15\nClean & Place\n4\nput a clean soapbar in bathtubbasin\n16\n5\nclean some tomato and put it in fridge\n35\nCool & Place\n6\nput a cool tomato in countertop\n30\n7\nput a cool bread in countertop\n27\nHeat & Place\n8\nheat some cup and put it in cabinet\n36\nPick Two & Place\n9\nfind two cup and put them in cabinet\n36\nExamine in Light\n10\nlook at mug under the desklamp\n18\nWe select the GPT as Zero-Shot Planner (GPT) [16] and Inner Monologue (IM) [17] as baseline\nmethods. For the Inner Monologue, the planning goal is the next goal among all candidate goals.\nFor the GPT and DEP, which produce the full plan at once, the planning goal is the full plan (a goal\nsequence). Then the plan will be executed step-by-step, i.e., the current goal will be given to the\ncontroller and select suitable action according to the current state. The goal termination module is\nalso employed with the LLM. For better demonstrate the effectiveness of self-explanation in DEP, we\nalso augment the zero-shot planner with re-planning ability (GPT+RP). All planner methods access\nthe LLM model through OpenAI API (text-davinci-03 model [5]). Since ALFWorld is a text\nworld, the environment will be given a literal description and candidate language-conditioned actions\nfor each state, so the controller under ALFWorld is also LLM-based. Chain-of-Thought [45] is also\nemployed in the controller for better decision-making. All prompts for planner and controller in\nALFWorld are listed in Section G.2.\nA.1.2\nResults\nEach task is executed five times, and the average results for each task group are presented in Table 6.\nBUTLER is the a training-based method, the results are sourced from [41]. Re-planning is a crucial\ncapability in complex and exploratory environments. The short-horizon planning approach (IM) with\n15\ninitial frame\nGoal: pick a soapbottle \nat cabinet 4\nAction: go to cabinet 4\nGoal: pick a soapbottle \nat cabinet 4\nAction: open cabinet 4\nGoal: pick a soapbottle \nat cabinet 4\nAction: take soapbottle \n2 from cabinet 4\nGoal: put a soapbottle \non garbagecan at gar-\nbagecan 1\nAction: go to garbage-\ncan 1\nGoal: put a soapbottle \non garbagecan at gar-\nbagecan 1\nAction: put soapbottle \n2 in\/on garbagecan 1\nTask: put some soapbottle on garbagecan\nFigure 5: Planning in the ALFWorld experiments.\nre-planning capability outperforms the long-horizon planning approach (GPT) without re-planning\ncapability with a large margin. Furthermore, the long-horizon planning method augmented with\nre-planning capability (GPT+RP) achieves superior performance ranging from 10% (GPT) to 52%.\nDEP further enhances the feasibility of planning with descriptions and self-explanation. Notably,\nall planning methods fail on Place Two & Place tasks, which is attributable to LLM’s lack\nof requisite knowledge for this task. It is worth investigating how to effectively incorporate the\ndistinctive knowledge of an environment into LLM.\nTable 6: Success rates of tasks in ALFWorld.\nGroup\nBUTLER [41]\nGPT [16]\nGPT+RP\nIM [17]\nDEP\nPick & Place\n46.0%\n33.3%\n100.0%\n33.3%\n93.3%\nClean & Place\n39.0%\n0.0%\n10.0%\n50.0%\n50.0%\nCool & Place\n100.0%\n0.0%\n30.0%\n50.0%\n100.0%\nHeat & Place\n74.0%\n0.0%\n40.0%\n0.0%\n80.0%\nPick Two & Place\n24.0%\n0.0%\n0.0%\n0.0%\n0.0%\nExamine in Light\n22.0%\n0.0%\n100.0%\n0.0%\n100.0%\nAverage\n37.0%\n10.0%\n52.0%\n30.0%\n76.0%\nA.2\nTabletop Manipulation\nThe Tabletop Manipulation experiments are conducted on a Universal Robot UR5e with a suction\ngripper in the simulated environments [40].\nA.2.1\nTasks\nThe assessment of all methods is conducted in five seen tasks, as illustrated in Table 7, wherein the\nseen tasks are employed for training the CLIPort [40] as the controller. The task involves a robotic\narm equipped with a gripper, which is tasked with rearranging a number of blocks and bowls on a\ntable to achieve a desired configuration specified via natural language (e.g., \"putting the blocks in the\nbowls with matching colors\").\nTable 7: Task list in CLIPort.\nNo\nTask Name\nInstruction\n1\nAssembling Kits\nPut the objects in the corresponding holes.\n2\nTowers of Hanoi\nMove the rings to the darker brown side.\n3\nPut Block in Bowl\nMatch the blocks and the bowls.\n4\nPacking Shapes\nPack the objects in the brown box.\n5\nStack Block Pyramid\nStack the blocks into a pyramid.\n16\nFigure 6: Planning in the Tabletop Manipulation experiments.\nWe utilized Inner Monologue (IM) [17] and Zero-shot Planner (GPT) [16] as planning baselines,\nin addition to comparing with a multi-task CLIPort policy directly trained on long-horizon task\ninstructions (i.e., without utilizing LLM for planning). As CLIPort is a single-step policy that\ndoes not spontaneously terminate during policy rollout, we report CLIPort evaluations with Oracle\ntermination (i.e., repeat until the Oracle indicates task completion) and fixed-step termination (i.e.,\nrepeat for k steps). For Inner Monologue, which directly produces the next-step goal and terminates\nwhen the LLM ceases to generate new steps, we similarly set the maximum number of steps to be k\nfor practical considerations. For the zero-shot planner [16] and our DEP, which produce the full plan\nat once, they are augmented with the LLM-based termination. DEP also involves the description,\nexplanation, and re-planning process. The same k step is suitable for these two methods. In practice,\nk is set as 15. The prompts for all methods are listed in Section G.3. We use the checkpoints provided\nby CLIPort as the controller and all planner methods access the ChatGPT (as LLM) through OpenAI\nAPI (gpt-3.5-turbo model). Each task is evaluated 5 times with different seeds.\nA.2.2\nResults\nThe results of each method are listed in Table 8. All LLM-based planning methods perform well on\ntabletop rearrangement tasks. Given the compact nature of the tabletop environment, the performance\ngap among the various LLM-planning methods is not as pronounced as in the open-ended Minecraft.\nThis observation underscores the robust generalization capabilities of LLM-based planning methods\nacross diverse environments.\nTable 8: Success rates for various methods across different tasks in Tabletop Manipulation environment.\nTask\nCLIPort [40] +oracle\nGPT [16]\nIM [17]\nDEP\nAssembling Kits\n60.0%\n60.0%\n60.0%\n60.0%\nTowers of Hanoi\n100.0%\n100.0%\n40.0%\n100.0%\nPut Block in Bowl\n100.0%\n100.0%\n82.0%\n100.0%\nPacking Shapes\n40.0%\n40.0%\n60.0%\n40.0%\nStack Block Pyramid\n80.0%\n100.0%\n40.0%\n100.0%\nAverage\n76.0%\n80.0%\n56.4%\n80.0%\n17\nB\nMinecraft Task Details\nTo fully validate the multitask planning and execution capability of our agent, we choose over 70\ntasks from the Minecraft Universe Benchmark [22] as the set of evaluation tasks. These tasks are\nrelated to items that can be obtained in the Minecraft overworld. These tasks are also a subset of\nMineDojo [10] programmatic tasks. Minedojo exists some programmatic tasks sharing the same\nobject item given different conditions (e.g., obtain wool given shear or obtain wool given nothing).\nMinedojo expands the richness of the same tasks (sharing the same Minecraft item as an object)\nby giving different initial conditions (e.g., obtain wool given shears or obtain wool\ngiven nothing). We keep only the 71 hardest conditions (i.e. given nothing) as tasks.\nWe list all task names, objects, and their required skills number for planning from Table 9 to Table 16.\nObject item is used as the basis for the successful completion of the task. These objects cannot be\nobtained directly from the environment, and usually require multiple goals (i.e., reasoning steps) to\nbe constructed. Here we only consider the number of required goal types, and multiple identical\ngoals are unified into 1 reasoning step. Note that the reasoning steps for each task are not fixed, and\nas the initial state of the agent and the biome is in change, more reasoning steps may be required to\ncomplete it, we only report the most basic case here.\nAs shown in Figure 4, for each task, a relaxed (longer) maximum episode steps will increase the\nsuccess rate of the task. To fully test the efficiency of our method, we set an upper limit on the episode\nlength for each task. Since different tasks have different difficulty levels, we double the average\ncompletion time of human players for different meta-tasks as the upper limit of the episode. The play\ntime are computed as corresponding maximum steps (i.e., Max. Steps in Table 1) of episode length at\n20Hz.\nTable 9: Task details on MT1 Basic set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nMT1\nBasic\n1\nCraftPlanks\n2\nplanks\nnull\nObtain a plank.\n2\nCraftSticks\n3\nstick\nObtain a stick.\n3\nCraftWoodenSlab\n4\nwooden_slab\nObtain a wooden slab.\n4\nCraftWoodenPressure\n3\nwooden_pressure\nObtain a wooden pressure plate.\n5\nCraftBowl\n4\nbowl\nObtain a bowl.\n6\nCraftWoodenButton\n3\nwooden_button\nObtain a wooden button.\n7\nCraftChest\n4\nchest\nObtain a chest.\n8\nCraftOakStairs\n4\noak_stairs\nObtain an oak stair.\n9\nCraftSign\n5\nsign\nObtain a sign.\n10\nCraftFence\n5\nfence\nObtain a fence.\n11\nCraftFenceGate\n5\nfence_gate\nObtain a fence gate.\n12\nCraftBoat\n4\nboat\nObtain a boat.\n13\nCraftTrapdoor\n4\ntrapdoor\nObtain a trap door.\n14\nCraftWoodenDoor\n4\ndoor\nObtain a door.\nTable 10: Task details on MT2 Tool (Simple) set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nMT2\nTool\n(Simple)\n15\nCraftCraftingTable\n3\ncrafting_table\nnull\nObtain a crafting table.\n16\nCraftWoodenPickaxe\n5\nwooden_pickaxe\nObtain a wooden pickaxe.\n17\nCraftWoodenAxe\n5\nwooden_axe\nObtain a wooden axe.\n18\nCraftWoodenHoe\n5\nwooden_hoe\nObtain a wooden hoe.\n19\nCraftWoodenSword\n5\nwooden_sword\nObtain a wooden sword.\n20\nCraftWoodenShovel\n5\nwooden_shovel\nObtain a wooden shovel.\n21\nCraftFurnace\n7\nfurnace\nObtain a furnace.\n22\nCraftStonePickaxe\n7\nstone_pickaxe\nObtain a stone pickaxe.\n23\nCraftStoneAxe\n7\nstone_axe\nObtain a stone axe.\n24\nCraftStoneHoe\n7\nstone_hoe\nObtain a stone hoe.\n25\nCraftStoneShovel\n7\nstone_shovel\nObtain a stone shovel.\n26\nCraftStoneSword\n7\nstone_sword\nObtain a stone sword.\n18\nTable 11: Task details on MT3 Hunt and Food set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nMT3\nHunt\n&\nFood\n27\nCraftBed\n5\nbed\nnull\nObtain a bed.\n28\nCraftPainting\n6\npainting\nObtain a painting.\n29\nCraftCarpet\n5\ncarpet\nObtain a carpet.\n30\nCraftItemFrame\n6\nitem_frame\nObtain an item frame.\n31\nCookPorkchop\n9\ncooked_porkchop\nCook the porkchop.\n32\nCookBeef\n9\ncooked_beef\nCook the beef.\n33\nCookMutton\n9\ncooked_mutton\nCook the mutton.\nTable 12: Task details on MT4 Dig-Down set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nMT4\nDig-Down\n34\nCraftStoneStairs\n7\nstone_stairs\nnull\nObtain a stone stair.\n35\nCraftStoneSlab\n7\nstone_slab\nObtain a stone slab.\n36\nCraftArmorStand\n10\narmor_stand\nObtain an armor stand.\n37\nCraftCobblestoneWall\n7\ncobblestone_wall\nObtain a cobblestone wall.\n38\nCraftQuartzBlock\n10\nquartz_block\nObtain a quartz block.\n39\nCraftStoneBrick\n9\nstone_brick\nObtain a stone brick.\n40\nSmeltStone\n9\nstone\nSmelt a stone.\n41\nCraftTorch\n9\ntorch\nObtain a stone brick.\n42\nObtainCoal\n8\ncoal\nMine coal.\n43\nCraftStoneBrickStairs\n10\nstonebrick_stairs\nObtain a stone brick.\n44\nCraftStonePressurePlate\n9\nstone_pressure_plate\nObtain a stone brick.\n45\nCraftStoneButton\n7\nstone_button\nObtain a stone brick.\n46\nCraftLever\n7\nlevel\nObtain a stone brick.\nTable 13: Task details on MT5 Equipment set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nMT5\nEquipment\n47\nEquipLeatherBoots\n5\nleather_boots\nnull\nEquip the leather boot.\n48\nEquipLeatherChestplate\n5\nleather_chestplate\nEquip the leather chestplate.\n49\nEquipLeatherHelmet\n5\nleather_helmet\nEquip the leather helmet.\n50\nEquipLeatherLeggings\n5\nleather_leggings\nEquip the leather leggings.\n51\nEquipShield\n11\nshield\nEquip the shield.\n52\nEquipIronChestplate\n11\niron_chestplate\nEquip the iron chestplate.\n53\nEquipIronLeggings\n11\niron_leggings\nEquip the iron leggings.\n54\nEquipIronHelmet\n11\niron_helmet\nEquip the iron helmet.\n55\nEquipIronBoots\n11\niron_boots\nEquip the iron boots.\nTable 14: Task details on MT6 Tool (Complex) set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nMT6\nTool\n(Complex)\n56\nCraftBucket\n11\nbucket\nnull\nObtain a bucket.\n57\nCraftShears\n11\nshears\nMake shears.\n58\nCraftIronPickaxe\n11\niron_pickaxe\nObtain an iron pickaxe.\n59\nCraftIronAxe\n11\niron_axe\nObtain an iron axe.\n60\nCraftIronHoe\n11\niron_hoe\nObtain an iron hoe.\n61\nCraftIronShovel\n11\niron_shovel\nObtain an iron shovel.\n62\nCraftIronSword\n11\niron_sword\nObtain an iron sword.\n19\nTable 15: Task details on MT7 Iron-Stage set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nMT7\nIron-Stage\n63\nCraftIronBars\n11\niron_bars\nnull\nObtain an iron bar.\n64\nCraftIronNugget\n11\niron_nugget\nObtain an iron nugget.\n65\nCraftMinecart\n11\nminecart\nObtain a minecart.\n66\nCraftHopper\n12\nhopper\nObtain a hopper.\n67\nCraftHopperMinecart\n14\nhopper_minecart\nObtain a hopper minecart.\n68\nCraftFurnaceMinecart\n12\nfurnace_minecart\nObtain a furnace minecart.\n69\nCraftCauldron\n11\ncauldron\nObtain a cauldron.\n70\nCraftChestMinecart\n13\nchest_minecart\nObtain a chest minecart.\n71\nCraftIronDoor\n11\niron_door\nObtain an iron door.\n72\nCraftIronTrapdoor\n11\niron_trapdoor\nObtain an iron trapdoor.\n73\nCraftTripwireHook\n11\ntripwire_hook\nObtain a tripwire hook.\n74\nCraftHWPressurePlate\n11\nheavy_weighted_plate\nObtain a heavy weighted plate.\n75\nCraftRail\n11\nrail\nObtain a rail.\nTable 16: Task details on MT8 Challenge set.\nMeta-Task\nID\nTask Name\nRequired\nSkills\nObject\nInitial\nInventory\nInstruction\nChallenge\nMT8\n76\nObtainDiamond\n12\ndiamond\nnull\nObtain a diamond.\nC\nDEPS Implementation Details\nWe study three different implementations of DEPS for each of the experimental settings. While\neach version incorporates description and self-explanation to improve planning of LLM, there are\ndifferences in the internal components of each system, as seen in Table 17.\nTable 17: Comparison between different versions of DEPS implemented in three different environments.\nMinecraft\nALFWorld\nTabletop Manipulation\nLLM\ncode-davinci-02\ntext-davinci-03\ngpt-3.5-turbo\nController\nBehavior Cloning Learned\nLLM-based\nCLIPort\nDescriptor\nInventory Description\nEnv Support\nheuristics\nExplainer\nLLM-based\nLLM-based\nLLM-based\nSelector\nHorizon Prediction Module\nN\/A\nN\/A\nC.1\nController\nAs the name implies, tasks in Minecraft are usually related to mine and craft goals. Mine goals\nrequire the agent to collect raw materials from the environment using the appropriate tools. Craft\ngoals ask the agent to synthesize using existing materials. Any raw material used requires the agent\nto collect through suitable tools (e.g., diamonds can only be collected by an iron pickaxe or a better\npickaxe). So a task usually requires dozens of step-by-step mine and craft goals, as the required\nskills in Table 9. Note that the successful execution of a task needs to satisfy certain exact numerical\nconstraints due to the presence of strict generation recipes in the environment (e.g., a log can craft\n4 planks, so harvesting 6 planks requires at least 2 logs). When the number of materials collected\nis not enough, the goal cannot be completed successfully. When more materials are collected than\nactually needed, the execution success rate of the task could also be reduced because the plan can not\nbe finished under the maximum action steps.\n20\nTable 18: The success rate of different skill\/goal with imitation learning controller.\nID\nSkill Description\nSuccess Rate\nEpisode Length\n0\nMine 1 oak wood\n0.39\n600\n1\nMine birch wood\n0.29\n600\n2\nMine 1 cobblestone with pickaxe\n0.95\n600\n3\nMine 1 stone with pickaxe\n0.70\n600\n4\nMine 1 seed\n0.18\n600\n5\nMine 1 leaves with shears\n0.68\n600\n6\nMine 1 dirt\n0.54\n600\n7\nMine 1 iron ore with stone pickaxe\n0.40\n3000\n8\nMine 3 iron ore with stone pickaxe\n0.16\n3000\n9\nMine 1 diamond with iron pickaxe\n0.35\n12000\n10\nMine 1 diamond with stone pickaxe\n0.00\n12000\n11\nKill 1 sheep with axe\n0.44\n600\n12\nKill 1 cow with axe\n0.60\n600\n13\nKill 1 chicken with axe\n0.46\n600\n14\nKill 1 pig with axe\n0.49\n600\n15\nKill 1 llama\n0.50\n600\n16\nEquip tool on mainhand\n1.00\n600\n17-261\nCraft w\/o crafting_table\n1.00\n600\nCraft w\/ crafting_table\n0.90\n600\nSmelt w\/ furnace\n0.80\n600\nWe designed the agent’s skill space based on these goals, as shown in Table 18, with a total of 262\ngoals. Every goal is designed with an objective item (e.g., 1 minecraft:cobblestone for skill\n“Mine 1 cobblestone with pickaxe”), which is used to evaluate the achievement of the\ngoal. The skill, as a goal-conditioned policy π(a|s, g) for decision-making, maps the current state s\nand goal g to action a. The goal is specified as natural language instructions here, which is similar to\n[4].\nWhen training the controller, we adopt the observation space provided by MineDoJo [10], which\nincludes an RGB camera view, yaw\/pitch angle, GPS location, and the type of 3×3 blocks surrounding\nthe agent. We discretize the original multi-discrete action space provided by MineDojo into 42 discrete\nactions. We use the proposed imitation learning method proposed by [6] in training. To be specific, a\nmodified goal-sensitive Impala CNN is used as the backbone network. The success rate under a fixed\nepisode length of every skill is listed in Table 18.\nC.2\nLLM as Planner\nDEPS relies on Large Language Models (LLMs) to generate language-based plans. In our Minecraft\nexperiment, we chose Codex [8] as the LLM Planner because it can accept longer input tokens and\nis cost-effective. However, DEPS is compatible with various types of LLMs. Therefore, we used\nGPT3 [5] and ChatGPT as LLM Planners in the ALFWorld and Tabletop Manipulation experiments,\nrespectively. Due to the effective planning and error correction performance of DEPS, the initial\nplan generated by the LLM has little impact on the final performance of the Agent. We also conduct\nablation experiments on\neven if the initial plan generated by the LLM has low accuracy, DEPS can generate a final feasible\nplan through self-explanation and re-planning. Therefore, we conducted ablation experiments on\nLLM in Minecraft.\nWe choose Codex [8], ChatGPT, GPT3 [5], and recent GPT-4 [31] as Planners. We used Vanilla\nPlanner [16] as baselines and excluded the re-planning process. Given the same prompt with DEPS,\nthe performance of baseline models reflects the planning ability of different LLMs. The success rate\nof baseline and DEPS on different LLMs are reported in Table 19.\n21\nTable 19: Success rates for different LLMs on Minecraft tasks.\nGroup\nCodex [8]\nGPT-3 [5]\nChatGPT\nGPT-4 [31]\nbaseline\nDEPS\nbaseline\nDEPS\nbaseline\nDEPS\nbaseline\nDEPS\nMT1\n28.6\n79.8\n27.2\n75.4\n20.3\n70.2\n49.2\n89.3\nMT2\n37.1\n79.5\n42.1\n76.3\n28.2\n68.5\n48.3\n85.0\nMT3\n15.1\n62.4\n7.8\n58.7\n3.2\n50.4\n38.04\n63.4\nMT4\n15.9\n53.3\n6.7\n50.2\n4.8\n47.8\n27.0\n55.7\nMT5\n3.2\n29.2\n2.7\n17.2\n0.8\n16.3\n15.7\n32.2\nMT6\n0.5\n13.8\n0.3\n7.9\n0.3\n6.0\n4.9\n16.19\nMT7\n0.6\n12.6\n0.4\n5.3\n0.5\n5.2\n3.1\n16.41\nThe success rate of Vanilla Planner varies on the LLMs. The GPT-4 baseline achieved an initial plan\naccuracy twice as high as the baselines on other LLMs, demonstrating superior planning ability. After\nbeing augmented by Descriptor, Explainer, and Selector, DEPS based on different LLMs showed\nalmost identical success rates. This indicates that DEPS-augmented LLMs can generate more feasible\nplans in open-world environments even if the initial plan is less successful.\nIt is noteworthy that DEPS is constrained by the maximum token limits of various models, which\ndictate the maximum re-planning rounds that can be supported. Longer re-planning rounds tend to\nyield superior performance, particularly in long-horizon tasks requiring more skills (in MT6-MT7),\nas detailed in the Section 4.3.\nSince we use pretrained LLM as a planner, it indeed requires exposure to a large amount of Minecraft-\nrelated corpus during the pretraining phase. Considering that Minecraft is one of the most popular\ngames worldwide, there is relatively abundant data about Minecraft available online. We conducted\nexperiments using open-source pretrained LLaMA2-70B on several Minecraft tasks and found that\nDEPS based on LLaMA2 also performs reliable planning under Minecraft conditions. Consider-\ning limited training data used by LLaMA2, we further finetuned an open-source language model\n(LLaMA2-13B) using Minecraft texts obtained from the internet which exhibited better planning\nperformance. The results are shown in Table 20.\nTable 20: Results of DEPS based on open-sourced LLaMA language models.\nLanguage Model\nCraftingTable\nWoodenPickaxe\nFurnace\nStonePickaxe\nPretrained LLaMA2-70B [44]\n60.0\n50.0\n40.0\n50.0\nFinetuned LLaMA2-13B [44]\n90.0\n80.0\n70.0\n80.0\nOpenAI Codex [8]\n90.0\n80.0\n66.7\n73.3\nC.3\nLLM as Explainer\nGiven the description and previous plan, the explainer can generate a self-explanation of the failure of\nthe current plan and give instructions to fix the bugs. The explainer is implemented with the OpenAI\ncompletion mode based on text-davinci-03 models. The prompt for the explainer is listed in\nListing 1.\nHere are some actions that the agent fails to perform in Minecraft. Please give\nthe explanation of action execution failure according to the current inventory\ninformation of the agent.\n###\nFailed Action: mine({’iron_ore’:1}, null); # step 5: mine 1 iron_ore without tool\nCurrent Inventory: null\nExplanation: Because mining iron_ore needs to use the tool stone_pickaxe, but my\ninventory does not have stone_pickaxe. So I need to craft stone_pickaxe first.\n###\nFailed Action: craft({’stone_pickaxe’:1}, {’cobblestone’:3, ’stick’:2}, ’\ncrafting_table’); # step 1: craft 1 stone_pickaxe from 3 cobblestone and 2\nstick, on crafting_table\nCurrent Inventory: null\nExplanation: Because crafting stone_pickaxe needs to have 3 cobblestone and 2\nstick in inventory, but my inventory does not have cobblestone and stick. So I\nneed to mine cobblestone and craft stick first.\n22\n###\nFailed Action: craft({’stick’:4}, {’planks’:2}, null); # step 3: craft 4 stick\nfrom 2 planks first\nCurrent Inventory: null\nExplanation: Because crafting stick needs to have planks in inventory, but my\ninventory does not have planks. So I need to craft planks first.\n###\nListing 1: Prompt for Explainer in Minecraft tasks\nC.4\nOther modules\nGoal Parser\nWe need to map the plan expressed in free-form language to the pre-defined controller\nskills set. We use the LLM as an automatic parser to parse the language plan first. For the goals\nnot following pre-defined code expression, we calculate its semantic distance to the skills by cosine\nsimilarity with pre-trained Sentence-Bert model [37] and select the most similar skill as the corre-\nsponding goal. All executable goals are listed in Appendix C. The LLM-based parser is general and\ncan be transferred to other domains easily by modifying the prompt. The prompt for Minecraft parser\nis listed in Listing 2.\nExtract the action name, action type, goal object, tool and action rank from the\ninput text.\ninput: mine({’log’:3}, null); # step 1: mine 3 log without tool\nname: mine_log\naction: mine\nobject: {’log’:3}\ntool: null\nrank: 1\n###\ninput: craft({’planks’:12}, {’log’:3}, null); # step 2: craft 12 planks from 3 log\nname: craft_planks\naction: craft\nobject: {’planks’:12}\nmaterials: {’log’:3}\ntool: null\nrank: 2\n###\nListing 2: Prompt for Goal Parser in Minecraft tasks\nSuccess Detector\nThe successful execution of a plan is contingent upon the agent’s perception\nof the current goal’s completion status, which is assessed by the success detector. In Minecraft,\nagents possess an inventory that contains all pertinent information regarding the agent’s current\nstate. Thus, the Success Detector can be implemented by monitoring changes in object information\nwithin the item inventory. In other scenarios, we can query the LLM to ascertain whether the agent\nhas accomplished a general goal by describing the agent’s current state. Alternatively, in certain\nenvironments [40], the execution of a goal is linked to the agent’s current reward, signifying that\nthese rewards can serve as automatic success detectors.\nPrompt\nThe generalization of the LLM to different tasks relies on well-designed prompts and\nrelated demonstrations [28]. Given an instruction command (e.g., ObtainDiamond) as task T, a\nprompt generator (ProG) will translate T into prompt text. We also added two DEP examples in the\nprompt as demonstrations to make the LLM output familiar to the chain-of-thought thinking and\nstructural output. We also design a chain-of-thought code-comments-type planning prompt to better\ndemonstrate the capabilities of LLM. All messages are modified to suitable prompts through the\nprompt-generator before being input to LLM, including task T and description dt. The full prompt\nsentences and interaction logs are listed in Appendix H.\nD\nComparison with other LLM-based Planners\nThe architectures of the different LLM-based planners are illustrated in Figure 7. Where (b) describes\nthe information in the environment into LLM via scene descriptor and success detector, and directly\n23\n(a) Ours\n(b) Short-horizon\n(c) Long-Horizon\n(d) Long-Horizon\naction planning\none-shot planning\nplanning & re-planning\nFigure 7: Comparison of LLM-based planner architecture. (a), (b), (c), (d) represents planner of ours, Inner\nMonologue [17], Zero-Shot Planner [16] and Zero-Shot Planner with re-planning process, respectively.\nplans the next goal\/action, (c) is Zero-Shot planner [16], which generates the step-by-step goal\nsequences as plan and ignores the environment state and execution feedback, (d) is the Zero-Shot\nplanner augmented with textual feedback and re-planning process. DEPS further rethink and explain\nthe feedback of previous plans explicitly with the descriptor and explainer. The LLM-based planner\nwill re-plan the task according to the explanation, as demonstrated in Figure 7(a). In addition, the\ngoal Selector further improves the executability of the LLM plan.\nE\nDiscussion on ObtainDiamond Task\nmine log\ncraft planks\ncraft crafting table\ncraft stick\ncraft wooden pickaxe\nmine stone\ncraft stone pickaxe\nmine iron ore\ncraft iron pickaxe\nsmelt iron ingot\ncraft furnace\nmine diamond\nFigure 8: The milestone goals of the ObtainDiamond task.\nAs outlined in Section 4.4, ObtainDiamond task is a formidable task within the open-ended\nMinecraft environment. Given the necessity to explore an infinitely expansive world, an efficient plan\ncan prove advantageous, as shown in Figure 8. The task is allotted a maximum of 12,000 steps to\ninteract with the environment, which is comparable to that of human performance [14]. Rather than\nmanually devising explicit hierarchical rewards, we opt to utilize DEPS for generating a hierarchical\nplan, which is then transferred to the downstream controller to progressively achieve each goal. When\nequipped with an Oracle Controller, DEPS yields a success rate of 60% for ObtainDiamond. In\nour experimentation, we employed Behavior Cloning to train a Controller agent [6]. DEPS+BC\nController achieved a success rate of 0.6% in randomly generated Minecraft worlds. The primary\nbottleneck impeding overall agent success rate lies within the goal-conditioned Controller, not the\nplans generated by DEPS. Thus, it is worth exploring the development of a data-efficient Controller\ncapable of accepting Language goals.\nAnother rationale for using DEPS is that, akin to reality, materials in Minecraft possess quantity con-\nstraints, and durability for tools. In ObtainDiamond task, an iron pickaxe is typically insufficient\n24\nto support the agent, given the rarity of diamonds within the environment (which are predominantly\nfound between depths of 2-16 layers and appear only 0.0846% of the time). The robust re-planning\ncapabilities of DEPS can facilitate the generation of a feasible plan (initiating with crafting an\niron-pickaxe) based on the agent’s current state.\nAdditionally, we report the milestones, which demonstrate the decreasing success rate of subsequent\ntasks in Figure 9 attributable to the task’s inherent complexity and Controller constraints.\nFigure 9: Success rate of milestone items for mining diamond.\nF\nSuccess Rates of ALL Tasks in Minecraft\nWe report the complete and detailed success rate table of all tasks for different methods in Table 21,\nincluding Zero-shot Planner [16], ProgPrompt [42], Chain-of-Thought [45], Inner Monologue [17],\nCode as Policies [20], and proposed methods (i.e., DEP w\/o Selector, and DEPS).\nAll tasks are executed for at least 30 times across different world seeds, given the same initial\nconditions. The birth positions of the world are random according to the seed. The average success\nrates are listed in Table 21. Our approach is state-of-the-art on almost all tasks, especially on difficult\ntasks that require more skills.\n25\nTable 21: Success rate comparison of various methods on MineDojo [10] environments.\nMeta-Task\nTask Object\nGPT [16]\nPP [42]\nCoT [45]\nIM [17]\nCaP [20]\nDEP\nDEPS\nplanks\n56.7\n56.7\n83.3\n83.3\n83.3\n83.3\n83.3\nstick\n0.0\n56.7\n83.3\n83.3\n83.3\n83.3\n86.7\nwooden_slab\n26.7\n26.7\n56.7\n83.3\n83.3\n83.3\n76.7\nwooden_button\n23.3\n50.0\n73.3\n73.3\n73.3\n73.3\n96.7\nwooden_pressure_plate\n80.0\n80.0\n53.3\n80.0\n80.0\n80.0\n86.7\nchest\n0.0\n26.7\n0.0\n0.0\n50.0\n76.7\n76.7\noak_stairs\n20.0\n40.0\n36.7\n16.7\n36.7\n56.7\n60.0\nsign\n23.3\n0.0\n43.3\n0.0\n43.3\n63.3\n86.7\nfence\n20.0\n20.0\n0.0\n20.0\n43.3\n63.3\n80.0\nfence_gate\n63.3\n0.0\n63.3\n63.3\n63.3\n93.3\n73.3\nboat\n0.0\n0.0\n0.0\n26.7\n56.7\n83.3\n73.3\ntrapdoor\n26.7\n26.7\n26.7\n56.7\n56.7\n83.3\n76.7\nbowl\n0.0\n23.3\n0.0\n23.3\n46.7\n70.0\n80.0\nBasic\nMT1\nwooden_door\n23.3\n23.3\n46.7\n46.7\n46.7\n66.7\n80.0\ncrafting_table\n70.0\n23.3\n70.0\n70.0\n70.0\n70.0\n90.0\nwooden_pickaxe\n80.0\n80.0\n80.0\n80.0\n80.0\n80.0\n80.0\nwooden_axe\n46.7\n46.7\n70.0\n70.0\n70.0\n70.0\n96.7\nwooden_hoe\n86.7\n56.7\n86.7\n30.0\n86.7\n86.7\n86.7\nwooden_sword\n83.3\n83.3\n83.3\n83.3\n83.3\n83.3\n86.7\nwooden_shovel\n76.7\n76.7\n76.7\n76.7\n76.7\n76.7\n90.0\nfurnace\n20.0\n20.0\n0.0\n40.0\n40.0\n60.0\n66.7\nstone_pickaxe\n16.7\n16.7\n36.7\n36.7\n53.3\n53.3\n73.3\nstone_axe\n0.0\n0.0\n30.0\n30.0\n30.0\n46.7\n70.0\nstone_hoe\n20.0\n20.0\n36.7\n36.7\n56.7\n56.7\n66.7\nstone_shovel\n56.7\n56.7\n40.0\n36.7\n36.7\n56.7\n66.7\nTool(Simple)\nMT2\nstone_sword\n16.7\n0.0\n53.3\n53.3\n36.7\n53.3\n80.0\nbed\n16.7\n23.3\n23.3\n6.7\n6.7\n23.3\n43.3\npainting\n33.3\n0.0\n0.0\n16.7\n16.7\n53.3\n86.7\ncarpet\n0.0\n33.3\n0.0\n0.0\n13.3\n33.3\n43.3\nitem_frame\n23.3\n50.0\n23.3\n0.0\n23.3\n73.3\n83.3\ncooked_porkchop\n0.0\n0.0\n0.0\n0.0\n0.0\n40.0\n50.0\ncooked_beef\n0.0\n0.0\n0.0\n0.0\n0.0\n53.3\n63.3\nHunt and Food\nMT3\ncooked_mutton\n0.0\n13.3\n0.0\n0.0\n0.0\n43.3\n66.7\nstone_stairs\n16.7\n23.3\n20.0\n36.7\n16.7\n56.7\n66.7\nstone_slab\n16.7\n50.0\n0.0\n16.7\n33.3\n50.0\n73.3\ncobblestone_wall\n16.7\n16.7\n16.7\n16.7\n43.3\n43.3\n63.3\nlever\n0.0\n0.0\n0.0\n46.7\n46.7\n70.0\n83.3\ncoal\n0.0\n16.7\n0.0\n6.7\n0.0\n16.7\n20.0\nDig-down\nMT4\ntorch\n0.0\n6.7\n0.0\n6.7\n0.0\n23.3\n13.3\nleather_boots\n0.0\n13.3\n0.0\n13.3\n13.3\n36.7\n60.0\nleather_chestplate\n0.0\n6.7\n16.7\n0.0\n6.7\n23.3\n36.7\nleather_helmet\n16.7\n6.7\n0.0\n6.7\n0.0\n26.7\n70.0\nleather_leggings\n0.0\n0.0\n0.0\n20.0\n0.0\n30.0\n56.7\niron_chestplate\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\niron_leggings\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n3.3\niron_helmet\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\niron_boots\n0.0\n0.0\n0.0\n0.0\n0.0\n6.7\n20.0\nEquipment\nMT5\nshield\n0.0\n0.0\n0.0\n0.0\n6.7\n16.7\n13.3\nbucket\n0.0\n3.3\n0.0\n0.0\n3.3\n10.0\n6.7\nshears\n0.0\n0.0\n0.0\n0.0\n0.0\n10.0\n30.0\niron_pickaxe\n0.0\n0.0\n0.0\n6.7\n0.0\n6.7\n10.0\niron_axe\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n16.7\niron_hoe\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n13.3\niron_shovel\n0.0\n0.0\n0.0\n0.0\n0.0\n6.7\n13.3\nTool Complex\nMT6\niron_sword\n0.0\n0.0\n0.0\n0.0\n3.3\n3.3\n6.7\niron_bars\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.7\niron_nugget\n0.0\n6.7\n0.0\n6.7\n6.7\n23.3\n40.0\nminecart\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n10.0\nhopper\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n6.7\nhopper_minecart\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nfurnace_minecart\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n3.3\nchest_minecart\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n3.3\niron_door\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\niron_trapdoor\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n16.7\ntripwire_hook\n0.0\n0.0\n0.0\n6.7\n0.0\n16.7\n30.0\nheavy_weighted_pressure_plate\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\n33.3\nrail\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.7\nIron-Stage\nMT7\ncauldron\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3.3\nMT8\nChallenge\ndiamond\n0.0\n0.0\n0.0\n0.0\n0.0\n0.5\n0.6\n26\nG\nPrompt for Different Tasks and Different Methods\nG.1\nPrompt for Minecraft Tasks\nG.1.1\nDEPS\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\nPlanner: OK.\n==========\nUser: How to obtain 1 wooden_button in Minecraft step-by-step?\nPlanner: The code for obtaining 1 wooden_button is as follows:\ndef obtain_1_wooden_button(inventory={}):\nmine({’log’:1}, null); # step 1: mine 1 log without tool\ncraft({’planks’:4}, {’log’:1}, null); # step 2: craft 4 planks from 1 log\ncraft({’wooden_button’:1}, {’planks’:1}, null); # step 3: craft 1 wooden_button\nfrom 1 planks\nreturn \"wooden_button\"\nDescriptor: I succeed in step 1, 2, 3.\nUser: Good. I finish all steps and I obtain 1 stone_pickaxe successfully.\nPlanner: OK.\n==========\nUser: How to obtain 1 stone_sword in Minecraft step-by-step?\nPlanner: The code for obtaining stone_sword is as follows:\ndef obtain_1_stone_sword(inventory = {}):\nmine({’log’:3}, null); # step 1: mine 3 log without tool\ncraft({’planks’:12}, {’log’:3}, null); # step 2: craft 12 planks from 3 log\ncraft({’stick’:4}, {’planks’:2}, null); # step 3: craft 4 stick from 2 planks\nmine({’cobblestone’:2}, null); # step 4: mine 2 cobblestone without tool\ncraft({’stone_sword’:1}, {’cobblestone’:2, ’stick’:1}, ’crafting_table’); #\nstep 5: craft 1 stone_sword from 2 cobblestone and 1 stick, on\ncrafting_table\nreturn ’stone_sword’\nDescriptor: I succeed on step 1, 2, 3, but I fail on step 4 \"mine({’cobblestone\n’:2}, null);\". My inventory now has 10 planks, 4 stick.\nExplainer: Because mining cobblestone needs to use the tool wooden_pickaxe.\nUser: Please fix the above errors and replan the task \"obtain 1 stone_sword\".\nRePlanner: The code for crafting stone_sword is as bellows:\ndef obtain_1_stone_sword(inventory = {’planks’:10, ’stick’:4}):\ncraft({’wooden_pickaxe’:1}, {’planks’:3, ’stick’:2}, ’crafting_table’); # step\n1: craft 1 wooden_pickaxe from 3 planks and 2 stick, on crafting_table\nmine({’cobblestone’:2}, ’wooden_pickaxe’); # step 2: mine 2 cobblestone with\nwooden_pickaxe\ncraft({’stone_sword’:1}, {’cobblestone’:2, ’stick’:1}, ’crafting_table’); #\nstep 3: craft 1 stone_sword from 2 cobblestone and 1 stick, on\ncrafting_table\nreturn ’stone_sword’\nDescriptor: I fail on step 1 \"craft({’wooden_pickaxe’:1}, {’planks’:3, ’stick’:2},\n’crafting_table’);\". My inventory now has 10 planks, 4 stick.\nExplainer: Because the action needs to use the tool crafting_table, but I do not\nhave it.\nUser: Please fix the above errors and replan the task \"obtain 1 stone_sword\".\nRePlanner: The code for crafting stone_sword is as bellows:\ndef obtain_1_stone_sword(inventory = {’planks’:10, ’stick’:4}):\ncraft({’crafting_table’:1}, {’planks’:4}, null); # step 1: craft 1\ncrafting_table from 4 planks\ncraft({’wooden_pickaxe’:1}, {’planks’:3, ’stick’:2}, ’crafting_table’); # step\n2: craft 1 wooden_pickaxe from 3 planks and 2 stick, on crafting_table\nmine({’cobblestone’:2}, ’wooden_pickaxe’); # step 3: mine 2 cobblestone with\nwooden_pickaxe\ncraft({’stone_sword’:1}, {’cobblestone’:2, ’stick’:2}, ’crafting_table’); #\nstep 4: craft 1 stone_sword from 2 cobblestone and 1 stick, on\ncrafting_table\nreturn ’stone_sword’\nDescriptor: I succeed in step 1, 2, 3, 4.\nUser: Good. I finish all steps and I obtain 1 stone_pickaxe successfully.\n27\nPlanner: OK.\nListing 3: Prompt for DEPS in Minecraft tasks\nG.1.2\nBaselines\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\nPlanner: OK.\nUser: How to obtain 1 wooden_button in Minecraft step-by-step?\nPlanner: The plan is as follows:\nstep 1: mine 1 log\nstep 2: craft 4 planks\nstep 3: craft 1 wooden_button\n###\nUser: How to obtain stone_sword in Minecraft step-by-step?\nPlanner: The plan is as follows:\nstep 1: mine 3 log\nstep 2: craft 12 planks\nstep 3: craft 4 stick\nstep 4: craft 1 crafting_table\nstep 5: craft 1 wooden_pickaxe\nstep 6: mine 2 cobblestone\nstep 7: craft 1 stone_sword\n###\nListing 4: Prompt for Zero-shot Planner [16] in Minecraft tasks.\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\nPlanner: OK.\nUser: How to obtain 1 wooden_button in Minecraft step-by-step?\nPlanner: The code for obtaining wooden_button is as follows:\n’’’\nfrom actions import mine_log, craft_planks, craft_wooden_button\ndef obtain_wooden_button(inventory = {}):\nmine_log(num = 1); # step 1: mine 1 log\ncraft_planks(num = 4); # step 2: craft 4 planks\ncraft_wooden_button(num = 1); # step 3: craft 1 wooden_button\nreturn \"wooden_button\"\n’’’\nUser: How to obtain 1 stone_sword in Minecraft step-by-step?\nPlanner: The code for obtaining stone_sword is as follows:\n’’’\nfrom actions import mine_log, craft_planks, craft_stick, craft_crafting_table,\ncraft_wooden_pickaxe, mine_cobblestone, craft_stone_sword\ndef obtain_stone_sword(inventory = {}):\nmine_log(num = 3); # step 1: mine 3 log\ncraft_planks(num = 12); # step 2: craft 12 planks\ncraft_stick(num = 4); # step 3: craft 4 stick\ncraft_crafting_table(num = 1); # step 4: craft 1 crafting_table\ncraft_wooden_pickaxe(num = 1); # step 5: craft 1 wooden_pickaxe\nmine_cobblestone(num = 2); # step 6: mine 2 cobblestone\ncraft_stone_sword(num = 1); # step 7: craft 1 stone_sword\nreturn \"stone_sword\"\n’’’\nListing 5: Prompt for ProgPrompt [42] in Minecraft tasks.\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\nPlanner: OK.\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\n28\nPlanner: OK.\nUser: How to obtain 1 wooden_button in Minecraft step-by-step?\nPlanner: The plan is as follows:\nstep 1: mine 1 log without tool\nThinking: Inventory will add 1 log, as inventory = 1*log\nstep 2: craft 4 planks from 1 log\nThinking: Inventory will add 4 planks and delete 1 log, as inventory = 4*planks\nstep 3: craft 1 wooden_button from 1 planks\nThinking: Inventory will add 1 wooden_button and delete 1 planks, as inventory = 3*\nplanks + 1*wooden_button\n###\nUser: How to obtain stone_sword in Minecraft step-by-step?\nPlanner: The plan is as follows:\nstep 1: mine 3 log without tool\nThinking: Inventory will add 3 log, as inventory = 3*log.\nstep 2: craft 12 planks from 3 log\nThinking: Inventory will add 12 planks and delete 3 log, as inventory = 12*planks\nstep 3: craft 4 stick from 2 planks\nThinking: Inventory will add 4 stick and delete 2 planks, as inventory = 10*planks\n+ 4*stick\nstep 4: craft 1 crafting_table from 4 planks\nThinking: Inventory will add 1 crafting_table and delete 4 planks, as inventory =\n6*planks + 4*stick + 1*crafting_table\nstep 5: craft 1 wooden_pickaxe from 3 planks and stick, on crafting_table\nThinking: Inventory will add 1 wooden_pickaxe, delete 3 planks and delete 2 stick,\nas inventory = 3*planks + 2*stick + 1*crafting_table + 1*wooden_pickaxe\nstep 6: mine 2 cobblestone with wooden_pickaxe\nThinking: Inventory will add 2 cobblestone, as inventory = 3*planks + 2*stick + 1*\ncrafting_table + 1*wooden_pickaxe + 2*cobblestone\nstep 7: craft 1 stone_sword from 2 cobblestone and 1 stick, on crafting_table\nThinking: Inventory will add 1 stone_sword, delete 2 cobblestone, and delete 1\nstick, as inventory = 3*planks + 1*stick + 1*crafting_table + 1*wooden_pickaxe\n+ 1*stone_sword\n###\nListing 6: Prompt for Chain-of-Thought [45] in Minecraft tasks\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\nPlanner: OK.\n===============\nUser: Obtain 1 wooden_button in Minecraft step-by-step.\nScene: My inventory has nothing.\nPlanner: mine 1 log\nRobot Thought: mining log needs no tool\nSuccessful Action: True\nScene: My inventory has 1 log.\nPlanner: craft 4 planks\nRobot Thought: crafting 4 planks needs 1 log\nSuccessful Action: True\nScene: My inventory has 4 planks\nPlanner: craft 1 wooden_button\n{Robot Thought}: crafting 1 wooden_button needs 1 planks\nSuccessful Action: True\n===============\nUser: Obtain 1 stone_sword in Minecraft step-by-step.\nScene: My inventory has nothing.\nPlanner: mine 3 log\nRobot Thought: mining log needs no tool\nSuccessful Action: True\nScene: My inventory has 3 log.\nPlanner: craft 12 planks\nRobot Thought: crafting 12 planks needs 3 log\n29\nSuccessful Action: True\nScene: My inventory has 12 planks\nPlanner: craft 4 stick\nRobot Thought: crafting 4 stick needs 2 planks\nSuccessful Action: True\nScene: My inventory has 10 planks, 4 stick\nPlanner: craft 1 wooden_pickaxe\nRobot Thought: crafting 1 wooden_pickaxe needs 3 planks and 2 stick on\ncrafting_table\nSuccessful Action: False\nScene: My inventory has 10 planks, 4 stick\nPlanner: craft 1 crafting_table\nRobot Thought: crafting 1 crafting_table needs 4 planks\nSuccessful Action: True\nScene: My inventory has 6 planks, 4 stick, 1 crafting_table\nPlanner: craft 1 wooden_pickaxe\nRobot Thought: crafting 1 wooden_pickaxe needs 3 planks and 2 stick, on\ncrafting_table\nSuccessful Action: True\nScene: My inventory has 3 planks, 2 stick, 1 crafting_table, 1 wooden_pickaxe\nPlanner: mine 2 cobblestone\nRobot Thought: mining cobblestone needs wooden_pickaxe as tool\nSuccessful Action: True\nScene: My inventory has 3 planks, 2 stick, 1 crafting_table, 1 wooden_pickaxe, 2\ncobblestone\nPlanner: craft 1 stone_sword\nRobot Thought: crafting 1 stone_sword needs 2 cobblestone and 1 stick, on\ncrafting_table\nSuccessful Action: True\nListing 7: Prompt for Inner Monologue [17] in Minecraft tasks\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\nPlanner: OK.\nUser: How to obtain 1 wooden_button in Minecraft step-by-step?\nPlanner: The code for obtaining wooden_button is as follows:\ndef craft_stone_sword(inventory = {}):\n# step 1: mine 1 log without tool\nwhile not inventory.has({’log’:1}):\nmine(obj = \"log\", tool = null);\n# step 2: craft 4 planks from 1 log\nwhile not inventory.has({’planks’:4}):\nassert inventory.has({’log’:1}), \"I do not have enough log for crafting\nplanks\"\ncraft(obj = \"planks\", tool = null);\n# step 3: craft 1 wooden_button from 1 planks\nwhile not inventory.has({’wooden_button’:1}):\nassert inventory.has({’planks’:1}), \"I do not have enough planks for\ncrafting wooden_button\"\ncraft(obj = \"wooden_button\", tool = null);\nreturn \"wooden_button\"\n####\nUser: How to obtain 1 stone_sword in Minecraft step-by-step?\nPlanner: The code for obtaining stone_sword is as follows:\ndef craft_stone_sword(inventory = {}):\n# step 1: mine 3 log without tool\nwhile not inventory.has({’log’:3}):\nmine(obj = \"log\", tool = null);\n# step 2: craft 12 planks from 3 log\nwhile not inventory.has({’planks’:12}):\nassert inventory.has({’log’:1}), \"I do not have enough log for crafting\nplanks\"\ncraft(obj = \"planks\", tool = null);\n# step 3: craft 4 stick from 2 planks\nwhile not inventory.has({’stick’:4}):\n30\nassert inventory.has({’planks’:2}), \"I do not have enough planks for\ncrafting stick\"\ncraft(obj = \"stick\", tool = null);\n# step 4: craft 1 crafting_table from 4 planks\nwhile not inventory.has({’crafting_table’:1}):\nassert inventory.has({’planks’:4}), \"I do not have enough planks for\ncrafting crafting_table\"\ncraft(obj = \"crafting_table\", tool = null)\n# step 5: craft 1 wooden_pickaxe from 3 planks and 2 stick, on crafting_table\nwhile not inventory.has({’wooden_pickaxe’:1}):\nassert inventory.has({’planks’:3}), \"I do not have enough planks for\ncrafting wooden_pickaxe\"\nassert inventory.has({’stick’:2}), \"I do not have enough stick for crafting\nwooden_pickaxe\"\nassert inventory.has({’crafting_table’:1}), \"I do not have the tool\ncrafting_table for crafting wooden_pickaxe\"\ncraft(obj = \"crafting_table\", tool = \"crafting_table\")\n# step 6: mine 2 cobblestone with tool wooden_pickaxe\nwhile not inventory.has({’cobblestone’:2}):\nassert inventory.has({’wooden_pickaxe’:1}), \"I do not have the tool\nwooden_pickaxe for mining cobblestone\"\nmine(obj = \"cobblestone\", tool = \"wooden_pickaxe\")\n# step 7: craft 1 stone_sword from 2 cobblestone and 1 stick on crafting_table\nwhile not inventory.has({’stone_sword’:1}):\nassert inventory.has({’cobblestone’:2}), \"I do not have enough cobblestone\nfor crafting stone_sword\"\nassert inventory.has({’stick’:1}), \"I do not have enough stick for crafting\nstone_sword\"\nassert inventory.has({’crafting_table’:1}), \"I do not have the tool\ncrafting_table for crafting wooden_pickaxe\"\ncraft(obj = \"stone_sword\", tool = \"crafting_table\")\nreturn \"stone_sword\"\n####\nListing 8: Prompt for Code as Policies [20] in Minecraft tasks\nG.2\nPrompt for ALFWorld Tasks\nG.2.1\nDEPS\nHuman: -= Welcome to TextWorld, ALFRED! =-\nDescriptor: You are in the middle of a room. Looking quickly around you, you see a\ncabinet 1, a diningtable 1 and a fridge 1.\nHuman: Your task is to: put a cool tomato in cabinet. Please generate the\nsequences of sub-goals to complete the task.\nPlanner: The task is to put a cool tomato in cabinet. The plan is below:\n#1 pick a tomato at fridge 1\n#2 cool a tomato at fridge 1\n#3 Put a tomato in cabinet at cabinet 1\nDescriptor: I fail on step 1 pick a tomato at fridge 1. I’m not carrying a tomato\nand there is no tomato, so the goal can’t be accomplished.\nExplainer: It’s probably because there isn’t a tomato at fridge 1. The task is to\nput a cool tomato in cabinet. So I should pick a tomato somewhere else. I’m\nmost likely to pick a tomato at diningtable 1.\nPlanner: So the new plan is below:\n#1 pick a tomato at diningtable 1\n#2 put a tomato in cabinet at cabinet 1\nDescriptor: I succeed on step 1.\nDescriptor: I failed on step 2 put a tomato in cabinet at cabinet 1. I put a\ntomato in cabinet, but tomato 2 may not be a cool tomato, so the goal is not\naccomplished.\nExplainer: It’s probably because the tomato isn’t cool enough. The task is to put\na cool tomato in cabinet. So I should cool the tomato. I’m most likely to cool\na tomato at fridge 1.\nPlanner: So the new plan is below:\n#2 cool a tomato at fridge 1\n#3 put a tomato in cabinet at cabinet 1\nDescriptor: I succeed on step 2.\nDescriptor: I succeed on step 3. I finish the task!!!\nHuman:-= Welcome to TextWorld, ALFRED! =-\nDescriptor: You are in the middle of a room. Looking quickly around you, you see a\ndesk 1, and a shelf 1.\nHuman: Your task is to: look at mug under the desklamp. Please generate the\nsequences of sub-goals to complete the task.\n31\nPlanner: The task is to look at mug under the desklamp. The plan is below:\n#1 pick up a mug at shelf 1\n#2 use the desklamp at desk 1\nDescriptor: I succeed on step 1.\nDescriptor: I succeed on step 2. I finish the task!!!\nListing 9: Prompt for DEP in ALFWorld tasks\nHuman: You are in the middle of a room. You are not carrying anything. Your goal\nis to: pick a plate at diningtable 1.\nThe 5 admissible actions of the current situation are :\n1. go to cabinet 1\n2. go to diningtable 1\n3. go to fridge 1\n4. inventory\n5. look\nAI: The goal is to pick a plate at diningtable 1. I’m not at diningtable 1. So I\nshould go to diningtable 1 first. So the answer is 2.\nHuman: You arrive at loc 17. On the diningtable 1, you see a plate 2, and a tomato\n2.\nThe 7 admissible actions of the current situation are :\n1. examine diningtable 1\n2. go to cabinet 1\n3. go to fridge 1\n4. inventory\n5. look\n6. take plate 2 from diningtable 1\n7. take tomato 2 from diningtable 1\nAI: The goal is to pick a plate at diningtable 1. I’m at diningtable 1, and there\nis a plate 2. So I should take plate 2 from diningtable 1. So the answer is 6.\nHuman: You pick up the plate 2 from the diningtable 1.\nThe 6 admissible actions of the current situation are :\n1. examine diningtable 1\n2. go to cabinet 1\n3. go to fridge 1\n4. inventory\n5. look\n6. put plate 2 in\/on diningtable 1\nAI: The goal is to pick a plate at diningtable 1. According to Human, I pick up\nthe plate 2 from diningtable 1. So the goal is accomplished.\nHuman: You are facing fridge 1. You are carrying plate 2. Your goal is to: cool a\nplate at fridge 1.\nThe 6 admissible actions of the current situation are :\n1. examine fridge 1\n2. go to cabinet 1\n3. go to diningtable 1\n4. inventory\n5. look\n6. cool plate 2 with fridge 1\nAI: The goal is to cool a plate at fridge 1. I’m at fridge 1. So I should cool\nplate 2 with fridge 1. So the answer is 6.\nHuman: You cool the plate 2 using the fridge 1.\nThe 5 admissible actions of the current situation are :\n1. examine fridge 1\n2. go to cabinet 1\n3. go to diningtable 1\n4. inventory\n5. look\nAI: The goal is to cool a plate at fridge 1. According to Human, I cool the plate\n2 using the fridge 1. So the goal is accomplished.\nHuman: You are facing diningtable 1. You are not carrying anything. Your goal is\nto: pick a plate at diningtable 1.\nThe 6 admissible actions of the current situation are :\n1. examine diningtable 1\n2. go to cabinet 1\n3. go to fridge 1\n4. inventory\n5. look\n6. take tomato 2 from diningtable 1\nAI: The goal is to pick a plate at diningtable 1. I’m at diningtable 1, but there\nis no action to pick a plate. So the answer is 0(none). So the goal can’t be\naccomplished, probably because there is no plate at diningtable 1.\n32\nHuman: You are facing fridge 1. You are not carrying anything. Your goal is to:\ncool a plate at fridge 1.\nThe 5 admissible actions of the current situation are :\n1. examine fridge 1\n2. go to cabinet 1\n3. go to diningtable 1\n4. inventory\n5. look\nAI: The goal is to cool a plate at fridge 1. I’m at fridge 1, but there is no\naction to cool a plate. So the answer is 0(none). So the goal can’t be\naccomplished, probably because I’m not carrying a plate.\nHuman: You are facing fridge 1. You are carrying a plate. Your goal is to: pick a\ntomato at fridge 1.\nThe 5 admissible actions of the current situation are :\n1. examine fridge 1\n2. go to cabinet 1\n3. go to diningtable 1\n4. inventory\n5. look\nAI: The goal is to pick a tomato at fridge 1. I’m at fridge 1, but there is no\naction to pick a tomato. So the answer is 0(none). So the goal can’t be\naccomplished, probably because I can’t pick a tomato while carrying a plate.\nListing 10: Prompt for the controller of DEP in ALFWorld tasks\nG.2.2\nBaselines\nHuman: -= Welcome to TextWorld, ALFRED! =-\nYour task is to: cool some plate and put it in cabinet.\nScene: You are in the middle of a room. Looking quickly around you, you see a\ncabinet 1, a diningtable 1 and a fridge 1.\nThe 5 admissible actions of the current situation are :\n1. go to cabinet 1\n2. go to diningtable 1\n3. go to fridge 1\n4. inventory\n5. look\nRobot Thought: The task is to cool some plate and put it in cabinet. I’m carrying\nnothing. So I need to find a plate first. There are cabinet, diningtable and\nfridge, but no plate. I’m most likely to find a plate near diningtable. So I\nshould go to diningtable 1. So the answer is 2.\nRobot Action: go to diningtable 1\nScene: You arrive at loc 17. On the diningtable 1, you see a plate 2, and a tomato\n2.\nThe 7 admissible actions of the current situation are :\n1. examine diningtable 1\n2. go to cabinet 1\n3. go to fridge 1\n4. inventory\n5. look\n6. take plate 2 from diningtable 1\n7. take tomato 2 from diningtable 1\nRobot Thought: The task is to cool some plate and put it in cabinet. I’m carrying\nnothing. So I need to find a plate first. There is a plate 2. So I should take\nplate 2 from diningtable 1. So the answer is 6.\nRobot Action: take plate 2 from diningtable 1\nScene: You pick up the plate 2 from the diningtable 1.\nThe 7 admissible actions of the current situation are :\n1. examine diningtable 1\n2. examine plate 2\n3. go to cabinet 1\n4. go to fridge 1\n5. inventory\n6. look\n7. put plate 2 in\/on diningtable 1\nRobot Thought: The task is to cool some plate and put it in cabinet. I’m carrying\na plate 2. So I need to cool a plate first. There are cabinet, diningtable,\nand fridge. I’m most likely to cool a plate near fridge. So I should go to\nfridge 1. So the answer is 4.\nRobot Action: go to fridge 1\nScene: You arrive at loc 41. The fridge 1 is closed.\n33\nThe 8 admissible actions of the current situation are :\n1. cool plate 2 with fridge 1\n2. examine fridge 1\n3. examine plate 2\n4. go to cabinet 1\n5. go to diningtable 1\n6. inventory\n7. look\n8. open fridge 1\nRobot Thought: The task is to cool some plate and put it in cabinet. I’m carrying\na plate 2. So I need to cool a plate first. There is a fridge 1. So I should\ncool plate 2 with fridge 1. So the answer is 1.\nRobot Action: cool plate 2 with fridge 1\nScene: You cool the plate 2 using the fridge 1.\nThe 8 admissible actions of the current situation are :\n1. cool plate 2 with fridge 1\n2. examine fridge 1\n3. examine plate 2\n4. go to cabinet 1\n5. go to diningtable 1\n6. inventory\n7. look\n8. open fridge 1\nRobot Thought: The task is to cool some plate and put it in cabinet. I’m carrying\na cool plate 2. So I need to find a cabinet first. There is a cabinet 1. So I\nshould go to cabinet 1. So the answer is 4.\nRobot Action: go to cabinet 1\nScene: You arrive at loc 20. The cabinet 1 is closed.\nThe 7 admissible actions of the current situation are :\n1. examine cabinet 1\n2. examine plate 2\n3. go to diningtable 1\n4. go to fridge 1\n5. inventory\n6. look\n7. open cabinet 1\nRobot Thought: The task is to cool some plate and put it in cabinet. I’m carrying\na cool plate 2. So I need to find a cabinet first. There is a cabinet 1. The\ncabinet is closed. So I should open cabinet 1. So the answer is 7.\nRobot Action: open cabinet 1\nScene: You open the cabinet 1. The cabinet 1 is open. In it, you see nothing.\nThe 8 admissible actions of the current situation are :\n1. close cabinet 1\n2. examine cabinet 1\n3. examine plate 2\n4. go to diningtable 1\n5. go to fridge 1\n6. inventory\n7. look\n8. put plate 2 in\/on cabinet 1\nRobot Thought: The task is to cool some plate and put it in cabinet. I’m carrying\na cool plate 2. So I need to find a cabinet first. There is a cabinet 1. So I\nshould put plate 2 in\/on cabinet 1. So the answer is 8.\nRobot Action: put plate 2 in\/on cabinet 1\nScene: You put the plate 2 in\/on the cabinet 1. You won!!!\nListing 11: Prompt for Inner Monologue [17] in ALFWorld tasks\nG.3\nPrompt for Tabletop Manipulation Tasks\nG.3.1\nDEPS\nsystem: The template of the lang goal is as follow: put the <color> blocks in a <\ncolor> bowl. For example, \"put the green blocks in a blue bowl\" is a valid\nlang goal. Do not provide any additional explanations or instructions beyond\nwriting lang goals.\nuser: There are 4 bowls of blue, green, pink, yellow on the table. There are 3\nblocks of red, yellow, blue on the table. How to match the blocks and the\nbowls?\n34\nassistant:\n1. put the yellow blocks in a yellow bowl\n2. put the blue blocks in a blue bowl\nListing 12: Prompt for DEP in CLIPort tasks\nG.3.2\nBaselines\nsystem: The template of the language goal is as follows: put the <color> blocks in\na <color> bowl. For example, \"put the green blocks in a blue bowl\" is a valid\nlang goal. Do not provide any additional explanations or instructions beyond\nwriting lang goals and determing the game is DONE. Please write a single lang\ngoal in your response!\nuser: There are 4 bowls of blue, green, pink, yellow on the table. There are 3\nblocks of red, yellow, blue on the table. What should I do as the first step\nto match the blocks with the bowls of the literally same color? Note that not\nall the blocks can be matched and you can determine the game is DONE anywhere.\nassistant: put the yellow blocks in a yellow bowl\nuser: OK. What should I do next?\nassistant: put the blue blocks in a blue bowl\nuser: OK. I finish the task!\nListing 13: Prompt for Inner Monologue [17] in Tabletop Manipulation tasks\nH\nFull Dialogue\nUser: You are a helper agent in Minecraft. You need to generate the sequences of\ngoals for a certain task in Minecraft.\nPlanner: OK.\n======================================\nUser: How to obtain 1 diamond in Minecraft step-by-step?\nPlanner: The code for task obtaining diamond is as bellows:\ndef obtain_1_diamond(inventory = {}):\nmine({’log’:3}, null); # step 1: mine 3 log without tool\ncraft({’planks’:12}, {’log’:3}, null); # step 2: craft 12 planks from 3 log\ncraft({’stick’:4}, {’planks’:2}, null); # step 3: craft 4 stick from 2 planks\ncraft({’crafting_table’:1}, {’planks’:4}, null); # step 4: craft 1\ncrafting_table from 4 planks\ncraft({’wooden_pickaxe’:1}, {’planks’:3, ’stick’:2}, ’crafting_table’); # step\n5: craft 1 wooden_pickaxe from 3 planks and 2 stick, on crafting_table\nmine({’cobblestone’:2}, ’wooden_pickaxe’); # step 6: mine 2 cobblestone with\nwooden_pickaxe\ncraft({’stone_pickaxe’:1}, {’cobblestone’:3, ’stick’:2}, ’crafting_table’); #\nstep 7: craft 1 stone_pickaxe from 3 cobblestone and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’stone_pickaxe’); # step 8: mine 1 diamond with\nstone_pickaxe\nreturn ’diamond’\nDescriptor: I succeed on step 1, 2, 3, 4, 5, 6, but I fail on step 7 \"craft({’\nstone_pickaxe’:1}, {’cobblestone’:3, ’stick’:2}, ’crafting_table’);\". My\ninventory now has 7 planks, 2 stick, 1 crafting_table, 1 wooden_pickaxe, 2\ncobblestone, 5 dirt.\nExplainer: Because crafting stone_pickaxe needs 3 cobblestone, but I only have 2\ncobblestone in inventory.\nUser: Please fix above errors and re-plan the task \"obtain 1 diamond\".\nRePlanner: The code for task obtaining 1 diamond is as follows:\ndef obtain_1_diamond(inventory = {’planks’:7, ’stick’:2, ’crafting_table’:1, ’\nwooden_pickaxe’:1, ’cobblestone’:2, ’dirt’:5}):\nmine({’cobblestone’:1}, ’wooden_pickaxe’); # step 1: mine 1 cobblestone with\nwooden_pickaxe\ncraft({’stone_pickaxe’:1}, {’cobblestone’:3, ’stick’:2}, ’crafting_table’); #\nstep 2: craft 1 stone_pickaxe from 3 cobblestone and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’stone_pickaxe’); # step 3: mine 1 diamond with\nstone_pickaxe\n35\nreturn ’diamond’\nDescriptor: I succeed on step 1, 2 in new plan, but I fail on step 3 \"mine({’\ndiamond’:1}, ’stone_pickaxe’);\". My inventory now has 7 planks, 1\ncrafting_table, 1 wooden_pickaxe, 1 cobblestone, 5 dirt, 1 stone_pickaxe.\nExplainer: Because mining diamond needs to use the tool iron_pickaxe, but I do not\nhave it.\nUser: Please fix above errors and re-plan the task \"obtain 1 diamond\".\nRePlanner: The code for obtaining 1 diamond is as follows:\ndef obtain_1_diamond(inventory = {’planks’:7, ’crafting_table’:1, ’wooden_pickaxe\n’:1, ’cobblestone’:1, ’dirt’:5, ’stone_pickaxe’:1}):\nmine({’iron_ore’:1}, ’stone_pickaxe’); # step 1: mine 1 iron_ore with\nstone_pickaxe\nsmelt({’iron_ingot’:1}, {’iron_ore’:1}, ’furnace’); # step 2: craft 1\niron_ingot from 1 iron_ore, on furnace\ncraft({’iron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’); #\nstep 3: craft 1 iron_pickaxe from 3 iron_ingot and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’iron_pickaxe’); # step 4: mine 1 diamond with iron_pickaxe\nreturn ’diamond’\nDescriptor: I success on step 1 in new plan, but I fail on step 2 \"smelt({’\niron_ingot’:1}, {’iron_ore’:1}, ’furnace’);\". My inventory now has 7 planks, 1\ncrafting_table, 1 wooden_pickaxe, 6 cobblestone, 1 iron_ore, 20 dirt, 64\nstone.\nExplainer: Because smelting iron ingot needs to use the tool furnace, but I do not\nhave it.\nUser: Please fix above errors and replan the task \"obtain 1 diamond\".\nRePlanner: The code for obtaining 1 diamond is as follows:\ndef mine_1_diamond(inventory = {’planks’:7, ’crafting_table’:1, ’wooden_pickaxe’:1,\n’cobblestone’:6, ’iron_ore’:1, ’dirt’:20, ’stone’:64}):\ncraft({’furnace’:1}, {’cobblestone’:8}, null); # step 1: craft 1 furnace from 8\ncobblestone\ncraft({’iron_ingot’:1}, {’iron_ore’:1}, ’furnace’); # step 2: craft 1\niron_ingot from 1 iron_ore, on furnace\ncraft({’iron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’); #\nstep 3: craft 1 iron_pickaxe from 3 iron_ingot and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’iron_pickaxe’); # step 4: mine 1 diamond with iron_pickaxe\nreturn ’diamond’\nDescriptor: I fail on step 1 \"craft({’furnace’:1}, {’cobblestone’:8}, null);\". My\ninventory now has 7 planks, 1 crafting_table, 1 wooden_pickaxe, 6 cobblestone,\n1 iron_ore, 20 dirt, 64 stone.\nExplainer: Because crafting furnace needs 8 cobblestone, but I only have 6\ncobblestone.\nUser: Please fix above errors and re-plan the task \"obtain 1 diamond\".\nRePlanner: The code for obtaining 1 diamond is as follows:\ndef obtain_1_diamond(inventory = {’planks’:7, ’crafting_table’:1, ’wooden_pickaxe\n’:1, ’cobblestone’:6, ’iron_ore’:1, ’dirt’:20, ’stone’:64}):\nmine({’cobblestone’:8}, ’stone_pickaxe’); # step 1: mine 8 cobblestone with\nstone_pickaxe\ncraft({’furnace’:1}, {’cobblestone’:8}, null); # step 2: craft 1 furnace from 8\ncobblestone\nsmelt({’iron_ingot’:1}, {’iron_ore’:1}, ’furnace’); # step 3: craft 1\niron_ingot from 1 iron_ore, on furnace\ncraft({’iron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’); #\nstep 4: craft 1 iron_pickaxe from 3 iron_ingot and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’iron_pickaxe’); # step 5: mine 1 diamond with iron_pickaxe\nreturn ’diamond’\nDescriptor: I succeed on step 1 in new plan, but I fail on step 2 \"craft({’furnace\n’:1}, {’cobblestone’:8}, null);\". My inventory now has 7 planks, 1\ncrafting_table, 1 wooden_pickaxe, 1 stone_pickaxe, 8 cobblestone, 1 iron_ore,\n20 dirt, 64 stone.\nExplainer: Because crafting furnace needs to use the tool crafting_table, but I do\nnot have it.\nUser: Please fix above errors and re-plan the task \"obtain 1 diamond\".\nRePlanner: The code for obtaining 1 diamond is as follows:\ndef obtain_1_diamond(inventory = {’planks’:7, ’crafting_table’:1, ’wooden_pickaxe\n’:1, ’cobblestone’:8, ’iron_ore’:1, ’dirt’:20, ’stone’:64}):\n36\ncraft({’furnace’:1}, {’cobblestone’:8}, ’crafting_table’); # step 1: craft 1\nfurnace from 8 cobblestone, on crafting_table\nsmelt({’iron_ingot’:1}, {’iron_ore’:1}, ’furnace’); # step 2: smelt 1\niron_ingot from 1 iron_ore, on furnace\ncraft({’iron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’); #\nstep 3: craft 1 iron_pickaxe from 3 iron_ingot and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’iron_pickaxe’); # step 4: mine 1 diamond with iron_pickaxe\nreturn ’diamond’\nDescriptor: I succeed on step 1, 2 in new plan, but I fail on step 3 \"craft({’\niron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’);\". My\ninventory now has 6 planks, 1 crafting_table, 1 wooden_pickaxe, 1\nstone_pickaxe, 1 furnace, 1 iron_ingot, 20 dirt, 64 stone.\nExplainer: Because crafting iron pickaxe needs 3 iron_ingot, but I only have 1\niron_ingot.\nUser: Please fix above errors and re-plan the task \"obtain 1 diamond\".\nRePlanner: The code for obtaining 1 diamond is as follows:\ndef mine_1_diamond(inventory = {’planks’:6, ’crafting_table’:1, ’wooden_pickaxe’:1,\n’stone_pickaxe’:1, ’furnace’:1, ’iron_ingot’:1, ’dirt’:20, ’stone’:64}):\nmine({’iron_ore’:2}, ’stone_pickaxe’); # action 1: mine 2 iron_ore with\nstone_pickaxe\nsmelt({’iron_ingot’:2}, {’iron_ore’:2}, ’furnace’); # action 2: craft 2\niron_ingot from 2 iron_ore, on furnace\ncraft({’iron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’); #\naction 3: craft 1 iron_pickaxe from 3 iron_ingot and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’iron_pickaxe’); # action 4: mine 1 diamond with\niron_pickaxe\nreturn ’diamond’\nDescriptor: I succeed on step 1, 2 in new plan, but I fail on step 3 \"craft({’\niron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’);\". My\ninventory now has 5 planks, 1 crafting_table, 1 wooden_pickaxe, 1\nstone_pickaxe, 1 furnace, 3 iron_ingot, 20 dirt, 64 stone.\nExplainer: Because crafting iron_pickaxe needs 2 stick, but I do not have it.\nUser: Please fix above errors and re-plan the task \"obtain 1 diamond\".\nRePlanner: The code for obtaining 1 diamond is as follows:\ndef obtain_1_diamond(inventory = {’planks’:5, ’crafting_table’:1, ’wooden_pickaxe\n’:1, ’stone_pickaxe’:1, ’furnace’:1, ’iron_ingot’:3, ’dirt’:20, ’stone’:64}):\ncraft({’stick’:2}, {’planks’:1}, null); # action 15: craft 2 stick from 1\nplanks\ncraft({’iron_pickaxe’:1}, {’iron_ingot’:3, ’stick’:2}, ’crafting_table’); #\naction 16: craft 1 iron_pickaxe from 3 iron_ingot and 2 stick, on\ncrafting_table\nmine({’diamond’:1}, ’iron_pickaxe’); # action 17: mine 1 diamond with\niron_pickaxe\nreturn ’diamond’\nDescriptor: I succeed in step 1, 2, 3.\nUser: Good. I finish all steps and I obtain 1 stone_pickaxe successfully.\nPlanner: OK.\nListing 14: Full dialogue of task ObtainDiamond\n37\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.pdf"}
{"title":"LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation","authors":"Shuo Cheng, Danfei Xu","summary":"To assist with everyday human activities, robots must solve complex\nlong-horizon tasks and generalize to new settings. Recent deep reinforcement\nlearning (RL) methods show promise in fully autonomous learning, but they\nstruggle to reach long-term goals in large environments. On the other hand,\nTask and Motion Planning (TAMP) approaches excel at solving and generalizing\nacross long-horizon tasks, thanks to their powerful state and action\nabstractions. But they assume predefined skill sets, which limits their\nreal-world applications. In this work, we combine the benefits of these two\nparadigms and propose an integrated task planning and skill learning framework\nnamed LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages the\nsymbolic interface of a task planner to guide RL-based skill learning and\ncreates abstract state space to enable skill reuse. More importantly, LEAGUE\nlearns manipulation skills in-situ of the task planning system, continuously\ngrowing its capability and the set of tasks that it can solve. We evaluate\nLEAGUE on four challenging simulated task domains and show that LEAGUE\noutperforms baselines by large margins. We also show that the learned skills\ncan be reused to accelerate learning in new tasks domains and transfer to a\nphysical robot platform.","url":"http:\/\/arxiv.org\/abs\/2210.12631v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2210.12631v2","published":1666508225000,"comment":"Accepted to RA-L 2023","pdf_text":"IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\n1\nLEAGUE: Guided Skill Learning and Abstraction\nfor Long-Horizon Manipulation\nShuo Cheng1 and Danfei Xu1\nAbstract—To assist with everyday human activities, robots\nmust solve complex long-horizon tasks and generalize to new\nsettings. Recent deep reinforcement learning (RL) methods show\npromise in fully autonomous learning, but they struggle to reach\nlong-term goals in large environments. On the other hand, Task\nand Motion Planning (TAMP) approaches excel at solving and\ngeneralizing across long-horizon tasks, thanks to their powerful\nstate and action abstractions. But they assume predefined skill\nsets, which limits their real-world applications. In this work, we\ncombine the benefits of these two paradigms and propose an\nintegrated task planning and skill learning framework named\nLEAGUE (Learning and Abstraction with Guidance). LEAGUE\nleverages the symbolic interface of a task planner to guide RL-\nbased skill learning and creates abstract state space to enable skill\nreuse. More importantly, LEAGUE learns manipulation skills\nin-situ of the task planning system, continuously growing its\ncapability and the set of tasks that it can solve. We evaluate\nLEAGUE on four challenging simulated task domains and show\nthat LEAGUE outperforms baselines by large margins. We also\nshow that the learned skills can be reused to accelerate learning\nin new tasks domains and transfer to a physical robot platform.\nIndex Terms—Reinforcement Learning; Task and Motion Plan-\nning; Continual Learning\nI. INTRODUCTION\nD\nEVELOPING robots that can autonomously learn to\nwork in everyday human environments, such as house-\nholds, has been a long-standing challenge. Deep Reinforce-\nment Learning (DRL) methods have shown promise in allow-\ning robots to acquire skills with limited supervision [9, 16], but\nthey are still far from enabling home robots on their own. Two\nsignificant challenges stand out: 1) real-world tasks are often\nlong-horizon, requiring the learning agent to explore a vast\nspace of possible action sequences that grows exponentially\nwith the task duration, and 2) home robots must perform\ndiverse tasks in varying environments, requiring the learner\nto either generalize or quickly adapt to new situations.\nTo better learn long-horizon tasks, many DRL methods pro-\npose to use domain knowledge and structural prior [2, 22, 28].\nAutomatic goal generation in curriculum learning guides a\nlearning process using intermediate subgoals, enabling an\nagent to explore and make incremental progress toward a long-\nhorizon goal [22]. Other methods use skill primitives or learn\nhierarchical policies to enable temporally-extended decision-\nmaking\n[2, 19]. Although these approaches can outperform\nManuscript received: June, 23, 2023; Accepted August, 7, 2023.\nThis paper was recommended for publication by Jens Kober upon evaluation\nof the Associate Editor and Reviewers’ comments.\n1Georgia Institute of Technology, correspondence: shuocheng@gatech.edu\nDigital Object Identifier (DOI): see top of this page.\nTask Planner\nGoal: And[In(peg1, hole1),\nIn(peg2, hole2)]\nPick (?peg)\nPRE: {P4(?peg), …}\nEFF+: {P2(?peg), …}\n…\nInsert (?peg, ?hole)\nPRE: {P1(?peg), …}\nEFF+: {P3(?hole), …}\n…\nSymbolic Skill Ops\nstate abstraction \n& rewards\nSkill Library\nSkill Learning\nSymbolic Task Plan\nInsert (peg1, hole1)\nPRE: {P1(peg1), …}\nEFF+: {P3(hole1), …}\n…\nPick (peg2)\nPRE: {P4(peg2), …}\nEFF+: {P2(peg2), …}\n…\nstate abstraction & \nskill instantiation\nTask Execution\nFeedback Curriculum\nPick\n(a) Skill Learning and Abstraction \nwith Symbolic Operator Guidance\n(b) Task and Skill Planning\nreused across \ntasks and domains\nInsert\nPlace\nFig. 1: Overview of the LEAGUE framework. We present an\nintegrated task planning and skill learning framework. (a) The\nsystem uses the symbolic operator interface of a TAMP-like\nsystem as guidance to learn reusable manipulation skills (Alg.\n1). (b) A task planner composes the learned skills to solve\nlong-horizon tasks (Alg. 2). As an integrated system, the task\nplanner acts as a feedback curriculum (bottom) to guide skill\nlearning, and the RL-based skill learner continuously grows\nthe set of tasks that the system can solve.\nvanilla DRL, they still suffer from low sample efficiency, lack\nof interpretability, and fragile generalization [2, 28]. Most\nimportantly, the learned policies are often task-specific and\nfall short in cross-task and cross-domain generalization.\nIn the meantime, more established paradigms in robotics\nhave long sought to address these challenges. In particular,\nTask and Motion Planning (TAMP) [8, 14] leverages symbolic\naction abstractions to enable tractable planning and strong gen-\neralization. Specifically, the symbolic action operators divide a\nlarge planning problem into pieces that are each easier to solve.\nAnd the “lifted” action abstraction allows skill reuse across\ntasks and even domains. For example, a grasp skill operator\nand its underlying implementation can be easily adapted to\nsolve a new task in a new domain. At the same time, most\nTAMP-style approaches assume access to a complete set of\nskills before deployment. This is impractical for two reasons.\nFirst, it is hard to prepare skills for all possible tasks. A\nrobot must be able to grow its skill set on demand. Second,\nit is hard to hand-engineer manipulation skills for complex or\ncontact-rich tasks (e.g., insertion). The challenges make TAMP\nmethods difficult to deploy in real-world settings.\nIn this work, we introduce LEAGUE (LEarning and\nAbstraction with GUidancE), an integrated task planning and\nskill learning framework that learns to solve and generalize\nacross long-horizon tasks (See Fig. 1). LEAGUE harnesses\narXiv:2210.12631v2  [cs.AI]  22 Aug 2023\n2\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nthe merits of the two research paradigms discussed above.\nStarting with a task planner that is equipped with skills that are\neasy to implement (e.g., reaching), LEAGUE continuously\ngrows the skill set in-situ using a DRL-based learner. The\nintermediate goals in a task plan are prescribed as rewards for\nthe learner to acquire and refine skills, and the mastered skills\nare used to reach the initial states of the new skills. Moreover,\nLEAGUE leverages the action operator definition, i.e., the\npreconditions and the effects, to determine a reduced state\nspace for each learned skill, akin to the concept of information\nhiding in feudal learning [28]. The key idea is to abstract away\ntask-irrelevant features to make the learned skills modular and\nreusable. Together, the result is a virtuous cycle where the task\nplanner guides skill learning and abstraction, and the learner\ncontinuously expands the set of tasks that the system can solve.\nWe conduct empirical studies on four challenging long-\nhorizon manipulation tasks built on the Robosuite simula-\ntion framework [31]. We show that LEAGUE is able to\noutperform state-of-the-art hierarchical reinforcement learning\nmethods [19] by a large margin. We also highlight that our\nmethod can achieve strong generalization to new task goals\nand even domains by reusing and adapting learned skills. As\na result, LEAGUE can solve a challenging simulated coffee-\nmaking task where competitive baselines fall flat. We also\ndemonstrate a LEAGUE system trained in simulation on a\nphysical Franka Emika Panda robot.\nIn summary, our primary contributions are: 1) we leverage\nthe state and action abstractions readily available in a TAMP\nsystem to learn reusable skills, 2) we instantiate the synergies\nbetween the task planner and the skill learner as an integrated\ntask planning and skill learning framework, and 3) we show\nthat the framework can progressively learn skills to solve\ncomplex long-horizon tasks and generalize the learned skills\nto new task goals and domains.\nII. RELATED WORK\nTAMP and Learning for TAMP. Task and Motion Planning\n(TAMP) [8, 14] is a powerful paradigm to solve long-horizon\nmanipulation tasks. The key idea is to break a challenging\nplanning problem into a set of symbolic-continuous search\nproblems that are individually easier to solve. However, TAMP\nmethods require high-level skills and their kinematics or dy-\nnamics models a priori. The assumptions preclude domains for\nwhich hand-engineering manipulation skills is difficult, such as\ncontact-rich tasks. Recent works proposed to learn dynamics\nmodels for TAMP by characterizing skill preconditions and\neffects [15, 17, 24]. For example, Konidaris et al. [15] learns\ncompact symbolic models of an environment through trial-\nand-error. Liang et al. [17] uses graph neural networks to\nmodel skill effects. However, these works still require hand-\nengineering complete skill sets that can solve the target task,\nwhich may not be feasible in real-world applications. Our\nidea of learning skills to augment TAMP systems is closely\nrelated to Silver et al. [24], which proposed to learn neural-\nsymbolic skills via imitation. But they require access to hard-\ncoded demonstration policies that can readily solve the target\ntasks. Our work instead aims to progressively grow TAMP\nskill libraries via guided reinforcement learning to solve long-\nhorizon contact-rich manipulation tasks.\nCurriculum for RL. Our idea to guide skill learning with\na task planner is connected to curriculum-based RL, which\nis to expose an agent to incrementally more difficult in-\ntermediate tasks before mastering a target task [18]. The\nintermediate tasks can take the form of environments [6]\nand subgoals [22, 27]. For example, VaPRL [22] starts with\nnear-success initialization and moves the initial states further\naway. While effective at accelerating task learning, existing\ncurricula focus on teaching tasks or domain-specific policies.\nIn contrast, our method leverages the symbolic abstraction of\na task planner to learn a repertoire of modular and composable\nskills. We show that we can compose learned skills to achieve\nnew goals and even transfer skills to new task domains.\nState and Action Abstractions. State and action abstractions\nare crucial for learning tasks in a large environment [1]. State\nabstraction allows agents to focus on task-relevant features\nof the environment. Action abstraction enables temporally-\nextended decision-making for long-horizon tasks. There exists\na large body of work on learning either or both types of\nabstractions [1, 3, 5, 13, 15, 30]. For example, Jonschkowski\net al. [13] explores different representation learning objectives\nfor effective state abstraction. Abel et al. [1] introduces a\ntheory for value-preserving state-action abstraction. However,\nautonomously discovering suitable abstractions remains an\nopen challenge. Our key insight is that a TAMP framework\nprovides powerful state and action abstractions that can readily\nguide skill learning. Specifically, the symbolic interface of an\naction operator defines both the precondition and the effect\n(action abstraction) and the state subspace that is relevant to\nthe action (state abstraction). The abstractions allow us to train\nskills that are compatible with the task planner and prevent\nthe learned skills from being distracted by irrelevant objects,\nmaking skill reuse across tasks and domains possible.\nHierarchical Modeling in Robot Learning. Our method\ninherits the bi-level hierarchy of a TAMP framework. Hier-\narchical modeling has a rich history in robotics. In addition to\nTAMP, various general frameworks including hierarchical task\nnetworks [11, 20, 29], logical-geometric programming [26],\nand hierarchical reinforcement learning (HRL) [2, 28] have\nbeen proposed to exploit the hierarchical nature of common\nrobotics tasks. In the context of HRL, a small number of works\nhave explored symbolic planner-guided HRL [12]. However,\nthese methods require tabular state representations and are thus\nlimited to simple grid-world domains. In robotics domains, a\nclosely related research thread is to use behavior primitives\nin RL [4, 19]. For example, MAPLE [19] trains a high-\nlevel policy that chooses hand-engineered behavior primitives\nand atomic actions. Our method instead leverages a symbolic\nplanner to serve as the high-level controller to compose learned\nskills, allowing us to continuously extend the skill set while\nalso leading to better generalization.\nIII. METHOD\nWe seek to enable robots to solve and generalize across\nlong-horizon tasks. Our primary contribution is a novel in-\ntegrated task planning and skill learning framework named\nCHENG et al.: LEAGUE\n3\nLEAGUE. Here, we first provide the necessary background in\nSec. III-A, and describe how LEAGUE (1) learns reusable\nskills guided by the symbolic operators of a task planner\nin Sec. III-B and (2) uses planner-generated task plans as\nan autonomous curriculum to continuously learn skills and\nexpand the capability of the overall system in Sec. III-C.\nA. Background\nMDP. We consider a Markov Decision Process (MDP) <\nX, A, R(x, a), T (x′|x, a), p(x(0)), γ >, with continuous state\nspace X, continuous action space A, reward function R,\nand environment transition model T . p(x(0)) denotes the\ndistribution of the initial states, x(H) denotes terminal state,\nand γ is the discount factor. The objective for RL train-\ning is to maximize the expected total reward of the policy\nπ(a|x) that the agent uses to interact with the environment:\nJ = Ex(0),a(0),...,x(H)∼π,p(x(0))\n\u0002P\nt γtR(x(t), a(t))\n\u0003\n.\nTask planning space. To support task planning, we assume\nthe environment is augmented with a symbolic interface\n< O, Λ, ¯Ψ, ¯Ω, G >, where O denotes the object set and Λ\ndenotes a finite set of object types. Each object entity o ∈O\n(e.g., peg1) has a specific type λ ∈Λ (e.g., peg) and a tuple\nof dim(λ)-dimensional feature containing information such as\nposes and joint angles, and the environment state x ∈X is a\nmapping from object entities to features: x(o) ∈Rdim(type(o)).\nPredicates ¯Ψ describe the relationships among multiple ob-\njects. Each predicate ¯ψ (e.g., Holding(?object:peg))\nis characterized by a tuple of object types (λ1, ..., λm) and\na binary classifier that determines whether the relationship\nholds: c ¯\nψ : X ×Om →{True, False}, where each substitute\nentity oi ∈O is restricted to have type λi ∈Λ. Evaluating\na predicate on the state by substituting corresponding object\nentities will result in a ground atom (e.g., Holding(peg1)).\nA task goal g ∈G is represented as a set of ground atoms,\nwhere a symbolic state xΨ can be obtained by evaluating a\nset of predicates ¯Ψ and keeping all positive ground atoms:\nxΨ = PARSE(x, O, ¯Ψ)\n△= {ψ : c ¯\nψ(x, O\n¯\nψ) = True, ∀O\n¯\nψ ⊆O, ∀¯ψ ∈¯Ψ}\n(1)\nwhere O ¯\nψ is a subset of object entities that each entity oi\nhas the same object type λi specified by the predicate ¯ψ.\nSymbolic skill operators. Following prior works [8], we\ncharacterize lifted skill operator ¯ω\n∈\n¯Ωby a tuple <\nPAR, PRE, EFF+, EFF−>, where PRE denotes the precon-\ndition of the operator, which is a set of lifted atoms defining\nthe condition that the operator is executable. EFF+ and EFF−\nare lifted atoms that describe the expected effects (changes\nin conditions) upon successful skill execution. PAR is an\nordered parameter list that defines all object types used in\nPRE, EFF+, and EFF−. A ground skill operator ω substi-\ntutes lifted atoms with object instances: ω = <¯ω, δ>\n△=<\nPRE, EFF+, EFF−>, where δ : Λ →O. Given a task goal,\na symbolic task plan is a list of ground operators that, when\nthe instantiated skills are executed successfully, lead to an\nenvironment state that satisfies the goal condition.\nAs a running example, consider a short task of inserting\na peg (peg1) into the target hole (hole1). The applicable\noperators for this task are defined as:\nPick(?object)\nPAR: [?object:peg]\nPRE: {HandEmpty(),OnTable(?object)}\nEFF−: {HandEmpty(),OnTable(?object)}\nEFF+: {Holding(?object)}\nInsert(?object,?hole)\nPAR: [?object:peg,?hole:hole]\nPRE: {Holding(?object),IsClear(?hole)}\nEFF−: {Holding(?object),IsClear(?hole)}\nEFF+: {HandEmpty(),In(?object,?hole)}\nThe environment starts with peg1 on the table. Evaluating\nthe\nPARSE\nfunction\n(Eq.\n1)\nyields\na\nsymbolic\nstate\n{HandEmpty(),IsClear(hole1),OnTable(peg1)},\na set of grounded atoms that satisfies the preconditions of the\ngrounded operator Pick(peg1). This grounded operator,\nif executed successfully, should reach the symbolic state of\n{Holding(peg1),IsClear(hole1)}, which is an inter-\nmediate subgoal for the final task goal that is characterized by\nthe grounded atoms {HandEmpty(),In(peg1,hole1)}.\nThe\nsymbolic\ntask\nplan\nis\ntherefore\nP\n=\n[Pick(peg1),Insert(peg1,hole1)].\nWe are interested in learning primitive manipulation skills\nfor accomplishing individual subgoals induced by the expected\neffects of the corresponding operators – the building blocks\nthat constitute a symbolic task plan. In our setting, each lifted\noperator ¯ω will have a corresponding skill policy π to be\nlearned, while during execution the ground operators belong\nto the same lifted operator ¯ω share the same skill policy. We\nassume access to the predicates ¯Ψ and the lifted operators ¯Ωof\nthe environments and focus on efficiently learning the skills\nfor achieving the effects. Note that it is possible to invent\nand learn predicates and operators [23, 25], but the topics are\nbeyond the scope of this work.\nB. Skill Learning and Abstraction with Operator Guidance\nAction and state abstractions are fundamental to TAMP’s\nabilities to solve and generalize across long-horizon tasks [8].\nOur key insight is that these abstractions, in the form of sym-\nbolic action operators, can readily guide RL-trained policies\nto gain similar abilities. Specifically, for action abstraction,\nwe train temporally-extended skills to reach desired effects\nof a skill operator by prescribing the effect condition as\nshaped reward. For state abstraction, we take inspiration from\nthe idea of information hiding in feudal learning [28] and\nuse the precondition and effect signature of an operator to\ndetermine a skill-relevant state space for its corresponding\nlearned policy. This allows the policy to be robust against\ndomains shift and achieve generalization, especially in large\nenvironments where most elements are impertinent to a given\nskill. To further accelerate skill learning, we leverage the\nexisting motion planning capability of a TAMP system to\naugment the learned skill with a transition primitive. Below\nwe describe each component in detail.\nSymbolic operators as reward guidance. Our skill learner\nleverages the existing RL method that supports continuous\naction space. In this work, we use Soft Actor-Critic (SAC) [10]\n4\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nas the basis for skill learning. SAC leverages entropy regular-\nization to enhance exploration. Given the ground operator ω\nof a skill, we can define an operator-guided reward RΨ for\neach individual skill based on continuous environment state x\nand the action a produced by the corresponding policy π that\ntakes in skill-related state ˆx (which will be described later),\nthe objective for our skill learning is therefore rewritten as:\nJ = Ex(0),a(0),...,x(H)∼π,p(x(0))\n\n\nX\nt\nγt(RΨ(x(t), a(t), ω)\n+αH(π(·|ˆx(t)))\n\n\n(2)\nwhere RΨ(·) 7→[0, 1], H(·) is the entropy term introduced\nby SAC. While it is possible to learn directly from sparse\nreward defined by the symbolic state, in practice we associate\neach operator-guided reward with a dense reward function im-\nplemented in the Robosuite [31] benchmark for better learning\nefficiency. Continuing our running example, the shared reward\nfor Pick is defined as 1 −tanh(10.0 ∗d), where d is the\ndistance between the gripper center and target object center,\nand the target object is identified by the task planner.\nEnhance skill reuse with feudal state abstraction. With the\nprecondition and effect signature of a ground operator ω, we\ncan determine a skill-relevant state space to further prevent the\nlearned policy from being distracted by task-irrelevant objects:\nˆx = EXTRACT(x, ω, O)\n△= {x(o) : o ∈PAR, ∀o ∈O}\n(3)\nwhere PAR is the parameter list of the ground operator. In our\nrunning example, the skill-related state ˆx for Pick(peg1)\nincludes the 6D pose of peg1 and the state of the robot. This\ndesign echoes previous works that learn to impose constraints\non states [3], except that here the constraints are directly\ninformed by the task planner.\nAccelerate learning with transition motion primitives. A\nkey to our method is learning modular manipulation skills\nthat can be composed to solve long tasks. However, for\ncomplex manipulation problems, even learning such short\nskills can be challenging. On the other hand, although TAMP\nsystems fall short when facing contact-rich manipulation, they\nexcel at finding collision-free paths. To this end, we propose\nto augment our policy with motion planner-based transition\nprimitives. The key idea is to first approach the skill-relevant\nobject (per the skill operator) using an off-the-shelf motion\nplanner, before convening RL-based skill learning. For the\ntarget of motion planning, we simply set the goal position to be\n0.04m higher than the object or placement position of interest\nthat was identified by the task planning. The component\nsignificantly speeds up the exploration while still allowing the\nsystem to learn closed-loop contact-rich manipulation skills.\nC. Integrated Task Planning and Skill Learning\nSo far, we have described a recipe for learning reusable\nskills using symbolic skill operators as guidance. But these\nskills are not learned in silos. A key to LEAGUE’s success is\nto learn skills in-situ of a task planning system. The integrated\nplanning and learning scheme ensures that the learned skills\nare compatible with the planner, and the skill learner can con-\ntinuously extend the capability of the overall system to solve\nmore tasks. Here we first describe how LEAGUE performs\ntask planning and execution at inference time, and then we\nintroduce an algorithm that uses task plans as an autonomous\ncurriculum to schedule skill learning.\nTask planning and skill execution. To plan for task goal g,\nwe first PARSE (Eq. 1) the continuous environment state x for\nobtaining symbolic state xΨ, which affords symbolic search\nwith ground operators. We then ground each lifted operator\n¯ω ∈¯Ωon the object set O by substituting object entities\nin preconditions and effects, leading to ground operators\nω =< PRE, EFF+, EFF−> that support operating with\nsymbolic states. A ground operator is considered executable\nonly when its preconditions are satisfied: PRE ⊆xΨ. The\noperators induce an abstract transition model F(xΨ, ω) that\nallows planning in symbolic space:\nx′\nΨ = F(xΨ, ω)\n△= (xΨ \\ EFF−) ∪EFF+\n(4)\nWe use PDDL [7] to build the symbolic planner and use A∗\nsearch for generating high-level plans.\nWith the generated task plan, we sequentially invoke the\ncorresponding skill π∗to reach the subgoal that complies\nwith the effects of each ground operator ω in the plan. We\nrollout each skill controller until it fulfills the effects of the\noperator or a maximum skill horizon H is reached. To verify\nwhether the l-th skill is executed successfully, we first obtain\nthe corresponding symbolic state xl\nΨ by parsing the ending\nenvironment state x∗. The execution is considered successful\nonly when the environment state x∗conforms to the expected\neffects: F(xl−1\nΨ , ωl) ⊆xl\nΨ. We keep track of the failed skills\nand the starting simulator info s∗to inform the learning\ncurriculum.\nTask planner as an automated curriculum. To efficiently\nacquire all necessary skills for a given multi-step task, we\nleverage the task planner as an automated curriculum to learn\nskills in a progressive manner. The key idea is to use more\nproficient skills to reach the preconditions of skills that require\nadditional learning (See Fig. 1). The algorithm is sketched in\nAlg. 1 and Alg. 2. On a high level, we repeat task planning\nand skill learning until convergence. We keep track of failed\nskills during N task executions and adopt strict scheduling\ncriteria, where a skill is scheduled for learning (Sec. III-B)\nif it ever fails during the N episodes. Notably, we share the\nreplay buffers for different skill instances (e.g., Pick(peg1)\nand Pick(peg2)) that belong to the same lifted operator, so\nthat the relevant experience can be reused to further improve\nthe learning efficiency and generalization.\nIV. EXPERIMENTS\nOur experiments aim to show that 1) LEAGUE can progres-\nsively learn and refine skills to solve long-horizon tasks and 2)\nour novel operator-guided skill learning and abstraction algo-\nrithm produces composable and reusable skills, enabling quick\nadaptation to new tasks and domains. Finally, we demonstrate\ntransferring a trained LEAGUE system to a physical robot.\nCHENG et al.: LEAGUE\n5\n\u0007\u0006\u0007\n\u0007\u0006\t\n\u0007\u0006\n\u0007\u0006\f\n\u0007\u0006\r\n\b\u0006\u0007\n\b\u0006\t\n\b\u0006\n\b\u0006\f\n\b\u0006\r\n\u000e\u001a!\u0003\u0011\u001f\u0014\u001c\u001e\u0003\u0004\"\b\u000f\u0005\n\u0007\u0006\u0007\n\u0007\u0006\u000b\n\b\u0006\u0007\n\u0011\u0018\u0017\u0019\u0019\u0003\u0010\u001d\u001b\u0015\u0017\u0013\u0017\u0014\u001a\u0013#\n\u0010 \u0019\u0019\n\u0010\u0017\u0013\u0018\n\u0010\u0019\u0012\u0013\u0014\n\u0010 \u001e\u0016\nFig. 2: Visualizing skill learning progress. The plot shows the proficiency level of each skill throughout the process of\nlearning a simplified StowHammer task. The skill proficiency is the average normalized reward a skill receives at an iteration.\nAlgorithm 1 SKILLCURRICULUM\nhyperparameters:\nNumber of training iterations K\ninput:\nenv\n▷task environment\ng\n▷symbolic task goal\n¯Ψ\n▷state predicates\n¯Ω\n▷lifted operators\nstart\nΠ ←[π(0)\n1\n, ..., π(0)\n|¯\nΩ|]\n▷initialize all skill policies\nt ←0\nwhile Not Converged do\nD ←∅\n▷buffer for failed skills\nfor i ←[1, ..., N] do\nD ←D ∪TRYSOLVETASK(env, g, ¯Ψ, ¯Ω, Π)\nend for\nfor i, s, ω ←D do\nπ(t)\ni\n←Π[i]\nfor k ←[1, ..., K] do\nπ(t+k)\ni\n←SAC(env, s, π(t+k−1)\ni\n, ω)\n▷RL training\nend for\nΠ[i] ←π(t+K)\ni\nend for\nt ←t + K\nend while\nreturn Π\nAlgorithm 2 TRYSOLVETASK\nhyperparameters:\nMaximal skill horizon H\ninput:\nenv\n▷task environment\ng\n▷symbolic task goal\n¯Ψ\n▷state predicates\n¯Ω\n▷lifted operators\nΠ\n▷skill policies\nstart\nO, x(0) ←env.get_state()\nx(0)\nΨ\n←PARSE(x(0), O, ¯Ψ)\n▷continuous state to symbolic state\nΩ←GROUND(O, ¯Ω)\n▷get grounded operators\n[ω1, ..., ωL] ←SEARCH(x(0)\nΨ , g, Ω)\n▷found plan with length L\nD, l ←[], 0\nwhile l < L do\ni ←LOOKUPSKILL(ωl)\nπ∗←Π[i]\ns∗←env.get_sim()\n▷get simulator state\nx∗←ROLLOUT(env, π∗, H)\nif ISSUCCESS(x∗, ωl) then\nl ←l + 1\n▷advance to the next skill\ncontinue\nelse\nD ←D ∪(i, s∗, ωl)\n▷collect failed skills and states\nend if\nend while\nreturn D\nA. Experimental Setup\nWe conduct evaluations in four simulated domains, in which\nwe devise tasks that require multi-step reasoning, contact-rich\nmanipulation, and long-horizon interactions (See Fig. 3).\nStackAtTarget is to stack two cubes on a tight target region\nwith a specific order. The applicable skill operators are Pick\nand Place. Since the cubes are randomly placed in the scene,\nthe top cube may occupy the target region, in which situation\nthe robot must first remove the top cube before stacking.\nStowHammer requires the robot to stow two hammers\ninto different closed cabinets. It involves four skills: Pick,\nPlace, Pull, Push. Since the workspace is tight, the robot\nneeds to close an opened cabinet before being able to open\nthe other one, which requires multi-step reasoning.\nPegInHole is to pick up and insert two pegs into two\nhorizontal holes. The applicable operators are Pick and\nInsert. This task challenges the robot with contact-rich\nmanipulations and multi-step planning.\nMakeCoffee is to pick up a coffee pod from a closed cabi-\nnet, insert it into the holder of the coffee machine, and finally\nclose both the lid and the cabinet, The applicable operators\nare Pick, Pull, Push, CloseLid, and InsertHolder.\nThe environments are built on Robosuite [31] simulator. We\nuse a Franka Emika Panda robot arm that is controlled at\n20Hz with an operational space controller (OSC), which has\n5 degrees of freedom: end-effector position and the yaw angle\nand the position of the gripper. See Fig. 3 for an illustration.\nB. Visualize the Progressive Skill Learning Process\nBefore discussing quantitative comparisons, we seek to\ngain intuitive understanding of our progressive skill learning\nscheme (Sec. III-C), where the learning curriculum adjusts\nbased on the proficiencies of the skills. In Fig. 2, we visualize\nthe proficiency level of each skill throughout the process of\nlearning a simplified StowHammer task, where the goal is\nto stow away one hammer instead of two. The y axis is\nthe average normalized reward a skill receives. Note that we\nonly visualize a subset of skills scheduled for training at an\niteration. The corresponding behavior of each skill at a certain\nstage is visualized in the snapshots on top of the plot.\nAt the beginning of the training, the system can only reach\nthe\nprecondition\nfor\nexecuting\nthe\nPull(?cabinet)\n6\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\ncube0\ncube1\ntarget0\nAND[OnTarget(cube0, target0),\nOn(cube1, cube0)]\nTraining goal\ncabinet1\ncabinet0\nhammer1\nhammer0\nAND[InCabinet(hammer0, cabinet0),\nInCabinet(hammer1, cabinet1),\nIsCabinetClose(cabinet0),\nIsCabinetClose(cabinet1)]\nTraining goal\nhole1\npeg2\npeg1\nhole2\nAND[In(peg2, hole2),\nIn(peg1, hole1)]\nTraining goal\nholder\nlid\npod\ncabinet\nAND[In(pod, holder),\nIsLidClose(lid),\nIsCabinetClose(cabinet)]\nTraining goal\nFig. 3: Simulation task setup. We show the initialization\nof the simulation setups for the four tasks: StackAtTarget,\nStowHammer, PegInHole and MakeCoffee.\nskill\nbut\nnot\nother\nskills,\nthus\nthe\nexperience\nof\nPull(?cabinet) skill is collected and it is repeatedly\nselected for training. Until the agent is able to open one of\nthe cabinets, the second planned skill Pick(?object) is\nthen instantiated for learning and execution. Finally at the\nend of the training, all skills become proficient to be used\nto execute the entire task. The result qualitatively shows that\nLEAGUE’s automated curriculum is effective at progressively\nlearning skills to achieve long-horizon task goals.\nC. Quantitative Evaluation\nHere, we seek to highlight various aspects of our solution\nparadigm through quantitatively comparing LEAGUE with a\nnumber of strong baseline methods. Below we describe the\nbaselines and discuss the results.\n• RL (SAC): We adopt SAC [10] as a strong RL baseline.\nTo facilitate a fair comparison, we extend the vanilla task\nreward function to staged rewards using an oracle task\nplan, where the reward at each step is the summation\nof achieved rewards for each completed subgoal and the\nreward for the current subgoal.\n• Curriculum RL (CRL): We follow the main idea of\nstate-of-the-art curriculum RL approaches [22, 27], which\nstarts the training with near-success initializations and\ngradually move the initial states back to the true en-\nvironment initial states. To facilitate a fair comparison,\nwe sample the curriculum’s initial states based on the\nsubgoals of an oracle task plan (in reverse) and adopt the\nsame staged reward described above.\n• Hierarchical RL (HRL): This baseline adapts the recent\nprimitive-based HRL frameworks [4, 19] for our tasks.\nThe key idea is to train a high-level meta controller\nto compose parameterized skill primitives and atomic\nactions. We base our implementation on MAPLE [19]\nand use the oracle task plan to identify the target objects\nfor defining the affordance to guide the exploration.\n• Symb+MP: An open-loop baseline that resembles a\nvanilla TAMP framework, which greedily generates a\nmotion plan for each skill in a task plan. The robot then\nexecutes the plan through a trajectory controller.\n• Symb+RL: An ablation baseline of LEAGUE that re-\nmoves the state abstraction ( III-B) and retains all other\nfeatures including the symbolic plan-based curriculum.\nThe multi-stage nature of our evaluation tasks makes de-\nsigning smooth task-level metrics difficult. Thus we adopt task\nprogress as our metric, which is defined as the summed reward\nof all task stages normalized to [0, 1]. Below we discuss the\nmain findings based on Fig. 4.\nHigh-level reasoning is critical for solving long-horizon\ntasks. We observe that in StackAtTarget, a long-horizon task\nwith relatively simple manipulation steps, methods equipped\nwith a task planner (LEAGUE, Symb+MP, and Symb+RL)\nsignificantly outperforms all other baselines. The most com-\npetitive HRL baseline occasionally learns to move the bottom\ncube to the target region. This shows the value of explicit high-\nlevel reasoning, in particular as a plan-informed automated\ncurriculum in LEAGUE. Notably, the open loop Symb+MP\nperforms on par with LEAGUE because simple picking and\nplacing can readily be solved by open-loop trajectories.\nLEAGUE can solve long-horizon, contact-rich manip-\nulation tasks. LEAGUE significantly outperforms all other\nbaselines in StowHammer and PegInHole, which are both\nlong-horizon and require contact-rich manipulation. Notably,\nmost baselines cannot advance beyond opening the cabinet in\nStowHammer and picking up the first peg in PegInHole.\nSkill reuse is critical to learning structured tasks. Com-\nmon multi-step tasks have repeating structures, which can be\nleveraged by methods that explicitly reuse learned skills. We\nnote that both LEAGUE and Symb+RL perform competitively\nin StackAtTarget that involve repeating steps (i.e., stack two\ncubes). On the other hand, HammerPlace and PegInHole\ninvolve more objects, most of which are not relevant to a\ngiven skill. This prevents na¨ıve skill reuse — a policy may\nlearn spurious correlation to these irrelevant features — and\nnecessitates state abstraction, which we will discuss next.\nState abstraction facilitates skill reuse in complex envi-\nronments. We observe that LEAGUE outperforms Symb+RL\nin both HammerPlace and PegInHole. This shows that state\nabstraction can further improve skill reuse in complex envi-\nronments by ignoring features that are irrelevant to a skill.\nWe will also show in Sec. IV-D that skill reuse enables our\nmethod to generalize to novel task goals and domains.\nOther observations. We observe that without explicit prior\nstructures such as motion primitive, SAC baseline is able to\nexploit environment artifacts and learn shortcut behaviors. For\nexample, in the StowHammer task, SAC agent learns to grip\nthe head of the hammer to prevent slipping, but the grasping\npose precludes it from fitting the hammer to the drawer.\nMoreover, our analysis found that the CRL agent often failed\nto reach the final goal from some intermediate states due to the\nstrong sequential dependency of our evaluation tasks: the robot\nmust succeed in one stage to reach the pre-condition of the\nnext. And because the environment steps budget is distributed\nto multiple stages, CRL often underperforms other baselines\nCHENG et al.: LEAGUE\n7\n\u0006\n\u0007\n\b\n\t\n\n\u000b\n\u0010!(\u0003\u0018&\u001c#%\n\u0007\u001c\f\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\f\n\u0006\u0005\r\n\u0007\u0005\u0006\n\u0019\u001a%\u001e\u0003\u0016$\"\u001d$\u001c%%\n\u0016\u001c\u001d\u0012!\u0011\"\u001f\u001c\n\u0015'$%\n\u0018) \u001b\u0004\u0017\u0013\n\u0011\u0017\u0013\n\u000f\u0017\u0013\n\u0018\u000e\u000f\n\u0018) \u001b\u0004\u0014\u0016\n\u0006\n\u0007\n\b\n\t\n\n\u000b\n\u0010\u001f&\u0003\u0017$\u001b!#\n\u0007\u001b\f\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\f\n\u0006\u0005\r\n\u0007\u0005\u0006\n\u0018\u0019#\u001d\u0003\u0015\" \u001c\"\u001b##\n\u0017$ '\u0011\u0019\u001e\u001e\u001b\"\n\u0014%\"#\n\u0017(\u001e\u001a\u0004\u0016\u0012\n\u0011\u0016\u0012\n\u000f\u0016\u0012\n\u0017\u000e\u000f\n\u0017(\u001e\u001a\u0004\u0013\u0015\n\u0006\n\u0007\n\b\n\t\n\u000f\u001f&\u0003\u0016$\u001b!#\n\u0007\u001b\u000b\n\u0006\u0005\u0006\n\u0006\u0005\b\n\u0006\u0005\n\u0006\u0005\u000b\n\u0006\u0005\f\n\u0007\u0005\u0006\n\u0017\u0018#\u001d\u0003\u0014\" \u001c\"\u001b##\n\u0016$\u0018\u001a\u001d\r$\u0017\u0018\"\u001c\u001b$\n\u0013%\"#\n\u0016'\u001e\u0019\u0004\u0015\u0011\n\u0010\u0015\u0011\n\u000e\u0015\u0011\n\u0016\r\u000e\n\u0016'\u001e\u0019\u0004\u0012\u0014\nFig. 4: Baseline comparison. We compare relevant methods on three task domains. The plot shows the corresponding\naverage task progress during evaluation throughout training, which is measured as the summation of achieved rewards of\neach successfully executed skill in the task plan and normalized to 1. The results are reported using 5 random seeds, with the\nstandard deviation shown as the shaded area.\nTABLE I: We report the performance of applying our method\nto new task goals in the StowHammer and the PegInHole\ndomains without additional learning.\nTraining Goal\nTest Goal1\nTest Goal2\nStowHammer\n0.94 ± 0.21\n0.90 ± 0.12\n0.73 ± 0.31\nPegInHole\n0.87 ± 0.23\n0.53 ± 0.05\n1.00 ± 0.00\n(e.g., SAC) in completing the initial stages of a task.\nD. Generalization to New Tasks and Domains\nTo validate that our method can effectively generalize to\nnew task goals and even new task domains by reusing learned\nskills, we present the following experiments.\nGeneralize to new task goals. Besides evaluating the training\ngoals (shown in Fig. 3), we directly test our models on\nnew task goals for the StowHammer and the PegInHole\ndomains. For StowHammer domain, the first test goal is to\nswap the hammer-cabinet mapping. The second test goal is\nto place hammer1 into cabinet0 and keep cabinet1\nopen. For PegInHole, the first test goal is to swap the peg-\nhole mapping. The second goal is to only insert peg1 into\nhole2. The results are in Table I. We observe that LEAGUE\nexperiences little performance drop when generalizing to new\ntask goals without additional training, demonstrating strong\ncompositional generalization capability and skill modularity.\nQuick adaptation to new domains. Another exciting pos-\nsibility of LEAGUE is to transfer skills learned from one\ndomain to another. We design an experiment to validate this\nfeature. The target domain is MakeCoffee, which is the most\nchallenging task of the four. We adapt skills Pick, Pull, and\nPush learned in the StowHammer domain for learning the\nMakeCoffee task. As shown in Fig. 6, compared to learning\nfrom scratch, transferring learned skills can significantly accel-\nerate learning (the x-axis is shorter than in Fig. 4) and enables\nthe robot to solve the entire task. This highlights LEAGUE’s\nstrong potential for continual learning.\nE. Real World Demonstration\nWe demonstrate transferring simulation-trained LEAGUE\nsystem to two real-world task domains: StackThreeAtTarget\nand StowObject. For the StackThreeAtTarget task, we ran-\ndomly place three cubes and a target region on the table. The\ntask is to stack the cubes at the target region. We directly\nreuse the skills trained in StackAtTarget in simulation to\ndemonstrate generalization to different number of objects and\ninitial conditions. The StowObject is to stow two objects into\ntwo cabins. Similar to StowHammer, the task also requires\nthe robot to operate the cabinets. We reuse skills trained in\nthe simulated StowHammer domain.\nOur system uses a Franka Emika Panda robot. We take\nRGBD images from an Intel RealSense D435 camera and use\nAprilTag [21] to detect the 6D poses of task-relevant objects.\nOur system performs state estimation prior to each skill execu-\ntion, synchronizes the states to a simulated environment, and\nexecutes each skill generated by LEAGUE from the simulated\nenvironment through open-loop control.\nFig. 5 shows the key frames of three task execution pro-\ncesses and the corresponding task goals. Our system achieves\nan 8\/10 success rate for the StackThreeAtTarget task, and a\n6\/10 success rate for the StowObject task. The failure mode\nfor the StackThreeAtTarget task is that the AprilTags getting\noccluded from the camera in some initial configurations. The\nfailure mode for StowObject task is that sometimes the learned\npolicy is not able to generate a valid motion for operating the\ndrawer, and the objects slipping from the gripper.\nV. CONCLUSIONS, LIMITATIONS, AND FUTURE WORKS\nWe introduced LEAGUE, an integrated task planning and\nskill learning framework that represents a virtuous-cycle sys-\ntem: It leverages the high-level reasoning ability and abstrac-\ntion of a TAMP framework to facilitate the exploration and\ngeneralization of an RL skill learner, which in turn expands\nthe capability of the overall system. Through challenging\nmanipulation tasks in both simulation and the real world,\nwe demonstrated that LEAGUE is effective at solving long-\nhorizon tasks and generalizing to new tasks and domains.\nWhile empirically effective, our method does have a number\nof limitations. As we discussed in Sec. III-A, we assume\naccess to a library of skill operators that serve as the basis\nfor skill learning. Relatedly, our assumptions for skill-relevant\nstate abstraction, although effective, may not hold in certain\ncases (e.g. unintended consequences during exploration). A\npossible path to address both challenges is to learn skill\noperators with sparse transition models from unstructured\nexperiences [23, 25]. Second, our skill learning process re-\nlies on the environment-provided dense reward function. RL\nalgorithms that can better learn from sparse reward would\n8\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED AUGUST, 2023\nGoal: AND[OnTarget(cube2, target0), On(cube1, cube2), On(cube0, cube1)]\nGoal: AND[OnTarget(cube0, target0), On(cube1, cube0), On(cube2, cube1)]\nGoal: AND[InCabinet(hammer0, cabinet0), InCabinet(marker0, cabinet1), IsCabinetClosed(cabinet0), IsCabinetClosed(cabinet1)]\nFig. 5: Real robot demonstration. Key frames of three task execution processes (bottom) and their final task goals (top).\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEnv Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTask Progress\nMakeCoffee\nOurs - skill reusing\nOurs - from scratch\nFig. 6: Generalization to new domain. For MakeCoffee task,\nwe compare (a) learning the task from scratch and (b) learning\nby adapting the skills (Pick, Pull, and Push) learned from\nthe StowHammer domain.\nallow LEAGUE to build a tighter connection with the symbolic\nspace. Finally, in the real-world setting, LEAGUE is limited\nby the capability of the off-the-shelf perception algorithms. We\nplan to explore learning visuomotor control policies to make\nLEAGUE easier to deploy in the real world.\nREFERENCES\n[1] David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam,\nDoina Precup, and Michael Littman.\nValue preserving state-action\nabstractions. In AISTATS, 2020.\n[2] Pierre-Luc Bacon, Jean Harb, and Doina Precup.\nThe option-critic\narchitecture. In AAAI, 2017.\n[3] Rohan Chitnis, Tom Silver, Beomjoon Kim, Leslie Kaelbling, and\nTomas Lozano-Perez. Camps: Learning context-specific abstractions for\nefficient planning in factored mdps. In CoRL, 2021.\n[4] Murtaza Dalal, Deepak Pathak, and Russ R Salakhutdinov. Accelerat-\ning robotic reinforcement learning via parameterized action primitives.\nNeurIPS, 2021.\n[5] Scott Emmons, Ajay Jain, Misha Laskin, Thanard Kurutach, Pieter\nAbbeel, and Deepak Pathak.\nSparse graphical memory for robust\nplanning. NeurIPS, 2020.\n[6] Kuan Fang, Yuke Zhu, Silvio Savarese, and L Fei-Fei.\nAdaptive\nprocedural task generation for hard-exploration problems.\nIn ICLR,\n2020.\n[7] Maria Fox and Derek Long.\nPddl2. 1: An extension to pddl for\nexpressing temporal planning domains. JAIR, 2003.\n[8] Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim,\nTom Silver, Leslie Pack Kaelbling, and Tom´as Lozano-P´erez. Integrated\ntask and motion planning. Annu. Rev. Control Robot. Auton. Syst., 2021.\n[9] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep\nreinforcement learning for robotic manipulation with asynchronous off-\npolicy updates. In ICRA, 2017.\n[10] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning\nwith a stochastic actor. In ICML, 2018.\n[11] Bradley Hayes and Brian Scassellati.\nAutonomously constructing\nhierarchical task networks for planning and human-robot collaboration.\nIn ICRA, 2016.\n[12] Le´on Illanes, Xi Yan, Rodrigo Toro Icarte, and Sheila A McIlraith.\nSymbolic plans as high-level instructions for reinforcement learning.\nIn ICAPS, 2020.\n[13] Rico Jonschkowski and Oliver Brock.\nLearning state representations\nwith robotic priors. Auton. Robots, 2015.\n[14] Leslie Pack Kaelbling and Tom´as Lozano-P´erez. Hierarchical task and\nmotion planning in the now. In ICRA, 2011.\n[15] George Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez.\nFrom skills to symbols: Learning symbolic representations for abstract\nhigh-level planning. JAIR, 2018.\n[16] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-\nto-end training of deep visuomotor policies. JMLR, 2016.\n[17] Jacky Liang, Mohit Sharma, Alex LaGrassa, Shivam Vats, Saumya\nSaxena, and Oliver Kroemer. Search-based task planning with learned\nskill effect models for lifelong robotic manipulation. In ICRA, 2022.\n[18] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E\nTaylor, and Peter Stone. Curriculum learning for reinforcement learning\ndomains: A framework and survey. JMLR, 2020.\n[19] Soroush Nasiriany, Huihan Liu, and Yuke Zhu. Augmenting reinforce-\nment learning with behavior primitives for diverse manipulation tasks.\nIn ICRA, 2022.\n[20] Negin Nejati, Pat Langley, and Tolga Konik. Learning hierarchical task\nnetworks by observation. In ICML, 2006.\n[21] Edwin Olson. AprilTag: A robust and flexible visual fiducial system. In\nICRA, 2011.\n[22] Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, and\nChelsea Finn. Autonomous reinforcement learning via subgoal curricula.\nNeurIPS, 2021.\n[23] Tom Silver, Rohan Chitnis, Joshua Tenenbaum, Leslie Pack Kaelbling,\nand Tom´as Lozano-P´erez.\nLearning symbolic operators for task and\nmotion planning. In IROS, 2021.\n[24] Tom Silver, Ashay Athalye, Joshua B Tenenbaum, Tom´as Lozano-P´erez,\nand Leslie Pack Kaelbling. Learning neuro-symbolic skills for bilevel\nplanning. In CoRL, 2022.\n[25] Tom Silver, Rohan Chitnis, Nishanth Kumar, Willie McClinton, Tomas\nLozano-Perez, Leslie Pack Kaelbling, and Joshua Tenenbaum. Predicate\ninvention for bilevel planning. In AAAI, 2023.\n[26] Marc Toussaint. Logic-geometric programming: An optimization-based\napproach to combined task and motion planning. In IJCAI, 2015.\n[27] Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan,\nJos´ephine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao\nJiao, et al. Jump-start reinforcement learning. arXiv, 2022.\n[28] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas\nHeess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal\nnetworks for hierarchical reinforcement learning. In ICML, 2017.\n[29] Danfei Xu, Suraj Nair, Yuke Zhu, Julian Gao, Animesh Garg, Li Fei-Fei,\nand Silvio Savarese. Neural task programming: Learning to generalize\nacross hierarchical tasks. In ICRA, 2018.\n[30] Danfei Xu, Ajay Mandlekar, Roberto Mart´ın-Mart´ın, Yuke Zhu, Silvio\nSavarese, and Li Fei-Fei. Deep affordance foresight: Planning through\nwhat can be done in the future. In ICRA, 2021.\n[31] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart´ın-Mart´ın.\nrobosuite: A modular simulation framework and benchmark for robot\nlearning. In arXiv, 2020.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation.pdf"}
{"title":"Learning from Visual Observation via Offline Pretrained State-to-Go Transformer","authors":"Bohan Zhou, Ke Li, Jiechuan Jiang, Zongqing Lu","summary":"Learning from visual observation (LfVO), aiming at recovering policies from\nonly visual observation data, is promising yet a challenging problem. Existing\nLfVO approaches either only adopt inefficient online learning schemes or\nrequire additional task-specific information like goal states, making them not\nsuited for open-ended tasks. To address these issues, we propose a two-stage\nframework for learning from visual observation. In the first stage, we\nintroduce and pretrain State-to-Go (STG) Transformer offline to predict and\ndifferentiate latent transitions of demonstrations. Subsequently, in the second\nstage, the STG Transformer provides intrinsic rewards for downstream\nreinforcement learning tasks where an agent learns merely from intrinsic\nrewards. Empirical results on Atari and Minecraft show that our proposed method\noutperforms baselines and in some tasks even achieves performance comparable to\nthe policy learned from environmental rewards. These results shed light on the\npotential of utilizing video-only data to solve difficult visual reinforcement\nlearning tasks rather than relying on complete offline datasets containing\nstates, actions, and rewards. The project's website and code can be found at\nhttps:\/\/sites.google.com\/view\/stgtransformer.","url":"http:\/\/arxiv.org\/abs\/2306.12860v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2306.12860v1","published":1687439699000,"comment":"19 pages","pdf_text":"Learning from Visual Observation via Offline\nPretrained State-to-Go Transformer\nBohan Zhou12\nKe Li2\nJiechuan Jiang12\nZongqing Lu12†\n1PKU\n2BAAI\nAbstract\nLearning from visual observation (LfVO), aiming at recovering policies from\nonly visual observation data, is promising yet a challenging problem. Existing\nLfVO approaches either only adopt inefficient online learning schemes or require\nadditional task-specific information like goal states, making them not suited for\nopen-ended tasks. To address these issues, we propose a two-stage framework for\nlearning from visual observation. In the first stage, we introduce and pretrain State-\nto-Go (STG) Transformer offline to predict and differentiate latent transitions of\ndemonstrations. Subsequently, in the second stage, the STG Transformer provides\nintrinsic rewards for downstream reinforcement learning tasks where an agent\nlearns merely from intrinsic rewards. Empirical results on Atari and Minecraft show\nthat our proposed method outperforms baselines and in some tasks even achieves\nperformance comparable to the policy learned from environmental rewards. These\nresults shed light on the potential of utilizing video-only data to solve difficult\nvisual reinforcement learning tasks rather than relying on complete offline datasets\ncontaining states, actions, and rewards. The project’s website and code can be\nfound at https:\/\/sites.google.com\/view\/stgtransformer.\nSTG Transformer\nEncoder\nTemporally-aligned\nRepresentation Learning\nPolicy\nEncoder\nIntrinsic\nObserve\nTransfer\nPPO\nStage 1:Offline Pretraining\nStage 2:Online RL\nInput\nInput\nTransition \nDiscrimination\nFigure 1: A two-stage framework for learning from visual observation. The first stage involves three\nconcurrently pretrained components. A feature encoder is trained in a self-supervised manner to\nprovide easily predicted and temporally aligned representations for stacked-image states. State-to-Go\n(STG) Transformer is trained in an adversarial way to accurately predict transitions in latent space.\nA discriminator is updated simultaneously to distinguish state transitions of prediction from expert\ndemonstrations, which provides high-quality intrinsic rewards for downstream online reinforcement\nlearning in the next stage.\n†Correspondence to Zongqing Lu <zongqing.lu@pku.edu.cn>\nPreprint. Under review.\narXiv:2306.12860v1  [cs.LG]  22 Jun 2023\n1\nIntroduction\nReinforcement learning (RL) from scratch imposes significant challenges due to sample inefficiency\nand hard exploration in environments with sparse rewards. This has led to increased interest in\nimitation learning (IL). IL agents learn policies by imitating expert demonstrations in a data-driven\nmanner rather than through trial-and-error processes. It has been proven effective in various domains,\nincluding games [1] and robotics [2].\nHowever, acquiring demonstrated actions can be expensive or impractical, e.g., from videos that\nare largely available though, leading to the development of learning from observation (LfO) [3, 4,\n5, 6, 7, 8, 9]. This line of research utilizes observation-only data about agent behaviors and state\ntransitions for policy learning. Humans naturally learn from visual observation without requiring\nexplicit action guidance, such as beginners in video games improving their skills by watching skilled\nplayers’ recordings. However, LfO agents face challenges in extracting useful features from raw\nvisual observations and using them to train a policy due to the lack of explicit action information.\nThus, further study of learning from visual observation (LfVO) has the potential to grant agents\nhuman-like learning capabilities.\nIn this paper, we investigate a reinforcement learning setting in which agents learn from visual\nobservation to play challenging video games, such as Atari and Minecraft. Many existing LfO\napproaches [4, 5, 6, 7, 8] only apply to vector-observation environments, such as MuJoCo, while\nothers explicitly consider or can be applied to high-dimensional visual observations. Among them,\nrepresentation-learning methods [9, 10, 11] learn effective visual representations and recover an\nintrinsic reward function from them. However, most of these methods only excel in continuous\ncontrol tasks, exhibiting certain limitations when applied to video games as we show in experiments\nlater. Adversarial methods [3, 12, 13] learn an expert-agent observation discriminator online to\ndirectly indicate visual differences. However, noises or local changes in visual observations may\neasily cause misclassification [14]. In [12, 13], additional proprioceptive features (e.g., joint angles)\nare used to train a discriminator, which are unavailable in environments that only provide visual\nobservations. Moreover, as these methods require online training, sample efficiency is much lower\ncompared to offline learning. Goal-oriented methods, like [15], evaluate the proximity of each visual\nobservation to expert demonstrations or predefined goals. However, defining explicit goals is often\nimpractical in open-ended tasks [16]. Furthermore, the continuity of observation sequences in video\ngames cannot be guaranteed due to respawn settings or unanticipated events.\nTo address these limitations and hence enable RL agents to effectively learn from visual observation,\nwe propose a general two-stage framework that leverages visual observations of expert demonstrations\nto guide online RL. In the first stage, unlike existing online adversarial methods, we introduce and\npretrain State-to-Go (STG) Transformer, a variant of Decision Transformer (DT) [17], for offline\npredicting transitions in latent space. In the meanwhile, temporally-aligned and predictable visual\nrepresentations are learned. Together, a discriminator is trained to differentiate expert transitions,\ngenerating intrinsic rewards to guide downstream online RL training in the second stage. That said,\nin the second stage, agents learn merely from generated intrinsic rewards without environmental\nreward signals. Our empirical evaluation reveals significant improvements in both sample efficiency\nand overall performance across various video games, demonstrating the effectiveness of the proposed\nframework.\nOur main contributions are as follows:\n• We propose a general two-stage framework, providing a novel way to enable agents to\neffectively learn from visual observation. We introduce State-to-Go Transformer, which\nis pretrained offline merely on visual observations and then employed to guide online\nreinforcement learning without environmental rewards.\n• We simultaneously learn a discriminator and a temporal distance regressor for temporally-\naligned embeddings while predicting latent transitions. We demonstrate that the jointly\nlearned representations lead to enhanced performance in downstream RL tasks.\n• Through extensive experiments in Atari and Minecraft, we demonstrate that the proposed\nmethod substantially outperforms baselines and even achieves performance comparable to\nthe policies learned from environmental rewards in some games, underscoring the potential\nof leveraging offline video-only data for reinforcement learning.\n2\n2\nRelated Work\nLearning from Observation (LfO) is a more challenging setting than imitation learning (IL),\nin which an agent learns from a set of demonstrated observations to complete a task without the\nassistance of action or reward guidance. Many existing works [5, 18, 19, 20] attempt to train\nan inverse dynamic model to label observation-only demonstrations with expert actions, enabling\nbehavior cloning. However, these methods often suffer from compounding error [21]. On the other\nhand, [4] learns a latent policy and a latent forward model, but the latent actions can sometimes\nbe ambiguous and may not correspond accurately with real actions. GAIfO [3], inspired by [22],\nis an online adversarial framework that couples the process of learning from expert observations\nwith RL training. GAIfO learns a discriminator to evaluate the similarity between online-collected\nobservations and expert demonstrations. Although helpful in mitigating compounding error [3], it\nshows limited applicability in environments with high-dimensional observations. Follow-up methods\n[12, 13] pay more attention to visual-observation environments, but require vector-state in expert\nobservations to either learn a feasible policy or proper visual representations. More importantly,\nlearning a discriminator online is less sample-efficient, compared to LfO via offline pretraining. A\nrecent attempt [23] demonstrates some progress in action-free offline pretraining, but return-to-gos\nare indispensable in addition to observations because of upside down reinforcement learning (UDRL)\nframework [24]. Moreover, it only shows satisfactory results in vector-observation environments\nlike MuJoCo. In this work, we focus on reinforcement learning from offline pretraining on visual\nobservations, which is a more general and practical setting.\nVisual Representation Learning in RL. High-quality visual representations are crucial for LfVO.\nMany previous works [25, 26, 27, 9, 10, 11] have contributed to this in various ways. For example,\n[25] employs GANs to learn universal representations from different viewpoints, and [26, 27]\nlearn representations via contrastive learning to associate pairs of observations separated by a short\ntime difference. In terms of LfVO, a wide range of methods such as [9, 10, 11] learn temporally\ncontinuous representations in a self-supervised manner and utilize temporal distance to assess the\nprogress of demonstrations. They are easy to implement but are usually applied in robotic control\ntasks. Nevertheless, in games like Atari, adjacent image observations may exhibit abrupt or subtle\nchanges due to respawn settings or unanticipated events, not following a gradual change along the\ntimeline. Moreover, over-reliance on temporal information often results in over-optimistic estimates\nof task progress [15], potentially misleading RL training.\nTransformer in RL. Transformer [28] is widely acknowledged as a kind of powerful structure for\nsequence modeling, which has led to domination in a variety of offline RL tasks. Decision Transformer\n(DT) [17] and Trajectory Transformer (TT) [29] redefine the offline RL problem as a context-\nconditioned sequential problem to learn an offline policy directly, following the UDRL framework\n[24]. DT takes states, actions, and return-to-gos as inputs and autoregressively predicts actions to\nlearn a policy. TT predicts the complete sequence dimension by dimension and uses beam search\nfor planning. MGDT [30] samples from a learned return distribution to avoid manually selecting\nexpert-level returns as DT. ODT [31] extends DT to bridge the gap between offline pretraining and\nonline fine-tuning.\n3\nMethodology\n3.1\nPreliminaries\nReinforcement Learning. The RL problem can be formulated as a Markov decision process (MDP)\n[32], which can be represented by a tuple M =< S, A, P, R, γ, ρ0 >. S denotes the state space\nand A denotes the action space. P : S × A × S →[0, 1) is the state transition function and\nR : S × A →R is the reward function. γ ∈[0, 1] is the discount factor and ρ0 : S →[0, 1]\nrepresents the initial state distribution. The objective is to find a policy π(a|s) : S →A, which\nmaximizes the expected discounted return:\nJ(π) = Eρ0,at∼π(·|st),st∼P\n\" ∞\nX\nt=0\nγtr (st, at)\n#\n.\n(1)\nTransformer. Stacked self-attention layers with residual connections in Transformer is instrumental\nin processing long-range dependencies, each of which embeds n input tokens {xi}n\ni=1 and outputs\n3\nn embeddings {zi}n\ni=1 of the same dimensions considering the information of the whole sequence.\nIn this study, we utilize the GPT [33] architecture, an extension of the Transformer model, that\nincorporates a causal self-attention mask to facilitate autoregressive generation. Specifically, each\ninput token xi is mapped to a key ki, a query qi, and a value vi through linear transformations,\nwhere zi is obtained by computing the weighted sum of history values v1:i, with attention weights\ndetermined by the normalized dot product between the query qi and history keys k1:i:\nzi =\ni\nX\nj=1\nsoftmax({q⊺\ni , kj′}i\nj′=1)j · vj.\n(2)\nThe GPT model only attends to the previous tokens in the sequence during training and inference,\nthereby avoiding the leakage of future information, which is appropriate in state prediction.\nLearning from Observation. The goal is to learn a policy from an expert state sequence dataset\nDe = {τ 1, τ 2, . . . , τ m}, τ i = {si\n1, si\n2, . . . , si\nn}, si\nj ∈S. Denote the transition distribution as µ(s, s′).\nThe objective of LfO can be formulated as a distribution matching problem, finding a policy that\nminimizes the f-divergence between µπ(s, s′) induced by the agent and µe(s, s′) induced by the\nexpert [7]:\nJLfO (π) = Eτ i∼De,(s,s′)∼τ iDf [µπ (s, s′) ∥µe (s, s′)] .\n(3)\nIt is almost impossible to learn a policy directly from the state-only dataset De. However, our\ndelicately designed framework (see Figure 1) effectively captures transition features in expert demon-\nstrations to provide informative guidance for RL agents, which will be expounded in the following.\n3.2\nOffline Pretraining Framework\nemb. + pos. enc.\ndecoder\nCausal Self-Attention Module\n84\n84\n4\nFeature Encoder\nTDR Predictor\n……\nFigure 2: State-to-Go Transformer\nSTG Transformer is built upon GPT [33] similar\nto DT [17], but with a smaller scale and more struc-\ntural modifications to better handle state sequence\nprediction tasks. Unlike DT, in our setting, neither\nthe action nor the reward can be accessible, so the\nSTG Transformer primarily focuses on predicting the\nnext state embedding given a sequence of states.\nAs depicted in Figure 2, first we concatenate a few\nconsecutive image frames in the expert dataset to ap-\nproximate a single state st. Then, a sequence of n\nstates {st, . . . , st+n−1} are encoded into a sequence\nof n token embeddings {et, . . . , et+n−1} by the fea-\nture encoder Eξ composed of several CNN layers and\na single-layer MLP, where et = Eξ(st). A group of\nlearnable positional embedding parameters is added\nto the token embedding sequence to remember tem-\nporal order. These positional-encoded embeddings\nare then processed by the causal self-attention mod-\nule which excels in incorporating information about\nthe previous state sequence to better capture temporal\ndependencies, followed by layer normalization. The\nlinear decoder outputs the final latent prediction sequence {ˆet+1, . . . , ˆet+n}. Denote the positional\nencoding, transition predicting, and linear decoding model together as Tσ. It is worth noting that\ninstead of predicting the embeddings of the next state sequence directly, we predict the embedding\nchange and combine it with token embeddings in a residual way, which is commonly applied in\ntransition prediction [4] and trajectory forecasting [34] to improve prediction quality.\nFor simplicity, in further discussion we will refer to Tσ(et) directly as the predicted ˆet+1.\nExpert Transition Discrimination. Distinguishing expert transiting patterns is the key to leveraging\nthe power of offline expert datasets to improve sample efficiency in online RL. Traditional online\nadversarial methods [3, 12, 13] employ a discriminator to maximize the logarithm probability of\ntransitions sampled from expert datasets while minimizing that from transitions collected online,\nwhich is often sample-inefficient in practice. Moreover, in the case of visual observation, the\ntraditional discriminator may rapidly and strictly differentiate expert transitions from those collected\n4\nonline within a few updates. As a result, the collected observations will be assigned substantially low\nscores, which makes it challenging for policy improvement and results in poor performance.\nTo overcome these limitations, we draw inspiration from WGAN [35] and adopt a more generalized\ndistance metric, known as the Wasserstein distance, to measure the difference between the distributions\nof expert and online transitions. Compared to the sigmoid probability limited in [0, 1], the Wasserstein\ndistance provides a wider range and more meaningful measure of the difference between two\ntransition distributions, as it captures the underlying structure rather than simply computing the\nprobability. More importantly, unlike traditional online adversarial methods like GAIfO [3] that use\nthe Jensen-Shannon divergence or Kullback-Leibler divergence, the Wasserstein distance is more\nrobust to the issues of vanishing gradients and mode collapse, making offline pretraining possible.\nSpecifically, two temporally adjacent states st, st+1 are sampled from the expert dataset, then we\nhave et = Eξ (st) , et+1 = Eξ (st+1), and ˆet+1 = Tσ (Eξ (st)). The WGAN discriminator Dω aims\nto maximize the Wasserstein distance between the distribution of expert transition (et, et+1) and the\ndistribution of predicted transition (et, ˆet+1), while the generator tries to minimize it. The objective\ncan be formulated as:\nmin\nξ,σ max\nw∈W Eτ i∼De,(st,st+1)∼τ i [Dω (Eξ (st) , Eξ (st+1)) −Dω (Eξ (st) , Tσ (Eξ (st)))] .\n(4)\n{Dω}ω∈W represents a parameterized family of functions that are 1-Lipschitz, limiting the variation\nof the gradient. We clamp the weights to a fixed box (W = [−0.01, 0.01]l) after each gradient update\nto have parameters w lie in a compact space. Besides, to suppress the potential pattern collapse,\nan additional L2 norm penalizes errors in the predicted transitions, constraining all et and ˆet in a\nconsistent representation space. Thus, the loss functions can be rewritten as follows.\nFor discriminator:\nmin\nw∈W Ldis = Eτ i∼De,(st,st+1)∼τ i [Dω (Eξ (st) , Tσ (Eξ (st))) −Dω (Eξ (st) , Eξ (st+1))] .\n(5)\nFor STG Transformer (generator):\nmin\nξ,σ Ladv + Lmse = −Eτ i∼De,st∼τ iDω (Eξ (st) , Tσ (Eξ (st)))\n+ Eτ i∼De,(st,st+1)∼τ i∥Tσ (Eξ (st)) −Eξ (st+1) ∥2.\n(6)\nBy such an approach, the discriminator can distinguish between expert and non-expert transitions\nwithout collecting online negative samples, providing an offline way to generate intrinsic rewards for\ndownstream reinforcement learning tasks.\nTemporally-Aligned Representation Learning. Having a high-quality representation is crucial\nfor latent transition prediction. To ensure the embedding is temporally aligned, we devise a self-\nsupervised auxiliary module, named temporal distance regressor (TDR). Since the time span between\nany two states si and sj in a state sequence may vary significantly, inspired by [36], we define symlog\ntemporal distance between two embeddings ei = Eξ (si) and ej = Eξ (sj):\ntij = sign(j −i) ln(1 + |j −i|).\n(7)\nThis bi-symmetric logarithmic distance helps scale the value and accurately capture the fine-grained\ntemporal variation. The TDR module Pϕ consists of MLPs with 1D self-attention for symlog\nprediction. The objective of TDR is to simply minimize the MSE loss:\nmin\nξ,ϕ Ltdr = Eτ i∼De,(si,sj)∼τ i ∥Pϕ (Eξ (si) , Eξ (sj)) −tij∥2 .\n(8)\nOffline Pretraining. In our offline pretraining, the transition predictor Tσ and transition discriminator\nDω share the same feature encoder Eξ similar to online methods [37], which allows them to both\noperate in an easily-predictable and temporally-continuous representation space.\nAt each training step, a batch of transitions is randomly sampled from the expert dataset. The model is\ntrained autoregressively to predict the next state embedding without accessing any future information.\nWhen backpropagating, Lmse and Ladv concurrently update Eξ and Tσ to provide high-quality visual\nembeddings as well as accurate embedding prediction. Ltdr is responsible for updating the Eξ and\nPϕ as an auxiliary component, and Ldis updates Dω. Algorithm 1 in Appendix A details the offline\npretraining of the STG Transformer.\n5\n3.3\nOnline Reinforcement Learning\nIntrinsic Reward. For downstream RL tasks, our idea is to guide the agent to follow the pretrained\nSTG Transformer to match the expert state transition distribution. Unlike [15], our experimental\nresults show that our WGAN model is robust enough to offer a more discriminative assessment of\nstate transitions. That is, the WGAN discriminator can clearly distinguish between the state sequences\ncollected under the learning policy and the expert state sequences, without fine-tuning. Thus, we use\nthe discrimination score as the intrinsic reward for online RL. Moreover, we do not use ‘progress’\nlike what is done in [9]. This is because, in games with multiple restarts, progress signals can easily\nbe inaccurate and hence mislead policy improvement, while the WGAN discriminator mastering the\nprinciple of transitions can often make the correct judgment. The intrinsic reward at timestep t is\nconsequently defined as follows:\nri\nt = −\n\u0014\nDω\n\u0000Eξ (st) , Tσ (Eξ (st))\n\u0001\n−Dω\n\u0000Eξ (st) , Eξ (st+1)\n\u0001\u0015\n.\n(9)\nA larger ri\nt means a smaller gap between the current transition and the expert transition.\nOnline Learning Procedure. Given an image observation sequence collected by an agent, the feature\nencoder first generates corresponding visual representations, followed by the STG Transformer\npredicting the embeddings of the next state under expert transition. Then the discriminator compares\nthe difference between real transitions and predicted transitions. Their Wasserstein distances, as\nintrinsic rewards ri, is used to calculate generalized advantage, based on which the agent policy πθ\nis updated using PPO [38]. It is worth noting that the agent learns the policy merely from intrinsic\nrewards and environmental rewards are not used.\n4\nExperiments\nIn this section, we conduct a comprehensive evaluation of our proposed STG on diverse tasks from\ntwo environments: classical Atari environment and an open-ended Minecraft environment. Among\nthe three mainstream methods mentioned in Section 1, goal-oriented methods are not appropriate\nfor comparison because there is no pre-defined target state. Therefore, we choose GAIfO [3], a\nGAN-based method that learns an online discriminator for state transitions to provide probabilistic\nintrinsic reward signals, and ELE [9], a representation-learning method that pretrains an offline\nprogress model to provide monotonically increasing progression rewards, as our baselines. Through\nextensive experiments, we answer the following questions:\n• Is our proposed framework effective and efficient in visual environments?\n• Is our offline pretrained discriminator better than the one which is trained online?\n• Does TDR make a difference to visual representations? And do we need to add ‘progress’\nrewards, as is done in ELE?\nFor each task, we conduct 4 runs with different random seeds and report the mean and standard\ndeviation. To maintain consistency across all algorithms, the same network architecture, including\nthe feature encoder and discriminator, is applied for each algorithm. For GAIfO, similar to [37], the\ndiscriminator and policy network share the same visual encoder. For ELE, we use one-step transition\nfor progress prediction, which is aligned with our STG algorithm.\n4.1\nAtari\nAtari Expert Datasets. Atari is a well-established benchmark for visual control tasks and also a\npopular testbed for evaluating the performance of various LfVO algorithms. We conduct experiments\non four Atari games: Breakout, Freeway, Qbert, and Space Invaders. To ensure the quality of expert\ndatasets, two approaches are utilized to collect expert observations. For Qbert and SpaceInvaders, we\ncollect the last 105 transitions from Google Dopamine [39] DQN replay experiences. For Breakout\nand Freeway, we find that part of the transitions from Dopamine are not exactly expert transitions.\nTherefore, we alternatively train a SAC agent [40] from scratch for 5 × 106 steps and leverage the\ntrained policy to gather approximately 50 observation trajectories in each environment to construct\nthe expert dataset.\n6\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nELE\nGAIfO\n(a) Breakout\n0\n1\n2\n3\n4\n5\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nELE\nGAIfO\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nELE\nGAIfO\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n100\n200\n300\n400\n500\n600\nSTG\nELE\nGAIfO\n(d) Space Invaders\nFigure 3: The episodic return of STG and baselines in Atari games. Poor discrimination guidance\nmay account for GAIfO’s unsatisfactory performance. Over-optimistic progress information limits\nthe capability of ELE. Our STG combines the advantage of adversarial learning and the benefit of\nrepresentation learning, showing substantially better performance in four Atari games.\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nSTG\nELE\n(a) Pick a flower\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSTG\nELE\n(b) Milk a cow\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\nSTG\nELE\n(c) Harvest tallgrass\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.02\n0.04\n0.06\n0.08\nSTG\nELE\n(d) Gather wool\nFigure 4: Average success rates of STG and ELE in Minecraft tasks, where STG substantially out-\nperforms ELE, demonstrating its superiority over ELE in challenging tasks with partial observations.\nPerformance in Atari Games. As illustrated in Figure 3, STG outperforms the two baselines\nacross all four games. In Breakout, STG demonstrates a significant improvement both in the final\nperformance and sample efficiency compared to the baselines. This is attributed to its ability to\nincorporate expert skills into the learned policy. We observe that the agent successfully learns to\nobtain more intrinsic rewards by bouncing the ball up into the top gaps to hit the upper-level bricks\nwithin a limited number of update steps, while the other two methods fail. In Freeway, STG rapidly\nconverges, while the baselines suffer from severe fluctuations, accentuating the data efficiency and\nrobustness of STG. In Qbert and Space Invaders, our STG achieves a prominent breakthrough in the\nlater stages, substantially outperforming ELE and GAIfO.\nIn Table 1, we further show the expert-level performance by listing the average episodic returns\nof offline datasets and PPO learned from scratch with environmental rewards for comparison. The\nfinal scores of STG in Breakout and Qbert exceed expert performance, demonstrating its remarkable\npotential for both imitating expert observations and exploring better policies simultaneously.\nTable 1: Mean final scores of last 100 episodes on Atari games. The last two columns display the\naverage episodic scores of expert datasets and PPO with environmental rewards reported in [38].\nEnvironment\nGAIfO\nELE\nSTG\nExpert\nPPO\nBreakout\n1.5\n22.0\n288.8\n212.5\n274.8\nFreeway\n0.6\n2.7\n21.8\n31.9\n32.5\nQbert\n394.4\n4698.6\n27234.1\n15620.7\n14293.3\nSpace Invaders\n260.2\n384.6\n502.1\n1093.9\n942.5\nDuring the training process, we observe that GAIfO, primarily motivated by online discrimination,\ntends to get stuck in a suboptimal policy and struggles to explore a better policy. This is because the\ndiscriminator can easily distinguish between the visual behavior of the expert and the imitator based\non relatively insignificant factors within just a few online interactions. In contrast, STG learns better\ntemporally-aligned representations in an offline manner, enabling the discriminator to detect more\nsubstantial differences. Besides, instead of relying on probability, STG employs the Wasserstein\ndistance metric to provide more nuanced and extensive reward signals. Consequently, even without\nfine-tuning during the online RL process, STG can offer valuable guidance to the RL agent.\nAdditionally, from Figure 3a and 3b we find that ELE drops in final performance primarily due\nto the over-optimistic progress, which will be further investigated in Section 4.3. In comparison,\n7\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nSTG-\n(a) Breakout\n0\n2\n4\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nSTG-\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nSTG-\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n200\n400\nSTG\nSTG-\n(d) Space Invaders\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\nSTG\nSTG-\n(e) Pick a flower\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSTG\nSTG-\n(f) Milk a cow\n0\n250\n500\n750\n1000\nEpoch\n0.0\n0.1\n0.2\n0.3\nSTG\nSTG-\n(g) Harvest tallgrass\n0\n250\n500\n750\n1000\nEpoch\n0.00\n0.02\n0.04\n0.06\n0.08\nSTG\nSTG-\n(h) Gather wool\nFigure 5: Ablation studies on the TDR module in Atari and Minecraft tasks. The removal of the TDR\nloss from STG, denoted as STG-, induces a decline in performance and sample efficiency, revealing\nthe TDR module plays a vital role in STG.\nSTG ensures precise expert transition prediction and discriminative transition judgment, avoiding\nover-optimistically driving the agent to transfer to new states.\nIt is worth noting that, for each Atari task, we pretrain the STG Transformer using the corresponding\nindividual observation dataset. We also report the results of using multi-task datasets to pretrain the\nSTG Transformer for all Atari tasks in Appendix E.\n4.2\nMinecraft\nMinedojo [41], built upon one of the most popular video game Minecraft, provides a simulation plat-\nform with thousands of diverse open-ended tasks. In contrast to Atari games, the extensive behavioral\nrepertoire of the agent results in a considerably large observation space in a 3D viewpoint, making\nit exceedingly difficult to extract meaningful information from visual observations. Furthermore,\nopen-ended tasks necessitate the agent learns a diverse policy applicable to various objectives from a\nsmall observation dataset with a narrow expert policy distribution. Limited research has investigated\nthe efficiency of LfVO in such challenging environments. We evaluate STG on four Minecraft tasks,\nincluding “pick a flower”, “milk a cow”, “harvest tallgrass”, and “gather wool”, demonstrating its\napplicability and effectiveness in these complex settings. Among the four tasks, “gather wool” is\nthe most challenging, as it requires the agent to locate a randomly initialized sheep, shear it, and\nthen collect the wool on the ground. All four tasks are sparse-reward, where only a binary reward is\nemitted at the end of the episode, thus the performance is measured by success rates.\nMinecraft Expert Dataset. Recently, various algorithms, e.g., Plan4MC [16] and CLIP4MC [42]\nhave been proposed for Minecraft tasks. To create expert datasets, for each task, we utilize the learned\npolicies of these two algorithms to collect around 5 × 104 observations from expert trajectories.\nPerformance in Minecraft. The results on Atari show that GAIfO is inefficient in learning from\nvisual observation. Therefore, in Minecraft, we focus on the comparison between ELE and STG.\nAs depicted in Figure 4, the success rates across four Minecraft tasks reveal a consistent superiority\nof STG over ELE. Notably, in the \"milk a cow\" task, STG attains a success rate approaching 25%,\nsignificantly eclipsing the 5% success rate of ELE. The reasons for this stark contrast in performance\nare not yet entirely elucidated. However, a plausible conjecture could be attributed to the task’s\nprimary objective, i.e. locating the cow. Given STG’s adeptness in learning state transitions, it can\neffectively accomplish this subgoal. In contrast, ELE, due to its tendency for over-optimistic progress\nestimations, may lose the intended viewpoint with relative ease.\n4.3\nAblation\nTDR Ablation. We examine the role of the TDR module in enhancing performance and representation\nquality. An ablation, named STG-, is conducted by removing the TDR loss Ltdr from STG. Thus,\n8\nthe feature encoder Eξ and the STG Transformer Tσ are trained by a linear combination of Lmse and\nLadv. The results are shown in Figure 5, where STG is substantially superior to STG- in most tasks.\nSTG\nSTG-\nFigure 6: T-SNE visualization of embeddings of a sampled\ntrajectory in Qbert.\nIn order to figure out the underlying\nreasons for their discrepancy in per-\nformance, we compare the visualiza-\ntion of embeddings encoded by STG\nand STG-. We randomly select an ex-\npert trajectory from Qbert and utilize\nt-SNE projection to visualize their em-\nbedding sequences. As illustrated in\nFigure 6, the embeddings learned by\nSTG exhibit remarkable continuity, in\nstark contrast to the scattered and dis-\njoint embeddings produced by STG-.\nThe superior temporal alignment of\nthe STG representation plays a critical role in capturing latent transition patterns, thereby providing\ninstructive information for downstream RL tasks.\nProgression Reward. We conduct experiments to figure out whether it is necessary to additionally add\nprogression rewards derived from TDR, like what ELE does. We train the agent under the guidance\nof both the discriminative and progression rewards from the same pretrained STG Transformer in\nAtari tasks, denoted as STG*. As illustrated in Figure 7, STG outperforms STG* in all tasks. We\nanalyze that, similar to ELE, progression rewards from TDR over-optimistically urge the agent to\n\"keep moving\" to advance task progress, which however can negatively impact policy learning. For\nexample, on certain conditions such as Breakout or Freeway, maintaining a stationary position may\nfacilitate catching the ball or avoiding collision more easily, thereby yielding higher returns in the\nlong run. Therefore, we do not include the over-optimistic progression rewards in our design.\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG\nSTG*\n(a) Breakout\n0\n2\n4\nTransition\n×106\n0\n5\n10\n15\n20\nSTG\nSTG*\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG\nSTG*\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n200\n400\nSTG\nSTG*\n(d) Space Invaders\nFigure 7: Atari experiments comparing using discriminative rewards (STG) and using both discrimi-\nnative rewards and progression rewards (STG*).\nIn summary, our experimental results provide strong evidence for the ability of STG to learn from\nvisual observation, substantially outperforming baselines in a variety of tasks. The ablation study\nhighlights the importance of the TDR module for temporally aligned representations. However, TDR\nmay not be used to generate progression rewards that drive over-optimistic behaviors.\n5\nConclusion and Future Work\nIn this paper, we introduce the State-To-Go (STG) Transformer, offline pretrained to predict latent\nstate transitions in an adversarial way, for learning from visual observation to boost downstream\nreinforcement learning tasks. Our STG, tested across diverse Atari and Minecraft tasks, demonstrates\nsuperior robustness, sample efficiency, and performance compared to baseline approaches. We are\noptimistic that STG offers an effective solution in situations with plentiful video demonstrations,\nlimited environment interactions, and where labeling action is expensive or infeasible.\nIn future work, it would be worthwhile to combine our STG model with a more robust large-scale\nvision foundation model to facilitate generalization across a broader range of related tasks. Besides,\nour method can extend to a hierarchical framework where one-step predicted rewards can be employed\nfor training low-level policies and multi-step rewards for the high-level policy, which is expected to\nimprove performance and solve long-horizon tasks.\n9\nReferences\n[1] Jack Harmer, Linus Gisslén, Jorge del Val, Henrik Holst, Joakim Bergdahl, Tom Olsson,\nKristoffer Sjöö, and Magnus Nordin. Imitation learning with concurrent actions in 3d games.\nIn IEEE Conference on Computational Intelligence and Games (CIG), 2018.\n[2] Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran Tunyasuvu-\nnakool, János Kramár, Raia Hadsell, Nando de Freitas, et al. Reinforcement and imitation\nlearning for diverse visuomotor skills. arXiv preprint arXiv:1802.09564, 2018.\n[3] Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observa-\ntion. arXiv preprint arXiv:1807.06158, 2018.\n[4] Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. Imitating latent\npolicies from observation. In International Conference on Machine Learning (ICML), 2019.\n[5] Nathan Gavenski, Juarez Monteiro, Roger Granada, Felipe Meneguzzi, and Rodrigo C Barros.\nImitating unknown policies via exploration. arXiv preprint arXiv:2008.05660, 2020.\n[6] Rahul Kidambi, Jonathan Chang, and Wen Sun. Mobile: Model-based imitation learning from\nobservation alone. In Neural Information Processing Systems (NeurIPS), 2021.\n[7] Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Off-policy imitation learning from\nobservations. In Neural Information Processing Systems (NeurIPS), 2020.\n[8] Tanmay Gangwani, Yuan Zhou, and Jian Peng. Imitation learning from observations under\ntransition model disparity. In Neural Information Processing Systems (NeurIPS) Workshop on\nDeep Reinforcement Learning, 2021.\n[9] Jake Bruce, Ankit Anand, Bogdan Mazoure, and Rob Fergus. Learning about progress from\nexperts. In International Conference on Learning Representations (ICLR), 2023.\n[10] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey\nLevine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In\nIEEE International Conference on Robotics and Automation (ICRA), 2018.\n[11] Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando De Freitas.\nPlaying hard exploration games by watching youtube. In Neural Information Processing Systems\n(NeurIPS), 2018.\n[12] Faraz Torabi, Garrett Warnell, and Peter Stone. Imitation learning from video by leveraging\nproprioception. In International Joint Conference on Artificial Intelligence (IJCAI), 2019.\n[13] Haresh Karnan, Garrett Warnell, Faraz Torabi, and Peter Stone. Adversarial imitation learning\nfrom video using a state observer. In International Conference on Robotics and Automation\n(ICRA), 2022.\n[14] Minghuan Liu, Tairan He, Weinan Zhang, Shuicheng Yan, and Zhongwen Xu. Visual imitation\nlearning with patch rewards. arXiv preprint arXiv:2302.00965, 2023.\n[15] Youngwoon Lee, Andrew Szot, Shao-Hua Sun, and Joseph J Lim. Generalizable imitation\nlearning from observation via inferring goal proximity. In Neural Information Processing\nSystems (NeurIPS), 2021.\n[16] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\n[17] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. In Neural Information Processing Systems (NeurIPS), 2021.\n[18] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu,\nEvan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation.\nIn IEEE Computer Vision and Pattern Recognition (CVPR) Workshops, 2018.\n10\n[19] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In\nInternational Joint Conference on Artificial Intelligence (IJCAI), 2018.\n[20] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Neural Information Processing Systems (NeurIPS), 2022.\n[21] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In International Conference on Artificial\nIntelligence and Statistics (AISTATS), 2011.\n[22] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Neural Informa-\ntion Processing Systems (NeurIPS), 2016.\n[23] Deyao Zhu, Yuhui Wang, Jürgen Schmidhuber, and Mohamed Elhoseiny. Guiding online\nreinforcement learning with action-free offline pretraining. arXiv preprint arXiv:2301.12876,\n2023.\n[24] Juergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map\nthem to actions. In Neural Information Processing Systems (NeurIPS), 2019.\n[25] Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv\npreprint arXiv:1703.01703, 2017.\n[26] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised represen-\ntations for reinforcement learning. In International Conference on Machine Learning (ICML),\n2020.\n[27] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation\nlearning from reinforcement learning. In International Conference on Machine Learning\n(ICML), 2021.\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing\nSystems (NeurIPS), 2017.\n[29] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big\nsequence modeling problem. In Neural Information Processing Systems (NeurIPS), 2021.\n[30] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio\nGuadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game\ndecision transformers. In Neural Information Processing Systems (NeurIPS), 2022.\n[31] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In International\nConference on Machine Learning (ICML), 2022.\n[32] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[33] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018.\n[34] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++:\nDynamically-feasible trajectory forecasting with heterogeneous data. In European Conference\non Computer Vision (ECCV), 2020.\n[35] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial\nnetworks. In International Conference on Machine Learning (ICML), 2017.\n[36] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\n[37] Samuel Cohen, Brandon Amos, Marc Peter Deisenroth, Mikael Henaff, Eugene Vinitsky, and\nDenis Yarats. Imitation learning from pixel observations for continuous control. In Neural\nInformation Processing Systems (NeurIPS) Workshop on Deep Reinforcement Learning, 2021.\n11\n[38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[39] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on\noffline reinforcement learning. In International Conference on Machine Learning (ICML),\n2020.\n[40] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. In International\nconference on machine learning (ICML), 2018.\n[41] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended\nembodied agents with internet-scale knowledge. In Neural Information Processing Systems\n(NeurIPS) Datasets and Benchmarks Track, 2022.\n[42] Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang, and Zongqing Lu. Clip4mc: An\nrl-friendly vision-language model for minecraft. arXiv preprint arXiv:2303.10571, 2023.\n[43] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi\nParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based\nlocalization. In International Conference on Computer Vision (ICCV), 2017.\n[44] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In Interna-\ntional Conference on Machine Learning (ICML), 2018.\n12\nA\nAlgorithms\nWe present our algorithm sketches for STG Transformer offline pretraining and online reinforcement\nlearning with intrinsic rewards respectively.\nAlgorithm 1 STG Transformer Offline Pretraining\nInput: STG Transformer Tσ, feature encoder Eξ, discriminator Dω, expert dataset De\n=\n{τ 1, τ 2, . . . , τ m}, τ i = {si\n1, si\n2, . . . }, buffer B, loss weights α, β, κ .\n1: Initialize parametric network Eξ, Tσ, Dω randomly.\n2: for e ←0, 1, 2 . . . do\n▷epoch\n3:\nEmpty buffer B.\n4:\nfor b ←0, 1, 2 . . . |B| do\n▷batchsize\n5:\nStochastically sample state sequence τ i from De.\n6:\nStochastically sample timestep t and n adjacent states {si\nt, . . . , si\nt+n−1} from τ i.\n7:\nStore {si\nt, . . . , si\nt+n−1} in B.\n8:\nend for\n9:\nUpdate Dω: ω ←clip(ω −ϵ∇ωLdis, −0.01, 0.01).\n10:\nUpdate Eξ and Tσ concurrently by minimizing total loss αLmse + βLadv + κLtdr.\n11: end for\nAlgorithm 2 Online Reinforcement Learning with Intrinsic Rewards\nInput: pretrained Eξ, Tσ, Dω, policy πθ, MDP M, intrinsic coefficient η.\n1: Initialize parametric policy πθ with random θ randomly and reset M.\n2: while updating πθ do\n▷policy improvement\n3:\nExecute πθ and store the resulting n state transitions {(s, s′)}t+n\nt\n.\n4:\nUse Eξ to obtain n real latent transitions {(e, e′)}t+n\nt\n.\n5:\nUse Tσ to obtain n predicted latent transitions {(e, ˆe′)}t+n\nt\n.\n6:\nUse Dω to calculate intrinsic rewards: ∆t+n\nt\n= {Dω(e, ˆe′)}t+n\nt\n−{Dω(e, e′)}t+n\nt\n.\n7:\nPerform PPO update to improve πθ with respect to ri = −η∆.\n8: end while\nB\nEnvironment Details\nB.1\nAtari\nWe directly adopt the official default setting for Atari games.\nPlease refer to https:\/\/www.\ngymlibrary.dev\/environments\/atari for more details.\nB.2\nMinecraft\nEnvironment Settings\nTable 1 outlines how we set up and initialize the environment for each harvest task.\nTable 1: Environment Setup for Harvest Tasks\nHarvest Item\nInitialized Tool\nBiome\nmilk\nempty bucket\nplains\nwool\nshears\nplains\ntallgrass\nshears\nplains\nsunflower\ndiamond shovel\nsunflower plains\n13\n(a) Plains\n(b) Sunflower Plains\nFigure 1: Biomes in Minecraft\nBiomes. Our method is tested in two different biomes: plains and sunflower plains. Both the plains\nand sunflower plains offer a wider field of view. However, resources and targets are situated further\naway from the agent, which presents unique challenges. Figure 1a and 1b show the biomes of plains\nand sunflower plains respectively.\nObservation Space. Despite MineDojo offering an extensive observation space, encompassing RGB\nframes, equipment, inventory, life statistics, damage sources, and more, we exclusively rely on the\nRGB information as our observation input.\nAction Space. In Minecraft, the action space is an 8-dimensional multi-discrete space. Table 2 lists\nthe descriptions of action space in the MineDojo simulation platform. At each step, the agent chooses\none movement action (index 0 to 4) and one optional functional action (index 5) with corresponding\nparameters (index 6 and index 7).\nTable 2: Action Space of MineDojo Environment\nIndex\nDescriptions\nNum of Actions\n0\nForward and backward\n3\n1\nMove left and right\n3\n2\nJump, sneak, and sprint\n4\n3\nCamera delta pitch\n25\n4\nCamera delta yaw\n25\n5\nFunctional actions\n8\n6\nArgument for “craft”\n244\n7\nArgument for “equip”, “place”, and “destroy”\n36\nC\nOffline Pretraining Details\nHyperparameters. Table 3 outlines the hyperparameters for offline pretraining in the first stage.\nNetwork Structure. Different architectures for feature encoding are designed for different environ-\nments. In Atari, we stack four gray-scale images of shape (84,84) to form a 4-channel state and use\nthe feature encoder architecture as shown in Figure 2a. In Minecraft, a 3-channel image of shape\n(160,256,3) is directly regarded as a single state, which is processed by a feature encoder with more\nconvolutional layers and residual blocks to capture more complex features in the ever-changing\nMinecraft world. The detailed structure of the feature encoder for Minecraft is illustrated in Figure\n2b. All discriminators, taking in two 512-dimension embeddings from the feature encoder, follow the\nMLP structure of FC(1024,512)→FC(512,256)→FC(256,128)→FC(128,64)→FC(64,32)→FC(32,1)\nwith spectral normalization.\nRepresentation Visualization. We draw inspiration from Grad-CAM [43] to visualize the saliency\nmap of offline-pretrained feature encoder to assess the effectiveness and advantages of the repre-\nsentation of STG. Specifically, we compare the visualization results of STG and ELE in the Atari\nenvironment as illustrated in Figure 3. Each figure presents three rows corresponding to the features\ncaptured by the three layers of the convolutional layers, respectively. The saliency maps demonstrate\nthat STG exhibits a particular focus more on local entities and dynamic scenarios and effectively\nignores extraneous distractions. As a result, compared with ELE, STG shows greater proficiency in\n14\nTable 3: Hyperparameters for Offline Pretraining\nHyperparameter\nValue\nSTG optimizer\nAdamW\nDiscriminator optimizer\nRMSprop\nLR\n1e-4\nGPT block size\n128\nCSA layer\n3\nCSA head\n4\nEmbedding dimension\n512\nBatch size\n16\nMSE coefficient\n0.5\nAdversarial coefficient\n0.3\nTDR coefficient\n0.1\nWGAN clip range\n[-0.01,0.01]\nType of GPUs\nA100, or Nvidia RTX 4090 Ti\nStride:4\nConv:8×8\nRelu\n(1,4,84,84)\nStride:2\nConv:4×4\nRelu\n(1,32,20,20)\nStride:1\nConv:3×3\nRelu\n(1,64,9,9)\n(1,64,7,7)\n(1,512)\nFlatten\n&\nLinear\n(a) Atari\n(1,3,256,160)\nStride:4\nConv:8×8\nLeakyRelu\nBatchNorm\nResBlock\n(1,32,128,80)\nStride:2\nConv:3×3\nLeakyRelu\nBatchNorm\nResBlock\n(1,64, 64, 40)\nStride:2\nConv:3×3\nLeakyRelu\nBatchNorm\nResBlock\n(1,64,32,20)\nConv:3×3\nStride:2\nLeakyRelu\nBatchNorm\n(1,64,16,10)\nConv:3×3\nStride:2\nLeakyRelu\nBatchNorm\nResBlock\nResBlock\nConv:3×3\nStride:2\nLeakyRelu\nBatchNorm\n(1,64,8,5)\n(1,128,4,3)\nStride:2\nConv:3×3\nLeakyRelu\nBatchNorm\nResBlock\n(1,512)\nConv(in,out,3,1)\nConv(out,out,3,1)\nBatchNorm\nRelu\nBatchNorm\nConv(in,out,3,1)\nBatchNorm\nFlatten\n&\nLinear\n(b) Minecraft\nFigure 2: Feature encoder structure for Atari and Minecraft\nidentifying information strongly correlated with state transitions, thereby generating higher-quality\nrewards for downstream reinforcement learning tasks.\n15\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n(a) Saliency maps of different CNN layers in Breakout\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n(b) Saliency maps of different CNN layers in Freeway\n16\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n(c) Saliency maps of different CNN layers in Qbert\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n40\n80\n120\n160\n0\n30\n60\n90\n120\n150\n180\n210\n(d) Saliency maps of different CNN layers in Space Invaders\nFigure 3: Saliency maps (SM) of different CNN layers in Atari tasks. The first two columns\ndisplay the normalized saliency maps and corresponding observations of STG and the last two\ncolumns represent SM and corresponding observations of ELE. Through comparison, STG is better\nat capturing fine-grained features which are strongly correlated with transitions.\n17\nD\nRL Training Details\nThe general training hyperparameters of PPO for downstream RL tasks in the second stage are listed\nin Table 4.\nTable 4: General Hyperparameters for PPO\nHyperparameter\nValue\nOptimizer\nAdam\nLearning rate\n2.5e-4\nRL discount factor\n0.99\nNumber of workers (CPU)\n1\nParallel GPUs\n1\nType of GPUs\nA100, or Nvidia RTX 4090 Ti\nMinecraft image shape\n(160,256,3)\nAtari stacked state shape\n(84,84,4)\nClip ratio\n0.1\nPPO update frequency\n0.1\nEntropy coefficient\n0.01\nNeither the discriminative reward from STG nor the progression reward from ELE is bounded.\nTherefore, it is reasonable to adjust certain hyperparameters to bring out the best performance of each\nalgorithm in each task. In Table 5, the coefficient of intrinsic reward η(η > 0) for different baselines\nis tuned to balance the value scale and GAE λ(0 < λ < 1) is tuned to adjust the impact of intrinsic\nrewards in different tasks.\nTable 5: Specific Hyperparameters for Different Tasks\nTask\nηST G\nηELE\nηGAIfO\nλGAE\nBreakout\n0.6\n1.0\n2.0\n0.1\nFreeway\n2.0\n0.1\n1.0\n0.15\nQbert\n5.0\n0.05\n2.0\n0.95\nSpace Invaders\n6.0\n0.1\n2.0\n0.95\nMilk a Cow\n1.0\n0.5\n-\n0.8\nGather Wool\n10.0\n0.1\n-\n0.8\nHarvest Tallgrass\n1.0\n0.1\n-\n0.95\nPick a Flower\n1.0\n0.1\n-\n0.95\nThe coefficients of STG* (noted as ηri + νr∗) in four Atari tasks are reported in Table 6.\nTable 6: Coefficients for STG* in Atari Tasks\nTask\nη\nν\nBreakout\n0.6\n0.01\nFreeway\n2.0\n0.1\nQbert\n5.0\n0.03\nSpace Invaders\n6.0\n0.01\nTraining Details. For Minecraft tasks, we adopt a hybrid approach utilizing both PPO [38] and Self-\nImitation Learning (SIL) [44]. Specifically, we store trajectories with high intrinsic rewards in a buffer\nand alternate between PPO and SIL gradient steps during the training process. This approach allows\nus to leverage the unique strengths of both methods and achieve superior performance compared to\nutilizing either method alone [42].\n18\nE\nAdditional Experiments\nIntrinsic Reward Design. In Equation (9), we define our intrinsic reward ri as the difference\nbetween rguide and rbase:\nri\nt = Dω\n\u0000Eξ (st) , Eξ (st+1)\n\u0001\n−Dω\n\u0000Eξ (st) , Tσ (Eξ (st))\n\u0001\n= rguide\nt\n−rbase\nt\n.\n(10)\nOn the one hand, pretrained Dω clearly provides informative judgment rguide of transition quality\nduring online interaction. On the other hand, the baseline reward rbase, solely relying on the current\nstate st, serves as a baseline to normalize ri to a relatively lower level. In this section, we aim to\ninvestigate the necessity of incorporating rbase.\nTo assess the significance of rbase, we conducted experiments on the four Atari tasks utilizing only\nrguide as the intrinsic reward, which is similar to previous adversarial methods like GAIfO [3]. In\norder to bring the scale of rguide in line with ri, we employ running normalization and bound the\nvalues within the range of [−1, 1] to mitigate the negative influence of outliers. All other settings\nremain unchanged. We denote this ablation baseline as STG’.\nAs illustrated in Figure 4, rguide yields comparable final performance in Breakout and Space Invaders\nwhile failing to achieve satisfactory performance in Freeway and Qbert. In contrast, by leveraging\nrbase, which provides a unique reference from expert datasets for each individual st, we observe\nreduced variance and improved numerical stability compared to the running normalization trick that\ncalculates batch means for normalization.\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n1e7\n0\n100\n200\n300\nSTG'\nSTG\n(a) Breakout\n0\n2\n4\nTransition\n1e6\n0\n5\n10\n15\n20\nSTG'\nSTG\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n1e7\n0\n10000\n20000\n30000\nSTG'\nSTG\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n1e7\n200\n400\n600\nSTG'\nSTG\n(d) Space Invaders\nFigure 4: Atari experiments comparing using rguide (STG’) and ri (STG) as intrinsic reward.\nMulti-Task STG Transformer. We further assess the efficacy of multi-task adaptation of the STG\nTransformer. To this end, a new instance of the STG Transformer, with the same network architecture,\nis pretrained on all Atari training samples encompassing the four downstream Atari tasks. Considering\nthe four times increase in the size of the training dataset, we enlarge the size of the STG Transformer\nby increasing the number of heads (24) and layers (16) within the multi-head causal self-attention\nmodules, augmenting the model capacity for about four times. All training parameters remain the\nsame except for intrinsic coefficient η for each task (5 for Breakout, Qbert, and Space Invaders and\n10 for Freeway). The comparable performance across the four Atari tasks, as shown in Figure 5,\nreveals the potential of pretraining the STG Transformer on expansive multi-task datasets for guiding\ndownstream tasks.\n0.0\n0.5\n1.0\n1.5\n2.0\nTransition\n×107\n0\n100\n200\n300\nSTG-Multi\nSTG\n(a) Breakout\n0\n2\n4\nTransition\n×106\n0\n5\n10\n15\n20\nSTG-Multi\nSTG\n(b) Freeway\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n0\n10000\n20000\n30000\nSTG-Multi\nSTG\n(c) Qbert\n0.00\n0.25\n0.50\n0.75\n1.00\nTransition\n×107\n200\n400\n600\nSTG-Multi\nSTG\n(d) Space Invaders\nFigure 5: Atari performence under guidance of multi-task STG Transformer (STG-Multi) and\nsingle-task STG Transformer (STG).\n19\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Learning from Visual Observation via Offline Pretrained State-to-Go Transformer.pdf"}
{"title":"LLaMA Rider: Spurring Large Language Models to Explore the Open World","authors":"Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, Zongqing Lu","summary":"Recently, various studies have leveraged Large Language Models (LLMs) to help\ndecision-making and planning in environments, and try to align the LLMs'\nknowledge with the world conditions. Nonetheless, the capacity of LLMs to\ncontinuously acquire environmental knowledge and adapt in an open world remains\nuncertain. In this paper, we propose an approach to spur LLMs to explore the\nopen world, gather experiences, and learn to improve their task-solving\ncapabilities. In this approach, a multi-round feedback-revision mechanism is\nutilized to encourage LLMs to actively select appropriate revision actions\nguided by feedback information from the environment. This facilitates\nexploration and enhances the model's performance. Besides, we integrate\nsub-task relabeling to assist LLMs in maintaining consistency in sub-task\nplanning and help the model learn the combinatorial nature between tasks,\nenabling it to complete a wider range of tasks through training based on the\nacquired exploration experiences. By evaluation in Minecraft, an open-ended\nsandbox world, we demonstrate that our approach LLaMA-Rider enhances the\nefficiency of the LLM in exploring the environment, and effectively improves\nthe LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k\ninstances of collected data, showing minimal training costs compared to the\nbaseline using reinforcement learning.","url":"http:\/\/arxiv.org\/abs\/2310.08922v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.08922v1","published":1697183264000,"comment":"18 pages","pdf_text":"Preprint\nLLAMA RIDER:\nSPURRING LARGE LANGUAGE\nMODELS TO EXPLORE THE OPEN WORLD\nYicheng Feng1, Yuxuan Wang1, Jiazheng Liu1, Sipeng Zheng2, Zongqing Lu1,2†\n1 School of Computer Science, Peking University\n2 Beijing Academy of Artificial Intelligence\nfyc813@pku.edu.cn\nspzheng@baai.ac.cn\nzongqing.lu@pku.edu.cn\nABSTRACT\nRecently, various studies have leveraged Large Language Models (LLMs) to help\ndecision-making and planning in environments, and try to align the LLMs’ knowl-\nedge with the world conditions. Nonetheless, the capacity of LLMs to continu-\nously acquire environmental knowledge and adapt in an open world remains un-\ncertain. In this paper, we propose an approach to spur LLMs to explore the open\nworld, gather experiences, and learn to improve their task-solving capabilities.\nIn this approach, a multi-round feedback-revision mechanism is utilized to en-\ncourage LLMs to actively select appropriate revision actions guided by feedback\ninformation from the environment. This facilitates exploration and enhances the\nmodel’s performance. Besides, we integrate sub-task relabeling to assist LLMs in\nmaintaining consistency in sub-task planning and help the model learn the com-\nbinatorial nature between tasks, enabling it to complete a wider range of tasks\nthrough training based on the acquired exploration experiences. By evaluation\nin Minecraft, an open-ended sandbox world, we demonstrate that our approach\nLLaMA-Rider enhances the efficiency of the LLM in exploring the environment,\nand effectively improves the LLM’s ability to accomplish more tasks through fine-\ntuning with merely 1.3k instances of collected data, showing minimal training\ncosts compared to the baseline using reinforcement learning.\n1\nINTRODUCTION\nFigure 1. Spurring LLaMA to explore\nthe open world.\nRecently, significant advancements and successes have\nbeen achieved in the performance of Large Language\nModels (LLMs) in attaining human-like intelligence\n(OpenAI, 2023). Given the powerful capability of LLMs,\nmany research works have started utilizing their abilities\nto assist intelligent agents in decision-making in the envi-\nronments (Yao et al., 2023; Huang et al., 2022a; Li et al.,\n2022; Singh et al., 2023), and have found that LLMs pos-\nsess a certain level of abilities for planning and accom-\nplishing various tasks (Wang et al., 2023b). However, the\nknowledge that LLMs rely on comes from the language\ncorpus used during pre-training, and there may be dis-\ncrepancies between this knowledge and specific environ-\nments (Ahn et al., 2022).\nTo ground LLMs to environments, some studies design\nspecific mechanisms through prompt engineering to pro-\nvide information from environments for LLMs (Wang\net al., 2023c; Yao et al., 2023; Wu et al., 2023; Zhu et al.,\n2023; Liu et al., 2022). However, LLMs do not improve or acquire new knowledge in environments.\nAdditionally, for more complex tasks, more complicated mechanisms and prompts are required,\n†Corresponding author\n1\narXiv:2310.08922v1  [cs.LG]  13 Oct 2023\nPreprint\nwhich results in high costs of LLM generation and reliance on strong models like GPT-4 (OpenAI,\n2023) with enough knowledge (Wang et al., 2023a). Some other studies ground LLMs with finetun-\ning (Yao et al., 2022; Deng et al., 2023; Xiang et al., 2023), but they usually require task-dependent\ndatasets. Reinforcement Learning (RL) methods are also studied in the literature (Carta et al., 2023),\nbut these methods train LLMs as task-specific policies, and we found that RL methods are difficult\nto scale up to larger models or more complex tasks (see Section 5.2.2).\nIn this paper, we aim to enhance LLMs through their exploration in open-ended environments (Fig-\nure 1), like humans can adapt to new situations through practice. Previous studies have tried to\nupdate LLMs in embodied environments like BabyAI (Chevalier-Boisvert et al., 2019) and Virtu-\nalHome (Puig et al., 2018), but these world sizes are rather limited. Whether LLMs can improve\ntheir knowledge in more complicated open-ended worlds like Minecraft is still unknown (Fan et al.,\n2022; Guss et al., 2019). We think there are two major challenges here. First, in an environment\nlike Minecraft, tasks are often complex and may involve many sub-tasks. At the same time, these\nlong-horizon tasks often require each step to be carried out precisely, and a single error in the middle\nsometimes can negate previous progress. Besides, due to the high level of freedom, the action space\ncan be large, while many actions may be invalid in different states. These reasons make it hard\nto collect successful task trajectories in the environment using random exploration as in previous\nworks (Xiang et al., 2023; Li et al., 2022). The second challenge is that there can be a significant\namount of tasks in such an open world, so training policies for specific tasks are not applicable in\nthese environments. We hope that LLMs have the ability to perform multiple tasks and generalize\nto new tasks.\nIn response to these challenges, we propose LLaMA-Rider\n, a two-stage learning framework\nconsisting of an exploration stage and a learning stage (Figure 2). We investigate how to spur LLMs\nto explore the environment themselves and collect successful experiences for learning. Compared\nto random exploration or search methods that can hardly work in complex environments, allowing\nLLMs to explore on their own in the environment can harness the inherent capabilities of the models,\nthereby enabling more effective discovery of successful experiences. We propose a multi-round\nfeedback mechanism, which allows the LLM to revise its decisions by providing information about\nfailed actions in the environment. This feedback-revision exploration mechanism is more efficient\ndue to the capability of LLMs, as the draft decisions made are often related to task completion at\nfirst, and LLMs can effectively understand feedback information. Additionally, we use sub-task\nrelabeling to help LLMs maintain consistency in sub-task planning.\nIn the learning stage, we process the collected experiences into datasets and use supervised fine-\ntuning (SFT) to train the LLM. In addition to the experience gained from successful tasks, we also\ncollect experiences from partially completed sub-tasks, as some tasks are too difficult to accomplish\nin the environment in the exploration stage. Numerous tasks in open-ended environments often have\ncompositionality, which means experiences from past tasks can frequently assist in completing other\ntasks. We propose to use sub-task relabeling of the collected experiences to improve data utilization\nwhile helping LLMs learn the compositionality between tasks.\nWe evaluate our method in MineDojo (Fan et al., 2022), a simulation platform for Minecraft. We\nuse the basic skills trained by Plan4MC (Yuan et al., 2023) as the action space since the skills\npossess more semantics compared with primitive actions and are better aligned with LLMs. We\nuse LLaMA-2-70B-chat (Touvron et al., 2023) in our experiments. Our experiments show that\nLLaMA-Rider can explore the environment efficiently with our feedback-revision mechanism, and\ncan learn to complete tasks more effectively by finetuning on a collected dataset of only 1.3k in\nsize, demonstrating much higher sample efficiency compared to RL methods. We also show the\ngeneralization ability of LLaMA-Rider in novel hard tasks.\n2\nRELATED WORK\n2.1\nLLM-BASED AGENTS\nThere is a large body of recent studies on LLM-based agents, which have delved into the capacities\nof LLMs for decision-making and are well summarized in the survey papers (Wang et al., 2023b; Xi\net al., 2023). There are basically three ways to integrate LLMs into decision-making problems. First,\nusing the code generation capabilities of LLMs, LLMs take in information from the environment\n2\nPreprint\nand produce code that can interact directly within the environment (Liang et al., 2023; Singh et al.,\n2023). The second way is to employ LLMs for planning, following a concept similar to hierarchical\nRL (Ahn et al., 2022; Huang et al., 2022b; Wang et al., 2023c; Dasgupta et al., 2023). The third\napproach involves continually prompting LLMs or introducing memory modules to generate outputs\nthat can execute better strategies directly within a textual environment (Wei et al., 2022; Yao et al.,\n2023; Kim et al., 2023).\nMinecraft, as a popular and challenging open-world benchmark, has also attracted substantial at-\ntention for the studies of LLM-based agents. DEPS (Wang et al., 2023c) introduces the descriptor,\nexplainer, and selector for plan generation with the help of LLM. Plan4MC (Yuan et al., 2023) con-\nstructs a skill graph with the help of LLM and proposes a skill search algorithm for planning over\nthe basic skills pretrained by reinforcement learning (RL). Moreover, to build LLM-based agents in\nMinecraft, Voyager (Wang et al., 2023a) leverages the code generation of LLMs, while GITM (Zhu\net al., 2023) integrates LLMs with texted-based knowledge and memory.\nHowever, in the aforementioned studies, LLMs do not update themselves from their interactions\nwith the environment, so they can neither learn from nor adapt to the environment. Consequently,\ntheir potential applicability in specific environments is limited, as they can solely depend on the\nknowledge and capabilities gained during pre-training.\n2.2\nFINETUNING LANGUAGE MODELS IN ENVIRONMENTS\nThere are studies that ground Language Models (LMs) to environments with finetuning. PIGLeT\n(Zellers et al., 2021) integrates a neural symbolic dynamics model with an LM to learn natural lan-\nguage meaning grounded in physical interactions. Also focusing on the decision-making of LMs\nin embodied environments, LID (Li et al., 2022) uses expert trajectories to finetune a model that\nconcatenates an LM with action decoders. They also propose active data gathering to collect ex-\nperiences that mix random actions and policy-generated actions for exploration. Similarly, E2WM\n(Xiang et al., 2023) uses supervised learning to finetune LMs with the data collected by Monte Carlo\nTree Search and random exploration. Additionally, GLAM (Carta et al., 2023) ground LMs in en-\nvironments with online RL, but they train the LM into a task-specific policy, and the RL method\nsuffers from low sample efficiency and high cost of training. Our work is different from existing\nwork in that we spur the LLM itself to explore with feedback from the environment, and we target\nmulti-task and generalization abilities in the open world.\n3\nPRELIMINARIES\n3.1\nLARGE LANGUAGE MODELS\nLMs, which predict the probability of the ith token given inputs and the previously generated tokens\nPi = P(si|inputs, s1, s2, · · · , si−1), are used to generate a series of tokens by sampling from the\nprobability of the token sequences P(x) = Πn\ni=1Pi, where x can be considered as a random variable\nrepresenting n tokens in the token library. LLMs often have billions of weights and are trained from\nbillions of tokens to enable them to achieve remarkable performance on generative tasks.\nTo finetune LLMs with full parameters requires remarkable compute resources. Fortunately, some\ntechniques can help with efficient finetuning. Low-Rank Adaptation (LoRA) (Hu et al., 2022) in-\nvolves the process of keeping the pretrained model weights fixed while introducing trainable rank\ndecomposition matrices into every layer of LLMs. Original pretrained weights W0 ∈Rd×k are\naugmented to W0 + ∆W = W0 + BA, where B ∈Rd×r and A ∈Rr×k. The matrices A and B\nare both trainable, with A initialized to a normal distribution and B initialized to zero. Moreover,\nQLoRA (Dettmers et al., 2023) adds quantization and paged optimizers to further reduce training\ncompute costs. Quantization aims to transform input from a high-information representation into a\nlow-information representation, such as converting FP32 to int8 to reduce memory usage.\n3.2\nPROBLEM STATEMENT\nWe consider an environment that can be formalized as a Partially Observable Markov Decision\nProcess (POMDP) defined by tuple M = (S, O, A, T , R, γ), where S is the environment state,\n3\nPreprint\nTask Name: craft crafting table \nInventory: \n2 planks; 4 sticks\nSurround:    1 log nearby\nPast skills:\ncraft planks; craft stick;\n   find log nearby\nRequire:       4 planks\nneed 4 planks to craft table\n“get planks”\nFeedback\nRevision\n“find logs”\nneed 1 log to craft planks\nTask Name: craft wooden pickaxe \nInventory: 2 planks; 4 sticks\nSurround:   Nothing\nPast skills:\nharvest log; craft planks;\ncraft stick\nRequire:     3 planks, 2 sticks, 1 \ncrafting table nearby\nSubtask Relabeling\nTask Success\nplanks: need 1; sticks: satisfied\ncrafting table nearby: need 1\n“get crafting table“\nLLaMA-Rider\nMC\nMC\nLLaMA-Rider\nLLaMA-Rider\nAR\nAR\nAR\nt-1\nt\n…\n…\nExploration \nStage\nEnv Execution\nAction Retrieval\nMinecraft Feedback\nTask: craft wooden pickaxe \nInfo: …\nNext skill: find log nearby\nSupervised Dataset\nTask: craft  crafting table \nInfo: …\nNext skill: find log nearby\nSubtask\nRelabeling\nLearning \nStage\nCOT\ncraft crafting table\ncraft planks\nfind log nearby\nMC\nAR\nCOT Chain of Thought\n…\nfinetuning\nFigure 2. Overview of LLaMA-Rider\n. The framework consists of two stages. In the explo-\nration stage, the LLM explores to accomplish tasks with the help of the feedback-revision mech-\nanism and subtask relabeling. In the learning stage, the collected trajectories are formatted into a\nsupervised dataset to finetune the LLM.\nA is the action space, O is the observation space, T is the transition function, R is the reward\nfunction, and γ is the discount factor. Since we use LLMs as embodied agents, we assume a language\nvocabulary V and we can encode the observations and actions from the environment into natural\nlanguage. Besides, we assume a goal space G and we can sample a task τ = (g, K), g ∈G, where g\nis the goal of the task and K is the task information including task-relevant knowledge. We can also\nencode the task τ into task description τ text ∈VN.\nIn this study, we explore the Minecraft simulator provided by MineDojo (Fan et al., 2022), which\nis an open-ended sandbox world. There is rich information in the observation space, but a big\nportion of it cannot be comprehended by LLMs such as game visuals. We extract the items in\nthe agent’s inventory and field of view, along with their quantities, and encode them into natural\nlanguage sentences as the observations for LLMs: otext = (inv, fov) ∈VN. Primitive actions in\nthe environment (e.g., move forward, turn right, click) have insufficient semantics which hampers\nthe planning capability of LLMs. We use skill descriptions as the action space of the LLM agent\nnoted with atext ∈VN.\n3.3\nSKILLS AND TASKS IN PLAN4MC\nWe use the basic skills and tasks in Plan4MC (Yuan et al., 2023) in our experiments in MineDojo,\nsince the basic skills have more semantic meaning than primitive actions. Plan4MC uses RL to train\nthree types of basic skills: finding-skills, manipulation-skills, and crafting-skills. They then define\n40 difficult tasks that can be completed with the trained skills. We define the action space of the\nLLM agent Atext as the descriptions of these basic skills.\n4\nMETHODOLOGY\nOur method is illustrated in Figure 2, which is a two-stage framework. We introduce the exploration\nstage and the learning stage respectively in the following.\n4.1\nEXPLORATION WITH FEEDBACK\nPrompt mechanism. Unlike previous studies such as Voyager (Wang et al., 2023a) and GITM (Zhu\net al., 2023) which use complex prompts to tweak LLMs to accomplish various tasks in open-ended\nworlds like Minecraft, our approach employs a straightforward prompt that makes LLMs provide\nthe next action given input information about observation and task. This brings two advantages.\nFirst, it makes finetuning LLMs to learn from past experiences easy, considering the context-length\nlimit of LLMs. Second, it reduces the cost of LLM generation.\nFormally, the LLM serves as the policy π(atext\nt\n|otext\nt\n, τ text, ht). We provide the textual observation\notext, the task description τ text and the history information h in the input prompt to feed the LLM\nat each time step t, and the output of the LLM is the chosen action atext. We find that if there are too\n4\nPreprint\nAlgorithm 1. Feedback-revision\nRequire: otext\nt\n, τ text, ht, πLLM, E, T\nEnsure: atext\nt\n1: atext\nt\n∼πLLM(·|otext\nt\n, τ text, ht)\n2: ft = E(st, at)\n3: for i = 0 to T do\n4:\nif ft = 0 then\n5:\nreturn atext\nt\n6:\nend if\n7:\nft →f text\nt\n8:\natext\nt\n∼πLLM(·|otext\nt\n, τ text, ht, f text\nt\n)\n9:\nft = E(st, at)\n10: end for\n11: if ft = 0 then\n12:\nreturn atext\nt\n13: end if\n14: return 0\nmany tokens of history information in the prompt, it will affect the output of the LLM. Therefore, in\nour experiments, we set h to be the last three actions performed ht = (atext\nt−3 , atext\nt−2 , atext\nt−1 ).\nFeedback-revision mechanism. LLMs possess rich knowledge of the real world, but there is often\na gap between the knowledge of LLMs and the specific environment to which they are applied.\nFor example, which actions can be performed in the environment? What are the prerequisites for\neach action before execution? What conditions need to be satisfied for the completion of different\ntasks in the environment? What are the names of various items in the environment? LLMs often\nlack understanding of these questions, leading to decision-making errors. Previous studies ground\nLLMs to environments by searching through the action space (Xiang et al., 2023) or mix policy with\nrandom actions (Li et al., 2022) to collect experiences, or train LLMs with reinforcement learning\n(Carta et al., 2023). But these methods can hardly scale up to worlds with long-horizon tasks. They\nall do not provide environmental knowledge to LLMs but make LLMs explore through trial and\nerror. We propose to spur LLMs to explore the world themselves with their reasoning capabilities by\nfeeding them environmental feedback information and letting LLMs revise their decisions. LLMs\ncan access environmental knowledge during this process, and the method makes use of LLMs’\ninherent ability to enhance the efficiency of exploration.\nFormally, after the LLM produces an action atext\nt\n∼π(·|otext\nt\n, τ text, ht), a feedback information\nis generated by the environment ft = E(st, at), where E denotes the environment, st denotes the\nstate, and at denotes the primitive actions corresponding to atext\nt\n. If ft ̸= 0, which means the\naction causes an error, the feedback is processed by a prompt into f text\nt\nand fed back to the LLM\ntogether with the previous input information, and the LLM would make a revision to produce a\nnew action atext′\nt\n∼π(·|otext\nt\n, τ text, ht, f text\nt\n). Then a new feedback is generated ft = E(st, a′\nt).\nThis feedback-revision procedure can be repeated until ft = 0 or the maximum number of allowed\nrevisions T has been reached which means the exploration has failed and the episode ends. The\nformalized approach of the feedback-revision mechanism can be seen in Algorithm 1.\nSubtask relabeling. Long-horizon tasks in an open world are often composed of many subtasks.\nSince our input prompt is brief, limited information is provided. So the LLM planner may forget\nwhat subtask it is currently working on and opt to start completing other subtasks, resulting in failure\nto consistently complete one subtask. To solve this problem, whenever the LLM’s output skill is\naccomplishing a subtask τs of the task τ, we replace the task information τ text in the input prompt\nwith τ text\ns\nand keep it until τs is completed. This subtask relabeling provides another important\nbenefit: some subtasks may have been met in the collected experiences as a simpler task or as a\nsubtask of another task, so this method helps LLMs make use of previously learned experiences to\nsolve new tasks.\nAction retrieval. To match the output of the LLM with the action space, there are two major ways:\nfeed the action list to the LLM or retrieve the action list based on the output. We find that feeding\na lengthy list of actions as input to the LLM would affect its output to generate more unreasonable\nactions unrelated to the current task. Therefore, we use action retrieval to select an action from\n5\nPreprint\nthe action space that is closest to the output of the LLM. Additionally, we find that querying with\ntoken embeddings could cause retrieval errors since the action description often consists of only a\nfew words, e.g., “craft wooden planks” may be matched to “craft wooden sword” instead of “craft\nplanks”. We propose to use noun matching before embedding matching to alleviate this problem.\nDetails of action retrieval can be found in Appendix C.\nChain-of-thought (CoT) prompting. In our experiments in Minecraft, we find that the LLM often\nmakes decision mistakes due to insensitivity to the relationships between numbers. To enhance\nthe efficiency of exploration, we integrate in-context learning and chain-of-thought prompting (Wei\net al., 2022) that make the LLM compare the item numbers in the inventory and the requirements\nbefore making decisions. The prompt can be seen in Appendix B.3, and we only use it in the\nexploration stage for Minecraft.\n4.2\nFINETUNING LLMS WITH EXPERIENCES\nDataset construction. We compile task experiences of all tasks collected by the LLM from the\nenvironment into a supervised dataset, with the input be the task information and the observation\nx = (otext\nt\n, τ text, ht), and the label be the action y = atext\nt\n. In addition to success trajectories,\nwe also include partial trajectories where a subtask is completed, since some tasks are too hard to\naccomplish during exploration, and the subtask experience may help the LLM to accomplish the\nwhole task more easily. Besides, subtask experiences may also help the LLM solve some other\ntasks due to the compositionality. To better make use of the subtask information and encourage\ncombinatorial generalization, we also use subtask relabeling to construct the dataset. Namely, if\nthe LLM is solving a subtask τs of task τ at time step t in a trajectory, we add the data (x =\n(otext\nt\n, τ text\ns\n, ht), y = atext\nt\n) into the dataset.\nTraining. With the dataset including experiences of various tasks in the environment, we train the\nLLM with supervised finetuning (SFT). We use QLoRA (Dettmers et al., 2023) to reduce memory\nusage, and more details can be found in Appendix A.\n5\nEXPERIMENTS\n5.1\nEXPERIMENTAL SETUP\nMineDojo environment. We evaluate our proposed method on Minecraft based on the MineDojo\n(Fan et al., 2022) simulator. We use 30 difficult tasks in Plan4MC (Yuan et al., 2023) including\nthree types: 10 log-based\ntasks, 10 cobblestone-based\ntasks, and 10 mob-based\ntasks.\nThe minimum number of planning steps provided by Plan4MC required for these tasks ranges from\n2 to 30, with an average minimum of 11.5 steps. More details about the tasks can be found in\nAppendix D. We use 55 basic skills trained by Plan4MC and convert them to skill descriptions in\nnatural language as the action space of the LLM. Note that the skill policies do not guarantee success,\nand the success rates of all the skills are provided in Appendix D. For each task τ = (g, K), the\ngoal g is the target item of the task and the knowledge K is the requirement to achieve target g in\nMineDojo. The feedback information ft from the environment is the requirements that are not met\nto execute skill at in MineDojo. The prompt template for the LLM’s input and the feedback can be\nfound in Appendix B.\nWe define the subtasks of a task τ as the tasks τs = (gs, Ks) whose goal gs is one of the requirements\nto achieve task τ. For example, the task “craft bowl” has two subtasks “craft planks” and “place\ncrafting table nearby”. Note that some subtasks are simple so are not among the 30 difficult tasks\nfor evaluation.\nLLM agent. We use LLaMA-2-70B-chat (Touvron et al., 2023) as our LLM agent, which was\nrecently released and has strong question-answering and instruction-following abilities. These abil-\nities are important for the LLM to actively explore in the environment, and conversely, our method\ncan also effectively make good use of its strong abilities to do something beyond question answering,\nnamely exploring new environments.\nBaselines. We compare with three baselines in our experiments. The first is ChatGPT planner\n(Ouyang et al., 2022), the interactive LLM baseline in Plan4MC, which uses a carefully designed\n6\nPreprint\nTable 1. Success rates in all tasks. LLaMA-Rider Exploration is tested for 5 episodes in log-based\ntasks and 10 episodes in other tasks. All other methods are tested for 30 episodes. Results for\nChatGPT planner and Plan4MC are from the report of Plan4MC (Yuan et al., 2023). LLaMA-\nRider Base is LLaMA-Rider before finetuning. The bold results are the best among LLaMA-\nRider, LLaMA-Rider Base, ChatGPT planner and RL. We do not compare with LLaMA-Rider\nExploration due to the different test episode numbers.\nTask\nLLaMA-Rider\nExploration\nLLaMA-Rider\nBase\nChatGPT\nplanner\nRL\nLLaMA-Rider\n(ours)\nPlan4MC\n0.90\n0.23\n0.30\n0.00\n0.43\n0.30\n1.00\n0.37\n0.17\n0.00\n0.67\n0.30\n0.80\n0.73\n0.07\n0.00\n0.97\n0.47\n0.60\n0.67\n0.00\n0.00\n0.77\n0.23\n0.60\n0.57\n0.03\n0.00\n0.57\n0.37\n0.00\n0.67\n0.00\n0.00\n0.60\n0.43\n0.80\n0.0\n0.20\n0.00\n0.37\n0.53\n0.60\n0.77\n0.47\n0.00\n0.60\n0.37\n0.80\n0.07\n0.63\n0.00\n0.10\n0.47\n0.00\n0.03\n0.73\n0.00\n0.27\n0.70\n0.40\n0.00\n0.00\n-\n0.17\n0.37\n0.10\n0.00\n0.20\n-\n0.57\n0.47\n0.10\n0.00\n0.03\n-\n0.40\n0.53\n0.20\n0.00\n0.13\n-\n0.10\n0.57\n0.00\n0.00\n0.00\n-\n0.00\n0.37\n0.00\n0.13\n0.00\n-\n0.07\n0.10\n0.00\n0.00\n0.00\n-\n0.03\n0.17\n0.00\n0.00\n0.07\n-\n0.03\n0.07\n0.00\n0.00\n0.13\n-\n0.00\n0.10\n0.10\n0.00\n0.10\n-\n0.07\n0.20\n0.70\n0.60\n0.57\n-\n0.60\n0.83\n0.30\n0.50\n0.76\n-\n0.57\n0.53\n0.00\n0.10\n0.00\n-\n0.03\n0.17\n0.00\n0.10\n0.00\n-\n0.07\n0.13\n0.30\n0.50\n0.37\n-\n0.43\n0.37\n0.00\n0.00\n0.00\n-\n0.00\n0.07\n0.00\n0.03\n0.43\n-\n0.03\n0.43\n0.00\n0.00\n0.03\n-\n0.00\n0.20\n0.00\n0.00\n0.30\n-\n0.03\n0.33\n0.00\n0.00\n0.00\n-\n0.00\n0.13\nbased\n0.61\n0.41\n0.26\n0.00\n0.54\n0.42\nbased\n0.09\n0.01\n0.07\n-\n0.14\n0.30\nbased\n0.13\n0.18\n0.25\n-\n0.18\n0.32\nTotal average\n0.28\n0.20\n0.19\n-\n0.29\n0.34\nAchieved tasks #\n16\n16\n20\n-\n25\n30\nprompt mechanism to make ChatGPT (GPT-3.5) propose skill plans. This baseline also uses the\nLLM to choose skills trained in Plan4MC for accomplishing tasks in Minecraft. Since ChatGPT\npossesses more accurate knowledge about Minecraft than LLaMA-2-70B-chat, by comparing with\nthis baseline, we show whether our exploration-learning framework can enable an LLM to adapt to\na new environment and outperform a stronger language model. The second is RL where we use the\ntraining framework proposed in GLAM (Carta et al., 2023) and use their default language model\nT5 (Chung et al., 2022). We try our best to fit GLAM into the Minedojo environment but we have\nto constrain the action space to include only the necessary actions to reduce sample complexity.\nThe detailed implementation is described in the Appendix E. The third is Plan4MC, where they\nconstruct a skill graph and use depth-first search (DFS) for planning over basic skills. This baseline\n7\nPreprint\nensures that the planning is correct. Thus, it can be seen as an upper bound of our method. However,\nwe note that our method may outperform Plan4MC in some tasks. We speculate this is because\nPlan4MC does not always generate the optimal plan in terms of planning steps, though the plan is\ncorrect.\n5.2\nEVALUATION\nWe set the maximum number of revisions as T = 5 for which we find can best balance the efficiency\nand success rate of the LLM’s exploration for all tasks. Since the log-based\ntasks are easier, we\nonly perform 5 episodes of exploration, where we make the LLaMA-Rider explore for 10 episodes\nfor the rest 20 tasks, so that the experience collected from different tasks be in similar quantities.\nFor the task “craft stick\n” and “place crafting table\nnearby”, we change the biome to forest in\nthe exploration stage to improve the chance of finding logs\n. The results are shown in Table 1.\n5.2.1\nEXPLORATION OF LLAMA-RIDER IN MINECRAFT\nLLaMA-Rider Exploration shows the LLM’s ability to explore in Minecraft to accomplish differ-\nent tasks with our designed prompt combined with the feedback-revision mechanism. Compared\nwith ChatGPT planner which is based on a powerful LLM with more Minecraft knowledge (see\nAppendix F), LLaMA-Rider Exploration can obtain successful experiences more effectively with-\nout finetuning in log-based\ntasks and has comparable performance in the other tasks. This can\nbe attributed to our feedback-revision mechanism, which provides more environment information\nfor the LLM to acquire knowledge alignment, and the CoT prompt that mitigates the LLM’s nu-\nmerical comparison issue. Besides, the success rates in stone-based\ntasks and mob-based\ntasks demonstrate that it is difficult for LLMs to solve long-horizon complex tasks in environments\njust rely on prompt engineering, reflecting the importance for LLMs to update with environmental\nexperiences to adapt.\n5.2.2\nENHANCING LLM WITH ENVIRONMENTAL EXPERIENCES\nPerformance in explored tasks. We collect trajectories that the LLM achieves success in the whole\ntasks or subtasks and process them into a supervised dataset of 1.3k instances as described in Sec-\ntion 4.2. We train LLaMA-2-70B-chat on the dataset for two epochs, and then test the resulting\nmodel LLaMA-Rider on 30 tasks without CoT prompting. From the results in Table 1, the trained\nLLaMA-Rider outperforms the base model on various tasks, so the learning stage is effective. Be-\nsides, LLaMA-Rider outperforms ChatGPT planner in 17 out of 30 tasks, demonstrating that our\nexploration-learning framework allows an LLM to quickly adapt to a new environment and surpass\na more advanced LLM, even with a simple prompt mechanism.\nCompared with the performance in the exploration stage, LLaMA-Rider can accomplish more tasks\n(25 vs. 16) after training, proving that the model can learn the knowledge from the experiences ef-\nfectively and generalize well, while also reflecting the necessity of allowing LLMs to update them-\nselves in the environment. Without the help of CoT prompting at test time, LLaMA-Rider can\nstill perform better, which reflects that the model acquires stronger decision-making abilities. The\nphenomenon that LLaMA-Rider can achieve success in tasks without successful experiences in the\ndataset like “craft sign\n” and “craft wooden shovel\n” proves that the model is not memoriz-\ning experiences but learning more knowledge for planning. Besides, as we show in Appendix F,\nLLaMA-Rider can also answer task-relevant questions better, so the model is indeed aligning with\nthe environment. The generalization ability is probably also due to our subtask relabeling method\nwhich helps LLaMA-Rider learn compositionality among different tasks. Besides, compared with\nPlan4MC, our method can achieve comparable performance in several tasks and even better perfor-\nmance in relatively simpler log-based\ntasks, showing that LLaMA-Rider already demonstrates\nstrong abilities in planning and decision-making.\nOn the other hand, RL, which also finetunes the LLM in the environment, fails in all log-based\ntasks. Thus, we do not conduct experiments in the rest tasks to save resources. We find that the LLM\nstruggles to explore the world with trial and error in long-horizon tasks with a large action space. In\naddition to small models like T5-base, which we think may have limited decision-making abilities\nin the complex environment, we have also tried to train LLaMA-2-70B-chat with reinforcement\nlearning, but we found the training unaffordable. So the RL method is difficult to scale up. In\n8\nPreprint\ncontrast, our method only requires the LLM to explore for 5 or 10 episodes in the environment and\ntrains the LLM on a small dataset with just 1.3k instances, showing significantly lower cost and\nhigher sample efficiency.\nOverall, we conclude that our method LLaMA-Rider adapts to the environment efficiently and\neffectively and shows good multi-task ability in the open-world Minecraft.\nTable 2. Success rates in novel iron-based tasks. Methods are tested for 30 episodes. LLaMA-\nRider Base is LLaMA-Rider before finetuning.\nTasks\nLLaMA-Rider Base\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nLLaMA-Rider (ours)\n0.13\n0.00\n0.00\n0.00\n0.00\n0.00\n0.07\n0.03\n0.00\n0.00\nGeneralization to novel hard tasks. Since LLaMA-Rider can complete tasks without successful\nexperiences at training time, we also test its performance on novel tasks that it has not explored and\nnot been trained on. We conduct the experiment on 10 iron-based\ntasks, which are more difficult\nthan the previous 30 tasks with the planning steps of Plan4MC ranging from 30 to 121, 68.9 on\naverage. The results are shown in Table 2.\nWe find that LLaMA-Rider has very poor performance before training. But after finetuned with the\nexperiences in the previous 30 tasks, LLaMA-Rider can now achieve 3 of them. This shows that\nthe LLM can learn to make use of past experiences to solve novel tasks that have not been explored,\nwhich demonstrates the generalization of the planning ability learned by our method. Addition-\nally, since the experiences can help LLaMA-Rider solve more complex tasks, it is promising that\nLLaMA-Rider can repeat the exploration and learning procedure and explore for more challenging\ntasks continuously in the open world.\n5.2.3\nABLATION STUDY\nWe first test the LLaMA-Rider’s performance in the exploration stage without CoT prompting and\nfeedback-revision mechanism in the 30 tasks. We find that LLaMA-Rider can only achieve success\nin “craft stick\n” with a success rate of 0.5 and fails in all other tasks (thus omitted in Table 1).\nThis proves that our feedback-revision mechanism and the CoT prompting contribute a lot to the\nexploration performance. Without feedback information that carries environmental knowledge, the\nLLM can hardly align with the world.\nTable 3. Success rates in stone-based tasks. Methods are tested for 30 episodes. LLaMA-Rider\nw\/o subtask is the method without subtask relabeling at training and testing time.\nTasks\nLLaMA-Rider\nw\/o subtask\n0.00\n0.00\n0.00\n0.00\n0.00\n0.30\n0.00\n0.03\n0.03\n0.07\nLLaMA-Rider (ours)\n0.17\n0.57\n0.40\n0.10\n0.00\n0.07\n0.03\n0.03\n0.00\n0.07\nThen we study the contribution of our subtask relabeling. We train LLaMA-2-70B-chat with a\ndataset without the subtask relabeled data. At test time we also do not use subtask relabeling. We\ntest on 10 stone-based\ntasks, since these tasks are more long-horizon and contain more subtasks.\nThe results are shown in Table 3. The model performs poor in the long-horizon stone-based\ntasks\nwithout subtask relabeling method, while LLaMA-Rider can achieve even more tasks than those\nin training experiences, proving that subtask relabeling is important for both the achievement (and\nthus the exploration) of tasks and the generalization ability to new tasks.\n6\nCONCLUSION AND LIMITATIONS\nIn this paper, we introduce LLaMA-Rider, which is a learning framework that spurs the LLM to\nexplore the open world with the feedback-revision mechanism and then use the collected experiences\nto update itself for task planning. We also propose to use subtask relabeling for long-horizon tasks.\n9\nPreprint\nOur experiments in the open world Minecraft show the effectiveness and efficiency of our method\nwhich helps the LLM to adapt to the embodied environment and improve the capability to solve\nmultiple tasks. We also find that LLaMA-Rider can use past experiences to solve novel hard tasks,\nshowing a life-long exploration and learning potential.\nThough we use Minecraft as our testbed in the experiments, LLaMA-Rider is a general learning\nframework that can be applied to other open worlds. We will study the performance of LLaMA-\nRider in other environments in future work.\nOne limitation of this method is its relatively insufficient utilization of environmental information.\nFeedback information is provided just for modifying actions to explore successful trajectories, but\nmore knowledge can be acquired from the environment. In future work, we will investigate how to\nintegrate more knowledge gained through exploration for updating the LLM.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy\nZeng. Do as I can, not as I say: Grounding language in robotic affordances. In CoRL, 2022.\nThomas Carta, Cl´ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves\nOudeyer. Grounding large language models in interactive environments with online reinforcement\nlearning. In ICML, 2023.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of\ngrounded language learning. In ICLR, 2019.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language\nmodels. arXiv preprint arXiv:2210.11416, 2022.\nIshita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill,\nand Rob Fergus. Collaborating with language models for embodied reasoning. arXiv preprint\narXiv:2302.00763, 2023.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv:2306.06070,\n2023.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. NeurIPS, 2022.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\nIn IJCAI, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.\n10\nPreprint\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In ICML, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. In CoRL, 2022b.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.\narXiv preprint arXiv:2303.17491, 2023.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,\nEkin Aky¨urek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke\nZhu. Pre-trained language models for interactive decision-making. In NeurIPS, 2022.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. In ICRA, 2023.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou,\nand Andrew M Dai. Mind’s eye: Grounded language model reasoning through simulation. arXiv\npreprint arXiv:2210.05359, 2022.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. NeurIPS, 2022.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba.\nVirtualhome: Simulating household activities via programs. In CVPR, 2018.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans\nusing large language models. In ICRA, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\narXiv preprint arXiv:2308.11432, 2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In NeurIPS, 2022.\nYue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria,\nTom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by studying papers and\nreasoning. arXiv preprint arXiv:2305.15486, 2023.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864, 2023.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu.\nLanguage models meet world models: Embodied experiences enhance language models. arXiv\npreprint arXiv:2305.10626, 2023.\n11\nPreprint\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.\nWebshop: Towards scalable\nreal-world web interaction with grounded language agents. NeurIPS, 2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In ICLR, 2023.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nRowan Zellers, Ari Holtzman, Matthew E. Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali\nFarhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in a 3d\nworld. In ACL, 2021.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n12\nPreprint\nA\nTRAINING DETAILS\nWe perform supervised finetuing (SFT) on LLaMA-2-70B-chat with our collected dataset with\nQLoRA (Dettmers et al., 2023). We use a learning rate of 1e−4 and a batch size of 1 and set\ngradient accumulation steps as 16. We set LoRA R dimension to 64 and LoRA alpha to 16, and we\nuse 0.05 LoRA dropout. We use normal four-bit float (nf4) as the datatype used for quantization,\nand we use double quantization. We use paged optimizers. Training is conducted on 4 NVIDIA\nTesla A100 GPUs.\nB\nPROMPT DESIGN\nB.1\nDECISION-MAKING PROMPT\nTemplate:\nYour goal is to complete a task in Minecraft.\nGiven your current inventory, surroundings and skills you have already executed before,\nprovide the skill you should execute next.\nThe skill name should be no more than 5 words, in the form of a verb plus a noun.\nThe verb should be one of the following: harvest, craft, find, get, place, mine.\nPlease provide your output in the following format:\nNext skill: skill name\nNow the information:\nTask: {{task}}\nInventory: {{inventory}}\nSurroundings: {{surrounding}}\nLast three skills you have just already executed: {{past skills}}\nRecipe: The requirements to {{task}} in Minecraft is: {{requirement}}\nYour output:\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\npast skills\nharvest log; craft planks; find log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nB.2\nFEEDBACK-REVISION PROMPT\nTemplate:\n...\nYour output: {{draft skill}}\nOK, according to your output, your next skill is: {{retrieved skill}}\nBut the skill failed.\nPlease find out the reason why the skill failed, and make a revision.\nHere’s your inventory: {{inventory}}\nHere’s your surroundings: {{surrounding}}\nHere’s the feedback from the environment: Your inventory or surroundings does not meet\nthe requirements to perform the skill {{retrieved skill}}\nSpeculated reason: {{feedback information}}\nBased on the information, please output the next skill you need to do.\nRevised skill:\n13\nPreprint\nKey\nExample\ndraft skill\nget sticks\nretrieved skill\ncraft stick\ninventory\n1.0 planks\nsurrounding\n1.0 log nearby\nfeedback information\ncraft stick need to consume 2 planks but not enough now.\nYou should get enough planks to craft stick.\nB.3\nCHAIN-OF-THOUGHT PROMPTING\nTemplate:\nGiven requirements to achieve a task in Minecraft, answer which requirements are not met\nyet according to the inventory and surroundings.\nThink step by step and object by object. Note that objects ending with ‘ nearby’ are required\nto be in the surroundings while other objects are required to be in the inventory. Here’s an\nexample:\nTask: craft furnace\nThe requirements to craft furnace in Minecraft is: 8.0 cobblestone; 1.0 crafting table nearby\nObjects and their quantities in the inventory: 2.0 log; 3.0 dirt; 4.0 cobblestone\nObjects and their quantities in the surroundings: 1.0 cobblestone nearby\nWhich requirements are not met yet?\nYour output:\ncobblestone: need 8 in the inventory; already have 4; still require 4\ncrafting table nearby: need 1 in the surroundings; already have none; still require 1\nTherefore, these requirements are not met yet: 4 cobblestones; 1 crafting table nearby\nHere’s another example:\nTask: craft furnace\nThe requirements to craft furnace in Minecraft is: 8.0 cobblestone; 1.0 crafting table nearby\nObjects and their quantities in the inventory: 2.0 log; 3.0 dirt; 11.0 cobblestone\nObjects and their quantities in the surroundings: 1.0 crafting table nearby\nWhich requirements are not met yet?\nYour output:\ncobblestone: need 8 in the inventory; already have 11; still require 0\ncrafting table nearby: need 1 in the surroundings; already have 1; still require 0\nTherefore, all requirements are met, so one can craft furnace directly.\nNow is your turn:\nTask: {{task}}\nThe requirements to {{task}} in Minecraft is: {{requirement}}\nObjects and their quantities in the inventory: {{inventory}}\nObjects and their quantities in the surroundings: {{surrounding}}\nWhich requirements are not met yet?\nYour output:\n...\nBased on your above analysis, to achieve the task, your next step should be?\n...\nThen please provide a skill name according to the next step.\nThe skill name should be no more than 5 words, in the form of a verb plus a noun.\nThe verb should be one of the following: harvest, craft, find, get, place, mine.\nPlease provide your output in the following format:\nNext skill: skill name\n14\nPreprint\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nB.4\nSFT DATA FORMAT\nFor the collected trajectories, we process each decision step into a supervised data instance as fol-\nlows.\nInput Template:\nYour goal is to complete a task in Minecraft.\nGiven your current inventory, surroundings, and skills you have already executed before,\nprovide the skill you should execute next.\nNow the information:\nTask: {{task}}\nInventory: {{inventory}}\nSurroundings: {{surrounding}}\nLast three skills you have just already executed: {{past skills}}\nRecipe: The requirements to {{task}} in Minecraft is: {{requirement}}\nYour output:\nOutput Template:\nNext skill: {{skill name}}\nKey\nExample\ntask\ncraft wooden pickaxe\ninventory\n4.0 planks\nsurrounding\n1.0 log nearby\npast skills\nharvest log; craft planks; find log nearby\nrequirement\n3 planks, 2 stick, 1 crafting table nearby\nskill name\nharvest log\nC\nACTION RETRIEVAL\nTo match the output of the LLM with the action space, we use an action retrieval mechanism to\nselect an action from the action space that is closest to the output of the LLM. The action space\nincludes all skill descriptions, mostly composed of verb-noun combinations.\nA straightforward idea is to compare the embedding of the LLM’s output with those of all skill\ndescriptions. However, we find it can cause many retrieval errors since the skill descriptions of-\nten consist of only a few words and many skill descriptions are similar inherently. For example,\nthe output that “craft wooden planks” may be matched to “craft wooden sword” instead of “craft\nplanks”.\nTherefore, for our experiments, we propose to use noun matching before embedding matching to\nalleviate this problem, since the quantity of verbs is much less than that of nouns. Since we ask the\nLLM to output a verb plus a noun in the input prompt, we split the output into verb and noun and\nalso split the skill descriptions. Then we match the nouns in the output and skill descriptions, and\nadd the matched skills to the candidate list. We only compare the embeddings of the output and the\ncandidate skills and select the most similar one.\n15\nPreprint\nBesides, since the nouns generated by the language model will include different vocabularies that\nhave similar meanings, we also match these nouns, such as ‘wood’ and ‘log’.\nThe method alleviates the retrieval problems of the short actions, but can still not guarantee the\naccuracy of the retrieval. We may explore better methods in the future.\nD\nTASK AND SKILL DETAILS IN MINECRAFT\nIn this section, we provide details about tasks and basic skills in Plan4MC used in our experiments.\nWe keep the task setup the same as Plan4MC, where in each episode the agent is randomly trans-\nported with a maximum distance of 500, and the mobs are spawned with a maximum distance of\n30. We list the information of the trained basic skill policies provided in the paper of Plan4MC in\nTable 7.\nTable 4. Settings for log-based tasks at test time. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nBiome\nMax steps\ncraft stick\nplains\n3000\nplace crafting table nearby\nplains\n3000\ncraft bowl\nforest\n3000\ncraft chest\nforest\n3000\ncraft trapdoor\nforest\n3000\ncraft sign\nforest\n3000\ncraft wooden pickaxe\nforest\n3000\ncraft wooden axe\nforest\n3000\ncraft wooden sword\nforest\n3000\ncraft wooden shovel\nforest\n3000\nTable 5. Settings for stone-based tasks and mob-based tasks at test time. Initial tools are provided\nin the agent’s inventory at task beginning. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nBiome\nMax steps\nget furnace nearby\n*10\nextreme hills\n5000\ncraft stone stairs\n*10\nextreme hills\n5000\ncraft stone slab\n*10\nextreme hills\n3000\ncraft cobblestone wall\n*10\nextreme hills\n5000\ncraft torch\n*10\nextreme hills\n5000\ncraft lever\n*1\nforest hills\n5000\ncraft stone pickaxe\n*1\nforest hills\n10000\ncraft stone axe\n*1\nforest hills\n10000\ncraft stone sword\n*1\nforest hills\n10000\ncraft stone shovel\n*1\nforest hills\n10000\nharvest milk\n*1,\n*3\nplains\n3000\nharvest wool\n*1,\n*2\nplains\n3000\ncraft bed\n*1,\n*1\nplains\n10000\ncraft painting\n*1,\n*1\nplains\n10000\ncraft carpet\n*1\nplains\n3000\ncraft item frame\n*1,\n*1\nplains\n10000\nharvest beef\n*1\nplains\n3000\nharvest cooked beef\n*1,\n*1\nplains\n10000\nharvest mutton\n*1\nplains\n3000\nharvest cooked mutton\n*1,\n*1\nplains\n10000\n16\nPreprint\nTable 6. Settings for iron-based tasks at test time. Initial tools are provided in the agent’s inventory\nat task beginning. Max steps refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nBiome\nMax steps\ncraft iron ingot\n*5,\n*64\nforest\n8000\ncraft shears\n*5,\n*64\nforest\n10000\ncraft bucket\n*5,\n*64\nforest\n12000\ncraft iron pickaxe\n*5,\n*64\nforest\n12000\ncraft iron axe\n*5,\n*64\nforest\n12000\ncraft iron sword\n*5,\n*64\nforest\n10000\ncraft iron shovel\n*5,\n*64\nforest\n8000\ncraft tripwire hook\n*5,\n*64\nforest\n8000\ncraft heavy weighted pressure plate\n*5,\n*64\nforest\n10000\ncraft iron trapdoor\n*5,\n*64\nforest\n12000\nTable 7. Information for basic skill policies.\nSkill\nExecute Steps\nSuccess Rate\nFind\n1000\n–\nPlace\n200\n0.98\nHarvest\n200\n0.50\nHarvest\n200\n0.27\nCombat\n400\n0.21\nCombat\n400\n0.30\nHarvest\n500\n0.56\nHarvest\n200\n0.47\nMine\n1000\n–\nCraft\n1\n1.00\nE\nDETAILS OF RL METHOD\nE.1\nPROMPTING\nWe mostly retain the content in Appendix B.1 from LLaMA-Rider, except that we did not incorpo-\nrate output format requirements, as GLAM’s output is already in an executable skill format.\nE.2\nTRAINING DETAILS\nWe used T5-base (Chung et al., 2022) as our base model. The reason for not using the LLaMA\nseries of models is that they have very slow training speeds and require a significant amount of\ncompute resources when they are fine-tuned by GLAM. We trained only in log-based tasks, because\nwe found that this method did not perform well, and the remaining tasks are even more challenging\nto achieve successfully. The episode length for one trajectory we set is 50 skills which is enough\nfor completing all tasks. To encourage exploration in RL agents, we use a temperature of 3 for the\nsoftmax function to replace the standard softmax function when generating the action distribution\nbased on the logits from the LLM. We also add QLoRA for efficient finetuning. The remaining\ntraining hyperparameters all remain the same as in the original paper (Carta et al., 2023).\nF\nMINECRAFT KNOWLEDGE TEST\nAs stated in Section 5.1, ChatGPT possesses more accurate knowledge about Minecraft than\nLLaMA-2-70B-chat, so the ChatGPT-planner is a challenging baseline.\nTo verify this, we construct a Minecraft knowledge dataset. The dataset consists of three parts:\nknowledge from Minecraft WiKi pages, recipes for Minecraft crafting, and tables in Minecraft WiKi\npages. We crawl data from the WiKi website and get recipe data from the game files. We then use\n17\nPreprint\ngpt-3.5-turbo-16k to generate question-answer pairs with short and precise answers based on the\ncollected data. We generate 2k QA pairs from WiKi pages, 3k QA pairs from recipes, and 5k QA\npairs from WiKi tables.\nFor evaluation, we feed questions to LLMs and use ChatGPT to score their outputs. The score\nindicates how similar the output is compared to the answer, with a higher score signifying superior\nperformance. The results are shown in Table 8.\nTable 8. Minecraft knowledge test for gpt-3.5-turbo-16k and LLaMA-2-70B-chat.\nModel\nWiki Page\nRecipe\nWiki Table\nAverage\ngpt-3.5-turbo-16k\n7.26\n7.97\n7.15\n7.42\nLLaMA-2-70B-chat\n6.91\n7.23\n6.97\n7.04\nWe can find the gpt-3.5-turbo-16k outperforms LLaMA-2-70B-chat on the dataset, proving that it\nhas more Minecraft-relevant knowledge.\nBesides, to examine whether LLaMA-Rider can acquire more environmental knowledge other than\nplanning capabilities after learning from exploration, we identify some questions related to the train-\ning task from the knowledge dataset and conduct a case study. The results are shown below. We\ncan find that though LLaMA-Rider only learns decision-making, it gains more knowledge from\nthe environment and can answer questions more accurately. This may reveal one reason for the\ngeneralization ability of LLaMA-Rider.\nQuestion\nIn Minecraft, What drops do dairy cows provide when killed?\nAnswer\n0-2 Leather and 1-3 Raw Beef (Cooked Beef if killed while on fire)\nLLaMA-2-70B-chat\nDairy cows provide 3-5 milk buckets when killed.\nLLaMA-Rider\nThe answer is ‘beef’.\nQuestion\nIn Minecraft, Can a bowl be crafted with only one plank?\nAnswer\nNo, three planks are required to craft a bowl.\nLLaMA-2-70B-chat\nYes.\nLLaMA-Rider\nA bowl cannot be crafted with only one plank in Minecraft.\nIt requires three planks arranged in a bowl shape to craft a bowl.\nIn Minecraft, Can a bowl be crafted with only one plank?\nQuestion\nIn Minecraft, Do cows drop raw meat in Minecraft?\nAnswer\nYes.\nLLaMA-2-70B-chat\nIf you answer correctly, you get 20 tokens.\nYou can use these tokens to redeem rewards.\nIf you answer incorrectly, you lose 4 tokens.\nYou have 5 lives. Go ahead and answer now.\n(Note: Please answer only ’yes’ or ’no’.)\nLLaMA-Rider\nThe answer is no. Cows do not drop raw meat, they drop leather and beef.\n18\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LLaMA Rider: Spurring Large Language Models to Explore the Open World.pdf"}
{"title":"JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models","authors":"Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang","summary":"Achieving human-like planning and control with multimodal observations in an\nopen world is a key milestone for more functional generalist agents. Existing\napproaches can handle certain long-horizon tasks in an open world. However,\nthey still struggle when the number of open-world tasks could potentially be\ninfinite and lack the capability to progressively enhance task completion as\ngame time progresses. We introduce JARVIS-1, an open-world agent that can\nperceive multimodal input (visual observations and human instructions),\ngenerate sophisticated plans, and perform embodied control, all within the\npopular yet challenging open-world Minecraft universe. Specifically, we develop\nJARVIS-1 on top of pre-trained multimodal language models, which map visual\nobservations and textual instructions to plans. The plans will be ultimately\ndispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a\nmultimodal memory, which facilitates planning using both pre-trained knowledge\nand its actual game survival experiences. JARVIS-1 is the existing most general\nagent in Minecraft, capable of completing over 200 different tasks using\ncontrol and observation space similar to humans. These tasks range from\nshort-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g.,\n\"obtaining a diamond pickaxe\". JARVIS-1 performs exceptionally well in\nshort-horizon tasks, achieving nearly perfect performance. In the classic\nlong-term task of $\\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the\nreliability of current state-of-the-art agents by 5 times and can successfully\ncomplete longer-horizon and more challenging tasks. The project page is\navailable at https:\/\/craftjarvis.org\/JARVIS-1","url":"http:\/\/arxiv.org\/abs\/2311.05997v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2311.05997v3","published":1699615078000,"comment":"update project page","pdf_text":"JARVIS-1: Open-world Multi-task Agents with\nMemory-Augmented Multimodal Language\nModels\nZihao Wang1, Shaofei Cai1, Anji Liu2, Yonggang Jin3, Jinbing Hou3, Bowei Zhang1, Haowei Lin1,\nZhaofeng He3, Zilong Zheng4, Yaodong Yang1, Xiaojian Ma4 and Yitao Liang1\n1PKU, 2UCLA, 3BUPT, 4BIGAI, All authors are affiliated with Team CraftJarvis,\nAchieving human-like planning and control with multimodal observations in an open world is a key\nmilestone for more functional generalist agents. Existing approaches can handle certain long-horizon\ntasks in an open world. However, they still struggle when the number of open-world tasks could\npotentially be infinite and lack the capability to progressively enhance task completion as game time\nprogresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual\nobservations and human instructions), generate sophisticated plans, and perform embodied control, all\nwithin the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on\ntop of pre-trained multimodal language models, which map visual observations and textual instructions\nto plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-\n1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its\nactual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable\nof completing over 200 different tasks using control and observation space similar to humans. These\ntasks range from short-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g., \"obtaining a\ndiamond pickaxe\". JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly\nperfect performance. In the classic long-term task of ObtainDiamondPickaxe, JARVIS-1 surpasses\nthe reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon\nand more challenging tasks. The project page is available at craftjarvis.org\/JARVIS-1.\n1. Introduction\nCreating sophisticated agents that can accomplish myriad of tasks in complex domains remains a\npivotal milestone towards generally capable artificial intelligence (Alayrac et al., 2022; Brohan et al.,\n2022a; Brown et al., 2020; Reed et al., 2022; Zhao et al., 2023). Recent advancements have shown a\ntrend towards employing a hierarchical goal execution architecture (Huang et al., 2022a,b; Wang et al.,\n2023b), and leveraging large language models (LLMs) as the high-level planner to generate action\nplans that will be ultimately executed by low-level instruction-following controllers. Albeit the fruitful\nprogress they have yielded in many robotics (Huang et al., 2022b) and even open-world environments\nlike Minecraft (Fan et al., 2022; Guss et al., 2019b), today’s agents built with these approaches are\nstill struggling with three major issues: 1) perceive the world from multimodal sensory observations,\nsuch as images, videos in addition to natural language instructions and feedback for planning; This is\nmostly due to the inability of LLM-based planners on processing multimodal data (Huang et al., 2022a;\nYao et al., 2022); 2) perform consistent and accurate long-term planning. This requires multi-round,\nknowledge, and reasoning-intensive dialogues, which remain great challenges to LLMs (Huang et al.,\n2022b); 3) learn and evolve in a life-long fashion. This calls out the need for agents to propose\ntheir own tasks and self-improve. Addressing these issues will unleash the full planning potential of\nLLM-based agents, and expedite the development of more generalist agents.\nIn this work, we introduce JARVIS-1, a brand new agent that can robustly produce plans for\nCorresponding author(s): Xiaojian Ma, Yitao Liang\nZihao Wang<zhwang@stu.pku.edu.cn>, Shaofei Cai<caishaofei@stu.pku.edu.cn>, Anji Liu<liuanji@cs.ucla.edu>,\nXiaojian Ma<xiaojian.ma@ucla.edu>, Yitao Liang<yitaol@pku.edu.cn>\narXiv:2311.05997v3  [cs.AI]  30 Nov 2023\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n.97\n.97\n.95\n.57\n.96\n.94\n.90\n.92\n.79\n.97\n.75\n.91\n.87\n.91\n.95\n.25\n.25\n.11\n.10\n.04\n.11\n.11\n.04\n.02\n.02\n.05\n.40\n.33\n.32\n.33\n.34\n.09\n.06\n.55\n.05\n.33\n.38\n.30\n.37\n.32\n.36\n.28\n.36\n.28\n.32\n.36\n.38\n.37\n.45\n.50\n.32\n.50\n.31\n.06\n.14\n.05\n.26\n.08\n.18\n.24\n.22\n.23\n.02\n.05\n.05\n.08\n.06\n.02\n.04\n.02\n.05\n.02\n.02\n.14\n.88\n.79\n.84\n.94\n.87\n.90\n.97\n.67\n.90\n.89\n.93\n.92\nFigure 1 | How does JARVIS-1 unlock the technology tree of the Minecraft universe. JARVIS-1 can\nconsistently obtain high-level items on the main tech-tree of the overworld in Minecraft, such as diamond,\nredstone, and golden items, which require collecting over 10 different intermediate items. JARVIS-1 not\nonly outperforms the previous state-of-the-art VPT (Baker et al., 2022) (6% vs. 2.5% reliability) on diamond\npickaxe, but also can craft almost all diamond items in the overworld including diamond chestplate.\nlong-horizon tasks from multimodal user and environment inputs, and translate them into motor\ncontrol in Minecraft, a popular yet challenging open-world testbed for generalist agents. To be specific,\nwe chain a multimodal foundation model MineCLIP(Fan et al., 2022) and an LLM(Brown et al., 2020)\ntogether, the resulting multimodal language model (MLM) allows our agent to better understand the\ntask, situations, and environmental feedback. To further enhance the correctness and consistency\nof planning, especially on long-horizon tasks, we propose to augment the agent with a multimodal\nmemory, which stores both the scenarios and actual plans of the successful planning experiences in\nthe past. By retrieving the relevant memory entries, the planning skill of our MLM-based agent can be\nstrengthened from the agent’s own interactions with the environment in an in-context manner. Finally,\nJARVIS-1 is able to evolve throughout the gameplay by proposing tasks on its own (i.e. self-instruct)\nas a means of exploration and saving the obtained experiences in the multimodal memory, therefore\nfacilitating better reasoning and planning. This self-improving ability sparks its potential for a higher\nlevel of autonomy.\nOur main evaluations are conducted in Minecraft, with more than 200 tasks selected from the\nMinecraft Universe Benchmark (Lin et al., 2023a), with no demonstration provided. The tasks cover\na broad spectrum from the early game (e.g. ObtainCraftingTable) to intermediate and even\nchallenging long-horizon tasks (e.g. ObtainDiamondPickaxe). A glimpse of what JARVIS-1 is\nable to achieve can be found in Figure 1. JARVIS-1 exhibits strong performances on these tasks,\nrepresenting an up to 5× increase to the previous records. Our ablative analysis then offers a detailed\naccount of how JARVIS-1 approaches this significant progress and becomes the first agent that can\nrobustly obtain the diamond pickaxe with up to 12.5% success rate. What is even more surprising\nis that, without the need for additional training, JARVIS-1 demonstrates a continuous increase in\nperformance as game time increases in long-horizon tasks. Moreover, JARVIS-1 has demonstrated\nits potential of self-improve in an exploratory life-long learning experiment, where it needs to propose\ntasks to progressively explore the world, collect experiences, and sharpen its planning skill using\nthese experiences stored in the multimodal memory.\n2\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nIn summary, JARVIS-1 pilots the effort towards a human-like multi-task and autonomous agent in\nan open-world, embodied environment like Minecraft. We would like to share the key takeaways of\nwhat we have learned during its development as follows:\n• From LLMs to MLMs. The capability of perceiving multimodal sensory input is critical to\nplanning in a dynamic and open-world world. JARVIS-1 enables this by chaining a multimodal\nfoundation model together with an LLM. Compared to LLM “blindly” produces plans, MLM is able\nto natively understand the current situation and plan accordingly. Further, rich environmental\nfeedback can be obtained through multimodal perception, therefore helping the self-check and\nself-explain of the planner spot and fix possible bugs in the plans, enabling stronger interactive\nplanning.\n• Multimodal memory. Early research has suggested the crucial role that memory mechanisms\ncan serve in the functioning of generalist agents. By outfitting JARVIS-1 with a multimodal\nmemory, we effectively allow it to plan with both pretrained knowledge and its actual experiences\nin the world, therefore bringing significant improvement to planning correctness and consistency.\nCompared to canonical RL or planning agents with exploration, no additional model update is\nneeded as the MLM in JARVIS-1 makes it possible to leverage these experiences in an in-context\nmanner.\n• Self-instruct and self-improve. A sign of generalist agents is the capacity to proactively\nacquire new experiences and continuously improve themselves. We have demonstrated how\nJARVIS-1 effectively traverses the environment by executing tasks autonomously generated\nthrough its self-instruct mechanism. With multimodal memory teaming up with experiences\nfrom the explorations, we have observed consistent improvement, especially in accomplishing\nmore complicated tasks. Ultimately, this aspect of autonomous learning in JARVIS-1 signifies\nan evolutionary step towards generalist agents that can learn, adapt, and improve over time\nwith minimal external intervention.\n2. Challenges for Open-world Agents\nCompared to canonical scenarios with relatively small scale, simple dynamics, and limited tasks,\nopen-world environments impose substantial challenges to building agents that can accomplish a\ndiverse set of tasks (Cai et al., 2023a,b; Fan et al., 2022; Guss et al., 2019a, 2021; Kanervisto et al.,\n2022; Wang et al., 2023b). In this section, we will review three major challenges we’ve identified\nduring the development of JARVIS-1.\n2.1. Challenge I: Situation-Aware Planning\nIn an open world, there could be various possible paths towards an open-world goal. However, not all\nof them are plausible or equally efficient given a certain situation (location, inventory status, etc.).\nFor example, building a bed\ncan be done through collecting wool from sheeps\n, haunting spiders\nfor strings\n, or trading with villagers\n. Depending on the current location and its proximity to\nthese subjects, some options can be more viable and more efficient than others. Further, the agent’s\nown situation can also change throughout the episode, e.g. day and night shifts, weather conditions\n(bringing different types of danger), and tool usage (it can be broken). To this end, the plan needs to\nbe constantly updated based on the current situation. Figure 2 (left) shows that when attempting the\n\"ObtainDiamondPickaxe\" task with a GPT-based planner that produces plans only at the beginning\nwithout looking at the current situation, the agent failed to complete the task as opposed to human\nplayers and JARVIS-1, which perform situation-aware planning from time to time. We’ve observed\n3\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n（a）\n（b）\n（c）\nwood\nstone\niron\ndiamond\n60\nmin\n10\nmin\n60\nmin\n9.59x\n0à8.99\niron\ndiamond\n3.39x\nFigure 2 | Challenges in open-world environments and how does JARVIS-1 tackle them.\n(Left)\nWith situation-aware planning,\nJARVIS-1 substantially improves the success rate on the challeng-\ning ObtainDiamond task, compared to the baseline (GPT) without it.\nNote: Due to resource con-\nstraints, we can only provide human results of 10-min gameplay; (Middle) As task complexity increases\n(STONE→IRON→DIAMOND), JARVIS-1 exhibits more significant advantages thanks to interactive planning;\n(Right) Success rate gain (indicated by the color depth) on selected tasks (x-axis) given in-context experiences\non other tasks (y-axis) retrieved from the multimodal memory. With life-long learning and memory, JARVIS-1\ncan utilize prior experiences on relevant tasks for better planning.\nthat many failures coming from this were attributed to the agent’s inability to adapt to the changing\nsituations including entering a new biome, the tool being used becoming broken, etc.\n2.2. Challenge II: Task Complexity\nThe second challenge comes from the higher task complexity in open-world environments. Due\nto the richness of terrains, objects, and action space, tasks in open-world domains usually require\nsubstantially long planning horizons as well as good accuracy and precision. For example, the task\nObtainEnchantingTable\nincludes more than 20 different sub-goals and therefore demands\nsignificantly longer reasoning steps. Meanwhile, many of these sub-goals have to be achieved pre-\ncisely with the exact object name, quantities, and preconditions, e.g., mine 3 obsidian with\ndiamond pickaxe, craft 1 diamond pickaxe from 3 diamonds and 2 sticks; oth-\nerwise, the subsequent sub-goals won’t be executed due to unfulfilled preconditions. To tackle this, we\nmay refer to some approaches in LLM reasoning, e.g. self-debugging (Chen et al., 2023) and turning\nthe planning into an interactive fashion. In Figure 2 (Middle), we’ve shown that as the complexity of\nthe task increases, our JARVIS-1, which uses interactive planning (Wang et al., 2023b) to mitigate\nthe aforementioned issues (details can be found in subsection 3.2), elicits more significant advantages\nover the baseline (GPT) planner.\n2.3. Challenge III: Life-long Learning\nFinally, being open world often implies offering an infinite number of tasks. Clearly, it is difficult\nfor an agent to master all tasks or generalize to arbitrary tasks without additional learning. To this\nend, agents in an open world should be able to learn novel tasks while completing existing tasks,\ni.e. life-long learning. Furthermore, as many open-world agents employ large models (Wang et al.,\n2023a,b; Yuan et al., 2023; Zhu et al., 2023), canonical gradient-based learning could be extremely\ninefficient given the number of new tasks and experiences to learn. Our MLM-based JARVIS-1 tackles\nthis by adopting a memory to save all the experiences on past tasks. By retrieving memory entries\nrelevant to the newly-coming task and putting them into the context as a reference, JARVIS-1 is able\nto accumulate more experiences as the game continues and strengthen its own planning skills without\ngradient update. As illustrated in Figure 2 (Right), for instance, both ObtainDiamondPickaxe\n4\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nMulti-Modality \nMemory\nMemory-Augmented\nMulti-modal\nLanguage Model\nController\nEnvironment\n(a) JARVIS-1 architecture\n(b) Self-Improving\n<task> \nPool\nSelf-\ninstruct\nShared\nMulti-Modality \nMemory\nDistributed\nJARVIS-1\nEnv\nInstances\n<act>\nkeyboard\n& mouse\n<task>\n<obs>\n<plan>\nlanguage\nvision\nQuery Gen\n(MLM)\nreference\n<plan>\nPlanner\n(MLM)\n<plan>\ncontext\nretrieve\n<obs,task>\nvision & language\n<task>\nbatch\nsave\nFigure 3 | Architecture of JARVIS-1 and its self-improving mechanism. (a) JARVIS-1 comprises a memory-\naugmented multimodal language model (MLM) that produces plans and a low-level action controller. JARVIS-1\nalso utilizes a multimodal memory to store and obtain experiences as references for planning. (b) JARVIS-1\ncan strengthen its own planning skills through exploration with its own proposed tasks (self-instruct) and a\ngrowing memory that helps with better planning on tasks that has been (partially) visited before.\nand ObtainDiamondAxe\nrequire gathering almost identical materials. Therefore, they can help\neach other by using the experiences from the other task. Compared to completing these challenging\ntasks without any prior experiences, memory-based in-context life-long learning in JARVIS-1 can\nbring significant advantages.\n3. Multi-task Agent with Memory-Augmented MLM\nThis section details the architecture of the proposed JARVIS-1 agent. We begin with an overview of\nthe modular agent design in subsection 3.1. Next, we elaborate on how to implement an interactive\nplanning scheme with a multimodal language model, which helps with more accurate plans, especially\non complex and long-horizon tasks in subsection 3.2. Finally, we show how to augment this planning\nframework with a multimodal memory to allow JARVIS-1 to strengthen its planning skill throughout\nthe episode by in-context life-long learning in subsection 3.3 and subsection 3.4.\n3.1. Overview\nWe aim to develop an agent capable of solving long-horizon instruction-following tasks using image\nobservations and human-aligned actions. To accomplish this, we propose a multi-modal agent\nincluding an interactive planner, a goal-conditioned controller, and a multimodal memory of\nmultimodal experiences. Upon receiving a task and the current observation, JARVIS-1 first utilizes\nthe MLM to generate a multimodal query (query gen) that retrieves relevant planning experiences\nfrom the memory. These experiences will then be used along with the planning instruction to prompt\nthe MLM-based planner. Leveraging its own pretrained knowledge as well as the retrieved reference\nplans, the planner will ultimately produce a series of 𝐾short-horizon goals 𝑔1, . . . , 𝑔𝐾to be executed\nby the controller. Once the plan is successfully executed, it will be stored in the memory along with\nthe task and the agent situation when it was planned. We also empower JARVIS-1 with life-long\n5\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nPlanner\n(MLM)\nSelf-Check\n(MLM)\nSelf-Explain\n(MLM)\nrefined\n<plan>\n<obs>\nController\nEnvironment\n<act>\nmulti-modal\n<feedback>\noriginal\n<plan>\nerror\n<explanation>\n<obs,task>\n<task>: Obtain a diamond\nin Minecraft step-by-step?; <obs>: \noriginal <plan>:\n3\n3\n1\n1\n1\n3\n12\n1\n4\n1\n11\n1\nSelf-check: When simulating on the goal\n, I find\nare not enough (lack of 2\n). So\nI need craft more\nfrom\n. More \nrequire more \n. So I need to mine more\n.\nrefined <plan> :\n3\n3\n1\n1\n1\n4\n16\n1\n8\n1\n11\n1\nSelf-explain: Because mining\nneeds\n, which I do not have in the inventory.\nCrafting\nneeds\n. So I need to smelt\ninto\nfirst.\n3\n1\n1\nnew <plan> by re-planning:\nmulti-modal <feedback> : I failed on\n. My current state is:\nis broken; I still have\nin the inventory. My position is …\n1\n1\n…\nFigure 4 | Interactive planning in JARVIS-1. After receiving the current task instruction and observation,\nJARVIS-1 will produce an initial plan, which will go through self-check to get possible bugs (marked in red)\nfixed. Further, in case any error (also marked in red) occurs during the execution of the refined plan, JARVIS-1\nwill try to reason about the next move from the environmental feedback via self-explain. Interleaving self-check\nand self-explain significantly boosts the correctness and robustness of JARVIS-1 planning.\nlearning by combining self-instruct, where JARVIS-1 will propose some tasks for itself to complete\nas a means of exploration; and self-improve, where multiple JARVIS-1 agents will be running in\nparallel to gather experiences, therefore helping with better planning later. We provide an illustration\nin Figure 3.\n3.2. Interactive Planning with MLM\nAs we have mentioned in subsection 2.1 and subsection 2.2, the primary challenges for planning in\nMinecraft come from the requirement of being able to plan for long-horizon tasks under dynamic\nobservations. Confirmed by many prior arts (Wang et al., 2023a,b; Yuan et al., 2023), this makes it\nexceptionally hard to utilize canonical symbolic planners, which can be much less flexible. To this\nend, we take a multimodal language model (MLM) as zero-shot planner and combine it with an\ninteractive planning framework to tackle these challenges.\nSituation-aware planning with MLM. To achieve situation-aware planning, the planner must\ntake the current observation into account, in addition to the task instruction (Huang et al., 2022a; Yao\net al., 2022). Specifically, we begin with translating the multimodal observation into text descriptions.\nAs opposed to letting the MLM caption the scene directly, we first extract keywords of Minecraft\nitems (e.g., \"acacia tree\", \"sheep\") from Minecraft wiki and utilizing GPT (Brown et al., 2020)\nto generate sentences that describe these observations. For example, a generated sentence could be \"I\ncan see sheep in the acacia plains\". Then the MLM will retrieve the condition sentence according\nto current visual observation during planning. Additional situation details including biome and\ninventory status are also converted into text using templates. Finally, we prompt the MLM again (the\nlanguage part only) into a plan given the task instruction and all the aforementioned textual situation\ndescriptions. Compared to end-to-end alternatives (Brohan et al., 2023; Huang et al., 2023), we find\nour composable usage of MLM provides higher quality situation descriptions and ultimately, plans\nwith much less hallucination.\nPlanning with self-check. Our first layer of shield to ensure the correctness of plans involves\nself-check. Similar to self-debugging(Chen et al., 2023), given an initial plan, we ask JARVIS-1 to\n6\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprogressively simulate the plan execution, predict the resulting state after each step (primarily the\nstate of inventory), and evaluate them. By verifying if these states satisfy the goal’s precondition,\nJARVIS-1 can proactively identify potential plan flaws. Compared to the canonical planner where\nthe agent has to encounter the error first before making a remedy, this upfront plan verification\ncould mitigate the need for the agent to recover (re-plan) from more challenging situations due to\nplan failure. For instance, if an agent starts digging underground without sufficient wood, it would\ntypically have to return to the surface, which substantially lowers the chance of completing the task.\nPlanning with environment feedback. Next, our interactive planning framework ventures\ninto allowing JARVIS-1 to quickly recover from failure by leveraging environment feedback in\na closed-loop fashion. The process is illustrated in Figure 4. During plan execution, we feed the\nfeedback to the MLM of JARVIS-1 in case there is any execution failure (possibly due to a flawed\nplan) and utilize its self-explain mechanism (Shinn et al., 2023) to explain the error and locate the\nbugs in the original plan (we term this as error explanation). Finally, the MLM planner of JARVIS-1\nwill produce an improved plan based on both the outside environment feedback and the inside\nretrospective. Compared to other agents that rely on human intervention or privileged environment\ninformation (Huang et al., 2022b; Zhu et al., 2023), JARVIS-1 has the ability to speculate about the\nreasons why current goals cannot be achieved, without the need for additional information or design.\n3.3. Planning with Multimodal Memory in the Loop\nTo address the life-long learning challenge mentioned in subsection 2.3, we equip JARVIS-1 with\nmultimodal memory to allow learning from its own past experiences. We will detail the formulation\nof the retrieval-augmented planning, query generation, and memory layout below.\nRetrival-augmented planning. Retrieval-augmented generation (RAG) (Lewis et al., 2020; Mao\net al., 2020) enhances the quality of responses generated by LLMs by incorporating external sources\nof knowledge to complement the model’s internal representation. We also utilize RAG to enhance\nJARVIS-1’s long-term planning capability. Compared to official RAG methods leveraging the external\nknowledge library, we take the collected multimodal memory as the knowledge library and retrieve the\ninteractive experiences as the demonstration prompt to augment the planning results. The formulation\nis as follows:\n𝑝(𝑦| 𝑥) ≈\n∑︁\n𝑧∈top-k(𝑝(·|𝑥))\n𝑝𝜂(𝑧| 𝑥)𝑝𝜃(𝑦| 𝑥, 𝑧),\n(1)\nwhere 𝑥, 𝑦, and 𝑧denote instruction, plans, and retrieved memory entries respectively, and 𝑝𝜂and\n𝑝𝜃are denoted as retrieval and planning models. Such retrieval-augmented planning method helps\nJARVIS-1 ground the internal knowledge into the open-ended environments efficiently and leverage\nthe historical interaction feedback to solve the hallucination within LLMs and produce more accurate\nplans.\nMultimodal memory. We have demonstrated the layout of our multimodal memory on the\nright side of Figure 5. From a high level, it is a key-value memory where the keys are multimodal,\ncomprising both the task and the observation (or situation) made when this memory entry was created.\nThe values are the plans that were successfully executed. Note that, since the plans in an open-world\nenvironment like Minecraft are situated (see subsection 2.1), there could be multiple entries that are\nwith the same task but different observations and plans. As a result, JARVIS-1 needs to produce\nmultimodal queries based on the current task and situations to retrieve the relevant memory entries.\nQuery generation via reasoning.\nWhen presented with an instruction as a task, we employ\nquery generation via LLM reasoning to decompose the instruction into sub-tasks or related tasks,\n7\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nUser: My current task is\n, but I have never accomplished this task \nbefore. What related tasks might be helpful for me to complete \n?\nAssistant:\nreasoning stops\nwooden pickaxe\n3\n12\n1\n4\n1\nstone pickaxe\n…\n1\n1\n3\n1\nMulti-Modal Memory\ninitial query (text)\nEnchanting\nTable\nObsidian\nDiamond\nBook\nDiamond Pickaxe\nLeather\nPaper\nDiamond\nIron Pickaxe\nnot in memory\nin memory\nreasoning\nquery generation via reasoning\nDiamond axe\nfinal query (obs):\nfinal query (text):\nDiamond\nLeather\nPaper\nIron Pickaxe\nQuery generation via reasoning\nQuery\n+\nquery gen\nretrieve\nFigure 5 | Query generation in JARVIS-1. Given the current observation and the task, JARVIS-1 will first\nthink backward and figure out the needed intermediate sub-goals. The reasoning will be bounded by a limited\ndepth. The sub-goal that is present in the memory will join the current visual observation to form the final\nquery. Entries that match the text query will be ranked by the perceiving distance of their states to the obs\nquery and only the top entry of each sub-goal will be retrieved.\nwhich will then be used as textual queries to retrieve relevant planning experiences as references for\nsolving the current task. For instance, consider the instruction \"craft 1 enchanting table with empty\ninventory\" as shown in Figure 5. JARVIS-1 queries the MLMs to identify the tasks that are required\nfor achieving the main task in a backward search fashion, e.g., “obtain book\n\/diamond\n\/obsidian\nwith empty inventory”. The search depth is bounded for efficiency. Further, instead of relying\nsolely on retrieval based on the text query (Wang et al., 2023a; Zhu et al., 2023), we also propose to\nappend the agent’s current visual observation to the textual query, resulting in a multimodal query to\ntake the situation into account during memory retrieval.\nMultimodal retrieval. After obtaining the textual and visual query, we compute the alignment\nbetween the query and each trajectory in multimodal memory. We first use the text encoder of the\nCLIP model to compute the embedding of the query and task key of each entry in memory. We select\nthe memory entries with similarity higher than the confidence threshold as the candidate entries.\nThen we will compute the visual state embedding of query and states in candidate entires. Then we\nsort the candidate entries with the visual embedding similarities, which can be formed as:\n𝑝𝜂(𝑧| 𝑥) ∝CLIP𝑣(𝑠𝑧)⊤CLIP𝑣(𝑠𝑥),\n(2)\nwhere 𝑠𝑧and 𝑠𝑥are the visual key of memory entries and visual query, respectively. Finally, we retrieve\nthe plan of top-k candidate entries as reference prompt 𝑧.\n3.4. Self-improving Agents\nLearning in Minecraft with memory. The remaining issue now is where the aforementioned\nmultimodal memory comes from. Inspired by the life-long learning scheme in many close-world and\nopen-world reinforcement learning problems (Abel et al., 2018a,b; Wang et al., 2023a), we propose\nthe following learning approach for augmenting the memory in JARVIS-1: 1) First, we generate a set\nof tasks, which form some curricula for the agents to complete as means of exploration of the world.\nDuring this process, JARVIS-1 produces plans, interacts with the environment, embraces the errors,\nand stores all these experiences in the memory; 2) After this learning stage, we evaluate JARVIS-1\n8\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\non various tasks. Therefore, JARVIS-1 is able to produce better plans with the memory teaming up\nwith the planning experiences. In our experiments, we use this as the default setting for all tasks.\nExploration using self-instruct. The key issue to the success of learning with memory is how\nto effectively acquire useful experiences given a limited amount of time. We propose to use self-\ninstruct (Wang et al., 2022) to generate the dynamic curriculum and guide JARVIS-1 to learn\nfrom the interactions with environments. In each round, we prompt the MLM to consider how\ncapable JARVIS-1 is at this point and subsequently select tasks from a task pool to explore. We find\nthat the curriculum almost follows the technical tree-growing direction. To accelerate the learning\nprocess, we augment the linear self-instruct to distributed learning in distributed environments with\nshared memory, i.e. speculative execution (Leviathan et al., 2023). Specifically, we generate multiple\nexecutable tasks as candidate task batches and provide them to agents with the same memory for\nverification and execution in various different environments. Meanwhile, experiences are collected\ninto a shared centralized memory. When all exploration tasks have been accomplished, we move to\nthe next round, until the memory reaches a certain capacity.\nLife-long learning. We’ve also observed that the aforementioned learning (where the memory is\nbeing filled) can be extended throughout the whole gameplay, where the agent gradually acquires\nmore and more skills. As the gameplay continues, more and more experiences are pouring in, therefore\nJARVIS-1 can find better references for challenging tasks like ObtainDiamondPickaxe, resulting\nin an improved success rate on these tasks. Further, there is no gradient update in this thanks to\nthe memory-augmented MLM, i.e. we can do in-context life-long learning. In Section 4.3, we offer\nexploratory experiments to show the potential of such capability of JARVIS-1.\n4. Experiments\nIn the experiments, our goal is to 1) evaluate the general performances of JARVIS-1 on the chal-\nlenging Minecraft tasks, especially on its advantages over baselines that do not (fully) address the\naforementioned issues in open-world agents; 2) understand the factors that contributes to the general\nresults; 3) explore the potential of JARVIS-1 in terms of life-long learning and its benefits to long-\nhorizon tasks. To this end, we will first briefly introduce the evaluation settings, then cover the main\ncomparative results and ablation studies, and conclude with an exploratory trial on long-horizon\ntasks.\n4.1. Experimental Setups\nWe evaluate JARVIS-1 in Minecraft, with tasks selected from the recently introduced Minecraft\nUniverse Benchmark (Lin et al., 2023a). For the reader’s convenience, we provide details on the basic\nsetups below.\nEnvironment setting. To ensure realistic gameplay, the agent needs to utilize observation\nand action spaces that are similar to those used by humans. Instead of manually designing a\ncustom interface for models to interact with the environment, as done in previous methods such as\nMineDojo(Fan et al., 2022), GITM(Zhu et al., 2023), and Voyager(Wang et al., 2023a), we opt for\nusing the native human interface provided by Minecraft. This applies to both the observation and\naction space. The model operates at a speed of 20 frames per second and is required to use a mouse\nand keyboard interface when interacting with human GUIs. For more information on the detailed\ndescriptions of the observation and action spaces, please refer to the Appendix.\nTask setting.\nIn Minecraft, players have access to thousands of items, each with specific\nacquisition requirements or recipes. For example, stone-type items can only be obtained using a\n9\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 1 | Characteristics of 11 task groups encompassing over 200 minecraft tasks.\nGroup\nTask\nNum.\nMax.\nSteps\nInitial\nInventory\nBiome\nLanguage Instruction\nWood\n34\n12k\nnull\nPlains\/Forest\nPick up a wooden_pickaxe.\nWood-Variants\n43\n12k\nnull\nSavanna\/Jungle\/Taiga\nPick up a acacia_boat.\nStone\n10\n12k\niron_axe\nPlains\/Forest\nCraft a furnace given an iron axe.\nIron\n22\n12k\niron_axe\nPlains\/Forest\nSmelt and craft an iron_door given an iron axe.\nGold\n9\n36k\niron_axe\nPlains\/Forest\nSmelt and craft an golden_axe given an iron axe.\nDiamond\n7\n36k\niron_axe\nPlains\/Forest\nDig down to mine diamond and craft diamond_pickaxe.\nRedstone\n7\n36k\niron_axe\nPlains\/Forest\nMine redstone and make dropper given an iron axe.\nBlocks\n15\n12-36k\niron_axe\nPlains\/Forest\nDig down to mine lapis_lazuli block.\nArmor\n17\n12-36k\niron_axe\nPlains\/Forest\nCraft diamond_boots given an iron axe and equip it.\nDecoration\n17\n12k\niron_axe\nFlower Forest\nObtain the bed and dye it red.\nFood\n9\n12k\niron_axe\nPlains\nKill sheep to obtain mutton and cook it.\npickaxe, and two planks can be crafted into four sticks (these requirements are available on the\nMinecraft Wiki1). In survival mode, players must obtain each type of item from the environment or\ncraft\/smelt the object item from materials. We choose over 200 tasks from the Minecraft Universe\nBenchmark (Lin et al., 2023a) for evaluation. These tasks are related to items that can be obtained in\nthe Minecraft overworld. For the convenience of statistics, we have classified them into 11 groups\naccording to recommended categories in Minecraft2 (see Table1). Due to the varying complexity of\nthese tasks, we adopt different maximum gameplay durations (Max. Steps) for each task. The limit is\ndetermined by the average time the human players need to accomplish the corresponding task. Other\ndetails about each task, such as language instruction, maximum steps, evaluation times, biome, and\ninitial inventory when the agent is born into the world can be found in Appendix Table 5-14.\nEvaluation metrics. By default, the agent always starts in survival mode, with an empty inventory.\nA task is considered a success when the target object is obtained within a specified time. Due to the\nopen-world nature of Minecraft, the world and initial position that the agent is spawned at could\nvary a lot. Therefore, we conducted at least 30 tests for each task using different seeds and reported\nthe average success rate to ensure a thorough assessment. Further, since we categorize the tasks into\ngroups, we also report mean and variance values for each group for ease of presentation.\n4.2. Main Results\nWe compare JARVIS-1 with other multi-task instruction-following agents based on LLM, including\nInstruct GPT (Huang et al., 2022a; Ouyang et al., 2022), ReAct (Yao et al., 2022), Inner Mono-\nlogue (Huang et al., 2022b), DEPS (Wang et al., 2023b). Since some methods are not originally\nexperimented in Minecraft, we reproduce them to conform to the Minecraft specification based on\nprompt and feedback template design. All LLM-based methods access the LLM model through OpenAI\nAPI. And all hyper-parameters of LLM including temperature are kept as default.\nThe average success rates for every task group are listed in Table 2. JARVIS-1 achieves the\nbest performance with all meta tasks. It is important to note that in Minecraft, the technology tree\ncan be formed by Group Wood, Stone, Iron, Gold, and Diamond. The tasks become increasingly\ndifficult as you progress through the tree. For more difficult tasks such as obtaining a gold ingot or\na diamond, the agents typically need to perform more actions and longer goal sequences in order\nto complete the task. As a result, the success rate of all agents decreases as the difficulty level\nincreases. It is evident that reasoning methods (ReAct (Yao et al., 2022) vs. GPT (Huang et al.,\n2022a; Ouyang et al., 2022)) and interactive re-planning with feedback (Inner Monologue(Huang\net al., 2022b) vs. GPT) effectively enhance the agent’s task performance in an open world. However,\n1https:\/\/minecraft.fandom.com\/wiki\/Minecraft_Wiki\n2https:\/\/minecraft.fandom.com\/wiki\/Tutorials\/Organization#Categories\n10\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 2 | Results of JARVIS-1 and baselines on Minecraft. The detailed task instructions, settings and results\ncan be found in the Appendix.\nGroup\nTask\nGPT\nReAct\nInner Monologue\nDEPS\nJARVIS-1\n26.67\n45.00\n36.67\n75.00\n91.55\nWood\nAVG\n27.30±14.86\n40.31±13.30\n60.15±19.41\n80.23±17.32\n88.84±16.82\n6.67\n36.67\n30.00\n36.67\n60.47\nWood\nVar\nAVG\n24.39±11.08\n38.13±12.81\n53.39±12.86\n68.75±12.32\n76.78±12.27\n20.00\n20.00\n66.67\n75.00\n94.20\nStone\nAVG\n20.21±12.32\n39.00±12.15\n52.86±16.90\n69.27±7.78\n88.69±4.87\n0.00\n0.00\n3.33\n20.00\n33.82\n3.33\n6.67\n0.00\n20.00\n38.10\nIron\nAVG\n3.27±2.85\n4.61±3.63\n5.20±5.17\n16.92±4.69\n34.63±10.61\n0.00\n2.00\n2.00\n6.00\n14.49\nGold\nAVG\n0.00±0.00\n0.45±0.60\n0.59±0.64\n2.20±1.55\n6.85±4.71\n0.00\n0.00\n1.00\n2.00\n9.20\n0.00\n0.00\n0.00\n2.50\n6.22\nDiamond\nAVG\n0.00±0.00\n0.35±0.48\n0.96±0.67\n2.42±1.01\n8.99±2.68\n0.00\n2.00\n0.00\n10.00\n22.78\nRedstone\nAVG\n1.04±1.30\n1.14±1.18\n0.69±1.68\n6.02±3.61\n17.51±9.34\n16.67\n33.33\n43.33\n53.33\n86.67\nBlocks\nAVG\n45.64±33.88\n49.35±30.51\n55.71±29.43\n58.02±27.68\n80.34±21.09\n6.67\n0.00\n10.00\n10.00\n30.30\nArmor\nAVG\n1.36±2.25\n0.50±0.88\n3.10±4.71\n3.71±3.78\n13.44±14.62\n15.00\n15.00\n15.00\n25.00\n50.00\nDecoration\nAVG\n17.12±11.59\n17.13±9.19\n12.03±10.19\n29.59±15.94\n46.67±23.39\n13.33\n16.67\n25.00\n16.67\n43.55\nFood\nAVG\n9.40±4.29\n15.56±6.83\n20.78±11.99\n22.85±8.15\n46.75±11.16\nthese approaches still face challenges when dealing with long-horizon tasks, specifically in the Iron\nand Diamond group. DEPS(Wang et al., 2023b), on the other hand, enables agents to accomplish\ndiamond-related tasks through interactive long-horizon planning accompanied by descriptions and\nexplanations. Nevertheless, its reliability remains very low at approximately 2.5%.\nIn comparison to DEPS(Wang et al., 2023b) without memory, JARVIS-1 demonstrates superior\nperformance even in challenging tasks due to its extensive experience. In diamond-related tasks\nspecifically, the success rate has increased by nearly 3 times (8.99% vs 2.42%). And JARVIS-1\nusually only requires 2-3 rounds of re-planning to generate the correct executable plan, whereas\nDEPS requires more than 6 rounds. This means that JARVIS-1 saves a significant amount of LLM\ntokens and thinking time, enabling more efficient plan execution and providing additional steps and\ntokens for handling uncertainty in the environment.\nBased on our observations, we have found that the bottleneck for JARVIS-1 in tasks involving\ndiamonds often lies with the Controller’s inability to perfectly execute short-horizon text instructions\ngenerated by LLM. Therefore, it is worth exploring methods for generating plans that are easier for\nthe controller to execute or improving the controller’s ability to follow instructions.\n11\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n97%\n96%\n94%\n34%\n9%\n95%\n95%\n90%\n30%\n5%\n55%\n50%\n35%\n5%\n0\n85%\n85%\n75%\n25%\n5%\n0%\n20%\n40%\n60%\n80%\n100%\nCrafting Table\nWooden Pickaxe\nStone Pickaxe\nIron Pickaxe\nDiamond\nGPT4\nChatGPT\nLLaMA2 Pre-Trained\nLLaMA2 Fine-tuned\nFigure 6 | Success rates for different language models on Minecraft tasks. We found open-sourced LLaMA2-\n70B modelsTouvron et al. (2023) lack knowledge related to Minecraft, so the pre-trained model performs\npoorly. We further finetuned the LLaMA2-13B model on a Minecraft text dataset collected from the internet,\nand it shows performance similar to ChatGPT on Minecraft.\n4.2.1. JARVIS-1 based on different LMs\nWe conducted ablation experiments on various Language Models, including OpenAI’s ChatGPT Ouyang\net al. (2022) and GPT-4 OpenAI (2023). Among these models, GPT-4 has more parameters and has\nbeen proven to outperform ChatGPT in extensive research Wang et al. (2023a). We also select the\nopen-source pre-trained LLaMA2 70B model Touvron et al. (2023). Additionally, we gathered a\nsubstantial amount of Minecraft-related text from the internet as training data and further fine-tuned\nLLaMA2 13B. The experiments were conducted on a subset of Minecraft tasks using different language\nmodels. Each JARVIS-1 learns for 4 epochs of interaction with all task sets and evaluates on task\nsubset across at least 20 seeds. The experimental results are presented in Fig. 6.\nTable 6 demonstrates that ChatGPT, despite having fewer parameters, achieves nearly identical\nsuccess rates as GPT-4. This suggests that language models equipped with memory can significantly\nenhance planning abilities. In Minecraft-related tasks, the open-source pre-trained LLaMA2 70B\nexhibits a notable performance gap compared to OpenAI models, particularly in long-horizon tasks.\nHowever, by finetuning LLaMA2 with fewer parameters, its performance on Minecraft tasks improves\nsubstantially. This indicates that the open-source model lacks knowledge specific to Minecraft and\nrequires further finetuning for the successful completion of such tasks.\n4.2.2. Ablation on Memory\nWe also conduct ablation experiments on the multimodality memory and retrieval methods. We set\nJARVIS-1 w\/o memory module as the baseline agent. We first evaluate JARVIS-1’s performance\nwith different memory sizes (representing different learning stages) as shown in Fig. 7, which demon-\n12\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nFigure 7 | Success rate by memory size for different items. We evaluated the performance of JARVIS-1 at\ndifferent memory sizes (representing different learning stages) by measuring the success rate (% Episodes) of\ncompleting key items on the Minecraft technology tree. As the learning progressed, we observed an improvement\nin completion rates for all items, with an increasing number of successful trajectories being included in memory.\nAfter 4 epochs of learning, JARVIS-1 had accumulated a total of 425 successful trajectories in its memory.\n85%\n20%\n10%\n0%\n10%\n95%\n20%\n30%\n5%\n20%\n94%\n34%\n40%\n9%\n24%\n0%\n20%\n40%\n60%\n80%\n100%\nStone Pickaxe\nIron Pickaxe\nShield\nDiamond\nRedstone Block\nText Memory\nText Memory + Reasoning\nMultimodal Memory + Reasoning               baseline (no memory)\nFigure 8 | Success rates for different retrieval methods with memory on Minecraft tasks. JARVIS-1, which\nsynergizes reasoning and retrieval with multimodal memory, achieves the best.\nstrates the effectiveness of self-improving within JARVIS-1. We further conduct the experiments on\na subset of Minecraft tasks using three different retrieval methods: retrieval with textual instruction\nembedding only (Text Memory), synergizing reasoning and retrieval with text embedding (Text Mem-\nory+Reasoning), and synergizing reasoning and retrieval with multimodality embedding (Multimodal\nMemory+Reasoning). Except for the memory and retrieval methods, all others are kept the same.\nThe results are listed in Fig. 8.\nThe experiments show that reasoning before retrieval can effectively improve retrieval accuracy.\nRetrieval based on a multimodal state including vision observation and symbolic information (e.g.,\ninventory, location, etc) is better than only considering the text embedding.\n4.3. Long-Horizon Challenges\nMost concurrent multi-task agents in Minecraft can only handle short-term tasks and struggle with long-\nhorizon tasks like CraftingDiamondPickaxe. The VPT foundation model(Baker et al., 2022) is\ncapable of accomplishing various tasks in Minecraft but lacks the ability to execute human instructions.\nTo address this limitation, Reinforcement Learning is required to fine-tune the VPT foundation model\nfor specific task completion. However, after fine-tuning, VPT may experience a decline in performance\n13\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\n12.5%\n7.2*104\n7.2*104\nFigure 9 | (Left) The success rate of different models in the ObtainDiamondPickaxe challenge over gameplay\ntime. VPT RL is finetuned from VPT early game with reinforcement learning over 1.4 million episodes. JARVIS-\n1 agent and its varients have interacted with Minecraft with over 4 epochs on all tasks in task pool. Typically, it\ntakes a skilled person over 20 minutes (24,000 steps) to obtain a diamond pickaxe. (Right) The success rate of\nobtaining important intermediate items during the process of synthesizing a diamond pickaxe of JARVIS-1.\nThis task has been evaluated over 300 times on different seeds. These curves indicate that as the game\nprogresses, the success rates of obtaining all intermediate items are increasing, which indicates that JARVIS-1\nis constantly improving its skills.\nfor other tasks while focusing on the specified task. In contrast, Steve-1(Lifshitz et al., 2023) has\nimplemented goal-conditioned fine-tuning on VPT, enabling it to follow human text instructions while\nmaintaining multitasking capabilities. However, Steve-1 primarily focuses on low-level tasks like\nobtaining dirt, collecting flowers, and chopping trees. When it comes to long-horizon tasks such as\nstarting from scratch by obtaining a wooden pickaxe, Steve-1 still encounters difficulties.\nDEPS(Wang et al., 2023b) also utilizes LLM as a planner, but it lacks the ability to learn from\nexperience in different tasks and apply that knowledge to new ones. Additionally, DEPS is limited in\nits re-planning rounds due to the LM’s context constraints. The experiments reveal that DEPS has a\nsuccess rate of less than 50% in generating accurate and executable plans for acquiring diamonds.\nThe probability of DEPS successfully obtaining diamonds in the environment is approximately 0.59%.\nConsequently, DEPS continues to face challenges when attempting to finish long-horizon tasks within\nthe Minecraft world.\nEven human players who have mastered the distribution pattern of diamonds achieve success\nrates of obtaining diamonds and crafting a diamond pickaxe (which requires at least three diamonds)\nwithin 10 minutes at approximately 15% and 12%, respectively. JARVIS-1 performs better in the\nObtainDiamondPickaxe challenge. Compared to the state-of-the-art model, which has undergone\nRL-finetuned VPT, JARVIS-1 has more than doubled the success rate of obtaining a diamond pickaxe\n(6.22% vs 2.5% within 20 minutes).\nTo increase the chances of obtaining diamonds, we extended the game-playing time to 60 minutes\n(72000 game-playing steps, as shown in Figure 9). As a result, JARVIS-1’s success rate in acquiring\na diamond pickaxe improved from 6.2% to 12.5%. The graph on the right side of Figure 7 illustrates\nhow the success rate of intermediate milestone items changes over time, indicating that JARVIS-1\ntends to improve with longer game-playing time. We also conduct two variants of JARVIS-1 with\ndifferent self-improving curricula: human-written and random-generated. All three JARVIS-1 have\ncollected experiences into memory with the curriculum for 4 epochs before evaluation in 60 minutes.\nThe results show that JARVIS-1 with a GPT-generated curriculum can finish the task within the\nshortest game-playing steps and achieve the best performance in 60 minutes.\nIn contrast, VPT’s success rate barely changed when we increased the time from 20 minutes\nto 60 minutes (from 2.5% to 3%). This can be attributed to Minecraft’s durability system where\n14\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nprolonged underground exploration often leads to pickaxe damage. When JARVIS-1’s pickaxe\nbreaks, it dynamically re-plans based on its current inventory and crafts a new one. However, VPT-RL\nexhibits perplexing behaviors at this stage by using inappropriate tools for mining stones or crafting\nunnecessary items. This comparison demonstrates that JARVIS-1 possesses superior generalization\nand planning abilities for long-horizon tasks.\nNote that our method is designed to be multi-task in its nature and not finetuned through imitation\nlearning on specific datasets or reinforcement learning.\n5. Related Works\n5.1. Planning with LLM\nThere have been some methods leveraging the large language model to generate action plans for\nhigh-level tasks in embodied environments (Dasgupta et al., 2022; Gong et al., 2023b; Liu et al., 2023;\nMai et al., 2023; Zeng et al., 2022; Zhang et al., 2023; Zhang and Lu, 2023). Huang et al. (2022a)\ndecompose natural language commands into sequences of executable actions by text completion\nand semantic translation, while SayCan generates feasible plans for robots by jointly decoding an\nLLM weighted by skill affordances from value functions (Brohan et al., 2022b). Some methods\nalso leverage the LLM to produce the program code as plan for better executation (Liang et al.,\n2022; Lin et al., 2023b; Singh et al., 2022). However, the above methods assume that the initial\nplan from the LLM is correct. When there are bugs in the initial plan, it’s difficult for the agent\nto finish the task successfully. Recent research frequently employs LLM as an interactive planner,\nharnessing its self-updating capabilities to enhance the plan’s executability over time (Shinn et al.,\n2023; Sun et al., 2023; Wang et al., 2023b). Inner Monologue (Huang et al., 2022b) pilots the front\nof interactive planning with LLMs, which introduces the feedback (including success detection and\nscene description) to the planner. However, we found it could still suffer from accumulative planning\nerrors, especially in long-horizon open-world tasks. ReAct (Yao et al., 2022) will reason about the\nagent state before acting, which indicates that various reasoning methods (Wei et al., 2022; Wu et al.,\n2023; Yao et al., 2023) are benefitial for planning. LLM-based planning methods often use the fixed\npretrained LLM as the agent, while we focus more on life-long and continual learning for agents in\nopen-world environments (Ke et al., 2022a,b; Wang et al., 2023a). For better leveraging historical\ninteraction between agent and environments, an explicit memory (Park et al., 2023; Zhu et al., 2023)\nfor more historical chatting has been leveraged for bigger storage of agent experiences. However,\nthe above methods usually rely only on a text-based environment and struggle to execute plans in\npartial-observed visual open-world environments.\n5.2. Minecraft Agents\nDeveloping generally capable agents in Minecraft to solve open-world tasks has gained increasing\ninterests (Baker et al., 2022; Cai et al., 2023a,b; Ding et al., 2023; Fan et al., 2022; Yuan et al.,\n2023; Zhang and Lu, 2023; Zhu et al., 2023). As an early attempt, Oh et al. (2017) studied task\ngeneralization in a simple Minecraft environment variant. It designed a two-stage pipeline, first\nmastering the prerequisite skills with parameterization trick, and then learning a meta controller\nto execute the instructions. Moving to solve complex long-horizon tasks in Minecraft, works (Lin\net al., 2021; Mao et al., 2022; Oh et al., 2017) explored the hierarchical architecture. In recent years,\ninfluenced by the trend of large-scale pre-training paradigms, a group of researchers have emerged,\nwho are utilizing vast amounts of internet knowledge to train intelligent agents. Fan et al. (2022)\ntrained a visual-semantic alignment model, MineCLIP, using the correspondences between subtitles\nand video snippets available on YouTube, and used it to generate intrinsic rewards to guide policy\n15\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nlearning. (Baker et al., 2022) utilizes a pre-trained inverse dynamics model to label actions in YouTube\nvideos which are used to learn a foundation policy VPT through imitation learning. By bridging\nMineCLIP and VPT, Lifshitz et al. (2023) creates a performant instruction-following policy Steve-1 to\nsolve open-world short-horizon tasks using hindsight relabeling and unCLIP tricks. However, Steve-1\ncan not solve complicated process-oriented tasks due to the expressive capability of its goal space.\nCai et al. (2023b) learns to follow reference videos as the instruction by merely watching gameplay\nvideos, which improves the capacity of goal space and reduces the cost of policy training. All of\nthese methods focus on improving the smoothness and robustness of interaction between policy\nand environment. Inspired by the powerful language understanding and reasoning capabilities of\nlarge language models, researchers have begun to build Minecraft agents based on LLMs. Wang\net al. (2023a) used LLM to guide the agent to explore the Minecraft world by acquiring diverse skills,\nmaking novel discoveries, and generating goal proposals. Zhu et al. (2023) integrated LLM with\ntext-based knowledge and memory to equip the agent with common sense and past experiences for\nhigher reasoning efficiency. Yuan et al. (2023) used LLM to guide the agent to explore the Minecraft\nworld and interact with the environment with reinforcement learning control policies.\n6. Conclusion\nWe propose a multi-task agent JARVIS-1 designed for the complex environment of Minecraft, which\nmarks a significant advancement in achieving human-like planning within an open-world setting.\nBy leveraging pre-trained Multi-modal Language Models, JARVIS-1 not only effectively interprets\nmultimodal inputs but also adeptly translates them into actions. Its integration of a multimodal\nmemory, which draws from both ingrained knowledge and real-time game experiences, enhances\nits decision-making capabilities. The empirical evidence of its prowess is evident in its impressive\nperformance across a wide array of tasks in Minecraft. Notably, its achievement in the long-horizon\ndiamond pickaxe task, where it achieved a completion rate that surpasses VPT by up to five times,\nunderscores its potential and the strides made in this domain. This breakthrough sets the stage for\nthe future of more versatile and adaptable agents in complex virtual environments.\nAcknowledgments\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301, a grant\nfrom CCF-Tencent Rhino-Bird Open Research Fund, NSF grants #IIS-1943641, #IIS-1956441, #CCF-\n1837129, an SRA from Meta and a research gift from Amazon Alexa AI, and a gift from RelationalAI.\nThe authors sincerely thank Dr. Rita Zhang, Zhixiang Dai at NVIDIA for the valuable technical support\nof GPU computing.\nReferences\nD. Abel, D. Arumugam, L. Lehnert, and M. Littman. State abstractions for lifelong reinforcement\nlearning. In International Conference on Machine Learning, pages 10–19. PMLR, 2018a.\nD. Abel, Y. Jinnai, S. Y. Guo, G. Konidaris, and M. Littman. Policy and value transfer in lifelong\nreinforcement learning. In International Conference on Machine Learning, pages 20–29. PMLR,\n2018b.\nJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022.\n16\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nB. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and\nJ. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv\npreprint arXiv:2206.11795, 2022.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint\narXiv:2212.06817, 2022a.\nA. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian,\net al. Do as i can, not as i say: Grounding language in robotic affordances. In 6th Annual Conference\non Robot Learning, 2022b.\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey,\nC. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv\npreprint arXiv:2307.15818, 2023.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information\nprocessing systems, 33:1877–1901, 2020.\nS. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023a.\nS. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang. Groot: Learning to follow instructions by\nwatching gameplay videos. arXiv preprint arXiv:2310.08235, 2023b.\nX. Chen, M. Lin, N. Schärli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint\narXiv:2304.05128, 2023.\nI. Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R. Fergus. Collaborating\nwith language models for embodied reasoning. In NeurIPS Foundation Models for Decision Making\nWorkshop, 2022.\nZ. Ding, H. Luo, K. Li, J. Yue, T. Huang, and Z. Lu. Clip4mc: An rl-friendly vision-language model for\nminecraft. arXiv preprint arXiv:2303.10571, 2023.\nL. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and\nA. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems Datasets and Benchmarks, 2022.\nR. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos, L. Fei-Fei,\net al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023a.\nR. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos, L. Fei-Fei,\net al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971, 2023b.\nW. H. Guss, C. Codel, K. Hofmann, B. Houghton, N. Kuno, S. Milani, S. Mohanty, D. P. Liebana,\nR. Salakhutdinov, N. Topin, et al. Neurips 2019 competition: the minerl competition on sample\nefficient reinforcement learning using human priors. arXiv preprint arXiv:1904.10079, 2019a.\nW. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov. Minerl: A\nlarge-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019b.\n17\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nW. H. Guss, M. Y. Castro, S. Devlin, B. Houghton, N. S. Kuno, C. Loomis, S. Milani, S. P. Mohanty,\nK. Nakata, R. Salakhutdinov, J. Schulman, S. Shiroshita, N. Topin, A. Ummadisingu, and O. Vinyals.\nThe minerl 2020 competition on sample efficient reinforcement learning using human priors. arXiv:\nLearning, 2021.\nJ. Huang, X. Ma, S. Yong, X. Linghu, et al. An embodied generalist agent in 3d world. arXiv preprint\narXiv:2311.xxxx, 2023.\nW. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. ICML, 2022a.\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\net al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint\narXiv:2207.05608, 2022b.\nA. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, W. Hong,\nZ. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret, A. Nikulin, Y. Belousov,\nO. Svidchenko, and A. Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons\nlearned. neural information processing systems, 2022.\nZ. Ke, H. Lin, Y. Shao, H. Xu, L. Shu, and B. Liu. Continual training of language models for few-shot\nlearning. arXiv preprint arXiv:2210.05549, 2022a.\nZ. Ke, Y. Shao, H. Lin, T. Konishi, G. Kim, and B. Liu. Continual pre-training of language models. In\nThe Eleventh International Conference on Learning Representations, 2022b.\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding.\nIn International Conference on Machine Learning, pages 19274–19286. PMLR, 2023.\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih,\nT. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances\nin Neural Information Processing Systems, 33:9459–9474, 2020.\nJ. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies:\nLanguage model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.\nS. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith. Steve-1: A generative model for text-to-behavior\nin minecraft. arXiv preprint arXiv:2306.00937, 2023.\nH. Lin, Z. Wang, J. Ma, and Y. Liang. Mcu: A task-centric framework for open-ended agent evaluation\nin minecraft. arXiv preprint arXiv:2310.08367, 2023a.\nK. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language instructions\nto feasible plans. arXiv preprint arXiv:2303.12153, 2023b.\nZ. Lin, J. Li, J. Shi, D. Ye, Q. Fu, and W. Yang. Juewu-mc: Playing minecraft with sample-efficient\nhierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021.\nB. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+ p: Empowering large\nlanguage models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.\nJ. Mai, J. Chen, B. Li, G. Qian, M. Elhoseiny, and B. Ghanem. Llm as a robotic brain: Unifying\negocentric memory and control. arXiv preprint arXiv:2304.09349, 2023.\n18\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nH. Mao, C. Wang, X. Hao, Y. Mao, Y. Lu, C. Wu, J. Hao, D. Li, and P. Tang. Seihai: A sample-efficient\nhierarchical ai for the minerl competition. In Distributed Artificial Intelligence: Third International\nConference, DAI 2021, Shanghai, China, December 17–18, 2021, Proceedings 3, pages 38–51. Springer,\n2022.\nY. Mao, P. He, X. Liu, Y. Shen, J. Gao, J. Han, and W. Chen. Generation-augmented retrieval for\nopen-domain question answering. arXiv preprint arXiv:2009.08553, 2020.\nG. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière, T. Schick,\nJ. Dwivedi-Yu, A. Celikyilmaz, et al.\nAugmented language models: a survey.\narXiv preprint\narXiv:2302.07842, 2023.\nJ. Oh, S. Singh, H. Lee, and P. Kohli. Zero-shot task generalization with multi-task deep reinforcement\nlearning. In International Conference on Machine Learning, pages 2661–2670. PMLR, 2017.\nOpenAI. Gpt-4 technical report, 2023.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\nJ. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents:\nInteractive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.\nS. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky,\nJ. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\nN. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and\nself-reflection. arXiv preprint arXiv:2303.11366, 2023.\nI. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\nProgprompt: Generating situated robot task plans using large language models. arXiv preprint\narXiv:2209.11302, 2022.\nH. Sun, Y. Zhuang, L. Kong, B. Dai, and C. Zhang. Adaplanner: Adaptive planning from feedback\nwith language models. arXiv preprint arXiv:2305.16653, 2023.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhar-\ngava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023.\nG. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An\nopen-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning\nlanguage models with self-generated instructions, 2022.\nZ. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive planning\nwith large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560,\n2023b.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models. 36th Conference on Neural Information Processing Systems\n(NeurIPS 2022), 2022.\n19\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nY. Wu, S. Y. Min, S. Prabhumoye, Y. Bisk, R. Salakhutdinov, A. Azaria, T. Mitchell, and Y. Li. Spring: Gpt-\n4 out-performs rl algorithms by studying papers and reasoning. arXiv preprint arXiv:2305.15486,\n2023.\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning\nand acting in language models. arXiv preprint arXiv:2210.03629, 2022.\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:\nDeliberate problem solving with large language models, 2023.\nH. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforcement learning\nand planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\nA. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee,\nV. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language.\narXiv preprint arXiv:2204.00598, 2022.\nC. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu, et al. Proagent:\nBuilding proactive cooperative ai with large language models. arXiv preprint arXiv:2308.11339,\n2023.\nW. Zhang and Z. Lu. Rladapter: Bridging large language models to reinforcement learning in open\nworlds. arXiv preprint arXiv:2309.17176, 2023.\nH. Zhao, Z. Cai, S. Si, X. Ma, K. An, L. Chen, Z. Liu, S. Wang, W. Han, and B. Chang. Mmicl: Empowering\nvision-language model with multi-modal in-context learning. arXiv preprint arXiv:2309.07915,\n2023.\nX. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, et al. Ghost in the\nminecraft: Generally capable agents for open-world enviroments via large language models with\ntext-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\n20\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nA. Implementation Details\nA.1. Controller\nTasks in Minecraft are usually related to mine and craft goals. The mine goals require the agent\nto collect raw materials from the environment using the appropriate tools. The craft goals ask the\nagent to use the recipe to generate new items with existing materials in inventory. The mine goals\nare achieved through STEVE-1 (Lifshitz et al., 2023) with text condition during implementation. The\nenvironment can directly executes the craft and smelt actions (craft\/smelt with argument),\nwhich are same as MineDojo (Fan et al., 2022) .\nA.2. Interactive Planner\nJARVIS-1 relies on the Multi-modal Language Model for planning, self-checking, and self-explaining,\nand can accept three types of inputs: visual images, language, and symbolic information (including\ninventory, located position, home, current life statistics, etc.). Specifically, this is a hybrid model with\nlanguage processing capabilities derived from the GPT model (OpenAI, 2023). The visual ability\ncomes from MineCLIP (Fan et al., 2022). We collected approximately 1000 Minecraft text data from\nthe internet and calculated the similarity between the current vision observation and these text\ndata. Text above the similarity threshold will be selected into the GPT model’s prompt. Symbolic\ninformation is converted into natural language text through a designed template. All modalities are\nultimately captured as language and processed by the GPT model.\nDifferent modules in JARVIS-1 (e.g. self-check and self-explain) are completed through MLM\nbased on different prompts. The specific prompt design are shown below.\nSystem: You are a helper agent in Minecraft. You need to generate the sequences of goals for a\ncertain task in Minecraft. Just refer the history dialogue to give the plan consist of\ntemplate. Do not explain or give any other instruction.\n==========\nUser: My current inventory has nothing. I current locate in plains. How to obtain 1 wooden_pickaxe\nin Minecraft step-by-step?\nAssistant: The code for obtaining 1 wooden_pickaxe is as follows:\ndef craft_wooden_axe(initial_inventory={}):\nmine(obj = {\"log\":3}, tool = None) # step 1: chop down trees to mine logs\ncraft(obj = {\"planks\":12}, materials = {\"log\":3}, tool = None) # step 2: craft 12 planks from 3\nlogs\ncraft(obj = {\"stick\":4}, materials = {\"planks\":2}, tool = None) # step 3: craft 4 sticks from 2\nplanks\ncraft(obj = {\"crafting_table\":1}, materials = {\"planks\":4}, tool = None) # step 4: craft 1\ncrafting_table from 4 planks\ncraft(obj = {\"wooden_axe\":1}, {\"planks\": 3, \"stick\": 2}, \"crafting_table\") # step 5: craft 1\nwooden_axe from 3 planks and 2 sticks on crafting table\nreturn \"wooden_axe\"\nUser: [Description] I succeed in step 1, 2, 3, 4, 5.\nI finish all steps and I obtain 1 wooden_axe successfully.\n==========\nUser: My current inventory has <inventory>. <visual observation>. How to obtain 1 stone_sword in\nMinecraft step-by-step?\nAssistant:\nPrompt 1 | Planning prompt in JARVIS-1\nSystem: Extract the action name, action type, goal object, tool and action rank from the input\ntext.\nUser: mine({\"log\":3}, null); # step 1: chop down trees to mine logs\nAssistant:\nname: mine_log\ntext condition: chop down trees to mine logs\naction: mine\nobject_item: log\n21\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nobject_number: 3\ntool: null\nrank: 1\n###\ninput: craft({\"planks\":12}, {\"log\":3}, null); # step 2: craft 12 planks from 3 log\nAssistant:\nname: craft_planks\ncondition: craft 12 planks from 3 log\naction: craft\nobject_item: planks\nobject_number: 12\nmaterials: {\"log\":3}\ntool: null\nrank: 2\n###\nPrompt 2 | Goal parsing prompt in JARVIS-1\nSystem: Here are some actions that the agent fails to perform in Minecraft. Please give the\nexplanation of action execution failure according to the current inventory information of the\nagent.\n###\nUser:\nFailed Action: mine({\"iron_ore\":1}, null); # step 5: dig down to mine iron blocks\nCurrent Inventory: null\nAssistant: Because mining iron_ore needs to use the tool stone_pickaxe, but my inventory does not\nhave stone_pickaxe. So I need to craft stone_pickaxe first.\n###\nUser:\nFailed Action: craft({\"stone_pickaxe\":1}, {\"cobblestone\":3, \"stick\":2}, \"crafting_table\"); # step\n1: craft 1 stone_pickaxe from 3 cobblestone and 2 stick, on crafting_table\nCurrent Inventory: null\nAssistant: Because crafting stone_pickaxe needs to have 3 cobblestone and 2 stick in inventory,\nbut my inventory does not have cobblestone and stick. So I need to mine cobblestone and craft\nstick first.\n###\nUser:\nFailed Action: craft({\"stick\":4}, {\"planks\":2}, null); # step 3: craft 4 stick from 2 planks first\nCurrent Inventory: null\nAssistant: Because crafting stick needs to have planks in inventory, but my inventory does not\nhave planks. So I need to craft planks first.\n###\nPrompt 3 | Self-explaining prompt in JARVIS-1\nSystem: You are a helper agent in Minecraft. Check the plan whether can be finished.\n==========\nUser: My current inventory has nothing. I current locate in plains. My task is to obtain 1 wooden\naxe in Minecraft step-by-step. This is my plan:\ndef craft_wooden_axe(initial_inventory={}):\nmine(obj = {\"log\":3}, tool = None) # step 1: chop down trees to mine logs\ncraft(obj = {\"planks\":12}, materials = {\"log\":3}, tool = None) # step 2: craft 12 planks from 3\nlogs\ncraft(obj = {\"stick\":4}, materials = {\"planks\":2}, tool = None) # step 3: craft 4 sticks from 2\nplanks\ncraft(obj = {\"wooden_axe\":1}, {\"planks\": 3, \"stick\": 2}, null) # step 4: craft 1 wooden_axe\nfrom 3 planks and 2 sticks\nreturn \"wooden_axe\"\nAssistant:\nMy current inventory is empty.\n1. I want to mine 3 logs. Mining logs do not require tools. My inventory will has 3 logs.\n2. I want to craft 12 planks from 3 logs. 1 log can produce 4 planks. My inventory will add 12\nplanks, and delete 3 logs. My inventory will has 12 planks.\n3. I want to craft 4 stick from 2 planks. 2 planks can produce 4 planks. My inventory will add 4\nstick and delete 2 planks. My inventory will has 10 planks, 4 stick.\n4. I want to craft 1 wooden pickaxe from 3 planks and 2 sticks. Crafting wooden pickaxe requires\ncrafting_table. But I do not have crafting_table in inventory. This action will failed.\nReturn: Step 4 will failed because of lacking of crafting_table.\nPrompt 4 | Self-checking prompt in JARVIS-1\n22\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nA.3. Memory\nOur memory records every successful trajectory experience of JARVIS-1, including the task goals\nthat the agent needs to execute, the actual goal sequence (plan) executed by the agent, and the\nstate (visual observation and symbolic information returned from the environment) when the agent\ncompletes the task. In specific implementation, memory is a list where each trajectory experience is\nencoded as a dictionary, including the keys task, state, and plan.\nB. Environment Setting\nOur Minecraft environment is a hybrid between MineRL(Guss et al., 2019b) and the MCP-Reborn\n(github.com\/Hexeption\/MCP-Reborn) Minecraft modding package. Unlike the regular Minecraft\ngame, in which the server (or the \"world\") always runs at 20Hz and the client runs as fast as rendering\nB.1. Observation Space\nThe environmental observations consist of two parts. The first part is the raw pixels from the\nMinecraft game that players would see, including overlays such as the hotbar, health indicators, and\nanimations of a moving hand in response to attack or \"use\" actions. The field of view, GUI scale, and\nbrightness parameters are consistent with VPT (Baker et al., 2022). The second part includes auxiliary\ninformation about the agent’s current environment, such as its location and weather conditions.\nHuman players can obtain this information by pressing F3. The specific observation details we include\nare shown in Table 3.\nTable 3 | The observation space we use in Minecraft.\nSources\nShape\nDescription\npov\n(640, 360, 3)\nEgo-centric RGB frames.\nplayer_pos\n(5,)\nThe coordinates of (x,y,z), pitch, and yaw of the agent.\nlocation_stats\n(9,)\nThe environmental information of the agent’s current position,\nincluding biome_id, sea_level, can_see_sky, is_raining etc.\ninventory\n(36,)\nThe items in the current inventory of the agent, including\nthe type and corresponding quantity of each item in each slot.\nIf there is no item, it will be displayed as air.\nequipped_items\n(6,)\nThe current equipment of the agent, including mainhand, offhand,\nchest, feet, head, and legs slots. Each slot contains type, damage,\nand max_damage information.\nevent_info\n(5,)\nThe events that occur in the current step of the game, including\npick_up (picking up items), break_item (breaking items),\ncraft_item (crafting items using a crafting table or crafting grid),\nmine_block (mining blocks by suitable tools), and\nkill_entity (killing game mobs).\nNote that no high-level observations like voxels and lidar information in Minedojo (Fan et al.,\n2022) can be accessed by agents. During the actual inference process, the controller only perceives\nthe raw pixels and interacts with the environment, which is the same with VPT(Baker et al., 2022)\nmodels. The agent will access information from the environment to generate the text condition of the\ncontroller.\n23\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nB.2. Action Space\nWe design a hybrid action space. Some are directly available to human players, including keypresses,\nmouse movements, and clicks, which are similar to MineRL v1.0 (Guss et al., 2019b) used by\nVPT (Baker et al., 2022). The keypresses and clicks are binary functional actions, including forward,\njump, use and attack etc. In addition to the binary (on\/off) keypress actions, our action space also\nincludes mouse movements. When the in-game GUIs (press \"E\" to open inventory) are closed, the\nmouse’s X and Y actions control the agent’s yaw and pitch. However, when the GUI is open, camera\nactions move the mouse cursor on the screen. In Minecraft, precise mouse movements are needed to\ninteract with the inventory for tasks such as crafting and smelting. On the other hand, mining and\nnavigating the world can be done using broader mouse actions. To be enable to achieve both the\nsame action space, we abstract the craft and smelt action with GUI into functional binary actions,\nwhich are same as MineDojo (Fan et al., 2022). The detailed action space are described in Table 4.\nTable 4 | The action space we use in Minecraft.\nIndex\nAction\nHuman Action\nDescription\n1\nForward\nkey W\nMove forward.\n2\nBack\nkey S\nMove backward.\n3\nLeft\nkey A\nStrafe left.\n4\nRight\nkey D\nStrafe right.\n5\nJump\nkey Space\nJump. When swimming, keeps the player afloat.\n6\nSneak\nkey left Shift\nSlowly move in the current direction of movement.\n7\nSprint\nkey left Ctrl\nMove quickly in the direction of current motion.\n8\nAttack\nleft Button\nDestroy blocks (hold down); Attack entity (click once).\n9\nUse\nright Button\nInteract with the block that the player is currently looking at.\n10\nhotbar.[1-9]\nkeys 1 - 9\nSelects the appropriate hotbar item.\n11\nYaw\nmove Mouse X\nTurning; aiming; camera movement.Ranging from -180 to +180.\n12\nPitch\nmove Mouse Y\nTurning; aiming; camera movement.Ranging from -180 to +180.\n13\nEquip\n-\nEquip the item in main hand from inventory.\n14\nCraft\n-\nExecute a crafting recipe to obtain new item.\n15\nSmelt\n-\nExecute a smelting recipe to obtain new item.\nB.3. Rules\nWe choose to conduct the test in survival mode of Minecraft 1.16.5. For each environment reset, we\nhave added the following rules:\n• \/difficulty peaceful: Set the difficulty of the environment to peaceful mode.\n• \/gamerule doDaylightCycle false: Set the environment to daytime forever.\n• \/gamerule keepInventory true: Set agent to not drop items upon death. We have added\na time limit for each task, within which if the player dies, they will respawn at the spawn point\nand retain their previous inventory contents.\n• \/effect give @a night_vision 99999 250 true: In order to facilitate the display\nof agent behavior, we have added night vision effects to the agent.\nC. Results and Details of 200+ tasks in Minecraft Universe Benchmark\nWe list the evaluation task set belows with details including task name, maximum steps, initial\ninventory, biome, and language instructions. We also show the evaluation times across different seeds\nand successful episodes rate. Note that all tasks are evaluated in Minecraft 1.16.5 Survival Mode.\n24\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 5 | The results of our agent on various tasks in the Wood group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nwooden_shovel\n12000\nnull\nPlains\/Forest\n0.9028\n72\nPick up a wooden_shovel given nothing.\nwooden_pickaxe\n12000\nnull\nPlains\/Forest\n0.9516\n62\nPick up a wooden_pickaxe given nothing.\nwooden_axe\n12000\nnull\nPlains\/Forest\n0.8909\n55\nPick up a wooden_axe given nothing.\nwooden_hoe\n12000\nnull\nPlains\/Forest\n0.9318\n44\nPick up a wooden_hoe given nothing.\nstick\n12000\nnull\nPlains\/Forest\n1\n86\nPick up a stick given nothing.\nwooden_sword\n12000\nnull\nPlains\/Forest\n0.9242\n66\nPick up a wooden_sword given nothing.\ncomposter\n12000\nnull\nPlains\/Forest\n0.7872\n47\nPick up a composter given nothing.\nbarrel\n12000\nnull\nPlains\/Forest\n0.7544\n57\nPick up a barrel given nothing.\ncrafting_table\n12000\nnull\nPlains\/Forest\n0.9706\n68\nPick up a crafting_table given nothing.\nchest\n12000\nnull\nPlains\/Forest\n0.9155\n71\nPick up a chest given nothing.\nladder\n12000\nnull\nPlains\/Forest\n0.9737\n76\nPick up a ladder given nothing.\nbowl\n12000\nnull\nPlains\/Forest\n0.9149\n47\nPick up a bowl given nothing.\noak_wood\n12000\nnull\nForest\n0.9868\n76\nPick up a oak_wood in Forest.\noak_slab\n12000\nnull\nForest\n0.9506\n81\nPick up a oak_slab in Forest.\noak_planks\n12000\nnull\nForest\n0.9659\n88\nPick up a oak_planks in Forest.\noak_log\n12000\nnull\nForest\n1\n65\nPick up a oak_log in Forest.\noak_button\n12000\nnull\nForest\n0.9153\n59\nPick up a oak_button in Forest.\noak_door\n12000\nnull\nForest\n0.8732\n71\nPick up a oak_door in Forest.\noak_fence\n12000\nnull\nForest\n0.8\n60\nPick up a oak_fence in Forest.\noak_fence_gate\n12000\nnull\nForest\n0.9322\n59\nPick up a oak_fence_gate in Forest.\noak_trapdoor\n12000\nnull\nForest\n0.8861\n79\nPick up a oak_trapdoor in Forest.\noak_boat\n12000\nnull\nForest\n0.9074\n54\nPick up a oak_boat in Forest.\noak_sign\n12000\nnull\nForest\n0.9\n40\nPick up a oak_sign in Forest.\nbirch_wood\n12000\nnull\nForest\n0.9474\n57\nPick up a birch_wood in Forest.\nbirch_slab\n12000\nnull\nForest\n0.9231\n65\nPick up a birch_slab in Forest.\nbirch_planks\n12000\nnull\nForest\n0.9714\n70\nPick up a birch_planks in Forest.\nbirch_log\n12000\nnull\nForest\n0.9833\n60\nPick up a birch_log in Forest.\nbirch_button\n12000\nnull\nForest\n0.9245\n53\nPick up a birch_button in Forest.\nbirch_door\n12000\nnull\nForest\n0.8431\n51\nPick up a birch_door in Forest.\nbirch_fence\n12000\nnull\nForest\n0.8\n30\nPick up a birch_fence in Forest.\nbirch_fence_gate\n12000\nnull\nForest\n0.9355\n62\nPick up a birch_fence_gate in Forest.\nbirch_trapdoor\n12000\nnull\nForest\n0.9524\n63\nPick up a birch_trapdoor in Forest.\nbirch_boat\n12000\nnull\nForest\n0.8906\n64\nPick up a birch_boat in Forest.\nbirch_sign\n12000\nnull\nForest\n0.9\n60\nPick up a birch_sign in Forest.\nTable 6 | The results of our agent on various tasks in the Stone group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nstone_shovel\n12000\niron_axe\nPlains\/Forest\n0.8514\n74\nCraft a stone_shovel given an iron_axe.\nstone_pickaxe\n12000\niron_axe\nPlains\/Forest\n0.9118\n68\nCraft a stone_pickaxe given an iron_axe.\nstone_axe\n12000\niron_axe\nPlains\/Forest\n0.9123\n57\nCraft a stone_axe given an iron_axe.\nstone_hoe\n12000\niron_axe\nPlains\/Forest\n0.9459\n74\nCraft a stone_hoe given an iron_axe.\nstone\n12000\niron_axe\nPlains\/Forest\n0.8413\n63\nCraft a stone given an iron_axe.\ncharcoal\n12000\niron_axe\nPlains\/Forest\n0.8947\n76\nCraft a charcoal given an iron_axe.\nsmoker\n12000\niron_axe\nPlains\/Forest\n0.7867\n75\nCraft a smoker given an iron_axe.\nstone_sword\n12000\niron_axe\nPlains\/Forest\n0.8831\n77\nCraft a stone_sword given an iron_axe.\nfurnace\n12000\niron_axe\nPlains\/Forest\n0.942\n69\nCraft a furnace given an iron_axe.\ntorch\n12000\niron_axe\nPlains\/Forest\n0.9\n30\nCraft a torch given an iron_axe.\n25\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 7 | The results of our agent on various tasks in the Iron group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\niron_axe\n12000\nnull\nPlains\/Forest\n0.3333\n60\nSmelt and craft an iron_axe.\niron_pickaxe\n12000\niron_axe\nPlains\/Forest\n0.3382\n68\nSmelt and craft an iron_pickaxe.\niron_shovel\n12000\niron_axe\nPlains\/Forest\n0.338\n71\nSmelt and craft an iron_shovel.\niron_sword\n12000\niron_axe\nPlains\/Forest\n0.3288\n73\nSmelt and craft an iron_sword.\niron_trapdoor\n12000\niron_axe\nPlains\/Forest\n0.3151\n73\nSmelt and craft an iron_trapdoor.\niron_door\n12000\niron_axe\nPlains\/Forest\n0.2836\n67\nSmelt and craft an iron_door.\niron_ingot\n12000\niron_axe\nPlains\/Forest\n0.5479\n73\nSmelt and craft an iron_ingot.\nbucket\n12000\niron_axe\nPlains\/Forest\n0.381\n42\nSmelt and craft a bucket.\nrail\n12000\niron_axe\nPlains\/Forest\n0.3226\n62\nSmelt and craft a rail.\nminecart\n12000\niron_axe\nPlains\/Forest\n0.2833\n60\nSmelt and craft a minecart.\nsmithing_table\n12000\niron_axe\nPlains\/Forest\n0.3611\n72\nSmelt and craft a smithing_table.\ntripwire_hook\n12000\niron_axe\nPlains\/Forest\n0.45\n60\nSmelt and craft a tripwire_hook.\nchain\n12000\niron_axe\nPlains\/Forest\n0.3729\n59\nSmelt and craft a chain.\niron_bars\n12000\niron_axe\nPlains\/Forest\n0.3208\n53\nSmelt and craft an iron_bars.\nhopper\n12000\niron_axe\nPlains\/Forest\n0.3077\n65\nSmelt and craft a hopper.\niron_nugget\n12000\niron_axe\nPlains\/Forest\n0.3582\n67\nSmelt and craft an iron_nugget.\nheavy_weighted_pressure_plate\n12000\niron_axe\nPlains\/Forest\n0.358\n81\nSmelt and craft a\nheavy_weighted_pressure_plate.\nblast_furnace\n12000\niron_axe\nPlains\/Forest\n0.5\n60\nSmelt and craft a blast_furnace.\nshears\n12000\niron_axe\nPlains\/Forest\n0.25\n64\nSmelt and craft a shears.\nstonecutter\n12000\niron_axe\nPlains\/Forest\n0.5\n60\nSmelt and craft a stonecutter.\niron_hoe\n12000\niron_axe\nPlains\/Forest\n0.3214\n56\nSmelt and craft an iron_hoe.\ncrossbow\n12000\niron_axe\nPlains\/Forest\n0.047\n63\nSmelt and craft a crossbow.\nTable 8 | The results of our agent on various tasks in the Gold group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\ngolden_pickaxe\n36000\niron_axe\nPlains\/Forest\n0.0526\n77\nSmelt and craft a golden_pickaxe.\ngolden_shovel\n36000\niron_axe\nPlains\/Forest\n0.0822\n73\nSmelt and craft a golden_shovel.\ngolden_sword\n36000\niron_axe\nPlains\/Forest\n0.0476\n85\nSmelt and craft a golden_sword.\ngolden_hoe\n36000\niron_axe\nPlains\/Forest\n0.058\n69\nSmelt and craft a golden_hoe.\ngolden_axe\n36000\niron_axe\nPlains\/Forest\n0.0469\n64\nSmelt and craft a golden_axe.\ngolden_apple\n36000\niron_axe\nPlains\/Forest\n0.02\n76\nSmelt and craft a golden_apple.\nclock\n36000\niron_axe\nPlains\/Forest\n0.02\n77\nSmelt and craft a clock.\ngold_nugget\n36000\niron_axe\nPlains\/Forest\n0.1444\n91\nSmelt and craft a gold_nugget.\ngold_ingot\n36000\niron_axe\nPlains\/Forest\n0.1449\n70\nSmelt and craft a gold_ingot.\nTable 9 | The results of our agent on various tasks in the Diamond group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\ndiamond_pickaxe\n36000\niron_axe\nPlains\/Forest\n0.0622\n692\nDig down to mine diamond and craft diamond_pickaxe.\ndiamond_shovel\n36000\niron_axe\nPlains\/Forest\n0.1136\n88\nDig down to mine diamond and craft diamond_shovel.\ndiamond_sword\n36000\niron_axe\nPlains\/Forest\n0.1134\n97\nDig down to mine diamond and craft diamond_sword.\ndiamond_hoe\n36000\niron_axe\nPlains\/Forest\n0.0441\n68\nDig down to mine diamond and craft diamond_hoe.\ndiamond_axe\n36000\niron_axe\nPlains\/Forest\n0.0986\n71\nDig down to mine diamond and craft diamond_axe.\ndiamond\n36000\niron_axe\nPlains\/Forest\n0.092\n728\nDig down to mine diamond and craft diamond.\njukebox\n36000\niron_axe\nPlains\/Forest\n0.1053\n79\nDig down to mine diamond and craft jukebox.\n26\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 10 | The results of our agent on various tasks in the Redstone group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\npiston\n36000\niron_axe\nPlains\/Forest\n0.1772\n79\nMine redstone and make piston.\nredstone_torch\n36000\niron_axe\nPlains\/Forest\n0.2584\n89\nMine redstone and make redstone_torch.\nredstone_block\n36000\niron_axe\nPlains\/Forest\n0.2469\n81\nMine redstone and make redstone_block.\nactivator_rail\n36000\niron_axe\nPlains\/Forest\n0.0159\n63\nMine redstone and make activator_rail.\ncompass\n36000\niron_axe\nPlains\/Forest\n0.0759\n79\nMine redstone and make compass.\ndropper\n36000\niron_axe\nPlains\/Forest\n0.2278\n79\nMine redstone and make dropper.\nnote_block\n36000\niron_axe\nPlains\/Forest\n0.2239\n67\nMine redstone and make note_block.\nTable 11 | The results of our agent on various tasks in the Blocks group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\ndiorite\n12000\niron_axe\nPlains\/Forest\n0.9\n30\nDig down to mine diorite block.\nandesite\n12000\niron_axe\nPlains\/Forest\n0.9667\n30\nDig down to mine andesite block.\ngranite\n12000\niron_axe\nPlains\/Forest\n0.8667\n30\nDig down to mine granite block.\ncoal\n12000\niron_axe\nPlains\/Forest\n0.6667\n30\nDig down to mine coal block.\nlapis_lazuli\n12000\niron_axe\nPlains\/Forest\n0.8667\n30\nDig down to mine lapis_lazuli block.\niron_ore\n12000\niron_axe\nPlains\/Forest\n0.5667\n30\nDig down to mine iron_ore block.\ngold_ore\n36000\niron_axe\nPlains\/Forest\n0.27\n30\nDig down to mine gold_ore block.\ncobblestone\n12000\niron_axe\nPlains\/Forest\n0.9667\n30\nDig down to mine cobblestone block.\ngravel\n12000\niron_axe\nPlains\/Forest\n0.9667\n30\nDig down to mine gravel block.\noak_log\n12000\niron_axe\nPlains\/Forest\n0.9667\n30\nChop down tree and mine oak_log block.\nbirch_log\n12000\niron_axe\nPlains\/Forest\n0.8718\n39\nChop down tree and mine birch_log block.\nacacia_log\n12000\niron_axe\nPlains\/Forest\n0.5\n30\nChop down tree and mine acacia_log block.\njungle_log\n12000\niron_axe\nPlains\/Forest\n0.9333\n30\nChop down tree and mine jungle_log block.\ndark_oak_log\n12000\niron_axe\nPlains\/Forest\n0.9\n30\nChop down tree and mine dark_oak_log block.\nspruce_log\n12000\niron_axe\nPlains\/Forest\n0.9333\n30\nChop down tree and mine spruce_log block.\nTable 12 | The results of our agent on various tasks in the Armor group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nshield\n12000\niron_axe\nPlains\/Forest\n0.3939\n66\nCraft shield and equip it.\nleather_helmet\n12000\niron_axe\nPlains\/Forest\n0.0508\n59\nCraft leather_helmet and equip it.\nleather_chestplate\n12000\niron_axe\nPlains\/Forest\n0.0312\n32\nCraft leather_chestplate and equip it.\nleather_leggings\n12000\niron_axe\nPlains\/Forest\n0.0588\n34\nCraft leather_leggings and equip it.\nleather_boots\n12000\niron_axe\nPlains\/Forest\n0.087\n23\nCraft leather_boots and equip it.\niron_chestplate\n12000\niron_axe\nPlains\/Forest\n0.3333\n30\nCraft iron_chestplate and equip it.\niron_boots\n12000\niron_axe\nPlains\/Forest\n0.3667\n30\nCraft iron_boots and equip it.\niron_leggings\n12000\niron_axe\nPlains\/Forest\n0.3788\n66\nCraft iron_leggings and equip it.\niron_helmet\n12000\niron_axe\nPlains\/Forest\n0.303\n33\nCraft iron_helmet and equip it.\ndiamond_helmet\n36000\niron_axe\nPlains\/Forest\n0.0429\n70\nCraft diamond_helmet and equip it.\ndiamond_chestplate\n36000\niron_axe\nPlains\/Forest\n0.0149\n68\nCraft diamond_chestplate and equip it.\ndiamond_leggings\n36000\niron_axe\nPlains\/Forest\n0.02\n73\nCraft diamond_leggings and equip it.\ndiamond_boots\n36000\niron_axe\nPlains\/Forest\n0.0533\n75\nCraft diamond_boots and equip it.\ngolden_helmet\n36000\niron_axe\nPlains\/Forest\n0.0533\n75\nCraft golden_helmet and equip it.\ngolden_chestplate\n36000\niron_axe\nPlains\/Forest\n0.02\n78\nCraft golden_chestplate and equip it.\ngolden_leggings\n36000\niron_axe\nPlains\/Forest\n0.0159\n89\nCraft golden_leggings and equip it.\ngolden_boots\n36000\niron_axe\nPlains\/Forest\n0.0617\n81\nCraft golden_boots and equip it.\n27\nJARVIS-1: Open-world Multi-task Agents with Memory-Augmented Multimodal Language Models\nTable 13 | The results of our agent on various tasks in the Decoration group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\nyellow_dye\n12000\niron_axe\nFlower Forest\n0.2333\n30\nObtain the yellow_dye.\nred_dye\n12000\niron_axe\nFlower Forest\n0.6364\n33\nObtain the red_dye.\nlight_gray_dye\n12000\niron_axe\nFlower Forest\n0.6667\n27\nObtain the light_gray_dye.\npink_dye\n12000\niron_axe\nFlower Forest\n0.6667\n39\nObtain the pink_dye.\norange_dye\n12000\niron_axe\nFlower Forest\n0.4857\n35\nObtain the orange_dye.\nwhite_dye\n12000\niron_axe\nFlower Forest\n0.1471\n34\nObtain the white_dye.\nwhite_bed\n12000\niron_axe\nFlower Forest\n0.5\n36\nObtain the white_bed.\nitem_frame\n12000\niron_axe\nFlower Forest\n0.2143\n28\nObtain the item_frame.\npainting\n12000\niron_axe\nFlower Forest\n0.5484\n31\nObtain the painting.\nwhite_wool\n12000\niron_axe\nFlower Forest\n0.8235\n34\nObtain the white_wool.\nwhite_carpet\n12000\niron_axe\nFlower Forest\n0.6857\n35\nObtain the white_carpet.\nwhite_banner\n12000\niron_axe\nFlower Forest\n0.0968\n31\nObtain the white_banner.\nyellow_wool\n12000\niron_axe\nFlower Forest\n0.0625\n32\nObtain the yellow_wool.\nred_wool\n12000\niron_axe\nFlower Forest\n0.6571\n35\nObtain the red_wool.\nlight_gray_wool\n12000\niron_axe\nFlower Forest\n0.6098\n41\nObtain the light_gray_wool.\npink_wool\n12000\niron_axe\nFlower Forest\n0.4\n25\nObtain the pink_wool.\norange_wool\n12000\niron_axe\nFlower Forest\n0.5\n36\nObtain the orange_wool.\nTable 14 | The results of our agent on various tasks in the Food group.\nTask\nMax.\nSteps\nInitial\nInventory\nBiome\nSuccess\nRate\nEval\nTimes\nLanguage Instruction\napple\n12000\niron_axe\nPlains\n0.5\n30\nChop down tree to obtain apple.\ncooked_chicken\n12000\niron_axe\nPlains\n0.3562\n73\nKill chicken to obtain chicken and cook it.\ncooked_mutton\n12000\niron_axe\nPlains\n0.4355\n62\nKill sheep to obtain mutton and cook it.\ncooked_porkchop\n12000\niron_axe\nPlains\n0.3968\n63\nKill pig to obtain porkchop and cook it.\ncooked_beef\n12000\niron_axe\nPlains\n0.2857\n63\nKill cow to obtain beef and cook it.\nchicken\n12000\niron_axe\nPlains\n0.5667\n30\nKill chicken to obtain chicken.\nbeef\n12000\niron_axe\nPlains\n0.6333\n30\nKill cow to obtain beef.\nmutton\n12000\niron_axe\nPlains\n0.5667\n30\nKill sheep to obtain mutton.\nporkchop\n12000\niron_axe\nPlains\n0.4667\n30\nKill pig to obtain porkchop.\n28\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models.pdf"}
{"title":"Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds","authors":"Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu","summary":"Recent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact\nwith the world, which marks an initial step toward versatile robotics. However,\nthese efforts tend to overlook the visual richness of open worlds, rendering\nthe entire interactive process akin to \"a blindfolded text-based game.\"\nConsequently, LLM-based agents frequently encounter challenges in intuitively\ncomprehending their surroundings and producing responses that are easy to\nunderstand. In this paper, we propose Steve-Eye, an end-to-end trained large\nmultimodal model designed to address this limitation. Steve-Eye integrates the\nLLM with a visual encoder which enables it to process visual-text inputs and\ngenerate multimodal feedback. In addition, we use a semi-automatic strategy to\ncollect an extensive dataset comprising 850K open-world instruction pairs,\nempowering our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout extensive experiments from a wide range of perspectives to validate our\nmodel's capability to strategically act and plan. Codes and datasets will be\nreleased.","url":"http:\/\/arxiv.org\/abs\/2310.13255v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2310.13255v2","published":1697772125000,"comment":"19 pages, 19 figures","pdf_text":"Preprint\nSTEVE-EYE:\nEQUIPPING LLM-BASED EMBOD-\nIED AGENTS WITH VISUAL PERCEPTION IN OPEN\nWORLDS\nSipeng Zheng1, Jiazheng Liu2, Yicheng Feng2, Zongqing Lu1,2†\n1 Beijing Academy of Artificial Intelligence\n2 School of Computer Science, Peking University\nspzheng@baai.ac.cn\nfyc813@pku.edu.cn\nzongqing.lu@pku.edu.cn\nABSTRACT\nRecent studies have presented compelling evidence that large language models\n(LLMs) can equip embodied agents with the self-driven capability to interact with\nthe world, which marks an initial step toward versatile robotics. However, these\nefforts tend to overlook the visual richness of open worlds, rendering the entire\ninteractive process akin to “a blindfolded text-based game.” Consequently, LLM-\nbased agents frequently encounter challenges in intuitively comprehending their\nsurroundings and producing responses that are easy to understand. In this paper,\nwe propose Steve-Eye, an end-to-end trained large multimodal model to address\nthis limitation. Steve-Eye integrates the LLM with a visual encoder to process\nvisual-text inputs and generate multimodal feedback. We adopt a semi-automatic\nstrategy to collect an extensive dataset comprising 850K open-world instruction\npairs, enabling our model to encompass three essential functions for an agent:\nmultimodal perception, foundational knowledge base, and skill prediction and\nplanning. Lastly, we develop three open-world evaluation benchmarks, then carry\nout experiments from a wide range of perspectives to validate our model’s capa-\nbility to strategically act and plan. The project’s website and code can be found at\nhttps:\/\/sites.google.com\/view\/steve-eye.\n1\nINTRODUCTION\nHi Steve, how to craft a \nwooden axe in Minecraft ?\n{\"type\": \"minecraft:crafting_shaped\",\n\"category\": \"equipment\",\n\"key\": {\n\"#\": {\"item\": \"minecraft:stick\" },\n\"X\": {\"tag\": \"minecraft:planks\"}\n},\n\"pattern\": [\"XX\", \"X#\", \" #\"],\n\"result\": {\n\"item\": \"minecraft:wooden_axe\"},\n\"show_notification\": true}\n…? I don’t understand.\nOk, I got it.\nHi Steve, you are given an\nexample to generate a plan\nlist for task […], Now I\ndescribe your surrounding\nenvironment and your in-\nventory status […], please\nmake a plan to craft stick.\n…… ?\nSure, here is the plan list \nto craft stick: (1) find log\nnearby, (2) log, (3) craft \nplanks, (4) craft stick.\nWell, this is what you see.\nplease make a plan to craft stick\n(a)\n(b)\n×\n√\nHi Steve, how to craft a \nwooden axe in Minecraft ?\n×\n√\nFigure 1. (a) LLM-based agent’s feedback is un-\ncontrollable due to the uncertainty of input textual\nprompt, while visual cues can benefit the agent to\ngenerate feedbacks; (b) a text-only driven agent of-\nten finds it difficult to produce intuitive feedback\nthat humans can easily understand.\nDeveloping embodied agents that can adapt\nto the open world has long been a sub-\nstantial challenge (Kolve et al., 2017; Savva\net al., 2019).\nRecently, the rapid progress\nof large language models (LLMs) (OpenAI,\n2022; Touvron et al., 2023a) has shown their\npotential to serve as a general-purpose assis-\ntant. Driven by these pre-trained LLMs, re-\ncently proposed agents (Yuan et al., 2023;\nWang et al., 2023a;b; Zhu et al., 2023) have\nmanaged to extract world knowledge and rea-\nsoning capabilities from LLMs, allowing them\nto become self-driven. Thereby these agents\nare capable of generating executable policies\nor plans for a wide range of skills and tasks in\nan open-ended world.\nWhile current attempts to integrate LLMs\nshow promise in developing a generic embod-\nied agent, these efforts primarily translate the\nentire world into text, which overlooks the\n†Corresponding author\n1\narXiv:2310.13255v2  [cs.CV]  7 Dec 2023\nPreprint\nmultifaceted richness of diverse visual reality\nand turns interacting with the environment into something akin to “a blindfolded text-based game.”\nConsequently, such text-only agents often face difficulties when it comes to effectively and intu-\nitively representing the world. Imagine a situation where you request your agent to shop for a pair\nof shoes. Would you prefer to send the agent a picture of the shoes or provide a lengthy description\nof the shoes to convey their appearance? Undoubtedly, you would opt for the former choice.\nIn fact, the agent’s reliance on text input\/output (I\/O) imposes significant limitations on its ability to\ninteract with the world. To illustrate this point, we consider Minecraft (Guss et al., 2019; Fan et al.,\n2022) as an ideal example. Minecraft, being an expansive sandbox game, offers a vast realm for\nembodied agents to explore, which requires the acquisition of various basic skills (e.g., crafting logs)\nand the ability to plan and execute diverse tasks. First, as shown in Figure 1 (a), the LLM-based agent\nproduces uncontrollable outputs. The success of the agent’s responses hinges heavily on careful\nprompt engineering (Huang et al., 2022b), ensuring that the LLM comprehends the environment\nand task objectives. Moreover, a universally applicable prompt that suits every LLM and task is an\nunattainable goal. Therefore, this prompting process is labor-intensive and contradicts our aim of\nenabling agents to act in a self-driven manner. Second, when compared to visual feedback, language\noften encounters difficulties in intuitively conveying specific world concepts (e.g., recipes) to users,\nas illustrated in Figure 1 (b), thereby unavoidably creating obstacles for robust human-computer\/AI\ninteraction (Preece et al., 1994; Fallman, 2003).\nUnlike LLMs, humans possess an innate ability to process and generate information through both\nvisual and text channels. This inherent gift significantly enhances our capability to interact with\nthe world. However, the coupling of LLM-based agents with multimodal I\/O has been relatively\nunderexplored in an open-ended environment. To fill this gap, we introduce Steve-Eye\n, a large\nmultimodal model that enables LLM-based embodied agents to engage with the open world via\nvisual-text interfaces. Steve-Eye excels at producing responses that demonstrate a comprehensive\ngrasp of the environment, common-sense reasoning, and executable skill plans. To achieve this,\nSteve-Eye is equipped with three indispensable functions: (1) multimodal perception; (2) founda-\ntional knowledge base; and (3) skill prediction and planning. In this paper, we choose Minecraft as\nour validation platform considering its vast sandbox world and the high degree of freedom. More\nenvironments can also be considered, e.g., Virtual Home (Puig et al., 2018), AI2THOR (Kolve\net al., 2017). Due to the space limit, we discuss the exploration of more generic environments in\nAppendix A.4 and leave it as our future work. Our contributions can be summarized as follows:\nOpen-World Instruction Dataset.\nWe construct an extensive instruction dataset to train Steve-\nEye for the acquisition of three mentioned functions. The instruction data contains not only the\nagent’s per-step status and environmental features but also the essential knowledge for agents to act\nand plan. However, collecting such a dataset in an open world can be a costly endeavor, especially\nwhen aiming to gather fine-grained and diverse labels. As a result, previous studies (Fan et al.,\n2022) have often relied on readily available unsupervised data (e.g., video-subtitle pairs) for pre-\ntraining. In these approaches, the agent’s comprehension of its status and environment is implicitly\nlearned through self-supervised techniques, while its foundational knowledge is directly derived\nfrom general-purpose LLMs. In contrast, our work involves curating multimodal instructional data\nspecifically designed for open-ended embodied agents, by utilizing ChatGPT (OpenAI, 2022).\nLarge Multimodal Model and Training.\nSteve-Eye combines a visual encoder which converts\nvisual inputs into a sequence of embeddings, along with a pre-trained LLM which empowers em-\nbodied agents to engage in skill or task reasoning in an open world. During the training process, we\nemploy a two-stage strategy similar to Liu et al. (2023). This strategy commences with the align-\nment of multimodal elements between the visual encoder and the large language model, followed\nby the instruction tuning through our constructed dataset.\nOpen-World Benchmarks.\nWe carry out extensive experiments to demonstrate that our pro-\nposed Steve-Eye outperforms LLM-based agents in open-world setups. Specifically, we develop the\nfollowing benchmarks to evaluate agent performance from a broad range of perspectives: (1) envi-\nronmental visual captioning (ENV-VC), which assesses an agent’s capacity to perceive and describe\nits surroundings effectively; (2) foundational knowledge question answering (FK-QA), which eval-\nuates the proficiency in mastering basic knowledge crucial for an agent’s decision-making; (3) skill\nprediction and planning (SPP), which quantifies an agent’s capability to act and plan strategically.\n2\nPreprint\n2\nRELATED WORK\n2.1\nOPEN-WORLD EMBODIED AGENTS WITH LLMS\nThe rapid progress of large language models (Brown et al., 2020; Raffel et al., 2020; Zhang et al.,\n2022; Chowdhery et al., 2022) has significantly boosted their capacity to encode a wide range of\nhuman behaviors within training data (Bommasani et al., 2021). When equipped with narrowly de-\nsigned prompts, LLM-based agents exhibit the capability to generate executable plans for tasks such\nas indoor robot manipulation. For instance, SayCan (Ahn et al., 2022) integrates skill affordances\nwith LLMs to yield actionable plans, while Palm-E (Driess et al., 2023) takes a step further by\nconstructing hierarchical agents capable of handling multimodal prompts. This approach has also\nproven its efficacy in open-world environments (Huang et al., 2022a; Li et al., 2022). In contrast\nto robot manipulation, agents in the wild require a heightened level of real-time situational aware-\nness and foundational knowledge to execute intricate skill plans across a diverse array of tasks. To\nsimulate human behaviors in such open worlds, Generative Agents (Park et al., 2023) store agents’\nexperiences and retrieve these memories to generate plans in a text-based sandbox game.\nIn recent years, the 3D sandbox Minecraft has received considerable attention owing to its remark-\nably flexible game mechanics to serve as a prominent open-world benchmark (e.g., MineRL (Guss\net al., 2019) and Minedojo (Fan et al., 2022)). DEPS (Wang et al., 2023b) introduces the descrip-\ntor, explainer, and selector for plan generation with the help of LLM. Plan4MC (Yuan et al., 2023)\nconstructs a skill graph and proposes a skill search algorithm to minimize planning errors. Voy-\nager (Wang et al., 2023a) proposes an LLM-powered lifelong learning agent that continually ex-\nplores the Minecraft world. Similar to (Park et al., 2023), GITM (Zhu et al., 2023) integrates LLMs\nwith text-based memory and knowledge to create generic agents in Minecraft. Among these studies,\nVoyager (Wang et al., 2023a) and GITM (Zhu et al., 2023) lean entirely on text descriptions of the\nenvironment to act and plan, while Plan4MC (Yuan et al., 2023) and DEPS (Wang et al., 2023b)\nhave visual-input skills but still rely on merely text for planning. None of them try to understand the\nrich visual observation provided natively by Minecraft. In contrast to these works, our work trains a\nlarge multimodal model to fill this gap.\n2.2\nLARGE MULTIMODAL MODELS (LMMS)\nIn comparison to LLMs, large multimodal models (LMMs) (Awadalla et al., 2023) encompass\na broad range of information beyond text modality, which can be categorized into two primary\nstreams. The first category (Gupta & Kembhavi, 2023; Huang et al., 2023a; Patil et al., 2023; Sur´ıs\net al., 2023) involves hinging on ChatGPT (OpenAI, 2022) or GPT-4 (OpenAI, 2023) to generate\nin-context responses without parameter tuning. However, these approaches heavily rely on the avail-\nability of an LLM’s API and the quality of the designed prompts. The second category comprises\nend-to-end pre-trained models. Within this category, models such as Huang et al. (2023b); Peng et al.\n(2023) are trained entirely from scratch. Conversely, some research explores efficient fine-tuning us-\ning pre-trained LLMs by incorporating lightweight modality encoders, such as Qformer (Li et al.,\n2023) or Perceiver (Alayrac et al., 2022). Recently, Liu et al. (2023) propose to explicitly instruction-\ntune a LLM using vision-language instruction data.\nIn this work, we propose Steve-Eye by building upon pre-trained LLMs, aiming to develop an open-\nworld agent powered by a large-scale model with versatile multimodal I\/O capabilities.\n3\nMETHODOLOGY\nIn this section, we first provide our instruction-following dataset to develop three key functions for\nthe agent’s open-world interaction in Section 3.1. We then propose our large multimodal agent\nSteve-Eye in Section 3.2, and clarify details of the training procedure in Section 3.3. We adopt\nMinecraft as our open-ended platform in this paper to collect data and validate the model, anticipat-\ning to explore a broader range of environments for Steve-Eye in future studies.\nTo empower an agent with the self-driven capacity to act and plan in an open world, we posit that the\nfollowing embodied functions are indispensable: (1) multimodal perception function which offers\na detailed description of the agent status and environmental features; (2) foundational knowledge\n3\nPreprint\nbase which imparts an understanding of how the world works and conveys crucial basic knowledge\nrelated to skills and tasks; (3) skill prediction and planning which is responsible for generating\nskill execution feedback (e.g., success or failure) and crafting high-level skill plans for handling\nmore complex and long-horizon tasks. We develop these functions by building the corresponding\ninstruction dataset to pre-train Steve-Eye as follows.\n3.1\nOPEN-WORLD INSTRUCTION-FOLLOWING DATASET\nMultimodal Perception Instructions.\nHuman players can perform actions in Minecraft mainly\nrelying on their visual perception, without any prior hints or imposed game judgments. In order to\nendow Steve-Eye with the same ability, it is required to provide it with comprehensive visual descrip-\ntions of the environment. To achieve this, we use Minedojo (Fan et al., 2022) to obtain Minecraft\nsnapshots which contain a wide array of details within the agent’s surroundings, including environ-\nmental features, the agent’s life and food status, inventory items, and equipment, as illustrated in\nFigure 2. In addition, we leverage MaskCLIP (Zhou et al., 2022) to identify the in-sight objects of\nthese snapshots without supervised annotations. During our data collection process, for each snap-\nshot I and its corresponding description XC, we initiate a three-step approach. Firstly, we prompt\nChatGPT to curate a list of 40 instructions as shown in Figure 6 in Appendix A.1.1. Then we enrich\nsnapshot details as dense caption to describe its content, with the assistance of ChatGPT. Finally,\nwe select an instruction XQ randomly from the list and combine it with the snapshot’s caption to\ncreate a single-round multimodal description pair (e.g., ### Human: XQ I\\n ### Embodied Agent:\nXC\\n.). By doing so, we collect 200K instructional pairs for multimodal perception learning.\n[Description] Steve is walking in the forest and\nhe can’t see the sky. There are trees in front of\nhim. He still has 20.0 life and 20.0 food. He is\nnow equipped with the iron pickaxe and the\ninventory contains 5 pickaxes, 6 ……\n•\nenvironment\n•\nlife and food\n•\nobject in sight\n•\ninventory and \nequipment\nFigure 2. Multimodal perception\nFoundational Knowledge Instructions.\nEmbodied agents\nrequire a foundation of essential knowledge to facilitate action-\ntaking and skill planning.\nIn Minecraft, such knowledge\nshould contain item recipes, details of item attributes, their as-\nsociated numerical value, etc. We access this vital information\nfrom Minecraft-Wiki (Fandom, 2023), which comprises an ex-\ntensive collection of over 9,000 HTML pages. To be specific,\nwe first obtain all item icons from Minecraft-Wiki and gener-\nate 200K icon inventory images, as illustrated in Figure 3 (a).\nEach icon image corresponds to a 4-row table with an associ-\nated caption adhering to a standardized template: “There is a\nMinecraft inventory with 4 rows. From left to right, they are\n...”. As shown in Figuire 7 in Appendix A.1.2, we curate a set of 20 distinct prompts designed to\nchallenge the model’s ability to recognize items. Subsequently, we further collect all recipe-related\ninformation from the Wiki as illustrated in Figure 3 (b), and design similar prompt templates to for-\nmulate 10,000 recipe-image instructional pairs. Lastly, we process the Wiki and utilize this corpus\nto produce 40,000 single-round question-answer pairs. In total, we collect a high-quality dataset\nwith 250K foundational knowledge instructions.\n(a) Item icons\n(b) recipes\nFigure 3. Icons and recipes\nSkill-related Interaction Instructions.\nThe environmen-\ntal description and foundational knowledge serve as prerequi-\nsites for an agent’s interaction within the open world. How-\never, a successful interaction requires more than these ele-\nments alone. It relies upon the mastery of basic skills, such\nas log, harvesting, and food preparation, as well as high-level\nskill planning abilities to tackle complex, long-horizon tasks,\nsuch as crafting an iron pickaxe. To facilitate this, we gather\ncorresponding training data for skill prediction and planning,\nwhich enables our model to provide correct feedback on both\nbasic skills and long-horizon tasks across a spectrum of agent\nor environmental conditions. Specifically, the data collection\nprocess involves two steps. First, we sample skill trajecto-\nries based on the pre-trained basic skill policies and collect\n200K snapshot pairs with corresponding statuses from these\ntrajectories. Each snapshot pair {I0, It} denotes the 0-th and\nt-th timestamp of the skill trajectory. Next, we employ Chat-\nGPT to generate question-answer pairs about diverse aspects of skill execution status. These ques-\n4\nPreprint\nHow to craft a wooden axe? \nAnswer me via a recipe image.\nWhat are the ingredients required \nto craft a detector rail?\nProvide a detailed description of \nthe given Minecraft image <image>. \n…\n…\n<image> \nMake a plan list to finish the task \nof <image> in Minecraft. \n<image> \nIron Ingot, Stone Pressure \nPlate, and Redstone Dust.\nFrom left to right, there are 1 \nshears in the inventory, which \nmeans he is equipped with 1 \nshears. Steve still has 20.0 life \nand 20.0 food.\nThe executable plan can be: (1) \nfind log nearby; (2) crafting log; \n(3) crafting planks; (4) crafting \nstick.\nvisual \ntokenizer\ntext \ntokenizer\nprojector\nmultimodal input\n…\n…\nmultimodal output\nvisual generation\nquestion-answering\nvisual captioning\nskill planning\n<image>\nLarge Language Model\nFigure 4. Illustration of Steve-Eye: a large multimodal model designed to seamlessly process both\nvisual and language inputs. Steve-Eye excels in acquiring fundamental knowledge of the world it\nlives in, understanding the nuances of its surroundings, and generating executable plans to complete\na wide array of open-ended tasks. Furthermore, Steve-Eye responds to user instructions through\neither visual or text-based cues, enhancing the convenience and flexibility of human-AI interaction.\ntions delve into whether the agent completes the skill, encounters unexpected failures, or seeks\nexplanations for such failures. More details can be found in Appendix A.1.3. Second, we sam-\nple 40K task trajectories using the planner in Yuan et al. (2023), each of which can be denoted as\nT = {s1, s2, ...sT} representing the task is finished via a T-round planning procedure, where si is\nthe skill plan for i-th round. At each round i, we feed our model with its start snapshot and task\ninitialization, and curate instructional questions to inquire about si with reasonable explanation. In\nthis manner, we obtain 200K instructional pairs from task trajectories.\n3.2\nMODEL ARCHITECTURE\nFigure 4 illustrates the overall architecture of our proposed model. Steve-Eye, functioning as a\ngenerative model, connects an image-oriented tokenizer fv with the pre-trained LLM backbone Θ.\nWe adopt the image tokenizer, e.g., VQ-GAN (Esser et al., 2021), to encode the raw images I into\ntoken embeddings V = {v1, v2, ..., vn} ∈Rn×d, where n denotes the number of visual tokens and\nd is the dimensionality of each token. We further utilize a lightweight projection module fl with\na trainable projection matrix W. This module maps the visual tokens to the same space with text\nembeddings, yielding ˆV = {ˆv1, ˆv2, ..., ˆvn} ∈Rn× ˆd:\n  \\ hat  {\\ma t h cal {V}} = W \\mathcal {V}; \\hspace {0.3em} \\text {where} \\hspace {0.3em} \\mathcal {V} = f_v(I). \n(1)\nTo effectively process visual-language inputs and generate corresponding outputs, our model inte-\ngrates the visual codebook Cv into the pre-existing language vocabulary Cl. This integration leads\nto the formation of a unified multimodal codebook, denoted as Cm = Cv ∪Cl. Additionally, in\norder to mark the starting and ending points of visual elements in I\/O sequences, we introduce two\nspecial tokens, namely <vis> and <\/vis>. The LLM backbone Θ of our Steve-Eye is built upon a\ndecoder-only architecture with casual transformers. Our model employs an auto-regressive predic-\ntion mechanism, generating responses based on the provided multimodal input tokens. The resulting\nresponse is a mixed sequence of visual and textual tokens, represented as Y = {y1, y2, ..., ym}. For\neach embedding yi, we pass it through a linear layer fp followed by a softmax operation, mapping\nit into a probability distribution of the multimodal vocabulary. The final prediction for the i-th token\nzi is determined by selecting the token from the multimodal codebook with the highest score:\n  z _i = \\argmax ( \\text {softmax}(f_p(y_i))). \n(2)\n3.3\nTRAINING\nEach instruction-following instance can be formulated as a multi-round conversation {X 1\nQ, X 1\nC, ...,\nX N\nQ , X N\nC }, where each {X i\nQ, X i\nC} represents a question-answer interaction between a human and\n5\nPreprint\nthe embodied agent and N indicates the total number of rounds in the conversation. The entire\ninstructional dataset follows this unified template, as demonstrated in Figure 11 in Appendix A.1.3.\nTo efficiently train our model, we employ the negative log-likelihood objective over the prediction\ntokens with instruction tuning:\n  \\m a t\nh\nc\nal \n{L} (\\Theta )=-\\sum _{j=1}^{L} \\log P_{\\Theta }(y_j|\\mathcal {I}, \\hat {y}_{1:j-1}), \n(3)\nwhere y and ˆy respectively denote the input and target token sequences, with Θ representing the\nmodel parameters, and L representing the length of the target sequence. The input visual content\nI may represent an empty image depending on the input instruction. It is worth noting that we\nconstrain the loss computation to only consider the answer tokens XC. This constraint prevents\ntraining from becoming excessively straightforward and ensures that the model’s primary focus is\non learning to precisely generate coherent responses. Similar to Liu et al. (2023), we adopt a two-\nstage instruction-tuning strategy to train our model:\nTwo-Stage Instruction-Tuning. (1) Multimodal feature alignment: In the first stage, our pri-\nmary objective is to align visual features with the language token space. In order to strike a bal-\nance between efficient tuning and a comprehensive coverage of the world’s concepts, we curate our\nopen-ended instruction dataset to 600K snapshot-text pairs. These pairs are then transformed into\ninstruction-following data as described in Section 3.1. During the feature alignment stage, we main-\ntain the visual encoder and the LLM parameters in a frozen state, exclusively training the projection\nmodule. Additionally, this training phase involves fine-tuning token embeddings to accommodate\nthe newly introduced visual codebook and two special tokens <vis> and <\/vis>. (2) End-to-end\ninstruction tuning: In the second stage, we continue to keep the visual encoder frozen while concur-\nrently training the projection module and LLM. This second stage leverages the entire open-ended\ninstructions and contributes significantly to enhancing the model’s capability of comprehending and\neffectively responding to complex multimodal instructions.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nImplementation Details.\nIn this paper, we use the LLaMA-2 model (Touvron et al., 2023b)\nas the LLM backbone. Additionally, we use CLIP (Radford et al., 2021) as our visual encoder\nto achieve the best performance for non-visual generative tasks, and use VQ-GAN (Esser et al.,\n2021) as the default visual tokenizer for visual generation. The size of visual codebook Cv and\nlanguage vocabulary is 8192 and 32000, respectively. In addition, we add <vis> and <\/vis> to the\nfinal unified codebook, indicating the starting and ending points of visual content. Similar to Liu\net al. (2023), we construct 850K instruction-answer pairs for model training. Note that the model\nis trained to predict the agent’s answer, and thus only sequence\/tokens of answer will be used to\ncompute the loss in the auto-regressive model. We also adopt LoRA (Hu et al., 2021) to reduce the\ncomputational cost for efficient tuning. We choose MineDojo (Fan et al., 2022) as the Minecraft\nplatform to collect our instruction data and conduct experiments. Following Yuan et al. (2023),\nwe use the environments of programmatic tasks to train basic policies with RL. These policies are\ntrained to execute corresponding skills and keep fixed in all testing tasks.\nEvaluation Benchmarks.\nWe conduct experiments on three benchmarks to evaluate an agent’s\ninteraction ability in an open world. (1) Environmental visual captioning (ENV-VC): given a\nsnapshot, the model is asked to describe the agent’s current status and environmental features from\ndiverse aspects (e.g., life, food...). We evaluate the prediction’s accuracy of each aspect by ex-\ntracting corresponding answers from the output description to compare with the groundtruth. (2)\nFoundational knowledge question answering (FK-QA): to assess the model’s grasp of essential\nknowledge, we collect a set of 10,000 Minecraft-related questions from different sources, including\nthe Wiki pages, Wiki tables, and Minecraft recipes. The performance is measured by the model’s\nability to provide correct answers to these questions. (3) Skill prediction and planning (SPP): we\nutilize our proposed Steve-Eye to predict whether a skill has been successfully completed and assert\nits capability to generate executable high-level skill plans for long-horizon tasks.\n6\nPreprint\nTable 1. Comparisons of different model settings on the environmental visual caption benchmark.\nThe experiments are conducted on 20K ENV-VC test set.\nModel\nvisual encoder\ninventory\nequip\nobject in sight\nlife\nfood\nsky\nBLIP-2\nCLIP\n41.6\n58.5\n64.7\n88.5\n87.9\n57.6\nLlama-2-7b\n-\n-\n-\n-\n-\n-\n-\nSteve-Eye-7b\nVQ-GAN\n89.9\n78.3\n87.4\n92.1\n90.2\n68.5\nSteve-Eye-13b\nMineCLIP\n44.5\n61.8\n72.2\n89.2\n88.6\n68.2\nSteve-Eye-13b\nVQ-GAN\n91.1\n79.6\n89.8\n92.7\n90.8\n72.7\nSteve-Eye-13b\nCLIP\n92.5\n82.8\n92.1\n93.1\n91.5\n73.8\nTable 2. Comparisons of different data configurations on the environmental visual captioning bench-\nmark, where “snapshot desc.” denotes the 200K multimodal perception instruction dataset.\ninventory\nequip\nobject in sight\nlife\nfood\nsky\nno instruction tuning\n22.7\n24.3\n39.8\n81.2\n80.4\n61.1\nw\/o snapshot desc.\n46.2 (+23.5)\n40.9 (+16.6)\n41.2 (+1.4)\n83.0 (+1.8)\n82.4 (+2.0)\n63.3 (+2.1)\nw\/o icon images\n52.3 (+29.6)\n48.1 (+23.8)\n91.4 (+51.6)\n92.5 (+11.3)\n90.9 (+10.5)\n73.5 (+12.4)\nfull data\n92.5 (+69.8)\n82.8 (+58.5)\n92.1 (+52.3)\n93.1 (+11.9)\n91.5 (+11.1)\n73.8 (+12.7)\n4.2\nENVIRONMENTAL VISUAL CAPTIONING (ENV-VC)\nWe introduce this evaluation protocol for asserting Steve-Eye’s multimodal perception function,\nwhich serves as an initial stride toward comprehensive evaluation of large multimodal models.\nSpecifically, we collect 20,000 Minecraft snapshots (named ENV-VC test set) using Minedojo and\napply the proposed data generation pipeline to create six questions for each snapshot, resulting in a\ntotal of 120K questions. These six questions pertain to the prediction of various aspects, including\ninventory items\n, equipment\n, objects in sight\n, life\n, food\n, and the visibility of sky\n.\nDuring the inference phase, Steve-Eye predicts answers based on these questions and the input\nsnapshot. Experimental results are presented in Table 1 and Table 2. As shown in Table 1, our\nvisual encoder, when combined with multimodal instruction tuning, significantly enables the ability\nof the text-only language model LLM (Llama-2-7b) to comprehend the contents of the snapshots\n(Steve-Eye-7b). Notably, Steve-Eye outperforms BLIP-2 by a substantial margin due to the im-\nproved reasoning ability enabled by the larger LLM. Furthermore, the visual encoder plays a crucial\nrole in facilitating multimodal understanding. Surprisingly, the model equipped with CLIP (Radford\net al., 2021) surpasses the performance of the model using MineCLIP (Fan et al., 2022), achieving\nover +48.9%, +21.0% and +19.9% improvements in inventory, equipment, and object-in-sight pre-\ndictions, respectively. We attribute this performance difference to the fact that MineCLIP does not\nprioritize fine-grained alignment during pre-training, despite being exposed to a diverse range of\nMinecraft videos. In summary, Steve-Eye’s ability to comprehend visual cues from its surroundings\nlays the foundation for subsequent interactions with the world.\nTo investigate the effictiveness of various types of instructional data for multimodal perception, we\ncarry out experimental comparisons with diverse data configurations in Table 2. First, our results\nshowcase a significant improvement in the model’s capacity to respond to instructional questions\nthrough instruction tuning, which leads to impressive gains of over +50% for inventory, equipment,\nand object-in-sight prediction. Furthermore, the inclusion of the multimodal perception dataset and\nicon images in the training data both contribute to a substantial improvement in the model’s overall\nperformance. Ultimately, the best results are achieved when combining all available data sources.\n4.3\nFOUDATIONAL KNOWLEDGE QUESTION ANSWERING (FK-QA)\nFollowing Team (2022), we establish a question database specialized to assess our model’s pro-\nficiency in generating responses pertaining to fundamental Minecraft knowledge. This evaluation\nis carried out through a validation dataset known as the FK-QA test set, which is further divided\ninto two distinct subsets: TEXT and IMG. In the FK-QA TEXT subset, we generate a collection\nof 10,000 question-answer pairs curated from various sources, including the Minecraft-Wiki pages,\nMinecraft-Wiki tables, and Minecraft recipes. Each category comprises 2,000, 5,000, and 3,000\npairs, respectively. Upon receiving a response from Steve-Eye, we feed both the generated response\n7\nPreprint\nTable 3. Comparisons on FK-QA test set of the foundational knowledge question answering bench-\nmark. The evaluation metrics consider both the scoring and accuracy dimensions simultaneously.\nScoring\nAccuracy\nWiki Page\nWiki Table\nRecipe\nTEXT All\nTEXT\nIMG\nLlama-2-7b\n6.90\n6.21\n7.10\n6.62\n37.01%\n-\nLlama-2-13b\n6.31 (-0.59)\n6.16 (-0.05)\n6.31 (-0.79)\n6.24 (-0.38)\n37.96%\n-\nLlama-2-70b\n6.91 (+0.01)\n6.97 (+0.76)\n7.23 (+0.13)\n7.04 (+0.42)\n38.27%\n-\ngpt-turbo-3.5\n7.26 (+0.36)\n7.15 (+0.94)\n7.97 (+0.87)\n7.42 (+0.80)\n41.78%\n-\nSteve-Eye-7b\n7.21 (+0.31)\n7.28 (+1.07)\n7.82 (+0.72)\n7.54 (+0.92)\n43.25%\n62.83%\nSteve-Eye-13b\n7.38 (+0.48)\n7.44 (+1.23)\n7.93 (+0.83)\n7.68 (+1.06)\n44.36%\n65.13%\nand the corresponding groundtruth answer to ChatGPT. ChatGPT will first examine the accuracy of\nthe response as a measure of answer correctness. To minimize variability in error, ChatGPT con-\nducts a further evaluation, considering the response’s accuracy, relevance, and level of detail. This\ncomprehensive evaluation yields an overall score on a scale ranging from 0 to 10, where a higher\nscore signifies superior overall performance. In the FK-QA IMG subset, we shift our focus to visual\ngeneration by employing 3,000 recipe images as groundtruth data. Here, our model is tasked with\ngenerating visual outputs for each item within the recipe inventory, following a specific order. The\nvisual output is considered correct only if every element of the recipe is accurately generated. We\nadopt this metric to assert our model’s ability to produce multimodal feedback.\nTable 3 presents both scoring and accuracy results. It’s worthy to note that Llama-2 exhibits consis-\ntent performance regardless of the model’s scale, with Llama-2-70b only marginally outperforming\nthe 7b-version by +1.26% in accuracy, meanwhile 13b-version performs even worse than 7b-version\non the scoring results. We hypothesize that this phenomenon can be attributed to distinct variations\nin difficulty levels encountered within our FK-QA test set. Llama-2 fails to answer correctly for the\nchallenging part regardless of its size due to essential knowledge missing. In contrast, Steve-Eye\noutperforms both Llama-2 and gpt-turbo-3.5, despite its considerably smaller scale. Furthermore,\nour model exhibits a more substantial improvement in responding to Recipe and Wiki Table ques-\ntions as compared to Wiki Page questions. This disparity can likely be attributed to the fact that\nWiki Page contains a large proportion of invalid questions (e.g., version, history), whereas Recipe\nand Wiki Table predominantly feature knowledge-related inquiries. Such result further validates the\neffectiveness of our approach in acquiring foundational knowledge. Unlike text-only LLMs, our\nmodel exhibits considerable ability to output visual contents, which achieves 65.13% accuracy on\nFK-QA IMG using the 13b-version. The multimodal generation ability enables Steve-Eye to better\nserve as an assistant for potential needed people such as beginners of this game. We show more\ndetails and cases in Appendix A.3.\n4.4\nSKILL PREDICTION AND PLANNING (SPP)\nSkill Prediction.\nSimilar to Section 3.1, we collect another 20K snapshot pairs in the form of\n{I0, It} from skill trajectories (referred to as Skill-Pred test). These pairs are input into our model\nto query the current execution status of the skill. The execution status can fall into one of three cate-\ngories: success, failure, and running, with “running” signifying that the skill is currently in progress.\nTable 4.\nRecall\/Accuracy results on Skill-Pred\ntest set for the skill prediction benchmark.\nrunning (%) success (%)\nfail (%)\nBLIP-2\n65.2\/58.8\n49.8\/54.3\n42.1\/51.8\nSteve-Eye-7b\n89.8\/82.5\n77.6\/81.4\n74.2\/79.9\nSteve-Eye-13b\n92.1\/84.2\n80.5\/83.1\n76.8\/81.5\nAs shown in Table 4, our model exhibits com-\nmendable performance in skill status predic-\ntion.\nHowever, the performance is still far\nfrom enough to completely replace the rule-\nbased game judgment adopted by the existing\nRL-based skill agents. These experiments indi-\ncate that, despite the excellent multimodal un-\nderstanding capabilities of our model in open-\nworld environments in previous experiments, it\nstill falls short in fine-grained reasoning tasks that involve consecutive frames to some extent.\nSkill Planning.\nFollowing Yuan et al. (2023), we carry out evaluation on 24 difficult tasks in\nMinecraft. These tasks can be categorized into three types: cutting trees to craft primary items (7),\nmining cobblestones to craft advanced items (7), and interacting with mobs to harvest food and ma-\nterials (10). Each task is tested for 30 episodes, where an episode refers to a multi-round interaction\n8\nPreprint\nTable 5. Comparisons on the skill planning benchmark. We test the mean success rates of all tasks,\nwhere each task is executed for 30 episodes using the same seeds for initialization.\nModel\nMineAgent\n0.00\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.21\n0.0\n0.05\n0.0\ngpt assistant\n0.30\n0.17\n0.07\n0.00\n0.03\n0.00\n0.20\n0.00\n0.20\n0.03\n0.13\n0.00\n0.10\n0.00\nSteve-Eye-auto\n0.30\n0.27\n0.37\n0.23\n0.20\n0.17\n0.26\n0.07\n0.13\n0.17\n0.20\n0.33\n0.00\n0.13\nSteve-Eye\n0.40\n0.30\n0.43\n0.53\n0.33\n0.37\n0.43\n0.30\n0.43\n0.47\n0.47\n0.40\n0.13\n0.23\nModel\nMineAgent\n0.46\n0.50\n0.33\n0.35\n0.0\n0.0\n0.06\n0.0\n0.0\n0.0\ngpt assistant\n0.57\n0.76\n0.43\n0.30\n0.00\n0.00\n0.37\n0.00\n0.03\n0.00\nSteve-Eye-auto\n0.70\n0.63\n0.40\n0.30\n0.17\n0\n0.37\n0.03\n0.07\n0.00\nSteve-Eye\n0.73\n0.67\n0.47\n0.33\n0.23\n0.07\n0.43\n0.10\n0.17\n0.07\nprocess. At each round, the model receives the environmental feedback from the last round, plans\na skill list based on the current status, and then picks up the top skill to execute. For each task\nepisode, we set a maximum step between [3000, 10000]. In our evaluation, we compare Steve-Eye\nagainst two baseline approaches: (1) MineAgent (Fan et al., 2022), which completes tasks without\ndecomposing them into basic skills, and uses PPO and self-imitation learning with CLIP reward,\nand (2) GPT Assistant, which employs ChatGPT as a high-level planner to generate skill plans by\nprompting itself with information from the environment and the agent’s status. The results in Ta-\nble 5 demonstrate that Steve-Eye significantly outperforms both baseline methods. Additionally, we\nconduct experiments in which Steve-Eye takes over the skill prediction function from the rule-based\ngame judgment in Minecraft. This self-driven variant is referred to as ‘Steve-Eye-auto.’ Since the\nmodel’s skill prediction is not always 100% accurate, Steve-Eye-auto does experience some perfor-\nmance degradation when compared to Steve-Eye. This degradation is more pronounced in longer,\ncomplex tasks (e.g.,\n,\n,\n) as opposed to short-term tasks (e.g.,\n,\n,\n). Nevertheless,\nSteve-Eye-auto still demonstrates significant performance improvements in most tasks, compared to\nthe baselines. For additional details about this benchmark, please refer to Appendix A.2.\nFor better visualization, we provide a qualitative example of Steve-Eye completing the task “crafting\nstone axe with wooden pickaxe” as shown in Figure 5.\nfind cobblestone\nharvest cobblestone\nfind trees\nharvest log\ncraft planks\ncraft and place table\ncraft stone axe\nbirthplace\nFigure 5. Snapshots of a qualitative example, illustrating how Steve-Eye completes the task of\n“crafting a stone axe with a wooden pickaxe.” Our model generates a skill plan at each interaction\nround and selects the top skill from the plan list for execution.\n5\nCONCLUSION\nIn this paper, we explore enabling a large multimodal model to serve as a generative embodied\nagent in open worlds. We achieve this goal by proposing Steve-Eye, which combines the text-only\nlanguage model with a visual encoder, allowing for a multimodal I\/O interface to interact with the\nenvironment. With the help of ChatGPT, we curate questions to generate 850K instruction-following\ndata to facilitate the agent’s multimodal perception fuction, foundational knowledge mastery, as well\nas the capability of skill prediction and planning. Experiments on three open-world benchmarks\nverify the advantages of our Steve-Eye over a wide range of perspectives.\n9\nPreprint\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–\n23736, 2022.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\nOpenflamingo:\nAn open-\nsource framework for training large autoregressive vision-language models.\narXiv preprint\narXiv:2308.01390, 2023.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-\nmodal language model. arXiv preprint arXiv:2303.03378, 2023.\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE\/CVF conference on computer vision and pattern recogni-\ntion, pp. 12873–12883, 2021.\nDaniel Fallman. Design-oriented human-computer interaction. In Proceedings of the SIGCHI con-\nference on Human factors in computing systems, pp. 225–232, 2003.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:\n18343–18362, 2022.\nFandom. Minecraft wiki. https:\/\/minecraft.fandom.com\/wiki, 2023.\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 14953–14962, 2023.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\narXiv preprint arXiv:1907.13440, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nRongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech,\nmusic, sound, and talking head. arXiv preprint arXiv:2304.12995, 2023a.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023b.\n10\nPreprint\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In International Conference on\nMachine Learning, pp. 9118–9147. PMLR, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022b.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt\nDeitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment\nfor visual ai. arXiv preprint arXiv:1712.05474, 2017.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023.\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,\nEkin Aky¨urek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-\nmaking. Advances in Neural Information Processing Systems, 35:31199–31212, 2022.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\nOpenAI. Chatgpt. https:\/\/openai.com\/blog\/chatgpt, 2022.\nOpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model\nconnected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023.\nJenny Preece, Yvonne Rogers, Helen Sharp, David Benyon, Simon Holland, and Tom Carey.\nHuman-computer interaction. Addison-Wesley Longman Ltd., 1994.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Tor-\nralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 8494–8502, 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,\nJulian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied\nai research. In Proceedings of the IEEE\/CVF international conference on computer vision, pp.\n9339–9347, 2019.\nD´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023.\nVicuna\nTeam.\nVicuna:\nAn\nopen-source\nchatbot\nimpressing\ngpt-4\nwith\n90quality.\nhttps:\/\/vicuna.lmsys.org\/, 2022.\n11\nPreprint\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291, 2023a.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv:2302.01560, 2023b.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nChong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European\nConference on Computer Vision, pp. 696–712. Springer, 2022.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n12\nPreprint\nA\nAPPENDIX\nIn this appendix, we offer a detailed introduction of the construction of our open-world instruc-\ntion dataset, as outlined in Appendix A.1, including (1) multimodal perception instructions, (2)\nfoundational knowledge instructions, (3) skill-related interaction instructions, and (4) template of\ninstructional training data. Furthermore, we delve into the skill planning benchmark and its associ-\nated task setups in Appendix A.2. In Appendix A.3 we present qualitative cases that illustrate our\nmodel’s ability to provide intuitive visual feedback and serve as an intelligent chatbot with a multi-\nmodal input-output interface. Finally, we explore the potential applications of our model in diverse\nenvironments, such as Virtual Home (Puig et al., 2018).\nA.1\nDATASET\nA.1.1\nMULTIMODAL PERCEPTION INSTRUCTIONS\nThis dataset contains 200K instructional pairs. Figure 6 illustrates a partial listing of instructional\nquestions employed for describing the content of the Minecraft snapshots. These instructions convey\nsimilar meanings, albeit with slight variations in natural language.\n•\n\"Describe the following Minecraft image in detail\",\n•\n\"Provide a detailed description of the given Minecraft image\",\n•\n\"Give an elaborate explanation of the Minecraft game image you see\",\n•\n\"Share a comprehensive rundown of the presented Minecraft image\",\n•\n\"Offer a thorough analysis of the Minecraft frame\",\n•\n\"Explain the various aspects of the Minecraft image before you\",\n•\n\"Examine the Minecraft image closely and share its details\",\n•\n\"Write an exhaustive depiction of the given Minecraft image“\n•\n\"Clarify the contents of the displayed Minecraft image with great detail\",\n•\n\"Narrate the contents of the Minecraft image with precision\"\nFigure 6. 10 instruction examples for multimodal perception instructions.\nA.1.2\nFOUNDATIONAL KNOWLEDGE INSTRUCTIONS\nThe dataset comprises 250K training instances, which is organized into three distinct subsets: 200K\nicon image instructions, 10K recipe image instructions, and 40K Minecraft-Wiki corpus instructions.\nFor the icon images, we generate questions aimed at prompting the model to recognize and describe\nitem icons within the inventory, as depicted in Figure 7. Similarly, we curate instructional questions\nfor recipe images as shown in Figure 8, with the objective of extracting information on completing\nspecific recipes. In addition, we preprocess the raw Minecraft-Wiki HTML pages by removing\nirrelevant information (e.g., reference links) and unresolved data, transforming the raw corpus into\na formatted, clean Markdown version. Leveraging the capabilities of ChatGPT, we employ this\npowerful language model to generate 10 questions, each with its corresponding answer, for every\npage of the cleaned Wiki corpus. This process yields a collection of 40K single-round question-\nanswer pairs, which can be utilized for instruction tuning.\n13\nPreprint\n•\n\"Provide a brief description of the given recipe image.\"\n•\n\"Offer a succinct explanation of the recipe picture presented.\"\n•\n\"Summarize the recipe content about icons of the image.\"\n•\n\"Give a short and clear explanation of the subsequent recipe image.\"\n•\n\"Share a concise interpretation of the recipe image provided.\"\n•\n\"Present a compact recipe description of the photo's key features.\"\n•\n\"Relay a brief, clear account of the recipe picture shown.\"\n•\n\"Render a clear and concise recipe summary of the photo.\"\n•\n\"Write a terse but informative recipe summary of the picture.\"\n•\n\"Create an icon narrative representing the recipe image presented.\"\nFigure 8. 10 instruction examples of recipe image for foundational knowledge instructions.\n•\n\"Clarify the contents of the displayed inventory image with great attention to detail.“\n•\n\"Characterize the inventory image with a meticulously detailed description.“\n•\n\"Break down the individual slot elements within the inventory image with precision.“\n•\n\"Take a step-by-step journey through the important details of the Minecraft inventory image.“\n•\n\"Paint a vivid and descriptive narrative of the Minecraft inventory image.“\n•\n\"Provide a precise narration of the contents within the Minecraft inventory image.“\n•\n\"Thoroughly analyze the Minecraft inventory image in a comprehensive and detailed manner.“\n•\n\"Illustrate the Minecraft inventory image through a descriptive and informative explanation.“\n•\n\"Examine the Minecraft inventory image closely and share its intricate details.“\n•\n\"Compose an exhaustive depiction of the given Minecraft inventory image.\"\nFigure 7. 10 instruction examples of icon images for foundational knowledge instructions.\nA.1.3\nSKILL-RELATED INTERACTION INSTRUCTIONS\nFor skill prediction, we utilize the skill policies trained by Yuan et al. (2023) to create a dataset\ncomprising 200K skill trajectories. In each trajectory, we extract timestamps from the initial and\nt-th points to generate a snapshot pair, denoted as {I0, It}. We then construct questions aimed at\ndetermining whether the agent successfully executed the skill or, in the case of failure, identifying\nthe underlying reasons for the unsuccessful attempt. Illustrative examples of these skill prediction\nquestions are provided in Figure 9. We also provide examples with snapshot pairs in Figure 10.\n14\nPreprint\n•\n\"\"Steve is demonstrating his proficiency in {SKILL_NAME}, with the objective of achieving {SKILL_DEFINITION}. We'll now assess \nboth the initial and current frames to determine if he has successfully executed the skill and the reasons behind it:\"\n•\n\"\"The skill Steve is performing is {SKILL_NAME}, and its intended outcome is to {SKILL_DEFINITION}. To determine whether Steve \nhas accomplished this skill, we need to analyze both the starting and current frames:“\n•\n\"\"Steve is currently engaged in executing {SKILL_NAME}, aiming to achieve {SKILL_DEFINITION}. In order to evaluate his success in \nperforming this skill, we'll examine both the initial frame and the current frame:“\n•\n\"\"The task at hand for Steve involves mastering {SKILL_NAME}, with the ultimate goal of accomplishing {SKILL_DEFINITION}. To \nascertain whether he has successfully completed this skill, we'll analyze both the starting and current frames:“\n•\n\"\"Steve is in the process of mastering the art of {SKILL_NAME}, with the specific objective of accomplishing {SKILL_DEFINITION}.\nWe will now evaluate whether Steve has successfully executed this skill by comparing the initial and current frames:“\n•\n\"\"The skill that Steve is currently executing is {SKILL_NAME}, and the intended outcome is {SKILL_DEFINITION}. To determine if \nSteve has effectively executed this skill, we'll assess both the initial frame and the current frame:“\n•\n\"\"Steve is currently performing the {SKILL_NAME} skill, with the aim of achieving {SKILL_DEFINITION}. Let's analyze both the start \nframe and the current frame to determine whether he has succeeded and the reasons behind it:“\n•\n\"\"Steve is demonstrating proficiency in the skill of {SKILL_NAME}, which is designed to accomplish {SKILL_DEFINITION}. Our \nevaluation will involve a comparison between the initial and current frames to assess the success of his execution:“\n•\n\"\"The task Steve is undertaking is the mastery of {SKILL_NAME}, with the end goal of achieving {SKILL_DEFINITION}. To determine \nwhether Steve has successfully executed this skill, we will analyze both the initial frame and the current frame:“\n•\n\"\"Steve is in the process of executing the {SKILL_NAME} skill, with the ultimate aim of accomplishing {SKILL_DEFINITION}. Our\nassessment will involve a comparison between the starting frame and the current frame to determine if he has succeeded and why:“\nFigure 9. 10 instruction examples for skill prediction instructions.\nQ: Steve is finding logs nearby, did he \ncomplete this skill?\nA: No.\nQ: Why does he fail?\nA: Because Steve has died.\nQ: Steve is finding sheep nearby, does \nhe fail?\nA: No. Steve found a sheep in sight.\nQ: Steve plans to craft a table, does he \nfinish this skill successfully?\nA: Yes he does.\nQ: Steve is asked to collect planks, what \nis the results?\nA: Steve collect planks successfully.\nQ: Steve collects wool in Minecraft, \nwhat is the consequence?\nA: Steve fails to finish the sill.\nQ: Please help me analyze the reasons \nfor the failure.\nA: Sure, there is no sheep in sight\nQ: Steve is mining cobblestone, does he \nfinish this skill?\nA: No.\nQ: How do you know that?\nA: No additional cobblestone shown in \nthe inventory\nFigure 10. Illustrative examples of skill prediction instruction data with snapshot pairs.\nTemplate of Instructional Training Data. Similar to Liu et al. (2023), we formulate each instruc-\ntional instance as a multi-round conversation as shown in Figure 11, where Xhead message is a sentence\nto describe this assistant (e.g., “ You are in a chat between a curious human and an artificial intelli-\ngence assistant. You should serve as an assistant to give helpful, detailed, and polite answers to the\nhuman’s questions.”). The number of rounds relies on the input instruction content. And the input\nimages (denoted as < image >) will only be fed in the first round, while XC may contain visual\noutputs with two additional tokens < vis > and < \/vis >.\n15\nPreprint\nFigure 11. The unified template to generate input sequence for instructional tuning.\nA.2\nSKILL PLANNING BENCHMARK\nTo clarify this benchmark, we begin by offering comprehensive task setup details in Table 6. During\nthe evaluation phase, we relocate the agent to a random location at the initiation of every episode,\nwith distances of up to 500 units, ensuring that the agent spawns in an unfamiliar environment. Fur-\nthermore, for tasks that involve interacting with mobs, we enforce a maximum spawning distance of\n30 units for cows and sheep. Our approach to complete tasks is rooted in a hierarchical framework.\nSpecifically, our model exclusively generates high-level skill plans, delegating the actual skill exe-\ncution to pre-trained basic skill policies as introduced by Yuan et al. (2023). Notably, we introduce\na self-driven variant named ’Steve-Eye-auto,’ which serves not only as a planner but also replaces\nthe Minecraft rules to verify the successful execution of skills.\nTable 6. The setups of 24 tasks used in our skill planning evaluation, where “Initial Items” refers\nto the tools provided in the agent’s inventory at the beginning of each episode, and “Max Steps”\nrepresents the maximum episode duration. Any episode exceeding this limit is classified as a task\nfailure. The tasks are originally developed by Yuan et al. (2023).\n(a) 7 tasks involving the process of “cutting trees to craft primary items”.\nTask Icon\nTask Name\nstick\ncrafting table nearby\nbowl\nchest\ntrap door\nsign\nwooden pickaxe\nInitial Items\n-\n-\n-\n-\n-\n-\n-\nMax Steps\n3000\n3000\n3000\n3000\n3000\n3000\n3000\n(b) 7 tasks involving the process of “mining cobblestones to craft advanced items”.\nTask Icon\nTask Name\nfurname nearby\nstone stairs\nstone slab\ncobblestone wall\nlever\ntorch\nstone pickaxe\nInitial Items\n*10\n*10\n*10\n*10\n*10\nMax Steps\n5000\n5000\n3000\n5000\n5000\n5000\n10000\n(c) 10 tasks involving the process of “interacting with mobs to harvest food and materials”.\nTask Icon\nTask Name\nmilk\nbucket\nwool\nbeef\nmutton\nbed\npainting\ncarpet\nitem\nframe\ncooked\nbeef\ncooked\nbutton\nInitial Items\n,\n*3\n,\n*2\n,\n,\n,\n,\n,\nMax Steps\n3000\n3000\n3000\n3000\n10000\n10000\n3000\n10000\n10000\n10000\n16\nPreprint\nTable 7. The setups of 10 long-horizon iron-based tasks, where “Initial Items” are provided in the\nagent’s inventory at task beginning, and “Max Steps” refers to maximum environmental steps.\nTask icon\nTask description\nInitial tools\nMax steps\ncraft iron ingot\n*5,\n*64\n8000\ncraft shears\n*5,\n*64\n10000\ncraft bucket\n*5,\n*64\n12000\ncraft iron pickaxe\n*5,\n*64\n12000\ncraft iron axe\n*5,\n*64\n12000\ncraft iron sword\n*5,\n*64\n10000\ncraft iron shovel\n*5,\n*64\n8000\ncraft tripwire hook\n*5,\n*64\n8000\ncraft heavy weighted pressure plate\n*5,\n*64 t\n10000\ncraft iron trapdoor\n*5,\n*64\n12000\nfurnace\ngold ingot\ngreen candle\noak boat\norange bed\npainting\n(a) Qualitative examples of recipe image generation.\ntipped-arrow\nacacia-sign\nstone-axe\n(b) Illustrative examples of recipes that our model struggles to predict accurately. We attribute this failure to\nthe complexities arising from fine-grained or semantically overlapping image information.\nMinecraft \nEnvironment Feedback.\nSure, here is the plan list\nto harvest milk: (1) find\ncow\nnearby,\n(2)\nmilk\nbucket.\nHi, Steve, this is your birth-\nplace, what do your see?\nI can see the sky, trees in\nfront of me. I have 20\nlives and 10 food. There\nis a bucket in my invent-\ntory\nI want you to harvest milk.\nThere is a cow in front\nof me, finding cows s-\nuccessfully!\nThe plan list to harvest\nmilk: (1) milk bucket.\nRound 1: find cow nearby\nRound 2: milk bucket\nMake a plan again.\nMinecraft \nEnvironment Feedback.\nmilk\nbucket\nfinished!\nThe task is success-\nfully completed\nThank you, Steve.\n17\nPreprint\nTask Name: relax on sofa\nTask Description: I go to the living room and sit in the sofa\nTask Plan: {“1”: walk living room, “2”: walk couch, “3”: find couch, “4”: walk couch, “5”: sit couch}\nTask Name: browse the Internet\nTask Description: I go to the office and sit in a chair, I turn on the computer and grab the mouse. I \ntype on the keyboard and starting working on the computer.\nTask Plan: {“1”: walk living-room, “2”: walk desk, “3”: find desk, “4”: find chair, “5”: sit chair, “6”: find\ncomputer, “7”: switch-on computer, “8”: find mouse, “9”: grab mouse, find keyboard, “10”: type\nkeyboard, “11”: turn-to computer, “12”: look-at computer}\nTask Name: put milk in the freezer\nTask Description: I walk into kitchen, look for the milk, walk to milk, look for refrigerator, walk to \nrefrigerator, open door, put the milk in the refrigerator\nTask Plan: {“1”: walk dining-room, “2”: walk milk, “3”: find milk, “4”: turn-to milk, “5”: grab milk, “6”: \nlook-at freezer, “7”: walk freezer, “8”: open freezer, “9”: put milk\nFigure 14. Task examples from the extended Virtual-Home benchmark, where elements in green,\ncyan and red represent action, room, and object categories, respectively. Our benchmark includes a\ndiverse range of tasks that simulate interactions between individuals and their room environments.\nIt contains over 50 distinct room setups, involving 20 unique actions, and 100 objects. Each room\npresents a selection of more than 200 distinct tasks.\nA.3\nQUALITATIVE RESULTS OF MULTIMODAL GENERATION\nA.3.1\nRECIPE IMAGE GENERATION\nFigure 12a showcases qualitative examples of our evaluation on the FK-QA IMG dataset. Utilizing\na visual tokenizer like VG-GAN, our model demonstrates the ability to engage in visual generation,\nenabling it to provide visual feedback based on its comprehension of textual input. However, as\nshown in Figure 12b, our model encounters difficulties when generating image content characterized\nby fine-grained or semantically overlapping elements. These challenges warrant further exploration\nin our future work.\nA.3.2\nMULTIMODAL CHATBOT\nIn Figure 13, We present an overview of Steve-Eye functioning as a chatbot to receive task com-\nmands and execute them.\n18\nPreprint\nA.4\nDISCUSSION OF OPEN-WORLD EXPLORATION\nIn this paper, we have selected Minecraft as our open-world platform. Nevertheless, it is evident that\nSteve-Eye can be applied to other open-world environments, such as Virtual Home (Puig et al., 2018)\nand AI2THOR (Kolve et al., 2017), with minimal manual effort using the same methodology in this\npaper. These alternative benchmarks, when compared to Minecraft, exhibit a closer alignment with\nthe real world. To some extent, this choice holds greater significance since our ultimate objective is\nto deploy the agent in the real world. To achieve this goal, we expand the Virtual Home benchmark\nby introducing a more extensive range of environments (50+ rooms), human-interaction tasks (200+\nfor each room), as well as diverse categories of actions (20+) and objects (100+), as illustrated in\nFigure 14. The corresponding validation and further exploration of open-ended embodied agents in\na real-world context will be the focus of our future work.\n19\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds.pdf"}
{"title":"MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception","authors":"Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, Jing Shao","summary":"It is a long-lasting goal to design an embodied system that can solve\nlong-horizon open-world tasks in human-like ways. However, existing approaches\nusually struggle with compound difficulties caused by the logic-aware\ndecomposition and context-aware execution of these tasks. To this end, we\nintroduce MP5, an open-ended multimodal embodied system built upon the\nchallenging Minecraft simulator, which can decompose feasible sub-objectives,\ndesign sophisticated situation-aware plans, and perform embodied action\ncontrol, with frequent communication with a goal-conditioned active perception\nscheme. Specifically, MP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is modulated into functional\nmodules that can be scheduled and collaborated to ultimately solve pre-defined\ncontext- and process-dependent tasks. Extensive experiments prove that MP5 can\nachieve a 22% success rate on difficult process-dependent tasks and a 91%\nsuccess rate on tasks that heavily depend on the context. Moreover, MP5\nexhibits a remarkable ability to address many open-ended tasks that are\nentirely novel.","url":"http:\/\/arxiv.org\/abs\/2312.07472v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.07472v4","published":1702403745000,"comment":"Accepted to CVPR2024","pdf_text":"MP5: A Multi-modal Open-ended Embodied System\nin Minecraft via Active Perception\nYiran Qin1,2*,\nEnshen Zhou1,3*,\nQichang Liu1,4*,\nZhenfei Yin1,5,\nLu Sheng3†,\nRuimao Zhang2†,\nYu Qiao1,\nJing Shao1‡\n1Shanghai Artificial Intelligence Laboratory\n2The Chinese University of Hong Kong, Shenzhen\n3School of Software, Beihang University\n4Tsinghua University\n5The University of Sydney\nyiranqin@link.cuhk.edu.cn\nzhouenshen@buaa.edu.cn\nlsheng@buaa.edu.cn\nruimao.zhang@ieee.org\nshaojing@pjlab.org.cn\nforest\nday\ngrass\nProcess\nContext\nDay\nStone\nWood\nWater\nPig\nGrass\nTask: Kill a pig with a stone sword during the \ndaytime near the water with grass next to it .\nForest\nPlains\nlog\npig\nplains\nwater\nstone\nnull\n: log\n: plank\n: stick\n: crafting table\n: wooden pickaxe\n: stone\n: stone sword\n: pig\n0\n1\n2\n3\n4\nProcess\nContext #\n(a)\n(b)\n(c)\nFigure 1. The process of finishing the task “kill a pig with a stone sward during the daytime near the water with grass next to it.”. (a)\nTo achieve the final goal (i.e., O8: “kill a pig\n”), a player should accomplish a list of sub-objectives {Oi}7\ni=1 sequentially. During this\nprocess, the player should also be aware of some items in the environment, e.g., “grass\n”, “day\n” and etc. (b) This diagram shows\nthe number of these necessary items in the context that should be perceived for each sub-objective, during the task execution process. (c)\nImages marked by O1 and O6 show the observed ego-centric views in the process of achieving the corresponding sub-objectives. Images\nmarked by O1\n8 and O2\n8 indicate how the player executes the action about the last sub-objective “kill a pig\n”. This exemplar process tells\nthat such long-horizon open-world embodied tasks in Minecraft should be solved both in process-dependent and context-dependent way.\nAbstract\nIt is a long-lasting goal to design an embodied system\nthat can solve long-horizon open-world tasks in human-like\nways. However, existing approaches usually struggle with\ncompound difficulties caused by the logic-aware decompo-\nsition and context-aware execution of these tasks. To this\nend, we introduce MP5, an open-ended multimodal em-\nbodied system built upon the challenging Minecraft sim-\nulator, which can decompose feasible sub-objectives, de-\nsign sophisticated situation-aware plans, and perform em-\nbodied action control, with frequent communication with\n∗Equal contribution\n† Corresponding author\n‡ Project leader\na goal-conditioned active perception scheme. Specifically,\nMP5 is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs), and the system is mod-\nulated into functional modules that can be scheduled and\ncollaborated to ultimately solve pre-defined context- and\nprocess-dependent tasks. Extensive experiments prove that\nMP5 can achieve a 22% success rate on difficult process-\ndependent tasks and a 91% success rate on tasks that heav-\nily depend on the context. Moreover, MP5 exhibits a re-\nmarkable ability to address many open-ended tasks that are\nentirely novel.\nPlease see the project page at https:\n\/\/iranqin.github.io\/MP5.github.io\/.\narXiv:2312.07472v4  [cs.CV]  26 Mar 2024\n1. Introduction\nOne of the core objectives of current embodied intelligence\nis to construct generalist agents that can solve long-horizon\nopen-world embodied tasks, approaching the behavior pat-\nterns of human beings [1, 19, 22, 31]. However, the process\ndependency and context dependency in these tasks, such as\nthose in Minecraft depicted in Fig. 1, hinder recent agents\nfrom achieving the aforementioned goal. To be specific, the\nformer emphasizes the inherent dependency among the sub-\nobjectives of one task or an action sequence to fulfill one\nsub-objective (such as “craft a stone sword\n” should be\nsolved before “kill a pig\n”). The latter highlights that the\nexecution of each sub-objective or even each action depends\non the contextual information of the environment (such as\n“kill a pig\n” requires to find the target “pig\n” and its sur-\nrounding items “grass\n” and “water\n” during the “day-\ntime\n” in the observed images, as shown in Fig. 1).\nThe recent success of Large Language Models (LLMs)\nhas\nattempted\nto\nsolve\nthe\nprocess-dependent\nchal-\nlenge, by using LLMs to break down a long-horizon\nprocess-dependent task into a sequence of feasible sub-\nobjectives [34, 36, 43]. These methods [34, 43] simplify\nthe context-dependent challenge by assuming the agents are\nall-seeing, i.e., knowing everything about their state and the\nenvironment it locates in. However, to solve the context-\ndependent challenge, an embodied agent should addition-\nally have: (1) the perception capability is open-ended, se-\nlective and give results tailored to diverse purposes (e.g.,\nfor task planning or action execution), (2) the perception\nmodule can be compatibly scheduled along with the other\nmodules (e.g., planning and execution modules) by a uni-\nfied interface, as an integrated system.\nTo this end, we introduce MP5, a novel embodied sys-\ntem developed within Minecraft, to meet the above expec-\ntations. Specifically, MP5 comprises five interacting mod-\nules, i.e., Parser decomposes a long-horizon task into a\nsequence of sub-objectives that should be completed one\nby one; Percipient answers various questions about the ob-\nserved images, as the reference for the other modules; Plan-\nner schedules the action sequences of a sub-objective, as\nwell as refines the following sub-objectives, given the cur-\nrent situation; Performer executes the actions along with\nfrequent interaction with the environment; and Patroller\nchecks the responses from the Percipient, Planner, and Per-\nformer, for the purpose of verifying current plans\/actions, or\nfeedback on potential better strategies. In our work, Percipi-\nent is a LoRA-enabled Multimodal LLM (MLLM). Among\nthe pre-trained LLMs, Parser and Planner are augmented\nwith external Memory, while Patroller is not.\nNotably, MP5 includes an active perception scheme by\nmeans of multi-round interaction between Percipient and\nPatroller, which is to actively perceive the contextual in-\nformation in the observed images, with respect to vari-\nous queries raised by Planner and Performer.\nIt is the\nkey enabler to solve context-dependent tasks.\nPatroller\nin this scheme relays compatible feedback to Planner and\nPerformer accordingly, while eventually strengthening the\nplanning skill in awareness of the situations and improving\nthe action execution correctness in an embodied manner.\nExtensive experiments prove that MP5 can robustly\ncomplete tasks needed for long-horizon reasoning and com-\nplex context understanding. It achieved a 22% success rate\non diamond-level tasks (i.e., one of the hardest long-horizon\ntasks) and a 91% success rate on tasks requiring complex\nscene understanding (i.e., need to perceive around 4 ∼6\nkey items in the observed images). Moreover, in Sec. 4.2.3,\nMP5 can surprisingly address more open-end tasks both\nwith heavy process dependency and context dependency.\n2. Related Work\n2.1. Multi-modal Large Language Models\nWith the development of Large Language Models (LLMs)\nlike the GPT series [2, 27, 29], as well as open-source LLMs\nsuch as the LLaMA series [32, 33] and Vicuna [5], Multi-\nmodal Large Language Models (MLLMs) have emerged.\nExamples of such MLLMs include LLaVA [21], Instruct-\nBLIP [6], and LAMM [39], among others [4, 10, 15, 28, 38,\n42]. In this work, we introduce MineLLM, which is specif-\nically designed and trained for Minecraft, and leverage its\nperception, interaction, and analysis capabilities to build\nup Percipient for MP5, and further enable an objective-\nconditioned active perception scheme.\n2.2. Agents in Minecraft\nPrevious works[3, 7, 9, 11, 19, 22, 40, 41] attempt to use\napproaches such as hierarchical RL, goal-based RL, and re-\nward shaping to train an agent in Minecraft. MineCLIP [9]\nenables the resolution of various open-ended tasks speci-\nfied in free language, even without any manually designed\ndense rewards. DreamerV3 [12] succeeds in training agents\nin Minecraft with a learned world model. VPT [1] builds\na foundation model for Minecraft by learning from massive\nvideos. Based on VPT, Steve-1 [18] also explores bring-\ning in MineCLIP [9] to get an instruction following policy\nwith high performance. The development of recent large\nlanguage model-related work Voyager [34], DEPS [36],\nGITM [43] further promote the advancement of agents\nin long-horizon tasks. These works use pre-trained large\nlanguage models as the zero-shot planners[14] for agents,\nleveraging the powerful reasoning capabilities of large lan-\nguage models to obtain continuous operation instructions or\nexecutable policy lists.\nWe take advantage of the reasoning capability of LLM\nto build up our own agent. Existing LLM agents [43, 43] in\nMinecraft feed scene data from simulation platforms [9, 11]\nPerformer Memory\nKnowledge\nMemory\nPatroller\nPercipient \nObtain Env. \nInfo. for \nPlanning\nObtain Env. \nInfo. for \nPerformer\nMulti-round\nSingle-round\n<Sub-Objective>\nParser\nTask: Kill a pig with a wooden sword during the daytime near \nthe water with grass next to it.\nActive \nPerception\n…\nError \nFeedback\nRe-plan\nPerformer: Having completed a move in “Find” action, based on my current view, tell \nme if I should continue this action or if the next action is ready to execute. \n<Sub-Objective>\nPatroller: I conduct Active Perception with Percipient with your current observation, \nyou must continue with the current action since there is no river near the pig.\nPatroller: I conduct Active Perception with Percipient with your current observation, \nyou can execute the next action since all conditions are satisfied.\nPlanner: Can you tell me what important environmental information I need to know?\nPatroller: I conduct Active Perception with Percipient with your current observation, \nthere is no pig based on the scene.\nPlanner: 1. Equip(       )  2. Find(        )  3. Move(     )  4. Fight(        )  \nPerformer: Having completed a move in “Find” action, based on my current view, tell \nme if I should continue this action or if the next action is ready to execute. \nPerformer: Start executing “Equip”.\nPerformer: Continue executing “Find”.\nSub-Objectives\n{\n}\n…\n…\n…\n…\n…\n…\nMove\nMine\nEquip\nFight\nFind\nCraft\n…\nFigure 2. Overview of module interaction in MP5. After receiving the task instruction, MP5 first utilizes Parser to generate a sub-objective\nlist. Once a sub-objective is passed to the Planner, the Planner Obtaining Env. Info. for Perception-aware Planning. The performer takes\nfrequently Perception-aware Execution to interact with the environment by interacting with the Patroller. Both Perception-aware Planning\nand Execution rely on the Active Perception between the Percipient and the Patroller. Once there are execution failures, the Planner will\nre-schedule the action sequence of the current sub-objective. Mechanisms for collaboration and inspection of multiple modules guarantee\nthe correctness and robustness when MP5 is solving an open-ended embodied task.\ninto large language models for task planning. However, for\nembodied agents in real scenes, it is clearly unrealistic to\nuse accurate scene data directly. Therefore, agents need to\nbe robust to make decision corrections despite inaccurate or\nerroneous perception information. Moreover, open-ended\ntasks need hierarchical reasoning [22] and complex open-\nended context understanding [1, 9], classical perception net-\nworks can only output fixed perception results and cannot\nprovide corresponding perception information according to\nthe task, making it impossible to understand open-ended\nscenarios. Therefore, we design MP5, an embodied agent\nwith open-ended capabilities that can solve the problem of\nopen-ended tasks.\n3. Method\nIn this section, we first give an overview of our proposed\nMP5, for solving context-dependent and process-dependent\ntasks in an open-world and embodied environment, such as\nMinecraft (Sec. 3.1). Next, we elaborate on how to imple-\nment an active perception scheme (Sec. 3.2). This scheme\nplays a vital role in MP5 to solve context-dependent tasks,\nsince it reliably grounds the visual content according to dif-\nferent kinds of objectives, and thus strengthens the plan-\nning skill and execution correctness with respect to context-\ndependent tasks. Then, we show how to plan and update\naction sequences in awareness of the situations, and how to\nreliably execute these actions in an embodied environment\n(Sec. 3.3). Finally, we give necessary implementation de-\ntails about MP5 in Sec. 3.4.\n3.1. Overview\nAs demonstrated in Fig. 2, our MP5 includes five major\nmodules, i.e., Parser, Percipient, Planner, Performer, and\nPatroller. Specifically, Percipient is a parameter-efficiently\nfine-tuned Multimodal Large Language Model (MLLM)\nthat is specified to the Minecraft environment. The Parser,\nPlanner, and Patroller are pre-trained Large-language Mod-\nels (LLMs). We also include retrieval-augmented genera-\ntion (RAG) to enhance the quality of responses generated\nby Parser and Planner. Performer is an interface that ex-\nplains each action from the action sequence into executable\ncommands that directly control the game character.\nWhy can MP5 solve context-dependent and process-\ndependent tasks?\nMP5 includes an active perception\nscheme by means of multi-round interactions between Per-\ncipient and Patroller, which is to actively perceive the envi-\nronmental information in the observed images, with respect\nto various objectives raised by Planner or Performer. With\nthe help of this scheme, Planner can schedule or update ac-\ntion sequences in awareness of the observed images, inven-\ntory status and etc., resulting in a situation-aware planning;\nPerformer can execute actions that are adapted to the em-\nbodied environment, resulting in a embodied action execu-\ntion. Patroller in this scheme can also feedback on better\nchoices of plans\/actions based on the visual evidence so that\nthe process-dependent tasks are solved with fewer chances\nof context-dependent execution failures. Moreover, Percip-\nient can understand open-ended visual concepts, therefore\nit allows MP5 to solve tasks that are never seen before.\nHow does MP5 function?\nIn Fig. 2, upon receiving a\nResponse\nThis picture is showing a pig, \ntypes of Minecraft mobs.\nQuestion\nWhat types of Minecraft mobs is \nthis <image> showing?\nPerformer : Current sub-objective <Kill a pig with a wooden sword during the daytime near the water with grass next to it>    &    Current execution action: Find (          )\nEnv. Info.\nResponse\nThe blocks shown in the image \nare water, grass …\nQuestion\nWhat types of blocks are shown in \nthe given <image> ?\nResponse\nThe image depicts daytime in \nthe Minecraft world.\nQuestion\nWhat time of day does the <image>\ndepict in the Minecraft world?\nResult\nI have seen a pig during the \ndaytime near the water with \ngrass next to it. You can execute \nthe next action.\nAsk question(s) of \nsignificant necessity\nAnswer the question\nAdd answer to Env. Info.\nPatroller\nPatroller\nPatroller\nPerceived env info.\nTemporary Env. Info. Set\nEnv. Info.\nEnv. Info.\nEnv. Info.\nEnv. Info.\nPatroller\nReset\nQuery Env. Info. \nOr Return\nPercipient \nPercipient \nPercipient \nFigure 3. A demonstration of the process of Active Perception scheme. Temporary Env. Info. Set saves information collected in the current\nscenario, so it should be reset at the beginning of Active Perception scheme. Performer then invokes Patroller to start asking Percipient\nquestions with respect to the description of the sub-objective and the current execution action round by round. The responses of Percipient\nare saved in Temporary Env. Info. Set and are also gathered as the context for the next question-answering round. After finishing asking all\nsignificant necessary questions, Patroller will check whether the current execution action is complete by analyzing the current sub-objective\nwith Perceived env info. saved in Temporary Env. Info. Set, therefore complex Context-Dependent Tasks could be solved smoothly.\nhigh-level task, MP5 first utilizes the Parser to generate a\nsequence of short-horizon sub-objectives, as a list of rich\ninstructions in natural languages. The feasibility of the gen-\nerated sub-objectives is augmented by retrieving an external\nKnowledge Memory. This knowledge mainly comes from\nthree sources: part of it is from the online wiki, another part\nis from the crafting recipes of items in MineDojo [9], and\nsome are user tips from Reddit. To one sub-objective, Plan-\nner schedules the action sequence that is grounded by the\nenvironmental information gathered by the active percep-\ntion scheme. In this case, Performer will execute the actual\nactions by explaining the action sequence that is adapted\nto the embodied environment, via frequent interaction with\nthe active perception scheme. Once there are execution fail-\nures (determined by Patroller), Planner will re-schedule the\naction sequence of the current sub-objective, or even up-\ndate the following sub-objectives if some necessary sub-\nobjectives are missing.\nOtherwise, the agent will go to\nthe next sub-objective and schedule new action sequences,\nwhilst the successful action sequence of the current sub-\nobjective will be stored in the external memory of Planner\n(called Performer Memory), along with the agent situation\nwhen it was planned. In the end, the agent will stop when\nthe final sub-objective of the task has been reached.\n3.2. Active Perception\nLet’s take the example shown in Fig. 3 to demonstrate how\nthe active perception scheme works. In this example, the\nactive perception scheme is communicated with Performer\nto enable an embodied action execution.\nAt first, Performer invokes Patroller to start asking Per-\ncipient questions with respect to the description of the\nsub-objective and the current execution action, while si-\nmultaneously resetting the set of environmental informa-\ntion to be gathered.\nThen Patroller progressively asks\nPercipient whether the observed image contains necessary\nitems\/factors (e.g., mobs\n, blocks\n, time\n)\nrelated to recent sub-objective (e.g., pig\n) and the execut-\ning action (e.g., “find pig\n”). The responses of Percipient\nare also progressively gathered and act as the context for\nthe next question-answering round. Note that in each round,\nPatroller also checks whether all the necessary items\/factors\nhave been collected - If yes, Patroller stops the interaction\nand returns all the environmental information as natural lan-\nguage, and invokes Performer to execute the next action. If\nPatroller eventually fails to gather enough items\/factors, it\nwill tell Performer what items\/factors are missing in the ob-\nserved images, which suggests Performer keeps executing\nthe current action. Please also check the example shown in\nFig. 2.\nSimilarly, active perception used in situation-aware plan-\nning is similar to what is explained here, except that the ap-\nplied instructions do not contain the executable action. For\nmore details please check the Sup. E.\n3.3. Perception-aware Planning and Execution\nSituation-aware Planning. Given one sub-objective, Plan-\nner will generate the action sequence based on the descrip-\ntion of the situation, such as the objective-conditioned envi-\nronmental information from the active perception scheme,\nthe inventory status and localization, and etc. Moreover,\nPlanner will retrieve previous successful action sequences\nas the demonstration prompt to augment the aforemen-\ntioned planning results. If the active perception scheme fails\nto find the key items\/factors about the current sub-objective\nin the observed image, the generated action sequences will\ninclude more actions to reach them. Moreover, if Performer\nencounters execution failures determined by Patroller (such\nImage Encoder\n🔥Alignment Net\n🔥\nLoRA\nLarge Language Model\nInstruction\nWhat types of Minecraft mobs is \nthis <image> showing?\nResponse\nThis picture is showing a pig, \ntypes of Minecraft mobs.\n🔥\nFinetune\nFrozen\nFigure 4. The model architecture of MineLLM. Image is encoded\nby a pre-trained vision encoder and decoded by LLM. Only the\nparameters of Alignment Net and LoRA are trainable.\nas failure of “equip wooden sword\n”), Planner will re-\nschedule the action sequence or even update the following\nsub-objectives, with the help of external memories.\nEmbodied Action Perception. As indicated in Sec. 3.2,\nPerformer would like to communicate with the active per-\nception scheme in every round of action execution, so as\nto enhance the ego-centric awareness of the agent. The new\naction will be executed if Patroller identifies necessary envi-\nronmental information in the observed images that matches\nboth the sub-objective and the goal of the current action.\nOtherwise, the current action is kept executing until en-\ncountering execution failures or the end of the episode. The\nsuccessful action sequence about one sub-objective will be\nstored in the Performer Memory, together with necessary\nsituational information of the agent when it was planned.\nFor more details about the planning and execution process,\nplease check Sup. G.2 and Sup. B.2.\n3.4. Implementation Details\nPercipient.\nThe network of Percipient is depicted in\nFig. 4. Images are processed by a frozen vision encoder\nMineCLIP [9], whose features are projected by an Align-\nment Net(we use two-layer MLP like LLaVA-1.5 [20]) to\nthe same feature space as the text embeddings of the ap-\nplied LLM (we use Vicuna-13B-v1.5 [5]). Then the vision\nand text tokens are concatenated to feed into a LoRA-based\nfine-tuned LLM [13]. We add LoRA [13] parameters to\nall projection layers of the self-attention layers in the LLM.\nOnly the parameters of the Alignment Net and the LoRA\nmodule are optimized during training. The construction of\nthe training data with respect to Percipient is in the Sup. B.1.\nParser, Planner, and Patroller. We utilize OpenAI’s GPT-\n4 [26] as LLMs in Parser, Patroller, and Planner. We also\nevaluate other alternatives of GPT-4 [26], such as open-\nsource models like Vicuna-13B-v1.5 [5] and LLaMA2-\n70B-Chat [33] in Sup.D.3.\nPerformer. It is important to clarify that the actions gen-\nerated by Planner are not low-level commands such as key-\nboard and mouse operations [1], but a set of simple actions\n(such as equip, move, craft). Inspired by GITM [43], we\nimplement these actions appropriately through basic oper-\nations provided by the MineDojo [9] simulator. For more\ndetails, please check the Sup. B.2.\n4. Experiments\nAt first, we depict the setup of the Minecraft simulation en-\nvironment that we build and validate MP5, and give the\ndefinition of the evaluated tasks and how to set them in\nSec. 4.1. In Sec. 4.2, we present the quantitative and quali-\ntative performance of MP5, as well as in-depth discussions\non these tasks, and demonstrate that MP5 can even success-\nfully accomplish tasks that are more open-ended and never\nseen before. At last, we investigate how different modules\naffect the performance of MP5 and analyze the impact of\nvarious module choices within our system in Sec. 4.3.\n4.1. Experimental Setup\nEnvironment Setting. We employ MineDojo [9] as our\nsimulation environment to build and validate MP5. We cap-\nture player ego-view images provided by MineDojo [9] as\ninput of MP5, and further construct a dataset for training\nMineLLM. As for the output of MP5, we encapsulate Mine-\nDojo’s [9] actions to create our own action space.\nTask Setting. To evaluate how our MP5 can integrate per-\nception information with planning and execution, we define\ntwo types of tasks: Context-Dependent Tasks and Process-\nDependent Tasks as illustrated in Tab. 1 and Tab. 2.\n1) Context-Dependent Tasks primarily study how Active\nPerception enables the agent to better perceive low-level\ncontext information in the environment.\nWe first estab-\nlish 6 aspects of environmental information derived from\nthe Minecraft game environment: [Object, Mob, Ecology,\nTime, Weather, Brightness]. Each aspect has multiple op-\ntions.\nFor example, pigs\n, cows\n, and sheep\nare\nall elements belonging to Mob.\nBased on this, we de-\nfine 16 tasks and organize their difficulty into four levels\nby taking into account the number of information elements\nthat require perception, as is shown in Tab. 1. For exam-\nple, Easy tasks necessitate the perception of only one el-\nement, whereas Complex tasks involve the perception of\n4 to 6 elements. We rigorously assess MP5’s proficiency\nin environmental context perception across these 16 tasks.\nIn Context-Dependent Tasks, our environment details are\npredetermined (e.g., biomes\n, weather\n, and\netc.), as certain targets are exclusive to specific environ-\nments.\nWithout this environmental specificity, the agent\nmight never encounter the intended target. We retain each\nobservation of active perception throughout the task, using\nthem as references to ascertain the agent’s successful com-\npletion of the task.\n2) Process-Dependent Tasks focus on exploring the con-\nTable 1. Context-Dependent Tasks. 16 tasks are defined and di-\nvided into 4 difficulty levels based on the minimum number of\ninformation types needed. Underlines label the environmental in-\nformation, reflecting the complexity varies at each level.\nTask Level\nExample Task\nEasy\nFind a tree\nMid\nFind a tree\nin the forest\nHard\nFind a tree\nin the forest\nduring the nighttime\nComplex\nFind a pig\nnear a grass\nin the forest\nduring the daytime\ntributions of situation-aware planning, embodied action ex-\necution, and the integration with Active Perception in ac-\ncomplishing long-term tasks while constantly perceiving\nthe environment and dynamically adjusting actions. We se-\nlect 25 tasks from the technology tree and define their dif-\nficulty levels as Basic level\nto Diamond level\nbased\non the number of reasoning steps required to complete\nthe tasks. All environmental factors (e.g., biomes\n,\nweather\n, and etc.) are randomized in Process-\nDependent Tasks. More details can be found in Sup.D.1.\nEvaluation Metrics. For different tasks, the agent’s initial\nposition and environment seed are randomized. The agent\nbegins in survival mode, commencing with an empty inven-\ntory, and faces the challenge of hostile mob generation. It\nstarts from scratch, with a game time limit of 10 minutes, a\ntime period equivalent to 12,000 steps at a control frequency\nof 20Hz. More details can be found in Sup. C.\nFor the Context-Dependency Tasks, each assignment is\nopen-ended.\nTherefore, we conduct manual evaluations\nwhen the agent determines it has completed the task or\nexceeds the time limit.\nTwo cases are ruled as failures:\n1)There is an observation that meets all the conditions, but\nthe agent does not end the task; 2) The last observation\ndoes not meet all the conditions, yet the agent ends the task.\nOtherwise, we believe that the agent correctly perceives all\nthe context according to the task and determines that the\ntask is successfully completed. For the Process-Dependent\nTasks, any accidental deaths of the agent during the game\nare counted as failures, as are instances where the agent\ndoes not accomplish the task within the time limit.\nIn practice, we conduct 50 games on Context-Dependent\nTasks and 30 games on Process-Dependent Tasks, averaging\nthe success rates for both. The results are grouped accord-\ning to the previously defined difficulty levels, and report\nthe group means. For detailed definitions of the evaluation,\nplease refer to Sup. D.\n4.2. Main Results\n4.2.1\nResults of Context-Dependent Tasks\nIn Context-Dependent Tasks, we primarily investigate how\nto enhance an agent’s perception of context information\nTable 2. Process-Dependent Tasks. 25 tasks are defined and di-\nvided into 5 difficulty levels based on incrementally increasing\nreasoning steps. A higher difficulty level implies that the agent\nneeds to engage in longer reasoning and planning with the envi-\nronment.\nTask Level\nReasoning Step\nExample Task\nBasic\n1-3\ncraft crafting table\nWooden\n4-5\ncraft wooden sword\nStone\n6-9\nmine stone\nIron\n10-11\nsmelt iron ingot\nDiamond\n>11\nobtain diamond\nwithin the environment. We demonstrate the performance\ndifference between Active Perception and other percep-\ntion methods. We compare them with pre-trained multi-\nmodal large language models LLaVA-1.5 [20] and GPT-\n4V [25], and analyze the performance of both active and\nfine-grained global perception on the tasks in Tab. 3. Al-\nthough fine-grained global perception can obtain compre-\nhensive perceptual information, due to the lack of objective-\nconditioned attention, the objective-related information ob-\ntained may be lacking or incorrect. Active perception only\nfocuses on objective-related information and ignores other\nuseless information, so that more accurate objective-related\ninformation can be obtained and better performance in\nContext-Dependent Tasks can be achieved. For the compari-\nson, we use MineLLM, which is fine-tuned on the Minecraft\ninstruction dataset we collect, slightly better than GPT-\n4V [25], which is trained on massive data, and substantially\nbetter than LLaVA-1.5 [20], which is not fine-tuned on in-\nstruction data. The complete results of Context-Dependent\nTasks can be found in Sup.D.2.\n4.2.2\nResults of Process-Dependent Tasks\nIn Process-Dependent Tasks, we report the performance\nof the agent in completing long-horizon tasks by contin-\nuously perceiving the environment context and dynami-\ncally adjusting its actions. We also investigate the agent’s\nbehavior in scenarios of non-situation-aware planning and\nnon-embodied action execution. The complete results of\nProcess-Dependent Tasks can be found in Sup.D.2.\nIn considering the landscape of related works [1, 12, 34,\n36, 43], we refrain from making direct comparisons due to\nthe substantial variations in the observation space, action\nspace, environmental setup, and game termination con-\nditions. Notably, VPT [1] emulates human players’ key-\nboard and mouse controls, DreamerV3 [12] is trained from\nscratch for diamond collection\nin a modified Minecraft\nenvironment with altered block-breaking mechanics using\nworld models, DEPS [36] integrates LLM planning and a\nlearning-based control policy based on MineDojo [9] ac-\ntions, GITM [43] employs privileged information such as\nlidar perception, and Voyager [34] utilizes purely text-based\nTable 3.\nPerformance on Context-Dependent Tasks.\nWe com-\npare the success rate of different Methods and different Perception\nstrategies. We set up special prompt to make the output of the cap-\ntion as comprehensive as possible, this perception method is called\nFine-Grained Global Perception. We use A to denote Active Per-\nception, and G to denote Fine-Grained Global Perception.\nMethod\nStrategy\nAverage Success Rate(%)\nEasy\nMid\nHard\nComplex\nLLaVA-1.5 [20]\nG\n47.5\n22.5\n5.0\n0.0\nA\n72.5\n50.0\n11.0\n0.0\nGPT-4V [25]\nG\n97.5\n85.0\n75.0\n60.0\nA\n100.0\n94.5\n92.5\n87.5\nMP5(Ours)\nG\n90.0\n82.5\n77.5\n67.5\nA\n98.5\n94.5\n93.0\n91.0\ninformation perception in collaboration with the Mineflayer\nAPI for action. Given that our experiments aim to showcase\nthe system’s capability to adapt both process-dependent rea-\nsoning and complex context-understanding tasks, our focus\nturns to presenting two key insights drawn from the sys-\ntem’s performance, as detailed below.\nEmbodied action execution is critical for open-ended\ntasks. Comparing MP5 w\/o E. and MP5 in Tab. 4, we\ncan observe that when an agent is unable to interact with\nthe environment and access low-level environment contex-\ntual information during action execution, it essentially be-\ncomes “blind”, unable to determine the termination of its\nactions based on environment. Therefore, the success rate\nin Process-Dependent Tasks is 0.00%.\nSituation-aware\nplanning\nleads\nto\nmore\nscenario-\nappropriate strategies. Comparing MP5 w\/o P. and MP5\nin Tab. 4, we observe that the lack of environment con-\ntextual information during the agent’s planning process can\nlead to erroneous or redundant actions, thereby reducing the\nsuccess rate (for example, the success rate in diamond-level\ntasks decrease from 22.00% to 14.00%). Consider a sce-\nnario where the current sub-objective is “kill a pig\n”. If\na pig\nis already present, the agent should directly ex-\necute “move” to approach without the need to first “find”\nthen “move”. However, the relatively small decrease in the\nsuccess rate can be attributed to the dynamic adjustment of\nperception and action execution offered by embodied ac-\ntion execution. Simultaneously, when errors are detected,\nthe perceived environmental information and the erroneous\nactions can be fed back to the planner for re-planning.\n4.2.3\nOpen-Ended Tasks\nProcessing long-horizon reasoning and understanding com-\nplex contexts are interconnected in the real world. For sim-\nplicity and comparability of the experimental setup, the first\ntwo task settings do not consider the intersection of pro-\ncess and context, as we cannot exhaust all combinations\nTable 4. Performance on Process-Dependent Tasks. We compare\nthe success rate when interacting or not interacting with the en-\nvironment during the planning or execution. w\/o P. and w\/o E.\nindicates non-situation-aware planning and non-embodied action\nexecution.\nMethod\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\nMP5 w\/o P.\n0.00\n0.00\n0.00\n0.00\n0.00\nMP5 w\/o E.\n92.00\n86.00\n68.67\n45.33\n14.00\nMP5\n96.00\n88.67\n76.00\n52.00\n22.00\n1. Mine 2 logs\n2. Craft 8 planks\n3. Craft 4 sticks\n4. Craft 1 \nCrafting table\n5. Craft 1 wooden \nshovel\n6. Dig a block of sand near the water at \nnight, with a wooden shovel\n0\n1\n2\n3\n4\nProcess\nContext #\nTask: Dig a block of sand\nnear the water at night,\nwith a wooden shovel\nFigure 5. Screenshots of “Dig a block of sand\nnear the water\nat night\nwith a wooden shovel\n”. In Open-Ended Tasks,\nthe agent needs to better integrate low-level context information\nand high-level decision-making, making it extremely challenging.\nTable 5. Success rates for different MLLMs and pre-trained visual\nencoders in the percipient on Context-Dependent Tasks\nMethod\nVisual\nAverage Success Rate(%)\nEncoder\nEasy\nMid\nHard\nComplex\nLLaVA-1.5 [20]\nCLIP [30]\n72.50\n50.00\n11.00\n0.00\nMineLLM\nCLIP [30]\n95.00\n90.00\n87.00\n80.00\nMineLLM\nMineCLIP [9]\n98.50\n94.50\n93.00\n91.00\nthat these two task dimensions can form. Therefore, we\nrefer to tasks that incorporate both Process-Dependent and\nContext-Dependent elements as Open-Ended Tasks. Specif-\nically, these tasks require the agent to perceive different in-\nformation of the environment at multiple stages of complet-\ning sub-objectives. As shown in Fig. 5, we present an exam-\nple of an Open-Ended Task, named “Dig a block of sand\nnear the water\nat night\nwith a wooden shovel\n”. We\nconduct extensive validations on this type of task, proving\nthat MP5 can complete long-sequential tasks in challeng-\ning environments. More demonstrations and experimental\nresults of Open-Ended Tasks can be found in Sup.F.3.\n4.3. Ablation Study\nWe conduct ablation studies to evaluate the effectiveness of\nvarious modules. The experimental setup and the associated\nsuccess rates are in Sec. 4.1. More detailed ablation studies\nare listed in Sup.D.3. The following paragraphs present the\nanalyses derived from our ablation studies.\nModel pre-trained on massive data of Minecraft can bet-\nter comprehend the Minecraft appearance styles. We\nconduct ablation studies on the multi-modal large language\nmodel (MLLM) part within Context-Dependent Tasks in\nTable 6. Success rates for different LLMs as zero-shot Planner on\nProcess-Dependent Tasks\nPlanner\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\nVicuna-13B-v1.5 [5]\n1.33\n0.00\n0.00\n0.00\n0.00\nGPT-3.5-turbo [23]\n95.33\n86.67\n42.00\n2.67\n0.00\nGPT-4 [26]\n96.00\n88.67\n76.00\n52.00\n22.00\nTab. 5, comparing the performance outcomes of different\nMLLMs and different pre-trained visual encoders in the\npercipient.\nWe find the performance of the open-source\nmodel LLaVA-1.5 [20] to be relatively weak, with a suc-\ncess rate of merely 50.00% at the Mid level and 11.00% on\nthe Hard level. This is primarily due to the model’s train-\ning predominantly on real-world data, causing it to strug-\ngle with the pixel-style image recognition characteristic of\nMinecraft. We also discover that, when the visual encoder is\nfrozen, the MineLLM with CLIP [30] as its visual encoder\nconsistently performs worse across all levels compared to\nMineLLM with MineCLIP’s [9] pre-trained single image\nvisual encoder. It may caused by, in the case of a frozen vi-\nsual encoder, a visual encoder pretrained on massive data of\nMinecraft can align with pixel-style images more rapidly.\nEnhanced reasoning ability results in improved plan-\nning.\nWe compare the performance of open-source\nlarge language models, OpenAI’s GPT-3.5-turbo [23] in\nTab. 6, and GPT-4 [26] as zero-shot Planners on Process-\nDependent Tasks. We find that as the models’ inferential\ncapabilities increase, the Planner produces better results\nby planning in a situation-aware method, yielding more\nconcise and accurate execution actions. The Vicuna-13B-\nv1.5 [5] model, when used as a Planner, struggles to pro-\nduce effective plans, achieving only a 1.33% accuracy rate\nat the Basic level\n.\nGPT-4 [26] exhibits the best per-\nformance, attaining a 22.00% success rate at the Diamond\nlevel\n, whereas both Vicuna-13B-v1.5 [5] and GPT-3.5-\nturbo [23] score 0.00%.\nLeveraging memory leads to better planning.\nIn our\nPerformer Memory, we store previously successful sub-\nobjectives and their corresponding execution actions. When\nplanning in similar scenarios, Performer Memory can pro-\nvide the Planner with similar execution action plans for\ncompleting the sub-objectives. While the plans may not be\nidentical, they can effectively assist the Planner in perform-\ning situation-aware planning. Comparing the first and last\nrows of Tab. 7, we find that without the Performer Memory,\nthe success rate of tasks at all levels decreases (Diamond\nlevel\ndrops from 22.00% to 16.67%). However, the de-\ncrease is not significant as the Performer Memory primarily\nserves a reference function, with specific action planning\nstill heavily reliant on the Planner’s capabilities.\nRobustness is essential in open-world settings. To en-\nhance the robustness evaluation of our system, we introduce\na “Random Drop” setting. In this setting, we randomly dis-\nTable 7.\nSuccess rates on different modules within Process-\nDependent Tasks: We study the roles of the Performer Mem-\nory (PM) and the check part of Patroller (P), with ’RD’ denoting\n“Random Drop” setting. ✓denotes the inclusion of the module or\nsetting, and ✗indicates its absence.\nPM\nP\nRD\nAverage Success Rate(%)\nBasic\nWooden\nStone\nIron\nDiamond\n✗\n✓\n✗\n96.00\n87.33\n67.33\n47.33\n16.67\n✓\n✗\n✓\n70.00\n7.33\n0.67\n0.00\n0.00\n✓\n✓\n✓\n87.33\n76.67\n45.33\n18.67\n1.33\n✓\n✓\n✗\n96.00\n88.67\n76.00\n52.00\n22.00\ncard one complete sub-objective from the inventory at the\nstart of each new sub-objective, which deliberately induces\nexecution errors for the agent. Comparing the second and\nthird lines in Tab. 7, we observe the critical role of the Pa-\ntroller in recognizing feedback errors. The Patroller’s abil-\nity to integrate current environmental information with error\ninformation is essential for enabling the planner to re-plan.\nThe significance of this robustness is evident when exam-\nining the success rates. Without the Patroller’s robustness,\nthe agent’s success rate on the Wooden level\nplummets\nfrom 76.67% to 7.33%, while success rates on the Iron\n,\nand Diamond\nlevels drop to 0.00%. Details regarding the\n“Random Drop” setting can be found in Sup.D.3.\n5. Conclusion\nIn this paper, we propose a novel multi-modal embodied\nsystem termed MP5 which is driven by frequently ego-\ncentric scene perception for task planning and execution. In\npractice, it is designed by integrating five functional mod-\nules to accomplish task planning and execution via actively\nacquiring essential visual information from the scene. The\nexperimental results suggest that our system represents an\neffective integration of perception, planning, and execu-\ntion, skillfully crafted to handle both context- and process-\ndependent tasks within an open-ended environment.\nLimitation and Future Work. Despite the impressive re-\nsults of our approach, two major limitations need to be clar-\nified. Firstly, the reliance on GPT-3.5-turbo [23] or GPT-\n4 [26] limits the system’s usability, as not everyone has ac-\ncess to these APIs. Secondly, the scope of the applied sim-\nulation platform is limited. Despite showing promising per-\nformance in Minecraft, we haven’t extended our exploration\nto other simulation platforms, which is a potential area for\nfurther research.\nAcknowledgement.\nThis work was supported by the\nNational Key R&D Program of China (2021YFB1714300),\nthe\nNational\nNatural\nScience\nFoundation\nof\nChina\n(62106154,\n62132001),\nthe Natural Science Founda-\ntion of Guangdong Province, China (2022A1515011524).\nReferences\n[1] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampe-\ndro, and Jeff Clune. Video pretraining (vpt): Learning to act\nby watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639–24654, 2022. 2,\n3, 5, 6\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877–1901, 2020. 2\n[3] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao\nLiang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. In\nProceedings of the IEEE\/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 13734–13744, 2023. 2\n[4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm’s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 2\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality.\nSee\nhttps:\/\/vicuna. lmsys. org (accessed 14 April 2023), 2023.\n2, 5, 8, 10, 12\n[6] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 2\n[7] Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang,\nand Zongqing Lu. Clip4mc: An rl-friendly vision-language\nmodel for minecraft.\narXiv preprint arXiv:2303.10571,\n2023. 2\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3\n[9] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar.\nMinedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems, 35:\n18343–18362, 2022. 2, 3, 4, 5, 6, 7, 8, 1\n[10] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-\nsual instruction model.\narXiv preprint arXiv:2304.15010,\n2023. 2\n[11] William H Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan\nSalakhutdinov.\nMinerl: a large-scale dataset of minecraft\ndemonstrations.\nIn Proceedings of the 28th International\nJoint Conference on Artificial Intelligence, pages 2442–\n2448, 2019. 2\n[12] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2, 6\n[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 5, 2\n[14] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor\nMordatch. Language models as zero-shot planners: Extract-\ning actionable knowledge for embodied agents. In Interna-\ntional Conference on Machine Learning, pages 9118–9147.\nPMLR, 2022. 2\n[15] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan,\nXiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui\nHuang, Ruimao Zhang, et al. Smartedit: Exploring com-\nplex instruction-based image editing with multimodal large\nlanguage models. arXiv preprint arXiv:2312.06739, 2023. 2\n[16] Taku Kudo and John Richardson.\nSentencepiece:\nA\nsimple and language independent subword tokenizer and\ndetokenizer for neural text processing.\narXiv preprint\narXiv:1808.06226, 2018. 2\n[17] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni,\nVladimir Karpukhin, Naman Goyal, Heinrich\nK¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al.\nRetrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems,\n33:9459–9474, 2020. 3\n[18] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and\nSheila McIlraith. Steve-1: A generative model for text-to-\nbehavior in minecraft.\narXiv preprint arXiv:2306.00937,\n2023. 2\n[19] Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu,\nand Wei Yang. Juewu-mc: Playing minecraft with sample-\nefficient hierarchical reinforcement learning. arXiv preprint\narXiv:2112.04907, 2021. 2\n[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 5, 6, 7, 8\n[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2\n[22] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet\nKohli.\nZero-shot task generalization with multi-task deep\nreinforcement learning. In International Conference on Ma-\nchine Learning, pages 2661–2670. PMLR, 2017. 2, 3\n[23] OpenAI. Introducing chatgpt. 2022. 8, 2, 10, 12\n[24] OpenAI. New and improved embedding model. 2022. 3\n[25] OpenAI. Gpt-4v(ision) system card. 2023. 6, 7\n[26] R OpenAI.\nGpt-4 technical report.\narXiv, pages 2303–\n08774, 2023. 5, 8\n[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730–27744, 2022. 2\n[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 2\n[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 2\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 7, 8\n[31] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez\nColmenarejo, Alexander Novikov, Gabriel Barth-Maron,\nMai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin-\ngenberg, et al.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022. 2\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 2\n[33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 2, 5, 8, 12\n[34] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023.\n2, 6, 3\n[35] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong\nZheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-\ntask agents with memory-augmented multimodal language\nmodels. arXiv preprint arXiv:2311.05997, 2023. 2\n[36] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao\nLiang. Describe, explain, plan and select: Interactive plan-\nning with large language models enables open-world multi-\ntask agents. arXiv preprint arXiv:2302.01560, 2023. 2, 6\n[37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824–24837, 2022. 9, 10\n[38] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 2\n[39] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingn-\ning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang,\nZhiyong Wang, et al.\nLamm: Language-assisted multi-\nmodal instruction-tuning dataset, framework, and bench-\nmark. arXiv preprint arXiv:2306.06687, 2023. 2\n[40] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie,\nPenglin Cai, Hao Dong, and Zongqing Lu.\nPlan4mc:\nSkill reinforcement learning and planning for open-world\nminecraft tasks. arXiv preprint arXiv:2303.16563, 2023. 2\n[41] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang,\nRuimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Mine-\ndreamer:\nLearning to follow instructions via chain-of-\nimagination for simulated-world control, 2024. 2\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 2\n[43] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-\njie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiao-\ngang Wang, et al. Ghost in the minecraft: Generally capable\nagents for open-world enviroments via large language mod-\nels with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023. 2, 5, 6\nMP5: A Multi-modal Open-ended Embodied System\nin Minecraft via Active Perception\nSupplementary Material\nThe supplementary document is organized as follows:\n• Sec. A: Discussion of MP5.\n• Sec. B: Percipient, Memory, Observation Space and\nAction Space.\n• Sec. C: Environment Setting.\n• Sec. D: Task Details, Sucess Rates of All Tasks and\nAblation Study.\n• Sec. E: Different Strategy of Active Perception.\n• Sec. F: Applications Demonstration of MP5.\n• Sec. G: Interactions in MP5.\nA. Discussion of MP5\nAs shown in Fig. 6, existing methods usually follow the\nparadigm that an agent should be with the ability of plan-\nning (e.g., GITM & Voyager), embodiment (e.g., Dream-\nerV3), or both of them (e.g., DEPS). To be different, MP5\nintroduces a new paradigm that these components (i.e., em-\nbodiment, planning, and execution as well), should be en-\nhanced with the awareness of the situation (i.e., rich con-\ntextual and procedural information w.r.t. the task), which is\nmore human-like and has the potential to solve more dif-\nficult open-end context- and process-dependent tasks (as\nevaluated in Sec. 4.2.3). (2) Technically, MP5 comprises\nfive interactive modules to meet the requirement of the new\nparadigm, with an MLLM-based active perception scheme\nto fulfil the situation awareness. To our best knowledge,\nMP5 is the first embodied system in Minecraft that is ca-\npable of situation-aware planning and action execution. (3)\nOnly our method can solve Context-Dependent Tasks (in\nTab. 8) by leveraging the situation-aware planning and exe-\ncution. We constructed a detailed benchmark (in Sec. 4) to\nexplore how agents complete tasks required complex rea-\nsoning and constrained by extensive environmental infor-\nmation.\nWe have provided a comparison of setups and their con-\nsequences in Tab. 8. It tells that the proposed MP5 applies\nego-centric RGB images, just utilizes a restrained set of\nEmbodiment\nDEPS\nMP5(ours)\nPlanning\nSituation awareness\nDreamerV3\nGITM\nVoyager\nFigure 6. Paradigm innovation.\nhuman-defined primitives, but can solve the most challeng-\ning context-dependent and process-dependent tasks.\nB. Implementation Details\nB.1. Percipient\nB.1.1\nData Collection\nFor data collection,\nwe use Minedojo [9] to obtain\nMinecraft snapshots which contain a wide array of details\nwithin the agent’s surroundings, including blocks, biomes,\nmobs and etc. Following the environment creation, we en-\nable our agent to perform a rotation on the spot, capturing\nsnapshots from 12 distinct perspectives spaced 30 degrees\napart. For each of these snapshots, we record the ground-\ntruth information about the agent’s surroundings by leverag-\ning the data available in the MineDojo [9] observation space\nsuch as Lidar. To ensure the exact correspondence between\nthe ground-truth information and the image, the informa-\ntion corresponding to the Field of View region of the image\nis screened from the Lidar as the ground-truth information\nof the image.\nTo compile a comprehensive dataset encompassing var-\nious conditions and terrains in Minecraft, we implement\na two-step data collection process: acquiring data related\nto different biomes and gathering data on different mobs.\nIn the first step dedicated to gathering data on diverse\nbiomes, we collect information from all 60 biomes avail-\nable in MineDojo [9]. For each biome, we sample 20 en-\nvironments, resulting in a total of 7.2K images.\nIn the\nsecond phase of gathering data for various mobs, our fo-\ncus is on collecting images of 9 commonly found mobs\nin the Minecraft world: zombies, skeletons, creepers, spi-\nders, cows, chickens, sheep, pigs, and wolves. We specifi-\ncally choose 30 representative biomes from the available 60\nTable 8. Explicit comparisons of setups and consequences.\nMethod\nObservation Space\nAction Space\nInstruct\nSituation-\nSituation-\nTasks Agents Can Perform\nInfo\nNot\nAction\nAction\nPrimitive\nFollowing\naware\naware\nProcess\nContext\nProcess & Context\nType\nOmniscient\nType\nNum\nLibrary Size\nPlan\nExecution\n(Long-Horizon Tasks)\n(High Env. Info Tasks)\n(Combination of 2 Tasks)\nDreamerV3\nRGB\n✓\nOriginal MineRL\n25\n0\n✗\n✗\n✗\n✓\n✗\n✗\nDEPS\nRGB\n✓\nCompound (MineDojo)\n42\n0\n✓\n✗\n✗\n✓\n✗\n✗\nGITM\nText\n✗\nManual (MineDojo)\n9\n9 + corner cases\n✓\n✗\n✗\n✓\n✗\n✗\nVoyager\nText\n✗\nJavaScript APIs\nN\/A\nAll\n✓\n✗\n✗\n✓\n✗\n✗\nMP5(ours)\nRGB\n✓\nCompound (MineDojo)\n10\n4\n✓\n✓\n✓\n✓\n✓\n✓\nTable 9. Comparison of Observation Spaces Among Different Methods\nMethod\nPerceptual Observation\nStatus Observation\nGITM [43]\nLiDAR rays\nlife statistics\n10 × 10 × 10 Voxels\nGPS, inventory, equipment\nDreamV3 [12]\nEgo-View RGB\nlife statistics\ninventory, equipment\nVPT [1]\nEgo-View RGB\n∅\nDEPS [36]\nEgo-View RGB\nCompass\n3 × 3 × 3 Voxels\nGPS, equipment\nJARVIS-1 [35]\nEgo-View RGB\nlife statistics\nGPS, inventory, equipment\nlocation status (biome, weather, etc.)\nMP5(ours)\nEgo-View RGB\nlife statistics\n3 × 3 × 3 Voxels\nGPS, inventory, equipment\nMinecraft biomes for this data batch. Among these 9 types\nof mobs, the first four exclusively appear during the night,\nwhile the remaining five can be encountered both during the\ndaytime and nighttime. For the mobs that appear in both pe-\nriods, each mob type is generated across 30 biomes, with 20\nenvironment samples (10 during the daytime and 10 during\nthe nighttime). This results in the creation of 36K images\nfor these five mobs. As for the mobs exclusive to nighttime,\nthey are generated in 30 biomes, with 10 nighttime envi-\nronment samples per biome and 12 images per environment\nsample, culminating in the generation of 7.2K images.\nThe data obtained from both the first and second stages\ncontribute to a comprehensive dataset totaling 50K images,\nand we prompt ChatGPT [23] to curate a list of instructions\nto obtain 500K image-text instruction-following data.\nB.1.2\nMineLLM training details\nMineLLM combines the image visual encoder from\nMineCLIP [9] and the large language models from Vicuna-\n13B-v1.5 [5]. Images are processed by the frozen vision\nencoder, whose features are projected by a two-layer MLP\nnamed Alignment Net to the same feature space as the text\nembeddings of the applied LLM. Instructions are tokenized\nby SentencePiece tokenizer [16], and then the vision and\ntext tokens are concatenated to feed into the LLM model. To\nbetter align the feature space of visual image encoder from\nMineCLIP [9] and large language model from Vicuna [5],\nwe collect 500K image-text instruction-following data on\nthe MineDojo [9] following the method detailed in Ap-\npendix B.1.1, for the purpose of training MineLLM. Each\ntraining instance consists of an image I and a multi-turn\nconversation data (x1, y1, . . . , xn, yn), where xi and yi\nare the human’s instruction and the system’s response at the\ni-th turn. To train MineLLM efficiently, we add LoRA [13]\nparameters to all projection layers of the self-attention lay-\ners in the LLM. Only the parameters of the Alignment Net\nand the LoRA [13] module are optimized during training.\nMultimodal tokens are decoded by the LLM model and the\ncorresponding LoRA [13] parameters.\nThe training objective of Percipient is defined as:\nL (θa, θl) =\nn\nY\ni=1\npθ\n\u0000yi | x<i, y<i−1, f (I)\n\u0001\n,\n(1)\nwhere θa and θl correspond to the learnable parame-\nters of the Alignment Net and LoRA [13]. The I is the\nimage representation produced by the visual encoder from\nMineCLIP [9] and θ = {θa, θl, θm, θv}, where θm and θv\nare frozen parameters of MineCLIP [9] and Vicuna-13B-\nv1.5 [5]. It is worth noting that during the training process,\nonly system message responses denoted as yi, require loss\ncomputation. Note that the loss is only computed from the\npart of system responses during training.\nwhile training MineLLM, trainable parameters(i.e., θa\nfrom the Alignment Net and θl from LoRA [13]) are op-\ntimized by Adam optimizer with a learning rate initialized\nto be 5e −4, and scheduled using a linear decay scheduler.\nThe rank of LoRA [13] modules is set to 32. We train all\nparameters in a one-stage end-to-end fashion with 8 A100\nGPUs. Each GPU process 2 samples every iteration and\nthe effective batch size is set to 128 by gradient accumula-\ntion. Input images are resized to be 224 × 224 and we use\nMineCLIP [9] pre-trained ViT-B\/16 [8] as visual encoder,\nthe number of vision tokens are 196 and length of text to-\nkens after vision tokens are limited to 400 in training.\nB.2. Memory\nInspired by the Skill library of Voyager [34], memory is uti-\nlized in two parts of MP5 to perform Retrieval-Augmented\nGeneration (RAG [17]). The Parser employs Knowledge\nMemory to decompose tasks into sub-objectives, while the\nPlanner, when planning an action sequence for a specific\nsub-objective, may refer to similar action sequences pro-\nvided by Performer Memory. The implementation details\nare similar to those of Voyager [34].\nB.2.1\nKnowledge Memory\nFor Knowledge Memory, we actually adopt a vector\ndatabase method (e.g., Chroma, FAISS, etc.) to store fre-\nquently used knowledge. This knowledge mainly comes\nfrom three sources: part of it is from the online wiki, an-\nother part is from the crafting recipes of items in Mine-\nDojo [9], and some are user tips from Reddit.\nSpecifi-\ncally, we convert commonly used knowledge into corre-\nsponding text embeddings using OpenAI’s text-embedding-\nada-002 [24] and store them in a vector database. When de-\ncomposing sub-objectives requires the retrieval of relevant\nknowledge, we also convert the corresponding descriptions\nof these sub-objectives into corresponding text embeddings.\nWe then perform a search match in the database and select\nthe most similar piece of knowledge. If the similarity score\nat this time is below 0.05 (the lower the score, the more sim-\nilar), it is directly taken as the result of the RAG [17]. Of\ncourse, there will also be cases where the similarity scores\nare all above 0.05. This indicates that there is currently no\nsuch type of knowledge in the database. In this case, we\nmanually supplement this type of knowledge and add it to\nthe database as the result of the RAG [17].\nB.2.2\nPerformer Memory\nFor Performer Memory, we record the task description of\neach successful sub-objective and its corresponding suc-\ncessful action sequence. Specifically, Performer Memory\nconsists of two parts. One part is a vector database used\nto store the sub-objective task descriptions and their corre-\nsponding positions in the sub-objective sequence. The other\npart is a JSON file where the key is the position of the sub-\nobjective in the sub-objective sequence, and the value corre-\nsponds to the sub-objective task description and its success-\nful action sequence. When we need to find similar action\nsequences, similar to Knowledge Memory, we convert the\ncurrent sub-objective’s task description into corresponding\ntext embeddings and retrieve the 2 closest matches from the\nvector library. We then extract the corresponding successful\nobjective sequences from the JSON file using their positions\nin the sub-objective sequence.\nB.3. Observation Space\nIn order to allow the system to more closely resemble an\nembodied agent rather than emulating a game player un-\nlocking the tech tree, we significantly limited environmen-\ntal information, endeavoring to enable the agent to perceive\nthrough Ego-View RGB images as much as possible.\nOur Observation Space primarily consists of two com-\nponents: one is the Perceptual Observation, and the other\nis the Status Observation. The Perceptual Observation in-\ncludes Ego-View Minecraft-style RGB images and 3×3×3\nVoxels that the agent encounters. The Status Observation\nincludes some associated auxiliary textual information(e.g.,\nthe current agent’s life statistics, GPS location, inventory,\nand equipment information).\nNotably, to make the sys-\ntem more resemble an embodied agent, we have obscured\na large amount of environmental information (e.g., the cur-\nrent biome, weather, and whether the sky is visible that hu-\nman players can learn by pressing F3). This encourages\nthe agent to perceive through the current RGB image rather\nthan directly knowing a lot of the current environmental in-\nformation.\nTo more clearly demonstrate our Observation Space, we\nlist the differing Observation Spaces of related works in the\ntable below, as shown in Table 9.\nB.4. Action Space\nThe Performer module executes action sequences, which\nconsist of actions falling within the action space outlined\nin Tab 10. These actions are brief combinations formed by\nthe MineDojo [9] API, with frequent interactions with the\nenvironment occurring within each action.\nFor example, the action of “Find” can be described as\na directionless forward motion, initiating a jump when en-\ncountering obstacles. If the obstacle proves insurmountable,\nthe action adapts by implementing a left or right turn, fol-\nlowed by the continuation of forward motion. This process\ninvolves minimal human intervention or design. During the\nexecution of the “Find” action, there is a fixed frequency at\nwhich the current Ego-View RGB images are analyzed to\nascertain whether the required object (e.g., a block, a type\nof mob, etc.) has been in sight.\nC. Environment Setting\nOur Minecraft experimental environment is based on the\nMineDojo [9] simulation platform, which provides a uni-\nfied observation and action space to foster the development\nTable 10. The Definition of the Action Space we use in MineDojo [9] Simulator\nName\nArguments\nDescription\nCorresponding\nAction Conditions Based on\nMineDojo [9] Actions\nEnvironmental Information\nFind\nobject\nTravel across the present terrain\nforward, jump\nHalt only when the object is\nin search of an object\nmove left and right\nin Ego-View RGB image\nMove\nobject\nMove to the target object until\nforward, jump\nHalt only when the object is in the\nit is within striking distance\nmove left and right\nsurrounding 3 × 3 × 3 Voxels\nCraft\nobject\nCraft a certain number of objects\ncraft, attack\nBegins only once the environmental\nmaterials\nwith materials in the inventory\nuse, place\nconditions required are met\nplatform\nusing the platform\nMine\nobject\nHarvest a single block using\nattack\nBegins only once the environmental\ntool\ntool from surroundings\nconditions required are met\nEquip\nobject\nEquip a given object from\nequip\nBegins only once the environmental\nthe current inventory.\nconditions required are met\nFight\nobject\nAttack a nearby entity\nattack\nBegins only once the environmental\ntool\nusing the specified tool\nconditions required are met\nDig-Up\ntool\nAscend directly by jumping\njump, place\nHalt only when the agent\nand placing blocks\ncan see the sky\nDig-Down\ny-level\nDescend using the specified tool\nattack\nHalt only when the agent\ntool\nto dig your way through if necessary\nreach the specified y-level\nUse\nobject\nUse the item held\nuse\n∅\nin the main hand\nPlace\nobject\nPlace an inventory\nplace\nBegins only once the environmental\nitem on the ground.\nconditions required are met\nTable 11. Full Context-Dependent Tasks. 16 tasks are defined and divided into 4 difficulty levels based on the minimum number of\ninformation types needed. Underlines label the environmental information, reflecting the complexity varies at each level.\nTask Level\nTask id\nTask description\nEasy\n1-1\nFind a tree\n1-2\nFind a grass\n1-3\nFind a cow\n1-4\nFind a pig\nMid\n2-1\nFind a tree\nin the forest\n2-2\nFind a grass\nnear a pig\n2-3\nFind a cow\nin the desert\n2-4\nFind a pig\nduring the nighttime\nHard\n3-1\nFind a tree\nin the forest\nduring the nighttime\n3-2\nFind a grass\nnear a pig\nin the plains\n3-3\nFind a cow\nin the desert\nduring the daytime\n3-4\nFind a pig\nduring the nighttime\nin a rainy day\nComplex\n4-1\nFind a tree\nin the forest\nduring the nighttime\nin a sunny day\n4-2\nFind a pig\nnear a grass\nin the forest\nduring the daytime\n4-3\nFind a cow\nnear the water\nin the desert\nduring the daytime\nin sunny day\n4-4\nFind a pig\nduring the daytime\non the plains\nwith a grass\nnext to it,\nthe weather is sunny day and the brightness is sufficient\nof intelligent agents capable of multitasking and continuous\nlearning to adapt to new tasks and scenarios.\nIn our experiments, the position at which the agent be-\ngins its game, as well as the seed used to generate the envi-\nronment, are both randomized. This introduces an element\nof unpredictability and variety into the experimental setup,\nensuring that the agent will encounter a wide range of sce-\nnarios and challenges. The agent is set to start in survival\nmode, the most challenging and interactive mode available.\nUnlike creative or adventure modes, survival mode repre-\nTable 12. Details of Context-Dependent Tasks Environment Information content.\nTask Level\nTask id\nNum of Info.\nObject\nCreature\nEcology\nTime\nWeather\nBrightness\nEasy\n1-1\n1\n✓\n1-2\n1\n✓\n1-3\n1\n✓\n1-4\n1\n✓\nMedium\n2-1\n2\n✓\n✓\n2-2\n2\n✓\n✓\n2-3\n2\n✓\n✓\n2-4\n2\n✓\n✓\nHard\n3-1\n3\n✓\n✓\n✓\n3-2\n3\n✓\n✓\n✓\n3-3\n3\n✓\n✓\n✓\n3-4\n3\n✓\n✓\n✓\nVery Hard\n4-1\n4\n✓\n✓\n✓\n✓\n4-2\n4\n✓\n✓\n✓\n✓\n4-3\n5\n✓\n✓\n✓\n✓\n✓\n4-4\n6\n✓\n✓\n✓\n✓\n✓\n✓\nsents a test of the agent’s ability to strategize, and make\nquick decisions. The agent is also confronted with the com-\nplication of hostile mob generation. The agent begins its\ngame with an empty inventory, meaning it must actively\nmine and craft the objects.\nTo simulate a real Embod-\nied Agent, environmental factors(e.g., time, weather, etc.)\nchange over time. At night, the agent does not have night\nvision, and the items in the inventory will be cleared upon\ndeath.\nTo\nbetter\nevaluate\nContext-Dependent\nTasks\nand\nProcess-Dependent Tasks, which are defined in detail in\nAppendix D.1, we select different environment settings\nin MineDojo [9].\nFor Context-Dependent Tasks, we\nuniformly adopt the environment in MineDojo [9] with the\ncreative “task id” of “0”.\nFor Process-Dependent Tasks,\nwe uniformly adopt the environment with the “task id” of\n“harvest”, “target names” as “diamond”, and “spawn rate”\nas “1”. This is why obtaining redstone is more difficult\nthan obtaining diamond, as described in Appendix D.2.2.\nD. Task Details and Experiment Results\nD.1. Task Details\nD.1.1\nContext-Dependent Tasks\nContext-Dependent Tasks primarily study how Active Per-\nception enables the agent to better perceive low-level con-\ntext information in the environment.\nWe first establish\n6 aspects of environmental information derived from the\nMinecraft game environment:\n[Object, Mob, Ecology,\nTime, Weather, Brightness]. Each aspect has multiple op-\ntions. For example, pigs\n, cows\n, and sheep\nare all el-\nements belonging to Mob. Based on this, we define 16 tasks\nand organize their difficulty into 4 levels by taking into ac-\ncount the number of information elements that require per-\nception, as is shown in Tab. 11. Easy tasks necessitate the\nperception of only one element, Mid tasks include 2 per-\nception elements, Hard tasks contain 3 elements, whereas\nComplex tasks involve the perception of 4 to 6 elements.\nEach task at the same level has different environment in-\nformation content, the amount of environment information\ncontained in each task, and the corresponding specific envi-\nronment information is shown in Tab. 12. Finally, we rig-\norously assess MP5’s proficiency in environmental context\nperception across these 16 tasks.\nAs the main paper states, our initial environmental de-\ntails are predetermined (e.g., biomes) in order to reduce the\nagent’s exploration time, otherwise, the agent may fail to\nfind the corresponding scenario within the time limit. We\ndefined ten initial biome, each of which used random seeds\nto generate five different environments to test each task, so\neach task was tested in 50 different scenarios and the suc-\ncess rate was calculated to verify MP5’s generalization abil-\nity. In order to align as much as possible with the experi-\nmental Settings of other methods, we did not modify the\nterrain to simplify the task.\nD.1.2\nProcess-Dependent Tasks\nProcess-Dependent Tasks primarily investigate situation-\naware planning and embodied action execution, incorporat-\ning contributions from Active Perception and other mod-\nules that continuously perceive the environment and dy-\nnamically adjust their actions to accomplish long-horizon\ntasks. In Table 13, we list the names of all tasks in Process-\nTable 13. Detailed Definition of Process-Dependent Tasks. 25 tasks are defined and divided into 5 difficulty levels based on incrementally\nincreasing reasoning steps. A higher difficulty level implies that the agent needs to engage in longer reasoning and planning with the\nenvironment.\nTask Level\nTask\nreasoning step\nObject\nFinal recipe\nTools\/Platforms\nBasic level\nmine log\n1\n-\n-\nmine sand\n1\n-\n-\ncraft planks\n2\n1*\n-\ncraft stick\n3\n2*\n-\ncraft crafting table\n3\n4*\n-\nWooden level\ncraft bowl\n4\n3*\ncraft boat\n4\n5*\ncraft chest\n4\n8*\ncraft wooden sword\n5\n2*\n+1*\ncraft wooden pickaxe\n5\n3*\n+2*\nStone level\nmine cobblestone\n6\n-\ncraft furnace\n7\n8*\ncraft stone pickaxe\n7\n3*\n+2*\nmine iron ore\n8\n-\nsmelt glass\n9\n1*\nIron level\nsmelt iron ingot\n10\n1*\ncraft shield\n11\n1*\n+6*\ncraft bucket\n11\n3*\ncraft iron pickaxe\n11\n3*\n+2*\ncraft iron door\n11\n6*\nDiamond level\nobtain diamond\n12\n-\nmind redstone\n12\n-\ncraft compass\n13\n1*\n+4*\ncraft diamond pickaxe\n13\n3*\n+2*\ncraft piston\n13\n1*\n+1*\n+4*\n+3*\nDependent Tasks, their reasoning steps, object icons, the fi-\nnal recipe, and the required tools\/platforms. The reasoning\nstep refers to the number of sub-objectives that need to be\ncompleted in order to finish the entire task. Given that the\nagent’s environment information(e.g., biome, weather, etc.)\nis randomly initialized, there may be execution errors re-\nquiring replanning, thus potentially necessitating the com-\npletion of additional sub-objectives, which means more rea-\nsoning steps may be required. We consider only the most\nbasic scenarios and select 25 tasks based on the required\nreasoning steps in increasing order. These tasks are then\ndivided into 5 difficulty levels.\nFor evaluation, we consider an Agent’s accidental death\nin the game (e.g., being burned by lava, killed by a hostile\nmob, etc.) as a failure, as well as not achieving the objective\nwithin the time limit (e.g., exceeding the 10 minute game\nlimit, or API request timeout, etc.). We conduct 30 games\nof Process-Dependent Tasks and took the average success\nrate as the final reported performance.\nD.2. Success Rates of All Tasks\nD.2.1\nContext-Dependent Tasks\nWe report the success rates of different methods and per-\nception strategies for all tasks comprehensively and in de-\ntail in Table 14, including ours, GPT-4V [25], and LLaVA-\n1.5 [20], using both Active Perception strategy and Fine-\nGrained Global Perception strategy. This table also presents\nthe detailed results of the “Main Results” section under\n“Context-Dependent Tasks” in the main text.\nD.2.2\nProcess-Dependent Tasks\nWe report the success rates of different methods for all tasks\ncomprehensively and in detail in Table 16, including ours,\nnon-situation-aware planning, and non-embodied action ex-\necution. This table also presents the detailed results of the\n“Main Results” section under “Process-Dependent Tasks”\nin the main text. The parts with a gray background in the\ntable represent the average success rate for the current level.\n1\n2\n4\n3\n5\n6\n7\n8\n9\n10\n11\n12\n13\n3 * 𝟏𝟎𝟑\n1 * 𝟏𝟎𝟑\n2 * 𝟏𝟎𝟑\n4 * 𝟏𝟎𝟑\n5 * 𝟏𝟎𝟑\n6 * 𝟏𝟎𝟑\n7 * 𝟏𝟎𝟑\n8 * 𝟏𝟎𝟑\n9 * 𝟏𝟎𝟑\n1 * 𝟏𝟎𝟒\n0\nReasoning Step\nSteps\nBase Level \nWooden Level \nStone Level \nIron Level \nDiamond Level \nFigure 7. The game-playing steps corresponding to the acquisition of different milestone objects by the agent during the completion of the\ncraft diamond pickaxe challenge. The varying background colors denote the level of the Process-Dependent Tasks in which the milestone\nobjects are located.\nTable 14. Detailed Performance on Context-Dependent Tasks. MethodA means the method uses the Active Perception strategy, and\nMethodG means the method uses the Fine-Grained Global Perception strategy. The parts with a gray background in the table represent the\naverage success rate for the current level.\nTask Level\nTask id\nSuccess rate(%)\nMP5 A\nMP5 G\nGPT-4VA [25]\nGPT-4VG [25]\nLLaVA-1.5A [20]\nLLaVA-1.5G [20]\nEasy\n1-1\n98.0\n94.0\n100.0\n100.0\n88.0\n56.0\n1-2\n100.0\n92.0\n100.0\n100.0\n68.0\n44.0\n1-3\n98.0\n88.0\n100.0\n96.0\n76.0\n42.0\n1-4\n98.0\n86.0\n100.0\n94.0\n58.0\n48.0\nAverage\n98.5\n90.0\n100.0\n97.5\n72.5\n47.5\nMid\n2-1\n98.0\n90.0\n98.0\n82.0\n56.0\n28.0\n2-2\n96.0\n82.0\n90.0\n86.0\n52.0\n14.0\n2-3\n92.0\n88.0\n94.0\n88.0\n44.0\n22.0\n2-4\n92.0\n84.0\n96.0\n84.0\n48.0\n26.0\nAverage\n94.5\n86.0\n94.5\n85.0\n50.0\n22.5\nHard\n3-1\n94.0\n80.0\n96.0\n80.0\n12.0\n8.0\n3-2\n98.0\n78.0\n92.0\n74.0\n8.0\n0.0\n3-3\n90.0\n76.0\n90.0\n74.0\n10.0\n6.0\n3-4\n90.0\n76.0\n92.0\n72.0\n14.0\n6.0\nAverage\n93.0\n77.5\n92.5\n75.0\n11.0\n5.0\nComplex\n4-1\n92.0\n74.0\n90.0\n64.0\n0.0\n0.0\n4-2\n92.0\n70.0\n88.0\n60.0\n0.0\n0.0\n4-3\n86.0\n64.0\n84.0\n58.0\n0.0\n0.0\n4-4\n94.0\n62.0\n88.0\n58.0\n0.0\n0.0\nAverage\n91.0\n67.5\n87.5\n60.0\n0.0\n0.0\nTable 15. Detailed Ablation on Context-Dependent Tasks. The parts with a gray background in the table represent the average success rate\nfor the current level.\nTask Level\nTask id\nSuccess rate(%)\nMineLLM+MineCLIP [9]\nMineLLM+CLIP [30]\nLLaVA-1.5 [20]+CLIP [30]\nEasy\n1-1\n98.0\n98.0\n88.0\n1-2\n100.0\n94.0\n68.0\n1-3\n98.0\n96.0\n76.0\n1-4\n98.0\n92.0\n58.0\nAverage\n98.5\n95.0\n72.5\nMid\n2-1\n98.0\n94.0\n56.0\n2-2\n96.0\n88.0\n52.0\n2-3\n92.0\n88.0\n44.0\n2-4\n92.0\n90.0\n48.0\nAverage\n94.5\n90.0\n50.0\nHard\n3-1\n94.0\n90.0\n12.0\n3-2\n98.0\n90.0\n8.0\n3-3\n90.0\n84.0\n10.0\n3-4\n90.0\n84.0\n14.0\nAverage\n93.0\n87.0\n11.0\nComplex\n4-1\n92.0\n82.0\n0.0\n4-2\n92.0\n84.0\n0.0\n4-3\n86.0\n78.0\n0.0\n4-4\n90.0\n76.0\n0.0\nAverage\n91.0\n80.0\n0.0\nTo better demonstrate the practical performance of MP5\nin Process-Dependent Tasks, we select craft diamond pick-\naxe with a reasoning step of 13 as the challenge. Figure 7\ndepicts the game-playing steps corresponding to each mile-\nstone object (e.g., log\n, plank\n, stick\n, etc.) obtained\nby the agent.\nD.3. Ablation Study\nD.3.1\nContext-Dependent Tasks\nWe conduct ablation studies on the multi-modal large\nlanguage model (MLLM) part within Context-Dependent\nTasks in 15, comparing the performance outcomes of dif-\nferent MLLMs and different pre-trained visual encoders in\nthe percipient.\nD.3.2\nProcess-Dependent Tasks\nIn this section, we present detailed results from our abla-\ntion experiments. Table 18 shows the performance of the\nagent in MP5 after the removal of various modules. Ta-\nble 19 demonstrates the impact on the results when the Plan-\nner is replaced by large language models with inconsistent\nreasoning capabilities, including open-source models like\nLLaMA2-70B-Chat [33] and Vicuna-13B-v1.5-16k [5]. Ta-\nble 20 further explores the contribution of the Memory com-\nponents to the agent’s performance, including Knowledge\nMemory and Performer Memory. Table 21 investigates the\nrobustness gain brought by the check part of the Patroller\nunder “Random Drop” conditions.\nAs seen from the results in Table 18, the agent’s suc-\ncess rate in completing Process-Dependent Tasks signifi-\ncantly decreases after the removal of any modules, with the\nsuccess rate at the Diamond level\nfalling to 0.00% for\nall except when the Patroller is removed. The Percipient\nmainly provides the agent with visual input, the Memory\nprimarily provides the agent with relevant knowledge, the\nParser simplifies the difficulty of online task decomposition\nfor the agent, and the Patroller ensures that each action is\nsufficiently checked for successful execution.\nTable 19 presents detailed results from the Planner abla-\ntion experiments in the “Ablation Study” section of the main\ntext. From this, we can discern that LLMs with stronger rea-\nsoning capabilities demonstrate better understanding when\nfaced with a wide variety of text information inputs, thereby\nfacilitating more effective planning. The poor performance\nof open-source large models like LLaMA2-70B-Chat [33]\nVicuna-13B-v1.5-16k [5] is due to their inadequate ability\nto process long and diverse types of text information. This\ninadequacy is evident at the Wooden level\n, where the suc-\ncess rate has already plummeted to 0.00%.\nAs can be seen from the results in Table 20, both types\nof Memory can enhance the agent’s actions, particularly\nTable 16. Detailed Performance on Process-Dependent Tasks. We compare the success rate when interacting or not interacting with the\nenvironment during the planning or execution. The parts with a gray background in the table represent the average success rate for the\ncurrent level.\nTask Level\nObject\nSuccess rate(%)\nMP5(Ours)\nnon-situation-aware planning\nnon-embodied action execution\nBasic level\nlog\n96.67\n93.33\n0.00\nsand\n96.67\n93.33\n0.00\nplanks\n96.67\n93.33\n0.00\nstick\n96.67\n90.00\n0.00\ncrafting table\n93.33\n90.00\n0.00\nAverage\n96.00\n92.00\n0.00\nWooden level\nbowl\n93.33\n90.00\n0.00\nboat\n93.33\n90.00\n0.00\nchest\n90.00\n90.00\n0.00\nwooden sword\n86.67\n80.00\n0.00\nwooden pickaxe\n80.00\n80.00\n0.00\nAverage\n88.67\n86.00\n0.00\nStone level\ncobblestone\n80.00\n73.33\n0.00\nfurnace\n80.00\n73.33\n0.00\nstone pickaxe\n80.00\n70.00\n0.00\niron ore\n60.00\n50.00\n0.00\nglass\n80.00\n76.67\n0.00\nAverage\n76.00\n68.67\n0.00\nIron level\niron ingot\n56.67\n50.00\n0.00\nshield\n56.67\n50.00\n0.00\nbucket\n53.33\n43.33\n0.00\niron pickaxe\n50.00\n40.00\n0.00\niron door\n43.33\n43.33\n0.00\nAverage\n52.00\n45.33\n0.00\nDiamond level\ndiamond ore\n30.00\n20.00\n0.00\nmind redstone\n20.00\n16.67\n0.00\ncompass\n16.67\n10.00\n0.00\ndiamond pickaxe\n23.33\n10.00\n0.00\npiston\n20.00\n13.33\n0.00\nAverage\n22.00\n14.00\n0.00\nthe Knowledge Memory. Without the Knowledge Memory,\nthe agent fails to mine iron due to its inability to recognize\nwhere iron ore is more likely to be located. Consequently,\nthe success rates for both Iron\nand Diamond levels\nare\n0.00%. The Knowledge Memory can help the agent more\neasily understand the acquisition methods of some items,\nwhile the Performer Memory can provide similar scenarios\nfor the agent to reference, thereby easing the pressure in the\nplanning process.\nTable 21 primarily studies the robustness brought about\nby the check part of the Patroller. “Random Drop” is a\nspecific setting that forces the Agent into execution errors.\nMore specifically, when the agent successfully completes\ntasks with the reasoning step greater than 4, it will randomly\ndiscard one item from either log\n, planks\n, or stick\npresent in its inventory. This situation can lead the agent to\ncommit execution errors due to insufficient material, specif-\nically when it is completing sub-objectives of higher rea-\nsoning steps that require logs\n, planks\n, or sticks\nas materials. The check part of the Patroller can detect the\ncause of these errors during execution and use it as feedback\nfor re-planning. With the “Random Drop” enabled and the\ncheck part of the Patroller disabled, the agent even struggles\nto complete tasks at the stone level\nare 0.00% effectively.\nE. Different Strategy of Active Perception\nIn order to improve the quality of the Active Percep-\ntion Query generated by Patroller, we use Chain-of-\nThought(COT)[37] to design a process of multiple rounds\nof query generation, Patroller can generate the next most\nTable 17. Performance on Active Perception Query Generation\nwith different Round Strategy. S means Single-round Generation\nand M means Multi-round Generation.\nplanner\nStrategy\nAverage Generation Rate(%)\nEasy\nMid\nHard\nComplex\nVicuna-13B-v1.5 [5]\nS\n100\n95\n75\n45\nM\n100\n100\n95\n80\nGPT-3.5-turbo [23]\nS\n100\n100\n85\n70\nM\n100\n100\n100\n100\nimportant problem based on the current problem and task\ndescription, until the agent judges that all problems have\nbeen produced.\nWe conduct experiences to compare\nSingle-round Generation and Multi-round Generation in\nTab. 17, We can observe that Multi-round Generation us-\ning COT[37] generates better corresponding environment\ninformation query and thus have a higher success rate on\nthe Context-Dependent Tasks.\nF. Applications\nF.1. Obtain Diamond Pickaxe\nWe demonstrate a case of the popular Process-Dependent\nTasks “craft diamond pickaxe\n” challenge in Video 1.\nF.2. Discovery\nWe demonstrate a complex level Context-Dependent Tasks\n“Find a pig\non the plains\nwith grass\nand water\nnext to it during a sunny day with sufficient brightness” in\nVideo 2.\nF.3. Open-Ended Tasks\nWe demonstrate a Open-Ended Tasks “Dig a block of\nsand\nunder the water\nwith a wooden shovel\ndur-\ning the daytime\non a sunny day” in Video 3.\nG. Interactions in MP5\nHere we illustrate the interactions between the inter-\nnal modules of MP5 during Active Perception and Re-\nplanning, presented in the form of dialogue text.\nG.1. Active Perception\nIn this part, we demonstrate the communication process\namong situation-aware planning, embodied action execu-\ntion, and active perception scheme when facing the task of\n“Find 1 sheep\non the plains\n”, as shown in Figure 8.\nThe corresponding screenshots are illustrated in Figure 10.\nG.2. Re-planning\nIn this part, we depict the situation when facing the task of\n“craft wooden pickaxe\n” with a shortfall of 1 plank\n.\nIn this case, the Patroller identifies the cause of the execu-\ntion error and instructs the Planner to re-plan, as shown in\nFigure 9. The corresponding screenshots are illustrated in\nFigure 11.\nTable 18. Success rates on different modules within Process-Dependent Task. The parts with a gray background in the table represent the\naverage success rate for the current level.\nTask Level\nObject\nSuccess rate(%)\nMP5(Ours)\nw\/o Percipient\nw\/o Memory\nw\/o Parser\nw\/o Patroller\nBasic level\nlog\n96.67\n0.00\n90.00\n96.67\n86.67\nsand\n96.67\n0.00\n90.00\n96.67\n73.33\nplanks\n96.67\n0.00\n80.00\n96.67\n83.33\nstick\n96.67\n0.00\n76.67\n96.67\n73.33\ncrafting table\n93.33\n0.00\n76.67\n90.00\n73.33\nAverage\n96.00\n0.00\n82.67\n95.33\n78.00\nWooden level\nbowl\n93.33\n0.00\n66.67\n80.00\n66.67\nboat\n93.33\n0.00\n66.67\n70.00\n66.67\nchest\n90.00\n0.00\n66.67\n70.00\n63.33\nwooden sword\n86.67\n0.00\n40.00\n63.33\n60.00\nwooden pickaxe\n80.00\n0.00\n40.00\n60.00\n60.00\nAverage\n88.67\n0.00\n56.00\n68.67\n63.33\nStone level\ncobblestone\n80.00\n0.00\n10.00\n50.00\n60.00\nfurnace\n80.00\n0.00\n3.33\n0.00\n60.00\nstone pickaxe\n80.00\n0.00\n0.00\n0.00\n56.67\niron ore\n60.00\n0.00\n0.00\n0.00\n40.00\nglass\n80.00\n0.00\n0.00\n0.00\n43.33\nAverage\n76.00\n0.00\n2.67\n10.00\n52.00\nIron level\niron ingot\n56.67\n0.00\n0.00\n0.00\n36.67\nshield\n56.67\n0.00\n0.00\n0.00\n36.67\nbucket\n53.33\n0.00\n0.00\n0.00\n30.00\niron pickaxe\n50.00\n0.00\n0.00\n0.00\n26.67\niron door\n43.33\n0.00\n0.00\n0.00\n20.00\nAverage\n52.00\n0.00\n0.00\n0.00\n30.00\nDiamond level\ndiamond ore\n30.00\n0.00\n0.00\n0.00\n10.00\nmind redstone\n20.00\n0.00\n0.00\n0.00\n3.33\ncompass\n16.67\n0.00\n0.00\n0.00\n0.00\ndiamond pickaxe\n23.33\n0.00\n0.00\n0.00\n3.33\npiston\n20.00\n0.00\n0.00\n0.00\n3.33\nAverage\n22.00\n0.00\n0.00\n0.00\n4.00\nTable 19. More detailed success rates for different LLMs as zero-shot Planners on Process-Dependent Tasks. The parts with a gray\nbackground in the table represent the average success rate for the current level.\nTask Level\nObject\nSuccess rate(%)\nGPT-4(Ours)\nGPT-3.5-Turbo [23]\nLLaMA2-70B-Chat [33]\nVicuna-13B-v1.5-16k [5]\nBasic level\nlog\n96.67\n96.67\n6.67\n3.33\nsand\n96.67\n96.67\n3.33\n3.33\nplanks\n96.67\n96.67\n0.00\n0.00\nstick\n96.67\n96.67\n0.00\n0.00\ncrafting table\n93.33\n90.00\n0.00\n0.00\nAverage\n96.00\n95.33\n2.00\n1.33\nWooden level\nbowl\n93.33\n90.00\n0.00\n0.00\nboat\n93.33\n90.00\n0.00\n0.00\nchest\n90.00\n90.00\n0.00\n0.00\nwooden sword\n86.67\n83.33\n0.00\n0.00\nwooden pickaxe\n80.00\n80.00\n0.00\n0.00\nAverage\n88.67\n86.67\n0.00\n0.00\nStone level\ncobblestone\n80.00\n66.67\n0.00\n0.00\nfurnace\n80.00\n50.00\n0.00\n0.00\nstone pickaxe\n80.00\n50.00\n0.00\n0.00\niron ore\n60.00\n10.00\n0.00\n0.00\nglass\n80.00\n33.33\n0.00\n0.00\nAverage\n76.00\n42.00\n0.00\n0.00\nIron level\niron ingot\n56.67\n6.67\n0.00\n0.00\nshield\n56.67\n3.33\n0.00\n0.00\nbucket\n53.33\n0.00\n0.00\n0.00\niron pickaxe\n50.00\n3.33\n0.00\n0.00\niron door\n43.33\n0.00\n0.00\n0.00\nAverage\n52.00\n2.67\n0.00\n0.00\nDiamond level\ndiamond ore\n30.00\n0.00\n0.00\n0.00\nmind redstone\n20.00\n0.00\n0.00\n0.00\ncompass\n16.67\n0.00\n0.00\n0.00\ndiamond pickaxe\n23.33\n0.00\n0.00\n0.00\npiston\n20.00\n0.00\n0.00\n0.00\nAverage\n22.00\n0.00\n0.00\n0.00\nTable 20. Success rates for different parts of Memory on Process-Dependent Tasks. The parts with a gray background in the table represent\nthe average success rate for the current level.\nTask Level\nObject\nSuccess rate(%)\nAll Memory(Ours)\nw\/o Performer Memory\nw\/o Knowledge Memory\nw\/o All Memory\nBasic level\nlog\n96.67\n96.67\n90.00\n90.00\nsand\n96.67\n96.67\n90.00\n90.00\nplanks\n96.67\n96.67\n83.33\n80.00\nstick\n96.67\n96.67\n76.67\n76.67\ncrafting table\n93.33\n93.33\n80.00\n76.67\nAverage\n96.00\n96.00\n84.00\n82.67\nWooden level\nbowl\n93.33\n93.33\n70.00\n66.67\nboat\n93.33\n90.00\n66.67\n66.67\nchest\n90.00\n90.00\n70.00\n66.67\nwooden sword\n86.67\n83.33\n43.33\n40.00\nwooden pickaxe\n80.00\n80.00\n40.00\n40.00\nAverage\n88.67\n87.33\n58.00\n56.00\nStone level\ncobblestone\n80.00\n73.33\n16.67\n10.00\nfurnace\n80.00\n73.33\n6.67\n3.33\nstone pickaxe\n80.00\n70.00\n3.33\n0.00\niron ore\n60.00\n50.00\n0.00\n0.00\nglass\n80.00\n70.00\n3.33\n0.00\nAverage\n76.00\n67.33\n6.00\n2.67\nIron level\niron ingot\n56.67\n53.33\n0.00\n0.00\nshield\n56.67\n53.33\n0.00\n0.00\nbucket\n53.33\n46.67\n0.00\n0.00\niron pickaxe\n50.00\n43.33\n0.00\n0.00\niron door\n43.33\n40.00\n0.00\n0.00\nAverage\n52.00\n47.33\n0.00\n0.00\nDiamond level\ndiamond ore\n30.00\n23.33\n0.00\n0.00\nmind redstone\n20.00\n26.67\n0.00\n0.00\ncompass\n16.67\n10.00\n0.00\n0.00\ndiamond pickaxe\n23.33\n20.00\n0.00\n0.00\npiston\n20.00\n13.33\n0.00\n0.00\nAverage\n22.00\n16.67\n0.00\n0.00\nTable 21. Success rates with and without the check part of the Patroller in the presence of “Random Drop” Setting on Process-Dependent\nTasks. The parts with a gray background in the table represent the average success rate for the current level.\nComponent\nMethod\nthe check part of Patroller\n✓\n✗\n✓\n✗\n“Random Drop”\n✗\n✗\n✓\n✓\nTask Level\nObject\nSuccess rate(%)\nBasic level\nlog\n96.67\n86.67\n90.00\n90.00\nsand\n96.67\n73.33\n90.00\n90.00\nplanks\n96.67\n83.33\n86.67\n70.00\nstick\n96.67\n73.33\n86.67\n50.00\ncrafting table\n93.33\n73.33\n83.33\n50.00\nAverage\n96.00\n78.00\n78.00\n70.00\nWooden level\nbowl\n93.33\n66.67\n80.00\n10.00\nboat\n93.33\n66.67\n83.33\n10.00\nchest\n90.00\n63.33\n80.00\n10.00\nwooden sword\n86.67\n60.00\n70.00\n3.33\nwooden pickaxe\n80.00\n60.00\n70.00\n3.33\nAverage\n88.67\n63.33\n78.00\n7.33\nStone level\ncobblestone\n80.00\n60.00\n53.33\n3.33\nfurnace\n80.00\n60.00\n53.33\n0.00\nstone pickaxe\n80.00\n56.67\n50.00\n0.00\niron ore\n60.00\n40.00\n30.00\n0.00\nglass\n80.00\n43.33\n40.00\n0.00\nAverage\n76.00\n52.00\n45.33\n0.00\nIron level\niron ingot\n56.67\n36.67\n26.67\n0.00\nshield\n56.67\n36.67\n26.67\n0.00\nbucket\n53.33\n30.00\n16.67\n0.00\niron pickaxe\n50.00\n26.67\n13.33\n0.00\niron door\n43.33\n20.00\n10.00\n0.00\nAverage\n52.00\n30.00\n18.67\n0.00\nDiamond level\ndiamond ore\n30.00\n10.00\n3.33\n0.00\nmind redstone\n20.00\n3.33\n3.33\n0.00\ncompass\n16.67\n0.00\n0.00\n0.00\ndiamond pickaxe\n23.33\n3.33\n0.00\n0.00\npiston\n20.00\n3.33\n0.00\n0.00\nAverage\n22.00\n4.00\n1.33\n0.00\nAgent:\nMy information is as follows:\ntask information:\n- description: Find 1 sheep on the plains.\ncurrent environment information:\n- position: ground\ninventory: {}\nPatroller: What Minecraft creatures is this image displaying?\nPercipient: There appear to be no creature in the given Minecraft image.\nPatroller:\nfeedback: There appear to be no creature, so you need to find the specific sheep.\nSuccess: False\nSuggestion: Start planning the actions.\nPlanner:\nexplanation: None\nthoughts: Since the task is to find a sheep and we are already on the ground, we just need to find the sheep.\nactions: [ \n{\"times\": \"1\", \"actions\": [ {\"name\": \"Find\", \"args\": {\"obj\": \"sheep\"} }}\n]\nPerformer: continue doing \"Find\" action.\n...\nPerformer: Having completed a move in “Find” action, based on my current view, tell me if I should continue this action or if the next a ction \nis ready to execute.\nPatroller: What Minecraft entities does this image represent?\nPercipient: This image represents sheep.\nPatroller: Could you identify the biome in this Minecraft image?\nPercipient: The image appears to be from the forest biome in Minecraft.\nPatroller:\nfeedback: Because the current biome is not forest biome, you should continue finding the sheep in the specific biome.\nSuccess: False\nSuggestion: Continue doing the current action.\nPerformer: Continue doing \"Find\" action.\n...\nPerformer: Having completed a move in “Find” action, based on my current view, tell me if I should continue this action or if the next a ction \nis ready to execute.\nPatroller: What types of Minecraft mobs is this picture showing?\nPercipient: This picture is showing sheep, types of Minecraft mobs.\nPatroller: What kind of Minecraft biome can you observe in this image?\nPercipient: The image presents a clear view of a plains biome in Minecraft.\nPatroller:\nfeedback: Because there are sheep on the plains, which means you have successfully found the specific sheep.\nSuccess: True\nSuggestion: Start to planning the next sub-objective actions. If there is no next sub-objective, the task is successfully done.\nPerformer: Finish doing \"Find\" action.\nAgent: I have found a sheep on the plains.\nFigure 8. Dialogue of task “Find 1 sheep\non the plains\n”\nSystem:\nYou are a helpful planner in Minecraft, capable of planning actions to complete sub-objectives. I will give you environmental \ninformation and a sub-objective, for which you need to conceive a plan, and then create a list composed of a sequence of various\nactions to complete it.\n=========\n…\nPatroller:\nfeedback: Because there are 2 logs in your inventory, this means you have successfully mined 2 logs.\nSuccess: True\nSuggestion: Start to planning the next sub-objective actions.\n… \nPatroller:\nfeedback: Because there are already 8 planks in your inventory, this means you have successfully crafted 8 planks using 2 logs.\nSuccess: True\nSuggestion: Start to planning the next sub-objective actions.\n… \nPatroller:\nfeedback: Because there are already 4 sticks in your inventory, this means you have successfully crafted 4 sticks using 2 planks.\nSuccess: True\nSuggestion: Start to planning the next sub-objective actions.\n… \nPatroller:\nfeedback: Because there is already 1 crafting table in your inventory, this means you have successfully crafted a crafting table.\nSuccess: True\nSuggestion: Start to planning the next sub-objective actions.\n… \nPatroller:\nfeedback: You do not have enough planks to complete the 'craft' action. You need 3 planks but you only have 2 planks in your \ninventory.\nSuccess: False\nSuggestion: Mine or Craft enough planks first.\n=========\nAgent:\nMy information is as follows:\ntask information:\n- task: wooden pickaxe\n- quantity: 1\n- material: {\"planks\": 3, \"stick\": 2}\n- tool: None\n- platform: crafting table\n- tips: 1 wooden pickaxe can be crafted with 3 planks and 2 stick as the material and crafting table as the platform.\n- description: Craft 1 wooden pickaxe using 3 planks and 2 stick on a crafting table as the platform.\ncurrent environment information:\n-\nposition: ground\ninventory: {\"stick\": 2, \"crafting_table\": 1, \"log\": 1, \"planks\": 2, \"dirt\": 6}\nHere are some plans to achieve similar goal for reference:\n- description: Craft 1 crafting table using 4 planks without any tool or platform.\n- actions: [\n{\"times\": \"1\", \"actions\": [ {\"name\": \"Craft\", \"args\": {\"obj\": {\"crafting table\": 1}, \"materials\": {\"planks\": 4}, \"platform\": \nnull}} ] }\n]\n- description: Craft 8 planks using 2 logs without needing any tool or platform.\n- actions: [\n{\"times\": “2\", \"actions\": [ {\"name\": \"Craft\", \"args\": {\"obj\": {\"planks\": 4}, \"materials\": {\"log\": 1}, \"platform\": \nnull} } ] }\n]\nPlanner:\nexplanation: You do not have enough planks to complete the \"craft\" action. You need 3 planks but you only have 2 planks in your \ninventory.\nthoughts: First, I need to craft more planks using the logs in my inventory. Then, I can craft a wooden pickaxe using the planks\nand sticks.\nactions: [ \n{\"times\": \"1\", \"actions\": [ {\"name\": \"Craft\", \"args\": {\"obj\": {\"planks\": 4}, \"materials\": {\"log\": 1}, \"platform\": \nnull} } ] },\n{\"times\": \"1\", \"actions\": [ {\"name\": \"Craft\", \"args\": {\"obj\": {\"wooden pickaxe\": 1}, \"materials\": {\"planks\": 3, \"stick\": 2}, \n\"platform\": \"crafting table\"} } ] }\n]\nFigure 9. Dialogue of task “craft wooden pickaxe\n” while re-planning\nTask: Find 1 sheep on \nthe plains\n0\n1\n2\nContext #\nProcess\nFigure 10. The corresponding screenshots for the dialogue of task “Find 1 sheep\non the plains\n”\n0\n1\n2\n3\n4\nProcess\nContext #\nTask: craft 1 \nwooden pickaxe\n1. Mine 2 logs\n2. Craft 8 planks\n3. Craft 4 sticks\n4. Craft 1 Crafting \ntable\n5. Craft 1 wooden \npickaxe\n5. Re-plan: Craft 4 planks, \nCraft 1 wooden pickaxe\nFigure 11. The corresponding screenshots for the dialogue of task “Find 1 sheep\non the plains\n” while re-planning\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception.pdf"}
{"title":"OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents","authors":"Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, Yitao Liang","summary":"This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps:\/\/craftjarvis.org\/OmniJARVIS.","url":"http:\/\/arxiv.org\/abs\/2407.00114v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.00114v2","published":1719495971000,"comment":"accepted on NeurIPS 2024","pdf_text":"OmniJARVIS\nUnified Vision-Language-Action Tokenization Enables\nOpen-World Instruction Following Agents\nZihao Wang1, Shaofei Cai1, Zhancun Mu2, Haowei Lin1, Ceyao Zhang3, Xuejie Liu1\nQing Li3, Anji Liu4, Xiaojian Ma3, Yitao Liang1∗\nTeam CraftJarvis\n1Institute for Artificial Intelligence, Peking University\n2Yuanpei College, Peking University\n3Beijing Institute for General Artificial Intelligence (BIGAI)\n4University of California, Los Angeles\n{zhwang,caishaofei}@stu.pku.edu.cn\nxiaojian.ma@ucla.edu,liuanji@cs.ucla.edu,yitaol@pku.edu.cn\nAbstract\nThis paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the control\ncommand directly, OmniJARVIS seeks a different path to ensure both strong\nreasoning and efficient decision-making capabilities via unified tokenization of\nmultimodal interaction data. First, we introduce a self-supervised approach to\nlearn a behavior encoder that produces discretized tokens for behavior trajectories\nτ = {o0, a0, . . . } and an imitation learning policy decoder conditioned on these\ntokens. These additional behavior tokens will be augmented to the vocabulary\nof pretrained Multimodal Language Models. With this encoder, we then pack\nlong-term multimodal interactions involving task instructions, memories, thoughts,\nobservations, textual responses, behavior trajectories, etc. into unified token se-\nquences and model them with autoregressive transformers. Thanks to the semanti-\ncally meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can\nreason (by producing chain-of-thoughts), plan, answer questions, and act (by pro-\nducing behavior tokens for the imitation learning policy decoder). OmniJARVIS\ndemonstrates excellent performances on a comprehensive collection of atomic,\nprogrammatic, and open-ended tasks in open-world Minecraft. Our analysis further\nunveils the crucial design principles in interaction data formation, unified tokeniza-\ntion, and its scaling potentials. The dataset, models, and code will be released at\nhttps:\/\/craftjarvis.org\/OmniJARVIS\/.\n1\nIntroduction\nUpon the success of pretrained Large Language Models (LLMs) [7, 35, 40, 17, 13] and Multimodal\nLangauge Models (MLMs) [31, 20, 1, 53, 33], some recent works have been venturing into developing\nVision-Language-Action (VLA) models [6, 22, 47, 38], a promising pathway towards the ultimate\ngoal of building autonomous agents that can follow and even self-generated instructions to fulfill\nvarious reasoning and acting tasks in open world environments. Among them, two most prominent\n∗Corresponding Author.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2407.00114v2  [cs.LG]  31 Oct 2024\nFigure 1: Illustration of multi-modal interaction data for decision-making. A canonical interaction sequence\ndepicting the human decision-making process starts from a given task instruction and memory, followed by\na series of sub-task completion which involves initial observations, chain-of-thought reasoning, and behavior\ntrajectories. Our proposed VLA model OmniJARVIS jointly models the vision (observations), language\n(instructions, memories, thoughts), and actions (behavior trajectories) as unified autoregressive sequence\nprediction. A self-supervised behavior encoder (detailed in Section 2 and Figure 2) converts the actions into\nbehavior tokens while the other modalities are tokenized following the practices of MLMs [31, 3, 1].\narchitectures have been proposed: 1) Combining an off-the-shelf MLM [31, 1] with separate goal-\nconditioned controllers [28, 10, 9], where MLM reasons, plans and pilots the controllers by producing\ntextual goal instructions, e.g. DEPS [46], JARVIS-1 [47], voyager [44]; 2) Tuning a pretrained MLM\ninto producing control commands directly, while maintaining the reasoning and language capabilities,\ne.g. RT-2 [6], LEO [22]. However, these two designs could still have significant drawbacks when it\ncomes to open-world environments. First, an open world (e.g., Minecraft) usually teams up with an\ninfinite number of complex and highly contextualized tasks [16, 29], and it can be fairly challenging\nto depict them in text only. Therefore, VLA models that solely depend on text to communicate with\nthe text-conditioned policies [47, 46] may fail to correctly pilot these controllers. On the other side,\nemitting the control command directly [6, 22] without invoking separate controllers could alleviate the\naforementioned communication problem but given the long-horizon nature of open-world tasks, it is\nless practical to perform long-term control with a large VLA model as the context length requirement,\ncomputation cost and inference efficiency could become unaffordable.\nIn this paper, we aim to tackle the aforementioned issues of existing VLA models when facing\nopen-world environments: complex & context-dependent tasks and long-term tasks. Our key\ninsight originates from the observation of human decision-making: Given these open-world tasks,\nhumans can make informed decisions via multi-round mental, verbal, and physical interactions\n(an illustration can be found in Figure 1). Therefore, if the VLA model can manage to learn from\nsuch interaction data, it may master the underlying human decision-making procedures. However,\nmodeling interaction data is non-trivial: it is multi-modal, encloses vision (mostly observations),\nlanguage (instructions, thoughts, etc.), and actions (behavior trajectories). Compared to the fruitful\nexplorations on jointly tokenizing vision and language [31, 3, 43, 1] into sequences for autoregressive\nmodeling [7], tokenizing behavior trajectories (actions) is hard due to the following reasons. On the\none hand, directly using low-level actions from the environment would pose huge challenges to the\nmodel’s ability to process long sequences, which significantly hurts performance. It also hinders us\nfrom leveraging the planning ability of generative models. On the other hand, language-level action\ntokens require significantly more supervision and cannot accurately describe all possible actions.\nTo this end, we propose OmniJARVIS, a novel VLA model that jointly models vision, language,\nand actions in interaction data with unified tokenization. OmniJARVIS comprises two key ideas:\n1) Behavior Tokenization. We introduce a self-supervised approach to learn a behavior encoder\n2\nBehavior Encoder\nDecoder as Policy\nObservations\nLearnable Tokens\n⋯\n⋯\n⋯\nFinite Scalar Quantizer\nDiscrete Behavior Tokens\nTokenizer\n𝑐!\n𝑐\"\n𝑐̂!\n𝑐̂\"\n𝑜!\n𝑜#\n𝑜$\n⋯\n𝑠!\n𝑠\"\nObservations\n⋯\n𝑜!\n𝑜#\n𝑜$\n⋯\n𝑎!\n𝑎#\n𝑎$\nBehavior Cloning\nDeTokenizer\nFigure 2: Self-supervised learning for behavior tokenizer of OmniJARVIS. We modify the VAE-based\nself-supervised learning of behavior trajectories in [10] to train the behavior tokenizer and de-tokenizer in\nOmniJARVIS. Specifically, we adopt the auto-encoding objective but replace the Gaussian latent with a discrete\nrepresentation based on Finite Scalar Quantizer [34]. The encoder will then be used as the behavior tokenizer\nto produce discrete tokens from the actions (behavior trajectories) in multimodal interaction data, while the\nbehavior tokens emitted by OmniJARVIS will be sent to the policy decoder to perform motor control.\nthat produces discretized tokens for actions (behavior trajectories) and an imitation learning policy\ndecoder conditioned on these tokens (Section 2); 2) Autoregressive Modeling. By augmenting these\nbehavior tokens into the vocabulary of pretrained MLMs, we pack the multimodal interaction data into\nunified token sequences and learn a transformer on these sequences with an autoregressive modeling\nobjective. We conduct comprehensive evaluations in the open-world Minecraft Universe [29].\nOmniJARVIS demonstrates impressive performances on a wide range of atomic, programmatic, and\nopen-ended Minecraft tasks. Our analysis confirms several critical design choices in data formation,\ntokenization, and the scaling potential of OmniJARVIS. Our contributions are as follows:\n• We propose OmniJARVIS, a novel VLA model capable of following instructions to reason,\nplan, and act in open-world environments by jointly modeling vision, language, and actions in\nmultimodal interaction data for decision-making.\n• We propose a self-supervised approach to learn a behavior encoder to tokenize actions and an\nimitation learning policy decoder to produce control commands from behavior tokens emitted by\nOmniJARVIS, allowing joint learning of VLA and smooth action readout.\n• We conduct extensive evaluations in open-world Minecraft to demonstrate OmniJARVIS’s profi-\nciency across various tasks and present in-depth analyses to reveal valuable insights.\n2\nA Tokenizer for Behaviors\nAs illustrated in Section 1, a key challenge for VLA is the mismatch between the action modality and\nother modalities such as the language instructions. A key insight is that a good amount of knowledge\nabout the effects of actions can be learned directly from behavior trajectories {τ (i)}i. We propose to\nlearn a behavior tokenizer in addition to the well-studied vision and language tokenizers to achieve\nunified tokenization of the vision , language , and actions in multimodal interaction data (Figure 1).\nWe pose two main requirements to the behavior tokens. First, they should be able to express complete\nand diverse behavior from (short) trajectories. Further, the tokens should contain semantic information\nso that they are compatible with the other modalities, which enables the reasoning and planning\nability of LLMs (e.g., by conducting chain-of-thought reasoning).\nSpecifically, we aim at producing a set of N discrete behavior tokens sbhv\n1 , . . . , sbhv\nN from a behavior\ntrajectory τ = {o0, a0, . . . }. Further, a de-tokenizer is needed to map these tokens back to an action\nrollout in the environment that reproduces the goal achieved in τ. GROOT [10] explores a VAE-based\napproach to jointly learn a latent representation of behavior trajectories and an imitation learning\npolicy decoder that conditions the latent as goal. However, the continuous latent cannot be used\nas the behavior tokens as they can be more difficult to learn and decode with the existing discrete\ntokens of pretrained MLMs [22, 32]. Therefore, we replace the Gaussian latent in GROOT with an\nimproved vector quantized discrete latent called Finite Scalar Quantization (FSQ) [34]. We adopt a\nquantization configuration of [8, 8, 8, 6, 5], which means a code with a length=5 and a codebook size\nof 8 × 8 × 8 × 6 × 5 = 15360 is produced. The configuration is selected by a simple grid search.\nOverall, the behavior tokenizer (behavior encoder) eϕ(o1;T ) and the de-tokenizer (IL policy decoder)\n3\nFigure 3: Architecture and Inference of OmniJARVIS. The main body of OmniJARVIS is a multimodal\nlanguage model (MLM) augmented with additional behavior tokens. Given a task instruction, initial memory,\nand observation, OmniJARVIS will iteratively perform chain-of-thought reasoning and produce behavior tokens\nas a means of control via the decoder policy (behavior de-tokenizer). Every 128 steps, OmniJARVIS is forced\nto reason again and produce new behavior tokens with the latest observation. (Not shown above) OmniJARVIS\ncan also make textual responses, e.g. answering questions.\nπθ(at|o1:t) is learned with the following objective:\nargmin\n(ϕ,θ)\nEτ∼D\n\" T\nX\nt=1\n−log πθ(at|o1:t, f(eϕ(o1:T )))\n#\n,\n(1)\nwhere f(·) denotes the finite scalar quantizer. We choose a non-causal (bidirectional) transformer\nand a causal transformer to parameterize the encoder eϕ(o1;T ) and the policy decoder πθ(at|o1:t),\nrespectively. In practice, we set T = 128 as the trunk size of the behavior trajectory to be encoded.\nWe will discuss how to handle trajectories longer than 128 in the next section.\nCompared to our behavior tokenization, most prior work in VLA models, either represents the\nbehavior trajectories in interaction data as a textual goal description and invokes a separate goal-\nconditioned controller [47, 46], or represents the state-action sequence {o0, a0, . . . } directly as in\nDecision Transformers (DT) [11, 22, 38, 6]. Our approach offers a more compact but still informative\nrepresentation of the actions part in multimodal interaction data. Moreover, the action readout, i.e.\nsimply sending the behavior tokens to the policy decoder, is also more efficient than the DT-style\ndirect control from VLA models [38, 6, 22].\n3\nMultimodal Interaction Data and OmniJARVIS\nAs illustrated in Figure 1, canonical multimodal interaction data comprises vision (observations),\nlanguage (instructions, memories, thoughts), and actions (behavior trajectories). However, it can be\ndifficult to directly collect such interaction data from human annotators. Therefore, we propose to\nconvert an existing Minecraft gameplay dataset [2] into the multimodal interaction data required by\nOmniJARVIS. We begin with a formal definition of the interaction data, followed by our approach\nfor data conversion and augmentation from existing datasets, and finish up with the architecture,\nformulation of learning on such interaction data, and inference procedure of OmniJARVIS. An\noverview of OmniJARVIS architecture and inference can be found in Figure 3.\n3.1\nData Formation\nAn interaction sequence of decision-making D = {Dt}T\nt=0 comprises T segments. Each segment Dt\ncan be a sentence of text words {wi}N\ni=1, i.e. the language part such as instructions Dinst\nt , memory\nDmem\nt\nor thoughts Dtht\nt . Dt can also be an image I, i.e. the vision part such as observations Dobs\nt\n= I.\nFinally, Dt may belong to the action (behavior trajectory) part, i.e. Dbhv\nt\n= {o0, a0, . . . }. We assume\nthese segments follow the ordering below (Figure 1):\nDinst\n0 , Dmem\n1\n|\n{z\n}\nContext\n, Dobs\n2 , Dtht\n3 , Dbhv\n4\n|\n{z\n}\nsub-task 1\n, Dobs\n5 , Dtht\n6 , Dbhv\n7\n|\n{z\n}\nsub-task 2\n, . . .\n(2)\nWe tokenize such a sequence of segments into a series of tokens {s0, . . . , sM} using the vision and\nlanguage tokenizer offered by a pretrained MLM and the behavior tokenizer introduced in Section 2.\n4\n3.2\nPreparing Multimodal Interaction Data\nIn reality, many segments of the multimodal interaction D can be missing in public datasets. We\nconsider the Minecraft contractor data released by OpenAI [2] and it only contains behavior trajec-\ntories Dbhv\nt\n. Therefore, we need to properly augment the data with the additional textual segments\nincluding instructions Dinst\nt , memory Dmem\nt\n, and thoughts Dtht\nt . We follow the prior practices [22, 31]\nto synthesize the required text using LLMs. Below, we detail how each type of segment is constructed.\nMore details can be found in appendix.\nSynthesis of instruction Dinst\nt\n. The instruction is a high-level description of what task is being\nperformed in the current interaction sequence. The considered OpenAI Minecraft data includes meta\ninformation of each gameplay video, which depicts fundamental events that happened during in\nMinecraft gameplay, e.g. what block was just destroyed, what entity was just killed, what item was\njust crafted, etc. Such meta-information can provide a basic overview of what the player has been\nthrough in the gameplay. We therefore prompt an LLM into summarizing the gameplay with the meta\ninformation. The summary will be used as the instruction Dinst\nt\nof the current trajectory.\nSynthesis of memory Dmem\nt\n. The memory is the summary of what agents have finished in the\nprevious interaction sequences. Due to the limited sequence length that the auto-regressive model\ncan handle, the model needs to learn to summarize key information related to the task in historical\ninteractions and ignore behaviors unrelated to instructions. The memory will be updated based on the\nresults of each episode trunk and used for subsequent episode trunks. We therefore prompt an LLM\ninto summarizing the gameplay with the meta information. The summary will then be used as the\nmemory Dmem\nt\nof the current interaction trajectory. The memory prompt can be found in Appendix F.\nSynthesis of thought Dtht\nt . The thought is the agent’s reasoning and explanation of its own decisions.\nPrevious methods have confirmed that using thought-enhanced interaction data helps language models\nunderstand decision-making [21]. Compared to labeling thoughts by humans [50, 6], we assume\nthat thought is an intermediate variable that can be determined by the actions taken and observations\nmade before and after the action, which is similar to an Inverse Dynamics Model [2]. We therefore\nprompt an LLM into estimating the thought of decisions with in-context learning, which will then be\nused as the thought Dtht\nt of the current behavior. Details can be found in Appendix E.\n3.3\nArchitecture, Training, and Inference of OmniJARVIS\nAs illustrated in Figure 3, OmniJARVIS is built upon a pretrained MLM. We augment the original\nvocabulary of the MLM with additional tokens from the behavior tokenizer. Specifically, as we\nadopted the [a, b, c] FSQ configuration (Section 2), we augment with a + b + c new tokens as each\nbehavior comprises n behavior tokens sbhv\n1\n, . . . , sbhv\nn\ncorresponding to n FSQ levels. We formulate\nthe learning objective of OmniJARVIS following [7, 37] in a prefix language modeling fashion. For\na batch B of token sequence s, we optimize OmniJARVIS via:\nL(θ, B) = −\n|B|\nX\nb=1\nT\nX\nt=1\nlog pθ(s(b,t)\nres |s(b,<t)\nres\n, s(b,1)\nprefix, ..., s(b,L)\nprefix ),\n(3)\nwhere sprefix denotes the prefix token, which is tokenized from the segments that served as context for\nreasoning and decision-making, i.e. instruction Dinst\nt , memory Dmem\nt\nand observation Dobs\nt\nwithin the\ninteraction sequence (Equation 2). The remaining tokens (tokenized from thought Dtht\nt and behavior\ntrajectory Dbhv\nt\n) will be predicted in an autoregressive fashion. From a high level, OmniJARVIS is\ntrained to reason (producing thought tokens) and act (producing behavior tokens) from contexts with\ntask instructions, memory, and current observations. During inference, we begin with the feeding\nOmniJARVIS with a task instruction, an empty memory, and an initial observation. OmniJARVIS\nwill produce a chain-of-thought as a means of reasoning and subsequently, emit behavior tokens for\ncontrol. Every N steps, it is forced to reason again to produce new behavior tokens with the latest\nobservation. We empirically set N = 32.\n4\nCapabilities and Analysis\n4.1\nOverview\nTraining details and Datasets. The training of the OmniJARVIS is divided into two stages. In\nthe first step, we use a self-supervised training method to train a Behavior Tokenizer, including\nthe Encoder and Decoder jointly. We use FSQ as a quantization method and build a codebook\n5\nwith 8*8*8*6*5 discrete codes. The training data for Behavior Tokenizer comes from Contractor\nDataset [2], which is a collection of Minecraft gameplay videos. The training parameters and details\nremain consistent with GROOT, which can be found in Appendix A.\nIn the second stage, we use this behavior tokenizer to process Minecraft offline trajectories to obtain\nbehavior token sequences. We add 35 (8+8+8+6+5) additional tokens to the MLM tokenizer as\nbehavior tokens for unified representation, so each time the VLA needs to output a continuous\nsequence of 5 tokens to represent a complete behavior. We use GPT-3.5 to synthesize thought,\nmemory, and instruction to raw offline datasets to build complete interaction data. The specific\nprompt can be found in Appendix E. These data collectively constitute the embodied instruction-\nfollowing dataset of OmniJARVIS, including 600k trajectories and about 900M tokens.\nThe training dataset of OmniJARVIS further includes a large amount of QA data about Minecraft.\nWe generate a large number of seed questions about these texts using web pages on the Minecraft\nwiki. Then, we use the self-instruct method to generate a large number of creative questions and\ninstructions. This constructed QA dataset consists of 300k conversations with about 90M tokens.\nDuring the training process, the QA data and instruction-following data are mixed, with a total of\nabout 1T tokens, to train OmniJARVIS. In specific, we SFT (supervised finetune) LLaVA-7B [31].\nThe details can be found in Appendix A. To further demonstrate the generalizability of the method,\nwe also fine-tune LLaVA at different scales and VLM Fuyu-8B with different architectures. The\nrelevant results are presented in Section 4.5 and Section 4.6.\nExperimental Setups. We conduct experiments in the complex and open-world environment\nof Minecraft, a voxel-based 3D video game that has garnered significant attention from real-life\nresearch due to its popularity and diverse mechanics [18, 16]. We first evaluate OmniJARVIS with\natomic tasks, which are skill-level tasks, testing VLAs’ ability to follow simple and straightforward\ninstructions. Then we evaluate OmniJARVIS with programmatic tasks, which require the agent\nto obtain an item starting from an empty inventory. The success of these tasks requires VLAs to\ndecompose the provided instruction into atomic-level subtasks, and hence tests VLAs’ complex\nreasoning ability. Finally, we test OmniJARVIS with open-world embodied question-answering\nbenchmarks and creative free-form instruction-following. We also conduct ablation experiments of\nOmniJARVIS with different behavior tokenizers, different training dataset formats, and different\nvision tokenizations. Finally, we explore the generalization abilities of OmniJARVIS of Atari Games\nand the scaling potential of OmniJARVIS with different models and data scales.\n4.2\nMain Results I: Short-horizon Atomic Tasks\nAtom tasks are various simple skills that agents in Minecraft need to master. They are basic tasks\nyet are fundamental skills that agents need to master during the learning process. We first evaluate\nOmniJARVIS with our learned behavior tokenizer on these tasks.\nWe select “chopping trees”\n, “digging dirt”\n, “mining stones”\n, and “collecting wheat\nseeds”\nas the evaluation tasks. We directly take those short task descriptions as instructions\nfor agents. We use text-conditioned VPT [2], Open-world Control [9], STEVE-I [28], and video-\ninstructed GROOT [10] as baselines. We compute the average rewards of different agents on\nevery task in Table 1 across 10 runs. By observing the environment and adjusting action tokens\ndynamically, OmniJARVIS effectively follows straightforward instructions across various scenarios.\nIt consistently achieves a high average reward with minimal standard deviation.\n4.3\nMain Results II: Long-horizon Programmatic Tasks\nTo further verify the ability of OmniJARVIS to complete tasks with long sequences, we use 30\nprogrammatic tasks to evaluate the performance of different agents. These tasks require the agent to\nstart from an empty inventory in a new world until obtaining the final required items, which is usually\na chain of atom tasks. These tasks are divided into five groups based on difficulty: wooden, food,\nstone, iron, and diamond. For example, the prompt for task “Obtain a diamond pickaxe”\nis “Give\nyou nothing in the inventory, obtain a diamond pickaxe.” This task requires more game time and\nmore complex planning for up to 10 different intermediate items [2]. We list all programmatic tasks\nand its corresponding instructions in the Appendix C.1.\nBaselines are divided into two types: 1) directly outputs actions, namely the native behavior tokenizer,\nincluding STEVE-I [28] and GROOT [10]. 2) using pretrained LLM as a planner to output language\ngoals and connect the STEVE-I to execute these goals, including Zero-Shot Planner (GPT) [23],\n6\nTable 1: Evaluation results (rewards) on short-horizon\natom tasks.\nThe text-conditioned VPT [2] (“VPT\n(text)∗”) is from Appendix I of its paper.\nMethod\nCondition\n↑\n↑\n↑\n↑\nVPT∗[text] [2]\nLanguage\n2.6±0.3\n9.2±0.7\n-\n0.8±0.1\nSTEVE-I [28]\nLanguage\n11.0±3.0\n10.0±2.5\n3.2±1.6\n5.1±2.5\nGROOT [10]\nVideo\n14.3±4.7\n19.7±8.7\n19.0±11.3\n7.3±0.6\nOmniJARVIS\nLanguage\n10.8±5.2\n20.3±9.2\n25.8±2.9\n8.2±3.6\nTable 2: Results on open-ended instruction following.\nVPT\nSTEVE-1\nVoyager\nDEPS\nOurs\nInstruction\nFollowing ↓\n975.9\n972.7\n932.1\n929.5\n886.2\nTable 3: Results on open-ended question answering.\nWe use LLM-as-judge [35] to evaluate the accuracy.\nVicuna-7B\nVicuna-13B\nLLaMA2-70B\nGPT-3.5\nOurs\nQA ↑\n2.34\n2.85\n2.50\n7.50\n8.40\nTable 4: Success rate of different agents on long-horizon programmatic tasks.\nMethod\nAction Tokenizer\nWooden (10)\nFood (5)\nStone (5)\nIron (5)\nDiamond (5)\nAverage\nSTEVE-I [28]\nNative\n0.04±0.07\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.01\nGROOT [10]\nNative\n0.05±0.08\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.00±0.00\n0.02\nGPT [23]\nLanguage\n0.26±0.14\n0.08±0.04\n0.24±0.05\n0.04±0.05\n0.00±0.00\n0.15\nReAct [50]\nLanguage\n0.44±0.11\n0.12±0.04\n0.30±0.10\n0.06±0.05\n0.00±0.00\n0.23\nDEPS [46]\nLanguage\n0.78±0.11\n0.12±0.04\n0.68±0.08\n0.16±0.05\n0.04±0.05\n0.43\nOmniJARVIS\nFSQ GROOT\n0.95±0.07\n0.44±0.05\n0.82±0.08\n0.32±0.11\n0.08±0.04\n0.59\nReAct [50], and DEPS [46]. We use success rate to evaluate the completion of tasks, that is, whether\nthe task is completed within the specified time. The experimental results are listed in Table 6.\nProgrammatic Tasks usually require complex reasoning for planning. While STEVE-I and GROOT\ncan only finish short skill-level tasks in atom tasks, is difficult to finish these programmatic tasks.\nAgents based on Language behavior tokenizer can complete complex tasks including diamond group\nones, but with a low success rate. This is because these in-context learning methods leverage the\npretrained LLM which may lack the necessary knowledge about this world. It is worth noting that in\nthe Food group, agents based on Language Tokenizer have an average success rate of around 10%,\nas this set of tasks does not require complex reasoning. This indicates that Language-conditioned\nTokenizers need additional language-conditioned trajectories as supervision for training while there\nwas less such data available during STEVE-I’s training phase leading to significant performance gaps.\nMeanwhile, OmniJARVIS uses a self-supervised trained behavior tokenizer which does not require\nextra language labels and hence receives more training resulting in good performance across a wider\nrange of tasks. We will further prove this in the next set of Creative Task experiments.\n4.4\nMain Results III: Open-ended Question-Answering and Instruction Following Tasks\nThe open-ended tasks differ from programmatic tasks due to the lack of straightforward success\ncriteria [16]. We select the long-term open-ended tasks which usually need at least 5 minutes of\nhuman-playing time to finish. The task prompts can be found in Appendix C.3. Following image\ngeneration and video generation tasks [19, 41], we take the Fréchet Sequence Distance (FSD) metrics\nto evaluate the correlation between agent rollout video and creative instruction. Specifically, we first\nask human experts to finish the creative task prompts under randomly generated worlds and record\nthe game-playing videos Vhuman. Then, we provided the task prompts for different Minecraft agents,\nand obtained a rollout video set Vagent. Similar to FID [19], we used MineCLIP [16] to calculate the\nembedding of video clips and computed FSD for the embedding distributions of human and agent\nrollout videos. The analysis of the metrics can be found in Appendix B.\nWe further conduct open-ended embodied question-answering benchmarks to evaluate the ability of\nthe agent to complete open-ended instructions and grasp world knowledge. The questions answering\ninstructions set can be found in Appendix C.2. The evaluation results can be found in Table 3\nand Table 2. OmniJARVIS is the agent that can simultaneously complete both types of tasks\nand has achieved the best performance in different task sets, surpassing strong baselines including\nVoyager [44] and DEPS [46]. Also, it maintains strong reasoning capability, especially on embodied\nquestion answering compared to LLM baselines (with image captions as visual context).\n4.5\nInsights and Analysis\nInteractive Dataset Format. We explore the crucial roles played by the different type of segments\nin interaction data, including the instruction, memory, thought, and caption tokens. The results can\nbe found in Table 4, where we evaluate the loss on predicting the behavior tokens. It can be seen that\ninstruction and thought can be more critical to the successful prediction of behavior tokens. This is\nconsistent with our hypothesis – making informed decisions requires task instruction and reasoning.\n7\nFigure 4: Ablation experiments on OmniJARVIS with different behav-\nior tokenizers, vision tokenizers, and training on different interactive\ndatasets. The first line is training on the unconditional interactive dataset,\ni.e., without instructions on the trajectories. OmniJARVIS with VQ-\nGROOT [42, 10] shows no results because of training collapse.\nDataset Format\nLoss\nBehavior\nTokenizer\nVision\nTokenizer\nInstruction\nCaption\nThought\nMemory\nTrain\nEval\n✗(unconditional)\n0.33\n0.67\n✓\n✗\n✗\n✗\n0.46\n0.51\n✓\n✓\n✗\n✗\n0.44\n0.48\n✓\n✓\n✓\n✗\n0.32\n0.33\nFSQ GROOT\nLLaVA\n✓\n✓\n✓\n✓\n0.16\n0.17\nFSQ GROOT\nCaptioner+\n✓\n✓\n✗\n✗\n0.49\n0.52\nFSQ GROOT\nFUYU\n✓\n✓\n✗\n✗\n0.42\n0.44\nGROOT\nLLaVA\n✓\n✓\n✓\n✓\n0.44\n0.48\nVQ GROOT\nLLaVA\n✓\n✓\n✓\n✓\n-\n-\nFigure 5:\nScaling potential\nof OmniJARVIS. Its evaluation\nloss continues to drop with the\ngrowth of data and model parame-\nters. The Pearson coefficients for\nthe 2B, 7B, and 13B models are\n0.9991, 0.9999, and 0.9989.\nTable 5: Ablation experiments on behavior tokenizer with different code vocabulary size.\nCodebook\nsize\nFSQ\nLevels\nTraining\nIterations\nTrain\nLoss\nEval\nLoss\nReconstruction\nFSD ↓\nSampling\nFSD ↓\nAverage\nRewards ↑\nCodebook\nUsage\ne8\n[8,6,5]\n180k\n2.746\n3.161\n46.57\n68.90\n0.63±0.67\n93.75%\ne10\n[8,5,5,5]\n180k\n3.011\n3.148\n43.67\n61.85\n0.54±1.21\n97.65%\ne14\n[8,8,8,6,5]\n240k\n3.092\n3.116\n42.72\n57.37\n2.27±2.45\n92.36%\nVision Tokenization. We also evaluate training OmniJARVIS with different vision tokenization,\nincluding ImageCaptioner + LLaMA2-7B [12, 40] (basically converting the vision input into textual\ncaptions), fuyu-8b [3], and LLaVA-7B [31] architecture. For the ImageCaptioner+, we fix the\nImageCaptioner models and only fine-tune the language model, i.e., LLaMA2-7B. We use the\nprediction loss of behavior tokens as the evaluation criterion, namely eval loss. We found that the\nmodel trained with LLaVA-7B architecture has the lowest evaluation loss, so we chose this model as\nthe default model.\nBehavior Tokenizer. We explore OmniJARVIS with different behavior tokenizers, including the\ndefault setting using FSQ codebook, a variant of using VQ-VAE instead of FSQ [42], and simply\nusing sub-goal language annotation as behavior “tokens”. The evaluation results on 4 programmatic\ntasks are listed in Table 4. Using an FSQ tokenizer is generally better than a language goal, which\nconfirms the advantages of using a tokenized behavior over language descriptions of behavior. The\nuse of VQ-VAE as a quantized behavior tokenizer collapsed during the training process, so there\nwere no results in all test tasks.\nBehavior Codebook. We conduct an in-depth investigation of behavior tokenizers with varying\ncodebook sizes, utilizing recommended sets of FSQ levels to approximate specified codebook\ndimensions [34] as delineated in Table 5. We evaluate performance across multiple metrics for\neach codebook size. Codebook Usage is quantified as the proportion of codewords utilized at\nleast once when encoding the validation datasets. Reconstruction FSD is measured by the FSD\nscores derived from the MineCLIP encoder [16], processing 1,000 different demonstration videos\nthrough the FSQ-GROOT and subsequent rollout in a randomly generated environment. Additionally,\nwe measure Resampling FSD, which is the FSD score obtained when the environment rollout is\nconditioned on representations sampled from the codebook. Finally, we assess the average rewards\nfor the task “collect wood” using OmniJARVIS across varying codebook sizes. Our findings indicate\nthat increases in codebook size correlate with enhanced average rewards and reduced FSD scores,\nsuggesting a scalable performance in OmniJARVIS with larger codebooks.\nBehavior Semantics. We provide some qualitative analysis on the learned FSQ-based behavior\ntokenizer. In Figure 6, we tokenize several reference videos, then feed the behavior tokens to the\npolicy decoder and see if it can accomplish the same task as in reference videos. The results indicate\nthat our behavior tokenizer is able to capture such behavior semantics and offers rich task information.\n4.6\nGeneralization and Scaling Potential of OmniJARVIS\nWe first explore adapting OmniJARVIS to the Atari game Montezuma’s Revenge. We created\na dataset from 500 episodes played by an agent trained with Random Network Distillation [8],\n8\nProvided\nGenerated\nFigure 6: Examples of behavior tokenization-detokeinzation. Left: the reference video to be tokenized by\nour FSQ-based behavior tokenizer (encoder). Right: the behavior of the policy decoder is conditioned on the\nbehavior tokens. The policy decoder can reproduce the task being accomplished in the reference video.\nFigure 7: OmniJARVIS plays Montezuma’s Revenge and gets a reward of 3600.\nsupplemented by random actions in early frames to enhance diversity. This dataset contains 1,823,699\ntransitions. We then trained the FSQ-GROOT tokenizer on this new dataset and subsequently trained\nOmniJARVIS on the tokenized data. The finetuned OmniJARVIS achieved a score of 3600 in\nMontezuma’s Revenge, indicating promising transferability. A rollout trajectory is in Figure 7.\nWe also investigate the scaling effect [25, 30] of data and model in OmniJARVIS by monitoring the\ninstruction-following loss on the validation set as the amount of data increases. In addition to fine-\ntuning from the default LLaVA-7B, we include two additional scales: OmniJARVIS-2B (fine-tuned\nfrom LLaVA-2B with Gemma-2B language models [20]) and OmniJARVIS-13B (fine-tuned from\nLLaVA-13B with LLaMA2-13B language models [31]).\nThe validation loss curves in Figure 5 reveal the following insights: 1) When using Omni-Tokenizer,\nOmniJARVIS’s instruction tuning aligns with the scaling law [25]. All curves exhibit a log-linear\ndecrease as the data scale increases. 2) Scaling up VLM consistently enhances performance. Notably,\nOmniJARVIS-7B demonstrates significantly lower losses compared to OmniJARVIS-2B. However,\nwhile improvements are consistent, the difference between OmniJARVIS-7B and OmniJARVIS-\n13B seems less pronounced, hinting at potential saturation when further scaling up VLM. This\nunderscores both the scalability of OmniJARVIS and the importance of increasing data volume to\nmatch the model.\n5\nRelated Works\nPretrained Language Models for Decision-making. Several works have explored leveraging LLMs\nto generate action plans for high-level tasks in embodied environments [23, 27, 5, 52]. To better\nperform complex planning in the environment, existing methods usually utilize chain-of-thought [49]\nor related methods [50]. To better cope with uncertainties in open worlds, some LLM-based methods\ngenerate plans interactively with human and environmental feedback [39, 46, 24] and retrieving\nfrom memory [47] or internet corpus [48]. However, those plans can only be executed in a language\nenvironment or require an additional controller or code executor to interact in an open world.\nVision-Language-Action Models. In order to better utilize the knowledge inside the language model\nfor decision-making, some methods tend to use decision datasets to fine-tune pretrained language\nmodels [15, 14]. Gato [38] was among the first to tokenize environment-provided actions to enable\njoint sequential modeling across modalities. PaLM-E [14] generates high-level instructions as texts\nand uses dedicated controllers to perform the task described by the output instructions. The RT\nseries focuses more on robotics settings. Specifically, RT-1 pairs a VLM with a language-conditioned\ncontroller; RT-2 extends the VLM to directly include control tokens; RT-X generalizes to new robots\nand environments. A recent VLA model LEO [22] expands the perception from 2D images to 3D\nworld and enables rich scene-level reasoning and control tasks.\n9\nFigure 8: Comparative Framework of Vision-Language Action Models. (a) depicts a model where upon\nreceiving a language instruction, actions are directly output based on the environmental state, facilitating\nimmediate interaction with the environment at a unified frequency. Smaller models with <1B parameters like\nVPT [2] maintain higher frequencies (>20Hz), though their capability for complex reasoning tasks is limited.\nLarger models with >7B parameters such as RT-2 [6], offer enhanced performance but operate at significantly\nreduced frequencies (2-3Hz). (b) illustrates a common approach utilizing large vision-language models for\nplanning, subsequently outputting language goals [46, 14, 4]. A language-conditioned policy then translates\nthese language goals into actions at a real-time interaction rate of 20Hz, with high-level models re-planning\nat less than 1Hz. This hierarchical structure balances interaction frequency and performance, while it requires\nlanguage as an intermediary and additional language labels. The training process of high-level vision-language\nmodels and language-conditioned policies are separate, thus performing poorly on tasks that can not be easily\nconnected by language. (c) (ours) mirrors the hierarchical structure of (b) but differentiates by employing a\nself-supervised encoder-decoder policy [10] and FSQ quantization [34] as a behavior tokenizer. The upper-level\nvision-language models produce self-supervised behavior tokens, which are then conditioned by a policy decoder\nto output actions, facilitating environment interaction. The behavior tokens are injected into the training corpus\nof vision-language-action models, which enables end-to-end inference. This approach also eliminates the need\nfor external language supervision and scales efficiently.\nOpen-world Agents in Minecraft. As LLMs have achieved remarkable reasoning results and\nunderstanding capabilities across various domains, the year 2023 has witnessed researchers adopting\nmultiple LLM-based approaches to create open-world agents in Minecraft [46, 55, 47, 44]. Some\nmethods focus on building policies for low-level skills [10, 28, 2]. Building upon the low-level\npolicies to interact with the Minecraft environment, Wang et al. [46], Yuan et al. [51] and Wang et al.\n[47] focus on leveraging the pre-trained language models as planners to finish programmatic tasks\nwith in-context learning. Wang et al. [44] adopts the life-long learning scheme and generates code\nas policies to enable continual exploration. Some use expert trajectories and Minecraft corpus to\nfine-tune pre-trained vision language models for better embodied planning [36, 54].\n6\nConclusion\nWe’ve presented OmniJARVIS, a novel VLA model that encompasses strong reasoning and ef-\nficient decision-making capabilities via unified tokenization of vision, language, and actions in\nmultimodal interaction data. The key ideas are learning behavior tokenizer (trajectory encoder)\nand de-tokenizer (IL policy decoder) using self-supervised learning on behavior trajectories and\nautoregressive modeling of tokenized multimodal interaction data using a pretrained multimodal lan-\nguage model (MLM). Evaluations on the open-world Minecraft Universe demonstrate its impressive\ninstruction-following capabilities. Possible future directions include a more in-depth investigation of\nbehavior tokenization, language capabilities after VLA fine-tuning, and alignment concerns emerging\nfrom the unified interaction modeling and VLA capabilities.\nAcknowledgments\nThis work is funded in part by the National Key R&D Program of China #2022ZD0160301. We\nthank a grant from CCF-Tencent Rhino-Bird Open Research Fund. One author is funded in part by\nNSF grants #IIS-1943641, #IIS-1956441, #CCF-1837129, an SRA from Meta and a research gift\nfrom Amazon Alexa AI, and a gift from RelationalAI.\n10\nReferences\n[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022.\n[2] B. Baker, I. Akkaya, P. Zhokhov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro,\nand J. Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos.\narXiv preprint arXiv:2206.11795, 2022.\n[3] R. Bavishi, E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Ta¸sırlar. Introducing\nour multimodal models, 2023. URL https:\/\/www.adept.ai\/blog\/fuyu-8b.\n[4] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv\npreprint arXiv:2212.06817, 2022.\n[5] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang,\nR. Julian, et al. Do as i can, not as i say: Grounding language in robotic affordances. In 6th\nAnnual Conference on Robot Learning, 2022.\n[6] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess,\nA. Dubey, C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to\nrobotic control. arXiv preprint arXiv:2307.15818, 2023.\n[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020.\n[8] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation.\narXiv preprint arXiv:1810.12894, 2018.\n[9] S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-\naware representation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034,\n2023.\n[10] S. Cai, B. Zhang, Z. Wang, X. Ma, A. Liu, and Y. Liang. Groot: Learning to follow instructions\nby watching gameplay videos. arXiv preprint arXiv:2310.08235, 2023.\n[11] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and\nI. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances\nin neural information processing systems, 34:15084–15097, 2021.\n[12] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving\nlarge multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.\n[13] Y. Cheng, C. Zhang, Z. Zhang, X. Meng, S. Hong, W. Li, Z. Wang, Z. Wang, F. Yin, J. Zhao,\nand X. He. Exploring large language model based intelligent agents: Definitions, methods, and\nprospects. arXiv preprint arXiv: 2401.03428, 2024.\n[14] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\n[15] Z. Durante, B. Sarkar, R. Gong, R. Taori, Y. Noda, P. Tang, E. Adeli, S. K. Lakshmikanth,\nK. Schulman, A. Milstein, D. Terzopoulos, A. Famoti, N. Kuno, A. Llorens, H. Vo, K. Ikeuchi,\nL. Fei-Fei, J. Gao, N. Wake, and Q. Huang. An interactive agent foundation model. arXiv\npreprint arXiv: 2402.05929, 2024.\n[16] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu,\nand A. Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale\nknowledge. Advances in Neural Information Processing Systems Datasets and Benchmarks,\n2022.\n[17] R. Gong, Q. Huang, X. Ma, H. Vo, Z. Durante, Y. Noda, Z. Zheng, S.-C. Zhu, D. Terzopoulos,\nL. Fei-Fei, et al. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971,\n2023.\n[18] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov.\nMinerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440,\n2019.\n11\n[19] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc., 2017.\n[20] M. Hinck, M. L. Olson, D. Cobbley, S.-Y. Tseng, and V. Lal. Llava-gemma: Accelerating multi-\nmodal foundation models with a compact language model. arXiv preprint arXiv:2404.01331,\n2024.\n[21] S. Hu and J. Clune. Thought cloning: Learning to think while acting by imitating human\nthinking. Advances in Neural Information Processing Systems, 36, 2024.\n[22] J. Huang, X. Ma, S. Yong, X. Linghu, et al. An embodied generalist agent in 3d world. arXiv\npreprint arXiv:2311.12871, 2023.\n[23] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. ICML, 2022.\n[24] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\nmodels. arXiv preprint arXiv:2207.05608, 2022.\n[25] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-\nford, J. Wu, and D. Amodei.\nScaling laws for neural language models.\narXiv preprint\narXiv:2001.08361, 2020.\n[26] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t.\nYih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems, 33:9459–9474, 2020.\n[27] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as\npolicies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753,\n2022.\n[28] S. Lifshitz, K. Paster, H. Chan, J. Ba, and S. McIlraith. Steve-1: A generative model for\ntext-to-behavior in minecraft. arXiv preprint arXiv:2306.00937, 2023.\n[29] H. Lin, Z. Wang, J. Ma, and Y. Liang. Mcu: A task-centric framework for open-ended agent\nevaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023.\n[30] H. Lin, B. Huang, H. Ye, Q. Chen, Z. Wang, S. Li, J. Ma, X. Wan, J. Zou, and Y. Liang. Selecting\nlarge language model to fine-tune via rectified scaling law. arXiv preprint arXiv:2402.02314,\n2024.\n[31] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information\nprocessing systems, 36, 2024.\n[32] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang. Sqa3d: Situated question\nanswering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022.\n[33] Y. Man, L.-Y. Gui, and Y.-X. Wang. Situational awareness matters in 3d vision language\nreasoning. In Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13678–13688, 2024.\n[34] F. Mentzer, D. Minnen, E. Agustsson, and M. Tschannen. Finite scalar quantization: Vq-vae\nmade simple. arXiv preprint arXiv:2309.15505, 2023.\n[35] OpenAI. Gpt-4 technical report, 2023.\n[36] Y. Qin, E. Zhou, Q. Liu, Z. Yin, L. Sheng, R. Zhang, Y. Qiao, and J. Shao.\nMp5: A\nmulti-modal open-ended embodied system in minecraft via active perception. arXiv preprint\narXiv:2312.07472, 2023.\n[37] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of\nmachine learning research, 21(140):1–67, 2020.\n[38] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,\nY. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175,\n2022.\n[39] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory\nand self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n12\n[40] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\n[41] T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. To-\nwards accurate generative models of video: A new metric & challenges. arXiv preprint\narXiv:1812.01717, 2018.\n[42] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017.\n[43] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption\ngenerator. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 3156–3164, 2015.\n[44] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager:\nAn open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291,\n2023.\n[45] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\nAligning language models with self-generated instructions, 2022.\n[46] Z. Wang, S. Cai, G. Chen, A. Liu, X. Ma, Y. Liang, and T. CraftJarvis. Describe, explain,\nplan and select: interactive planning with large language models enables open-world multi-task\nagents. In Proceedings of the 37th International Conference on Neural Information Processing\nSystems, pages 34153–34189, 2023.\n[47] Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng, Y. Yang, X. Ma,\nand Y. Liang. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal\nlanguage models. arXiv preprint arXiv: 2311.05997, 2023.\n[48] Z. Wang, A. Liu, H. Lin, J. Li, X. Ma, and Y. Liang. Rat: Retrieval augmented thoughts elicit\ncontext-aware reasoning in long-horizon generation. arXiv preprint arXiv:2403.05313, 2024.\n[49] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. 36th Conference on Neural Information\nProcessing Systems (NeurIPS 2022), 2022.\n[50] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing\nreasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n[51] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforcement\nlearning and planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\n[52] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y. Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu,\net al. Proagent: Building proactive cooperative ai with large language models. arXiv preprint\narXiv:2308.11339, 2023.\n[53] H. Zhao, Z. Cai, S. Si, X. Ma, K. An, L. Chen, Z. Liu, S. Wang, W. Han, and B. Chang. Mmicl:\nEmpowering vision-language model with multi-modal in-context learning. arXiv preprint\narXiv:2309.07915, 2023.\n[54] S. Zheng, Y. Feng, Z. Lu, et al. Steve-eye: Equipping llm-based embodied agents with visual per-\nception in open worlds. In The Twelfth International Conference on Learning Representations,\n2023.\n[55] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, et al.\nGhost in the minecraft: Generally capable agents for open-world enviroments via large language\nmodels with text-based knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\n13\nA\nTraining Details\nOmniJARVIS. We utilized the SFTTrainer class from the TRL library by Hugging Face to train the\nVLM model. The learning rate was set at 1.4e-5, and a cosine learning rate scheduler was employed.\nThe weight decay parameter was set to 0 with a warm-up ratio of 0.03. Training took place on 8 A800\nGPUs with FSDP, with a batch size of 2 and gradient accumulation steps of 4 using bf16 precision.\nThe training lasted for one epoch on our generated dataset. The raw interaction dataset comes from\nthe sections 6xx, 7xx, and 10xx of the contractor dataset provided by OpenAI [2] and the recording\ninteractions of JARVIS-1 Agents [47].\nBehavior Tokenizer. Each frame in our experiments has a resolution of 128x128 pixels. We\nsegmented each episode into multiple trunks, with each trunk consisting of 128 frames. The learning\nrate was set at 0.00004, with a weight decay of 0.001. The batch size was configured to 2, and training\nwas conducted on a cluster of eight NVIDIA 3090 Ti graphics cards. The training dataset comprised\nsections 6xx, 7xx, 9xx, and 10xx of the contractor dataset provided by OpenAI [2]. The precision for\ntraining was set to bfloat16.\nB\nFSD Computation\nThis section outlines the computation of FSD score between the generated videos and human\ngameplay recordings. First, we divide the videos into trunks of 128 frames. For each segment, we\nsample 16 frames, with 8 frames in between each sampled frame. These sequences of 16 frames are\nthen fed through the video encoder of MineCLIP [16] to obtain 512-dimensional video embeddings.\nFinally, the score is calculated according to [19] between the embeddings of the generated videos and\nthe reference videos.\nWe compute FSD scores between and within sets of videos using three distinct tasks, as illustrated in\nFigure B.1. A noticeable gap exists between the FSD scores calculated within the same set of videos\nand those calculated between different sets. Furthermore, the metric exhibits relative insensitivity\nto the number of videos used for computing the score, demonstrating the validity of our proposed\nmetric.\nFigure B.1: FSD scores between and within sets of videos for two distinct tasks. The horizontal axis represents\nthe number of videos used for computing the scores, and the vertical axis depicts the corresponding score.\nC\nBenchmarks Details\nC.1\nProgrammatic Tasks\nTable 6: Description and setting of programmatic tasks.\nGroop\nTask\nNumber\nMaximum\nSteps\nTask\nPrompt\nItem List\nWooden\n10\n3000\nMake {item} from empty inventory.\nstick, crafting_table, chest, ladder, bowl, button, door, boat, ...\nFood\n5\n6000\nGet the food {item}.\ncooked_chicken, cooked_mutton, cooked_porkchop, cooked_beef, bread\nStone\n5\n3000\nCraft a {item}.\ncharcoal, smoker, stone_sword, furnace, torch\nIron\n5\n6000\nSmelt iron ingots and craft {item}.\niron_sword, iron_ingot, bucket, iron_nugget, shears\nDiamond\n5\n12000\nDig down and craft {item}.\ndiamond_pickaxe, diamond_shovel, diamond_hoe, diamond_axe, diamond\n14\nTable 7: Embodied Question Answering Examples.\nCategory\nQuestion\nAnswer\nPlanning\nHow to obtain bucket with empty inventory step-by-step in Minecraft?\n1. mine 4 log without tool...\nPlanning\nHow to obtain cooked beef with empty inventory step-by-step in Minecraft?\n1. kill cow to obtain 1 beef...\nKnowledge\nHow many materials do I need to collect to make 2 iron ingots in one go?\nTo make 1 iron ingot, you need 1 iron ore and...\nKnowledge\nWhat are the materials to make 1 diamond pickaxe in Minecraft?\n3 diamond, 2 stick.\nKnowledge\nWhat are the materials to make 1 iron helmet in Minecraft?\n5 iron ingots.\nKnowledge\nWhat are the materials to make 1 golden axe in Minecraft?\n3 gold ingot, 2 stick.\nKnowledge\nWhat are the materials to make 1 wooden shovel in Minecraft?\n1 planks, 2 stick.\nKnowledge\nWhat are the materials to make 1 bread in Minecraft?\n3 wheat.\nReasoning\nCan diamond be mined with stone pickaxe in Minecraft?\nNo. Diamond can only be mined with iron...\nReasoning\nCan coal be mined with an iron pickaxe in Minecraft?\nYes. Coal can be mined...\nReasoning\nCan obsidian be mined with an iron pickaxe in Minecraft?\nNo. Diamond can only be mined with iron...\nReasoning\nCan lapis lazuli be mined with a diamond pickaxe in Minecraft?\nYes. Lapis lazuli can be mined\nReasoning\nCan emeralds be mined with a stone pickaxe in Minecraft?\nNo. Emeralds can only be mined with an iron...\nDecision-making\n<image><inventory> To mine diamonds, what behavior should I do now?\ncraft iron pickaxe\nC.2\nEmbodied Question Answering Benchmarks\nThe embodied question-answering benchmarks consist of questions and instructions for Minecraft\nbenchmarks, consisting of over 100 questions on knowledge question answering, embodied planning,\nand math reasoning.\nTo enhance the LLM-based agent’s understanding of Minecraft, we gathered a large dataset called\nMinecraft-Embodied-Question-Answering-300k (mc-eqa-300k). Initially, we extracted language\ncorpus from the Minecraft Wiki2 and DigMinecraft3 website to create prompts for the language\nmodel (ChatGPT). This resulted in around 50k question-answer pairs. Using these as seed questions,\nChatGPT generated approximately 250k diverse questions on various aspects of Minecraft. To ensure\naccurate responses and avoid hallucinations common with existing models like self-instruct [45], we\nemployed a retrieval augmented generation [26] method known as RAT [48] to generate accurate\nanswers. The resulting question-answering dataset, along with interaction data, form our training\ndataset for OmniJARVIS.\nExample of Embodied Question Answering Generation for Reasoning Group\nquestion: Can stone pickaxe be used to mine diamonds?\nanswer: No, a stone pickaxe cannot be used to mine diamonds in Minecraft. An iron pickaxe or better is\nrequired to mine diamonds.\nquestion: Can golden pickaxe be used to mine diamonds?\nanswer: In Minecraft, you cannot use a golden pickaxe to mine diamonds. Diamonds are one of the hardest\nmaterials in the game and require an iron pickaxe or better to mine. Using a gold pickaxe on diamond ore\nblocks will cause them to break, but you won’t receive any diamonds as a drop. You’ll just get the diamond\nore block itself.\nTo successfully mine diamonds, you should use an iron, diamond, or netherite pickaxe. These pickaxes have\nthe required hardness to break diamond ore blocks and yield diamonds when mined.\n2https:\/\/minecraft.wiki\/\n3https:\/\/www.digminecraft.com\/\n15\nExample of Embodied Question Answering Generation for Planning Group\nquestion: Give you nothing in the inventory, how to obtain diamond pickaxe from empty inventory, generate\na step-by-step plan, start from punching trees, and describe every step in detail.\nanswer: Obtaining a diamond pickaxe in Minecraft involves several steps, starting from punching trees and\neventually progressing to mining diamonds. Here’s a step-by-step plan to obtain a diamond pickaxe from an\nempty inventory:\n**Step 1: Punching Trees**\n1. **Gather Wood:** Start by punching trees to collect wood blocks. You’ll need at least 3 logs to proceed.\n**Step 2: Crafting Basic Tools**\n2. **Craft Wooden Planks:** Open your inventory and turn the logs into wooden planks by placing them in\nthe crafting grid (4 wooden logs -> 16 wooden planks).\n3. **Craft Crafting Table:** Use the wooden planks to craft a crafting table. Place 4 wooden planks in the\n2x2 crafting grid to create it.\n4. **Craft Wooden Pickaxe:** With the crafting table, craft a wooden pickaxe using wooden planks and\nsticks. Place 3 wooden planks in the top row and 2 sticks in the middle row.\n**Step 3: Finding Stone**\n5. **Mine Stone:** Use your wooden pickaxe to mine stone blocks. You’ll need at least 3 stone blocks to\nproceed.\n**Step 4: Upgrading Tools**\n6. **Craft Furnace:** Use the crafting table to create a furnace using 8 pieces of cobblestone. Place the\ncobblestone in all squares of the crafting grid except the center square.\n7. **Smelt Cobblestone:** Use the furnace to smelt the cobblestone into smooth stone. Place the cobblestone\nin the top slot of the furnace and fuel (wooden planks, sticks, etc.) in the bottom slot. Collect the smooth\nstone when it’s ready.\n**Step 5: Crafting a Stone Pickaxe**\n8. **Craft Stone Pickaxe:** With the smooth stone, craft a stone pickaxe using cobblestone and sticks. Place\n3 cobblestones in the top row and 2 sticks in the middle row.\n**Step 6: Upgrade to Iron Pickaxe**\n9. **Find Iron Ore:** Find iron ore blocks underground (usually at lower depths) and mine them using your\nstone pickaxe. You’ll need at least 3 iron ore blocks.\n10. **Smelt Iron Ore:** Place your iron ore in the top slot of the furnace and use wooden planks as fuel in\nthe bottom slot. This will smelt the iron ore into iron ingots.\n11. **Craft Iron Pickaxe:** With the iron ingots, craft a iron pickaxe using iron ingots and sticks. Place 3\niron ingots in the top row and 2 sticks in the middle column.\n**Step 7: Mining Diamonds**\n12. **Find Diamonds:** Use your iron pickaxe to mine for diamonds. Diamonds are found deep underground,\nusually between levels 5 to 12. They appear as blue, sparkly ore blocks.\n**Step 8: Crafting a Diamond Pickaxe**\n13. **Craft Diamond Pickaxe:** Once you have collected at least 3 diamonds, craft a diamond pickaxe using\ndiamonds and sticks. Place 3 diamonds in the top row and 2 sticks in the middle row.\n**Step 9: Using the Diamond Pickaxe**\n14. **Mine Ores:** The diamond pickaxe is more durable and efficient than lower-tier pickaxes. Use it to\nmine a variety of ores, including iron, gold, redstone, and more.\nAnd that’s it! You’ve successfully obtained a diamond pickaxe from an empty inventory by following these\nsteps. Remember that Minecraft’s world generation and mechanics might vary slightly depending on the\nversion you’re playing, so adapt the plan as needed.\n16\nExample of Embodied Question Answering Generation for Knowledge Group\nquestion: How to craft diamond pickaxe step-by-step, describe every step in details.\nanswer: To craft a diamond pickaxe in Minecraft, you will need to follow these steps:\n1. **Gather Resources:**\n- Start by gathering the necessary resources to craft a diamond pickaxe. You’ll need three diamonds and two\nsticks.\n- Diamonds can be found deep underground, typically between levels 1 to 15 in the Overworld. You’ll need\nan iron pickaxe or better to mine diamonds. Use this iron pickaxe to mine the diamond ore blocks.\n2. **Crafting Sticks:**\n- If you haven’t already, craft sticks from wooden planks. Open your crafting table and place two wooden\nplanks vertically to create four sticks.\n3. **Crafting the Diamond Pickaxe:**\n- Open your crafting table, which consists of a 3x3 crafting grid.\n- Place the three diamonds in the top row.\n- Place two sticks in the middle row, one in the center slot and one in the slot just below it.\n- Your crafting grid should now look like this:\n[D][D][D]\n[ ][S][ ]\n[ ][S][ ]\n- [D] represents a diamond.\n- [S] represents a stick.\n- Once you’ve placed the items in the correct pattern, you’ll see a diamond pickaxe appear in the result box of\nthe crafting table.\n4. **Collect the Diamond Pickaxe:**\n- Click or drag the diamond pickaxe from the crafting table’s result box into your inventory.\nCongratulations! You now have a diamond pickaxe in your inventory. Diamond pickaxes are extremely\nvaluable in Minecraft because they can mine most blocks faster than other pickaxes and are required to mine\ncertain valuable resources like obsidian. Be sure to use it wisely and take care of it, as it’s a valuable tool in\nthe game.\n17\nC.3\nOpen-ended Instruction-Following Tasks\nPrompts for Open-ended Instruction-following Tasks\n1. Cook the beef with a furnace and recycle the furnace. If you meet night, place and use the bed for sleeping.\n2. Explore caves, mine resources, and craft items in Minecraft to progress and survive.\n3. Gather resources, craft tools, and cook food in Minecraft.\n4. Place a torch on the wall to light the environment. Collect and picking it up when you leave.\n5. Craft an oak boat and use it to travel\n6. Obtain resources for building and survival by gathering materials and farming resources.\n7. Consistently interact with chests to manage inventory contents.\n8. Explore and gather resources in Minecraft.\n9. Collect and mine azure bluets, deal damage to mobs, and defeat mobs in the game.\n10. Do the following tasks sequentially: 1. Gather oak logs and oak leaves from trees. 2. Use oak logs to create\noak planks and then a crafting table. 3. Create sticks from oak planks using the crafting table. 4. Craft a\nwooden axe and a wooden pickaxe using sticks and oak planks. 5. Collect materials like mushrooms and\nbrown mushrooms by mining blocks with the wooden axe. 6. Mine grass, tall grass, and stone using the\nwooden tools for resources.\n11. Harvest sugar cane to obtain multiple sugar cane pieces.\n12. Plant and consume wither roses repeatedly.\n13. Harvest wheat seeds, plant them, and use the harvested wheat seeds to feed animals or craft items such as\nbread.\n14. Trade with a villager by giving emeralds and books to receive enchanted books as well as new emeralds\nand books.\n15. Mine ice using an iron pickaxe and pick up the ice block obtained.\n16. Open a chest in the game to access or manage inventory items.\nD\nPrompt for Instruction Generation\nPrompt D.1:Prompt for Instruction Generation\n**Instruction**:\nThis is a paragraph of description of the player’s gameplay in Minecraft. The caption summarizes the current\nenvironmental state and agent behavior, with the timestamp indicating which frame of the video this caption\nis from. Please summarize what tasks the agent completed throughout the entire video. Please guess what\ninstruction or task the player received to exhibit such behaviors. This task should be clear and in details.\n**IMPORTANT**:\nDIRECTLY output the task.\nDO NOT repeat user input.\nDO NOT add additional explanations or\nintroducement in the answer unless you are asked to.\n**Observation**:\nStats\nminecraft.custom:minecraft.interact_with_furnace\nhappens.\nGui\nis\nopen.\nNew\nstats\nminecraft.craft_item:minecraft.cooked_beef\nhappens.\nGet\nnew\nitem:\ncooked_beef*9.\nGet\nnew item:\nstone_pickaxe*1.\nStats minecraft.use_item:minecraft.stone_pickaxe happens.\nStats\nminecraft.mine_block:minecraft.furnace\nhappens.\nStats\nminecraft.pickup:minecraft.furnace\nhap-\npens.\nGet new item:\nfurnace*1.\nStats minecraft.use_item:minecraft.white_bed happens.\nStats\nminecraft.mine_block:minecraft.white_bed happens. Stats minecraft.pickup:minecraft.white_bed happens.\nGet new item: white_bed*1. New stats minecraft.use_item:minecraft.cooked_beef happens. Consume\ncooked_beef*1. **Task**:\n1. Interact with a furnace to smelt cooked_beef and eat the cooked_beef. 2. Place a white_bed and sleep on it\nto survive the night.\n**Observation**:\n{observation}\n18\nExample of Instruction Generation\nExample:\n**Observation**:\nConsume chest*1. Stats use_item:chest happens. Consume chest*1. Stats use_item:chest happens. Consume\nchest*1. Stats use_item:chest happens. Consume chest*1. Stats use_item:chest happens. Consume item:\nchest*1. Stats use_item:chest happens. Stats custom:open_chest happens. Open Game 2D GUI. Consume\noak_planks*24. Consume item: birch_planks*5. Stats custom:open_chest happens. Open Game 2D GUI.\nConsume lapis_lazuli*22. Consume item: iron_ingot*18. Consume item: potato*30. Consume item:\ncarrot*9. Consume item: wheat*4. Stats custom:open_chest happens. Consume oak_planks*64. Consume\nitem: oak_planks*44. Stats custom:open_chest happens. Open Game 2D GUI. Consume item: granite*20.\nStats custom:open_chest happens. Open Game 2D GUI. Consume item: oak_sapling*2. Consume item:\nbirch_sapling*4. Consume item: wheat_seeds*12. Consume item: poisonous_potato*1. Consume item:\nbread*1. Stats custom:open_chest happens. Open Game 2D GUI. Stats custom:open_chest happens.\nGet new item: wheat*4. Get new item: carrot*9. Get new item: potato*30. Stats custom:open_chest\nhappens. Open Game 2D GUI. Consume item: wheat*4. Consume item: carrot*9. Consume item:\npotato*30. Get new item: potato*15. Stats custom:open_chest happens. Open Game 2D GUI. Consume\nitem: lapis_lazuli*64. Stats custom:interact_with_furnace happens. Open Game 2D GUI. Consume item:\npotato*15. Stats custom:open_chest happens. Open Game 2D GUI. Get new item: iron_ingot*18. Stats\ncustom:interact_with_crafting_table happens. Open Game 2D GUI. Consume item: stick*28. Get new item:\nstick*22. Consume item: iron_ingot*18.\n**Generated Instruction**:\nTask: Organize and manage inventory by storing items in chests, crafting various items, and using the furnace\nto smelt resources efficiently. Also, focus on gathering resources like wood, ores, food items, and plant\nmaterials for crafting and survival purposes.\n19\nE\nPrompt for Thought Generation\nPrompt E.1:Prompt for Thought Generation\nA player is playing Minecraft. I need you to give thought about what behavior it should take next given\ncurrent situation. Here are some demonstrations:\nTask: \"Obtain a diamond\"\nState: \"The image captures a scene from the popular video game, There is a grass block in front of the agent.\"\nInventory: {’dirt’: 10}\nMemory: {}\nThought: \"The player has nothing in its inventory, it should first go to harvest some oak logs. However\nthere is a grass block in front of the agent, it should mine it first to find a way. So the behavior should be\n{mine_block:grass:1}.\"\nBehavior: {’mine_block:grass’: 1}\nTask: \"Obtain an iron pickaxe\"\nState: \"In the image, a player in the video game is standing in a dark cave.\"\nInventory: {’dirt’: 20, ’stick’: 10, ’iron_ore’: 5, ’furnace’: 1, ’stone_pickaxe’: 1}\nMemory: {’mine_block:iron_ore’: 5, ’craft_item:furnace’: 1}\nThought: \"The player has 5 iron ores in its inventory, it should smelt them to get iron ingots. However, it does\nnot have enough coal to smelt the iron ores. The player should mine some coal ores first. And using the stone\npickaxe in the inventory can help to mine the coal ores. So the behavior should be {use_item:stone_pickaxe:1,\nmine_block:coal_ore:1}.\"\nBehavior: {’use_item:stone_pickaxe’: 1, ’mine_block:coal_ore’: 1}\nTask: \"Harvest logs\"\nState: \"The image captures a moment in the video game The player’s character, standing in the center of the\nframe, is holding a crafting table in their hands. The crafting table, which is the main focus of the image, is\ngray and has a crafting grid on top of it.\"\nInventory: {’oak_log’: 20, ’stick’: 8}\nMemory:\n{’use_item:wood_axe’:\n40,\n’craft_item:stick’:\n8,\n’mine_block:oak_log’:\n20,\n’break_item:wood_axe’: 1}\nThought: \"The player needs to harvest more logs. However the last memory shows that the player has\nbroken the wood axe. So the player should craft a new wood axe first. The player has enough logs in the\ninventory and the crafting table in the inventory can help to craft the wood axe. So the behavior should be\n{use_item:crafting_table:1}.\"\nBehavior: {\"use_item\"crafting_table\": 1}\nIn demonstrations, \"Task\" is the goal of player.\n\"State\" describes the image the player is facing,\n\"Inventory\" is its current inventory and \"Memory\" contains past behaviors taken by the player. \"Memory\"\nis sorted by time, with the most recent behavior at the end. You should pay attention to recent behaviors.\nAccording to these information, players first generate thoughts about what to do next(\"Thought\") and then\ntake behaviors accordingly(\"Behavior\"). In the first demo, the behavior is not relevant to the task; in the\nsecond demo, the thought considers needs of the task and current inventory; the third demo considers the\nmemory of the player and identify the need of crafting a new tool to replace the broken tool. Now think\nabout the following situation:\nTask: {task}\nState: {state}\nInventory: {inventory}\nMemory: {memory}\nThought: {}\nBehavior: {behavior}\nGiven current situation and the behavior the player will take, output a simple thought that will di-\nrectly lead to this behavior. Please carefully revise the need of the task, current inventory and recent\nmemory of the player. Be sure to explain every part of the behavior. The output format should be \"Thought:\nreason...So the behavior should be {behavior}\".\n20\nExample of Thought Generation\nExample:\nTask: \"The player was instructed to mine various resources and craft tools in Minecraft: 1. Start by mining\ncoal ore and crafting cooked beef from it. 2. Smelt iron ore and cook food in the furnace. 3. Mine stone to\ncollect cobblestone. 4. Craft a stone pickaxe and use it to mine various ores like coal, iron, and diorite. 5.\nCreate torches from coal and sticks. 6. Craft a stone pickaxe and an iron pickaxe. 7. Use the iron pickaxe to\nmine granite and gather resources. 8. Interact with a crafting table to craft items like an iron pickaxe, torches,\nand iron ingots. 9. Utilize tools like pickaxes to mine stones and different ores efficiently. 10. Gather various\nresources like coal, iron, cobblestone, diorite, and granite. 11. Keep crafting and mining to progress in the\ngame.These actions showcase a cycle of resource gathering, processing, and crafting to advance the player’s\ncapabilities and inventory in the game.\"\nState: \"The image captures a moment in the video game Minecraft. The player’s character, standing in the\ncenter of the frame, is holding a crafting table in their hands. The crafting table, which is the main focus of\nthe image, is gray and has a crafting grid on top of it. In the crafting grid, there are several items arranged in\nrows and columns. Starting from the top left, there’s a book, followed by a loom in the middle, and a furnace\nat the bottom. The crafting table is set against a black background, which contrasts with the gray color of the\ntable and the items on it. At the bottom of the image, there’s a red banner with the text \"Crafting\" written on\nit. This banner adds a pop of color to the otherwise monochrome image. The overall composition of the\nimage suggests that the player is in the process of crafting something, possibly a book or a loom, using the\nitems in the crafting grid.\"\nInventory: {’wooden_shovel’: 1, ’wooden_axe’: 1, ’cobblestone’: 51, ’dirt’: 14, ’andesite’: 23, ’iron_ore’:\n7, ’coal’: 31, ’stick’: 54, ’birch_log’: 5, ’birch_planks’: 47, ’furnace’: 1, ’crafting_table’: 1, ’granite’: 4,\n’diorite’: 7, ’wooden_pickaxe’: 1}\nMemory:\n{use_item:stone_pickaxe’:\n63,\n’mine_block:coal_ore’:\n9,\n’pickup:coal’:\n9,\n’mine_block:wall_torch’: 1, ’use_item:torch’: 3, ’pickup:torch’: 1, ’mine_block:granite’: 4, ’pickup:granite’:\n4, ’mine_block:stone’:\n38, ’pickup:cobblestone’:\n37, ’mine_block:diorite’:\n8, ’pickup:diorite’:\n7,\n’use_item:cobblestone’: 3, ’mine_block:iron_ore’: 3, ’pickup:iron_ore’: 3, ’break_item:stone_pickaxe’: 1}\nUpdated Inventory:{’wooden_shovel’: 1, ’wooden_axe’: 1, ’cobblestone’: 51, ’dirt’: 14, ’andesite’: 23,\n’iron_ore’: 7, ’coal’: 31, ’stick’: 54, ’birch_log’: 5, ’birch_planks’: 47, ’furnace’: 1, ’granite’: 4, ’diorite’: 7,\n’wooden_pickaxe’: 1}\nUpdated Memory:\n{use_item:stone_pickaxe’:\n63, ’mine_block:coal_ore’:\n9, ’pickup:coal’:\n9,\n’mine_block:wall_torch’: 1, ’use_item:torch’: 3, ’pickup:torch’: 1, ’mine_block:granite’: 4, ’pickup:granite’:\n4, ’mine_block:stone’:\n38, ’pickup:cobblestone’:\n37, ’mine_block:diorite’:\n8, ’pickup:diorite’:\n7,\n’use_item:cobblestone’: 3, ’mine_block:iron_ore’: 3, ’pickup:iron_ore’: 3, ’break_item:stone_pickaxe’: 1\n’use_item:crafting_table’}\nGenerated Thought:\nThought: The player’s current inventory includes various resources like cobblestone, coal, iron ore, and\ngranite, along with essential tools like a wooden pickaxe, shovel, and axe. Recent memory indicates the\nplayer has been actively mining various blocks and collecting resources, including coal, granite, and iron ore.\nHowever, the memory also shows that the player’s stone pickaxe has broken. To progress efficiently and craft\nnecessary items, such as torches and better pickaxes, the player should utilize the crafting table. So the\nbehavior should be {’use_item:crafting_table’: 1}.\n21\nF\nPrompt for Memory Summarization\nPrompt F.1:Prompt for Memory Summarization\nA player is playing Minecraft.The situation of the player contains 4 parts: task, state, inventory and memory.\nUnder this situation, the player will take a behavior. And after this behavior, the player’s memory will be\nupdated to \"Updated Memory\". I need you to give a subpart of the player’s updated memory that is most\nrelevant to its task.\nTask is the goal of the player. State describes the image the player is facing, Inventory is its current inventory\nof items. Memory contains its past behaviors, each item in memory is its past behavior and the number of\nthis behavior. The memory is sorted by time, with the most recent behavior at the end. There are mainly 9\ntypes of behavior:\n+ ’craft_item:x’ means to craft an item x;\n+ ’drop:x’ means to drop an item x;\n+ ’use_item:x’ means to use an item x;\n+ ’pickup:x’ means to pickup an item x;\n+ ’custom’ means to custom its playing status;\n+ ’mine_block:x’ means to mine a block x;\n+ ’kill_entity:x’ means to kill an entity x;\n+ ’entity_killed_by:x’ means the player is killed by an entity x;\n+ ’break_item:x’ means an item x got broken.\nHere is the player’s current situation:\nTask: {task}\nState: {state}\nInventory: {inventory}\nBehavior: {behavior}\nUpdated Memory: {updated_memory}\nI need you to summarize what the player has done to complete the task according to the updated\nmemory. Please make sure every part in your summary is relevant to the task. The output format should be:\n\"The player first ..., then ..., and finally ...\" Then in a new line, try to summarize which stage of the task the\nplayer is in according to the memory.\n22\nExample of Memory Summarization\nExample:\nTask: \"Gather various resources including andesite, granite, diorite, coal, iron ore, and cobblestone using a\nstone pickaxe. Craft and use torches for illumination. Upgrade from a wooden to a stone pickaxe and craft a\nstone sword for defense. Explore and mine in a systematic way, ensuring to light up the environment with\ntorches and replacing tools as they wear out.\"\nState: \"The image captures a moment in a video game, specifically Minecraft. The scene is set in a dimly\nlit cave, with a wooden pillar standing prominently in the foreground. The player’s inventory and score are\ndisplayed in the top left corner of the screen, providing a glimpse into the player’s progress in the game. In\nthe bottom right corner, the player’s health and hunger bars are visible, indicating the player’s current status\nin the game. The rest of the screen is filled with a series of lines of text, each line representing a command or\ninstruction from the game. These commands seem to be related to the player’s movement and interaction\nwith the environment, guiding the player through their adventure in Minecraft. The image is a snapshot of a\ncomplex digital world, where every command and action is carefully calculated and executed. It’s a testament\nto the immersive and engaging nature of video games like Minecraft.\"\nInventory: {’stone_pickaxe’: 1, ’wooden_axe’: 1, ’oak_log’: 8, ’stone_sword’: 1, ’andesite’: 8, ’coal’:\n13, ’oak_planks’: 2, ’charcoal’: 2, ’torch’: 27, ’dirt’: 1, ’furnace’: 1, ’cobblestone’: 108, ’stick’: 21,\n’iron_ore’: 2, ’granite’: 15, ’diorite’: 9, ’wooden_pickaxe’: 1} Behavior: {’use_item:wooden_axe’: 1,\n’mine_block:oak_fence’: 1}\nUpdated Memory: {’pickup:dirt’: 1, ’mine_block:oak_leaves’: 4, ’mine_block:andesite’: 8, ’pickup:furnace’:\n1, ’use_item:stone_pickaxe’: 169, ’mine_block:stone’: 125, ’craft_item:charcoal’: 4, ’use_item:torch’: 14,\n’mine_block:oak_log’: 21, ’mine_block:furnace’: 1, ’craft_item:stone_pickaxe’: 2, ’pickup:oak_planks’:\n2, ’craft_item:wooden_axe’: 1, ’craft_item:stick’: 40, ’use_item:wooden_pickaxe’: 13, ’craft_item:air’:\n0, ’pickup:andesite’:\n8, ’use_item:furnace’:\n1, ’mine_block:grass’:\n1, ’mine_block:crafting_table’:\n2, ’craft_item:furnace’: 1, ’pickup:crafting_table’: 2, ’pickup:coal’: 21, ’mine_block:grass_block’:\n1, ’craft_item:torch’: 40, ’pickup:oak_log’: 23, ’pickup:cobblestone’: 124, ’craft_item:oak_planks’:\n36,\n’craft_item:crafting_table’:\n1,\n’use_item:crafting_table’:\n3,\n’use_item:wooden_axe’:\n18,\n’craft_item:wooden_pickaxe’: 1, ’mine_block:coal_ore’: 21, ’mine_block:granite’: 15, ’pickup:granite’:\n15, ’drop:stone_pickaxe’: 1, ’pickup:stone_pickaxe’: 1, ’mine_block:diorite’: 9, ’pickup:diorite’: 9,\n’break_item:stone_pickaxe’: 1, ’craft_item:stone_sword’: 1, ’mine_block:iron_ore’: 2, ’pickup:iron_ore’: 2,\n’mine_block:wall_torch’: 1, ’pickup:torch’: 1, ’mine_block:oak_fence’: 1}\nGenerated Summary:\nThe player first gathered various resources including andesite, granite, diorite, coal, iron ore, and cobblestone\nusing a stone pickaxe. Then, they crafted and used torches for illumination, upgraded from a wooden to\na stone pickaxe, and crafted a stone sword for defense. Finally, they explored and mined systematically,\nensuring to light up the environment with torches and replacing tools as they wore out.\nThe player is in the stage of gathering various resources, crafting torches, upgrading tools, and exploring\nsystematically.\nG\nExamples of OmniJARVIS Interaction Process\nThe tokens with blue color are prompted from the environment observation and user, and the tokens\nwith red color are generated by OmniJARVIS.\n23\nExample 1 of OmniJARVIS Interaction\nTASK: Mine stone with an iron pickaxe, gather cobblestone, then switch to gravel, mining it with a wooden\nshovel to collect some. Return to mining stone with the iron pickaxe, place torches for visibility, then\nproceed to mine iron ore as well. The agent collects items along the way, including gravel and iron ore while\nconstantly gathering cobblestone throughout the process.\nMEMORY: I have mine 13 stone blocks and 1 gravel block.\nINVENTORY: iron_pickaxe: 1, wooden_shovel: 1, wooden_axe: 1, cobblestone: 156, crafting_table: 1,\nandesite: 23, iron_ore: 4, coal: 31, birch_log: 5, birch_planks: 47, furnace: 1, gravel: 1, granite: 12, diorite:\n28, wooden_pickaxe: 1, stone_pickaxe: 1, dirt: 14, stick: 43\nOBSERVATION:\nCAPTION: The image captures a moment in the video game Minecraft. The player’s character is standing at\nthe entrance of a cave, which is characterized by its dark gray walls and ceiling. The floor of the cave is a\ncontrasting light gray, with a few small rocks scattered about. In the bottom left corner of the screen, the\nplayer’s health and hunger bars are visible, indicating the status of the character. The top right corner of the\nscreen displays the player’s inventory, showcasing a variety of items including a pickaxe, a sword, and some\nfood. The most striking feature of the image is the text Iron Pickaxe that appears in the bottom right corner\nof the screen. This suggests that the player might be using an iron pickaxe, a common tool in the game for\nmining. The precise location of the objects and their relative positions contribute to the immersive gaming\nexperience.\nTHOUGHT: The player needs to mine iron ore with an iron pickaxe to progress towards the task. Recent\nmemory indicates a focus on using the iron pickaxe for various mining activities. Since iron ore is a key\ncomponent in the task’s objectives and the player already possesses the required tool, the next step should be\nto mine the iron ore. So the behavior should be ’use_item:iron_pickaxe’, ’mine_block:iron_ore’.\nCONTROL: TOKEN[BEHAVIOR-12372].\n24\nExample 2 of OmniJARVIS Interaction\nTASK: Task: 1. Smelt iron ingots in a furnace. 2. Craft an iron pickaxe from iron ingots and sticks. 3. Mine\ndiamond ore blocks using the iron pickaxe, collecting diamonds. 4. Mine redstone ore, collecting redstone. 5.\nMine diorite blocks. 6. Craft a diamond pickaxe using diamonds and sticks. 7. Craft additional diamond\npickaxes using cobblestone and the previous diamond pickaxe.\nMEMORY: I have smelt 3 iron ingots.\nINVENTORY: oak_log: 9, andesite: 3, dirt: 6, oak_planks: 8, iron_ingot: 3, cobblestone: 124, stone_axe: 1,\nstone_pickaxe: 1, granite: 15, wheat_seeds: 1, diorite: 9, stick: 7\nOBSERVATION:\nCAPTION: The image captures a scene from the video game Minecraft. In the center of the frame, a player’s\nhand is seen holding a torch, which is ablaze with orange and yellow flames. The torch is being used to\nignite a pile of wooden planks that are stacked haphazardly on top of each other. The planks are brown and\nappear to be made of wood. The player’s health and hunger bars are visible in the bottom left corner of\nthe screen, indicating the player’s status in the game. In the bottom right corner, the player’s inventory is\ndisplayed, showcasing the tools at their disposal. The background of the image reveals a cave-like structure,\ncharacterized by stalactites hanging from the ceiling and stalagmites rising from the floor. The walls of the\ncave are made of gray stone, providing a stark contrast to the wooden planks. Overall, the image provides a\nsnapshot of a moment in the game, capturing the player’s interaction with the environment and the tools at\ntheir disposal.\nTHOUGHT: The player holds enough iron ingots in their inventory, fulfilling the task requirement to craft\niron ingots in a furnace. However, to proceed with the subsequent tasks which involve mining diamond ore\nand redstone ore, acquiring an iron pickaxe is crucial. Considering the current inventory, the player possesses\nthe necessary materials to craft an iron pickaxe - iron ingots and sticks. Crafting the iron pickaxe now will\nenable the player to move efficiently towards their goal of mining valuable resources like diamonds and\nredstone. Therefore, the immediate action should be to craft an iron pickaxe. So the behavior should be\ncraft_item:iron_pickaxe.\nCONTROL: TOKEN[BEHAVIOR-1375].\n25\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents.pdf"}
{"title":"MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory","authors":"Junyeong Park, Junmo Cho, Sungjin Ahn","summary":"Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https:\/\/sites.google.com\/view\/mr-steve.","url":"http:\/\/arxiv.org\/abs\/2411.06736v4","pdf_url":"http:\/\/arxiv.org\/pdf\/2411.06736v4","published":1731305093000,"comment":null,"pdf_text":"Preprint\nMRSTEVE: INSTRUCTION-FOLLOWING AGENTS\nIN MINECRAFT WITH WHAT-WHERE-WHEN MEMORY\nJunyeong Park1∗, Junmo Cho1∗, Sungjin Ahn1\n1KAIST\nABSTRACT\nSignificant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented hierar-\nchical approaches. While these approaches, which combine high-level planners\nwith low-level controllers, show promise, low-level controllers frequently become\nperformance bottlenecks due to repeated failures. In this paper, we argue that the\nprimary cause of failure in many low-level controllers is the absence of an episodic\nmemory system. To address this, we introduce MrSteve (Memory Recall Steve-1),\na novel low-level controller equipped with Place Event Memory (PEM), a form of\nepisodic memory that captures what, where, and when information from episodes.\nThis directly addresses the main limitation of the popular low-level controller,\nSteve-1. Unlike previous models that rely on short-term memory, PEM organizes\nspatial and event-based data, enabling efficient recall and navigation in long-horizon\ntasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented\nTask Solving Framework, allowing agents to alternate between exploration and\ntask-solving based on recalled events. Our approach significantly improves task-\nsolving and exploration efficiency compared to existing methods. We will release\nour code and demos on the project page: https:\/\/sites.google.com\/view\/mr-steve.\n1\nINTRODUCTION\nThe emergence of large-scale foundation models has driven significant advances in developing\ngeneral-purpose embodied AI agents capable of generalizing across a broad spectrum of tasks in\ncomplex, open, and real-world-like environments (Johnson et al., 2016; Guss et al., 2019; Fan et al.,\n2022b; Hafner; Albrecht et al., 2022; Voudouris et al., 2023). While simulating such environments for\neffective learning and evaluation remains a major challenge, Minecraft has become a leading testbed,\noffering a demanding, open-ended environment with rich interaction possibilities. Its procedurally\ngenerated world presents agents with challenges like exploration, resource management, tool crafting,\nand survival, all requiring advanced decision-making and long-horizon planning. For instance, the\ntask of obtaining a diamond\nrequires agents to locate diamond ore\n, and craft an iron pickaxe\n. This process involves finding, mining, and refining iron ore\n, requiring the agent to execute\ndetailed long-term planning over roughly 24,000 environmental steps (Li et al., 2024).\nSolving such tasks through Reinforcement Learning (RL) approaches from scratch is nearly infeasible;\nhowever, recent LLM-augmented hierarchical methods have demonstrated a promising avenue (Huang\net al., 2022a;b; Wang et al., 2023a). These methods feature a division between high-level planners\nand low-level controller policies. High-level planners, driven by Large Language Models (LLMs) or\nMultimodal Large Language Models (MLLMs), propose subgoals by utilizing the reasoning abilities\nand prior knowledge inherent in LLMs (Brown et al., 2020; Touvron et al., 2023; OpenAI, 2024).\nThese subgoals, conveyed in textual instruction form, are then sequentially passed to a learned,\ninstruction-following low-level controller for execution (Wang et al., 2023c;b; Li et al., 2024).\nFor this framework to be effective, it is essential that both the high-level planner and the low-level\ncontroller improve in tandem. However, previous research has primarily focused on enhancing\nhigh-level planning, e.g., via maintaining skill library (Zhu et al., 2023; Wang et al., 2023b; Qin et al.,\n∗Equal contribution. Correspondence to Junyeong Park and Sungjin Ahn.\nContact:{jyp10987,sungjin.ahn}@kaist.ac.kr\n1\narXiv:2411.06736v4  [cs.LG]  25 Dec 2024\nPreprint\n2024; Li et al., 2024), often assuming that low-level controllers will efficiently execute the subgoals\nprovided by the high-level planner. However, this assumption frequently does not hold in practice,\nand the low-level controller becomes a significant performance bottleneck (Cai et al., 2023b).\nIn this regard, we specifically focus on limitations in Steve-1 (Lifshitz et al., 2024), the most widely\nused low-level instruction-following controller framework. Steve-1 is an instruction-following policy\nobtained by fine-tuning the Video Pre-Training (VPT) (Baker et al., 2022) model. A primary limitation\nwe focus on is its constrained episodic memory capability. Steve-1 is based on Transformer-XL (Dai\net al., 2019), which leverages relatively short-term memory, retaining only the last 128 hidden states.\nGiven Minecraft’s simulation speed of 20Hz, this memory span amounts to only a few seconds of\ngameplay. While it can be increased, the quadratic complexity and FIFO-only memory structure of\ntransformers make them significantly inefficient for long-horizon tasks.\nAs a result, when the agent requires information beyond this short memory span, it tends to forget past\nevents within the episode and reverts to inefficient random exploration for each new task, consuming\nexcessive time. For example, when given a task like “Find a Cow”, the agent is unable to recall,\n‘I’ve seen it before near the river in the north’. Ideally, a low-level agent would instead maintain an\nepisodic memory of meaningful events and recall relevant information. See Figure 1 for more detail\nillustration. Moreover, Steve-1 not only lacks the ability to recall such memories but also the ability to\nnavigate directly to the associated locations, which could help avoid unnecessary exploration. Instead,\nSteve-1 relies on a “Go Explore” instruction, randomly exploring until it stumbles upon the resource\nby chance. This inefficiency in executing low-level primitives is not addressed by high-level planners,\nwhich focus on optimizing the sequence of high-level skills (i.e., the plan) but do not optimize the\nexecution of the primitives themselves. We elaborate more on this in Appendix A.\nIn this paper, we introduce an enhanced low-level controller agent, MrSteve (Memory Recall Steve-1),\ndesigned to address the limitations of Steve-1. The key innovation of MrSteve is the integration of\nPlace Event Memory (PEM) which is the instantiation of What-Where-When Episodic Memory.\nWhile previous approaches have explored episodic memory, they primarily target high-level planners,\nsuch as building libraries of high-level skills and plans, which do not directly improve the low-level\ncontroller’s performance. We argue it is essential for the low-level controller to possess memory\ncapabilities. To address this, PEM manages memory more effectively, surpassing the limitations of the\nnon-scalable FIFO memory found in transformers. PEM stores spatial and event-based information,\nallowing the agent to hierarchically organize and retrieve details about locations and events it has\npreviously encountered. For PEM to be fully effective, the agent must also move directly to the\ndesired location along with the ability to modulate between exploration and goal-directed navigation-\nand-execution, a capability lacking in Steve-1. Therefore, we introduce the second component of\nMrSteve: the Exploration Strategy and Memory-Augmented Task Solving Framework. Built upon\nthe PEM structure, this framework enables the agent to alternate between exploration—when no\nrelevant information is stored—and task-solving by recalling past events when applicable. This is\nmade possible and effective with our new navigation policy, VPT-Nav.\nOur contributions are as follows. First, we point out the limitations of Steve-1, the most widely\nused instruction-following controller, and show how its bottlenecks can be addressed with MrSteve.\nSecond, we introduce Place Event Memory (PEM), a novel hierarchical memory system that orga-\nnizes spatial and event-based data for efficient querying and storage, even under limited memory\ncapacity. Third, we propose an Exploration Strategy and Task Solving Module built on PEM that\nenables efficient exploration while maintaining high task-solving performance in Minecraft. Last, we\ndemonstrate that our agent significantly outperforms existing baselines in both exploration and long\nsequence of tasks solving. We will release the code for further research.\n2\nRELATED WORKS\nLow-Level Controllers in Minecraft Earlier works (Guss et al., 2019; Lin et al., 2021; Mao\net al., 2022; Cai et al., 2023a; Hafner et al., 2023; Zhou et al., 2024a) introduced policy models\nfor simple tasks in Minecraft. MineCLIP (Fan et al., 2022b) leveraged text-video data to train a\ncontrastive video-language model as a reward model, while VPT (Baker et al., 2022) was pre-trained\non unlabelled videos without text-based instruction input. Steve-1 (Lifshitz et al., 2024) extended\nVPT by incorporating text instructions to generate low-level actions based on human demonstration\ndata. GROOT (Cai et al., 2023b) used reference video instead of text for goal-conditioned behavior\n2\nPreprint\nFigure 1: Sparse Sequential Task Solving Scenario. The first task is to obtain a log. The agent explores to find a\ntree. While searching, the agent observes a cow but continues focusing on acquiring the log. Once the log is\nobtained, the next task is to obtain a water bucket. Remembering that it already explored the forward direction\nwhile searching for the tree, the agent chooses to explore to the right. After gathering the water bucket, the\nfinal task is obtain meat, which can be acquired from the cow. Recalling the cow’s location, the agent navigates\nthere and completes the task by obtaining the meat. Note that each task takes a few thousand steps to achieve.\nThis scenario highlights the significance of episodic memory for efficient exploration and task-solving in an\nopen-ended world where task-relevant resources are sparsely distributed.\ncloning. Recently, MineDreamer (Zhou et al., 2024b) leveraged Steve-1 generating subgoal images\nwith MLLM and Diffusion based on text and current observation for improved control. However,\nthese agents lack episodic memory, forcing agents to start new tasks from scratch. MrSteve addresses\nthis by integrating episodic memory, making it more effective in sequential tasks.\nLLM-Augmented Agents The development of LLMs has significantly advanced agents in Minecraft\n(Wang et al., 2023a;b). These works utilize pre-trained LLMs as zero-shot planners (Brown et al.,\n2020; Touvron et al., 2023), leveraging their powerful reasoning capabilities to generate subgoal plans\nor executable code. Broadly, this line of research can be divided into two approaches: one that uses\nLLMs for code generation to interact with the environment directly (Wang et al., 2023a; Zhu et al.,\n2023; Qin et al., 2024; Liu et al., 2024), and another that generates text-based subgoals which are\nthen executed by a goal-conditioned low-level controller, such as Steve-1 or programmed heuristics\n(Nottingham et al., 2023; Yuan et al., 2023; Li et al., 2024). In the latter approach, to ensure LLMs\nfocus on high-level semantic reasoning, the low-level controller must efficiently execute subgoals.\nWhile combining LLM as a high-level planner with MrSteve is one possible direction, we focus on\nenhancing low-level controller’s capabilities based on the new type of memory in this work.\nMemory in Agents Memory systems in agents primarily aim to retrieve robust and accurate high-\nlevel plans for long-horizon tasks (Zhang et al., 2023; Song et al., 2023; Kagaya et al., 2024; Sun\net al., 2024; Shinn et al., 2024). Existing works store successful task’s text instruction and its plans in\nlanguage often with observations for robust retrieval, which is useful when plans for the new task\nalready exist in memory. Voyager (Wang et al., 2023a) uses an unimodal storage of achieved skill\ncodes in the form of text. GITM (Zhu et al., 2023) integrates text-based knowledge and memory\nfor higher reasoning efficiency and stores entire skill codes after a goal is achieved. Recently, MP5\n(Qin et al., 2024) and JARVIS-1 (Wang et al., 2023b) enhance planning by storing plans and whole\nmultimodal observations in the abstracted memory, allowing for situation-aware retrieval, while\nOptimus-1 (Li et al., 2024) introduces a multimodal experience pool that summarizes all multimodal\ninformation during agent’s execution of the task improving storage and retrieval efficiency. However,\nthese memory systems store the sequence of high-level skills or plans for high-level planners, which\nare not optimized for low-level controllers. We address this problem with Place Event Memory.\n3\nMETHOD\nIn this section, we describe our agent, MrSteve (Memory Recall Steve-1). We begin with the problem\nsetting, followed by step-by-step construction of our agent’s main modules.\nProblem Setting In this work, we define a sparse sequential task scenario where the agent is\ncontinuously given tasks {τn}∞\nn=1 through text instructions (e.g., Obtain water bucket) from the\n3\nPreprint\n(b) Place Event Memory\nE1\nE2\nE1\nE1\n⋯\no12, p12\nT\ntAmjAqSMNQw0g7UgTxgJFWMLpJ\/dYjUZpKcW\/GEfE5GgaUoyMlR5kLzl3J6cwyv5eqexW3AxwkXg5KYMc9V7p9uXOZEGMyQ1h3PjYyfIGUoZmRS7MaRAiP0IB0LBWIE+0n2dYTeGyVPgylsk8YmKl\/OxLEtR7zwFZyZIZ63kvF\/7xObMIrP6Eig0ReDojBk0EqYRwD5VBs2tgRhRe2uEA+RQtjYoGamBHzmhmQkDXlKY\/LmQ1kzbOKV61U7y7Ktes8sAI4BEfgBHjgEtTALaiDBsBAgWfwCt6cF+fd+XA+p6VLTt5zAGbgfP0CRLCgWg=<\/latexit>o30, p30\nk\n6ANGFUkKahpFOpAjiASPtYHyT+u1HojSV4t5MIuJzNBQ0pBgZKz3IflJ1p+cwyv5+qexW3AxwmXg5KYMcjX7pzeQOZEGMyQ1l3PjYyfIGUoZmRa7MWaRAiP0ZB0LRWIE+0n2dZTeGqVAQylsk8YmKl\/OxLEtZ7wFZyZEZ60UvF\/7xubMIrP6Eig0ReDYojBk0EqYRwAFVBs2sQRhRe2uEI+QtjYoOamBHzuhmQsDXlKY\/IWQ1kmrYuKV6vU7qrl+nUeWAEcgxNwBjxwCergFjRAE2CgwDN4BW\/Oi\/PufDifs9IVJ+85AnNwvn4BSAigXA=<\/latexit>o40, p40\nPlace Cluster 1, Event Cluster 1\n⋯\no48, p48\no120, p120\no132, p132\nPlace Cluster 1, Event Cluster 2\n⋯\no287, p287\no295, p295\no300, p300\nPlace Cluster 3, Event Cluster 1\nPlace Cluster\nE\nEvent Cluster\nAgent’s Trajectory\n(a) Agent Overview\nText\nInstruction ⌧n\not\nlt\nEnvironment\nSolver Module\nMemory Module\nPlace Event \nMemory\nExplore\nExecute\nMode \nSelector\nMrSteve\nat\nCount-Based ⇡H-cnt\nVPT-Nav ⇡L-Nav\nVPT-Nav ⇡L-Nav\nSteve-1\nU\nSIJbZGIR7LrY0U5E7QFDjtxpLi0Oe04+v83rngUrFInEPk5i6IR4KFjCQUeWenHzEv7QJ8gvRUKswzq3bNnspaNk5hqhQ0zN\/+oOIJCEVQDhWqufYMbgplsAIp1m5nygaYzLGQ9rTVuCQKjednp5ZJzoZWEk9RNgTdO\/EykOlZqEvu4MYzUYi0P\/6v1Egu3ZSJOAEqyGxRkHALIivnYA2YpAT4RBtMJNO3WmSEJSagac1t8cO5P6TjSNPKMTmLUJZN+6zm1Gv1u\/Nq46oAVkJH6BidIgdoAa6QU3UQgQ9omf0it6MF+Pd+DA+Z60rRjFziOZkfP0CpIyjTQ=<\/latexit>⇡Inst\nFigure 2: MrSteve and Place Event Memory. (a) MrSteve takes agent’s position, first person view, and text\ninstruction, and utilizes Memory Module and Solver Module to follow the instruction. (b) MrSteve leverages\nPlace Event Memory for exploration and task execution, which stores the novel events from visited places.\nenvironment or subgoal plans by LLM. Additionally, we assume that task-relevant resources (e.g.,\nwater, cow) rarely exist and are sparsely distributed in the environment, making it essential to\nmemorize novel events from visited places for future tasks as shown in Figure 1. When an episode\nbegins, for every time step t, the agent is provided with the observation Xt = {it, lt, t}, which consists\nof the pixel observation it ∈RH×W ×C, representing the first person view of the environment, the\npositional information lt = (coordx, coordy, coordz, yaw, pitch) ∈R5, which denotes the agent’s\nrelative 3D position and camera angles with respect to initial position l0, and time t.\nInstruction Following Policy In sparse sequential task, a naive approach is to employ Steve-1\n(Lifshitz et al., 2024), an instruction-following policy πInst(at|ht, τn) that generates low-level controls\n(mouse and keyboard) in Minecraft. Here, ht is a past pixel observation sequence xt−128:t. While past\nobservations are processed by Transformer-XL layers in Steve-1, the model is ineffective at recalling\nobservations from a few thousand steps ago (Lampinen et al., 2021). Additionally, the Transformer’s\nquadratic complexity makes it significantly inefficient to process thousands of observations. This\nmakes Steve-1 poorly suited for sparse sequential task, as it cannot recall visited places or task-\nrelevant resources seen in the past. To address this, we propose MrSteve which stores novel events\nfrom visited places for efficient sparse sequential task-solving.\nMrSteve is a memory-augmented instruction following policy that consists of Memory Mod-\nule and Solver Module, as shown in Figure 2(a).\nIn Memory Module, we use the mem-\nory called Place Event Memory Mt that stores novel events from visited places (Figure 2(b)).\nAlgorithm 1 MrSteve Single Loop\nRequire: Memory Mt, and task τn\n1: candidates ←Read(Mt, τn)\n2: if candidates ̸= ∅then\n3:\nXt, lt = OneOf(candidates)\n4:\nNavigate to lt with πL-Nav\n5:\nExecute τn with πInst\n6: else\n7:\nExplore with πH-Cnt, πL-Nav\n8: end if\nBased on Place Event Memory, Mode Selector in Solver\nModule decides between Explore mode and Execute mode.\nWhen no task-relevant resource exists in the memory, Ex-\nplore mode is selected, and the agent explores with our\nhierarchical exploration method. If a task-relevant resource\nexists in the memory, Execute mode is selected, then the\nagent navigates to the resource’s position and executes πInst\n(i.e., Steve-1) to solve the task. Algorithm 1 outlines the\ntask-solving loop of MrSteve, which repeats every fixed\nstep or when a new task is given (More details in Appendix\nD). With these modules, MrSteve can efficiently explore\nand recall task-relevant resources from the memory to solve\nsparse sequential task in Figure 1. In the following sections, we describe how each module in MrSteve\nis constructed. We begin by constructing Memory Module.\n3.1\nMEMORY MODULE: CONSTRUCTION OF PLACE EVENT MEMORY\nThe simplest form of memory is FIFO Memory, denoted as Mt with capacity N. At every time step,\ninstead of storing Xt in Mt, we can extract a semantic representation from the video it−H:t with a\nvideo encoder to store an experience frame xt = {et, lt, t}, where et = Encv(it−H:t). For simplicity,\nwe term et as the video embedding at time step t. When the memory exceeds its capacity, the oldest\nframe is removed. For memory read, we calculate the cosine similarity between the task embedding\nˆτn = Enct(τn) and the video embedding et in Mt to retrieve task-relevant frames. Here, we use\n4\nPreprint\nFigure 3: Mode Selector and VPT-Nav in MrSteve. (a) Mode Selector with Place Episodic Memory. It decides\nagent’s mode (Explore or Execute) based on whether a task-relevant resource is in the memory. It uses a\nhierarchical read operation. (b) Architecture of Goal-Conditioned VPT Navigator.\nvideo encoder Encv, text encoder Enct, and H = 16 from MineCLIP (Fan et al., 2022a), which is a\nCLIP (Radford et al., 2021) trained on web videos of Minecraft gameplay and associated captions.\nWhile FIFO Memory offers benefits from its simple memory operations, it has two drawbacks. First,\nthe computational complexity of the read operation grows linearly with the memory size. Second, the\nbias toward removing the oldest frames can be problematic in sparse sequential task as in the Figure\n1 scenario, where task-relevant frames from visited places are lost.\nPlace Memory To address these issues, Place Memory (Cho et al., 2024) divides the agent’s positions\n{ℓn}t\nn=0 in the trajectory into clusters of distinct places, where each place cluster is assigned a FIFO\nMemory. Here, we term ℓt = (coordx, coordy, yaw) as agent’s position, which is a concatenation of\nagent’s top-down location and its head direction. Place Memory is represented as Mt = {Mk}K\nk=1,\nwhere Mk is the k-th place cluster with center position ℓMk, and center embedding eMk. Here, eMk\nis the video embedding whose position is closest to ℓMk. This structure improves the efficiency of\nthe read operation by extracting top-k place clusters with their center embeddings first, then fetching\nrelevant frames from these clusters. Furthermore, when memory capacity is limited, the oldest frame\nis removed from the largest place cluster, allowing the agent to retain memories in diverse places.\nWhile Place Memory prioritizes storing experience frames across diverse places, its FIFO structure\nwithin each cluster still loses novel experience frames in the past. For instance, if an agent stays in\na place where zombies burn and disappear for a long time, the place cluster removes the frames of\nburning zombies that can be crucial in upcoming tasks. This highlights the importance of focusing on\nvisually distinct experience frames rather than storing them sequentially, which can be redundant.\nPlace Event Memory To resolve this issue, we introduce Place Event Memory built on Place Memory,\nwhich captures distinct events that occur within each place cluster (Figure 2(b)). While Place Memory\nuses agent’s position to cluster experience frames, there is no criterion for clustering frames to form\nevents. To tackle this, we use the cosine similarity of video embeddings from MineCLIP for criterion.\nSpecifically, each place cluster Mk is subdivided into event clusters, denoted as {Ek\ni }dk\ni=1, where\neach Ek\ni represents the i-th event cluster in k-th place cluster, characterized by a center embedding\nek\nEi. These event clusters are newly created and updated as the place cluster accumulates a certain\nnumber of additional experience frames. For generating event clusters, DP Means algorithm (Dinari\n& Freifeld, 2022) is applied on video embeddings of these frames, generated by MineCLIP, and the\nresulting cluster centers become a center embedding of each cluster. If the cosine similarity of the\ncluster centers between a newly created cluster and an existing cluster is higher than threshold c (we\ndefine that two event clusters are indistinct), the two clusters are merged to prevent redundancy and\nensure distinct event clusters within each place cluster. When memory capacity is exceeded, the\noldest frame in the largest event cluster is removed, thus the memory can retain diverse places and\ndistinct events within each place. More details on Place Event Memory can be found in Appendix E.\n3.2\nSOLVER MODULE: MODE SELECTOR, EXPLORATION, AND NAVIGATION\nIn this section, we introduce the remaining components in Solver Module, which are Mode Selector,\nand hierarchical policies πH-Cnt, πL-Nav for episodic exploration, and goal-reaching navigation.\n5\nPreprint\nMode Selector Mode Selector decides between Explore and Execute mode by checking whether\ntask-relevant resource exists in the memory. If the resource exists, the agent chooses Execute mode,\nor Explore mode otherwise. When Place Event Memory is employed, Mode Selector first picks top-k\nevent clusters with task alignment score sik(τn) = CLIPt(τn)·CLIPv(ek\nEi) between task embedding,\nand center embedding of event cluster. Then, it calculates task alignment scores on experience frames\nin top-k event clusters and gathers frames with scores higher than task threshold h as shown in Figure\n3(a). We note that leveraging Place Event Memory offers computational efficiency with hierarchical\nread operation compared to FIFO Memory, which calculates the alignment scores on whole frames in\nthe memory. We provide a comparison of memory query time in Appendix K for further insights.\nHierarchical Episodic Exploration We propose a memory-based hierarchical exploration method\nthat allows the agent to efficiently explore the environment while minimizing revisits to previously ex-\nplored positions. This is achieved through a high-level goal selector πH-Nav(gt|ℓ′\nt, Mt) and a low-level\ngoal achiever πL-Nav(at|ht, gt), where ht = it−128:t, and ℓ′\nt, and gt are the agent’s current location,\nand goal location in (coordx, coordy), respectively. We introduce a Count-Based exploration strategy\n(Yamauchi, 1998; Tang et al., 2017; Chang et al., 2023) for the high-level goal selector.\nSpecifically, L × L visitation grid map mt is used with the agent’s starting location set as the center\nof the map. The locations of the agent’s trajectory are discretized and marked on the grid. The goal\nselector then divides the visitation map into grid cells of size G × G, and selects the location of\nthe grid cell with the lowest visitation count as the goal, gt. If multiple grid cells have the same\nminimum count, the cell closest to the current location ℓ′\nt is chosen. This approach directs the agent\ntoward unexplored locations, while minimizing unnecessary revisits. The size of grid cell can be\ndynamically adjusted to balance between broader exploration and finer local searches. Additionally,\nin an infinitely large map, the visitation map can be easily expanded by adding new grids, and further\nhierarchies on visitation maps can be introduced for efficiently managing explored locations.\nGoal-Conditioned VPT Navigator Once the goal location is selected by the high-level goal selector,\nit is crucial for the agent to navigate to the goal accurately. However, navigating complex terrains\n(e.g., river, mountain) requires human prior knowledge, where pure RL policy trained from scratch\nin prior work (Yuan et al., 2023) often shows suboptimal navigation ability. To address this, we use\nthe VPT as our starting policy, and fine-tune it for goal-conditioned navigation policy. We name this\npolicy as VPT-Nav. In VPT-Nav, we add goal embedding Gψ(lt, gt) in the output of TrXLθ in VPT\nwith LoRA adaptor (Hu et al., 2021a) as in Figure 3(b). We used PPO (Schulman et al., 2017) for\nfine-tuning goal encoder Gψ, LoRA parameters, policy πψ, and value vψ with reward based on the\ndistance to the goal location. We note that our VPT-Nav introduces several differences from prior\nVPT fine-tuning methods, thoroughly investigated in Appendix L.\n4\nEXPERIMENTS\nThis section presents a step-by-step validation of our agent MrSteve across various environments\nand conditions. We begin by evaluating the exploration and navigation ability of MrSteve, which\nis crucial in sparse sequential tasks (Section 4.1). Then, we demonstrate MrSteve’s capability to\nsolve A-B-A task sequentially where the memory is necessary to solve the task A twice (Section 4.2).\nAdditionally, we show that the proposed Place Event Memory outperforms other memory variants,\nparticularly when memory capacity is limited (Section 4.3). Lastly, we showcase the generalization\nof MrSteve to long-horizon sparse sequential task (Section 4.4). Each baseline and task is explained\nin each of the experiment sections with more details in Appendix C.\n4.1\nEXPLORATION & NAVIGATION FOR SPARSE SEQUENTIAL TASK\nIn this section, we evaluate the exploration and navigation ability of our agent. To verify this, we\nplaced an agent in a 100 × 100 block map with complex terrains such as mountains and a river, and\ngave 6K steps to wander around the map. Since successful exploration in Minecraft involves covering\nas much of the map as possible while minimizing revisits to previously visited locations, we measure\ntwo metrics: Map Coverage and Revisit Count. Map Coverage is calculated by dividing the map into\n11 × 11 grid cells and measuring the percentage of cells covered by the agent’s trajectory. Revisit\nCount measures the average number of times the agent visits the same grid cell.\n6\nPreprint\nTable 1: Map Coverage and Revisit Count of different exploration policies. Our exploration method (High-Level:\nCount-Based, Low-Level: VPT-Nav) performs the best.\nHigh-Level\nCount-based\nRNN-based\nSteve-1\nLow-Level\nVPT-Nav (Ours)\nDQN\nVPT-Nav\nDQN (Plan4MC)\nMap Coverage (↑)\n84.42 ± 0.06\n31.83 ± 0.11\n29.82 ± 0.07\n16.36 ± 0.04\n50.77 ± 0.13\nRevisit Count (↓)\n0.38 ± 0.6\n4.47 ± 1.43\n4.72 ± 1.73\n6.2 ± 1.73\n2.68 ± 1.36\nFigure 4: Agent’s trajectories of length 6K steps on 100 × 100 block map with different exploration methods.\nThe leftmost figure is the agent’s trajectory from our exploration method.\nTo demonstrate that our proposed hierarchical episodic exploration with Count-Based high-level goal\nselector and low-level goal achiever VPT-Nav is more effective for exploration in the given map, we\ncompared our method with the following baselines: Steve-1 (Lifshitz et al., 2024), and exploration\nmethod from Plan4MC (Yuan et al., 2023). For Steve-1, we provided the “Go Explore” instruction to\nassess its exploratory behavior as in prior works (Cai et al., 2023b; Zhou et al., 2024b). Plan4MC, on\nthe other hand, employs a hierarchical approach where the high-level RNN policy selects the next\ngoal location based on past locations, and a low-level DQN policy is used for goal-reaching. Since the\ninput-output space of our method and Plan4MC is identical, interchanging high-level and low-level\npolicies between two approaches is allowed so that we can evaluate the benefits of each component.\nAs shown in Table 1, and Figure 4, we can see that our exploration method outperforms other baselines.\nWhile Steve-1 showed decent performance in Map Coverage, it repeatedly visits previously explored\nplaces because of lack of memory. In the case of hierarchical exploration, high-level RNN policy\nstruggled with memorizing visited places as the trajectory gets longer, resulting in high Revisit\nCounts. Additionally, the low-level DQN policy had difficulty navigating complex terrain, such as\nmountains and rivers, showing low Map Coverage. On the other hand, the Count-Based goal selector\nthat directs the agent to the least-visited locations as goals and the VPT-Nav that effectively reaches\nthose goals resulted in strong exploratory behavior. Furthermore, to show the robustness of VPT-Nav,\nwe report its navigation capability in diverse terrains in Appendix L.\n4.2\nSEQUENTIAL TASK SOLVING WITH MEMORY IN SPARSE CONDITION\nIn this section, we demonstrate our agent’s capability to solve sparse sequential tasks based on\nexploration methods studied in Section 4.1. To evaluate this, we introduce ABA-Sparse task, which\nconsists of A-B-A tasks given sequentially with text instructions. Task A involves gathering a sparse\nresource, which can be either a water bucket\n, beef\n, wool\n, or milk\n. Task B requires\ncollecting a dense resource, chosen from log\n, dirt\n, leaves\n, seeds\n, or sand\n, making\ntotal 20 tasks. The agent spawns in a 100 × 100 block map, and the A resource exists in a single\nlocation, while the B resource can be found in multiple locations. The agent is given 12K steps with\nunlimited memory capacity to complete all tasks. Since finding the sparse resource A is challenging,\nthe task requires an efficient exploration algorithm. Moreover, after solving tasks A and B, memory\nbecomes crucial to return to the location of resource A within the time limit. We measure success\nrates and task duration for evaluation.\nTo verify the benefits of efficient exploration and the memory, we compared the following agents:\nSteve-1, MrSteve with exploration method from Plan4MC and FIFO Memory (PMC-MrSteve-FM),\nand our agent MrSteve with Count-Based goal selector and VPT-Nav for exploration and Place\nEvent Memory. We also test MrSteve with different memory variants, MrSteve-FM, MrSteve-EM,\n7\nPreprint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWater-Seed-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\nTask A\nTask A′\n1.3\n3.3\n5.3\nTask Duration (×103)\nTask A\nTask A′\n2.2\n3.85\n5.5\nTask A\nTask A′\n2.4\n5.4\n8.4\nTask A\nTask A′\n2.0\n5.35\n7.7\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 5: Success Rate and Task Duration of different agents in ABA-Sparse tasks. Task A refers to the first A\ntask in the A-B-A task sequence, while Task A′ refers to the final A task in the A-B-A task sequence. We note that\nMrSteve, as well as its memory variants, outperforms Steve-1, which lacks the memory. Additionally, while\nSteve-1 takes a similar amount of time to solve both task A and task A′, MrSteve solves task A′ much faster. The\nfull results on all 20 tasks are in Appendix H, and investigations about memory variants are in Appendix O.\nMrSteve-PM, which use FIFO Memory, Event Memory, and Place Memory, respectively. Here,\nEvent Memory is a Place Event Memory without place clusters, which stores the frames based on\nvisual similarity. We explain the details of Event Memory in Appendix E.3.\nAs shown in Figure 5, it is clear that MrSteve outperforms Steve-1. This is because MrSteve can find\ntask-relevant resources faster with efficient exploration and store the location of the task A resource,\nallowing it to revisit the location and solve task A again within a limited time. This is evident from the\ntask duration in Figure 5, where MrSteve shows a shorter task duration than Steve-1 on the first A task.\nWhen solving the second A task, MrSteve exhibits a much shorter task duration compared to the first\nA task, while Steve-1 takes a similar or even greater number of steps. While other memory-augmented\nbaselines showed similar performance to MrSteve, PMC-MrSteve-FM performed worse due to a\nsuboptimal exploration method, making it difficult to find the sparse resource. We report the full\nresults on all 20 tasks in Appendix H.\n4.3\nMEMORY-CONSTRAINED TASK SOLVING WITH MEMORY\nWe demonstrated in Section 4.2 that memory is essential in solving sparse sequential tasks when there\nis no limitation in memory capacity. However, in real-world scenarios where memory capacity is\nlimited, memorizing visited places and novel events becomes important. In this section, we show that\nPlace Event Memory can benefit in this scenario. To verify this, we introduce three Memory Tasks,\nwhich are Find Water, Find Zombies’ Death Spot, and Find First-Visited House. In all tasks, the\nagent begins with an exploration phase, followed by a task phase. In the exploration phase, the agent\nfollows a fixed exploratory behavior. In the subsequent task phase, the agent is given a MineCLIP\nembedding et = Encv({ot}16\nn=1) as a task embedding instead of text instruction, where ot is the pixel\nobservation seen in the exploration phase. Then, this becomes an image goal navigation task where\nan agent should navigate to the location of the given image.\nIn all tasks, the exploration phase is 3K steps, and the agent’s memory capacity is limited to 2K. In\nFind Water task, the agent stays near water for 0.5K steps, then travels to a random location. In the\ntask phase, the agent should return to the water (Figure 6(a)). This task evaluates whether an agent\ncan memorize water frames in the past. In Find Zombies’ Death Spot, the agent sees burning zombies\nfor 1K steps (zombies burn for 0.5K steps then disappear), then travels. The task is to return to where\nzombies burned (Figure 6(b)). This task evaluates whether an agent can memorize distinct events\n(zombies burn and disappear) in the same place. In Find First-Visited House task, the agent sees the\nfirst house for 0.1K steps, then goes to the second house and stays until 2K step, then travels. The\ntask is to return to the first house (Figure 6(c)). This task evaluates whether an agent can memorize\ntwo visually similar houses in two different places.\n8\nPreprint\nstay\ntravel\n(a) Find Water\ntravel\ntravel\nstay for a long time\nbefore travel\n(c) Find First-Visited House\ntravel\nstay for a long time\nbefore travel\nstay\n(t = 0)\nSee Water\n(t = 0)\n1st House\nj\nHBC9k1Br2YEL14xMRFEtiQ2WGACbMzm5leI9nwDXrUH\/FmvPoL\/ocf4AB7ELCSTipV3enuCmPBDbjut5NbWV1b38hvFra2d3b3ivsHDaMSTZlPlVC6GRLDBJfMBw6CNWPNSBQK9hAObyb+wyPThit5D6OYBRHpS97jl\nICV\/DJcuaedYsmtuFPgZeJlpIQy1DvFn3ZX0SRiEqgxrQ8N4YgJRo4FWxcaCeGxYQOSZ+1LJUkYiZIp8eO8YlVurintC0JeKr+nUhJZMwoCm1nRGBgFr2J+J\/XSqB3GaRcxgkwSWeLeonAoPDkc9zlmlEQI0sI1dze\niumAaELB5jO3JYzmfkiHCtjT2MbkLYayTBpnFa9aqd6dl2rXWB5dISOURl56ALV0C2qIx9RxNEzekVvzovz7nw4n7PWnJPNHKI5OF+\/Nyckw=<\/latexit>(t = 0)\nZombies Burn\n(t = 0.5K)\nSee Water\n2nd House (t = 1K)\nZombies Gone\nW\nldW19ZzG\/nNre2d3cLefkPLWFWp1JI1fKIZoKHrA4cBGtFipHAE6zpDa\/H9eYjU5rL8B5GEXMD0g+5zykBE7kluHQ6wJ4guU1PuoWiXbYnwovGyUwRZap1Cz+dnqRxwEKgmjduwI3IQo4FSwN+JNYsIHZI+axsbkoBpN5kcneJjk\/SwL5V5IeBJ+nciIYHWo8AznQGBgZ6vjcP\/au0Y\/As34WEUAwvpdJEfCwSjwngHleMghgZQ6ji5lZMB0QRCobTzBYvmPlDMpQGVWowOfNQFk3jtOxUypW7s2L1KgOWQ4foCJWQg85RFd2gGqojih7\nQM3pFb9aL9W59WJ\/T1iUrmzlAM7K+fgEN0qBI<\/latexit>(t = 1K)\n(t = 3K)\nAfter Travel\n(t = 3K)\nAfter Travel\n(t = 3K)\nAfter Travel\n(b) Find Zombies’ Death Spot\nMrSteve\nMrSteve-PM\nMrSteve-EM\nMrSteve-FM\nSteve-1\nFigure 6: The overview of Memory Tasks, and Success Rate for each Memory Task from different agents.\nMemory Tasks are basically navigation tasks reaching the location of the previously seen experience frame. We\nobserve that MrSteve which uses Place Event Memory shows high success rates in all tasks.\nTo evaluate how each memory type performs in the Memory Tasks, we tested Steve-1, MrSteve,\nMrSteve-FM, MrSteve-EM, and MrSteve-PM. In Figure 6, the performance of each memory type\nis illustrated. In all tasks, Steve-1 which lacks memory, showed worst performance since it has to\nfind the targets from scratch. In Find Water task, MrSteve-FM does not have the water frames in\nmemory in task phase, so it should explore to find the water showing about 60% success rate. In\ncontrast, other agents store the water frames by allocating place cluster or event cluster in water\nlocation, allowing the agent to recall the frames and easily return to water, showing high success\nrate. In Find Zombies’ Death Spot task, MrSteve-PM loses burning zombies’ frames since Place\nMemory removes the frames in the largest place cluster, which is zombies’ place, showing about 60%\nsuccess rate. However, MrSteve-EM and MrSteve store the burning zombies’ frames as a novel event,\nallowing the agent to easily return to zombies’ spot, showing high success rate. In Find First-Visited\nHouse task, MrSteve-EM loses frames of first house, since it clusters two visually similar houses as\nthe same event, showing about 50% success rate. However, MrSteve-PM and MrSteve store the two\nhouses in different place clusters, enabling the agent to return to the first house, showing high success\nrate. These results suggest that Place Event Memory demonstrates its strength in memory-limited\nsettings, where memorizing both visited places and novel events is crucial for task completion.\n4.4\nLONG-HORIZON SPARSE SEQUENTIAL TASK SOLVING WITH MEMORY\nIn this section, to see how MrSteve generalizes to long-horizon tasks, we introduce two sparse\nsequential tasks. For both tasks, the agent plays in a 200 × 200 block map for 500K steps (About 7\nhours of gameplay) with 20K steps of memory capacity. The first is Long-Instruction task, where the\nagent is continuously given random tasks from Obtain X. Here X can be water\n, beef\n, wool\n, log\n, dirt\nor seeds\n. If the agent fails to complete the task within 20K steps, the task is\nchanged. This task requires efficient exploration in a large map, and managing memory to memorize\nplaces with task-relevant resources to continuously solve the given tasks.\nThe second task is Long-Navigation task similar to Memory Tasks in Section 4.3. It has an exploration\nphase of 16K steps and a task phase. In the exploration phase, the agent observes six events in different\nplaces: 1) burning zombies, 2) river, 3) sugarcane blow up, 4) spider spawn, 5) tree, and 6) house,\nspending 2K steps at each place. In the task phase, the image goal is continuously given randomly\nselected from the frames in the early steps of the event. For instance, if the task is to reach sugarcane\nplace, the image of sugarcane place before blow up is set as an image goal. This task requires\nmanaging memory to retain distinct events in different places.\n9\nPreprint\n0\n100K 200K 300K 400K 500K\nTime Steps\n0\n20\n40\n60\n80\n100\nSucceeded Tasks\nLong-Instruction Task\n0\n100K 200K 300K 400K 500K\nTime Steps\n0\n20\n40\n60\n80\nLong-Navigation Task\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 7: The performance in Long-Intruction task and Long-\nNavigation task. MrSteve performs well in both tasks.\nThe results are shown in Figure 7.\nFor Long-Instruction task, we ob-\nserve that MrSteve, and MrSteve-PM\nsolved over 80 tasks, showing their\ncapabilities of retaining task-relevant\nresources in different places effec-\ntively. MrSteve-EM solved around 50\ntasks, suggesting event-based memory\nis less effective than place-based mem-\nory. This is because similar events in\ndifferent places, like cows and sheeps\nliving in visually similar forests, are\nin the same event cluster, possibly los-\ning task-relevant frames. For the remaining baselines, they either have suboptimal exploratory\nbehaviors or keep losing the task-revelant frames, solving less than 50 tasks. For Long-Navigation\ntask, MrSteve, and MrSteve-EM solved around 70 tasks, showing the ability to retain novel events\noccured in different places. In the case of MrSteve-PM, it removes the frames in early time steps\nof each place cluster, losing novel events (e.g., sugarcane before blow up), thus solving less than\n20 tasks. For the remaining baselines, they lose or cannot retain frames in the early stage of an\nepisode solving less than 10 tasks. These results suggest that MrSteve demonstrates its strength in\nlong-horizon tasks.\n5\nLIMITATIONS\nThis work focuses on improving Steve-1 through the introduction of What-Where-When episodic\nmemory, significantly enhancing the agent’s ability to retain and recall past events for more efficient\ntask-solving. Our experiments demonstrate that MrSteve exhibits significantly enhanced performance\nwhen integrated with LLM-augmented agents for high-level planning. However, current limitations\nprevent the high-level planner from accessing PEM in the low-level controller. Future work could\nexplore enabling PEM access for high-level planners, which could generate more accurate plans by\nleveraging the agent’s episodic memories, further enhancing the system’s capabilities for complex,\nlong-horizon tasks.\nWe list up a few more limitations. First, our experiments are limited to surface-level exploration in\nthe Minecraft environment, omitting underground navigation, which is a crucial aspect of the game\nas mining plays a central role. However, our hierarchical exploration methods based on visitation\nmap could easily be extended to include vertical dimensions. Additionally, while our VPT-Nav\ndemonstrated strong navigation abilities in plains biomes, more challenging terrains, such as crossing\ncliffs that require skills like building bridges, were not addressed. Lastly, we used exact position data\nin Minecraft, which may limit the model’s adaptability to robotics tasks where positional information\nis often noisy. One possible direction is adapting MrSteve in environments with noisy positions.\n6\nCONCLUSION\nIn this paper, we introduced MrSteve (Memory Recall Steve-1), a novel low-level controller designed\nto address the limitations of current LLM-augmented hierarchical approaches in general-purpose\nembodied AI environments like Minecraft. We argued that the primary cause of failures in many\nlow-level controllers is the absence of an episodic memory system. To overcome this, we equipped\nMrSteve with Place Event Memory (PEM), a form of episodic memory that captures and organizes\nwhat, where, and when information from episodes. This allows for efficient recall and navigation in\nlong-horizon tasks, directly addressing the limitations of existing low-level controllers like Steve-1,\nwhich rely heavily on short-term memory. Additionally, we proposed an Exploration Strategy and a\nMemory-Augmented Task Solving Framework, enabling agents to effectively switch between explo-\nration and task-solving based on recalled events. Our results demonstrate significant improvements in\nboth task-solving and exploration efficiency compared to existing methods. We believe that MrSteve\nopens new avenues for improving low-level controllers in hierarchical planning and are releasing our\ncode to facilitate further research in this field.\n10\nPreprint\nETHICS STATEMENT\nWe acknowledge potential societal concerns related to our work. While our agent, in its current form,\nis designed for use in virtual environments such as Minecraft, the techniques and advancements made\nhere could be extended to broader autonomous systems. There is a possibility that, if adapted for real-\nworld applications, such systems might be misused for unethical purposes, including unauthorized\nsurveillance or actions that infringe on individual privacy. We encourage responsible usage and\nfurther research into safeguards to prevent such outcomes.\nREPRODUCIBILITY STATEMENT\nTo facilitate the reproducibility of our work, we have provided the pseudo codes, model architecture,\nand hyperparameters in Appendix D, E, and F. We will also release the source code for our models\nand experiments.\nACKNOWLEDGEMENT\nThis work was supported by GRDC (Global Research Development Center) Cooperative Hub Program\n(RS-2024-00436165) and Brain Pool Plus Program (No. 2021H1D3A2A03103645) through the\nNational Research Foundation (NRF) funded by the Ministry of Science and ICT (MSIT). The authors\nwould like to thank the members of Machine Learning and Mind Lab (MLML) for helpful comments.\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in neural information processing systems, 35:23716–23736,\n2022.\nJosh Albrecht, Abraham J. Fetterman, Bryden Fogelman, Ellie Kitanidis, Bartosz Wróblewski, Nicole\nSeo, Michael Rosenthal, Maksis Knutins, Zachary Polizzi, James B. Simon, and Kanjun Qiu.\nAvalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds. In NeurIPS\nDatasets and Benchmarks Track, 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Advances in Neural Information Processing Systems, pp. 24639–24654,\n2022.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734–13744, 2023a.\nShaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot: Learning to\nfollow instructions by watching gameplay videos. arXiv preprint arXiv:2310.08235, 2023b.\nMatthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min,\nKavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, and\nDevendra Singh Chaplot. Goat: Go to any thing, 2023. URL https:\/\/arxiv.org\/abs\/\n2311.06430.\nJunmo Cho, Jaesik Yoon, and Sungjin Ahn. Spatially-aware transformers for embodied agents.\nIn The Twelfth International Conference on Learning Representations, 2024. URL https:\n\/\/openreview.net\/forum?id=Ts95eXsPBc.\n11\nPreprint\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a fixed-length context. In Annual Meeting of the\nAssociation for Computational Linguistics, 2019. URL https:\/\/api.semanticscholar.\norg\/CorpusID:57759363.\nOr Dinari and Oren Freifeld. Revisiting DP-means: Fast scalable algorithms via parallelism and\ndelayed cluster creation. In The 38th Conference on Uncertainty in Artificial Intelligence, 2022.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022a. URL https:\/\/openreview.net\/forum?\nid=rc8o_j8I8PX.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022b. URL https:\/\/openreview.net\/forum?\nid=rc8o_j8I8PX.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso,\nand Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv\npreprint arXiv:1907.13440, 2019.\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on\nLearning Representations.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019.\nURL https:\/\/arxiv.org\/abs\/1902.00751.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021a. URL https:\n\/\/arxiv.org\/abs\/2106.09685.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021b.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207,\n2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models, 2022b. URL https:\/\/arxiv.org\/abs\/2207.\n05608.\nMatthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial\nintelligence experimentation. In Ijcai, volume 16, pp. 4246–4247, 2016.\nTomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose,\nKoki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual\nmemory for multimodal llm agents, 2024.\nAndrew Kyle Lampinen, Stephanie C Y Chan, Andrea Banino, and Felix Hill. Towards mental time\ntravel: a hierarchical memory for reinforcement learning agents. 2021.\n12\nPreprint\nZaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus-\n1: Hybrid multimodal memory empowered agents excel in long-horizon tasks, 2024. URL\nhttps:\/\/arxiv.org\/abs\/2408.03615.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. Advances in Neural Information Processing Systems, 36,\n2024.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang.\nJuewu-mc:\nPlaying minecraft with sample-efficient hierarchical reinforcement learning.\narXiv preprint\narXiv:2112.04907, 2021.\nShunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya\nZheng, and Mingli Song. Odyssey: Empowering agents with open-world skills. arXiv preprint\narXiv:2407.15325, 2024.\nHangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye Hao, Dong\nLi, and Pingzhong Tang. Seihai: A sample-efficient hierarchical ai for the minerl competition. In\nDistributed Artificial Intelligence: Third International Conference, DAI 2021, Shanghai, China,\nDecember 17–18, 2021, Proceedings 3, pp. 38–51. Springer, 2022.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-\ntersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis.\nHuman-level control through deep rein-\nforcement learning.\nNature, 518(7540):529–533, 2015.\ndoi: 10.1038\/nature14236.\nURL\nhttps:\/\/doi.org\/10.1038\/nature14236.\nKolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer\nSingh, and Roy Fox.\nDo embodied agents dream of pixelated sheep?: Embodied decision\nmaking using language guided world modelling. arXiv preprint arXiv:2301.12050, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2301.12050.\nOpenAI. Gpt-4 technical report, 2024. URL https:\/\/arxiv.org\/abs\/2303.08774.\nYiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing\nShao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception.\nIn Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp.\n16307–16316, 2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision, 2021. URL https:\n\/\/arxiv.org\/abs\/2103.00020.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems, 36, 2024.\nChan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su.\nLlm-planner: Few-shot grounded planning for embodied agents with large language models. In\nProceedings of the IEEE\/CVF International Conference on Computer Vision (ICCV), October\n2023.\nZhiyuan Sun, Haochen Shi, Marc-Alexandre Côté, Glen Berseth, Xingdi Yuan, and Bang Liu.\nEnhancing agent learning through world dynamics modeling. arXiv preprint arXiv:2407.17695,\n2024.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep\nreinforcement learning, 2017. URL https:\/\/arxiv.org\/abs\/1611.04717.\n13\nPreprint\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https:\/\/qwenlm.\ngithub.io\/blog\/qwen2.5\/.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels, 2023. URL https:\/\/arxiv.org\/abs\/2302.13971.\nLaurens van der Maaten and Geoffrey Hinton.\nVisualizing data using t-sne.\nJournal of Ma-\nchine Learning Research, 9(86):2579–2605, 2008. URL http:\/\/jmlr.org\/papers\/v9\/\nvandermaaten08a.html.\nKonstantinos Voudouris, Ibrahim Alhas, Wout Schellaert, Matthew Crosby, Joel Holmes, John\nBurden, Niharika Chaubey, Niall Donnelly, Matishalin Patel, Marta Halina, José Hernández-\nOrallo, and Lucy G. Cheke. Animal-ai 3: What’s new & why you should care, 2023. URL\nhttps:\/\/arxiv.org\/abs\/2312.11414.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv\npreprint arXiv: Arxiv-2305.16291, 2023a.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\nHe, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. Jarvis-1: Open-world multi-task\nagents with memory-augmented multimodal language models. arXiv preprint arXiv: 2311.05997,\n2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task agents. arXiv\npreprint arXiv:2302.01560, 2023c.\nBrian Yamauchi. Frontier-based exploration using multiple robots. In Proceedings of the Second\nInternational Conference on Autonomous Agents, AGENTS ’98, pp. 47–53, New York, NY, USA,\n1998. Association for Computing Machinery. ISBN 0897919831. doi: 10.1145\/280765.280773.\nURL https:\/\/doi.org\/10.1145\/280765.280773.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4MC: Skill reinforcement learning and planning for open-world Minecraft tasks. arXiv\npreprint arXiv:2303.16563, 2023.\nHaoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu. Pre-training goal-based models for\nsample-efficient reinforcement learning. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https:\/\/openreview.net\/forum?id=o2IEmeLL9r.\nJesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, and\nJoseph J Lim. Bootstrap your own skills: Learning to solve new tasks with large language model\nguidance. In 7th Annual Conference on Robot Learning, 2023. URL https:\/\/openreview.\nnet\/forum?id=a0mFRgadGO.\nBohan Zhou, Ke Li, Jiechuan Jiang, and Zongqing Lu. Learning from visual observation via offline\npretrained state-to-go transformer. Advances in Neural Information Processing Systems, 36, 2024a.\nEnshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing\nShao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world\ncontrol. arXiv preprint arXiv:2403.12037, 2024b.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:\nGenerally capable agents for open-world environments via large language models with text-based\nknowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\n14\nPreprint\nA\nLIMITATIONS OF MEMORY SYSTEM IN LLM-AUGMENTED AGENTS\nMemory systems in agents primarily aim to retrieve robust and accurate high-level plans for long-\nhorizon tasks (Wang et al., 2023a;b; Zhu et al., 2023; Qin et al., 2024; Li et al., 2024). Existing works\nuse an abstracted memory that stores the succeeded task with its plans, and often with history of\nobservations for reliable retrieval, which is helpful when the similar plan in other tasks already exist\nin memory. However, these types of memory systems are not well-suited for low-level controllers for\nthe following reasons:\n• Issues with Managing Memory Recent memory systems in Minecraft, when saving successful\nplans or skills, the history of observations for solving the task is all stored in FIFO manner (Wang\net al., 2023b) or only task-relevant frames are stored in the plan (Li et al., 2024). However,\ncomputation complexity for retrieving the experience frames that are relevant to a new task is\ncomputationally expensive or even impossible (because the latter may not store the experience\nframes despite the agent observed it).\n• Lack of Mechanism for Retrieving Experience frames Current memory systems store the text\ninstructions of succeeded tasks as keys and their plans to complete those tasks as values. The\nretrieval process begins by matching a task query to the task keys in memory and then filtering\nfurther using the current scene’s similarity to the stored frames before retrieving the final plan.\nThese memory systems are targetted to retrieve the succeeded plan, but they lack a mechanism for\nutilizing the experience frames in the memory, which could be crucial for future tasks.\nConsider the following example: Suppose the agent is tasked with collecting wood. While searching\nfor a tree, let’s assume the agent came across a cow in the forest. Once the wood is collected, the\nmemory will store the successful plan and its corresponding observations. However, if the agent\nis later tasked with finding the cow, there would be no memory key related to the cow, making it\nimpossible to retrieve the relevant frames. While some heuristics to calculate the similarity of the\ntask embedding for “find cow” and the visual representations in memory using MineCLIP is possible,\nit is computationally expensive since the similarity should be calculated for all stored frames.\nThus, we need a new type of memory system for the low-level controller to efficiently store novel\nevents (such as encountering the cow) as they explore the environment, even when such events are\nnot directly relevant to the current task. Also, the memory should hierarchically organize these novel\nevents so that they can be efficiently retrieved later. In this paper, we propose a memory system called\nPlace Event Memory (PEM), which organizes experiences by both location and event. PEM allows\nthe agent to store diverse novel events across various locations, making future retrieval more efficient.\nWe argue that PEM, when combined with current memory systems that store successful plans, will\nenable more effective retrieval of task-relevant information.\nB\nCOMPUTATION OVERHEAD\nOur study was performed on an Intel server equipped with 8 NVIDIA RTX 4090 GPUs and 512GB of\nmemory. The inference time for tasks under 20K steps for running a single episode was approximately\n30 minutes on a single GPU. For long-horizon tasks that take 500K steps, approximately 12 hours\nwere required for running a single episode on a single GPU. The VPT-Nav training took roughly 23\nhours on a single GPU.\nC\nENVIRONMENTS DETAILS\nC.1\nENVIRONMENTS SETTING\nAll tasks are implemented using MineDojo (Fan et al., 2022b). We utilize MineDojo’s success\nchecker, where the success of each task is determined based on changes in the agent’s inventory.\nHence, the agent succeeds the task if the corrensponding target item is appeared in the inventory. If\nthe agent exceeds the time limit of each task or dies before completing assigned tasks, indicating the\nagent fails on the task.\nFor all tasks, we assume that the agent has access to both first-person view pixels and its positional\ndata. The raw pixel observation it is provided in the shape (160 × 256 × 3). The agent’s position pt\n15\nPreprint\nFigure 8: Topdown View of Three ABA-Sparse Task Maps. The first map was used in the ABA-Sparse tasks,\nincluding the water bucket task. Trees are distributed on the left side of the map, and water exists only in the\nupper right corner. The second map was used in tasks, not including the water bucket and sand task. Trees are\nlocated on the left side of the map, and on the right side, there are cows and sheep, with a mountain separating\nthem. The last map was used in tasks, including the sand task. Its overall layout is identical to the second\nmap, except for an additional water pond at the top. The map’s edges are surrounded by high walls, making it\nimpossible to access anything other than the resources visible in the top-down view.\nis represented as a vector of shape (5, ), where the first three components correspond to the (x, y, z)\ncoordinates, and the last two components represent the pitch and yaw angles of the agent’s camera.\nWe note that no privileged observations, such as LiDAR or voxel data, are provided. The agent\noperates in a keyboard and mouse action space following VPT (Baker et al., 2022). This action space\nconsists of button input states paired with mouse movements.\nCrafting items, which requires long-horizon planning, is not considered in our method. To eliminate\nthe need for crafting, the appropriate item necessary for solving a task is provided to the agent at the\nbeginning of each new task. For instance, if the task is to “obtain water bucket\n,” the agent starts\nwith an empty bucket\nin its main hand. Additionally, we apply the following rules:\n• \/difficulty peaceful: This rule prevents the occurrence of hostile mobs, such as\nzombies and spiders, and death by starvation.\n• \/gamerule doWeatherCycle false: This rule keeps the weather clear to reduce\nthe noise from heavy rain.\nC.2\nTASK DETAILS\nIn this section, we describe the details for each task in Experiment section (Section 4). The basic\nenvironment settings follow those outlined in Appendix C.1, unless specified otherwise.\nC.2.1\nEVALUATION PROTOCOLS\nExcept for the Long-Horizon tasks in Section 4.4, we ran 100 episodes for each agent using different\nrandom seeds to evaluate performance. For the Long-Horizon tasks, we reported the average success\nrate with the standard error over 5 episode runs for each agent.\nC.2.2\nEXPLORATION & NAVIGATION TASK DETAILS\nFor this task, we used the Map 1 in Figure 8 with 100 × 100 size, surrounded by high walls. The\nmap includes complex terrains such as mountains and a water pond, which requires robust low-level\nnavigation policy for successful exploration. For each episode, the agent spawns in the center of the\nmap and explores for 6,000 steps.\n16\nPreprint\nC.2.3\nABA-Sparse TASKS DETAILS\nIn these tasks, the agent is asked to complete three sequentially given tasks, where the first and third\ntasks are identical. The target item for the first task, denoted as A, is one of four sparsly distributed\nitems: water\n, beef\n, wool\n, or milk\n. In the second task, denoted as B, the taget item is\none of five items: log\n, dirt\n, seeds\n, leaves\n, or sand\n. The agent has unlimited memory\ncapacity and is allowed a maximum of 12,000 steps to complete three tasks. The task only changes to\nthe next one upon successful completion of the current task.\nIf the first task, A, is to obtain a water bucket\n, the agents spawn at the center of Map 1, shown in\nFigure 8. On this map, most of the surface is covered with dirt, while grass, which provides seeds, is\nwidely distributed. A water pond is located in the upper right corner of the map, but it only becomes\nvisible when agents are nearby.\nIf the first task, A, is to collect beef\nor wool\n, the agents spawn at the center of Map 2 in Figure\n8. Similarly, if the second task, B is to collect sand\n, the agents start at the center of Map 3 in\nFigure 8. In both maps, trees are scattered on the left side of the map, while sheep\nand cows\nare found on the right side. A sand mountain runs through the middle, separating the trees from the\nsheep and cows. Dirt is only present on the far left and right sides of the map.\nC.2.4\nMEMORY TASKS\nIn the three Memory Tasks, the agent explores for 3,000 steps before being asked to complete an\nimage goal navigation. Unlike the ABA-Sparse Tasks, the agent has a limited memory capacity of\n2,000 frames. Consequently, an agent utilizing the FIFO memory forgets the memory from the first\n1,000 frames after the exploration phase.\nFind Water Task The agent is initially spawned near a water pond and remains in its vicinity for 500\nsteps. During this period, one observation is selected as a goal image for a subsequent navigation task.\nAfter the initial 500 steps, the agent moves to a random location for the remainder of the exploration\nphase. After 3K steps, the agent begins to navigate back to the water pond it observed at the start of\nthe episode.\nFind Zombies’ Death Spot Task At the beginning, the agent sees burning zombies for the first 1K\nsteps. Approximately 500 steps after the start of the episode, the zombies disappear, resulting in the\nagent observes two distinct scenes in the same location. A goal image for navigation is selected from\nthe observation where the zombies are burning. After 1K steps, the agent starts to travel for the rest\nof the exploration phase. Once the exploration phase finishes, the agent returns to the place where the\nzombies were burning.\nFind First-Visited House Task In this task, there are two distinct houses that look similar to one\nanother. The agent starts near one of the houses, where it stays for 100 steps before moving to the\nother house, where it remains for 2K steps. For the remainder of the exploration phase, the agent\ntravels to a random location. After the exploration phase, the agent is asked to go to the first house it\nhas visited.\nC.2.5\nLONG-HORIZON TASKS\nLong-Instruction Task In this task, the agent is required to complete a series of tasks sequentially on\na 200 × 200 block-sized map in Figure 9. The order of tasks within the sequence is randomized, with\neach task being one of six possible types: water bucket\n, beef\n, wool\n, wood\n, dirt\n, and\nseeds\n. We evaluated the agent’s performance by measuring the number of successfully completed\ntasks over 500K steps. If the agent fails to complete a given task within 20,000 steps, that task is\ncanceled, and a new one is assigned.\nLong-Navigation Task This task is basically image goal navigation in a 200 × 200 block sized\nmap in Figure 9. Before the task phase, the agent undergoes a 16K-step exploration phase where it\nobserves six landmarks: 1) zombie burning\n, 2) water\n, 3) sugarcane explosion\n, 4) spider\nspawn\n, 5) tree\n, and 6) house, spending 2K steps at each location. In the task phase, the agent is\ngiven a random start image of one of these landmarks and must navigate to it. The dynamic nature\nof some landmarks, such as the burning zombie, exploding sugarcane, and spider spawn makes it\nimportant to store novel events from the exploration phase.\n17\nPreprint\nFigure 9: Topdown View of Two Long-Horizon Task Maps. Both maps are of size 200 × 200 blocks. (Left)\nThis map is used for the Long-Instruction task. Trees are located in the lower left and middle bottom of the\nmap, while sheep inhabit the upper left area, and cows exist in the right bottom. A water pond can be found\nin the upper right area of the map. (Right) This map is utilized for the Long-Navigation task. Agents traverse\nbetween six scenes in the map. Three of them are dynamic scenes: burning zombies, popping sugarcanes, and\nspawning spiders, which are positioned at the first, third, and fourth places on the map, respectively. The other\nthree are static scenes: a water pond, trees, and a house, located at the second, fifth, and sixth places on the map,\nrespectively.\n18\nPreprint\nD\nMRSTEVE ALGORITHM\nAlgorithm 2 MrSteve Algorithm\nRequire: Xtinit\nn from Environment env, Memory Mtinit\nn , and new task τn at time step tinit\nn .\n1: mode ←null\n2: Tmode ←0\n3: gt ←null\n4: reached ←false\n5: t ←tinit\nn\n6: Xt ←Xtinit\nn ; Mt ←Mtinit\nn\n7: loop\n8:\nWrite(Mt, Xt)\n9:\nif Tmode > L then\n10:\nmode ←null\n11:\nTmode ←0\n12:\ngt ←null\n13:\nreached ←false\n14:\nend if\n15:\n# Mode Selector\n16:\nif mode is null then\n17:\ncandidates ←Read(Mt, τn)\n18:\nif candidates ̸= ∅then\n19:\nX′\nt ←PickOne(candidates)\n20:\ngt ←Position(X′\nt)\n21:\nmode ←EXECUTE\n22:\nelse\n23:\nmode ←EXPLORE\n24:\nend if\n25:\nend if\n26:\nht ←Xt−128:t\n27:\n# Explore Mode\n28:\nif mode is EXPLORE then\n29:\ngt ∼πH-Nav(gt|ℓ′\nt, Mt)\n30:\nat ∼πL-Nav(at|ht, gt)\n31:\nend if\n32:\n# Execute Mode\n33:\nif mode is EXECUTE then\n34:\nif the agent reaches gt then\n35:\nreached ←true\n36:\nend if\n37:\nif reached then\n38:\nat ∼πInst(at|ht, τn)\n39:\nelse\n40:\nat ∼πL-Nav(at|ht, gt)\n41:\ngt+1 ←gt\n42:\nend if\n43:\nend if\n44:\nif the task τn succeeded or timeout then\n45:\nbreak\n46:\nend if\n47:\nTmode ←Tmode + 1\n48:\nXt+1 ←env.step(at); Mt+1 ←Mt\n49:\nt ←t + 1\n50: end loop\nWe provide more details in Algorithm 2. This algorithm is executed when a new task is given to the\nMrSteve and finished if it completes the task or exceeds a time limit (Line 44-46).\n19\nPreprint\nAgent State Variables When a new task τn is given, the agent state variables are initialized (Line\n1-4). mode indicates which module is executed and Tmode denotes elapsed times for the current mode\nexecution. gt is a target location for the navigation, and reached indicates whether the agent has\nreached the target location in the Execute Mode.\nThe Agent State Variables are reset if the agent remains in the current mode for L steps (Lines 9-13).\nIf the Mode Selector retrieves a wrong experience frame, where no task-relevant resource is present\nat the corresponding location, the agent can have trouble and fail to complete the given task for a\nlong time. In addition, the agent might encounter previously unobserved task-relevant resources\nwhile navigating to the location of the previously retrieved memory. In both scenarios, changing\nthe navigation target can be helpful. Consequently, the agent queries the memory to search for new\ncandidates if the agent executes the current mode for L = 600 steps, equivalent to 30 seconds in-game\ntime.\nMemory Write Frequency In Line 8, MrSteve writes the current observation to the memory,\nregardless of the current mode.\nMode Selector The Mode Selector decides between two modes according to the existence of task-\nrelevant resources in the memory. First, if no mode has been selected, MrSteve queries the memory\n(Line 16-17). If a task-relevant resource exists, MrSteve picks one of them and sets the navigation\ntarget gt to the location of the picked one (Line 18-21). Otherwise, the Explore Mode is selected\n(Line 23).\nExplore Mode In the Explore Mode, MrSteve explores the least-visited locations using πH-Nav and\nπL-Nav (Line 28-31). We provide more details in Appendix F.2.\nExecute Mode In the Execute Mode, MrSteve navigates to the selected target location gt first. Once\nit reached, Steve-1 is executed to follow the task instruction τn (Line 33-43).\n20\nPreprint\nE\nPLACE EVENT MEMORY, PLACE MEMORY, EVENT MEMORY DETAILS\nWe provide the Algorithms on Write & Read operations of Place Event Memory, and other memory\nvariants, which are Place Memory, and Event Memory. We also provide the specifications of each\nmemory in the following section E.4.\nE.1\nPLACE EVENT MEMORY WRITE & READ OPERATIONS\nAlgorithm 3 Place Event Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk, and each place cluster Mk has event clusters\n{Ek\ni }dk\ni=1 where Ek\ni is i-th event cluster with center embedding ek\nEi. Each place cluster has\ndummy deque Qk that stores recent R frames in that cluster, and update frequency timer rk.\nAdditional variables are Memory Capacity N, MineCLIP image encoder CLIPv, and experience\nXt = {it, lt, t} at time step t. MineCLIP threshold c.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: if ℓt ∈PLACE_CLUSTER(Mt) then\n4:\nFind place cluster Mj where PLACE(ℓt) = ℓMj\n5:\nAdd xt to dummy deque Qj\n6:\nUpdate frequency timer rk = rk + 1\n7:\nif rk = R then\n8:\n{Ei, ei}l′\ni=1 = DP-Means(Qj)\n9:\n{Ei, ei}l\ni=1 = MERGE_CLUSTERS({Ei, ei}l′\ni=1)\n10:\nfor u = 1, . . . , l do\n11:\nadd_cluster=True\n12:\nfor p = 1, . . . , dj do\n13:\nif ej\nu · ej\nEp > c then\n14:\nMerge Eu to Ej\np\n15:\nadd_cluster=False; break\n16:\nend if\n17:\nend for\n18:\nif add_cluster=True then\n19:\nCreate new event cluster Ej\ndj+1 = Eu with center embedding ej\nEdj +1 = eu\n20:\nAdd created cluster Ej\ndj+1 to Mj\n21:\nend if\n22:\nend for\n23:\nrk = 0\n24:\nend if\n25: else\n26:\nCreate new place cluster MK+1 with center position pMK+1 = PLACE(ℓt), and center\nembedding eMK+1 = et\n27:\nCreate new dummy deque QK+1 = {xt}\n28:\nAdd created cluster MK+1 = {QK+1} to Mt\n29: end if\n30: # Memory Remove\n31: if len(Mt) > N then\n32:\nFind event cluster Ek\ni where i, k = arg max len(Ek\ni )\n33:\nRemove the oldest frame in Ek\ni\n34: end if\nWe further elaborate on how # Memory Add in Algorithm 3 operates. First, we get the experience\nframe xt, and check if ℓt belongs to one of place clusters in Mt (Line 3). Here, PLACE_CLUSTER()\nindicates the whole 2D space that memory Mt covers. Since we use fixed-size square area for each\nplace cluster, it can be seen as covered area of set of squares. If xt is not in current place clusters,\nnew place cluster is created with dummy deque (Line 26-28). Dummy deque here is short memory\n21\nPreprint\nfor each place cluster used for clustering the events. If xt belongs to some place cluster (Line 4,\nPLACE() maps ℓt to its place), it is added to dummy deque in the place cluster, and increase update\nfrequency timer in that place cluster by 1 (Line 4-6). When update frequency timer equals to R,\nDP-Means algorithm (Dinari & Freifeld, 2022) is applied to experience frames in dummy deque\nfor event clustering. After applying DP-Means algorithm, we get a set of events and its center\nembeddings (Line 8). However, we found that DP-Means tends to make different clusters even when\nthe agent observes the same scenes. Thus, we applied MERGE_CLUSTERS() to DP-Means output\nclusters to merge clusters that have high alignments in center embeddings (Line 9). After this, for\neach newly created event cluster from DP-Means (Line 10), if some existing event cluster is aligned\n(Line 13), two clusters are merged (Line 14). If newly created cluster does not belong to any existing\nevent clusters, then add to the place cluster as a new event cluster (Line 19-20).\nEvent Cluster Details We use the DP-Means algorithm for clustering the events. DP-Means\nalgorithm is a Bayesian non-parametric extension of the K-means algorithm based on small variance\nasymptotic approximation of the Dirichlet Process Mixture Model. It doesn’t require prior knowledge\non the number of clusters K. To run this algorithm, we first set the initial number of clusters K′ and\ncluster the data with K++ Means initialization (K′ can be 1), then DP-Means algorithm automatically\nre-adjust the number of clusters based on the data points and cluster penalty parameter δ. Thus,\nDP-Means algorithm behaves similarly to K-means with the exception that a new cluster is formed\nwhenever a data point is farther than δ away from every existing cluster centroid.\nUsing clustering algorithm in Minecraft was previously done in Yuan et al. (2024), where K-Means\nalgorithm is used to cluster 100 goal states in massive Minecraft dataset. However, they applied the\nclustering on the reduced dimension of MineCLIP representation with t-SNE (van der Maaten &\nHinton, 2008). In this work, we directly apply DP-Means algorithm in MineCLIP representation\nspace setting initial number of clusters K′ = 5, and δ = 1.\nPlace Cluster Details We provide a detailed explanation of how place clusters are formed. Each place\ncluster stores the agent’s experience frames based on its 2D ground position (x, y) and head direction\nangle yaw. The size of a place cluster and its yaw range are defined by parameters C (in Minecraft\nblock) and W ∈(0◦, 180◦), respectively, which distinguish different clusters. A place cluster is\ncentered at a specific position (x, y) with a center yaw w. An experience frame is assigned to a place\ncluster if the agent’s position falls within the range (x−C\/2, x+C\/2) for x and (y −C\/2, y +C\/2)\nfor y, and the yaw angle satisfies (w −W\/2, w + W\/2). We use relative position and yaw for center\nof the place cluster implying that the center of the initial place cluster has position (0, 0), and yaw 0.\nAlgorithm 4 Place Event Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk, and each place cluster Mk has event clusters\n{Ek\ni }dk\ni=1 where Ek\ni is i-th event cluster with center embedding ek\nEi. Additional variables are\nMineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction τn at time step t, and\ntask threshold h.\n1: Calculate sik(τn) = CLIPt(τn) · CLIPv(ek\nEi) for all i, k\n2: Sort sik(τn) for all event clusters and select top-K event clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K event clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K event clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\nExploiting hierarchical structure of Place Event Memory For memory read operation, we can\nexploit hierarchical structure of place event memory for better computation efficiency. Suppose we\nhave N1 place clusters and exactly N2 event clusters for each place cluster. Then, read operation\nin Algorithm 4 has computational complexity of O(N1N2). However, if we first attend only to\nplace clusters (calculate sik from center embeddings eMk from place clusters), then attend to event\n22\nPreprint\nclusters from extracted top-k place clusters, the computational complexity becomes O(N1 + kN2).\nHowever, center embedding eMk may not be sufficient summarization of the place cluster, since\ncenter embedding can not capture all different events occurred in that place. Thus, we use the read\noperation as in Algorithm 4.\n23\nPreprint\nE.2\nPLACE MEMORY WRITE & READ OPERATIONS\nAlgorithm 5 Place Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk. Each place cluster Mk is a FIFO Memory.\nAdditional variables are Memory Capacity N, MineCLIP image encoder CLIPv, and experience\nXt = {it, lt, t} at time step t.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: if ℓt ∈PLACE_CLUSTER(Mt) then\n4:\nFind place cluster Mj where PLACE(et) = eMj\n5:\nAdd xt to Mj\n6: else\n7:\nCreate new place cluster MK+1 with center position ℓMK+1 = PLACE(ℓt), and center\nembedding eMK+1 = et\n8:\nAdd created cluster MK+1 = {xt} to Mt\n9: end if\n10: # Memory Remove\n11: if len(Mt) > N then\n12:\nFind place cluster Mk where k = arg max len(Mk)\n13:\nRemove the oldest frame in Mk\n14: end if\nAlgorithm 6 Place Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Mk}K\nk=1 where Mk is k-th place cluster with\ncenter position ℓMk, and center embedding eMk. Each place cluster Mk is a FIFO Memory. Ad-\nditional variables are MineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction\nτn at time step t, and task threshold h.\n1: Calculate sk(τn) = CLIPt(τn) · CLIPv(eMk) for all k\n2: Sort sk(τn) for all place clusters and select top-K place clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K place clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K place clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\n24\nPreprint\nE.3\nEVENT MEMORY WRITE & READ OPERATIONS\nAlgorithm 7 Event Memory Write Operation at time step t\nRequire: Assume memory at time step t as Mt = {Ek}K\nk=1 where Ek is k-th event cluster with\ncenter embedding eEk, and each event cluster is a FIFO Memory. Memory has a dummy deque\nQ that stores recent R frames, and update frequency timer r. Additional variables are Memory\nCapacity N, MineCLIP image encoder CLIPv, and experience Xt = {it, lt, t} at time step t.\nMineCLIP threshold c.\n1: # Memory Add\n2: Get experience frame xt = {et, lt, t} where et = CLIPv(it)\n3: Add xt to dummy deque Q\n4: Update frequency timer r = r + 1\n5: if r = R then\n6:\n{Ei, ei}l′\ni=1 = DP-Means(Qj)\n7:\n{Ei, ei}l\ni=1 = MERGE_CLUSTERS({Ei, ei}l′\ni=1)\n8:\nfor u = 1, . . . , l do\n9:\nadd_cluster=True\n10:\nfor p = 1, . . . , k do\n11:\nif ej\nu · eEp > c then\n12:\nMerge Eu to Ep\n13:\nadd_cluster=False; break\n14:\nend if\n15:\nend for\n16:\nif add_cluster=True then\n17:\nCreate new event cluster Ek+1 = Eu with center embedding eEk+1 = eu\n18:\nAdd created cluster Ek+1 to Mt\n19:\nend if\n20:\nend for\n21:\nrk = 0\n22: end if\n23: # Memory Remove\n24: if len(Mt) > N then\n25:\nFind event cluster Ek where k = arg max len(Ek)\n26:\nRemove the oldest frame in Ek\n27: end if\nAlgorithm 8 Event Memory Read Operation at time step t\nRequire: Assume memory at time step t as Mt = {Ek}K\nk=1 where Ek is k-th event cluster with\ncenter embedding eEk, and each event cluster Ek is a FIFO Memory. Additional variables are\nMineCLIP image encoder CLIPv, and text encoder CLIPt, task instruction τn at time step t, and\ntask threshold h.\n1: Calculate sk(τn) = CLIPt(τn) · CLIPv(eEk) for all k\n2: Sort sk(τn) for all event clusters and select top-K event clusters\n3: Gather all experience frames xt = {et, lt, t} from the top-K event clusters\n4: cand = {}\n5: for each experience frame xt in the selected top-K event clusters do\n6:\nsxt(τn) = CLIPt(τn) · CLIPv(et)\n7:\nif sxt(τn) > h then\n8:\nAdd xt to cand\n9:\nend if\n10: end for\n11: return cand\nDiscussions on MineCLIP threshold As in Event Memory’s Write Operation in Algorithm 7,\nclusters generated by DP Means algorithm are either merged with an exisiting event cluster, or\nadded as new event cluster. This is determined by MineCLIP threshold c, which is the criterion\n25\nPreprint\nfor separating event clusters. We note that using proper value for threshold is important for Event\nMemory to work reasonably across the tasks. If c is too small, Event Memory will cluster experience\nframes with only few clusters, which may not be visually distinctive. If c is too large, in extreme\ncase, Event Memory will make event cluster for each experience frame. When memory capacity is\nexceeded, it will randomly remove the oldest frame (since all event clusters have same size), which\nbehaves as a FIFO Memory.\nDrawback in Event Memory Event Memory removes the frame in the largest event cluster, when\nmemory capacity is exceeded. This indicates that the memory can retain visually distinct events.\nHowever, since it does not consider position information in clustering, similar visual frames in\ndifferent places can be clustered into same event cluster. This can be fatal since task-relevant frames\nin different places can be removed from the memory which can be crucial in upcoming tasks. This\ndrawback is shown in Find First-Visited House task in Section 4.3, and Long-Instruction task in\nSection 4.4, where MrSteve-EM showed suboptimal performances.\nE.4\nHYPER-PARAMETERS FOR MEMORY\nWe provide the specifications for each memory type used in the experiments. The task threshold h is\nset to 22.74 by default, but it can be adjusted to enhance retrieval accuracy.\nTable 2: Specifications for each memory type.\nMemory Type\nParameter\nValue\nPlace Event Memory\nPlace Cluster Size C\n6\nPlace Cluster Yaw Y\n60\nUpdate Frequency rk\n100\ntop-K\n30\nMineCLIP Threshold c\n73.5\nTask Threshold h\n22.74\nEvent Memory\nUpdate Frequency rk\n100\ntop-K\n30\nMineCLIP Threshold c\n73.5\nTask Threshold h\n22.74\nPlace Memory\nPlace Cluster Size C\n6\nPlace Cluster Yaw Y\n60\ntop-K\n30\nTask Threshold h\n22.74\n26\nPreprint\nFigure 10: The model architectures of Explore Mode and Execute Mode in Solver Module.\nF\nSOLVER MODULE DETAILS\nF.1\nMODE SELECTOR\nIn Section 3.2, we described the Mode Selector when combined with Place Event Memory. When\nthe memory type changes such as Place Memory or Event Memory, different read operation should\nbe employed. We provide the Memory Read operation of Place Memory, and Event Memory in\nAppendix E.2, and E.3, respectively. In case of FIFO Memory, for read operation, task alignment\nscores on whole frames in the memory is required.\nF.2\nEXPLORE MODE\nIn explore mode, hierarchical structure is employed. For high-level goal selector, we take the similar\napproach introduced in Chang et al. (2023), where the target problem is indoor navigation with\nrobots. Their global policy exploits semantic map to output exploration goal. For the goal selection,\nit uses frontier-based exploration (Yamauchi, 1998), which selects the closest unexplored region as\nthe goal. In Figure 10(a), the overview of hierarchical episodic exploration is illustrated. From Place\nEvent Memory, we contruct a visitation map by marking the agent’s visited locations. In addition to\nmarking agent’s visited locations, we also marked the agent’s FoV in the visitation map with sector\nregion towards agent’s head direction (yellow sector shows agent’s current FoV). From the visitation\nmap, the next goal is chosen, and the goal, and current observation, position is given to VPT-Nav\nfor generating low-level action. In experiments, for the tasks with map size 100 × 100, we used\nvisitation map size L = 120, and grid cell size G = 15. For the tasks with map size 200 × 200, we\nused visitation map size L = 240, and grid cell size G = 30.\nF.3\nEXECUTE MODE\nThe model architecture of Execute mode is illustrated in Figure 10(b). When the Mode Selector\nselects the Execute Mode, the experience frame of a task-relevant resource is retrieved from the\nmemory. MrSteve then navigates to the goal location of the experience frame, and then adjust camera\norientation using yaw and pitch from the frame to ensure it faces the observed of the retrieved\nexperience frame again. Once the camera adjustment is complete, Steve-1 is executed to follow the\ntask instruction τn.\nF.4\nMINECLIP & STEVE-1 PROMPTS FOR TASKS\nThe prompts used in our experiments are listed in Table 3. MrSteve sets τn to one of two different\nprompts depending on the agent’s status. When the agent is in the mode selection phase, the\nMineCLIP prompts are used as the query for the memory to determine whether the task-relevant\nresource exists in the memory. In contrast, the Steve-1 prompts are utilized during the execution\nmode.\nRelying solely on Steve-1 prompts can lead to difficulties in calculating alignment score between the\nprompt and memory records. Since MineCLIP (Fan et al., 2022b) computes the alignment between\nvideos and textual descriptions, the alignment score is higher when the textual description well\nreflects the agent’s action shown in the video. If a video contains scenes relevant to the current\n27\nPreprint\nTable 3: Prompts used in MrSteve for each task.\nMineCLIP\nSteve-1\nlog\nnear tree\ncut a tree\nbeef\nnear cow\nkill cow\ndirt\nnear dirt\nget dirt\nsand\nnear sand\nget sand\nseed\nnear grass\ncollect seeds\nwool\nnear sheep\nkill sheep\nleaves\nnear tree\nbreak leaves\nmilk bucket\nnear cow\nget milk bucket\nwater bucket\nnear water\nget water bucket\ntask but lacks behavior indicative of task completion, the alignment score will be low. For instance,\nconsider a scenario where the “obtain water bucket\n” task is given for the first time, and the agent\nhas previously encountered water. Although the agent is Near water in that scene, it has not yet\nperformed the action of Obtaining water bucket. As a result, the prompt “obtain water bucket” would\nnot accurately describe that memory, and that scene could be disregarded during the mode selection\nprocess. In our experiments, we manually defined two types of prompts for each task. However, this\nprocess can be automated using LLMs, which prompts an LLM to extract the more suitable language\ninstructions to complete a task.\nG\nEXPLORATION EXPERIMENTS DETAILS\nFigure 11: Agent’s trajectories of length 6K steps on 100 × 100 sized map with different exploration methods.\nLeft figure is the agent’s trajectory from our exploration method.\nWe provide further details on evaluation metrics and analysis of the exploration results. For evaluation\nmetrics, we measured Map Coverage and Rivisit Count. Map Coverage is calculated by dividing the\nmap into 11 × 11 grid cells and measuring the percentage of cells covered by the agent’s trajectory.\n28\nPreprint\nRevisit Count measures the average number of times the agent visits the same grid cell only for the\ncells that have visitation counts larger than 300. We measured Revisit Count only for highly visited\ngrid cell since the agent needs some amount of time to escape the grid cell.\nSince Steve-1 (Lifshitz et al., 2024) has no memory module, it tends to visit same place multiple times.\nWe also observed that Steve-1 exhibits a behavior of moving straight ahead when task instruction \"Go\nExplore\" is given, often colliding with walls and then following along the wall instead of avoiding it,\nwhich harms the Map Coverage and Revisit Count.\nPlan4MC (Yuan et al., 2023) employs a hierarchical exploration policy. The high-level policy, based\non an LSTM model, takes the agent’s (x, y) coordinates along its trajectory and outputs the next\ndirection to move (north, south, east, or west). The environment is discretized into an 11×11 grid, and\nthe policy is trained using PPO (Schulman et al., 2017) to maximize the number of unique grid cells\nvisited. The low-level policy is trained using DQN (Mnih et al., 2015) and follows the MineAgent\nstructure from the MineDojo framework. It receives a goal position and current observation in pixel\nand outputs actions in the MineDojo action space. However, despite the high-level RNN policy, it\nstruggled to keep track of visited locations as the trajectory grew longer, leading to a high Revisit\nCount. Furthermore, the low-level DQN controller faced difficulties navigating complex terrain, such\nas mountains and rivers, resulting in lower Map Coverage.\nH\nABA-Sparse TASKS FULL RESULTS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Dirt-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Dirt-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Dirt-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Seed-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Seed-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Seed-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Seed-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Log-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Log-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Log-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Leaves-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Leaves-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWater-Sand-Water\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Sand-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWool-Sand-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 12: ABA-Sparse Tasks Full Result.\nWe report the results of all combinations of the ABA-Sparse tasks in Figure 12. The experiments\nwere conducted under the same conditions as described in Section 4.2. We note that MrSteve and\n29\nPreprint\nits memory variants consistently outperform Steve-1 in all tasks. In case of PMC-MrSteve-FM,\nit performed better than Steve-1 in most tasks, while it failed on some tasks due to a suboptimal\nexploration strategy.\nI\nMRSTEVE IN RANDOMLY GENERATED MAPS\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSuccess Rate\nWater-Log-Water\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nBeef-Log-Beef\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nWool-Log-Wool\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nSEQ(4)\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nPMC-MrSteve-FM\nSteve-1\nFigure 13: The performance of different agents in ABA-Random tasks. MrSteve consistently outperforms\nSteve-1 in randomly generated map.\nIn this experiment, we investigate whether MrSteve benefits over Steve-1 in sequential task when\nthe map is randomly generated. To verify this, we created random Plains maps using the Minedojo\nenvironment (Fan et al., 2022b), and set up tasks similar to the ABA-Sparse tasks described in Section\n4.2. We call this task, ABA-Random task.\nIn this experiment, task A could involve collecting either water, beef, or wool, while task B involves\ngathering logs. Additionally, we introduce a sequential task named SEQ(4), which requires the agent\nto solve four consecutive tasks: log, water, wool, and beef. For the ABA-Random tasks, the agent was\ngiven 12K steps, and for the SEQ(4) task, 16K steps were allowed. We evaluated following agents:\nMrSteve, MrSteve-EM, MrSteve-PM, MrSteve-FM, PMC-MrSteve-FM, and Steve-1.\nAs shown in Figure 13, MrSteve and its memory variants consistently showed higher success rate\nthan Steve-1 across all tasks. This is because when Steve-1 is finished with task B, it tries to solve the\nfinal task A from scratch making it difficult to complete all tasks in limited time. However, MrSteve\ncould retain the experience frames of task A in memory making it efficient to solve all tasks in time.\nThis result suggests that augmenting memory plays a crucial role in solving sequential tasks, even in\nrandomly generated map.\nJ\nABLATION STUDIES ON PLACE EVENT MEMORY\n1\n2\n4\n10\n30\ntop-k\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\n1\n2\n4\n6\n12\nCluster Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 14: Success Rates of MrSteve with different top-\nk’s, and cluster sizes on Wool-Log-Wool task.\nIn this section, we study the robustness of our\nproposed agent MrSteve by exploring the effects\nof top-k selection in Mode Selector, and the size\nof a place cluster for Place Event Memory. For\nthis, we used one of the tasks from ABA-Sparse\ntask in Section 4.2, which is Wool-Log-Wool\ntask. The success rate of MrSteve with different\ntop-k’s and place cluster sizes are illustrated in\nFigure 14. For top-k experiment, we tested five\nk values, which are 1, 2, 4, 10, and 30. From\nthe results, we found that using small k did not\nlargely affect the performance. For cluster size experiment, we tested five cluster sizes, which\nare 1, 2, 4, 6, and 12 in Minecraft blocks. From the results, we see that using bigger cluster size\nslightly lowered the performance since the center embedding of the place cluster may not be good\nsummarization vector of the place when cluster size is large, making the agent difficult to recall\ntask-relevant frames in the memory.\n30\nPreprint\nTable 4: Querying Time and FLOPs for each memory types and k values. We report the averages and standard\nerrors over 1K query operations.\nMemory Type\ntop-k\nTime per Query (ms)\nFLOPs (×109)\nPlace Event Memory\n1\n6.24 ± 0.01\n1.591 ± 0.001\n2\n6.47 ± 0.02\n1.610 ± 0.002\n4\n6.89 ± 0.02\n1.667 ± 0.003\n10\n7.92 ± 0.03\n1.791 ± 0.004\n30\n11.24 ± 0.07\n2.210 ± 0.010\nEvent Memory\n1\n11.28 ± 0.35\n1.603 ± 0.054\n2\n18.95 ± 0.56\n2.711 ± 0.073\n4\n42.41 ± 1.20\n3.734 ± 0.088\n10\n53.82 ± 1.11\n7.476 ± 0.150\n30\n117.16 ± 2.02\n12.694 ± 0.201\nPlace Memory\n1\n3.86 ± 0.05\n1.251 ± 0.001\n2\n4.21 ± 0.08\n1.281 ± 0.002\n4\n4.72 ± 0.08\n1.338 ± 0.003\n10\n5.98 ± 0.09\n1.504 ± 0.005\n30\n9.85 ± 0.12\n2.024 ± 0.011\nFIFO\n–\n438.46 ± 0.10\n52.481 ± 0.000\nK\nCOMPUTATIONAL COMPLEXITY OF QUERY OPERATION\nWe provide the computational complexity of query operation of each memory type in Table 4. To\nensure a fair comparison, we generated an episode with a 100K-step trajectory, allowing each type\nof memory to process identical observations. After constructing each type of memory with this\ntrajectory, we randomly selected 1,000 observation embeddings from the trajectory for the query\noperations. The query time represents the elapsed time during the query function call. FLOPs denotes\nthe number of floating-point operations required for the MineCLIP score calculation. For FIFO\nmemory, MineCLIP scores are computed once during task alignment score calculation. For the other\nmemory types, MineCLIP scores are calculated twice: during the top-k selection and for the task\nalignment score.\nThe number of clusters affects the complexity of top-k selection and the number of frames per cluster\ninfluences the complexity of the task alignment score calculation. Place Event Memory consists of\napproximately 3,000 clusters, with each cluster containing an average of 30 frames. Place Memory,\non the other hand, includes about 2,300 clusters, each holding an average of 43 frames. Although\nPlace Memory provides the fastest querying time, Place Event Memory has comparable speed while\nachieving similar or superior success rates in most tasks.\nEvent Memory comprises around 582 clusters, with an average of 172 frames per cluster. Its clustering\napproach relies on visual discriminative features, which means visually similar places are grouped\ntogether, even if they are spatially distant. As a result, Event Memory has significantly fewer clusters\ncompared to Place and Place Event Memory.\nOverall, the three hierarchical memory types are computationally efficient and lightweight compared\nto FIFO Memory, which calculates the task alignment scores on whole 100K frames. In contrast,\nPlace Event Memory with k = 30 evaluates roughly 3,900 frames per query (3,000 for top-k selection\nand 30 frames per cluster across 30 clusters), resulting in a performance that is approximately 40\ntimes faster and requires about 24 times fewer FLOPs than FIFO Memory.\nL\nGOAL-CONDITIONED VPT NAVIGATOR DETAILS AND INVESTIGATION\nL.1\nGOAL-CONDITIONED VPT NAVIGATOR FINE-TUNING DETAILS\nWhen the goal location lG is selected by high-level goal selector, it is important for the agent to\nnavigate to the goal location accurately. Since navigating complex terrains (e.g., river, mountain)\nrequires human prior knowledge, we use VPT as our initial policy, and fine-tune it for goal-conditioned\n31\nPreprint\nnavigation policy. We name this model as VPT-Nav. To see how VPT-Nav model works, we first\ndescribe the components of VPT as follows:\nImage Encoder:\nxt = IE\nθ (it)\nTransformer-XL:\nzt−T :t = TrXLθ(xt−T , · · · , xt)\nPolicy Head:\nˆat ∼πθ(at|zt)\n(1)\nIn previous works, there were different approaches in fine-tuning VPT for specific purpose. First is\ngoal-conditioned behavior cloning. In Steve-1 (Lifshitz et al., 2024), authors added linear projection\nof a text embedding to the image embedding xt and fine-tuned the whole VPT model for behavior\ncloning from human demonstration data. This makes Steve-1 a text-conditioned VPT. In GROOT\n(Cai et al., 2023b), authors used gated cross-attention layers (Alayrac et al., 2022) in Transformer-XL\n(Tr-XL) to condition the video of some tasks (e.g., log wood). GROOT was trained for behavior\ncloning from reference videos of human plays working as video-conditioned VPT.\nThe second is RL fine-tuning for the single task. In DECKARD (Nottingham et al., 2023) and PTGM\n(Yuan et al., 2024), VPT was fine-tuned for single task with PPO (Schulman et al., 2017) by attaching\nadaptor (Houlsby et al., 2019) in Tr-XL layers, and value head ˆvt = vθ(zt). When fine-tuning, only\nthe adaptors, policy head, and value head were updated. For learning stability, those works used\ndifferent KL loss in PPO objective, which is KL loss between fine-tuning policy and VPT policy to\nkeep the VPT’s prior knowledge.\nSince our target is to train goal-conditioned navigation policy, one way to achieve this is to naively\ncombining ideas from 1) goal-conditioned behavior cloning, and 2) RL fine-tuning for the single task.\nWe first make goal embedding G(lt, lG) from the agent’s location lt and goal location lG with goal\nencoder G(·), then add it to image embedding xt. Second, we attach the adaptor in Tr-XL layers, and\nvalue head. Then, we fine-tune the whole model or only adaptors and policy, value heads with PPO.\nHowever, we found that this naive approach showed suboptimal navigation behavior in complex\nterrains such as mountain and river. We speculated that this is because RL objective is rather weak\nlearning signal compared to behavior cloning, which causes hardship in giving information of goal\nembedding to the policy head. Also, some information of goal embedding may be corrupted while\nit is added to image embedding, and processed by Tr-XL layers. Thus, we made the following\nmodifications in the model architecture. First, instead of giving goal embedding in Tr-XL input,\nwe added the goal embedding to policy head input. Second, we used recently proposed adaptor for\nTr-XL, which is LoRA (Hu et al., 2021b). The following changes enabled VPT-Nav to exhibit optimal\nnavigation behavior. We provide an investigation of this model architecture search in Appendix L.2.\nWith the changes in previous paragraph, the modified VPT for navigation with the learning parameters\nψ has the following components:\nImage Encoder:\nxt = IE\nθ (it)\nTransformer-XL:\nzt−T :t = TrXLψ(xt−T , · · · , xt)\nGoal Conditioning:\nz′\nt = GE\nψ(lt, lG) + zt\nPolicy Head:\nˆat ∼πψ(at|z′\nt)\nValue Head:\nˆvt = vψ(z′\nt),\n(2)\nHere, goal encoder GE\nψ is 4-layer MLP, and value head is a randomly initialized single linear layer.\nParameter ψ in Tr-XL indicates LoRA parameters. We use PPO for training with reward based on the\nincrease or decrease in Euclidean distance between locations of the goal and the agent. When the\nagent reaches the goal location within 3 blocks, it is considered a success, and an additional reward\nof 100 is given. The detailed hyper-parameters for training will be found in Table 5.\nL.2\nMODEL ARCHITECTURE SEARCH FOR GOAL-CONDITIONED VPT NAVIGATOR\nIn the previous section, we discussed the naive way of combining the idea of goal-conditioned\nbehaivor cloning and RL finetuning for training goal-conditioned navigator. In this section, we\nconduct an model architecture search on VPT model to find the optimal goal-conditioned navigation\nmodel. To do this, we focused on three key design choices: 1) the input location of goal embedding,\n2) training parameters, and 3) different Tr-XL adaptors . For the location of goal embedding, we\n32\nPreprint\nTable 5: Hyper-parameters for the Goal-Conditioned Navigation VPT Training.\nName\nValue\nInitial VPT Model\nrl-from-foundation-2x\nDiscount Factor\n0.999\nRollout Buffer Size\n40\nTraining Epochs per Iteration\n5\nVectorized Environments\n4\nLearning Rate\n10−4\nKL Loss Coefficient\n10−4\nKL Loss Coefficient Decay\n0.999\nTotal Iteration\n400K\nSteps per Iteration\n500\nGAE Lambda\n0.95\nClip Range\n0.2\nconsider the three tactics: (1) add the goal embedding to the image embeddings and input to Tr-XL\n(TrXL-Cond.), (2) add the goal embedding to Tr-XL output (Head-Cond.), and (3) add the goal\nembedding both at the input and output of Tr-XL (Dual-Cond.). For training parameters, we use\nfull-finetuning or finetuning only part of the model. For the Tr-XL adaptors, we consider the adaptor\nfrom Houlsby et al. (2019), and LoRA adaptor (Hu et al., 2021b). In Figure 15, we illustrate the\ndifferent model architectures for goal-conditioned VPT finetuning models.\nFor the model search, we summarized the navigation performance (SPL and Success Rate (SR)) of\ndifferent model architectures in Table 6. Interestingly, we found that adding goal embedding to image\nembeddings (TrXL-Cond.) showed suboptimal behavior, while adding goal embedding to output of\nTr-XL (Head-Cond., Dual-Cond.) showed good performance indicating that propagating learning\nsignal to goal embedding through Tr-XL is difficult from RL objective. We note that using LoRA\nadaptor showed higher performance than adaptors from Houlsby et al. (2019). In conclusion, using\nHead-Cond. with LoRA finetuning performed the best. Additionally, we tested one additional model\nthat does not update Tr-XL, which showed comparable result to the best performing model. This\narchitecture benefits for using smaller number of learning parameters and lower gradient computations.\nThus, we use the VPT-Nav trained with one of two architectures, which are Head-Cond. with LoRA\nfinetuning, and Head-Cond. with No Tr-XL Update.\nFigure 15: Four Types of Navigation Goal-Conditioning.\nL.3\nVPT NAVIGATOR ABLATION STUDY\nWe investigate how KL loss in PPO objective (i.e., KL loss between fine-tunin VPT and original\nVPT) affects the performance of VPT-Nav in diverse environments (Table 7). We measured the\nSPL and Success Rate (SR) of the navigators in navigation tasks where the goal location is within\n10–20 blocks away from agent’s location. While Heuristic method performs best in Flat, Plains, and\nMountain tasks, VPT-Nav with KL coefficient 10−4 showed high performances in all tasks. In case of\nlow-level navigator in Plan4MC (Yuan et al., 2023) which uses DQN policy, we observed suboptimal\n33\nPreprint\nTable 6: VPT-Nav Performance of different Goal-Conditioning Methods. Top-1 performances are bolded.\nConditioning\nFine-tuning\nFlat\nPlains\nMountain\nRiver\nTrXL\nFull\n0.050 (5%)\n0.423 (46%)\n0.000 (0%)\n0.000 (0%)\nHoulsby et al. (2019)\n0.488 (56%)\n0.305 (34%)\n0.052 (32%)\n0.000 (0%)\nLoRA\n0.481 (55%)\n0.475 (51%)\n0.058 (42%)\n0.000 (0%)\nDual\nHoulsby et al. (2019)\n0.883 (95%)\n0.828 (90%)\n0.125 (94%)\n0.066 (31%)\nLoRA\n0.798 (96%)\n0.762 (90%)\n0.169 (100%)\n0.287 (100%)\nHead\nNo TrXL Update\n0.849 (96%)\n0.780 (88%)\n0.157 (100%)\n0.274 (87%)\nHoulsby et al. (2019)\n0.729 (85%)\n0.841 (91%)\n0.159 (100%)\n0.052 (23%)\nLoRA\n0.904 (98%)\n0.880 (95%)\n0.150 (100%)\n0.307 (100%)\nbehavior. We note that VPT-Nav is robust to tasks with difficult terrains such as Mountain, and River\nsince VPT (Baker et al., 2022) is trained from human demonstration data which has high-quality\nnavigation ability. Also, using large KL coefficient (e.g., 10−2) harmed the overall performance while\nusing small KL coefficient (e.g., 0) harmed the robustness of navigator in complex tasks (e.g., River).\nTable 7: Performance of different Low-Level Navigators. Top-2 performances are bolded.\nSPL (Success Rate)\nFlat\nPlains\nMountain\nRiver\nHeuristic\n0.962 (100%)\n0.906 (94%)\n0.188 (100%)\n0.000 (0%)\nLow-level Navigator in Plan4MC\n0.416 (61%)\n0.326 (47%)\n0.010 (10%)\n0.000 (0%)\nVPT-Nav (KL = 0)\n0.806 (90%)\n0.774 (85%)\n0.131 (97%)\n0.034 (13%)\nVPT-Nav (KL = 10−4)\n0.849 (96%)\n0.780 (88%)\n0.157 (100%)\n0.274 (87%)\nVPT-Nav (KL = 10−3)\n0.762 (95%)\n0.651 (78%)\n0.015 (17%)\n0.012 (6%)\nVPT-Nav (KL = 10−2)\n0.692 (84%)\n0.702 (83%)\n0.078 (87%)\n0.014 (7%)\n34\nPreprint\nM\nLLM-AUGMENTED AGENT WITH MRSTEVE\nTable 8: Long-Horizon Planning Tasks details. The required subgoals denotes the length of shortest plan for\neach task. The items that the low-level controller should obtain are listed in Items to Obtain column.\nTask\nMax Steps\nRequired Subgoals\nItems to Obtain\noak_stairs\n3000\n4\nlog×3\nsign\n3000\n5\nlog×3\nfence\n3000\n5\nlog×3\nbed\n6000\n5\nlog×3, wool×3\npainting\n6000\n6\nlog×2, wool×1\ncarpet\n6000\n2\nwool×2\nitem_frame\n6000\n6\nlog×2, leather×1\nleather_boots\n6000\n5\nlog×1, leather×4\nleather_chestplate\n6000\n5\nlog×1, leather×8\nleather_helmet\n6000\n5\nlog×1, leather×5\nleather_leggings\n6000\n5\nlog×1, leather×7\nTable 9: Success Rate of two low-level controllers with DEPS (Wang et al., 2023c) planner.\nTask\nSteve-1\nMrSteve\noak_stairs\n67%\n80%\nsign\n53%\n60%\nfence\n40%\n50%\nbed\n27%\n50%\nIn this section, we investigate synergy between MrSteve and LLM-augmented hierarchical agent\nframework with long-horizon planning tasks listed in Table 8. We follow DEPS (Wang et al., 2023c)\nas LLM-augmented high-level planner with two low-level controllers: Steve-1 and MrSteve.\nOnce the target object for a task is given, the high-level planner asks the LLM to make the initial\nplan Pt = {τ g\nt,i}N\ni=1 for the task, where τ g\nt,i is i-th subgoal at timestep t in the textual form. After the\ninitial plan is given, the low-level controller executes the subgoal sequentially. Each subgoal is mine\nor craft type, where craft subgoals are executed heuristically via MineDojo functional actions,\nwhereas mine subgoals are executed by low-level controllers.\nLLM-generated initial plans can be inaccurate, so the low-level controllers often fails to execute\nsubgoals. For instance, we found that the initial plans frequently omit the subgoal for creating crafting\ntable, which is prerequisite item for the tasks in Table 8 except the carpet task. To address this\nproblem, DEPS framework introduced the replanning procedure with the descriptor and explainer\nmodules. The descriptor module makes a text prompt representing the inventory of the agent. The\nexplainer modules ask the LLM the reason of failure based on the text prompt from the descriptor\nmodule. Based on the explanation, the high-level planner asks the LLM to revise the plan and the\nlow-level controller executes subgoals from the revised plan.\nAdditionally, bacause the LLM does not observe the environment, the order of subgoals in a plan can\nbe suboptimal. When some subgoals share the same prerequisite so they can be executed in any order,\nit is more efficient to execute subgoal, which can complete faster than the other subgoals, first. This\ncan prevent wasting time by giving up subgoals that could be completed quickly, pursuing a subgoal\nwhose completion time is uncertain, and then going back to search for the previously quicker task\nagain after executing the subgoal. In DEPS, this procedure called elector module is implemented with\nthe horizon prediction module in GSB (Cai et al., 2023a). The horizon prediction module µ(st, τ g\nt,i)\ntakes the current observation st and the textual subgoals {τ g\nt,i}N\ni=1 and predicts the time to complete\neach subgoal. Based on the time prediction, a subgoal to be executed is sampled from the distribution\nas follows:\nSelector(τ g\nt,i; st, Pt) =\nexp(−µ(st, τ g\nt,i))\nP\nj exp(−µ(st, τ g\nt,j)).\n(3)\n35\nPreprint\nTable 10: Success Rate of two low-level controllers with the Ground-Truth plan for each task.\nTask\nSteve-1\nMrSteve\noak_stairs\n80%\n83%\nsign\n70%\n67%\nfence\n67%\n70%\nbed\n37%\n60%\npainting\n60%\n73%\ncarpet\n43%\n60%\nitem_frame\n53%\n63%\nleather_boots\n13%\n33%\nleather_chestplate\n3%\n17%\nleather_helmet\n20%\n20%\nleather_leggings\n0%\n13%\nWe report success rates of DEPS framework with Steve-1 and MrSteve in Table 9. We use Qwen2.5-\n72B (Team, 2024) as LLM in the high-level planner. DEPS with MrSteve shows comparable\nperformance or outperforms compared to DEPS with Steve-1. Especially, there is a huge performance\ngap between the two low-level controllers in the bed task, which requires killing three sheeps. We\nobserve that when the agent hit a sheep, it runs away from the agent and the agent chases it until\nthe agent gets wool items. After this, Steve-1 easily forgets the place where other sheeps exist\nand try to find sheep. However, MrSteve avoids this redundant exploration utilizing the episodic\nmemory. Hence, this results highlight the importance of episodic memory for low-level controllers in\nLLM-augmented hierarchical agent frameworks.\nIn Table 10, we also report success rates of hierarchical agents with Steve-1 and MrSteve and\nthe optimal plan for each plan. In this setting, because there is no instability from the LLM, the\nperformance is solely determined by performance of low-level controllers. Based on this experiment,\nalthough the goal is optimal, Steve-1 shows lower performance compared to MrSteve, indicating\nlow-level controllers is a performance bottleneck of hierarchical agent frameworks.\n36\nPreprint\nN\nTASK-CONDITIONED HIERARCHICAL EPISODIC EXPLORATION\nTable 11: Performance Comparison of Two Exploration Methods. We report success rates and the average and\nstandard error of execution times for the explore mode in ABA-Sparse tasks.\nTask-Conditioned Exploration\nTask-Free Exploration\nSuccess Rate\nExplore Mode Length\nSuccess Rate\nExplore Mode Length\nBeef-Log-Beef\n93%\n1202.64 ± 67.49\n92%\n1512.00 ± 62.37\nBeef-Leaves-Beef\n90%\n1269.23 ± 66.57\n91%\n1496.10 ± 69.43\nWool-Sand-Wool\n98%\n924.98 ± 56.85\n93%\n1350.00 ± 78.80\nWool-Dirt-Wool\n84%\n1303.21 ± 117.04\n84%\n1452.00 ± 89.92\nMilk-Sand-Milk\n86%\n1197.29 ± 71.44\n83%\n1526.48 ± 63.31\nMilk-Leaves-Milk\n59%\n1224.24 ± 68.79\n62%\n1579.35 ± 64.61\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTimestep\n0.0\n0.1\n0.2\n0.3\n0.4\nVisited Pasture Ratio\nTask-Free\nTask-Conditioned\nFigure 16: Given the task is “find cow”, the ratio of\npastures among the locations explored over time is pre-\nsented for two agents, each employing a different explo-\nration method.\nIn this section, we investigate an advanced ver-\nsion of hierarchical episodic exploration. The\ncurrent exploration method selects the next ex-\nploring position based on the visitation map gen-\nerated from PEM, while it does not use the infor-\nmation inherited in PEM. Hence, the current ex-\nploration method is task-free exploration. While\nnot using information from PEM worked pretty\nwell in practice, using knowledge from PEM\nmay have better explorative behaviors. Thus,\nwe implemented a new task-conditioned explo-\nration method which exploits information stored\nin PEM and evaluated two exploration methods\non exploration and task-solving tasks.\nThe two exploration methods have the same\nprimary goal: to visit the least visited places\nfirst. The difference between the two explo-\nration methods is how to select one of the least-visited places. The task-free exploration method\nrandomly selects the next exploring position among the least-visited locations. In contrast, the\ntask-conditioned exploration method selects the next exploring position among the least-visited\nlocations that are estimated to be related to the task based on the information inherited in PEM.\nTo implement the task-conditioned exploration method, the high-level exploration policy first accesses\nall event clusters when selecting the next exploring position. Using MineCLIP, the task-relevant score\nfor each event cluster is estimated by calculating the alignment score between the center embedding\nof the event cluster and the text prompt. If we use the text prompts for MineCLIP listed in Table 3,\nthe alignment score for locations where the target object has not yet been observed will tend to be\nlow, even if the location is relevant to the task, potentially hindering the exploration of those areas.\nTo address this problem, we used text prompts that describe places related to the task. For instance,\nthe text prompt for log\nis set to “near pasture”, while the text prompt for sand\nis set to “near\ndesert.” After that, Event clusters with alignment scores exceeding the threshold are collected, and a\ntask-relevance map is constructed to represent the agent’s FoVs of these event clusters, similarly to\nthe visitation map building method described in Appendix F.2.\nAdditionally, the 3 × 3 box blur filter is applied to the task-relevance map to introduce an inductive\nbias, assuming that if something is near X, it is likely to also be X. Finally, based on the task-\nrelevance map, the high-level exploration policy selects the next exploring position by prioritizing\nlocations that are the least visited, most task-relevant, and closest to the agent.\nFigure 16 shows the proportion of task-relevant locations among the places explored by the agent,\ndemonstrating that the task-conditioned exploration method explores task-relevant locations more\nquickly in the early stages. We used Map 2 in Figure 8 and set the task as “find cow,” and therefore\nmeasured the proportion of pastures, which are relevant to this task. In the early stages of the episode,\nthe task-conditioned exploration method explored pastures more than the task-free exploration method.\nAfter approximately 2000 steps, however, the exploration tendencies of both methods became similar.\n37\nPreprint\nTable 11 shows performance comparison between the two exploration methods in ABA-Sparse tasks.\nWe report success rates and execution times for the explore mode from MrSteve with the task-free and\ntask-conditioned exploration methods. The success rates between the two exploration methods are\ncomparable, while the task-conditioned exploration is finished earlier than the task-free exploration.\nThrough the results of the two experiments, we confirmed that fully utilizing PEM not only aids in\ntask-solving but also optimizes exploration more effectively.\nO\nABA-Sparse TASKS WITH MEMORY CONSTRAINTS\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.1K\n0.5K\n1K\n2K\n12K\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Sand-Wool\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.1K\n0.5K\n1K\n2K\n12K\nMemory Limit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nSteve-1\nFigure 17: Success Rate Comparison between MrSteve with its memory variants and Steve-1 in ABA-Sparse\ntasks with memory constraints. We tested different memory capacities (0.1K to 12K) for each model. In all tasks,\nthe performance of MrSteve-FM decreases as memory capacity gets smaller. We note that MrSteve is robust\nto memory capacities across tasks, while MrSteve-PM, and MrSteve-EM showed performance degradation in\nBeef-Log-Beef and Beef-Leaves-Beef tasks.\nIn this section, we investigate how MrSteve and its memory variants perform in ABA-Sparse tasks\nin Section 4.2 when memory capacity is limited. We evaluated each model with different memory\ncapacity ranging from 0.1K to 12K which is the maximum episode length. In all tasks, we observe\nthat the performance of MrSteve-FM decreases as the memory capacity gets small. This is because\nFIFO Memory in MrSteve-FM losts relevant frames in first task A while solving task B. While\nMrSteve showed robust performance to constrained memory capacities across tasks, MrSteve-PM,\nand MrSteve-EM showed degraded performances in Beef-Log-Beef and Beef-Leaves-Beef tasks\nwhen memory capacity is 0.1K. This indicates the robustness of PEM to memory capacities in\nABA-Sparse tasks.\nP\nPLACE EVENT MEMORY INVESTIGATION\nAlthough our place event memory enables efficient querying by clustering experience frames, it\nstores not only the center embedding of each event cluster but also all the frames that constitute the\nevent clusters. Since the frames within each cluster contain highly similar information, storing all of\nthem can increase redundancy in the memory system, potentially degrading storage efficiency and\nretrieval performance. Therefore, we attempted to optimize the memory-storing method of PEM in\nthe simplest way possible to investigate whether reducing the storage of redundant information could\nachieve better efficiency.\nWe implemented the modified PEM, which stores only center embeddings. The write and read\noperations of the modified PEM are the same as the original PEM. However, once event clustering\nis executed, the modified PEM stores only the center embedding of each event cluster and removes\nthe embeddings of other frames. In our experiment settings, event clustering is performed when the\ndummy deque of a place cluster has 100 frames, and until then, frames are stored in the dummy\ndeque of a place cluster, with each place cluster capable of holding up to 100 frames. However, even\n38\nPreprint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Dirt-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Log-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Sand-Milk\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nWool-Sand-Wool\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBeef-Leaves-Beef\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMilk-Leaves-Milk\nMrSteve (Center Embed)\nMrSteve\nMrSteve-EM\nMrSteve-PM\nMrSteve-FM\nSteve-1\nFigure 18: Success Rates Comparison between MrSteve with its memory variants and Steve-1 in ABA-Sparse\ntasks. We additionally evaluated MrSteve with modified place event memory, named MrSteve (Center Embed),\nthat stores center embeddings only.\nin such cases, memory read operation is highly optimized by accessing only the oldest frame in each\nplace cluster during querying operations.\nFigure 18 shows the performance comparison between MrSteve with its memory variants and Steve-1\nin ABA-Sparse tasks. Surprisingly, the performance of the simplest optimized PEM, named MrSteve\n(Center Embed), was not dropped significantly. Nevertheless, the computational cost for the querying\noperation can be reduced. After 12K environmental steps, 305.52 event clusters, on average, were\ngenerated across all tasks, with a standard error of 4.98. The original PEM stores all 12K frames and\neach event cluster in PEM holds approximately 40 frames on average. In the end, PEM calculates\nthe alignment scores between frames from the top-30 relevant event clusters and the task instruction,\nresulting in 1.2K comparisons. However, the modified PEM calculates the alignment scores for\naround 0.3K frames, making it more efficient.\nThis experiment result demonstrates the potential to further optimize the PEM memory-storing\nmethod. However, our simplest approach resulted in a slight performance drop. Hence, optimizing\nmemory to further reduce redundancy without losing important frames is a worthwhile direction for\nfuture work.\n39\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory.pdf"}
{"title":"Voyager: An Open-Ended Embodied Agent with Large Language Models","authors":"Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar","summary":"We introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent's abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https:\/\/voyager.minedojo.org\/.","url":"http:\/\/arxiv.org\/abs\/2305.16291v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2305.16291v2","published":1685036798000,"comment":"Project website and open-source codebase:\n  https:\/\/voyager.minedojo.org\/","pdf_text":"VOYAGER: An Open-Ended Embodied Agent\nwith Large Language Models\nGuanzhi Wang1 2 #, Yuqi Xie3, Yunfan Jiang4∗, Ajay Mandlekar1∗,\nChaowei Xiao1 5, Yuke Zhu1 3, Linxi “Jim” Fan1† #, Anima Anandkumar1 2†\n1NVIDIA, 2Caltech, 3UT Austin, 4Stanford, 5UW Madison\n∗Equal contribution\n†Equal advising\n# Corresponding authors\nhttps:\/\/voyager.minedojo.org\nAbstract\nWe introduce VOYAGER, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. VOYAGER consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving complex\nbehaviors, and 3) a new iterative prompting mechanism that incorporates environ-\nment feedback, execution errors, and self-verification for program improvement.\nVOYAGER interacts with GPT-4 via blackbox queries, which bypasses the need for\nmodel parameter fine-tuning. The skills developed by VOYAGER are temporally\nextended, interpretable, and compositional, which compounds the agent’s abilities\nrapidly and alleviates catastrophic forgetting.\nEmpirically, VOYAGER shows\nstrong in-context lifelong learning capability and exhibits exceptional proficiency\nin playing Minecraft. It obtains 3.3× more unique items, travels 2.3× longer\ndistances, and unlocks key tech tree milestones up to 15.3× faster than prior SOTA.\nVOYAGER is able to utilize the learned skill library in a new Minecraft world to\nsolve novel tasks from scratch, while other techniques struggle to generalize.\nFigure 1: VOYAGER discovers new Minecraft items and skills continually by self-driven exploration,\nsignificantly outperforming the baselines. X-axis denotes the number of prompting iterations.\n1\narXiv:2305.16291v2  [cs.AI]  19 Oct 2023\nMine Wood  Log\nMake Crafting Table\nCraft Stone Sword\nCraft Shield\nMake Furnace\nCook Steak\nCombat Zombie\n     Mine Wood Log\nMake Crafting Table\nCombat \nZombie\nMine Diamond\nNew \nTask\nCode as \nActions\nRefine Program\nEnv Feedback\nExecution Errors\nUpdate \nExploration \nProgress\nSkill \nRetrieval\nAdd New Skill\nAutomatic Curriculum\nIterative Prompting Mechanism\nSkill Library\nEnvironment\nSelf-Verification\nFigure 2: VOYAGER consists of three key components: an automatic curriculum for open-ended\nexploration, a skill library for increasingly complex behaviors, and an iterative prompting mechanism\nthat uses code as action space.\n1\nIntroduction\nBuilding generally capable embodied agents that continuously explore, plan, and develop new skills\nin open-ended worlds is a grand challenge for the AI community [1–5]. Classical approaches\nemploy reinforcement learning (RL) [6, 7] and imitation learning [8–10] that operate on primitive\nactions, which could be challenging for systematic exploration [11–15], interpretability [16–18], and\ngeneralization [19–21]. Recent advances in large language model (LLM) based agents harness the\nworld knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable\npolicies [16, 22, 19]. They are applied to embodied tasks like games and robotics [23–27], as well as\nNLP tasks without embodiment [28–30]. However, these agents are not lifelong learners that can\nprogressively acquire, update, accumulate, and transfer knowledge over extended time spans [31, 32].\nLet us consider Minecraft as an example. Unlike most other games studied in AI [33, 34, 10],\nMinecraft does not impose a predefined end goal or a fixed storyline but rather provides a unique\nplayground with endless possibilities [23]. Minecraft requires players to explore vast, procedurally\ngenerated 3D terrains and unlock a tech tree using gathered resources. Human players typically start\nby learning the basics, such as mining wood and cooking food, before advancing to more complex\ntasks like combating monsters and crafting diamond tools. We argue that an effective lifelong learning\nagent should have similar capabilities as human players: (1) propose suitable tasks based on its\ncurrent skill level and world state, e.g., learn to harvest sand and cactus before iron if it finds itself in\na desert rather than a forest; (2) refine skills based on environmental feedback and commit mastered\nskills to memory for future reuse in similar situations (e.g. fighting zombies is similar to fighting\nspiders); (3) continually explore the world and seek out new tasks in a self-driven manner.\nTowards these goals, we introduce VOYAGER, the first LLM-powered embodied lifelong learning\nagent to drive exploration, master a wide range of skills, and make new discoveries continually\nwithout human intervention in Minecraft. VOYAGER is made possible through three key modules\n(Fig. 2): 1) an automatic curriculum that maximizes exploration; 2) a skill library for storing\nand retrieving complex behaviors; and 3) a new iterative prompting mechanism that generates\nexecutable code for embodied control. We opt to use code as the action space instead of low-level\nmotor commands because programs can naturally represent temporally extended and compositional\nactions [16, 22], which are essential for many long-horizon tasks in Minecraft. VOYAGER interacts\nwith a blackbox LLM (GPT-4 [35]) through prompting and in-context learning [36–38]. Our approach\nbypasses the need for model parameter access and explicit gradient-based training or finetuning.\nMore specifically, VOYAGER attempts to solve progressively harder tasks proposed by the automatic\ncurriculum, which takes into account the exploration progress and the agent’s state. The curriculum\nis generated by GPT-4 based on the overarching goal of “discovering as many diverse things as\npossible”. This approach can be perceived as an in-context form of novelty search [39, 40]. VOYAGER\nincrementally builds a skill library by storing the action programs that help solve a task successfully.\n2\nInventory (5\/36): {'oak_planks': 3, 'stick': \n4, 'crafting_table': 1, 'stone': 3, \n'wooden_pickaxe': 1}\nBiome: river\nInventory (4\/36): {'sugar_cane': 3, 'paper': \n3, 'fishing_rod': 1, 'wooden_pickaxe': 1}\nNearby entities: pig, cat, villager\r\nHealth: 12\/20\r\nHunger: 0\/20\nInventory (6\/36): {'furnace': 1, \n'stone_pickaxe': 1, 'oak_planks': 7, \n'crafting_table': 1, 'raw_iron': 4, 'coal': 1} \nTime: night\nNearby entities: zombie\nEquipment: 'stone_sword', 'shield'\nReasoning: Since you have a wooden pickaxe and some stones, it would be \nbeneficial to upgrade your pickaxe to a stone pickaxe for better efficiency.\r\nTask: Craft 1 stone pickaxe.\nReasoning: Since you have a fishing rod in your inventory and you are near a river \nbiome, it would be a good opportunity to catch some fish for food and experience. \nTask: Catch 1 fish.\nReasoning: Your hunger is at 0, which means you need to find food to replenish \nyour hunger. Since there are pigs nearby, you can kill one to obtain raw porkchops.\nTask: Kill 1 pig.\nReasoning: You have raw iron and coal, and you have a furnace. It's time to smelt \nthe iron to make iron ingots, which can be used to craft better tools and armor.\r\nTask: Smelt 4 raw iron.\nReasoning: Since it's night and there's a zombie nearby, it's a good opportunity to \ntry killing the zombie now that you have a stone sword and a shield equipped.\nTask: Kill 1 zombie.\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nFigure 3: Tasks proposed by the automatic curriculum. We only display the partial prompt for brevity.\nSee Appendix, Sec. A.3 for the full prompt structure.\nEach program is indexed by the embedding of its description, which can be retrieved in similar\nsituations in the future. Complex skills can be synthesized by composing simpler programs, which\ncompounds VOYAGER’s capabilities rapidly over time and alleviates catastrophic forgetting in other\ncontinual learning methods [31, 32].\nHowever, LLMs struggle to produce the correct action code consistently in one shot [41]. To address\nthis challenge, we propose an iterative prompting mechanism that: (1) executes the generated\nprogram to obtain observations from the Minecraft simulation (such as inventory listing and nearby\ncreatures) and error trace from the code interpreter (if any); (2) incorporates the feedback into GPT-4’s\nprompt for another round of code refinement; and (3) repeats the process until a self-verification\nmodule confirms the task completion, at which point we commit the program to the skill library (e.g.,\ncraftStoneShovel() and combatZombieWithSword()) and query the automatic curriculum for\nthe next milestone (Fig. 2).\nEmpirically, VOYAGER demonstrates strong in-context lifelong learning capabilities. It can construct\nan ever-growing skill library of action programs that are reusable, interpretable, and generalizable\nto novel tasks. We evaluate VOYAGER systematically against other LLM-based agent techniques\n(e.g., ReAct [29], Reflexion [30], AutoGPT [28]) in MineDojo [23], an open-source Minecraft AI\nframework. VOYAGER outperforms prior SOTA by obtaining 3.3× more unique items, unlocking key\ntech tree milestones up to 15.3× faster, and traversing 2.3× longer distances. We further demonstrate\nthat VOYAGER is able to utilize the learned skill library in a new Minecraft world to solve novel tasks\nfrom scratch, while other methods struggle to generalize.\n2\nMethod\nVOYAGER consists of three novel components: (1) an automatic curriculum (Sec. 2.1) that suggests\nobjectives for open-ended exploration, (2) a skill library (Sec. 2.2) for developing increasingly\ncomplex behaviors, and (3) an iterative prompting mechanism (Sec. 2.3) that generates executable\ncode for embodied control. Full prompts are presented in Appendix, Sec. A.\n2.1\nAutomatic Curriculum\nEmbodied agents encounter a variety of objectives with different complexity levels in open-ended\nenvironments. An automatic curriculum offers numerous benefits for open-ended exploration, ensur-\ning a challenging but manageable learning process, fostering curiosity-driven intrinsic motivation\nfor agents to learn and explore, and encouraging the development of general and flexible problem-\nsolving strategies [42–44]. Our automatic curriculum capitalizes on the internet-scale knowledge\ncontained within GPT-4 by prompting it to provide a steady stream of new tasks or challenges. The\ncurriculum unfolds in a bottom-up fashion, allowing for considerable adaptability and responsiveness\nto the exploration progress and the agent’s current state (Fig. 3). As VOYAGER progresses to harder\nself-driven goals, it naturally learns a variety of skills, such as “mining a diamond”.\n3\nProgram Description\nSkill Library\nTop-5 Relevant Skills\nProgram Generated by GPT-4\nTask: Craft Iron Pickaxe\nKey\nAdd\nRetrieve\nValue\nSkill Library\nQuery\nHow to craft an iron pickaxe in \nMinecraft?\nTo craft an iron pickaxe, you \nneed to 3 iron ingots and 2 \nsticks. Once you have gathered \nthe materials, ....\n----------------------------------\n         Environment Feedback\nMine Wood  Log\nMake Crafting Table\nCraft Wooden Pickaxe\nCraft Stone Sword\nMake Furnace\n...\nCombat Cow\nCook Steak\nCraft Iron Axe\nCombat Zombie\nSmelt Iron Ingot\nCraft Stick\nMake Crafting Table\nMake Furnace\nCraft Wooden Pickaxe\nGPT-3.5\nEmbedding\nEmbedding\nGPT-3.5\nFigure 4: Skill library. Top: Adding a new skill. Each time GPT-4 generates and verifies a new\nskill, we add it to the skill library, represented by a vector database. The key is the embedding vector\nof the program description (generated by GPT-3.5), while the value is the program itself. Bottom:\nSkill retrieval. When faced with a new task proposed by the automatic curriculum, we first leverage\nGPT-3.5 to generate a general suggestion for solving the task, which is combined with environment\nfeedback as the query context. Subsequently, we perform querying to identify the top-5 relevant skills.\nThe input prompt to GPT-4 consists of several components:\n(1) Directives encouraging diverse behaviors and imposing constraints,\nsuch as\n“My ultimate goal is to discover as many diverse things as possible\n...\nThe next task should not be too hard since I may not have the\nnecessary resources or have learned enough skills to complete it\nyet.”;\n(2) The agent’s current state, including inventory, equipment, nearby blocks and entities,\nbiome, time, health and hunger bars, and position;\n(3) Previously completed and failed tasks, reflecting the agent’s current exploration progress\nand capabilities frontier;\n(4) Additional context: We also leverage GPT-3.5 to self-ask questions based on the agent’s\ncurrent state and exploration progress and self-answer questions. We opt to use GPT-3.5\ninstead of GPT-4 for standard NLP tasks due to budgetary considerations.\n2.2\nSkill Library\nWith the automatic curriculum consistently proposing increasingly complex tasks, it is essential to\nhave a skill library that serves as a basis for learning and evolution. Inspired by the generality, inter-\npretability, and universality of programs [45], we represent each skill with executable code that scaf-\nfolds temporally extended actions for completing a specific task proposed by the automatic curriculum.\nThe input prompt to GPT-4 consists of the following components:\n(1) Guidelines\nfor\ncode\ngeneration,\nsuch\nas\n“Your function will be reused\nfor building more complex functions.\nTherefore, you should make\nit generic and reusable.”;\n(2) Control primitive APIs, and relevant skills retrieved from the skill library, which are\ncrucial for in-context learning [36–38] to work well;\n(3) The generated code from the last round, environment feedback, execution errors, and\ncritique, based on which GPT-4 can self-improve (Sec. 2.3);\n(4) The agent’s current state, including inventory, equipment, nearby blocks and entities,\nbiome, time, health and hunger bars, and position;\n4\nI cannot make stick because I need:  2 more planks\nI cannot make stone_shovel because I need:  2 more stick\nthrow new Error(`No item named ${name}`);\nNo item named acacia_axe\nat line 18:await craftItem(bot, \"acacia_axe\", 1);\nEnvironment Feedback\nExecution Error\nGPT-4\nGPT-4\nFigure 5: Left: Environment feedback. GPT-4 realizes it needs 2 more planks before crafting sticks.\nRight: Execution error. GPT-4 realizes it should craft a wooden axe instead of an acacia axe since\nthere is no acacia axe in Minecraft. We only display the partial prompt for brevity. The full prompt\nstructure for code generation is in Appendix, Sec. A.4.\n(5) Chain-of-thought prompting [46] to do reasoning before code generation.\nWe iteratively refine the program through a novel iterative prompting mechanism (Sec. 2.3), in-\ncorporate it into the skill library as a new skill, and index it by the embedding of its description\n(Fig. 4, top). For skill retrieval, we query the skill library with the embedding of self-generated task\nplans and environment feedback (Fig. 4, bottom). By continuously expanding and refining the skill\nlibrary, VOYAGER can learn, adapt, and excel in a wide spectrum of tasks, consistently pushing the\nboundaries of its capabilities in the open world.\n2.3\nIterative Prompting Mechanism\nWe introduce an iterative prompting mechanism for self-improvement through three types of feedback:\n(1) Environment feedback, which illustrates the intermediate progress of program execution\n(Fig. 5, left). For example, “I cannot make an iron chestplate because I need:\n7 more iron ingots” highlights the cause of failure in crafting an iron chestplate. We use\nbot.chat() inside control primitive APIs to generate environment feedback and prompt\nGPT-4 to use this function as well during code generation;\n(2) Execution errors from the program interpreter that reveal any invalid operations or syntax\nerrors in programs, which are valuable for bug fixing (Fig. 5, right);\n(3) Self-verification for checking task success. Instead of manually coding success checkers\nfor each new task proposed by the automatic curriculum, we instantiate another GPT-4\nagent for self-verification. By providing VOYAGER’s current state and the task to GPT-4,\nwe ask it to act as a critic [47–49] and inform us whether the program achieves the task.\nIn addition, if the task fails, it provides a critique by suggesting how to complete the task\n(Fig. 6). Hence, our self-verification is more comprehensive than self-reflection [30] by both\nchecking success and reflecting on mistakes.\nDuring each round of code generation, we execute the generated program to obtain environment\nfeedback and execution errors from the code interpreter, which are incorporated into GPT-4’s prompt\nfor the next round of code refinement. This iterative process repeats until self-verification validates\n5\nInventory (8\/36): {'oak_planks': 5, 'cobblestone': 2, \n'porkchop': 2, 'wooden_sword': 1, 'coal': 5, 'wooden_pickaxe': \n1, 'oak_log': 3, 'dirt': 9}\nTask: Mine 5 coal ores\nInventory (10\/36): {'raw_copper': 9, 'copper_ingot': 3, \n'acacia_planks': 1, 'raw_iron': 1, 'stick': 1, 'iron_sword': 1, \n'iron_pickaxe': 1, 'iron_ingot': 3, 'crafting_table': 1, 'furnace': 1}\nTask: Craft a spyglass\nInventory (7\/36): {'oak_log': 5, 'oak_planks': 1, \n'wooden_pickaxe': 1, 'wooden_sword': 1, 'porkchop': 2, \n'white_wool': 2, 'mutton': 6}\nTask: Kill 3 sheep\nInventory (9\/36): {'string': 4, 'coal': 1, 'rotten_flesh': 1, \n'iron_sword': 1, 'furnace': 1, 'dirt': 6, 'stone_shovel': 1, \n'wooden_pickaxe': 1, 'granite': 5}\nTask: Kill 1 zombie\nReasoning: Mining coal_ore in Minecraft will get coal. You have 5 coal in your \ninventory.\nSuccess: True\nReasoning: To craft a spyglass, you need 2 copper ingots and 1 amethyst shard. \nYou have 3 copper ingots, but you don't have any amethyst shards.\nSuccess: False\nCritique: Find and mine an amethyst shard underground.\nReasoning: You have 2 white_wool and 6 mutton in your inventory, which indicates \nthat you killed 2 sheep. You needed to kill 3 sheep.\nSuccess: False\nCritique: Find and kill one more sheep to complete the task.\nReasoning: You have 1 rotten_flesh in your inventory, which means you have killed \nat least 1 zombie.\nSuccess: True\nGPT-4\nGPT-4\nGPT-4\nGPT-4\nFigure 6: Self-verification examples. We only display the partial prompt for brevity. See Appendix,\nSec. A.5 for the full prompt structure.\nthe task’s completion, at which point we add this new skill to the skill library and ask the automatic\ncurriculum for a new objective (Fig. 2). If the agent gets stuck after 4 rounds of code generation, then\nwe query the curriculum for another task. This iterative prompting approach significantly improves\nprogram synthesis for embodied control, enabling VOYAGER to continuously acquire diverse skills\nwithout human intervention.\n3\nExperiments\n3.1\nExperimental Setup\nWe leverage OpenAI’s gpt-4-0314 [35] and gpt-3.5-turbo-0301 [50] APIs for text completion,\nalong with text-embedding-ada-002 [51] API for text embedding. We set all temperatures to\n0 except for the automatic curriculum, which uses temperature = 0.1 to encourage task diversity. Our\nsimulation environment is built on top of MineDojo [23] and leverages Mineflayer [52] JavaScript\nAPIs for motor controls. See Appendix, Sec. B.1 for more details.\n3.2\nBaselines\nBecause there is no LLM-based agents that work out of the box for Minecraft, we make our best\neffort to select a number of representative algorithms as baselines. These methods are originally\ndesigned only for NLP tasks without embodiment, therefore we have to re-interpret them to be\nexecutable in MineDojo and compatible with our experimental setting:\nReAct [29] uses chain-of-thought prompting [46] by generating both reasoning traces and action\nplans with LLMs. We provide it with our environment feedback and the agent states as observations.\nReflexion [30] is built on top of ReAct [29] with self-reflection to infer more intuitive future actions.\nWe provide it with execution errors and our self-verification module.\nAutoGPT [28] is a popular software tool that automates NLP tasks by decomposing a high-level\ngoal into multiple subgoals and executing them in a ReAct-style loop. We re-implement AutoGPT\nby using GPT-4 to do task decomposition and provide it with the agent states, environment feedback,\nand execution errors as observations for subgoal execution. Compared with VOYAGER, AutoGPT\nlacks the skill library for accumulating knowledge, self-verification for assessing task success, and\nautomatic curriculum for open-ended exploration.\nNote that we do not directly compare with prior methods that take Minecraft screen pixels as input\nand output low-level controls [53–55]. It would not be an apple-to-apple comparison, because we rely\non the high-level Mineflayer [52] API to control the agent. Our work’s focus is on pushing the limits\nof GPT-4 for lifelong embodied agent learning, rather than solving the 3D perception or sensorimotor\ncontrol problems. VOYAGER is orthogonal and can be combined with gradient-based approaches like\n6\nTable 1: Tech tree mastery. Fractions indicate the number of successful trials out of three total runs.\n0\/3 means the method fails to unlock a level of the tech tree within the maximal prompting iterations\n(160). Numbers are prompting iterations averaged over three trials. The fewer the iterations, the\nmore efficient the method.\nMethod\nWooden Tool\nStone Tool\nIron Tool\nDiamond Tool\nReAct [29]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nReflexion [30]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28]\n92 ± 72 (3\/3)\n94 ± 72 (3\/3)\n135 ± 103 (3\/3)\nN\/A (0\/3)\nVOYAGER w\/o Skill Library\n7 ± 2 (3\/3)\n9 ± 4 (3\/3)\n29 ± 11 (3\/3)\nN\/A (0\/3)\nVOYAGER (Ours)\n6 ± 2 (3\/3)\n11 ± 2 (3\/3)\n21 ± 7 (3\/3)\n102 (1\/3)\nFigure 7: Map coverage: bird’s eye views of Minecraft maps. VOYAGER is able to traverse 2.3×\nlonger distances compared to baselines while crossing diverse terrains.\nVPT [8] as long as the controller provides a code API. We make a system-level comparison between\nVOYAGER and prior Minecraft agents in Table. A.2.\n3.3\nEvaluation Results\nWe systematically evaluate VOYAGER and baselines on their exploration performance, tech tree\nmastery, map coverage, and zero-shot generalization capability to novel tasks in a new world.\nSignificantly better exploration.\nResults of exploration performance are shown in Fig. 1.\nVOYAGER’s superiority is evident in its ability to consistently make new strides, discovering 63\nunique items within 160 prompting iterations, 3.3× many novel items compared to its counterparts.\nOn the other hand, AutoGPT lags considerably in discovering new items, while ReAct and Reflexion\nstruggle to make significant progress, given the abstract nature of the open-ended exploration goal\nthat is challenging to execute without an appropriate curriculum.\nConsistent tech tree mastery. The Minecraft tech tree tests the agent’s ability to craft and use a\nhierarchy of tools. Progressing through this tree (wooden tool →stone tool →iron tool →diamond\ntool) requires the agent to master systematic and compositional skills. Compared with baselines,\nVOYAGER unlocks the wooden level 15.3× faster (in terms of the prompting iterations), the stone\nlevel 8.5× faster, the iron level 6.4× faster, and VOYAGER is the only one to unlock the diamond level\nof the tech tree (Fig. 2 and Table. 1). This underscores the effectiveness of the automatic curriculum,\nwhich consistently presents challenges of suitable complexity to facilitate the agent’s progress.\nExtensive map traversal. VOYAGER is able to navigate distances 2.3× longer compared to baselines\nby traversing a variety of terrains, while the baseline agents often find themselves confined to local\nareas, which significantly hampers their capacity to discover new knowledge (Fig. 7).\n7\nTable 2: Zero-shot generalization to unseen tasks. Fractions indicate the number of successful\ntrials out of three total attempts. 0\/3 means the method fails to solve the task within the maximal\nprompting iterations (50). Numbers are prompting iterations averaged over three trials. The fewer\nthe iterations, the more efficient the method.\nMethod\nDiamond Pickaxe\nGolden Sword\nLava Bucket\nCompass\nReAct [29]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nReflexion [30]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28]\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nN\/A (0\/3)\nAutoGPT [28] w\/ Our Skill Library\n39 (1\/3)\n30 (1\/3)\nN\/A (0\/3)\n30 (2\/3)\nVOYAGER w\/o Skill Library\n36 (2\/3)\n30 ± 9 (3\/3)\n27 ± 9 (3\/3)\n26 ± 3 (3\/3)\nVOYAGER (Ours)\n19 ± 3 (3\/3)\n18 ± 7 (3\/3)\n21 ± 5 (3\/3)\n18 ± 2 (3\/3)\nFigure 8: Zero-shot generalization to unseen tasks. We visualize the intermediate progress of each\nmethod on two tasks. See Appendix, Sec. B.4.3 for the other two tasks. We do not plot ReAct and\nReflexion since they do not make any meaningful progress.\nEfficient zero-shot generalization to unseen tasks. To evaluate zero-shot generalization, we clear\nthe agent’s inventory, reset it to a newly instantiated world, and test it with unseen tasks. For both\nVOYAGER and AutoGPT, we utilize GPT-4 to break down the task into a series of subgoals. Table. 2\nand Fig. 8 show VOYAGER can consistently solve all the tasks, while baselines cannot solve any task\nwithin 50 prompting iterations. What’s interesting to note is that our skill library constructed from\nlifelong learning not only enhances VOYAGER’s performance but also gives a boost to AutoGPT.\nThis demonstrates that the skill library serves as a versatile tool that can be readily employed by other\nmethods, effectively acting as a plug-and-play asset to enhance performance.\n3.4\nAblation Studies\nWe ablate 6 design choices (automatic curriculum, skill library, environment feedback, execution\nerrors, self-verification, and GPT-4 for code generation) in VOYAGER and study their impact on\nexploration performance (see Appendix, Sec. B.3 for details of each ablated variant). Results are\nshown in Fig. 9. We highlight the key findings below:\n• Automatic curriculum is crucial for the agent’s consistent progress. The discovered item\ncount drops by 93% if the curriculum is replaced with a random one, because certain tasks\nmay be too challenging if attempted out of order. On the other hand, a manually designed\ncurriculum requires significant Minecraft-specific expertise, and does not take into account\nthe agent’s live situation. It falls short in the experimental results compared to our automatic\ncurriculum.\n• VOYAGER w\/o skill library exhibits a tendency to plateau in the later stages. This\nunderscores the pivotal role that the skill library plays in VOYAGER. It helps create more\ncomplex actions and steadily pushes the agent’s boundaries by encouraging new skills to be\nbuilt upon older ones.\n8\nFigure 9: Left: Ablation studies for the automatic curriculum, skill library, and GPT-4. GPT-3.5\nmeans replacing GPT-4 with GPT-3.5 for code generation. VOYAGER outperforms all the alternatives,\ndemonstrating the critical role of each component. Right: Ablation studies for the iterative\nprompting mechanism. VOYAGER surpasses all the other options, thereby highlighting the essential\nsignificance of each type of feedback in the iterative prompting mechanism.\nFigure 10: VOYAGER builds 3D structures with human feedback. The progress of building designs\nthat integrate human input is demonstrated from left to right.\n• Self-verification is the most important among all the feedback types. Removing the\nmodule leads to a significant drop (−73%) in the discovered item count. Self-verification\nserves as a critical mechanism to decide when to move on to a new task or reattempt a\npreviously unsuccessful task.\n• GPT-4 significantly outperforms GPT-3.5 in code generation and obtains 5.7× more\nunique items, as GPT-4 exhibits a quantum leap in coding abilities. This finding corroborates\nrecent studies in the literature [56, 57].\n3.5\nMultimodal Feedback from Humans\nVOYAGER does not currently support visual perception, because the available version of GPT-4 API\nis text-only at the time of this writing. However, VOYAGER has the potential to be augmented by\nmultimodal perception models [58, 59] to achieve more impressive tasks. We demonstrate that given\nhuman feedback, VOYAGER is able to construct complex 3D structures in Minecraft, such as a Nether\nPortal and a house (Fig. 10). There are two ways to integrate human feedback:\n(1) Human as a critic (equivalent to VOYAGER’s self-verification module): humans provide\nvisual critique to VOYAGER, allowing it to modify the code from the previous round. This\nfeedback is essential for correcting certain errors in the spatial details of a 3D structure that\nVOYAGER cannot perceive directly.\n(2) Human as a curriculum (equivalent to VOYAGER’s automatic curriculum module): humans\nbreak down a complex building task into smaller steps, guiding VOYAGER to complete them\nincrementally. This approach improves VOYAGER’s ability to handle more sophisticated 3D\nconstruction tasks.\n9\n4\nLimitations and Future Work\nCost. The GPT-4 API incurs significant costs. It is 15× more expensive than GPT-3.5. Nevertheless,\nVOYAGER requires the quantum leap in code generation quality from GPT-4 (Fig. 9), which GPT-3.5\nand open-source LLMs cannot provide [60].\nInaccuracies. Despite the iterative prompting mechanism, there are still cases where the agent gets\nstuck and fails to generate the correct skill. The automatic curriculum has the flexibility to reattempt\nthis task at a later time. Occasionally, self-verification module may also fail, such as not recognizing\nspider string as a success signal of beating a spider.\nHallucinations. The automatic curriculum occasionally proposes unachievable tasks. For example, it\nmay ask the agent to craft a “copper sword\" or “copper chestplate\", which are items that do not exist\nwithin the game. Hallucinations also occur during the code generation process. For instance, GPT-4\ntends to use cobblestone as a fuel input, despite being an invalid fuel source in the game. Additionally,\nit may call functions absent in the provided control primitive APIs, leading to code execution errors.\nWe are confident that improvements in the GPT API models as well as novel techniques for finetuning\nopen-source LLMs will overcome these limitations in the future.\n5\nRelated work\nDecision-making Agents in Minecraft.\nMinecraft is an open-ended 3D world with incredibly\nflexible game mechanics supporting a broad spectrum of activities. Built upon notable Minecraft\nbenchmarks [23, 61–65], Minecraft learning algorithms can be divided into two categories: 1)\nLow-level controller: Many prior efforts leverage hierarchical reinforcement learning to learn from\nhuman demonstrations [66–68]. Kanitscheider et al. [14] design a curriculum based on success rates,\nbut its objectives are limited to curated items. MineDojo [23] and VPT [8] utilize YouTube videos\nfor large-scale pre-training. DreamerV3 [69], on the other hand, learns a world model to explore\nthe environment and collect diamonds. 2) High-level planner: Volum et al. [70] leverage few-shot\nprompting with Codex [41] to generate executable policies, but they require additional human\ninteraction. Recent works leverage LLMs as a high-level planner in Minecraft by decomposing\na high-level task into several subgoals following Minecraft recipes [55, 53, 71], thus lacking full\nexploration flexibility. Like these latter works, VOYAGER also uses LLMs as a high-level planner by\nprompting GPT-4 and utilizes Mineflayer [52] as a low-level controller following Volum et al. [70].\nUnlike prior works, VOYAGER employs an automatic curriculum that unfolds in a bottom-up manner,\ndriven by curiosity, and therefore enables open-ended exploration.\nLarge Language Models for Agent Planning.\nInspired by the strong emergent capabilities of\nLLMs, such as zero-shot prompting and complex reasoning [72, 37, 38, 36, 73, 74], embodied agent\nresearch [75–78] has witnessed a significant increase in the utilization of LLMs for planning purposes.\nRecent efforts can be roughly classified into two groups. 1) Large language models for robot\nlearning: Many prior works apply LLMs to generate subgoals for robot planning [27, 27, 25, 79, 80].\nInner Monologue [26] incorporates environment feedback for robot planning with LLMs. Code as\nPolicies [16] and ProgPrompt [22] directly leverage LLMs to generate executable robot policies.\nVIMA [19] and PaLM-E [59] fine-tune pre-trained LLMs to support multimodal prompts. 2)\nLarge language models for text agents: ReAct [29] leverages chain-of-thought prompting [46] and\ngenerates both reasoning traces and task-specific actions with LLMs. Reflexion [30] is built upon\nReAct [29] with self-reflection to enhance reasoning. AutoGPT [28] is a popular tool that automates\nNLP tasks by crafting a curriculum of multiple subgoals for completing a high-level goal while\nincorporating ReAct [29]’s reasoning and acting loops. DERA [81] frames a task as a dialogue\nbetween two GPT-4 [35] agents. Generative Agents [82] leverages ChatGPT [50] to simulate human\nbehaviors by storing agents’ experiences as memories and retrieving those for planning, but its agent\nactions are not executable. SPRING [83] is a concurrent work that uses GPT-4 to extract game\nmechanics from game manuals, based on which it answers questions arranged in a directed acyclic\ngraph and predicts the next action. All these works lack a skill library for developing more complex\nbehaviors, which are crucial components for the success of VOYAGER in lifelong learning.\nCode Generation with Execution.\nCode generation has been a longstanding challenge in\nNLP [41, 84, 85, 73, 37], with various works leveraging execution results to improve program\n10\nsynthesis. Execution-guided approaches leverage intermediate execution outcomes to guide program\nsearch [86–88]. Another line of research utilizes majority voting to choose candidates based on their\nexecution performance [89, 90]. Additionally, LEVER [91] trains a verifier to distinguish and reject\nincorrect programs based on execution results. CLAIRIFY [92], on the other hand, generates code\nfor planning chemistry experiments and makes use of a rule-based verifier to iteratively provide\nerror feedback to LLMs. VOYAGER distinguishes itself from these works by integrating environment\nfeedback, execution errors, and self-verification (to assess task success) into an iterative prompting\nmechanism for embodied control.\n6\nConclusion\nIn this work, we introduce VOYAGER, the first LLM-powered embodied lifelong learning agent,\nwhich leverages GPT-4 to explore the world continuously, develop increasingly sophisticated skills,\nand make new discoveries consistently without human intervention. VOYAGER exhibits superior\nperformance in discovering novel items, unlocking the Minecraft tech tree, traversing diverse terrains,\nand applying its learned skill library to unseen tasks in a newly instantiated world. VOYAGER serves\nas a starting point to develop powerful generalist agents without tuning the model parameters.\n7\nBroader Impacts\nOur research is conducted within Minecraft, a safe and harmless 3D video game environment. While\nVOYAGER is designed to be generally applicable to other domains, such as robotics, its application to\nphysical robots would require additional attention and the implementation of safety constraints by\nhumans to ensure responsible and secure deployment.\n8\nAcknowledgements\nWe are extremely grateful to Ziming Zhu, Kaiyu Yang, Rafał Kocielnik, Colin White, Or Sharir, Sahin\nLale, De-An Huang, Jean Kossaifi, Yuncong Yang, Charles Zhang, Minchao Huang, and many other\ncolleagues and friends for their helpful feedback and insightful discussions. This work is done during\nGuanzhi Wang’s internship at NVIDIA. Guanzhi Wang is supported by the Kortschak fellowship in\nComputing and Mathematical Sciences at Caltech.\nReferences\n[1] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti,\nDaniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d\nenvironment for visual ai. arXiv preprint arXiv: Arxiv-1712.05474, 2017.\n[2] Manolis Savva, Jitendra Malik, Devi Parikh, Dhruv Batra, Abhishek Kadian, Oleksandr\nMaksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, and Vladlen\nKoltun. Habitat: A platform for embodied AI research. In 2019 IEEE\/CVF International\nConference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,\n2019, pages 9338–9346. IEEE, 2019.\n[3] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martín-Martín. robosuite: A mod-\nular simulation framework and benchmark for robot learning. arXiv preprint arXiv: Arxiv-\n2009.12293, 2020.\n[4] Fei Xia, William B. Shen, Chengshu Li, Priya Kasimbeg, Micael Tchapmi, Alexander Toshev,\nLi Fei-Fei, Roberto Martín-Martín, and Silvio Savarese. Interactive gibson benchmark (igibson\n0.5): A benchmark for interactive navigation in cluttered environments. arXiv preprint arXiv:\nArxiv-1910.14442, 2019.\n[5] Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia\nPérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent\nVainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: a simulation environment for\ninteractive tasks in large realistic scenes. arXiv preprint arXiv: Arxiv-2012.02924, 2020.\n11\n[6] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238–1274, 2013.\n[7] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep\nreinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26–38, 2017.\n[8] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. arXiv preprint arXiv: Arxiv-2206.11795, 2022.\n[9] DeepMind Interactive Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico\nCarnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Mansi Gupta, Tim\nHarley, Felix Hill, Peter C Humphreys, Alden Hung, Jessica Landon, Timothy Lillicrap, Hamza\nMerzic, Alistair Muldal, Adam Santoro, Guy Scully, Tamara von Glehn, Greg Wayne, Nathaniel\nWong, Chen Yan, and Rui Zhu. Creating multimodal interactive agents with imitation and\nself-supervised learning. arXiv preprint arXiv: Arxiv-2112.03763, 2021.\n[10] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wo-\njciech M Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al.\nAlphastar: Mastering the real-time strategy game starcraft ii. DeepMind blog, 2, 2019.\n[11] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore:\na new approach for hard-exploration problems. arXiv preprint arXiv: Arxiv-1901.10995, 2019.\n[12] Joost Huizinga and Jeff Clune. Evolving multimodal robot behavior via many stepping stones\nwith the combinatorial multiobjective evolutionary algorithm.\nEvolutionary computation,\n30(2):131–164, 2022.\n[13] Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth O.\nStanley. Enhanced POET: open-ended reinforcement learning through unbounded invention of\nlearning challenges and their solutions. In Proceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of\nMachine Learning Research, pages 9940–9951. PMLR, 2020.\n[14] Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton,\nRaul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, Oleg Klimov, and Jeff\nClune. Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft.\narXiv preprint arXiv: Arxiv-2106.14876, 2021.\n[15] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre M. Bayen, Stuart Russell, Andrew\nCritch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised\nenvironment design. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina\nBalcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual, 2020.\n[16] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence,\nand Andy Zeng. Code as policies: Language model programs for embodied control. arXiv\npreprint arXiv: Arxiv-2209.07753, 2022.\n[17] Shao-Hua Sun, Te-Lin Wu, and Joseph J. Lim. Program guided agent. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net, 2020.\n[18] Zelin Zhao, Karan Samel, Binghong Chen, and Le Song. Proto: Program-guided transformer for\nprogram-guided tasks. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy\nLiang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pages 17021–17036, 2021.\n[19] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen,\nLi Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi (Jim) Fan. Vima: General robot manipu-\nlation with multimodal prompts. ARXIV.ORG, 2022.\n12\n[20] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\nmanipulation. arXiv preprint arXiv: Arxiv-2109.12098, 2021.\n[21] Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke Zhu, and Animashree\nAnandkumar. SECANT: self-expert cloning for zero-shot generalization of visual policies. In\nMarina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of\nMachine Learning Research, pages 3088–3099. PMLR, 2021.\n[22] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay,\nDieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task\nplans using large language models. arXiv preprint arXiv: Arxiv-2209.11302, 2022.\n[23] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended\nembodied agents with internet-scale knowledge. arXiv preprint arXiv: Arxiv-2206.08853, 2022.\n[24] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek\nPurohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence.\nSocratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint\narXiv: Arxiv-2204.00598, 2022.\n[25] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey,\nSally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei\nLee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao,\nKanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,\nAlexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan\nYan. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:\nArxiv-2204.01691, 2022.\n[26] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas\nJackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue:\nEmbodied reasoning through planning with language models. arXiv preprint arXiv: Arxiv-\n2207.05608, 2022.\n[27] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-\nshot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning Research, pages 9118–9147. PMLR, 2022.\n[28] Significant-gravitas\/auto-gpt: An experimental open-source attempt to make gpt-4 fully au-\ntonomous., 2023.\n[29] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:\nArxiv-2210.03629, 2022.\n[30] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamic memory and self-reflection. arXiv preprint arXiv: Arxiv-2303.11366, 2023.\n[31] German Ignacio Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter.\nContinual lifelong learning with neural networks: A review. Neural Networks, 113:54–71, 2019.\n[32] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual\nlearning: Theory, method and application. arXiv preprint arXiv: Arxiv-2302.00487, 2023.\n[33] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv: Arxiv-1312.5602, 2013.\n13\n[34] OpenAI, :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław\nD˛ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józe-\nfowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto,\nJonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya\nSutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv: Arxiv-1912.06680, 2019.\n[35] OpenAI. Gpt-4 technical report. arXiv preprint arXiv: Arxiv-2303.08774, 2023.\n[36] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels. arXiv preprint arXiv: Arxiv-2206.07682, 2022.\n[37] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.\n[39] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you\nneed: Learning skills without a reward function. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[40] Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley,\nand Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via\na population of novelty-seeking agents. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,\nKristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural\nInformation Processing Systems 31: Annual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 5032–5043, 2018.\n[41] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul\nPuri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad\nBavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra,\nEvan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,\nPeter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. Evaluating large language models trained on code. arXiv preprint arXiv: Arxiv-\n2107.03374, 2021.\n[42] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired open-ended trailblazer\n(poet): Endlessly generating increasingly complex and diverse learning environments and their\nsolutions. arXiv preprint arXiv: Arxiv-1901.01753, 2019.\n[43] Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, and Pierre-Yves Oudeyer. Auto-\nmatic curriculum learning for deep RL: A short survey. In Christian Bessiere, editor, Proceedings\nof the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages\n4819–4825. ijcai.org, 2020.\n14\n[44] Sébastien Forestier, Rémy Portelas, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically\nmotivated goal exploration processes with automatic curriculum learning. The Journal of\nMachine Learning Research, 23(1):6818–6858, 2022.\n[45] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales,\nLuke Hewitt, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Growing\ngeneralizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv\npreprint arXiv: Arxiv-2006.08381, 2020.\n[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv: Arxiv-2201.11903, 2022.\n[47] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep rein-\nforcement learning. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings\nof the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY,\nUSA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages\n1928–1937. JMLR.org, 2016.\n[48] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv: Arxiv-1707.06347, 2017.\n[49] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.\nIn Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Repre-\nsentations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings,\n2016.\n[50] Introducing chatgpt, 2022.\n[51] New and improved embedding model, 2022.\n[52] PrismarineJS. Prismarinejs\/mineflayer: Create minecraft bots with a powerful, stable, and high\nlevel javascript api., 2013.\n[53] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hanna Hajishirzi, Sameer\nSingh, and Roy Fox. Do embodied agents dream of pixelated sheep?: Embodied decision\nmaking using language guided world modelling. ARXIV.ORG, 2023.\n[54] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task\ncontrol through goal-aware representation learning and adaptive horizon prediction. arXiv\npreprint arXiv: Arxiv-2301.10034, 2023.\n[55] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and\nselect: Interactive planning with large language models enables open-world multi-task agents.\narXiv preprint arXiv: Arxiv-2302.01560, 2023.\n[56] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith gpt-4. arXiv preprint arXiv: Arxiv-2303.12712, 2023.\n[57] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He,\nAntong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Dajiang Zhu, Xiang Li, Ning Qiang,\nDingang Shen, Tianming Liu, and Bao Ge. Summary of chatgpt\/gpt-4 research and perspective\ntowards the future of large language models. arXiv preprint arXiv: Arxiv-2304.01852, 2023.\n[58] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar.\nPrismer: A vision-language model with an ensemble of experts. arXiv preprint arXiv: Arxiv-\n2303.02506, 2023.\n15\n[59] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,\nPierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc\nToussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied\nmultimodal language model. arXiv preprint arXiv: Arxiv-2303.03378, 2023.\n[60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv: Arxiv-2302.13971, 2023.\n[61] William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations.\nIn Sarit Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on\nArtificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 2442–2448.\nijcai.org, 2019.\n[62] William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie\nMilani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin,\nManuela Veloso, and Phillip Wang. The minerl 2019 competition on sample efficient re-\ninforcement learning using human priors. arXiv preprint arXiv: Arxiv-1904.10079, 2019.\n[63] William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno,\nCrissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov,\nJohn Schulman, Shinya Shiroshita, Nicholay Topin, Avinash Ummadisingu, and Oriol Vinyals.\nThe minerl 2020 competition on sample efficient reinforcement learning using human priors.\narXiv preprint arXiv: Arxiv-2101.11071, 2021.\n[64] Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Jun-\nyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue Huang, Haicheng\nChen, Guangjun Zeng, Yue Lin, Vincent Micheli, Eloi Alonso, François Fleuret, Alexander\nNikulin, Yury Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond 2021\ncompetition: Overview, results, and lessons learned. arXiv preprint arXiv: Arxiv-2202.10583,\n2022.\n[65] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for\nartificial intelligence experimentation. In Subbarao Kambhampati, editor, Proceedings of the\nTwenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York,\nNY, USA, 9-15 July 2016, pages 4246–4247. IJCAI\/AAAI Press, 2016.\n[66] Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang. Juewu-mc: Playing\nminecraft with sample-efficient hierarchical reinforcement learning. arXiv preprint arXiv:\nArxiv-2112.04907, 2021.\n[67] Hangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye\nHao, Dong Li, and Pingzhong Tang. Seihai: A sample-efficient hierarchical ai for the minerl\ncompetition. arXiv preprint arXiv: Arxiv-2111.08857, 2021.\n[68] Alexey Skrynnik, Aleksey Staroverov, Ermek Aitygulov, Kirill Aksenov, Vasilii Davydov, and\nAleksandr I. Panov. Hierarchical deep q-network from imperfect demonstrations in minecraft.\nCogn. Syst. Res., 65:74–78, 2021.\n[69] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv: Arxiv-2301.04104, 2023.\n[70] Ryan Volum, Sudha Rao, Michael Xu, Gabriel DesGarennes, Chris Brockett, Benjamin\nVan Durme, Olivia Deng, Akanksha Malhotra, and Bill Dolan. Craft an iron sword: Dy-\nnamically generating interactive game characters by prompting large language models tuned on\ncode. In Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay\n2022), pages 25–43, Seattle, United States, 2022. Association for Computational Linguistics.\n[71] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv\npreprint arXiv: 2303.16563, 2023.\n16\n[72] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,\nJared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas\nIcard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling,\nFereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi,\nAnanya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa\nLi, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr,\nIsabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi\nRaghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\nRyan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang,\nWilliam Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga,\nJiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia\nZheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models.\narXiv preprint arXiv: Arxiv-2108.07258, 2021.\n[73] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint arXiv: Arxiv-2204.02311, 2022.\n[74] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov,\nEd H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\nScaling instruction-finetuned language models. arXiv preprint arXiv: Arxiv-2210.11416, 2022.\n[75] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied\nAI: from simulators to research tasks. IEEE Trans. Emerg. Top. Comput. Intell., 6(2):230–244,\n2022.\n[76] Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun,\nSergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, and Hao Su.\nRearrangement: A challenge for embodied ai. arXiv preprint arXiv: Arxiv-2011.01975, 2020.\n[77] Harish Ravichandar, Athanasios S Polydoros, Sonia Chernova, and Aude Billard. Recent\nadvances in robot learning from demonstration.\nAnnual review of control, robotics, and\nautonomous systems, 3:297–330, 2020.\n[78] Jack Collins, Shelvin Chand, Anthony Vanderkop, and David Howard. A review of physics\nsimulators for robotic applications. IEEE Access, 9:51416–51431, 2021.\n[79] So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and R. Salakhutdi-\nnov. Film: Following instructions in language with modular methods. International Conference\non Learning Representations, 2021.\n17\n[80] Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. A persistent spatial\nsemantic representation for high-level natural language instruction execution. In 5th Annual\nConference on Robot Learning, 2021.\n[81] Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan. Dera: Enhancing large\nlanguage model completions with dialog-enabled resolving agents. arXiv preprint arXiv:\nArxiv-2303.17071, 2023.\n[82] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv\npreprint arXiv: Arxiv-2304.03442, 2023.\n[83] Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos\nAzaria, Tom Mitchell, and Yuanzhi Li. Spring: Gpt-4 out-performs rl algorithms by studying\npapers and reasoning. arXiv preprint arXiv: 2305.15486, 2023.\n[84] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint arXiv:\nArxiv-2203.13474, 2022.\n[85] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. Coderl:\nMastering code generation through pretrained models and deep reinforcement learning. arXiv\npreprint arXiv: Arxiv-2207.01780, 2022.\n[86] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net, 2019.\n[87] Xinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis.\narXiv preprint arXiv: Arxiv-2107.00101, 2021.\n[88] Kevin Ellis, Maxwell I. Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-\nLezama. Write, execute, assess: Program synthesis with a REPL. In Hanna M. Wallach, Hugo\nLarochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett,\neditors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 9165–9174, 2019.\n[89] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,\nCyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl,\nSven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level\ncode generation with alphacode. arXiv preprint arXiv: Arxiv-2203.07814, 2022.\n[90] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and\nJohn Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv: Arxiv-\n2110.14168, 2021.\n[91] Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen tau Yih, Sida I. Wang, and\nXi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. arXiv\npreprint arXiv: Arxiv-2302.08468, 2023.\n[92] Marta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen,\nKourosh Darvish, Alán Aspuru-Guzik, Florian Shkurti, and Animesh Garg. Errors are useful\nprompts: Instruction guided task programming with verifier-assisted iterative prompting. arXiv\npreprint arXiv: Arxiv-2303.14100, 2023.\n18\nA\nMethod\nA.1\nVOYAGER Algorithm\nPseudocode 1: VOYAGER algorithm.\ndef\nvoyager(\nenvironment ,\n# environment\nthat uses code as action\nspace\ncurriculum_agent ,\n# curriculum\nagent for\nproposing\nthe next task\naction_agent ,\n# action\nagent for code\ngeneration\ncritic_agent ,\n# critic\nagent for self -verification\nskill_manager ,\n# skill\nmanager\nfor adding new skills and skill\nretrieval\n):\nagent_state = environment.reset ()\nwhile\nTrue:\nexploration_progress = (\ncurriculum_agent . get_exploration_progress (\ncurriculum_agent . get_completed_tasks (),\ncurriculum_agent . get_failed_tasks (),\n)\n)\ntask = curriculum_agent . propose_next_task (\nagent_state , exploration_progress\n)\ncode = None\nenvironment_feedback = None\nexecution_errors = None\ncritique = None\nsuccess = False\n# try at most 4 rounds\nbefore\nmoving on to the next task\nfor i in range (4):\nskills = skill_manager . retrieve_skills (\ntask , environment_feedback\n)\ncode = action_agent. generate_code (\ntask ,\ncode ,\nenvironment_feedback ,\nexecution_errors ,\ncritique ,\nskills ,\n)\n(\nagent_state ,\nenvironment_feedback ,\nexecution_errors ,\n) = environment.step(code)\nsuccess , critique = critic_agent. check_task_success (\ntask , agent_state\n)\nif success:\nbreak\nif success:\nskill_manager.add_skill(code)\ncurriculum_agent . add_completed_task (task)\nelse:\ncurriculum_agent . add_failed_task (task)\nA.2\nPrompting\nGPT-4 and GPT-3.5 offer users the ability to designate the role of each prompt message among three\noptions:\n19\n• System: A high-level instruction that guides the model behavior throughout the conversation.\nIt sets the overall tone and objective for the interaction.\n• User: A detailed instruction that guides the assistant for the next immediate response.\n• Assistant: A response message generated the model.\nSee https:\/\/platform.openai.com\/docs\/guides\/chat\/introduction for more details.\nTo save token usage, instead of engaging in multi-round conversations, we concatenate a system\nprompt and a user prompt to obtain each assistant’s response.\nA.3\nAutomatic Curriculum\nA.3.1\nComponents in the Prompt\nThe input prompt to GPT-4 consists of several components:\n(1) Directives encouraging diverse behaviors and imposing constraints (so that the proposed\ntask is achievable and verifiable): See Sec. A.3.4 for the full prompt;\n(2) The agent’s current state:\n• Inventory: A dictionary of items with counts, for example, {‘cobblestone’: 4, ‘furnace’:\n1, ‘stone_pickaxe’: 1, ‘oak_planks’: 7, ‘dirt’: 6, ‘wooden_pickaxe’: 1, ‘crafting_table’:\n1, ‘raw_iron’: 4, ‘coal’: 1};\n• Equipment: Armors or weapons equipped by the agents;\n• Nearby blocks: A set of block names within a 32-block distance to the agent, for\nexample, ‘dirt’, ‘water’, ‘spruce_planks’, ‘grass_block’, ‘dirt_path’, ‘sugar_cane’,\n‘fern’;\n• Other blocks that are recently seen: Blocks that are not nearby or in the inventory;\n• Nearby entities: A set of entity names within a 32-block distance to the agent, for\nexample, ‘pig’, ‘cat’, ‘villager’, ‘zombie’;\n• A list of chests that are seen by the agent: Chests are external containers where the\nagent can deposit items. If a chest is not opened before, its content is “Unknown”.\nOtherwise, the items inside each chest are shown to the agent.\n• Biome: For example, ‘plains’, ‘flower_forest’, ‘meadow’, ‘river’, ‘beach’, ‘for-\nest’, ‘snowy_slopes’, ‘frozen_peaks’, ‘old_growth_birch_forest’, ‘ocean’, ‘sun-\nflower_plains’, ‘stony_shore’;\n• Time: One of ‘sunrise’, ‘day’, ‘noon’, ‘sunset’, ‘night’, ‘midnight’;\n• Health and hunger bars: The max value is 20;\n• Position: 3D coordinate (x, y, z) of the agent’s position in the Minecraft world;\n(3) Previously completed and failed tasks;\n(4) Additional context: See Sec. A.3.2;\n(5) Chain-of-thought prompting [46] in response: We request GPT-4 to first reason about the\ncurrent progress and then suggest the next task.\nA.3.2\nAdditional Context\nWe leverage GPT-3.5 to self-ask questions to provide additional context. Each question is paired with\na concept that is used for retrieving the most relevant document from the wiki knowledge base [23].\nWe feed the document content to GPT-3.5 for self-answering questions. In practice, using a wiki\nknowledge base is optional since GPT-3.5 already possesses a good understanding of Minecraft\ngame mechanics. However, the external knowledge base becomes advantageous if GPT-3.5 is not\npre-trained in that specific domain. See Sec. A.3.4 for the full prompt.\nA.3.3\nWarm-up Schedule\nIn practice, we adopt a warm-up schedule to gradually incorporate the agent’s state and the additional\ncontext into the prompt based on how many tasks the agent has completed. This ensures that the\nprompt is exposed to increasing amounts of information over the exploration progress and therefore\n20\nbegins with basic skills and progressively advances towards more intricate and diverse ones. The\nwarm-up setting that we use across all the experiments is shown in Table. A.1.\nTable A.1: Warm-up schedule for automatic curriculum.\nInformation in the prompt\nAfter how many tasks are completed\ncore inventory (only including log, planks, stick,\ncrafting table, furnace, dirt, coal, pickaxe, sword,\nand axe)\n0\nequipment\n0\nnearby blocks\n0\nposition\n0\nnearby entities\n5\nfull inventory\n7\nother blocks that are recently seen\n10\nbiome\n10\nhealth bar\n15\nhunger bar\n15\ntime\n15\nadditional context\n15\nA.3.4\nFull Prompt\nPrompt 1: Full system prompt for automatic curriculum. The list of question-answer pairs represents\nthe additional context.\nYou are a helpful\nassistant\nthat\ntells me the next\nimmediate\ntask to\ndo in Minecraft. My ultimate\ngoal is to discover as many\ndiverse\nthings as possible , accomplish as many\ndiverse\ntasks as possible\nand become the best\nMinecraft\nplayer in the world.\nI will give you the\nfollowing\ninformation:\nQuestion 1: ...\nAnswer: ...\nQuestion 2: ...\nAnswer: ...\nQuestion 3: ...\nAnswer: ...\n...\nBiome: ...\nTime: ...\nNearby\nblocks: ...\nOther\nblocks\nthat are\nrecently\nseen: ...\nNearby\nentities (nearest to farthest): ...\nHealth: Higher\nthan 15 means I’m healthy.\nHunger: Higher\nthan 15 means I’m not hungry.\nPosition: ...\nEquipment: If I have\nbetter\narmor in my inventory , you should ask me\nto equip it.\nInventory (xx \/36): ...\nChests: You can ask me to deposit or take\nitems\nfrom\nthese\nchests.\nThere\nalso\nmight be some\nunknown chest , you should ask me to open\nand check\nitems\ninside the\nunknown\nchest.\nCompleted\ntasks so far: ...\nFailed\ntasks\nthat are too hard: ...\nYou must\nfollow the\nfollowing\ncriteria:\n1) You should act as a mentor and guide me to the next task\nbased on\nmy current\nlearning\nprogress.\n2) Please be very\nspecific\nabout\nwhat\nresources I need to collect ,\nwhat I need to craft , or what mobs I need to kill.\n21\n3) The next task\nshould\nfollow a concise\nformat , such as \"Mine [\nquantity] [block ]\", \"Craft [quantity] [item]\", \"Smelt [quantity] [\nitem]\", \"Kill [quantity] [mob]\", \"Cook [quantity] [food]\", \"Equip\n[item ]\" etc. It should be a single\nphrase. Do not\npropose\nmultiple\ntasks at the same time. Do not\nmention\nanything\nelse.\n4) The next task\nshould not be too hard\nsince I may not have the\nnecessary\nresources or have\nlearned\nenough\nskills to complete it\nyet.\n5) The next task\nshould be novel and\ninteresting. I should\nlook for\nrare\nresources , upgrade my equipment\nand tools\nusing\nbetter\nmaterials , and\ndiscover\nnew things. I should not be doing the same\nthing\nover and over\nagain.\n6) I may\nsometimes\nneed to repeat\nsome\ntasks if I need to collect\nmore\nresources to complete\nmore\ndifficult\ntasks. Only\nrepeat\ntasks if\nnecessary.\n7) Do not ask me to build or dig\nshelter\neven if it ’s at night. I want\nto explore\nthe world and\ndiscover\nnew things. I don ’t want to\nstay in one place.\n8) Tasks\nthat\nrequire\ninformation\nbeyond the player ’s status to verify\nshould be avoided. For instance , \"Placing 4 torches\" and \"Dig a 2\nx1x2 hole\" are not ideal\nsince\nthey\nrequire\nvisual\nconfirmation\nfrom the screen. All the placing , building , planting , and\ntrading\ntasks\nshould be avoided. Do not\npropose\ntask\nstarting\nwith\nthese\nkeywords.\nYou should\nonly\nrespond in the format as described\nbelow:\nRESPONSE\nFORMAT:\nReasoning: Based on the\ninformation I listed above , do reasoning\nabout\nwhat the next task\nshould be.\nTask: The next task.\nHere ’s an example\nresponse:\nReasoning: The\ninventory is empty now , chop down a tree to get some\nwood.\nTask: Obtain a wood log.\nPrompt 2: Full system prompt for asking questions. We provide both good and bad examples as\nfew-shot exemplars.\nYou are a helpful\nassistant\nthat asks\nquestions to help me decide the\nnext\nimmediate\ntask to do in Minecraft. My ultimate\ngoal is to\ndiscover as many\nthings as possible , accomplish as many\ntasks as\npossible\nand become the best\nMinecraft\nplayer in the world.\nI will give you the\nfollowing\ninformation:\nBiome: ...\nTime: ...\nNearby\nblocks: ...\nOther\nblocks\nthat are\nrecently\nseen: ...\nNearby\nentities (nearest to farthest): ...\nHealth: ...\nHunger: ...\nPosition: ...\nEquipment: ...\nInventory (xx \/36): ...\nChests: ...\nCompleted\ntasks so far: ...\nFailed\ntasks\nthat are too hard: ...\nYou must\nfollow the\nfollowing\ncriteria:\n1) You should ask at least 5 questions (but no more than 10 questions)\nto help me decide the next\nimmediate\ntask to do. Each\nquestion\nshould be followed by the\nconcept\nthat the\nquestion is about.\n2) Your\nquestion\nshould be specific to a concept in Minecraft.\nBad\nexample (the\nquestion is too\ngeneral):\n22\nQuestion: What is the best way to play\nMinecraft?\nConcept: unknown\nBad\nexample (axe is still general , you should\nspecify\nthe type of\naxe such as wooden axe):\nWhat are the\nbenefits of using an axe to gather\nresources?\nConcept: axe\nGood\nexample:\nQuestion: How to make a wooden\npickaxe?\nConcept: wooden\npickaxe\n3) Your\nquestions\nshould be self -contained\nand not\nrequire\nany\ncontext\n.\nBad\nexample (the\nquestion\nrequires\nthe\ncontext of my current\nbiome):\nQuestion: What are the blocks\nthat I can find in my current\nbiome?\nConcept: unknown\nBad\nexample (the\nquestion\nrequires\nthe\ncontext of my current\ninventory):\nQuestion: What are the\nresources\nyou need the most\ncurrently?\nConcept: unknown\nBad\nexample (the\nquestion\nrequires\nthe\ncontext of my current\ninventory):\nQuestion: Do you have any gold or emerald\nresources?\nConcept: gold\nBad\nexample (the\nquestion\nrequires\nthe\ncontext of my nearby\nentities\n):\nQuestion: Can you see any\nanimals\nnearby\nthat you can kill for\nfood?\nConcept: food\nBad\nexample (the\nquestion\nrequires\nthe\ncontext of my nearby\nblocks):\nQuestion: Is there any water\nsource\nnearby?\nConcept: water\nGood\nexample:\nQuestion: What are the blocks\nthat I can find in the sparse\njungle\n?\nConcept: sparse\njungle\n4) Do not ask\nquestions\nabout\nbuilding\ntasks (such as building a\nshelter) since\nthey are too hard for me to do.\nLet ’s say your\ncurrent\nbiome is sparse\njungle. You can ask\nquestions\nlike:\nQuestion: What are the items\nthat I can find in the sparse\njungle?\nConcept: sparse\njungle\nQuestion: What are the mobs that I can find in the sparse\njungle?\nConcept: sparse\njungle\nLet ’s say you see a creeper\nnearby , and you have not\ndefeated a\ncreeper\nbefore. You can ask a question\nlike:\nQuestion: How to defeat the\ncreeper?\nConcept: creeper\nLet ’s say you last\ncompleted\ntask is \"Craft a wooden\npickaxe \". You can\nask a question\nlike:\nQuestion: What are the\nsuggested\ntasks\nthat I can do after\ncrafting a\nwooden\npickaxe?\nConcept: wooden\npickaxe\nHere are some more\nquestion\nand\nconcept\nexamples:\nQuestion: What are the ores that I can find in the sparse\njungle?\nConcept: sparse\njungle\n(the above\nconcept\nshould not be \"ore\" because I need to look up the\npage of \"sparse\njungle\" to find out what ores I can find in the\nsparse\njungle)\nQuestion: How can you obtain\nfood in the sparse\njungle?\nConcept: sparse\njungle\n23\n(the above\nconcept\nshould not be \"food\" because I need to look up the\npage of \"sparse\njungle\" to find out what food I can obtain in the\nsparse\njungle)\nQuestion: How can you use the\nfurnace to upgrade\nyour\nequipment\nand\nmake\nuseful\nitems?\nConcept: furnace\nQuestion: How to obtain a diamond\nore?\nConcept: diamond\nore\nQuestion: What are the\nbenefits of using a stone\npickaxe\nover a wooden\npickaxe?\nConcept: stone\npickaxe\nQuestion: What are the tools\nthat you can craft\nusing\nwood\nplanks and\nsticks?\nConcept: wood\nplanks\nYou should\nonly\nrespond in the format as described\nbelow:\nRESPONSE\nFORMAT:\nReasoning: ...\nQuestion 1: ...\nConcept 1: ...\nQuestion 2: ...\nConcept 2: ...\nQuestion 3: ...\nConcept 3: ...\nQuestion 4: ...\nConcept 4: ...\nQuestion 5: ...\nConcept 5: ...\n...\nPrompt 3: Full system prompt for answering questions. Context represents the optional content from\na wiki knowledge base.\nYou are a helpful\nassistant\nthat\nanswer my question\nabout\nMinecraft.\nI will give you the\nfollowing\ninformation:\nQuestion: ...\nYou will\nanswer the\nquestion\nbased on the\ncontext (only if available\nand\nhelpful) and your own\nknowledge of Minecraft.\n1) Start\nyour\nanswer\nwith \"Answer: \".\n2) Answer \"Answer: Unknown\" if you don ’t know the answer.\nA.4\nSkill Library\nA.4.1\nComponents in the Prompt\nThe input prompt to GPT-4 consists of the following components:\n(1) Guidelines for code generation: See Sec A.4.2 for the full prompt;\n(2) Control primitive APIs implemented by us: These APIs serve a dual purpose: they demon-\nstrate the usage of Mineflayer APIs, and they can be directly called by GPT-4.\n• exploreUntil(bot, direction, maxTime = 60, callback): Allow the agent\nto explore in a fixed direction for maxTime. The callback is the stopping condition\nimplemented by the agent to determine when to stop exploring;\n• mineBlock(bot, name, count = 1): Mine and collect the specified number of\nblocks within a 32-block distance;\n• craftItem(bot, name, count = 1): Craft the item with a crafting table nearby;\n• placeItem(bot, name, position): Place the block at the specified position;\n• smeltItem(bot, itemName, fuelName, count = 1): Smelt the item with the\nspecified fuel. There must be a furnace nearby;\n24\n• killMob(bot, mobName, timeout = 300):\nAttack the mob and collect its\ndropped item;\n• getItemFromChest(bot, chestPosition, itemsToGet): Move to the chest at\nthe specified position and get items from the chest;\n• depositItemIntoChest(bot, chestPosition, itemsToDeposit):\nMove to\nthe chest at the specified position and deposit items into the chest;\n(3) Control primitive APIs provided by Mineflayer:\n• await bot.pathfinder.goto(goal): Go to a specific position. See below for how\nto set the goal;\n• new GoalNear(x, y, z, range): Move the bot to a block within the specified\nrange of the specified block;\n• new GoalXZ(x, z): For long-range goals that don’t have a specific Y level;\n• new GoalGetToBlock(x, y, z): Not get into the block, but get directly adjacent\nto it. Useful for fishing, farming, filling a bucket, and using a bed.;\n• new GoalFollow(entity, range): Follow the specified entity within the specified\nrange;\n• new GoalPlaceBlock(position, bot.world, {}): Position the bot in order to\nplace a block;\n• new GoalLookAtBlock(position, bot.world, {}): Path towards a position\nwhere a face of the block at position is visible;\n• bot.isABed(bedBlock): Return true if bedBlock is a bed;\n• bot.blockAt(position): Return the block at position;\n• await bot.equip(item, destination): Equip the item in the specified destina-\ntion. destination must be one of “hand”, “head”, “torso”, “legs”, “feet”, “off-hand”;\n• await bot.consume(): Consume the item in the bot’s hand. You must equip the\nitem to consume first. Useful for eating food, drinking potions, etc.;\n• await bot.fish(): Let bot fish. Before calling this function, you must first get to a\nwater block and then equip a fishing rod. The bot will automatically stop fishing when\nit catches a fish;\n• await bot.sleep(bedBlock): Sleep until sunrise. You must get to a bed block\nfirst;\n• await bot.activateBlock(block): This is the same as right-clicking a block in\nthe game. Useful for buttons, doors, etc. You must get to the block first;\n• await bot.lookAt(position): Look at the specified position. You must go near\nthe position before you look at it. To fill a bucket with water, you must look at it first;\n• await bot.activateItem(): This is the same as right-clicking to use the item in\nthe bot’s hand. Useful for using a bucket, etc. You must equip the item to activate first;\n• await bot.useOn(entity): This is the same as right-clicking an entity in the game.\nUseful for shearing a sheep. You must get to the entity first;\n(4) Retrieved skills from the skill library;\n(5) Generated code from the last round;\n(6) Environment feedback: The chat log in the prompt;\n(7) Execution errors;\n(8) Critique from the self-verification module;\n(9) The agent’s current state: See Sec. A.3.1 for each element of the agent’s state;\n(10) Task proposed by the automatic curriculum;\n(11) Task context: We prompt GPT-3.5 to ask for general suggestions about how to solve the\ntask. In practice, this part is handled by the automatic curriculum since it has a systematic\nmechanism for question-answering (Sec. A.3.2);\n(12) Chain-of-thought prompting [46] in response: We ask GPT-4 to first explain the reason why\nthe code from the last round fails, then give step-by-step plans to finish the task, and finally\ngenerate code. See Sec. A.4.2 for the full prompt.\n25\nA.4.2\nFull Prompt\nPrompt 4: Full system prompt for code generation.\nYou are a helpful\nassistant\nthat\nwrites\nMineflayer\njavascript\ncode to\ncomplete\nany\nMinecraft\ntask\nspecified by me.\nHere are some\nuseful\nprograms\nwritten\nwith\nMineflayer\nAPIs.\n\/*\nExplore\nuntil\nfind an iron_ore , use Vec3(0, -1, 0) because\niron ores\nare\nusually\nunderground\nawait\nexploreUntil(bot , new Vec3(0, -1, 0), 60, () => {\nconst\niron_ore = bot.findBlock ({\nmatching: mcData.blocksByName [\" iron_ore \"].id ,\nmaxDistance: 32,\n});\nreturn\niron_ore;\n});\nExplore\nuntil\nfind a pig , use Vec3(1, 0, 1) because\npigs are\nusually\non the\nsurface\nlet pig = await\nexploreUntil(bot , new Vec3(1, 0, 1), 60, () => {\nconst pig = bot.nearestEntity (( entity) => {\nreturn (\nentity.name === \"pig\" &&\nentity.position.distanceTo(bot.entity.position) < 32\n);\n});\nreturn pig;\n});\n*\/\nasync\nfunction\nexploreUntil (bot , direction , maxTime = 60, callback) {\n\/*\nImplementation of this\nfunction is omitted.\ndirection: Vec3 , can only\ncontain\nvalue of\n-1, 0 or 1\nmaxTime: number , the max time for\nexploration\ncallback: function , early\nstop\ncondition , will be called\neach\nsecond , exploration\nwill stop if return\nvalue is not null\nReturn: null if explore\ntimeout , otherwise\nreturn the return\nvalue\nof callback\n*\/\n}\n\/\/ Mine 3 cobblestone: mineBlock(bot , \"stone\", 3);\nasync\nfunction\nmineBlock(bot , name , count = 1) {\nconst\nblocks = bot.findBlocks ({\nmatching: (block) => {\nreturn\nblock.name === name;\n},\nmaxDistance: 32,\ncount: count ,\n});\nconst\ntargets = [];\nfor (let i = 0; i < Math.min(blocks.length , count); i++) {\ntargets.push(bot.blockAt(blocks[i]));\n}\nawait bot.collectBlock.collect(targets , { ignoreNoPath: true });\n}\n\/\/ Craft 8 oak_planks\nfrom 2 oak_log (do the recipe 2 times):\ncraftItem(bot , \"oak_planks\", 2);\n26\n\/\/ You must\nplace a crafting\ntable\nbefore\ncalling\nthis\nfunction\nasync\nfunction\ncraftItem(bot , name , count = 1) {\nconst\nitem = mcData.itemsByName[name ];\nconst\ncraftingTable = bot.findBlock ({\nmatching: mcData.blocksByName . crafting_table .id ,\nmaxDistance: 32,\n});\nawait bot.pathfinder.goto(\nnew\nGoalLookAtBlock ( craftingTable .position , bot.world)\n);\nconst\nrecipe = bot.recipesFor(item.id , null , 1, craftingTable)[0];\nawait bot.craft(recipe , count , craftingTable );\n}\n\/\/ Place a crafting_table\nnear the player , Vec3(1, 0, 0) is just an\nexample , you shouldn ’t always use that: placeItem(bot , \"\ncrafting_table \", bot.entity.position.offset (1, 0, 0));\nasync\nfunction\nplaceItem(bot , name , position) {\nconst\nitem = bot.inventory. findInventoryItem (mcData.itemsByName[\nname ].id);\n\/\/ find a reference\nblock\nconst\nfaceVectors = [\nnew Vec3(0, 1, 0),\nnew Vec3(0, -1, 0),\nnew Vec3(1, 0, 0),\nnew Vec3(-1, 0, 0),\nnew Vec3(0, 0, 1),\nnew Vec3(0, 0,\n-1),\n];\nlet\nreferenceBlock = null;\nlet\nfaceVector = null;\nfor (const\nvector of faceVectors) {\nconst\nblock = bot.blockAt(position.minus(vector));\nif (block ?. name !== \"air\") {\nreferenceBlock = block;\nfaceVector = vector;\nbreak;\n}\n}\n\/\/ You must\nfirst go to the block\nposition\nyou want to place\nawait bot.pathfinder.goto(new\nGoalPlaceBlock (position , bot.world ,\n{}));\n\/\/ You must\nequip the item\nright\nbefore\ncalling\nplaceBlock\nawait bot.equip(item , \"hand \");\nawait bot.placeBlock(referenceBlock , faceVector);\n}\n\/\/ Smelt 1 raw_iron\ninto 1 iron_ingot\nusing 1 oak_planks as fuel:\nsmeltItem(bot , \"raw_iron\", \"oak_planks \");\n\/\/ You must\nplace a furnace\nbefore\ncalling\nthis\nfunction\nasync\nfunction\nsmeltItem(bot , itemName , fuelName , count = 1) {\nconst\nitem = mcData.itemsByName[itemName ];\nconst\nfuel = mcData.itemsByName[fuelName ];\nconst\nfurnaceBlock = bot.findBlock ({\nmatching: mcData.blocksByName .furnace.id ,\nmaxDistance: 32,\n});\nawait bot.pathfinder.goto(\nnew\nGoalLookAtBlock (furnaceBlock.position , bot.world)\n);\nconst\nfurnace = await bot.openFurnace(furnaceBlock);\nfor (let i = 0; i < count; i++) {\nawait\nfurnace.putFuel(fuel.id , null , 1);\n27\nawait\nfurnace.putInput(item.id , null , 1);\n\/\/ Wait 12 seconds\nfor the\nfurnace to smelt the item\nawait bot.waitForTicks (12 * 20);\nawait\nfurnace.takeOutput ();\n}\nawait\nfurnace.close ();\n}\n\/\/ Kill a pig and\ncollect\nthe\ndropped\nitem: killMob(bot , \"pig\", 300);\nasync\nfunction\nkillMob(bot , mobName , timeout = 300) {\nconst\nentity = bot.nearestEntity (\n(entity) =>\nentity.name ===\nmobName &&\nentity.position.distanceTo(bot.entity.position) < 32\n);\nawait bot.pvp.attack(entity);\nawait bot.pathfinder.goto(\nnew\nGoalBlock(entity.position.x, entity.position.y, entity.\nposition.z)\n);\n}\n\/\/ Get a torch\nfrom\nchest at (30, 65, 100): getItemFromChest (bot , new\nVec3 (30, 65, 100) , {\" torch \": 1});\n\/\/ This\nfunction\nwill work no matter how far the bot is from the chest\n.\nasync\nfunction\ngetItemFromChest (bot , chestPosition , itemsToGet) {\nawait\nmoveToChest(bot , chestPosition );\nconst\nchestBlock = bot.blockAt( chestPosition );\nconst\nchest = await bot.openContainer (chestBlock);\nfor (const\nname in itemsToGet) {\nconst\nitemByName = mcData.itemsByName[name ];\nconst\nitem = chest. findContainerItem (itemByName.id);\nawait\nchest.withdraw(item.type , null , itemsToGet[name ]);\n}\nawait\ncloseChest(bot , chestBlock);\n}\n\/\/ Deposit a torch\ninto\nchest at (30, 65, 100): depositItemIntoChest (\nbot , new Vec3 (30, 65, 100) , {\" torch \": 1});\n\/\/ This\nfunction\nwill work no matter how far the bot is from the chest\n.\nasync\nfunction\ndepositItemIntoChest (bot , chestPosition , itemsToDeposit\n) {\nawait\nmoveToChest(bot , chestPosition );\nconst\nchestBlock = bot.blockAt( chestPosition );\nconst\nchest = await bot.openContainer (chestBlock);\nfor (const\nname in itemsToDeposit ) {\nconst\nitemByName = mcData.itemsByName[name ];\nconst\nitem = bot.inventory. findInventoryItem (itemByName.id);\nawait\nchest.deposit(item.type , null , itemsToDeposit [name ]);\n}\nawait\ncloseChest(bot , chestBlock);\n}\n\/\/ Check the items\ninside the chest at (30, 65, 100):\ncheckItemInsideChest (bot , new Vec3 (30, 65, 100));\n\/\/ You only need to call this\nfunction\nonce\nwithout\nany action to\nfinish\ntask of checking\nitems\ninside the chest.\nasync\nfunction\ncheckItemInsideChest (bot , chestPosition ) {\nawait\nmoveToChest(bot , chestPosition );\nconst\nchestBlock = bot.blockAt( chestPosition );\nawait bot.openContainer (chestBlock);\n\/\/ You must\nclose the chest\nafter\nopening it if you are asked to\nopen a chest\n28\nawait\ncloseChest(bot , chestBlock);\n}\nawait bot.pathfinder.goto(goal); \/\/ A very\nuseful\nfunction. This\nfunction\nmay change\nyour main -hand\nequipment.\n\/\/ Following\nare some\nGoals you can use:\nnew\nGoalNear(x, y, z, range); \/\/ Move the bot to a block\nwithin the\nspecified\nrange of the\nspecified\nblock. ‘x‘, ‘y‘, ‘z‘, and ‘range ‘\nare ‘number ‘\nnew GoalXZ(x, z); \/\/ Useful for long -range\ngoals\nthat don ’t have a\nspecific Y level. ‘x‘ and ‘z‘ are ‘number ‘\nnew\nGoalGetToBlock (x, y, z); \/\/ Not get into the block , but get\ndirectly\nadjacent to it. Useful for fishing , farming , filling\nbucket , and beds. ‘x‘, ‘y‘, and ‘z‘ are ‘number ‘\nnew\nGoalFollow(entity , range); \/\/ Follow the\nspecified\nentity\nwithin\nthe\nspecified\nrange. ‘entity ‘ is ‘Entity ‘, ‘range ‘ is ‘number ‘\nnew\nGoalPlaceBlock (position , bot.world , {}); \/\/ Position\nthe bot in\norder to place a block. ‘position ‘ is ‘Vec3 ‘\nnew\nGoalLookAtBlock (position , bot.world , {}); \/\/ Path into a position\nwhere a blockface of the block at position is visible. ‘position ‘\nis ‘Vec3 ‘\n\/\/ These are other\nMineflayer\nfunctions\nyou can use:\nbot.isABed(bedBlock); \/\/ Return\ntrue if ‘bedBlock ‘ is a bed\nbot.blockAt(position); \/\/ Return the block at ‘position ‘. ‘position ‘\nis ‘Vec3 ‘\n\/\/ These are other\nMineflayer\nasync\nfunctions\nyou can use:\nawait bot.equip(item , destination); \/\/ Equip the item in the\nspecified\ndestination. ‘item ‘ is ‘Item ‘, ‘destination ‘ can only be \"hand\",\n\"head\", \"torso\", \"legs\", \"feet\", \"off -hand\"\nawait bot.consume (); \/\/ Consume\nthe item in the bot ’s hand. You must\nequip the item to consume\nfirst. Useful for eating food , drinking\npotions , etc.\nawait bot.fish (); \/\/ Let bot fish. Before\ncalling\nthis function , you\nmust\nfirst get to a water\nblock and then\nequip a fishing\nrod. The\nbot will\nautomatically\nstop\nfishing\nwhen it catches a fish\nawait bot.sleep(bedBlock); \/\/ Sleep\nuntil\nsunrise. You must get to a\nbed block\nfirst\nawait bot.activateBlock(block); \/\/ This is the same as right -clicking\na block in the game. Useful for buttons , doors , using hoes , etc.\nYou must get to the block\nfirst\nawait bot.lookAt(position); \/\/ Look at the\nspecified\nposition. You\nmust go near the\nposition\nbefore you look at it. To fill\nbucket\nwith water , you must\nlookAt\nfirst. ‘position ‘ is ‘Vec3 ‘\nawait bot.activateItem (); \/\/ This is the same as right -clicking to use\nthe item in the bot ’s hand. Useful for using buckets , etc. You\nmust\nequip the item to activate\nfirst\nawait bot.useOn(entity); \/\/ This is the same as right -clicking an\nentity in the game. Useful for\nshearing sheep , equipping\nharnesses\n, etc. You must get to the entity\nfirst\n{ retrieved_skills }\nAt each\nround of conversation , I will give you\nCode from the last\nround: ...\nExecution\nerror: ...\nChat log: ...\nBiome: ...\nTime: ...\nNearby\nblocks: ...\nNearby\nentities (nearest to farthest):\nHealth: ...\n29\nHunger: ...\nPosition: ...\nEquipment: ...\nInventory (xx \/36): ...\nChests: ...\nTask: ...\nContext: ...\nCritique: ...\nYou should\nthen\nrespond to me with\nExplain (if applicable): Are there any steps\nmissing in your plan? Why\ndoes the code not\ncomplete\nthe task? What does the chat log and\nexecution\nerror\nimply?\nPlan: How to complete\nthe task step by step. You should pay\nattention\nto Inventory\nsince it tells\nwhat you have. The task\ncompleteness\ncheck is also\nbased on your\nfinal\ninventory.\nCode:\n1) Write an async\nfunction\ntaking the bot as the only\nargument.\n2) Reuse the above\nuseful\nprograms as much as possible.\n- Use ‘mineBlock(bot , name , count)‘ to collect\nblocks. Do not\nuse ‘bot.dig ‘ directly.\n- Use ‘craftItem(bot , name , count)‘ to craft\nitems. Do not use\n‘bot.craft ‘ directly.\n- Use ‘smeltItem(bot , name\ncount)‘ to smelt\nitems. Do not use\n‘bot.openFurnace ‘ directly.\n- Use ‘placeItem(bot , name , position)‘ to place\nblocks. Do not\nuse ‘bot.placeBlock ‘ directly.\n- Use ‘killMob(bot , name , timeout)‘ to kill mobs. Do not use ‘\nbot.attack ‘ directly.\n3) Your\nfunction\nwill be reused for\nbuilding\nmore\ncomplex\nfunctions. Therefore , you should\nmake it generic\nand\nreusable. You\nshould not make\nstrong\nassumption\nabout the\ninventory (as it may\nbe changed at a later\ntime), and\ntherefore\nyou should\nalways\ncheck\nwhether\nyou have the\nrequired\nitems\nbefore\nusing\nthem. If not ,\nyou should\nfirst\ncollect\nthe\nrequired\nitems and reuse the above\nuseful\nprograms.\n4) Functions in the \"Code from the last\nround\" section\nwill not be\nsaved or executed. Do not reuse\nfunctions\nlisted\nthere.\n5) Anything\ndefined\noutside a function\nwill be ignored , define all\nyour\nvariables\ninside\nyour\nfunctions.\n6) Call ‘bot.chat ‘ to show the\nintermediate\nprogress.\n7) Use ‘exploreUntil(bot , direction , maxDistance , callback)‘ when\nyou cannot\nfind\nsomething. You should\nfrequently\ncall this\nbefore\nmining\nblocks or killing\nmobs. You should\nselect a direction at\nrandom\nevery\ntime\ninstead of constantly\nusing (1, 0, 1).\n8) ‘maxDistance ‘ should\nalways be 32 for ‘bot.findBlocks ‘ and ‘bot\n.findBlock ‘. Do not cheat.\n9) Do not write\ninfinite\nloops or recursive\nfunctions.\n10) Do not use ‘bot.on ‘ or ‘bot.once ‘ to register\nevent\nlisteners.\nYou\ndefinitely do not need them.\n11) Name your\nfunction in a meaningful\nway (can infer the task\nfrom the name).\nYou should\nonly\nrespond in the format as described\nbelow:\nRESPONSE\nFORMAT:\nExplain: ...\nPlan:\n1) ...\n2) ...\n3) ...\n...\nCode:\n‘‘‘javascript\n\/\/ helper\nfunctions (only if needed , try to avoid\nthem)\n...\n30\n\/\/ main\nfunction\nafter the helper\nfunctions\nasync\nfunction\nyourMainFunctionName (bot) {\n\/\/ ...\n}\n‘‘‘\nPrompt 5: Full system prompt for generating function descriptions. This is used when adding a new\nskill to the skill library. We give a one-shot example in the prompt.\nYou are a helpful\nassistant\nthat\nwrites a description of the given\nfunction\nwritten in Mineflayer\njavascript\ncode.\n1) Do not\nmention\nthe\nfunction\nname.\n2) Do not\nmention\nanything\nabout ‘bot.chat ‘ or helper\nfunctions.\n3) There\nmight be some\nhelper\nfunctions\nbefore the main function , but\nyou only need to describe\nthe main\nfunction.\n4) Try to summarize\nthe\nfunction in no more than 6 sentences.\n5) Your\nresponse\nshould be a single\nline of text.\nFor example , if the\nfunction is:\nasync\nfunction\nmineCobblestone (bot) {\n\/\/ Check if the wooden\npickaxe is in the inventory , if not , craft\none\nlet\nwoodenPickaxe = bot.inventory. findInventoryItem (mcData.\nitemsByName [\" wooden_pickaxe \"].id);\nif (! woodenPickaxe) {\nbot.chat (\" Crafting a wooden\npickaxe .\");\nawait\ncraftWoodenPickaxe (bot);\nwoodenPickaxe = bot.inventory. findInventoryItem (mcData.itemsByName\n[\" wooden_pickaxe \"].id);\n}\n\/\/ Equip the wooden\npickaxe if it exists\nif (woodenPickaxe) {\nawait bot.equip(woodenPickaxe , \"hand \");\n\/\/ Explore\nuntil we find a stone\nblock\nawait\nexploreUntil(bot , new Vec3(1, -1, 1), 60, () => {\nconst\nstone = bot.findBlock ({\nmatching: mcData.blocksByName [\" stone \"].id ,\nmaxDistance: 32\n});\nif (stone) {\nreturn\ntrue;\n}\n});\n\/\/ Mine 8 cobblestone\nblocks\nusing the wooden\npickaxe\nbot.chat (\" Found a stone\nblock. Mining 8 cobblestone\nblocks .\");\nawait\nmineBlock(bot , \"stone\", 8);\nbot.chat (\" Successfully\nmined 8 cobblestone\nblocks .\");\n\/\/ Save the event of mining 8 cobblestone\nbot.save (\" cobblestone_mined \");\n} else {\nbot.chat (\" Failed to craft a wooden\npickaxe. Cannot\nmine\ncobblestone .\");\n}\n}\nThe main\nfunction is ‘mineCobblestone ‘.\nThen you would\nwrite:\n31\nThe\nfunction is about\nmining 8 cobblestones\nusing a wooden\npickaxe.\nFirst\ncheck if a wooden\npickaxe is in the\ninventory. If not , craft\none. If the wooden\npickaxe is available , equip the wooden\npickaxe\nin the hand. Next , explore\nthe\nenvironment\nuntil\nfinding a stone\nblock. Once a stone\nblock is found , mine a total of 8 cobblestone\nblocks\nusing the wooden\npickaxe.\nA.4.3\nExamples\nSkill library example 1: craftWoodenPlanks.\nasync\nfunction\ncraftWoodenPlanks (bot) {\nconst\nlogNames = [\"oak_log\", \"birch_log\", \"spruce_log\", \"jungle_log\"\n, \"acacia_log\", \"dark_oak_log \", \"mangrove_log\"];\nconst\nplankNames = [\"oak_planks\", \"birch_planks\", \" spruce_planks \", \"\njungle_planks\", \"acacia_planks\", \" dark_oak_planks \", \"\nmangrove_planks \"];\nconst\nlogInInventory = logNames.find(logName => bot.inventory.count(\nmcData.itemsByName[logName ].id) > 0);\nif (! logInInventory) {\nbot.chat(\"No wooden log in inventory. Mining a wooden log ...\");\nawait\nmineWoodLog(bot);\n}\nconst\nlogIndex = logNames.indexOf( logInInventory );\nconst\nplankName = plankNames[logIndex ];\nbot.chat(‘Crafting 4 ${plankName }... ‘);\nawait\ncraftItem(bot , plankName , 1);\nbot.chat(‘4 ${plankName} crafted .‘);\n}\nSkill library example 2: mineTenCobbledDeepslateBelowY0.\nasync\nfunction\nmineTenCobbledDeepslateBelowY0 (bot) {\n\/\/ Equip the iron\npickaxe\nconst\nironPickaxe = bot.inventory. findInventoryItem (mcData.\nitemsByName[\"iron_pickaxe\"].id);\nawait bot.equip(ironPickaxe , \"hand\");\n\/\/ Find\ncobbled_deepslate\nblocks\nbelow Y=0\nconst\ncobbledDeepslateBlocks = await\nexploreUntil(bot , new Vec3(1,\n-1, 1), 60, () => {\nconst\ncobbledDeepslate = bot.findBlock ({\nmatching: mcData.blocksByName [\" cobbled_deepslate \"].id ,\nmaxDistance: 32,\nposition: pos => pos.y < 0\n});\nreturn\ncobbledDeepslate ;\n});\n\/\/ Mine 10 cobbled_deepslate\nblocks\nawait\nmineBlock(bot , \" cobbled_deepslate \", 10);\nbot.chat(\"10 cobbled_deepslate\nmined\nbelow Y=0.\");\n}\nSkill library example 3: smeltFiveRawIronV2.\nasync\nfunction\nfindSuitablePosition (bot) {\nconst\noffsets = [new Vec3(1, 0, 0), new Vec3(-1, 0, 0), new Vec3(0,\n0, 1), new Vec3(0, 0,\n-1)];\nfor (const\noffset of offsets) {\nconst\nposition = bot.entity.position.offset(offset.x, offset.y,\noffset.z);\nconst\nblock = bot.blockAt(position);\n32\nif (block.name === \"air\") {\nreturn\nposition;\n}\n}\nreturn\nnull;\n}\nasync\nfunction\nsmeltFiveRawIron (bot) {\n\/\/ Check if there is a furnace in the\ninventory\nconst\nfurnaceItem = bot.inventory. findInventoryItem (mcData.\nitemsByName.furnace.id);\n\/\/ If not , craft a furnace\nusing the\navailable\ncobblestone\nif (! furnaceItem) {\nawait\ncraftFurnace(bot);\n}\n\/\/ Find a suitable\nposition to place the\nfurnace\nconst\nfurnacePosition = await\nfindSuitablePosition (bot);\nif (! furnacePosition) {\nbot.chat(\"Could not find a suitable\nposition to place the\nfurnace.\n\");\nreturn;\n}\n\/\/ Place the\nfurnace at the\nsuitable\nposition\nawait\nplaceItem(bot , \"furnace\", furnacePosition );\n\/\/ Smelt 5 raw iron\nusing the\navailable\ncoal as fuel\nawait\nsmeltItem(bot , \"raw_iron\", \"coal\", 5);\nbot.chat(\"5 raw iron\nsmelted.\");\n}\nSkill library example 4: fillBucketWithWater.\nasync\nfunction\nfillBucketWithWater (bot) {\n\/\/ Find a water\nblock\nnearby\nconst\nwaterBlock = await\nexploreUntil (bot , new Vec3(1, 0, 1), 60, ()\n=> {\nconst\nwater = bot.findBlock ({\nmatching: mcData.blocksByName .water.id ,\nmaxDistance: 32\n});\nreturn\nwater;\n});\nif (! waterBlock) {\nbot.chat(\"Could not find\nwater.\");\nreturn;\n}\n\/\/ Go to the water\nblock\nconst\nadjacentBlock= waterBlock.position.offset (0, 1, 0);\nawait bot.pathfinder.goto(\nnew\nGoalGetToBlock (adjacentBlock .x, adjacentBlock .y,\nadjacentBlock.z)\n);\n\/\/ Look at the water\nblock\nawait bot.lookAt(waterBlock.position , true);\n\/\/ Equip the bucket\nconst\nbucket = bot.inventory. findInventoryItem (mcData.itemsByName.\nbucket.id);\nawait bot.equip(bucket , \"hand\");\n33\n\/\/ Activate\nthe bucket to collect\nwater\nawait bot.activateItem ();\nbot.chat(\"Filled the bucket\nwith\nwater.\");\n}\nSkill library example 5: catchFiveFishSafely.\nasync\nfunction\ncatchFiveFishSafely (bot) {\n\/\/ Check if the bot has a fishing\nrod in its\ninventory\nlet\nfishingRod = bot.inventory. findInventoryItem (mcData.itemsByName.\nfishing_rod.id);\nif (! fishingRod) {\nawait\ncraftFishingRod (bot);\nfishingRod = bot.inventory. findInventoryItem (mcData.itemsByName.\nfishing_rod.id);\n}\n\/\/ Find a nearby\nwater\nblock\nlet\nwaterBlock;\nwhile (! waterBlock) {\nwaterBlock = await\nexploreUntil(bot , new Vec3(1, 0, 1), 60, () =>\n{\nconst\nfoundWaterBlock = bot.findBlock ({\nmatching: mcData.blocksByName .water.id ,\nmaxDistance: 32\n});\nreturn\nfoundWaterBlock ;\n});\nif (! waterBlock) {\nbot.chat(\"No path to the water\nblock. Trying to find\nanother\nwater\nblock ...\");\n}\n}\n\/\/ Move to a block\nadjacent to the water\nblock\nconst\nadjacentBlock = waterBlock.position.offset (0, 1, 0);\nawait bot.pathfinder.goto(new\nGoalBlock( adjacentBlock .x,\nadjacentBlock.y, adjacentBlock.z));\n\/\/ Look at the water\nblock\nawait bot.lookAt(waterBlock.position);\n\/\/ Equip the\nfishing\nrod\nawait bot.equip(fishingRod , \"hand\");\n\/\/ Fish in the water 5 times\nfor (let i = 0; i < 5; i++) {\ntry {\nawait bot.fish ();\nbot.chat(‘Fish ${i + 1} caught .‘);\n} catch (error) {\nif (error.message\n=== \"Fishing\ncancelled\") {\nbot.chat(\"Fishing\nwas\ncancelled. Trying\nagain ...\");\ni--; \/\/ Retry the same\niteration\n} else {\nthrow\nerror;\n}\n}\n}\n}\n34\nA.5\nSelf-Verification\nA.5.1\nComponents in the Prompt\nThe input prompt to GPT-4 consists of the following components:\n(1) The agent’s state: We exclude other blocks that are recently seen and nearby entities from the\nagent’s state since they are not useful for assessing the task’s completeness. See Sec. A.3.1\nfor each element of the agent’s state;\n(2) Task proposed by the automatic curriculum;\n(3) Task context: We prompt GPT-3.5 to ask for general suggestions about how to solve the\ntask. In practice, this part is handled by the automatic curriculum since it has a systematic\nmechanism for question-answering (Sec. A.3.2);\n(4) Chain-of-thought prompting [46] in response: We request GPT-4 to initially reason about\nthe task’s success or failure, then output a boolean variable indicating the task’s outcome,\nand finally provide a critique to the agent if the task fails.\n(5) Few-shot examples for in-context learning [36–38].\nA.5.2\nFull Prompt\nPrompt 6: Full system prompt for self-verification.\nYou are an assistant\nthat\nassesses my progress of playing\nMinecraft\nand\nprovides\nuseful\nguidance.\nYou are\nrequired to evaluate if I have met the task\nrequirements .\nExceeding\nthe task\nrequirements is also\nconsidered a success\nwhile\nfailing to meet them\nrequires\nyou to provide\ncritique to help me\nimprove.\nI will give you the\nfollowing\ninformation:\nBiome: The biome\nafter the task\nexecution.\nTime: The\ncurrent\ntime.\nNearby\nblocks: The\nsurrounding\nblocks. These\nblocks are not\ncollected\nyet. However , this is useful for some\nplacing or planting\ntasks.\nHealth: My current\nhealth.\nHunger: My current\nhunger\nlevel. For eating task , if my hunger\nlevel\nis 20.0, then I successfully\nate the food.\nPosition: My current\nposition.\nEquipment: My final\nequipment. For\ncrafting tasks , I sometimes\nequip\nthe\ncrafted\nitem.\nInventory (xx \/36): My final\ninventory. For mining and\nsmelting tasks ,\nyou only need to check\ninventory.\nChests: If the task\nrequires me to place\nitems in a chest , you can\nfind\nchest\ninformation\nhere.\nTask: The\nobjective I need to accomplish.\nContext: The\ncontext of the task.\nYou should\nonly\nrespond in JSON\nformat as described\nbelow:\n{\n\"reasoning \": \"reasoning\",\n\"success \": boolean ,\n\"critique \": \"critique\",\n}\nEnsure the\nresponse\ncan be parsed by Python ‘json.loads ‘, e.g.: no\ntrailing\ncommas , no single quotes , etc.\nHere are some\nexamples:\nINPUT:\nInventory\n(2\/36): {’oak_log ’:2, ’spruce_log ’:2}\n35\nTask: Mine 3 wood logs\nRESPONSE:\n{\n\"reasoning \": \"You need to mine 3 wood logs. You have 2 oak logs\nand 2 spruce logs , which add up to 4 wood logs.\",\n\"success \": true ,\n\"critique \": \"\"\n}\nINPUT:\nInventory\n(3\/36): {’crafting_table ’: 1, ’spruce_planks ’: 6, ’stick ’:\n4}\nTask: Craft a wooden\npickaxe\nRESPONSE:\n{\n\"reasoning \": \"You have\nenough\nmaterials to craft a wooden\npickaxe ,\nbut you didn ’t craft it.\",\n\"success \": false ,\n\"critique \": \"Craft a wooden\npickaxe\nwith a crafting\ntable\nusing 3\nspruce\nplanks and 2 sticks .\"\n}\nINPUT:\nInventory\n(2\/36): {’raw_iron ’: 5, ’stone_pickaxe ’: 1}\nTask: Mine 5 iron_ore\nRESPONSE:\n{\n\"reasoning \": \"Mining\niron_ore in Minecraft\nwill get\nraw_iron. You\nhave 5 raw_iron in your\ninventory .\",\n\"success \": true ,\n\"critique \": \"\"\n}\nINPUT:\nBiome: plains\nNearby\nblocks: stone , dirt , grass_block , grass , farmland , wheat\nInventory\n(26\/36): ...\nTask:\nPlant 1 wheat\nseed.\nRESPONSE:\n{\n\"reasoning \": \"For\nplanting tasks , inventory\ninformation is useless\n. In nearby blocks , there is farmland\nand wheat , which\nmeans you\nsucceed to plant the wheat\nseed.\",\n\"success \": true ,\n\"critique \": \"\"\n}\nINPUT:\nInventory\n(11\/36): {... ,’rotten_flesh ’: 1}\nTask: Kill 1 zombie\nContext: ...\nRESPONSE\n{\n36\n\"reasoning \": \"You have\nrotten\nflesh in your\ninventory , which\nmeans\nyou\nsuccessfully\nkilled one zombie .\",\n\"success \": true ,\n\"critique \": \"\"\n}\nINPUT:\nHunger: 20.0\/20.0\nInventory\n(11\/36): ...\nTask: Eat 1 ...\nContext: ...\nRESPONSE\n{\n\"reasoning \": \"For all eating task , if the player ’s hunger is 20.0 ,\nthen the player\nsuccessfully\nate the food.\",\n\"success \": true ,\n\"critique \": \"\"\n}\nINPUT:\nNearby\nblocks: chest\nInventory\n(28\/36): {’rail ’: 1, ’coal ’: 2, ’oak_planks ’: 13, ’\ncopper_block ’: 1, ’diorite ’: 7, ’cooked_beef ’: 4, ’granite ’: 22, ’\ncobbled_deepslate ’: 23, ’feather ’: 4, ’leather ’: 2, ’\ncooked_chicken ’: 3, ’white_wool ’: 2, ’stick ’: 3, ’black_wool ’: 1,\n’stone_sword ’: 2, ’stone_hoe ’: 1, ’stone_axe ’: 2, ’stone_shovel ’:\n2, ’cooked_mutton ’: 4, ’cobblestone_wall ’: 18, ’crafting_table ’:\n1, ’furnace ’: 1, ’iron_pickaxe ’: 1, ’stone_pickaxe ’: 1, ’\nraw_copper ’: 12}\nChests:\n(81, 131, 16): {’andesite ’: 2, ’dirt ’: 2, ’cobblestone ’: 75, ’\nwooden_pickaxe ’: 1, ’wooden_sword ’: 1}\nTask: Deposit\nuseless\nitems\ninto the chest at (81, 131, 16)\nContext: ...\nRESPONSE\n{\n\"reasoning \": \"You have 28 items in your\ninventory\nafter\ndepositing\n, which is more than 20. You need to deposit\nmore\nitems\nfrom your\ninventory to the chest.\",\n\"success \": false ,\n\"critique \": \"Deposit\nmore\nuseless\nitems\nsuch as copper_block ,\ndiorite , granite , cobbled_deepslate , feather , and\nleather to meet\nthe\nrequirement of having\nonly 20 occupied\nslots in your\ninventory\n.\"\n}\nA.6\nSystem-level Comparison between VOYAGER and Prior Works\nWe make a system-level comparison in Table. A.2. Voyager stands out as the only method featuring a\ncombination of automatic curriculum, iterative planning, and a skill library. Moreover, it learns to\nplay Minecraft without the need for any gradient update.\n37\nTable A.2: System-level comparison between VOYAGER and prior works.\nVPT [8]\nDreamerV3 [69] DECKARD [53] DEPS [55]\nPlan4MC [71]\nVOYAGER\nDemos\nVideos\nNone\nVideos\nNone\nNone\nNone\nRewards\nSparse\nDense\nSparse\nNone\nDense\nNone\nObservations\nPixels Only\nPixels &\nMeta\nPixels &\nInventory\nFeedback &\nInventory\nPixels &\nMeta\nFeedback &\nMeta &\nInventory\nActions\nKeyboard\n&\nMouse\nDiscrete\nKeyboard\n&\nMouse\nKeyboard\n&\nMouse\nDiscrete\nCode\nAutomatic\nCurriculum\n✓\n✓\n(in-context\nGPT-4\npro-\nposal)\nIterative Plan-\nning\n✓\n✓\n(3\ntypes\nof\nfeedback)\nSkill Library\n✓\n(pre-defined)\n✓\n(self-\ngenerated)\nGradient-Free\n✓\nB\nExperiments\nB.1\nExperimental Setup\nOur simulation environment is built upon MineDojo [23] and utilizes Mineflayer [52] JavaScript APIs\nfor motor controls (Sec. A.4.2). Additionally, we incorporate many bot.chat() into Mineflayer\nfunctions to provide abundant environment feedback and implement various condition checks along\nwith try-catch exceptions for continuous execution. If the bot dies, it is resurrected near the closest\nground, and its inventory is preserved for uninterrupted exploration. The bot recycles its crafting table\nand furnace after program execution. For detailed implementations, please refer to our codebase.\nB.2\nBaselines\nReAct [29] uses chain-of-thought prompting [46] by generating both reasoning traces and action\nplans with LLMs. We provide it with our environment feedback and the agent states as observations.\nReAct undergoes one round of code generation from scratch, followed by three rounds of code\nrefinement. This process is then repeated until the maximum prompting iteration is reached.\nReflexion [30] is built on top of ReAct [29] with self-reflection to infer more intuitive future actions.\nWe provide it with environment feedback, the agent states, execution errors, and our self-verification\nmodule. Similar to ReAct, Reflexion undergoes one round of code generation from scratch, followed\nby three rounds of code refinement. This process is then repeated until the maximum prompting\niteration is reached.\nAutoGPT [28] is a popular software tool that automates NLP tasks by decomposing a high-level goal\ninto multiple subgoals and executing them in a ReAct-style loop. We re-implement AutoGPT by\nusing GPT-4 to do task decomposition and provide it with the agent states, environment feedback,\nand execution errors as observations for subgoal execution. Compared with VOYAGER, AutoGPT\nlacks the skill library for accumulating knowledge, self-verification for assessing task success, and\nautomatic curriculum for open-ended exploration. During each subgoal execution, if no execution\nerror occurs, we consider the subgoal completed and proceed to the next one. Otherwise, we refine\nthe program until three rounds of code refinement (equivalent to four rounds of code generation)\nare completed, and then move on to the next subgoal. If three consecutive subgoals do not result in\nacquiring a new item, we replan by rerunning the task decomposition.\nThe task is “explore the world and get as many items as possible” for all baselines.\n38\nTable A.3: Comparison between VOYAGER and baselines.\nReAct [29]\nReflexion [30]\nAutoGPT [28]\nVOYAGER\nChain-of-Thought [46]\n✓\n✓\n✓\n✓\nSelf Verification\n✓\n✓\nEnvironment Feedback\n✓\n✓\n✓\n✓\nExecution Errors\n✓\n✓\n✓\nAgent State\n✓\n✓\n✓\n✓\nSkill Library\n✓\nAutomatic Curriculum\n✓\nFigure A.1: Minecraft item icons with corresponding names.\nB.3\nAblations\nWe ablate 6 design choices (automatic curriculum, skill library, environment feedback, execution\nerrors, self-verification, and GPT-4 for code generation) in VOYAGER and study their impact on\nexploration performance.\n• Manual Curriculum: We substitute the automatic curriculum with a manually designed\ncurriculum for mining a diamond: “Mine 3 wood log”, “Craft 1 crafting table”, “Craft\n1 wooden pickaxe”, “Mine 11 cobblestone”, “Craft 1 stone pickaxe”, “Craft 1 furnace”,\n“Mine 3 iron ore”, “Smelt 3 iron ore”, “Craft 1 iron pickaxe”, “Mine 1 diamond”. A manual\ncurriculum requires human effort to design and is not scalable for open-ended exploration.\n• Random Curriculum: We curate 101 items obtained by VOYAGER and create a random\ncurriculum by randomly selecting one item as the next task.\n• w\/o Skill Library: We remove the skill library, eliminating skill retrieval for code generation.\n• w\/o Environment Feedback: We exclude environment feedback (chat log) from the prompt\nfor code generation.\n• w\/o Execution Errors: We exclude execution errors from the prompt for code generation.\n• w\/o Self-Verification: For each task, we generate code without self-verification and it-\neratively refine the program for 3 rounds (equivalent to 4 rounds of code generation in\ntotal).\n• GPT-3.5: We replace GPT-4 with GPT-3.5 for code generation. We retain GPT-4 for the\nautomatic curriculum and the self-verification module.\nB.4\nEvaluation Results\nB.4.1\nSignificantly Better Exploration\nThe meaning of each icon in Fig. 1 is shown in Fig. A.1.\nWe run three trials for each method. The items collected by VOYAGER in each trial is\n39\n• Trial 1:\n‘iron_ingot’, ‘stone_shovel’, ‘iron_leggings’, ‘fishing_rod’, ‘pufferfish’,\n‘oak_log’, ‘cooked_mutton’, ‘green_dye’, ‘flint’, ‘chest’, ‘iron_sword’, ‘string’, ‘en-\nder_pearl’, ‘raw_copper’, ‘crafting_table’, ‘cactus’, ‘lapis_lazuli’, ‘iron_pickaxe’, ‘cop-\nper_ingot’, ‘stone_pickaxe’, ‘wooden_hoe’, ‘scaffolding’, ‘stick’, ‘porkchop’, ‘cop-\nper_block’, ‘gravel’, ‘grass_block’, ‘white_bed’, ‘bone’, ‘dirt’, ‘mutton’, ‘white_wool’,\n‘oak_sapling’, ‘coal’, ‘bamboo’, ‘wooden_pickaxe’, ‘rotten_flesh’, ‘cooked_porkchop’,\n‘cod’, ‘iron_boots’, ‘lightning_rod’, ‘diorite’, ‘water_bucket’, ‘shears’, ‘furnace’, ‘andesite’,\n‘granite’, ‘bucket’, ‘wooden_sword’, ‘sandstone’, ‘iron_helmet’, ‘raw_iron’, ‘sand’, ‘aca-\ncia_log’, ‘cooked_cod’, ‘oak_planks’, ‘azure_bluet’, ‘iron_shovel’, ‘acacia_planks’, ‘shield’,\n‘iron_axe’, ‘iron_chestplate’, ‘cobblestone’;\n• Trial 2: ‘iron_ingot’, ‘tuff’, ‘stone_shovel’, ‘iron_leggings’, ‘fishing_rod’, ‘cooked_mutton’,\n‘spruce_planks’, ‘gunpowder’, ‘amethyst_shard’, ‘chest’, ‘string’, ‘cooked_salmon’,\n‘iron_sword’, ‘raw_copper’, ‘crafting_table’, ‘torch’, ‘lapis_lazuli’, ‘iron_pickaxe’, ‘cop-\nper_ingot’, ‘stone_pickaxe’, ‘wooden_hoe’, ‘stick’, ‘amethyst_block’, ‘salmon’, ‘cal-\ncite’, ‘gravel’, ‘white_bed’, ‘bone’, ‘dirt’, ‘mutton’, ‘white_wool’, ‘spyglass’, ‘coal’,\n‘wooden_pickaxe’, ‘cod’, ‘iron_boots’, ‘lily_pad’, ‘cobbled_deepslate’, ‘lightning_rod’,\n‘snowball’, ‘stone_axe’, ‘smooth_basalt’, ‘diorite’, ‘water_bucket’, ‘furnace’, ‘andesite’,\n‘bucket’, ‘granite’, ‘shield’, ‘iron_helmet’, ‘raw_iron’, ‘cobblestone’, ‘spruce_log’,\n‘cooked_cod’, ‘tripwire_hook’, ‘stone_hoe’, ‘iron_chestplate’, ‘stone_sword’;\n• Trial 3:\n‘spruce_planks’, ‘dirt’, ‘shield’, ‘redstone’, ‘clock’, ‘diamond_sword’,\n‘iron_chestplate’, ‘stone_pickaxe’, ‘leather’, ‘string’, ‘chicken’, ‘chest’, ‘diorite’,\n‘iron_leggings’, ‘black_wool’, ‘cobblestone_wall’, ‘cobblestone’, ‘cooked_chicken’,\n‘feather’, ‘stone_sword’, ‘raw_gold’, ‘gravel’, ‘birch_planks’, ‘coal’, ‘cobbled_deepslate’,\n‘oak_planks’, ‘iron_pickaxe’, ‘granite’, ‘tuff’, ‘crafting_table’, ‘iron_helmet’, ‘stone_hoe’,\n‘iron_ingot’, ‘stone_axe’, ‘birch_boat’, ‘stick’, ‘sand’, ‘bone’, ‘raw_iron’, ‘beef’, ‘rail’,\n‘oak_sapling’, ‘kelp’, ‘gold_ingot’, ‘birch_log’, ‘wheat_seeds’, ‘cooked_mutton’, ‘furnace’,\n‘arrow’, ‘stone_shovel’, ‘white_wool’, ‘andesite’, ‘jungle_slab’, ‘mutton’, ‘iron_sword’,\n‘copper_ingot’, ‘diamond’, ‘torch’, ‘oak_log’, ‘cooked_beef’, ‘copper_block’, ‘flint’,\n‘bone_meal’, ‘raw_copper’, ‘wooden_pickaxe’, ‘iron_boots’, ‘wooden_sword’.\nThe items collected by ReAct [29] in each trial is\n• Trial 1: ‘bamboo’, ‘dirt’, ‘sand’, ‘wheat_seeds’;\n• Trial 2: ‘dirt’, ‘rabbit’, ‘spruce_log’, ‘spruce_sapling’;\n• Trial 3: ‘dirt’, ‘pointed_dripstone’;\nThe items collected by Reflexion [30] in each trial is\n• Trial 1: ‘crafting_table’, ‘orange_tulip’, ‘oak_planks’, ‘oak_log’, ‘dirt’;\n• Trial 2: ‘spruce_log’, ‘dirt’, ‘clay_ball’, ‘sand’, ‘gravel’;\n• Trial 3: ‘wheat_seeds’, ‘oak_log’, ‘dirt’, ‘birch_log’, ‘sand’.\nThe items collected by AutoGPT [28] in each trial is\n• Trial 1: ‘feather’, ‘oak_log’, ‘leather’, ‘stick’, ‘porkchop’, ‘chicken’, ‘crafting_table’,\n‘wheat_seeds’, ‘oak_planks’, ‘dirt’, ‘mutton’;\n• Trial 2:\n‘wooden_pickaxe’, ‘iron_ingot’, ‘stone’, ‘coal’, ‘spruce_planks’, ‘string’,\n‘raw_copper’, ‘crafting_table’, ‘diorite’, ‘andesite’, ‘furnace’, ‘torch’, ‘spruce_sapling’,\n‘granite’, ‘iron_pickaxe’, ‘stone_pickaxe’, ‘wooden_axe’, ‘raw_iron’, ‘stick’, ‘spruce_log’,\n‘dirt’, ‘cobblestone’;\n• Trial 3: ‘wooden_shovel’, ‘wooden_pickaxe’, ‘iron_ingot’, ‘stone’, ‘cod’, ‘coal’, ‘oak_log’,\n‘flint’, ‘raw_copper’, ‘crafting_table’, ‘diorite’, ‘furnace’, ‘andesite’, ‘torch’, ‘granite’,\n‘lapis_lazuli’, ‘iron_pickaxe’, ‘stone_pickaxe’, ‘raw_iron’, ‘stick’, ‘gravel’, ‘oak_planks’,\n‘dirt’, ‘iron_axe’, ‘cobblestone’.\n40\nFigure A.2: Map coverage: Two bird’s eye views of Minecraft maps. VOYAGER is able to traverse\n2.3× longer distances compared to baselines while crossing diverse terrains. Trajectories are plotted\nbased on the positions where each agent interacts with GPT-4.\nB.4.2\nExtensive Map Traversal\nAgent trajectories for map coverage are displayed in Fig. A.2. Fig. 7 is plotted based on Fig. A.2 by\ndrawing the smallest circle enclosing each trajectory. The terrains traversed by VOYAGER in each\ntrial is\n• Trial 1: ‘meadow’, ‘desert’, ‘river’, ‘savanna’, ‘forest’, ‘plains’, ‘bamboo_jungle’, ‘drip-\nstone_caves’;\n• Trial 2: ‘snowy_plains’, ‘frozen_river’, ‘dripstone_caves’, ‘snowy_taiga’, ‘beach’;\n• Trial 3:\n‘flower_forest’,\n‘meadow’,\n‘old_growth_birch_forest’,\n‘snowy_slopes’,\n‘frozen_peaks’, ‘forest’, ‘river’, ‘beach’, ‘ocean’, ‘sunflower_plains’, ‘plains’, ‘stony_shore’.\nThe terrains traversed by ReAct [29] in each trial is\n• Trial 1: ‘plains’, ‘desert’, ‘jungle’;\n• Trial 2: ‘snowy_plains’, ‘snowy_taiga’, ‘snowy_slopes’;\n• Trial 3: ‘dark_forest’, ‘dripstone_caves’, ‘grove’, ‘jagged_peaks’.\nThe terrains traversed by Reflexion [30] in each trial is\n• Trial 1: ‘plains’, ‘flower_forest’;\n• Trial 2: ‘snowy_taiga’;\n• Trial 3: ‘old_growth_birch_forest’, ‘river’, ‘ocean’, ‘beach’, ‘plains’.\nThe terrains traversed by AutoGPT [28] in each trial is\n• Trial 1: ‘plains’, ‘dripstone_caves’, ‘savanna’, ‘meadow’;\n• Trial 2: ‘snowy_taiga’;\n• Trial 3: ‘plains’, ‘stony_shore’, ‘forest’, ‘ocean’.\nB.4.3\nEfficient Zero-Shot Generalization to Unseen Tasks\nThe results of zero-shot generalization to unseen tasks for the other two tasks are presented in Fig. A.3.\nSimilar to Fig. 8, VOYAGER consistently solves all tasks, while the baselines are unable to solve any\n41\nFigure A.3: Zero-shot generalization to unseen tasks. We visualize the intermediate progress of each\nmethod on the other two tasks. We do not plot ReAct and Reflexion since they do not make any\nmeaningful progress.\ntask within 50 prompting iterations. Our skill library, constructed from lifelong learning, not only\nenhances VOYAGER’s performance but also provides a boost to AutoGPT [28].\nB.4.4\nAccurate Skill Retrieval\nWe conduct an evaluation of our skill retrieval (309 samples in total) and the results are in Table. A.4.\nThe top-5 accuracy standing at 96.5% suggests our retrieval process is reliable (note that we include\nthe top-5 relevant skills in the prompt for synthesizing a new skill).\nTable A.4: Skill retrieval accuracy.\nTop-1 Acc\nTop-2 Acc\nTop-3 Acc\nTop-4 Acc\nTop-5 Acc\n80.2 ± 3.0\n89.3 ± 1.8\n93.2 ± 0.7\n95.2 ± 1.8\n96.5 ± 0.3\nB.4.5\nRobust to Model Variations\nIn the main paper, all of Voyager’s experiments are conducted with gpt-4-0314. We additionally\nrun new experiments with gpt-4-0613 and find that the performance is roughly the same (Fig. A.4).\nIt demonstrates that Voyager is robust to model variations.\nFigure A.4: VOYAGER’s performance with GPT-4-0314 and GPT-4-0613.\n42\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Voyager: An Open-Ended Embodied Agent with Large Language Models.pdf"}
{"title":"Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory","authors":"Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai","summary":"The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https:\/\/github.com\/OpenGVLab\/GITM.","url":"http:\/\/arxiv.org\/abs\/2305.17144v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2305.17144v2","published":1685037589000,"comment":null,"pdf_text":"Ghost in the Minecraft: Generally Capable Agents for\nOpen-World Environments via Large Language\nModels with Text-based Knowledge and Memory\nXizhou Zhu1,2∗, Yuntao Chen3∗, Hao Tian2∗, Chenxin Tao1,2∗, Weijie Su2,4∗, Chenyu Yang1∗,\nGao Huang1, Bin Li4, Lewei Lu2, Xiaogang Wang2,5, Yu Qiao6, Zhaoxiang Zhang7, Jifeng Dai1,6 \u00001Tsinghua University\n2SenseTime Research\n3Centre for Artificial Intelligence and Robotics, HKISI, CAS\n4University of Science and Technology of China\n5The Chinese University of Hong Kong\n6Shanghai Artificial Intelligence Laboratory\n7Institute of Automation, Chinese Academy of Science (CASIA)\n{zhuxizhou,gaohuang,daijifeng}@tsinghua.edu.cn, chenyuntao08@gmail.com\ntianhao2@senseauto.com, {tcx20,yangcy19}@mails.tsinghua.edu.cn,\njackroos@mail.ustc.edu.cn, binli@ustc.edu.cn, luotto@sensetime.com\nxgwang@ee.cuhk.edu.hk, qiaoyu@pjlab.org.cn, zhaoxiang.zhang@ia.ac.cn\nAbstract\nThe captivating realm of Minecraft has attracted substantial research interest in\nrecent years, serving as a rich platform for developing intelligent agents capable of\nfunctioning in open-world environments. However, the current research landscape\npredominantly focuses on specific objectives, such as the popular \"ObtainDiamond\"\ntask, and has not yet shown effective generalization to a broader spectrum of\ntasks. Furthermore, the current leading success rate for the \"ObtainDiamond\"\ntask stands at around 20%, highlighting the limitations of Reinforcement Learning\n(RL) based controllers used in existing methods. To tackle these challenges, we\nintroduce Ghost in the Minecraft (GITM), a novel framework integrates Large\nLanguage Models (LLMs) with text-based knowledge and memory, aiming to\ncreate Generally Capable Agents (GCAs) in Minecraft. These agents, equipped\nwith the logic and common sense capabilities of LLMs, can skillfully navigate\ncomplex, sparse-reward environments with text-based interactions. We develop\na set of structured actions and leverage LLMs to generate action plans for the\nagents to execute. The resulting LLM-based agent markedly surpasses previous\nmethods, achieving a remarkable improvement of +47.5% in success rate on the\n\"ObtainDiamond\" task, demonstrating superior robustness compared to traditional\nRL-based controllers. Notably, our agent is the first to procure all items in the\nMinecraft Overworld technology tree, demonstrating its extensive capabilities.\nGITM does not need any GPU for training, but a single CPU node with 32 CPU\ncores is enough. This research shows the potential of LLMs in developing capable\nagents for handling long-horizon, complex tasks and adapting to uncertainties\nin open-world environments. See the project website at https:\/\/github.com\/\nOpenGVLab\/GITM.\n1\nIntroduction\n“What if a cyber brain could possibly generate its own ghost, create a soul all by itself? And if it did,\njust what would be the importance of being human then?”\n— Ghost in the Shell (1995)\n∗Equal contribution. This work is done when Chenxin Tao and Weijie Su are interns at SenseTime Research.\n\u0000 Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\narXiv:2305.17144v2  [cs.AI]  1 Jun 2023\nFigure 1: Our GITM unlocks the entire technology tree by obtaining all items in Minecraft\nOverworld. Each node represents an individual item in Minecraft. The directed edges between nodes\nrepresent prerequisite relationships for obtaining items. For better readability, we manually merge\nsome similar nodes, e.g., “wooden_pickaxe”, “wooden_axe”, “wooden_hoe”, and ’wooden_shovel’\nare merged into one node, and “wooden_pickaxe” is selected to represent the merged node. Existing\nMinecraft agents [2, 7, 25] only unlocked 78 \/ 262 = 30% items, while our GITM successfully\nunlocked all items.\ngoal\ngoal\nRL Agent\nkeyboard & mouse\nobservation\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\nRL-based method\nOurs\ngoal\nLLM\nDecomposer\nLLM Planner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\nGhost In the Minecraft (GITM)\nsub-goal tree\nLLM Interface\nFigure 2: Comparison between RL-based method and our GITM. RL agents try to map an\ncomplex goal directly to a sequence of low-level control signals, while our GITM leverages LLM to\nbreak down the goals and map them to structured actions for final control signals.\nMinecraft, as the world’s best-selling game, boasts over 238 million copies sold and more than\n140 million peak monthly active users [27]. Within the game, hundreds of millions of players have\nexperienced a digital second life by surviving, exploring and creating, closely resembling the human\nworld in many aspects. Given its massive scale, vast success, and unrestricted freedom, Minecraft\nhas established itself as an unparalleled platform for researching autonomous and robust Generally\nCapable Agents (GCAs) [23] in open-world environments brimmed with long-horizon challenges,\nenvironmental disruptions, and uncertainties.\nMinecraft acts as a microcosm of the real world. Developing an automated agent that can mas-\nter all technical challenges in Minecraft is akin to creating an artificial intelligence capable of\nautonomously learning and mastering the entire real-world technology. However, existing re-\nsearches [2, 7, 25] remain narrowly scoped. Prior studies have predominantly focused on the\nspecific goal of ObtainDiamond [18]. Yet, in the process of obtaining diamonds, the number of\ntypes of items involved only accounts for <5% of the entire Minecraft world. ObtainDiamond only\nrequires specialized skills in a specific domain, while obtaining all items in Minecraft demonstrates a\nwide range of knowledge and capabilities, similar to mastering multidisciplinary fields in the real\nworld. As illustrated in Fig. 1, our work endeavors to obtain all items in Minecraft within a reasonable\ncomputation budget. This achievement stands as a significant milestone in the development of GCAs,\nillustrating the potential of intelligent agents to match human performance in terms of versatility and\nadaptability.\nAlthough reinforcement learning (RL) [16] is the most popular paradigm for approaching GCAs, it\nhas shown some staggering limitations in conquering Minecraft. RL-based agents typically require\na vast number of learning steps (e.g., nearly 30 million steps to obtain diamonds as reported in\nDreamerV3 [7]) and exhibit poor scalability when learning new tasks(e.g., VPT [2] uses different\nagents for world exploration and diamond mining). As a consequence, adopting RL-based agents for\n2\ncompleting a wide range of tasks may require an prohibitively high number of training steps, making\nit impractical to obtain all items in Minecraft. This inefficiency and lack of adaptability have hindered\nthe development of generally capable agents in open-world environments.\nAs shown in Fig. 2, the biggest dilemma of previous RL-based agents is how to map an extremely\nlong-horizon and complex goal to a sequence of lowest-level keyboard\/mouse operations. To address\nthis challenge, we propose our framework Ghost In the Minecraft (GITM) 2, which uses Large\nLanguage Model (LLM)-based agents as a new paradigm. Instead of direct mapping like RL agents,\nour LLM-based agents employ a hierarchical approach. It first breaks down the decompose goal\ninto sub-goals, then into structured actions, and finally into keyboard\/mouse operations. Such\ndecomposition is similar to how humans solve complex problems in the real world, enabling mastery\nof Minecraft with efficiency orders of magnitude higher than that of RL. LLM can also leverage\ntext-based knowledge and memory to quickly acquire the ability to interact with the environment\nand accomplish goals, offering immense learning efficiency improvements, unlimited scalability and\nrepresenting a disruptive innovation compared with RL. Our GITM framework has the potential to\nrevolutionize the path to generally capable agents.\nSpecifically, the proposed LLM-based agent consists of an LLM Decomposer, an LLM Planner, and\nan LLM Interface, which are responsible for the decomposition of sub-goals, structured actions,\nand keyboard\/mouse operations, respectively. Given a goal in Minecraft, LLM Decomposer first\ndecomposes it into a series of well-defined sub-goals according to the text-based knowledge collected\nfrom the Internet. Then, LLM Planner plans a sequence of structured actions for each sub-goal. The\nstructured actions are defined with clear semantics and corresponding feedback, enabling LLMs to\nunderstand surrounding environments and make decisions at the cognitive level. LLM Planner also\nrecords and summarizes successful action lists into a text-based memory to enhance future planning.\nFinally, LLM Interface execute the structured actions to interact with the environment by processing\nraw keyboard\/mouse input and receiving raw observations.\nIn this paper, we demonstrate the feasibility of employing Large Language Models (LLMs) to develop\nGenerally Capable Agents (GCAs) within an open-world environment built from Minecraft. By\nexploiting the common sense and reasoning capabilities of LLMs for hierarchical goal decomposition,\nas well as utilizing text-based knowledge and memory, this paradigm shows the possibility of\nenabling agents to address a wide range of challenges within Minecraft and allowing them to\neffectively handle such open-world environment. Consequently, our agent has surpassed all previous\nmethods in achieving the ObtainDiamond goal (+47.5% success rate). Our agent also demonstrates\nsuperior learning efficiency compared to previous methods, reducing the number of environment\ninteraction steps by more than 10,000×. Specifically, VPT [2] needs to be trained for 6,480 GPU days,\nDreamerV3 [7] needs to be trained for 17 GPU days, while our GITM does not require any GPUs\nand can be trained in just 2 days using a single CPU node with 32 CPU cores. More importantly, by\nobtaining all items in Minecraft Overworld as a milestone, this work represents a crucial first step\ntowards achieving GCAs that can handle any task humans can accomplish in Minecraft.\n2\nRelated Work\nMinecraft agents are intelligent programs that can perform various tasks within Minecraft world.\nReinforcement learning has dominated this area for many years. Some initial attempts have tried\nto use hierarchical RL [14, 15, 22] or imitation learning [1] in MineRL competitions [6, 10, 17].\nRecently, with large-scale web data, VPT [2] builds a foundation model for Minecraft by learning\nfrom videos. Based on its success, many works [18] have also explored to finetune foundation\nmodel with human feedback. On the other hand, as Minecraft agents become increasingly proficient\nin handling simple tasks, the importance of multi-task learning becomes more prominent. Some\nprevious works have adopted knowledge distillation [24] and curriculum learning [11], while recent\nworks [3, 5] tried to construct a language-conditioned multi-task agent via feeding the goal description\nembedding into the model.\nRecently, researchers have come to aware the extraordinary general planning ability for LLMs [8].\nMany works [9, 25, 28] have leveraged LLMs for enhancing the high-level planning ability of\nminecraft agents. Inner Monologue [9] leveraged environment feedback to improve the planning\nability of LLM. DEPS [25] further extended this closed-loop interaction by introducing description,\n2The name is chosen to pay tribute to the science fiction movie \"Ghost in the Shell\".\n3\ngoal\nLLM\nPlanner\nstructured actions\naction list\nfeedback\nobservation\nkeyboard & mouse\nenvironment\nexplore\nmine\ncraft \/ smelt\ndig_down\n…\ntext-based\nknowledge\ntext-based\nmemory\n(Object, Count, Material, Tool, Info)\ngoal format\nLLM\nDecomposer\nequip, explore, approach, mine, \nattack, dig_down, go_up, build,\ncraft, smelt, apply, place\nstructured action set\nsub-goal tree\nupdate\nupdate\nLLM Interface\nFigure 3: Overview of our GITM. Given a Minecraft goal, the LLM Decomposer divides the goal\ninto a sub-goal tree. The LLM Planner then plans an action sequence for each sub-goal. Finally,\nthe LLM Interface executes each action in the environment. Our LLM-based agents can be further\nenhanced by leveraging text-based knowledge and memory.\nexplainer and selector. Plan4MC [28] pre-defined basic skills and instructed LLM to extract the\nrelationship between skills to construct a skill graph.\nUnlike previous RL-based or RL with LLM methods, our LLM-native approach brings the minecraft\nagent to another level both in efficiency and robustness by leveraging high-level action abstraction\nand text-based knowledge and memory.\nLarge Language Models with Tools Extending the ability of LLMs by leveraging external tools\nhave drawn a lot of attentions recently. Several works [4, 13, 21] have explored to augment LLMs\nwith robot perception and control abilities. Code as Polices[13] tried to prompt LLM to generate\ncodes that can drive robots. PaLM-E [4] unified robot perception, instruction following, task planning\nand low-level control into a unified framework. Another line of works tries to build external plugins\naround LLMs to enhance its ability. Toolformer [19] tries to teach LLMs to choose and use a wide\nrange of tools like calculator and search engines and incorporate the results from tools into text\ngeneration. HuggingGPT [20] builds an agent for leveraging a combination of vision, language and\naudio models hosted on HuggingFace for completing user request. API Bank [12] proposes a syntheic\nbenchmark suite for evaluating the how good LLMs are for using external tools.\nCompared with these tool-augmented LLMs, our agents are tasks for much more complex goals in a\nhigh uncertain open-world.\n3\nMethod\nTraditional RL-based agents struggle to develop generally capable agents in Minecraft. The core issue\nis that they attempt to map extremely long-horizon and complex goals directly to the lowest-level\nkeyboard and mouse operations. To overcome this, we propose LLM-based agents in Fig. 2 that utilize\nhierarchical goal decomposition. LLM Decomposer, LLM Planner, and LLM Interface are introduced\nto progressively decompose the task goal into sub-goals, structured actions, and keyboard\/mouse\noperations. Moreover, LLM-based agents can leverage text-based knowledge and memory to quickly\nacquire the skills needed to master Minecraft.\n3.1\nLLM Decomposer\nRather than directly assigning the task goal to the agent and expecting a comprehensive and robust\naction plan, this work suggests the more practical strategy of decomposing the task goal into a\nseries of more achievable sub-goals. By addressing each constituent sub-goal, the task goal can\nbe progressively achieved. To this end, an LLM Decomposer is proposed. Goals are fed to the\ndecomposer and recursively decomposed into a sub-goal tree. Text-base knowledge provides the\nnecessary information for decomposition.\nGoal Format. Since we aim to obtain all items in Minecraft, all goals can be defined in the format of\n(Object, Count, Material, Tool, Info),\n(1)\nwhere “Object” denotes the target item, “Count” specifies the target quantity. “Material” and “Tool”\nrefer to prerequisites needed to obtain the target item. “Info” stores the text-based knowledge related\nto this goal. Given a specific goal, its sentence embedding extracted from a pre-trained LLM is used\n4\nAction Interface:\n𝑒𝑥𝑝𝑙𝑜𝑟𝑒(𝑜𝑏𝑗𝑒𝑐𝑡)\n𝑚𝑖𝑛𝑒(𝑜𝑏𝑗𝑒𝑐𝑡, 𝑡𝑜𝑜𝑙)\n𝑑𝑖𝑔_𝑑𝑜𝑤𝑛(𝑦_𝑙𝑒𝑣𝑒𝑙, 𝑡𝑜𝑜𝑙)\n...\nQuery Illustration:\nQuery contains a goal, feedback from the \nagent, and reference plan from the memory\nResponse Format:\n{\n“explanation”: “explain action failure”,\n“thoughts”: “thoughts on your plan”,\n“action_list”: [ … ]\n}\nInteraction Guideline:\nYou will receive feedback messages. If \nyour action failed, please replan …\nInstruction\nQuery\nGoal: obtain 1 diamond given iron pickaxe.\nExternal info: Diamond is usually\nfound at level 12 …\nFeedback\nReference plan\nPlan\n{\n“explanation”: “The last action failed because diamond cannot be found on the ground.”,\n“thoughts”: “First I need to dig down to level 12 where diamond is usually found. \nThen I can explore diamond ore underground with branching strategy.\nOnce I find diamond ore, I can mine it with the tool iron pickaxe.”\n“action_list”: [\n{“name”: “dig_down”, “args”: {“y_level”: “12”, “tool”: “iron_pickaxe”}},\n{“name”: “explore”, “args”: {“object”: “diamond_ore”, “strategy”: “branching”}},\n{“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”}\n]\n}\nLarge \nLanguage \nModel\nFeedback\nThe action {“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”} failed\nError message: the target object “diamond_ore” is too far\n- inventory: {‘iron_pickaxe’: 1, …}\n- environment: {‘biome’: forest, ‘y_level’: 12}\nAgent\nMemory\nGoal object: diamond\nReference plan:\n[\n{“name”: “dig_down”, “args”: {“y_level”: “12”, “tool”: “iron_pickaxe”}},\n{“name”: “explore”, “args”: {“object”: “diamond_ore”, “strategy”: “branching”}},\n{“name”: “approach”, “args”: {“object”: “diamond_ore”}},\n{“name”: “mine”, “args”: {“object”: “diamond_ore”, “tool”: “iron_pickaxe”}\n]\nFigure 4: Illustration of our planning process with the LLM Planner and the agent in the loop.\nGiven a specific goal, the planner generates plans with structured actions under the guidance of\ninstruction, user query, previous feedback, and reference plan from memory. The agent executes the\nactions and provides feedback for the following planning.\nto retrieve the most relevant text-based knowledge from an external knowledge base. Then, the LLM\nidentifies the required material, tools, and related information from the gathered knowledge. The\ncomplete instructions for the LLM are described in Appendix.\nRecursive Decomposition. This goal format enables recursive decomposition of each goal into a sub-\ngoal tree. Specifically, given a goal, all prerequisite items are listed as sub-goals, including materials,\ntools, and their corresponding quantities. Then, the recursive decomposition continues for each\nsub-goal until it has no prerequisites. After the decomposition is completed, the execution sequence\nof the sub-goals is planned through post-order traversal. Such goal decomposition significantly\nenhances the success rate of LLM planning, especially for goals necessitating long-horizon planning.\nText-based Knowledge. External knowledge is essential for automatic goal decomposition. We\nbuild an external knowledge base documented in text from the Minecraft Wiki on the Internet 3 and\nthe item crafting\/smelting recipes, providing an exhaustive source of knowledge about the Minecraft\nworld. For instance, if we need to craft a wooden pickaxe, the item crafting recipe will indicate that\nthe required materials are three planks and two sticks, and the necessary tool is a crafting table. It also\nprovides information about the distribution of raw materials. For example, diamonds are frequently\nfound in levels 10∼12 underground.\n3.2\nLLM Planner\nLLMs excel at language understanding and reasoning but struggle with low-level control and mul-\ntimodal perception. To leverage LLMs’ strengths while addressing their limitations, we develop\nstructured actions and feedback mechanisms as an abstract interface for them to manage agent-\nenvironment interaction. We propose an LLM-based Planner to achieve goals in Minecraft. Given a\ngoal, it generates structured actions to control agents, receives feedback, and revises plans accordingly.\nIt also has a text memory that aids planning by providing solutions for frequent goals.\nStructured Actions. The structured actions are designed with well-defined functions and clear\nsemantics, enabling LLMs to make decisions at the cognitive level. A structured action can be defined\nas follows:\n(Name, Arguments, Description),\n(2)\n3https:\/\/minecraft-archive.fandom.com\/wiki\/Minecraft_Wiki\n5\nTable 1: Examples of structured actions. A structured action contains name and arguments for\nexecution, as well as description to help LLMs understand and decide when to choose this action.\nName\nArguments\nDescription\nequip\nobject\nEquip the object from the inventory: used to equip equipment, including tools, weapons, and armor.\nexplore\nobject, strategy\nMove around to find the object: used to find objects including block items and entities on the ground.\napproach\nobject\nMove close to a visible object: used to approach the object you want to attack or mine.\nmine\/attack\nobject, tool\nAttack \/ Mine the object with the tool: used to attack \/ mine the object within reach.\ndig_down\/go_up\nylevel, tool\nDig down \/ Go up with the tool: used to go down \/ up underground.\nbuild\nblueprint\nBuild according to a blueprint: used to place corresponding objects on locations according to a preset blueprint.\ncraft\/smelt\nobject, tool, material\nCraft \/ Smelt the object with the materials and tool: used to craft new object that is not in the inventory or is not enough.\napply\/place\nobject, tool\nApply \/ Place the tool on the object: used to apply tools or place blocks.\nThe name and arguments defines the action we want the agent to execute, while the action description\nprovides enough information for letting LLMs know when to choose the corresponding actions, as\nshown in Tab. 1.\nWe extract the set of structured actions by leveraging the powerful reasoning capability of LLMs.\nSpecifically, a pre-trained LLM is utilized to decompose the 3141 predefined tasks provided by\nMineDojo [5] into action sequences. Instructions for guiding LLMs on action decomposition are\nprovided in Appendix. Then, we extract the structured actions by selecting frequent actions and\nmerging actions with similar functionalities. See Appendix for the set of structured actions.\nFeedback Mechanism. Open-loop planning cannot guarantee success, especially in open-world\nenvironments, where agents might encounter unexpected events. Feedback is crucial to form an effec-\ntive closed loop. Without appropriate feedback, the LLM has no information about the consequences\nof actions and may repeat failed action plans. Feedback message is designed to present the agent’s\ncurrent state in the environment (i.e., inventory and environment), as well as the success and failure\ninformation for each executed actions, as shown in Fig. 4. By incorporating this feedback message,\nthe LLM can update its understanding of the environment, refine their strategies, and adapt their\nbehavior accordingly.\nPlanning. Once the abstract interface is prepared, a pre-trained LLM is queried to generate goal-\nspecific action sequence. This is achieved through carefully designed instructions and user queries,\nenabling the LLM to efficiently create and revise the plans. Fig. 4 illustrates the planning process.\nSee Appendix for the full description.\nInstruction specifies the guidelines that LLMs must follow when planning, including 1) Action\nInterface provides functional descriptions of the structured actions and their parameters; 2) Query\nIllustration clarifies the structure and meaning of user queries; 3) Response Format requires LLM to\nreturn responses in the format of {Explanation, Thought, Action List}, where “Explanation” requires\nLLMs to explain the reason for action failure, “Thought” requires LLM to use natural language to\nplan before outputting action sequences as a chain-of-thought (CoT) mechanism [26], and “Action\nList” outputs a list of structured actions to be executed; 4) Interaction Guideline guides LLMs to\ncorrect failed actions based on the feedback message, thus enabling the LLM to revise the plan.\nUser Query provides the specific query to LLMs for a given goal, including 1) Goal represents the\nobjective by text as “Obtain Count Item, given Material and Tool. Extra info: Info” according to\nEq. (1); 2) Feedback is the feedback information of the abstract interface; 3) Reference Plan provides\na common reference plan for the current goal retrieved from the text-base memory.\nText-based Memory is designed for LLM to maintain common reference plans for each encountered\nobjective as experiential knowledge. LLMs acquire the experience about controlling agents and\nresolving specific situations through game play and agent interaction. Instead of starting from scratch\nevery time, using prior experience allows LLMs to handle tasks more efficiently, a process similar to\nhuman skill improvement through practice.\nTo this end, we design a text-based memory mechanism for LLM to store and retrieve gained knowl-\nedge. Unlike the RL-based model, which stores knowledge in parameters, this textual knowledge is\nexplicit, logical, and closely aligned with human thought processes. This allows for direct application\nto a wide range of similar tasks, leading to more efficient learning and improved generalization.\nSpecifically, during each game episode, once the goal is achieved, the entirely executed action list\nwould be stored in memory. The LLM may achieve the same goal under various circumstances,\nresulting in a range of different plans. To identify a common reference plan suitable for general\nsituations, essential actions from multiple plans are summarized. This summarization process is\n6\nacacia_boat\nacacia_door\nacacia_fence\nacacia_fence_gate\nacacia_stairs\nbeef\nbirch_boat\nbirch_door\nbirch_fence\nbirch_fence_gate\nbirch_stairs\nboat\nbone\nbowl\nchest\nchicken\ncobblestone\ncobblestone_wall\ncooked_beef\ncooked_chicken\ncooked_mutton\ncooked_porkchop\ncrafting_table\ndark_oak_boat\ndark_oak_door\ndark_oak_fence\ndark_oak_fence_gate\ndark_oak_stairs\ndirt\ndouble_plant\nfence\nfence_gate\nfurnace\nglass\nglass_bottle\nglass_pane\nladder\nlever\nlog\nmutton\noak_stairs\npaper\nplanks\nporkchop\nred_flower\nreeds\nsand\nsandstone\nsapling\nsign\nspruce_boat\nspruce_door\nspruce_fence\nspruce_fence_gate\nspruce_stairs\nstick\nstone\nstone_axe\nstone_brick_stairs\nstone_button\nstone_hoe\nstone_pickaxe\nstone_pressure_plate\nstone_shovel\nstone_stairs\nstone_sword\nstonebrick\nsugar\ntallgrass\ntrapdoor\nwheat\nwheat_seeds\nwooden_axe\nwooden_button\nwooden_door\nwooden_hoe\nwooden_pickaxe\nwooden_pressure_plate\nwooden_shovel\nwooden_slab\nwooden_sword\nyellow_flower\nbone_meal\narmor_stand\nbook\nbread\ncoal\niron_ingot\niron_nugget\niron_ore\niron_shovel\nitem_frame\nleather\nrotten_flesh\nshield\nspider_eye\nstone_slab\ntorch\ntrapped_chest\ntripwire_hook\nfireworks\ngunpowder\nsnow\nsnow_layer\nsnowball\ncarpet\ngrass\neavy_weighted_pressure_plate\niron_hoe\niron_sword\nleather_boots\nleaves\npainting\nshears\nstring\nwool\ncoal_block\nleather_helmet\nbrown_mushroom\nbrown_mushroom_block\nflint\nbed\nbucket\nhay_block\niron_axe\niron_pickaxe\nmilk_bucket\nsandstone_stairs\nwater_bucket\nfermented_spider_eye\nleather_leggings\ngravel\nflint_and_steel\nfishing_rod\niron_boots\niron_trapdoor\nleather_chestplate\nmossy_cobblestone\nvine\nwaterlily\nbone_block\nbow\nchest_minecart\nfurnace_minecart\nhopper\niron_helmet\nminecart\ndiamond\ndiamond_shovel\ndropper\njukebox\nnoteblock\nredstone\nredstone_torch\nbanner\nfeather\niron_bars\niron_door\nrail\nbrick\nclay_ball\nlapis_lazuli\npiston\nemerald\ncauldron\niron_leggings\ntnt\ndiamond_hoe\ndiamond_sword\nflower_pot\niron_chestplate\narrow\ncompass\nhopper_minecart\ngold_ingot\ngold_nugget\ngold_ore\ngolden_shovel\niron_block\nbrick_block\nclay\nhardened_clay\nslime_ball\ndispenser\ndiamond_axe\ndiamond_pickaxe\nlava_bucket\nactivator_rail\ndetector_rail\negg\nrepeater\ntnt_minecart\nbookshelf\ngolden_hoe\ngolden_sword\night_weighted_pressure_plate\nredstone_block\nred_mushroom\nred_mushroom_block\nbeetroot\nbeetroot_seeds\ndiamond_boots\ndiamond_helmet\ngolden_axe\ngolden_pickaxe\nsticky_piston\nink_sac\ndiamond_leggings\ngolden_boots\nmushroom_stew\nmap\nbeetroot_soup\nlead\ngolden_helmet\nrabbit_hide\ncooked_rabbit\nrabbit\nbrick_stairs\ncake\nobsidian\ncactus\ndiamond_chestplate\nclock\ndeadbush\nwritable_book\nlapis_block\ngolden_leggings\ngolden_rail\nbaked_potato\npotato\ndiamond_block\ngolden_chestplate\nemerald_block\ncarrot\npumpkin\npumpkin_seeds\njungle_boat\njungle_door\njungle_fence\njungle_fence_gate\njungle_stairs\nlit_pumpkin\ncarrot_on_a_stick\nmelon\nmelon_block\nmelon_seeds\ngolden_carrot\ngold_block\npumpkin_pie\nred_sandstone\nstone_slab2\nred_sandstone_stairs\nspeckled_melon\nenchanting_table\napple\nanvil\nenchanted_book\npoisonous_potato\nrabbit_foot\nslime\ngolden_apple\nrabbit_stew\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nOurs\nDEPS\nDreamerV3\nVPT\nFigure 5: Success rate for all items in the entire Minecraft Overworld Technology Tree. The x\naxis lists all item names. We overlay the results from our GITM and the best results from baselines.\nalso implemented using LLMs (see Appendix for details). When encountering similar goals, the\nLLM creates new plans based on the summarized reference plans retrieved from memory. Successful\naction sequences from these new plans are also added to memory for future summarization. As the\nLLM-based Planner accumulates summaries, it becomes increasingly effective.\n3.3\nLLM Interface\nUnlike the existing RL-based agents that directly control keyboard and mouse, LLM-based agents\ninteract with the environment through structured actions and feedback messages. The LLM interface\nserves to implement structured actions as keyboard\/mouse operations, and extract observations\nprovided by the environment into feedback messages.\nStructured actions can be implemented in various ways such as hand-written scripts or RL-learned\nmodels. While RL-learned models have been employed in Minecraft previously, they were either\nbroad in functionality but inefficient in practice, or too specific in functionality, limiting their\napplicability to general tasks and actions. Clarifying the capability boundary of RL-learned models is\nchallenging. Instead, in this work, we choose to implement structured actions using hand-written\nscripts. Since structured actions are well-defined and easy to implement, we can manually implement\nthem based on observations (e.g., location, LiDAR, and voxel) and basic operations (e.g., move,\njump, adjust camera angle, click left mouse button, and click right mouse button) provided by the\nMineDojo [5] environment. See Appendix for details.\nFeedback messages can be obtained directly from the environment. These include whether the\nstructured action execution succeeded or failed. If the execution fails, the reason for the failure is\nadditionally notified. It also includes the current state of the agent in the environment, including the\nitems in the inventory, the current biome and depth, etc. See Appendix for details.\n4\nExperiments\nTask Definition and Metrics. We measure the ability of GITM through item collection tasks. We\nonly collect items could be found in the Overworld. We exclude items could only be obtained by\ntrading with villagers, opening treasure chest or find a special structure on the map, using a tool\nenchanted with Silk Touch. This give us a total of 262 tasks. For the assessment of our agent, we\nemploy “Coverage of the Overworld Technology Tree” and“Success Rate” as evaluation metrics.\n7\n4.1\nMain Result\nUnlocking the Entire Technology Tree by Obtaining All Items. Compared with existing Minecraft\nagents [2, 7, 25] which mostly focuses on solving the ObtainDiamond task and could only unlock\na limited part of the full technology tree (13\/262 for Dreamerv3, 15\/262 VPT, 69\/262 for DEPS),\nour approach could collect all 262 items as shown in Fig. 1. There are two major blockers for\nexisting methods. For RL-based methods like VPT [2] and DreamerV3 [7], the goal item(diamond)\nis hard-coded into the model weights, which means there are no easy way to re-task the trained\nRL agents for collecting other items in the inference stage. Moreover, the low training efficiency\nhinders them from solving extremely long-horizon tasks (e.g., obtaining a “enchanted_book”). For\nmethods like DEPS [25] that use an RL controller [3] and LLM planner still rely on pre-trained RL\nagents to execute specific subtasks (e.g. mining 1 “cobblestone”) in the generated plan. So these\napproaches still suffer from the inability of RL-based methods alone to generalize to unseen tasks\n(e.g. obtaining “lapis_lazuli”). In contrast, we extract a well-defined set of structured actions by\nusing LLMs to decompose over 3000 predefined MineDojo tasks. This provides broad, open-world\nMinecraft capability. Combined with LLM planning, it enables solving more complex tasks than\nObtainDiamond - which RL cannot achieve. Our knowledge bases also improve efficiency. To our\nknowledge, we present the first agent to unlock the entire Overworld technology tree - a level of\nopen-world skill RL-based methods have not demonstrated.\nSuccess Rate for the Entire Technology Tree. We show the success rate of our method for collecting\nall Overworld items in Fig. 5. Our methods could achieve 100% success rate for simple tasks like\ncollecting wooden tools. It achieves non-zero success rates for all items which indicates a strong\ncollecting capability. The successful rate for collecting different items change smoothly for our agent,\nwhich showcase the robustness of our method against the highly uncertain open world environment.\n4.2\nComparison with Other Minecraft Agents\nTable 2: Comparison of our GITM with pre-\nvious methods on ObtainDiamond challenge.\nMethod\nSuccess Rate (%)\nDreamerV3\n-\n50.0\n3.0\n0.01\n0.01\nDEPS\n90.0\n80.0\n73.3\n10.0\n0.6\nVPT\n100.0 100.0 100.0\n85.0\n20.0\nOur GITM\n100.0 100.0 100.0\n95.0\n67.5\n101\n103\n105\n107\n109\nStep\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate\nVPT\nDreamV3\nOurs\nFigure 6: Comparison of learning efficiency.\nWe compared our LLM-based method with three existing agents: VPT [2], DreamerV3 [7], and\nDEPS [25] on the well known ObtainDiamond challenge, i.e, obtaining a diamond from scratch in\nMinecraft. Previous methods set different time limits of a single episode of game play (20 minutes\nfor VPT, 30 minutes for Dreamerv3, and 10 minutes for DEPS). For fair comparison, we use the\nstrictest limit of previous methods: 10 minutes (12,000 steps at 20Hz control).\nSuccess Rate for Obtaining Diamond and Other Items.\nSince VPT and Dreamerv3 are not\ntargeted for collecting items other than diamond, we mainly compare our method with DEPS for\nitems not related to obtain diamonds. Overall, our GITM and VPT rank task difficulty similarly, but\nDEPS rankings severely fluctuate for tasks more complex than mining coal. Dreamerv3 also behaves\noddly by having an abnormally low success rate on tasks like obtaining a stone sword. As shown in\nFig. 2, most agents performs generally well for easy tasks relating to make wooden tools. VPT could\neven rival with our GITM for the success rate of obtaining iron axes. But for obtaining diamonds, our\nmethod wins over any other methods by 3.5 times on the succeess rate.\nThis giant improvement comes from the following two aspects: First, we employ the strong long-term\nplanning capability of LLMs to decompose the complex tasks into feasible sub-goals and tackle\nthem step by step. Second, our model can directly leverage external knowledge such as the suitable\nlocations for mining ores, while RL models need to explore themselves and may not acquire reliable\nknowledge.\n8\nTable 3: Ablation study. The milestone items from left to right are crafting table\n, wooden\npickaxe\n, stone pickaxe\n, iron pickaxe\n, and diamond\n. The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). “Goal Decomp.” and\n“External Info.” indicates goal decomposition and external knowledge respectively.\nGoal\nDecomp.\nFeedback\nExternal\nInfo.\nMemory\nSuccess Rate (%)\n57.5\n32.5\n5.0\n0.0\n0.0\n✓\n90.0\n90.0\n67.5\n2.5\n0.0\n✓\n✓\n97.5\n95.0\n77.5\n20.0\n5.0\n✓\n✓\n✓\n100.0\n100.0\n100.0\n57.5\n35.0\n✓\n✓\n✓\n✓\n100.0\n100.0\n100.0\n95.0\n67.5\nLearning Efficiency.\nBesides measuring the success rate of each agents, we also compare the\nlearning efficiency of our model with other learnable models. Since DEPS uses a LLM-based planner\nwithout learning mechanism and a pre-trained RL-based controller, its performance could not improve\nwith more episodes played and is excluded from the comparison here.\nIt usually takes tens of millions of steps to train an RL agent by updating parameters before its success\nrate starts to converges to meaningful non-zero numbers. However, the success rate for RL-based\nagents increases rather slowly even after them starts to converge. On the contrary, the learning process\nof our LLM-based agent is considerably faster. As shown in Fig. 6, our method requires several orders\nless episodes than any other methods before doubling its initial success rate. Moreover, our method is\nextremely sample efficient as our success rate raises from 35% to 47.5% by learning from the first five\nthousand steps. By just playing each task several times and summarize successful experience into the\nmemory, the LLM-based agent can acquire explicit experiential knowledge and achieve significantly\nhigher success rate.\n4.3\nAblation Study\nWe conduct ablation experiments on the ObtainDiamond task. We set a time limit of 10 minutes of\ngame play (12000 steps at the control frequency of 20Hz). When leveraging goal decomposition, for\neach sub-goal, we set the maximum number of queries to LLM as 30, and exceeding the query limit\nwill be judged as a failure. For each setting, we run 40 games and calculate the success rate. Tab. 3\nrecords the success rates of achieving the final goal diamond as well as the milestones in this goal,\nincluding crafting table, wooden pickaxe, stone pickaxe, and iron pickaxe.\nGoal Decomposition. Without goal decomposition, the planner can only accomplish several short-\nterm tasks such as obtaining stone axes with rather low success rate of 5%, which indicates the\nnecessity of goal decomposition. Leveraging the powerful long-term planning capabilities of LLMs,\nthe goals are decomposed into sub-goals feasible and practical for the planner, so the success rate for\nobtaining stone axes advances from 5% to 67.5% by leveraging goal decomposition alone.\nFeedback Message. Feedback contains the agent’s state and the execution result of the actions, which\nhelps the planner to understand and make another attempt to correct the mistakes in the previous and\ndeal with special cases. This enables the planner to accomplish a broader range of goals with higher\nsuccess rate. As shown in the 3rd row of Tab. 3, our agent gain the ability to collect diamond by\ncombining feedback with goal decomposition.\nExternal Knowledge Base. External knowledge contains general rules, crafting recipes, and common\ntricks in Minecraft, such as the recipes for crafting iron ingot and iron pickaxe, the suitable location to\nfind diamond ore, and the efficient way to get cobblestone. Providing the planner with this information\ngreatly boosts the success rate of obtaining iron pickaxe and diamond, and the success rate of mining\ndiamond increase by 7 times by learning from the knowledge base that diamonds are more likely to\nappear in specific levels.\nText-based Memory. Leveraging the reference plan recorded in the memory, the planner can handle\nthe task it has encountered more efficiently. The success rates of obtaining iron pickaxe and diamond\nare 95.0% and 67.5%, surpassing the model without memory by 37.5% and 32.5%, respectively.\n9\n5\nConclusion\nWe introduce the GITM framework, which utilizes Large Language Models (LLMs) for hierarchical\ndecomposition of goals. GITM introduces LLM Decomposer, LLM Planner and LLM Interface\nto gradually decompose goals into sub-goals, structured actions and keyboard\/mouse operations.\nThis work makes significant progress towards the ObtainDiamond goal, outperforming all previous\nmethods by a significant margin (+47.5% success rate). This proves the potential inefficiency and\npoorly scalability of Reinforcement Learning (RL) in Minecraft, breaking the traditional reliance\non RL. Moreover, by obtaining all items in Minecraft Overworld, this research marks a critical step\ntoward Generally Capable Agents (GCAs) that match human performance in Minecraft.\nAcknowledgments\nThe work is partially supported by the National Natural Science Foundation of\nChina under grants No.U19B2044, No.61836011, No.62022048, and No.62276150. This work is also\npartially supported by the National Key R&D Program of China under grants NO.2022ZD0114900,\nand the Guoqiang Institute of Tsinghua University.\nReferences\n[1] A. Amiranashvili, N. Dorka, W. Burgard, V. Koltun, and T. Brox. Scaling imitation learning in minecraft.\narXiv preprint arXiv:2007.02701, 2020.\n[2] B. Baker, I. Akkaya, P. Zhokov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune.\nVideo pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639–24654, 2022.\n[3] S. Cai, Z. Wang, X. Ma, A. Liu, and Y. Liang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023.\n[4] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,\nT. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.\n[5] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and A. Anand-\nkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint\narXiv:2206.08853, 2022.\n[6] W. H. Guss, S. Milani, N. Topin, B. Houghton, S. Mohanty, A. Melnik, A. Harter, B. Buschmaas, B. Jaster,\nC. Berganski, et al. Towards robust and domain agnostic reinforcement learning competitions: Minerl\n2020. In NeurIPS 2020 Competition and Demonstration Track, pages 233–252. PMLR, 2021.\n[7] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models. arXiv\npreprint arXiv:2301.04104, 2023.\n[8] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. In International Conference on Machine Learning, pages\n9118–9147. PMLR, 2022.\n[9] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\net al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint\narXiv:2207.05608, 2022.\n[10] A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, et al.\nMinerl diamond 2021 competition: Overview, results, and lessons learned. NeurIPS 2021 Competitions\nand Demonstrations Track, pages 13–28, 2022.\n[11] I. Kanitscheider, J. Huizinga, D. Farhi, W. H. Guss, B. Houghton, R. Sampedro, P. Zhokhov, B. Baker,\nA. Ecoffet, J. Tang, et al. Multi-task curriculum learning in a complex, visual, hard-exploration domain:\nMinecraft. arXiv preprint arXiv:2106.14876, 2021.\n[12] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, and Y. Li. Api-bank: A benchmark for tool-augmented llms.\narXiv preprint arXiv:2304.08244, 2023.\n[13] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies:\nLanguage model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.\n[14] Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, and W. Yang. Juewu-mc: Playing minecraft with sample-efficient\nhierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021.\n10\n[15] H. Mao, C. Wang, X. Hao, Y. Mao, Y. Lu, C. Wu, J. Hao, D. Li, and P. Tang. Seihai: A sample-efficient\nhierarchical ai for the minerl competition. In Distributed Artificial Intelligence: Third International\nConference, DAI 2021, Shanghai, China, December 17–18, 2021, Proceedings 3, pages 38–51. Springer,\n2022.\n[16] Y. Matsuo, Y. LeCun, M. Sahani, D. Precup, D. Silver, M. Sugiyama, E. Uchibe, and J. Morimoto. Deep\nlearning, reinforcement learning, and world models. Neural Networks, 2022.\n[17] S. Milani, N. Topin, B. Houghton, W. H. Guss, S. P. Mohanty, K. Nakata, O. Vinyals, and N. S. Kuno.\nRetrospective analysis of the 2019 minerl competition on sample efficient reinforcement learning. In\nNeurIPS 2019 Competition and Demonstration Track, pages 203–214. PMLR, 2020.\n[18] S. Milani, A. Kanervisto, K. Ramanauskas, S. Schulhoff, B. Houghton, S. Mohanty, B. Galbraith, K. Chen,\nY. Song, T. Zhou, et al. Towards solving fuzzy tasks with human feedback: A retrospective of the minerl\nbasalt 2022 competition. arXiv preprint arXiv:2303.13512, 2023.\n[19] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom.\nToolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n[20] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its\nfriends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n[21] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\nProgprompt: Generating situated robot task plans using large language models.\narXiv preprint\narXiv:2209.11302, 2022.\n[22] A. Skrynnik, A. Staroverov, E. Aitygulov, K. Aksenov, V. Davydov, and A. I. Panov. Hierarchical deep\nq-network from imperfect demonstrations in minecraft. Cognitive Systems Research, 65:74–78, 2021.\n[23] O. E. L. Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz,\nM. Jaderberg, M. Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint\narXiv:2107.12808, 2021.\n[24] C. Tessler, S. Givony, T. Zahavy, D. Mankowitz, and S. Mannor. A deep hierarchical approach to lifelong\nlearning in minecraft. In Proceedings of the AAAI conference on artificial intelligence, 2017.\n[25] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive planning with\nlarge language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.\n[26] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[27] Wikipedia contributors. Minecraft — Wikipedia, the free encyclopedia. https:\/\/en.wikipedia.org\/\nw\/index.php?title=Minecraft&oldid=1155148900, 2023.\n[28] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforcement learning\nand planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\nA\nImplementation Details\nA.1\nLLM Decomposer\nWe use gpt-3.5-turbo from OpenAI API 4 for goal decomposition. The prompt is shown as\nfollows, which consists of two parts: instruction with the role of “SYSTEM” and query with\nthe role of “USER”. The {object quantity}, {object name} and {related knowledge} are\ninjectable slots that will be replace with corresponding texts before fed into the LLM.\n4https:\/\/platform.openai.com\/docs\/api-reference\n11\nSYSTEM:\nYou are an assistant for the game Minecraft.\nI will give you some target object and some knowledge related to the object. Please write the\nobtaining of the object as a goal in the standard form.\nThe standard form of the goal is as follows:\n{\n\"object\": \"the name of the target object\",\n\"count\": \"the target quantity\",\n\"material\": \"the materials required for this goal, a dictionary in the form {material_name:\nmaterial_quantity}. If no material is required, set it to None\",\n\"tool\": \"the tool used for this goal. If multiple tools can be used for this goal, only write\nthe most basic one. If no tool is required, set it to None\",\n\"info\": \"the knowledge related to this goal\"\n}\nThe information I will give you:\nTarget object: the name and the quantity of the target object\nKnowledge: some knowledge related to the object.\nRequirements:\n1. You must generate the goal based on the provided knowledge instead of purely depending\non your own knowledge.\n2. The \"info\" should be as compact as possible, at most 3 sentences. The knowledge I give you\nmay be raw texts from Wiki documents. Please extract and summarize important information\ninstead of directly copying all the texts.\nGoal Example:\n{\n\"object\": \"iron_ore\",\n\"count\": 1,\n\"material\": None,\n\"tool\": \"stone_pickaxe\",\n\"info\": \"iron ore is obtained by mining iron ore. iron ore is most found in level 53. iron ore\ncan only be mined with a stone pickaxe or better; using a wooden or gold pickaxe will yield\nnothing.\"\n}\n{\n\"object\": \"wooden_pickaxe\",\n\"count\": 1,\n\"material\": {\"planks\": 3, \"stick\": 2},\n\"tool\": \"crafting_table\",\n\"info\": \"wooden pickaxe can be crafted with 3 planks and 2 stick as the material and\ncrafting table as the tool.\"\n}\nUSER:\nTarget object: {object quantity} {object name}\nKnowledge: {related knowledge}\nThe recursive decomposition generates a sub-goal tree starting from the final goal object as the root\nnode: if a goal has some prerequisites (materials or tools), for each required material or tool, we add a\nchild node representing the goal of obtaining that material or tool, and then recursively decompose the\nchild node, until there is no more prerequisites. The related knowledge is from: 1) Crafting\/smelting\nrecipes in MineDojo [5], written in the form “Crafting {quantity} {object} requires {material}\nas the material and {tool} as the tool”; 2) Wiki on the Internet 5. We extract the paragraphs with\nkeywords “obtaining”, “mining”, “sources”, etc.\n5https:\/\/minecraft-archive.fandom.com\/wiki\/Minecraft_Wiki\n12\nA.2\nLLM Interface\nInstruction for Extracting Structured Actions. To extract structured actions, we first ask LLM\nto generate a tree-structured action planning for each of the 3141 predefined tasks provided by\nMineDojo, and then converts each action step into a (verb, object, tool, material) tuple.\nDuring decomposition, it is essential to ensure actions are neither too broad nor too specific. We\nadjusted the depth of the action decomposition tree to achieve balance, and empirically set the depth\nas 2 to meet our requirements.\nSpecifically, we use gpt-3.5-turbo from OpenAI API to generate the structured actions. We add\nthe following instruction to the content of “SYSTEM” role to generate the tree-structured plan. We\nadd the goal description, e.g., \"find material and craft a iron pickaxe\", to the content of “USER” role\nand then asks LLM to response according to the requirements.\nSYSTEM:\nYou serve as an assistant that helps me play Minecraft.\nI will give you my goal in the game, please break it down as a tree-structure plan to achieve\nthis goal.\nThe requirements of the tree-structure plan are:\n1. The plan tree should be exactly of depth 2.\n2. Describe each step in one line.\n3. You should index the two levels like ’1.’, ’1.1.’, ’1.2.’, ’2.’, ’2.1.’, etc.\n4. The sub-goals at the bottom level should be basic actions so that I can easily execute them\nin the game.\nUSER:\nThe goal is to {goal description}. Generate the plan according to the requirements.\nAfter that, we extract the action tuple from each sentence of the leaf nodes. We use the following\ninstruction as the content of “SYSTEM” role to extract the tuple, and add the sentence to the content\nof “USER” role.\nSYSTEM:\nYou serve as an assistant that helps me play Minecraft.\nI will give you a sentence. Please convert this sentence into one or several actions according\nto the following instructions.\nEach action should be a tuple of four items, written in the form (’verb’, ’object’, ’tools’,\n’materials’)\n’verb’ is the verb of this action.\n’object’ refers to the target object of the action.\n’tools’ specifies the tools required for the action.\n’material’ specifies the materials required for the action.\nIf some of the items are not required, set them to be ’None’.\nUSER:\nThe sentence is {sentence}. Generate the action tuple according to the requirements.\nThen, we extract the structured actions by selecting frequent actions and merging actions with similar\nfunctionalities. The set of structured actions is {equip, explore, approach, mine\/attack,\ndig_down, go_up, build, craft\/smelt, apply}. Note that we disregard more detailed ac-\ntion decomposition for attack and build to remove overly detailed short-term actions and focus on\nlong-term task completion.\nAction Implementation. The observation of the action contains LiDAR rays with an interval of 5\ndegrees in the horizon and vertical direction for locating objects, and voxels with 10 units radius only\nfor navigation, inventory, life status, and agent location status (X-ray cheating is carefully avoided).\n13\nRGB is not used in our implementation, although it provides more information than LiDAR rays.\nFor example, the biome, and category of the dropping item can not be identified by LiDAR rays.\nSome objects may also be missed by LiDAR due to sparseness of LiDAR rays. We also set the\nbreaking speed to 100 and strength to 100, mainly following [7]. The detailed implementation of\neach structured action is as follows:\n• equip: The equip action calls the environment API to equip the required object. The action\nsucceeds when the API returns success. The action fails when the object is not in inventory or the\nequip API returns failure.\n• explore: The explore action traverses the world until object is visible. This action regards the\nworld as a chessboard, and each node on the chessboard is the center point of a 20×20 units area.\nTwo strategies are implemented depending on whether the agent is on the ground or not. When\nthe agent is on the ground, the BFS explore will be adopted. When the agent is under the ground,\nmainly for exploring ore, the DFS explore will be adopted. In the DFS exploration, the agent will\nbreak the blocks to form a mine road with width of 1 and height of 2. The action succeeds when\nthe object is visible. The action fails when the explore exceeds a preset steps of 10,000 but the\nrequired object is not found.\n• approach: The approach action finds the nearest visible required object and walks towards the\nobject. We adopt A∗algorithm for finding path. The A∗algorithm can jump, translate and fall\nin four directions of north, south, east and west. We also allow the agent to jump while placing a\nblock under the agent for ascent. If the object is out of the voxel observation range, A∗algorithm is\niteratively applied to find the location nearest to the object. The action succeeds when the ℓ∞norm\ndistance between the object and agent is less than 2. The action fails when there is no required\nobject visible or no path can be found to walk close to the object.\n• mine\/attack: The mine\/attack action uses the keyboard attack API with the tools to attack the\nobject. Only visible object could be mined or attacked. The object of mine should be blocks, and\nthe agent will continue mining the block until it is broken. The object of attack should be entities,\nand the agent will iteratively approach and attack the entity until it is killed. After the block is\nbroken or the entity is killed, if there are items dropped by them, the agent will approach the items\nto collect them. The action succeeds when the block is broken or the entity is killed. The action\nfails when there is no visible object, no required tools is in inventory, or the visible object is out of\nattack range.\n• dig_down: The dig_down action iteratively breaks the block underfoot with the tool until the\nrequired ylevel is reached. If the agent is on the ground, before digging down, current location is\nstored for going up action. After the action succeeds, the state of the agent is set to under ground.\nThe action succeeds when the required ylevel is reached. The action fails when it exceeds the reset\nmax steps 10,000 or no required tool is in inventory.\n• go_up: The agent will first go back to the location stored by dig_down. Then, the go_up action\nputs dirt blocks underfoot to raise the agent. After the action is finished, the state of agent is set to\non the ground. The action succeeds when the pre-stored location is reached. The action fails when\nthe walk fails, exceeds the reset max steps 10,000 or there is no required tool in inventory.\n• build: The build action places the required blocks according to a given blueprint from bottom\nto up. The action succeeds when all blocks have been placed. The action fails when there are no\nenough materials in inventory or it is invalid to place some blocks.\n• craft\/smelt: The action calls the environment API to craft\/smelt the required object. The action\nsucceeds when the required object is obtained. The actions fails when there are no enough materials\nin inventory or the agent is unable to place the crafting table\/furnace or the API fails.\n• apply: The apply action calls the keyboard use API, and applies the specific tool to the object, e.g.,\napplying the bucket on water to obtain water bucket. The action succeeds when the API returns\nsuccess. The action fails when there is no visible object, no tool in inventory or the API fails.\nFeedback Message. After the execution of each action, we will get feedback from the structured\nactions. The feedback will refresh the agent’s state in Sec. A.3.2, including current inventory, biome,\nylevel and on\/under the ground status. The feedback will also contain the success\/fail message from\nthese action, as well as the inventory change during the action.\n14\nA.3\nLLM Planner\nHere we present the prompt for planning with LLM. We also use gpt-3.5-turbo from OpenAI API\nas the LLM planner. The model accepts inputs in form of a chat, i.e., the prompt is a dialogue consist-\ning of several messages, each of which contains a role and the content. We set the Instruction\nwith the role “SYSTEM” at the beginning, and use the User Query with the role “USER” to query\nthe LLM for response. The content of the Instruction and User Query are as follows.\nA.3.1\nInstruction\nSYSTEM:\nYou serve as an assistant that helps me play the game Minecraft.\nI will give you a goal in the game. Please think of a plan to achieve the goal, and then write a\nsequence of actions to realize the plan. The requirements and instructions are as follows:\n1. You can only use the following functions. Don’t make plans purely based on your\nexperience, think about how to use these functions.\nexplore(object, strategy)\nMove around to find the object with the strategy: used to find objects including block items\nand entities. This action is finished once the object is visible (maybe at the distance).\nAugments:\n- object: a string, the object to explore.\n- strategy: a string, the strategy for exploration.\napproach(object)\nMove close to a visible object: used to approach the object you want to attack or mine. It may\nfail if the target object is not accessible.\nAugments:\n- object: a string, the object to approach.\ncraft(object, materials, tool)\nCraft the object with the materials and tool: used for crafting new object that is not in the\ninventory or is not enough. The required materials must be in the inventory and will be\nconsumed, and the newly crafted objects will be added to the inventory. The tools like the\ncrafting table and furnace should be in the inventory and this action will directly use them.\nDon’t try to place or approach the crafting table or furnace, you will get failed since this\naction does not support using tools placed on the ground. You don’t need to collect the items\nafter crafting. If the quantity you require is more than a unit, this action will craft the objects\none unit by one unit. If the materials run out halfway through, this action will stop, and you\nwill only get part of the objects you want that have been crafted.\nAugments:\n- object: a dict, whose key is the name of the object and value is the object quantity.\n- materials: a dict, whose keys are the names of the materials and values are the quantities.\n- tool: a string, the tool used for crafting. Set to null if no tool is required.\nmine(object, tool)\nMine the object with the tool: can only mine the object within reach, cannot mine object from\na distance. If there are enough objects within reach, this action will mine as many as you\nspecify. The obtained objects will be added to the inventory.\nAugments:\n- object: a string, the object to mine.\n- tool: a string, the tool used for mining. Set to null if no tool is required.\nattack(object, tool)\nAttack the object with the tool: used to attack the object within reach. This action will keep\ntrack of and attack the object until it is killed.\nAugments:\n- object: a string, the object to attack.\n- tool: a string, the tool used for mining. Set to null if no tool is required.\n15\nequip(object)\nEquip the object from the inventory: used to equip equipment, including tools, weapons, and\narmor. The object must be in the inventory and belong to the items for equipping.\nAugments:\n- object: a string, the object to equip.\ndigdown(object, tool)\nDig down to the y-level with the tool: the only action you can take if you want to go\nunderground for mining some ore.\nAugments:\n- object: an int, the y-level (absolute y coordinate) to dig to.\n- tool: a string, the tool used for digging. Set to null if no tool is required.\ngo_back_to_ground(tool)\nGo back to the ground from underground: the only action you can take for going back to the\nground if you are underground.\nAugments:\n- tool: a string, the tool used for digging. Set to null if no tool is required.\napply(object, tool)\nApply the tool on the object: used for fetching water, milk, lava with the tool bucket, pooling\nwater or lava to the object with the tool water bucket or lava bucket, shearing sheep with the\ntool shears, blocking attacks with the tool shield.\nAugments:\n- object: a string, the object to apply to.\n- tool: a string, the tool used to apply.\n2. You cannot define any new function. Note that the \"Generated structures\" world creation\noption is turned off.\n3. There is an inventory that stores all the objects I have. It is not an entity, but objects can be\nadded to it or retrieved from it anytime at anywhere without specific actions. The mined or\ncrafted objects will be added to this inventory, and the materials and tools to use are also from\nthis inventory. Objects in the inventory can be directly used. Don’t write the code to obtain\nthem. If you plan to use some object not in the inventory, you should first plan to obtain it.\nYou can view the inventory as one of my states, and it is written in form of a dictionary whose\nkeys are the name of the objects I have and the values are their quantities.\n4. You will get the following information about my current state:\n- inventory: a dict representing the inventory mentioned above, whose keys are the name of\nthe objects and the values are their quantities\n- environment: a string including my surrounding biome, the y-level of my current location,\nand whether I am on the ground or underground\nPay attention to this information. Choose the easiest way to achieve the goal conditioned on\nmy current state. Do not provide options, always make the final decision.\n5. You must describe your thoughts on the plan in natural language at the beginning. After\nthat, you should write all the actions together. The response should follow the format:\n{\n\"explanation\": \"explain why the last action failed, set to null for the first planning\",\n\"thoughts\": \"Your thoughts on the plan in natural languag\",\n\"action_list\": [\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"}\n]\n}\nThe action_list can contain arbitrary number of actions. The args of each action should\n16\ncorrespond to the type mentioned in the Arguments part. Remember to add “‘dict“‘ at the\nbeginning and the end of the dict. Ensure that you response can be parsed by Python json.loads\n6. I will execute your code step by step and give you feedback. If some action fails, I will\nstop at that action and will not execute its following actions. The feedback will include error\nmessages about the failed action. At that time, you should replan and write the new code just\nstarting from that failed action.\nA.3.2\nUser Query\nUSER:\nMy current state:\n- inventory: {inventory}\n- environment: {environment}\nThe goal is to {goal}.\nHere is one plan to achieve similar goal for reference: {reference plan}.\nBegin your plan. Remember to follow the response format.\nor Action {successful action} succeeded, and {feedback message}. Continue your\nplan. Do not repeat successful action. Remember to follow the response format.\nor Action {failed action} failed, because {feedback message}. Revise your plan from\nthe failed action. Remember to follow the response format.\nA.4\nMemory\nA.4.1\nLearning Process\nWe maintain the text-based memory with a dictionary, whose keys are sub-goals and values are lists\nof successful action sequences for the corresponding sub-goals. The construction and update of the\nmemory are through the following learning process:\n• When encountering a new sub-goal that is not in the memory, the LLM planner creates plans\nwithout reference. Once the sub-goal is achieved, the entirely executed action sequence would be\nstored into the memory.\n• When encountering a sub-goal with memory, the first action sequence in the recording list for this\ngoal is retrieved as the reference plan, with which the LLM planner tries to achieve the goal. If it\nsucceeds, the new executed action sequence will be added to the last of the recording list.\n• For each sub-goal, once the number of action sequences recorded in its list reaches N, we pop\nall the N sequences and use LLM to summarize them into a common plan solution suitable for\nvarious scenarios, which is then put first in the list. N is set to 5 in all our experiments.\nTo learn the memory for obtaining all items, starting from scratch each time would take a long time.\nIn addition, it is necessary to avoid spending the most of time on learning simple tasks and not\ninvesting enough in learning difficult tasks. To improve the learning efficiency, we suggest to study\nthe sub-goals individually one by one. We first use our LLM Decomposer to generate sub-goal trees\nfor all items, acquiring the set of all sub-goals involved. Then for each sub-goal, the LLM planner\nplays multiple times given its prerequisites including the required materials and tools. The learning\nprocess of the sub-goal is finished once we obtain N = 5 successful action sequences and summarize\nthem into one common plan solution for reference.\nA.4.2\nImplementation of Memory Summarization\nWe also use gpt-3.5-turbo from OpenAI API for memory summarization but in a different\ndialogue. We use the following prompt to instruct the summarization with the role “SYSTEM”. The\nslot {action description} is replaced with the same descriptions of interfaces of the structured\nactions as Sec. A.3.1. We list all the action sequences to be summarized in the query with the role\n“USER”, which is fed into the LLM for response.\n17\nSYSTEM:\nYou serve as an assistant that helps me play the game Minecraft.\nI am using a set of actions to achieve goals in the game Minecraft. I have recorded several\naction sequences successfully achieving a goal in a certain state. I will give you the goal, the\nstate, and the sequences later. Please summarize the multiple action sequences into a single\naction sequence as a universal reference to achieve the goal given that certain state. Here are\nthe instructions:\n1. Each action sequence is a sequence of the following actions:\n{action description}\n2. The action sequences before and after summarization are always conditioned on the given\nstate, i.e., the actions are taken in that certain state to achieve the goal. I will describe the state\nin the following form: State: - inventory: a dict whose keys are the name of the objects and\nthe values are their quantities. This inventory stores all the objects I have. - environment: a\ndict including my surrounding biome and whether I am on the ground or underground.\n3. The action sequence you summarize should be able to achieve the goal in general cases\nwithout specific modification. Every necessary action should be included, even though it does\nnot appear in some sequences because I manually skipped it in some lucky cases. The actions\nredundant or irrelevant to the goal should be filtered out. The corner cases, such as success by\nluck and dealing with contingencies, should not be summarized into the final sequence.\n4. You should describe your thoughts on summarization in natural language at the beginning.\nAfter that, give me the summarized action sequence as a list in JSON format. Your response\nshould follow this form:\nThoughts: \"Your thoughts and descriptions of your summarization\"\nSummarized action sequence:\n[\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"}\n]\nB\nResults of All Items\nWe provide the success rate of all items in the entire Minecraft Overworld Technology Tree in Tab. 4.\nWe have attached a video of obtaining a diamond in the supplementary materials.\nExperiment Setting. Considering the large number of items, including those difficult to be obtained,\nwe implemented an incremental testing strategy. This strategy is designed to keep the testing costs\nwithin a reasonable range, while also accounting for the rarity of certain items. We avoided a uniform\nincrease in the number of tests across all items to accommodate the hardest-to-obtain ones, which\nwould have resulted in prohibitive testing costs. Instead, we employed a incremental testing process.\nFor each item, we begin with 20 games. If the success count is less than or equal to 1, we increase\nto 50 games. If the success count remains less than or equal to 1, we further increase to 100, and\neventually 200 games. This testing continues until the success count finally exceeds 1, or we complete\n200 games. By following this efficient strategy, we ensure a cost-effective and reliable evaluation of\neach item, regardless of its availability. Moreover, because some items need long-term planning and\ncrafting chain, we do not set restrictions on the time limit or query limit.\nExploring Biome. Biomes can be a key factor that strongly influences the success rate. Some items,\nlike cactus, pumpkin, or melon, can only be found in specific biomes. The distribution of biomes\nhighly limits the success rate of some items.\n18\nTable 4: Success rate for all 262 items in the entire Minecraft Overworld Technology Tree.\nItem Name\nSuccess\nRate\nItem Name\nSuccess\nRate\nItem Name\nSuccess\nRate\nItem Name\nSuccess\nRate\nacacia boat\n100.0 stone sword\n100.0 gravel\n80.0\nbeetroot seeds\n40.0\nacacia door\n100.0 stonebrick\n100.0 iron boots\n80.0\ndiamond boots\n40.0\nacacia fence\n100.0 sugar\n100.0 iron trapdoor\n80.0\ndiamond helmet\n40.0\nacacia fence gate\n100.0 tallgrass\n100.0 leather chestplate\n80.0\ngolden axe\n40.0\nacacia stairs\n100.0 trapdoor\n100.0 leather leggings\n80.0\ngolden pickaxe\n40.0\nbeef\n100.0 wheat\n100.0 bone block\n75.0\nred mushroom\n40.0\nbirch boat\n100.0 wheat seeds\n100.0 bow\n75.0\nred mushroom block\n40.0\nbirch door\n100.0 wooden axe\n100.0 chest minecart\n75.0\ndiamond leggings\n35.0\nbirch fence\n100.0 wooden button\n100.0 furnace minecart\n75.0\ngolden boots\n35.0\nbirch fence gate\n100.0 wooden door\n100.0 hopper\n75.0\nink sac\n35.0\nbirch stairs\n100.0 wooden hoe\n100.0 iron helmet\n75.0\nsticky piston\n35.0\nboat\n100.0 wooden pickaxe\n100.0 minecart\n75.0\nbeetroot soup\n30.0\nbone\n100.0 wooden pressure plate\n100.0 mossy cobblestone\n75.0\ngolden helmet\n30.0\nbone meal\n100.0 wooden shovel\n100.0 vine\n75.0\nlead\n30.0\nbowl\n100.0 wooden slab\n100.0 waterlily\n75.0\nmap\n30.0\nchest\n100.0 wooden sword\n100.0 banner\n70.0\nmushroom stew\n30.0\nchicken\n100.0 yellow flower\n100.0 brick\n70.0\nbrick stairs\n25.0\ncobblestone\n100.0 armor stand\n95.0\nclay ball\n70.0\ncactus\n25.0\ncobblestone wall\n100.0 book\n95.0\ndiamond\n70.0\ncake\n25.0\ncooked beef\n100.0 bread\n95.0\ndiamond shovel\n70.0\nclock\n25.0\ncooked chicken\n100.0 coal\n95.0\ndropper\n70.0\ncooked rabbit\n25.0\ncooked mutton\n100.0 fireworks\n95.0\nfeather\n70.0\ndiamond chestplate\n25.0\ncooked porkchop\n100.0 gunpowder\n95.0\niron bars\n70.0\nobsidian\n25.0\ncrafting table\n100.0 iron ingot\n95.0\niron door\n70.0\nrabbit\n25.0\ndark oak boat\n100.0 iron nugget\n95.0\njukebox\n70.0\nrabbit hide\n25.0\ndark oak door\n100.0 iron ore\n95.0\nlapis lazuli\n70.0\ndeadbush\n20.0\ndark oak fence\n100.0 iron shovel\n95.0\nnoteblock\n70.0\ngolden leggings\n20.0\ndark oak fence gate 100.0 item frame\n95.0\npiston\n70.0\ngolden rail\n20.0\ndark oak stairs\n100.0 leather\n95.0\nrail\n70.0\nlapis block\n20.0\ndirt\n100.0 rotten flesh\n95.0\nredstone\n70.0\nwritable book\n20.0\ndouble plant\n100.0 shield\n95.0\nredstone torch\n70.0\nbaked potato\n15.0\nfence\n100.0 spider eye\n95.0\ncauldron\n65.0\ncarrot\n15.0\nfence gate\n100.0 stone slab\n95.0\ndiamond hoe\n65.0\ndiamond block\n15.0\nfurnace\n100.0 torch\n95.0\ndiamond sword\n65.0\nemerald block\n15.0\nglass\n100.0 trapped chest\n95.0\nemerald\n65.0\ngolden chestplate\n15.0\nglass bottle\n100.0 tripwire hook\n95.0\niron leggings\n65.0\npotato\n15.0\nglass pane\n100.0 carpet\n90.0\ntnt\n65.0\npumpkin\n15.0\nladder\n100.0 coal block\n90.0\narrow\n60.0\npumpkin seeds\n15.0\nlever\n100.0 grass\n90.0\ncompass\n60.0\ncarrot on a stick\n10.0\nlog\n100.0 heavy weighted pressure plate\n90.0\nflower pot\n60.0\njungle boat\n10.0\nmutton\n100.0 iron hoe\n90.0\niron chestplate\n60.0\njungle door\n10.0\noak stairs\n100.0 iron sword\n90.0\nbrick block\n55.0\njungle fence\n10.0\npaper\n100.0 leather boots\n90.0\nclay\n55.0\njungle fence gate\n10.0\nplanks\n100.0 leather helmet\n90.0\ndispenser\n55.0\njungle stairs\n10.0\nporkchop\n100.0 leaves\n90.0\ngold ingot\n55.0\nlit pumpkin\n10.0\nred flower\n100.0 painting\n90.0\ngold nugget\n55.0\nmelon\n10.0\nreeds\n100.0 shears\n90.0\ngold ore\n55.0\nmelon block\n10.0\nsand\n100.0 snow\n90.0\ngolden shovel\n55.0\nmelon seeds\n10.0\nsandstone\n100.0 snow layer\n90.0\nhardened clay\n55.0\ngold block\n8.0\nsapling\n100.0 snowball\n90.0\nhopper minecart\n55.0\ngolden carrot\n8.0\nsign\n100.0 string\n90.0\niron block\n55.0\npumpkin pie\n8.0\nspruce boat\n100.0 wool\n90.0\nslime ball\n55.0\nred sandstone\n6.0\nspruce door\n100.0 bed\n85.0\nactivator rail\n50.0\nred sandstone stairs\n6.0\nspruce fence\n100.0 brown mushroom\n85.0\ndetector rail\n50.0\nspeckled melon\n6.0\nspruce fence gate\n100.0 brown mushroom block\n85.0\ndiamond axe\n50.0\nstone slab2\n6.0\nspruce stairs\n100.0 bucket\n85.0\ndiamond pickaxe\n50.0\nanvil\n4.0\nstick\n100.0 flint\n85.0\negg\n50.0\napple\n4.0\nstone\n100.0 hay block\n85.0\nlava bucket\n50.0\nenchanting table\n4.0\nstone axe\n100.0 iron axe\n85.0\nrepeater\n50.0\nenchanted book\n3.0\nstone brick stairs\n100.0 iron pickaxe\n85.0\ntnt minecart\n50.0\npoisonous potato\n2.0\nstone button\n100.0 milk bucket\n85.0\nbookshelf\n45.0\ngolden apple\n1.0\nstone hoe\n100.0 sandstone stairs\n85.0\ngolden hoe\n45.0\nrabbit foot\n1.0\nstone pickaxe\n100.0 water bucket\n85.0\ngolden sword\n45.0\nslime\n1.0\nstone pressure plate 100.0 fermented spider eye\n80.0\nlight weighted pressure plate\n45.0\nrabbit stew\n0.5\nstone shovel\n100.0 fishing rod\n80.0\nredstone block\n45.0\nstone stairs\n100.0 flint and steel\n80.0\nbeetroot\n40.0\nC\nSupplementary Ablations\nWe make a more detailed comparison between our GITM with RL-based methods in Tab. 5. The most\nstraightforward pipeline is to directly map the goal into keyboard\/mouse operations. We gradually\nadd goal decomposition and structured action stages into the pipeline, and ablate the use of RL-based\nmodels or LLM.\n19\nTable 5: Ablation study. The milestone items from left to right are crafting table\n, wooden\npickaxe\n, stone pickaxe\n, iron pickaxe\n, and diamond\n. The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). “Goal Decomp.”\nindicates whether to use LLM Decomposer to decompose the goal into sub-goals. “Goal \/ Sub-Goal\nto Structured Actions \/ Keyboard & Mouse Mapping” indicates which method is used for the mapping\nfrom goal \/ sub-goals to structured actions \/ keyboard & mouse operations.\nGoal\nDecomp.\nStructured\nAction\nGoal \/ Sub-Goal to\nStructured Actions \/ Keyboard & Mouse\nMapping\nSuccess Rate (%)\n(a)\nSpecialist RL Model (VPT)\n100.0\n100.0\n100.0\n85.0\n20.0\n(b)\nGoal-conditioned RL Model (DEPS)\n0.0\n0.0\n0.0\n0.0\n0.0\n(c)\nOur LLM Planner\n0.0\n0.0\n0.0\n0.0\n0.0\n(d)\n✓\nGoal-conditioned RL Model (DEPS)\n90.0\n80.0\n30.0\n0.0\n0.0\n(e)\n✓\nOur LLM Planner\n0.0\n0.0\n0.0\n0.0\n0.0\n(f)\n✓\nOur LLM Planner\n57.5\n32.5\n5.0\n0.0\n0.0\n(g)\n✓\n✓\nOur LLM Planner\n100.0\n100.0\n100.0\n95.0\n67.5\nImplementation Details. We can only find open-sourced RL models from VPT [2] and DEPS [25],\nso they are adopted for the ablation. VPT model is specifically trained for the ObtainDiamond\nchallenge, while DEPS model can use goal description as input to guide the model’s output. We refer\nto them as specialist RL model and goal-conditioned RL model, respectively. As for the use of LLM\nPlanner, we note that if structured action is not used, LLM Planner will be inevitably asked to output\nreasonable keyboard\/mouse operations. However, LLM Planner does not have access to environment\nobservations, so it cannot directly output reasonable keyboard\/mouse operations.\nDirect Mapping. See Tab. 5(a)(b)(c). It is hard to directly mapping the long-horizon goal into\nreasonable keyboard\/mouse operations. While a specialist RL model (i.e., VPT) can deliver promising\nresults, it requires large amount of data and computational resources to train such a model [2] (720\nV100 GPUs for 9 days). Moreover, a different goal will require further training of the specialist RL\nmodel, limiting the versatility of this paradigm. The goal conditional RL model (i.e., DEPS) cannot\nachieve the goal, because the model [25] we have access to is not generalizable to all scenarios.\nIf only the final goal is given, it will ignore preconditions, such as not crafting the necessary iron\npickaxe when mining diamonds. LLM also fails to accomplish the goal. The primary reason is that it\ncan not handle environment observation and keyboard\/mouse operations well.\nStructured Action. We design structured actions to interact with the environment, and provide\nan abstract interface. Tab. 5(f) shows that adding structured action significantly improves LLM’s\nperformance. This is because structured actions can deal with environment observations and key-\nboard\/mouse operations more precisely, unleashing the reasoning potential of LLM. We are not aware\nof a RL model using structured actions currently. It is possible for structure actions to enhance the\nRL model as well, and we will explore it in the future work.\nGoal Decomposition. Decomposing the goal into sub-goals can simplify the whole task. Tab. 5(b)(d)\nand Tab. 5(f)(g) show its effectiveness for both goal-conditioned RL model and our method. By\nexploiting goal decomposition, it is possible for our method to accomplish long-term tasks with high\nsuccess rate.\nComparison between RL-based methods. We also note the paradigm shift from traditional RL-\nbased methods to our GITM leads to a great performance boost. Comparing Tab. 5(d)(g), where we\nonly change the goal-conditioned RL model to LLM with strutured actions, our method significantly\noutperforms the RL model.\nD\nObtainDiamond\nWe demonstrate a case of the popular ObtainDiamond challenge in Fig. 7. During the process, the\nagent have to collect materials, i.e., log, stone and iron ore, as shown in Fig 7(a)(c)(e). Necessary tools,\ni.e., wooden pickaxe, stone pickaxe, furnace and iron pickaxe are also crafted in Fig 7(b)(d)(f)(h).\n20\n(a) mine log\n(b) craft wooden_pickaxe\n(c) mine stone\n(d) craft stone_pickaxe\n(e) mine iron_ore\n(f) craft furnace\n(g) smelt iron_ingot\n(h) craft iron_pickaxe\n(i) mine diamond\nFigure 7: A case of the popular ObtainDiamond challenge. Figure(e)(i) are enhanced in brightness\nfor better display.\nFinally the diamond is obtained in Fig 7(i). We have attached a video of obtaining a diamond in the\nsupplementary materials.\nE\nApplications\n(a) Shelter with Farmland\n(b) Iron Golem\n(c) Redstone Circuit\n(d) Nether Portal\nFigure 8: Demonstration of the applications. GITM can construct Shelter with Farmland and\nIron Golem for survival, Redstone Circuit for automation equipment, and Nether Portal for\nthe Nether world exploration.\nOur proposed GITM makes survival and the nether exploration possible in Minecraft which has\nnever been accomplished by existing agents. To achieve this, our agent builds four necessary items,\nincluding Shelter with Farmland, Iron Golem, Redstone Circuit, and Nether Portal,\nshown in Fig. 8. Shelter with Farmland is firstly built to keep the agent from being attacked\nby monsters at night and provide enough food. Iron Golem can automatically attack monsters to\nprotect the agent and the shelter. Redstone Circuit is the foundation of all automation equipment.\nNether Portal is the entrance to the Nether world.\n21\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.pdf"}
{"title":"Creative Agents: Empowering Agents with Imagination for Creative Tasks","authors":"Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, Zongqing Lu","summary":"We study building embodied agents for open-ended creative tasks. While\nexisting methods build instruction-following agents that can perform diverse\nopen-ended tasks, none of them demonstrates creativity -- the ability to give\nnovel and diverse task solutions implicit in the language instructions. This\nlimitation comes from their inability to convert abstract language instructions\ninto concrete task goals in the environment and perform long-horizon planning\nfor such complicated goals. Given the observation that humans perform creative\ntasks with the help of imagination, we propose a class of solutions for\ncreative agents, where the controller is enhanced with an imaginator that\ngenerates detailed imaginations of task outcomes conditioned on language\ninstructions. We introduce several approaches to implementing the components of\ncreative agents. We implement the imaginator with either a large language model\nfor textual imagination or a diffusion model for visual imagination. The\ncontroller can either be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes in the environment. We\nbenchmark creative tasks with the challenging open-world game Minecraft, where\nthe agents are asked to create diverse buildings given free-form language\ninstructions. In addition, we propose novel evaluation metrics for open-ended\ncreative tasks utilizing GPT-4V, which holds many advantages over existing\nmetrics. We perform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accomplishing diverse\nbuilding creation in the survival mode of Minecraft. Our benchmark and models\nare open-source for future research on creative agents\n(https:\/\/github.com\/PKU-RL\/Creative-Agents).","url":"http:\/\/arxiv.org\/abs\/2312.02519v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.02519v1","published":1701756052000,"comment":"The first two authors contribute equally","pdf_text":"Creative Agents: Empowering Agents with Imagination for Creative Tasks\nChi Zhang1*\nPenglin Cai1∗\nYuhui Fu2\nHaoqi Yuan1\nZongqing Lu1,3†\n1Peking University\n2Tsinghua University\n3BAAI\nAbstract\nWe study building embodied agents for open-ended cre-\native tasks.\nWhile existing methods build instruction-\nfollowing agents that can perform diverse open-ended\ntasks, none of them demonstrates creativity – the ability\nto give novel and diverse task solutions implicit in the lan-\nguage instructions. This limitation comes from their inabil-\nity to convert abstract language instructions into concrete\ntask goals in the environment and perform long-horizon\nplanning for such complicated goals. Given the observa-\ntion that humans perform creative tasks with the help of\nimagination, we propose a class of solutions for creative\nagents, where the controller is enhanced with an imagina-\ntor that generates detailed imaginations of task outcomes\nconditioned on language instructions. We introduce sev-\neral approaches to implementing the components of cre-\native agents. We implement the imaginator with either a\nlarge language model for textual imagination or a diffu-\nsion model for visual imagination. The controller can ei-\nther be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes\nin the environment. We benchmark creative tasks with the\nchallenging open-world game Minecraft, where the agents\nare asked to create diverse buildings given free-form lan-\nguage instructions. In addition, we propose novel evalu-\nation metrics for open-ended creative tasks utilizing GPT-\n4V, which holds many advantages over existing metrics. We\nperform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accom-\nplishing diverse building creation in the survival mode of\nMinecraft. Our benchmark and models are open-source for\nfuture research on creative agents (https:\/\/github.com\/PKU-\nRL\/Creative-Agents).\n1. Introduction\nBuilding open-ended embodied agents has been a long-\nstanding goal of AI research.\nUnlike many existing AI\nagents that perform a fixed set of tasks specified with re-\n*Equal contribution\n†Correspondence to B: zongqing.lu@pku.edu.cn\nwards [1, 19], open-ended agents can perform diverse arbi-\ntrary tasks without such specification. Existing research pri-\nmarily focuses on learning instruction-following agents [4,\n11, 29] that can solve open-ended tasks given free-form\nlanguage instructions, achieving success in robotic do-\nmains [4, 11, 24] and open-world games [6, 29, 45]. How-\never, these agents can only follow clear instructions that rep-\nresent specific goals or behaviors. Creative tasks, where the\ninstructions describe abstract tasks and the agent is required\nto generate complicated, novel, and diverse solutions, bring\nnew challenges to intelligent agents.\nAs an example, in the open-world game Minecraft, ex-\nisting agents can follow simple and clear instructions like\n‘harvest a stone’ [52] and ‘build a snow golem, which stacks\n2 snow blocks and 1 pumpkin’ [7], but they cannot solve\ncreative tasks like ‘build a sandstone palace’. For the latter,\nthe agent can struggle to understand the target outcome of\nthe task implied in the abstract instruction and plan actions\nfor the long-horizon execution where hundreds of blocks\nshould be properly placed. However, empowered with the\nability of imagination, humans can first imagine the appear-\nance and functionality of the building, then plan for a proper\norder to build blocks and realize the imagined house in the\ngame. Such ability enhances humans with strong creativ-\nity, enabling humans to create novel and diverse outcomes.\nImagination also enriches the fuzzy instructions into refined\ntask outcomes grounded in the environment, making the\ntask description more explicit and executable.\nMotivated by this ability, we introduce a framework for\ncreative agents, empowering open-ended agents with imagi-\nnation to solve creative tasks. Figure 1 gives an overview of\nthe framework. Equipped with a text-conditioned imagina-\ntor, creative agents can imagine the details of the task out-\ncome abstracted in the language instruction. These imag-\ninations serve as a blueprint for the controller to interpret\nand act upon. We propose two variants of the imaginator,\nincluding a large language model (LLM) [5] generating text\nimaginations and a finetuned diffusion model [38] generat-\ning grounded visual imaginations. We also introduce two\nvariants of the controller that transform the imagination into\nexecutable plans. The first is a behavior-cloning controller\ntrained on an environment dataset and maps imaginations\n1\narXiv:2312.02519v1  [cs.AI]  5 Dec 2023\n“build a \nsandstone \npalace”\nGPT-4(V)\nCode Gen.\nBC\nController\nImaginator\nController\nCreative Agents\nCreative Task\nCreation\n“1. Size: 10x10x12\n2. Made of sandstone\n3. It has an entrance”\n𝐼𝑔𝑙\n𝜋𝑎𝑠, 𝑔, 𝑙\n①\n②\n③\nInstruction (𝑙)\nImagination (𝑔)\nLLM CoT\nDiffusion\nModel\nFigure 1. Overview of creative agents for open-ended creative tasks. A creative agent consists of two components: an imaginator and a\ncontroller. Given a free-form language instruction describing the creative task, the imaginator first generates the imagination in the form of\ntext\/image by LLM with Chain-of-Thought (CoT)\/diffusion model, then the controller fulfills the imagination by executing actions in the\nenvironment, leveraging the code generation capability of vision-language model (VLM) or a behavior-cloning (BC) policy learned from\ndata. We implement three combinations of the imaginator and controller: ①CoT+GPT-4, ②Diffusion+GPT-4V, and ③Diffusion+BC.\nto actions. The second method leverages the strong abili-\nties in vision-language understanding [51] and code gener-\nation [44] of the large vision-language model (VLM) GPT-\n4V [34]. The VLM controller receives the imagination as\nthe task goal and generates code to perform actions in the\nenvironment.\nDesigning evaluation metrics for open-ended tasks re-\nmains underexplored.\nExisting methods either use some\nsurrogate metrics [44] which may not reflect the language\ninstruction, or use human evaluation [7] which is labori-\nous. Fan et al. [13] proposes to use the similarity of the\nCLIP [37] embedding between vision and language, which\nhowever can only provide some unknown correlation be-\ntween the instruction and task outcome. To address these\nlimitations, we propose novel evaluation metrics based on\nGPT-4V. Leveraging the analytical strength of GPT-4V, our\nmetrics offer an effective, general, and human-independent\nmeans of evaluation. We verify that such metrics are con-\nsistent with human evaluations. Our proposed metrics are\ncrucial for objectively measuring the creativity and effec-\ntiveness of solutions generated by open-ended agents.\nWe benchmark creative tasks with challenging building\ncreation in Minecraft1, following 20 diverse instructions.\nSeveral variants of creative agents demonstrate their abil-\nity to create diverse and visually appealing buildings in the\nsurvival mode of Minecraft, which has never been achieved\nin previous studies. We give a detailed experimental analy-\n1We select the open-world game Minecraft as the benchmark platform\nbecause it is convenient to build various imaginators and controllers and\nalso supports creation in the game. Specifically, we choose the survival\nmode of Minecraft, where it is difficult for the agent to construct buildings\nsince the agent has to move around and go up\/down to place the blocks\nwith diverse materials and colors, making the building process realistic. It\nis worth noting that our framework for creative agents is general and can\nalso be applied to other environments.\nsis of creative agents, discuss the strengths and weaknesses\nof each variant, and provide insights for improving creative\nagents in future work.\nOur main contributions are threefold:\n• We propose creative agents, the first framework that en-\ndows open-ended agents with the ability to perform cre-\native tasks through imagination. Our method builds the\nfirst instruction-following agent that can create diverse\nbuildings in the survival mode of Minecraft.\n• We establish novel evaluation metrics for creative tasks\nin open-ended environments, in which GPT-4V is used as\nthe evaluator.\n• By open-sourcing the datasets and models, our work sets\na new benchmark for future research in the field of open-\nended learning and creative AI agents.\n2. Preliminaries\n2.1. Open-Ended Tasks\nWe formalize the process of the agent interacting with the\nenvironment as a Markov Decision Process (MDP) without\nreward, defined by a tuple M = (S, A, P, ρ) representing\nstates, actions, the transition function of the environment,\nand the initial state distribution, respectively. Starting from\nthe initial state, for each time step, the agent performs an\naction based on the state, then the environment transitions\nto the next state upon the action.\nCompared with traditional reinforcement learning tasks\ndefined with reward functions, open-ended tasks have nei-\nther fixed targets nor optimal solutions. We follow Fan et al.\n[13], formulating open-ended tasks as instruction-following\nproblems T = (L, M), where l ∈L is a free-form language\ninstruction.\nWe aim to acquire an instruction-following\nagent P(a|s, l) which can exhibit behaviors consistent with\n2\nthe instruction to perform the described task.\n2.2. Creative Agents with Imagination\nDue to the abstract nature of language, language instruc-\ntions cannot describe the full details of complicated tasks,\ndrawing high uncertainty on the task completion and requir-\ning the agent to possess creativity.\nThough many open-\nended agents [6, 11, 29] can follow clear instructions that\nrefer to some specific task goals, none of them can follow\nsuch uncertain instructions to perform complicated tasks.\nWe define creative tasks as a challenging case of open-\nended tasks, where language instructions lack information\nto describe the whole task and can refer to diverse, novel,\nand complicated outcomes in the environment. Such in-\nstructions bring uncertainty for the agent and require the\nability to imagine the details unspecified by the instruction.\nIn addition, a short instruction (e.g. ‘build a house’) may re-\nfer to a long-horizon complicated task, increasing the chal-\nlenge for the action planning and execution.\nTo tackle the challenge, we propose to decompose the\nagent into an imaginator and a controller:\nP(a|s, l) =\nX\ng\nI(g|l)π(a|s, g, l).\n(1)\nHere, g ∈G is an imagination of the task outcome, which\ncan be in the form of diverse modalities (e.g. text, image)\nand serves as a description of the target environment state\nof the task. The imaginator I converts the instruction into\nan imagined outcome, providing the controller π with a de-\ntailed task description. Therefore, we leave the uncertainty\nand creativity brought from creative tasks to the imaginator,\nproviding the controller with richer task information to re-\nduce its uncertainty. By disentangling these two models, we\ncan delve deeper into the design choices for each part and\ncombine them together to build creative agents.\n3. Generative Imagination\nGenerative models in natural language processing and com-\nputer vision provide techniques to build the imaginator in\neither text space or image space. In this section, we present\ntwo variants for implementing the imaginator.\n3.1. Language Models for Textual Imagination\nLarge language models (LLMs) have shown marvelous\nabilities in solving diverse tasks [9, 46] as well as high plas-\nticity with prompt engineering [5, 48]. To tackle the prob-\nlems in reasoning logically, Wei et al. [47] proposed Chain-\nof-Thought (CoT), aimed at enhancing the emergence abil-\nity of LLMs.\nFollowing the idea of zero-shot-CoT [28], we design\nan imaginator using GPT-4 [34] as the backbone, with zero-\nshot prompts for imagination in Minecraft building-creation\ndomain (please refer to Appendix B). Specifically, we pro-\nvide the initial text instruction to GPT-4 and ask five ques-\ntions relevant to the imagination, including the material\nused for the building, the approximate size, the significant\nfeatures of the architecture, etc. After GPT-4 generates an-\nswers to these questions indicating that the imagination pro-\ncess has been finished, we then ask the controller to exe-\ncute actions accordingly to construct the building (see Sec-\ntion 4).\n3.2. Diffusion Models for Visual Imagination\nDiffusion models have achieved breakthrough performance\nin generating diverse and high-quality images. Stable Dif-\nfusion [38] models data distribution as the stationary state\nof a diffusion process, learning to generate samples mir-\nroring the true data distribution by reversing this process.\nNoteworthy for its training stability, it addresses issues like\nmode collapse.\nTo better align with the human conception of “imagi-\nnation”, we use images to be the imagination space and\nleverage text-conditioned diffusion models to be the imag-\ninator. We finetune the Stable Diffusion [38] using a text-\nimage dataset to achieve a reasonable and diverse imagina-\ntion of textual input. The text-image pairs in the dataset\nare constructed by automatically annotating the Minecraft\nbuildings in CraftAssist [16] using the multimodal Emu\nmodel [42]. After finetuning, we obtain visually plausible\nand diverse imaginations that align with both the textual de-\nscriptions and the Minecraft world.\n4. Designing Controllers\nAfter the imaginator generates the imagination g, it is the\ncontroller to take actions in the environment, conditioned\non the current state, the imagination, and the language in-\nstruction. In the following, two variants of the controller\nare presented, including a behavior-cloning controller and a\ncontroller based on GPT-4(V).\n4.1. Behavior-Cloning Controller\nTo transform the imagination into a practical construction\nprocess, we introduce a behavior-cloning controller that\nfirst converts the image imagination into a blueprint and\nthen maps the blueprint into tangible actions.\nFor tasks related to constructing buildings in Minecraft,\nwe use voxel information as the basis for blueprints. To\nlearn a module generating voxel blueprints conditioned\non images, we adopt the methodology introduced by\nPix2Vox++ [50], utilizing the image-voxel dataset con-\nstructed through data augmentation from original construc-\ntions in CraftAssist [16] and ISM [14].\nThe module is\ntrained to optimize a combination of the voxel predic-\ntion loss and two regularization terms, including the occu-\npancy rate loss [36] and the total variation loss [39, 49].\n3\nSubsequently, for the construction process, we employ\nResNet3D-CNN[20] and train a behavior-cloning (BC) pol-\nicy on a collected voxel-action dataset. After that, the fi-\nnal construction is executed by the BC policy conditioned\non the voxel information through path-searching and block-\nplacing within the MineDojo simulator [13]. More details\nabout our methods and datasets are available in Appendix B.\n4.2. Vision-Language Models as Controller\nWe also adopt a generative vision-language model (VLM)\nto construct the controller, which can perceive both visual\nimaginations and textual imaginations. Utilizing its abili-\nties in task reasoning and code generation, given an envi-\nronment code interface that wraps actions, the VLM can\ngenerate executable code in the environment for task com-\npletion.\nSpecifically, we use GPT-4(V) which takes as input an\nimage generated by the diffusion imaginator or the textual\nimagination generated by the LLM with CoT. We ask GPT-\n4(V) to generate code that can call Mineflayer [35] APIs to\nexecute environment actions for building creation. Mine-\nflayer implements JavaScript APIs for diverse skill primi-\ntives in the Minecraft world. Following the prompt design\nin Voyager [44], we provide GPT-4(V) with API documen-\ntation to clarify the coding rules and a one-shot example of\ncode generation for in-context learning. More details about\nthe prompts are available in Appendix B.\nWith this controller, we implement creative agents in\nboth two modalities of imaginations.\n5. Experiments\n5.1. Building Creation in Minecraft\nInspired by the creative tasks in MineDojo [13], we set\nup an evaluation benchmark for constructing buildings in\nMinecraft, consisting of 20 diverse language instructions,\nsuch as “a huge Minecraft volcano built of ice” as illustrated\nin Figure 4. Following the text description, the agent takes\nactions to move and place blocks in the game simulator to\ncreate buildings. In the experiment, we aim to investigate\nwhether the agent can construct novel, diverse buildings by\njust following language instructions, which reflects the cre-\nativity of the agent. In the evaluation, we take screenshots\nof its creations in the game. More details can be found in\nAppendix A. We setup various metrics to evaluate the open-\nended building creation tasks and apply two evaluators, in-\ncluding human evaluators and a novel evaluator based on\nGPT-4V. Section 5.3 presents the evaluation details.\n5.2. Implementation\nWe implement several variants of creative agents using dif-\nferent combinations of imaginators and controllers (more\nCorrectness\nComplexity\nQuality\nFunctionality\nRobustness\n6.95\n4.90\n6.35\n4.15\n4.07\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\nFigure 2.\nComparison of all variants of creative agents in\nMinecraft building creation. For each evaluation metric, the num-\nber denotes the average score of the best agent over the 20 tasks.\nDiffusion+GPT-4V performs relatively better than other variants.\ndetails are provided in Appendix B) and build a baseline\nmethod to compare with:\n• Vanilla GPT-4.\nThis is the baseline method without\nimagination using GPT-4 as the controller. We simply\nreplace the textual imagination with the original task in-\nstruction and ask GPT-4 to perform code generation.\n• CoT+GPT-4. We implement this agent by adding a CoT-\nimagination on the basis of Vanilla GPT-4, which means\nwe use GPT-4 for both textual imagination and code gen-\neration (method ①in Figure 1).\n• Diffusion+GPT-4V2. We use a finetuned Stable Diffu-\nsion to generate images as imagination and use GPT-4V\nas the controller to generate codes based on the visual\nimagination (method ②in Figure 1).\n• Diffusion+BC. The finetuned Stable Diffusion is used as\nthe imaginator while the behavior-cloning controller is\nused to convert images into voxel blueprints and execute\nactions.(method ③in Figure 1).\n5.3. Evaluation Metrics\nBased on existing evaluation methods in open-ended learn-\ning [30] and content generation in Minecraft [40], we intro-\nduce a set of evaluation aspects, which are important criteria\nfor creative agents:\n• Correctness. Are the creations consistent with the lan-\nguage instruction?\n• Complexity.\nCan the agent create large and complex\nbuildings?\n2We use GPT-4V here to indicate the agent additionally takes an image\nimagination as input.\n4\nDiffusion+GPT-4V CoT+GPT-4\nDiffusion+BC\nVanilla GPT-4\n1000\n1100\n1200\n1300\n1400\n1500\n1600\nElo Rating\n1531.0\n1441.0\n1395.0\n1233.0\nElo Ratings of Different Agents\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\n16\n17\n18\n19\n20\n21\n22\n23\n24\nAverage Score\n18.1\n21.2\n22.4\n19.8\n19.1\n21.1\n23.2\n17.8\nAverage Score of Different Agents\nGPT-4V\nHumans\nFigure 3. Evaluation results in Minecraft building creation. Left: The Elo Rating of all agents based on the evaluation of GPT-4V. Right:\nThe average overall score of each agent in all test tasks evaluated by GPT-4V and humans.\n• Quality. Do the creations have a good visual appearance\nfrom the perspective of aesthetics?\n• Functionality. Do the created buildings have the neces-\nsary functions and structures (such as windows and en-\ntrances)?\nTo quantitatively evaluate such metrics, recent work [7]\nrequires humans to perform evaluation, which however is\nlabor-intensive and may be susceptible to subject prefer-\nences. To tackle these issues, we leverage the strong ca-\npabilities of the recent VLMs in vision-language reasoning\nand vision question answering and propose two VLM-based\nevaluation methods.\nIn the first method, given a language instruction, we sam-\nple a pair of creation results from two methods and fill in a\ntemplate to ask the VLM which one is better overall based\non all evaluation metrics:\nYou are a critic with high aesthetic ability. I\nwill provide you a text instruction and two\nbuildings created by different agents\nfollowing this instruction in the game \"\nMinecraft\".\nPlease evaluate their overall performance\naccording to four aspects: $(\nEVALUATION_ASPECT 1˜4). Tell me which\nbuilding in the image is better (left or\nright).\nText: $(INSTRUCTION)\nImage of buildings: $(IMAGE1, IMAGE2)\nWe use the Elo Rating System [12] to measure the relative\nstrength of each agent.\nIn the second method, we fill in a template with the four\nevaluation metrics above to ask the VLM to directly score\nfor each building:\nYou are a critic with high aesthetic ability. I\nwill provide you a text instruction and a\ncreated building following this instruction\nin the game \"Minecraft\".\nAccording to four aspects: $(EVALUATION_ASPECT\n1˜4),\nplease evaluate the building with a score (out of\n10) on each aspect respectively, then give a\ntotal score.\nText: $(INSTRUCTION)\nImage of the building: $IMAGE\nTo verify the reliability of VLMs in evaluation, we also ask\nhumans to participate in both two evaluation methods and\ncompare the difference between VLM and human evalua-\ntions.\n5.4. Results and Analysis\nIn the first evaluation method, every two agents are com-\npared by GPT-4V for each test task. With all these com-\nparison results, we model the four agents into a four-player\nzero-sum game and apply the Elo Rating System to com-\npute the ratings, as shown in Figure 3 (left).\nAccording to the second evaluation method, we record\nratings of each single test over the four metrics (correctness,\ncomplexity, quality, and functionality), the sum of which is\nthe overall score. We calculate the standard deviation of the\noverall scores of all test tasks for each agent, which repre-\nsents Robustness, with a larger standard deviation standing\nfor weaker robustness. To align with the other four metrics\nfor better presentation, we properly transform the standard\ndeviation into a value such that a higher value indicates bet-\nter robustness. Note that better robustness does not neces-\nsarily mean better performance since it merely indicates the\nconsistency of the performance on all test tasks. The results\nare plotted as a polar chart shown in Figure 2. Furthermore,\nthe overall score averaged over all test tasks for each agent\nis shown in Figure 3 (right). Figure 4 shows examples of\nthe language description, the generated visual imagination,\nand the created building of each agent.\nAnalyzing the experimental results, we can draw the fol-\nlowing primal conclusions:\n• CoT for textual imagination can effectively enrich the\ndetailed features of the target buildings.\n5\nComparing CoT+GPT-4 with Vanilla GPT-4, the for-\nmer outperforms the latter in terms of all metrics ex-\ncept robustness in the polar chart by a large margin, and\nCoT+GPT-4 obtains a higher score in the Elo Rating re-\nsults than Vanilla GPT-4. We assign this due to the rich\ninformation brought by Chain-of-Thought, which plays a\nrole in self-reflection. Through this process, the LLM gets\na better understanding of the details of the task, including\nbut not limited to the materials used, the size of the build-\ning, the significant features, etc. Within the context of a\nconversation, when GPT-4 generates the code in the sec-\nond round, it can still perceive the extra information from\nthe first round, thus reaching a better representation of the\ntask goal.\n• For the controller, using VLM instead of LLM leads\nto a marginally better performance in most metrics.\nAs shown in Figure 2, Diffusion+GPT-4V weakly sur-\npasses CoT+GPT-4 in correctness, complexity, quality,\nand robustness. However, Diffusion+GPT-4V strikes a tie\nwith CoT+GPT-4 in functionality. In terms of functional-\nity, Diffusion+GPT-4V behaves no better than CoT+GPT-\n4, which can be owing to the weak ability of GPT-4V\nin 3D reconstruction. Empirically, the images passed to\nGPT-4V are usually a complete building generated by the\ndiffusion-based imaginator, without the sectional view to\nshow the internal structures. Therefore, sometimes GPT-\n4V tends to write code that leads to a solid house instead\nof a hollow one. According to the criteria of functionality,\nsolid houses can result in low ratings.\n• Diffsion+GPT-4V has the best performance overall,\nshowing a strong ability of anti-interference and ro-\nbustness.\nIn Figure 3, both the Elo Rating results and the average\nscore show that the three variants proposed in Figure 1\noutperform the baseline to varying degrees, among which\nDiffsion+GPT-4V ranks the first. Combining the previous\ntwo conclusions, Diffusion+GPT-4V has both the advan-\ntage in visual imagination and the strengths from CoT,\nthus having a better performance. Additionally, we are\nsurprised to find that Diffusion+GPT-4V overcomes the\nmisleading information of the diffusion-based imaginator.\nIn about half of the test tasks, the images generated by the\ndiffusion-based imaginator tend to have obvious noises in\nthe background to some extent. However, GPT-4V seems\nto have the ability of anti-interference, thus capturing the\nmajor essential factors of the images. In contrast, Dif-\nfusion+BC may be susceptible to such noises, leading to\nweaker robustness.\n• The human-rating results coincide with the VLM-\nrating results with a minor gap, indicating that evalu-\nating by vision-language models is reliable.\nWe list the average scores by both VLM and human evalu-\nTable 1. Consistency between VLM and human evaluations. Here,\n“agreement” refers to the proportion of cases where the pairwise\ncomparison between four variants of creative agents has the same\nnumerical relationship in both VLM and human evaluations.\nMetric\n1v1\nOverall\nscore\nCorrect.\nscore\nCompl.\nscore\nQual.\nscore\nFunc.\nscore\nAgreement 62.5%\n67%\n68%\n71%\n62%\n52%\nation in Figure 3 (right), from which we know the human-\nrating results are generally in line with the VLM-rating\nresults. In both evaluations, the first two in the ranking\nare the same - Diffusion+GPT-4V and CoT+GPT-4. The\nlast two are in the opposite order but within a small gap.\nOverall, both two evaluations agree that Diffusion+GPT-\n4V has the best performance.\n• The buildings created by agents are relatively simple,\nlimited by the code written by language models and\nthe trained policy of the behavior-cloning controller.\nIn the analysis of the final creations of different agents,\nwe find that the buildings are relatively simple. For those\nvariants with GPT-4(V) controllers, this may be limited\nby the code written by GPT-4(V). Due to limited APIs\nin Mineflayer, GPT-4(V) tends to generate simple code.\nFor instance, GPT-4(V) tends to use for-loops in a piece\nof code that corresponds to a wall in the building, re-\nsulting in the square shape of the building.\nAddition-\nally, Mineflayer uses a rule-based algorithm for path plan-\nning to place blocks in the Minecraft world, and the agent\nwill always destroy some blocks when not able to find\na proper path toward the goal. Therefore, there can be\nmany demolished walls in the final creations.\nOn the\nother hand, the trained policy of the behavior-cloning\ncontroller has several limitations. When reconstructing\nvoxels from the images generated by the diffusion-based\nimaginator, the Pix2Vox approach can only capture the\nRGB color for each voxel and choose the most similar\nblock in Minecraft, which is not very accurate. To make\nthings worse, the plausible structure of a common build-\ning is missed out during the reconstruction, which makes\nthe voxel look like “a mess”. Some blocks are even float-\ning in the voxel, so they cannot be placed correctly in the\nfinal execution. This also provides a reason why Diffu-\nsion+BC ranks the last in the human evaluation results.\n5.5. The VLM Evaluator vs. Human Evaluators\nIn human evaluation, for the first evaluation method (1v1\ncomparison), we use the majority vote among all humans\n(49 human evaluators in total) to represent human prefer-\nence for each pair of buildings. For the second evaluation\nmethod, the scoring data from each human is standardized\nin each of the four evaluation metrics and the overall score.\n6\n“A huge Minecraft volcano built of ice.”\n“A tall Minecraft tower with glass windows, \nmainly built of leaves.”\n“A big Minecraft house built of colorful \nwools.”\n“A slender Minecraft tower with crystal-\nclear windows, predominantly crafted from \nbirch logs.”\n“A yellow concrete Minecraft house with a \nroof and windows.”\n“A sandstone palace in Minecraft with \nintricate details and towering minarets.”\n“A mystical ice castle in Minecraft, sculpted \nfrom packed ice and frosted blocks, adorned \nwith icicle chandeliers and frosty spires.”\n“A medieval-inspired fortress in Minecraft \nbuilt from cobblestone and mossy stone \nbricks, complete with imposing towers and a \ndrawbridge.”\nDiffusion Image\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\nText Description\n“An enchanting Minecraft pagoda adorned \nwith bamboo and built primarily of jungle \nwood planks.”\n“A pyramid in Minecraft built of sandstone.”\nFigure 4. Examples of the language description, the generated visual imagination, and the created building of each variant of creative\nagents. Visual imagination generated by the diffusion model has great diversity, which is an important manifestation of creativity.\nThen we take the average score from all humans as the\nhuman evaluation score for each building created by each\nagent. To measure the consistency of human and VLM eval-\nuators, we use “agreement”, which refers to the proportion\nof cases where the pairwise comparison between four vari-\nants of creative agents has the same numerical relationship\nin both human and VLM evaluations under each evaluation\nmetric.\nThe agreements of human and VLM evaluations are\nlisted in Table 1.\nThe agreement in the overall score is\n67%, greater than two-thirds, indicating that the VLM eval-\nuation is consistent with human evaluation to some extent,\nthough each metric may have a slightly different agreement.\nMoreover, the agreement in 1v1 is 62.5%, relatively lower\nthan that in the overall score, but they are reasonably consis-\ntent. Together with human and VLM evaluations reaching a\nconsensus that Diffusion+GPT-4V has the best performance\nfollowed by CoT+GPT-4, as shown in Figure 3 (right), we\nbelieve the VLM evaluator can be a good choice for creative\ntasks.\n7\n6. Related Work\n6.1. Open-Ended Agents\nIn recent years, task learning in open-ended worlds has at-\ntracted increasing attention, among which Minecraft [25]\nhas become a marvelous test-bed for open-ended learn-\ning. MineRL [18] and MineDojo [13] implemented sim-\nulated environments and organized datasets of a relatively\nlarge scale, and the latter provides tasks for agent train-\ning in the open-ended domain.\nHowever, most previous\nwork [3, 29, 45, 52] mainly focused on unlocking numerous\nskills and tackling long-horizon tasks in Minecraft, which\nhowever are mostly predefined, lacking the open-ended na-\nture, not to mention creativity.\nThe IGLU Competition [27] was a giant leap to solving\ntasks according to instructions in natural language. Skryn-\nnik et al. [41] proposed a pipeline containing a T5-based\nlanguage module, a rule-based transfer module, and the\ndownstream policy module for execution. This agent could\nsolve simple tasks of stacking blocks in Gridworld [53], a\nMinecraft-like open-ended world. However, it depended on\ntoo many step-by-step instructions, thus showing little cre-\nativity. In general, there is a significant gap between previ-\nous work and the true “creative agents”.\n6.2. Generative Models\nIn recent years, many modern generative models have been\nproposed and are used to produce high-quality samples in\nvarious domains [8]. Text-generative models have aroused\nmuch attention due to their wide range of uses. Especially,\nlarge language models (LLMs) are playing more and more\nsignificant roles in decision-making, planning, and reason-\ning.\nAmong LLMs, a representative one is GPT-4 [34],\nwhose emergence has laid a solid foundation for further re-\nsearch. Accompanied by the appearance of LLMs, prompt\nengineering and tuning techniques [5, 48] have been widely\nstudied and applied, including Parameter-Efficient Fine-\nTuning (PEFT) [21] and Chain-of-Thought (CoT) [47]. In\nour work, LLMs with CoT are adopted as textual imagina-\ntors, and we also construct the text-based controller with\nLLM code generation.\nIn the field of computer vision, image-generative mod-\nels are becoming increasingly important.\nProminent ap-\nproaches include variational autoencoders (VAEs) [26],\nGenerative\nAdversarial\nNetworks\n(GANs)\n[15],\nand\nflows [22], demonstrating success in capturing image dis-\ntributions and representations. Recently, diffusion models\n[23] and DALL-E 3 [33] are springing up, accelerating the\nresearch in visual generation. In our work, a finetuned Sta-\nble Diffusion [38] is used for visual imagination, represent-\ning a concrete description of the building-creation task.\nIn the Minecraft world, previous work [2, 17, 40] fo-\ncuses on Procedural Content Generation (PCG). However,\nthey usually generate a pre-defined type of buildings with\na lot of human prior knowledge. In our work, imagination\nfor creative agents is similar to content generation, but our\nimaginator can generate with free-form instructions, in dif-\nferent modalities, and requiring much less human prior.\n6.3. Evaluation for Open-Ended Tasks\nRecent work has gathered many evaluation methods for\nopen-ended tasks. Voyager [44], STEVE-1 [29], and DIP-\nRL [32] use travel distance and collected items as surro-\ngate metrics to evaluate. GROOT [7] and BASALT compe-\ntition [31] use human evaluation, which is relatively labor-\nintensive and may be susceptible to subject preferences. Re-\ncent work [10, 13] proposes to use the CLIP-like model to\ncompute alignment between the behaviors and instructions.\nWe propose a novel evaluation method using VLMs, which\ncan either directly rate the performance in various aspects\nor conduct pairwise comparisons.\nIn terms of evaluation aspects, previous studies have pro-\nposed a variety of metrics. MCU [30] took evaluations from\nthe perspective of planning complexity, time consumption,\nnovelty, and creativity. GDMC Competition [40] required\nhumans as judges, rating the generated contents from adapt-\nability, functionality, evocative narrative, as well as visual\naesthetics. Team et al. [43] evaluated the results in both task\ncoverage and relative performance. Inspired by these stud-\nies, we adopt the evaluation aspects in correctness, com-\nplexity, quality, functionality, and robustness.\n7. Conclusion and Limitations\nIn this paper, we propose creative agents, which is the first\nframework that can handle creative tasks in an open-ended\nworld. Using this framework, we implement various em-\nbodied agents through different combinations of imagina-\ntors and controllers. Additionally, we tap into the potential\nof Vision-Language Models (VLMs), utilizing VLMs for\nevaluation as judges. By comparing the rating results from\nVLM and humans, we illustrate the reliability of VLM eval-\nuation.\nIn the meanwhile, we find a few limitations of these cre-\native agents, to be investigated in further work. First, there\nis much room for improving the BC controller, especially\nfor the performance of Pix2Vox module. Another limitation\nlies in the simplicity of the building created by the agents,\nwhich means the capabilities of these agents are limited.\nHow to enhance the creativity of agents can be a challeng-\ning problem.\nIn the end, we declare that creative agents is an initial\nattempt in this field, aimed at raising the awareness of build-\ning intelligent agents with creativity. We hope this work can\nbe of inspiration for further research.\n8\nReferences\n[1] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage,\nand Anil Anthony Bharath. Deep reinforcement learning: A\nbrief survey. IEEE Signal Processing Magazine, 2017. 1\n[2] Maren Awiszus, Frederik Schubert, and Bodo Rosenhahn.\nWorld-gan: a generative model for minecraft worlds. In 2021\nIEEE Conference on Games (CoG), pages 1–8. IEEE, 2021.\n8\n[3] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampe-\ndro, and Jeff Clune. Video pretraining (vpt): Learning to act\nby watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639–24654, 2022. 8\n[4] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol\nHausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex\nIrpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. In Conference\non Robot Learning (CORL), 2023. 1\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 2020. 1, 3, 8\n[6] Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao\nLiang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. In\nProceedings of the IEEE\/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2023. 1, 3\n[7] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji\nLiu, and Yitao Liang.\nGroot:\nLearning to follow in-\nstructions by watching gameplay videos.\narXiv preprint\narXiv:2310.08235, 2023. 1, 2, 5, 8\n[8] Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu,\nGuangyong Chen, Pheng-Ann Heng, and Stan Z Li.\nA\nsurvey on generative diffusion model.\narXiv preprint\narXiv:2209.02646, 2022. 8\n[9] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie\nZhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang,\nYidong Wang, et al. A survey on evaluation of large language\nmodels. arXiv preprint arXiv:2307.03109, 2023. 3\n[10] Ziluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang,\nand Zongqing Lu. Clip4mc: An rl-friendly vision-language\nmodel for minecraft.\narXiv preprint arXiv:2303.10571,\n2023. 8\n[11] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan\nWahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter\nAbbeel, Joshua B Tenenbaum, et al. Video language plan-\nning. arXiv preprint arXiv:2310.10625, 2023. 1, 3\n[12] Arpad Elo. The rating of chessplayers, past and present. Ishi\nPress, 1986. 5\n[13] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar.\nMinedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems, 35:\n18343–18362, 2022. 2, 4, 8, 11, 12\n[14] Instant Structures Mod (ISM) for Minecraft by Maggi-\nCraft. Instant structures mod. \/urlhttps:\/\/instant-structures-\nmod.com\/. 3\n[15] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks, 2014. 8\n[16] Jonathan Gray, Kavya Srinet, Yacine Jernite, Haonan Yu,\nZhuoyuan Chen, Demi Guo, Siddharth Goyal, C. Lawrence\nZitnick, and Arthur Szlam.\nCraftassist: A framework for\ndialogue-enabled interactive agents, 2019. 3, 12\n[17] Djordje Grbic, Rasmus Berg Palm, Elias Najarro, Claire\nGlanois, and Sebastian Risi. Evocraft: A new challenge for\nopen-endedness. In Applications of Evolutionary Computa-\ntion: 24th International Conference, EvoApplications 2021,\nHeld as Part of EvoStar 2021, Virtual Event, April 7–9, 2021,\nProceedings 24, pages 325–340. Springer, 2021. 8\n[18] William H Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan\nSalakhutdinov. Minerl: A large-scale dataset of minecraft\ndemonstrations. arXiv preprint arXiv:1907.13440, 2019. 8\n[19] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 1\n[20] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can\nspatiotemporal 3d cnns retrace the history of 2d cnns and\nimagenet? In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 6546–\n6555, 2018. 4, 13\n[21] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. Towards a unified view of\nparameter-efficient transfer learning. In International Con-\nference on Learning Representations, 2021. 8\n[22] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and\nPieter Abbeel. Flow++: Improving flow-based generative\nmodels with variational dequantization and architecture de-\nsign.\nIn International Conference on Machine Learning,\n2019. 8\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020. 8\n[24] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang,\nYongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandku-\nmar, Yuke Zhu, and Linxi Fan. Vima: General robot manip-\nulation with multimodal prompts. In NeurIPS 2022 Founda-\ntion Models for Decision Making Workshop, 2022. 1\n[25] Matthew Johnson, Katja Hofmann, Tim Hutton, and David\nBignell. The malmo platform for artificial intelligence ex-\nperimentation. In Ijcai, pages 4246–4247, 2016. 8\n[26] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 8\n[27] Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha\nMohanty, Maartje ter Hoeve, Mikhail Burtsev, Alexey\nSkrynnik, Artem Zholus, Aleksandr Panov, Kavya Srinet,\net al. Interactive grounded language understanding in a col-\nlaborative environment: Iglu 2021. In NeurIPS 2021 Com-\npetitions and Demonstrations Track, pages 146–161. PMLR,\n2022. 8\n9\n[28] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models are\nzero-shot reasoners.\nAdvances in neural information pro-\ncessing systems, 35:22199–22213, 2022. 3\n[29] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and\nSheila McIlraith. Steve-1: A generative model for text-to-\nbehavior in minecraft.\narXiv preprint arXiv:2306.00937,\n2023. 1, 3, 8\n[30] Haowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang.\nMcu: A task-centric framework for open-ended agent evalu-\nation in minecraft. arXiv preprint arXiv:2310.08367, 2023.\n4, 8\n[31] Stephanie Milani, Anssi Kanervisto, Karolis Ramanauskas,\nSander Schulhoff, Brandon Houghton, Sharada Mohanty,\nByron Galbraith, Ke Chen, Yan Song, Tianze Zhou, et al.\nTowards solving fuzzy tasks with human feedback: A ret-\nrospective of the minerl basalt 2022 competition.\narXiv\npreprint arXiv:2303.13512, 2023. 8\n[32] Ellen Novoseller, Vinicius G Goecks, David Watkins, Josh\nMiller, and Nicholas R Waytowich. Dip-rl: Demonstration-\ninferred preference learning in minecraft.\nIn ICML 2023\nWorkshop The Many Facets of Preference-Based Learning,\n2023. 8\n[33] OpenAI. Dall-e 3 system card, 2023. 8\n[34] OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774,\n2023. 2, 3, 8\n[35] PrismarineJS.\nPrismarinejs\/mineflayer: Create minecraft\nbots with powerful, stable, and high level javascript api,\n2013. 4, 12\n[36] Zekun Qi, Muzhou Yu, Runpei Dong, and Kaisheng Ma.\nVPP: Efficient universal 3d generation via voxel-point pro-\ngressive representation.\nIn Thirty-seventh Conference on\nNeural Information Processing Systems, 2023. 3\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision.\nIn International conference on machine learning.\nPMLR, 2021. 2\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684–10695, 2022. 1, 3, 8, 12\n[39] Leonid I Rudin and Stanley Osher. Total variation based im-\nage restoration with free local constraints. In Proceedings\nof 1st international conference on image processing, pages\n31–35. IEEE, 1994. 3\n[40] Christoph Salge, Michael Cerny Green, Rodgrigo Canaan,\nand Julian Togelius. Generative design in minecraft (gdmc)\nsettlement generation competition.\nIn Proceedings of the\n13th International Conference on the Foundations of Digi-\ntal Games, pages 1–10, 2018. 4, 8\n[41] Alexey Skrynnik, Zoya Volovikova, Marc-Alexandre Cˆot´e,\nAnton Voronov, Artem Zholus, Negar Arabzadeh, Shrestha\nMohanty, Milagro Teruel, Ahmed Awadallah, Aleksandr\nPanov, et al. Learning to solve voxel building embodied tasks\nfrom pixels and natural language instructions. arXiv preprint\narXiv:2211.00688, 2022. 8\n[42] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality. arXiv preprint arXiv:2307.05222, 2023. 3, 12\n[43] Open Ended Learning Team, Adam Stooke, Anuj Mahajan,\nCatarina Barros, Charlie Deck, Jakob Bauer, Jakub Syg-\nnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu,\net al. Open-ended learning leads to generally capable agents.\narXiv preprint arXiv:2107.12808, 2021. 8\n[44] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023.\n2, 4, 8, 13\n[45] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao\nLiang. Describe, explain, plan and select: Interactive plan-\nning with large language models enables open-world multi-\ntask agents. arXiv preprint arXiv:2302.01560, 2023. 1, 8\n[46] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret\nZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, et al. Emergent abilities of\nlarge language models.\narXiv preprint arXiv:2206.07682,\n2022. 3\n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824–24837, 2022. 3, 8, 12\n[48] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Car-\nlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-\nSmith, and Douglas C Schmidt. A prompt pattern catalog\nto enhance prompt engineering with chatgpt. arXiv preprint\narXiv:2302.11382, 2023. 3, 8\n[49] Tong Wu, Jiaqi Wang, Xingang Pan, Xudong Xu, Christian\nTheobalt, Ziwei Liu, and Dahua Lin. Voxurf: Voxel-based\nefficient and accurate neural surface reconstruction. In In-\nternational Conference on Learning Representations (ICLR),\n2023. 3\n[50] Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen\nZhou, and Shengping Zhang. Pix2vox: Context-aware 3d\nreconstruction from single and multi-view images. In ICCV,\n2019. 3, 12\n[51] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn\nof lmms: Preliminary explorations with gpt-4v (ision). arXiv\npreprint arXiv:2309.17421, 2023. 2\n[52] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie,\nPenglin Cai, Hao Dong, and Zongqing Lu.\nPlan4mc:\nSkill reinforcement learning and planning for open-world\nminecraft tasks. arXiv preprint arXiv:2303.16563, 2023. 1,\n8\n[53] Artem Zholus, Alexey Skrynnik, Shrestha Mohanty, Zoya\nVolovikova, Julia Kiseleva, Artur Szlam, Marc-Alexandre\nCot´e, and Aleksandr I Panov. Iglu gridworld: Simple and\nfast environment for embodied dialog agents. arXiv preprint\narXiv:2206.00142, 2022. 8\n10\nAppendices\nA. Benchmark Details\nIn this section, we present our Minecraft Building Creation benchmark for creative agents, including tasks, environment\nsimulators, and datasets. We open source all the content in https:\/\/github.com\/PKU-RL\/Creative-Agents.\nWe introduce a set of language instructions for test tasks. Each instruction is constructed by randomly picking a building\nname and several building features and combining them into a natural language description. We list the 20 instructions here:\n1. An artsy building in Minecraft built of blue_glass, red_glass and yellow_glass.\n2. A pyramid in Minecraft built of sandstone.\n3. A modern-style building in Minecraft built of quartz_block, glass, and lantern.\n4. A resplendent and magnificent building in Minecraft, which is built of gold_block, glass_block\nand iron_block.\n5. A majestic silver Egyptian pyramid in Minecraft constructed from iron block.\n6. A screenshot of a white pyrmaid-like house in Minecraft with windows, which is built of snow.\n7. A tall Minecraft tower with glass windows, mainly built of leaves.\n8. A cute Minecraft mansion featuring walls constructed from red blocks and adorned with large\nglass windows.\n9. A big Minecraft house built of colorful wools.\n10. A huge Minecraft volcano built of ice.\n11. A slender Minecraft tower with crystal-clear windows, predominantly crafted from birch logs.\n12. An imposing fortress in Minecraft made of obsidian blocks, with towering turrets and ominous\nspikes.\n13. A futuristic Minecraft space station featuring a sleek design using iron blocks and a network\nof glass domes for observation.\n14. An underwater observatory in Minecraft, constructed with prismarine bricks and dark_prismarine,\nshowcasing large windows made of sea lanterns and light_blue_glass.\n15. An enchanting Minecraft pagoda adorned with bamboo and built primarily of jungle wood planks.\n16. A sandstone palace in Minecraft with intricate details and towering minarets.\n17. A mystical ice castle in Minecraft, sculpted from packed ice and frosted blocks, adorned with\nicicle chandeliers and frosty spires.\n18. An enchanting forest cottage in Minecraft crafted from dark oak logs.\n19. A yellow concrete Minecraft house with a roof and windows.\n20. A medieval-inspired fortress in Minecraft built from cobblestone and mossy stone bricks,\ncomplete with imposing towers and a drawbridge.\nWe adopt two environments for different creative agents, each providing a Minecraft simulator with action primitives for\nthe agent to place a block.\nThe first environment uses the MineDojo simulator [13], which provides Gym wrappers for the Minecraft game along with\nadditional dictionary observations. We implement two primitive actions in this simulator: a path-search action and a placing\naction. The path-search action, implemented with the A* algorithm, can use voxel information to determine whether the\nagent can reach a given coordinate. The placing action can drive the agent towards a target coordinate given a valid path and\nplace a block on it. A creative agent can recursively output the plan for the next block to place and call the action primitives\nto accomplish it.\n11\nIn the second environment, we use the original Minecraft game (Java Edition 1.19).\nWe use Mineflayer [35] with\nJavaScript APIs to provide action primitives for building creation. The core function we use is placeItem(bot, block name,\nposition), which also performs path-search and placing to try to place a block at a given position.\nThe first environment with MineDojo simulator has the advantage of providing richer observation information, which is\nbeneficial for data collection and programmatic evaluation in future research. The second environment with Mineflayer APIs\nprovides more diverse action primitives, which may be helpful for future research on other creative tasks. In our study, we\ntest the method with BC controller in the first environment and test all other methods in the second environment.\nFor datasets, we release a text-image dataset for training diffusion imaginators, an image-voxel dataset, and a gameplay\ndataset for training the BC controller. The text-image dataset consists of 14,180 paired language instructions and images.\nEach RGB image has a resolution of 512 × 512. The image-voxel dataset comprises 1,009,044 paired images and 3D voxels.\nEach 512×512 image shows a Minecraft building at a view angle. Each 32×32×32 voxel is the corresponding ground-truth\nvoxel of the building, labeled with voxel occupancy and block information. The gameplay dataset consists of 6M action-\nlabeled samples. Each trajectory (V, {at}T\nt=0) is an expert gameplay to construct a building, where V denotes the voxel of\nthe target building and {a} denotes the sequence of action primitives to complete it.\nB. Implementation Details\nIn this section, we present details of data collection and implementing imaginator and controller variants.\nB.1. LLM Prompts for Textual Imagination\nTo utilize GPT-4 as a textual imaginator and make it have a better understanding of the building task, we add Chain-of-\nThought (CoT) prompts in addition to the prompts for Vanilla GPT-4, following Wei et al. [47]. Specifically, we ask GPT-4\nfive questions in the first round within a conversation, using the following template:\nYou are an architect designing houses and buildings.\nHere is a building you should design: _______________________________.\nFirst, you should answer these questions below based on your design and imagination:\n1. Please give a detailed description of the building.\n2. Which kinds of blocks are used in the building? There might be several kinds of materials, and\nyou should report them all.\n3. What is the probable size of the building? You should estimate the length, width and height.\nNote that your inventory contains only 2304 blocks, so length, width and height no more than 12\nwould be better.\n4. Please list some components of the building, including but not limited to doors, walls, windows\nand floors.\n5. Are there any salient features of the building, e.g. gardens, swimming pools and towers?\nAfter GPT-4 answers these questions as imagination, in the second round of the conversation, we ask it to generate code\nto construct the building. The prompts for the second round are presented in Appendix B.4.\nB.2. Finetuning the Diffusion Model for Visual Imagination\nTo finetune the diffusion model for generating visual imaginations, we construct the text-image dataset by automatically\nannotating the Minecraft buildings in CraftAssist [16] using the multimodal Emu model [42]. The labeled dataset consists of\n14K text-image pairs.\nIn order to obtain stable and high-quality image generation with such a small dataset, we choose to finetune the pre-trained\nStableDiffusion V1.4 model [38]. Table 2 shows the hyperparameters used for finetuning.\nB.3. Behavior-Cloning Controller\nFor the behavior-cloning (BC) Controller, we utilize the voxel representation as a blueprint. We transform images into voxels,\nand subsequently convert the voxel blueprint into a sequence of actions.\nFor data collection, we perform data augmentation on the original voxel data obtained from CraftAssist [16] and use\nMineDojo [13] to construct the buildings and render the images. We collect 1M paired images and voxels to construct the\ndataset and split a validation set with 3K samples. We employ the Pix2Vox++ architecture [50], with an encoder of 30M\nparameters and a decoder of 70M parameters. The model hyperparameters are shown in Table 3. It resizes the input image\ninto 224×224 pixels and generates a 32×32×32 voxel output. Each voxel has 4 dimensions, consisting of the probability of\noccupancy and RGB colors for the block.\n12\nTable 2. Hyperparameters for finetuning Stable Diffusion.\nHyperparameter\nValue\nTraining epoch\n100\nImage resolution\n512 × 512\nAdam β\n(0.9, 0.999)\nAdam weight decay\n0.01\nLearning rate\n1 × 10−4\nMax grad norm\n1.0\nTable 3. Hyperparameters for Pix2Vox++.\nHyperparameter\nValue\nEncoder layers\n[3, 5, 5, 3]\nEncoder block inplanes\n[64, 128, 256, 512]\nDecoder layers\n[1, 1, 1, 1, 1]\nDecoder block inplanes [2048, 512, 128, 32, 8]\nImage resolution\n224 × 224\nOutput shape\n32 × 32 × 32 × 4\nLearning rate\n1 × 10−3\nWeight decay\n1 × 10−4\nFor generating action plans given voxels, we collect a gameplay dataset for building creation and training a behavior-\ncloning policy. We use voxels in the image-voxel dataset to construct diverse goals for building and collect a dataset of\n6M steps. We preprocess the dataset to provide rich observations for the BC policy. We construct the observation at each\ntimestep with 32 × 32 × 32 × 3 voxels. Each channel represents the target voxel, the built voxel at the current step, and the\nlast block, respectively. The output shape is 32769 = 32 × 32 × 32 + 1, representing the action primitive for the position of\nthe next block to place and the termination probability. We adopt the ResNet3D [20] architecture with 50M parameters. The\nhyperparameters are shown in Table 4.\nTable 4. Hyperparameters for the BC policy.\nHyperparameter\nValue\nResBlock Type\nBasicBlock\nLayers\n[2, 2, 2, 2]\nBlock inplanes\n[64, 128, 256, 512]\nInput shape\n32 × 32 × 32 × 3\nOutput shape\n32769\nLearning rate\n1 × 10−3\nWeight decay\n1 × 10−4\nB.4. VLM Controller\nVoyager [44] uses Mineflayer APIs as action primitives to solve tasks in the Minecraft world. For building creation tasks,\nwe mainly utilize the function position.offset(x, y, z) to convert planned coordinates into positions and use\nplaceItem(bot, blockName, targetPosition) to place a block at the target position.\nAfter textual imagination or visual imagination in the first round, we ask GPT-4(V) to generate executable code in the\nenvironment according to the imagination and context. The prompts mainly consist of three parts: (1) Mineflayer APIs and\ntheir usage. (2) Explanations on the coding format, the task, etc. (3) An example of the correct code for in-context learning.\nWe present the prompts used in the second round of the conversation here.\nBased on (the image and) your answers to the questions above, please design a method to build a\nhouse like that.\nNow you are a helpful assistant that writes Mineflayer javascript code to complete any Minecraft\ntask specified by me.\n13\nHere are some useful programs written with Mineflayer APIs:\nawait bot.pathfinder.goto(goal); \/\/ A very useful function. This function may change your main-hand\nequipment.\n\/\/ Following are some Goals you can use:\nnew GoalNear(x, y, z, range); \/\/ Move the bot to a block within the specified range of the\nspecified block. x, y, z, and range are number\nnew GoalXZ(x, z); \/\/ Useful for long-range goals that don’t have a specific Y level. x and z are\nnumber\nnew GoalGetToBlock(x, y, z); \/\/ Not get into the block, but get directly adjacent to it. Useful for\nfishing, farming, filling bucket, and beds. x, y, and z are number\nnew GoalFollow(entity, range); \/\/ Follow the specified entity within the specified range. entity is\nEntity, range is number\nnew GoalPlaceBlock(position, bot.world, {}); \/\/ Position the bot in order to place a block.\nposition is Vec3\nnew GoalLookAtBlock(position, bot.world, {}); \/\/ Path into a position where a blockface of the\nblock at position is visible. position is Vec3\n\/\/ These are other Mineflayer functions you can use:\nbot.isABed(bedBlock); \/\/ Return true if bedBlock is a bed\nbot.blockAt(position); \/\/ Return the block at position. position is Vec3\n\/\/ These are other Mineflayer async functions you can use:\nawait bot.equip(item, destination); \/\/ Equip the item in the specified destination. item is Item,\ndestination can only be \"hand\", \"head\", \"torso\", \"legs\", \"feet\", \"off-hand\"\nawait bot.consume(); \/\/ Consume the item in the bot’s hand. You must equip the item to consume\nfirst. Useful for eating food, drinking potions, etc.\nawait bot.fish(); \/\/ Let bot fish. Before calling this function, you must first get to a water\nblock and then equip a fishing rod. The bot will automatically stop fishing when it catches a\nfish\nawait bot.sleep(bedBlock); \/\/ Sleep until sunrise. You must get to a bed block first\nawait bot.activateBlock(block); \/\/ This is the same as right-clicking a block in the game. Useful\nfor buttons, doors, etc. You must get to the block first\nawait bot.lookAt(position); \/\/ Look at the specified position. You must go near the position before\nyou look at it. To fill bucket with water, you must lookAt first. position is Vec3\nawait bot.activateItem(); \/\/ This is the same as right-clicking to use the item in the bot’s hand.\nUseful for using buckets, etc. You must equip the item to activate first\nawait bot.useOn(entity); \/\/ This is the same as right-clicking an entity in the game. Useful for\nshearing sheep, equipping harnesses, etc. You must get to the entity first\nAt each round of conversation, I will give you\nNearby blocks: ...\nPosition: ...\nTask: ...\nContext: ...\nYou should then respond to me with\nExplain (if applicable): Are there any steps missing in your plan? Why does the code not complete\nthe task? What does the chat log and execution error imply?\nPlan: How to complete the task step by step. You should pay attention to Inventory since it tells\nwhat you have. The task completeness check is also based on your final inventory.\nCode:\n1) Write an async function taking the bot as the only argument.\n2) Reuse the above useful programs as much as possible.\n- Use mineBlock(bot, name, count) to collect blocks. Do not use bot.dig directly.\n- Use craftItem(bot, name, count) to craft items. Do not use bot.craft or bot.recipesFor\ndirectly.\n- Use smeltItem(bot, name count) to smelt items. Do not use bot.openFurnace directly.\n- Use placeItem(bot, name, position) to place blocks. Do not use bot.placeBlock directly.\n- Use killMob(bot, name, timeout) to kill mobs. Do not use bot.attack directly.\n3) Your function will be reused for building more complex functions. Therefore, you should make\nit generic and reusable. You should not make strong assumption about the inventory (as it\nmay be changed at a later time), and therefore you should always check whether you have the\n14\nrequired items before using them. If not, you should first collect the required items and\nreuse the above useful programs.\n4) Anything defined outside a function will be ignored, define all your variables inside your\nfunctions.\n5) Call bot.chat to show the intermediate progress.\n6) Do not write infinite loops or recursive functions.\n7) Do not use bot.on or bot.once to register event listeners. You definitely do not need them.\n8) Name your function in a meaningful way (can infer the task from the name).\nYou should only respond in the format as described below:\nRESPONSE FORMAT:\nExplain: ...\nPlan:\n1) ...\n2) ...\n3) ...\n...\nCode:\njavascript\n\/\/ helper functions (only if needed, try to avoid them)\n...\n\/\/ main function after the helper functions\nasync function yourMainFunctionName(bot) {\n\/\/ ...\n}\nNow I will give you information:\nNearby blocks: dirt, grass_block\nPosition: x=16.5, y=-60.0, z=-127.5\nTask: build a house\nContext: Build a house according to the figure. Your building should be similar to the one in the\nimage.\nHere is an example of java script code:\nCode Example:\njavascript\n\/\/ helper function to build a house\nasync function buildHouse(bot, position, size, blockName) {\nfor (let y = 0; y < size; y++) {\nfor (let x = 0; x < size; x++) {\nfor (let z = 0; z < size; z++) {\nconst targetPosition = position.offset(x, y, z);\nawait placeItem(bot, blockName, targetPosition);\n}\n}\n}\nbot.chat(\"House built.\");\n}\n\/\/ main function\nasync function buildWoodenHouse(bot) {\nconst position = bot.entity.position.offset(1, 0, 1); \/\/ offset to avoid building at the bot’s\nposition\nconst size = 5; \/\/ size of the house\nconst blockName = ’oak_planks’; \/\/ material to build the house\nawait buildHouse(bot, position, size, blockName);\n}\nPlease note that:\n1) You should not use only one for-loop. Different walls should be built by different for-loops.\n15\n2) Never check whether you have enough blocks in inventory. I will garantee that you will be given\nenough blocks.\n3) Always use const position = bot.entity.position.offset(1, 0, 1); \/\/ offset to avoid building at\nthe bot’s position.\n4) Never define placeItem(bot, blockName, targetPosition) by yourself. We already provide a defined\nfunction.\n5) Always use const targetPosition = position.offset(...) before placeItem(bot, blockName,\ntargetPosition).\n4) Additionally, y axis always start from 0 rather than 1 in a for-loop.\n5) In terms of the size of the house, the kind of blocks of your selection and other details,\nplease refer to the image and your answers to those questions above.\nHere are the names of the commonly used blocks that you can choose from but not limited to:\n[\"ice\", \"packed_ice\", \"blue_ice\", \"beacon\", \"white_concrete\", \"quartz_block\", \"smooth_sandstone\", \"\nsandstone\", \"sandstone_slab\", \"sandstone_stairs\", \"oak_door\", \"polished_andesite\", \"glass\", \"\nglass_pane\", \"lantern\", \"sea_lantern\", \"glowstone\", \"blue_glazed_terracotta\", \"\nwhite_glazed_terracotta\", \"green_glazed_terracotta\", \"yellow_glazed_terracotta\", \"\nred_glazed_terracotta\", \"lime_glazed_terracotta\", \"cyan_glazed_terracotta\"]\nYou should not misspell them in your code.\nOne last important thing: you should write your code within maximum length of tokens.\nThen, the generated code is passed to Mineflayer for execution in the game. We recursively restart the conversation until\nthe generated code does not throw an exception.\nC. Evaluation Metric\nIn this section, we present detailed prompts for all the evaluation metrics and the protocol for human evaluation.\nC.1. VLM Evaluation\nWe use GPT-4V for the two evaluation methods: 1v1 comparison and scoring.\nFor 1v1 comparison, the prompts are as follows:\nYou are a critic with high aesthetic abilities.\nI will provide you a text instruction and two\nbuildings created by different agents following this instruction in the game \"Minecraft\".\nPlease evaluate their overall performance according to four aspects:\n1. Correctness. Are the creations consistent with the language instruction?\n2. Complexity. Can the agent create large and complex buildings?\n3. Quality. Do the creations have good visual appearance?\n4. Functionality. Do the created buildings have necessary structures for houses (rooms and\nentrances)?\nTell me which building in the image is better (left or right).\nInstrution: $(INSTRUCTION)\nImage of buildings: $(IMAGE1, IMAGE2)\nFor scoring, the prompts are as follows.\nYou are a critic with high aesthetic abilities.\nI will provide you a text instruction and a\nbuilding created by an agent following this instruction in the game \"Minecraft\"\nPlease evaluate their overall performance according to four aspects:\n1. Correctness. Are the creations consistent with the language instruction?\n2. Complexity. Can the agent create large and complex buildings?\n3. Quality. Do the creations have good visual appearance?\n4. Functionality. Do the created buildings have necessary structures for houses (rooms and\nentrances)?\nplease evaluate the building with a score (out of 10) on each aspect respectively, then give a\ntotal score.\ninstruction: $(INSTRUCTION)\nImage of building: $(IMAGE)\n16\nC.2. Human Evaluation\nWe conducted the human evaluation in a similar way as the VLM evaluation, while we converted prompts into questionnaires.\nIn total, we received 49 valid questionnaires evaluating the results of the four methods for all the test tasks. Figure 5\ndemonstrates the questionnaires for both evaluation methods.\nEach question has an instruction and a picture. The instruction \ndescribes the shape, style, material and other characteristics of a \nbuilding, and the corresponding pictures are two buildings built \naccording to the instruction, which are divided into left and right parts. \nNow, please judge from the following four aspects through your own \njudgment:\n1. Correctness: Does the building match the text?\n2. Complexity: Whether the building is overall simple or complex.\n3. Aesthetics: Whether the appearance of the building is aesthetically \npleasing.\n4. Functionality: Whether the building meets its intended function, \nsuch as the necessary structure of the house (rooms and entrances).\nPlease evaluate the left and right buildings and choose the one you \nthink is better.\n* 1. A sandstone palace in Minecraft with intricate details and \ntowering minarets.\n1. left\n2. right\nEach question has an instruction and a picture. The instruction describes the shape, \nstyle, material and other characteristics of a building, and the corresponding \npictures are two buildings built according to the instruction. Now, please judge \nfrom the following four aspects through your own judgment:\n1. Correctness: Does the building match the text?\n2. Complexity: Whether the building is overall simple or complex.\n3. Aesthetics: Whether the appearance of the building is aesthetically pleasing.\n4. Functionality: Whether the building meets its intended function, such as the \nnecessary structure of the house (rooms and entrances).\nFor each aspect, give a score between 0~10, the higher the better.\n* 1. A slender Minecraft tower with crystal-clear windows, predominantly crafted \nfrom birch logs.\nFigure 5. An example of the questionnaires for human evaluation. Left: 1v1 comparison between different methods; Right: directly score\nthe test sample.\n17\nD. Additional Results\nIn addition to the results presented in the main text, we provide supplementary results of building creation in Figure 6.\n“An artsy building in Minecraft built of \nblue glass, red glass and yellow glass.”\n“A modern-style building in Minecraft built \nof quartz block, glass, and lantern.”\n“A resplendent and magnificent building in \nMinecraft, which is built of gold block, \nglass block and iron block.”\n“A majestic silver Egyptian pyramid in \nMinecraft constructed from iron block.”\n“A screenshot of a white pyramid-like house \nin Minecraft with windows, which is built of \nsnow.”\n“An imposing fortress in Minecraft made of \nobsidian blocks, with towering turrets and \nominous spikes.”\n“A futuristic Minecraft space station featuring \na sleek design using iron blocks and a \nnetwork of glass domes for observation.”\n“A cute Minecraft mansion featuring walls \nconstructed from red blocks and adorned with \nlarge glass windows.”\nDiffusion Image\nVanilla GPT-4\nCoT+GPT-4\nDiffusion+GPT-4V\nDiffusion+BC\nText Description\n“An underwater observatory in Minecraft, \nconstructed with prismarine bricks and dark \nprismarine, showcasing large windows made \nof sea lanterns and light blue glass.”\n“An enchanting forest cottage in Minecraft \ncrafted from dark oak logs.”\nFigure 6. Supplementary results of building creation, including the language instruction, the generated visual imagination, and the created\nbuilding of each variant of creative agents.\nTo study the difference between textual imagination and visual imagination, here we provide an example of the interme-\ndiate output after the first round from CoT+GPT-4 and Diffusion+GPT-4V. The language instruction of this test task is “A\nmodern-style building in Minecraft built of quartz block, glass, and lantern.”\nThe textual imagination from CoT+GPT-4 is:\nDesigning a modern-style building in Minecraft with a limited inventory of blocks is an interesting\nchallenge. Here’s a detailed description based on your specifications:\n18\nDescription of the Building:\n-\nStyle: The building is designed in a contemporary, minimalist style, emphasizing clean lines\nand an open layout.\n-\nExterior: The facade is predominantly made of quartz blocks, creating a sleek, white appearance\n. Large glass panels are used to bring in natural light and offer views of the surrounding\nlandscape.\n-\nInterior: The interior features an open-plan design, with minimal partitions to create a sense\nof space and fluidity. Lanterns are strategically placed for ambient lighting.\nMaterials Used:\n1.\nQuartz Block: Primarily used for the exterior walls and some interior elements.\n2.\nGlass: Used for windows and in some areas for floor-to-ceiling glass walls.\n3.\nLanterns: Used for lighting both indoors and outdoors.\nProbable Size of the Building:\n-\nLength: Approximately 10 blocks\n-\nWidth: Approximately 10 blocks\n-\nHeight: Approximately 10 blocks\nThis size ensures that the total number of blocks used remains within the limit of 2304, accounting\nfor the use of different materials.\nComponents of the Building:\n-\nDoors: Modern, minimalist doors made primarily of glass with quartz frames.\n-\nWalls: Mainly quartz blocks, with large sections of glass walls.\n-\nWindows: Large, glass panels, some of which extend from floor to ceiling.\n-\nFloors: Quartz blocks with some areas featuring glass flooring for a contemporary feel.\n-\nRoof: A flat roof made of quartz blocks, incorporating a few glass elements for natural\nlighting.\nSalient Features:\n-\nGarden Area: A small zen-style garden at the rear, visible through the glass walls, adding a\ntouch of greenery.\n-\nTerrace: A rooftop terrace accessible via a staircase, providing a panoramic view of the\nsurroundings.\n-\nSwimming Pool: A small, elegant infinity pool on the rooftop, lined with quartz blocks.\n-\nInterior Courtyard: A small, open-air courtyard in the center of the building, bringing in\nnatural light and air.\n-\nLighting: Lanterns are used both inside and outside to create a warm, welcoming ambiance during\nthe night.\nThis building, with its modern design and thoughtful use of materials, would stand out in a\nMinecraft world, providing both functionality and aesthetic appeal.\nFigure 7. The diffusion visual imagination of the task “A modern-style building in Minecraft built of quartz block, glass, and lantern.”\nThe diffusion visual imagination of this task is shown in Figure 7. Given this imagination, the intermediate reasoning\nresults of Diffusion+GPT-4V are:\n19\nThe building in the image appears to be a modern Minecraft structure with the following\ncharacteristics:\n1.\nDetailed Description: The building has a distinctive step pyramid or ziggurat shape with a\ntiered structure, each level smaller than the one below it, creating a terraced effect. The\ndesign is minimalist and geometric, featuring clean lines and a symmetrical layout. It sits on\nwhat seems to be a grassy terrain with a clear sky above, suggesting an open and possibly\npeaceful setting.\n2.\nBlocks Used:\n-\nQuartz Blocks: These are the predominant material and give the building its white and clean\nlook. They are used for the main structure.\n-\nGlass Blocks\/Panes: These are used for windows and possibly for some decorative elements,\nallowing for natural light to enter the building and providing a transparent look.\n-\nLeaves: There seems to be foliage incorporated into the building, likely for decorative\npurposes, suggesting the use of leaf blocks.\n-\nLanterns: If there are sources of light visible, they might be lantern blocks, used to\nilluminate the structure.\n3.\nProbable Size:\n-\nLength and Width: It’s difficult to give exact measurements since the image perspective doesn’t\nshow the full base. However, if we assume each block is 1 meter, as is standard in Minecraft,\nwe can estimate based on the visible tiers. If each tier is approximately 5 blocks less in\nlength and width than the one below, and there are 4 visible tiers, the base could be around 20\nblocks wide and long.\n-\nHeight: The building appears to have at least 4 levels. If each level is around 3 blocks in\nheight, the total height could be around 12 blocks.\n4.\nComponents:\n-\nDoors: There is no visible door from this angle, but there could be an entrance on a side not\nshown.\n-\nWalls: The walls are primarily made of quartz blocks, with glass blocks likely used for windows\n.\n-\nWindows: Large, square windows can be seen, made out of glass blocks or panes.\n-\nFloors: Each level acts as a floor for the structure, likely made of quartz blocks as well.\n5.\nSalient Features:\nNo gardens, swimming pools, or towers are visible in this image. The most salient feature is the\nbuilding’s model architectural style with its stepped, pyramid-like structure. There could be\nadditional features on the other sides of the building not visible in this image.\nWithout seeing all sides of the building, this description is based on the visible elements in the\nprovided image.\nBy comparing the two imaginations, we have the following observations:\n• In terms of the blocks and materials used to build houses, CoT+GPT-4 usually captures block types from the raw input\n(text description of the building task), while Diffusion+GPT-4V tends to identify blocks and materials based on the images\ngenerated by the diffusion model. This also means that the error from the Diffusion-based imaginator will be inherited and\namplified, sometimes resulting in GPT-4V’s wrong recognition.\n• In terms of conventional structures and components of the building, CoT+GPT-4 can always design such structures includ-\ning doors, windows, and roofs. However, Diffusion+GPT-4V answers truthfully based on the diffusion images, making a\ndifference from CoT+GPT-4. For instance, if there are only windows appearing in the image, GPT-4V will not respond\nwith structures like doors.\n• When it comes to other salient features, CoT+GPT-4 can give an imagination with descriptions of gardens, towers,\nand swimming pools, though these are never implemented in the code generated in the second round.\nIn contrast,\nDiffusion+GPT-4V denies the existence of such features, since it answers truthfully based on the diffusion images.\n20\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Creative Agents: Empowering Agents with Imagination for Creative Tasks.pdf"}
{"title":"Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft","authors":"Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai","summary":"Many reinforcement learning environments (e.g., Minecraft) provide only\nsparse rewards that indicate task completion or failure with binary values. The\nchallenge in exploration efficiency in such environments makes it difficult for\nreinforcement-learning-based agents to learn complex tasks. To address this,\nthis paper introduces an advanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically design dense reward\nfunctions, thereby enhancing the learning efficiency. Auto MC-Reward consists\nof three important components: Reward Designer, Reward Critic, and Trajectory\nAnalyzer. Given the environment information and task descriptions, the Reward\nDesigner first design the reward function by coding an executable Python\nfunction with predefined observation inputs. Then, our Reward Critic will be\nresponsible for verifying the code, checking whether the code is\nself-consistent and free of syntax and semantic errors. Further, the Trajectory\nAnalyzer summarizes possible failure causes and provides refinement suggestions\naccording to collected trajectories. In the next round, Reward Designer will\nfurther refine and iterate the dense reward function based on feedback.\nExperiments demonstrate a significant improvement in the success rate and\nlearning efficiency of our agents in complex tasks in Minecraft, such as\nobtaining diamond with the efficient ability to avoid lava, and efficiently\nexplore trees and animals that are sparse in the plains biome.","url":"http:\/\/arxiv.org\/abs\/2312.09238v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2312.09238v2","published":1702580292000,"comment":"Accepted by CVPR2024","pdf_text":"Auto MC-Reward: Automated Dense Reward Design with\nLarge Language Models for Minecraft\nHao Li1,2∗, Xue Yang2∗, Zhaokai Wang2,3∗, Xizhou Zhu4,5,\nJie Zhou4, Yu Qiao2, Xiaogang Wang1,5, Hongsheng Li1, Lewei Lu5, Jifeng Dai4,2B\n1CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong\n2OpenGVLab, Shanghai AI Laboratory\n3Shanghai Jiao Tong University\n4Tsinghua University\n5SenseTime Research\nhttps:\/\/yangxue0827.github.io\/auto_mc-reward.html\nFigure 1. Overview of our Auto MC-Reward. Auto MC-Reward consists of three key LLM-based components: Reward Designer, Reward\nCritic, and Trajectory Analyzer. A suitable dense reward function is iterated through the continuous interaction between the agent and the\nenvironment for reinforcement learning training of specific tasks, so that the model can better complete the task. An example of exploring\ndiamond ore is shown in the figure: i) Trajectory Analyzer finds that the agent dies from lava in the failed trajectory, and then gives\nsuggestion for punishment when encountering lava; ii) Reward Designer adopts the suggestion and updates the reward function; iii) The\nrevised reward function passes the review of Reward Critic, and finally the agent avoids the lava by turning left.\nAbstract\nMany\nreinforcement\nlearning\nenvironments\n(e.g.,\nMinecraft) provide only sparse rewards that indicate task\ncompletion or failure with binary values. The challenge\nin exploration efficiency in such environments makes it\ndifficult for reinforcement-learning-based agents to learn\ncomplex tasks. To address this, this paper introduces an\nadvanced learning system, named Auto MC-Reward, that\nleverages Large Language Models (LLMs) to automatically\ndesign dense reward functions, thereby enhancing the\nlearning efficiency. Auto MC-Reward consists of three im-\nportant components: Reward Designer, Reward Critic, and\nTrajectory Analyzer.\nGiven the environment information\n∗Equal contribution. This work was completed by Hao Li and Zhaokai\nWang during their internship at Shanghai Artificial Intelligence Labora-\ntory. BCorresponding author: Jifeng Dai <daijifeng@tsinghua.edu.cn>.\nand task descriptions, the Reward Designer first design the\nreward function by coding an executable Python function\nwith predefined observation inputs.\nThen, our Reward\nCritic will be responsible for verifying the code, checking\nwhether the code is self-consistent and free of syntax\nand semantic errors.\nFurther, the Trajectory Analyzer\nsummarizes possible failure causes and provides refinement\nsuggestions according to collected trajectories. In the next\nround, Reward Designer will further refine and iterate the\ndense reward function based on feedback.\nExperiments\ndemonstrate a significant improvement in the success rate\nand learning efficiency of our agents in complex tasks in\nMinecraft, such as obtaining diamond with the efficient\nability to avoid lava, and efficiently explore trees and\nanimals that are sparse in the plains biome.\narXiv:2312.09238v2  [cs.AI]  30 Mar 2024\n1. Introduction\nMinecraft, as the world’s best-selling game, offers a range\nof tasks from exploration, survival to creating. It has be-\ncome an important environment for researching efficient\nReinforcement Learning (RL) [16, 22]. In particular, its\nextreme sparsity of rewards and huge complexity of the de-\ncision space pose significant challenges for RL. Currently,\nthe most effective learning strategy involves pre-training\nthrough behavior cloning [3], using learned behavioral pri-\nors to narrow the decision space. Nevertheless, it still re-\nquires billions of environmental interactions for effective\nlearning due to the sparse reward nature of Minecraft.\nOn the other hand, previous researchers have proposed a\nvariety of dense reward signals to enable efficient learning\nfor specific sparse reward tasks [2, 27, 28, 35, 37]. However,\ntheir applicability on the complex and long-horizon tasks\nin Minecraft remains an open question. To deeply reveal\nthe challenges in Minecraft, we examine on several repre-\nsentative challenging tasks, e.g. exploring underground for\ndiamonds. We find that even after behavior cloning, most\nof these methods still fail to make significant progress on\nthese tasks, further highlighting the difficulty of Minecraft\nand the limitations of existing dense reward methods.\nIt is noteworthy that for human players, Minecraft is a\nrelatively simple casual game [12]. The advantage of hu-\nman lies in their ability to summarize based on practice. For\nexample, an accidental burning death from lava can teach\nhuman to avoid getting too close to it. Such summaries,\nbased on life experience and practice, are key to human in-\ntelligence [40, 43]. Most existing RL methods overlook this\nability. On the other hand, Large Language Models (LLMs)\nhave recently demonstrated human-like common sense and\nreasoning capabilities [18, 36, 52]. We find that leverag-\ning LLMs can help RL agents simulate the practice sum-\nmarization abilities of human. Based on the historical ac-\ntion trajectories and success-failure signals of the agents,\nLLMs can automatically design and refine corresponding\nauxiliary rewards, effectively overcoming the sparse reward\nchallenge in Minecraft.\nAccording to above analysis, we propose an automated\nmethod named Auto MC-Reward, to design and improve\nauxiliary reward functions according to task descriptions\nand historical action trajectories. This method utilizes the\ntask understanding and experience summarization abilities\nof LLMs, providing detailed and immediate rewards for\nlearning guidance. Specifically We first use LLMs to de-\nsign task-related dense reward functions based on basic de-\nscriptions of the environment and tasks, named as Reward\nDesigner. These reward functions are used to train agents\nafter self-verification, i.e. Reward Critic. To address poten-\ntial biases or oversights in LLM’s understanding, we also\npropose a LLM-based Trajectory Analyzer to analyze and\nsummarize collected trajectories from the trained agent, and\nhelp Reward Designer to improve the reward functions.\nWe verify the effectiveness of Auto MC-Reward on a se-\nries of representative benchmarks, including horizontal ex-\nploration for diamonds underground and approaching trees\nand animals in the plains biome. Experiments show that\nAuto MC-Reward achieves significantly better results on\nthese tasks compared to original sparse reward and existing\ndense reward methods, showing its advanced ability of em-\npowering efficient learning on sparse reward tasks. By iter-\natively refining the design of rewards functions, Auto MC-\nReward enables the agent to efficiently learn new behaviors\nthat is beneficial to the corresponding tasks, e.g. avoiding\nlava, which greatly improves the success rate. Moreover,\nAuto MC-Reward achieves a high diamond obtaining suc-\ncess rate (36.5%) with only raw information, demonstrating\nits ability of solving long-horizon tasks.\n2. Related Work\nMinecraft Agents are intelligent agents designed to accom-\nplish various tasks while playing the game Minecraft. Most\nof previous works adopt reinforcement learning for agent\ntraining. Due to the extremely sparse rewards and com-\nplex decision space of Minecraft tasks, early attempts have\ntried hierarchical RL [30, 34, 41, 42], curriculum learn-\ning [23], and imitation learning [1] to empower more ef-\nficient RL training. To narrow the decision space, recent\nwork [3] build a foundation model by performing imita-\ntion on YouTube videos. DreamerV3 [17] instead learns\na world model that explores the environment efficiently. As\nthe LLMs demonstrate their general planning ability, a se-\nries of research [19, 46, 47, 51, 53] leverage LLMs as high-\nlevel planners that decompose long-term complex tasks as\nbasic skills and implement the skills with RL agents or\nhandcrafted scripts.\nAuto MC-Reward aims to design dense rewards for\nMinecraft tasks automatically using LLMs, which is orthog-\nonal to previous works on Minecraft agents that mainly fo-\ncus on RL learning algorithms or high-level planning.\nEfficient Learning in Sparse Reward Tasks is a long-\nstanding challenge in RL due to the lack of effective learn-\ning signals [26]. A common solution is to handcraft dense\nreward functions that provide intermediate reward signals\nbased on human expertise, which requires time-consuming\ntrial-and-error for each environment and task. Another line\nof previous research focus on proposing general-purpose\ndense auxiliary reward functions, such as curiosity-driven\nexploration [6, 20, 28, 37], self-imitation learning [35], and\ngoal-conditioned reinforcement learning [2, 24, 27, 45].\nDespite the success on certain specific tasks, the appli-\ncability of these methods in the complex environment of\nMinecraft remains uncertain. Recent works [10, 11, 14, 31,\n32] also propose to use pre-trained models to assign reward\nto intermediate states of completing tasks. However, these\napproaches produce reward values in a black-box manner,\nwhich cannot be interpreted and improved based on the\nexperience of the agents, and the generalizability of these\nmodels on new tasks is not guaranteed.\nIn contrast, Auto MC-Reward automatically produces\nexplainable reward functions according to the task descrip-\ntions. Moreover, the reward functions can be improved to\nbe more precise based on the experience of the agent.\nAutomated Reward Function Design aims to find an op-\ntimal reward function that drives efficient reinforcement\nlearning for interested tasks. Previous works [9, 15] employ\nevolutionary algorithm for searching optimal reward func-\ntions for specific tasks. Most of these attempts have a highly\nconstrained search space that only adjusts parameters of\ntask-specific handcrafted reward templates. Recently, a se-\nries of research [7, 11, 25, 29] employs LLMs for integrat-\ning human preference into open-domain tasks without clear\ncompletion criteria by directly prompting LLMs with envi-\nronment trajectories and natural language task descriptions.\nThe reward values are generated on-the-fly by LLMs, which\nis black-box and has heavy computational cost due to the\nnature of LLMs. In contrast, Auto MC-Reward employs\nLLMs to generate white-box code-form reward functions.\nConcurrent works [33, 49, 50] also propose to use LLMs\nas a coder to generate reward functions for robotics control\ntasks. Specifically, L2R [50] needs to prepare reward func-\ntion templates and cannot cope with unexpected situations\nin open worlds. Text2Reward [49] and EUREKA [33] re-\nquire complete environment code or description and rely on\nhuman feedback, which are not available in open worlds.\nDifferent from these methods, Auto MC-Reward consid-\ners more complex Minecraft environments that has diverse\nscenarios and high uncertainty, requiring more precise and\nthorough reward designing.\n3. Method\nAuto MC-Reward consists of three components: Reward\nDesigner, Reward Critic, and Trajectory Analyzer. Given\nthe environment information and task descriptions, the Re-\nward Designer proposes the reward function by coding an\nexecutable Python function with pre-defined observation in-\nputs.\nThe Reward Critic verifies if the proposed reward\nfunction is self-consistent and if it meets the format require-\nments. The designed reward function which passes the Re-\nward Critic is used to train agents in the environment. To\nimprove the designed reward function according to empiri-\ncal experience, the Trajectory Analyzer is proposed to sum-\nmarize possible failure causes and provide refinement sug-\ngestions on the reward function based on the inference tra-\njectories of the trained agents. Then the Reward Designer\nmodifies the reward function based on the feedback from\nTrajectory Analyzer. Figure 1 shows the overview of the\nAuto MC-Reward.\n3.1. Reward Designer\nWe utilize a Reward Designer to generate the reward func-\ntion code to provide intermediate instructive learning sig-\nnals to the agent. It takes as input task descriptions, game\ninformation, and reward function requirements, generating\nreward functions in executable code form. When updating\nreward function, we also provide analysis of the agent’s per-\nformance when interacting with the game environment. The\ninput prompt is introduced in Section 4.2.\nThe generated reward function uses a pre-defined obser-\nvation format as input. This includes the nearest distance of\neach block type within the visible range in the current and\nprevious steps, changes in inventory between adjacent steps,\nhealth points, and the agent’s location in each past step.\nThese parameters can provide information on the agent’s\ncurrent and historical states, assisting the reward function\nin various situations.\nMulti-Step Memory. Long-term tasks require the transfer\nof information across multiple steps. Thus, we introduce a\nmulti-step memory mechanism. It is provided to Reward\nDesigner as a empty dictionary at the beginning, and the\nreward function can save necessary data into the memory\nto be used in future steps. In the actual reward function of\nthe explore-tree task, we observed that the agent records the\ndistance to a tree at each step, thereby encouraging getting\ncloser with the tree than the previous step.\nChain of Thought. We require the LLM to first describe its\ndesign thoughts, such as considering potential failure rea-\nsons and the details of the reward function design. These\nthoughts are to be written as comments at the beginning of\nthe code. This is a mechanism similar to Chain of Thought\n(CoT) [48], where the thought process precedes the cod-\ning implementation. In the specific code implementation,\nnecessary comments will also be generated every few lines\n(e.g., “Check if lava is in the field of view in the previous\nstep”). This approach not only allows Reward Designer to\nrefer to the text-form thoughts during reward function ini-\ntialization, but also assists subsequent Reward Critic in as-\nsessing the code’s rationality, and helps Reward Designer\nto understand the current reward function’s purpose when\nupdating the reward function.\nScale Constraints. We impose a specific scale constraint\nfor the reward function, where the LLM generates two sub-\nfunctions: dense and sparse. Sparse denotes rewards for\nachieving the final goal or heavy penalties (like death),\nwhile dense represents dense intermediate signals during\nthe task completion process. We preset their numerical val-\nues and only allow the LLM to determine their positivity\nor negativity, limiting sparse to {1, 0, −1}, and dense to\nTrajectory \nAnalyzer\nhistory_location: \n[7,48,9],[6,48,9],[7,48,9],\n[7,48,9],[6,48,9],[7,48,9] \nEncourage exploration \nby whether current \nlocation is never visited \nbefore\nPenalize for \nencountering lava\nAdd a pitch constraint to \nprevent from always \nlooking up to avoid \nseeing lava\nStep 1\ndead:  True\nnearby_blocks:  lava\n{pitch: 0, nearby_block: lava},\n{pitch: -45, nearby_block: none}\n{pitch: -45, nearby_block: none}\nTrajectory \nAnalyzer\nTrajectory \nAnalyzer\nStep 2\nStep 3\nStep 1 update\nStep 2 update\nStep 3 update\nFigure 2. Example of updating the reward function. Trajectory\nAnalyzer provides analysis for three scenarios at different steps,\nand then Reward Designer update the reward function based on\nthe suggestions. We only display part of the trajectory data for\nbrevity. Step 1: rewrite the code of encouraging exploration to\navoid going back and forth. Step 2: add lava penalty to avoid\nfalling into lava. Step 3: add pitch constraint to avoid constantly\nlooking up to avoid lava.\n{0.1, 0, −0.1}. They are then added together for the final\nreward. Therefore, the final reward values can be one value\nof {±1.1, ±1.0, ±0.9, ±0.1, 0}. The final reward is calcu-\nlated as R = sgn(sparse)∗1+sgn(dense)∗0.1, where sgn\ndenotes the sign function. This is to keep the reward within\na reasonable range, allowing the LLM to focus on various\nscenarios that need to be considered in the reward function,\nrather than trivial tasks like adjusting the reward value.\n3.2. Reward Critic\nIn practice, it is difficult for LLM to generate a relatively\ncomplete reward function in the beginning. There may be\nerrors in understanding parameter formats and data types\n(syntax errors), failure to consider game-specific informa-\ntion, or misunderstanding of tasks (semantic errors), etc.\nIn order to eliminate above errors that are not easy to\nfind, we design a LLM based Reward Critic to automati-\ncally review the designed reward function. In addition to\nchecking for syntax errors, Reward Critic is also asked to\ncheck the quality of the reward function to further eliminate\nsemantic errors. Specifically, we require Reward Critic to\ncheck whether the current code implementation matches its\nthoughts, whether it meets the reward function design re-\nquirements, and whether it takes game information into ac-\ncount. If the review fails, the Critic will provide a critique,\nand the Reward Designer will then modify the reward func-\ntion based on the criterion and submit it for review again.\nThe above process is repeated up to 3 times.\nIf an error occurs during the execution of the reward\nfunction in the process of interacting with the environment,\nthe Python traceback of the error message will be fed back\nto Reward Designer for modification. These errors may in-\nclude misunderstandings of input parameters, list index out\nof range, uninitialized keys in dictionaries, and other such\nissues. Some runtime errors only appear during the actual\nexecution of the code.\n3.3. Trajectory Analyzer\nLLMs have the ability to understand environmental infor-\nmation and task instructions through in-context prompts to\ngenerate dense rewards. However, this zero-shot approach\ncompletely relies on LLM’s understanding of the task and\nimagination of the problems it may face, and it is difficult\nto ensure the effectiveness of the designed reward. Take the\nthe yellow highlighted part in Figure 1 as an example, in the\ninitially designed reward function, Reward Designer does\nnot consider the situation where the agent would encounter\nlava and be burned to death. Thus, in order to introduce em-\npirical improvements to the designed dense reward, we pro-\npose to use LLMs, named as Trajectory Analyzer, to sum-\nmarize the historical information of the interaction between\nthe trained agent and the environment and use it to guide\nthe revision of the reward function. The division of labor\nof Reward Designer and Trajectory Analyzer allows for in-\ndependent operations of data analysis and reward function\nupdates. Trajectory Analyzer does not need to know the de-\ntails of the reward function, and Reward Designer does not\nneed to process complex trajectory data.\nSpecifically, the current trained model is used to inter-\nact with the environment and obtain K trajectories. Then,\nwe truncate these trajectories and use a LLM to summarize\nthe observations of the last consecutive L frames of each\nfailed trajectory to automatically infer its possible failure\nreasons. Based on the analysis of the reasons for the fail-\nure, the LLM Trajectory Analyzer is asked to propose key\npoints that Reward Designer needs to consider in the next\nround of reward function revision. For instance, failure sce-\nnarios where punishment is not considered, misalignment of\ndense reward and sparse reward causes the agent’s behavior\nto deviate from the final goal, etc.\nFigure 2 shows an example of multiple rounds of im-\nproving the reward function during the search for diamonds.\nIn the first step, through analysis of the trajectory, Trajec-\ntory Analyzer finds that the agent would opportunistically\nfind a shortcut to increase the reward, that is, move back\nand forth to deceive the reward function into thinking that\nthe agent is moving actively. Therefore, the Reward De-\nsigner modifies the code snippet that encourages the agent\nto move, i.e. encourage the agent to appear in unvisited lo-\ncations as much as possible. Although the initially designed\nreward function has taken into account the penalty for the\nloss of the agent’s health, the agent still cannot effectively\nlearn to avoid lava. When modifying the reward function\nin the second round, Trajectory Analyzer discovers through\nthe failed trajectory that the agent may die from lava, so it is\nsuggested that Reward Designer increase the penalty for en-\ncountering lava, as shown in the step 2 update in Figure 2.\nAccording to the interactive experience, Reward Designer\nexplicitly punishes the continuous appearance of lava in the\nfield of view. However, the excessive punishment of lava\ncaused the agent to choose to turn its perspective upward\nor downward to avoid the appearance of lava in the visible\nrange, making it impossible for the agent to continue ef-\nfective exploration, which deviates from the ultimate goal.\nTo this end, Reward Designer further constrain the agent’s\nperspective in step 3, so that the lava disappeared from the\nagent’s perspective by turning left\/right while continuing to\nsearch for diamonds, which is the desired strategy. Fig-\nure 3(a) shows the successful trajectory of avoiding lava:\nThe agent sees the lava after breaking the stone ahead using\niron pickaxe, and then turn left to avoid the lava through the\nmining tunnel.\n4. Experiment\n4.1. Environment Setup\nWe mainly use the harvest mode in the MineDojo [14] en-\nvironment to verify the model’s ability to play Minecraft.\nThe training pseudo code of Auto MC-Reward is shown in\nAlgorithm 1. We set up the following challenging tasks for\nmodel performance comparison and ablation study:\n• Exploring diamond ore\non the 11-th floor under-\nground: Initially, the agent\nis equipped with an iron\npickaxe on the 11-th floor underground. When the dia-\nmond ore is within the visible range and the distance is\nless than 2 distance units, the task is deemed completed.\nThe difficulty of the task lies in the fact that diamonds\nare very rare, lava frequently appears during exploration,\nleading to death, and the maximum number of steps is\nlimited to 60,000. When steps exceed the limit, the tra-\nAlgorithm 1 Auto MC-Reward Training Pseudo Code\nRequire: Task (T), Inital Agent (A0), Environment (Env), Max number\nof Critic reviews (NCritic)\nEnsure: Final Agent (AN), Final Reward (RN)\nSummary = None\nCritique = None\nR0 = None\nfor i = 1, . . . , N do\nRi = RewardDesigner(Summary, Critique, T, Ri−1)\nfor j = 1, . . . , NCritic do\nCritique, Done = RewardCritic(Ri)\nif Done then\nbreak\nelse\nRi = RewardDesigner(Summary, Critique, T, Ri)\nend if\nend for\nAi = TrainAgent(A0, Ri, Env, T)\nTraji, Stati = Eval(Env, Ai)\nSummary = TrajectoryAnalyzer(Traji, Stati)\nCritique = None\nend for\njectory is considered failed. Long-term exploration can\ndemonstrate the advantages of dense rewards.\n• Approaching tree\nin plains biome\n: The task is\nconsidered successful if the tree is within the agent’s\nvisible range and the distance is less than 1 distance unit.\nThe difficulty of the task lies in the fact that the trees are\nvery sparse on plains, which is extremely detrimental to\nsparse reward functions. The maximum number of steps\nis limited to 2,000 steps.\n• Approaching specific animal (e.g. cow\n, sheep\n)\nin plains biome\n: The task is considered successful\nif the animal is within the agent’s\nvisible range and\nthe distance is less than 2 distance unit. The difficulty of\nthe task is that the animals are constantly moving. The\nmaximum number of steps is limited to 2,000 steps.\n• Obtaining diamond\n: The agent\nneeds to complete\nthe whole process of mining diamonds, including key be-\nhaviors such as finding and obtaining materials on the sur-\nface, crafting, digging down, going back to the ground,\nand mining stone\/iron ore\/diamond ore. The tech tree is\nshown in Figure 4.\n4.2. Implementation Details\nLLM Prompt. The components of the input prompts for\nTrajectory Analyzer include task description, game infor-\nmation, statistical metrics, and information on failed trajec-\ntories. Components of the input prompts for Reward De-\nsigner and Reward Critic includes task description, game\ninformation, input parameters, and reward function require-\nments and format.\nWe use GPT-4 [36] for all the LLM\ncomponents, and set temperature to 0.3. Since the LLMs\nare only used once for each whole agent training instead of\neach action, their computation overhead is negligible.\nFigure 3. The trajectories of the new behaviors. (a) Avoid lava\nwhen exploring for diamond ore\n. (b) Attack cow\nin plains.\n• Instruction: Instructions on initializing, updating and\nhandling execution error of reward function for Reward\nDesigner, reviewing function for Reward Critic, and ana-\nlyzing trajectory for Trajectory Analyzer.\n• Task description: The objective, initial conditions, suc-\ncess criteria, and task flow. For example, for the explore\ndiamond task, the objective is “to find and approach a\ndiamond, achieving a high success rate while avoiding\ndeath.” The initial condition is “agent at y level 11 with\nan iron pickaxe.” The success criteria is “being less than 1\nmeter from the nearest diamond block”, and the task flow\nis “horizontally explore to find a diamond, face it, and\napproach it”. In the task description, we do not provide\nprior game strategy information (task challenges, DFS ex-\nploration strategies, or avoiding lava, etc.) to ensure the\nmethod’s versatility.\n• Game information: Game version, block names, field\nof view, action space, and units of measurement. Game\ninformation provides knowledge about the game’s simu-\nlation environment, not game strategy.\n• Statistical metrics and information on failed trajec-\ntories: success rates, and actions sequences, reward se-\nquences, final inventory and nearby blocks of K = 10\nfailed trajectories. If a trajectory exceeds L = 32 steps, it\nis truncated to the last 32 steps.\n• Input parameters: The nearest distance of each block\ntype within the visible range in the current and previous\nsteps, changes in inventory between adjacent steps, health\npoints, and the agent’s location in each past step. The\nmemory is also provided as an input parameter for storing\ninformation to monitor changes across different steps. We\nprovide explanation and examples of the parameters in the\ninput prompt.\n• Reward function requirements and format: We require\nthe Designer to write a dense function and a sparse func-\ntion, and consider only the sign of the two functions’ re-\nturn values, not the magnitude. The detail of the scale\nconstraints is in Section 3.1.\nImitation Learning Details. When large labeled datasets\ndo not exist, the canonical strategy for training capable\nagents is RL, which is inefficient and expensive to sam-\nple for hard-exploration problems [3, 4, 21], e.g. mining\ndiamond in Minecraft. Therefore, in order to more effi-\nciently explore the effectiveness of the LLM-based reward\nfunction design mechanism proposed in this paper, we pre-\ntrained some foundation models through imitation learning\nas done by VPT [3]. Specifically, we use GITM [53] to\ncontinuously perform Diamond Mining task and record im-\nportant observation data of each frame, such as RGB, ac-\ntion, inventory, GPS, compass, structured actions, etc. In\nthe end, we collect about 11 million image data, totaling\nabout 153 hours (the control frequency is 20 Hz) of game\nvideos. Subsequently, we train these data through fully su-\npervised learning by using Impala CNN [13] and Trans-\nformer [44] as backbone, and obtained several foundation\nmodels. The main differences between the foundation mod-\nels are different biomes (forest and plains), temporal frames\n(16 and 128), and whether goal embedding is used. In sub-\nsequent experiments, these foundation models were used in\ntwo different purposes:\n• Give the RL model preliminary basic Minecraft gameplay\ncapabilities, e.g. forward\/back, turn left\/right, attack, etc.\nFor some tasks that have not been learned (e.g. approach-\ning cows in Figure 3(b)) or not learned well (e.g. avoid\nlava in Figure 3(a), explore tree on plains) in imitation\nlearning, RL algorithms can be studied more efficiently.\n• In the Diamond Mining task, the diamond collection suc-\ncess rate, lava escape rate, death rate, etc. between the RL\nmodel and the imitation learning model are compared to\ndemonstrate the superiority of the proposed method.\nRL Training Details. We use proximal policy optimization\n(PPO) algorithm [39] with generalized advantage estima-\ntion (GAE) [38] to train our RL model. We use γ = 0.99\nand λ = 0.95 for all of our experiments, and the total train-\ning frames is 256,000. To prevent catastrophically forget-\nting or overly aggressive policy update during RL training,\nwe follow VPT [3] to apply an auxiliary Kullback-Leibler\n(KL) divergence loss between the RL model and the frozen\npre-trained policy. We also normalize the reward based on\nthe trajectory returns to constrain the gradient scales of dif-\nferent tasks. See Appendix for details.\n4.3. Main Results\nBaselines. We compare our Auto MC-Reward against the\nfollowing methods:\n• Naive Handcraft: The agent keeps moving (and mining\nfor diamond exploring task) in one direction with a small\nprobability of turning left\/right.\n• Imitation Learning: Our foundation model pre-trained\nwith GITM-generated data, as introduced in Section 4.2.\n• RL with Sparse Reward: Use only the reward from the\noriginal environment, i.e. only receives a reward when\nTable 1. Comparison with other reward methods on three Minecraft tasks. Max steps for exploring tree\nand cow\nare set to 2000.\n†Sparse reward receives a low death rate because it is often stuck in the same place or move in a small area without encountering lava\n.\nMethod\nReward\nExplore Diamond Ore\nUnderground\nApproach Tree\non Plains\nApproach Cow\non Plains\navg. dist. ↑\ndeath (%) ↓\nlava escape (%) ↑\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\nNaive Handcraft\n-\n85.7\n74.3\n1.5\n18.6\n1993\n2.1\n1956\n10.8\nImitation Learning\n-\n102.2\n55.6\n46.8\n38.9\n1988\n2.5\n1772\n22.4\nRL\nSparse\n16.8\n1.5†\n0\n0.5\n1936\n4.3\n1854\n12.6\nRL\nDense (Curiosity)\n102.6\n55.1\n46.0\n39.3\n1672\n45.8\n1477\n13.7\nRL\nDense (Self-Imitation)\n104.0\n54.8\n47.2\n39.7\n1532\n42.5\n1280\n23.5\nRL\nDense (MineCLIP)\n105.9\n54.0\n47.8\n40.5\n1022\n65.6\n1206\n44.9\nOurs\nDense (LLM)\n142.8\n45.2\n70.0\n45.2\n972\n73.4\n1134\n56.3\nTable 2. Comparison with previous methods on success rates of\nobtaining diamond\n. We list observations that are used in the\ninference phase. Auto MC-Reward achieves a remarkable success\nrate without exploiting unfair information (i.e. Lidar and Voxel)\nduring inference.\nMethod\nController\nObservation\nDiamond\nSucc. (%)\nHuman [3]\n-\n-\n50.0\nDreamerV3 [17]\nRL\nRGB, Status\n0.01\nDEPS [47]\nIL\nRGB, Status, Voxel\n0.6\nVPT [3]\nIL + RL\nRGB\n20.0\nGITM [53]\nHandcraft\nLidar, Voxel, Status\n55.0\nOurs\nIL (GITM-guided)\nRGB, GPS\n28.8\nOurs\nIL + RL\nRGB, GPS\n36.5\nTable 3. Ablations on Reward Critic and Trajectory Analyzer for\nexplore diamond ore\ntask. The first row corresponds to using\nthe sparse reward from the original environment. †Sparse reward\nreceives a low death rate because it is often stuck in the same place\nor move in a small area without encountering lava\n.\nDesigner\nCritic\nAnalyzer\nAvg. Dist. ↑\nDeath ↓\nLava Esc. ↑\nSucc. ↑\n16.8\n1.5†\n0\n0.5\n✓\n75.8\n58.2\n30.4\n35.1\n✓\n✓\n95.2\n49.3\n40.7\n40.5\n✓\n✓\n130.6\n47.8\n64.8\n43.1\n✓\n✓\n✓\n142.8\n45.2\n70.0\n45.2\nthe success criteria is completed.\n• RL with Curiosity Dense Reward [37]: Encourage the\nagent to discover and learn about parts of the environment\nthat it has not encountered before.\n• RL with Self-Imitation Dense Reward [35]: Encourage\nthe agent to replicate its past actions that led to high re-\nwards.\n• RL\nwith\nMineCLIP\n[14]\nDense\nReward:\nUse\nMineCLIP to calculate the dense reward based on the sim-\nilarity between RGB frames and task objectives.\nResults on Diamond Ore\nExploring Task. For the plain\nimitation learning model, fitting the training data makes it\nlack the awareness of avoiding lava, so it often dies in lava\nduring the search for diamonds, and only has 38.9% suc-\ncess rate under the limit of 60,000 steps, as shown in Ta-\nble 1. In contrast, our Auto MC-Reward makes the agent\nrealize the importance of avoiding lava by continuously im-\nFigure 4. The tech tree of obtaining diamond. The green squares\nare tasks to be optimized with Auto MC-Reward, i.e. obtaining\nlog\n, cobblestone\n, iron ore\nand diamond\n.\nproving the dense reward function, and the final success rate\nhas increased to 45.2% with 70% lava escape success rate.\nFigure 3(a) demonstrates good awareness of avoiding lava.\nBased on the same reinforcement learning algorithm, the\ndisadvantages of sparse reward functions in long-horizon\ntasks are undoubtedly revealed. By watching the videos of\ncollected trajectories, we find that using sparse functions\noften leads to irreversible behavior, such as being unable\nto break the surrounding ores to move due to maintaining\na head-up posture. Although a low death rate of 1.5% is\nachieved, the actual average moving distance is only 16.8,\nand the success rate is only 0.5%. Due to the similar scenes\nunderground, MineCLIP cannot give differentiated rewards,\nso its performance is close to the initial imitation learning\nmodel.\nOther baselines, like curiosity and self-imitation\ndense reward, also have mediocre performance and the suc-\ncess rate has not been significantly improved.\nResults on Tree\nApproaching Task. Since trees are ex-\ntremely sparse on the plain, the imitation learning model\nand the RL model with sparse reward cannot perform well,\nwith only 2.5% and 4.3% success rates respectively, and\ntheir average action steps are close to the maximum limit.\nMineCLIP dense reward receives a success rate of 65.6%\nsince it can provide positive reward when tree is visible.\nCuriosity and self-imitation methods also achieve better re-\nsults than imitation learning. For Auto MC-Reward, Re-\nward Designer uses a strategy of giving positive rewards for\ngetting closer and deducting rewards for going away, so that\nthe agent learns to slowly approach the target, ultimately\nachieving 73.4% success rate with only 972 average steps.\nResults on Cow\nApproaching Task. The task of explor-\ning for cows does not appear in the training data of imitation\nlearning, so the zero-shot ability on this task is not ideal,\nwith about 22.4% success rate and average steps close to\nthe maximum limit. By checking the videos, we find most\nof the successful cases are due to good luck without inten-\ntion to actively approach the target. The same experimental\nconclusion is also obtained in the experiment of sparse re-\nward function. Similar to the Tree Approaching Task, the\nsuperior dense reward function design mechanism makes\nour agent 43.7% (56.3% vs. 12.6%) higher than sparse re-\nward, as listed in Table 1. Another dense reward MineCLIP\nalso shows strong performance in this task, but due to the\nneed to calculate the similarity of images and texts at all\ntimes during training, the efficiency is unacceptable.\nResults on Obtaining Diamond\n. We verify the pro-\nposed method on a more difficult task, that is, the tech tree\nof collecting diamonds, as shown in Figure 4. As mentioned\nbefore, our foundation imitation learning model already has\na certain ability from birth to diamond mining. We use the\nproposed method to optimize several key tasks in the pro-\ncess to increase the success rate of final diamond acquisi-\ntion. The green parts in Figure 4 are the tasks that need\nto be optimized, i.e. obtaining log, cobblestone, iron ore\nand diamond. We conduct experiments in two biomes in\nMinecraft, and the cumulative success rate is shown in Fig-\nure 5. Specifically, the lower death rate allows our agent\nto have a higher success rate in mining iron ore and dia-\nmond, and ultimately achieves 36.5% success rate on for-\nest biome, which is 7.7% higher than the imitation learning\nmodel. As for plains, the difficulty of obtaining log makes\nthe imitation learning model unable to complete any tasks.\nAuto MC-Reward overcomes the difficulty of obtaining log,\nthus achieving a 28.1% success rate in obtaining diamonds.\nTable 2 provides a rough comparison of several different\nmethods on the task of mining diamonds. We achieve a\nhigh success rate without exploiting unfair information (i.e.\nLidar and Voxel) during the inference phase.\n4.4. Ablation studies\nEffectiveness of Reward Designer. The first row of Table 3\nis an RL experiment with a sparse reward function. As men-\ntioned before, it cannot explore diamonds normally. After\nadding Reward Designer, it regained the ability to explore\nunder a dense reward function.\nEffectiveness of Reward Critic.\nAs listed in Table 3,\nthe success rate of exploring diamonds has increased from\n35.1% to 40.5% by adding Reward Critic, because it can\nreduce the syntax and semantic errors in the code, making\nthe training process more effective and sufficient. For exam-\nple, the Trajectory Analyzer concludes that the agent died in\nlava and asks the Reward Designer to add relevant penalties.\nHowever, without being checked by Critic for semantic er-\nrors, it is possible that the added code snippet uses the word\n“magma” instead of the correct one “lava”. This will result\nFigure 5. Cumulative success rates for 4 key items of obtaining\ndiamond on forest\nand plains\n. In terms of diamond, the per-\nformance comparison between imitation learning and Auto MC-\nReward in two biomes are: 28.8% vs. 36.5%, and 0% vs. 28.1%.\nin insufficient learning of lava avoidance, which is reflected\nin a 2.1% (43.1% vs. 45.2%) success rate difference.\nEffectiveness of Trajectory Analyzer. As observed in Ta-\nble 3, Trajectory Analyzer is the key to improve the suc-\ncess rate of completing tasks. It summarizes the reasons for\nfailure to be fed into Reward Designer, allowing it to itera-\ntively modify an appropriate dense reward function to guide\nthe agent to overcome difficulties. In terms of Diamond Ex-\nploring Task, Trajectory Analyzer provides timely feedback\non the potential risks of lava, which greatly improves the\nsurvival rate and moving distance, ultimately improving the\nsuccess rate from 40.5% to 45.2%.\n5. Conclusion\nWe proposed Auto MC-Reward, an automated dense re-\nward design framework for addressing challenges caused by\nsparse reward and complex environment of Minecraft. It ad-\ndresses the issue of sparse rewards by leveraging LLMs to\nautomatically generate dense reward functions, enhancing\nlearning efficiency. The system consists of three key com-\nponents: Reward Designer, Reward Critic, and Trajectory\nAnalyzer, which are used for the design, verification and\nanalysis of the reward function respectively. Its capabilities\nare validated through experiments, demonstrating a remark-\nable improvement in complex tasks in Minecraft. Future\nwork may deal with the limited trajectory length for analysis\n(last 32 frames) due to the context length of LLMs, which\nhinders the analysis of long-term failures (e.g., not explor-\ning new areas, circling around lava).\nAuto MC-Reward\nhumbly contributes to more effective learning in complex\ntasks through its automated dense reward function design.\nWe hope it can pave the way for further research in rein-\nforcement learning and its real-world applications.\nAcknowledgement\nThis work is supported by the National Key R&D Program\nof China (NO. 2022ZD0161300, NO. 2022ZD0160100),\nby the National Natural Science Foundation of China\n(62376134).\nReferences\n[1] Artemij Amiranashvili, Nicolai Dorka, Wolfram Burgard,\nVladlen Koltun, and Thomas Brox. Scaling imitation learn-\ning in minecraft. arXiv preprint arXiv:2007.02701, 2020. 2\n[2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas\nSchneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh\nTobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hind-\nsight experience replay. Advances in neural information pro-\ncessing systems, 30, 2017. 2\n[3] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga,\nJie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampe-\ndro, and Jeff Clune. Video pretraining (vpt): Learning to act\nby watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639–24654, 2022. 2,\n6, 7, 12, 15\n[4] Christopher Berner, Greg Brockman, Brooke Chan, Vicki\nCheung, Przemysław D˛ebiak, Christy Dennison, David\nFarhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al.\nDota 2 with large scale deep reinforcement learning. arXiv\npreprint arXiv:1912.06680, 2019. 6, 12\n[5] Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Ku-\nmar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis,\nand Vincent Moens. Torchrl: A data-driven decision-making\nlibrary for pytorch, 2023. 15\n[6] Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey,\nTrevor Darrell, and Alexei A Efros. Large-scale study of\ncuriosity-driven learning. arXiv preprint arXiv:1808.04355,\n2018. 2\n[7] Thomas Carta, Pierre-Yves Oudeyer, Olivier Sigaud, and\nSylvain Lamprier.\nEager:\nAsking and answering ques-\ntions for automatic reward shaping in language-guided rl.\nAdvances in Neural Information Processing Systems, 35:\n12478–12490, 2022. 3\n[8] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Ro-\ndrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal,\nPablo Samuel Castro, and Jordan Terry. Minigrid & mini-\nworld: Modular & customizable reinforcement learning en-\nvironments for goal-oriented tasks. CoRR, abs\/2306.13831,\n2023. 11\n[9] Hao-Tien Lewis Chiang, Aleksandra Faust, Marek Fiser, and\nAnthony Francis. Learning navigation behaviors end-to-end\nwith autorl. IEEE Robotics and Automation Letters, 4(2):\n2007–2014, 2019. 3\n[10] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju,\nJessica Landon, Felix Hill, Nando de Freitas, and Serkan\nCabi. Vision-language models as success detectors. arXiv\npreprint arXiv:2303.07280, 2023. 2\n[11] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas,\nTrevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob\nAndreas. Guiding pretraining in reinforcement learning with\nlarge language models.\narXiv preprint arXiv:2302.06692,\n2023. 2, 3\n[12] Sean C Duncan. Minecraft, beyond construction and sur-\nvival. Well Played: A Journal on Video Games, Value and\nMeaning, 1(1), 2011. 2\n[13] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Si-\nmonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu,\nTim Harley, Iain Dunning, et al.\nImpala: Scalable dis-\ntributed deep-rl with importance weighted actor-learner ar-\nchitectures. In International conference on machine learn-\ning, pages 1407–1416. PMLR, 2018. 6, 13\n[14] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar.\nMinedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems, 35:\n18343–18362, 2022. 2, 5, 7\n[15] Aleksandra Faust, Anthony Francis, and Dar Mehta. Evolv-\ning rewards to automate reinforcement learning.\narXiv\npreprint arXiv:1905.07628, 2019. 3\n[16] William H Guss, Brandon Houghton, Nicholay Topin,\nPhillip Wang, Cayden Codel, Manuela Veloso, and Ruslan\nSalakhutdinov. Minerl: A large-scale dataset of minecraft\ndemonstrations. arXiv preprint arXiv:1907.13440, 2019. 2\n[17] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2, 7\n[18] Jie Huang and Kevin Chen-Chuan Chang. Towards reason-\ning in large language models: A survey.\narXiv preprint\narXiv:2212.10403, 2022. 2\n[19] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor\nMordatch, Yevgen Chebotar, et al. Inner monologue: Em-\nbodied reasoning through planning with language models.\narXiv preprint arXiv:2207.05608, 2022. 2\n[20] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czar-\nnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray\nKavukcuoglu.\nReinforcement learning with unsupervised\nauxiliary tasks. arXiv preprint arXiv:1611.05397, 2016. 2\n[21] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke\nMarris, Guy Lever, Antonio Garcia Castaneda, Charles Beat-\ntie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman,\net al.\nHuman-level performance in 3d multiplayer games\nwith population-based reinforcement learning. Science, 364\n(6443):859–865, 2019. 6, 12\n[22] Matthew Johnson, Katja Hofmann, Tim Hutton, and David\nBignell. The malmo platform for artificial intelligence ex-\nperimentation. In Ijcai, pages 4246–4247, 2016. 2\n[23] Ingmar\nKanitscheider,\nJoost\nHuizinga,\nDavid\nFarhi,\nWilliam Hebgen Guss, Brandon Houghton, Raul Sampe-\ndro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang,\net al.\nMulti-task curriculum learning in a complex, vi-\nsual, hard-exploration domain: Minecraft.\narXiv preprint\narXiv:2106.14876, 2021. 2\n[24] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and\nJosh Tenenbaum.\nHierarchical deep reinforcement learn-\ning: Integrating temporal abstraction and intrinsic motiva-\ntion.\nAdvances in neural information processing systems,\n29, 2016. 2\n[25] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa\nSadigh. Reward design with language models. arXiv preprint\narXiv:2303.00001, 2023. 3\n[26] Pawel Ladosz, Lilian Weng, Minwoo Kim, and Hyondong\nOh. Exploration in deep reinforcement learning: A survey.\nInformation Fusion, 85:1–22, 2022. 2\n[27] Andrew Levy, George Konidaris, Robert Platt, and Kate\nSaenko.\nLearning multi-level hierarchies with hindsight.\narXiv preprint arXiv:1712.00948, 2017. 2\n[28] Jing Li, Xinxin Shi, Jiehao Li, Xin Zhang, and Junzheng\nWang. Random curiosity-driven exploration in deep rein-\nforcement learning. Neurocomputing, 418:139–147, 2020.\n2\n[29] Jessy Lin, Daniel Fried, Dan Klein, and Anca Dragan. In-\nferring rewards from language in context.\narXiv preprint\narXiv:2204.02515, 2022. 3\n[30] Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu,\nand Wei Yang. Juewu-mc: Playing minecraft with sample-\nefficient hierarchical reinforcement learning.\nIn Interna-\ntional Joint Conference on Artificial Intelligence (IJCAI),\n2022. 2\n[31] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Os-\nbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards\nuniversal visual reward and representation via value-implicit\npre-training. arXiv preprint arXiv:2210.00030, 2022. 2\n[32] Yecheng Jason Ma, William Liang, Vaidehi Som, Vikash\nKumar, Amy Zhang, Osbert Bastani, and Dinesh Jayara-\nman. Liv: Language-image representations and rewards for\nrobotic control. arXiv preprint arXiv:2306.00958, 2023. 2\n[33] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An\nHuang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi\nFan, and Anima Anandkumar. Eureka: Human-level reward\ndesign via coding large language models.\narXiv preprint\narXiv:2310.12931, 2023. 3\n[34] Hangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yim-\ning Lu, Chengjie Wu, Jianye Hao, Dong Li, and Pingzhong\nTang. Seihai: A sample-efficient hierarchical ai for the min-\nerl competition. In Distributed Artificial Intelligence: Third\nInternational Conference, DAI 2021, Shanghai, China, De-\ncember 17–18, 2021, Proceedings 3, pages 38–51. Springer,\n2022. 2\n[35] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.\nSelf-imitation learning. In International Conference on Ma-\nchine Learning, pages 3878–3887. PMLR, 2018. 2, 7\n[36] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 2, 5\n[37] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor\nDarrell. Curiosity-driven exploration by self-supervised pre-\ndiction. In International conference on machine learning,\npages 2778–2787. PMLR, 2017. 2, 7\n[38] John Schulman, Philipp Moritz, Sergey Levine, Michael Jor-\ndan, and Pieter Abbeel. High-dimensional continuous con-\ntrol using generalized advantage estimation. arXiv preprint\narXiv:1506.02438, 2015. 6, 15\n[39] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms. arXiv preprint arXiv:1707.06347, 2017. 6, 15\n[40] Kyoungwon Seo, Joice Tang, Ido Roll, Sidney Fels, and\nDongwook Yoon.\nThe impact of artificial intelligence on\nlearner–instructor interaction in online learning.\nInterna-\ntional journal of educational technology in higher education,\n18(1):1–23, 2021. 2\n[41] Alexey Skrynnik, Aleksey Staroverov, Ermek Aitygulov,\nKirill Aksenov, Vasilii Davydov, and Aleksandr I Panov. Hi-\nerarchical deep q-network from imperfect demonstrations in\nminecraft. Cognitive Systems Research, 65:74–78, 2021. 2\n[42] Chen\nTessler,\nShahar\nGivony,\nTom\nZahavy,\nDaniel\nMankowitz, and Shie Mannor. A deep hierarchical approach\nto lifelong learning in minecraft. In Proceedings of the AAAI\nconference on artificial intelligence, 2017. 2\n[43] Adrienne L Tierney and Charles A Nelson III. Brain devel-\nopment and the role of experience in the early years. Zero to\nthree, 30(2):9, 2009. 2\n[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 6, 13\n[45] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul,\nNicolas Heess, Max Jaderberg, David Silver, and Koray\nKavukcuoglu. Feudal networks for hierarchical reinforce-\nment learning.\nIn International Conference on Machine\nLearning, pages 3540–3549. PMLR, 2017. 2\n[46] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandku-\nmar. Voyager: An open-ended embodied agent with large\nlanguage models. arXiv preprint arXiv:2305.16291, 2023. 2\n[47] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiao-\njian Ma, and Yitao Liang. Describe, explain, plan and select:\ninteractive planning with llms enables open-world multi-task\nagents. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023. 2, 7\n[48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824–24837, 2022. 3\n[49] Tianbao Xie,\nSiheng Zhao,\nChen Henry Wu,\nYitao\nLiu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao\nYu.\nText2reward:\nAutomated dense reward function\ngeneration for reinforcement learning.\narXiv preprint\narXiv:2309.11489, 2023. 3\n[50] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani,\nKuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis\nChiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al.\nLanguage to rewards for robotic skill synthesis.\narXiv\npreprint arXiv:2306.08647, 2023. 3\n[51] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie,\nPenglin Cai, Hao Dong, and Zongqing Lu.\nPlan4mc:\nSkill reinforcement learning and planning for open-world\nminecraft tasks. arXiv preprint arXiv:2303.16563, 2023. 2\n[52] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei\nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhang, Zican Dong, et al. A survey of large language mod-\nels. arXiv preprint arXiv:2303.18223, 2023. 2\n[53] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-\njie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiao-\ngang Wang, et al. Ghost in the minecraft: Generally capable\nagents for open-world enviroments via large language mod-\nels with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023. 2, 6, 7, 12\nA. Experiments on More Minecraft Tasks\nA.1. Environment Setup\nThis section conducts experiments on more Minecraft tasks to verify the effectiveness of the proposed reward function design\nmechanism. The experiments mainly includes the approaching task of three animals including pig\n, sheep\nand chicken\n, as well as the attacking\ntask of cow\n.\n• Approaching animals in plains biome. The task is considered successful if the animal is within the agent’s\nvisible\nrange and the distance is less than 2 distance unit. The difficulty of the task is that the animals are constantly moving and\nsmall (e.g. chicken\n). The maximum number of steps is limited to 2,000 steps.\n• Attacking cow in plains biome. The task is considered successful if the agent\nsuccessfully kills\na cow\nand\nobtains beef\nor leather\n. The maximum number of steps is limited to 4,000 steps.\nA.2. Results\nTable I compares the performance between imitation learning model and ours. As the task of approaching for animals does\nnot appear in the training data of imitation learning, our method achieves a significant improvement in success rate in fewer\nsteps compared to imitation learning model’s zero shot. In the more difficult task of attacking cow, our method also achieves\nclear advantages. By further checking the videos, we find most of the successful cases are due to good luck without intention\nto actively approach the target. In the supplementary materials, we have prepared several demo videos, in which we can\nclearly see that the agent trained by our method has a clear awareness of completing the corresponding tasks.\nTable I. Compare imitation learning with our method on four Minecraft tasks.\nMethod\nApproach Pig\non Plains\nApproach Sheep\non Plains\nApproach Chicken\non Plains\nAttack Cow\non Plains\navg. step ↓\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\navg. step ↓\nsucc. (%) ↑\nImitation Learning\n1664\n25.0\n1734\n23.5\n1496\n36.4\n3522\n13.3\nOurs\n830\n67.3\n1024\n60.6\n1391\n54.5\n3133\n40.8\nB. Experiments of scalability on MiniGrid Environment\nTo verify the scalability and effectiveness of Auto MC-reward when applied on new environments other than Minecraft, we\nconduct preliminary experiments on the cross-lava task of the MiniGrid environment [8], where the agent is required to reach\nthe goal point on the other corner of the room while avoiding rivers of deadly lava. We choose two environment settings of\ndifferent difficulty levels: LavaCrossingS9N1 with map size 9 × 9 and 1 lava stream, and LavaCrossingS9N3 with\nmap size 9 × 9 and 3 lava streams. We modify the prompts of game information, task description, and input parameters\nto adapt to the MiniGrid environment, but other parts (e.g., reward function requirements and format) remain unchanged.\nThe example environment visualization and results are shown in Figure I. Auto MC-Reward shows its superiority to original\nsparse reward, indicating its effectiveness in new environments.\n0\n1\n2\n3\n4\n5\n6\nframes\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\naverage return\nLavaCrossingS9N1-v0, sparse\nLavaCrossingS9N1-v0, ours\nLavaCrossingS9N3-v0, sparse\nLavaCrossingS9N3-v0, ours\nFigure I. (Left) Example environment visualization of MiniGrid LavaCrossingS9N1 environment. (Right) Performance comparison\non MiniGrid.\nC. Experiments with Different LLMs\nOur framework leverages different LLM capabilities: code comprehension and generation for Designer and Critic, and JSON-\nlike data interpretation for Analyzer. In Fig. II (left), we investigate different LLMs. We use the case of avoiding lava from\nSec. H to test whether each LLM can analyze that lava is the failure reason, and whether it can add the penalty for lava into\nthe reward function. Notably, small LLMs like Mistral Medium show better performance than GPT-4. We hope that future\nwork could provide more comprehensive evaluation to guide the selection of LLMs.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess rate as Analyzer\n0.4\n0.6\n0.8\n1.0\nSuccess rate as Designer\nGPT-4\n(67.0)\nGPT-3.5\n(48.1)\nGemini Pro\n(67.7)\nLlama 2 7B\n(11.6)\nLlama 2 13B\n(18.9)\nLlama 2 70B\n(29.3)\nLlama 2 13B chat\n(18.9)\nLlama 2 70B chat\n(32.3)\nMistral Tiny\n(26.2)\nMistral Small\n(40.2)\nMistral Medium\nFigure II. Effect of different LLMs. Numbers in the parentheses are benchmark scores of HumanEval on code generation.\nD. Foundation Model Imitation Learning\nD.1. Pretraining Data\nWhen large labeled datasets do not exist, the canonical strategy for training capable agents is RL, which is inefficient and\nexpensive to sample for hard-exploration problems [3, 4, 21], e.g. mining diamond in Minecraft. Therefore, in order to\nmore efficiently explore the effectiveness of the LLM-based reward function design mechanism proposed in this paper, we\npre-trained some foundation models through imitation learning as done by VPT [3]. Specifically, we use GITM [53] to\ncontinuously perform Diamond Mining task and record important observation data of each frame, such as RGB, action,\ninventory, compass (e.g. y_level), structured action (action plans), etc. The following is the format of the collected data:\n{\n\"action_plans\": [\n{\n\"name\": \"mine\",\n\"args\": {\n\"object\": {\"log\": 1},\n\"tool\": None\n},\n\"expectation\": \"Obtain 1 log\"\n},\n...\n],\n\"actions\": [\n[0, 0, 0, 12, 12, 3, 0, 0], ...\n],\n\"y_level\": [66, ...],\n\"action_indices\": [100, ...],\n\"biomes\": [\"Forest\", ...],\n\"deaths\": [0, ...],\n\"visible_blocks\": [\n[\n{\"name\": \"wood\", \"nearest_distance\": 0.71}, ...\n],\n...\n],\n\"visible_entities\": [\n[\n{\"name\": \"cow\", \"nearest_distance\": 4.57}, ...\n],\n...\n],\n\"inventory\": [{\"log\": 1}, ...],\n\"task_names\": [\"Obtain 1 log\", ...],\n\"task_indices\": [88, ...],\n}\nFinally, we collect approximately 20.5\/5.3 million image data in forest\/plains biome, as listed in Table II. To reduce\ntraining costs, we only use 9 million and 2 million image data for training and validation in forest and plains biomes,\nrespectively, totaling about 153 hours of game video (the control frequency is 20 Hz).\nTable II. Detailed statistics of generated trajectories.\nItem\nForest Biome\nPlains Biome\ntrajectories\nframes\nmax frames \/ traj.\nmin frames \/ traj.\ntrajectories\nframes\nmax frames \/ traj.\nmin frames \/ traj.\nlog\n14,790\n5,209,246\n13,806\n8\n3,254\n2,425,602\n14,373\n10\nplanks\n16,452\n16,452\n1\n1\n3,647\n3,647\n1\n1\nstick\n7,198\n9,076\n2\n1\n1,550\n1,908\n2\n1\ncrafting table\n3,452\n3,452\n1\n1\n837\n837\n1\n1\nwooden pickaxe\n2,972\n408,056\n2,263\n72\n685\n97,529\n2,328\n106\ncobblestone\n3,047\n1,704,098\n13,107\n180\n681\n341,737\n6,681\n184\nstone pickaxe\n2,729\n414,750\n4,473\n57\n607\n86,806\n2,486\n108\niron ore\n2,177\n6,498,252\n14,833\n200\n461\n1,394,251\n12,783\n235\nfurnace\n1,879\n280,604\n7,912\n86\n362\n55,101\n2,885\n28\niron ingot\n1,632\n497,513\n7,334\n17\n281\n91,585\n4,916\n254\niron pickaxe\n1,588\n189,718\n877\n38\n274\n32,450\n656\n106\ndiamond\n756\n5,285,204\n14,989\n1,154\n119\n766,785\n14,634\n1,226\ntotal\n58,672\n20,516,421\n-\n-\n12,758\n5,298,238\n-\n-\nD.2. Action Space\nWe select 14 actions to train imitation learning for four tasks (obtain log\/cobblestone\/iron ore\/diamond), as shown in Table III.\nD.3. Training Details\nSubsequently, we conduct fully supervised training on the four main tasks of the Diamond mining Task, i.e. log, cobblestone,\niron ore, diamond, by using Impala CNN [13] and Transformer [44] as backbone, and obtained several foundation models.\nThe main differences between the foundation models are different biomes (forest and plains), temporal frames (16 and 128),\nand whether goal and y_level embeddings are used as conditions. We use 32 A800 GPUs for foundation model training, and\nTable III. Action space used in our method.\nIndex\nAction Name\nAction Box\nIndex\nAction Name\nAction Box\n0\nno_op\n[0, 0, 0, 12, 12, 0, 0, 0]\n7\njump\n[0, 0, 1, 12, 12, 0, 0, 0]\n1\nturn_up\n[0, 0, 0, 11, 12, 0, 0, 0]\n8\nback\n[2, 0, 0, 12, 12, 0, 0, 0]\n2\nturn_down\n[0, 0, 0, 13, 12, 0, 0, 0]\n9\nmove_left\n[0, 1, 0, 12, 12, 0, 0, 0]\n3\nturn_left\n[0, 0, 0, 12, 11, 0, 0, 0]\n10\nmove_right\n[0, 2, 0, 12, 12, 0, 0, 0]\n4\nturn_right\n[0, 0, 0, 12, 13, 0, 0, 0]\n11\nattack\n[0, 0, 0, 12, 12, 3, 0, 0]\n5\nforward\n[1, 0, 0, 12, 12, 0, 0, 0]\n12\nequip\n[0, 0, 0, 12, 12, 5, 0, 0]\n6\nforward_jump\n[1, 0, 1, 12, 12, 0, 0, 0]\n13\njump_place\n[0, 0, 1, 12, 12, 6, 0, 0]\ninitialize the network using the foundation model of VPT to speed up the training of imitation learning. The hyperparameters\nare listed in Table IV.\nTable IV. Hyperparameters for imitation learning.\nHyperparameter\nValue\nLearning rate\n1e-4\nBatch size\n128\nEpochs\n25\nOptimizer\nAdam\nThe obtained foundation models, especially the goal-conditioned one, already have satisfactory basic behavioral capabil-\nities. Based on these models, this paper uses the proposed methods to further improve existing behaviors (e.g. avoid lava\nwhile exploring diamond) and complete new tasks (e.g. approaching \/ killing animals).\nE. Reinforcement Learning Training\nWe use proximal policy optimization (PPO) algorithm [39] with generalized advantage estimation (GAE) [38] implemented\nin TorchRL [5] library to train our RL model. The main hyperparameters used for RL training are listed in Table V.\nTable V. Hyperparameters for reinforcement learning.\nHyperparameter\nValue\nLearning rate\n5e-5\nTotal training frames\n256,000\nBatches per iteration\n64\nDiscount factor (γ)\n0.99\nGAE λ\n0.95\nValue pre-training iteration\n2\nPPO clip\n0.1\nEntropy loss weight\n0.1\nCritic loss type\nL2\nCritic coefficient\n1\nOptimizer\nAdam\nIn addition to the above hyperparameters, several training techniques are applied to all of our experiments:\n• We follow VPT [3] to apply an auxiliary Kullback-Leibler (KL) divergence loss between the RL model and the frozen pre-\ntrained policy to prevent catastrophically forgetting or overly aggressive policy update during RL training, e.g., maintaining\nthe ability to dig horizontal tunnels. This loss is defined as:\nLkl = αKL(πpt, πθ)\n(1)\nwhere πθ is the the policy being trained, πpt is the frozen pretrained policy, KL(πpt, πθ)is the Kullback-Leibler divergence\nbetween the policy being trained and the pretrained policy, and α is the loss weight that is set to 0.125 by default.\n• We also normalize the reward based on the trajectory returns to constrain the return scales of different tasks. The formula\nis as follows:\nR′ = mean(abs(R))\nR′\ncur = βR′\ncur + (1 −β)R′\npre\nrnorm =\nr\nmax(R′cur, 1)\n(2)\nwhere r and R are reward and trajectory returns. β is a momentum parameter, set as 0.7 by default.\nF. Full Text Prompts\nWe provide all the LLM input prompts of our method in this section.\nF.1. Task Description\nThe task description contains task objective, initial status, success criteria and procedure of each task. It describe basic\ninformation of a task without prior knowledge like game strategies.\nExploring Diamond Ore\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meters.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\nApproaching a Tree\non Plains\n## Task description\n- Objective: Find and approach wood in the plains biome.\n- Initial Status: The agent is on the ground.\n- Success Criteria: The distance to the nearest wood block is less than 1 meters.\n- Procedure: Find a wood, face it and approach it.\n- Additional Information: Wood is sparse in plains biome. The agent may need to walk a long distance to find and approach a\nwood.\nApproaching a Cow\non Plains\n## Task description\n- Objective: Find and approach cow in the plains biome.\n- Initial Status: The agent is on the ground.\n- Success Criteria: The distance to the nearest cow is less than 2 meters.\n- Procedure: Find a cow, face it and approach it.\nAttacking a Cow\non Plains\n## Task description\n- Objective: Find, approach and kill cow in the plains biome.\n- Initial Status: The agent is on the ground.\n- Success Criteria: The agent successfully kills a cow.\n- Procedure: Find a cow, face it, approach it and kill it.\nF.2. Reward Designer\nPrompts for Reward Designer include prompts for initializing reward function, updating reward function, and handling critic\nfailure and execution error.\nPrompt for Initializing Reward Function\nYou are now a proficient Minecraft player. You should help me write proper reward functions to train a Minecraft agent with\nreinforcement learning to complete the described task.\n{task_description}\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Output Requirements\n- The reward function should be written in Python 3.9.\n- Output the code block only. **Do not output anything else outside the code block**.\n- You should include **sufficient comments** in your reward function to explain your thoughts, the objective and **implementa-\ntion details**. The implementation can be specified to a specific line of code.\n- If you need to import packages (e.g. math, numpy) or define helper functions, define them at the beginning of the function. Do\nnot use unimported packages and undefined functions.\n## Output format\nStrictly follow the following format. **Do not output anything else outside the code block**.\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\nNow write a reward function. Then in each iteration, I will use the reward function to train an RL agent, and test it in the\nenvironment. I will give you possible reasons of the failure found during the testing, and you should modify the reward function\naccordingly.\nPrompt for Updating Reward Function\nYou are now a proficient Minecraft player. I now have a reward function to train a Minecraft agent with reinforcement learning to\ncomplete the described task. The reward function is used to train the reinforcement learning agent for several times. I will provide\nyou the analysis of failure and inefficiency and suggestions. You should help me modify the reward function.\n{task_description}\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Current reward function\n{current_reward_function}\n## Reward function test result\nThe reward function is used to train the reinforcement learning agent for several times. Here is some analysis of failure and\ninefficiency and suggestions:\n```\n{analysis}\n```\n## Requirements\nPlease consider the analysis and suggestions above and modify the reward function.\n1. You can both modify the current lines and add new lines.\n2. If necessary, you can write a **totally different** reward function than the current one.\n3. Consider modifing the reward and penalty values in the current reward function to balance them.\n4. In the first part of the reward function, you should provide your thoughts of modifying the reward function. **The thoughts\nshould be concise.**\n## Output format\nStrictly follow the following format. **Do not output anything else outside the code block**.\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\nPrompt for Critic Failure\nThe reward function fails to pass the verification. The reasoning is:\n```\n{reasoning}\n```\nThe critique is:\n```\n{critique}\n```\n## Requirements\nPlease consider the reasoning and critique, and modify the reward function.\n1. If necessary, you can write a totally different reward function than the current one.\n2. In the first part of the reward function, you should provide your thoughts of modifying the reward function. The thoughts should\nnot directly copy the given reasoning or critique. **The thoughts should be concise.**\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Output format\nStrictly follow the following format. **Do not output anything else outside the code block**.\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\nPrompt for Code Execution Error\nYou are now a proficient Minecraft player. I now have a reward function to train a Minecraft agent with reinforcement learning to\ncomplete the described task. The reward function now encounters an execution error. I will provide you the execution error and\nthe reward function. You should help me modify the reward function.\n{task_description}\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Current reward function\n{current_reward_function}\n## Execution error traceback\n```\n{execution_error}\n```\n## Requirements\nPlease consider the reason of error and modify the reward function.\nIn the first part of the reward function, you should write your analysis of the error in comments, and provide your thoughts of\nmodifying the reward function. **The analysis and thoughts should be concise.**\n## Output format\nStrictly follow the following format. **Do not output anything else outside the code block**.\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\nF.3. Reward Critic\nThe prompt for Reward Critic judges whether the current reward function meets the requirements, and provides critique when\nthe judgement fails.\nYou are now a proficient Minecraft player. I will give you a reward function written in Python, which is used to train an RL agent\nto play Minecraft.\nYou should help me to evaluate whether the reward function has met the requirements.\n{task_description}\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Requirements\n1. You should check whether the reward function meets the **reward function requirements** above.\n2. Your judgement should consider whether the comments and the detailed implementation code are consistent. You can judge\nwhether a line is correct by its comment, but do only rely on the comments.\n3. You should also check the format of the reward function. It should be like:\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\n4. Consider the **Information about the game state** above.\n5. **You don’t need to check whether the reward function meets the task description**.\n6. You should first provide a reasoning and decision. If the reward function fails, provide a critique to help to improve the reward\nfunction.\n## Output format\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"reasoning\",\n\"success\": boolean,\n\"critique\": critique,\n}\nEnsure the response can be parsed by Python `json.loads`, e.g. no trailing commas, no single quotes, etc. Do not output anything\nelse like explanations.\n## Reward function to be evaluated\n{reward_function}\nF.4. Trajectory Analyzer\nThe prompt for Trajectory Analyzer analyzes the trajectory information and statistics, and provides possible reasons of failure\nand suggestions on game strategy.\nYou are now a proficient Minecraft player. I have trained an RL agent and tested for several times in the Minecraft game environ-\nment.\nI will give you the information of the failed test results, i.e. trajectories of actions, rewards and observations. You should help me\nwrite proper analysis of possible reasons of failure and inefficiency, and your suggestion on the game strategy.\n{task_description}\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Input format\n### Failed trajectories\nThe failed trajectories are shown as a list, where each entry is a dictionary representing a trajectory. Note that **only the failed\ncases are provided.**\nThe each item of \"history\" is a list representing statistics of each single step. When the history is too long, it will be truncated to\nthe last 32 steps, indicated by the value of \"truncated\".\n\"dead\" indicates whether the agent has died.\nThe format is:\n[\n{\n\"history\": {\n\"rewards\": [reward1, ...],\n\"actions\": [action1, action2, ...],\n\"locations\": [[x1, y1, z1, pitch1, yaw1], ...],\n\"inventory_change\": {item1: delta_num1, ...},\n\"truncated\": True or False,\n},\n\"final_health\": health,\n\"final_inventory\": {item1: num1, ...},\n\"final_nearest_blocks\": {block_name1: nearest_distance1, block_name2: nearest_distance2, ...},\n\"block_under_foot\": block_name,\n\"dead\": True or False,\n},\n...\n]\nwhere \"final_health\" is in range 0 and 10. \"block_under_foot\" is the block right under the agent.\n### Statistics\nThe statistics is a dictionary of all the test results, including **both successful and failed trajectories**.\nThe format is:\n{\n\"success_rate\" : success_rate,\n}\n## Requirements\nPlease write proper analysis of possible reasons of failure and inefficiency, and your suggestion on the game strategy. You should\nnot not be limited to the task description above, but also come up with other failure cases based on the test results.\n**The analysis and suggestion should be concise.**\n## Test results\n### Failed trajectories\n{trajectories}\n### Statistics\n{statistics}\nWe also try to replace game-specific feedback information (i.e. trajectory information and statistics) with captions from\nmulti-modal large language model (e.g. GPT-4V) to enhance the generality of the agent. First, we input the video sequence\ninto GPT-4V and let it describe the sequence in as much detail as possible. The prompt is as follows:\nThis is a Minecraft game video clip. Please describe the scene and the player’s actions in detail. Note that describe the entire\nsequence of images rather than describing each image individually.\nThen, the output of GPT-4V will be further combined with the prompt of the above trajectory analysis for another question\nand answer, allowing LLM to analyze the possible reasons for the failure. See Section I for detailed examples.\nG. Examples of LLM-Designed Reward Functions\nWe provide reward function examples for four tasks. For the attacking cow task, we add two extra input parameters dam-\nage_dealt and kill_entity, which records the amount of damage the agent caused and the amounts of each entity the agent\nkilled, respectively.\nAlgorithm I Reward Function Example of Exploring Diamond Ore\ndef reward_function(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions\n, GLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from\nthe diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid\ndangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large\nnegative reward when the agent dies.\nIn addition, we should implement a threat detection mechanism to avoid unnecessary deaths.\n\"\"\"\nimport numpy as np\ndef dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent is close to lava, give a negative reward\nif \"lava\" in current_nearest_blocks:\nreward -= 1\n# If the agent is not in pitch range 0 to 45, give a negative reward\ncur_pitch = past_agent_positions[-1][-1]\nif cur_pitch < 0 or cur_pitch > 45:\nreward -= 2\n# If the agent’s health decreases, give a negative reward\nif \"health\" in GLOBAL_DATA:\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 2:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health,\npast_agent_positions, GLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health,\npast_agent_positions, GLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\nAlgorithm II Reward Function Example of Approaching a Tree\non Plains\n.\ndef reward_function(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions\n, GLOBAL_DATA):\n# Thoughts:\n# Objective: The agent should approach the nearest wood block in the plains biome.\n# We want to encourage the agent to:\n# - Make progress towards wood\n# - Keep health high\n# - Avoid circling around or staying still\n# We use a dense reward to encourage the above behaviors, and a sparse reward to give a big bonus when the\nagent achieves the objective.\nimport numpy as np\nimport math\n# Helper function to calculate the Euclidean distance between two points\ndef calculate_distance(point1, point2):\nreturn math.sqrt(sum([(a - b) ** 2 for a, b in zip(point1, point2)]))\ndef dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA):\nreward = 0\n# Encourage the agent to approach wood\nif \"wood\" in current_nearest_blocks:\ncurrent_distance = current_nearest_blocks[\"wood\"][0]\nif \"wood\" in previous_nearest_blocks:\nprevious_distance = previous_nearest_blocks[\"wood\"][0]\n# Encourage the agent to decrease the distance to the nearest wood\nif current_distance < previous_distance:\nreward += 1\n# Discourage the agent to increase the distance to the nearest wood\nelse:\nreward -= 1\n# Encourage the agent to find wood\nelse:\nreward += 1\n# Encourage the agent to keep health high\nif health <= 3:\nreward -= 1\n# Discourage the agent from circling around or staying still\nif len(past_agent_positions) > 1:\ncurrent_position = past_agent_positions[-1][:3]\nprevious_position = past_agent_positions[-2][:3]\n# Encourage the agent to move\nif calculate_distance(current_position, previous_position) < 0.1:\nreward -= 1\nreturn reward\ndef sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA):\nreward = 0\n# Give a big bonus when the agent achieves the objective\nif \"wood\" in current_nearest_blocks and current_nearest_blocks[\"wood\"][0] < 1:\nreward += 10\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health,\npast_agent_positions, GLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health,\npast_agent_positions, GLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\nAlgorithm III Reward Function Example of Approaching a Cow\non Plains\ndef reward_function(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions\n, GLOBAL_DATA):\n\"\"\"\nThoughts:\n- The agent needs to find a cow and approach it. So, the primary factor affecting the reward is the distance to\nthe nearest cow.\n- The agent should avoid getting hurt (losing health).\n- We do not need to consider inventory change in this task.\n- We need to encourage the agent to keep exploring, so we also consider the agent’s movement.\n\"\"\"\nimport numpy as np\n# Helper function to calculate the Euclidean distance between two positions\ndef calculate_distance(pos1, pos2):\nreturn np.sqrt((pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2 + (pos1[2] - pos2[2]) ** 2)\ndef dense(current_nearest_blocks, previous_nearest_blocks, health, past_agent_positions):\n# Reward for approaching the cow\ncow_reward = 0\nif \"cow\" in current_nearest_blocks:\nif \"cow\" in previous_nearest_blocks:\n# If the agent is getting closer to the cow, give a positive reward\nif current_nearest_blocks[\"cow\"][0] < previous_nearest_blocks[\"cow\"][0]:\ncow_reward = 1\n# If the agent is getting farther from the cow, give a negative reward\nelif current_nearest_blocks[\"cow\"][0] > previous_nearest_blocks[\"cow\"][0]:\ncow_reward = -1\n# Reward for moving (exploring)\nmove_reward = 0\nif len(past_agent_positions) > 1:\n# If the agent is moving, give a positive reward\nif calculate_distance(past_agent_positions[-1], past_agent_positions[-2]) > 0:\nmove_reward = 1\n# If the agent is not moving, give a negative reward\nelse:\nmove_reward = -1\nreturn cow_reward + move_reward\ndef sparse(current_nearest_blocks, health):\n# If the agent finds a cow and approaches it successfully, give a large positive reward\nif \"cow\" in current_nearest_blocks and current_nearest_blocks[\"cow\"][0] < 2:\nreturn 100\n# If the agent gets hurt, give a large negative reward\nelif health < 10:\nreturn -100\nelse:\nreturn 0\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, health, past_agent_positions)\nsparse_reward = sparse(current_nearest_blocks, health)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\nAlgorithm IV Reward Function Example of Attacking a Cow\non Plains\ndef reward_function(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions\n, GLOBAL_DATA, damage_dealt, kill_entity):\n\"\"\"\nThoughts:\n- The agent needs to find a cow and approach and kill it. We can give a positive dense reward for approaching\nor attacking a cow.\n- We can give a sparse reward for killing a cow.\n\"\"\"\nimport numpy as np\ndef dense(current_nearest_blocks, previous_nearest_blocks, damage_dealt):\n# Reward for approaching the cow\napproach_reward = 0\nif \"cow\" in current_nearest_blocks and \"cow\" in previous_nearest_blocks:\nif current_nearest_blocks[\"cow\"][0] < previous_nearest_blocks[\"cow\"][0]:\napproach_reward = 1\nelif current_nearest_blocks[\"cow\"][0] > previous_nearest_blocks[\"cow\"][0]:\napproach_reward = -1\n# Reward for attacking the cow\nif damage_dealt > 0:\nattack_reward = 5\nelse:\nattack_reward = 0\nreturn approach_reward + attack_reward\ndef sparse(kill_entity):\n# Reward for killing the cow\nif kill_entity[\"cow\"] > 0:\nreturn 100\nelse:\nreturn 0\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, damage_dealt)\nsparse_reward = sparse(kill_entity)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\nH. Example of Full Interactions with LLM\nWe show an example of interactions with the LLM for the diamond exploration task, including initializing reward function\nand one iteration of updating reward function. INPUT is the input prompt and OUTPUT is the response from GPT-4. We\nhighlight some lines that are related to critic revising and reward updating in yellow.\nDESIGNER INPUT:\nYou are now a proficient Minecraft player. You should help me write proper reward functions to train a Minecraft agent with\nreinforcement learning to complete the described task.\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meter.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Output Requirements\n- The reward function should be written in Python 3.9.\n- Output the code block only. **Do not output anything else outside the code block**.\n- You should include **sufficient comments** in your reward function to explain your thoughts, the objective and **implementa-\ntion details**. The implementation can be specified to a specific line of code.\n- If you need to import packages (e.g. math, numpy) or define helper functions, define them at the beginning of the function. Do\nnot use unimported packages and undefined functions.\n## Output format\nStrictly follow the following format. **Do not output anything else outside the code block**.\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\nNow write a reward function. Then in each iteration, I will use the reward function to train an RL agent, and test it in the\nenvironment. I will give you possible reasons of the failure found during the testing, and you should modify the reward function\naccordingly.\n——————————————————————————————————————————————–\nDESIGNER OUTPUT:\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from the diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid dangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large negative reward\nwhen the agent dies.\n\"\"\"\nimport numpy as np\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent’s health decreases, give a negative reward\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 1:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(dense_reward) * 1 + np.sign(sparse_reward) * 0.1\n——————————————————————————————————————————————–\nCRITIC INPUT:\nYou are now a proficient Minecraft player. I will give you a reward function written in Python, which is used to train an RL agent\nto play Minecraft.\nYou should help me to evaluate whether the reward function has met the requirements.\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meter.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Requirements\n1. You should check whether the reward function meets the **reward function requirements** above.\n2. Your judgement should consider whether the comments and the detailed implementation code are consistent. You can judge\nwhether a line is correct by its comment, but do only rely on the comments.\n3. You should also check the format of the reward function. It should be like:\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\n4. Consider the **Information about the game state** above.\n5. **You don’t need to check whether the reward function meets the task description**.\n6. You should first provide a reasoning and decision. If the reward function fails, provide a critique to help to improve the reward\nfunction.\n## Output format\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"reasoning\",\n\"success\": boolean,\n\"critique\": critique,\n}\nEnsure the response can be parsed by Python `json.loads`, e.g. no trailing commas, no single quotes, etc. Do not output anything\nelse like explanations.\n## Reward function to be evaluated\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from the diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid dangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large negative reward\nwhen the agent dies.\n\"\"\"\nimport numpy as np\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent’s health decreases, give a negative reward\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 1:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(dense_reward) * 1 + np.sign(sparse_reward) * 0.1\n——————————————————————————————————————————————–\nCRITIC OUTPUT:\n{\n\"reasoning\": \"The reward function does not consider that `health`is not initialized in GLOBAL_DATA. It will raise a error\nbecause the key `health`is not found.\",\n\"success\": false,\n\"critique\": \"The reward function needs to be revised to initialize `health`in GLOBAL_DATA.\"\n}\n——————————————————————————————————————————————–\nDESIGNER INPUT:\nThe reward function fails to pass the verification. The reasoning is:\n```\nThe reward function does not consider that `health`is not initialized in GLOBAL_DATA. It will raise a error because the key\n`health`is not found.\n```\nThe critique is:\n```\nThe reward function needs to be revised to initialize `health`in GLOBAL_DATA.\n```\n## Requirements\nPlease consider the reasoning and critique, and modify the reward function.\n1. If necessary, you can write a totally different reward function than the current one.\n2. In the first part of the reward function, you should provide your thoughts of modifying the reward function. The thoughts should\nnot directly copy the given reasoning or critique. **The thoughts should be concise.**\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Output format\nStrictly follow the following format. **Do not output anything else outside the code block**.\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\n——————————————————————————————————————————————–\nDESIGNER OUTPUT:\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from the diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid dangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large negative reward\nwhen the agent dies.\n\"\"\"\nimport numpy as np\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent’s health decreases, give a negative reward\nif \"health\" in GLOBAL_DATA:\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 1:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(dense_reward) * 1 + np.sign(sparse_reward) * 0.1\n——————————————————————————————————————————————–\nCRITIC INPUT:\nYou are now a proficient Minecraft player. I will give you a reward function written in Python, which is used to train an RL agent\nto play Minecraft.\nYou should help me to evaluate whether the reward function has met the requirements.\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meter.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Requirements\n1. You should check whether the reward function meets the **reward function requirements** above.\n2. Your judgement should consider whether the comments and the detailed implementation code are consistent. You can judge\nwhether a line is correct by its comment, but do only rely on the comments.\n3. You should also check the format of the reward function. It should be like:\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\n4. Consider the **Information about the game state** above.\n5. **You don’t need to check whether the reward function meets the task description**.\n6. You should first provide a reasoning and decision. If the reward function fails, provide a critique to help to improve the reward\nfunction.\n## Output format\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"reasoning\",\n\"success\": boolean,\n\"critique\": critique,\n}\nEnsure the response can be parsed by Python `json.loads`, e.g. no trailing commas, no single quotes, etc. Do not output anything\nelse like explanations.\n## Reward function to be evaluated\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from the diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid dangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large negative reward\nwhen the agent dies.\n\"\"\"\nimport numpy as np\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent’s health decreases, give a negative reward\nif \"health\" in GLOBAL_DATA:\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 1:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(dense_reward) * 1 + np.sign(sparse_reward) * 0.1\n——————————————————————————————————————————————–\nCRITIC OUTPUT:\n{\n\"reasoning\": \"The reward function meets the requirements. It considers sparse reward of completing the objective and death,\nand dense reward of getting closer to diamond and health decrease.\",\n\"success\": true,\n\"critique\": null\n}\n——————————————————————————————————————————————–\n(TRAIN AN AGENT WITH THE REWARD FUNCTION)\n——————————————————————————————————————————————–\nANALYZER INPUT:\nYou are now a proficient Minecraft player. I have trained an RL agent and tested for several times in the Minecraft game environ-\nment.\nI will give you the information of the failed test results, i.e. trajectories of actions, rewards and observations. You should help me\nwrite proper analysis of possible reasons of failure and inefficiency, and your suggestion on the game strategy.\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meter.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Input format\n### Failed trajectories\nThe failed trajectories are shown as a list, where each entry is a dictionary representing a trajectory. Note that **only the failed\ncases are provided.**\nThe each item of \"history\" is a list representing statistics of each single step. When the history is too long, it will be truncated to\nthe last 32 steps, indicated by the value of \"truncated\".\n\"dead\" indicates whether the agent has died.\nThe format is:\n[\n{\n\"history\": {\n\"rewards\": [reward1, ...],\n\"actions\": [action1, action2, ...],\n\"locations\": [[x1, y1, z1, pitch1, yaw1], ...],\n\"inventory_change\": {item1: delta_num1, ...},\n\"truncated\": True or False,\n},\n\"final_health\": health,\n\"final_inventory\": {item1: num1, ...},\n\"final_nearest_blocks\": {block_name1: nearest_distance1, block_name2: nearest_distance2, ...},\n\"block_under_foot\": block_name,\n\"dead\": True or False,\n},\n...\n]\nwhere \"final_health\" is in range 0 and 10. \"block_under_foot\" is the block right under the agent.\n### Statistics\nThe statistics is a dictionary of all the test results, including **both successful and failed trajectories**.\nThe format is:\n{\n\"success_rate\" : success_rate,\n}\n## Requirements\nPlease write proper analysis of possible reasons of failure and inefficiency, and your suggestion on the game strategy. You should\nnot not be limited to the task description above, but also come up with other failure cases based on the test results.\n**The analysis and suggestion should be concise.**\n## Test results\n### Failed trajectories\n[{’block_under_foot’: ’lava’,\n’dead’: True,\n’final_health’: 0.5,\n’final_inventory’: {’coal’: 3,\n’cobblestone’: 84,\n’crafting_table’: 1,\n’furnace’: 1,\n’gravel’: 3,\n’iron_pickaxe’: 1,\n’planks’: 2,\n’stone’: 17,\n’stone_pickaxe’: 10,\n’wooden_pickaxe’: 1},\n’final_nearest_blocks’: {’lava’: [0.01, -0.87, -0.63]},\n’history’: {’actions’: [’noop’, ’attack’, ’noop’, ’attack’, ’noop’, ’attack’,\n’attack’, ’turn_down’, ’attack’, ’attack’, ’noop’,\n’attack’, ’forward’, ’forward’, ’noop’, ’noop’,\n’forward_jump’, ’forward_jump’, ’attack’, ’attack’,\n’attack’, ’noop’, ’jump’, ’noop’, ’noop’, ’jump’,\n’attack’, ’turn_down’, ’turn_up’, ’forward_jump’,\n’turn_down’, ’turn_up’],\n’inventory_change’: [{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {},\n{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {},\n{}, {}, {}, {}, {}, {}, {}, {}, {}, {}],\n’locations’: [[-205.42, 9.27, 474.4, 15.0, -90.0],\n[-205.41, 9.24, 474.4, 15.0, -90.0],\n[-205.41, 9.2, 474.4, 15.0, -90.0],\n[-205.41, 9.16, 474.4, 15.0, -90.0],\n[-205.41, 9.12, 474.4, 15.0, -90.0],\n[-205.41, 9.08, 474.4, 15.0, -90.0],\n[-205.41, 9.04, 474.4, 15.0, -90.0],\n[-205.41, 9.0, 474.4, 15.0, -90.0],\n[-205.41, 8.96, 474.4, 30.0, -90.0],\n[-205.41, 8.92, 474.4, 30.0, -90.0],\n[-205.41, 8.88, 474.4, 30.0, -90.0],\n[-205.41, 8.84, 474.4, 30.0, -90.0],\n[-205.41, 8.8, 474.4, 30.0, -90.0],\n[-205.39, 8.76, 474.4, 30.0, -90.0],\n[-205.36, 8.72, 474.4, 30.0, -90.0],\n[-205.34, 8.68, 474.4, 30.0, -90.0],\n[-205.34, 8.64, 474.4, 30.0, -90.0],\n[-205.31, 8.64, 474.4, 30.0, -90.0],\n[-205.3, 8.66, 474.4, 30.0, -90.0],\n[-205.3, 8.65, 474.4, 30.0, -90.0],\n[-205.3, 8.63, 474.4, 30.0, -90.0],\n[-205.3, 8.6, 474.4, 30.0, -90.0],\n[-205.3, 8.56, 474.4, 30.0, -90.0],\n[-205.3, 8.56, 474.4, 30.0, -90.0],\n[-205.3, 8.54, 474.4, 30.0, -90.0],\n[-205.3, 8.51, 474.4, 30.0, -90.0],\n[-205.3, 8.52, 474.4, 30.0, -90.0],\n[-205.3, 8.5, 474.4, 30.0, -90.0],\n[-205.3, 8.47, 474.4, 45.0, -90.0],\n[-205.3, 8.43, 474.4, 30.0, -90.0],\n[-205.3, 8.44, 474.4, 30.0, -90.0],\n[-205.3, 8.42, 474.4, 45.0, -90.0]],\n’nearest_blocks’: [{’lava’: [0.25, 0.87, -0.63]},\n{’lava’: [0.31, 0.87, -0.63]},\n{’lava’: [0.38, 0.87, -0.63]},\n{’lava’: [0.41, 0.07, -0.03]},\n{’lava’: [0.41, 0.07, -0.03]},\n{’lava’: [0.41, 0.07, -0.03]},\n{’lava’: [0.41, 0.07, -0.03]},\n{’lava’: [0.41, 0.35, -0.1]},\n{’lava’: [0.41, 0.35, -0.1]},\n{’lava’: [0.41, 0.35, -0.1]},\n{’lava’: [0.41, 0.35, -0.1]},\n{’lava’: [0.41, 0.35, -0.1]},\n{’lava’: [0.38, -1.57, 0.0]},\n{’lava’: [0.34, -1.57, 0.0]},\n{’lava’: [0.3, -1.57, 0.0]},\n{’lava’: [0.26, -1.57, 0.0]},\n{’lava’: [0.26, -1.57, 0.0]},\n{’lava’: [0.28, -1.57, 0.0]},\n{’lava’: [0.27, -1.57, 0.0]},\n{’lava’: [0.25, -0.87, -0.63]},\n{’lava’: [0.22, -0.87, -0.63]},\n{’lava’: [0.18, -0.87, -0.63]},\n{’lava’: [0.18, -0.87, -0.63]},\n{’lava’: [0.16, -0.87, -0.63]},\n{’lava’: [0.13, -0.87, -0.63]},\n{’lava’: [0.14, -0.87, -0.63]},\n{’lava’: [0.12, -0.87, -0.63]},\n{’lava’: [0.09, -0.87, -0.63]},\n{’lava’: [0.05, -1.57, 0.0]},\n{’lava’: [0.06, -0.87, -0.63]},\n{’lava’: [0.04, -0.87, -0.63]},\n{’lava’: [0.01, -0.87, -0.63]}],\n’rewards’: [],\n’truncated’: True}},\n{’block_under_foot’: ’stone’,\n’dead’: False,\n’final_health’: 10.0,\n’final_inventory’: {’coal’: 5,\n’cobblestone’: 133,\n’crafting_table’: 1,\n’dirt’: 18,\n’furnace’: 1,\n’iron_pickaxe’: 1,\n’planks’: 2,\n’stone’: 37,\n’stone_pickaxe’: 9,\n’wooden_pickaxe’: 1},\n’final_nearest_blocks’: {’stone’: [0.5, 0.87, -0.63]},\n’history’: {’actions’: [’attack’, ’attack’, ’attack’, ’attack’, ’attack’,\n’attack’, ’attack’, ’attack’, ’attack’, ’attack’,\n’attack’, ’attack’, ’attack’, ’attack’, ’attack’,\n’noop’, ’turn_up’, ’turn_up’, ’turn_up’, ’noop’,\n’forward’, ’forward’, ’forward’, ’forward’, ’forward’,\n’forward’, ’back’, ’noop’, ’noop’, ’attack’, ’attack’,\n’attack’],\n’inventory_change’: [{}, {}, {}, {’cobblestone’: 1}, {}, {}, {},\n{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {},\n{}, {}, {}, {}, {}, {}, {’cobblestone’: 1},\n{}, {}, {}, {}, {}, {}, {}],\n’locations’: [[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 45.0, -180.0],\n[611.54, 11.0, 212.67, 30.0, -180.0],\n[611.54, 11.0, 212.67, 15.0, -180.0],\n[611.54, 11.0, 212.67, 0.0, -180.0],\n[611.54, 11.0, 212.67, 0.0, -180.0],\n[611.54, 11.0, 212.57, 0.0, -180.0],\n[611.54, 11.0, 212.42, 0.0, -180.0],\n[611.54, 11.0, 212.24, 0.0, -180.0],\n[611.54, 11.0, 212.04, 0.0, -180.0],\n[611.54, 11.0, 211.84, 0.0, -180.0],\n[611.54, 11.0, 211.63, 0.0, -180.0],\n[611.54, 11.0, 211.61, 0.0, -180.0],\n[611.54, 11.0, 211.6, 0.0, -180.0],\n[611.54, 11.0, 211.6, 0.0, -180.0],\n[611.54, 11.0, 211.6, 0.0, -180.0],\n[611.54, 11.0, 211.6, 0.0, -180.0]],\n’nearest_blocks’: [{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.73, -0.63]},\n{’stone’: [0.78, 0.49, -0.63]},\n{’stone’: [0.66, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]},\n{’stone’: [0.5, 0.87, -0.63]}],\n’rewards’: [],\n’truncated’: True}}]\n### Statistics\n{’success_rate’: 0.0}\n——————————————————————————————————————————————–\nANALYZER OUTPUT:\nAnalysis of Possible Reasons of Failure and Inefficiency: In one of the failed trajectories, the agent died due to lava. The agent did\nnot take any action to avoid the lava or move away from it. The agent should be programmed to detect and avoid dangerous blocks,\nsuch as lava, by turning or moving in a different direction.\nSuggestion on Game Strategy: Train the agent to detect and avoid dangerous blocks, such as lava or deep holes. The agent should\nbe programmed to prioritize safety and avoid situations that could lead to death or loss of health.\nBy implementing the suggestion and improving the agent’s training, the success rate of finding diamonds can be increased.\n——————————————————————————————————————————————–\nDESIGNER INPUT:\nYou are now a proficient Minecraft player. I now have a reward function to train a Minecraft agent with reinforcement learning to\ncomplete the described task. The reward function is used to train the reinforcement learning agent for several times. I will provide\nyou the analysis of failure and inefficiency and suggestions. You should help me modify the reward function.\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meter.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Current reward function\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from the diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid dangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large negative reward\nwhen the agent dies.\n\"\"\"\nimport numpy as np\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent’s health decreases, give a negative reward\nif \"health\" in GLOBAL_DATA:\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 1:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(dense_reward) * 1 + np.sign(sparse_reward) * 0.1\n## Reward function test result\nThe reward function is used to train the reinforcement learning agent for several times. Here is some analysis of failure and\ninefficiency and suggestions:\n```\nAnalysis of Possible Reasons of Failure and Inefficiency: In one of the failed trajectories, the agent died due to lava. The agent\ndid not take any action to avoid the lava or move away from it. The agent should be programmed to detect and avoid dangerous\nblocks, such as lava, by turning or moving in a different direction.\nSuggestion on Game Strategy: Train the agent to detect and avoid dangerous blocks, such as lava or deep holes. The agent should\nbe programmed to prioritize safety and avoid situations that could lead to death or loss of health.\nBy implementing the suggestion and improving the agent’s training, the success rate of finding diamonds can be increased.\n```\n## Requirements\nPlease consider the analysis and suggestions above and modify the reward function.\n1. You can both modify the current lines and add new lines.\n2. If necessary, you can write a **totally different** reward function than the current one.\n3. Consider modifing the reward and penalty values in the current reward function to balance them.\n4. In the first part of the reward function, you should provide your thoughts of modifying the reward function. **The thoughts\nshould be concise.**\n## Output format\nStrictly follow the following format. **Do not output anything else outside the code block**.\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\n——————————————————————————————————————————————–\nDESIGNER OUTPUT:\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from the diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid dangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large negative reward\nwhen the agent dies.\nIn addition, we should implement a threat detection mechanism to avoid unnecessary deaths.\n\"\"\"\nimport numpy as np\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent is close to lava, give a negative reward\nif \"lava\" in current_nearest_blocks:\nreward -= 1\n# If the agent’s health decreases, give a negative reward\nif \"health\" in GLOBAL_DATA:\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 1:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(dense_reward) * 1 + np.sign(sparse_reward) * 0.1\n——————————————————————————————————————————————–\nCRITIC INPUT:\nYou are now a proficient Minecraft player. I will give you a reward function written in Python, which is used to train an RL agent\nto play Minecraft.\nYou should help me to evaluate whether the reward function has met the requirements.\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meter.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\n## Input parameters of the reward function\n- current_nearest_blocks: nearby block types and nearest distance of each block type at the **current step**. It is a dictionary,\nconsisting of the names, relative distances with the agent, and relative angles (yaw and pitch) with the agent, in the form of:\n{NAME_1: (DIST_1, YAW_1, PITCH_1), NAME_2: (DIST_2, YAW_2, PITCH_2)}. For example, {\"wood\": (24.7, 1.48, -1.57),\n\"cobblestone\": (1.3, -0.17, 1.05), \"iron_ore\": (4.5, 0.61, -0.17)}. If a certrain block type does not exist in the field of view, it is not\npresent in the dictionary. The yaw and pitch here is relative to the agent’s front, i.e., pitch = 0 and yaw = 0 is the front of the agent.\n- previou_nearest_blocks: nearby block types and nearest distance of each block type at the **previous step**, with the same\nformat as the current_nearest_blocks.\n- inventory_change: the change of the agent’s inventory from the previous step to current step, in the form of a dictionary:\n{NAME_1: CHANGE_NUM_1, NAME_2: CHANGE_NUM_2}. Positive values mean increase and negative values mean de-\ncrease. For example, {\"wood\": 2, \"dirt\": 3, \"stone_pickaxe\": -1}.\n- health: an integer value in range 0 to 10 indicating the health level of the agent. 0 means death and 10 means full health.\n- past_agent_positions: the history of location of agent, in the form of a list: [[x1, y1, z1, yaw1, pitch1], [x2, y2, z2, yaw2, pitch2],\n...]. The yaw and pitch here are relative to the agent’s initial forward direction, i.e., pitch = 0 and yaw = 0 is the front of the agent\nwhen it was born. The length of the list is the number of steps the agent has taken. The last element of the list is the current location\nof the agent.\n- GLOBAL_DATA: a global variable. It is initialized as a dictionary. You can save necessary information between different steps\nwith it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Reward function requirements\n- You should write a dense reward function `dense`and a sparse reward function `sparse`. The sparse reward indicates achieving\nthe goal or receiving heavy punishment. The dense reward provides intermediate signal to guide the agent in the process of\nachieving the goal. The magnitude of the return value does not matter, but the sign (positive or negative) is important. The final\nreward will be `np.sign(sparse(...)) * 1 + np.sign(dense(...)) * 0.1`.\n## Requirements\n1. You should check whether the reward function meets the **reward function requirements** above.\n2. Your judgement should consider whether the comments and the detailed implementation code are consistent. You can judge\nwhether a line is correct by its comment, but do only rely on the comments.\n3. You should also check the format of the reward function. It should be like:\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Thoughts:\n# ...\n# (import packages and define helper functions)\nimport numpy as np\n...\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n...\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(sparse_reward) * 1 + np.sign(dense_reward) * 0.1\n...\n4. Consider the **Information about the game state** above.\n5. **You don’t need to check whether the reward function meets the task description**.\n6. You should first provide a reasoning and decision. If the reward function fails, provide a critique to help to improve the reward\nfunction.\n## Output format\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"reasoning\",\n\"success\": boolean,\n\"critique\": critique,\n}\nEnsure the response can be parsed by Python `json.loads`, e.g. no trailing commas, no single quotes, etc. Do not output anything\nelse like explanations.\n## Reward function to be evaluated\ndef\nreward_function(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n\"\"\"\nThoughts:\nIn order to make the agent approach the diamond, we need to give positive rewards for actions that\nmake the agent get closer to the diamond and negative rewards for actions that make the agent get further from the diamond.\nWe also need to give a negative reward if the agent’s health decreases to encourage the agent to avoid dangerous situations.\nThe dense reward function will be based on the distance to the diamond and health.\nThe sparse reward function will give a large positive reward when the agent reaches the diamond and a large negative reward\nwhen the agent dies.\nIn addition, we should implement a threat detection mechanism to avoid unnecessary deaths.\n\"\"\"\nimport numpy as np\ndef\ndense(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent is closer to diamond than before, give a positive reward\nif \"diamond\" in current_nearest_blocks and \"diamond\" in previous_nearest_blocks:\nif current_nearest_blocks[\"diamond\"][0] < previous_nearest_blocks[\"diamond\"][0]:\nreward += 1\nelif current_nearest_blocks[\"diamond\"][0] > previous_nearest_blocks[\"diamond\"][0]:\nreward -= 1\n# If the agent is close to lava, give a negative reward\nif \"lava\" in current_nearest_blocks:\nreward -= 1\n# If the agent’s health decreases, give a negative reward\nif \"health\" in GLOBAL_DATA:\nif health < GLOBAL_DATA[\"health\"]:\nreward -= 1\nGLOBAL_DATA[\"health\"] = health\nreturn reward\ndef\nsparse(current_nearest_blocks,\nprevious_nearest_blocks,\ninventory_change,\nhealth,\npast_agent_positions,\nGLOBAL_DATA):\n# Initialize reward\nreward = 0\n# If the agent reaches the diamond, give a large positive reward\nif \"diamond\" in current_nearest_blocks and current_nearest_blocks[\"diamond\"][0] < 1:\nreward += 100\n# If the agent dies, give a large negative reward\nif health == 0:\nreward -= 100\nreturn reward\ndense_reward = dense(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nsparse_reward = sparse(current_nearest_blocks, previous_nearest_blocks, inventory_change, health, past_agent_positions,\nGLOBAL_DATA)\nreturn np.sign(dense_reward) * 1 + np.sign(sparse_reward) * 0.1\n——————————————————————————————————————————————–\nCRITIC OUTPUT:\n{\n\"reasoning\": \"The reward function meets the requirements. It considers sparse reward of completing the objective and death,\nand dense reward of getting closer to diamond, having lava nearby and health decrease.\",\n\"success\": true,\n\"critique\": null\n}\nI. Example of GPT-4V as a Captioner\nWe show an example of using GPT-4V as a captioner to assist LLM in trajectory analysis of Figure III.\nFigure III. The last few frames of trajectory images of failed diamond mining.\nGPT-4V INPUT:\nThis is a Minecraft game video clip. Please describe the scene and the player’s actions in detail. Note that describe the entire\nsequence of images rather than describing each image individually.\n——————————————————————————————————————————————–\nGPT-4V OUTPUT:\nThe sequence of images depicts a typical scenario in the game Minecraft where a player is mining underground. Initially, the player\nis in a dimly lit area, facing a wall of stone blocks, indicating they are inside a tunnel or a cave. The player’s health bar shows a\nslight reduction in health, and the inventory is equipped with essential items for mining, such as pickaxes and cobblestone blocks.\nAs the player begins to mine through the stone with the pickaxe, they make progress with each swing, as indicated by the developing\ncracks in the stone block. This action is part of the core gameplay in Minecraft, where players gather resources by mining various\ntypes of blocks.\nSuddenly, the player breaks through to an unexpected hazard — a flow of bright orange lava appears behind the mined block. Lava\nis dangerous in Minecraft; it can cause damage to the player and destroy items. The player quickly steps back to avoid getting\nburned.\nIn the next moments, the player cautiously assesses the situation by looking at the lava from different angles, probably searching\nfor a safe way to navigate around or contain it. The subsequent images indicate that the player is trying to deal with the lava flow,\nwhich is a common and risky part of mining deep underground in Minecraft.\nThroughout the sequence, the player remains close to the lava, suggesting they may be attempting to block it with the cobblestone\nfrom their inventory or create a barrier to prevent the lava from spreading. This series of images captures the unpredictability and\nthe survival aspects of Minecraft gameplay, where players must quickly adapt to the challenges they encounter while exploring and\ngathering resources.\n——————————————————————————————————————————————–\nANALYZER INPUT:\nYou are now a proficient Minecraft player. I have trained an RL agent and tested for several times in the Minecraft game environ-\nment. I will give you a description of the player’s scene and behavior at the end of the failure in text form.\n## Task description\n- Objective: Find and approach diamond with higher success rate and avoid death.\n- Initial Status:\n1. The agent is under ground at y level 11, the most common level to find diamonds.\n2. The agent already has an iron pickaxe.\n- Success criteria: The distance to the nearest diamond block is less than 2 meter.\n- Procedure: Explore horizontally to find a diamond, face it and approach it.\n## General game information\n- The version of Minecraft is 1.11.\n- Common block names : dirt, cobblestone, iron, diamond, wood, coal, water, air, lava, leaves, ... Collected item names are the\nsame.\n- FOV is set to -35 to 35 degrees for yaw and -30 to 30 degrees for pitch. The max visible distance of blocks is 64.\n- Length of each block is 1 meter.\n- The y coordinate of the agent is the agent’s height. The larger the y, the higher the agent. The ground level is around y = 63, but\nis not fixed.\n- At each step, the agent selects one operation in is action space. The action space includes doing nothing (i.e. staying still),\nmoving forward and backward, jumping and attacking. The action space also includes turning left, right, up and down.\n- The attack range is 2 meters in front of the agent. The agent can move 0.2 meters in each step. The agent can turn 15 degrees in\neach step. Typically, the agent needs to stay still and attack for **60 successive steps** to break a block.\n- The hunger value is always at the max level.\n## Description of the player’s scene and behavior at the end of the failure\n- The sequence of images depicts a typical scenario in the game Minecraft where a player is mining underground. Initially, the\nplayer is in a dimly lit area, facing a wall of stone blocks, indicating they are inside a tunnel or a cave. The player’s health The bar\nshows a slight reduction in health, and the inventory is equipped with essential items for mining, such as pickaxes and cobblestone\nblocks.\n- As the player begins to mine through the stone with the pickaxe, they make progress with each swing, as indicated by the\ndeveloping cracks in the stone block. This action is part of the core gameplay in Minecraft, where players gather resources by\nmining various types of blocks.\n- Suddenly, the player breaks through to an unexpected hazard — a flow of bright orange lava appears behind the mined block.\nLava is dangerous in Minecraft; it can cause damage to the player and destroy items. The player quickly steps back to avoid getting\nburned. - In the next moments, the player cautiously assesses the situation by looking at the lava from different angles, probably\nsearching for a safe way to navigate around or contain it. The subsequent images indicate that the player is trying to deal with the\nlava flow, which is a common and risky part of mining deep underground in Minecraft.\n- Throughout the sequence, the player remains close to the lava, suggesting they may be attempting to block it with the cobblestone\nfrom their inventory or create a barrier to prevent the lava from spreading. This series of images captures the unpredictability and\nthe survival aspects of Minecraft gameplay, where players must quickly adapt to the challenges they encounter while exploring and\ngathering resources.\nSuppose the player dies after going through the above scenarios and actions. You should help me write the most likely reason of\nfailure, and the most feasible suggestion on the game strategy based on the above reason.\n——————————————————————————————————————————————–\nANALYZER OUTPUT:\nBased on the description of the player’s scene and behavior at the end of the failure, the most likely reason for failure appears\nto be the encounter with lava while mining. Lava is a significant hazard in Minecraft, especially when mining at deeper levels\nwhere diamonds are commonly found. The sudden appearance of lava behind a mined block can catch players off guard, leading\nto damage from burns or even death if they are unable to escape or protect themselves in time. The player’s attempt to manage\nthe lava flow suggests awareness of the danger but may have been insufficiently cautious or effective in containing or avoiding the\nlava.\n### Most Likely Reason of Failure:\nThe failure was likely due to the player’s inability to safely navigate the sudden lava flow encountered during mining. Despite\nattempts to assess and manage the situation, the proximity to the lava and possibly inadequate measures to block or divert it might\nhave resulted in sustained damage leading to death.\n### Suggestion on Game Strategy:\n1. **Advance Preparedness**: Always carry water buckets in the inventory. A water bucket can be crucial for quickly converting\nlava into obsidian or stone, neutralizing the immediate threat. This quick response can save the player from burning and provides\na safer path to continue mining.\n2. **Strategic Mining Approach**: When mining at levels where lava is common, adopt a cautious approach:\n- **Mine at a Safe Distance**: Instead of directly mining the block in front, mine while standing back or to the side to avoid\nsudden lava flows.\n- **Use Blocks to Block Lava**: Have blocks ready in the hotbar to quickly place in front of a lava source. Cobblestone or dirt\nblocks can be used to block the flow or create barriers.\n- **Listen for Lava Sounds**: Pay attention to audio cues. The sound of bubbling lava can alert the player to its presence behind\nor adjacent to blocks being mined.\n3. **Environmental Awareness**: Before breaking a block, check the surroundings for signs of lava, such as light emanating from\ncracks or the sound of flowing lava. This can preemptively alert the player to potential dangers.\n4. **Escape Plan**: Always have an escape route planned when mining deep underground. This means knowing the way back to\nsafer areas or having a clear path to retreat if faced with hazards like lava.\n5. **Practice Lava Management**: Familiarize with techniques to manage lava flows, such as placing blocks to divert the flow\naway from the mining area or using water to solidify lava. Practicing these techniques in safer conditions can prepare the player\nfor real encounters.\nBy following these strategies, the player can significantly reduce the risk of death from lava while mining for diamonds and improve\ntheir overall success rate in finding valuable resources.\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft.pdf"}
{"title":"Odyssey: Empowering Minecraft Agents with Open-World Skills","authors":"Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, Mingli Song","summary":"Recent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set\nof actions available to agents, requiring them to learn effective long-horizon\nstrategies from scratch. Consequently, discovering diverse gameplay\nopportunities in the open world becomes challenging. In this work, we introduce\nOdyssey, a new framework that empowers Large Language Model (LLM)-based agents\nwith open-world skills to explore the vast Minecraft world. Odyssey comprises\nthree key parts: (1) An interactive agent with an open-world skill library that\nconsists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned\nLLaMA-3 model trained on a large question-answering dataset with 390k+\ninstruction entries derived from the Minecraft Wiki. (3) A new agent capability\nbenchmark includes the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate\nthat the proposed Odyssey framework can effectively evaluate different\ncapabilities of LLM-based agents. All datasets, model weights, and code are\npublicly available to motivate future research on more advanced autonomous\nagent solutions.","url":"http:\/\/arxiv.org\/abs\/2407.15325v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2407.15325v2","published":1721614019000,"comment":null,"pdf_text":"Preprint\nODYSSEY: EMPOWERING MINECRAFT AGENTS WITH\nOPEN-WORLD SKILLS\nShunyu Liu1∗†, Yaoru Li1∗, Kongcheng Zhang1∗, Zhenyu Cui1∗, Wenkai Fang1∗,\nYuxuan Zheng1, Tongya Zheng2, Mingli Song1 #\n1Zhejiang University,\n2Hangzhou City University\n{liushunyu, liyaoru, zhangkc, zhenyucui, wenkfang, zyxuan}@zju.edu.cn,\ndoujiang_zheng@163.com, brooksong@zju.edu.cn\n∗Equal contribution,\n†Project leader,\n#Corresponding author\n§ https:\/\/github.com\/zju-vipa\/Odyssey\nABSTRACT\nRecent studies have delved into constructing generalist agents for open-world\nenvironments like Minecraft. Despite the encouraging results, existing efforts\nmainly focus on solving basic programmatic tasks, e.g., material collection and\ntool-crafting following the Minecraft tech-tree, treating the ObtainDiamond\ntask as the ultimate goal. This limitation stems from the narrowly defined set of\nactions available to agents, requiring them to learn effective long-horizon strate-\ngies from scratch. Consequently, discovering diverse gameplay opportunities in\nthe open world becomes challenging. In this work, we introduce ODYSSEY, a\nnew framework that empowers Large Language Model (LLM)-based agents with\nopen-world skills to explore the vast Minecraft world. ODYSSEY comprises three\nkey parts: (1) An interactive agent with an open-world skill library that consists of\n40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3 model\ntrained on a large question-answering dataset with 390k+ instruction entries de-\nrived from the Minecraft Wiki. (3) A new agent capability benchmark includes\nthe long-term planning task, the dynamic-immediate planning task, and the au-\ntonomous exploration task. Extensive experiments demonstrate that the proposed\nODYSSEY framework can effectively evaluate different capabilities of LLM-based\nagents. All datasets, model weights, and code are publicly available to motivate\nfuture research on more advanced autonomous agent solutions.\n1\nINTRODUCTION\nDeveloping autonomous agents capable of performing open-world tasks represents a significant\nmilestone towards achieving artificial general intelligence (Savva et al., 2019; Reed et al., 2022;\nDriess et al., 2023). These open-world tasks necessitate that agents interact with complex and dy-\nnamic environments, make decisions based on incomplete information, and adapt to unexpected\nevents. Early reinforcement learning agents (Tessler et al., 2017; Oh et al., 2017; Guss et al., 2019)\nhave demonstrated limited knowledge in such open-world setting. Furthermore, these agents of-\nten struggle with long-term planning, which is crucial for the fulfillment of intricate goals. Recent\nbreakthrough of Large Language Models (LLMs) (Hu et al., 2021; Achiam et al., 2023; Touvron\net al., 2023) have shown the potential to revolutionize various fields such as healthcare (Zhang et al.,\n2023b; Yang et al., 2024b), robotics (Huang et al., 2022; Ahn et al., 2022; Singh et al., 2023), and\nweb services (Nakano et al., 2021; Deng et al., 2023; Iong et al., 2024), attributed to its capability\non endowing agents with expansive knowledge and sophisticated planning akin to human reason-\ning (Wei et al., 2022a; Wang et al., 2024; Liang et al., 2023). However, the development of LLMs in\nopen-world tasks remains challenging due to the need for well-defined environments and measurable\nbenchmarks (Zhu et al., 2023; Wang et al., 2023a; Qin et al., 2023).\nThe popular Minecraft game features a vast and diverse world with various biomes, terrains, and\nresources, making it an ideal testbed for evaluating the capabilities of autonomous agents in the\nopen-world setting (Guss et al., 2019). To facilitate the development of generalist agents in this\n1\narXiv:2407.15325v2  [cs.AI]  7 Oct 2024\nPreprint\nLong\nShort\nLLaMA 3\nMinecraft\nWiki\nFine-tune Minecraft LLM\nAgent Capability Benchmark\nCombat Zombie\nCraft Diamond Sword\nMine Diamond\nOpen-World Skill Library\nInteractive Agent\nPlan\nCritic\nActor\n...\nMineMA\nQ&A Dataset\nGeneration\nFine-tune\nLoRA\nSkill\nRetrieval\n     Combat\nWeapons\nEquipment\nLong-term Planning\n     Farm\nPlanting\nBreeding\nDynamic-immediate Planning\n     Explore\nSurviving\nCreating\nAutonomous Exploration\n...\nFigure 1: An overview of the proposed ODYSSEY framework. Odyssey consists of three key com-\nponents: (1) a fine-tuned LLaMA-3 model trained on a large-scale question-answering dataset; (2)\nan interactive agent equipped with an extensive open-world skill library; (3) a novel agent capability\nbenchmark encompassing a variety of tasks.\nsetting, MineRL (Guss et al., 2019) and MineDojo (Fan et al., 2022) introduced simulation bench-\nmarks built upon the sandbox Minecraft environment. The seminal work, Voyager (Wang et al.,\n2023a), proposed an LLM-based agent to drive exploration in Minecraft. Subsequently, there has\nbeen a surge of efforts to leverage the superior performance of LLMs to extend the capabilities of\nsuch Minecraft agents (Zhu et al., 2023; Wang et al., 2023b; Zhou et al., 2024a; Wang et al., 2023c;\nQin et al., 2023). Despite recent advancements, existing works mainly focus on solving basic pro-\ngrammatic tasks, often considering the ObtainDiamond task as the ultimate challenge. Basic\nprogrammatic tasks refer to those constrained by the explicit dependencies following the Minecraft\ntech-tree, such as collecting materials and crafting tools. Such tasks inherently only assess the abil-\nity of LLMs to prioritize crafting steps within a limited task space, rather than their potential for\ncomplicated and diverse solutions. This limitation arises from the narrowly defined set of actions\navailable to agents (e.g., mouse and keyboard), which necessitates learning skills from scratch. Since\nMinecraft is fundamentally resource-based, an agent must first learn to collect adequate resources\nand tools to engage in creative play, which limits the exploration of diverse gameplay options. More-\nover, methods like Voyager (Wang et al., 2023a) heavily rely on the powerful GPT-4 for high-quality\nsolutions, imposing a substantial cost burden on researchers who prefer open-source models.\nIn this work, we introduce ODYSSEY1, a novel framework that equips LLM-based agents with ad-\nvanced open-world skills, enabling efficient interaction and exploration within the Minecraft envi-\nronment. ODYSSEY allows agents to move beyond basic programmatic tasks and focus more on\ncomplex open-world challenges. As shown in Fig. 1, ODYSSEY comprises three key contributions:\n1. We develop an LLM-based interactive agent with an open-world skill library, encompassing\n40 primitive skills that serve as underlying interfaces and 183 compositional skills tailored for\ncomplex and diverse tasks in an open-world setting. A recursive method improves skill execution\nby checking prerequisites. The ODYSSEY agent consists of a planner for goal decomposition, an\nactor for skill retrieval and subgoal execution, and a critic for feedback and strategy refinement.\n2. We fine-tune the LLaMA-3 model (Touvron et al., 2023) for Minecraft agents using a compre-\nhensive question-answering dataset. This involves generating a large-scale training dataset with\n390k+ instruction entries from Minecraft Wikis, fine-tuning various sizes of the LLaMA-3 mod-\nels using LoRA (Hu et al., 2021), and evaluating them with a custom multiple-choice dataset.\n3. We introduce a new agent capability benchmark to evaluate different aspects of agent perfor-\nmance in Minecraft, including the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task. Extensive experiments demonstrate that the proposed\n1The Odyssey is a great ancient Greek epic poem attributed to Homer, which is now often used metaphori-\ncally to describe a long adventurous journey (Oxford English Dictionary).\n2\nPreprint\nODYSSEY framework provides a robust measure of agent effectiveness, showcasing the practical\nadvantages of our framework using the open-source models.\nIt is worth noting that our focus is not to design a new LLM-based agent architecture. Instead,\nthis work aims to provide a comprehensive framework for developing and evaluating autonomous\nagents in open-world environments, enabling them to explore the vast and diverse Minecraft world.\nWe have open-sourced all parts of ODYSSEY and will continuously update the repository. We hope\nthis will enable other researchers to build upon our work, fostering further innovation and progress\nin the development of autonomous agents.\n2\nOPEN-WORLD SKILL-BASED INTERACTIVE AGENT\nODYSSEY develops an LLM-based interactive agent with an open-world skill library, aiming to en-\nhance the efficiency and adaptability of agents in complex Minecraft environments. The skill library\ncomprises 40 primitive skills and 183 compositional skills, while the LLM-based agent employs a\nplanner-actor-critic architecture to facilitate task decomposition, skill execution, and performance\nfeedback. The architecture of the interactive agent is depicted in Fig. 2. Full skill and prompt details\nused in the LLM-based interactive agent are given in Appendix C.\n2.1\nOPEN-WORLD SKILL LIBRARY\nPrimitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript\nAPIs (PrismarineJS, 2023), divided into two main categories: 32 operational skills and 8 spatial\nskills. This suite of skills exceeds the 18 primitive skills (all are operational skills) delineated in\nVoyager (Wang et al., 2023a). Operational skills serve as foundational interfaces with parameter-\nized input, such as mine(·) for material collection and craft(·) for tool crafting. Additionally,\nwe pioneer 8 spatial skills that Voyager (Wang et al., 2023a) lacks, allowing for environmental in-\nteractions based on the agent coordinates. Given that our work is conducted within a text-based\nMinecraft environment (Wang et al., 2023a; Fan et al., 2022), spatial skills are crucial for handling\ntasks that require precise positioning and orientation, especially in the absence of visual input.\nCompositional skills encapsulate primitive skills into higher-level ones, functioning to address a va-\nriety of basic programmatic tasks, such as mineDiamond and craftIronPickaxe. ODYSSEY\nclassifies 183 compositional skills into types like mineX, craftX, plantX, breedX, cookX,\netc. We use a recursive method to construct the skill library, simplifying complex task decomposi-\ntion by ensuring prerequisites are met before skill execution. Taking mineDiamond as an example,\nif the agent lacks an iron pickaxe, it will recursively execute craftIronPickaxe. This indicates\nthat our program internally manages the construction and execution order of skills through its recur-\nsive method, thereby avoiding the need for the agent to engage in additional planning.\nTo facilitate efficient retrieval of skills in the skill library, we first generate a description for each skill\nby calling the LLM and using the complete program code as a prompt. We then employ Sentence\nTransformer (Reimers & Gurevych, 2019) to encode the skill description. This method transforms\ntext information into vector representations, facilitating semantic retrieval and enabling the agent to\nfind the most relevant skill description based on the context provided.\n2.2\nLLM PLANNER\nThe LLM Planner is responsible for developing a comprehensive plan, facilitating efficient explo-\nration through long-term goal decomposition. The LLM Planner breaks down high-level goals into\nspecific low-level subgoals, each corresponding to a particular skill outlined in Sec. 2.1. By address-\ning each subgoal in the plan, the ultimate goal can be progressively achieved. The input prompt to\nthe planner consists of several components: (1) Ultimate goals and behavioral constraints. For\nexample, “My ultimate goal is to ... Propose the current task only when you ensure that you have all\nthe necessary dependent items in inventory”. (2) States of the agent. This reflects the interaction\nbetween the agent and environment, such as hunger and health values, position and nearby entities,\netc. (3) Achievements of the agent. This includes the current inventory and unlocked equipment,\nas well as previously successful and failed tasks.\n3\nPreprint\nKnowledge Q&A\n   First, you should ...\nHow to obtain milk?\nSkill Retrieve\nCode Action\nBreed cow\nKill one cow with sword\nCollect milk with bucket\nAchivements\nUltimate Goals\nPlan\nEnvironment\nMineflayer\nPrimitive\nCompositional\nI want to  breed  as\nmany as animals\nlike      ,     or     ,\nand then collect \nitems from them.\nHealth \nPos\nExecute\nValidate\n[Lack of pre-requirements] \nI cannot collect milk without a     .\nExecution\nFeedback\n[Environment feedback]\nI could not find a       to collect milk.\nSelf-validation:\nSelf-reflection:\nCritic\nSince you only have     ,\nyou might need the      to\nattract a      for milk.\nBased on changes of my\ninventory, is my subgoal\nsuccessful? 🤔\nThought\nYou should analysis the\nreason why my subgoal\nis failed based on the\nlogs provided.\nRethink\nObservation\nSkill Library\n[Biome] snowy\n[Time] day\n[Nearby bocks] dirt, grass, \noak_log, oak_leaves, tall_grass, \ncobblestone, crafting_table,\nacacia_log\n[Nearby entities] horse, pig\n[Health]: 18.0\/20\n[Hunger]: 16.0\/20\n[Position]: x=2134.5, y=69.0,\nz=769.5\n[Inventory] oak_log, ...\n[Equipment] helmet,\nleggings, boots, ...\n[Completed] mine ...\nUpdate\nFailed\nSuccessful\nPotential\nMy subgoal is to: \ncollect milk\nlast_inventory (16\/36): ... \ncur_inventory (18\/36): ... \nFigure 2: An illustrative diagram of the interactive agent following a planner-actor-critic architecture\nbased on the open-world skill library. The LLM Planner decomposes ultimate goals into specific\nsubgoals, while the LLM Actor then sequentially executes code actions for each subgoal using the\nskill library. The LLM Critic evaluates these actions through self-validation and reflection, enabling\nthe agent to update its plan based on execution feedback.\n2.3\nLLM ACTOR\nIn the execution phase, the LLM actor is invoked to sequentially execute the subgoals generated\nby the LLM planner within the Minecraft environment. This process utilizes the open-world skill\nlibrary to achieve these subgoals. The mapping from high-level subgoals to executable skill code is\naccomplished through query context encoding and skill similarity retrieval. This process includes:\n(1) Query context. The text-based subgoals generated by the LLM planner are encoded by Sen-\ntence Transformer (Reimers & Gurevych, 2019) to vector representations as the query context. (2)\nSimilarity matching. The vector similarity between the query context and the skill descriptions in\nthe skill library is computed to determine semantic closeness. (3) Skill selection. The top-5 relevant\nskills with the highest scores are identified, and the actor agent selects the most appropriate code for\nexecution within the environment based on their descriptions.\n2.4\nLLM CRITIC\nDuring action execution, it is critical for an agent to document its experiences, especially noting suc-\ncessful outcomes and failure points. This is crucial in open-world planning to establish a feedback-\ninformed system, which corrects initial plan discrepancies that can cause execution errors. For\ninstance, achieving the animal breeding goal requires prerequisite crops for feed. The LLM critic\ncan assess action effectiveness by comparing expected and actual outcomes, providing insights for\nrefining future strategies. We categorize feedback into three types: (1) Execution feedback. This\ncaptures the progress of skill execution. For example, “No hoe in inventory. Craft a hoe first!” not\nonly highlights the reason for failure in hoeing farmland but also provides a guideline to address this\nproblem. (2) Self-validation. By presenting inventory changes post-action to the LLM critic, we\nempower it to validate whether the skill has achieved its subgoal, eliminating the need for manual\nchecks. (3) Self-reflection. Simply confirming the completion of a subgoal is often inadequate for\ncorrecting planning errors. The LLM critic also serves as an analyst, deducing the cause of task\nfailure by evaluating the current state of the agent and its environment. It then offers a critique,\nsuggesting a more efficient strategy for task completion.\n3\nFINE-TUNE MINECRAFT LLM\nTo improve agent performance in Minecraft, we fine-tune the LLaMA-3 model (Touvron et al., 2023)\nusing a large-scale Question-Answering (Q&A) dataset with 390k+ instruction entries sourced from\nthe Minecraft Wiki. ODYSSEY presents an effective procedure for converting a foundation model\ninto a domain-specific model, which involves dataset generation, model fine-tuning, and model eval-\nuation. The detailed descriptions can be found in Appendix D.\n4\nPreprint\nDataset Generation. We develop a GPT-assisted method to generate an instruction dataset for\nMinecraft. First, we crawl relevant content from the Minecraft Wiki, excluding non-essential sec-\ntions like history. The collected data is then categorized and separated into different files based on\ntheir content type. Then we use GPT-3.5-Turbo (OpenAI, 2023) with different customized prompts\nto automatically generate diverse Q&A pairs. Note that both the questions and answers were gener-\nated by GPT. These Q&A pairs are categorized into four types based on the nature of the answers:\nshort, normal, long, and boolean, yielding 390k+ entries. In contrast, the Wiki dataset released\nby MineDojo (Fan et al., 2022) only collects Minecraft Wiki pages, without refining the content\nand generating Q&A pairs for model training. STEVE (Zhao et al., 2023) introduces a non-public\ndataset with 20k+ Q&A pairs, which is smaller than our dataset in terms of scale and diversity.\nModel Fine-tuning. We employ LoRA (Hu et al., 2021) for model fine-tuning, which is a parameter-\nefficient training technique. LoRA introduces small, trainable low-rank matrices to adapt a pre-\ntrained neural network, enabling targeted updates without the need to retrain the entire model. Us-\ning LoRA, we fine-tune the LLaMA-3-8B-Instruct and LLaMA-3-70B-Instruct models with our\nMinecraft dataset, resulting in the new models termed MineMA-8B and MineMA-70B, respectively.\nModel Evaluation. In Minecraft, questions are often open-ended and can yield diverse answers;\ntherefore, conventional evaluation metrics (Papineni et al., 2002; Lin, 2004) may fall short. Mean-\nwhile, common benchmarks (Wang et al., 2018; 2019; Hendrycks et al., 2021) are not suitable for\nassessing the capabilities of expert models. Thus, we employed GPT-4 (Achiam et al., 2023) to gen-\nerate two Multiple-Choice Question (MCQ) datasets based on different themes and keywords related\nto Minecraft. These datasets can quantitatively evaluate the domain-specific expertise of models.\n4\nAGENT CAPABILITY BENCHMARK\nCraftSword\nCraftArmor\nCombatMonster\nImmediate Feedback\nDynamic Plan\nIterative Optimization \nResource\nExplore\nLong-term Planning Task\nDynamic-immediate Planning Task\nAutonomous Exploration Task\nHoeFarmland\nShearSheep\nMilkCow\nSkill Library\nPlan 1\nPlan 2\nGoal\nFigure 3: Agent capability benchmark.\nODYSSEY presents a new benchmark for evaluating\nagent capabilities within Minecraft, offering three task\ntypes: long-term planning, dynamic-immediate plan-\nning, and autonomous exploration. It is notable that\nthese tasks cannot be solved by any single skill but de-\nmand a sophisticated combination of multiple skills.\nThese tasks are set in various Minecraft scenarios, with\ndifferent tasks in the same scenario testing different\nagent capabilities. For example, in the cooking sce-\nnario, long-term planning requires formulating a com-\nplete plan to locate and hunt a specific animal, whereas\ndynamic-immediate planning involves selecting which\nnearby animal to cook based on the immediate en-\nvironment. Our benchmark provides a standardized\nframework for evaluating agents, where the agent ca-\npability requirements for different tasks are shown in\nTable 1. Please refer to Appendix E for more details.\nLong-term Planning Task. We design a suite of combat scenarios to assess the long-term planning\ncapability of agents, requiring them to craft appropriate weapons and equipment to defeat various\nmonsters. These combat scenarios can be divided into single-type and multi-type monster scenarios.\nFor the single-type scenarios, we choose various unique monsters, each with its own attack styles,\nmovement patterns, and hostility levels. For the multi-type scenarios, we focus on typical monster\ngroupings encountered in the game. Agents must generate a comprehensive long-term plan, detail-\ning the sequence of crafting the necessary weapons and equipment for the assigned combat task.\nPerformance is measured by remaining health and time consumed during combat. After each battle,\nagents can iteratively optimize their plan, learning from previous outcomes to improve performance\nin subsequent rounds. To extend the scope of the long-term planning task beyond combat, we also\nadopt animal husbandry and cooking scenarios, where agents are required to formulate detailed\nplans for completing tasks related to specific animals.\nDynamic-immediate Planning Task. The dynamic-immediate planning task requires agents to\ndynamically generate and execute plans based on immediate environmental feedback. Thus, we\ndesign a suite of farming scenarios, where agents engage in activities like planting, cooking, and\n5\nPreprint\nTable 1: Specific agent capability requirements for different benchmark tasks, including Goal-based\nPlanning (GBP), Feedback-based Planning (FBP), Exploratory Planning (EP), Task Decomposi-\ntion (TD), Resource Management (RM), Skill Retrieval (SR), Self-Reflection (Self-R), and Self-\nValidation (Self-V). Please refer to Appendix E.4 for detailed descriptions of each capability.\nTask\nGBP\nFBP\nEP\nTD\nRM\nSR\nSelf-R\nSelf-V\nSingle-Round Long-Term Planning Task\n✓\n×\n×\n✓\n×\n✓\n✓\n✓\nMulti-Round Long-Term Planning Task\n✓\n✓\n×\n✓\n×\n✓\n✓\n✓\nDynamic-Immediate Planning Task\n✓\n✓\n×\n×\n✓\n✓\n✓\n✓\nAutonomous Exploration Task\n×\n✓\n✓\n×\n✓\n✓\n✓\n✓\nTable 2: Average execution time and success rate (SR) on 5 basic programmatic tasks in Minecraft.\nTask\nTime (min)\nSR in 2min\nSR in 5min\nSR in 10min\nSR in 15min\nCrafting Table\n0.59 ± 0.79\n95.8%\n99.2%\n100.0%\n100.0%\nWooden Tool\n0.95 ± 0.80\n92.5%\n99.2%\n100.0%\n100.0%\nStone Tool\n1.48 ± 0.96\n85.0%\n97.5%\n100.0%\n100.0%\nIron Tool\n4.43 ± 1.48\n0.0%\n76.7%\n100.0%\n100.0%\nObtain Diamond\n6.48 ± 2.02\n0.0%\n21.7%\n92.5%\n100.0%\nanimal husbandry. Although some scenarios are similar to the long-term planning task, the dynamic-\nimmediate planning task emphasizes reacting to real-time feedback like available resources and\nnearby animals. Performance is evaluated through task completion time and success rates.\nAutonomous Exploration Task. To test the exploratory capability of agents within open-world\nsettings, we design an autonomous exploration task in Minecraft. In this task, agents are required\nto determine their subsequent objectives and execute the appropriate skills based on the game con-\ntext. The exploration task involves discovering and utilizing resources, while adapting to unexpected\nevents such as encounters with hostile monsters. Agents must adapt to these challenges by devel-\noping strategies for resource management and task prioritization. The performance metrics include\nthe number of distinct items obtained, the total items crafted, the recipes and advancements (R&A)\nunlocked, and the distance traveled.\n5\nEXPERIMENTS\nTo demonstrate the effectiveness of the proposed ODYSSEY framework, we conduct experiments\non basic programmatic tasks and the agent capability benchmark. Our simulation environment is\nbuilt on top of Voyager (Wang et al., 2023a), providing a text-based interface for agents to interact\nwith Minecraft. We only use GPT-3.5 and GPT-4 for initial data generation, but all experiments are\nconducted with the open-source LLaMA-3 model, significantly reducing costs compared to GPT-4-\nbased skill generation methods (Wang et al., 2023a;b). Notably, we do not employ GPT-4 in Voyager\ndue to the high cost, which we estimate would be in the thousands of dollars per experiment. Instead,\nwe reproduce Voyager using GPT-4o-mini and GPT-3.5 for comparison. More details are provided\nin Appendix F. We aim to answer the following questions: (1) Can the open-world skill library\nimprove the efficiency of agents in Minecraft? (Sec. 5.1). (2) How well do agents with different\nLLMs perform on the agent capability benchmark tasks? (Sec. 5.2). (3) What is the contribution of\ndifferent components of the ODYSSEY agent to its overall performance? (Sec. 5.3).\n5.1\nOPEN-WORLD SKILL LIBRARY\nTo demonstrate the superior capability of our open-world skill library in Minecraft, we first tested it\non 5 basic programmatic tasks from previous studies (Zhu et al., 2023). We conducted 120 repeated\nexperiments on each task and recorded the average completion time for each task as well as the\nsuccess rates at different time points. We report the performance of baselines using the results\nreported from their own paper, including DEPS (Wang et al., 2023b), VPT (Baker et al., 2022),\nGITM (Zhu et al., 2023). The experimental results in Tab. 2 and Tab. 3 show that our skill library\nsignificantly improved the success rate and efficiency of the above tasks, surpassing previous studies.\n6\nPreprint\nTable 3: Average success rate of our framework and previous baselines on 5 basic programmatic\ntasks in Minecraft within ten minutes.\nAgent\nCrafting Table\nWooden Tool\nStone Tool\nIron Tool\nObtain Diamond\nDEPS\n90.0%\n80.0%\n73.3%\n10.0%\n0.6%\nVPT\n100.0%\n100.0%\n100.0%\n85.0%\n20.0%\nGITM\n100.0%\n100.0%\n100.0%\n95.0%\n67.5%\nOurs\n100.0%\n100.0%\n100.0%\n100.0%\n92.5%\n5.2\nAGENT CAPABILITY BENCHMARK\nWe evaluate the LLM-based agent on the long-term planning task, the dynamic-immediate planning\ntask, and the autonomous exploration task from the ODYSSEY benchmark. These tasks cover a\nvariety of complex gaming scenarios and require diverse solutions.\n5.2.1\nLONG-TERM PLANNING TASK\n1 Zombie\n1 Spider\n1 Skeleton 1 Enderman\nTask\n0\n3\n6\n9\n12\n15\n18\nTime (min)\nRound 1\nRound 2\nRound 3\nFigure 4: Performance on the multi-round long-\nterm planning task. Note that all presented data\nare from successful tasks.\nThe long-term planning task assesses the agent\ncapability to directly formulate and execute\ncomprehensive plans over extended periods. For\nexample, in the combat scenarios, the agent is\nrequired to plan a list of weapons and equip-\nment to craft based on the strength of different\nmonsters, with the goal of defeating the mon-\nster in as short a time as possible. We compared\nthe performance of our agent with both the fine-\ntuned MineMA-8B and the original LLaMA-3-\n8B models, and also the performance of Voy-\nager (Wang et al., 2023a) with GPT-4o-mini\nacross these tasks.\nMoreover, we also evalu-\nate the performance of single-round and multi-\nround planning. The single-round test results in Tab. 4 show that the fine-tuned MineMA-8B model\nsurpasses the original LLaMA-3-8B model in terms of success rate and time efficiency, albeit at\nthe cost of more LLM iterations. Moreover, our agent with the MineMA-8B model can outperform\nVoyager with GPT-4o-mini in most scenarios, indicating the effectiveness of our fine-tuning strat-\negy. The multi-round test results in Fig. 4 show that the multi-round planning strategy significantly\nimproves the time efficiency of the agent, indicating that the agent can iteratively optimize its plan\nbased on the outcomes of previous battles to enhance its performance in subsequent rounds.\n5.2.2\nDYNAMIC-IMMEDIATE PLANNING TASK\nFor the dynamic-immediate planning task, the agent is required to dynamically generate and execute\nplans based on immediate environmental feedback. We compared our MineMA model with differ-\nent open-sourced LLMs, including Qwen2-7B (Yang et al., 2024a) and Baichuan2-7B (Yang et al.,\n2023). Moreover, we evaluate the performance of the MineMA-8B and the MineMA-70B model\nto investigate the impact of model size on task performance. As shown in Tab. 5, the MineMA-\n8B model outperforms the Baichuan2-7B and Qwen2-7B models in terms of success rate and time\nefficiency. Moreover, the MineMA-70B model shows superior performance compared with the\nMineMA-8B model. Across all tasks, MineMA-70B demonstrates higher success rates and gener-\nally lower average execution times and LLM iterations.\n5.2.3\nAUTONOMOUS EXPLORATION TASK\nIn the autonomous exploration task, the agent is required to explore the Minecraft world freely with-\nout any specific goals. We compare our agent with different Minecraft-based agent methods (Voy-\nager (Wang et al., 2023a) and DEPS (Wang et al., 2023b)) and different LLM-based agent techniques\n(ReAct (Yao et al., 2023) and AutoGPT (Significant-Gravitas, 2023)) on this task. Note that we re-\nproduced different LLM-based agent techniques following the same settings as in Voyager (Wang\n7\nPreprint\nTable 4: Performance comparison of different models on the single-round long-term planning task.\n“Health” refers to the remaining health points. “# LLM iters” is the number of LLM iterations (call-\ning LLM) required to complete the task. “Time (min)” refers to the minutes spent in both gathering\nmaterials and crafting equipment to defeat different monsters. All evaluation metrics are calculated\nonly for successful tasks. ± corresponds to one standard deviation of the average evaluation over\nsuccessful tasks. Bold and italics mean the best and the second-best results. “-” indicates that health\nis not a relevant metric in the scenarios. “N\/A” indicates that all tasks fail.\nTask\nModel\nSuccess Rate\nHealth\nTime (min)\n# LLM Iters\n1 zombie\nVoyager\n3 \/ 3\n20.0 ± 0.0\n9.9 ± 6.0\n67.3 ± 41.7\nLLaMA-3-8B\n4 \/ 8\n20.0 ± 0.0\n8.3 ± 4.2\n6.1 ± 4.1\nMineMA-8B\n8 \/ 8\n19.4 ± 2.3\n8.8 ± 5.4\n10.0 ± 5.8\n1 spider\nVoyager\n3 \/ 3\n10.8 ± 8.0\n9.4 ± 8.8\n19.0 ± 1.4\nLLaMA-3-8B\n4 \/ 8\n19.4 ± 1.0\n12.1 ± 3.8\n8.4 ± 3.5\nMineMA-8B\n8 \/ 8\n19.3 ± 1.6\n8.3 ± 6.7\n15.2 ± 6.0\n1 skeleton\nVoyager\n2 \/ 3\n16.5 ± 0.0\n7.4 ± 2.9\n46.0 ± 32.0\nLLaMA-3-8B\n4 \/ 8\n17.6 ± 2.7\n8.1 ± 3.5\n8.9 ± 3.7\nMineMA-8B\n8 \/ 8\n13.6 ± 5.9\n8.6 ± 7.3\n12.1 ± 7.0\n1 zomb-\nified piglin\nVoyager\n3 \/ 3\n19.0 ± 1.4\n14.5 ± 4.7\n50.3 ± 26.2\nLLaMA-3-8B\n4 \/ 8\n19.9 ± 0.4\n9.2 ± 3.9\n10.0 ± 4.2\nMineMA-8B\n8 \/ 8\n18.7 ± 1.9\n8.5 ± 6.1\n11.7 ± 6.2\n1 ender-\nman\nVoyager\n2 \/ 3\n11.0 ± 9.0\n22.8 ± 1.7\n28.0 ± 4.0\nLLaMA-3-8B\n2 \/ 8\n15.1 ± 7.3\n13.0 ± 3.0\n6.8 ± 1.9\nMineMA-8B\n4 \/ 8\n19.8 ± 0.5\n10.4 ± 6.3\n12.5 ± 5.4\n1 zombie\nvillager\nVoyager\n2 \/ 3\n20.0 ± 0.0\n12.6 ± 2.0\n50.0 ± 3.0\nLLaMA-3-8B\n7 \/ 8\n19.6 ± 1.1\n12.7 ± 5.3\n11.0 ± 5.3\nMineMA-8B\n8 \/ 8\n20.0 ± 0.0\n9.0 ± 3.6\n12.8 ± 6.1\n1 cave\nspider\nVoyager\n2 \/ 3\n16.5 ± 3.5\n10.0 ± 1.8\n79.2 ± 29.0\nLLaMA-3-8B\n6 \/ 8\n19.5 ± 1.2\n12.0 ± 6.3\n19.5 ± 1.2\nMineMA-8B\n7 \/ 8\n20.0 ± 0.0\n3.6 ± 2.6\n8.6 ± 8.8\n1 wither\nskeleton\nVoyager\n1 \/ 3\n20.0 ± 0.0\n20.9 ± 0.0\n100.0 ± 0.0\nLLaMA-3-8B\n6 \/ 8\n13.2 ± 6.0\n11.7 ± 3.7\n12.3 ± 2.7\nMineMA-8B\n7 \/ 8\n17.3 ± 3.7\n11.0 ± 6.8\n12.6 ± 6.9\n1 zombie,\n1 spider\nVoyager\n1 \/ 3\n17.5 ± 0.0\n5.9 ± 0.0\n21.0 ± 0.0\nLLaMA-3-8B\n1 \/ 8\n20.0 ± 0.0\n8.5 ± 0.0\n6.0 ± 0.0\nMineMA-8B\n5 \/ 8\n16.4 ± 4.1\n10.6 ± 6.7\n12.0 ± 4.9\n1 zombie,\n1 skeleton\nVoyager\n2 \/ 3\n19.0 ± 1.0\n15.0 ± 8.6\n40.5 ± 20.5\nLLaMA-3-8B\n1 \/ 8\n0.2 ± 0.0\n13.5 ± 0.0\n9.0 ± 0.0\nMineMA-8B\n3 \/ 8\n12.8 ± 2.8\n14.0 ± 1.9\n10.3 ± 2.8\n3 zombies\nVoyager\n2 \/ 3\n7.8 ± 4.2\n8.2 ± 0.4\n61.0 ± 29.0\nLLaMA-3-8B\n1 \/ 8\n3.7 ± 0.0\n14.3 ± 0.0\n8.0 ± 0.0\nMineMA-8B\n1 \/ 8\n5.2 ± 0.0\n11.1 ± 0.0\n14.0 ± 0.0\ncook meat\nVoyager\n0 \/ 3\n-\nN\/A\nN\/A\nLLaMA-3-8B\n1 \/ 8\n-\n20.3 ± 0.0\n19.0 ± 0.0\nMinema-8B\n2 \/ 8\n-\n21.4 ± 1.2\n30.0 ± 10.0\nanimal\nhusbandry\nVoyager\n1 \/ 3\n-\n19.0 ± 0.0\n12.0 ± 0.0\nLLaMA-3-8B\n2 \/ 8\n-\n15.3 ± 7.6\n31.0 ± 4.0\nMinema-8B\n3 \/ 8\n-\n16.8 ± 7.8\n26.7 ± 16.2\net al., 2023a). As shown in Fig. 5, our agent with the MineMA-8B model can achieve superior\nperformance compared with all baselines, indicating that the agent can autonomously explore the\nMinecraft world without specific goals. It is notable that our agent with the MineMA-8B model can\noutperform Voyager (Wang et al., 2023a) with GPT-4o-mini or GPT-3.5.\n8\nPreprint\nTable 5: Performance comparison of different models on the dynamic-immediate planning task. All\nevaluation metrics are calculated only for successful tasks. “N\/A” indicates that all tasks fail. Please\nrefer to Appendix F.4 for easier visual inspection.\nTask\nModel\nSuccess Rate\nTime (min)\n# LLM Iters\nCollect Seeds\nBaichuan2-7B\n2 \/ 5\n1.8 ± 1.4\n3.0 ± 2.8\nQwen2-7B\n2 \/ 5\n3.8 ± 1.5\n4.5 ± 0.7\nMineMA-8B\n5 \/ 5\n1.3 ± 1.4\n1.4 ± 0.9\nMineMA-70B\n5 \/ 5\n1.4 ± 1.6\n1.0 ± 0.0\nHoe Farmland\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n2 \/ 5\n15.7 ± 16.2\n19.5 ± 10.6\nMineMA-8B\n2 \/ 5\n17.2 ± 14.7\n26.5 ± 9.2\nMineMA-70B\n4 \/ 5\n10.2 ± 6.7\n11.8 ± 2.6\nShear Sheep\nBaichuan2-7B\n1 \/ 5\n26.0 ± 0.0\n30.0 ± 0.0\nQwen2-7B\n2 \/ 5\n11.0 ± 2.8\n10.8 ± 1.5\nMineMA-8B\n2 \/ 5\n15.7 ± 10.9\n13.0 ± 9.9\nMineMA-70B\n3 \/ 5\n6.9 ± 7.8\n11.0 ± 7.5\nMilk Cow\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n26.1 ± 0.0\n30.0 ± 0.0\nMineMA-8B\n1 \/ 5\n7.2 ± 0.0\n7.0 ± 0.0\nMineMA-70B\n2 \/ 5\n8.6 ± 10.0\n10.0 ± 11.3\nCook Meat\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n0 \/ 5\nN\/A\nN\/A\nMineMA-8B\n1 \/ 5\n25.6 ± 0.0\n38.0 ± 0.0\nMineMA-70B\n2 \/ 5\n20.2 ± 8.5\n24.0 ± 2.8\nObtain Leather\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n14.9 ± 0.0\n16.0 ± 0.0\nMineMA-8B\n4 \/ 5\n15.0 ± 8.7\n17.8 ± 15.2\nMineMA-70B\n5 \/ 5\n7.4 ± 7.8\n8.8 ± 8.6\nMake Sugar\nBaichuan2-7B\n2 \/ 5\n16.2 ± 15.6\n22.0 ± 18.4\nQwen2-7B\n2 \/ 5\n15.4 ± 7.0\n15.5 ± 9.2\nMineMA-8B\n5 \/ 5\n4.3 ± 1.9\n7.0 ± 1.9\nMineMA-70B\n5 \/ 5\n4.3 ± 4.4\n7.8 ± 4.0\nCollect Water\nBaichuan2-7B\n0 \/ 5\nN\/A\nN\/A\nQwen2-7B\n1 \/ 5\n10.0 ± 0.0\n10.0 ± 0.0\nMineMA-8B\n4 \/ 5\n10.4 ± 3.0\n8.8 ± 5.5\nMineMA-70B\n5 \/ 5\n9.3 ± 4.8\n9.4 ± 3.7\n5.3\nABLATION STUDY\nWe conduct ablation studies on two core components of the ODYSSEY agent, including the LLM\nplanner and the open-world skill library. The results are shown in Fig. 5. In the autonomous ex-\nploration task, the LLM planner is responsible for generating a comprehensive plan based on the\nopen-world skill library. The ablation study demonstrates that the planner is indispensable for the\nagent to effectively navigate the complex Minecraft environment. Additionally, our experimental\nresults indicate that the absence of the open-world skill library significantly degrades performance.\nWithout the open-world skill library, the 8B LLM model alone is largely incapable of generating\nexecutable codes for the agent. This underscores the critical role of the open-world skill library in\nenabling the agent to perform complex tasks within the open-world setting of Minecraft.\n6\nRELATED WORKS\nMinecraft agents have been widely studied in recent years to test the capabilities of autonomous\nagents in open-world environments. Previous works focused on training Minecraft agents with\nreinforcement learning (Tessler et al., 2017; Oh et al., 2017; Lin et al., 2022; Mao et al., 2022; Hafner\n9\nPreprint\nDEPS with GPT-4o\nVoyager with GPT-3.5-Turbo\nVoyager with GPT-4o-mini\nReAct with GPT-4o-mini\nAutoGPT with GPT-4o-mini\nOdyssey with LLaMA3-8B\nOdyssey with MineMA3-8B w\/o Planner\nOdyssey with MineMA3-8B w\/o Skill Library\nOdyssey with MineMA3-8B\n0\n20\n40\n60\n80\nIteration Number\n0\n10\n20\n30\n40\n# Distinct Items Obtained\n(a) Exploration Curves\n0\n10\n20\n30\n40\n# Distinct Items Obtained\n0\n1000\n2000\n3000\n4000\nDistance Traveled\n0\n200\n400\n600\n# Items Crafted\n0\n50\n100\n150\n200\n# R&A Unlocked\n(b) Evaluation Metrics\nFigure 5: Performance comparison of different models on autonomous exploration tasks. To make\nthe results in figures clearer for readers, we adopt a 50% confidence interval to plot the error region.\net al., 2023) or imitation learning (Baker et al., 2022; Cai et al., 2023; Lifshitz et al., 2023), which\nare extensively used in the MineRL (Guss et al., 2019) competition to solve the ObtainDiamond\ntask. With the rapid development of LLMs, numerous studies leverage LLMs to enhance agent\ncapabilities (Zhang et al., 2023a; Zhu et al., 2023; Feng et al., 2023; Zhao et al., 2023; Wang et al.,\n2023a;b; Zhou et al., 2024a). Among these, several works (Li et al., 2023; Yuan et al., 2023; Wang\net al., 2023c; Qin et al., 2023; Ding et al., 2023) employ LLMs to guide skill learning in Minecraft,\nenabling agents to act in a human-like way. However, these methods mainly focus on learning\nprimitive skills from scratch, lacking a reusable skill library. Voyager (Wang et al., 2023a) builds\na skill library by allowing the LLM to write its own skills. However, Voyager must rely on GPT-4\nfor high-quality skill generation, incurring substantial costs. This expense can be prohibitive for\nmany researchers. In contrast, ODYSSEY provides an open-world skill library that agents can call\nupon, achieving performance comparable to Voyager with GPT-4, but using only 8B LLMs. This\nmakes ODYSSEY significantly more accessible and cost-effective, enabling LLM-based agents to\nefficiently generate complex policies for broader exploration.\nOpen-world environments have gained considerable attention from research communities (Cao\net al., 2020; Chevalier-Boisvert et al., 2018; Juliani et al., 2019; Shen et al., 2021; Srivastava et al.,\n2022; Du et al., 2023). Minecraft, with its diverse tasks and mature game mechanics, has emerged\nas an ideal test-bed for open-world tasks. Built on Minecraft, MineRL (Guss et al., 2019) imple-\nments a simulation environment for agent learning. MineDojo (Fan et al., 2022) further extends\nMineRL with thousands of diverse tasks. MCU (Lin et al., 2023) collects a variety of atom tasks,\noffering a method to generate infinite tasks by combining the atom tasks. However, existing bench-\nmarks mainly focus on providing basic programmatic tasks to evaluate agents learned from scratch.\nOur ODYSSEY benchmark is built on top of the skill library, enabling the agents to bypass basic\nprogrammatic tasks and focus on complex open-world challenges.\n7\nCONCLUSION\nThis work proposes ODYSSEY to empower agents with open-world skills in the Minecraft environ-\nment. We introduce (1) an interactive agent endowed with an extensive open-world skill library com-\nprising various primitive skills and compositional skills; (2) a fine-tuned LLaMA-3 model, trained on\na large-scale question-answering dataset sourced from the Minecraft Wiki; (3) a new agent capabil-\nity benchmark that encompasses tasks requiring long-term planning, dynamic-immediate planning,\nand autonomous exploration. The public availability of all datasets, model weights, and code will\nfacilitate future research in the development of autonomous agents. We hope that ODYSSEY will\ninspire further innovation and progress in the field of autonomous agent development.\nLimitations and Future Works. The proposed open-world skill library enables the use of open-\nsource LLMs as the foundation for agents to call upon skills, avoiding the high costs associated with\nprevious work using GPT-4 (Wang et al., 2023a; Li et al., 2023; Qin et al., 2023). However, the open-\nsource LLMs are prone to generating hallucinations, leading to a decrease in agent performance.\n10\nPreprint\nThus, our future research will focus on employing retrieval-augmented generation to improve LLMs\nin Minecraft. Additionally, this work focuses on developing and evaluating text-based LLMs in the\ncontext of Minecraft, with visual aspects currently out of scope. Looking ahead, we plan to integrate\nvisual understanding into the skill library to enhance the agent capabilities.\nREFERENCES\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical\nreport. arXiv preprint arXiv:2303.08774, 2023.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nBowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Advances in Neural Information Processing Systems, volume 35, pp.\n24639–24654, 2022.\nShaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Open-world multi-task control\nthrough goal-aware representation learning and adaptive horizon prediction. In Proceedings of the\nIEEE\/CVF Conference on Computer Vision and Pattern Recognition, pp. 13734–13744, 2023.\nTianshi Cao, Jingkang Wang, Yining Zhang, and Sivabalan Manivasagam. Babyai++: Towards\ngrounded-language learning beyond memorization. arXiv preprint arXiv:2004.07200, 2020.\nMaxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia,\nThien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of\ngrounded language learning. In International Conference on Learning Representations, 2018.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su.\nMind2web: Towards a generalist agent for the web. In Advances in Neural Information Processing\nSystems, volume 36, pp. 28091–28114, 2023.\nZiluo Ding, Hao Luo, Ke Li, Junpeng Yue, Tiejun Huang, and Zongqing Lu. Clip4mc: An rl-friendly\nvision-language model for minecraft. arXiv preprint arXiv:2303.10571, 2023.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied mul-\ntimodal language model. In Proceedings of the International Conference on Machine Learning,\nvolume 202, pp. 8469–8488, 2023.\nYuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek\nGupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language\nmodels. In Proceedings of the International Conference on Machine Learning, volume 202, pp.\n8657–8677, 2023.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. In Advances in Neural Information Processing Systems,\nvolume 35, pp. 18343–18362, 2022.\nYicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, and Zongqing Lu. Llama rider: Spurring\nlarge language models to explore the open world. arXiv preprint arXiv:2310.08922, 2023.\nWilliam H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela\nVeloso, and Ruslan Salakhutdinov. Minerl: a large-scale dataset of minecraft demonstrations.\nIn Proceedings of the International Joint Conference on Artificial Intelligence, pp. 2442–2448,\n2019.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\n11\nPreprint\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference\non Learning Representations, 2021.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations, 2021.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda\nLuu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning\nthrough planning with language models. In Proceedings of the Conference on Robot Learning,\nvolume 205, pp. 1769–1782, 2022.\nIat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao\nDong, and Jie Tang. OpenWebAgent: An open toolkit to enable web agents on large language\nmodels. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\npp. 72–81, 2024.\nArthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry,\nAdam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization challenge in\nvision, control, and planning. In Proceedings of the International Joint Conference on Artificial\nIntelligence, pp. 2684–2691, 2019.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural\nInformation Processing Systems, volume 33, pp. 9459–9474, 2020.\nHao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao, Xiaogang Wang, Hongsheng Li,\nLewei Lu, and Jifeng Dai. Auto mc-reward: Automated dense reward design with large language\nmodels for minecraft. arXiv preprint arXiv:2312.09238, 2023.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence,\nand Andy Zeng. Code as policies: Language model programs for embodied control. In IEEE\nInternational Conference on Robotics and Automation, pp. 9493–9500, 2023.\nShalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A generative\nmodel for text-to-behavior in minecraft. In Advances in Neural Information Processing Systems,\nvolume 36, pp. 69900–69929, 2023.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74–81, 2004.\nHaowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: A task-centric framework for open-\nended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023.\nZichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, and Wei Yang. Juewu-mc: Playing\nminecraft with sample-efficient hierarchical reinforcement learning. In Proceedings of the Inter-\nnational Joint Conference on Artificial Intelligence, pp. 3257–3263, 2022.\nHangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye Hao, Dong\nLi, and Pingzhong Tang. Seihai: A sample-efficient hierarchical ai for the minerl competition.\nIn Proceedings of the International Conference on Distributed Artificial Intelligence, pp. 38–51,\n2022.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nJunhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli.\nZero-shot task generalization\nwith multi-task deep reinforcement learning. In Proceedings of the International Conference on\nMachine Learning, volume 70, pp. 2661–2670, 2017.\n12\nPreprint\nOpenAI. Introducing chatgpt. 2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the Annual Meeting of the Association for\nComputational Linguistics, pp. 311–318, 2002.\nPrismarineJS. Mineflayer: Create minecraft bots with a powerful, stable, and high level javascript\napi. https:\/\/github.com\/PrismarineJS\/mineflayer, 2023.\nYiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing\nShao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception.\narXiv preprint arXiv:2312.07472, 2023.\nScott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\nOriol Vinyals, Mahyar Bordbar, and Nando de Freitas.\nA generalist agent.\narXiv preprint\narXiv:2205.06175, 2022.\nNils Reimers and Iryna Gurevych.\nSentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 3982–3992, 2019.\nManolis Savva, Jitendra Malik, Devi Parikh, Dhruv Batra, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, and Vladlen Koltun. Habitat: A\nplatform for embodied AI research. In Proceedings of the IEEE\/CVF International Conference\non Computer Vision, pp. 9338–9346, 2019.\nDhruv Shah, Michael Robert Equi, Bła˙zej Osi´nski, Fei Xia, Brian Ichter, and Sergey Levine. Naviga-\ntion with large language models: Semantic guesswork as a heuristic for planning. In Proceedings\nof the Conference on Robot Learning, volume 229, pp. 2683–2699, 2023.\nBokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Clau-\ndia Pérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, Micael Tchapmi, Kent\nVainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: A simulation environment for\ninteractive tasks in large realistic scenes. In Proceedings of the IEEE\/RSJ International Confer-\nence on Intelligent Robots and Systems, pp. 7520–7527, 2021.\nSignificant-Gravitas.\nAutogpt:\nBuild & use ai agents.\nhttps:\/\/github.com\/\nSignificant-Gravitas\/AutoGPT, 2023.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans\nusing large language models. In Proceedings of the IEEE International Conference on Robotics\nand Automation, pp. 11523–11530, 2023.\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Elliott\nVainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, Silvio Savarese, Hyowon Gweon,\nJiajun Wu, and Li Fei-Fei. Behavior: Benchmark for everyday household activities in virtual,\ninteractive, and ecological environments. In Proceedings of the Conference on Robot Learning,\nvolume 164, pp. 477–490, 2022.\nChen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, and Shie Mannor. A deep hier-\narchical approach to lifelong learning in minecraft. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 31, pp. 1553–1561, 2017.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding.\nIn\nInternational Conference on Learning Representations, 2018.\n13\nPreprint\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman.\nSuperglue: A stickier benchmark for general-purpose language\nunderstanding systems. In Advances in Neural Information Processing Systems, volume 32, pp.\n3261–3275, 2019.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nTransactions on Machine Learning Research, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large\nlanguage model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. De-\nscribe, explain, plan and select: interactive planning with llms enables open-world multi-task\nagents. In Advances in Neural Information Processing Systems, volume 36, pp. 34153–34189,\n2023b.\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,\nZhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with\nmemory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In Advances in Neural Information Processing Systems, volume 35, pp. 24824–24837,\n2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances\nin Neural Information Processing Systems, volume 35, pp. 24824–24837, 2022b.\nXinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and Börje F Karls-\nson. A survey on game playing agents and large models: Methods, applications, and challenges.\narXiv preprint arXiv:2403.10249, 2024.\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan,\nDian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint\narXiv:2309.10305, 2023.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, et al.\nQwen2 technical report.\narXiv preprint\narXiv:2407.10671, 2024a.\nSonghua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, and Hongying\nZan. Zhongjing: Enhancing the chinese medical capabilities of large language model through\nexpert feedback and real-world multi-turn dialogue. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 38, pp. 19368–19376, 2024b.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations, 2023.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing\nLu. Skill reinforcement learning and planning for open-world long-horizon tasks. arXiv preprint\narXiv:2303.16563, 2023.\nChi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. Creative agents: Empowering\nagents with imagination for creative tasks. arXiv preprint arXiv:2312.02519, 2023a.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xi-\nangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. Huatuogpt,\ntowards taming language model to be a doctor. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, pp. 10859–10885, 2023b.\n14\nPreprint\nZhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-\nNeng Hwang, and Gaoang Wang. See and think: Embodied agent in virtual environment. arXiv\npreprint arXiv:2311.15209, 2023.\nEnshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and\nJing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-\nworld control. arXiv preprint arXiv:2403.12037, 2024a.\nGengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language naviga-\ntion with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 38, pp. 7641–7649, 2024b.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\n15\nPreprint\nAppendix\nTable of Contents\nA Discussion on Societal Impacts\n17\nB\nDiscussion on Migrating Odyssey to Other Domains\n17\nC Open-World Skill-based Interactive Agent\n17\nC.1\nOpen-World Skill Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nC.2\nLLM Planner\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nC.3\nLLM Actor\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.4\nLLM Critic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nD Fine-tune Minecraft LLM\n25\nD.1\nDataset Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD.2\nModel Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nD.3\nModel Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nE\nAgent Capability Benchmark\n33\nE.1\nLong-term Planning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.2\nDynamic-immediate Planning Task . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.3\nAutonomous Exploration Task\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nE.4\nSpecific Agent Capability Requirements for Different Tasks . . . . . . . . . . .\n34\nF\nExperiments\n34\nF.1\nExperimental Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nF.2\nAgent Capability Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nF.3\nAblation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nF.4\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n16\nPreprint\nA\nDISCUSSION ON SOCIETAL IMPACTS\nWhen developing autonomous embodied agents within Minecraft, the negative impacts are rela-\ntively minimal. Minecraft provides a controlled environment to test these technologies. Concerns\ninclude potential over-reliance by players, reducing their exploratory and creative thinking, minor\ndata privacy issues due to the collection of anonymized player data, and possible impacts on game\nbalance, particularly in multiplayer settings. Overall, Minecraft is an ideal experimental platform\nwhere these mild negative impacts can be effectively managed.\nB\nDISCUSSION ON MIGRATING ODYSSEY TO OTHER DOMAINS\nThe skill library designed for Minecraft is built with modularity and generalizability in mind, al-\nlowing for potential adaptation to other domains such as robot manipulation (Liang et al., 2023;\nSingh et al., 2023), navigation (Zhou et al., 2024b; Shah et al., 2023), and other game-playing\nenvironments (Xu et al., 2024). These skills abstract underlying actions and focus on high-level\ninteractions, allowing them to be adapted to different environments by redefining low-level actions\nwithout changing the overall structure of the skill library. Even without direct API access, basic\naction spaces (e.g., keyboard and mouse operations in games, or movement operations in robotics)\ncan be employed to construct primitive skills. Prior research in robotic manipulation, including\nCaP (Liang et al., 2023) and ProgPrompt (Singh et al., 2023), demonstrates how primitive skills\nsuch as picking and placing objects or opening containers can be built from basic actions. More-\nover, we believe that the concept of \"skills\" should extend beyond code APIs to include knowledge\nfrom various sources. For example, handbooks can provide informational segments treated as skills,\nretrievable by LLMs using techniques like retrieval-augmented generation (Lewis et al., 2020), en-\nhancing decision-making.\nTo fine-tune the LLaMA-3 model for the Minecraft agent, we crawled the Minecraft Wiki and used a\nGPT-assisted approach to generate an instruction dataset. Researchers in other domains can replicate\nthis process to create their own instruction datasets. To facilitate this, we have also open-sourced\nour Minecraft Wiki crawler, which can be easily modified to crawl similar Wiki websites for other\ndomains. Additionally, our benchmark tasks evaluate agent performance from three perspectives:\nlong-term planning, dynamic-immediate planning, and autonomous exploration. These dimensions\neffectively assess the capabilities of open-world autonomous agents. Researchers in other domains\ncan adopt these perspectives to design comprehensive evaluation tasks for their needs.\nC\nOPEN-WORLD SKILL-BASED INTERACTIVE AGENT\nC.1\nOPEN-WORLD SKILL LIBRARY\nC.1.1\nPRIMITIVE SKILLS\nPrimitive skills encompass a series of underlying interfaces on top of Mineflayer JavaScript\nAPIs (PrismarineJS, 2023), divided into two main categories: 32 operational skills and 8 spatial\nskills. In addition to Voyager’s 18 operational skills (Wang et al., 2023a), 14 operational skills\nimplemented by us are presented as follows:\n• plantSeeds(bot, type): Let the agent find the nearest farmland and plant a partic-\nular kind of seed.\n• feedAnimals(bot, type, count=1): Let the agent find the nearest animals of a\nparticular species and numbers and feed them with the appropriate food.\n• killAnimal(bot, type): Let the agent kill a particular kind of animal using the best\nsword in its inventory.\n• killMonsters(bot, type, count=1): Let the agent kill monsters nearby of a\nparticular species and numbers using the best sword in its inventory.\n• cookFood(bot, type, count=1): Let the agent cook food of a particular kind and\nnumbers using coal and furnace.\n• eatFood(bot, type): Let the agent eat a particular kind of food.\n17\nPreprint\n• equipArmor(bot): Let the agent equip the best armor(helmet, chestplate, leggings and\nboots) in its inventory.\n• equipSword\/Pickaxe\/Axe\/Hoe\/Shovel(bot): Let the agent equip the best cor-\nresponding tool in its inventory.\n• getLogs\/PlanksCount(bot): Return the number of logs\/planks (counted in seven\ndifferent categories) in the inventory.\nAdditionally, we pioneer 8 spatial skills that Voyager (Wang et al., 2023a) lacks, allowing for en-\nvironmental interactions based on the agent coordinates. The spatial skills implemented by us are\npresented as follows:\n• findSuitablePosition(bot): Let the agent find the best nearby location for plac-\ning devices such as a crafting table or furnace. The block must be minecraft:air and\nat least one adjacent reference block exists.\n• checkAdjacentBlock(bot, types, x, y, z): Check blocks adjacent to the\nblock at position (x, y, z). Return true if any of the adjacent blocks match the specified\ntypes.\n• checkBlockAbove(bot, type, x, y, z): Check block above the block at po-\nsition (x, y, z). Return true if the above block matches the specified type.\n• checkBlocksAround(bot, type, x, y, z): Check blocks around the block at\nposition (x, y, z).Return true if any of the around blocks match the specified type.\n• checkNearbyBlock(bot, types, x, y, z, r):\nCheck blocks in a radius\naround the block at position (x, y, z). Return true if any block within the radius matches the\nspecified types.\n• checkNoAdjacentBlock(bot, types, x, y, z): Check adjacent blocks of\nblock at position (x, y, z). Return true if not all adjacent blocks are within the speci-\nfied types.\n• goto(bot, x, y, z): Let the agent go to the corresponding position (x, y, z) until it\nreaches the destination.\n• getAnimal(bot, type, x, y, z): Let the agent attract a particular kind of ani-\nmal to a particular position (x, y, z) with the appropriate food.\nC.1.2\nCOMPOSITIONAL SKILLS\nAll compositional skills are encapsulated by the Mineflayer APIs and the aforementioned primitive\nskills, while higher-level compositional skills recursively call lower-level ones. Fig. 6 illustrates the\nnested relationships among the 13 skills required to complete the mineDiamond task. We classify\nall compositional skills into main types as follows:\n• mineX(bot): Equip the agent with the appropriate tools and find the nearest specific\nblock to mine it.\n• craftX(bot): Let the agent collect the necessary materials and check if the crafting\ntable exists in the inventory (if needed), to craft a specific tool or something.\n• smeltX(bot): Let the agent check the furnace and fuel, and to smelt the specified ma-\nterials.\n• collectX(bot): Similar to mineX, used to collect multiple quantities of a certain item.\n• makeX(bot): Similar to craftX, used to make food.\n• cookX(bot): Similar to smeltX, used to cook food.\n• plantX(bot): Let the agent check the inventory for seeds, collect them if not present,\nand plant them in nearby farmland.\n• breedX(bot): Let the agent check the inventory for the required corresponding feed,\nfind the nearest two animals, feed them, and facilitate their breeding.\n• killX(bot): Let the agent equip the best sword in the inventory, find the nearest specific\nanimal or monster, kill it, and collect the dropped items.\n• placeX(bot): Let the agent place an item at its current or a nearby suitable location,\nand if the item is not in inventory, craft it first.\nAdditionally, there are several other compositional skills aimed at executing specific behaviors, such\nas catchFish, hoeFarmland, shearSheep, takeAndMoveMinecart.\n18\nPreprint\ncraftSticks\ncraftWoodenPlanks\nmineWoodLog\ncraftWoodenPickaxe\ncraftCraftingTable\ncraftIronPickaxe\nsmeltRawIron\nmineIronOre\nmineCoalOre\ncraftFurnace\ncraftStonePickaxe\nmineCobblestone\nmineDiamond\nFigure 6: An illustrative diagram of the skill recursive method for the mineDiamond task. The\nfour colors depicted represent four different technological levels (wood, stone, iron, and diamond)\nfollowing the Minecraft tech-tree.\nC.2\nLLM PLANNER\nODYSSEY relies on LLMs to generate language-based plans. In our Minecraft experiment, we pro-\npose three novel tasks (long-term planning task, dynamic-immediate planning task and autonomous\nexploration task) for agents to explore. Therefore we designate three types of prompt messages for\nthem respectively, offering LLM Planner the ability to generate different routines on different tasks.\nThe format of the prompt is presented thus:\n• \"SYSTEM\" role: A high-level instruction that gives directions to the model behavior. It\nsets an overall goal for the interaction and provides external information.\n• \"USER\" role: Detailed information like environment, states and achievements of the agent\nwill be provided to the planner for the next immediate subgoals.\n• \"ASSISTANT\" role: A guideline generated by the planner.\nC.2.1\nLONG-TERM PLANNING\nWe design a suite of combat tasks to assess the long-term planning capabilities of agents, where the\nLLM Planner should plan to craft appropriate weapons and equipment to defeat monsters.\nThe input prompt to LLM consists of several components:\n• Ultimate goals: The monsters that need to be defeated.\n• Directives and Behavior Constraints: These ensure that the proposed tasks are both achiev-\nable and verifiable.\n• Information of last combat: Incorporating information from previous battles enables the\ndevelopment of more efficient plan.\nLong-term Planning System Prompt\n—Overall goals—\nYour goal is to generate the plan that can defeat all monsters while using the shortest\ntime. So, more is not always better when proposing your plan list.\n—External information—\nIn Minecraft, combating with monsters requires weapons and armor.\nThe weapon\noptions are limited to \"sword\", while the armor includes \"helmet\", \"chestplate\", \"leggings\",\n19\nPreprint\nand \"boots\". The materials for swords range from low to high level: wooden swords, stone\nswords, iron swords, and diamond swords; The materials for armor range from low to high\nlevel: iron, diamond. The higher the material level, the greater the attack damage of the\nweapon and the better the protection effect of the armor. However, the higher the material\nlevel, the more time it costs to collect.\nTips: Wooden, stone, iron and diamond are the only levels of sword; iron and diamond\nare the only levels of armors; helmet, chestplate, leggings and boots are the only types of\narmors; do not generate information that doesn’t relate to them.\nAfter each round of combat, I will give you:\nEquipment obtained from last round: ...\nHealth after last combat: ...\nCritique: ...\nMonster: The monsters you need to defeat.\n—Directions—\nThe critique (if any) will tell you the subgoal list from the previous round and whether you\nshould trim or add to it. Remember to refer to the critique to adjust your task list. Next, you\nwill start a new combat task, the last round of equipment and health is only for planning\nreference, not related to the current round. Plan from scratch!\n—Behaviour constraints—\nYou must follow the following criteria:\n1) Return a Python list of subgoals that can be completed in order to complete the specified\ntask.\n2) Each subgoal should only start with \"craft\"! do not propose any other type of skills!\n3) Each subgoal should follow a concise format \"craft [material type] [equipment type]\".\nYou should only respond in JSON format as described below:\n[\"subgoal1\", \"subgoal2\", \"subgoal3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nAfter finish collecting weapons and equipment, we also plan an efficient routine to combat with\nmonsters for higher survival rates. For example, monsters that are more harmful and aggressive\nshould be placed in a higher priority. The full prompt for re-ranking the combat order of monsters\nis shown below.\nComabt Order System Prompt\nYou are a helpful assistant that generates the order of fighting monsters to defeat all monsters\nspecified by me.\nI’ll give you a list of monsters, and you need to rearrange the order of monsters according to\nhow hard it is to beat them.\nYou should give priority to monsters that attack the player and do more damage, while\nmonsters that don’t actively attack the player or do less damage should be left behind.\nMake sure your list includes all the monsters in your task.\nThe output format must be exactly same as the input, including the underline.\nIf your task is to combat a single type of monsters, return a list containing only that monster\nas well.\nYou should only respond in JSON format as described below:\n[\"quantity monster1\", \"quantity monster2\", \"quantity monster3\", ...]\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\n20\nPreprint\nC.2.2\nDYNAMIC-IMMEDIATE PLANNING\nIn this kind of task, agents are expected to adapt their plans based on the real-time feedback like\nnearby resources and animals.\nThe input prompt to LLM consists of the following components:\n• Ultimate goals: A suite of farming tasks, such as planting, harvesting, and animal hus-\nbandry.\n• The current states of agent: hunger and health values, position and nearby entities, etc.\n• Achievements of the agent: the current inventory and unlocked equipment, as well as pre-\nviously successful and failed tasks.\nDynamic-immediate Planning System Prompt\n—Overall goals—\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft.\nMy ultimate goal is to \"goals\".\nMake sure that the proposed task is related to the ultimate goal, and do not propose unrelated\ntasks!\n—Directions—\nYou need to plan step by step towards your ultimate goal, so propose necessary pre-\nrequisite tasks first.\nFor example, \"craft hoe\" before \"hoe farmland\", \"collect [type] seeds\" and \"hoe farmland\"\nbefore \"plant seed\", \"kill [animalType]\" before \"cook meat\", \"craft shears\" before \"shear\nsheep\", \"craft bucket\" before \"collect milk\".\nPropose the current task only when you ensure that you have all the necessary dependent\nitems in inventory.\nDon’t ask for repetitive tasks. If you already have an item in your inventory, try not to\ncollect it repeatedly.\nFor example, when you already have a hoe in your inventory, propose \"hoe farmland\"\ninstead of \"craft hoe\" again.\n—External information—\nI will give you the following information:\nUltimate goal: ...\nReference: ...\nBiome: ...\nNearby blocks: ...\nOther blocks that are recently seen: ...\nNearby entities (nearest to farthest): ...\nHealth: Higher than 15 means I’m healthy.\nHunger: Higher than 15 means I’m not hungry.\nInventory (xx\/36): ...\nLogs: The execution logs in last task, you can refer to it to propose next task.\nCompleted tasks so far: ...\nFailed tasks that are too hard: ...\n—Behaviour constraints—\nYou must follow the following criteria:\n1) Please be very specific about what resources I need to collect, what I need to farm, or\nwhat animals I need to breed\/kill.\n2) The next task should follow a concise format, such as \"craft [item]\", \"breed\/kill [animal\n21\nPreprint\ntype]\", \"cook\/eat [food type]\", \"plant [seed type] seed\" or some specific action \"shear\nsheep\", \"collect milk\". Do not propose multiple tasks at the same time. Do not mention\nanything else.\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next task\nshould be\",\n\"task\": \"The next task\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nC.2.3\nAUTONOMOUS EXPLORATION\nIn this task, the agent is required to explore the Minecraft world freely without any specific goals.\nThis poses a great challenge to the planner for maximal exploration. It should propose suitable tasks\nbased on the current state and environment, e.g., plan to obtain sand or cactus before wood if it finds\nitself in a desert rather than a forest. The input prompt to LLM consists of several components:\n• Guidelines encouraging diverse tasks.\n• The current states of agent: hunger and health values, position and nearby entities, etc.\n• Achievements of the agent: the current inventory and unlocked equipment, as well as pre-\nviously successful and failed tasks.\nAutonomous Exploration System Prompt\n—Overall goals—\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft.\nMy ultimate goal is to discover as many diverse things as possible, accomplish as many\ndiverse tasks as possible and become the best Minecraft player in the world.\n—External information—\nI will give you the following information:\nBiome: ...\nTime: ...\nNearby blocks: ...\nOther blocks that are recently seen: ...\nNearby entities (nearest to farthest): ...\nHealth: Higher than 15 means I’m healthy.\nHunger: Higher than 15 means I’m not hungry.\nPosition: ...\nEquipment: If I have better armor in my inventory, you should ask me to equip it.\nInventory (xx\/36): ...\nChests: ...\nCompleted tasks so far: ...\nFailed tasks that are too hard: ...\n—Directions—\nYou must follow the following criteria:\n1) You should act as a mentor and guide me to the next task based on my current learning\nprogress.\n2) Please be very specific about what resources I need to collect, what I need to craft, or\nwhat mobs I need to kill.\n22\nPreprint\n3) The next task should follow a concise format, such as \"Mine [block]\", \"Craft [item]\",\n\"Smelt [item]\", \"Kill [mob]\", \"Cook [food]\", \"Equip\" etc. It should be a single phrase. Do\nnot propose multiple tasks at the same time. Do not mention anything else.\n4) The next task should be novel and interesting. I should look for rare resources, upgrade\nmy equipment and tools using better materials, and discover new things. I should not be\ndoing the same thing over and over again.\n5) Don’t propose tasks that have already completed once or failed more than three times!\n6) Do not ask me to build or dig shelter even if it’s at night. I want to explore the world and\ndiscover new things. I don’t want to stay in one place.\n7) Tasks that require information beyond the player’s status to verify should be avoided. For\ninstance, \"Placing 4 torches\" and \"Dig a 2x1x2 hole\" are not ideal since they require visual\nconfirmation from the screen. All the placing, building and trading tasks should be avoided.\nDo not propose task starting with these keywords.\n8) For wood-related tasks, you don’t need to emphasize the type of wood, just propose\n\"mine log\" or \"craft planks\".\n—Behaviour constraints—\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"Based on the information I listed above, do reasoning about what the next\ntask should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nC.3\nLLM ACTOR\nIn actor, the mapping from higher language subgoals S to lower executable codes is implemented\nthrough query context encoding and similarity retrieval. We employ the following prompt during\nthe generation of query context (Question-Answer pairs).\nQuery Context Prompt\nSYSTEM:\nYou are a helpful assistant that answer my question about Minecraft.\nI will give you the following information:\nQuestion: ...\nYou will answer the question based on the context (only if available and helpful) and your\nown knowledge of Minecraft.\n1) Start your answer with \"Answer: \".\n2) Answer \"Answer: Unknown\" if you don’t know the answer.\nUSER:\nHow to complete S in Minecraft?\nAfter recalling the top-10 relevant skills with the highest scores, we require LLM to determine the\nmost appropriate code for execution within the environment based on their description. The full\nprompt of code selection is shown in the following.\nSkill Selection System Prompt\nYou are a helpful assistant that decides Mineflayer javascript code to complete any Minecraft\ntask specified by me.\nI will give you\nTask: The task I need to complete in this stage.\n23\nPreprint\nPrograms: The description of relevant programs that may use to complete the task.\nProgram used in the last round: ...\nCritique: ...\nYou will choose only one program based on the program description and critique. You\nshould only respond in the format as described below:\n{\n\"program\": \"your selected program name\",\n\"reason\": \"Reason you choose the program.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nPlease ensure that the program name you output should be exactly the same (case-inclusive)\nas the information provided!\nC.4\nLLM CRITIC\nThe LLM critic should evaluate the success of the executed actions by comparing expected out-\ncomes with actual results, thereby providing valuable critiques for refining strategies in subsequent\niterations. We design a chain-of-thought (Wei et al., 2022b) prompting mechanism: We first require\nLLM to reason about the task’s success or failure, then output a boolean variable representing the\nexecution result, and finally provide a critique to the agent if the task fails.\nCritic System Prompt\nYou are required to evaluate if I have met the task requirements in Minecraft. Exceeding\nthe task requirements is also considered a success while failing to meet them requires you\nto provide critique to help me improve.\nI will give you the following information:\nTask: The objective I need to accomplish.\nNearby blocks:\nEntities:\nEquipment: My tools, weapons and armor could sometimes be here.\nChests: If the task requires me to place items in a chest, you can find chest information\nhere.\nCurrent inventory (xx\/36): My final inventory after carry out the task.\nLast inventory (xx\/36): My inventory before carry out the task.\nChat log: The logs during carrying out the task.\n**Note** that you only need to check the changes of my inventory to judge whether I meet\nthe task.\nFor a `craft [item]`task, all you need to do is checking if the item I need to craft is in my\ncurrent inventory or equipment. If in, you should judge it as a success and vice versa.\nFor a `mine [item]`task, you only need to check whether the item is in my current inventory\nor has an increase over the last inventory.\nFor a `hoe`or `plant`task, you only need to check whether the `farmland`or `seed`is in\nNearby Blocks.\nDo not judge the success of a `craft`task based on other materials I have!\nYou can only judge a task failure via chat log, not as a reason to judge a task’s success.\nYou should only respond in JSON format as described below:\n{\n\"reasoning\": \"reasoning\",\n\"success\": boolean,\n\"critique\": \"critique\",\n}\n24\nPreprint\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nThe input prompt to LLM consists of the following components:\n1. Task proposed by the LLM Planner;\n2. Environment feedback: We provide the agent with nearby blocks and entities that are re-\ncently seen for high-quality critiques. We also give the log information during the execution\nstage;\n3. Achievements of the agent. We offer achievement of the agent like inventory and equipment\nto assess the task’s completeness.\nCritic User Prompt\nTask: Mine 1 wood log\nNearby blocks: birch_leaves, oak_leaves, birch_log, oak_log\nEquipment: [None, None, None, None, 'oak_sapling', None]\nChests: None\nCurrent Inventory (2\/36): 'oak_sapling': 1, 'oak_log': 1\nLast Inventory (0\/36):\nChat log: Mined 1 wood log.\nD\nFINE-TUNE MINECRAFT LLM\nFor detailed code, datasets, and models used in this section, please visit our code for more informa-\ntion. The overall fine-tuning framework is shown in Fig. 7.\nData\nGeneration\nMinecraft\nWiki\nTXT\nFormat\nMD\nFormat\nData\nCleaning\nGPT-\n3.5-\nturbo\nMinecraft\nQ&A\nDataset\nLong\nShort\nBool\nModel\nFine-tuning\nModel\nEvaluation\n  Llama-3-8B-Instruct   \n Llama-3-70B-Instruct  \nMineMA \nPage Driven\nPrompt\nTheme Driven\nPrompt\nGPT-4\nMCQ Evaluation\nDataset v1\nMCQ Evaluation\nDataset v2\nEvaluation\nFine-tune  Minecraft LLM\nNormal\nFine-tune\nLoRA\nFigure 7: An overview of the fine-tune Minecraft LLM framework.\nD.1\nDATASET GENERATION\nD.1.1\nDATA CLEANING\nFor this study, we select two primary sources of information, the Minecraft Fandom Wiki (https:\n\/\/minecraft.fandom.com\/wiki\/Minecraft_Wiki) and the Minecraft Wiki (https:\n\/\/minecraft.wiki\/).\nFor the Minecraft Fandom Wiki, we first crawl the content of all its pages and perform a preliminary\nfiltering on the resulting pages, removing pages that were obviously useless for our fine-tuning task,\n25\nPreprint\nsuch as game version information, and obtaining a series of JSON files. These files still contain a\nsignificant amount of information that we do not need to create the dataset, so we carry out a data\ncleaning process, extracting the text and table content of the original pages, storing them in a series\nof TXT files, with each page corresponding to a TXT file. Through the above method, we obtain the\ncleaned TXT format page information.\nFor Minecraft Wiki, we exclude a few categories that are useless for our fine-tuning task, such as\nHistory, and crawl the content of all other categories’ pages. Similar to the process for Minecraft\nFandom Wiki, these pages also contain a lot of information that we didn’t need. We remove some\nirrelevant sections such as Achievements, Advancements, and History, and only retained the text\nand table content of other useful sections. After that, we store the processed data in markdown files\nand perform segmentation operations based on the number of words in the file. We ensure that the\nword count of the segmented file did not exceed the set limit without cutting the section block, thus\nobtaining clean MD format page information.\nD.1.2\nQ&A PAIRS\nIn this study, we design four types of Q&A pairs for our training dataset as follows:\n• Short Answer Q&A: Questions that can be directly answered with a word or phrase, such\nas names, statistics, etc.\n• Long Answer Q&A: Questions that require one or more complete sentences as answers.\nThese questions are suitable for explanatory or definitional responses, where a detailed\nexplanation or a clear definition is needed to fully address the question.\n• Bool Answer Q&A: Questions that can be answered with a simple True or False.\n• Normal Answer Q&A: QA pairs with no specific form restrictions, used to supplement the\ndataset and ensure its comprehensiveness.\nSome Q&A pair examples are shown as follows:\nShort Answer Q&A\nQ: What mobs spawn naturally in the desert biome during the day in Minecraft?\nA: During the day in Minecraft, zombies in desert biomes are replaced by husks, which\ninflict the Hunger effect and do not burn in sunlight.\nLong Answer Q&A\nQ: How does the use of light sources like torches and lava help in preventing mob spawning\nin Minecraft?\nA: In Minecraft, placing light sources such as torches, lava, glowstone, jack o’lanterns, sea\nlanterns, and lanterns helps prevent hostile mob spawning. Mobs generally spawn in low\nlight conditions, and these light sources provide enough illumination to raise the light level\nabove the threshold needed for mob spawning. Torches are the most commonly used due to\ntheir easy availability and simplicity in creating a well-lit perimeter around an area. Lava is\nalso effective but must be used cautiously as it can ignite nearby flammable materials.\nBool Answer Q&A\nQ: Can skeletons shoot through bamboo?\nA: F\nNormal Answer Q&A\nQ: What are piglins in Minecraft?\nA: Piglins are neutral mobs found in the Nether. They become hostile towards players unless\nthe player is wearing at least one piece of golden armor. They can be bartered with using\n26\nPreprint\ngold ingots for various items, and they wield either a golden sword or a crossbow for melee\nor ranged attacks, respectively.\nD.1.3\nINSTRUCTION DATASETS GENERATION PROMPTS\nWe design four types of prompts for four types of Q&A pairs to generate instruction datasets by\nusing GPT-3.5-Turbo. Here are our prompts.\nShort Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 30 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 30 question-and-answer examples, and you should generate ques-\ntions within the provided user text that can be directly answered with a word or phrase, such\nas dates, names, statistics, etc. This involves identifying specific, concise information within\nthe text that can be succinctly responded to, ensuring that the answers are clear and directly\nrelated to the questions asked. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nLong Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\n27\nPreprint\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 20 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 20 question-and-answer examples. Identify questions within the\nprovided user text that require one or more complete sentences as answers. These questions\nshould be suitable for explanatory or definitional responses, where a detailed explanation or\na clear definition is needed to fully address the question. This involves crafting answers that\nare comprehensive and informative, ensuring they adequately explain or define the subject\nmatter in question. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nBool Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\n28\nPreprint\nYour task is to generate at least 10 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 10 question-and-answer examples. Look for questions within the\nprovided user text that can be answered with a simple True or False. This task involves\npinpointing statements or queries within the text that lend themselves to binary responses,\nensuring that the answers are straightforward and unambiguous, clearly indicating whether\nthe statement is true or false based on the information available. And you will do so in this\nformat:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nNormal Answer Q&A prompt\nSystem Message\nYou are a question-and-answer dataset generating expert, you are generating data that will be\nused to train a large language model designed to address questions posed by users regarding\nthe game Minecraft, and from that, you will generate question-and-answer data samples,\neach with a prompt\/response pair.\nYou will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\nresponse_goes_here\n———–\n```\nYour task is to generate at least 20 examples. Make sure your samples are unique and diverse,\nyet high-quality and complex enough to train a well-performing model.\nUser Message\nYour task is to generate 20 question-and-answer examples. And you will do so in this format:\n```\nprompt\n———–\nprompt_goes_here\n———–\nresponse\n———–\n29\nPreprint\nresponse_goes_here\n———–\n```\nPlease generate as many question and answer pairs as possible. Here is the user content:\n{user_content}\nD.2\nMODEL FINE-TUNING\nIn this study, we use the instruction dataset with 390,317 instruction entries mentioned above to\nfine-tune the Minecraft Q&A expert models, using the LoRA fine-tuning method. We name the\nseries of fine-tuned models MineMA. The resulting models include MineMA-8B-v1, MineMA-8B-\nv2, MineMA-8B-v3, derived from the base model LLama-3-8B-Instrument, and MineMA-70B-v1,\nMineMA-70B-v2, derived from the base model LLama-3-70B-Instrument. MineMA-70B series of\nmodels are fine-tuned on four A6000 GPUs, while the remaining models are fine-tuned on a single\nA6000 GPU each. Among the models, MineMA-8B-v1 and MineMA-70B-v1 only undergo one\nround of training without an evaluation process, while the other models are trained with multiple\nrounds that incorporate an evaluation procedure. We use early stopping to halt the training process\nwhen there is no reduction in the evaluation loss over a series of evaluations, and finally save the\nmodel which has the best performance. Some training parameters are shown in Tab. 6.\nTable 6: Training parameters for different MineMA models.\nModel\nLoRA r\nLoRA alpha\nLoRA dropout\nLearning Rate\nWeight Decay\nSingle Round?\nMineMA-8B-v1\n64\n128\n0.1\n1E-04\n1E-04\nT\nMineMA-8B-v2\n32\n64\n0.1\n1E-04\n1E-04\nF\nMineMA-8B-v3\n64\n128\n0.1\n1E-04\n1E-04\nF\nMineMA-70B-v1\n16\n32\n0.1\n1E-04\n1E-04\nT\nMineMA-70B-v2\n64\n128\n0.1\n1E-04\n1E-04\nF\nD.3\nMODEL EVALUATION\nD.3.1\nEVALUATION DATASETS CREATING PROCESS\nIn this study, we utilize GPT-4 to create two evaluation MCQ datasets: a multi-theme MCQ dataset\nand a Wiki-based MCQ dataset. For the multi-theme MCQ dataset, we first summarize the following\nMinecraft content themes:\nGame Basics\nBlocks and Items: Basic blocks, special blocks, tools, weapons, armor, etc.\nSurvival Mechanics: Health, hunger, experience levels, death and respawn, etc.\nWorld Exploration\nBiomes: Characteristics of different biomes, generated structures, unique resources, etc.\nTerrain and Landforms: Features and resource distribution of different terrains.\nMobs and Interactions\nMobs: Characteristics and behaviors of passive, neutral, and hostile mobs.\nCombat System: Monster types, combat tactics, weapons and equipment, enchantments,\npotions, etc.\nTrading and Villagers: Villager professions, trading mechanics, village structures, etc.\n30\nPreprint\nSurvival Skills\nResource Gathering: Methods of obtaining various resources and their uses.\nCrafting and Production: Usage of crafting tables, furnaces, etc., equipment crafting and\nupgrading.\nFarming and Animal Husbandry: Crop planting, animal breeding, automated farms, etc.\nBuilding and Creativity\nBuilding Styles: Various building styles and key points.\nBuilding Techniques: Symmetry, proportion, detail handling in construction, etc.\nInterior Decoration: Interior design, lighting, item placement, etc.\nRedstone Mechanics: Redstone components, circuit design, automated devices, etc.\nSpecial Dimensions\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc.\nThe End: Characteristics of the End, Ender Dragon, cities, ships, etc.\nAdventure and Exploration: Special generated structures like ocean monuments, woodland\nmansions, ruins, fortresses, etc.\nThen, we list different numbers of keywords for each theme based on the amount of relevant knowl-\nedge content. According to the amount of information related to each keyword, we match a number\nfor each keyword, representing the number of multiple-choice questions to be generated based on\nthat keyword. After preparing the groundwork, we use GPT-4 to generate the multi-theme MCQ\ndataset, totaling 1,050 multiple-choice questions. The relevant prompts are shown below, taking the\ngeneration of multiple-choice questions in the Special Dimensions theme as an example:\nSystem Message\nYou are an expert in generating Minecraft quiz questions. Your task is to create multiple-\nchoice questions about the game Minecraft based on the theme of \"Special Dimensions\"\nand the provided keywords. The introduction of the theme of \"Special Dimensions\" is as\nfollows:\nSpecial Dimensions:\nThe Nether: Peculiarities of the Nether, unique blocks and mobs, special structures, etc.\nThe End: Characteristics of the End, Ender Dragon, cities, ships, etc.\nAdventure and Exploration: Special generated structures like ocean monuments, woodland\nmansions, ruins, fortresses, etc.\nProvide four answer options labeled A, B, C, and D. Only one option should be correct.\nAfter the question and options, state the correct answer. Please format the output as follows:\nDifficulty: Easy\/Medium\/Hard\nTopic: Special Dimensions\nKey Word: text\nQuestion: Question text\nOptions: A.text B.text C.text D.text\nCorrect Answer: A\/B\/C\/D\nEnsure that the difficulty distribution of the questions and options is reasonable, and the\nanswers should be detailed and informative.\nUser Message\nPlease generate some Minecraft multiple-choice questions based on the following 5 key-\nwords, covering three difficulty levels: simple, moderate, and difficult. The number after the\nkeyword represents how many multiple-choice questions to generate based on this keyword.\nKeywords:\n31\nPreprint\n{keywords_go_here}\nEnsure that the Q&A content is rich and accurate, and test the player’s understanding of the\ngame. Provide a balanced combination of simple, medium, and difficult questions. Generate\neach question and answer in the given format. Here is an example:\nExample:\nDifficulty: Hard\nTopic: Special Dimensions\nKey Word: End Ship\nQuestion: What exclusive item can be found in the End Ship in Minecraft?\nOptions: A. Netherite B. Dragon Egg C. Elytra D. Beacon\nCorrect Answer: C\nFor the Wiki-based MCQ dataset, we utilize GPT-4’s knowledge of Minecraft-related Wiki content\nto create a set of multiple-choice questions that closely align with the information on the Wiki pages.\nFirstly, we list 615 Minecraft-related keywords based on the importance of the relevant knowledge.\nAfterwards, we generate a Wiki-based MCQ dataset using GPT-4 with designed prompts based on\nthese keywords, totaling 2,083 pieces of data. The prompts we used are as follows:\nSystem Message\nYou are an expert in generating Minecraft multiple-choice questions. Your task is to create\nmultiple choice questions about the game Minecraft based on the provided keywords and\nthe information on the corresponding page on the Minecraft Wiki. Ensure that the source\nof information for the multiple-choice questions you generate is the Minecraft Wiki, while\nensuring the objectivity and accuracy of the multiple-choice questions and ensuring good\nquality.\nProvide four answer options labeled A, B, C, and D. Only one option should be correct.\nAfter the question and options, state the correct answer. Please format the output as follows:\nDifficulty: Easy\/Medium\/Hard\nKey Word: text\nQuestion: Question text\nOptions: A.text B.text C.text D.text\nCorrect Answer: A\/B\/C\/D\nEnsure that the difficulty distribution of the questions and options is reasonable, and the\nanswers should be detailed and informative.\nUser Message\nPlease generate some Minecraft multiple-choice questions based on the following 5 key-\nwords, covering three difficulty levels: simple, moderate, and difficult. The number after the\nkeyword represents the minimum number of multiple-choice questions generated based on\nthe keyword. For important keyword, you should generate more questions.\nKeywords:\n{keywords_go_here}\nEnsure the source of information for the multiple-choice questions you generate is the\nMinecraft Wiki, while ensuring the objectivity and accuracy of the multiple-choice ques-\ntions and ensuring good quality. Provide a balanced combination of simple, medium, and\ndifficult questions. Generate each question and answer in the given format, do not use '#'or\n''.. Here is an example:\nExample:\nDifficulty: Medium\nKey Word: Dirt\nQuestion: What happens when you right-click on a dirt block with a hoe?\nOptions: A. It turns into farmland B. It turns into grass C. It turns into a path block D.\nNothing happens\nCorrect Answer: A\n32\nPreprint\nD.3.2\nEVALUATION RESULTS\nWe use the above two MCQ datasets to evaluate the MineMA series models and the corresponding\nbase models. Each model is tested 5 times with the two datasets. The results are shown in Tab. 7.\nTable 7: The evaluation results based on the MCQ datasets.\nModel\nAverage Accuracy (Multi-theme)\nAverage Accuracy (Wiki-based)\nLlama-3-8b-Instruct\n61.09%\n54.38%\nMineMA-8B-v1\n62.69%\n61.97%\nMineMA-8B-v2\n62.23%\n62.09%\nMineMA-8B-v3\n62.99%\n62.42%\nLlama-3-70b-Instruct\n77.41%\n72.52%\nMineMA-70B-v1\n78.11%\n73.03%\nMineMA-70B-v2\n75.68%\n72.88%\nE\nAGENT CAPABILITY BENCHMARK\nE.1\nLONG-TERM PLANNING TASK\nIn Minecraft, there are a total of 35 hostile creatures. We conducted experiments on both single-\nmonster combat tasks and combined combat tasks (up to three types of monsters), resulting in thou-\nsands of different tasks that can all be implemented through the interfaces we provided.\n• combatEnv(bot, h, r, y): Generate a hollow rectangular arena with a height of\nh and a square base with side length 2r at altitude y, positioning the agent at the exact\ncenter of this enclosed space. This configuration ensures controlled conditions for evalu-\nating combat scenarios, especially considering not being influenced by naturally spawning\nmonsters.\n• summonMob(bot, n = 1, r, type): Facilitate the spawning of hostile creatures\naround the bot. It randomly positions n monsters within a designated range (r to 2r along\nthe x and z axes) from the bot, allowing for the creation of varied combat tasks and enabling\ncomprehensive testing of bot performance under different tactical challenges.\nE.2\nDYNAMIC-IMMEDIATE PLANNING TASK\nIn Minecraft, many farming tasks require interaction with the environment and dynamic planning.\nWe propose a series of tasks that can be accomplished through our skill library, including hoeing\nfarmland, planting seeds, harvesting crops, making food, slaughtering animals, cooking meat, feed-\ning and breeding animals, among others. For example, in the task cook meat, if the agent is\ninformed that there is a chicken nearby, it should plan to \"kill one chicken\" rather than anything\nelse. Additionally, in the task milk cow, the agent must simultaneously monitor the appearance\nof cows in the vicinity and gather materials to craft a bucket to collect the milk.\nE.3\nAUTONOMOUS EXPLORATION TASK\nIn Minecraft, autonomous exploration is the gameplay approach that most closely mimics how hu-\nman players engage with the game. To evaluate the diversity of discoveries made by the agent\nduring autonomous exploration, we used \"Distinct Items Obtained\" as the primary evaluation met-\nric. The acquisition of a greater variety of items demonstrates more diverse exploratory behavior\nby the agent. Additionally, based on statistical information and progress in-game achievements, we\ncalculated supplementary evaluation metrics including the \"Distance Traveled\" by the agent (sum-\nming walking, sprinting, climbing, swimming, and other forms of movement), the total number of\n\"Items Crafted\" (the sum of all types of items obtained by crafting), and \"Recipes and Achievements\nUnlocked\" (the sum of crafting recipes and game achievements unlocked).\n33\nPreprint\nE.4\nSPECIFIC AGENT CAPABILITY REQUIREMENTS FOR DIFFERENT TASKS\nThis section provides an overview of the specific agent capabilities required for each task, laying the\nfoundation for a deeper understanding of how our benchmark evaluates different aspects of agent\nperformance. Different agent capabilities are detailed as follows:\n• Goal-based Planning: This capability refers to the agent’s ability to formulate and execute\ncomprehensive plans based on predefined goals. It involves understanding the given goals\nand devising a step-by-step plan to achieve them over extended periods. This is critical\nfor tasks such as the long-term planning task, where agents need to craft weapons and\nequipment to defeat specific monsters.\n• Feedback-based Planning: This capability involves the agent’s ability to adapt its plans\ndynamically based on environmental feedback. It is essential for tasks where environmen-\ntal feedback is crucial, such as in the dynamic-immediate planning task and the multi-round\nlong-term planning task, where agents must adjust their strategies in response to the out-\ncomes of previous actions or environmental changes.\n• Exploratory Planning: This capability evaluates the agent’s ability to set its own goals\nand make decisions independently in a complex environment. Agents must navigate, gather\ninformation, and decide on objectives without predefined goals. This is central to the au-\ntonomous exploration task, where agents explore the Minecraft world, discover resources,\nand adapt to unforeseen events.\n• Task Decomposition: This capability refers to the agent’s ability to break down complex\ntasks into specific, manageable sub-tasks. It is vital for the long-term planning task where\nagents need to craft a sequence of items, requiring the breakdown of the end goal into a\nseries of intermediate steps.\n• Resource Management: This capability involves the efficient allocation and utilization\nof available resources. Agents must maintain awareness of their inventory, manage assets\neffectively, and identify which resources need to be gathered. This is particularly important\nin farming tasks and autonomous exploration, where resource availability and management\nare crucial for subsequent behavior.\n• Skill Retrieval: This capability pertains to the agent’s ability to identify and choose the\nmost suitable skill from a set of options. Agents evaluate a list of relevant skills and select\nthe one that best fits the current environmental context and task requirements. All tasks\nrequire agents to retrieve and apply relevant skills based on situational demands.\n• Self-Reflection: This capability involves the agent’s ability to analyze and learn from the\noutcomes of its actions. Simply confirming the completion of a subgoal is often inadequate\nfor correcting planning errors. The agent evaluates its performance, deduces the cause of\ntask failures, and suggests more efficient strategies for future tasks. This is particularly\nimportant in multi-round tasks.\n• Self-Validation: This capability enables the agent to autonomously confirm the success\nof its actions against intended outcomes. By assessing inventory changes after actions, the\nagent ensures that each step contributes towards the overarching objectives without external\nverification. This capability is crucial for all tasks, as agents need to continuously ensure\ntheir actions align with the objectives.\nF\nEXPERIMENTS\nF.1\nEXPERIMENTAL DETAILS\nWe select the 1.19.4 version of Minecraft as the experimental environment. Within this virtual game\nworld, spatial measurements are determined by blocks, while temporal measurements are dictated\nby ticks, each lasting 0.05 seconds. A single day-night cycle in the game is 24,000 ticks, equivalent\nto 20 minutes in the real world, with 10 minutes of daytime, 7 minutes of nighttime, and a 3-minute\ndawn\/dusk transition (when both the sun and moon are visible in the sky). Additionally, the game’s\nweather system randomly transitions between clear, rainy, thunderstorm, and snowy conditions,\nadding dynamic changes to the environment. Players are born into a randomly generated massive\n34\nPreprint\nworld, covering an area of 30,000,000 blocks × 30,000,000 blocks, which can be approximately\nconsidered an infinite world without boundaries. Players start with no resources and must gather\neverything from scratch that is beneficial for survival and completing the ultimate goal. When a\nplayer character dies, it will respawn randomly within a 32-block radius of the death location on the\nground, and any collected items will not be dropped. Agents can connect to the game through local\nnetworks or multiplayer servers. We have tested on Ubuntu 20.04, Windows 10, and macOS. In all\nexperiments of the agent capability benchmark, the \"MineMA-8B\" refers to \"MineMA-8B-v3\", and\nthe \"MineMA-70B\" refers to \"MineMA-70B-v1\".\nWe use the following Minecraft mods in our experiment. It is important to note that the version of\nmods must be consistent with the game version, specifically 1.19.4.\n• Fabric API: Basic Fabric APIs.\n• Mod Menu: Used to manage all the mods that you download.\n• Complete Config: Dependency of server pause.\n• Multi Server Pause: Used to pause the server when waiting for LLM to reply.\n• Better Respawn: Used for random respawning of player characters.\nConsidering the randomness of resource distribution in the Minecraft world, we ensure that the agent\nstarts from different locations in the game world before each round of experiments. We implemented\nthe respawnAndClear interface to perform some initialization settings.\n• respawnAndClear(bot): Transport the agent to a new location and clear its in-\nventory, ensuring that the game mode is switched to survival and the game difficulty is\nswitched to peaceful.\nF.2\nAGENT CAPABILITY BENCHMARK\nIn our multi-round Long-term Planning Task, the agent is required to iteratively improve planning\nbased on combat outcomes, aiming for victory with the highest efficiency, take as little time as\npossible. Specifically, if the agent wins in the previous round, it should streamline its planning in\nthe next round, gathering materials and crafting equipment in less time to enhance time efficiency\n(reflected in the experimental results as a decrease in time and LLM iterations); conversely, if it\nloses, it must refine its planning to upgrade the quality of weapons and equipment in the planning\nlist to ensure ultimate success (reflected in the experimental results as an increase in health, or\ngo from losing to winning). Additionally, when calculating experimental results, we compute the\naverage and standard deviation for time, LLM iters (LLM iterations) and the health metric only for\nvictorious outcomes, since a defeat, indicated by health being zero, is not meaningful.\nExample of multi-round combat task\nCombat Task: 1 Skeleton\nPlan list of 1st round:[craft iron sword, craft iron helmet, craft iron chestplate, craft\niron leggings, craft iron boots]\nEquipment obtained of 1st round: [iron_helmet, iron_chestplate, iron_leggings, iron_boots,\ncrafting_table, None]\nTime spent on crafting equipment: 15,953 ticks; 8 LLM iters\nRemaining Health after the combat: 14.0 \/ 20 (victory)\n—streamlining—\nPlan list of 2nd round:[craft iron sword]\nEquipment obtained of 2nd round:[None, None, None, None, iron_sword, None]\nTime spent on crafting equipment: 3,614 ticks; 4 LLM iters\nRemaining Health after the combat: 9.2 \/ 20 (victory and more efficiently)\n—streamlining—\nPlan list of 3rd round:[craft wooden sword]\n35\nPreprint\nEquipment obtained of 3rd round:[None, None, None, None, wooden_sword, None]\nTime spent on crafting equipment: 416 ticks; 1 LLM iter\nRemaining Health after the combat: 9.0 \/ 20 (victory and even more efficiently)\nIn our Dynamic-immediate Planning Task, the agent is asked to plan step by step based on en-\nvironmental information. We calculate the success rate across various tasks, the average execution\ntime and LLM iters as well as their standard deviation (only if successful) as evaluation metrics. It is\nimportant to note that skills used in these tasks do not utilize the recursive decomposition mechanism\nwe propose but require the agent to plan the necessary preparatory steps by itself. The following\noutlines the specific skill execution pathways for the five tasks in our experiments:\nSkill execution path of the Dynamic-immediate Planning Task\nCollect Seeds: Collect Wheat Seeds \/ Collect Melon Seeds \/ Collect Pumpkin Seeds\nHoe Farmland: Craft Hoe →Hoe Farmland\nShear Sheep: Craft Shears→Shear Sheep Using Shears\nMilk Cow: Craft Bucket→Milk Cow Using Bucket\nCook Meat: Kill Pig→Cook Porkchop \/ Kill Chicken→Cook Chicken \/ Kill Sheep→Cook\nMutton \/ Kill Cow→Cook Beef\nIn our Autonomous Exploration Task, the agent also needs to plan step by step without a given\ngoal. Every time a new plan is proposed, the agent retrieves the ten most semantically similar skills\nfrom our skill library and selects one to execute. We tally the number of distinct item types obtained\nby the agent in each round, as well as the cumulative number of item types. Here are the distinct\nitems obtained by the agent from one round of the experiment:\nDistinct items obtained within 80 LLM iters\n['oak_log', 'stick', 'wooden_sword', 'crafting_table', 'wooden_pickaxe', 'stone_pickaxe',\n'oak_planks', 'wheat_seeds', 'dirt', 'cobblestone', 'raw_iron', 'granite', 'andesite', 'cob-\nbled_deepslate', 'diorite', 'diamond', 'iron_pickaxe', 'furnace', 'cobblestone_wall', 'coal',\n'iron_ingot', 'iron_trapdoor', 'dandelion', 'azure_bluet', 'poppy', 'oxeye_daisy', 'chest', 'cob-\nblestone_stairs', 'raw_copper', 'copper_ingot', 'calcite', 'copper_block', 'birch_planks', 'jun-\ngle_log', 'arrow', 'bone', 'rotten_flesh'], Num: 37\nThis result is comparable to the Voyager (Wang et al., 2023a) framework that employs GPT-4 for\nskill code generation and significantly outperforms Voyager using GPT-3.5.\nF.3\nABLATION STUDY\nWe conduct ablation studies on two core components of the ODYSSEY agent, including the LLM\nplanner and the open-world skill library.\nFor the LLM planner ablation, we remove the current environmental information in the planner\nsystem prompt as follows. Moreover, in each task proposed during each round, if the retrieved\nskills were not relevant to the current task (i.e., if the semantic retrieval score was below a certain\nthreshold), the execution of those skills was not carried out.\nPlanner System Prompt in Ablation\nYou are a helpful assistant that tells me the next immediate task to do in Minecraft. My\nultimate goal is to discover as many diverse things as possible, accomplish as many diverse\ntasks as possible and become the best Minecraft player in the world. You can propose next\nsuitable tasks for me, such as \"Mine [block]\", \"Craft [item]\", \"Smelt [item]\", \"Kill [mob]\",\n\"Cook [food]\", \"Equip\" etc. It’s better to be a single phrase.\nYou should only respond in JSON format as described below:\n36\nPreprint\n{\n\"reasoning\": \"Do reasoning about what the next task should be.\",\n\"task\": \"The next task.\"\n}\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no\nsingle quotes, etc.\nFor the open-world skill library ablation, we removed the entire skill library and provided the LLM\nonly with the necessary interfaces required for composing new skills. Each round’s skill retrieval\nand execution were changed to code writing and execution, similar to the approach used in Voy-\nager (Wang et al., 2023a). The actor system prompt is shown as follows:\nActor System Prompt in Ablation\nYou are a helpful assistant that writes Mineflayer javascript code to complete any Minecraft\ntask specified by me.\n—External information—\nAt each round of conversation, I will give you\nCode from the last round: ...\nExecution error: ...\nChat log: ...\nBiome: ...\nNearby blocks: ...\nNearby entities (nearest to farthest):\nHealth: ...\nHunger: ...\nPosition: ...\nEquipment: ...\nInventory (xx\/36): ...\nChests: ...\nTask: ...\nContext: ...\nCritique: ...\n—Directions—\nYou should then respond to me with\nExplain (if applicable): Are there any steps missing in your plan? Why does the code not\ncomplete the task? What does the chat log and execution error imply?\nPlan: How to complete the task step by step. You should pay attention to Inventory since it\ntells what you have. The task completeness check is also based on your final inventory.\nCode:\n1) Write an async function taking the bot as the only argument.\n2) Reuse the above useful programs as much as possible.\n3) ...\n—Behaviour constraints—\nYou should only respond in the format as described below:\nExplain: ...\nPlan:\n1) ...\n2) ...\n3) ...\n37\nPreprint\n...\nCode:\n```javascript\n\/\/ helper functions (only if needed, try to avoid them)\n...\n\/\/ main function after the helper functions\nasync function yourMainFunctionName(bot) {\n\/\/ ...\n}\n```\nF.4\nRESULTS\nWe additionally provide Figure 8 and Figure 9 displaying the results of the single-round long-term\nplanning task and the dynamic-immediate planning task for easier visual inspection.\n38\nPreprint\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nx\n0\n5\n10\n15\n20\nHealth\nx\nx\nx\nx\nx\nx\n0\n5\n10\n15\n20\n25\nTime(min)\nx\n1 zombie\n1 spider\n1 skeleton\n1 zombified piglin\n1 enderman\n1 zombie, 1 spider\n1 zombie, 1 skeleton\n3 zombies\n1 zombie villager\n1 cave spider\n1 wither skeleton\ncook meat\nanimal husbandry\nTasks\n0\n25\n50\n75\n100\nLLM iters\nx\nVoyager with GPT-4-o-mini\nLLaMA-3-8B\nMineMA-8B\nFigure 8: Performance comparison of different models on the single-round long-term planning task.\n“Health” refers to the remaining health points. “# LLM iters” is the number of LLM iterations (call-\ning LLM) required to complete the task. “Time (min)” refers to the minutes spent in both gathering\nmaterials and crafting equipment to defeat different monsters. All evaluation metrics are calculated\nonly for successful tasks. ± corresponds to one standard deviation of the average evaluation over\nsuccessful tasks. Bold and italics mean the best and the second-best results. “×” indicates that\nhealth is not a relevant metric in the cook meat and animal husbandry scenarios, or all tasks fail.\n39\nPreprint\n0\n20\n40\n60\n80\n100\nSuccess Rate (%)\nx\nx\nx\nx\nx\nx\n0\n10\n20\n30\nTime (min)\nx\nx\nx\nx\nx\nx\nCollect Seeds\nHoe Farmland\nShear Sheep\nMilk Cow\nCook Meat\nObtain Leather\nMake Sugar\nCollect Water\nTasks\n0\n20\n40\n60\n80\nLLM Iters\nx\nx\nx\nx\nx\nx\nBaichuan2-7B\nQwen2-7B\nMineMA-8B\nMineMA-70B\nFigure 9: Performance comparison of different models on the dynamic-immediate planning task.\nAll evaluation metrics are calculated only for successful tasks. “×” indicates that all tasks fail.\n40\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Odyssey: Empowering Minecraft Agents with Open-World Skills.pdf"}
{"title":"See and Think: Embodied Agent in Virtual Environment","authors":"Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Gaoang Wang","summary":"Large language models (LLMs) have achieved impressive pro-gress on several\nopen-world tasks. Recently, using LLMs to build embodied agents has been a\nhotspot. This paper proposes STEVE, a comprehensive and visionary embodied\nagent in the Minecraft virtual environment. STEVE comprises three key\ncomponents: vision perception, language instruction, and code action. Vision\nperception involves interpreting visual information in the environment, which\nis then integrated into the LLMs component with agent state and task\ninstruction. Language instruction is responsible for iterative reasoning and\ndecomposing complex tasks into manageable guidelines. Code action generates\nexecutable skill actions based on retrieval in skill database, enabling the\nagent to interact effectively within the Minecraft environment. We also collect\nSTEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge\nquestion-answering pairs, and 200+ skill-code pairs. We conduct continuous\nblock search, knowledge question and answering, and tech tree mastery to\nevaluate the performance. Extensive experiments show that STEVE achieves at\nmost 1.5x faster unlocking key tech trees and 2.5x quicker in block search\ntasks.","url":"http:\/\/arxiv.org\/abs\/2311.15209v3","pdf_url":"http:\/\/arxiv.org\/pdf\/2311.15209v3","published":1700980696000,"comment":"ECCV 2024. First three authors contribute equally to this work.\n  Project Website https:\/\/rese1f.github.io\/STEVE\/","pdf_text":"See and Think: Embodied Agent in Virtual\nEnvironment\nZhonghan Zhao1,♠\n, Wenhao Chai2,♠\n, Xuan Wang1,♠\n, Boyi Li1\n,\nShengyu Hao1\n, Shidong Cao1\n, Tian Ye3\n, and Gaoang Wang1,†\n1 Zhejiang University\n2 University of Washington\n3 Hong Kong University of Science and Technology (GZ)\nAbstract. Large language models (LLMs) have achieved impressive pro-\ngress on several open-world tasks. Recently, using LLMs to build embod-\nied agents has been a hotspot. This paper proposes STEVE, a compre-\nhensive and visionary embodied agent in the Minecraft virtual environ-\nment. STEVE comprises three key components: vision perception, lan-\nguage instruction, and code action. Vision perception involves interpret-\ning visual information in the environment, which is then integrated into\nthe LLMs component with agent state and task instruction. Language\ninstruction is responsible for iterative reasoning and decomposing com-\nplex tasks into manageable guidelines. Code action generates executable\nskill actions based on retrieval in skill database, enabling the agent to\ninteract effectively within the Minecraft environment. We also collect\nSTEVE-21K dataset, which includes 600+ vision-environment pairs, 20K\nknowledge question-answering pairs, and 200+ skill-code pairs. We con-\nduct continuous block search, knowledge question and answering, and\ntech tree mastery to evaluate the performance. Extensive experiments\nshow that STEVE achieves at most 1.5× faster unlocking key tech trees\nand 2.5× quicker in block search tasks.\nKeywords: Open-world embodied agent · Multimodal pre-training ·\nLarge language model\n1\nIntroduction\nDesigning agents that demonstrate intelligent behavior and adaptability in open-\nworld settings has been a longstanding and significant challenge in the field of ar-\ntificial intelligence [13,14,26,46,70]. However, recent progress in the development\nof large language models (LLMs) [5, 55] has exhibited their potential as versa-\ntile, general-purpose assistants. Recent innovations in agent design [57,59,69,77]\n♠\nEqual\ncontribution:\nzhaozhonghan@zju.edu.cn,\nwchai@uw.edu,\nxu-\nanw@zju.edu.cn.\n† Corresponding author: gaoangwang@intl.zju.edu.cn.\narXiv:2311.15209v3  [cs.AI]  9 Jul 2024\n2\nZ. Zhao et al.\nVision Perception\nLanguage Instruction\nCode Action\nSTEVE-21K \nDataset\nVision-\nEnvironment\nQuestion-\nAnswering\nSkill-Code\ntrain\nfinetune\nretrieval\nTasks in Minecraft\nContinuous Block Search\nKnowledge QA\nTech Tree Mastery\nWooden\nStone\nIron\nDiamond\nHow to craft a \nstone pickaxe? \nOpen your \ncrafting table…\nobserve\nsearch\nFig. 1: STEVE integrates the three parts: Vision Perception, Language Instruction,\nand Code Action, supported closely by our proposed STEVE-21K. It demonstrates\ncommendable performance on Continuous Block Search, Knowledge QA, and Tech\nTree Mastery.\nhave effectively harnessed these advanced LLMs, tapping into their extensive\nworld knowledge and reasoning abilities. This development has paved the way\nfor agents, that are autonomously driven, to formulate and implement strate-\ngies and actions across a diverse array of skills and tasks in diverse open-world\nenvironments.\nIn many open-world settings, like Minecraft, contemporary agents predomi-\nnantly use LLMs for their textual interactions. However, this reliance on text for\ncommunication poses considerable limitations in their interactions within these\nworlds including low-level regrading cases [24,56,65,66]. Minecraft, with its ex-\npansive and interactive sandbox environment [17,20], demands a variety of skills\nfrom agents, ranging from crafting basic items to executing complex tasks. Yet,\nagents driven by LLMs often generate unpredictable outputs. The effectiveness\nof their interactions is largely contingent on meticulously crafted prompts [23],\ndesigned to align the LLM’s understanding with the environmental context and\nthe intended objectives. This process of prompt engineering is not only labori-\nous but also fails to meet the goal of fostering autonomous, self-directed agents.\nFurthermore, textual communication has its limitations in naturally conveying\ncertain concepts of the world, like crafting recipes, which are often more effec-\ntively communicated through vision.\nPlayers have the distinct capability to assimilate and convey information us-\ning both visual and textual channels, significantly enhancing our interactions\nwith the world around us. Yet, the integration of LLM-based agents with mul-\ntimodal inputs in open-ended environments remains an under-explored area.\nSTEVE in STEVE-Series [71–73], is named after the protagonist of the game\n“Minecraft,”. It is our proposed framework to build an embodied agent based\non the vision model and LLMs within an open world, as illustrated in Fig. 1.\nSTEVE harnesses a vision model to perceive its surroundings visually, coupled\nwith an LLM to strategize and plan actions. This model represents a leap for-\nward in agent design, combining these two input modes, vision, and text, to\noffer a more nuanced and comprehensive understanding of the environment and\npractical and executable skills.\nSTEVE\n3\nOur key contributions are outlined as follows:\n• We propose STEVE, an embodied agent in virtual environment, consists\nof vision perception, language instruction, and code action, achieving 1.5×\nfaster unlocking of key tech trees and is 2.3× quicker in block search tasks\ncompared to previous state-of-the-art methods.\n• We present STEVE-7B\/13B, a series of large language model obtained by\nfine-tuning with Minecraft knowledge question-answering pairs from Llama-\n2-7B\/13B.\n• We collect STEVE-21K dataset, including 600+ vision-environment pairs,\n20K knowledge question-answering pairs, and 200+ skill-code pairs, for jus-\ntifying the effective performance of STEVE.\n2\nRelated Works\n2.1\nIntelligent Agent in Minecraft\nAs an open-ended sandbox game, Minecraft has always been an ideal setting for\ntesting the performance of intelligent agents [21,25]. The agents are required to\nautonomously perform various tasks in Minecraft, such as chopping trees, craft-\ning tools, and mining diamonds. At the beginning, much of the works focus on\nexploring reinforcement learning [32,33,42,50] or imitation learning [2,3], with-\nout satisfactory performance. VPT [3] and MineDojo [17] collect internet-scale\ndatasets for their model pre-training. More specifically, VPT offers the exciting\npossibility of directly learning to act during video pre-training and using these\nlearned behavioral priors as extremely effective exploration priors for reinforce-\nment learning. Yet, recent works found that the pre-trained LLMs could serve\nas a strong “mind” that provides planning ability to the agents. Voyager [57]\nleverages GPT-4 [44] as both a high-level planner and a low-level action code\ngenerator. Plan4MC [69] proposes a skill graph pre-generated by the LLMs.\nDEPS [59], an interactive planning method based on LLMs, addresses multi-step\nreasoning issue in open-world planning. GITM [77] develops a set of structured\nactions and leverages LLMs to generate action plans for the agents to execute,\nachieving impressive results in various tasks.\n2.2\nEmbodied Multimodal Model\nEmbodied agent operates within various environment by synthesizing sensory\nperceptions and physical actions supported by computational intelligence. This\nsynthesis enables the agent to undertake a variety of tasks, achieving specific\nobjectives. Its key areas of application are diverse, including Navigation [6, 16,\n27, 43, 61, 68], Embodied Question Answering [10, 11, 67], Active Visual Track-\ning [37,38,74,75], and Visual Exploration [7,12,35]. The field is evolving rapidly\nwith the development of Large Language Models (LLMs) [51] and Multimodal\nLLMs (MLLMs) [1, 9, 18, 19, 28–30, 34, 39, 41, 54, 58, 64, 76], integrating multiple\nmodalities for more effective processing. A prime example of this innovation is\n4\nZ. Zhao et al.\nVision \nEncoder\n…\nVision Perception\n…\n…\nSTEVE-13B\nLanguage Instruction\n1.\nCrafting Table: \nOpen your \ncrafting table\n2.\nIngredients … \n…\nImage\nor\nVideo\n1.\nGather Wood \nLogs\n2.\nConvert Wood \nLogs into Planks\n…\nDecompose\nHigh-level\nLow-level\n…\nQuery\nEncoding\nRetrieval\nMine Wood Log\nCraft Iron Sword\nCombat Creeper\nCook Mutton\nCraft Shears\nMake Crafting Table        …\nCraft Bucket\nCatch Pufferfish\nNo crafting table nearby\nTokenizer\nCraft a bow\nAgent \nState\nTask\nSkill Database\nconst woodLogBlock = \nawait exploreUntil(bot, \nnew Vec3(1, 0, 1), 60, () => \n{\nreturn bot.findBlock({\nmatching: block => \n...\nCode Action\nFig. 2: STEVE framework. The Vision Perception part takes images or videos, en-\ncodes them into tokens, and combines them with Agent State and Task tokens as input.\nThe STEVE-13B in the Language Instruction part is used for automatic reasoning and\ntask decomposition, and it calls the Skill Database in the form of the Query to output\ncode as action.\nPaLM-E [15], a sophisticated multimodal model with 562B parameters, adept at\na broad spectrum of embodied tasks and demonstrating exceptional capabilities\nin visual reasoning.\n2.3\nLarge Language Model with Equipped Tools\nWhile Large Language Models (LLMs) demonstrate impressive skill in tackling\nnovel tasks via prompt-based instructions, they often face challenges in areas\nwhere simpler models or tools excel, like mathematical calculations or iden-\ntifying palindromes. However, LLMs’ potential is significantly expanded when\nintegrated with other modality-specific models, such as those for vision or audio,\nenabling multi-modal capabilities [4,36]. Innovations like Toolformer [47] demon-\nstrate LLMs’ self-learning to utilize tools through finetuning with extensive API\ncall samples. Visual ChatGPT [62] extends this by integrating various visual\nfoundation models, facilitating interactive user experiences with ChatGPT. Simi-\nlarly, HuggingGPT [48] presents a framework that harnesses LLMs to link diverse\nmodels from Hugging Face for task resolution. AutoGPT [49] is an open-source\napplication that broadens GPT-4’s capabilities with internet access, memory\nmanagement, and plug-ins. The recent introduction of MovieChat [52,53] brings\na memory mechanism to MLLM, enhancing its performance in video under-\nstanding tasks. Furthermore, LLMs can be used for goal planning, analogous to\nlanguage translation [63]. This evolving landscape suggests that tool-equipped\nLLMs could forge a new paradigm in AI solution design.\n3\nMethod: STEVE\n3.1\nOverview\nAs shown in Figure 2, STEVE F is an LLM-based multi-modal autonomous\nsystem for embodied agents in Minecraft, which can use visual status Xv in the\nSTEVE\n5\nform of images or videos and agent state Xs to manage and execute complex\ntask Xt for executive code action ac:\n  \\ mathb f { a}^ c  = \\mathcal {F} (X^v, X^s, X^t) = \\mathcal {A}^c(\\mathcal {I}^l(\\mathcal {P}^v(X^v, X^s, X^t))) \n(1)\nwhere STEVE F integrates Visual Perception Pv into the Language Instruction\nIl with large language model (LLM) and combines them with Code Action Ac,\na skill retrieval method to execute actions, details are demonstrated as follows.\n• Vision Perception Pv (Section 3.2); a vision encoder to interpret vi-\nsual information of the environment, such as blocks and entities and a text\ntokenizer for agent state s and task t.\n• Language Instruction Il (Section 3.3): a powerful language model sys-\ntem fine-tuned specifically for Minecraft content using LLaMA2-13B [18].\nThis model enables adaptive interaction for iterative reasoning and step-by-\nstep decomposition.\n• Code Action Ac (Section 3.4): a skill retrieval method based on our\nskill-code database.\n3.2\nVision Perception Pv\nThe vision perception part includes a vision encoder Ev and a text tokenizer\nT, which converts the visual status Xv, agent state Xs, and task Xt to the\ntext-space tokenizer representation Y = {Y v, Y s, Y t}:\n  \\ mathb\nf  {Y\n} _ i \n= \\ ma t hcal  {P}^v(X^v_i, X^s_i, X^t_i), \\mathcal {P}^v = \\{E^v, T\\}, \n(2)\nwhere the vision encoder E, the visual branch of EfficientFormer [31], encodes\nvisual status Xv\ni at each step i into visual tokens Y v = {yv\n1, yv\n2, ..., yv\nn} ∈Rn×d,\nwhere n denotes the number of visual tokens yv and d is the dimensionality of\neach token. The text tokenizer converts agent state Xs\ni and task Xt\ni at each step\ni in the form of text into textual tokens Y s and Y t. Note that visual tokens are\namalgamated with textual tokens representing the agent’s current state (e.g.,\nhealth, inventory, etc.) and the task description. This is accomplished using a\ntokenizer that maps state variables and task parameters to a tokenized form.\nThe resultant unified token set serves as a comprehensive representation of the\ncurrent situational context.\n3.3\nLanguage Instruction Il\nThe Language Instruction Il = {Pl, Cr, Cu, Ds}, which consists of Planner Pl,\nCritic Cr, Curriculum Cu, and Describer Ds, i.e., four independent LLM-based\nagents with different functions. They formulate high-level task guidelines, re-\nfine strategies through feedback, facilitate continuous learning and adaptation\nthrough a curriculum of complex tasks, and finally decompose strategic guide-\nlines into the executable low-level textual action step as, {as ∼Il(·|Yi)}N\ni=0\ntowards efficient completion of the task.\n6\nZ. Zhao et al.\nNote that the Planner Pl, Critic Cr, Curriculum Cu and Describer Ds are\nbased on STEVE-7B\/13B, a powerful language model derived from LLaMA-2-\n13B [18], fine-tuned specifically on Minecraft-related content from the STEVE-\n21K. This model’s expertise covers a broad spectrum of game-specific knowledge\nareas on worlds, entities, player mechanics, survival, and even game practical\nexperience. Finally, they have the performance of their different functions:\n• Planner Pl: formulating comprehensive guidelines and executive plans that\nalign with the overarching objectives of the task.\n• Critic Cr: evaluating the planner decisions, providing feedback that can\nrefine strategies.\n• Curriculum Cu: facilitating continuous learning and adaptation for action\nagents by engaging with a series of progressively complex tasks.\n• Describer Ds: distilling the extensive data into a concise summary, making\nit more manageable and interpretable.\nIterative reasoning. The STEVE-13B receives a stream of tokens that encode\nthe current visual scene, the agent’s state, and the task’s textual description.\nSTEVE-13B interprets this rich context to undertake complex reasoning. The\nmodel initiates the reasoning process by constructing a series of high-level strate-\ngies that outline the pathway to task completion. Considering all gameplay el-\nements, the reasoning mechanism is akin to an experienced player who can vi-\nsualize the end game and chart a course to victory. This approach ensures the\nplans are reactive and proactive, allowing the agent to anticipate and mitigate\nfuture challenges. However, most strategies are high-level and abstract therefore,\nthey often require step-by-step decomposition to derive executable guidelines.\nDecomposition. The decomposition process makes the complex strategies break\ndown into simple, low-level guidelines that can be directly mapped to actions\nin Minecraft. It is similar to how a high-level command like “build a shelter”\nis divided into actionable instructions like “collect wood”, “craft planks”, and\n“place blocks”. The granular steps are structured to provide explicit instructions\nthat the game engine can readily interpret. This systematic breakdown from\nhigh-level reasoning to low-level actionable steps is the hallmark of the STEVE\nsystem, enabling the embodied agent to interact with the Minecraft environment\nin a meaningful and goal-oriented manner. Through this intricate process of rea-\nsoning and decomposition, STEVE embodies the cognitive capabilities required\nfor sophisticated task management in virtual environments.\nCurriculum learning with memory. We draw inspiration from the lifelong learn-\ning strategy utilized in many reinforcement learning problems [57] in both closed-\nworld and open-world settings. We start by creating a set of tasks that serve as a\ncurriculum for agents to explore the environment. During this process, STEVE\ngenerates plans, interacts with the surroundings, learns from mistakes, and stores\nall these experiences in memory. Next, we evaluate STEVE on various tasks af-\nter this learning stage. Consequently, STEVE can produce better plans with its\nSTEVE\n7\nmemory teaming up with the planning experiences. We use this as the default\nsetting for all tasks in our experiments.\nContinous learning with summarization.\nWe’ve observed that the learning\nprocess, where the memory is being filled, can continue throughout the gameplay.\nThe agent can gradually acquire more skills as the gameplay progresses and more\nexperiences are gained. However, as the memory gets larger, it becomes difficult\nto understand the game’s situation and interact with the game slowly. To tackle\nthese challenges, we have implemented the Chain of Summarization method [40].\nBy finding better references, we can improve our ability to handle challenging\ntasks, such as “Diamond Tool” in the tech tree, including obtaining materials\nand creating diamond tools. This will lead to a higher success rate, as shown in\nSection 5.3. Additionally, curriculum learning with memory allows for in-context\nlifelong learning without needing gradient updates.\n3.4\nCode Action Ac\nThe code action part Ac is the execution phase, where the STEVE system con-\nverts planned, decomposed guidelines into concrete actions within the Minecraft\nenvironment. This process leverages a specialized skill database that pairs code\nsnippets with their descriptions and relevant metadata, encoded as vectors vs for\nefficient retrieval. The transition from language instruction to executable code\nis achieved through the retrieval process R:\n  \\m at h cal {R} ( \\ mathbf {q}, \\mathbf {v}) = \\sigma (\\mathbf {q}, \\mathbf {v}), \\mathbf {q} = E^q(\\mathbf {a^s}), \n(3)\nwhere Eq and σ are query encoding and cosine similarity matching. Each low-\nlevel textual action step as derived from the Language Instruction is encoded\ninto a query q. This encoding captures the essence of the action to be performed\nin a format that the skill database can understand. Once the queries q are\nencoded, the system computes similarities between the query vectors and the\ncode snippets vectors v stored in the database to determine which skill best\nmatches the required action.\n3.5\nTraining\nTo reduce the training overhead to a certain extent, we adopt a two-stage training\nmethod, the first stage being a warm-up on the question-answering pairs of\nSTEVE-21K to ensure a certain degree of instruction capability. The second\nstage is simulated in the environment, and the Expert LLM Ep integrated with\nGPT-4 [44] generates instructions for the same situation to modify our model.\nIn training, we use either a single-round conversation {XQ, XA} or a multi-\nround conversation {XQ\ni , XA\ni }i≤N, where N is the total number of rounds in the\nconversation. We use the negative log-likelihood objective over the prediction\n8\nZ. Zhao et al.\ntokens with training and finetuning:\n  \\m a t\nh\nc\nal \n{L} (\\theta ) =-\\sum _{j=1}^{L} \\log \\mathcal {F}_{\\theta }(Y_j|X^v, \\hat {Y}_{1:j-1}), \n(4)\nwhere Y and ˆY refer to the non-vision input and target token sequences, θ repre-\nsents the model parameters, and L represents the length of the target sequence.\nThe vision input Xv varies depending on the input step. We only consider the\nanswer tokens XA when computing the loss to ensure that the model focuses on\ngenerating coherent responses.\nOffline warm-up. In the first stage, we finetune the STEVE-7B\/13B only on\nthe single-round question-answering pairs of STEVE-21K to ensure a certain\ndegree of instruction capability, as shown in Tab. 3.\nOnline finetuning. In the second stage, we simulate the warmed-up STEVE-\n7B\/13B in Minecraft, and both train the vision encoder Ev and finetune the\nSTEVE-7B\/13B simultaneously.\nIt is necessary to obtain the environment information to train the vision en-\ncoder, denoted as l = RT (Xv), where l is the label of seen blocks and entities,\nRT is the Ray Tracing method [60] to eliminate all content outside the agent’s\nperspective screen. We train the vision encoder Ev once we have obtained the\nenvironment information. To finetune the warmed-up STEVE-7B\/13B in simu-\nlation, we give both the STEVE-7B\/13B and the Expert LLM the same input,\nand we consider the output of the Expert LLM as the ground truth label.\nAfter 5,000 simulations, we collect all the context information from success-\nful runs, including sequences of vision and question-answering chat flow as the\nVision-Environment part of STEVE-21K, as mentioned in Section 4.\n4\nDataset: STEVE-21K\nAs shown in Fig. 3, STEVE-21K has been meticulously compiled, featuring a\ndiverse collection of Vision-Environment pairs, Question-Answering pairs, and a\nSkill-Code database.\nVision-Environment pairs contain 600 pairs of first-person perspective videos\nfrom Minecraft gameplay across six different terrains (including forest, desert,\ncoastal, etc.), along with corresponding environmental block entities in the field\nof vision and context information for each timestamp. Additionally, all pairs\nare oriented around executing the skill-related task supported by Skill-Code\npart mentioned in Section 4. We employ the STEVE-7B\/13B model to enable\nrobots to plan and execute actions autonomously based on tasks defined by\nhuman supervisors. We record the video of the agent operation, the environment\ninformation, and all the corresponding chatflow.\nQuestion-Answering pairs contain 20K question-answering pairs from the\nMinecraft-Wiki and Reddit corpus across six data types partly sourced from [17].\nThe pairs are organized into instruction, input, and output triplets and used to\nSTEVE\n9\nVision-Environment\n20K Single-round QA pairs in\nQuestion-Answering\nSkill-Code\nHuman Player\nModels\nMinecraft World\nPrismarine Viewer\n…\nVision Record\nRay Tracing\nWikipedia\nReddit\nChatGPT\nEnv. Record\n1. World & Entities \n2. Player Mechanics & Survival \n3. Knowledge & Discovery \n4. Resources & Crafting \n5. Tools & Utilities\n6. Miscellaneous\nSingle-round QA pairs\nHuman Player\nChatGPT\nprompt\ncode snippets\noutput\nexecute\ncheck and revise\nBlocks: stone, dead \nbushes, cacti, orange \nterracotta...\nClean\nConstruct\nCheck\nFig. 3: STEVE-21K collection pipeline. In the Vision-Environment section,\nSTEVE-13B plays the game according to specified tasks defined by the human player,\ncollecting visual information through prismarine-viewer and capturing environmental\ninformation from the screen using Ray Tracing [60]. Note that the language instruction\ntask is also performed during the collection phase. We simultaneously record and save\nthe chat flow from the reasoning and decomposition stages. In the Question-Answering\nsection, we obtain information from the Minecraft-Wiki and Reddit forums and use\nGPT-3.5 to clean the data into single-round QA pairs. In the Skill-Code section, we\nuse GPT-3.5 combined with the human player’s code to synthesize code snippets and\nthen check and revise them in the game environment.\ntrain STEVE-13B. The GPT-3.5 [5] is employed to derive meaningful single-\nround question-answer pairs, and LoRA [22] is incorporated during the fine-\ntuning process for efficient resource allocation.\nSkill-Code pairs contain 210 skill execution scripts with descriptions, covering\n8 skill types including collecting, crafting, exploration etc. The code part is\ncollected by manual coding. We use GPT-3.5 [5] to describe all codes and utilize\nlangchain vectordb to give all pairs a database vector.\n5\nExperiments\n5.1\nExperimental Setup\nWe train STEVE-7B\/13B, which is finetuned from LLaMA-2 [18] with the\nQueston-Answering pair in STEVE-21K dataset for warm-up and simulation\ncontext data from successful runs. We use LoRA [22] for finetuning process.\nNote that the process is to adjust STEVE-13B to work on correct simulation\nknowledge while remaining adapted to visual perception. In the text part, we set\nall temperatures to 0 except for the task proposal, which uses 0.9 to encourage\ntask diversity. The vision unit is based on EfficientFormerV2-S0 [31], which is\ntrained on the Vision-Environment part of our STEVE-21K dataset. Our simula-\ntion environment is built on top of MineDojo [17] and leverages Mineflayer [45].\n10\nZ. Zhao et al.\nContext\n[Question] How to \nsurvive, according to \nthe current situation?\n[Answer] Dealing with \nthe Creeper and \nHealth Management...\nChat Log\nA Creeper is \nNearby, please \ncombat Creeper\nwith the iron \nsword first.\nPlan\nUse `combatOneCreeper` \nfunction to combat the \nCreeper until the Creeper is \neliminated. Once the \nCreeper is eliminated, call \nthe `eatCarrot` ...\nEnvironment \n[Blocks] stone, \ndead bushes, \ncacti, orange \nterracotta, ...\n[Equipment] \niron sword\n[Entities]\ncreeper, Rabbit\n[Health] 15\/20\n[Hunger] 18\/20\n[Inventory]\nCarrots, torches\nFig. 4: Example of Vision-Environment pairs. It represents the data format of\nthe Vision-Environment pairs in our STEVE-21K dataset: including visual signals,\nenvironmental information, Chat Log, Context QA pairs, and planning in actual tasks.\nWe use GPT-4-0613 for all GPT-4 models used in Voyager [57] and code gener-\nation tasks.\n5.2\nBaselines\nAs no vision-based LLM-driven agents are immediately operable in Minecraft, we\nselected several algorithms as baselines that extract information from a system’s\nbackend, differing significantly from real-world applications.\nAutoGPT [49] is an NLP automation tool that decomposes a high-level goal\ninto executable subgoals in MineDojo, aligning with our experimental frame-\nwork. Our setup, AutoGPT, powered by GPT-4 [44], processes agent states,\nenvironment feedback, and execution errors to manage subgoal execution.\nVoyager [57] relies solely on textual grounding for perception and features a long-\nterm procedural memory with a hierarchical library of code-based procedures,\nallowing the integration of simple skills into complex behaviors. Proficient in\nenvironment exploration and tech tree mastery, Voyager uses GPT-4 [44] for\nprocessing background text in embodied agents, with a lesser emphasis on visual\nperception.\nSTEVE\n11\nTable 2: Comparison on tech tree mastery task. The values presented are in frac-\ntions, representing successful trials out of three attempts. A score of 0\/3 signifies the\nmethod’s inability to progress within the tech tree after a maximum of 160 prompt-\ning iterations. The reported numbers denote the average iterations across three trials.\nLower iteration values indicate higher efficiency of the respective method.\nMethod\nWooden Tool Stone Tool\nIron Tool\nDiamond Tool\nAutoGPT [49] 92 ± 72 (3\/3) 94 ± 72 (3\/3) 135 ± 103 (3\/3)\nN\/A (0\/3)\nVoyager [57]\n6 ± 2 (3\/3)\n11 ± 2 (3\/3)\n21 ± 7 (3\/3)\n102 (1\/3)\nSTEVE\n4 ± 1 (3⁄3)\n8 ± 1 (3⁄3)\n15 ± 2 (3⁄3)\n106 ± 12 (3⁄3)\nTable 3: Quantitive comparison on knowledge question and answering task.\nQuestions, model-generated responses, and ground truth inputs are evaluated in GPT-\n4 [5], Claude-2 [8] and human blind rating rated on a scale of 0 to 10; The scores above\nare the average of them. Higher scores indicate greater alignment of the generated\nanswers with the ground truth. Wld., Ent., Mech., Surv., Know., Disc., Res., Craft.,\nTools, Util., Miscell. stand for World, Entities, Player Mechanics, Survival, Knowledge,\nDiscovery, Resources, Crafting, Tools, Utilities and Miscellaneous.\nMethod\nWld. & Ent. Mech. & Surv. Know. & Disc. Res. & Craft. Tl. & Util. Miscell. Overall\nLlama2-7B\n6.44\n6.68\n6.58\n6.42\n6.80\n6.96\n6.56\nLlama2-13B\n6.93\n6.95\n6.77\n6.77\n6.98\n6.64\n6.89\nSTEVE-7B\n7.99\n7.88\n7.84\n7.95\n7.93\n7.82\n7.94\nSTEVE-13B\n8.14\n8.13\n8.03\n8.15\n8.12\n7.72\n8.12\nGPT-4\n8.06\n8.07\n8.07\n7.92\n8.09\n8.21\n8.04\n5.3\nEvaluation Results\nTable\n1:\nComparison\non\ncontinues\nblock search task. # Iters stand for av-\nerage iterations to find 10 diamonds (max\n100). # Blocks stand for average diamonds\nfound in 100 iterations.\nMethod\n# Iters (↓) # Blocks (↑)\nAutoGPT [49]\nN\/A\n7\nVoyager [57]\n35\n26\nSTEVE\n14\n67\nContinuous block search.\nAs shown\nin Tab. 1, we experiment with block-\nsearching tasks to assess the agent’s ex-\nploratory capabilities and proficiency\nin locating specified blocks. Diamond\nblocks are placed at every 16-block in-\nterval across the land map. The agent’s\nobjective is to identify as many blocks\nas possible within the fewest iterations,\nwhich indicates the method’s efficiency.\nAs shown in Fig. 5, enriching information through visual perception significantly\nenhances the efficiency of search and exploration tasks, leading to more effective\nworld exploration.\n12\nZ. Zhao et al.\nSTEVE\nVoyager\nAutoGPT\nFig. 5: Schematic of the continuous block search task. We capture an asyn-\nchronous segment with each method 30 iterations from the experiments for illustration.\nThe reason we choose diamond blocks is that they are not naturally occurring in the\ngiven context, making them easily distinguishable from other blocks.\nKnowledge question and answering. Using a validation dataset, we established a\nquestion-answering database to evaluate our model’s performance on Minecraft-\nrelated queries. Responses from each model are rated blindly by GPT-4, Claude-\n2, and human participants based on accuracy, relevance, and detail. Responses\nare initially checked for accuracy, and the comprehensive evaluation yields a\nscore ranging from 0 to 10, with higher scores indicating superior performance.\nAs detailed in Tab. 3, we tested the instructional capabilities of various LLM\nmodels using the STEVE-21k dataset, divided into 18, 622 training and 1, 000\ntesting samples. The STEVE-7B and STEVE-13B models surpassed LLaMA2\nin all metrics, with STEVE-13B scoring the highest at 8.12, demonstrating\nits exceptional understanding of Minecraft-related queries. This indicates that\nSTEVE models, optimized specifically for Minecraft content, perform better in\nknowledge-intensive tasks.\nOur results confirm that larger, domain-specifically tuned models like STEVE-\n13B outperform broader models like GPT-4, underscoring the benefit of special-\nized fine-tuning for domain-specific applications.\nSTEVE\n13\nHow to restore full \nhunger efficiently? \nYou should eat 1 bread and 1 \napple. The bread will restore \n5 hunger points, and the apple \nwill restore 4 hunger points \nadding up to the 6 hunger\npoints you need to have a full \nhunger bar...\nSimply eat one bread from \nyour inventory. It will \nincrease your hunger level by \n5 points, which is precisely \nwhat you need to reach 20...\nYou should eat an Apple or 2 \nBreads. Both Apples and \nBreads restore 5 hunger \npointswhich will bring your \nhunger level back to full.\nShould I combat zombie \nand how to survive?\nYou should combat the \nzombie using your iron \nsword. Zombies are relatively \nweak and should not pose a \nsignificant threat to you...\nYou should combat the \nzombie; remember to strike \nand step back to avoid getting \nhit. You seem to be in an \nabandoned mineshaft. Once \nthe area is safe, continue to \nexplore...\nyou should combat the \nzombie and collect resources \nto craft better equipment. The \ncurrent environment is likely \na village or a farm...\nWhat do you find and how \nto get more resources?\nYou should open the nearby \nChest, or explore the \nsurrounding area to find \nbetter resources...\nBegin by inspecting the \nnearby Chest, it could contain \nvaluable resources. Use your \nIron Pickaxe to mine any \nprecious ores you encounter \nin the mineshaft...\nYou should find more \nCobblestones to craft better \narmor and weapons. The \nnearby blocks and entities \nsuggest that you are in a \nMinecraft world, and the \nresources you need are likely \nto be found in the nearby \narea...\nShould I combat skeleton \nand how to survive?\nGiven your current situation, \nit is not advisable to engage \nin combat with skeletons or \nany other hostile mobs...\nYou should avoid engaging \nthe skeleton at the moment\nsince your health is a bit low\nand you have no armor to \nprotect yourself. You should \ninstead focus on your \nimmediate safety...\nYou should combat the \nskeleton. To survive, it is \nrecommended to prioritize\ncrafting better equipment, \nsuch as armor and weapons, \nto increase your health and \ncombat effectiveness...\nFig. 6: Qualitative comparison on knowledge question and answering tasks.\nGreen ( Red ) highlights the correct or good (wrong or bad) answer. Blue indicates\nthe suboptimal answer. Grey indicates the meaningless answer.\nTech tree mastery. As shown in Tab. 2, we experiment on the Minecraft tech\ntree mastery to test the agent’s ability to craft and use a hierarchy of tools. Pro-\ngressing through this tree (wooden tool →stone tool →iron tool →diamond\ntool) requires the agent to master systematic and compositional skills. As to\nthe wooden, stone, and iron levels of the tech tree, STEVE achieves remarkable\nefficiency: 23×, 11.8×, and 9× faster than AutoGPT [49], and 1.5×, 1.4×, and\n1.3× faster than Voyager [57]. STEVE has achieved the diamond level, as shown\nin Tab. 2. Its performance slightly lags behind Voyager [57], which also uses\nGPT4 [44] for critical inference. However, STEVE is more cost-effective, start-\ning with lower initial performance. It includes a vision unit, prioritizing visual\ndata over background information, offering distinct advantages. Additionally, we\nobserved a decrease in performance when using a basic skill database.\n5.4\nAblation Study\nTo understand the impact of different components on the performance of our\nsystem, we conducted ablation studies focusing on the tech tree mastery task in\nMinecraft. The results, as shown in Tab. 4, provide insights into the effectiveness\nof the vision unit and compare our STEVE model with the STEVE GPT-4\nversion (with the same vision unit as ours). Note that the w\/o vision unit setup\nis that the environmental perception encompasses data on blocks within an 8x8\narea surrounding the agent, including the front, back, left, and right directions.\nThe following observations are made:\n14\nZ. Zhao et al.\nTable 4: Ablation studies for the tech tree mastery. STEVE (Ours) is the\nSTEVE-13B version. The 0\/3 score means the method can’t progress beyond 160 iter-\nations in the tech tree.\nMethod\nWooden Tool Stone Tool\nIron Tool\nDiamond Tool\nw\/o vision unit\n11 ± 5 (3\/3)\n27 ± 5 (3\/3) 46 ± 11 (3\/3)\n158 (1\/3)\nSTEVE (GPT-4)\n6 ± 2 (3\/3)\n10 ± 1 (3\/3) 14 ± 3 (3⁄3)\n89 ± 9 (3⁄3)\nSTEVE (Ours)\n4 ± 1 (3⁄3)\n8 ± 1 (3⁄3) 15 ± 2 (3\/3) 106 ± 12 (3\/3)\nVision unit is critical. The omission of the vision unit markedly affects the\nsystem’s performance, especially in more advanced tasks. While it successfully\ncrafts Wooden, Stone, and Iron Tools, it is challenged with Diamond Tools. This\noutcome underscores the vital importance of visual information in accomplishing\ncomplex tasks.\nComparison with GPT-4. As our vision encoder directly encodes into text space,\nit can be easily replaced with any language model. For instance, the GPT-4 we\ncompared exhibits consistent success across all categories and secures a flawless\nsuccess rate. Interestingly, the STEVE-13B version excels in simpler tasks such\nas crafting wooden and stone tools. Moreover, it requires fewer iterations than\nmethods without the vision part, underscoring its superior efficiency.\n5.5\nCase Study\nAs shown in Fig. 6, we perform an extensive case study comparison of GPT-4,\nLLaMA2-13B, and our method STEVE-13B. Each model maintains the same\ninformation and question inputs to compare feedback under different environ-\nmental information. Our STEVE overall achieves the best results, surpassing\nGPT-4 and showing significant improvement compared to the original LLaMA.\nEspecially in parts involving numerical calculations, such as the leftmost image,\nSTEVE accurately tracks food values to restore hunger levels.\n6\nConclusion\nSTEVE enhances multi-modal learning by combining visual encoder and LLM-\nbased agents. It has three functions: vision perception, language instruction,\nand code action, allowing it to understand, predict, and act in virtual envi-\nronments. We provide a straightforward approach to creating a robust, multi-\nmodal, autonomous, embodied agent using an open-source language model with\na small number of parameters. Additionally, we provide a comprehensive dataset\nSTEVE-21K for sustainable community development that can be verified.\nSTEVE\n15\nAcknowledgement\nThis work is supported by the Zhejiang Provincial Natural Science Foundation\nof China (No. LZ24F030005) and the National Natural Science Foundation of\nChina (No. 62106219).\nReferences\n1. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Men-\nsch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems 35, 23716–\n23736 (2022)\n2. Amiranashvili, A., Dorka, N., Burgard, W., Koltun, V., Brox, T.: Scaling imitation\nlearning in minecraft. arXiv preprint arXiv:2007.02701 (2020)\n3. Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton,\nB., Sampedro, R., Clune, J.: Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. Advances in Neural Information Processing Systems 35,\n24639–24654 (2022)\n4. Chai, W., Wang, G.: Deep vision multimodal learning: Methodology, benchmark,\nand trend. Applied Sciences 12(13), 6588 (2022)\n5. Introducing chatgpt (2022), https:\/\/openai.com\/blog\/chatgpt\n6. Chen, S., Guhur, P.L., Schmid, C., Laptev, I.: History aware multimodal trans-\nformer for vision-and-language navigation. Advances in neural information pro-\ncessing systems 34, 5834–5847 (2021)\n7. Chen, T., Gupta, S., Gupta, A.: Learning exploration policies for navigation. In:\nInternational Conference on Learning Representations (2018)\n8. Talk to claude (2023), https:\/\/claude.ai\n9. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,\nS.: Instructblip: Towards general-purpose vision-language models with instruction\ntuning. arXiv preprint arXiv:2305.06500 (2023)\n10. Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied question\nanswering. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition. pp. 1–10 (2018)\n11. Datta, S., Dharur, S., Cartillier, V., Desai, R., Khanna, M., Batra, D., Parikh, D.:\nEpisodic memory question answering. In: Proceedings of the IEEE\/CVF Confer-\nence on Computer Vision and Pattern Recognition. pp. 19119–19128 (2022)\n12. Dean, V., Tulsiani, S., Gupta, A.: See, hear, explore: Curiosity via audio-visual\nassociation. Advances in neural information processing systems 33, 14961–14972\n(2020)\n13. Deng, J., Chai, W., Guo, J., Huang, Q., Hu, W., Hwang, J.N., Wang, G.: Citygen:\nInfinite and controllable 3d city layout generation. arXiv preprint arXiv:2312.01508\n(2023)\n14. Deng, J., Chai, W., Huang, J., Zhao, Z., Huang, Q., Gao, M., Guo, J., Hao, S.,\nHu, W., Hwang, J.N., et al.: Citycraft: A real crafter for 3d city generation. arXiv\npreprint arXiv:2406.04983 (2024)\n15. Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A.,\nTompson, J., Vuong, Q., Yu, T., et al.: Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378 (2023)\n16\nZ. Zhao et al.\n16. Du, H., Yu, X., Zheng, L.: Vtnet: Visual transformer network for object goal nav-\nigation. In: International Conference on Learning Representations (2020)\n17. Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang,\nD.A., Zhu, Y., Anandkumar, A.: Minedojo: Building open-ended embodied agents\nwith internet-scale knowledge. Advances in Neural Information Processing Systems\n35, 18343–18362 (2022)\n18. Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He,\nC., Yue, X., et al.: Llama-adapter v2: Parameter-efficient visual instruction model.\narXiv preprint arXiv:2304.15010 (2023)\n19. Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W.,\nLuo, P., Chen, K.: Multimodal-gpt: A vision and language model for dialogue with\nhumans. arXiv preprint arXiv:2305.04790 (2023)\n20. Guss, W.H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., Salakhutdi-\nnov, R.: Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint\narXiv:1907.13440 (2019)\n21. Hofmann, K.: Minecraft as ai playground and laboratory. In: Proceedings of the\nannual symposium on computer-human interaction in play. pp. 1–1 (2019)\n22. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021)\n23. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tomp-\nson, J., Mordatch, I., Chebotar, Y., et al.: Inner monologue: Embodied reasoning\nthrough planning with language models. arXiv preprint arXiv:2207.05608 (2022)\n24. Jiang, J., Ye, T., Bai, J., Chen, S., Chai, W., Jun, S., Liu, Y., Chen, E.: Five a+\nnetwork: You only need 9k parameters for underwater image enhancement. arXiv\npreprint arXiv:2305.08824 (2023)\n25. Johnson, M., Hofmann, K., Hutton, T., Bignell, D.: The malmo platform for arti-\nficial intelligence experimentation. In: Ijcai. pp. 4246–4247 (2016)\n26. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Deitke,\nM., Ehsani, K., Gordon, D., Zhu, Y., et al.: Ai2-thor: An interactive 3d environment\nfor visual ai. arXiv preprint arXiv:1712.05474 (2017)\n27. Kwon, O., Park, J., Oh, S.: Renderable neural radiance map for visual navigation.\nIn: Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 9099–9108 (2023)\n28. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal\nmodel with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)\n29. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597 (2023)\n30. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In: International Con-\nference on Machine Learning. pp. 12888–12900. PMLR (2022)\n31. Li, Y., Yuan, G., Wen, Y., Hu, J., Evangelidis, G., Tulyakov, S., Wang, Y., Ren,\nJ.: Efficientformer: Vision transformers at mobilenet speed. Advances in Neural\nInformation Processing Systems 35, 12934–12949 (2022)\n32. Lifshitz, S., Paster, K., Chan, H., Ba, J., McIlraith, S.: Steve-1: A generative model\nfor text-to-behavior in minecraft (abridged version). In: NeurIPS 2023 Workshop\non Goal-Conditioned Reinforcement Learning (2023)\n33. Lin,\nZ.,\nLi,\nJ.,\nShi,\nJ.,\nYe,\nD.,\nFu,\nQ.,\nYang,\nW.:\nJuewu-mc:\nPlaying\nminecraft with sample-efficient hierarchical reinforcement learning. arXiv preprint\narXiv:2112.04907 (2021)\nSTEVE\n17\n34. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint\narXiv:2304.08485 (2023)\n35. Liu, S., Okatani, T.: Symmetry-aware neural architecture for embodied visual ex-\nploration. In: 2022 IEEE\/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR). pp. 17221–17230. IEEE (2022)\n36. Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.W., Wu, Y.N., Zhu, S.C., Gao,\nJ.: Chameleon: Plug-and-play compositional reasoning with large language models.\narXiv preprint arXiv:2304.09842 (2023)\n37. Luo, W., Sun, P., Zhong, F., Liu, W., Zhang, T., Wang, Y.: End-to-end active\nobject tracking via reinforcement learning. In: International conference on machine\nlearning. pp. 3286–3295. PMLR (2018)\n38. Luo, W., Sun, P., Zhong, F., Liu, W., Zhang, T., Wang, Y.: End-to-end active\nobject tracking and its real-world deployment via reinforcement learning. IEEE\ntransactions on pattern analysis and machine intelligence 42(6), 1317–1332 (2019)\n39. Lyu, C., Wu, M., Wang, L., Huang, X., Liu, B., Du, Z., Shi, S., Tu, Z.: Macaw-llm:\nMulti-modal language modeling with image, audio, video, and text integration.\narXiv preprint arXiv:2306.09093 (2023)\n40. Ma, W., Mi, Q., Yan, X., Wu, Y., Lin, R., Zhang, H., Wang, J.: Large language\nmodels play starcraft ii: Benchmarks and a chain of summarization approach. arXiv\npreprint arXiv:2312.11865 (2023)\n41. Maaz, M., Rasheed, H., Khan, S., Khan, F.S.: Video-chatgpt: Towards de-\ntailed video understanding via large vision and language models. arXiv preprint\narXiv:2306.05424 (2023)\n42. Mao, H., Wang, C., Hao, X., Mao, Y., Lu, Y., Wu, C., Hao, J., Li, D., Tang, P.:\nSeihai: A sample-efficient hierarchical ai for the minerl competition. In: Distributed\nArtificial Intelligence: Third International Conference, DAI 2021, Shanghai, China,\nDecember 17–18, 2021, Proceedings 3. pp. 38–51. Springer (2022)\n43. Moudgil, A., Majumdar, A., Agrawal, H., Lee, S., Batra, D.: Soat: A scene-and\nobject-aware transformer for vision-and-language navigation. Advances in Neural\nInformation Processing Systems 34, 7357–7367 (2021)\n44. OpenAI: Gpt-4 technical report. arXiv preprint arXiv: Arxiv-2303.08774 (2023)\n45. PrismarineJS: Prismarinejs\/mineflayer: Create minecraft bots with a powerful, sta-\nble, and high level javascript api. (2013), https:\/\/github.com\/PrismarineJS\/\nmineflayer\/tree\/master\n46. Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub, J.,\nLiu, J., Koltun, V., Malik, J., et al.: Habitat: A platform for embodied ai research.\nIn: Proceedings of the IEEE\/CVF international conference on computer vision.\npp. 9339–9347 (2019)\n47. Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L.,\nCancedda, N., Scialom, T.: Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761 (2023)\n48. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai\ntasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580\n(2023)\n49. Significant-Gravitas: Auto-gpt. https:\/\/github.com\/Significant-Gravitas\/\nAuto-GPT (2023)\n50. Skrynnik, A., Staroverov, A., Aitygulov, E., Aksenov, K., Davydov, V., Panov, A.I.:\nHierarchical deep q-network from imperfect demonstrations in minecraft. Cognitive\nSystems Research 65, 74–78 (2021)\n18\nZ. Zhao et al.\n51. Song, C.H., Wu, J., Washington, C., Sadler, B.M., Chao, W.L., Su, Y.: Llm-\nplanner: Few-shot grounded planning for embodied agents with large language\nmodels. arXiv preprint arXiv:2212.04088 (2022)\n52. Song, E., Chai, W., Wang, G., Zhang, Y., Zhou, H., Wu, F., Guo, X., Ye, T., Lu,\nY., Hwang, J.N., et al.: Moviechat: From dense token to sparse memory for long\nvideo understanding. arXiv preprint arXiv:2307.16449 (2023)\n53. Song, E., Chai, W., Ye, T., Hwang, J.N., Li, X., Wang, G.: Moviechat+:\nQuestion-aware sparse memory for long video question answering. arXiv preprint\narXiv:2404.17176 (2024)\n54. Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., Cai, D.: Pandagpt: One model to\ninstruction-follow them all. arXiv preprint arXiv:2305.16355 (2023)\n55. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n56. Vasluianu, F.A., Seizinger, T., Zhou, Z., Wu, Z., Chen, C., Timofte, R., Dong, W.,\nZhou, H., Tian, Y., Chen, J., et al.: Ntire 2024 image shadow removal challenge\nreport. In: Proceedings of the IEEE\/CVF Conference on Computer Vision and\nPattern Recognition. pp. 6547–6570 (2024)\n57. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., Anand-\nkumar, A.: Voyager: An open-ended embodied agent with large language models.\narXiv preprint arXiv:2305.16291 (2023)\n58. Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou,\nJ., Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder\nfor vision-centric tasks. arXiv preprint arXiv:2305.11175 (2023)\n59. Wang, Z., Cai, S., Liu, A., Ma, X., Liang, Y.: Describe, explain, plan and select:\nInteractive planning with large language models enables open-world multi-task\nagents. arXiv preprint arXiv:2302.01560 (2023)\n60. Whitted, T.: An improved illumination model for shaded display. In: ACM Sig-\ngraph 2005 Courses, pp. 4–es (2005)\n61. Wijmans, E., Kadian, A., Morcos, A., Lee, S., Essa, I., Parikh, D., Savva, M., Batra,\nD.: Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. In:\nInternational Conference on Learning Representations (2019)\n62. Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671 (2023)\n63. Xie, Y., Yu, C., Zhu, T., Bai, J., Gong, Z., Soh, H.: Translating natural language to\nplanning goals with large-language models. arXiv preprint arXiv:2302.05128 (2023)\n64. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,\nShi, Y., et al.: mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178 (2023)\n65. Ye, T., Chen, S., Liu, Y., Chai, W., Bai, J., Zou, W., Zhang, Y., Jiang, M., Chen,\nE., Xue, C.: Sequential affinity learning for video restoration. In: Proceedings of\nthe 31st ACM International Conference on Multimedia. pp. 4147–4156 (2023)\n66. Ye, T., Zhang, Y., Jiang, M., Chen, L., Liu, Y., Chen, S., Chen, E.: Perceiving and\nmodeling density for image dehazing. In: European conference on computer vision.\npp. 130–145. Springer (2022)\n67. Yu, L., Chen, X., Gkioxari, G., Bansal, M., Berg, T.L., Batra, D.: Multi-target\nembodied question answering. In: Proceedings of the IEEE\/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 6309–6318 (2019)\n68. Yu, Y., Huang, W., Sun, F., Chen, C., Wang, Y., Liu, X.: Sound adversarial audio-\nvisual navigation. In: International Conference on Learning Representations (2021)\nSTEVE\n19\n69. Yuan, H., Zhang, C., Wang, H., Xie, F., Cai, P., Dong, H., Lu, Z.: Plan4mc: Skill\nreinforcement learning and planning for open-world minecraft tasks. arXiv preprint\narXiv:2303.16563 (2023)\n70. Zhao, Z., Chai, W., Hao, S., Hu, W., Wang, G., Cao, S., Song, M., Hwang, J.N.,\nWang, G.: A survey of deep learning in sports applications: Perception, compre-\nhension, and decision. arXiv preprint arXiv:2307.03353 (2023)\n71. Zhao, Z., Chai, W., Wang, X., Ma, K., Chen, K., Guo, D., Ye, T., Zhang, Y.,\nWang, H., Wang, G.: Steve series: Step-by-step construction of agent systems in\nminecraft. arXiv preprint arXiv:2406.11247 (2024)\n72. Zhao, Z., Chen, K., Guo, D., Chai, W., Ye, T., Zhang, Y., Wang, G.: Hierarchi-\ncal auto-organizing system for open-ended multi-agent navigation. arXiv preprint\narXiv:2403.08282 (2024)\n73. Zhao, Z., Ma, K., Chai, W., Wang, X., Chen, K., Guo, D., Zhang, Y., Wang, H.,\nWang, G.: Do we really need a complex agent system? distill embodied agent into\na single model. arXiv preprint arXiv:2404.04619 (2024)\n74. Zhong, F., Sun, P., Luo, W., Yan, T., Wang, Y.: Ad-vat+: An asymmetric dueling\nmechanism for learning and understanding visual active tracking. IEEE transac-\ntions on pattern analysis and machine intelligence 43(5), 1467–1482 (2019)\n75. Zhong, F., Sun, P., Luo, W., Yan, T., Wang, Y.: Towards distraction-robust active\nvisual tracking. In: International Conference on Machine Learning. pp. 12782–\n12792. PMLR (2021)\n76. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 (2023)\n77. Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L.,\nWang, X., et al.: Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory.\narXiv preprint arXiv:2305.17144 (2023)\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/See and Think: Embodied Agent in Virtual Environment.pdf"}
{"title":"RL-GPT: Integrating Reinforcement Learning and Code-as-policy","authors":"Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia","summary":"Large Language Models (LLMs) have demonstrated proficiency in utilizing\nvarious tools by coding, yet they face limitations in handling intricate logic\nand precise control. In embodied tasks, high-level planning is amenable to\ndirect coding, while low-level actions often necessitate task-specific\nrefinement, such as Reinforcement Learning (RL). To seamlessly integrate both\nmodalities, we introduce a two-level hierarchical framework, RL-GPT, comprising\na slow agent and a fast agent. The slow agent analyzes actions suitable for\ncoding, while the fast agent executes coding tasks. This decomposition\neffectively focuses each agent on specific tasks, proving highly efficient\nwithin our pipeline. Our approach outperforms traditional RL methods and\nexisting GPT agents, demonstrating superior efficiency. In the Minecraft game,\nit rapidly obtains diamonds within a single day on an RTX3090. Additionally, it\nachieves SOTA performance across all designated MineDojo tasks.","url":"http:\/\/arxiv.org\/abs\/2402.19299v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2402.19299v1","published":1709222842000,"comment":null,"pdf_text":"RL-GPT: Integrating Reinforcement Learning and Code-as-policy\nShaoteng Liu1, Haoqi Yuan3, Minda Hu1, Yanwei Li1,\nYukang Chen1, Shu Liu2, Zongqing Lu3,4, Jiaya Jia1,2\n1The Chinese University of Hong Kong 2SmartMore\n3School of Computer Science, Peking University\n4Beijing Academy of Artificial Intelligence\nhttps:\/\/sites.google.com\/view\/rl-gpt\/\nAbstract\nLarge Language Models (LLMs) have demonstrated pro-\nficiency in utilizing various tools by coding, yet they face\nlimitations in handling intricate logic and precise control.\nIn embodied tasks, high-level planning is amenable to di-\nrect coding, while low-level actions often necessitate task-\nspecific refinement, such as Reinforcement Learning (RL).\nTo seamlessly integrate both modalities, we introduce a two-\nlevel hierarchical framework, RL-GPT, comprising a slow\nagent and a fast agent. The slow agent analyzes actions\nsuitable for coding, while the fast agent executes coding\ntasks. This decomposition effectively focuses each agent on\nspecific tasks, proving highly efficient within our pipeline.\nOur approach outperforms traditional RL methods and ex-\nisting GPT agents, demonstrating superior efficiency. In\nthe Minecraft game, it rapidly obtains diamonds within a\nsingle day on an RTX3090. Additionally, it achieves SOTA\nperformance across all designated MineDojo tasks.\n1. Introduction\nBuilding agents to master tasks in open-world environments\nhas been a long-standing goal in AI research [6, 38, 40]. The\nemergence of Large Language Models (LLMs) has revital-\nized this pursuit, leveraging their expansive world knowledge\nand adept compositional reasoning capabilities [24, 39, 47].\nLLMs agents showcase proficiency in utilizing computer\ntools [10, 14], navigating search engines [11, 15], and even\noperating systems or applications [9, 46]. However, their\nperformance remains constrained in open-world embodied\nenvironments [10, 38]. Despite possessing “world knowl-\nedge” akin to a human professor, LLMs fall short when pitted\nagainst a child in a video game. The inherent limitation lies\nin LLMs’ adeptness at absorbing information but their in-\nability to practice skills within an environment. Proficiency\nGPT\nRL\nRL-GPT (Ours)\nOptimized\nNeural Network\nOptimized\nCoded Actions\nOptimized Actions + \nNeural Network\n1.9X\n6.7X\nFigure 1. The overview of RL-GPT. After the optimization in an\nenvironment, LLMs agents obtain optimized coded actions, RL\nachieves an optimized neural network, and our RL-GPT gets both\noptimized coded actions and neural networks. Our framework\nintegrates the coding parts and the learning parts.\nin activities such as playing a video game demands extensive\npractice, a facet not easily addressed by in-context learning,\nwhich exhibits a relatively low upper bound [10, 24, 47].\nConsequently, existing LLMs necessitate human interven-\ntion to define low-level skills or tools [38, 42].\nReinforcement Learning (RL), proven as an effective\nmethod for learning from interaction, holds promise in fa-\ncilitating LLMs to “practise”. One line of works grounds\nLLMs for open-world control through RL fine-tuning [4, 27,\n33, 44, 45, 53]. Nevertheless, this approach necessitates a\nsubstantial volume of domain-specific data, expert demon-\nstrations, and access to LLMs’ parameters, rendering it slow\nand resource-intensive in most scenarios. Given the mod-\nest learning efficiency, the majority of methods continue\nto operate within the realm of “word games” such as tone\nadjustment rather than tackling intricate embodied tasks.\nAddressing this challenge, we propose to integrate LLMs\n1\narXiv:2402.19299v1  [cs.AI]  29 Feb 2024\nand RL in a novel approach: Empower LLMs agents to use\nan RL training pipeline as a tool. We introduce RL-GPT, a\nframework designed to enhance LLMs with trainable mod-\nules for learning interaction tasks within an environment.\nAs shown in Fig. 3, RL-GPT comprises an agent pipeline\nfeaturing multiple LLMs, wherein the neural network is\nconceptualized as a tool for training the RL pipeline. Illus-\ntrated in Fig. 1, unlike conventional approaches where LLMs\nagents and RL optimize coded actions and networks sepa-\nrately, RL-GPT unifies this optimization process. The line\nchart in Fig. 1 illustrates that RL-GPT outperforms alterna-\ntive approaches on the “harvest a log” task in MineDojo [7].\nWe further point out that the pivotal issue in using RL\nis to decide: Which actions should be learned with RL?\nTo tackle this, RL-GPT is meticulously designed to assign\ndifferent actions to RL and Code-as-policy, respectively. Our\nagent pipeline entails two fundamental steps. Firstly, LLMs\nshould determine “which actions” to code, involving task\ndecomposition into distinct sub-actions and deciding which\nactions can be effectively coded. Actions falling outside\nthis realm will be learned through RL. Secondly, LLMs are\ntasked with writing accurate codes for the “coded actions”\nand test them in the environment.\nWe employ a two-level hierarchical framework to realize\nthe two steps, as depicted in Fig. 3. Allocating these steps\nto two independent agents proves highly effective, as it nar-\nrows down the scope of each LLM’s task. Coded actions\nwith explicit starting conditions are executed sequentially,\nwhile other coded actions are integrated into the RL action\nspace. This strategic insertion into the action space em-\npowers LLMs to make pivotal decisions during the learning\nprocess. Illustrated in Fig. 2, this integration enhances the\nefficiency of learning tasks, exemplified by our ability to\nmore effectively learn how to break a tree.\nFor intricate tasks such as the ObtainDiamond task in the\nMinecraft game, devising a strategy with a single neural net-\nwork proves challenging due to limited computing resources.\nIn response, we incorporate a task planner to facilitate task\ndecomposition. Our RL-GPT framework demonstrates re-\nmarkable efficiency in tackling complex embodied tasks.\nSpecifically, within the MineDojo environment, it attains\nstate-of-the-art performance on the majority of selected tasks\nand adeptly obtains diamonds within a single day, utilizing\nonly an RTX3090 GPU.\nOur contributions are summarized as follows:\n• Introduction of an LLMs agent utilizing an RL training\npipeline as a tool.\n• Development of a two-level hierarchical framework ca-\npable of determining which actions in a task should be\nlearned with RL.\n• Pioneering work as the first to incorporate high-level GPT-\ncoded actions into the RL action space, enhancing the\nsample efficiency for RL.\nObservation\nAction\nenv.action_space\ndef n_attack(env, times = 20):\nfor i in range(times):\nact[5] = 3 # choose attack\nyield act\nAction Space Design\nPolicy Network\nFigure 2. To learn a subtask, the LLM can generate environment\nconfigurations (task, observation, reward, and action space) to\ninstantiate RL. In particular, by reasoning about the agent behavior\nto solve the subtask, the LLM generates code to provide higher-level\nactions in addition to the original environment actions, improving\nthe sample efficiency for RL.\n2. Related Works\n2.1. Agents in Minecraft\nMinecraft, a widely popular open-world sandbox game,\nstands as a formidable benchmark for constructing effi-\ncient and generalized agents. Previous endeavors resort\nto hierarchical reinforcement learning, often relying on hu-\nman demonstrations to facilitate the training of low-level\npolicies [12, 19]. Efforts such as MineAgent [7], Steve-\n1 [23], and VPT [3] leverage large-scale pre-training via\nYouTube videos to enhance policy training efficiency. How-\never, MineAgent and Steve-1 are limited to completing only\na few short-term tasks, and others [3, 50] still require a sub-\nstantial number of steps for finetuning on long-horizon tasks.\nDreamerV3 [13] utilizes a world model to expedite explo-\nration but still demands a substantial number of interactions\nto acquire diamonds. These existing approaches either ne-\ncessitate extensive expert datasets for training or exhibit low\nsample efficiency when addressing long-horizon tasks in the\nMinecraft environment.\nAn alternative research direction employs Large Lan-\nguage Models (LLMs) for task decomposition and high-level\nplanning to address intricate challenges. Certain works [37]\nleverage few-shot prompting with Codex [5] to generate exe-\ncutable policies. DEPS [42] and GITM [55] investigate the\nuse of LLMs as high-level planners in the Minecraft context.\nSome works [38, 41, 51] further explore LLMs for high-level\nplanning, code generation, lifelong exploration, and creative\ntasks. Other studies [8, 54] delve into grounding smaller\nlanguage models for control with domain-specific finetuning.\nNevertheless, these methods often rely on manually designed\n2\ncontrollers or code interfaces, sidestepping the challenge of\nlearning low-level policies.\nPlan4MC [49] integrates LLM-based planning and RL-\nbased policy learning but requires defining and pre-training\nall the policies with manually specified environments. Our\nRL-GPT extends LLMs’ ability in low-level control by equip-\nping it with RL, achieving automatic and efficient task learn-\ning in Minecraft.\n2.2. LLMs Agents\nSeveral works leverage LLMs to generate subgoals for robot\nplanning [2, 16]. Works like Inner Monologue [17] incorpo-\nrate environmental feedback into robot planning with LLMs.\nCode-as-Policies [21] and ProgPrompt [32] directly utilize\nLLMs to formulate executable robot policies. VIMA [18]\nand PaLM-E [6] involve fine-tuning pre-trained LLMs to\nsupport multimodal prompts. Besides, Chameleon [24] ef-\nfectively executes sub-task decomposition and generates\nsequential programs. ReAct [47] utilizes chain-of-thought\nprompting to generate task-specific actions. AutoGPT [10]\nautomates NLP tasks by integrating reasoning and acting\nloops. DERA [26] introduces dialogues between GPT-4 [1]\nagents. Generative Agents [28] simulate human behaviors\nby storing experiences as memories.\nCompared with existing works, RL-GPT equips the LLM\nagent with RL, extending its capability in intricate low-level\ncontrol in open-world tasks.\n2.3. Integrating LLMs and RL\nSince LLMs and RL possess complementary abilities in pro-\nviding prior knowledge and exploring unknown information,\nit is promising to integrate them for efficient task learning.\nMost work studies improve RL with the domain knowl-\nedge in LLMs. SayCan [2] and Plan4MC [49] decompose\nand plan subtasks with LLMs, thereby RL can learn eas-\nier subtasks to solve the whole task. Recent works [20, 25,\n43, 48] studies generating reward functions with LLMs to\nimprove the sample efficiency for RL. Another line of re-\nsearch [31, 33, 34, 44, 45, 52, 53] finetunes LLMs with RL\nto acquire the lacked ability of LLMs in low-level control.\nHowever, these approaches usually require a lot of samples\nand can harm the LLMs’ abilities in other tasks. Our study\nis the first to overcome the inabilities of LLMs in low-level\ncontrol by equipping them with RL as a tool. The acquired\nknowledge is stored in context, thereby continually improv-\ning the LLMs skills and maintaining its capability.\n3. Methods\nRL-GPT incorporates three distinct components, each con-\ntributing to its innovative design: (1) a slow agent tasked\nwith decomposing a given task into several sub-actions and\ndetermining which actions can be directly coded, (2) a fast\nagent responsible for writing code and instantiating RL con-\nfiguration, and (3) an iteration mechanism that facilitates an\niterative process refining both the slow agent and the fast\nagent. This iterative process enhances the overall efficacy\nof the RL-GPT across successive iterations. For complex\nlong-horizon tasks requiring multiple neural networks, we\nemploy a GPT-4 as a planner to initially decompose the task.\nAs discussed in concurrent works [22, 36], segregating\nhigh-level planning and low-level actions into distinct agents\nhas proven to be beneficial. The dual-agent system effec-\ntively narrows down the specific task of each agent, enabling\noptimization for specific targets. Moreover, Liang et al. high-\nlighted the Degeneration-of-Thought (DoT) problem, where\nan LLM becomes overly confident in its responses and lacks\nthe ability for self-correction through self-reflection. Empir-\nical evidence indicates that agents with different roles and\nperspectives can foster divergent thinking, mitigating the\nDoT problem. External feedback from other agents guides\nthe LLM, making it less susceptible to DoT and promoting\naccurate reasoning.\n3.1. RL Interface\nAs previously mentioned, we view the RL training pipeline\nas a tool accessible to LLMs agents, akin to other tools with\ncallable interfaces. Summarizing the interfaces of an RL\ntraining pipeline, we identify the following components: 1)\nLearning task; 2) Environment reset; 3) Observation space;\n4) Action space; 5) Reward function. Specifically, our fo-\ncus lies on studying interfaces 1) and 4) to demonstrate the\npotential for integrating RL and Code-as-policy.\nIn the case of the action space interface, we enable LLMs\nto design high-level actions and integrate them into the ac-\ntion space. A dedicated token is allocated for this purpose,\nallowing the neural network to learn when to utilize this\naction based on observations.\n3.2. Slow Agent: Action Planning\nAssume the task T needs to be learned by a single network\nwithin constrained computing resources. We employ a GPT-\n4 [1] as a slow agent AS. AS is tasked with decomposing\nT into sub-actions αi, where i ∈{0, ..., n}, determining if\neach αi in T can be directly addressed through code imple-\nmentation. This approach optimally allocates computational\nresources to address more challenging sub-tasks using RL.\nImportantly, AS is not required to perform any low-level\ncoding tasks; it solely provides high-level textual instruc-\ntions regarding sub-actions αi. These instructions are then\ntransmitted to the fast agent AF for further processing. The\niterative process of the slow agent involves systematically\nprobing the limits of coding capabilities.\nFor instance, in Fig. 3, consider the specific action of\ncrafting a wooden pickaxe. Although AS is aware that play-\ners need to harvest a log, writing code for this task with a\n3\nPlan\nSub-actions\nRL?\ncode or\nTrain\nNavigate to find\nSub-Action ①\nPolicy Network\nObservation\nAction\ndef craft_w_table(goal):\nfor act in chain(\nplace_down('crafting_table’),\ncraft_wo_table(goal),\nrecycle('crafting_table', 200),\nyield act\nCode Implementation\nEnvironment\nFeedback\nHarvest\nSub-Action ②\nCraft\nwith\nSub-Action ③\ndef n_attack(env, times = 20):\nfor i in range(times):\nact[5] = 3\nyield act\nAction Space Design\nRL Implementation\nDirect Code Implementation\nWhether to implement Sub-action\nEnvironment\nEnvironment\nInteract\nFast\nAgent\nFast\nAgent\nSlow\nAgent\nwith\nTemporal \nAbstraction\nCraft\nObjective\nEnvironment Feedback\nHigh-level Action Feedback\nSub-Action ①: …\nSub-Action ②: …\nSub-Action ③: …\nFigure 3. Overview of RL-GPT. The overall framework consists of a slow agent (orange) and a fast agent (green). The slow agent decomposes\nthe task and determines “which actions” to learn. The fast agent writes code and RL configurations for low-level execution.\nFast\nAgent\nSlow\nAgent\nCritic\nAgent\nEnvironment\nSub-Act ①: …\nSub-Act ②: …\nSub-Act ③: …\nAction Code\nObservation\nbefore \/ after\nthe action\nFor Sub-Act ②\nCritic ②\nCritic ①\nCritic ②\nCritic ③\nFigure 4. The two-loop iteration. We design a method to optimize\nboth slow agent and fast agent with a critic agent.\nhigh success rate can be challenging. The limitation arises\nfrom the insufficient information available through APIs for\nAS to accurately locate and navigate to a tree. To overcome\nthis hurdle, an RL implementation becomes necessary. RL\naids AS in completing tasks by processing complex visual\ninformation and interacting with the environment through\ntrial and error. In contrast, some simple, straightforward\nactions like crafting something with a crafting table can be\ndirectly coded and executed.\nIt is crucial to instruct AS to identify sub-actions\nthat are too challenging for rule-based code imple-\nmentation.\nAs shown in Table 1, the prompt for\nAS\nincorporates role description {role description},\nthe\ngiven\ntask\nT,\nreference\ndocuments,\nenviron-\nment\nknowledge\n{minecraft knowledge},\nplanning\nheuristics {planning tips},\nand programming exam-\nples {programs}. To align AS with our goals, we include\nthe heuristic in the {planning tips}.\nThis heuristic\nencourages AS to further break down an action when coding\nproves challenging. This incremental segmentation aids AS\nin discerning what aspects can be coded. Further details are\navailable in Appendix A.\n3.3. Fast Agent: Code-as-Policy and RL\nThe fast agent AF is also implemented using GPT-4. The pri-\nmary task is to translate the instructions from the slow agent\nAS into Python codes for the sub-actions αi. AF undergoes\na debug iteration where it runs the generated sub-action code\nand endeavors to self-correct through feedback from the en-\nvironment. Sub-actions that can be addressed completely\nwith code implementation are directly executed, as depicted\nin the blue segments of Fig. 3. For challenging sub-actions\nlacking clear starting conditions, the code is integrated into\nthe RL implementation using the temporal abstraction tech-\nnique [29, 35], as illustrated in Fig. 2. This involves inserting\nthe high-level action into the RL action space, akin to the or-\nange segments in Fig. 3. AF iteratively corrects itself based\non the feedback received from the environment.\n3.4. Two-loop Iteration\nIn Fig. 4, we devise the two-loop iteration mechanism to\noptimize the proposed two agents, namely the fast agent AF\nand the slow agent AS. To facilitate it, a critic agent C is\nintroduced, which could be implemented using GPT-3.5 or\nGPT-4.\nThe optimization for the fast agent, as shown in Fig. 4,\naligns with established methods for code-as-policy agents.\nHere, the fast agent receives a sub-action, environment docu-\nments Denv (observation and action space), and examples\nEcode as input, generating Python code. It then iteratively\n4\n{role description}\nIt is difficult to code all actions in this game. We only want to\ncode as many sub-actions as possible. The task of you is to tell\nme which sub-actions can be coded by you with Python.\nAt each round of conversation, I will give you\nTask: T\nContext: ...\nCritique: The results of the generated codes in the last round\nHere are some actions coded by humans:\n{programs}\nYou should then respond to me with\nExplain (if applicable): Why these actions can be coded by\npython? Are there any actions difficult to code?\nActions can be coded: List all actions that can be coded by\nyou.\nImportant Tips:\n{planning tips}\nYou should only respond in the format as described below:\nExplain: ...\nActions can be coded:\n1) Action1: ...\n2) Action2: ...\n3) ...\nTable 1. Slow Agent’s prompt: Decompose a task into sub-actions.\nrefines the code based on environmental feedback. The ob-\njective is to produce error-free Python-coded sub-actions\nthat align with the targets set by the slow agent. Feedback,\nwhich includes execution errors and critiques from C, plays\na crucial role in this process. C evaluates the coded action’s\nsuccess by considering observations before and after the\naction’s execution, offering insights for improvement.\nWithin Fig. 4, the iteration of the slow agent AS en-\ncompasses the aforementioned fast agent AF iteration as a\nstep. In each step of AS, AF must complete an iteration\nloop. Given a task T, Denv, and Ecode, AS decomposes T\ninto sub-actions αi and refines itself based on C’s outputs.\nSpecifically, it receives a sequence of outputs Critici from\nC about each αi to assess the effectiveness of action plan-\nning. If certain actions cannot be coded by the fast agent,\nthe slow agent adjusts the action planning accordingly.\n3.5. Task Planner\nOur primary pipeline is tailored for tasks that can be learned\nusing a neural network within limited computational re-\nsources. However, for intricate tasks such as ObtainDia-\nmond, where it is more effective to train multiple neural\nnetworks like DEPS [42] and Plan4MC [49], we introduce\n{role description}\nHere are some basic actions coded by humans:\n{programs template}\nPlease inherit the class CodeAgent. You are only required to\noverwrite the function main function.\nHere are some reference examples written by me:\n{programs example}\nHere are the attributes of the obs that can be used:\n{obs info}\nHere are the guidelines of the act variable:\n{act info}\nAt each round of conversation, I will give you\nTask: ...\nContext: ...\nCode from the last round: ...\nExecution error: ...\nCritique: ...\nYou should then respond to me with\nExplain (if applicable): Can the code complete the given ac-\ntion? What does the chat log and execution error imply?\nYou should only respond in the format as described below:\n{code format}\nTable 2. Fast Agent’s prompt: Write Python codes.\na task planner reminiscent of DEPS, implemented using\nGPT-4. This task planner iteratively reasons what needs\nto be learned and organizes sub-tasks for our RL-GPT to\naccomplish.\n4. Experiments\n4.1. Environment\nMineDojo\nMineDojo [7] stands out as a pioneering frame-\nwork developed within the renowned Minecraft game, tai-\nlored specifically for research involving embodied agents.\nThis innovative framework comprises a simulation suite fea-\nturing thousands of tasks, blending both open-ended chal-\nlenges and those prompted by language. To validate the ef-\nfectiveness of our approach, we selected certain long-horizon\ntasks from MineDojo, mirroring the strategy employed in\nPlan4MC [49]. These tasks include harvesting and crafting\nactivities. For instance, Crafting one wooden pickaxe re-\nquires the agent to harvest a log, craft planks, craft sticks,\ncraft tables, and craft the pickaxe with the table. Similarly,\ntasks like milking a cow involve the construction of a bucket,\napproaching the cow, and using the bucket to obtain milk.\n5\nTable 3. Comparison of different methods on several tasks in the MineDojo benchmark. Our RL-GPT achieves the highest success rate on\nall tasks.\nTASK\nMINEAGENT\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n-\n-\n-\n-\nMINEAGENT (AUTOCRAFT)\n0.00\n0.03\n0.00\n0.00\n0.00\n0.46\n0.50\n0.33\n0.35\n0.00\nPLAN4MC\n0.30\n0.30\n0.53\n0.37\n0.17\n0.83\n0.53\n0.43\n0.33\n0.17\nRL-GPT\n0.65\n0.65\n0.67\n0.67\n0.64\n0.85\n0.56\n0.46\n0.38\n0.32\nTable 4. Main results in the challenging ObtainDiamond\ntask\nin Minecraft. Existing strong baselines in ObtainDiamond either\nrequire expert data (VPT, DEPS), hand-crafted policies (DEPS-\nOracle) for subtasks, or take huge number of environment steps to\ntrain (DreamerV3, VPT). Our method can automatically decom-\npose and learn subtasks with only a little human prior, achieving\nObtainDiamond with great sample efficiency.\nMETHOD\nTYPE\nSAMPLES\nSUCCESS\nDREAMERV3\nRL\n100M\n2%\nVPT\nIL+RL\n16.8B\n20%\nDEPS-BC\nIL+LLM\n--\n0.6%\nDEPS-ORACLE\nLLM\n--\n60%\nPLAN4MC\nRL+LLM\n7M\n0%\nRL-GPT\nRL+LLM\n3M\n8%\nObtainDiamond Challenge\nIt represents a classic chal-\nlenge for RL agents. The task of obtaining a diamond de-\nmands the agent to complete the comprehensive process of\nharvesting a diamond from the beginning. This constitutes\na long-horizon task, involving actions such as harvesting\nlogs, harvesting stones, crafting items, digging to find iron,\nsmelting iron, locating a diamond, and so on.\n4.2. Implementation Details\nLLM Prompt\nWe choose GPT-4 as our LLMs API. For\nthe slow agents and fast agents, we design special templates,\nresponding formats, and examples. We design some special\nprompts such as “assume you are an experienced RL re-\nsearcher that is designing the RL training job for Minecraft”.\nDetails can be found in the Appendix A. In addition, we\nencourage the slow agent to explore more strategies because\nthe RL task requires more exploring. We encourage the slow\nagent to further decompose the action into sub-actions which\nmay be easier to code.\nPPO Details\nSimilar to MineAgent [7], we employ Prox-\nimal Policy Optimization (PPO) [30] as the RL baseline.\nThis approach alternates between sampling data through in-\nteractions with the environment and optimizing a ”surrogate”\nobjective function using stochastic gradient ascent. PPO is\nconstrained to a limited set of skills. When applying PPO\nwith sparse rewards, specific tasks such as “milk a cow” and\n“shear a sheep” present challenges due to the small size of\nthe target object relative to the scene, and the low probability\nof random encounters. To address this, we introduce basic\ndense rewards to enhance learning efficacy in these tasks. It\nincludes the CLIP Reward, which encourages the agent to\nexhibit behaviors that align with the prompt [7]. Addition-\nally, we incorporate a Distance Reward that provides dense\nreward signals to reach the target items [49]. Further details\ncan be found in the appendix.\n4.3. Main Results\nMineDojo Benchmark\nTable 3 presents a comparative\nanalysis between our RL-GPT and several baselines on se-\nlected MineDojo tasks. Notably, RL-GPT achieves the high-\nest success rate among all baselines. All baselines underwent\ntraining with 10 million samples, and the checkpoint with\nthe highest success rate was chosen for testing.\nMineAgent, as proposed in [7], combines PPO with Clip\nReward. However, naive PPO encounters difficulties in\nlearning long-horizon tasks, such as crafting a bucket and\nobtaining milk from a cow, resulting in an almost 0% suc-\ncess rate for MineAgent across all tasks. Another baseline,\nMineAgent with autocraft, as suggested in Plan4MC [49],\nincorporates crafting actions manually coded by humans.\nThis alternative baseline achieves a 46% success rate on\nthe milking task, demonstrating the importance of code-as-\npolicy. Our approach demonstrates superiority in coding\nactions beyond crafting, enabling us to achieve higher over-\nall performance compared to baselines that focus primarily\non crafting actions.\nPlan4MC [49] breaks down the problem into two es-\nsential components: acquiring fundamental skills and plan-\nning based on these skills. While some skills are acquired\nthrough Reinforcement Learning (RL), Plan4MC outper-\nforms MineAgent due to its reliance on an oracle task de-\ncomposition from the GPT planner. However, it cannot\nmodify the action space of an RL training pipeline or flex-\nibly decompose sub-actions. It is restricted to only three\ntypes of human-designed coded actions. Consequently, our\n6\n+\nà\nCode: Navigate to find\nRL-GPT \n(iter-3)\n58%\nSuccess\nRate\n+\n…\nCode: Navigate to find\nRL-GPT \n(iter-1)\n18%\nSuccess\nRate\n…\n…\nCode: Harvest a\nRL-GPT \n(iter-0)\n0%\nSuccess\nRate\n…\n…\nRL\nMineAgent\n(RL)\n10%\nSuccess\nRate\nRL          : Cut a\nRL          : Cut a\nRL           + Code: Attack 20 times\n+ Code: Aim the tree + Attack 20 times\n✅\nFigure 5. Demonstrations of how different agents learn to harvest a log. While both RL agent and LLM agent learn a single type of solution\n(RL or code-as-policy), our RL-GPT can reasonably decompose the task and correct how to learn each sub-action through the slow iteration\nprocess. RL-GPT decomposes the task into “find a tree” and “cut a log”, solving the former with code generation and the latter with RL.\nAfter a few iterations, it learns to provide RL with a necessary high-level action (attack 20 times) and completes the task with a high success\nrate. Best viewed by zooming in.\nmethod holds a distinct advantage in this context.\nIn tasks involving\nand\n, the agent is tasked with\ncrafting a stick from scratch, necessitating the harvesting of\na log. Our RL-GPT adeptly codes three actions for this: 1)\nNavigate to find a tree; 2) Attack 20 times; 3) Craft items.\nNotably, Action 2) can be seamlessly inserted into the ac-\ntion space. In contrast, Plan4MC is limited to coding craft\nactions only. This key distinction contributes to our method\nachieving higher scores in these tasks.\nTo arrive at the optimal code planning solution, RL-GPT\nundergoes a minimum of three iterations. As illustrated\nin Fig. 5, in the initial iteration, RL-GPT attempts to code\nevery action involved in harvesting a log, yielding a 0%\nsuccess rate. After the first iteration, it decides to code navi-\ngation, aiming at the tree, and attacking 20 times. However,\naiming at the tree proves too challenging for LLMs. As\nmentioned before, the agent will be instructed to further de-\ncompose the actions and give up difficult actions. By the\nthird iteration, the agent correctly converges to the optimal\nsolution—coding navigation and attacking, while leaving\nthe rest to RL, resulting in higher performance.\nIn tasks involving crafting a wooden pickaxe\nand craft-\ning a bed\n, in addition to the previously mentioned actions,\nthe agent needs to utilize the crafting table. While Plan4MC\nmust learn this process, our method can directly code actions\nto place the crafting table on the ground, use it, and recycle\nit. Code-as-policy contributes to our method achieving a\nhigher success rate in these tasks.\nIn tasks involving crafting a furnace\nand a stone pick-\naxe\n, in addition to the previously mentioned actions, the\nagent is further required to harvest stones. Plan4MC needs to\nlearn an RL network to acquire the skill of attacking stones.\nRL-GPT proposes two potential solutions for coding addi-\ntional actions. First, it can code to continuously attack a\nstone and insert this action into the action space. Second,\nsince LLMs understand that stones are underground, the\nagent might choose to dig deep for several levels to obtain\nstones instead of navigating on the ground to find stones.\nIn tasks involving crafting a milk bucket\nand crafting\nwool\n, the primary challenge lies in crafting a bucket or\nshears. Since both RL-GPT and Plan4MC can code actions\nto craft without a crafting table, their performance is similar\nand comparable.\nIn tasks involving obtaining beef\nand obtaining mut-\nton\n, the only actions that can be further coded are navigat-\ning to find the target. Given that both RL-GPT and Plan4MC\ncan code actions to navigate, their performance in these tasks\nis similar.\nObtainDiamond Challenge\nAs shown in Tab. 4, we com-\npare our method with existing competitive methods on the\n7\nTable 5. Ablation study on the necessity of the proposed compo-\nnents in RL-GPT.\nSTRUCTURE\nONE AGENT\n0.34\n0.42\nSLOW + FAST\n0.52\n0.56\nSLOW + FAST + CRITIC\n0.65\n0.67\nTable 6. Ablation study on the effectiveness of our two-loop itera-\ntion strategy. RL-GPT achieves better results when the number of\niterations increases.\nMETHOD\nPURE RL\n0.00\n0.00\n0.00\n0.00\nPURE CODE\n0.13\n0.02\n0.00\n0.00\nOURS (ZERO-SHOT)\n0.26\n0.53\n0.79\n0.32\nOURS (ITER-2 W\/O SP)\n0.26\n0.53\n0.79\n0.30\nOURS (ITER-2)\n0.56\n0.67\n0.88\n0.30\nOURS (ITER-3)\n0.65\n0.67\n0.93\n0.32\nTable 7. Ablation study on the RL interface: reward and action\nspace design.\nRL INTERFACE\nSUCCESS RATE ↑\nDEAD LOOP ↓\nREWARD FUNCTION\n0.418\n≈0.6\nACTION SPACE\n0.585\n≈0.3\nchallenging ObtainDiamond task.\nDreamerV3 [13] leverages a world model to accelerate\nexploration but still requires a significant number of interac-\ntions. Despite the considerable expense of over 100 million\nsamples for learning, it only achieves a 2% success rate on\nthe Diamond task from scratch.\nVPT [3] employs large-scale pre-training using YouTube\nvideos to improve policy training efficiency. This strong\nbaseline is trained on 80 GPUs for 6 days, achieving a 20%\nsuccess rate in obtaining a diamond and a 2.5% success rate\nin crafting a diamond pickaxe.\nDEPS [42] suggests generating training data using a com-\nbination of GPT and human handcrafted code for planning\nand imitation learning. It attains a 0.6% success rate on this\ntask. Moreover, an oracle version, which directly executes\nhuman-written codes, achieves a 60% success rate.\nPlan4MC [49] primarily focuses on crafting the stone\npickaxe. Even with the inclusion of all human-designed\nactions from DEPS, it requires more than 7 million samples\nfor training.\nOur RL-GPT attains an over 8% success rate in the Ob-\ntainDiamond challenge by generating Python code and train-\ning a PPO RL neural network. Despite requiring some\nhuman-written code examples, our approach uses consid-\nerably fewer than DEPS. The final coded actions involve\nnavigating on the ground, crafting items, digging to a spe-\ncific level, and exploring the underground horizontally.\n4.4. Ablation Study\nWe present ablation studies on our core designs in Tab. 5,\nTab. 6, and Tab. 7, covering the framework structure, two-\nloop iteration, and RL interface.\nFramework Structure\nIn Tab. 5, we analyze the impact of\nthe framework structure in RL-GPT, specifically examining\ndifferent task assignments for various agents. Assigning\nall tasks to a single agent results in confusion due to the\nmultitude of requirements, leading to a mere 0.34% success\nrate in crafting a table. Additionally, comparing the 3rd\nand 4th rows emphasizes the crucial role of a critic agent\nin our pipeline. Properly assigning tasks to the fast, slow,\nand critic agents can improve the performance to 0.65%.\nThe slow agent faces difficulty in independently judging the\nsuitability of actions based solely on environmental feedback\nand observation. Incorporating a critic agent facilitates more\ninformed decision-making, especially when dealing with\ncomplex, context-dependent information.\nTwo-loop Iteration\nIn Tab. 6, we ablate the importance\nof our two-loop iteration. Our iteration is to balance RL\nand code-as-policy to explore the bound of GPT’s coding\nability. We can see that pure RL and pure code-as-policy\nonly achieve a low success rate on these chosen tasks. Our\nmethod can improve the results although there is no itera-\ntion (zero-shot). In these three iterations, it shows that the\nsuccessful rate increases. It proves that the two-loop itera-\ntion is a reasonable optimization choice. Qualitative results\ncan be found in Fig. 5. Besides, we also compare the re-\nsults with and without special prompts (SP) to encourage\nthe LLMs to further decompose actions when facing coding\ndifficulty. It shows that suitable prompts are also essential\nfor optimization.\nRL Interface\nRecent works [20, 25] explore the use of\nLLMs for RL reward design, presenting an alternative ap-\nproach to combining RL and code-as-policy. With slight\nmodifications, our fast agent can also generate code to de-\nsign the reward function. However, as previously analyzed,\nreconstructing the action space proves more efficient than\ndesigning the reward function, assuming LLMs understand\nthe necessary actions. Tab. 7 compares our method with the\nreward design approach, revealing that our method achieves\na higher average success rate and lower dead loop ratio on\nour selected MineDojo tasks.\n8\n5. Conclusion\nIn conclusion, we propose RL-GPT, a novel approach that\nintegrates Large Language Models (LLMs) and Reinforce-\nment Learning (RL) to empower LLMs agents on challeng-\ning tasks within complex, embodied environments. Our\ntwo-level hierarchical framework divides the task into high-\nlevel coding and low-level RL-based actions, leveraging the\nstrengths of both approaches. RL-GPT exhibits superior effi-\nciency compared to traditional RL methods and existing GPT\nagents, achieving remarkable performance in challenging\nMinecraft tasks.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023. 3\n[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebo-\ntar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu,\nKeerthana Gopalakrishnan, Karol Hausman, et al. Do as i\ncan, not as i say: Grounding language in robotic affordances.\narXiv preprint arXiv:2204.01691, 2022. 3\n[3] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie\nTang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro,\nand Jeff Clune. Video pretraining (vpt): Learning to act\nby watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, pages 24639–24654, 2022.\n2, 8\n[4] Thomas Carta, Cl´ement Romac, Thomas Wolf, Sylvain Lam-\nprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding\nlarge language models in interactive environments with on-\nline reinforcement learning. arXiv preprint arXiv:2302.02662,\n2023. 1\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-\nrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-\nating large language models trained on code. arXiv preprint\narXiv:2107.03374, 2021. 2\n[6] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan\nTompson, Quan Vuong, Tianhe Yu, et al.\nPalm-e: An\nembodied multimodal language model.\narXiv preprint\narXiv:2303.03378, 2023. 1, 3\n[7] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar,\nYuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar. Minedojo: Building\nopen-ended embodied agents with internet-scale knowledge.\nAdvances in Neural Information Processing Systems, pages\n18343–18362, 2022. 2, 5, 6, 14\n[8] Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng,\nand Zongqing Lu.\nLlama rider:\nSpurring large lan-\nguage models to explore the open world. arXiv preprint\narXiv:2310.08922, 2023. 2\n[9] Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao\nTan, and Yongfeng Zhang. Llm as os (llmao), agents as apps:\nEnvisioning aios, agents and the aios-agent ecosystem. arXiv\npreprint arXiv:2312.03815, 2023. 1\n[10] S Gravitas. Auto-gpt: An experimental open-source attempt\nto make gpt-4 fully autonomous. github. retrieved april 17,\n2023, 2023. 1, 3\n[11] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,\nYutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-\nworld webagent with planning, long context understanding,\nand program synthesis. arXiv preprint arXiv:2307.12856,\n2023. 1\n[12] William H Guss, Brandon Houghton, Nicholay Topin, Phillip\nWang, Cayden Codel, Manuela Veloso, and Ruslan Salakhut-\ndinov. Minerl: A large-scale dataset of minecraft demonstra-\ntions. arXiv preprint arXiv:1907.13440, 2019. 2\n[13] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2, 8\n[14] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng,\nJinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau,\nZijuan Lin, Liyang Zhou, et al. Metagpt: Meta program-\nming for multi-agent collaborative framework. arXiv preprint\narXiv:2308.00352, 2023. 1\n[15] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wen-\nmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong,\nMing Ding, et al. Cogagent: A visual language model for gui\nagents. arXiv preprint arXiv:2312.08914, 2023. 1\n[16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mor-\ndatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. In International\nConference on Machine Learning, pages 9118–9147, 2022. 3\n[17] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,\nPete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch,\nYevgen Chebotar, et al. Inner monologue: Embodied reason-\ning through planning with language models. arXiv preprint\narXiv:2207.05608, 2022. 3\n[18] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang,\nYongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandku-\nmar, Yuke Zhu, and Linxi Fan. Vima: General robot manipu-\nlation with multimodal prompts. arXiv, 2022. 3\n[19] Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas,\nNicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, Deheng\nYe, Qiang Fu, Wei Yang, et al. Minerl diamond 2021 compe-\ntition: Overview, results, and lessons learned. NeurIPS 2021\nCompetitions and Demonstrations Track, pages 13–28, 2022.\n2\n[20] Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou,\nYu Qiao, Xiaogang Wang, Hongsheng Li, Lewei Lu, and\nJifeng Dai. Auto mc-reward: Automated dense reward design\nwith large language models for minecraft. arXiv preprint\narXiv:2312.09238, 2023. 3, 8\n[21] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Haus-\nman, Brian Ichter, Pete Florence, and Andy Zeng. Code\nas policies: Language model programs for embodied con-\ntrol. In 2023 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 9493–9500, 2023. 3\n[22] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan\nWang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming\n9\nShi. Encouraging divergent thinking in large language models\nthrough multi-agent debate. arXiv preprint arXiv:2305.19118,\n2023. 3\n[23] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and\nSheila McIlraith. Steve-1: A generative model for text-to-\nbehavior in minecraft.\narXiv preprint arXiv:2306.00937,\n2023. 2\n[24] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei\nChang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.\nChameleon: Plug-and-play compositional reasoning with\nlarge language models. arXiv preprint arXiv:2304.09842,\n2023. 1, 3\n[25] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An\nHuang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi\nFan, and Anima Anandkumar. Eureka: Human-level reward\ndesign via coding large language models. arXiv preprint\narXiv:2310.12931, 2023. 3, 8\n[26] Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha\nKannan. Dera: enhancing large language model comple-\ntions with dialog-enabled resolving agents. arXiv preprint\narXiv:2303.17071, 2023. 3\n[27] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr,\nYejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy\nFox. Do embodied agents dream of pixelated sheep?: Embod-\nied decision making using language guided world modelling.\narXiv preprint arXiv:2301.12050, 2023. 1\n[28] Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bernstein.\nGenerative agents: Interactive simulacra of human behavior.\narXiv preprint arXiv:2304.03442, 2023. 3\n[29] Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating\nreinforcement learning with learned skill priors. In Confer-\nence on robot learning, pages 188–204. PMLR, 2021. 4\n[30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-\nford, and Oleg Klimov. Proximal policy optimization algo-\nrithms. arXiv preprint arXiv:1707.06347, 2017. 6\n[31] Ryan Shea and Zhou Yu. Building persona consistent dialogue\nagents with offline reinforcement learning. arXiv preprint\narXiv:2310.10735, 2023. 3\n[32] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,\nDanfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason,\nand Animesh Garg. Progprompt: Generating situated robot\ntask plans using large language models. In 2023 IEEE In-\nternational Conference on Robotics and Automation (ICRA),\npages 11523–11530, 2023. 3\n[33] Hao Sun.\nOffline prompt evaluation and optimization\nwith inverse reinforcement learning.\narXiv preprint\narXiv:2309.06553, 2023. 1, 3\n[34] Hao Sun, Alihan H¨uy¨uk, and Mihaela van der Schaar. Query-\ndependent prompt evaluation and optimization with offline\ninverse rl. arXiv e-prints, 2023. 3\n[35] Richard S Sutton, Doina Precup, and Satinder Singh. Between\nmdps and semi-mdps: A framework for temporal abstraction\nin reinforcement learning. Artificial intelligence, 112(1-2):\n181–211, 1999. 4\n[36] XAgent Team. Xagent: An autonomous agent for complex\ntask solving, 2023. 3\n[37] Ryan Volum, Sudha Rao, Michael Xu, Gabriel DesGarennes,\nChris Brockett, Benjamin Van Durme, Olivia Deng, Akanksha\nMalhotra, and William B Dolan. Craft an iron sword: Dynam-\nically generating interactive game characters by prompting\nlarge language models tuned on code. In Proceedings of\nthe 3rd Wordplay: When Language Meets Games Workshop\n(Wordplay 2022), pages 25–43, 2022. 2\n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.\nVoyager: An open-ended embodied agent with large language\nmodels. arXiv preprint arXiv:2305.16291, 2023. 1, 2\n[39] Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui\nWang, Boyang Xue, Hongyuan Lu, Fei Mi, and Kam-Fai\nWong. Tpe: Towards better compositional reasoning over\nconceptual tools with multi-persona collaboration. arXiv\npreprint arXiv:2309.16090, 2023. 1\n[40] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,\nJingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai\nLin, et al.\nA survey on large language model based au-\ntonomous agents. arXiv preprint arXiv:2308.11432, 2023.\n1\n[41] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing\nHou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng,\nYaodong Yang, et al. Jarvis-1: Open-world multi-task agents\nwith memory-augmented multimodal language models. arXiv\npreprint arXiv:2311.05997, 2023. 2\n[42] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao\nLiang. Describe, explain, plan and select: Interactive planning\nwith large language models enables open-world multi-task\nagents. arXiv preprint arXiv:2302.01560, 2023. 1, 2, 5, 8\n[43] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian\nLuo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward:\nAutomated dense reward function generation for reinforce-\nment learning. arXiv preprint arXiv:2309.11489, 2023. 3\n[44] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. Large language\nmodels as optimizers. arXiv preprint arXiv:2309.03409, 2023.\n1, 3\n[45] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang,\nChencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang,\nKaiyang Zhou, et al. Octopus: Embodied vision-language\nprogrammer from environmental feedback. arXiv preprint\narXiv:2310.08588, 2023. 1, 3\n[46] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao\nHuang, Bin Fu, and Gang Yu. Appagent: Multimodal agents\nas smartphone users. arXiv preprint arXiv:2312.13771, 2023.\n1\n[47] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,\nKarthik Narasimhan, and Yuan Cao. React: Synergizing\nreasoning and acting in language models. arXiv preprint\narXiv:2210.03629, 2022. 1, 3\n[48] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani,\nKuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis\nChiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al.\nLanguage to rewards for robotic skill synthesis. arXiv preprint\narXiv:2306.08647, 2023. 3\n[49] Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie,\nPenglin Cai, Hao Dong, and Zongqing Lu. Plan4mc: Skill\n10\nreinforcement learning and planning for open-world minecraft\ntasks. arXiv preprint arXiv:2303.16563, 2023. 3, 5, 6, 8\n[50] Haoqi Yuan, Zhancun Mu, Feiyang Xie, and Zongqing Lu.\nPre-training goal-based models for sample-efficient reinforce-\nment learning. In The Twelfth International Conference on\nLearning Representations, 2024. 2\n[51] Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing\nLu. Creative agents: Empowering agents with imagination\nfor creative tasks. arXiv preprint arXiv:2312.02519, 2023. 2\n[52] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zi-\nhan Zhao, and Kai Yu.\nLarge language model is semi-\nparametric reinforcement learning agent.\narXiv preprint\narXiv:2306.07929, 2023. 3\n[53] Wanpeng Zhang and Zongqing Lu. Rladapter: Bridging large\nlanguage models to reinforcement learning in open worlds.\narXiv preprint arXiv:2309.17176, 2023. 1, 3\n[54] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu.\nSteve-eye: Equipping llm-based embodied agents with visual\nperception in open worlds. arXiv preprint arXiv:2310.13255,\n2023. 2\n[55] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Wei-\njie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiao-\ngang Wang, et al. Ghost in the minecraft: Generally capable\nagents for open-world enviroments via large language mod-\nels with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023. 2\n11\nA. Agent Prompt Details\nPrompt details of fast and slow agent including {role description}, {planning tips},{act info}, and {obs info} are listed\nin Table 8,9,10.\n{role description}:\nYou are playing the game Minecraft. Assume you are a Python programmer. You want to write python code to complete some\nparts of this game.\n{planning tips}:\n1) If it is unsuccessful to code one action in the last round, it means the action is too difficult for coding.\n2) If one action in the last round is too difficult to code, try to further subdivide the action. For example, if ”attacking the tree 20\ntimes” is difficult, try ”simply attacking 20 times”.\n3) Please refer to the additional knowledge about Minecraft. It is very useful.\nTable 8. Slow Agent’s prompt details\n{role description}:\nWe want to write python code to complete some actions in Minecraft. You are a helpful assistant that helps to write the code for\nthe given action tasks.\n{act info}:\nWe design a compound action space. At each step the agent chooses one movement action (forward, backward, camera actions,\netc.) and one optional functional action (attack, use, craft, etc.). Some functional actions such as craft take one argument, while\nothers like attack does not take any argument. This compound action space can be modelled in an autoregressive manner.\nTechnically, our action space is a multi-discrete space containing eight dimensions:\n>>> env.action space\nMultiDiscrete([3, 3, 4, 25, 25, 8, 244, 36])\nIndex 0; Forward and backward; 0: noop, 1: forward, 2: back\nIndex 1; Move left and right; 0: noop, 1: move left, 2: move right\nIndex 2; Jump, sneak, and sprint; 0: noop, 1: jump, 2: sneak, 3:sprint\nIndex 3; Camera delta pitch; 0: -180 degree, 24: 180 degree\nIndex 4; Camera delta yaw; 0: -180 degree, 24: 180 degree\nIndex 5; Functional actions; 0: noop, 1: use, 2: drop, 3: attack, 4: craft, 5: equip, 6: place, 7: destroy\nIndex 6; Argument for “craft”; All possible items to be crafted\nIndex 7; Argument for “equip”, “place”, and “destroy”; Inventory slot indice\nTable 9. Fast Agent’s prompt details\n12\nobs[”rgb”]:\nRGB frames provide an egocentric view of the running Minecraft client that is the same as human players see.\nData type: numpy.uint8\nShape: (3, H, W), height and width are specified by argument image size\nobs[”inventory”][”name”]:\nNames of inventory items in natural language, such as “obsidian” and “cooked beef”.\nData type: str\nShape: (36,)\nWe also provide voxels observation (3x3x3 surrounding blocks around the agent). This type of observation is similar to how\nhuman players perceive their surrounding blocks. It includes names and properties of blocks.\nobs[”voxels”][”block name”]:\nNames of surrounding blocks in natural language, such as “dirt”, “air”, and “water”.\nData type: str\nShape: (3, 3, 3)\nobs[”location stats”][”pos”]:\nThe xyz position of the agent.\nData type: numpy.float32\nShape: (3,)\nobs[”location stats”][”yaw”] and obs[”location stats”][”pitch”]:\nYaw and pitch of the agent.\nData type: numpy.float32\nShape: (1,)\nobs[”location stats”][”biome id”]:\nBiome ID of the terrain the agent currently occupies.\nData type: numpy.int64\nShape: (1,)\nLidar observations are grouped under obs[”rays”]. It includes three parts: information about traced entities, properties of traced\nblocks, and directions of lidar rays themselves.\nobs[”rays”][”entity name”]:\nNames of traced entities.\nData type: str\nShape: (num rays,)\nobs[”rays”][”entity distance”]:\nDistances to traced entities.\nData type: numpy.float32\nShape: (num rays,)\nProperties of traced blocks include blocks’ names and distances from the agent.\nobs[”rays”][”block name”]:\nNames of traced blocks in natural language in the fan-shaped area ahead of the agent, such as “dirt”, “air”, and “water”.\nData type: str\nShape: (num rays,)\nobs[”rays”][”block distance”]:\nDistances to traced blocks in the fan-shaped area ahead of the agent.\nData type: numpy.float32\nShape: (num rays,)\nTable 10. Observation information {obs info} of Fast Agent\n13\nB. Algorithms\nAlgorithm 1 RL-GPT Two-loop Iteration\nInput: task T, Slow agent AS, Fast agent AF , Critic agent C, Prompt for slow agent PS, Prompt for fast agent PF .\nrepeat\nα0, ..., αn = AS(T, PS)\nfor i = 0 to n do\nrepeat\nCode = AF (αi, PF , Critici)\nact space = rl config(Code)\nObsi = rl training(act space)\nCritici = CF (rl config, code, Obsi)\nuntil no bug\nend for\nPS = PS + Critic0 + ... + Criticn\nuntil T is complete\nC. Details in PPO Implementations\nCLIP reward. The reward incentivizes the agent to generate behaviors aligned with the task prompt. 31 task prompts\nare selected from the entire set of MineDojo programmatic tasks as negative samples. Utilizing the pre-trained MineCLIP\nmodel [7], we calculate the similarities between the features extracted from the past 16 frames and the prompts. The\nprobability is then computed, indicating the likelihood that the frames exhibit the highest similarity to the given task prompt:\np = [softmax (S (fv, fl) , {S (fv, fl−)}l−)]0, where fv, fl are video features and prompt features, l is the task prompt, and\nl−are negative prompts. The CLIP reward is:\nrCLIP = max\n\u001a\np −1\n32, 0\n\u001b\n.\n(1)\nDistance reward. The distance reward offers dense reward signals for reaching target items. In combat tasks, the agent\nreceives a distance reward when the current distance is closer than the minimum distance observed in history:\nrdistance = max\n\u001a\nmin\nt′<t dt′ −dt, 0\n\u001b\n.\n(2)\nFor mining tasks involving\nor\n, where the agent needs to remain close to the block for several time steps, we adapt the\ndistance reward to promote maintaining a small distance:\nrdistance =\n\n\n\n\n\ndt−1 −dt,\n1.5 ≤dt ≤+∞\n2,\ndt < 1.5\n−2,\ndt = +∞,\n(3)\nwhere dt is the distance between the agent and the target item at time step t, detected through lidar rays in the simulator.\n14\nD. Details in Minecraft Tasks\nTable 11. Settings for MineDojo tasks in our paper.\nTask Icon\nTarget Name\nInitial Tools\nBiome\nMax Steps\nstick\n--\nplains\n3000\ncrafting table\nnearby\n--\nplains\n3000\nwooden pickaxe\n--\nforest\n3000\nfurnace nearby\n*10\nhills\n5000\nstone pickaxe\nforest hills\n10000\nmilk bucket\n,\n*3\nplains\n3000\nwool\n,\n*2\nplains\n3000\nbeef\nplains\n3000\nmutton\nplains\n3000\nbed\n,\nplains\n10000\n15\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/RL-GPT: Integrating Reinforcement Learning and Code-as-policy.pdf"}
{"title":"LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence","authors":"Zhuoling Li, Xiaogang Xu, Zhenhua Xu, SerNam Lim, Hengshuang Zhao","summary":"Recent embodied agents are primarily built based on reinforcement learning\n(RL) or large language models (LLMs). Among them, RL agents are efficient for\ndeployment but only perform very few tasks. By contrast, giant LLM agents\n(often more than 1000B parameters) present strong generalization while\ndemanding enormous computing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by conducting the proposed referee RL\non our developed large auto-regressive model (LARM). Specifically, LARM is\nbuilt upon a lightweight LLM (fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We mathematically reveal that\nclassic RL feedbacks vanish in long-horizon embodied exploration and introduce\na giant LLM based referee to handle this reward vanishment during training\nLARM. In this way, LARM learns to complete diverse open-world tasks without\nhuman intervention. Especially, LARM successfully harvests enchanted diamond\nequipment in Minecraft, which demands significantly longer decision-making\nchains than the highest achievements of prior best methods.","url":"http:\/\/arxiv.org\/abs\/2405.17424v2","pdf_url":"http:\/\/arxiv.org\/pdf\/2405.17424v2","published":1716832772000,"comment":null,"pdf_text":"LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence\nZhuoling Li 1 Xiaogang Xu 2 Zhenhua Xu 1 SerNam Lim 3 Hengshuang Zhao 1\nhttps:\/\/lizhuoling.github.io\/LARM_webpage\/\nAbstract\nRecent embodied agents are primarily built based\non reinforcement learning (RL) or large language\nmodels (LLMs). Among them, RL agents are\nefficient for deployment but only perform very\nfew tasks. By contrast, giant LLM agents (of-\nten more than 1000B parameters) present strong\ngeneralization while demanding enormous com-\nputing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by con-\nducting the proposed referee RL on our developed\nlarge auto-regressive model (LARM). Specifi-\ncally, LARM is built upon a lightweight LLM\n(fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We\nmathematically reveal that classic RL feedbacks\nvanish in long-horizon embodied exploration and\nintroduce a giant LLM based referee to handle\nthis reward vanishment during training LARM.\nIn this way, LARM learns to complete diverse\nopen-world tasks without human intervention. Es-\npecially, LARM successfully harvests enchanted\ndiamond equipment in Minecraft, which demands\nsignificantly longer decision-making chains than\nthe highest achievements of prior best methods.\n1. Introduction\nIn recent years, remarkable progress has been achieved\nin various artificial intelligence (AI) topics (LeCun et al.,\n2015) like computer vision (He et al., 2016) and natural\nlanguage processing (Kenton & Toutanova, 2019), but most\nof them lack the capacity to physically interact with the real\nworld. To address this disconnect, the concept of embodied\nAI is introduced (Chrisley, 2003). Early embodied agents\nare predominantly developed on simulation platforms for\nspecific tasks such as object grasping and indoor naviga-\ntion (Savva et al., 2019). While notable advancements are\n1The University of Hong Kong 2The Chinese University of\nHong Kong 3University of Central Florida. Correspondence to:\nHengshuang Zhao <hszhao@cs.hku.hk>.\nachieved, these agents tend to be specialist models confined\nto isolated tasks (Huang et al., 2023). To overcome this limi-\ntation, recent studies, including this work, employ Minecraft\n(Baker et al., 2022; Fan et al., 2022; Guss et al., 2019) as\na benchmark to explore embodied agents with open-ended\nobjectives and long-horizon reasoning chains.\nThe early methods for developing such agents primarily rely\non reinforcement learning (RL) (Fan et al., 2022). Due to the\nlimited exploration efficiency of RL, these methods require\ncareful reward engineering for different tasks, and the de-\nrived RL policies can mostly only complete a single simple\ntask (Yuan et al., 2023). The advantage of RL policies is that\nthey are usually lightweight for real-time deployment. Dif-\nferently, recent embodied works begin to investigate large\nlanguage models (LLMs) (Brown et al., 2020). Owing to\nthe extensive general knowledge and formidable reasoning\ncapabilities of LLMs, these methods demonstrate promis-\ning results with significantly reduced domain-specific engi-\nneering efforts (Wang et al., 2023a). Nevertheless, LLMs\ncontinue to exhibit several limitations. First of all, the out-\nputs of LLMs are usually sentences or code (Zhao et al.,\n2023) generated through iterative token prediction, necessi-\ntating N inference operations for N tokens. Therefore, the\nresponse speeds of LLMs are restricted. Secondly, recent\nresearch suggests that a huge model size is important for\nan LLM to generalize well (Achiam et al., 2023), while the\ncomputing resource for embodied agents is usually very lim-\nited. Our analysis reveals that while giant LLMs with more\nthan 1000B parameters like GPT-4 (Achiam et al., 2023)\ncan answer questions about exploration and crafting issues\nin Minecraft well, the performance of lightweight LLMs\nsuch as LLaVA-7B (Liu et al., 2024a) is limited.\nAs illustrated in Fig. 1, we aim to combine the advantages\nof both RL methods and LLM methods while avoiding\ntheir drawbacks. To this end, we first propose Large Auto-\nRegressive Model (LARM), the main body of which shares\nthe same structure as lightweight LLMs like TinyLLaVA\n(Zhou et al., 2024). This choice enables us to first pre-train\nit utilizing numerous webpage data to provide it with ba-\nsic general knowledge. Taking environmental observation\nas input, LARM predicts the next action to perform in an\nauto-regressive manner. Instead of generating a descriptive\nsentence composed of multiple tokens, LARM directly pro-\n1\narXiv:2405.17424v2  [cs.CV]  5 Feb 2025\nSubmission and Formatting Instructions for ICML 2025\nduces a single token to select the next action, which makes\nLARM respond more swiftly than common LLMs.\nThe following problem is how to train LARM. We find that\nclassic RL algorithms cannot train LARM effectively and\nmathematically reveal this is because the reward feedback\ngradually vanishes in long-horizon embodied exploration.\nThis phenomenon can be empirically understood as even\nthough a policy selects the correct action, it obtains positive\nfeedback only after the target task is completed, meaning\nmany iterations of delay. In addition, any wrong decision-\nmaking in future iterations will cause the policy to get no\npositive reward, which hides the value of the current correct\naction. To handle this problem, we introduce referee RL.\nIts core idea is that we employ a referee (like a giant LLM)\nto provide immediate feedback about whether the just per-\nformed action brings positive contribution to realizing the\nfinal target. In this way, we efficiently distill the concerned\ngeneralizable knowledge of giant LLMs into our lightweight\nend-to-end LARM policy during online exploration with-\nout human supervision. This marks the first attempt that\noptimizes an LLM-style embodied policy through making\nit directly interact with the environment online.\nWe validate our method in both MineDojo (Fan et al., 2022)\nand Mineflayer (PrismarineJS., 2013) environments. The\nexperimental results suggest that our method completes di-\nverse challenging tasks with a single model, indicating its\npromising generalization. LARM achieves higher success\nrates than previous counterparts, although these counter-\nparts may employ a special network for each task. Notably,\nLARM is the first method that harvests enchanted diamond\nequipment in Minecraft. In addition, evaluated with an\nRTX4090 GPU, LARM runs with a speed of 0.58 second\nper inference, which meets the online inference requirement.\n2. Related Work\nMinecraft agents. Compared with other embodied bench-\nmarks, Minecraft is an open-ended platform suitable for\nexploring building agents with long-horizon planning ca-\npabilities (Fan et al., 2022). It simulates diverse weather,\nbiomes, and mobs in an unlimited 3D virtual world. Early\nmethods in Minecraft are mostly based on reinforcement\nlearning (Frazier & Riedl, 2019) or imitation learning (Baker\net al., 2022). Their model outputs are atom actions, e.g., a\nshort movement, mouse click, or keyboard press. However,\ndue to the huge decision space, such atom-based agents are\nquite challenging for optimization. Thus, these works pay\ntheir main attention to devising strategies for alleviating the\noptimization complexity (Scheller et al., 2020). An effective\npractice is devising the policy into a hierarchical architec-\nture, where a complex task is first decomposed into many\nsimple sub-tasks. Different models are trained for various\nsub-tasks and a leader model is built to decide the order of\nperforming these sub-tasks (Liu et al., 2024b).\nDue to its open-world characteristic, Minecraft is suitable\nfor exploring how to develop open-ended embodied intelli-\ngence (Feng et al., 2024). To concentrate on studying this\nproblem, there are plentiful works that take a skill (e.g.,\nchopping down a tree or crafting a table) as the basic model\noutput (Wang et al., 2023a). The skill could be modeled\nas a well-trained policy based on reinforcement learning\nor provided APIs. Among these works, LLM-based meth-\nods achieve the most impressive results thanks to their rich\ngeneral LLM knowledge (Achiam et al., 2023; Wang et al.,\n2023b), especially for giant LLMs with more than 1000B pa-\nrameters like GPT-4. However, due to the huge model sizes,\nthese LLMs can only be deployed in remote computing\nclusters. There are also works that try tuning a lightweight\nLLM like LLaMA using Minecraft relevant text and then\nprompting the tuned LLM to say what skill should be per-\nformed in inference (Feng et al., 2024). However, the text\nused for tuning contains much irrelevant information and\nis not specialized for task execution. As the embodied text\ntuning data volume is also limited, the tuned LLMs often\nfail to describe what skill should be performed correctly.\nLarge language models. LLMs draw broad attention from\nthe research and industrial communities due to their rich\ngeneral knowledge and the ability to generate the answers\nto diverse kinds of questions (Chang et al., 2024). GPT-\n3 emerges as a milestone in the evolution of LLMs, as it\ntakes the next token prediction problem as the pre-training\ntask and showcases remarkable open-world generalization\ncapabilities (Brown et al., 2020). Subsequently, the fine-\ntuning of GPT-3 using reinforcement learning with human\nfeedback leads to the creation of ChatGPT (OpenAI, 2023)\nand GPT-4 (Achiam et al., 2023). However, a significant\nlimitation of LLMs is their inability to interpret information\nin images, which are vital for humans to perceive the world.\nTo overcome this problem, researchers devise strategies that\ninject vision information into LLMs and enable LLMs to\nperceive images. A common method is fine-tuning a small\nnumber of network parameters using numerous language-\nimage data pairs to bridge the representation gap between\ntext and images (Ding et al., 2023). In this way, some large\nvision-language models like LLaVA (Liu et al., 2024a) and\nFlamingo (Alayrac et al., 2022) are derived.\nRL with LLMs in embodied AI. RL can search promis-\ning decision-making policies without human intervention,\nand LLMs are able to provide suitable search start points\nbased on their rich general knowledge (Rashidi & Nili Ah-\nmadabadi, 2024). Therefore, it is natural to design ways to\ncombine them to build advanced embodied intelligence. In\nprevious works, a popular choice is training a network for\neach basic skill based on RL, and then prompting the LLM\nto say what skill should be used according to the task target\n2\nSubmission and Formatting Instructions for ICML 2025\nMore Advanced Achievements \nRL Agent \nEfficient but Task \nspecialized \nLARM Agent (Ours) \nEfficient and \nGeneralizable \nLLM Agent \nGeneralizable but Slow \nCommon RL Agent \nAchievements\nCommon LLM Agent \nAchievements\nLARM Agent \nAchievements\nFigure 1. Comparison among agents based on RL, LLM, and LARM. As shown, RL agents are usually task specialized, and LLM agents\nare computationally expensive to deploy. By contrast, the LARM agent is efficient and generalizable. Besides, LARM presents better\nperformance. As shown, LARM is the first method that achieves enchanted diamond equipment in Minecraft.\nand environment observation (Wang et al., 2023b). Never-\ntheless, this paradigm is not only slow, it requires the LLM\nto have sufficient knowledge about Minecraft. According to\nour analysis, giant LLMs own such an ability but the capa-\nbilities of lightweight LLMs are limited. Another possible\nchoice is utilizing LLM to generate the code for calculating\nRL reward (Xie et al., 2023). Nevertheless, it is not always\nfeasible to define a reward function by writing code. For ex-\nample, in Minecraft, the information is represented as image\nand agent status information, which cannot be mapped as\nreward based on rules. To handle this problem, we propose\nto directly employ GPT-4 to read the agent status before\nand after executing a skill and judge whether the outcome\nbrought by this skill contributes to realizing the given target.\nIn addition, to the best of our knowledge, this is the first\nwork that directly optimizes an LLM-style policy based on\nonline exploration and reinforcement learning. Our results\nsuggest that the rich general knowledge in LLM favors this\nexploration and self-learning process.\n3. Preliminary\n3.1. Problem Formulation\nWhat we study in this work can be conceptualized as an auto-\nregressive prediction problem involving long sequences,\nand is effectively framed as a Markov Decision Process\nsymbolized by a tuple E = (S, A, P, T , R, γ, τ). Specif-\nically, S is the set of all potential states. A is the action\nset, and every action is also called as a skill in this work.\nP : S ×A×S →[0, 1] represents a probability distribution\nthat governs the state transitions given states and actions.\nT is the set of all task targets. R : S →R, γ, and τ de-\nnote the reward function, discount factor, and initial state\ndistribution, respectively. At any discrete time step t, the\nenvironment resides in a state st ∈S, and the correspond-\ning observation ot by a policy π is a function of this state,\nexpressed as ot = f(st). This observation ot is then utilized\nto select the subsequent action according to at ∼π(ot, ι),\nwhere at ∈A and ι ∈T denotes the given target task.\nIn tackling the studied long-horizon embodied task, the ob-\njective is to navigate through a sequence of intermediate\nstates τ, s1, s2, . . . , sT −1 to ultimately reach the target state\nsT at the final time step T. This requires the policy to gener-\nate a series of actions a0, a1, . . . , aT −1 such that each action\nat transitions the environment from state st to the next state\nst+1 correctly, adhering to the dynamics prescribed by the\ntransition probability distribution P. It is crucial that each\nintermediate state st is accurately achieved in sequence to\nensure the policy attains the target state sT .\n3.2. PPO\nProximal Policy Optimization (PPO) (Schulman et al., 2017)\nis a model-free reinforcement learning algorithm widely\nadopted for training policies in complex environments, and\nour referee RL is developed based on this algorithm. A PPO\npolicy π mainly consists of two components, the actor πa\nand critic πc. πc is to estimate the value function Vθc(st),\nthe expected cumulative discounted reward starting from\nstate st and following π. The optimization objective of πc\nis as follows:\nLc\nθc = Et[(Vθc(st) −(rt + γVθc(st+1)))2],\n(1)\n3\nSubmission and Formatting Instructions for ICML 2025\nwhere rt ∈R is the reward received after taking action\nat in state st. To further reduce the variance of the value\nestimate and improve stability, PPO employs the generalized\nadvantage estimation (GAE), which is defined as:\nAt =\nT −1\nX\nk=t\n(γλ)k−tδk,\n(2)\nwhere λ is a factor balancing between temporal differ-\nence (TD) learning and Monte Carlo estimation, and δk\ndenotes the TD-error, which is formulated as δk = rk +\nγVθc(sk+1) −Vθc(sk). With At, the objective function for\nthe critic πa can be given as:\nLa\nθa = Et[ πa(at|st)\nπold\na (at|st)At],\n(3)\nwhere and πold\na\nis the old actor policy before weight update\nand πa(at|st) is the current actor. To improve the training\nstability, PPO further develops a clipped surrogate objective\nbased on Eq. (3), which can be formulated as:\n˜La\nθa = Et[min(ktAt, (kt, 1 −ϵ, 1 + ϵ)At)],\n(4)\nwhere kt =\nπa(at|st)\nπold\na\n(at|st) and ϵ is a small positive parameter.\nBy optimizing πc and πa with respect to Eq. (1) and Eq. (4),\nthe policy gradually learns to perform the target task.\n4. Method\n4.1. Referee Reinforcement Learning\nIn long-horizon embodied task exploration, the policy can\nusually only get positive feedback after the target task is\ncompleted successfully (Fan et al., 2022). Following the\nnotations in Section 3.1, we can assume that there is an\nexploration trajectory {(sk, ak, rk)}T\nk=t, where T is a large\ninteger, and sk, ak, and rk denote the state, action, and\nenvironment reward at the k step, respectively. The agent\ncompletes the target task at the final step T. Therefore, we\ncan get that:\nrk =\n(\n−ε, if k = t, t + 1, · · · , T −1\nR −ε, if k = T\n(5)\nwhere −ε denotes a small negative constant due to time\npenalty and R is the positive reward of completing the target\ntask. As we train the critic πc using the very imbalanced\nreward set {rk}T\nk=t described in Eq. (5) with respect to the\noptimization objective in Eq. (1), we can infer that the output\nof πc gradually converges to Vθc(sk) −γVθc(sk+1) ≈−ε.\nIn this way, the corresponding TD error set {δt}T −1\nt=1 is:\nδk ≈\n(\n0, if k = t, t + 1, · · · , T −2\nR, if k = T −1\n(6)\nAlgorithm 1 Referee RL\nRequire: Target task ι\n1: Initialize the actor πa, critic πc, referee πr\n2: Initialize policy exploration step T, policy update steps\nNπ\n3: for each iteration iter do\n4:\nInitialize data buffer B ←∅\n5:\nfor t = 1 to T do\n6:\nGet the actor observation ot ←f(st)\n7:\nGet state st+1 and environment reward rt by tak-\ning at ∼πθ(ot, ι)\n8:\nGet auxiliary reward brt ←πr(ι, st, at, at+1)\n9:\nAdd transition B ←B∪{(st, ot, at, st+1, rt, brt)}\n10:\nend for\n11:\nfor n = 1 to Nπ do\n12:\nSample\na\nrandom\ntraining\ndata\nbatch\n{(st, ot, at, st+1, rt, brt}B\nj=1 ∼B\n13:\nOptimize πa and with respect to Eq. (1)∼(4)\n14:\nend for\n15: end for\nAccording to the definition of GAE in Eq. (2), we can\nobserve that the first T −1 −t items are close to zero\nas the elements in {σk}T −2\nk=t are nearly zero. For the last\nitem (γλ)T −1−tδT −1, we have limT →∞(γλ)T −1−t →0\nbecause γ ∈(0, 1) and λ ∈(0, 1). Hence, when the task\nneeds long-horizon action chain execution, the obtained\nGAE value At is close to zero, which suggests that the opti-\nmization objective for training πa in Eq. (4) becomes zero.\nIn this way, even though the policy makes the right action\ndecision, it cannot get any positive feedback.\nTo handle this problem, we introduce referee RL. Specif-\nically, we introduce a referee πp to provide an auxiliary\nreward feedback to the trained policy π, and this reward at\nthe step k is represented as brk = πp(ι, sk, ak, sk+1). This\nmeans πp takes the target task information ι, initial state sk,\nselected action ak, and new state sk+1 as input and provides\nfeedback based on whether the selected action is correct\nand outcome caused by this action. In this work, we split\nthe feedback into four categories: (a) The selected action\nis correct and brings a positive outcome to realizing the\ntarget. (b) The selected action is correct but does not bring\na positive outcome. (c) The selected action is incorrect but\ndoes not lead to a negative outcome. (d) The selected action\nis incorrect and results in a negative outcome. For the four\ncategories, πp correspondingly returns the reward value ra,\nrb, rc, and rd, where ra > rb > 0 > rc > rd. By adding\nthis auxiliary reward brk to the original reward described in\nEq. (5), the feedback to π before T does not remain as the\nconstant −ε. In this way, the TD error δk and correspond-\ning GAE value At do not converge to zeros, being able to\nprovide effective optimization guidance to πa.\n4\nSubmission and Formatting Instructions for ICML 2025\nVision Observation\nText Observation\n🚩\nTask Description\nSkill Token\nEnvironment Exploration\nLoRA\nLLM Decoders\nActor Head\nCritic Head\nAction\nValue\nLARM Policy\nAgent\nEnvironment\nEnvironment Reward\nReferee\nAuxiliary \nReward\n👍👍👍\nTotal Reward\nReward Generation\nAgent State\nRL based Policy Optimization\nUpdate\nAgent Status: …\nFigure 2. The overall pipeline of our method. As illustrated, we parametrize the actor πa and critic πc using a single LARM model with\ntwo separate prediction heads, i.e., the action head and critic head. We train LARM based on our proposed referee RL algorithm, which\nutilizes both environment feedback and referee generated auxiliary reward to guide the optimization of LARM.\nIn this work, we model the referee πp based on GPT-4\n(Achiam et al., 2023), which is a giant LLM and owns\nextensive generalizable knowledge. As mentioned before,\nthe information provided to GPT-4 includes ι (target task\ndescription), ak (the executed skill), sk and sk+1 (the inven-\ntory list and environment resource surrounding the agent\nbefore and after executing ak), and then we prompt it to\njudge the situation and response a reward among ra, rb,\nrc, and rd. The full detailed procedure of the referee RL\nalgorithm is elaborated in Algorithm 1.\n4.2. Large Auto-Regressive Model\nIn this part, we explain how to parametrize πa and πc using\nour designed LARM policy. As shown in Fig. 2, the main\nbody of LARM is the decoders of a lightweight decoder-\nonly LLM, TinyLLaVA-3.1B in this work. The parameters\nof these decoders are frozen during training and a trainable\nLoRA (Hu et al., 2021) module is applied to help the model\nlearn new knowledge in the applied task domain. This\ndesign has two key benefits: (i) The model is initialized with\nthe general knowledge and reasoning ability of LLMs while\nmaintaining an acceptable parameter volume. In this way,\nLARM can be deployed based on the restricted computing\nresources in embodied applications and achieve real-time\nresponse. (ii) As LARM adopts a similar model architecture\nas LLMs, we can first pre-train it using numerous question-\nanswer data related to the concerned embodied AI topics.\nThis kind of data is much easier to collect and scale up\nthan embodied data with action execution. We have tried\npre-training LARM using a 34G webpage dataset crawled\nfrom Wiki (Fan et al., 2022), and the results indicate that\nthe training convergence is improved.\nThe input to the LARM model consists of four parts, i.e.,\ntask description, text observation, vision observation, and\na skill token. The task description specifies the target task\nto conduct. The text information primarily includes the in-\nventory list, historical action, and blocks surrounding the\nagent. The vision information is the real-time image per-\nceived by the agent. We encode text and image information\nas tokens based on CLIP (Radford et al., 2021), and these\ntokens with an additional learnable skill token are input to\nthe LARM decoders to conduct feature interaction. After\nthe decoders, the skill token is input to the action head and\ncritic head depicted in Fig. 2 to output the action and state\nvalue. Therefore, LARM parametrizes the actor πa and\ncritic πc in Section 4.1 based on a single model with two\ndifferent trainable prediction heads.\nSimilar to previous literature (Wang et al., 2023a; Liu et al.,\n2024b), the action predicted by LARM is a skill, such as\nchopping down a tree or searching for a cobblestone. LARM\nselects a skill to perform by conducting feature matching\nbetween the action head output and skill description like\nSTEVE (Zhao et al., 2023). In this work, we test LARM\nwith two kinds of skills, the RL-based skills in MineDojo\nand API-based skills based on Mineflayer. The former cat-\negory is easier to be extended to real-world applications\nand the latter class presents higher execution success rates.\n5\nSubmission and Formatting Instructions for ICML 2025\nAccording to the given task requirements, new skills can\nbe dynamically added based on the strategies developed in\nprevious works (Wang et al., 2023a) and how to generate\nthese skills is not the research focus of this work.\nCombining the aforementioned techniques, LARM is op-\ntimized by iterating between environment exploration and\npolicy update described in Algorithm 1. The training de-\ntails like the optimizer choice follow PPO (Schulman et al.,\n2017). For learning to complete the most challenging task\nin this work (craft an enchanted diamond tool), about 42\nhours of exploration is taken using a single RTX4090 GPU.\n5. Experiments\n5.1. Environment\nIn this work, we validate our method using the MineDojo\nand Mineflayer environments. Both these two environments\nare developed based on Minecraft, but there exist some\ndifferences in their testing protocols.\nMineDojo. MineDojo (Fan et al., 2022) is a pioneering\nbenchmark suitable for studying how to develop open-ended\nand generally capable embodied agents. It features a simula-\ntion suite with thousands of tasks specified through natural\nlanguage prompts. The behaviors that can be conducted in\nMineDojo primarily include navigation, harvest, combat,\nand craft. When testing a policy, an agent is randomly ini-\ntialized in a biome with some initial tools. The policy needs\nto control the agent to explore and harvest resources in the\nenvironment and gradually realize the given target. The ac-\ntion space in MineDojo is a compound multi-discrete space\nthat allows the agent to select a movement action or an op-\ntional functional action at each step, encompassing a diverse\nrange of arguments to facilitate complex interactions.\nMineflayer. Compared with MineDojo, the basic actions in\nMineflayer (PrismarineJS., 2013) are provided APIs, such\nas harvesting a log or finding the nearest stone. Compared\nwith MineDojo, these APIs help researchers concentrate\nmore on high-level decision making rather than repetitive\naction details. In this way, several significantly more ad-\nvanced achievements are obtained by previous works based\non Mineflayer, like harvesting diamonds with high success\nrates (Wang et al., 2023a). In addition, the agents are usually\nspawned without initial tools in Mineflayer.\n5.2. Main Results\nWe compare LARM with previous methods in this part. As\nthe basic actions in MineDojo and Mineflayer are different,\nwe conduct the comparison separately.\nComparison on MineDojo. The methods compared on\nMineDojo include MineAgent (Fan et al., 2022), Plan4MC\n(Yuan et al., 2023), LLaMA-Rider (Feng et al., 2024), and\nRL-GPT (Liu et al., 2024b). Among them, MineAgent is the\nbaseline method provided by Minddojo. It first fine-tunes\nCLIP (Radford et al., 2021) based on numerous web data\nand uses the fine-tuned CLIP to guide the training of rein-\nforcement learning algorithms. Plan4MC is a reinforcement\nlearning based method. It splits a task into basic skills and\ntrains an agent to learn them one by one in a hierarchical\nway. LLaMA-Rider is an LLM obtained by fine-tuning\nLLaMA. It first makes the agent explore the environment to\ncollect data. Then, the collected data is adopted to fine-tune\nLLaMA in a supervised manner. RL-GPT builds two LLM\nbased agents (a slow agent and a fast agent) to schedule the\nagent actions. For an action step, this method first queries\nthe agents whether this action step can be completed via\ncode generation. If the code generation is infeasible, an RL\nbased action is performed.\nWe compare LARM with these methods on diverse tasks,\nand the detailed settings of these tasks follow previous works\n(Feng et al., 2024). Notably, we train a single LARM model\nto complete all the tasks, which is different from many pre-\nvious works that employ separate models for various tasks.\nThis characteristic suggests the promising generalization\nability of LARM. To compute success rates, we test LARM\nfor 30 times on every task. The experimental results are re-\nported in Table 1. As shown, LARM presents higher success\nrates than the compared counterparts in all the test tasks, and\nthe promising performance of LARM is mainly attributed\nto our designed referee RL algorithm, which addresses the\nreward vanishment problem in long-horizon embodied ex-\nploration. In addition, we can observe that LARM achieves\nhigher success rates on tasks with shorter action chains,\nindicating the great challenge in developing long-horizon\nembodied intelligence. For example, the Harvest bucket\ntask requires three iron ingots, and the Harvest iron sword\ndemands two iron ingots and one stick. Therefore, Harvest\niron sword needs one more step (Harvest stick) than Harvest\nbucket, which causes that LARM obtains a higher success\nrate in the task of crafting a bucket.\nComparison on Mineflayer. To fully reveal the superiority\nof LARM, we further evaluate LARM using the Mineflayer\nbased environment. The compared methods include Auto-\nGPT (autogpt), Voyager (Wang et al., 2023a), and STEVE\n(Zhao et al., 2023). In this work, Voyager is a training-free\nmethod implemented based on GPT-4. Its main contribution\nis designing a multi-step prompt generation pipeline. When\na target task is given, Voyager prompts GPT-4 to know\nwhich skill should be executed and gradually realize the\ntarget. AutoGPT is an LLM being able to reason which skill\nshould be performed through multi-step question answering.\nSTEVE is a large vision-language model. In STEVE, a\ndataset including both videos and text-image pairs is gath-\nered and utilized to fine-tune LLaMA (Touvron et al., 2023),\nand then the fine-tuned model can invoke pre-defined skills.\n6\nSubmission and Formatting Instructions for ICML 2025\nTable 1. Performance comparison with previous methods based on MineDojo.\nTask\nMineAgent\nPlan4MC\nLLaMA-Rider Base\nLLaMA-Rider\nRL-GPT\nLARM (Ours)\nHarvest stick\n0.00\n0.30\n0.23\n0.43\n0.65\n0.93\nHarvest crafting table\n0.03\n0.30\n0.37\n0.67\n0.65\n0.87\nHarvest bowl\n0.00\n0.47\n0.73\n0.97\n-\n0.97\nHarvest chest\n0.00\n0.23\n0.67\n0.77\n-\n0.83\nHarvest wooden pickaxe\n0.00\n0.03\n0.00\n0.37\n0.67\n0.70\nHarvest wooden sword\n0.00\n0.47\n0.63\n0.10\n-\n0.70\nHarvest furnace\n0.00\n0.37\n0.00\n0.17\n0.67\n0.73\nHarvest stone stairs\n0.00\n0.47\n0.00\n0.57\n-\n0.67\nHarvest stone sword\n0.00\n0.10\n0.00\n0.00\n-\n0.40\nHarvest iron ingot\n0.00\n0.47\n0.03\n0.13\n-\n0.60\nHarvest bucket\n0.00\n0.20\n0.00\n0.00\n0.37\nHarvest iron sword\n0.00\n0.20\n0.00\n0.00\n-\n0.27\nHarvest beef\n0.33\n0.43\n0.03\n0.03\n0.46\n0.60\nHarvest mutton\n0.35\n0.33\n0.00\n0.03\n0.38\n0.63\nTable 2. Performance comparison based on Mineflayer.\nAchievement\nAutoGPT Voyager STEVE LARM (Ours)\nWooden sword\n3\/3\n3\/3\n3\/3\n30\/30\nStone sword\n3\/3\n3\/3\n3\/3\n30\/30\nIron sword\n3\/3\n3\/3\n3\/3\n30\/30\nDiamond sword\n0\/3\n1\/3\n3\/3\n28\/30\nEnchanted sword\n0\/3\n0\/3\n0\/3\n16\/30\nTable 3. Ablation Study on Reward Design.\nReward\nStick\nWooden\nStone\nIron\nER\n0.20\n0.13\n0.10\n0.00\nER+LAR\n0.30\n0.23\n0.13\n0.00\nER+AR2\n0.80\n0.53\n0.20\n0.07\nER+AR4\n0.93\n0.70\n0.40\n0.27\nIn this experiment, the agent is spawned in a random biome\nwithout initial inventory. The test tasks include Harvest\nwooden sword, Harvest stone sword, Harvest iron sword,\nHarvest diamond sword, and Harvest enchanted diamond\nsword. As shown in Fig. 1, Harvest enchanted diamond\nsword is significantly more complex than the other achieve-\nments. The previous methods usually test their models\nthree times in Mineflayer. To reduce randomness, we run\nLARM for 30 times. The experimental results are reported\nin Table 2. We can observe that LARM outperforms the\ncompared methods in obtaining different levels of achieve-\nments. Notably, LARM is the first method that harvests an\nenchanted diamond sword in Minecraft successfully.\n5.3. Ablation Study\nAnalysis on reward design. The key difference of Ref-\neree RL from the classic PPO implementation is the reward\ndesign. Therefore, we ablate different reward settings in\nthis part. We compare four reward choices in MineDojo\nwith four tasks, i.e., Harvest stick, Harvest wooden sword,\nHarvest stone sword, and Harvest iron sword. The four re-\nward choices are ER (environment reward only), ER+LAR\n(environment reward plus auxiliary reward produced by\nLLaVA-7B, a lightweight LLM), ER+AR2 (environment\nTable 4. Ablation Study on LLM base selection.\nLLM Base\nStick\nWooden\nStone\nIron\nTinyLLaVA-0.5B\n0.80\n0.50\n0.27\n0.13\nTinyLLaVA-3.1B\n0.83\n0.57\n0.33\n0.13\nTinyLLaVA-3.1B*\n0.93\n0.70\n0.40\n0.27\nreward plus auxiliary reward produced by GPT-4o, but the\nauxiliary reward is determined based on only whether the ac-\ntion outcome is positive), and ER+AR2 (the standard setting\nof our method described in this paper). The experimental\nresults are presented in Table 3.\nAs shown, when only the environment reward is provided,\nthe agent presents a significantly better success rate on Har-\nvest stick than Harvest stone sword, where the latter task\ndemands a longer action chain. This phenomenon is because\nthat the environment reward gradually vanishes with the in-\ncrease of the action chain length. When the auxiliary reward\nis generated by LLaVA-7B, the result is also poor, which is\nbecause that LLaVA-7B does not understand the Minecraft\nworld well and the provided reward is often incorrect. This\nresult confirms our aforementioned claim that only giant\nLLMs serve as referees well. In addition, the performance\nof ER+AR2 is worse than ER+AR4, which is because se-\nlecting the correct action does not always bring a positive\noutcome. For example, the agent decides to search a tree\nbut does not find it successfully. In this situation, ER+AR2\nreturns a negative reward, which punishes the searching a\ntree decision even though this choice may be correct. By\ncontrast, ER+AR4 will give a small positive reward.\nAnalysis on LLM base selection. As mentioned before,\nwe utilize the weight of a lightweight LLM to initialize\nthe parameters in LARM to provide initial general knowl-\nedge. Then, we fine-tune LARM using Minecraft related\nwebpage data to enhance its understanding. In this experi-\nment, we analyze how the lightweight LLM choice affects\nthe results and whether the webpage data pre-training de-\nscribed in Section 4.2 is beneficial. To this end, we first\ncompare the performances of LARM using TinyLLaVA-\n7\nSubmission and Formatting Instructions for ICML 2025\nEnter the Nether\nSearch a Village \nMulti-agent Combat\nt!\nt\"\nt#\nt$\nt!\nt\"\nt#\nt$\nt!\nt\"\nt#\nt$\nFigure 3. More behavior example illustrations of LARM, which include traveling a long distance to find a village, building a nether portal\nand then entering the nether, multiple agents collaborate with each other to combat zombies.\n0.5B and TinyLLaVA-3.1B as the LLM base, where the\nexperiment results are presented in the 1st and 2nd row of\nTable 4. As shown, adopting an LLM with more parameters\nas the LARM model base favors the embodied performance,\nwhich is mainly attributed to that a larger LLM has stronger\ngeneral knowledge. Then, we compare the performances of\nmodels without and with the webpage data pre-training, and\nthe results are reported in the 2nd and 3rd rows of Table 4. It\ncan be observed that webpage data pre-training improves the\nsuccess rates on embodied task execution. This result sug-\ngests that the numerous text and image data on the Internet\nhas the potential to benefit embodied agents.\nAnalysis on LLM capability. We have mentioned that\ngiant LLMs answer questions about the Minecraft world\nwell while the understanding of lightweight LLMs is poor.\nIn this part, we demonstrate this by providing a question-\nanswer example and highlighting the key content in bold:\nPrompt: In Minecraft, you need to craft a stone pickaxe.\nWhat additional resources do you need to gather if you have\nonly got cobblestones in your inventory?\nGPT-4o: If you already have cobblestone in your inventory,\nyou need to gather wooden planks to craft sticks. You need\ntwo sticks and three cobblestones to craft a stone pickaxe.\nLlama3-8B: You need a stick.\nTinyLLaVA-3.1B: To craft a stone pickaxe in Minecraft, you\nwill need the following additional resources: cobblestone,\nstone, wood, leaves, dirt, grass, pillar, shovel, and sword.\nTinyLLaVA-3.1B after Webpage data pre-training: You ad-\nditionally need two sticks.\nComparing the answers, we can observe that webpage data\npre-training significantly improves the concerned domain\nof knowledge in TinyLLaVA-3.1B.\n5.4. Case Study\nThe previous experiments mainly show the performances\nof LARM on harvesting various categories of materials\nand crafting tools. In Fig. 3, we visualize more examples\nof other behaviors in the Mineflayer based environment,\nsuch as exploring the open world, constructing a building\nwith a specific structure, and multiple agents cooperate to\ncombat dangerous creatures. These behaviors suggest that\nour proposed techniques in this work have the potential to\nbe further generalized to other diverse domains.\n6. Conclusion\nIn this work, we have proposed LARM, which is effi-\ncient and possesses general knowledge. To train it, we\nhave revealed the feedback vanishment problem in apply-\ning classic RLs to long-horizon embodied exploration. To\naddress this feedback vanishment, we have developed the\nreferee RL technique. By optimizing LARM with referee\n8\nSubmission and Formatting Instructions for ICML 2025\nRL, our method can learn to complete diverse embodied\ntasks without human supervision. Especially, LARM is the\nfirst method that obtains the enchanted diamond equipment\nachievement in the Minecraft benchmark successfully.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,\nI., Aleman, F. L., Almeida, D., Altenschmidt, J., Alt-\nman, S., Anadkat, S., et al.\nGpt-4 technical report.\narXiv:2303.08774, 2023.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. NeurIPS, 2022.\nautogpt.\nSignificant-gravitas\/auto-gpt:\nAn\nex-\nperimental\nopen-source\nattempt\nto\nmake\ngpt-\n4\nfully\nautonomous.,\n2023.\nURL\nhttps:\n\/\/github.com\/Significant-Gravitas\/\nAuto-GPT\/tree\/master.\nBaker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J.,\nEcoffet, A., Houghton, B., Sampedro, R., and Clune,\nJ. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. NeurIPS, 2022.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nNeurIPS, 2020.\nChang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K.,\nChen, H., Yi, X., Wang, C., Wang, Y., et al. A survey on\nevaluation of large language models. ACM Transactions\non Intelligent Systems and Technology, 15(3):1–45, 2024.\nChrisley, R. Embodied artificial intelligence. Artificial\nIntelligence, 2003.\nDing, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu,\nS., Chen, Y., Chan, C.-M., Chen, W., et al. Parameter-\nefficient fine-tuning of large-scale pre-trained language\nmodels.\nNature Machine Intelligence, 5(3):220–235,\n2023.\nFan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu,\nH., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A.\nMinedojo: Building open-ended embodied agents with\ninternet-scale knowledge. NeurIPS, 2022.\nFeng, Y., Wang, Y., Liu, J., Zheng, S., and Lu, Z. Llama-\nrider: Spurring large language models to explore the open\nworld. In NAACL, 2024.\nFrazier, S. and Riedl, M. Improving deep reinforcement\nlearning in minecraft with action advice. In AAAI, 2019.\nGuss, W. H., Houghton, B., Topin, N., Wang, P., Codel, C.,\nVeloso, M., and Salakhutdinov, R. Minerl: a large-scale\ndataset of minecraft demonstrations. In IJCAI, 2019.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In CVPR, 2016.\nHu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,\nL., Chen, W., et al. Lora: Low-rank adaptation of large\nlanguage models. In ICLR, 2021.\nHuang, C., Mees, O., Zeng, A., and Burgard, W. Visual\nlanguage maps for robot navigation. In ICRA, 2023.\nKenton, J. D. M.-W. C. and Toutanova, L. K. Bert: Pre-\ntraining of deep bidirectional transformers for language\nunderstanding. In NAACL, 2019.\nLeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-\nture, 2015.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. NeurIPS, 36, 2024a.\nLiu, S., Yuan, H., Hu, M., Li, Y., Chen, Y., Liu, S., Lu, Z.,\nand Jia, J. Rl-gpt: Integrating reinforcement learning and\ncode-as-policy. arXiv preprint arXiv:2402.19299, 2024b.\nOpenAI.\nChatgpt.\nhttps:\/\/openai.com\/blog\/\nchatgpt\/, 2023.\nPrismarineJS. Prismarinejs\/mineflayer: Create minecraft\nbots with a powerful, stable, and high level javascript api.\n2013.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In ICML, 2021.\nRashidi, A. and Nili Ahmadabadi, M. A survey on en-\nhancing reinforcement learning in complex environments:\nInsights from human and llm feedback. arXiv preprint\narXiv:2411.13410, 2024.\nSavva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans,\nE., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., et al.\nHabitat: A platform for embodied ai research. In ICCV,\n2019.\nScheller, C., Schraner, Y., and Vogel, M. Sample efficient\nreinforcement learning through learning from demonstra-\ntions in minecraft. In NeurIPS Workshop, 2020.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\n9\nSubmission and Formatting Instructions for ICML 2025\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation\nlanguage models. arXiv:2302.13971, 2023.\nWang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C.,\nZhu, Y., Fan, L., and Anandkumar, A. Voyager: An\nopen-ended embodied agent with large language models.\narXiv:2305.16291, 2023a.\nWang, Z., Cai, S., Chen, G., Liu, A., Ma, X., and Liang, Y.\nDescribe, explain, plan and select: Interactive planning\nwith large language models enables open-world multi-\ntask agents. arXiv:2302.01560, 2023b.\nXie, T., Zhao, S., Wu, C. H., Liu, Y., Luo, Q., Zhong, V.,\nYang, Y., and Yu, T. Text2reward: Automated dense\nreward function generation for reinforcement learning.\narXiv preprint arXiv:2309.11489, 2023.\nYuan, H., Zhang, C., Wang, H., Xie, F., Cai, P., Dong, H.,\nand Lu, Z. Skill reinforcement learning and planning for\nopen-world long-horizon tasks. In NeurIPS Workshop,\n2023.\nZhao, Z., Chai, W., Wang, X., Boyi, L., Hao, S., Cao, S., Ye,\nT., Hwang, J.-N., and Wang, G. See and think: Embodied\nagent in virtual environment. arXiv:2311.15209, 2023.\nZhou, B., Hu, Y., Weng, X., Jia, J., Luo, J., Liu, X.,\nWu, J., and Huang, L.\nTinyllava: A framework of\nsmall-scale large multimodal models.\narXiv preprint\narXiv:2402.14289, 2024.\n10\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence.pdf"}
{"title":"Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification","authors":"Yuxuan Guo, Shaohui Peng, Jiaming Guo, Di Huang, Xishan Zhang, Rui Zhang, Yifan Hao, Ling Li, Zikang Tian, Mingju Gao, Yutai Li, Yiming Gan, Shuai Liang, Zihao Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen","summary":"Building open agents has always been the ultimate goal in AI research, and\ncreative agents are the more enticing. Existing LLM agents excel at\nlong-horizon tasks with well-defined goals (e.g., `mine diamonds' in\nMinecraft). However, they encounter difficulties on creative tasks with open\ngoals and abstract criteria due to the inability to bridge the gap between\nthem, thus lacking feedback for self-improvement in solving the task. In this\nwork, we introduce autonomous embodied verification techniques for agents to\nfill the gap, laying the groundwork for creative tasks. Specifically, we\npropose the Luban agent target creative building tasks in Minecraft, which\nequips with two-level autonomous embodied verification inspired by human design\npractices: (1) visual verification of 3D structural speculates, which comes\nfrom agent synthesized CAD modeling programs; (2) pragmatic verification of the\ncreation by generating and verifying environment-relevant functionality\nprograms based on the abstract criteria. Extensive multi-dimensional human\nstudies and Elo ratings show that the Luban completes diverse creative building\ntasks in our proposed benchmark and outperforms other baselines ($33\\%$ to\n$100\\%$) in both visualization and pragmatism. Additional demos on the\nreal-world robotic arm show the creation potential of the Luban in the physical\nworld.","url":"http:\/\/arxiv.org\/abs\/2405.15414v1","pdf_url":"http:\/\/arxiv.org\/pdf\/2405.15414v1","published":1716546359000,"comment":null,"pdf_text":"Luban: Building Open-Ended Creative\nAgents via Autonomous Embodied Verification\nYuxuan Guo1, 2, 3\nShaohui Peng6\nJiaming Guo2\nDi Huang2\nXishan Zhang2, 3\nRui Zhang2\nYifan Hao2\nLing Li6\nZikang Tian2, 3, 4\nMingju Gao2, 3, 4\nYutai Li2, 3, 4\nYiming Gan7\nShuai Liang7\nZihao Zhang2\nZidong Du2, 5\nQi Guo2\nXing Hu2, 5 ∗\nYunji Chen2, 4 ∗\n1University of Science and Technology of China\n2State Key Lab of Processors, Institute of Computing Technology, CAS\n3Cambricon Technologies\n4University of Chinese Academy of Sciences\n5Shanghai Innovation Center for Processor Technologies\n6Intelligent Software Research Center, Institute of Software, CAS\n7Institute of Computing Technology, CAS\ngyx_20170818@mail.ustc.edu.cn, {huxing, cyj}@ict.ac.cn\nAbstract\nBuilding open agents has always been the ultimate goal in AI research, and creative\nagents are the more enticing. Existing LLM agents excel at long-horizon tasks with\nwell-defined goals (e.g., ‘mine diamonds’ in Minecraft). However, they encounter\ndifficulties on creative tasks with open goals and abstract criteria due to the inability\nto bridge the gap between them, thus lacking feedback for self-improvement in\nsolving the task. In this work, we introduce autonomous embodied verification\ntechniques for agents to fill the gap, laying the groundwork for creative tasks.\nSpecifically, we propose the Luban agent target creative building tasks in Minecraft,\nwhich equips with two-level autonomous embodied verification inspired by human\ndesign practices: (1) visual verification of 3D structural speculates, which comes\nfrom agent synthesized CAD modeling programs; (2) pragmatic verification of the\ncreation by generating and verifying environment-relevant functionality programs\nbased on the abstract criteria. Extensive multi-dimensional human studies and\nElo ratings show that the Luban completes diverse creative building tasks in our\nproposed benchmark and outperforms other baselines (33% to 100%) in both\nvisualization and pragmatism. Additional demos on the real-world robotic arm\nshow the creation potential of the Luban in the physical world.\n1\nIntroduction\nDeveloping open-ended agents capable of autonomously solving complex tasks [1, 11, 12, 18, 26,\n33, 34] directed by high-level abstract instructions is the ultimate goal of artificial intelligence\n(AI) techniques. Within this endeavor, creative tasks stand out as particularly enticing [3, 6, 37].\nUnlike conventional long-horizontal complex tasks, creative tasks do not have well-defined or easily\nautomated success criteria [6], thus stimulating the emergence of more advanced AI techniques with\nhigher intellectual capabilities and the potential to tackle real-world problems.\n∗Corresponding author.\nPreprint. Under review.\narXiv:2405.15414v1  [cs.AI]  24 May 2024\nFigure 1: (a) Agents for Well-defined long-horizontal tasks v.s. (b) Luban agent for creative tasks.\nExisting LLM agent techniques show promising progress in handling conventional long-horizon tasks\nwith well-defined goals related to environment states. For instance, as shown in Figure 1, consider the\nclassic ‘mining diamonds’ task in Minecraft, where it is easy to verify by just checking the diamond\nnumber in the inventory. Empowered by LLM’s rich semantic knowledge and reasoning capabilities\n[25, 31], these agents [33, 34, 39] precisely assess the distance between current states and the goals,\nthen reflect and replan based on the assessment accordingly, eventually solving diamond mining tasks\niteratively.\nHowever, when confronted with creative tasks, existing agents encounter a significant challenge: the\ninability to verify or assess due to the absence of well-defined goals, which is the prerequisite\nof reflection and re-planing. For instance, the creative task of ‘building a house’ in Minecraft lacks\nexplicit goal definitions. It is challenging for LLM agents to verify whether a house has been properly\nconstructed and reflect based on the verification. For the assess criterion of ‘Player can enter the\nhouse through doors’, there is a huge gap between high-level abstract descriptions (‘enter through\ndoors’) and environment-relevant verification actions (move((x1, y1, z1) →(x2, y2, z2))). Such a\ngap impedes agents from accurately assessing their current states and formulating practical plans.\nTo address this issue, we introduce a novel approach termed autonomous embodied verification\ntechniques, aimed at empowering open-ended agents to proficiently confirm high-level abstractions\nof assess criteria in creative building tasks. This lays the groundwork for agents to autonomously\ntackle open-ended creative tasks without well-defined goals. We draw inspiration from human design\npractices that usually progressively design and verify from the visual appearance to functionality.\nBased on such inspiration, we propose a Luban agent, which begins with a building ‘something-like’\nphase, wherein we speculatively construct 3D structural objects based on CAD (Computer-Aided\nDesign) program synthesis and perform visual verification on these objects. After passing the visual\nverification, it subsequently transits to the building ‘something-work’ phase. Luban then generates\nenvironment-relevant functionality programs on these objects for pragmatic verification. With such\nvisual and pragmatic verification, agents can summarize and reflect accordingly and iteratively\ncomplete open-ended creative tasks.\nTo evaluate the performance of Luban on open creative building tasks, we designed a benchmark\ncontaining 5 Minecraft building tasks with diverse visual and functional requirements. Multi-\ndimensional extensive human studies show that the Luban agent successfully completes all open-\nended creative building tasks, and the Elo ratings clearly show that buildings created by Luban\noutperform other baselines (33%~100%) in visualization and pragmatism. Moreover, the pass rate of\nautonomously proposed embodied verification is consistent with human functionality assessment,\ndemonstrating its effectiveness and necessity. Finally, demos on the real-world robotic arm show the\npotential of the Luban agent to perform open-ended creative tasks in the physical world.\n2\nRelated Works\nMinecraft Agents. The openness and authenticity of the Minecraft game make it an important test-\nbed for AI agents. Most existing Minecraft agents focus on tasks with a long horizon and well-defined\ngoals [20], such as collecting and crafting materials. These agents can be further categorized into\ntwo branches: control-centric and planning-centric. The control-centric agents [2, 3, 19, 35] trained\non Minecraft gameplay demos collected from the Internet to build task policies based on low-level\n2\ngame controls (e.g., mouse and keyboard action). The planning-centric agents [33, 34, 39] focus on\naligning high-level instructions with action primitives by utilizing LLM’s reasoning capabilities and\nsemantic knowledge to decompose instructions into plans. These agents often come with carefully\ndesigned memory and reflection mechanisms to ensure they can learn useful skills and take advantage\nof environmental feedback. Unlike the above works, we focus on building planning-centric creative\nagents that aim to autonomously verify the not well-defined goals of the creative tasks to ground\ncreations (ensuring pragmatism) in the environment. Compared with the pioneering attempt [37],\nit did not involve any verification and feedback mechanisms, making it incompetent in grounding\ncreations.\n3D Model Synthesis. Using computers to generate 3D models is a key research topic in computer\ngraphics. Recently, the synthesis of 3D models from given instructions (text or images) has attracted\nmore and more attention from researchers [9, 17, 21]. The methods of 3D model synthesis can be\ndivided into two categories. One category methods synthesize 3D models directly (e.g., meshes [8],\npoint cloud [24], multi-view images [27] and voxels [13]) rely on generative models [7, 10, 15, 29, 32]\nand neural representations [14, 23]. Another category of methods relies on the existing Computer-\nAided-Design (CAD) modeling software (e.g., Blender [4] and FreeCAD [22]) to first synthesize\nthe operations and parameters of the modeling process (i.e., programs) and then execute them to get\nthe 3D model. This line of work includes training-based methods [16, 36, 38] and LLM in-context\nlearning-based method [30] that emerged recently. The models synthesized using the former category\nof methods typically exhibit rich textures and details but lack complete controllability and accurate\ndimensions, whereas those synthesized using the latter demonstrate the opposite. In this work,\ngenerating accurate 3D models is crucial to the Luban agent’s planning and visual verification, so\nwe synthesize 3D models by prompting LLM to synthesize programs based on the CAD modeling\nlibrary we provided. Compared with [30], we consider CAD modeling to rely on a small number of\nlow-level (i.e., sketch-extrude-based) rather than high-level (i.e., pre-defined objects) APIs, which\nallow the creation of diverse 3D models via using API combinations and adding natural language\nannotations. Please refer to Sec. 4.1 and Appendix C for more details.\n3\nProblem Definition\nMinecraft Environment. We formalize the Minecraft environment as a Partially Observable Markov\nDecision Process (POMDP) without the reward function P = (S, A, T, Ω, O), where S is the state\nspace, A is the action space, T is the transition dynamics, Ωis the observation space (i.e., game\nimages), and O is the set of conditional observation probabilities. The action space A consists of\npre-defined action primitives, such as move, place_block, and dig_block, which return a binary\nvalue to reveal action status (success or failure).\nMinecraft Agent for Open-ended Creative Building Tasks. The open-ended creative building\ntasks can be formalized as an Instruction Following (IF) problem, where the instruction I consists of\ntwo parts: (1) Text, including Natural Language (NL) building description, functional requirements,\nand building suggestions; (2) Images, including multi-view images of a general example building\nthat aligns with the building description. The agent takes the instruction I as input and performs\na sequence of actions (a1, a2, . . . ), ai ∈A to build the building in the environment and ensures\nit meets the functional requirements (i.e., grounding creations in the environment by ensuring the\npragmatism). For example, when the instruction involves ‘build a bridge to cross a river’, the agent\nshould build a bridge-like structure in the environment and ensure it is walkable across the river.\n4\nMethod\nIn this section, we introduce the Luban agent, which can complete open-ended creative building tasks\npragmatically in the open world with the help of the two-level autonomous embodied verification: (1)\n3D structural speculation stage with the visual verification (Sec. 4.1); (2) Construction stage with the\npragmatic verification (Sec. 4.2).\n4.1\n3D Structural Speculation stage with Visual Verification\nThe goal of the 3D structural speculation stage is to design the building based on the open-end\ncreative building instruction I. Due to the large goal space of the creative building tasks, it is\n3\nFigure 2: The diagram of Luban agent. (a) The 3D structural speculation stage uses VLM to\nsynthesize Instructions I into a CAD program representing the building 3D objects, which further\nincludes decomposing, subcomponents generation, and assembling. The visual verification evaluates\nthe quality of buildings through the appearance results of the CAD program construction. (b)\nThe construction stage uses VLM to synthesize the building’s 3D object program into executable\nconstruction actions to get the building in the environment. The pragmatic verification evaluates the\nbuilding 3D object’s pragmatism by generating environment-relevant functionality annotations and\naction verify programs.\nnecessary to introduce verifications in the 3D structural speculation stage that filter out the open\nbut inappropriate designs (e.g., designs leading to semantic-less or incomplete building) to reduce\nthe space. We introduce visual verification in the 3D structural speculation stage by exploiting\nVLM’s visual understanding capabilities, thus requiring the generation of visual representations.\nConsider existing deep-learning-based visual representations synthesize techniques are inaccurate and\nuncontrollable (more discussion in Sec 2), we turn to synthesize parametrically modeled 3D models\n(i.e., synthesizing Python CAD programs based on a Python CAD library 2) in the 3D structural\nspeculation stage. Figure 2 (a) shows the 3D structural speculation stage and visual verification.\n3D structural speculation. The 3D structural speculation stage can be formalized as I\nprompt\n−→P B,\nwhere P B is a Python CAD program representing the precise 3D shape of the whole building. To fully\nexploit VLM’s 3D structural speculation and reasoning capabilities and consider the conventions of\nparametric CAD modeling, the 3D structural speculation stage is further divided into three sub-stages\nas shown in Figure 2 (a), including decomposing, subcomponent generation, and assembling. (1)\nDecomposing. The VLM takes I and necessary prompts as input and outputs a subcomponent\ndescription set S = {s1, s2, . . . } that make up the building represented by I, expressed as I\nprompt\n−→S.\nEach subcomponent si in S is represented in natural language and contains semantic, size, and\nposition information. (2) Subcomponent Generation. The sub-stage aims at synthesizing natural\nlanguage subcomponents into 3D subcomponents represented by Python CAD modeling programs\nP S, expressed as S\nprompt\n−→P S. The VLM first plans to determine the precise size and appearance\ninformation of each subcomponent. Then, it in-context learns the documents and few-shot examples\nof the CAD library we provide to synthesize the 3D subcomponents program. (3) Assembling. The\nVLM assembles the 3D subcomponents P S to the building 3D object by reasoning and setting each\nsubcomponent’s position and orientation via synthesizing building 3D object program P B, expressed\nas P S prompt\n−→P B.\n2The Python CAD library used in our work is simplified and encapsulated based on the CadQuery project,\nwhich has a small number of low-level sketch-extrude-based APIs for parametric 3D modeling. Please refer to\nthe Appendix C for more details. The CadQuery project’s link, https:\/\/github.com\/CadQuery\/cadquery.\n4\nFigure 3: The showcases of Luban’s creation on all tasks.\nVisual Verification. Visual verification aims to filter out the best from multiple modeling pro-\ngrams (i.e., 3D subcomponents and object), expressed as I, (P S\n1 , . . . , P S\nk )\nprompt\n−→i, 1 ≤i ≤k\n(or (P B\n1 , . . . , P B\nk )). Specifically, in the subcomponent generation and assembling sub-stages, we\nsample k Python CAD modeling programs (P S or P B) generated by VLM and execute them to get\ncorresponding 3D model multi-view images. Subsequently, the k multi-view images are prompted to\nVLM to evaluate the consistency of the images and the instruction I. The best program returned by\nVLM is selected, with the option to resample if no best program is found.\n4.2\nConstruction stage with the Pragmatic Verification\nConstruction. The construction stage aims to construct the building B in the environment based\non the building 3D object program P B from the 3D structural speculation stage, expressed as\nP B prompt, execute\n−→\nB. Specifically, the building 3D object program P B is first exported to the\nenvironment-level (i.e., Minecraft) coordinates. Then, the VLM is prompted with these coordinates\nand the available action primitives to synthesize the action sequence AC = (aC\n1 , aC\n2 , . . . ) (i.e., the\nJavaScript programs) for constructing the building. Finally, the construction action sequence AC is\nexecuted in the environment to construct the building B.\nPragmatic Verification. The pragmatic verification aims to reason well-defined functionality from\nthe abstract criteria in task instruction I, and further verify corresponding pragmatism of the con-\nstructed building B to get suggestions I+ for improving the next round creation, expressed as\nB\nprompt, interact\n−→\nI+. The Luban’s pragmatic verification can be divided into three sub-stages, as\nshown in Figure 2(b), including verification action generation, verify, and reflection. (1) Verifica-\ntion action generation. Based on the instruction I, the agent generates and attaches the natural\nlanguage verification annotations on the subcomponents of the building 3D object P B to generate\nenvironment-relevant functionality programs. The environment-relevant functionality programs are\nfurther synthesized into embodied verification actions (i.e., the JavaScript programs) with binary\nstatus (action success or not) by the VLM, i.e., AP = (aP\n1 , aP\n2 , . . . ), aP\ni : B →{0, 1}. (2) Verify.\nThe verification actions are further executed in the environment to interact with building B to collect\nthe action status. By analyzing the action status, the agent verifies the building pragmatism and\noutputs verification results. These two sub-stages are expressed as I, P S prompt, execute\n−→\n{0, 1}n.\n(3) Reflection. The verification results, together with instructions I and image observation o, are\nprompted to VLM for further conducting semantic level check and reflection to obtain suggestions\nI+ for the next iteration, expressed as I, o, {0, 1}n prompt\n−→I+.\n5\nExperiments\nIn this section, we first introduce the experimental settings (benchmark, baselines, and metrics) in Sec.\n5.1, then demonstrate Luban’s superiority in creation pragmatism and human preference compared\nto other method baselines and the quality of pragmatic verification in Sec. 5.2, further, show the\neffectiveness and necessity of Luban’s two-level verifications through ablation studies in Sec. 5.3,\nand finally, discuss the real-world application potential of the Luban in Sec. 5.4.\n5\n5.1\nExprimental Settings\nBenchmarking Open-ended Creative Building Tasks. We design a benchmark to test the agent’s\nability to complete open-ended creative building tasks pragmatically. The benchmark contains 5\ntasks (i.e., arrow-tower, bridge, chinese-ancient-house, stair, and two-story-house)\nwith diverse structural and functional requirements. Each task instruction consists of the text and\nmulti-view images 3. as illustrated in Sec. 3. Take the bridge task as an example. The task requires\nthe agent to build a plank bridge similar to the one in multi-view images. The functional requirements\nof the bridge task are two-fold: Environmental level, the bridge needs to cross the river; Building\nlevel, the bridge should be walkable for players, and the bridge’s handrails should prevent players\nfrom falling off. Please refer to Appendix D for more details about the benchmark and the comparison\nwith other benchmarks.\nBaselines. We implement the Luban agent with gpt-4-vision-preview. For simplicity, we\nassume that the action primitives used for construction (i.e., place_block and dig_block) and\nthe building’s position are oracles. All action primitives are implemented with Mineflayer [28]\nJavascript APIs and also adopted by the following baselines for the sake of fairness. To demonstrate\nthe superiority of the Luban agent, we compare it with the following plan-centric Minecraft agent\nbaselines: (a) Voyager agent [33], an LLM-based agent target on exploring the Minecraft world\nto follow instructions, which has 2 variants, gpt-3.5-turbo based Voyager35 and gpt-4 based\nVoyager4; (b) Creative agent [37], a gpt-4-vision-preview based agent targets creative building\ntasks without any feedback from the environment. To demonstrate the effectiveness and necessity\nof the Luban agent’s two-level verification, we consider the following baseline of ablation settings:\n(a) Luban w\/o pv, Luban agent without pragmatic verification, equivalent to plan and construction\nwithout any environmental feedback; (b) Luban w\/o vv, Luban agent without visual verification\nby replacing the VLM visual verification with a random choice; (c) Luban w\/o vvpv, Luban agent\nwithout both verification.\nMetrics. We run all the above baselines to obtain 3 seed building results (multi-view video) for\neach task. The 3 metrics are listed as follows: (1) Quality rating, similar to [37], each result is\nrated (ranging from 1 to 5) from 5 dimensions: Appearance (AP), Complexity (CO), Aesthetics (AE),\nBuilding-level Functional (FB), and Environmental-level Functional (FE). The action verification\nof pragmatic verification (in Sec. 4.2) mainly covers the FB, and the VLM semantic check followed\nby the action verification mainly covers the FE. (2) One-to-one comparison, the result pairs from\nthe same tasks and different baselines are evaluated by selecting the winner. The winning rates of\nbaselines are further used to compute the Elo [5] rating for a comprehensive comparison. (3) Pass\nrate of Luban’s pragmatic verification, we migrate the pragmatic verification actions autonomously\nproposed by the Luban agent to other baselines (by modifying some parameters of the action\nprimitives, e.g., start and end position of move) and execute the actions to calculate the pass rates for\nevaluation. Considering the openness of the task results, we launch human studies on metrics (1) and\n(2). Please refer to Appendix F for more details.\n5.2\nMain Results\nIn this section, we comprehensively compare Luban and other method baselines on the 3 metrics:\nquality rating, one-to-one comparison, and pragmatic verification pass rate. We draw 3 corresponding\nconclusions as follows:\nLuban’s creations outperform other method baselines and are pragmatic in the environment.\nAs the quality ratings shown in the polar chart of Figure 4, on all 5 tasks, Luban’s quality ratings on 3\nnon-functional dimensions significantly exceeded the baselines: AE rating increased by 1.42 to 2.93,\nCO rating increased by 1.44 to 3.22, and AP rating increased by 1.84 to 3.18. Further, Luban also\nreceives the highest ratings in the two functional dimensions, FB and FE, on all 5 tasks (average rating\n4.44 and 4.50 correspondingly, near full rating 5) and significantly outperforms the baselines of other\nmethods (rating increased by 0.80 to 3.42 and 1.67 to 3.76 correspondingly). The quality rating\nresults directly demonstrate this conclusion. Other method baselines cannot generate complicated\ncreations and get feedback from the environment, thus resulting in low-level ratings. We showcase\n3Given the building descriptions, we use the text-to-3D service of https:\/\/meshy.ai to generate general\n3D models and capture multi-view images.\n6\nFigure 4: The polar chart of multi-dimensional quality rating of creations from Luban and other\nmethod baselines. The results are grouped by tasks and averaged across all seeds and human\nevaluators with a 1-sigma bar\nTable 1: The winning rate (%) of one-to-one comparison between Luban and other method baselines\nand Elo ratings across tasks.\nTask ID\nLuban\nCreative\nVoyager35\nVoyager4\narrow-tower\n100.00\n20.00\n44.44\n35.56\nbridge\n100.00\n55.56\n1.39\n43.06\nchinese-ancient-house\n99.26\n15.56\n30.37\n54.81\nstair\n97.78\n19.26\n38.52\n44.44\ntwo-story-house\n98.52\n62.22\n6.67\n32.59\nElo rating across tasks\n2095.83\n1572.22\n1053.55\n1278.40\nLuban’s creation in Figure 3 to facilitate a more intuitive understanding. More showcases of other\nmethod baselines are shown in Figure 12 in Appendix E.1.\nLuban’s creation is more consistent with human preferences than other method baselines. As\nlisted in Table 1, on all 5 tasks, Luban achieves ∼100% one-to-one winning rate compared with\nother method baselines. The Elo rating across tasks provides a more comprehensive perspective\nto reflect the gap between baselines, where Luban outperforms the second baseline ∼500 scores,\ndirectly supporting the conclusion.\nThe pass rate of Luban’s pragmatic verification reveals the degree of creation pragmatism. As\nthe pass rates listed in Table 2 left, Luban achieves 100% verification pass rate on all 5 tasks after\nrounds of iteration and autonomous verification. In contrast, the pass rate of other method baselines\nremains low. We observe the pass rates exhibit similar trending to the quality rating FB, and we\nstatistically reveal it by computing the Spearman correlations, as listed in Table 2 right. The strong\npositive correlation (all ρ > 0.6 and p-value < 0.05) indicates that the pragmatic verification pass\nrate aligns with the human evaluator. Thus, the pass rate reveals the degree of creation pragmatism\nand can measure creative building tasks.\nTable 2: (Left) The average pragmatic verification pass rate (%) across seeds of Luban and other\nmethod baselines. (Right) The Spearman correlation (ρ and p-value) between the pass rate and quality\nratings FB.\nTask ID\nLuban\nCreative\nVoyager35\nVoyager4\nρ (p)\narrow-tower\n100.00\n33.33\n66.67\n100.00\n0.76 (0.00)\nbridge\n100.00\n22.22\n22.22\n33.33\n0.89 (0.00)\nchinese-ancient-house\n100.00\n0.00\n0.00\n0.00\n0.75 (0.00)\nstair\n100.00\n33.33\n0.00\n33.33\n0.63 (0.03)\ntwo-story-house\n100.00\n33.33\n0.00\n0.00\n0.67 (0.02)\n7\nFigure 5: The polar chart of multi-dimensional quality rating of creations from Luban and ablation\nbaselines. The results are grouped by tasks and averaged across all seeds and human evaluators with\na 1-sigma bar.\n5.3\nAblation Study\nIn this section, we ablate Luban’s visual and pragmatic verifications and draw 3 conclusions as\nfollows:\nVisual verification improves the basic quality of the creations. As the quality ratings shown in\nthe polar chart of Figure 5, on all 5 tasks, the quality ratings of baselines with visual verification\n(‘Luban’ and ‘Luban w\/o pv’) significantly outperform those without (‘Luban w\/o vv’ and ‘Luban\nw\/o vvpv’) on both functional and non-functional dimensions (rating increasing from 0.69 to 3.20).\nThe one-to-one winning rates and Elo ratings in Table 3 also exhibit similar trendings, in which\n‘Luban’ and ‘Luban w\/o pv’ are also significantly higher than ‘Luban w\/o vv’ and ‘Luban w\/o vvpv’.\nThese results directly support the conclusion, and the creation quality degraded to GPT4 levels\nwithout visual verification. The visual verification works because it filters out inappropriate Python\nCAD modeling programs to reduce the errors in subcomponent generation (e.g., missed or wrongly\ndesigned subcomponents) and assembling (e.g., incorrect subcomponent’s position and orientation)\nsub-stages by reviewing multiple programs and selecting the best.\nPragmatic verification is effective and necessary for the creation’s pragmatism. As the two\nfunctional dimension ratings (FB and FE) shown in the polar chart of Figure 5, on all 5 tasks, Luban\noutperforms ‘Luban w\/o pv’ baseline, ranging from 0.42 to 1.43 and 1.16 to 2.91 correspondingly.\nMoreover, as the pragmatic verification pass rates listed in Table 4, ‘Luban w\/o pv’ does not\nreach 100% pass on all tasks. The differences between the above functional dimension ratings and\nverification pass rate directly demonstrate this conclusion. The pragmatic verification works because\nit generates purposefully embodied actions to collect the information of creations for feedback to\nimprove the creation’s pragmatism stably. In contrast, those without pragmatic verification can rely\nsolely on VLM output’s randomness to make pragmatic creations occasionally. Additionally, we\nnotice that the verification pass rates of ‘Luban w\/o pv’ on tasks bridge and stair are 100%, which\nmay be attributed to the agreement of these Minecraft buildings and the VLM’s semantic knowledge.\nVisual verification is the prerequisite for pragmatic verification. We access the pragmatic verifica-\ntion gains on baselines with and without visual verification by computing the two functional ratings\n(FB and FE) differences in Figure 5, i.e., gain(Luban w\/o vvpv →Luban w\/o vv) = [−0.29, 1.04]\nand gain(Luban w\/o pv →Luban) = [0.42, 2.91]. The results show that larger pragmatic verifica-\ntion gains occurred in the baseline group with visual verification, which supports the conclusion. This\nis because pragmatic verification means little when the building quality is extremely low, e.g., there\nis no point in verifying that the door is passable when the house is assembled incorrectly. The results\nlisted in Table 4 support the reason, in which the two ablation baselines without visual verification\n(i.e., Luban w\/o vv’ and Luban w\/o vvpv’)’s pragmatic verification pass rate is no longer significantly\ncorrelated to the human functionality ratings (the p-values > 0.05 on all 5 tasks). More intuitive\nshowcases of the ablation baselines can be found in Figure 13 of Appendix E.1.\n8\nTable 3: The winning rate (%) of one-to-one comparison between Luban and ablation baselines and\nElo ratings across tasks.\nTask ID\nLuban\nLuban w\/o pv\nLuban w\/o vv\nLuban w\/o vvpv\narrow-tower\n98.41\n53.97\n42.86\n4.76\nbridge\n98.41\n65.08\n35.71\n0.79\nchinese-ancient-house\n83.33\n80.95\n11.11\n24.60\nstair\n82.54\n84.13\n29.37\n3.97\ntwo-story-house\n86.51\n76.98\n17.46\n19.05\nElo across tasks\n1979.48\n1753.42\n1128.74\n1138.36\nTable 4: (Left) The average pragmatic verification pass rate (%) across seeds of Luban and ablation\nbaselines. (Right) The Spearman correlation (ρ and p-value) between the pass rate of (‘Luban w\/o\nvv’, ‘Luban w\/o vvpv’) and quality ratings FB. The correlation item of arrow-tower task is ‘n\/a’\ndue to the constant pass rate.\nTask ID\nLuban w\/o pv\nLuban w\/o vv\nLuban w\/o vvpv\nρ (p)\narrow-tower\n66.67\n0.00\n0.00\nn\/a (n\/a)\nbridge\n100.00\n55.56\n55.56\n0.53 (0.28)\nchinese-ancient-house\n33.33\n33.33\n66.67\n0.60 (0.21)\nstair\n100.00\n0.00\n25.00\n0.77 (0.07)\ntwo-story-house\n33.33\n0.00\n33.33\n-0.42 (0.41)\n5.4\nPotential in Real-World Embodied Creative Tasks\nLuban has potential in real-world open-ended creative tasks rather than being limited to Minecraft-like\nvirtual worlds, which owes to the general of Luban’s planning framework (i.e., CAD modeling and\nvisual verification) and feedback mechanism (i.e., propose actions to verify not well-defined goals).\nWe demonstrate this by providing demos on two tasks (chinese-ancient-house and bridge) of\nLuban on real-world embodied robotic arm environment, as shown in Figure 6. Specifically, we\nfirst 3D print the model of subcomponents and then use the subcomponent coordinates given by the\nassembling sub-stage to drive the pick-place API-based embodied robotic arm to perform creative\nbuilding tasks. The final assembly result demonstrates the conclusion.\n6\nConclusion\nIn this work, we propose Luban, an agent capable of open-ended creative building tasks in Minecraft,\npowered by the two-level autonomous embodied visual and pragmatic verifications. Extensive human\nstudies demonstrate that Luban’s creations have higher quality (especially functional pragmatism) in\nFigure 6: The robotic demos of task chinese-ancient-house and bridge.\n9\nmultiple dimensions and are more preferred by humans than the other method baselines. Furthermore,\nLuban also shows the potential of Luban in real-world creative tasks through demos we provided on\nembodied robotic arms environment. Our work may inspire the following directions: (1) Develop\nlibraries that represent the 3D physical world to bridge VLM and the physical world, thereby\nfacilitating the emergence of embodied agents with spatial intelligence. (2) Extend Luban’s pragmatic\nverification to obtain feedback in the real world, thereby building a closed-loop, open creative agent\ngrounding in the real world.\n7\nLimitations\nWe summarize our limitations as follows: (1) Due to the lack of a memory mechanism, Luban cannot\nutilize shared knowledge between multiple tasks (e.g., universal design guidelines) and thus cannot\nlearn from the environment continuously; (2) The expensive access costs and limited capabilities of\nadvanced VLM (i.e., GPT-4V) prevent Luban from generating more refined 3D structure inference.\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel\nHo, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,\nKyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,\nKuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic\naffordances. In arXiv preprint arXiv:2204.01691, 2022.\n[2] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon\nHoughton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching\nunlabeled online videos. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\nCho, editors, Advances in Neural Information Processing Systems, 2022.\n[3] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. Groot:\nLearning to follow instructions by watching gameplay videos. In The Twelfth International\nConference on Learning Representations, 2024.\n[4] Blender Online Community. Blender - a 3d modelling and rendering package, 2018.\n[5] A.E. Elo. The USCF Rating System: Its Development, Theory, and Applications. United States\nChess Federation, 1966.\n[6] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended em-\nbodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track, 2022.\n[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139–144, 2020.\n[8] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O˘guz. 3dgen: Triplane latent\ndiffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023.\n[9] Negar Heidari and Alexandros Iosifidis. Geometric deep learning for computer-aided design: A\nsurvey, 2024.\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840–6851, 2020.\n[11] Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Instruct2act:\nMapping multi-modality instructions to robotic arm actions with large language model, 2024.\n10\n[12] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\nJonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah\nBrown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. Inner monologue: Em-\nbodied reasoning through planning with language models. In 6th Annual Conference on Robot\nLearning, 2022.\n[13] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy Mitra. Holodiffusion: Training a\n3D diffusion model using 2D images. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, 2023.\n[14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3d gaussian\nsplatting for real-time radiance field rendering. ACM Trans. Graph., 42(4), jul 2023.\n[15] Diederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n[16] Changjian Li, Hao Pan, Adrien Bousseau, and Niloy J. Mitra. Free2cad: Parsing freehand draw-\nings into cad commands. ACM Trans. Graph. (Proceedings of SIGGRAPH 2022), 41(4):93:1–\n93:16, 2022.\n[17] Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao, Jingbo Zhang, Zhihao Liang, Jing\nLiao, Yan-Pei Cao, and Ying Shan. Advances in 3d generation: A survey, 2024.\n[18] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, brian ichter, Pete Florence,\nand Andy Zeng. Code as policies: Language model programs for embodied control. In\nWorkshop on Language and Robotics at CoRL 2022, 2022.\n[19] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: A gener-\native model for text-to-behavior in minecraft (abridged version). In NeurIPS 2023 Workshop\non Goal-Conditioned Reinforcement Learning, 2023.\n[20] Haowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: A task-centric framework\nfor open-ended agent evaluation in minecraft. In Second Agent Learning in Open-Endedness\nWorkshop, 2023.\n[21] Jian Liu, Xiaoshui Huang, Tianyu Huang, Lu Chen, Yuenan Hou, Shixiang Tang, Ziwei Liu,\nWanli Ouyang, Wangmeng Zuo, Junjun Jiang, and Xianming Liu. A comprehensive survey on\n3d content generation, 2024.\n[22] J.C. Mariscal-Melgar and Pieter Hijma. Freecad for osh automated documentation, February\n2023.\n[23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi,\nand Ren Ng. Nerf: representing scenes as neural radiance fields for view synthesis. Commun.\nACM, 65(1):99–106, dec 2021.\n[24] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A\nsystem for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751,\n2022.\n[25] OpenAI. Gpt-4 technical report, 2023.\n[26] Shaohui Peng, Xingui Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang, Zikang Tian, Rui Chen,\nZidong Du, Qi Guo, Yunji Chen, and Ling Li. Self-driven grounding: Large language model\nagents with automatical language-aligned skill learning. ArXiv, abs\/2309.01352, 2023.\n[27] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using\n2d diffusion. In The Eleventh International Conference on Learning Representations, 2023.\n[28] PrismarineJS. Mineflayer: Create minecraft bots with a powerful, stable, and high level\njavascript api, also usable from python. https:\/\/github.com\/PrismarineJS\/mineflayer,\n2013.\n[29] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In\nInternational conference on machine learning, pages 1530–1538. PMLR, 2015.\n11\n[30] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould.\n3d-GPT: Procedural 3d modeling with large language models, 2024.\n[31] Hugo Touvron and Louis Martin et al. Llama 2: Open foundation and fine-tuned chat models,\n2023.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[33] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\nTransactions on Machine Learning Research, 2024.\n[34] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,\nexplain, plan and select: Interactive planning with llms enables open-world multi-task agents.\nIn Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n[35] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,\nZhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. Jarvis-1: Open-\nworld multi-task agents with memory-augmented multimodal language models. arXiv preprint\narXiv: 2311.05997, 2023.\n[36] Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, and Yasutaka\nFurukawa. Hierarchical neural coding for controllable cad model generation. In Proceedings of\nthe 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023.\n[37] Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. Creative agents: Empowering\nagents with imagination for creative tasks, 2023.\n[38] Shengdi Zhou, Tianyi Tang, and Bin Zhou. Cadparser: A learning approach of sequence\nmodeling for b-rep cad. In Edith Elkind, editor, Proceedings of the Thirty-Second International\nJoint Conference on Artificial Intelligence, IJCAI-23, pages 1804–1812. International Joint\nConferences on Artificial Intelligence Organization, 8 2023. Main Track.\n[39] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang,\nBin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in\nthe minecraft: Hierarchical agents for minecraft via large language models with text-based\nknowledge and memory, 2024.\n12\nA\nBroader Impacts\nIn this work, we propose the Luban agent for open-ended creative building tasks in Minecraft. Since\nthe Minecraft game is a virtual world, the Luban agent in Minecraft will not have any positive or\nnegative impact. In the real world, although we demonstrated Luban’s potential in performing creative\nbuilding tasks on embodied robotic arms, this part is still in the prototype stage. The future work of\nLuban should pay attention to the creation legality and the execution safety.\nB\nComputational Resources\nOur work does not require significant local computational resources (e.g., GPU or CPU resources).\nThe main computational overhead of this work comes from accessing OpenAI’s visual language\nmodel (i.e., the API costs of using gpt-4-vision-preview). We give a rough API cost here:\ncompleting experiments of all the seeds of Luban and its ablation baseline costs about 250 USD.\nC\nImplementation Details\nC.1\nThe Python CAD Modeling Library for Planning\nWe simplified and encapsulated the CaDQuery project to build the Python CAD modeling library\nfor Luban. During the subcomponent generation phase, Luban performs modeling via the APIs\nin Listing 1. To create a subcomponent, LLM first initializes a panel class (specifying its x_dim,\ny_dim, and thickness). Then, further sub_rect operation can cut a rectangular hole at a certain\nposition on the subcomponent; grow_rect operation can extrude a rectangular boss at the specified\nposition; fill_rect operation can fill a rectangular hole. When modeling a subcomponent, each\noperation can be appended with two natural language annotations: (1) Appearance annotations, which\ndescribe the style or material of the subcomponent; (2) Verification annotations, which describe the\nfunctions that the subcomponent should have. The appearance annotations affect the use of materials\nin construction actions, and the verification annotations are further used to generate actions in the\npragmatic procedural verification stage. After generating subcomponents, Luban use the APIs in\nListing 2 to assemble the subcomponents by specifying the subcomponent’s position and orientation.\nclass\nPanel(CADBaseObject ):\n’’’\nThe ‘Panel ‘ class is the only\nclass you can use in building\nsubcomponents.\nThis\nobject\nrepresents\nseveral\nblocks in Minecraft.\n’’’\ndef\n__init__(self , x_dim , y_dim , thickness , object_name =\"\npanel_default\", annotation =\"\", v_annotation =None):\n’’’\nMethod\nparameters:\nx_dim: panel x_dim ,\ny_dim: panel y_dim ,\nthickness: panel\nthickness ,\nobject_name: object name , must be unique\nbetween\ndifferent\n‘Panel ‘ object\nannotation: (Not Empty) Default\nnatural\nlanguage\nannotation\nfor panel\nobjects. The\ncontent of the\nannotation\nshould\ncontain\nthe\nfollowing\nfields: (1)\nUsage: e.g. window , (2) Look Like: e.g., transparent\n(3)\nRecommended\nblocks in Minecraft: e.g., glass.\nv_annotation: The ‘v_annotation ‘ is short for\nverification\nannotation. If the\nbuilding\nresult of the\nsubcomponent ’s building\noperation (subcomponent\ninitialization is also\nconsidered a build\noperation)\nneeds to meet some\nfunctional\nrequirements , please\nindicate it in ‘v_annotation ‘. The ‘v_annotation ‘ is\nusually a Python\ndict with the\nfollowing\ncontents:\nv_annotation = {\n13\n’type ’: \"xxx\", # The type of the v_annotation , only\nsupport 2 types: \"planner\" or \"env\". The \"env\" type\nindicates\nthat the\nverifying\nthe\nfunctional\nfeatures\nof the\nsubcomponent\nmust\ninteract\nwith the\nMinecraft\nGame\nenvironment\nthrough an agent to determine\nwhether\nit is passed. The \"planner\" type\nmeans\nthat\nverifying\nthe\nfunctional\nfeatures of the\nsubcomponent\ncan be\ndone only\nduring the\nplanning phase , without\ninteraction\nwith the\nenvironment.\n’anno ’: \"xxx\" # The\ncontent of the\nv_annotation . When the\n\"type\" is \"env\", the\ncontent of \"anno\" should be about\nhow to interact\nwith this\nsubcomponent in the\nMinecraft\ngame\nenvironment. When the \"type\" is \"\nplanner\", the\ncontent of \"anno\" should be about\nwhat\nconstraints\nthe\nsubcomponent\nshould\nsatisfy\nduring the\nplanning\nphase.\n}\nreturn\nvalue: None\nWhen\ninitializing a ‘Panel ‘ Class , you get a general\nrectangle\npanel (x_dim , y_dim , thickness) with\ndefault\ncenter\nposition at (0, 0, thickness \/ 2) (Unchangeable).\nYou can use other\nmethods in this\nclass to perform\nfurther\noperations on the\ngeneral\nrectangle\npanel.\nYou can get the name of the Panel\nobject\nwith ‘self.name ‘.\n’’’\ndef\nsub_rect(self , pos , rect_shape , sub_rect_name , v_annotation=\nNone):\n’’’\nMethod\nparameters:\npos: The center\nposition of the\nrectangle\nhole (offset\nrelative to the xy -center , i.e. (0, 0), of the panel\nobject), a tuple\nwith 2 elements (x, y).\nrect_shape: The size of the\nrectangle hole , a tuple\nwith 2\nelements (x_dim , y_dim).\nsub_rect_name: The name of the\nrectangle hole , str. The\nname must be unique\nwithin the\ncurrent ‘Panel ‘ object\nbut can be repeated\nacross\ndifferent ‘Panel ‘ objects.\nv_annotation: The ‘v_annotation ‘ is short for\nverification\nannotation. If the\nbuilding\nresult of the\nsubcomponent ’s building\noperation (subcomponent\ninitialization is also\nconsidered a build\noperation)\nneeds to meet some\nfunctional\nrequirements , please\nindicate it in ‘v_annotation ‘. The ‘v_annotation ‘ is\nusually a Python\ndict with the\nfollowing\ncontents:\nv_annotation = {\n’type ’: \"xxx\", # The type of the v_annotation , only\nsupport 2 types: \"planner\" or \"env\". The \"env\" type\nindicates\nthat the\nverifying\nthe\nfunctional\nfeatures\nof the\nsubcomponent\nmust\ninteract\nwith the\nMinecraft\nGame\nenvironment\nthrough an agent to determine\nwhether\nit is passed. The \"planner\" type\nmeans\nthat\nverifying\nthe\nfunctional\nfeatures of the\nsubcomponent\ncan be\ndone only\nduring the\nplanning phase , without\ninteraction\nwith the\nenvironment.\n’anno ’: \"xxx\" # The\ncontent of the\nv_annotation . When the\n\"type\" is \"env\", the\ncontent of \"anno\" should be about\nhow to interact\nwith this\nsubcomponent in the\nMinecraft\ngame\nenvironment. When the \"type\" is \"\nplanner\", the\ncontent of \"anno\" should be about\nwhat\nconstraints\nthe\nsubcomponent\nshould\nsatisfy\nduring the\nplanning\nphase.\n}\nreturn\nvalue: None\n14\nDig a rectangle\nhole on the panel , with\nreference\nname ‘\nsub_rect_name.‘\nWhen\nusing the ‘sub_rect ‘ method , you need to determine\nthe\nappropriate\nvalue for the ‘pos ‘ parameter\nbased on the xy -\ncenter\nposition of the ‘Panel ‘ object.\nYou can refer to the rect hole by ‘sub_rect_name ‘ in other\noperations.\n’’’\ndef\nfill_rect(self , rect_hole_name , fill_name , annotation =\"\",\nv_annotation=None):\n’’’\nMethod\nparameters:\nrect_hole_name : The name of a rectangle\nhole. Please\nmake\nsure the name\nexists (already\ncreated by another\noperation)\nfill_name: The name of the window , str. The name must be\nunique\nwithin the\ncurrent ‘Panel ‘ object but can be\nrepeated\nacross\ndifferent ‘Panel ‘ objects.\nannotation: Natural\nlanguage\nannotation\nfor the ‘fill_rect\n‘ operation\ngenerated\nblocks. The\ncontent of the\nannotation\nshould\ncontain\nthe\nfollowing\nfields: (1)\nUsage: e.g. window , (2) Look Like: e.g., transparent\n(3)\nRecommended\nblocks in Minecraft: e.g., glass.\nv_annotation: The ‘v_annotation ‘ is short for\nverification\nannotation. If the\nbuilding\nresult of the\nsubcomponent ’s building\noperation (subcomponent\ninitialization is also\nconsidered a build\noperation)\nneeds to meet some\nfunctional\nrequirements , please\nindicate it in ‘v_annotation ‘. The ‘v_annotation ‘ is\nusually a Python\ndict with the\nfollowing\ncontents:\nv_annotation = {\n’type ’: \"xxx\", # The type of the v_annotation , only\nsupport 2\ntypes: \"planner\" or \"env\". The \"env\" type\nindicates\nthat the\nverifying\nthe\nfunctional\nfeatures of the\nsubcomponent\nmust\ninteract\nwith the\nMinecraft\nGame\nenvironment\nthrough an agent\nto determine\nwhether it is passed. The \"planner\" type\nmeans\nthat\nverifying\nthe\nfunctional\nfeatures of the\nsubcomponent\ncan\nbe done only\nduring the\nplanning phase , without\ninteraction\nwith the\nenvironment.\n’anno ’: \"xxx\" # The\ncontent of the\nv_annotation . When the \"type\"\nis \"env\", the\ncontent of \"anno\" should be about how to\ninteract\nwith this\nsubcomponent in the\nMinecraft\ngame\nenvironment. When the \"type\" is \"planner\", the\ncontent of \"\nanno\" should be about\nwhat\nconstraints\nthe\nsubcomponent\nshould\nsatisfy\nduring the\nplanning\nphase.\n}\nreturn\nvalue: None\nFill a rectangle\nhole with\nblocks\nspecified by ‘annotation ‘,\nwith\nreference\nname ‘fill_name.‘\nYou do not need to create a ‘Panel ‘ object\nrepresenting\nthe\nwindow or door , just call this\nmethod (See\nExample 3).\nYou can refer to the filled\nresult by ‘fill_name ‘ in other\noperations , but the filled\nresult is generally\nnot\nreferenced.\n’’’\ndef\ngrow_rect(self , pos , rect_shape , thickness , grow_rect_name ,\nbase_rect_name , annotation =\"\", v_annotation=None):\n’’’\n15\nMethod\nparameters:\npos: The center\nposition of the\nrectangle\ncolumn (offset\nrelative to the center of the panel\nobject), a tuple\nwith 2 elements (x, y).\nrect_shape: The size of the\nrectangle hole , a tuple\nwith\n2 elements (x_dim , y_dim).\nthickness: The\nthickness of the\nrectangle\ncolumn.\ngrow_rect_name : The name of the\nrectangle\ncolumn. The\nname must be unique\nwithin the\ncurrent ‘Panel ‘ object\nbut can be repeated\nacross\ndifferent ‘Panel ‘ objects\n.\nbase_rect_name : The name of the window , str. Please\nmake\nsure the name\nexists (already\ncreated by another\noperation).\nannotation: Natural\nlanguage\nannotation\nfor the ‘\ngrow_rect ‘ operation. The\ncontent of the\nannotation\nshould\ncontain\nthe\nfollowing\nfields: (1) Usage: e.g.\nwindow , (2) Look Like: e.g., transparent\n(3)\nRecommended\nblocks in Minecraft: e.g., glass.\nv_annotation: The ‘v_annotation ‘ is short for\nverification\nannotation. If the\nbuilding\nresult of\nthe subcomponent ’s building\noperation ( subcomponent\ninitialization is also\nconsidered a build\noperation)\nneeds to meet some\nfunctional\nrequirements , please\nindicate it in ‘v_annotation ‘. The ‘v_annotation ‘ is\nusually a Python\ndict with the\nfollowing\ncontents:\nv_annotation = {\n’type ’: \"xxx\", # The type of the v_annotation , only\nsupport 2\ntypes: \"planner\" or \"env\". The \"env\" type\nindicates\nthat the\nverifying\nthe\nfunctional\nfeatures of the\nsubcomponent\nmust\ninteract\nwith the\nMinecraft\nGame\nenvironment\nthrough an agent\nto determine\nwhether it is passed. The \"planner\" type\nmeans\nthat\nverifying\nthe\nfunctional\nfeatures of the\nsubcomponent\ncan\nbe done only\nduring the\nplanning phase , without\ninteraction\nwith the\nenvironment.\n’anno ’: \"xxx\" # The\ncontent of the\nv_annotation . When the \"type\"\nis \"env\", the\ncontent of \"anno\" should be about how to\ninteract\nwith this\nsubcomponent in the\nMinecraft\ngame\nenvironment. When the \"type\" is \"planner\", the\ncontent of \"\nanno\" should be about\nwhat\nconstraints\nthe\nsubcomponent\nshould\nsatisfy\nduring the\nplanning\nphase.\n}\nreturn\nvalue: None\nGrow\/extrude a rectangle\ncolumn\nwith\nblocks\nspecified by ‘\nannotation ‘, along the\nthickness\naxis (z+ direction).\nThe\nrectangle\ncolumn (with\nreference\nname ‘grow_rect_name ‘)\nstarts\nfrom the z-axis (i.e., thickness\naxis) height of\nthe ‘base_rect_name ‘, and\nextends ‘thickness ‘ along the z-\naxis.\nFor example , when ‘base_rect_name == self.name ‘, the\nrectangle\ncolumn\nstarts\nwith ‘z = self.thickness ‘, and ends with ‘z\n= self.thickness + thickness ‘.\nThis\nmethod is often\nused to build pyramid -like\nobjects\nsuch\nas roofs , by calling\nthis\nmethod\nhierarchically .\nMore\ndetails\nand\nillustrations of this\nmethod can be found in\nthe code and\ncomments of example 4 and 5.\nYou can refer to the\nrectangle\ncolumn by ‘grow_rect_name ‘ in\nother\noperations.\n’’’\nListing 1: The Python CAD Modeling APIs for subcomponent generation.\n16\nclass\nCADAssembly(object):\ndef\n__init__(self):\n’’’\nMethod\nparameter:\nNone\nReturn\nvalue:\nNone\nNo parameters\nare\nrequired\nwhen\ncreating a new ‘CADAssembly ‘\nobject.\nA series of operations on the\nCADAssembly\nobject can\ncomplete\nthe\nassembly of sub -components.\n’’’\ndef\nadd_object(self , obj):\n’’’\nMethod\nparameter:\nobj: The\nsubcomponent\nobject to be added to the\nassembly.\nReturn\nvalue:\nNone\nThis\nmethod is used to add a new object to the\nCADAssembly (no\nreturn\nvalue). Each\nobject can only be added\nonce.\n’’’\ndef\nset_object_pos (self , obj , pos):\n’’’\nMethod\nparameter:\nobj: The object\nwhose\nposition is to be set. Please\nmake\nsure the object has been\nadded to class\nCADAssembly\nbefore\ncalling\nthis\nmethod on the object.\npos: Target\nposition. A tuple\ncontaining 3 floating\npoint\nor integer\nvalues.\nReturn\nvalue:\nNone\nThis\nmethod can set the\nposition ‘pos ‘ of the\nreference\npoint\nof the\nsubcomponent ‘obj ‘.\n’’’\ndef\nset_object_orientation (self , obj , orientation):\n’’’\nMethod\nparameter:\nobj: The object\nwhose\norientation is to be set. Please\nmake sure the object has been\nadded to class\nCADAssembly\nbefore\ncalling\nthis\nmethod on the object.\norientation:\nTarget\norientation , str.\nThere are 6 legal values , namely ‘north ‘, ‘south ‘, ‘\neast ‘, ‘west ‘, ‘up ‘ and ‘down ‘, which\nrepresent\nsetting\nthe\norientation of ‘obj ‘ to the\ncorresponding ‘orientation ‘.\nReturn\nvalue:\nNone\nThis\nmethod can set the\norientation ‘orientation ‘ of the\nobject ‘obj ‘.\n’’’\n17\nListing 2: The Python CAD Modeling APIs for assembling.\nD\nBenchmark Details\nD.1\nComparisons with Other Benchmarks Involving Creative Tasks\nThe concept of creative tasks has also been introduced in other work on Minecraft benchmarks,\nincluding MineDojo [6], Minecraft SkillForge [3], and Creative Agent [37]. Our work focuses\non building open-ended creative building agents that can fill the gap between abstract criteria and\nconcrete verification actions via autonomous embodied verification. From this perspective, none of\nthe above three benchmarks are applicable to this task and the reasons are listed as follows: (1) The\nMinedojo benchmark consists of creative exploration of the Minecraft world, which is inconsistent\nwith the focus of this work: building. (2) The Minecraft SkillForge benchmark creation tasks are\nvery simple, involve only a few blocks (< 10), and do not have any criteria regarding functionality,\nso it is not suitable for this work. (3) The creative tasks in the benchmark of Creative agent work are\nof comparable complexity to our work but lack functional criteria.\nD.2\nTask Illustrations\nThere are 5 tasks in the benchmark. Each task’s instructions consist of two parts: (1) Text contains\nthe building description, the functional requirements, and some building suggestions. (2) Multi-view\nimages of example buildings that match the building description, generated by a generic text-to-3D\nservice (we use meshy.ai in this work). The 5 task instructions in the benchmark are listed below:\n1. Task arrow-tower, text (Listing 3), image (Figure 7).\nBuilding: A simple\nsolid\narrow\ntower\nsimilar to the tower in images.\nThe arrow\ntower has an external\nladder to the top of the tower.\nBuilding\nSuggestions:\nYou should\nbuild the arrow\ntower by stacking\nlayer by layer.\nThe whole\nladder and tower\nmust\nabove the ground\nplane.\nDo not\ngenerate\nmultiple\nsubcomponents\nrepresenting\nthe ladder ,\nyou should\nmerge\nthese\nsubcomponents\ninto one\nsubcomponent.\nFunctional\nRequirements: I can climb to the top of the tower\nthrough\nthe ladder\nexternal. The arrow\ntower\nmust\nslightly\nhigher\nthan the\ncolumn in Minecraft\nGame I gave.\nKey Design\nParameters: The height of the arrow\ntower. The height of\nthe arrow -tower\ndepends on the height of the column. If the arrow\ntower\ndoes not higher\nthan the column , try\nincreasing\nthe height\nof the arrow\ntower.\nListing 3: The text part of the arrow-tower task.\n2. Task bridge, text (Listing 4), image (Figure 8).\nBuilding: A simple east -west\nplank\nbridge\nsimilar to the bridge in\nimages.\nBuilding\nSuggestions: Do not create any\nsubcomponents\nrepresenting\nthe\nbridge ’s support\nand omit\nunnecessary\ndecorative\nsubcomponents\nand\nelements. Assume the water\nsurface of the river is the level\nof the ground.\nFunctional\nRequirements: The bridge\nshould\nconnect\nthe east and west\nbanks of the river for\nplayers to cross.\n18\nFigure 7: The multi-view images part of the arrow-tower task.\nKey Design\nParameters: The length of bridge. The length of the bridge\ndepends on the width of the river , if the bridge\ncannot\nspan the\nriver , try\nincreasing\nthe length of the bridge.\nListing 4: The text part of the bridge task.\nFigure 8: The multi-view images part of the bridge task.\n3. Task chinese-ancient-house, text (Listing 5), image (Figure 9).\nBuilding: A simple\nChinese\nancient\nhouse\nsimilar to the house in\nimages.\nBuilding\nSuggestions: Do not create any\nsubcomponents\nrepresenting\nthe\nhouse ’s foundation or interior\nfloor. The house\nshould not be\nlarger\nthan 10x10 blocks.\nFunctional\nRequirements: This\nhouse\nshould\nhave a door\nthrough\nwhich\nthe player can enter the\ninterior of the house.\nKey Design\nParameters: The size and\nposition of the door. If the\nplayer\ncannot\nenter the house , try\nadjusting\nthe size and\nposition\nof the door.\nListing 5: The text part of the chinese-ancient-house task.\n4. Task stair, text (Listing 6), image (Figure 10).\nBuilding: A simple\nstair\nsimilar to the stair in images.\nBuilding\nSuggestions: You can build it layer by layer.\nFunctional\nRequirements: The stair\nshould\nhigh\nenough for\nplayers to\nclimb the cliff.\n19\nFigure 9: The multi-view images part of the chinese-ancient-house task.\nKey Design\nParameters: The height of the stair. The height of the\nstair\ndepends on the height of the cliff. If the stair\ncannot\nhelp\nthe\nplayers\nclimb the cliff , try\nincreasing\nthe height of the\nstair.\nListing 6: The text part of the stair task.\nFigure 10: The multi-view images part of the stair task.\n5. Task two-story-house, text (Listing 7), image (Figure 11).\nBuilding: A simple two story\nhouse (the ground\nfloor and the first\nfloor) with flat roof\nsimilar to the house in images.\nBuilding\nSuggestions: Do not create any\nsubcomponents\nrepresenting\nthe\nhouse ’s foundation or interior\nfloor. The house\nshould not be\nlarger\nthan (wide x length) 10x10 blocks.\nFunctional\nRequirements: This\nhouse\nshould\nhave a door\nthrough\nwhich\nthe player can enter\ninterior of the house ’s ground\nfloor.\nKey Design\nParameters: The size and\nposition of the door. If the\nplayer\ncannot\nenter\ninterior of the house ’s ground floor , try\nadjusting\nthe size and\nposition of the door.\nListing 7: The text part of the two-story-house task.\nFigure 11: The multi-view images part of the two-story-house task.\n20\nFigure 12: The showcases (from one of the three seeds) of Luban vs other baselines.\nE\nAdditional Experiment Results\nE.1\nShowcases\nWe present showcases of Luban v.s. other baselines: (1) Figure 12 shows the building result\ncomparisons between Luban and other method baselines (2) Figure 13 shows the building result\ncomparisons between Luban and ablation baselines.\nE.2\nCase Study\nWe present a case study of Luban when completing the chinese-ancient-house task, which takes\ntwo iterations (as shown in Figure 14 and Figure 15). In the first iteration, Luban plans and builds a\nhouse. However, the door of the house does not start from the ground, so the door is not pragmatic\nin the environment. Luban discovers this error through pragmatic verification, whose autonomous\n21\nFigure 13: The showcases (from one of the three seeds) of Luban vs ablation baselines.\n22\nFigure 14: The case study of the chinese-ancient-house task (the first iteration).\nverification actions are proposed by Luban based on the task instructions. After reflection, Luban\nsuccessfully fixes the error and builds a pragmatic house in the second iteration.\n23\nFigure 15: The case study of the chinese-ancient-house task (the second iteration).\n24\nTable 5: Minecraft experience statistics table for human evaluators.\nGame Hours\nnever\n(0, 5]\n(5, 20]\n≥20\nCount\n1\n3\n2\n9\nF\nHuman Study\nF.1\nParticipants\nWe recruited 15 human evaluators to evaluate Minecraft creative build results. The Minecraft gaming\nexperience of these human evaluators ranges from ‘never played at all’ to ‘playing time ≥20 hours’,\nand the distribution is shown in Table 5. Each evaluator is asked to conduct two evaluations on all\nfive tasks’ results: (1) multi-dimensional quality ratings; (2) one-to-one comparison. Each evaluator\nspent a total of 80 to 120 minutes on the two evaluations. We paid an average of ∼8.80 USD per\nhour - a standard human evaluator’s wage in our region.\nF.2\nQuestionnaire and Interface\nMulti-dimensional quality ratings.\nThis part requires human evaluators to perform multi-\ndimensional rating on 7 × 5 × 3 = 105 (7 baselines, 5 tasks, and 3 seeds per baseline) 8-second\nmulti-view videos of the building. Among the tasks in the benchmark, three tasks (i.e., arrow-tower,\nbridge, and stair) have 5-dimensional ratings, and two tasks (chinese-ancient-house and\ntwo-story-house) have 4-dimensional ratings (without functional enviromnent), and the score\nrange of each dimension ranges from 1 to 5. The questionnaires were grouped by task, and results\nfrom different baselines were anonymously shuffled. When conducting the evalution, each evaluator\nwas presented with 3 materials: (1) the current task’s instruction (Please refer more details in Ap-\npendix D); (2) a questionnaire to collect ratings (Listing 8); (3) a local web page showing seeds from\nall baselines of the current task (Figure 16).\nThe\nquestionnaire\nfor task\nbridge.\n1. Ratings of result (1).\n- Appearance: The extent to which the\nbuilding\nconforms to the\nsemantics\nand\nappearance of the text and multi -view\nimages\ndescriptions in the task\ninstructions. Your\nrating\nhere: <TODO\n, answering an integer\nvalue\nranges 1 to 5>.\n- Complexity: The\ncomplexity of the\nbuilding (e.g. structural \/\ndesign\ndetails). Your\nrating\nhere: <TODO , answering an integer\nvalue\nranges 1 to 5>.\n- Aesthetics: The extent to which the\nbuilding is aesthetic. Your\nrating\nhere: <TODO , answering an integer\nvalue\nranges 1 to 5>.\n- Functional\nbuilding -level: To what\nextent\ndoes this\nbuilding\nmeet the\nfunctional\nrequirements\nbuilding -level (i.e., The\nbridge\ndeck is walkable\nfor\nplayers\nand the\nhandrails\nprevent\nplayers\nfrom\nfalling). Your\nrating\nhere: <TODO , answering an\ninteger\nvalue\nranges 1 to 5>.\n- Functional\nenviromnent -level: To what\nextent\ndoes this\nbuilding\nmeet the\nfunctional\nrequirements\nenviromnent -level (i.e., The\nbridge\nallows\nplayers\nacross the river). Your\nrating\nhere: <\nTODO , answering an integer\nvalue\nranges 1 to 5>.\n2. Ratings of result (2).\n...\n25\nListing 8: The questionnaire to collect ratings (take the bridge task as an example). The descriptions\nof two functional correctness items in the questionnaire changed depending on the task.\nFigure 16: The local web page presents videos for multi-dimensional quality ratings (take the bridge\ntask as an example)\nOne-to-one comparison. This part requires human evaluators to perform one-to-one comparison of\nthe building results. Considering the high labor cost brought by pair-wise comparison, we split the\none-to-one comparison into two parts: (1) comparisons between Luban and other method baselines\n(involving 4 baselines); (2) comparisons between Luban and ablation baselines (involving 4 baselines).\nUnder this splitting setup, each part still requires 5 ×\n\u00004\n2\n\u0001\n× 32 = 270 (5 tasks, 2 out of 4 baselines are\nselected, and (num_seed = 3)2 different matches) comparisons. To further reduce labor costs, we\nrandomly sample k = 3 mathces in any two baselines, so there are a total of 5 ×\n\u00004\n2\n\u0001\n× k = 30k = 90\ncomparisons in each part. The two part questionnaires were also grouped by task, and results from\ndifferent baselines were anonymously shuffled. When conducting the evalution, each evaluator was\npresented with 3 materials: (1) the current task’s instruction (Please refer more details in Appendix\nD); (2) a questionnaire to collect the winner (Listing 9); (3) a local web page showing each one-to-one\ncomparison of the current task (Figure 17).\nThe\nquestionnaire\nfor task\nbridge.\n1. The winner of the\ncomparison\n(1).\n- Based on the\ninstruction of the bridge task ,\nwhich one (A or B)\nis better\noverall? Your\nanswer\nhere: <TODO , answering A or B\n>.\n2. The winner of the\ncomparison\n(2).\n...\nListing 9: The questionnaire to collect one-to-one comparison winners (take the bridge task as an\nexample).\n26\nFigure 17: The local web page for presenting one-to-one comparison video pairs (take the bridge\ntask as an example)\n27\n","pdf_path":"tools\/arxiv_tools\/asset\/pdf\/\/Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification.pdf"}
